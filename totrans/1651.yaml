- en: 'Pipeline Dreams: Automating ML Training on AWS'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pipeline-dreams-automating-ml-training-on-aws-8e90a33061fd](https://towardsdatascience.com/pipeline-dreams-automating-ml-training-on-aws-8e90a33061fd)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@raicik.zach?source=post_page-----8e90a33061fd--------------------------------)[![Zachary
    Raicik](../Images/860760b53fcc75013007067190e8ca65.png)](https://medium.com/@raicik.zach?source=post_page-----8e90a33061fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8e90a33061fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8e90a33061fd--------------------------------)
    [Zachary Raicik](https://medium.com/@raicik.zach?source=post_page-----8e90a33061fd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8e90a33061fd--------------------------------)
    ·11 min read·Oct 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c696ca663854fee564e07b9749427f82.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Arnold Francisca](https://unsplash.com/@clark_fransa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In the world of machine learning, automated training pipelines streamline the
    journey from data to insight. They automate various parts of the machine learning
    life cycle such as data ingestion, preprocessing, model training, evaluation and
    deployment. Amazon Web Services (“AWS”) provides various tools to develop an automated
    training pipeline. In this article, we will walk through setting up a basic automated
    training pipeline using the classic iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting the Stage: Requirements and AWS Toolkit'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover some high level requirements as well as a brief
    overview of the AWS tools we will use.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you choose to follow along by building your own training pipeline, you will
    need the following.
  prefs: []
  type: TYPE_NORMAL
- en: An active AWS account (you can sign up [here](https://aws.amazon.com/free/?trk=78b916d7-7c94-4cab-98d9-0ce5e648dd5f&sc_channel=ps&ef_id=CjwKCAjws9ipBhB1EiwAccEi1K7ugfy2KiyndD6pN_A9_pVTNfC-K3Xp41WNOcLEEDoPj1Lyu_7WHRoCtjEQAvD_BwE%3AG%3As&s_kwcid=AL%214422%213%21432339156165%21e%21%21g%21%21aws+account%219572385111%21102212379047))
    with Administrator Access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic knowledge of **AWS CLI (**We will explore alternatives to the AWS CLI
    in future posts)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up your AWS account and connecting to AWS via the CLI is beyond the
    scope of this post, however- feel free to reach out directly if you need help.
  prefs: []
  type: TYPE_NORMAL
- en: Toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting up the automated training pipeline will require the use of the following
    AWS products.
  prefs: []
  type: TYPE_NORMAL
- en: '**S3**: Scalable object storage service designed to store and retrieve any
    amount of data from anywhere on the web'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lambda**: Serverless compute service that automatically runs your code in
    response to events, such as changes to data in an Amazon S3 bucket'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker:** Docker is a platform that packages, distributes, and manages applications
    inside lightweight, portable containers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sagemaker**: Fully managed service that provides developers and data scientists
    with the ability to build, train, and deploy machine learning models quickly and
    easily'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step Functions**: Serverless workflow service that lets you coordinate distributed
    applications and microservices using visual workflows, enabling you to build,
    run, and visualize complex processes at scale'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Automated Training Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assuming you made it past the requirements, we can switch our focus to building
    our automated training pipeline. For this simple example, we will focus on the
    following steps.
  prefs: []
  type: TYPE_NORMAL
- en: Create AWS S3 bucket to store data and artifacts related to our training pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create AWS Lambda function data ingestion, preprocessing, and training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an AWS Step Functions state machine to orchestrate the execution of your
    pipeline stages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We take a deep dive into each of these steps below.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. S3 Bucket
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing we will do is create a new S3 bucket to store data and artifacts
    using the AWS CLI.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If this command runs successfully, it should output something like this in your
    terminal.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Taking a look at our management console, we can see the S3 bucket was succesfully
    created.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4be38174ded05e2d492ad20c7e6c28f.png)'
  prefs: []
  type: TYPE_IMG
- en: 2\. Lambda Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For our automated training pipeline, we will rely heavily on the use of lambda
    functions to execute and trigger certain parts of our process. Specifically, we
    will use lambdas for:'
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2a. Data Ingestion***'
  prefs: []
  type: TYPE_NORMAL
- en: Lambda functions require lambda handlers. The Lambda handler is a user-defined
    function in a Lambda deployment package that the AWS Lambda service can invoke
    when the service executes the Lambda function. The handler function receives and
    processes the event data from the invoker.
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, let’s define our `lambda_handler` for data ingestion. The
    `lambda_handler` function serves as the entry point for AWS Lambda to execute
    the code, which in this case retrieves a CSV file from a specified URL and uploads
    it to an Amazon S3 bucket. This file has been stored in `./src/data`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will package and deploy your AWS Lambda function as a container image using
    docker. To do so, we first need to create an ECR repository to house our image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our ECR repository stood up, we create our `Dockerfile` in
    the root of our project directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We use this Dockerfile to build our image and push it to the ECR registry.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We need to authenticate Docker to the ECR registry.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, we tag the image to match the repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can push our image to the ECR registry.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we will create the lambda function and attach the container.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You will now be able to see the lambda function in your AWS management console.
    I can invoke the function and confirm the file was downloaded to the S3 bucket
    correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '***2b. Preprocessing***'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have the data we need for our modeling task. For most machine learning
    tasks, we will want to process that data in some way. In the file below, we define
    a script that will process our Iris data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We upload our preprocessing script to S3\. Run this command in the same directory
    as `preprocess.py`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Similar to how we built an image for the data ingestion step, we will build
    an image for the preprocessing step. Like before, let’s start by creating our
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our ECR repository stood up, we create our `Dockerfile` in
    the root of our project directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We build the image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Let’s authenticate Docker to the ECR registry. You might not have to do this
    if your access hasn’t expired.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Again, we tag the image to match the repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can push our image to the ECR registry.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Like before, we will create a lambda function that will kick off a SageMaker
    processing job. We define our lambda handler below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We zip up our Lambda handler and store in S3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we create our lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Since we haven’t strung the Lambda functions together yet, we can manually invoke
    the lambda and check the contents of S3 to ensure our training and testing datasets
    are there.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '***2c. Model training***'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of creating our own image for train, we will use a built SageMaker image
    for logistic regression. You can create your own image if you want a customized
    training process. Consequently, we write our lambda handler right away.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Before we can create our lambda function, we have to zip up our handler and
    send it to S3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now we can create our lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Again, we can manually invoke our training lambda to ensure it works correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Step Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we have a very basic pipeline consisting of data ingestion, preprocessing,
    and model training. However, each of these components exist on their own — how
    do we string them together to create an automated pipeline. Step functions!
  prefs: []
  type: TYPE_NORMAL
- en: To string our lambda functions together, the first thing we will do is define
    a state machine using Amazon States Language and save it in a `.json` file so
    we can push it to AWS using the CLI. For our purposes, we will define our state
    machine in the following way.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We can deploy our step function to AWS.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Once the state machine has been created, we can visualize it in the management
    console.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b434ffa1f6a9df02f39e5a150c01294c.png)'
  prefs: []
  type: TYPE_IMG
- en: We can manually invoke the pipeline as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Additional Considerations & Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pipeline we built in this example is extremely basic — we don’t even consider
    model evaluation! In a real world setting, this approach can be expanded to cover
    additional parts of the model building lifecycle including model evaluation &
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we built our pipeline but we have to trigger it manually. In
    the real world, you can trigger the execution of a Step Functions state machine
    based on various events. Using triggers provides a more event-driven architecture.
    Here are some common ways to set up triggers for your state machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon S3 Events**: If you want to run your state machine when a new file
    is uploaded to an S3 bucket, you can set up an event within the bucket to trigger
    a Lambda function, which in turn starts the execution of your state machine.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**AWS CodeCommit**: If you’re using AWS CodeCommit as your repository, you
    can use AWS Lambda and Amazon CloudWatch Events to trigger the state machine whenever
    there’s a new commit.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**GitHub or Other Repositories**: If you’re using a service like GitHub, you
    can use webhooks to notify an AWS service of a new commit or a pull request merge.
    Typically, you’d set up an API Gateway endpoint to receive the webhook, trigger
    a Lambda function from the API Gateway call, and then have the Lambda function
    start your state machine.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Amazon DynamoDB Streams**: If you want to trigger your state machine based
    on changes in a DynamoDB table, you can use DynamoDB Streams. Whenever there’s
    a change in the table, the stream can trigger a Lambda function, which then starts
    the state machine.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**AWS EventBridge (previously CloudWatch Events)**: AWS EventBridge allows
    you to create rules based on a wide range of AWS service events. You can target
    a Lambda function with these rules. Then, like in other scenarios, the Lambda
    function would start your state machine.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In conclusion, an automated training pipeline in the context of machine learning
    and data science offers multiple advantages. Here’s a concise list of pros for
    your conclusion:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency and Reproducibility**: Automation ensures that the training process
    remains consistent across different runs. This aids in reproducing results and
    eliminates variations due to manual interventions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Efficiency**: Automated pipelines can streamline processes, reducing the
    time required for training and retraining models. This can be especially beneficial
    when iterating over different model architectures or parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Scalability**: As your data or model complexity grows, an automated pipeline
    can scale up resources and processes without the need for manual oversight.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Resource Optimization**: Automation can manage resources more effectively,
    potentially leading to cost savings. For instance, cloud resources can be automatically
    scaled down when not in use.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feel free to reach out with any questions!
  prefs: []
  type: TYPE_NORMAL
