- en: Linear Regression In Depth (Part 2)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入解析线性回归（第二部分）
- en: 原文：[https://towardsdatascience.com/linear-regression-in-depth-part-2-5d40fd19efd4](https://towardsdatascience.com/linear-regression-in-depth-part-2-5d40fd19efd4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/linear-regression-in-depth-part-2-5d40fd19efd4](https://towardsdatascience.com/linear-regression-in-depth-part-2-5d40fd19efd4)
- en: Deep dive into multiple linear regression with examples in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨多重线性回归及其在 Python 中的示例
- en: '[](https://medium.com/@roiyeho?source=post_page-----5d40fd19efd4--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----5d40fd19efd4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d40fd19efd4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d40fd19efd4--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----5d40fd19efd4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----5d40fd19efd4--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----5d40fd19efd4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d40fd19efd4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d40fd19efd4--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----5d40fd19efd4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d40fd19efd4--------------------------------)
    ·14 min read·Apr 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d40fd19efd4--------------------------------)
    ·14 分钟阅读·2023年4月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/89c03a1c4491d8f8e9ce98926d1fbb9e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89c03a1c4491d8f8e9ce98926d1fbb9e.png)'
- en: Photo by [ThisisEngineering RAEng](https://unsplash.com/@thisisengineering?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/GzDrm7SYQ0g?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[ThisisEngineering RAEng](https://unsplash.com/@thisisengineering?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    在 [Unsplash](https://unsplash.com/photos/GzDrm7SYQ0g?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: In the [first part](https://medium.com/towards-data-science/linear-regression-in-depth-part-1-485f997fd611)
    of this article we formally defined the linear regression problem and showed how
    to solve simple linear regression problems, where the data set contains only one
    feature. In the second part of the article, we will discuss multiple linear regression
    problems, where the data set may contain any number of features.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章的 [第一部分](https://medium.com/towards-data-science/linear-regression-in-depth-part-1-485f997fd611)
    中，我们正式定义了线性回归问题，并展示了如何解决简单线性回归问题，即数据集仅包含一个特征。在文章的第二部分，我们将讨论多重线性回归问题，其中数据集可能包含任意数量的特征。
- en: We will first generalize the closed-form solution we have found for simple linear
    regression to any number of features. Then we will suggest an alternative approach
    for solving linear regression problems that is based on gradient descent, and
    discuss the pros and cons of this approach vs. using the closed-form solution.
    In addition, we will explore the classes in Scikit-Learn that implement both approaches,
    and demonstrate how to use them on a real-world data set.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先将简单线性回归的封闭形式解推广到任意数量的特征。然后，我们将建议一种基于梯度下降的解决线性回归问题的替代方法，并讨论这种方法与使用封闭形式解的利弊。此外，我们将探讨
    Scikit-Learn 中实现这两种方法的类，并演示如何在实际数据集上使用它们。
- en: Multiple Linear Regression Definition
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多重线性回归定义
- en: 'Recall that in regression problems we are given a set of *n* labeled examples:
    *D* = {(**x**₁, *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ* is
    an *m*-dimensional vector containing the **features** of example *i,* and *yᵢ*
    is a real value that represents the **label** of that example.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下，在回归问题中，我们会给出一组 *n* 个标记示例： *D* = {(**x**₁, *y*₁), (**x**₂, *y*₂), … , (**x**ₙ,
    *y*ₙ)}, 其中 **x**ᵢ 是一个 *m* 维向量，包含示例 *i* 的 **特征**，*yᵢ* 是一个实值，表示该示例的 **标签**。
- en: 'In **linear regression** problems, we assume that there is a linear relationship
    between the feature vector **x** and the label *y*, so our model hypothesis takes
    the following form:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **线性回归** 问题中，我们假设特征向量 **x** 和标签 *y* 之间存在线性关系，因此我们的模型假设采取以下形式：
- en: '![](../Images/87768db1046e6151ab03d8383811426b.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87768db1046e6151ab03d8383811426b.png)'
- en: The linear regression model hypothesis
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型假设
- en: 'Our goal is to find the parameters **w** of this model that will minimize the
    sum of squared residuals:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到该模型的参数 **w**，以最小化平方残差的总和：
- en: '![](../Images/cc275f18bbc9a318345bd53dde5c8f8a.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc275f18bbc9a318345bd53dde5c8f8a.png)'
- en: The least squares cost function
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘成本函数
- en: In the previous part of the article, we have shown how to find the optimal **w**
    for the case of *m* = 1 using the **normal equations**. We will now extend these
    equations for any number of features *m*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在文章的前面部分，我们展示了如何使用**正规方程**找到当*m* = 1时的最佳**w**。现在我们将扩展这些方程以适应任意数量的特征*m*。
- en: 'To simplify the derivation of the normal equations for the general case, we
    first define a matrix *X* that contains the values of all the features in the
    data set, including the intercept terms:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化正规方程在一般情况下的推导，我们首先定义一个矩阵*X*，它包含数据集中所有特征的值，包括截距项：
- en: '![](../Images/556ac465f5a256b7559bf19ff400e054.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/556ac465f5a256b7559bf19ff400e054.png)'
- en: The design matrix
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 设计矩阵
- en: This matrix is called the **design matrix**. Each row in the design matrix represents
    an individual sample, and the columns represent the explanatory variables. The
    dimensions of the matrix are *n* × (*m* + 1), where *n* is the number of samples
    and *m* is the number of features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵称为**设计矩阵**。设计矩阵中的每一行表示一个单独的样本，列代表解释变量。矩阵的维度为*n* × (*m* + 1)，其中*n*是样本数量，*m*是特征数量。
- en: 'In addition, we define the vector **y** as an *n*-dimensional vector that contains
    all the target values:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将向量**y**定义为一个包含所有目标值的*n*维向量：
- en: '![](../Images/903a71637769932a07d8236fc908fa62.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/903a71637769932a07d8236fc908fa62.png)'
- en: The target vector
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 目标向量
- en: 'These definitions allow us to write the least squares cost function in the
    following matrix form:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些定义使我们能够以以下矩阵形式写出最小二乘成本函数：
- en: '![](../Images/15bfe63991293e9ddc4bbee52636c1ef.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15bfe63991293e9ddc4bbee52636c1ef.png)'
- en: The OLS cost function in matrix form
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵形式的OLS成本函数
- en: '**Proof**:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明**：'
- en: 'We first note that:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先注意到：
- en: '![](../Images/bbfe8ea2eabf117235b436b728c73a94.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbfe8ea2eabf117235b436b728c73a94.png)'
- en: 'The dot product of a vector with itself **u***ᵗ***u** is just the sum of all
    its components squared, therefore we have:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 向量与自身的点积**u**ᵗ**u**只是其所有分量平方的总和，因此我们有：
- en: '![](../Images/8fd43bfe980b656fdfca69650dbaa0ce.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fd43bfe980b656fdfca69650dbaa0ce.png)'
- en: Closed-Form Solution
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 封闭解
- en: As was the case in simple linear regression, the function *J*(**w**) is convex,
    hence it has a single local minimum, which is also the global minimum. In order
    to find this global minimum, we need to compute the gradient of *J*(**w**) with
    respect to **w** and set it to zero.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与简单线性回归的情况一样，函数*J*(**w**)是凸的，因此它有一个唯一的局部最小值，这也是全局最小值。为了找到这个全局最小值，我们需要计算*J*(**w**)对**w**的梯度并将其设为零。
- en: 'The gradient of *J*(**w**) with respect to **w** is:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*J*(**w**)对**w**的梯度是：'
- en: '![](../Images/ea760389b8e2c2ea201c87bb36b8f186.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea760389b8e2c2ea201c87bb36b8f186.png)'
- en: 'During this proof we have used some basic rules of matrix calculus, which are
    explained in this article:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在此证明过程中，我们使用了一些矩阵微积分的基本规则，这些规则在本文中有解释：
- en: '[](https://medium.com/@roiyeho/a-gentle-introduction-to-matrix-calculus-14584f2c4f60?source=post_page-----5d40fd19efd4--------------------------------)
    [## A Gentle Introduction to Matrix Calculus'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho/a-gentle-introduction-to-matrix-calculus-14584f2c4f60?source=post_page-----5d40fd19efd4--------------------------------)
    [## 矩阵微积分的温和介绍'
- en: Including Applications in Machine Learning
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 包括在机器学习中的应用
- en: medium.com](https://medium.com/@roiyeho/a-gentle-introduction-to-matrix-calculus-14584f2c4f60?source=post_page-----5d40fd19efd4--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@roiyeho/a-gentle-introduction-to-matrix-calculus-14584f2c4f60?source=post_page-----5d40fd19efd4--------------------------------)
- en: 'We now set this gradient to zero in order to obtain the normal equations:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将这个梯度设为零以得到正规方程：
- en: '![](../Images/f63cb9a0a0ad01574e74cd4facdb881f.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f63cb9a0a0ad01574e74cd4facdb881f.png)'
- en: 'Therefore, the optimal **w*** that minimizes the least squares cost function
    is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最小化最小二乘成本函数的最佳**w**是：
- en: '![](../Images/da030c54924b6a62658f99f29720d4de.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da030c54924b6a62658f99f29720d4de.png)'
- en: The closed-form solution to ordinary least squares
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 普通最小二乘的封闭解
- en: Note that we assumed here that the columns of *X* are linearly independent (i.e.,
    *X* has a **full column rank**), otherwise *XᵗX* is not invertible, and there
    is no unique solution for **w***.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里假设*X*的列是线性独立的（即*X*具有**满列秩**），否则*XᵗX*是不可逆的，并且**w**没有唯一解。
- en: 'When the columns of *X* are linearly dependent, we call this phenomenon **multicollinearity**.
    Mathematically, a set of variables is **perfectly multicollinear** if for all
    samples *i*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当*X*的列线性相关时，我们称这种现象为**多重共线性**。在数学上，如果对于所有样本*i*，一组变量是**完全多重共线**的：
- en: '![](../Images/7e1c0f53726e66ba3045a751f042343c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e1c0f53726e66ba3045a751f042343c.png)'
- en: Perfect multicollinearity
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 完全多重共线性
- en: where *λₖ* are constants, and *xᵢₖ* is the value of feature *k* in sample *i*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*λₖ*是常数，*xᵢₖ*是样本*i*中特征*k*的值。
- en: In practice, perfect multicollinearity is rare (e.g., it can be caused by accidentally
    duplicating a variable in the data). However, even lesser degrees of multicollinearity,
    where two or more features are highly correlated with each other (but not perfectly
    correlated), can cause issues both when fitting the model (the coefficients become
    very sensitive to small changes in the data) and when interpreting the results
    (it becomes hard to identify which features have the most impact on the model’s
    predictions).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，完全多重共线性很少见（例如，它可能是由于不小心重复了数据中的一个变量引起的）。然而，即使是较低程度的多重共线性，即两个或更多特征之间高度相关（但不是完全相关），也会在拟合模型时（系数对数据中的小变化非常敏感）和解释结果时（很难识别哪些特征对模型预测的影响最大）造成问题。
- en: The interested reader can find more info about the multicollinearity problem
    and how to handle it in this [Wikipedia article](https://en.wikipedia.org/wiki/Multicollinearity).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的读者可以在这个[维基百科条目](https://en.wikipedia.org/wiki/Multicollinearity)中找到有关多重共线性问题及其处理方法的更多信息。
- en: Multiple Linear Regression Example
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多重线性回归示例
- en: To demonstrate the usage of the closed-form solution, let’s build a linear regression
    model for the [California housing data set](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing),
    available from the sklearn.datasets module. The goal in this data set is to predict
    the median house value of a given district (house block) in California, based
    on 8 different features of that district, such as the median income or the average
    number of rooms per household.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示闭式解的使用，让我们为[加州住房数据集](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing)构建一个线性回归模型，该数据集可从sklearn.datasets模块获取。该数据集的目标是基于该区域的8个不同特征（如中位收入或每户的平均房间数）预测加州某个地区（房屋区块）的中位房价。
- en: 'We first import the required libraries, and initialize the random seed in order
    to have reproducible results:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入所需的库，并初始化随机种子，以便获得可重复的结果：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we fetch the data set:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们获取数据集：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To explore the data set, we merge the features (X) and the labels (y) into
    a Pandas DataFrame, and display the first rows from the table:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索数据集，我们将特征（X）和标签（y）合并到一个Pandas DataFrame中，并显示表格的前几行：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/cf3a1f1da39af8d9d5961f8296b6516b.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf3a1f1da39af8d9d5961f8296b6516b.png)'
- en: The first five rows from the California housing dataset
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 加州住房数据集的前五行
- en: 'We can further investigate the data set by calling the DataFrame’s [info()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html)
    method, which provides information about the type of the columns and whether they
    contain any missing values:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用DataFrame的[info()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html)方法进一步调查数据集，该方法提供有关列的类型以及是否包含任何缺失值的信息：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/3a133f23ac4c2657134805c37bda2ea1.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a133f23ac4c2657134805c37bda2ea1.png)'
- en: Luckily, this data set contains only numerical features and has no missing values.
    Therefore, no data preprocessing is required here (the closed-form solution does
    not require normalization of the data).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这个数据集只包含数值特征且没有缺失值。因此，这里不需要数据预处理（闭式解不需要数据的归一化）。
- en: 'Next, we need to append a column of 1s to the matrix *X_train* in order to
    represent the intercept terms. This can be easily done with the function [np.column_stack](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html)():'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要向矩阵*X_train*中添加一列1，以表示截距项。这可以通过函数[np.column_stack](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html)()轻松完成：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We now split the data into 80% training set and 20% test set:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将数据划分为80%的训练集和20%的测试集：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s now write a general function to find the optimal **w*** for any given
    data set, using the closed-form solution we have found earlier:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写一个通用函数，使用我们之前找到的闭式解来找到任何给定数据集的最佳**w***：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The closed-form solution can be implemented in a single line of code!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 闭式解法可以用一行代码实现！
- en: 'Let’s use this function to find the optimal **w*** for our training set:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个函数来找到我们训练集的最优 **w***：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The optimal **w*** is:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最优的 **w*** 是：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first component in this vector is the intercept (*w*₀), and the rest are
    the coefficients of the eight features in the data set.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个向量中的第一个分量是截距 (*w*₀)，其余的是数据集中八个特征的系数。
- en: Let’s now evaluate the model on the training and the test sets. It is important
    to evaluate your model on both of them, because a large discrepancy between the
    training and the test scores may indicate that your model is overfitting.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在训练集和测试集上评估模型。评估模型在这两者上的表现是很重要的，因为训练分数和测试分数之间的较大差异可能表明你的模型存在过拟合现象。
- en: 'We start by finding the *R*² score on the training set. To that end, we first
    get the predictions of the model on the training examples by multiplying the matrix
    *X_train_b* by the vector **w**:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在训练集上找到 *R*² 分数。为此，我们首先通过将矩阵 *X_train_b* 乘以向量 **w** 来获取模型在训练示例上的预测：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We now use the [r2_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)()
    function from sklearn.metrics to find the *R*² score on the training set:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用来自 sklearn.metrics 的 [r2_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)()
    函数来找到训练集上的 *R*² 分数：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The score we get is:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的分数是：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s do the same on the test set:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在测试集上做同样的操作：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The score we get is:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的分数是：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The score is not high, which indicates that the relationship between the features
    and the label might not be linear. In such cases, non-linear regression models,
    such as [regression trees](https://medium.com/@roiyeho/decision-trees-part-2-72adc626cca7)
    or k-nearest neighbors can provide better results.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 分数不高，这表明特征与标签之间的关系可能不是线性的。在这种情况下，非线性回归模型，如 [回归树](https://medium.com/@roiyeho/decision-trees-part-2-72adc626cca7)
    或k-近邻算法可以提供更好的结果。
- en: Exercise
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Let’s say we accidentally duplicated every point in the data set and then ran
    linear regression again. How will this affect the weights of the model?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们不小心将数据集中每个点都重复了一次，然后再次运行线性回归。这将如何影响模型的权重？
- en: '*Hint*: Think what will happen to the design matrix *X* and the labels vector
    **y**, and how these changes will affect the normal equations.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示*：考虑一下设计矩阵 *X* 和标签向量 **y** 将会发生什么变化，以及这些变化如何影响正规方程。'
- en: The solution can be found at the end of this article.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案可以在本文的最后找到。
- en: Linear Regression in Scikit-Learn
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn中的线性回归
- en: Scikit-Learn provides a class named [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
    that also implements the closed-form solution to the ordinary least squares problem.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 提供了一个名为 [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
    的类，它也实现了普通最小二乘问题的闭式解法。
- en: 'By default, this class automatically adds a column of 1s to the design matrix,
    so you do not need to add it manually as we did earlier (unless in the constructor
    you set the parameter *fit_intercept* to False). Therefore, we need to re-split
    the original data set (without the extra ones) into training and test sets:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，该类会自动将一列1添加到设计矩阵中，因此你不需要像我们之前那样手动添加（除非在构造函数中将参数 *fit_intercept* 设置为 False）。因此，我们需要重新将原始数据集（没有额外的1）拆分为训练集和测试集：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s create an instance of the LinearRegression class and fit it to the training
    set:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个 LinearRegression 类的实例，并将其拟合到训练集上：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The fitted parameter vector **w** is stored in two attributes of this class:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合的参数向量 **w** 被存储在该类的两个属性中：
- en: '*coef_* is an array that contains all the weights except for the intercept'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*coef_* 是一个包含所有权重的数组，除了截距项'
- en: '*intercept_* is the intercept term (*w*₀)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*intercept_* 是截距项 (*w*₀)'
- en: 'Let’s print them:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印它们：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We get exactly the same coefficients as we did with the computation in NumPy.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的系数与在NumPy中计算的完全一样。
- en: 'The [score()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score)
    method of the LinearRegression class returns the *R*² score of the model. It only
    requires the matrix *X* and the vector **y** of the data set you want to get the
    score on (so no need to compute the model’s predictions). For example, we can
    get the *R*² score on the training and the test sets as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[score()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score)方法返回模型的*R*²分数。它只需要*X*矩阵和数据集**y**的向量（因此不需要计算模型的预测）。例如，我们可以如下获得训练集和测试集的*R*²分数：'
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As expected, we get the same *R*² scores as before.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们获得了与之前相同的*R*²分数。
- en: Analyzing the Regression Errors
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归误差分析
- en: In addition to evaluating the overall performance of the model, we often want
    to investigate the behavior of our regression errors. For example, are the errors
    normally distributed around 0 or are they skewed? Are there any inputs for which
    our model has particularly high prediction errors?
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 除了评估模型的整体性能，我们通常还希望研究回归误差的行为。例如，误差是否围绕0正态分布，还是有偏？是否存在模型预测误差特别大的输入？
- en: Answers to these questions will help us find the source of these errors. For
    example, if the errors are not normally distributed around 0, this might indicate
    that the linear regression model is not suitable for our data set and we need
    to try other regression models (e.g., polynomial regression). Or, if our model
    has particularly high prediction errors on some samples, they might be outliers
    and we need to investigate where they come from.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些问题的回答将帮助我们找到这些错误的来源。例如，如果误差没有围绕0正态分布，这可能表明线性回归模型不适合我们的数据集，我们需要尝试其他回归模型（例如多项式回归）。或者，如果我们的模型在某些样本上的预测误差特别高，这些样本可能是异常值，我们需要调查它们的来源。
- en: A plot that can help you answer these questions is called a **residuals plot**.
    This plot shows the residuals on the *y*-axis vs. the predicted values of the
    model on the *x*-axis.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可以帮助你回答这些问题的图称为**残差图**。该图显示了*y*轴上的残差与模型在*x*轴上的预测值。
- en: 'Let’s write a function to create this plot:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个函数来创建这个图：
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can now call this function to show the residuals both on the training and
    the test sets:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以调用这个函数，展示训练集和测试集的残差：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/b025c1ac92da2bf2ee1fb28d77785cca.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b025c1ac92da2bf2ee1fb28d77785cca.png)'
- en: We can see that most of the errors are symmetrically distributed around 0, but
    there are some outliers on the far ends of the input range, which may require
    further investigation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，大多数误差围绕0对称分布，但在输入范围的远端有一些异常值，这可能需要进一步调查。
- en: Exercise
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Download the [Students Marks dataset](https://www.kaggle.com/datasets/yasserh/student-marks-dataset)
    from Kaggle. Build a linear regression model to predict a student’s mark based
    on their study time and number of courses they took. Compute the RMSE and *R*²
    score of the model both on the training and the test sets. Plot the residuals
    vs. the predicted values. What can you learn from this plot on the data set?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从Kaggle下载[学生成绩数据集](https://www.kaggle.com/datasets/yasserh/student-marks-dataset)。建立一个线性回归模型，以根据学生的学习时间和课程数量预测成绩。计算模型在训练集和测试集上的RMSE和*R*²分数。绘制残差与预测值的关系图。你能从这个图中学到什么？
- en: Gradient Descent
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'Although the closed-form solution gives us a direct way to find the optimal
    parameters of the regression model, it suffers from a few drawbacks:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管闭式解法为我们提供了一种直接找到回归模型最佳参数的方法，但它也有一些缺点：
- en: The closed-form solution is computationally inefficient when we have a large
    number of features, since it requires the computation of the inverse of *XᵗX*,
    which is a *m* × *m* matrix (*m* is the number of features). Computing the inverse
    of a matrix has a runtime complexity of *O*(*m*³) under most implementations.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们有大量特征时，闭式解法在计算上效率低下，因为它需要计算*XᵗX*的逆，这是一个*m* × *m*的矩阵（*m*是特征的数量）。在大多数实现中，计算矩阵的逆的时间复杂度为*O*(*m*³)。
- en: It requires to have the entire design matrix *X* in memory, which is not always
    feasible if we have a very large data set.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它需要将整个设计矩阵*X*保存在内存中，这在数据集非常大的情况下并不总是可行。
- en: It does not support online (incremental) learning, since any change to the design
    matrix *X* requires re-computation of the inverse of *XᵗX.*
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它不支持在线（增量）学习，因为任何对设计矩阵*X*的更改都需要重新计算*XᵗX*的逆。
- en: 'Gladly, there is an alternative approach for finding the optimal **w**, which
    is **gradient descent**. Gradient descent is an iterative approach for finding
    a minimum of a function, where we take small steps in the opposite direction of
    the gradient in order to get closer to the minimum:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 值得庆幸的是，还有一种找到最佳 **w** 的替代方法，即 **梯度下降**。梯度下降是一种迭代方法，用于找到函数的最小值，其中我们在梯度的相反方向上迈出小步，以接近最小值：
- en: '![](../Images/130066017b655d6ac82d7b133def837d.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/130066017b655d6ac82d7b133def837d.png)'
- en: Gradient descent
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降
- en: In order to use gradient descent to find the minimum of the least squares cost,
    we need to compute the partial derivatives of *J*(**w**) with respect to each
    one of the weights.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用梯度下降来找到最小二乘成本的最小值，我们需要计算 *J*(**w**) 对每个权重的偏导数。
- en: 'The partial derivative of *J*(**w**) with respect to any of the weights *wⱼ*
    is:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*J*(**w**) 对任何权重 *wⱼ* 的偏导数是：'
- en: '![](../Images/95fcc96fd0a0a5dbf3cf3993c219c2d7.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95fcc96fd0a0a5dbf3cf3993c219c2d7.png)'
- en: 'Therefore, the gradient descent update rule is:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，梯度下降更新规则是：
- en: '![](../Images/d376113e69bf8f9703d15ba66822ce4b.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d376113e69bf8f9703d15ba66822ce4b.png)'
- en: The gradient descent update rule
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降更新规则
- en: where *α* is a learning rate that controls the step size (0 < *α* < 1).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *α* 是一个控制步长的学习率（0 < *α* < 1）。
- en: 'Instead of updating each component of **w** separately, we can update the entire
    vector **w** in one step:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以一步更新整个向量**w**，而不是单独更新每个组件。
- en: '![](../Images/e7f57c4da0a280e5d70bd4292e58c67f.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7f57c4da0a280e5d70bd4292e58c67f.png)'
- en: The gradient descent update rule in vector form
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 向量形式的梯度下降更新规则
- en: 'Gradient descent can be applied in one of the following modes:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降可以应用于以下模式之一：
- en: '**Batch gradient descent** — the weights are updated after we compute the error
    on the entire training set.'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**批量梯度下降** — 在计算整个训练集的误差后更新权重。'
- en: '**Stochastic gradient descent (SGD)** — a gradient descent step is performed
    after every training example. In this case, the gradient descent update rule takes
    the following form:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**随机梯度下降（SGD）** — 在每个训练样本后执行一次梯度下降步骤。在这种情况下，梯度下降更新规则如下：'
- en: '![](../Images/89c4dc75a0f5d5d8b163a0ceb267dc40.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89c4dc75a0f5d5d8b163a0ceb267dc40.png)'
- en: SGD update rule
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 更新规则
- en: SGD typically converges faster than batch gradient descent as it makes progress
    after each example, and it also supports online learning since it can process
    new data points one at a time. On the other hand, SGD is less stable than batch
    gradient descent, and its convergence to the global optimum is not guaranteed
    (although in practice it gets very close to the optimum).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于批量梯度下降，SGD 通常收敛更快，因为它在每个样本后都取得进展，并且它也支持在线学习，因为它可以一次处理一个新数据点。另一方面，SGD 比批量梯度下降不那么稳定，其收敛到全局最优解的保证并不确定（尽管在实践中它非常接近最优解）。
- en: Note that whenever you use gradient descent, you must make sure that your data
    set is **normalized** (otherwise gradient descent may take steps of different
    sizes in different directions, which will make it unstable).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每当使用梯度下降时，必须确保数据集是 **标准化** 的（否则梯度下降可能在不同方向上采取不同大小的步骤，从而导致不稳定）。
- en: The SGDRegressor Class
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SGDRegressor 类
- en: The class [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)
    in Scikit-Learn implements the SGD approach for fitting a linear regression model.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 中的 [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)
    类实现了用于拟合线性回归模型的 SGD 方法。
- en: 'The important hyperparameters of this class are:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的重要超参数是：
- en: '*loss* — the loss function used as the objective of the optimization. The options
    of this parameter are: *squared_error* (squared loss, this is the default option),
    *huber* (Huber loss) and *epsilon_intensive* (the loss function used in Support
    Vector Regression). The difference between these loss functions is explained in
    [this article](https://medium.com/towards-data-science/loss-functions-in-machine-learning-9977e810ac02).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*loss* — 用作优化目标的损失函数。该参数的选项包括：*squared_error*（平方损失，这是默认选项）、*huber*（Huber 损失）和
    *epsilon_intensive*（支持向量回归中的损失函数）。这些损失函数之间的区别在[这篇文章](https://medium.com/towards-data-science/loss-functions-in-machine-learning-9977e810ac02)中有说明。'
- en: '*penalty* — the type of regularization to use (defaults to ‘l2’).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*penalty* — 使用的正则化类型（默认为‘l2’）。'
- en: '*alpha* — the regularization coefficient (defaults to 0.0001).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*alpha* — 正则化系数（默认为 0.0001）。'
- en: '*max_iter —* the maximum number of epochs over the training data (defaults
    to 1000).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*max_iter —* 训练数据上的最大迭代次数（默认为1000）。'
- en: '*learning_rat*e — learning rate schedule for the weight updates (defaults to
    ‘invscaling’).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*learning_rate* — 权重更新的学习率调度（默认为‘invscaling’）。'
- en: '*eta0 —* the initial learning rate used (defaults to 0.01).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*eta0 —* 使用的初始学习率（默认为0.01）。'
- en: '*early_stopping —* whether to stop the training when the validation score is
    not improving (defaults to False).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*early_stopping —* 是否在验证分数没有改善时停止训练（默认为False）。'
- en: '*validation_fraction* — the proportion of the training set to set aside for
    validation (defaults to 0.1).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*validation_fraction* — 从训练集中抽取用于验证的比例（默认为0.1）。'
- en: 'Since we need to normalize our data before using SGD, we will create a [pipeline](https://medium.com/@roiyeho/pipelines-in-scikit-learn-46c61c5c60b2)
    that consists of two steps:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要在使用SGD之前规范化数据，因此我们将创建一个[管道](https://medium.com/@roiyeho/pipelines-in-scikit-learn-46c61c5c60b2)，该管道包括两个步骤：
- en: A [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)
    that normalizes the features by removing the mean and scaling them to unit variance.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)，它通过去除均值并将特征缩放到单位方差来规范化特征。
- en: An SGDRegressor with its default settings.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认设置的SGDRegressor。
- en: '[PRE22]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s fit the pipeline to our training set:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将管道拟合到我们的训练集：
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And now let’s evaluate the model on the training and test sets:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在训练集和测试集上评估模型：
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The scores we get are:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的分数是：
- en: '[PRE25]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: These are very bad scores! What has just happened?
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是非常差的分数！刚刚发生了什么？
- en: 'When you get such bad scores with gradient descent, it usually means that your
    learning rate is too high, which causes the algorithm to oscillate between the
    two sides of the minimum:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用梯度下降法得到如此差的分数时，通常意味着你的学习率过高，这导致算法在最小值的两侧之间震荡：
- en: '![](../Images/bd28dc752b736236937083b0eea82537.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd28dc752b736236937083b0eea82537.png)'
- en: Oscillations in gradient descent due to a high learning rate
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 由于学习率过高导致梯度下降法的震荡
- en: 'Let’s reduce the learning rate from 0.01 to 0.001 by changing the parameter
    *eta0* of the SGDRegressor:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将学习率从0.01降低到0.001，通过更改SGDRegressor的*eta0*参数：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let’s refit the pipeline to the training set and reevaluate it:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新拟合训练集并重新评估：
- en: '[PRE27]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The scores we get this time are:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这次得到的分数是：
- en: '[PRE29]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: which are similar to the *R*² scores we obtained with the closed-form solution
    (they are a bit lower, since SGD gets close to the global minimum, but not to
    the minimum itself).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分数类似于我们使用闭式解得到的*R*²分数（这些分数略低，因为SGD接近全局最小值，但未达到最小值本身）。
- en: Key Takeaways
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键要点
- en: In linear regression we are trying to find a linear relationship between a set
    of features and a target variable.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线性回归中，我们试图找到一组特征与目标变量之间的线性关系。
- en: The key assumptions are that the features are linearly independent, and that
    the error terms are independent of each other and normally distributed with a
    zero mean.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键假设是特征线性无关，且误差项彼此独立且正态分布，均值为零。
- en: In ordinary least squares (OLS) regression we try to minimize the sum of squared
    residuals. The cost function of OLS is convex, so it has a single minimum point.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在普通最小二乘（OLS）回归中，我们试图最小化残差的平方和。OLS的成本函数是凸的，因此具有一个最小点。
- en: The two main approaches for finding the optimal parameters of the model are
    the closed-form solution and gradient descent. Gradient descent is favored when
    the data set is large, or when we need to support online learning.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找模型最佳参数的两种主要方法是闭式解和梯度下降法。当数据集很大或需要支持在线学习时，梯度下降法更受青睐。
- en: When using gradient descent, it is important to normalize your features and
    choose an appropriate learning rate.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用梯度下降法时，重要的是规范化你的特征并选择合适的学习率。
- en: We evaluate the performance of regression models using *R*² score, which varies
    between 0 and 1 and measures how better the model is than a baseline model that
    always predicts the mean of the target, and RMSE, which is the square root of
    the mean squared errors.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用*R*²分数评估回归模型的性能，该分数在0到1之间变化，衡量模型比始终预测目标均值的基准模型好多少，以及RMSE，即均方误差的平方根。
- en: Solution to the Duplicate Data Exercise
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重复数据练习的解决方案
- en: 'Recall that the closed-form solution is:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下，闭式解为：
- en: '![](../Images/299e4c7665e6cf58582ea24aabadd43d.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/299e4c7665e6cf58582ea24aabadd43d.png)'
- en: 'If we double the data points, then instead of *X* and **y**, we will have the
    following design matrix and target vector:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们加倍数据点，那么代替 *X* 和**y**，我们将有以下设计矩阵和目标向量：
- en: '![](../Images/abd717c62b151fef3234f6c1154f6383.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abd717c62b151fef3234f6c1154f6383.png)'
- en: The matrix *A* has 2*n* rows and *m* columns, where rows 1, …, *n* of the matrix
    are identical to rows *n* + 1, …, 2*n*. Similarly, the vector **z** has 2*n* rows,
    where the first *n* rows are identical to the last *n* rows.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵*A*有2*n*行和*m*列，其中矩阵的第1行到第*n*行与第*n* + 1行到第2*n*行相同。类似地，向量**z**有2*n*行，其中前*n*行与最后*n*行相同。
- en: We can easily show that
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地证明
- en: '![](../Images/f22846b616ac5f3b5417140ca4f73d18.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f22846b616ac5f3b5417140ca4f73d18.png)'
- en: '**Proof**:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明**：'
- en: '![](../Images/07a91b342cb2459ccb7f6dd059c020fa.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07a91b342cb2459ccb7f6dd059c020fa.png)'
- en: 'Similarly, we have (we can treat **z** as a matrix with one column):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们有（我们可以将**z**视为具有一列的矩阵）：
- en: '![](../Images/2ae9f2deb17c1a053b496c3303886134.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ae9f2deb17c1a053b496c3303886134.png)'
- en: 'Therefore, we can write:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以写成：
- en: '![](../Images/78a4d9beeb7144beaebcfbf6b605ea6d.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78a4d9beeb7144beaebcfbf6b605ea6d.png)'
- en: We get the same weights as in the original model! i.e., the regression model
    will not change.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到与原始模型相同的权重！也就是说，回归模型不会改变。
- en: Final Notes
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最后的说明
- en: All images unless otherwise noted are by the author.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均为作者提供。
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/multiple_linear_regression](https://github.com/roiyeho/medium/tree/main/multiple_linear_regression)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的github上找到本文的代码示例：[https://github.com/roiyeho/medium/tree/main/multiple_linear_regression](https://github.com/roiyeho/medium/tree/main/multiple_linear_regression)
- en: Thanks for reading!
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
