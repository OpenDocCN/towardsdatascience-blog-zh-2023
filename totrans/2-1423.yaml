- en: Linear Regression In Depth (Part 2)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/linear-regression-in-depth-part-2-5d40fd19efd4](https://towardsdatascience.com/linear-regression-in-depth-part-2-5d40fd19efd4)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deep dive into multiple linear regression with examples in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roiyeho?source=post_page-----5d40fd19efd4--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----5d40fd19efd4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d40fd19efd4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d40fd19efd4--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----5d40fd19efd4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d40fd19efd4--------------------------------)
    ·14 min read·Apr 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89c03a1c4491d8f8e9ce98926d1fbb9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [ThisisEngineering RAEng](https://unsplash.com/@thisisengineering?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/GzDrm7SYQ0g?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: In the [first part](https://medium.com/towards-data-science/linear-regression-in-depth-part-1-485f997fd611)
    of this article we formally defined the linear regression problem and showed how
    to solve simple linear regression problems, where the data set contains only one
    feature. In the second part of the article, we will discuss multiple linear regression
    problems, where the data set may contain any number of features.
  prefs: []
  type: TYPE_NORMAL
- en: We will first generalize the closed-form solution we have found for simple linear
    regression to any number of features. Then we will suggest an alternative approach
    for solving linear regression problems that is based on gradient descent, and
    discuss the pros and cons of this approach vs. using the closed-form solution.
    In addition, we will explore the classes in Scikit-Learn that implement both approaches,
    and demonstrate how to use them on a real-world data set.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Linear Regression Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that in regression problems we are given a set of *n* labeled examples:
    *D* = {(**x**₁, *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ* is
    an *m*-dimensional vector containing the **features** of example *i,* and *yᵢ*
    is a real value that represents the **label** of that example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In **linear regression** problems, we assume that there is a linear relationship
    between the feature vector **x** and the label *y*, so our model hypothesis takes
    the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87768db1046e6151ab03d8383811426b.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear regression model hypothesis
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to find the parameters **w** of this model that will minimize the
    sum of squared residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc275f18bbc9a318345bd53dde5c8f8a.png)'
  prefs: []
  type: TYPE_IMG
- en: The least squares cost function
  prefs: []
  type: TYPE_NORMAL
- en: In the previous part of the article, we have shown how to find the optimal **w**
    for the case of *m* = 1 using the **normal equations**. We will now extend these
    equations for any number of features *m*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simplify the derivation of the normal equations for the general case, we
    first define a matrix *X* that contains the values of all the features in the
    data set, including the intercept terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/556ac465f5a256b7559bf19ff400e054.png)'
  prefs: []
  type: TYPE_IMG
- en: The design matrix
  prefs: []
  type: TYPE_NORMAL
- en: This matrix is called the **design matrix**. Each row in the design matrix represents
    an individual sample, and the columns represent the explanatory variables. The
    dimensions of the matrix are *n* × (*m* + 1), where *n* is the number of samples
    and *m* is the number of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we define the vector **y** as an *n*-dimensional vector that contains
    all the target values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/903a71637769932a07d8236fc908fa62.png)'
  prefs: []
  type: TYPE_IMG
- en: The target vector
  prefs: []
  type: TYPE_NORMAL
- en: 'These definitions allow us to write the least squares cost function in the
    following matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15bfe63991293e9ddc4bbee52636c1ef.png)'
  prefs: []
  type: TYPE_IMG
- en: The OLS cost function in matrix form
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first note that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bbfe8ea2eabf117235b436b728c73a94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The dot product of a vector with itself **u***ᵗ***u** is just the sum of all
    its components squared, therefore we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8fd43bfe980b656fdfca69650dbaa0ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Closed-Form Solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As was the case in simple linear regression, the function *J*(**w**) is convex,
    hence it has a single local minimum, which is also the global minimum. In order
    to find this global minimum, we need to compute the gradient of *J*(**w**) with
    respect to **w** and set it to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient of *J*(**w**) with respect to **w** is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea760389b8e2c2ea201c87bb36b8f186.png)'
  prefs: []
  type: TYPE_IMG
- en: 'During this proof we have used some basic rules of matrix calculus, which are
    explained in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roiyeho/a-gentle-introduction-to-matrix-calculus-14584f2c4f60?source=post_page-----5d40fd19efd4--------------------------------)
    [## A Gentle Introduction to Matrix Calculus'
  prefs: []
  type: TYPE_NORMAL
- en: Including Applications in Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@roiyeho/a-gentle-introduction-to-matrix-calculus-14584f2c4f60?source=post_page-----5d40fd19efd4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'We now set this gradient to zero in order to obtain the normal equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f63cb9a0a0ad01574e74cd4facdb881f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the optimal **w*** that minimizes the least squares cost function
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da030c54924b6a62658f99f29720d4de.png)'
  prefs: []
  type: TYPE_IMG
- en: The closed-form solution to ordinary least squares
  prefs: []
  type: TYPE_NORMAL
- en: Note that we assumed here that the columns of *X* are linearly independent (i.e.,
    *X* has a **full column rank**), otherwise *XᵗX* is not invertible, and there
    is no unique solution for **w***.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the columns of *X* are linearly dependent, we call this phenomenon **multicollinearity**.
    Mathematically, a set of variables is **perfectly multicollinear** if for all
    samples *i*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e1c0f53726e66ba3045a751f042343c.png)'
  prefs: []
  type: TYPE_IMG
- en: Perfect multicollinearity
  prefs: []
  type: TYPE_NORMAL
- en: where *λₖ* are constants, and *xᵢₖ* is the value of feature *k* in sample *i*.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, perfect multicollinearity is rare (e.g., it can be caused by accidentally
    duplicating a variable in the data). However, even lesser degrees of multicollinearity,
    where two or more features are highly correlated with each other (but not perfectly
    correlated), can cause issues both when fitting the model (the coefficients become
    very sensitive to small changes in the data) and when interpreting the results
    (it becomes hard to identify which features have the most impact on the model’s
    predictions).
  prefs: []
  type: TYPE_NORMAL
- en: The interested reader can find more info about the multicollinearity problem
    and how to handle it in this [Wikipedia article](https://en.wikipedia.org/wiki/Multicollinearity).
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Linear Regression Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To demonstrate the usage of the closed-form solution, let’s build a linear regression
    model for the [California housing data set](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing),
    available from the sklearn.datasets module. The goal in this data set is to predict
    the median house value of a given district (house block) in California, based
    on 8 different features of that district, such as the median income or the average
    number of rooms per household.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import the required libraries, and initialize the random seed in order
    to have reproducible results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we fetch the data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To explore the data set, we merge the features (X) and the labels (y) into
    a Pandas DataFrame, and display the first rows from the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cf3a1f1da39af8d9d5961f8296b6516b.png)'
  prefs: []
  type: TYPE_IMG
- en: The first five rows from the California housing dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'We can further investigate the data set by calling the DataFrame’s [info()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html)
    method, which provides information about the type of the columns and whether they
    contain any missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3a133f23ac4c2657134805c37bda2ea1.png)'
  prefs: []
  type: TYPE_IMG
- en: Luckily, this data set contains only numerical features and has no missing values.
    Therefore, no data preprocessing is required here (the closed-form solution does
    not require normalization of the data).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to append a column of 1s to the matrix *X_train* in order to
    represent the intercept terms. This can be easily done with the function [np.column_stack](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html)():'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We now split the data into 80% training set and 20% test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now write a general function to find the optimal **w*** for any given
    data set, using the closed-form solution we have found earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The closed-form solution can be implemented in a single line of code!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use this function to find the optimal **w*** for our training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The optimal **w*** is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The first component in this vector is the intercept (*w*₀), and the rest are
    the coefficients of the eight features in the data set.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now evaluate the model on the training and the test sets. It is important
    to evaluate your model on both of them, because a large discrepancy between the
    training and the test scores may indicate that your model is overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by finding the *R*² score on the training set. To that end, we first
    get the predictions of the model on the training examples by multiplying the matrix
    *X_train_b* by the vector **w**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We now use the [r2_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)()
    function from sklearn.metrics to find the *R*² score on the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The score we get is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s do the same on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The score we get is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The score is not high, which indicates that the relationship between the features
    and the label might not be linear. In such cases, non-linear regression models,
    such as [regression trees](https://medium.com/@roiyeho/decision-trees-part-2-72adc626cca7)
    or k-nearest neighbors can provide better results.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s say we accidentally duplicated every point in the data set and then ran
    linear regression again. How will this affect the weights of the model?
  prefs: []
  type: TYPE_NORMAL
- en: '*Hint*: Think what will happen to the design matrix *X* and the labels vector
    **y**, and how these changes will affect the normal equations.'
  prefs: []
  type: TYPE_NORMAL
- en: The solution can be found at the end of this article.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression in Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scikit-Learn provides a class named [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
    that also implements the closed-form solution to the ordinary least squares problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, this class automatically adds a column of 1s to the design matrix,
    so you do not need to add it manually as we did earlier (unless in the constructor
    you set the parameter *fit_intercept* to False). Therefore, we need to re-split
    the original data set (without the extra ones) into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s create an instance of the LinearRegression class and fit it to the training
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The fitted parameter vector **w** is stored in two attributes of this class:'
  prefs: []
  type: TYPE_NORMAL
- en: '*coef_* is an array that contains all the weights except for the intercept'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*intercept_* is the intercept term (*w*₀)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s print them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We get exactly the same coefficients as we did with the computation in NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [score()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score)
    method of the LinearRegression class returns the *R*² score of the model. It only
    requires the matrix *X* and the vector **y** of the data set you want to get the
    score on (so no need to compute the model’s predictions). For example, we can
    get the *R*² score on the training and the test sets as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As expected, we get the same *R*² scores as before.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the Regression Errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to evaluating the overall performance of the model, we often want
    to investigate the behavior of our regression errors. For example, are the errors
    normally distributed around 0 or are they skewed? Are there any inputs for which
    our model has particularly high prediction errors?
  prefs: []
  type: TYPE_NORMAL
- en: Answers to these questions will help us find the source of these errors. For
    example, if the errors are not normally distributed around 0, this might indicate
    that the linear regression model is not suitable for our data set and we need
    to try other regression models (e.g., polynomial regression). Or, if our model
    has particularly high prediction errors on some samples, they might be outliers
    and we need to investigate where they come from.
  prefs: []
  type: TYPE_NORMAL
- en: A plot that can help you answer these questions is called a **residuals plot**.
    This plot shows the residuals on the *y*-axis vs. the predicted values of the
    model on the *x*-axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s write a function to create this plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now call this function to show the residuals both on the training and
    the test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b025c1ac92da2bf2ee1fb28d77785cca.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that most of the errors are symmetrically distributed around 0, but
    there are some outliers on the far ends of the input range, which may require
    further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download the [Students Marks dataset](https://www.kaggle.com/datasets/yasserh/student-marks-dataset)
    from Kaggle. Build a linear regression model to predict a student’s mark based
    on their study time and number of courses they took. Compute the RMSE and *R*²
    score of the model both on the training and the test sets. Plot the residuals
    vs. the predicted values. What can you learn from this plot on the data set?
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although the closed-form solution gives us a direct way to find the optimal
    parameters of the regression model, it suffers from a few drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: The closed-form solution is computationally inefficient when we have a large
    number of features, since it requires the computation of the inverse of *XᵗX*,
    which is a *m* × *m* matrix (*m* is the number of features). Computing the inverse
    of a matrix has a runtime complexity of *O*(*m*³) under most implementations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It requires to have the entire design matrix *X* in memory, which is not always
    feasible if we have a very large data set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It does not support online (incremental) learning, since any change to the design
    matrix *X* requires re-computation of the inverse of *XᵗX.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gladly, there is an alternative approach for finding the optimal **w**, which
    is **gradient descent**. Gradient descent is an iterative approach for finding
    a minimum of a function, where we take small steps in the opposite direction of
    the gradient in order to get closer to the minimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/130066017b655d6ac82d7b133def837d.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient descent
  prefs: []
  type: TYPE_NORMAL
- en: In order to use gradient descent to find the minimum of the least squares cost,
    we need to compute the partial derivatives of *J*(**w**) with respect to each
    one of the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'The partial derivative of *J*(**w**) with respect to any of the weights *wⱼ*
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95fcc96fd0a0a5dbf3cf3993c219c2d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the gradient descent update rule is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d376113e69bf8f9703d15ba66822ce4b.png)'
  prefs: []
  type: TYPE_IMG
- en: The gradient descent update rule
  prefs: []
  type: TYPE_NORMAL
- en: where *α* is a learning rate that controls the step size (0 < *α* < 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of updating each component of **w** separately, we can update the entire
    vector **w** in one step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7f57c4da0a280e5d70bd4292e58c67f.png)'
  prefs: []
  type: TYPE_IMG
- en: The gradient descent update rule in vector form
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient descent can be applied in one of the following modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch gradient descent** — the weights are updated after we compute the error
    on the entire training set.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent (SGD)** — a gradient descent step is performed
    after every training example. In this case, the gradient descent update rule takes
    the following form:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/89c4dc75a0f5d5d8b163a0ceb267dc40.png)'
  prefs: []
  type: TYPE_IMG
- en: SGD update rule
  prefs: []
  type: TYPE_NORMAL
- en: SGD typically converges faster than batch gradient descent as it makes progress
    after each example, and it also supports online learning since it can process
    new data points one at a time. On the other hand, SGD is less stable than batch
    gradient descent, and its convergence to the global optimum is not guaranteed
    (although in practice it gets very close to the optimum).
  prefs: []
  type: TYPE_NORMAL
- en: Note that whenever you use gradient descent, you must make sure that your data
    set is **normalized** (otherwise gradient descent may take steps of different
    sizes in different directions, which will make it unstable).
  prefs: []
  type: TYPE_NORMAL
- en: The SGDRegressor Class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The class [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)
    in Scikit-Learn implements the SGD approach for fitting a linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The important hyperparameters of this class are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*loss* — the loss function used as the objective of the optimization. The options
    of this parameter are: *squared_error* (squared loss, this is the default option),
    *huber* (Huber loss) and *epsilon_intensive* (the loss function used in Support
    Vector Regression). The difference between these loss functions is explained in
    [this article](https://medium.com/towards-data-science/loss-functions-in-machine-learning-9977e810ac02).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*penalty* — the type of regularization to use (defaults to ‘l2’).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*alpha* — the regularization coefficient (defaults to 0.0001).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*max_iter —* the maximum number of epochs over the training data (defaults
    to 1000).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*learning_rat*e — learning rate schedule for the weight updates (defaults to
    ‘invscaling’).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*eta0 —* the initial learning rate used (defaults to 0.01).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*early_stopping —* whether to stop the training when the validation score is
    not improving (defaults to False).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*validation_fraction* — the proportion of the training set to set aside for
    validation (defaults to 0.1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we need to normalize our data before using SGD, we will create a [pipeline](https://medium.com/@roiyeho/pipelines-in-scikit-learn-46c61c5c60b2)
    that consists of two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: A [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)
    that normalizes the features by removing the mean and scaling them to unit variance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An SGDRegressor with its default settings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s fit the pipeline to our training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And now let’s evaluate the model on the training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The scores we get are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: These are very bad scores! What has just happened?
  prefs: []
  type: TYPE_NORMAL
- en: 'When you get such bad scores with gradient descent, it usually means that your
    learning rate is too high, which causes the algorithm to oscillate between the
    two sides of the minimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd28dc752b736236937083b0eea82537.png)'
  prefs: []
  type: TYPE_IMG
- en: Oscillations in gradient descent due to a high learning rate
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s reduce the learning rate from 0.01 to 0.001 by changing the parameter
    *eta0* of the SGDRegressor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s refit the pipeline to the training set and reevaluate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The scores we get this time are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: which are similar to the *R*² scores we obtained with the closed-form solution
    (they are a bit lower, since SGD gets close to the global minimum, but not to
    the minimum itself).
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In linear regression we are trying to find a linear relationship between a set
    of features and a target variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key assumptions are that the features are linearly independent, and that
    the error terms are independent of each other and normally distributed with a
    zero mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In ordinary least squares (OLS) regression we try to minimize the sum of squared
    residuals. The cost function of OLS is convex, so it has a single minimum point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two main approaches for finding the optimal parameters of the model are
    the closed-form solution and gradient descent. Gradient descent is favored when
    the data set is large, or when we need to support online learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using gradient descent, it is important to normalize your features and
    choose an appropriate learning rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We evaluate the performance of regression models using *R*² score, which varies
    between 0 and 1 and measures how better the model is than a baseline model that
    always predicts the mean of the target, and RMSE, which is the square root of
    the mean squared errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solution to the Duplicate Data Exercise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall that the closed-form solution is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/299e4c7665e6cf58582ea24aabadd43d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we double the data points, then instead of *X* and **y**, we will have the
    following design matrix and target vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/abd717c62b151fef3234f6c1154f6383.png)'
  prefs: []
  type: TYPE_IMG
- en: The matrix *A* has 2*n* rows and *m* columns, where rows 1, …, *n* of the matrix
    are identical to rows *n* + 1, …, 2*n*. Similarly, the vector **z** has 2*n* rows,
    where the first *n* rows are identical to the last *n* rows.
  prefs: []
  type: TYPE_NORMAL
- en: We can easily show that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f22846b616ac5f3b5417140ca4f73d18.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Proof**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07a91b342cb2459ccb7f6dd059c020fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we have (we can treat **z** as a matrix with one column):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ae9f2deb17c1a053b496c3303886134.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78a4d9beeb7144beaebcfbf6b605ea6d.png)'
  prefs: []
  type: TYPE_IMG
- en: We get the same weights as in the original model! i.e., the regression model
    will not change.
  prefs: []
  type: TYPE_NORMAL
- en: Final Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/multiple_linear_regression](https://github.com/roiyeho/medium/tree/main/multiple_linear_regression)'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
