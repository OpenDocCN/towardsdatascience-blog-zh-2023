- en: Self-Supervised Learning in Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自监督学习在计算机视觉中的应用
- en: 原文：[https://towardsdatascience.com/self-supervised-learning-in-computer-vision-fd43719b1625](https://towardsdatascience.com/self-supervised-learning-in-computer-vision-fd43719b1625)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/self-supervised-learning-in-computer-vision-fd43719b1625](https://towardsdatascience.com/self-supervised-learning-in-computer-vision-fd43719b1625)
- en: How to train models with only a few labeled examples
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何用仅有的几个标记示例来训练模型
- en: '[](https://michaloleszak.medium.com/?source=post_page-----fd43719b1625--------------------------------)[![Michał
    Oleszak](../Images/61b32e70cec4ba54612a8ca22e977176.png)](https://michaloleszak.medium.com/?source=post_page-----fd43719b1625--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fd43719b1625--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fd43719b1625--------------------------------)
    [Michał Oleszak](https://michaloleszak.medium.com/?source=post_page-----fd43719b1625--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://michaloleszak.medium.com/?source=post_page-----fd43719b1625--------------------------------)[![Michał
    Oleszak](../Images/61b32e70cec4ba54612a8ca22e977176.png)](https://michaloleszak.medium.com/?source=post_page-----fd43719b1625--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fd43719b1625--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fd43719b1625--------------------------------)
    [Michał Oleszak](https://michaloleszak.medium.com/?source=post_page-----fd43719b1625--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fd43719b1625--------------------------------)
    ·18 min read·Jan 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----fd43719b1625--------------------------------)·18分钟阅读·2023年1月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/94977b17c73bc401bb5cd47c4a23f8ad.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94977b17c73bc401bb5cd47c4a23f8ad.png)'
- en: A large portion of the value delivered by AI so far comes from supervised models
    trained on increasingly large datasets. Many of these datasets have been labeled
    by humans, a job that is mundane, time-consuming, error-prone, and sometimes expensive.
    Self-supervised learning (SSL) is a different learning paradigm allowing machines
    to learn from unlabeled data. In this article, we will discuss how SSL works and
    how to apply it to computer vision. We will compare simple approaches to state-of-the-art
    and see SSL in action for medical diagnosis, a domain that can benefit a lot from
    it but at the same time requires a deep understanding of the approach to be able
    to implement it correctly.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，AI所提供的大部分价值来自于在越来越大的数据集上训练的监督模型。许多这些数据集由人工标注，这是一项单调、耗时、容易出错且有时昂贵的工作。自监督学习（SSL）是一种不同的学习范式，允许机器从未标注的数据中学习。在本文中，我们将深入探讨SSL的工作原理以及如何将其应用于计算机视觉。我们将比较简单的方法和最先进的技术，并展示SSL在医学诊断中的实际应用，这个领域可以从中受益颇多，但同时也需要深入理解该方法以正确实施。
- en: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
- en: What is self-supervised learning?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是自监督学习？
- en: '[According to Yann LeCun](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/),
    Chief AI Scientist at Meta, self-supervised learning is “one of the most promising
    ways to build background knowledge and approximate a form of common sense in AI
    systems”. The idea behind the self-supervised approach is to train models on data
    without annotations.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[根据Yann LeCun的说法](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)，Meta的首席AI科学家，自监督学习是“构建背景知识并在AI系统中逼近一种常识形式的最有前途的方法之一”。自监督方法背后的理念是用没有注释的数据来训练模型。'
- en: Self-supervised learning is one of the most promising ways to build background
    knowledge and approximate a form of common sense in AI systems.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自监督学习是构建背景知识并在AI系统中逼近一种常识形式的最有前途的方法之一。
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ~ Yann LeCun
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ~ Yann LeCun
- en: 'Consider two other learning paradigms: supervised and unsupervised learning.
    In supervised learning, we present the model with some inputs and the corresponding
    labels and its job is to find a mapping between them that would generalize to
    new data.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑另外两种学习范式：监督学习和无监督学习。在监督学习中，我们向模型提供一些输入及相应的标签，模型的任务是找到一种映射关系，使其能够对新数据进行泛化。
- en: In unsupervised learning, on the other hand, we have just the inputs and no
    labels, and the learning goal is to explore patterns in the input data with the
    objective to cluster similar examples, reduce data dimensionality or detect anomalies,
    among others.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在无监督学习中，我们只有输入而没有标签，学习目标是探索输入数据中的模式，目的是对相似的示例进行聚类、减少数据维度或检测异常等。
- en: '![](../Images/e7a14ca14a000424f3e2cdc278d9d215.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7a14ca14a000424f3e2cdc278d9d215.png)'
- en: Learning paradigms. Image by the author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 学习范式。图片由作者提供。
- en: Self-supervised learning sits in between these two in some sense. It is similar
    to unsupervised learning in that it learns from unlabeled data, but it is also
    supervised at the same time since the model creates its own pseudo-labels to learn
    from during training.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习在某种程度上介于这两者之间。它类似于无监督学习，因为它从未标记的数据中学习，但同时也具有监督性质，因为模型在训练过程中创建自己的伪标签来进行学习。
- en: The idea is not exactly new. Self-supervised learning has been used a lot in
    the past, most notably in NLP to train large language models such as BERT or GPT.
    Such models might be given raw text with the task to predict the next token in
    the sequence from the preceding tokens. And so, for each training example, the
    model would set up its pseudo label as the next word in the sentence, let’s say.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法并不完全新鲜。自监督学习在过去已经被广泛使用，最著名的是在 NLP 中用于训练大型语言模型，如 BERT 或 GPT。这些模型可能会被提供原始文本，并要求预测序列中的下一个标记。因此，对于每个训练样本，模型会将其伪标签设置为句子中的下一个词，例如。
- en: Models trained in a self-supervised way create their own pseudo-labels from
    unlabeled data.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 以自监督方式训练的模型从未标记的数据中创建自己的伪标签。
- en: 'But it is the past three years that have seen self-supervised learning re-discovered
    for computer vision with breakthrough papers from [Google](https://arxiv.org/abs/2002.05709),
    [DeepMind](https://arxiv.org/pdf/2006.07733.pdf), and [Meta](https://arxiv.org/pdf/1911.05722.pdf).
    The principle, however, is still the same: the model creates its own pseudo-labels,
    for example by masking part of the image and trying to predict it, or by rotating
    the image and attempting to predict the rotation angle. We will discuss the exact
    techniques in a moment.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但在过去的三年中，自监督学习在计算机视觉领域被重新发现，取得了突破性的进展，相关论文来自 [Google](https://arxiv.org/abs/2002.05709)、[DeepMind](https://arxiv.org/pdf/2006.07733.pdf)
    和 [Meta](https://arxiv.org/pdf/1911.05722.pdf)。然而，原则仍然相同：模型创建自己的伪标签，例如通过遮挡图像的一部分并尝试预测它，或通过旋转图像并尝试预测旋转角度。我们稍后将讨论具体的技术。
- en: Now that we have a basic grasp of what self-supervised learning is all about,
    let’s see why it is particularly useful in medical applications.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对自监督学习有了基本的了解，让我们看看它在医疗应用中为何特别有用。
- en: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
- en: Scarce annotations in medical data
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 医疗数据中的注释稀缺。
- en: The medical sector produces a lot of images. According to IBM, up to 90% of
    all medical data comes in the form of an image. These data are a result of performing
    various examinations such as the X-ray, of which there are 3.6 billion made each
    year, according to WHO.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗行业产生了大量的图像。根据 IBM 的数据，高达 90% 的医疗数据以图像形式存在。这些数据是通过进行各种检查获得的，例如 X 射线，根据 WHO
    的数据，每年进行 36 亿次 X 射线检查。
- en: This vast amount of data seems to offer a great opportunity to apply machine
    learning algorithms, which feed on data, in order to help humans in diagnostic
    and treatment processes. If not for one issue.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些大量的数据似乎为应用机器学习算法提供了极大的机会，这些算法依赖于数据，以帮助人类进行诊断和治疗。然而，有一个问题存在。
- en: 'Traditional, supervised machine learning models, in order to learn from data,
    require more than just the training examples. They also need annotations, or labels:
    when we present a supervised model with an X-ray image during training, we need
    to tell it what medical conditions are to be recognized in it.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的监督学习模型，为了从数据中学习，除了训练样本，还需要注释或标签：当我们在训练过程中向监督模型展示一张 X 射线图像时，我们需要告诉它需要识别哪些医疗条件。
- en: Unfortunately, annotations are a scarce good in the medical domain, and obtaining
    them is a challenging quest. They usually need to be provided by expert doctors
    whose time is expensive and arguably could be better spent attending to their
    patients. Enters self-supervised learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在医学领域，注释资源稀缺，获取这些注释是一项具有挑战性的任务。它们通常需要由专家医生提供，而专家医生的时间昂贵，且无疑更好地用于照顾他们的病人。这时自监督学习就派上用场了。
- en: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
- en: Self-supervised learning solves scarce annotations
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自监督学习解决了注释稀缺的问题。
- en: 'In scarce annotations settings such as recognizing medical conditions in X-ray
    images, we typically find ourselves in the situation presented on the left-hand
    side in the picture below: we have a lot of data, but only a small portion of
    it is annotated.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在诸如识别X光图像中的医疗条件等标注稀少的环境中，我们通常发现自己处于下图左侧所示的情况：我们有大量数据，但只有一小部分是标注的。
- en: If we were to follow a traditional, supervised approach, we could only use the
    small labeled set for training the model. Thanks to self-supervised learning,
    however, we can learn from unlabeled images, too. Let’s see how to do it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们采用传统的监督方法，我们只能使用少量的标记数据来训练模型。然而，得益于自监督学习，我们也可以从未标记的图像中学习。让我们看看怎么做。
- en: '![](../Images/c524e3329e716701652b6d094323101c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c524e3329e716701652b6d094323101c.png)'
- en: Self-supervised learning workflow. Image by the author.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习工作流程。图像来源：作者。
- en: As a first step, we let the self-supervised model generate its pseudo-labels
    from the unlabeled data and train on them. This is referred to as **self-supervised
    pre-training**, in which the model solves what is called a **pretext task**. This
    could be predicting a masked image fragment or the rotation angle, as we have
    said before; we will discuss choosing the pretext task later on.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们让自监督模型从未标记的数据中生成其伪标签并进行训练。这被称为**自监督预训练**，在这个过程中模型解决一个称为**预文本任务**的问题。之前提到过，这可能是预测一个被遮蔽的图像片段或旋转角度，我们会在后面讨论如何选择预文本任务。
- en: The result of the above is a pre-trained model that has learned the patterns
    present in the unlabeled data. This model knows nothing about particular medical
    conditions (as such information would only be available in the labels that it
    has not seen) but it might have learned that **some X-rays are different from
    others in a consistent way**. This is what LeCun refers to as building background
    knowledge.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果是一个预训练的模型，它已经学习了未标记数据中存在的模式。这个模型对特定的医疗条件一无所知（因为这种信息只有在标签中才会出现，而它未曾见过），但它可能已经学会了**一些X光图像在一致的方式上有所不同**。这就是LeCun所说的建立背景知识。
- en: 'The self-supervised learning workflow comprises two steps: pre-training on
    unlabeled data to build background knowledge and fine-tuning on labeled data to
    learn to solve the downstream task.'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自监督学习工作流程包括两个步骤：在未标记数据上进行预训练以建立背景知识，并在标记数据上进行微调以学习解决下游任务。
- en: The second step is to **fine-tune this pre-trained model in a regular, supervised
    manner** on the labeled portion of the data. The trick here is that now that the
    model has some background knowledge about the patterns in the dataset, providing
    it with just a couple of annotated examples should be enough for it to learn how
    to solve the **downstream task**, which in our case is detecting medical conditions
    in X-ray images.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是**以常规、监督的方式对这个预训练模型进行微调**，在数据的标记部分进行。关键在于，现在模型已经有了一些关于数据集中模式的背景知识，提供给它仅仅几个带注释的例子就足以让它学习如何解决**下游任务**，在我们的例子中就是检测X光图像中的医疗条件。
- en: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
- en: Pretext tasks
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预文本任务
- en: Let us now discuss the pretext tasks that the model is solving in the pre-training
    step. Many have been proposed in the literature and the possibilities are limitless.
    The only requirement is that we must be able to create the pseudo-label from the
    input data itself. Let’s take a look at a couple of the most popular traditional
    approaches.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论模型在预训练步骤中解决的预文本任务。文献中提出了许多这种任务，可能性几乎是无限的。唯一的要求是我们必须能够从输入数据本身创建伪标签。让我们看几个最受欢迎的传统方法。
- en: Masked prediction
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 被遮蔽的预测
- en: Masked prediction simply means masking some part of the input image and having
    the model try to predict it from the remainder of the image.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 被遮蔽的预测简单地意味着遮蔽输入图像的一部分，并让模型尝试从剩余的图像中预测。
- en: '![](../Images/92816326dd4518afbbfef7651de74ad7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92816326dd4518afbbfef7651de74ad7.png)'
- en: Masked prediction. Image by the author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 被遮蔽的预测。图像来源：作者。
- en: Transformation prediction
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换预测
- en: 'An entire family of methods exists that can be grouped together under the broad
    name of transformation prediction. In these tasks, we apply some transformations
    to the image, such as rotating it, shifting the colors, and so on. The model is
    then tasked with predicting the parameters of the transformation: the angle of
    ratio, the amount of color shift, etc.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个完整的家族方法，这些方法可以归纳在一个广泛的名称下——变换预测。在这些任务中，我们对图像应用一些变换，比如旋转它、改变颜色等。然后，模型的任务是预测变换的参数：角度比例、颜色变化量等。
- en: '![](../Images/283be383b7d1de5f423425412393bb17.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/283be383b7d1de5f423425412393bb17.png)'
- en: Transformation prediction. Image by the author.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 变换预测。图像由作者提供。
- en: Jigsaw puzzle
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拼图
- en: Yet another approach is to have the model solve a jigsaw puzzle. We cut the
    input image into a number of pieces, re-arrange them randomly, and ask the model
    to come up with the correct original arrangement.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是让模型解决拼图。我们将输入图像切成若干块，随机重新排列，然后要求模型找出正确的原始排列。
- en: '![](../Images/9a5bb3f08391a9d02add6c253b2e52c7.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a5bb3f08391a9d02add6c253b2e52c7.png)'
- en: Jigsaw puzzle. Image by the author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 拼图。图像由作者提供。
- en: Instance discrimination
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实例识别
- en: Some other approaches focus the pretext task on instance discrimination. This
    requires one to have multiple views of the same object, for example, pictures
    of the same cat taken from different angles or in different places. Variations
    of this method generate the views automatically, e.g. from 3D point clouds or
    using generative models. The pretext task is then to recognize whether two images
    represent the exact same object or not.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些方法将预训练任务集中在实例识别上。这要求你拥有同一对象的多个视图，例如，从不同角度或不同地点拍摄的同一只猫的照片。这种方法的变体会自动生成视图，例如，从3D点云中生成或使用生成模型。预训练任务就是识别两张图像是否表示完全相同的对象。
- en: The goal of each of the pretext tasks we have discussed so far is to force the
    model to learn the structure and patterns in the data. The most recent research,
    however, has found a somewhat different approach to work best in achieving this
    goal. The winning approaches are based on **contrastive learning**.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我们讨论的每一个预训练任务的目标都是迫使模型学习数据中的结构和模式。然而，最新的研究发现，有一种稍微不同的方法在实现这一目标时效果最佳。成功的方法基于**对比学习**。
- en: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
- en: Contrastive learning
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对比学习
- en: Contrastive learning is based on the principle of contrasting samples against
    each other in order to learn which patterns are common between the samples and
    which set them apart.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习的原则是将样本相互对比，以学习样本之间的共同模式以及区分它们的模式。
- en: Contrastive learning contrasts samples against each other to learn which patterns
    are common between them and which set them apart.
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对比学习将样本相互对比，以学习它们之间的共同模式以及区分它们的模式。
- en: Supervised contrastive learning
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督式对比学习
- en: This approach is not limited to self-supervised learning. In fact, it originated
    as a solution to supervised, few-shot problems. Imagine you are responsible for
    security in an office building and you want to install an automatic door that
    would only open for verified employees. You only get a few pictures of each employee
    to train your model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不仅限于自监督学习。事实上，它最初是作为监督式少样本问题的解决方案出现的。想象一下，你负责办公楼的安全，并且你想安装一个只有经过验证的员工才能打开的自动门。你只有少量每个员工的照片来训练模型。
- en: 'A solution here could be to train a model to recognize, given two images, whether
    they depict the same person or not. The system can then compare each incoming
    person''s face to your database of employees’ pictures and look for a match. Such
    models are typically trained in a contrastive way. During training, they are presented
    with three pictures to contrast against each other: two of the same person, and
    one of a different one. The goal is to learn that the first two are similar to
    each other and dissimilar to the third one. This contrastive approach is supervised
    since you know who is in which picture and can use this knowledge to generate
    the training samples.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个解决方案可能是训练一个模型来识别给定的两张图像是否描绘了同一个人。系统可以将每个来访者的面部与员工照片数据库进行比对，并寻找匹配。这些模型通常以对比方式进行训练。在训练过程中，它们会被呈现三张照片以进行对比：两张是同一个人，一张是不同的。目标是学习前两张彼此相似，而与第三张不同。这种对比方法是监督的，因为你知道每张照片中的人是谁，并可以利用这些知识生成训练样本。
- en: Contrastive learning can be used for both supervised and self-supervised learning
    tasks.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对比学习可以用于有监督和自监督学习任务。
- en: A slightly different flavor of contrastive learning turns out to be very well-suited
    for self-supervised problems.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一种略有不同的对比学习方法证明非常适合自监督问题。
- en: Self-supervised contrastive learning
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自监督对比学习
- en: In this approach, we also present the model with three images.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们还向模型呈现三张图片。
- en: The first one is a random image from the training data set and is referred to
    as the **anchor image**.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一张是训练数据集中一张随机图片，被称为**锚点图片**。
- en: The second image is the same anchor image but transformed in some way, for instance
    through a rotation or a color shift, and is referred to as a **positive example**.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二张图片是相同的锚点图片，但以某种方式进行了变换，例如通过旋转或颜色偏移，被称为**正样本**。
- en: The third image is another random image from the training data, different from
    the first one, and is referred to as a **negative example**.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三张图片是训练数据中的另一张随机图片，与第一张不同，被称为**负样本**。
- en: '![](../Images/5907e93f3080dcf653bd9435b0bcbef4.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5907e93f3080dcf653bd9435b0bcbef4.png)'
- en: Contrastive self-supervised learning. Image by the author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对比自监督学习。图片来自作者。
- en: The goal of learning is to teach the model that the first two images are similar
    and we want their latent representations to be close to one another (after all,
    a rotated black and white cat is still a cat), while the last image is dissimilar
    from the first two and its latent representation, or embedding, should be far
    away.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的目标是教会模型前两张图片是相似的，我们希望它们的潜在表示彼此接近（毕竟，旋转的黑白猫仍然是一只猫），而最后一张图片与前两张图片不同，它的潜在表示或嵌入应该远离。
- en: Let’s now discuss a couple of self-supervised contrastive architectures in greater
    detail.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来更详细地讨论几种自监督对比架构。
- en: Triplet loss
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 三元组损失
- en: The arguably simplest approach one could imagine is a triplet loss-based model.
    We pass the anchor, the positive, and the negative images through a backbone model,
    such as a ResNet or a visual transformer, in order to obtain their embeddings,
    and then pass these to a triplet loss function, whose goal is to teach the model
    to place the anchor and positive images close together in the latent space, and
    the anchor and negative — far apart.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 可以想象的最简单的方法是基于三元组损失的模型。我们将锚点、正样本和负样本图像通过一个骨干模型，例如ResNet或视觉变换器，以获取它们的嵌入，然后将这些嵌入传递给三元组损失函数，其目标是教会模型将锚点和正样本图像在潜在空间中彼此靠近，而将锚点和负样本图像远离。
- en: '![](../Images/23d2aaa8196ecb212f81da94a8668467.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23d2aaa8196ecb212f81da94a8668467.png)'
- en: Simple triplet loss-based model architecture. Image by the author.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基于简单三元组损失的模型架构。图片来自作者。
- en: The triplet loss-based model is a simplistic one. Now, let’s take a look at
    a few state-of-the-art approaches.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 基于三元组损失的模型是一个简单的模型。现在，让我们看一看一些最先进的方法。
- en: SimCLR
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SimCLR
- en: The Simple Framework for Contrastive Learning, or SimCLR for short, has been
    proposed in a [paper by Google Research](https://arxiv.org/abs/2002.05709) in
    2020.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习的简单框架，简称SimCLR，已在2020年由[Google Research的一篇论文](https://arxiv.org/abs/2002.05709)中提出。
- en: 'The model takes two input images: the anchor, and its transformed version,
    or the positive example, and passes each of them through a ResNet encoder, a multi-layer
    perception, and a learnable non-linear layer. The Noise Contrastive Estimation
    (NCE) loss then aims to maximize the similarity between the two embeddings while
    minimizing the similarity to embeddings of other images from the same mini-batch.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 模型接收两张输入图片：锚点图片及其变换版本或正样本，然后将每张图片通过ResNet编码器、多层感知器和一个可学习的非线性层。噪声对比估计（NCE）损失旨在最大化两个嵌入之间的相似度，同时最小化与同一小批次中其他图片的嵌入的相似度。
- en: '![](../Images/7f98af8534a4794c163509645d73666b.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f98af8534a4794c163509645d73666b.png)'
- en: 'SimCLR architecture. Image source: [LINK](https://generallyintelligent.com/blog/2020-08-24-understanding-self-supervised-contrastive-learning/).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: SimCLR架构。图片来源：[LINK](https://generallyintelligent.com/blog/2020-08-24-understanding-self-supervised-contrastive-learning/)。
- en: SimCLR achieved great results in image recognition. Unfortunately, as the authors
    show, it works best with a batch size as large as 4096 and when trained for a
    long time. This makes it practically unusable for individuals and companies not
    willing to spend many dollars on cloud computing.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: SimCLR在图像识别中取得了很好的结果。不幸的是，正如作者所示，它在批量大小达到4096时效果最佳，并且需要长时间训练。这使得它对于那些不愿在云计算上花费大量资金的个人和公司几乎不可用。
- en: MoCo
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MoCo
- en: Facebook AI Research team’s [Momentum Contrast](https://arxiv.org/abs/1911.05722),
    or MoCo, alleviates some of SimCLR’s disadvantages. They managed to reduce the
    batch size to 256 thanks to a clever trick.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook AI 研究团队的 [Momentum Contrast](https://arxiv.org/abs/1911.05722)（简称 MoCo）缓解了
    SimCLR 的一些缺点。他们通过一个巧妙的技巧成功将批量大小减少到 256。
- en: MoCo has two encoder networks whose parameters are optimized separately, one
    for the anchor image (*online encoder*) and the other one for the positive example
    (*momentum encoder*). The online encoder is optimized by a regular gradient descent-based
    algorithm, while the momentum encoder is updated based on an exponential moving
    average of the online encoder’s weights.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: MoCo 有两个编码器网络，它们的参数分别优化，一个用于锚点图像（*在线编码器*），另一个用于正例（*动量编码器*）。在线编码器通过基于梯度下降的算法进行优化，而动量编码器则基于在线编码器权重的指数移动平均进行更新。
- en: Most importantly, MoCo keeps a memory bank of the momentum encoder’s embeddings
    and samples negative examples from it to compute the NCE loss. This eliminates
    the need for large batch size.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，MoCo 维护了一个动量编码器嵌入的记忆库，并从中抽取负例来计算 NCE 损失。这消除了对大批量大小的需求。
- en: '![](../Images/e91a19d230f3fa1d4a4daa151155c34c.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e91a19d230f3fa1d4a4daa151155c34c.png)'
- en: 'MoCo architecture. Image source: [LINK](https://generallyintelligent.com/blog/2020-08-24-understanding-self-supervised-contrastive-learning/).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: MoCo 架构。图片来源：[LINK](https://generallyintelligent.com/blog/2020-08-24-understanding-self-supervised-contrastive-learning/)。
- en: Memory banks have been used for contrastive learning before MoCo, but they typically
    stored representations produced by an online encoder. As a result, such a memory
    bank would at the same time store images produced at various stages of the training
    and thus be inconsistent. The introduction of the momentum update based on the
    moving average of the online weights allows MoCo to keep a consistent memory bank,
    a source of good negative examples for the calculation of the loss.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MoCo 之前，记忆库已经被用于对比学习，但它们通常存储由在线编码器生成的表示。因此，这样的记忆库同时存储了在训练的不同阶段生成的图像，从而导致不一致。基于在线权重移动平均的动量更新的引入，使
    MoCo 能够保持一致的记忆库，成为计算损失的良好负例来源。
- en: At the time of its publication, MoCo surpassed top supervised models in many
    different computer vision tasks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在发布时，MoCo 在许多不同的计算机视觉任务中超越了顶级监督模型。
- en: BYOL
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BYOL
- en: '[Bootstrap Your Own Latent](https://arxiv.org/abs/2006.07733), or BYOL is a
    DeepMind’s child. It builds on top of MoCo, also taking advantage of two networks,
    one updated with the moving average of the other’s weights.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[Bootstrap Your Own Latent](https://arxiv.org/abs/2006.07733)（简称 BYOL）是 DeepMind
    的一项成果。它基于 MoCo， 同样利用了两个网络，其中一个网络的权重由另一个网络的移动平均更新。'
- en: However, instead of using a contrastive loss function, BYOL learns to map the
    positive example and the normalized anchor to the same location in the embedding
    space. In other words, the online network is trained to predict the other network’s
    representation. This eliminates the need for negative examples and the memory
    bank altogether.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，BYOL 并没有使用对比损失函数，而是学习将正例和归一化的锚点映射到嵌入空间中的同一位置。换句话说，在线网络被训练去预测另一网络的表示。这消除了对负例和记忆库的需求。
- en: '![](../Images/c0f5b1964c6b7bbaa9f4dd8260329417.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0f5b1964c6b7bbaa9f4dd8260329417.png)'
- en: 'BYOL architecture. Image source: [LINK](https://generallyintelligent.com/blog/2020-08-24-understanding-self-supervised-contrastive-learning/).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: BYOL 架构。图片来源：[LINK](https://generallyintelligent.com/blog/2020-08-24-understanding-self-supervised-contrastive-learning/)。
- en: Although BYOL does not explicitly contrast different images against each other,
    [a thorough investigation](https://generallyintelligent.com/blog/2020-08-24-understanding-self-supervised-contrastive-learning/)
    found that it is actually learning in a contrastive fashion, albeit indirectly.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 BYOL 没有明确地将不同图像进行对比，但 [一次全面调查](https://generallyintelligent.com/blog/2020-08-24-understanding-self-supervised-contrastive-learning/)
    发现它实际上是在以对比的方式进行学习，尽管是间接的。
- en: Others
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他
- en: There exist many other modern self-supervised architectures and more appear
    every almost month, beating the results of their predecessors. Current research
    tends to focus in large measure on the models’ transferability to many diverse
    downstream tasks. Most of them come from Meta researchers. Some notable examples
    are [Barlow Twins](https://arxiv.org/abs/2103.03230), [SwAV](https://arxiv.org/pdf/2006.09882.pdf),
    [SimSiam](https://arxiv.org/abs/2011.10566), and the most recent [Tico](https://arxiv.org/pdf/2206.10698.pdf)
    and [VICRegL](https://arxiv.org/abs/2210.01571).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 目前存在许多其他现代自监督架构，几乎每个月都会出现更多新的架构，这些新架构的结果超越了它们的前任。目前的研究往往更多关注模型在许多不同下游任务中的迁移性。大多数这些研究来自Meta研究人员。一些显著的例子包括[Barlow
    Twins](https://arxiv.org/abs/2103.03230)、[SwAV](https://arxiv.org/pdf/2006.09882.pdf)、[SimSiam](https://arxiv.org/abs/2011.10566)，以及最新的[Tico](https://arxiv.org/pdf/2206.10698.pdf)和[VICRegL](https://arxiv.org/abs/2210.01571)。
- en: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
- en: Choosing transformations
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择变换
- en: We have talked about how self-supervised learning works and how it solves the
    scarce annotations problem, so prevalent with medical data. We have also inspected
    various pretext tasks and state-of-the-art contrastive architectures, which contrast
    the anchor image against its transformed version. The final piece of the puzzle
    we are missing is how to choose transformations to be applied to the anchor image.
    And this choice turns out to be the crucial step to successfully applying self-supervised
    learning to real-world problems.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了自监督学习是如何工作的，以及它如何解决医学数据中普遍存在的稀缺标注问题。我们还检查了各种前置任务和最先进的对比架构，这些架构将锚点图像与其变换版本进行对比。我们缺少的最后一块拼图是如何选择要应用于锚点图像的变换。而这个选择被证明是成功将自监督学习应用于实际问题的关键步骤。
- en: The correct choice of transformations in contrastive learning is crucial to
    successfully solving real-world problems.
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在对比学习中正确选择变换对于成功解决实际问题至关重要。
- en: State-of-the-art literature including the SimCLR and MoCo papers discussed earlier
    claim to have identified the best set of transformations to use. They suggest
    random cropping, color jitter, and blur. The authors have proved these to work
    best on a wide set of downstream tasks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 包括SimCLR和MoCo论文在内的最先进文献声称已识别出最佳的变换集合。他们建议使用随机裁剪、颜色抖动和模糊。作者已经证明这些变换在广泛的下游任务中效果最佳。
- en: Unfortunately, things are not that simple. Different transformations introduce
    different invariances to the model, which might not always be desirable.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，事情并不那么简单。不同的变换会向模型引入不同的不变性，这可能并不总是令人满意的。
- en: What should not be contrastive
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不应对比的内容
- en: A great paper by Xiao et al. titled [What Should Not Be Contrastive in Contrastive
    Learning](https://arxiv.org/abs/2008.05659) illustrates this phenomenon pretty
    neatly.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao等人撰写的一篇优秀论文，[What Should Not Be Contrastive in Contrastive Learning](https://arxiv.org/abs/2008.05659)，很巧妙地展示了这一现象。
- en: 'Consider a dataset consisting of three classes of images: birds, flowers, and
    elephants, and three possible transformations to apply to the anchor images during
    contrastive pre-training: color shift, rotation, and texture change. Depending
    on which transformation you choose, you will be able to solve some downstream
    tasks but not others.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含三类图像的数据集：鸟类、花卉和大象，以及在对比预训练期间可以应用于锚点图像的三种可能变换：颜色偏移、旋转和纹理变化。根据你选择的变换，你将能够解决一些下游任务，但不能解决其他任务。
- en: Different transformations introduce different invariances to the model, which
    might not always be desirable.
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不同的变换会向模型引入不同的不变性，这可能并不总是令人满意的。
- en: 'If you use color shift as your transformation, you will **introduce color invariance
    to the model**: in the contrastive pre-training step, the loss function will force
    the model to encode semantically similar but differently-colored images close
    to one another in the embedding space.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将颜色偏移作为你的变换，你将**向模型引入颜色不变性**：在对比预训练步骤中，损失函数会迫使模型在嵌入空间中将语义相似但颜色不同的图像靠近彼此。
- en: '![](../Images/219203e1c29a80a0bf0d89f75a8fade3.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/219203e1c29a80a0bf0d89f75a8fade3.png)'
- en: 'Different transformations introduce different invariances to the model. Source:
    [LINK](https://arxiv.org/pdf/2008.05659.pdf).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的变换会向模型引入不同的不变性。来源：[LINK](https://arxiv.org/pdf/2008.05659.pdf)。
- en: You can then fine-tune your model to do coarse-grained classification tasks
    such as distinguishing birds from elephants since they differ in so many more
    things than color. However, fine-grained classification tasks such as distinguishing
    between different bird or flower species will be much harder. In these cases,
    the classes often differ mostly in color, and because the model has been taught
    during pre-training to ignore color, it might do poorly at these downstream tasks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以对模型进行微调，以执行粗粒度分类任务，例如区分鸟类和大象，因为它们在许多方面的差异远超过颜色。然而，细粒度分类任务，例如区分不同的鸟类或花卉物种，将会更困难。在这些情况下，类别通常主要通过颜色来区分，而因为模型在预训练期间被教导忽略颜色，它可能在这些下游任务中表现不佳。
- en: The choice of transformations should be guided by the downstream task we want
    to solve.
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 选择转换应该根据我们想要解决的下游任务来指导。
- en: I encourage you to take a moment to look at the figure from the paper above
    and think about how rotation and texture transformations influence which tasks
    the model will be predestined to perform poorly at.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你花一点时间查看上面论文中的图，并思考旋转和纹理转换如何影响模型可能表现不佳的任务。
- en: The takeaway from the examples above is that the choice of transformations should
    be guided by the specifics of the downstream task we want to solve. Pre-training
    with wrong transformations might actually hinder the model from performing well
    down the road.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从以上示例中得到的启示是，转换的选择应该根据我们想要解决的下游任务的具体情况来指导。使用错误的转换进行预训练实际上可能会阻碍模型在后续任务中的表现。
- en: Transformations for chest X-rays
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 胸部 X 光片的转换
- en: Let’s now take a look at how important the transformations choice is for X-ray
    images. Imagine we ignore our downstream task of classifying the images into different
    medical conditions and just follow Google’s and Meta’s researchers' advice and
    use the random crop as our transformation.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看转换选择对 X 光图像的重要性。假设我们忽略将图像分类为不同医学状况的下游任务，而仅仅遵循 Google 和 Meta 的研究人员的建议，使用随机裁剪作为我们的转换。
- en: '![](../Images/626b6245a338bdc0e508ab549399f372.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/626b6245a338bdc0e508ab549399f372.png)'
- en: Choosing the right transformations for X-ray images. Image by the author.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适合 X 光图像的转换。图片来源：作者。
- en: 'Let the salmon-colored circle represent a part of the image that indicates
    a particular condition, some kind of damage in the lung. With random cropping,
    we might end up with a positive example like the one in the figure above: with
    the damaged area cropped out.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让橙色圆圈代表图像中的一部分，指示某种特定的情况，例如肺部的某种损伤。通过随机裁剪，我们可能会得到一个积极的例子，如上图所示：损伤区域被裁剪掉。
- en: The contrastive loss would then teach the model that a lung with the damage
    and a lung without it are similar. Such a pre-training could make it difficult
    to fine-tune the model to recognize this type of lung damage.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对比损失会教会模型损伤的肺和没有损伤的肺是相似的。这种预训练可能使微调模型以识别这种类型的肺损伤变得困难。
- en: The other two allegedly best transformations are also no good with X-ray data.
    Applying color jitter or blur will likely be counter-productive with gray-scale
    images where the shade of gray or a local blur presence might indicate a particular
    medical condition. Once again, the transformations must always be chosen with
    the particular dataset and downstream task in mind.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两个声称最佳的转换对于 X 光数据也不适用。对灰度图像应用颜色抖动或模糊可能会适得其反，因为灰度的阴影或局部模糊可能表明某种特定的医学状况。再次强调，转换必须始终根据特定的数据集和下游任务来选择。
- en: That’s it for theoretical considerations; let’s see self-supervised contrastive
    learning in practice for X-ray classification!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 理论方面的内容就到这里了；让我们看看自监督对比学习在 X 光分类中的实际应用！
- en: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
- en: X-ray classification with self-supervised learning
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自监督学习下的 X 光分类
- en: Together with my colleagues at Tooploox, we set off to discover how much value
    self-supervised learning can bring to medical diagnosis.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与 Tooploox 的同事一起，开始探索自监督学习在医学诊断中的价值。
- en: We have used the [CheXpert dataset](https://aimi.stanford.edu/chexpert-chest-x-rays)
    consisting of around 220k chest X-ray images labeled with ten mutually non-exclusive
    classes indicating different medical conditions and the presence of supporting
    devices in the patient. We have only used the subset of more than 200k frontal
    images.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了[CheXpert数据集](https://aimi.stanford.edu/chexpert-chest-x-rays)，该数据集包含约220k张标注有十个互斥类别的胸部X光图像，指示不同的医疗条件和病人是否使用了辅助设备。我们仅使用了超过200k张正面图像的子集。
- en: We selected a random subset of around 200k images for self-supervised pre-training.
    After a series of experiments, we decided to use a slight random rotation, horizontal
    flip, and random perspective as transformations to apply to the anchor image.
    All images in CheXpert come with labels but we have ignored them for the pre-training
    data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了约200k张图像的随机子集进行自监督预训练。经过一系列实验，我们决定使用轻微的随机旋转、水平翻转和随机透视作为对锚图像应用的变换。所有CheXpert中的图像都有标签，但我们在预训练数据中忽略了这些标签。
- en: '![](../Images/535cb30ca68051daaa0dedba019d68df.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/535cb30ca68051daaa0dedba019d68df.png)'
- en: 'Example image from the CheXpert dataset. Source: [LINK](https://aimi.stanford.edu/chexpert-chest-x-rays).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 来自CheXpert数据集的示例图像。来源：[链接](https://aimi.stanford.edu/chexpert-chest-x-rays)。
- en: 'After pre-training, we fine-tuned the model in a supervised fashion on the
    labeled data sets of varying sizes: from 250 to 10k images. The goal was to study
    how the performance varies with the size of the labeled set.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练之后，我们在不同大小的标注数据集上以监督方式微调了模型：从250张到10k张图像。目标是研究性能如何随着标注集大小的变化而变化。
- en: Finally, we tested the models on a small test of 300 manually labeled images
    (the fine-tuning data labels have been obtained by the dataset authors by automatically
    parsing the patient’s records, which might have introduced some noise to these
    labels; the test labels, on the contrary, are of high-quality thanks to manual
    labeling by doctors).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在300张手动标注的图像上测试了这些模型（微调数据的标签是由数据集作者通过自动解析病人的记录获得的，这可能引入了一些噪声；而测试标签则是由医生手动标注的，质量较高）。
- en: Performance evaluation
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能评估
- en: 'We compare three model architectures:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了三种模型架构：
- en: A traditional transfer learning approach with ResNet18\. It is trained in a
    supervised way only on the labeled fine-tuning set. It reflects the scenario in
    which we do not use self-supervised learning and thus are forced to ignore that
    unlabeled data.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种传统的迁移学习方法，使用ResNet18。仅在标注微调集上以监督方式进行训练。这反映了我们不使用自监督学习的情况，因此不得不忽略那些未标注的数据。
- en: A simple triplet loss model as discussed earlier with the same ResNet18 as a
    backbone, but pre-trained in a contrastive manner using the triplet loss and our
    choice of transformations.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述的简单三重损失模型，使用相同的ResNet18作为骨干，但使用三重损失和我们选择的变换以对比方式进行预训练。
- en: Meta’s MoCo, also with ResNet18 backbone and our transformations set.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta的MoCo，使用相同的ResNet18骨干和我们的变换集。
- en: Each model has been trained and tested ten times, each with a different size
    of the labeled fine-tuning set. We compare them via the area under the ROC curve
    or the AUC.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型已被训练和测试十次，每次使用不同大小的标注微调集。我们通过ROC曲线下面积或AUC来比较它们。
- en: Results
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: The AUC for different architectures and labeled set sizes are shown in the figure
    below.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 不同架构和标注集大小的AUC如下面的图所示。
- en: '![](../Images/4e8971ba6554c7172b4a0119acb1cb0d.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e8971ba6554c7172b4a0119acb1cb0d.png)'
- en: Models comparison. Image by the author.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 模型比较。图片由作者提供。
- en: 'The self-supervised models clearly beat the supervised baseline. There are,
    however, other interesting conclusions to be drawn from these results:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督模型明显超越了监督基准。然而，从这些结果中还有其他有趣的结论：
- en: 'Self-supervised pre-training provides the **largest gain when the labeled set
    is the smallest**: it beats the supervised baseline by 10 percentage points with
    only 250 labeled examples.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自监督预训练在**标注集最小**时提供了**最大的提升**：仅用250个标注样本就超越了监督基准10个百分点。
- en: 'Self-supervised pre-training improves upon the supervised baseline **even when
    the labeled dataset is larger**: with 10k labeled examples, the gain still amounts
    to around 6 percentage points.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自监督预训练**即使在标注数据集较大**时也能改进监督基准：即使有10k个标注样本，提升仍达到约6个百分点。
- en: '**MoCo yields more gain over the baseline than Triplet Loss**, especially when
    the labeled dataset is small.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MoCo相比Triplet Loss在基准上获得的增益更多**，特别是当标注数据集较小时。'
- en: Let’s also take a closer look at the class frequencies in our data. The left
    panel in the figure below shows MoCo’s advantage over the baseline (as measured
    by the AUC difference) for each class separately. The right panel shows the class
    frequencies in the dataset.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地查看数据中的类别频率。下图左侧面板显示了MoCo相对于基线的优势（通过AUC差异来衡量），按每个类别单独计算。右侧面板显示了数据集中各类别的频率。
- en: '![](../Images/0d6efa5b4e42c750f61f69a7895f5341.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d6efa5b4e42c750f61f69a7895f5341.png)'
- en: While self-supervision yields some gain for each of the ten classes, it seems
    to be the largest for relatively rare classes. This is consistent with the results
    from the previous plot showing the strongest improvement for small sizes of the
    labeled set.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自监督学习对每个类别都带来了一些收益，但对于相对稀有的类别收益似乎最大。这与之前的图表结果一致，后者显示了在标记集较小的情况下改进最为显著。
- en: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
- en: Conclusions
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Self-supervised learning for computer vision has made great progress over the
    last three years. Contrastive architectures released by big AI research labs,
    with Meta in the lead, are constantly raising the bar higher and higher. This
    has two major consequences.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习在计算机视觉领域在过去三年里取得了巨大的进展。由大型AI研究实验室发布的对比架构（以Meta为首）不断提高标准。这有两个主要的影响。
- en: First, the ability to efficiently take advantage of unlabeled datasets will
    come as a game changer in many industries where annotated data are scarce.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，高效利用未标记数据集的能力将在许多数据稀缺的行业中引发变革。
- en: And second, training these so-called foundation models that learn from unlabeled
    data to get the background knowledge which can transfer to multiple different
    downstream task is an important step in generalizing AI.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，训练这些所谓的基础模型，从未标记的数据中学习背景知识，并将其转移到多个不同的下游任务中，是推动AI泛化的重要一步。
- en: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
- en: Acknowledgments
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: This article is based on a talk I gave at the Data Science Summit conference
    in Warsaw, Poland on Dec 18, 2022\. Presentation slides are available [here](https://drive.google.com/file/d/1op183zxDNBchTNdih91yDsWbizXkgieR/view).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章基于我在2022年12月18日在波兰华沙举办的数据科学峰会上的演讲。演示文稿幻灯片可以在[这里](https://drive.google.com/file/d/1op183zxDNBchTNdih91yDsWbizXkgieR/view)查看。
- en: Research into applying self-supervised learning to medical applications was
    a joint effort with my colleagues at Tooploox. You can read more about it [on
    the company blog](https://tooploox.com/self-supervised-learning-in-healthcare).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 将自监督学习应用于医疗应用的研究是我与Tooploox的同事们的共同努力。你可以在[公司博客](https://tooploox.com/self-supervised-learning-in-healthcare)上阅读更多内容。
- en: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75d563ec9011ce2508cbf4feb267de73.png)'
- en: Thanks for reading!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you liked this post, why don’t you [**subscribe for email updates**](https://michaloleszak.medium.com/subscribe)
    on my new articles? And by [**becoming a Medium member**](https://michaloleszak.medium.com/membership),
    you can support my writing and get unlimited access to all stories by other authors
    and yours truly.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，为什么不[**订阅电子邮件更新**](https://michaloleszak.medium.com/subscribe)以便获取我新文章的通知呢？通过[**成为Medium会员**](https://michaloleszak.medium.com/membership)，你可以支持我的写作，并无限制访问其他作者以及我自己的所有故事。
- en: Want to always keep your finger on the pulse of the increasingly faster-developing
    field of machine learning and AI? Check out my new newsletter, [**AI Pulse**](https://pulseofai.substack.com/).
    Need consulting? You can ask me anything or book me for a 1:1 [**here**](https://topmate.io/michaloleszak).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 想要始终保持对不断加速发展的机器学习和AI领域的关注吗？查看我的新通讯[**AI Pulse**](https://pulseofai.substack.com/)。需要咨询？你可以[**在这里**](https://topmate.io/michaloleszak)问我任何问题或预约1对1咨询。
- en: 'You can also try one of [my other articles](https://michaloleszak.github.io/blog/).
    Can’t choose? Pick one of these:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以尝试[我其他的文章](https://michaloleszak.github.io/blog/)。无法决定？选择以下其中之一：
- en: '[](/monte-carlo-dropout-7fd52f8b6571?source=post_page-----fd43719b1625--------------------------------)
    [## Monte Carlo Dropout'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/monte-carlo-dropout-7fd52f8b6571?source=post_page-----fd43719b1625--------------------------------)
    [## 蒙特卡洛 Dropout'
- en: Improve your neural network for free with one small trick, getting model uncertainty
    estimate as a bonus.
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用一个小技巧免费提高你的神经网络，同时获得模型不确定性的估计作为额外福利。
- en: towardsdatascience.com](/monte-carlo-dropout-7fd52f8b6571?source=post_page-----fd43719b1625--------------------------------)
    [](/on-the-importance-of-bayesian-thinking-in-everyday-life-a74475fcceeb?source=post_page-----fd43719b1625--------------------------------)
    [## On the Importance of Bayesian Thinking in Everyday Life
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/monte-carlo-dropout-7fd52f8b6571?source=post_page-----fd43719b1625--------------------------------)
    [](/on-the-importance-of-bayesian-thinking-in-everyday-life-a74475fcceeb?source=post_page-----fd43719b1625--------------------------------)
    [## 生活中贝叶斯思维的重要性'
- en: This simple mind-shift will help you better understand the uncertain world around
    you
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这种简单的思维转变将帮助你更好地理解你周围不确定的世界。
- en: 'towardsdatascience.com](/on-the-importance-of-bayesian-thinking-in-everyday-life-a74475fcceeb?source=post_page-----fd43719b1625--------------------------------)
    [](/establishing-causality-part-4-5d3b5e917790?source=post_page-----fd43719b1625--------------------------------)
    [## Establishing Causality: Part 4'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/on-the-importance-of-bayesian-thinking-in-everyday-life-a74475fcceeb?source=post_page-----fd43719b1625--------------------------------)
    [](/establishing-causality-part-4-5d3b5e917790?source=post_page-----fd43719b1625--------------------------------)
    [## 确立因果关系：第4部分'
- en: Taking advantage of policy shifts with Difference-In-Differences
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用政策变动的差分法
- en: towardsdatascience.com](/establishing-causality-part-4-5d3b5e917790?source=post_page-----fd43719b1625--------------------------------)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/establishing-causality-part-4-5d3b5e917790?source=post_page-----fd43719b1625--------------------------------)'
