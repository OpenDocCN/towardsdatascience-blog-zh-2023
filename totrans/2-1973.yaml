- en: Text Classification with Transformer Encoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/text-classification-with-transformer-encoders-1dcaa50dabae](https://towardsdatascience.com/text-classification-with-transformer-encoders-1dcaa50dabae)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Step-by-step explanation of utilizing Transformer encoders to classify text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellusruben?source=post_page-----1dcaa50dabae--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----1dcaa50dabae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1dcaa50dabae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1dcaa50dabae--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----1dcaa50dabae--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1dcaa50dabae--------------------------------)
    ·15 min read·Aug 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7af55b2adc4de88204e68d923263996.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mel Poole](https://unsplash.com/@melpoole?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/lBsvzgYnzPU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Transformer is, without a doubt, one of the most important breakthroughs in
    the field of deep learning. The encoder-decoder architecture of this model has
    proven to be powerful in cross-domain applications.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, Transformer was used solely for language modeling tasks, such as
    machine translation, text generation, text classification, question-answering,
    etc. However, recently, Transformer has also been used for computer vision tasks,
    such as image classification, object detection, and semantic segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Given its popularity and the existence of numerous Transformer-based sophisticated
    models such as BERT, Vision-Transformer, Swin-Transformer, and the GPT family,
    it is crucial for us to understand the inner workings of the Transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will dissect only the encoder part of Transformer, which
    can be used mainly for classification purposes. Specifically, we will use the
    Transformer encoders to classify texts. Without further ado, let’s first take
    a look at the dataset that we’re going to use in this article.
  prefs: []
  type: TYPE_NORMAL
- en: About the Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset that we’re going to use is the email dataset. You can download
    this dataset on Kaggle via this [**link**](https://www.kaggle.com/datasets/team-ai/spam-text-message-classification?resource=download).
    This dataset is licensed under CC0: Public Domain, which means that you can use
    and distribute this dataset freely.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The task is very simple: it’s a binary classification problem and given a text
    of an email, our Transformer encoder model needs to predict whether that text
    is a spam or not.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s create a label mapping from the label to its index, i.e *‘ham’*
    would be 0 and *‘spam’* would be 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s get into the overall workflow of a Transformer encoder model.
  prefs: []
  type: TYPE_NORMAL
- en: How Transformer Encoder Works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand how Transformer encoder works, let’s start from the very beginning
    of the process, which is data preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: As you already know, we will be dealing with text data in this article and Transformer
    can’t process text in its raw format. Hence, what we’re going to do first is transform
    our text into a machine-readable format, which can be achieved by the tokenization
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tokenization is the process of splitting the input text into tokens. A token
    can consist of one character, one word, or one subword, depending on the type
    of tokenizer used. In this post, we will be using word-level tokenization, which
    means that each token represents one word.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, each token will be mapped into its integer representation according to
    the so-called vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Vocabulary is basically a collection of characters, words, or subwords and their
    integer mappings. Since we’re tokenizing our text at a word-level, then our vocabulary
    would be a collection of words and their integer mappings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build a vocabulary based on our training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the code snippet above, each word in our training data has
    its own unique integer in our vocabulary. If you notice, we also add two special
    tokens called ***<unk>*** and ***<pad>*** into our vocabulary. The ***<pad>***
    token is useful for batch training later on, to make sure that every batch of
    our training data has the same sequence length.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the ***<unk>*** token is useful for handling out-of-vocabulary words.
    Whenever we encounter a word that is not available in our vocabulary, it will
    be assigned as ***<unk>*** token.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s create a toy example that we’ll use throughout the entire article.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Embedding Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The integer representation of each token is what we pass as an input to the
    very first layer of a Transformer encoder model, which is the embedding layer.
    This layer will transform each integer into a vector with a particular dimension
    that we set in advance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The dimension of each vector normally corresponds to the hidden size that we
    choose for our Transformer model. As an example, BERT-base model has a hidden
    size of 768.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, each token in our sequence (*[‘this’, ‘is’, ‘text’])*
    will be transformed into 4D vector embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The output of embedding layer is a tensor of `[batch, sequence_length,embedding_dim]`
    .
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/577e942e3c5a5e62f8e033ad565d2067.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Positional Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have obtained the embeddings of each token in our sequence, but these
    embeddings don’t have the sense of order. Meanwhile, we know that the order of
    words in any text and language is crucial to capture the semantic meaning of a
    sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'To capture the order of our input sequence, Transformer applies a method called
    positional encoding. There are many ways we can apply positional encoding, but
    it should fulfill the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: The encoding should be unique for each token in the sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The delta value or distance between any two neighboring tokens should be consistent
    and independent of sequence lengths.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoding should be deterministic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And it should also generalizes well when we have longer sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the original Transformer paper, the authors proposed a positional encoding
    method that utilizes a combination of sine and cosine waves. This approach fulfills
    all the mentioned conditions and enables the model to capture the sequential order
    of tokens effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff109a2e9c753843ecdb58bbb47f34e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The position encoding should have the same dimension as the token embedding
    so that we can add our position encoding into token embedding. Also, position
    encodings are fixed, meaning that there is no learnable parameter to be updated
    during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86f1a2b93993e91df7f42feb071ab7b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output embeddings from the addition of token embeddings and positional encodings
    would be the input to the next step, which is the Transformer encoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A Transformer encoder stack consists of several parts, as you can see in the
    image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1c2bf1c5c2d0dbc2f8461e0b53ecb58.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, our input embeddings will enter the so-called self-attention
    layer. This layer is the major factor why Transformer-based language models are
    able to differentiate the context of each word and the semantic meaning of a whole
    sequence/sentence.
  prefs: []
  type: TYPE_NORMAL
- en: The self-attention layer will project the input embeddings into query, key,
    and value vectors using separate linear layers. Query, key, and value are the
    terms that we usually find in retrieval systems or recommendation systems.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s say that you want to see a specific movie on Netflix. The
    query would be the name of the movie title that you type in the search bar; the
    key would be the description of each movie on Netflix’s catalog; and the values
    would be the result of movie recommendations based on the movie title you entered
    in the search bar before.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/abf748714f2aecf8680e0eb6025f8a44.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the visualization above, the query, key, and values are
    all coming from the same source. This is why this attention mechanism is called
    self-attention.
  prefs: []
  type: TYPE_NORMAL
- en: If you use the full Transformer architecture (with the decoder part) for autoregressive
    task like machine translation, then there will also be another attention mechanism
    called cross-attention where the query comes from the decoder, but the key and
    values come from the encoder stack. However, we’re not going to address cross-attention
    mechanism in this article since we will only use the encoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: After we get the query, key, and values, then we are ready to perform self-attention
    mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: First, we multiply the query with the key (also called dot product operation).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b40047595221014c28b02efd8f7362fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: What we get from dot production operation is a square attention matrix with
    the size equal to the number of input tokens in our sequence in both dimensions.
    This matrix indicates the attention or relevance each token should give to the
    other tokens in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we normalize the attention matrix with the dimension of our linear layer
    to obtain stable gradient during the training process. Then, we normalize the
    matrix with Softmax function such that the value in each row of our matrix will
    all be positive and add up to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f24c358751b5673e0a404eedf419ffb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The final step of self-attention mechanism is another dot product between the
    values and our normalized attention matrix. This will give us a final output with
    the size of `[batch, no_of_sequence, hidden_size_dim]` .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Multi-Head Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: However, the Transformer model doesn’t use only one self-attention block, or
    ‘head’ as it is normally called. It uses multi-head attention, in which multiple
    single self-attentions are conducted in parallel. The minor difference is that
    we need to divide the output of the three linear layers in each single-head attention
    with the total number of heads that we use. This ensures that the computation
    time of multi-head attention is comparable to single self-attention.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26db5a72a8804e11fbd0b7634d1889b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we need to concatenate the output from each single self-attention
    layer and then project it into an additional linear layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc6c12dd3d81ed5c4cf015fd180bd838.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it. The output tensor of this multi-head attention layer has the
    same dimensionality as the input.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Normalization Layer and Residual Connection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we take a look at the architecture of Transformer encoder block, we need
    to add the output of multi-head attention with the input of multi-head attention
    (also called residual connection) and then normalize it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d7276c23529873dfcb06e8da067eddd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The reason behind these two operations is so that the Transformer model can
    converge faster during training process and they can also help the model to perform
    more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Again, the output tensor dimension after the residual connection and normalization
    layer would be the same as the output tensor dimension of multi-head attention
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Feed Forward Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The output of residual connection and normalization layer then become the input
    of a feed-forward layer. This layer is just an ordinary linear layer, as you can
    see below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This layer also won’t change the dimension of our tensor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: After the feed-forward layer, we need to apply the second residual connection,
    in which we add the output of feed-forward layer with the input of feed-forward
    layer. After the addition, we normalize the tensor with normalization layer as
    described in Normalization Layer section above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Transformer Encoder Stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process from multi-head self-attention layer until the normalization layer
    after the feed-forward layer above corresponds to one single Transformer encoder
    stack.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5303b2845ece723e4d07b4c5800419a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can encapsulate all of the process above in a class called `SingleEncoder()`
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In its application, we usually use several Transformer encoders instead of just
    one. BERT-base model, for example, uses 12 stacks of Transformer encoders.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: With the `EncoderBlocks()` above, then we can initialize several stacks of Transformer
    encoders according to our need.
  prefs: []
  type: TYPE_NORMAL
- en: Model Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know the inner architecture of a Transformer encoder, now let’s
    use it to train our data for a text classification purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Model Definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we will use six stacks of Transformer encoders. The hidden
    size would be 300 and there will be four different heads in the multi-head self-attention
    layer. You can tweak these values according to your own need.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If you notice, we also add an additional linear layer on top of the output of
    the last Transformer encoder stack. This linear layer will act as the classifier.
    Since we only have two distinct classes (spam/ham), then the output of this linear
    layer would be two.
  prefs: []
  type: TYPE_NORMAL
- en: Also, one more important thing that we need to address is the fact that the
    output of the final stack would be `[batch, no_of_sequence, hidden_size]` , while
    our final linear layer expects an input of `[batch, hidden_size]` . There are
    several methods that we can do to match the output of the stack with the input
    of linear layer.
  prefs: []
  type: TYPE_NORMAL
- en: BERT, for example, uses only the output of a special token called ***[CLS]***
    that is prepended into our sequence before the positional encoding steps in the
    Transformer architecture above. Here, we don’t have that special ***[CLS]*** token.
    Thus, what we do instead is averaging all of the output embedding values after
    the last encoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: Data Loader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we need to create a data loader for our training data such that it will
    be supplied into our model in batches during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In addition to the dataloader class, we also need to create supplementary function
    called `collate_fn()` above. This function is essential because, in order to supply
    our training data in batches, each batch needs to have the same dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Since we’re dealing with text data with varying sentence length, then the dimension
    of each batch isn’t guaranteed to be the same. In the `collate_fn` , we first
    fetch the maximum length of a sequence in a batch, and then add a bunch of ***<pad>***
    tokens to the shorter sequence until its length equals to the length of the longest
    sequence in the batch.
  prefs: []
  type: TYPE_NORMAL
- en: Another method that you can use is by defining the maximum number of tokens.
    Next, you can truncate the sentence if it has more tokens than the maximum value
    or add a bunch of ***<pad>*** tokensif it has fewer tokens than the maximum value.
  prefs: []
  type: TYPE_NORMAL
- en: Training Loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have defined the model architecture and the data loader class, then
    we can start to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'And you’ll get the output that looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88ed0aea8063b6f56ced477e5dad1d3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Model Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After we train the model, we can naturally use it to predict unseen data on
    our test set. To do so, first we need to create a function that encapsulates data
    preprocessing step and the model prediction step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if we want to predict a text from our test set, we can just call the function
    above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have discussed the step-by-step process to utilize the encoder
    part of Transformer to classify text. As you already know, there are a lot of
    large language models out there that use the encoder part of Transformer. BERT,
    as an example, achieved state-of-the-art performance in many language tasks, thanks
    to its Transformer-encoder architecture combined with a large corpus of training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article helps you to getting started with Transformer architecture.
    As usual, you can find the code implemented in this article via [**this notebook**](https://github.com/marcellusruben/medium-resources/blob/main/Text_Classification_Transformer_Encoders/Transformer_Encoder.ipynb).
  prefs: []
  type: TYPE_NORMAL
