- en: Text Classification with Transformer Encoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Transformer 编码器进行文本分类
- en: 原文：[https://towardsdatascience.com/text-classification-with-transformer-encoders-1dcaa50dabae](https://towardsdatascience.com/text-classification-with-transformer-encoders-1dcaa50dabae)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/text-classification-with-transformer-encoders-1dcaa50dabae](https://towardsdatascience.com/text-classification-with-transformer-encoders-1dcaa50dabae)
- en: Step-by-step explanation of utilizing Transformer encoders to classify text
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Transformer 编码器进行文本分类的逐步说明
- en: '[](https://medium.com/@marcellusruben?source=post_page-----1dcaa50dabae--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----1dcaa50dabae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1dcaa50dabae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1dcaa50dabae--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----1dcaa50dabae--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellusruben?source=post_page-----1dcaa50dabae--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----1dcaa50dabae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1dcaa50dabae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1dcaa50dabae--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----1dcaa50dabae--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1dcaa50dabae--------------------------------)
    ·15 min read·Aug 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1dcaa50dabae--------------------------------)
    ·阅读时间 15 分钟·2023年8月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e7af55b2adc4de88204e68d923263996.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7af55b2adc4de88204e68d923263996.png)'
- en: Photo by [Mel Poole](https://unsplash.com/@melpoole?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/lBsvzgYnzPU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[Mel Poole](https://unsplash.com/@melpoole?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    的照片，来自[Unsplash](https://unsplash.com/photos/lBsvzgYnzPU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)'
- en: Transformer is, without a doubt, one of the most important breakthroughs in
    the field of deep learning. The encoder-decoder architecture of this model has
    proven to be powerful in cross-domain applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 毋庸置疑，Transformer 是深度学习领域最重要的突破之一。该模型的编码器-解码器架构在跨领域应用中证明了其强大功能。
- en: Initially, Transformer was used solely for language modeling tasks, such as
    machine translation, text generation, text classification, question-answering,
    etc. However, recently, Transformer has also been used for computer vision tasks,
    such as image classification, object detection, and semantic segmentation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，Transformer 仅用于语言建模任务，如机器翻译、文本生成、文本分类、问答等。然而，最近 Transformer 也被用于计算机视觉任务，如图像分类、物体检测和语义分割。
- en: Given its popularity and the existence of numerous Transformer-based sophisticated
    models such as BERT, Vision-Transformer, Swin-Transformer, and the GPT family,
    it is crucial for us to understand the inner workings of the Transformer architecture.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于其受欢迎程度以及存在许多基于 Transformer 的复杂模型，如 BERT、Vision-Transformer、Swin-Transformer
    和 GPT 家族，我们必须深入了解 Transformer 架构的内部工作原理。
- en: In this article, we will dissect only the encoder part of Transformer, which
    can be used mainly for classification purposes. Specifically, we will use the
    Transformer encoders to classify texts. Without further ado, let’s first take
    a look at the dataset that we’re going to use in this article.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将仅分析 Transformer 的编码器部分，该部分主要用于分类目的。具体来说，我们将使用 Transformer 编码器来分类文本。事不宜迟，让我们首先查看一下我们将在本文中使用的数据集。
- en: About the Dataset
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于数据集
- en: 'The dataset that we’re going to use is the email dataset. You can download
    this dataset on Kaggle via this [**link**](https://www.kaggle.com/datasets/team-ai/spam-text-message-classification?resource=download).
    This dataset is licensed under CC0: Public Domain, which means that you can use
    and distribute this dataset freely.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集是电子邮件数据集。您可以通过这个 [**链接**](https://www.kaggle.com/datasets/team-ai/spam-text-message-classification?resource=download)
    在 Kaggle 上下载此数据集。该数据集的许可证为 CC0：公共领域，这意味着您可以自由使用和分发此数据集。
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The task is very simple: it’s a binary classification problem and given a text
    of an email, our Transformer encoder model needs to predict whether that text
    is a spam or not.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 任务非常简单：这是一个二分类问题，给定一封电子邮件的文本，我们的 Transformer 编码器模型需要预测该文本是否为垃圾邮件。
- en: Next, let’s create a label mapping from the label to its index, i.e *‘ham’*
    would be 0 and *‘spam’* would be 1.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个从标签到其索引的映射，例如 *‘ham’* 将是 0，而 *‘spam’* 将是 1。
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now let’s get into the overall workflow of a Transformer encoder model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们了解 Transformer 编码器模型的整体工作流程。
- en: How Transformer Encoder Works
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 编码器如何工作
- en: To understand how Transformer encoder works, let’s start from the very beginning
    of the process, which is data preprocessing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 Transformer 编码器如何工作，我们从过程的最开始部分开始，即数据预处理。
- en: As you already know, we will be dealing with text data in this article and Transformer
    can’t process text in its raw format. Hence, what we’re going to do first is transform
    our text into a machine-readable format, which can be achieved by the tokenization
    process.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所知道的，我们将在本文中处理文本数据，而 Transformer 无法处理原始文本。因此，我们首先要做的就是将文本转换为机器可读的格式，这可以通过标记化过程来实现。
- en: Tokenization
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记化
- en: Tokenization is the process of splitting the input text into tokens. A token
    can consist of one character, one word, or one subword, depending on the type
    of tokenizer used. In this post, we will be using word-level tokenization, which
    means that each token represents one word.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化是将输入文本拆分为标记的过程。一个标记可以由一个字符、一个单词或一个子词组成，这取决于使用的标记化器类型。在这篇文章中，我们将使用单词级标记化，这意味着每个标记代表一个单词。
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, each token will be mapped into its integer representation according to
    the so-called vocabulary.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，每个标记将根据所谓的词汇表映射到其整数表示。
- en: Vocabulary is basically a collection of characters, words, or subwords and their
    integer mappings. Since we’re tokenizing our text at a word-level, then our vocabulary
    would be a collection of words and their integer mappings.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表基本上是字符、单词或子词及其整数映射的集合。由于我们在单词级别进行标记化，因此我们的词汇表将是单词及其整数映射的集合。
- en: 'Let’s build a vocabulary based on our training dataset:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们基于训练数据集建立一个词汇表：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see from the code snippet above, each word in our training data has
    its own unique integer in our vocabulary. If you notice, we also add two special
    tokens called ***<unk>*** and ***<pad>*** into our vocabulary. The ***<pad>***
    token is useful for batch training later on, to make sure that every batch of
    our training data has the same sequence length.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如上面的代码片段所示，我们训练数据中的每个单词在词汇表中都有其独特的整数。如果你注意到，我们还将两个特殊标记 ***<unk>*** 和 ***<pad>***
    添加到词汇表中。***<pad>*** 标记对于后续的批量训练非常有用，以确保每个训练数据批次具有相同的序列长度。
- en: Meanwhile, the ***<unk>*** token is useful for handling out-of-vocabulary words.
    Whenever we encounter a word that is not available in our vocabulary, it will
    be assigned as ***<unk>*** token.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，***<unk>*** 标记对处理词汇表外的单词非常有用。每当我们遇到一个词汇表中没有的单词时，它将被分配为 ***<unk>*** 标记。
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now let’s create a toy example that we’ll use throughout the entire article.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个玩具示例，贯穿整个文章。
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Embedding Layer
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入层
- en: The integer representation of each token is what we pass as an input to the
    very first layer of a Transformer encoder model, which is the embedding layer.
    This layer will transform each integer into a vector with a particular dimension
    that we set in advance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标记的整数表示就是我们传递给 Transformer 编码器模型第一层的输入，这一层是嵌入层。该层将每个整数转换为我们预先设置维度的向量。
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The dimension of each vector normally corresponds to the hidden size that we
    choose for our Transformer model. As an example, BERT-base model has a hidden
    size of 768.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 每个向量的维度通常对应于我们为 Transformer 模型选择的隐藏层大小。例如，BERT-base 模型的隐藏层大小为 768。
- en: In the following example, each token in our sequence (*[‘this’, ‘is’, ‘text’])*
    will be transformed into 4D vector embeddings.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们序列中的每个标记 (*[‘this’, ‘is’, ‘text’]*) 将被转换为 4D 向量嵌入。
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output of embedding layer is a tensor of `[batch, sequence_length,embedding_dim]`
    .
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的输出是一个 `[batch, sequence_length, embedding_dim]` 的张量。
- en: '![](../Images/577e942e3c5a5e62f8e033ad565d2067.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/577e942e3c5a5e62f8e033ad565d2067.png)'
- en: Image by author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Positional Encoding
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码
- en: So far, we have obtained the embeddings of each token in our sequence, but these
    embeddings don’t have the sense of order. Meanwhile, we know that the order of
    words in any text and language is crucial to capture the semantic meaning of a
    sentence.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经获得了序列中每个标记的嵌入，但这些嵌入并没有顺序感。同时，我们知道在任何文本和语言中，词语的顺序对捕捉句子的语义意义至关重要。
- en: 'To capture the order of our input sequence, Transformer applies a method called
    positional encoding. There are many ways we can apply positional encoding, but
    it should fulfill the following conditions:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉输入序列的顺序，Transformer应用了一种叫做位置编码的方法。我们可以使用多种方式来应用位置编码，但它应满足以下条件：
- en: The encoding should be unique for each token in the sequence.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码应该对序列中的每个标记都是唯一的。
- en: The delta value or distance between any two neighboring tokens should be consistent
    and independent of sequence lengths.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任意两个相邻标记之间的**delta**值或距离应该是一致的，并且与序列长度无关。
- en: The encoding should be deterministic.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码应该是确定性的。
- en: And it should also generalizes well when we have longer sequence.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并且它在我们处理更长序列时也应表现出良好的泛化能力。
- en: In the original Transformer paper, the authors proposed a positional encoding
    method that utilizes a combination of sine and cosine waves. This approach fulfills
    all the mentioned conditions and enables the model to capture the sequential order
    of tokens effectively.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始Transformer论文中，作者提出了一种利用正弦和余弦波组合的位置信息编码方法。这种方法满足了所有提到的条件，使模型能够有效地捕捉标记的顺序。
- en: '![](../Images/ff109a2e9c753843ecdb58bbb47f34e4.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff109a2e9c753843ecdb58bbb47f34e4.png)'
- en: Image by author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The position encoding should have the same dimension as the token embedding
    so that we can add our position encoding into token embedding. Also, position
    encodings are fixed, meaning that there is no learnable parameter to be updated
    during the training process.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码应该具有与标记嵌入相同的维度，以便我们可以将位置编码添加到标记嵌入中。此外，位置编码是固定的，这意味着在训练过程中没有可更新的可学习参数。
- en: '![](../Images/86f1a2b93993e91df7f42feb071ab7b2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86f1a2b93993e91df7f42feb071ab7b2.png)'
- en: Image by author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output embeddings from the addition of token embeddings and positional encodings
    would be the input to the next step, which is the Transformer encoder stack.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从标记嵌入和位置编码的加法中得到的输出嵌入将成为下一步骤的输入，即Transformer编码器堆栈。
- en: Self-Attention
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自注意力
- en: 'A Transformer encoder stack consists of several parts, as you can see in the
    image below:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer编码器堆栈由几个部分组成，如下图所示：
- en: '![](../Images/f1c2bf1c5c2d0dbc2f8461e0b53ecb58.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1c2bf1c5c2d0dbc2f8461e0b53ecb58.png)'
- en: Image by author
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: As a first step, our input embeddings will enter the so-called self-attention
    layer. This layer is the major factor why Transformer-based language models are
    able to differentiate the context of each word and the semantic meaning of a whole
    sequence/sentence.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们的输入嵌入将进入所谓的自注意力层。这一层是Transformer语言模型能够区分每个词的上下文和整个序列/句子的语义意义的主要因素。
- en: The self-attention layer will project the input embeddings into query, key,
    and value vectors using separate linear layers. Query, key, and value are the
    terms that we usually find in retrieval systems or recommendation systems.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力层将使用不同的线性层将输入嵌入投影到查询、键和值向量中。查询、键和值是我们通常在检索系统或推荐系统中找到的术语。
- en: As an example, let’s say that you want to see a specific movie on Netflix. The
    query would be the name of the movie title that you type in the search bar; the
    key would be the description of each movie on Netflix’s catalog; and the values
    would be the result of movie recommendations based on the movie title you entered
    in the search bar before.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你想在Netflix上观看特定的电影。查询将是你在搜索栏中输入的电影标题；键将是Netflix目录中每部电影的描述；值将是基于你之前在搜索栏中输入的电影标题的电影推荐结果。
- en: '![](../Images/abf748714f2aecf8680e0eb6025f8a44.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abf748714f2aecf8680e0eb6025f8a44.png)'
- en: Image by author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: As you can see from the visualization above, the query, key, and values are
    all coming from the same source. This is why this attention mechanism is called
    self-attention.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上面的可视化所示，查询、键和值都来自同一来源。这就是为什么这种注意力机制被称为自注意力。
- en: If you use the full Transformer architecture (with the decoder part) for autoregressive
    task like machine translation, then there will also be another attention mechanism
    called cross-attention where the query comes from the decoder, but the key and
    values come from the encoder stack. However, we’re not going to address cross-attention
    mechanism in this article since we will only use the encoder stack.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用完整的Transformer架构（包括解码器部分）来进行像机器翻译这样的自回归任务，那么还会有另一个称为交叉注意力的注意力机制，其中查询来自解码器，而键和值来自编码器堆栈。然而，由于我们只使用编码器堆栈，所以在本文中不会讨论交叉注意力机制。
- en: After we get the query, key, and values, then we are ready to perform self-attention
    mechanism.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取查询、键和值之后，我们就可以执行自注意力机制了。
- en: First, we multiply the query with the key (also called dot product operation).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查询与键进行相乘（也称为点积操作）。
- en: '![](../Images/b40047595221014c28b02efd8f7362fe.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b40047595221014c28b02efd8f7362fe.png)'
- en: Image by author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: What we get from dot production operation is a square attention matrix with
    the size equal to the number of input tokens in our sequence in both dimensions.
    This matrix indicates the attention or relevance each token should give to the
    other tokens in the sequence.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从点积操作中得到的是一个方形的注意力矩阵，其尺寸在两个维度上都等于序列中输入标记的数量。该矩阵指示了每个标记应该给序列中其他标记的注意力或相关性。
- en: Next, we normalize the attention matrix with the dimension of our linear layer
    to obtain stable gradient during the training process. Then, we normalize the
    matrix with Softmax function such that the value in each row of our matrix will
    all be positive and add up to 1.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用线性层的维度对注意力矩阵进行归一化，以在训练过程中获得稳定的梯度。然后，我们使用Softmax函数对矩阵进行归一化，使矩阵中每一行的值都为正且总和为1。
- en: '![](../Images/4f24c358751b5673e0a404eedf419ffb.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f24c358751b5673e0a404eedf419ffb.png)'
- en: Image by author
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The final step of self-attention mechanism is another dot product between the
    values and our normalized attention matrix. This will give us a final output with
    the size of `[batch, no_of_sequence, hidden_size_dim]` .
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制的最后一步是值与我们的归一化注意力矩阵之间的另一个点积。这将给我们一个大小为`[batch, no_of_sequence, hidden_size_dim]`的最终输出。
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Multi-Head Attention
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力
- en: However, the Transformer model doesn’t use only one self-attention block, or
    ‘head’ as it is normally called. It uses multi-head attention, in which multiple
    single self-attentions are conducted in parallel. The minor difference is that
    we need to divide the output of the three linear layers in each single-head attention
    with the total number of heads that we use. This ensures that the computation
    time of multi-head attention is comparable to single self-attention.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Transformer模型不仅仅使用一个自注意力块，或通常称为“头”。它使用多头注意力，其中多个单自注意力并行进行。小的区别在于，我们需要将每个单头注意力中的三个线性层的输出除以我们使用的头的总数。这确保了多头注意力的计算时间与单自注意力相当。
- en: '![](../Images/26db5a72a8804e11fbd0b7634d1889b8.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26db5a72a8804e11fbd0b7634d1889b8.png)'
- en: Image by author
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In the end, we need to concatenate the output from each single self-attention
    layer and then project it into an additional linear layer.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将每个自注意力层的输出连接起来，然后将其投影到一个额外的线性层中。
- en: '![](../Images/fc6c12dd3d81ed5c4cf015fd180bd838.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc6c12dd3d81ed5c4cf015fd180bd838.png)'
- en: Image by author
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: And that’s it. The output tensor of this multi-head attention layer has the
    same dimensionality as the input.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。这个多头注意力层的输出张量与输入的维度相同。
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Normalization Layer and Residual Connection
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归一化层和残差连接
- en: If we take a look at the architecture of Transformer encoder block, we need
    to add the output of multi-head attention with the input of multi-head attention
    (also called residual connection) and then normalize it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看Transformer编码器块的架构，我们需要将多头注意力的输出与多头注意力的输入（也称为残差连接）相加，然后对其进行归一化。
- en: '![](../Images/9d7276c23529873dfcb06e8da067eddd.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d7276c23529873dfcb06e8da067eddd.png)'
- en: Image by author
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The reason behind these two operations is so that the Transformer model can
    converge faster during training process and they can also help the model to perform
    more accurately.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个操作的原因是为了使Transformer模型在训练过程中能够更快收敛，并且它们还可以帮助模型更准确地进行预测。
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Again, the output tensor dimension after the residual connection and normalization
    layer would be the same as the output tensor dimension of multi-head attention
    layer.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，残差连接和归一化层后的输出张量维度将与多头注意力层的输出张量维度相同。
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Feed Forward Layer
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前馈层
- en: 'The output of residual connection and normalization layer then become the input
    of a feed-forward layer. This layer is just an ordinary linear layer, as you can
    see below:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接和归一化层的输出将成为前馈层的输入。这个层只是一个普通的线性层，如下所示：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This layer also won’t change the dimension of our tensor.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层也不会改变我们张量的维度。
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: After the feed-forward layer, we need to apply the second residual connection,
    in which we add the output of feed-forward layer with the input of feed-forward
    layer. After the addition, we normalize the tensor with normalization layer as
    described in Normalization Layer section above.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在前馈层之后，我们需要应用第二个残差连接，将前馈层的输出与前馈层的输入相加。加法之后，我们使用上面描述的归一化层对张量进行归一化。
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Transformer Encoder Stack
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 编码器堆栈
- en: The process from multi-head self-attention layer until the normalization layer
    after the feed-forward layer above corresponds to one single Transformer encoder
    stack.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从多头自注意力层到前馈层之后的归一化层的过程对应一个单独的 Transformer 编码器堆栈。
- en: '![](../Images/5303b2845ece723e4d07b4c5800419a0.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5303b2845ece723e4d07b4c5800419a0.png)'
- en: Image by author
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Now we can encapsulate all of the process above in a class called `SingleEncoder()`
    below:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将上述所有过程封装在一个名为 `SingleEncoder()` 的类中：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In its application, we usually use several Transformer encoders instead of just
    one. BERT-base model, for example, uses 12 stacks of Transformer encoders.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，我们通常使用多个 Transformer 编码器，而不是仅使用一个。例如，BERT-base 模型使用了 12 个 Transformer
    编码器堆栈。
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: With the `EncoderBlocks()` above, then we can initialize several stacks of Transformer
    encoders according to our need.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述 `EncoderBlocks()`，我们可以根据需要初始化多个 Transformer 编码器堆栈。
- en: Model Training
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: Now that we know the inner architecture of a Transformer encoder, now let’s
    use it to train our data for a text classification purpose.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了 Transformer 编码器的内部结构，接下来让我们使用它来训练数据以进行文本分类。
- en: Model Definition
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型定义
- en: In this article, we will use six stacks of Transformer encoders. The hidden
    size would be 300 and there will be four different heads in the multi-head self-attention
    layer. You can tweak these values according to your own need.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将使用六个 Transformer 编码器堆栈。隐藏层大小为 300，多头自注意力层中将有四个不同的头。你可以根据自己的需要调整这些值。
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If you notice, we also add an additional linear layer on top of the output of
    the last Transformer encoder stack. This linear layer will act as the classifier.
    Since we only have two distinct classes (spam/ham), then the output of this linear
    layer would be two.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你注意到，我们在最后一个 Transformer 编码器堆栈的输出上添加了一个额外的线性层。这个线性层将作为分类器。由于我们只有两个不同的类别（垃圾邮件/正常邮件），因此这个线性层的输出将是二。
- en: Also, one more important thing that we need to address is the fact that the
    output of the final stack would be `[batch, no_of_sequence, hidden_size]` , while
    our final linear layer expects an input of `[batch, hidden_size]` . There are
    several methods that we can do to match the output of the stack with the input
    of linear layer.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们需要解决的一个重要问题是，最终堆栈的输出将是`[batch, no_of_sequence, hidden_size]`，而我们的最终线性层期望的输入是`[batch,
    hidden_size]`。我们可以采用几种方法来使堆栈的输出与线性层的输入匹配。
- en: BERT, for example, uses only the output of a special token called ***[CLS]***
    that is prepended into our sequence before the positional encoding steps in the
    Transformer architecture above. Here, we don’t have that special ***[CLS]*** token.
    Thus, what we do instead is averaging all of the output embedding values after
    the last encoder stack.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，BERT 只使用了一个特殊的称为 ***[CLS]*** 的标记，该标记在 Transformer 架构中的位置编码步骤之前插入到序列中。在这里，我们没有这个特殊的
    ***[CLS]*** 标记。因此，我们改为在最后一个编码器堆栈之后对所有输出嵌入值进行平均。
- en: Data Loader
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据加载器
- en: Next, we need to create a data loader for our training data such that it will
    be supplied into our model in batches during the training process.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要为训练数据创建一个数据加载器，以便在训练过程中将数据分批输入到模型中。
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In addition to the dataloader class, we also need to create supplementary function
    called `collate_fn()` above. This function is essential because, in order to supply
    our training data in batches, each batch needs to have the same dimension.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 dataloader 类，我们还需要创建上述的辅助函数 `collate_fn()`。这个函数是必不可少的，因为为了将训练数据按批次提供，每个批次需要具有相同的维度。
- en: Since we’re dealing with text data with varying sentence length, then the dimension
    of each batch isn’t guaranteed to be the same. In the `collate_fn` , we first
    fetch the maximum length of a sequence in a batch, and then add a bunch of ***<pad>***
    tokens to the shorter sequence until its length equals to the length of the longest
    sequence in the batch.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们处理的是具有不同句子长度的文本数据，因此每个批次的维度不一定相同。在 `collate_fn` 中，我们首先获取批次中序列的最大长度，然后向较短的序列中添加一堆***<pad>***标记，直到其长度等于批次中最长序列的长度。
- en: Another method that you can use is by defining the maximum number of tokens.
    Next, you can truncate the sentence if it has more tokens than the maximum value
    or add a bunch of ***<pad>*** tokensif it has fewer tokens than the maximum value.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种你可以使用的方法是通过定义最大标记数。接下来，如果句子的标记数超过最大值，可以截断句子；如果标记数少于最大值，则可以添加一堆***<pad>***标记。
- en: Training Loop
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练循环
- en: Now that we have defined the model architecture and the data loader class, then
    we can start to train the model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了模型架构和数据加载器类，那么我们可以开始训练模型了。
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And you’ll get the output that looks something like this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到类似这样的输出：
- en: '![](../Images/88ed0aea8063b6f56ced477e5dad1d3b.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88ed0aea8063b6f56ced477e5dad1d3b.png)'
- en: Image by author
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Model Prediction
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型预测
- en: After we train the model, we can naturally use it to predict unseen data on
    our test set. To do so, first we need to create a function that encapsulates data
    preprocessing step and the model prediction step.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练模型之后，我们可以自然地使用它来预测测试集中的未见数据。为此，我们首先需要创建一个函数，封装数据预处理步骤和模型预测步骤。
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now if we want to predict a text from our test set, we can just call the function
    above:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想预测测试集中的文本，可以直接调用上述函数：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Conclusion
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have discussed the step-by-step process to utilize the encoder
    part of Transformer to classify text. As you already know, there are a lot of
    large language models out there that use the encoder part of Transformer. BERT,
    as an example, achieved state-of-the-art performance in many language tasks, thanks
    to its Transformer-encoder architecture combined with a large corpus of training
    data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了利用 Transformer 的编码器部分来分类文本的逐步过程。如你所知，许多大型语言模型使用 Transformer 的编码器部分。例如，BERT
    由于其 Transformer 编码器架构结合了大量的训练数据，在许多语言任务中取得了最先进的表现。
- en: I hope this article helps you to getting started with Transformer architecture.
    As usual, you can find the code implemented in this article via [**this notebook**](https://github.com/marcellusruben/medium-resources/blob/main/Text_Classification_Transformer_Encoders/Transformer_Encoder.ipynb).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章能帮助你入门 Transformer 架构。像往常一样，你可以通过 [**这个笔记本**](https://github.com/marcellusruben/medium-resources/blob/main/Text_Classification_Transformer_Encoders/Transformer_Encoder.ipynb)
    查找本文实现的代码。
