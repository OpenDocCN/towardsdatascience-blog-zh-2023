- en: Explainable AI with TCAV from Google AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/explainable-ai-with-tcav-from-google-ai-5408adf905e](https://towardsdatascience.com/explainable-ai-with-tcav-from-google-ai-5408adf905e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explain deep neural networks using concept-based explanations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://adib0073.medium.com/?source=post_page-----5408adf905e--------------------------------)[![Aditya
    Bhattacharya](../Images/d0f79ad4a85330c58327aea499b7eea0.png)](https://adib0073.medium.com/?source=post_page-----5408adf905e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5408adf905e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5408adf905e--------------------------------)
    [Aditya Bhattacharya](https://adib0073.medium.com/?source=post_page-----5408adf905e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5408adf905e--------------------------------)
    ·13 min read·Feb 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08876ebee32ea4c27b44a794161d9bab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: [Pixabay](https://pixabay.com/illustrations/tick-tock-tiktok-network-computer-7730760/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Explainable AI (XAI)**](https://amzn.to/3cY4c2h) is a subfield of artificial
    intelligence (AI) that aims to develop AI systems that can provide clear and understandable
    explanations of their decision-making processes to humans. The goal of XAI is
    to make AI more transparent, trustworthy, responsible, and ethical. XAI is an
    important element in increasing AI adoption, especially for high stake domains
    such as healthcare, finance, and law enforcement. In these domains, it is crucial
    to understand how an AI system arrived at a particular decision or recommendation.'
  prefs: []
  type: TYPE_NORMAL
- en: There are various techniques used in XAI, including model transparency, rule-based
    systems, and model-agnostic methods such as LIME and SHAP. XAI approaches can
    vary depending on the type of AI system, the application domain, and the level
    of explainability required. Overall, XAI is an essential field for developing
    AI systems that can be trusted and used ethically and effectively in real-world
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want a brief introduction to XAI in a short 45 mins video, then you
    can watch one of my past sessions on XAI delivered at the **AI Accelerator Festival
    APAC, 2021**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Explainable AI: Making ML and DL models more interpretable (Talk by the author)'
  prefs: []
  type: TYPE_NORMAL
- en: One major limitation of popular [XAI methods](https://amzn.to/3J2QNnz) such
    as LIME and SHAP is that these methods are not extremely consistent and intuitive
    with how non-technical end users would explain an observation. For example, if
    you have an image of a glass filled with Coke and use LIME and SHAP to explain
    a black-box model used to correctly classify the image as Coke, both LIME and
    SHAP would highlight regions of the image that lead to the correct prediction
    by the trained model. But if you ask a non-technical user to describe the image,
    the user would classify the image as Coke due to the presence of a dark-colored
    carbonated liquid in a glass that resembles a Cola drink. In other words, human
    beings tend to relate any observation with known *concepts* to explain it.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Testing with Concept Activation Vector (TCAV)** from *Google AI*](https://arxiv.org/pdf/1711.11279.pdf)
    also follows a similar approach in terms of explaining model predictions with
    known *human concepts*. So, in this article, we will cover how TCAV can be used
    to provide concept-based human-friendly explanations. Unlike LIME and SHAP, TCAV
    works beyond *feature attribution* and refers to concepts such as *color*, *gender*,
    *race*, *shape*, *any known object*, or an *abstract idea* to explain model predictions.
    We will discuss the following topics about TCAV in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding TCAV intuitively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differences between TCAV and other XAI frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential applications of concept-based explanations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this article, I will refer to some of the XAI frameworks discussed in my
    book [**Applied Machine Learning Explainability Techniques**](https://amzn.to/3cY4c2h)**.**
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to get started now!
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to TCAV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Testing with Concept Activation Vectors (TCAV)** is an XAI method to understand
    what signals neural network models use for prediction. TCAV shows the importance
    of high-level concepts (e.g., color, gender, race) for a prediction class, similar
    to how humans communicate! TCAV gives an explanation that is generally true for
    a class of interest, beyond one image (global explanation). For example, for a
    given class, we can show how much race or gender was important for classifications
    in InceptionV3\. Even though neither race nor gender labels were part of the training
    input!'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm depends on **Concept Activation Vectors (CAV)**, which provide
    an interpretation of the internal state of ML models using human-friendly concepts.
    In a more technical sense, TCAV uses **directional derivatives** to quantify the
    importance of human-friendly, high-level concepts for model predictions. For example,
    while describing hairstyles, concepts such as *curly hair*, *straight hair*, or
    *hair color* can be used by TCAV. These user-defined concepts are not the input
    features of the dataset that are used by the algorithm during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Other popular XAI methods, such as LIME and SHAP, depend on features that are
    considered important by the model. There is no scope for adding customized user-defined
    concepts as input features for the basis of explainability. The following figure
    illustrates the key question addressed by TCAV:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/617dc5e692affa342609d8ea8d426f5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Key question addressed by TCAV — *What is the importance of a concept for predicting
    the output?* (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Explaining with abstract concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you may have an intuitive understanding of the method of providing explanations
    with abstract concepts. But why do you think this is an effective approach?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take another example. Suppose you are working on building a deep learning-based
    image classifier for detecting doctors from images. After applying TCAV, let’s
    say that you have found out that the *concept importance* of the concept *white
    male* is maximum, followed by *stethoscope* and *white coat*. The concept importance
    of a *stethoscope* and *white coat* is expected, but the high concept importance
    of *white male* indicates a biased dataset. Hence, TCAV can help to evaluate **fairness**
    in trained models.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, the goal of CAVs is to estimate the importance of a concept (such
    as color, gender, and race) for predicting a trained model, even though the *concepts*
    were not used during the model training process. This is because TCAV learns *concepts*
    from a few example samples.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in order to learn a *gender* concept, TCAV needs a few data instances
    that have a *male* concept and a few *non-male* examples. Hence, TCAV can quantitatively
    estimate the trained model’s sensitivity to a particular *concept* for that class.
  prefs: []
  type: TYPE_NORMAL
- en: For generating explanations, TCAV perturbs data points toward a *concept* that
    is relatable to humans, and so it is a type of **global perturbation method**.
    Next, let’s try to learn the main objectives of TCAV.
  prefs: []
  type: TYPE_NORMAL
- en: Goals of TCAV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I found the approach of TCAV to be unique compared to other explanation methods.
    One of the main reasons is that the developers of this framework established clear
    goals that resonate with my own understanding of human-friendly explanations.
    The following are the established goals of TCAV:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accessibility**: The developers of TCAV wanted this approach to be accessible
    to any end user, irrespective of their knowledge of ML or data science.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customization**: The framework can adapt to any user-defined concept. This
    is not limited to concepts considered during the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plug-in readiness**: The developers wanted this approach to work without
    the need to retrain or fine-tune trained ML models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global interpretability**: TCAV can interpret the entire class or multiple
    samples of the dataset with a single quantitative measure. It is not restricted
    to the local explainability of data instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know what can be achieved using TCAV, let’s discuss the general
    approach to how TCAV works.
  prefs: []
  type: TYPE_NORMAL
- en: Approach of TCAV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will cover the workings of TCAV in more depth. The overall
    workings of this algorithm can be summarized in the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying directional derivatives to quantitatively estimate the sensitivity
    of predictions of trained ML models for various user-defined concepts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the final quantitative explanation, which is termed **TCAVq measure**,
    without any model re-training or fine-tuning. This measure is the relative importance
    of each concept to each model prediction class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/f0e58de286513f79414f96434b9a9bef.png)'
  prefs: []
  type: TYPE_IMG
- en: The approach used by TCAV to estimate the concept importance of stripes in a
    tiger image classifier (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Now, I will try to further simplify the approach of TCAV without using too many
    mathematical notions. Let’s assume we have a model for identifying zebras from
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply TCAV, the following approach can be taken:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Defining a concept of interest**: The very first step is to consider the
    concepts of interest. For our zebra classifier, either we can have a given set
    of examples that represent the concept (such as black stripes are important in
    identifying a zebra) or we can have an independent dataset with the concepts labeled.
    The major benefit of this step is that it does not limit the algorithm from using
    features used by the model. Even non-technical users or domain experts can define
    the concepts based on their existing knowledge.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Learning concept activation vectors**: The algorithm tries to learn a vector
    in the space of activation of the layers by training a linear classifier to differentiate
    between activations generated by a concept’s instances and instances present in
    any layer. So, a **CAV** is defined as the normal projection to a hyperplane that
    separates instances with a concept and instances without a concept in the model’s
    activation. For our zebra classifier, CAVs help to distinguish representations
    that denote *black stripes* and representations that do not denote *black stripes*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Estimating directional derivatives**: Directional derivatives are used to
    quantify the sensitivity of a model prediction toward a concept. So, for our zebra
    classifier, directional directives help us to measure the importance of the *black
    stripes* representation in predicting zebras. Unlike saliency maps, which use
    per-pixel saliency, directional derivatives are computed on the entire dataset
    or a set of inputs but for a specific concept. This helps to give a global perspective
    for the explanation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Estimating the TCAV score**: To quantify the concept importance of a particular
    class, the TCAV score (**TCAVq**) is calculated. This metric helps to measure
    the positive or negative influence of a defined concept on a particular activation
    layer of a model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**CAV validation**: CAV can be produced from randomly selected data. But unfortunately,
    this might not produce meaningful concepts. So, in order to improve the generated
    concepts, TCAV runs multiple iterations for finding concepts from different batches
    of data, instead of training CAV once, on a single batch of data. Then, a **statistical
    significance test** is performed using *two-side t-test* for selecting the statistically
    significant concepts. Necessary corrections, such as the *Bonferroni correction*,
    are also performed to control the false discovery rate.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thus, we have covered the intuitive workings of the TCAV algorithm. Next, let’s
    cover how TCAV can actually be implemented in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Differences between TCAV and other XAI methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let us summarize how TCAV differs from popular XAI methods such as LIME
    and SHAP.
  prefs: []
  type: TYPE_NORMAL
- en: XAI frameworks such as LIME can generate contradicting explanations for two
    data instances for the same class. Whereas, TCAV-generated explanations are not
    only true for a single data instance but also true for the entire class. This
    is a major advantage of TCAV over LIME, which increases the user’s trust in the
    explanation method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concept-based explanations are closer to how humans would explain an unknown
    observation, rather than feature-based explanations as adopted in LIME and SHAP.
    So, TCAV-generated explanations are indeed more human-friendly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature-based explanations are limited to the features used in the model. To
    introduce any new feature for model explainability, we would need to re-train
    the model, whereas a concept-based explanation is more flexible and is not limited
    to features used during model training. To introduce a new concept, we do not
    need to retrain the model. You would just have to make the necessary datasets
    to generate concepts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model explainability is not the only benefit of TCAV. TCAV can help to detect
    issues during the training process, such as imbalanced datasets leading to bias
    in the dataset vis-à-vis the majority class. In fact, concept importance can be
    used as a metric to compare models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current limitations of TCAV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unfortunately, like everything in this beautiful world, even TCAV is not perfect!
    Although TCAV is unique in its own way, there are certain limitations of TCAV
    which restrict its wider adoption for model explainability. Some of the prominent
    current limitations of TCAV is discussed below:'
  prefs: []
  type: TYPE_NORMAL
- en: Currently, the approach of concept-based explanation using TCAV is limited to
    just neural networks. In order to increase its adoption, TCAV would need an implementation
    that can work with *classical machine learning algorithms* such as *Decision Trees*,
    *Support Vector Machines*, and *Ensemble Learning algorithms*. Both LIME and SHAP
    can be applied with classical ML algorithms to solve standard ML problems and
    that is probably why LIME and SHAP have more adoption. Similarly, with text data,
    too, TCAV has very limited applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TCAV is highly prone to *data drift*, *adversarial effects*, and *other data
    quality issues*. If you are using TCAV, you would need to ensure that training
    data, inference data, and even concept data have similar statistical properties.
    Otherwise, the concepts generated can become affected due to noise or data impurity
    issues:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Guillaume Alain* and *Yoshua Bengio*, in their paper *Understanding intermediate
    layers using linear classifier probes* ([https://arxiv.org/abs/1610.01644](https://arxiv.org/abs/1610.01644)),
    have expressed some concern about applying TCAV to shallower neural networks.
    Many similar research papers have suggested that concepts in deeper layers are
    more separable as compared to concepts in shallower networks and, hence, the use
    of TCAV is limited to mostly deep neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing a concept dataset can be a challenging and expensive task. Although
    you don’t need ML knowledge to prepare a concept dataset, still, in practice,
    you do not expect any common end user to spend time creating an annotated concept
    dataset for any customized user-defined concept.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I felt that the TCAV Python framework would require further improvements before
    being used in any production-level system. In my opinion, at the time of writing
    this chapter, this framework would need to mature further so that it can be used
    easily with any production-level ML system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: All these limitations can indeed be solved to make TCAV a much more robust framework
    that is widely adopted. You can also reach out to authors and developers of the
    TCAV framework and contribute to the open-source community! In the next section,
    let’s discuss some potential applications of concept-based explanations.
  prefs: []
  type: TYPE_NORMAL
- en: Potential applications of concept-based explanations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I do see great potential for concept-based explanations such as TCAV! In this
    section, you will get exposure to some potential applications of concept-based
    explanations that can be important research topics for the entire AI community,
    which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Estimation of transparency and fairness in AI**: Most regulatory concerns
    for black-box AI models are related to concepts such as gender, color, and race.
    Concept-based explanations can actually help to estimate whether an AI algorithm
    is fair in terms of these abstract concepts. The detection of bias for AI models
    can actually improve their transparency and help to address certain regulatory
    concerns. For example, in terms of doctors using deep learning models, TCAV can
    be used to detect whether the model is biased toward a specific gender, color,
    or race as ideally, these concepts are not important as regards the model’s decision.
    High concept importance for these concepts indicates the presence of bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/bc81db0624102425c9a529d6b920eb18.png)'
  prefs: []
  type: TYPE_IMG
- en: TCAV can be used to detect model bias based on concept importance (Image by
    author)
  prefs: []
  type: TYPE_NORMAL
- en: '**Detection of adversarial attacks with CAV**: If you go through the appendix
    of the TCAV research paper ([https://arxiv.org/pdf/1711.11279.pdf](https://arxiv.org/pdf/1711.11279.pdf)),
    the authors have mentioned that the concept importance of actual samples and adversarial
    samples are quite different. This means that if an image gets impacted by an adversarial
    attack, the concept importance would also change. So, CAVs can be a potential
    method for detecting adversarial attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concept-based image clustering**: Using CAVs to cluster images based on similar
    concepts can be an interesting application. Deep learning-based image search engines
    are a common application in which clustering or similarity algorithms are applied
    to feature vectors to locate similar images. However, these are feature-based
    methods. Similarly, there is a potential to apply concept-based image clustering
    using CAVs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated concept-based explanations (ACE)**: *Ghorbani, Amirata*, *James
    Wexler*, *James Zou*, *and Been Kim*, in their research work — *Towards automatic
    concept-based explanations*, mentioned an automated version of TCAV that goes
    through the training images and automatically discovers prominent concepts. This
    is an interesting work, as I think it can have an important application in identifying
    incorrectly labeled training data. In industrial applications, getting a perfectly
    labeled curated dataset is extremely challenging. This problem can be solved to
    a great extent using ACE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/202b85a7ad2b7941c6704787b028d337.png)'
  prefs: []
  type: TYPE_IMG
- en: Source — [*Towards Automatic Concept-based Explanations*, **Ghorbani et al.**](https://arxiv.org/abs/1902.03129)
  prefs: []
  type: TYPE_NORMAL
- en: '**Concept-based Counterfactual Explanation**: Another important XAI method
    is **counterfactual explanation (CFE)** which can be a mechanism for generating
    actionable insights by suggesting changes to the input features that can change
    the overall outcome. CFE provides minimum feature values required to flip the
    predicted class. However, CFE is a feature-based explanation method. It would
    be a really interesting topic of research to have a concept-based counterfactual
    explanation, which is one step closer to human-friendly explanations. There is
    no existing algorithm or framework that can help us achieve this, yet this can
    be a useful application of concept-based approaches in computer vision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/a8c576c7b1f91da3d2cf2a264cf0ea1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Concept-based counterfactual examples (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: I feel this is a wide-open research field and the potential to come up with
    game-changing applications using concept-based explanations is immense. I do sincerely
    hope that more and more researchers and AI developers start working on this area
    to make significant progress in the coming years! Thus, we have arrived at the
    end of this article.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This article covers the concepts of TCAV, a novel approach, and a framework
    developed by Google AI. You have received a conceptual understanding of TCAV,
    learned about some key advantages and limitations of TCAV, and finally, we discussed
    some interesting ideas regarding potential research problems that can be solved
    using concept-based explanations. I recommend reading this book: **“**[**Applied
    Machine Learning Explainability Techniques**](https://amzn.to/3cY4c2h)**”** and
    exploring the [GitHub repository](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques)
    for getting hands-on code examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'OTHER XAI RELATED ARTICLES ON TDS BY THE AUTHOR:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Essential Explainable AI Python frameworks that you should know about](/essential-explainable-ai-python-frameworks-that-you-should-know-about-84d5063b75e9)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Explainable Machine Learning for Models Trained on Text Data: Combining SHAP
    with Transformer Models](/explainable-machine-learning-for-models-trained-on-text-data-combining-shap-with-transformer-5095ea7f3a8)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[EUCA — An effective XAI framework to bring artificial intelligence closer
    to end-users](/euca-an-effective-xai-framework-to-bring-artificial-intelligence-closer-to-end-users-74bb0136ffb1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Understand the Workings of SHAP and Shapley Values Used in Explainable AI](/understand-the-working-of-shap-based-on-shapley-values-used-in-xai-in-the-most-simple-way-d61e4947aa4e)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[How to Explain Image Classifiers Using LIME](/how-to-explain-image-classifiers-using-lime-e364097335b4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: REFERENCE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Applied Machine Learning Explainability Techniques](https://amzn.to/3cY4c2h)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GitHub repo from the book Applied Machine Learning Explainability Techniques
    — [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Interpretability Beyond Feature Attribution: Quantitative Testing with Concept
    Activation Vectors (TCAV): [https://arxiv.org/pdf/1711.11279.pdf](https://arxiv.org/pdf/1711.11279.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TCAV Python framework — [https://github.com/tensorflow/tcav](https://github.com/tensorflow/tcav)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Koh et al. “Concept Bottleneck Models”: [https://arxiv.org/abs/2007.04612](https://arxiv.org/abs/2007.04612)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guillaume Alain and Yoshua Bengio, “Understanding intermediate layers using
    linear classifier probes”: [https://arxiv.org/abs/1610.01644](https://arxiv.org/abs/1610.01644)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ghorbani, Amirata, James Wexler, James Zou and Been Kim, “Towards automatic
    concept-based explanations”: [https://arxiv.org/abs/1902.03129](https://arxiv.org/abs/1902.03129)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Detecting Concepts, Chapter 10.3 Molnar, C. (2022). Interpretable Machine Learning:
    A Guide for Making Black Box Models Explainable (2nd ed.).: [https://christophm.github.io/interpretable-ml-book/detecting-concepts.html](https://christophm.github.io/interpretable-ml-book/detecting-concepts.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
