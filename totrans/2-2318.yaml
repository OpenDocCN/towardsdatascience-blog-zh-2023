- en: What Are Gradients, and Why Do They Explode?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是梯度，为什么会爆炸？
- en: 原文：[https://towardsdatascience.com/what-are-gradients-and-why-do-they-explode-add23264d24b](https://towardsdatascience.com/what-are-gradients-and-why-do-they-explode-add23264d24b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/what-are-gradients-and-why-do-they-explode-add23264d24b](https://towardsdatascience.com/what-are-gradients-and-why-do-they-explode-add23264d24b)
- en: By reading this post you will have a firm understanding of the most important
    concept in deep learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阅读这篇文章，你将对深度学习中最重要的概念有一个坚定的理解。
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----add23264d24b--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----add23264d24b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----add23264d24b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----add23264d24b--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----add23264d24b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1?source=post_page-----add23264d24b--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----add23264d24b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----add23264d24b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----add23264d24b--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----add23264d24b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----add23264d24b--------------------------------)
    ·10 min read·Jun 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----add23264d24b--------------------------------)
    ·10 分钟阅读·2023年6月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bad1bd95b279ce0cae1fa6328cb658d5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bad1bd95b279ce0cae1fa6328cb658d5.png)'
- en: “Gradient Explosion”, made with MidJourney. All images by the author unless
    otherwise specified.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “梯度爆炸”，由 MidJourney 制作。所有图像均由作者提供，除非另有说明。
- en: Gradients are arguably the most important fundamental concept in machine learning.
    In this post we will explore the concept of gradients, what makes them vanish
    and explode, and how to rein them in.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度可以说是机器学习中最重要的基础概念。在这篇文章中，我们将探讨梯度的概念、使梯度消失和爆炸的原因，以及如何控制梯度。
- en: '**Who is this useful for?** Beginning to Intermediate data scientists. I’ll
    include some math references which may be useful for more advanced readers.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**这对谁有用？** 对初学者到中级的数据科学家有用。我将包括一些可能对更高级读者有用的数学参考。'
- en: '**What will you get from this post?** An in depth conceptual understanding
    of gradients, how they relate to machine learning, the issues that come from gradients,
    and approaches used to mitigate those issues.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**你将从这篇文章中获得什么？** 对梯度的深入概念理解，它们如何与机器学习相关，梯度带来的问题，以及用来减轻这些问题的方法。'
- en: Table Of Contents
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: Click the links to navigate to specific sections
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 点击链接以导航到特定部分
- en: '**1)** [**What is a Gradient?**](#c1ad) **2)** [**Actual Gradients (mathematically)**](#8666)
    **3)** [**Gradients in a Simple Model (an example)**](#e9e0) **4)** [**What Are
    Exploding and Vanishing Gradients?**](#f436) **5)** [**Why Are Exploding and Vanishing
    Gradients Bad?**](#1c5d) **6)** [**How Do We Fix Exploding and Vanishing Gradients?**](#f0ed)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**1)** [**什么是梯度？**](#c1ad) **2)** [**实际梯度（数学上）**](#8666) **3)** [**简单模型中的梯度（一个例子）**](#e9e0)
    **4)** [**什么是梯度爆炸和消失？**](#f436) **5)** [**为什么梯度爆炸和消失不好？**](#1c5d) **6)** [**我们如何修复梯度爆炸和消失？**](#f0ed)'
- en: What is a Gradient?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是梯度？
- en: '[table of contents](#073c)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[目录](#073c)'
- en: Imagine you have some surface with hills and valleys. This surface is defined
    by some multidimensional function (a function with multiple inputs).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你有一些表面，有山丘和山谷。这个表面由一些多维函数（具有多个输入的函数）定义。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/81cea1c0e4f705b8fb185dabb33e71e5.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81cea1c0e4f705b8fb185dabb33e71e5.png)'
- en: Some surface, with hills and vallys
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一些表面，有山丘和山谷
- en: '**A gradient tells you,** **for any given point on the surface, both the direction
    to get to a higher point on the surface, as well as how steep the surface is at
    that point.**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度告诉你，** **对于表面上的任何给定点，它既指明了到达更高点的方向，也描述了该点表面的陡峭程度。**'
- en: Because this is a 3d surface (the inputs are x and y, and the output is z) we
    can compute a simplified version of the gradient by computing the slope of a given
    point along the X axis and the Y axis.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个三维表面（输入是 x 和 y，输出是 z），我们可以通过计算在 X 轴和 Y 轴上某个点的坡度来计算梯度的简化版本。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/ab46b0411248f8365a950ab82cd80a52.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab46b0411248f8365a950ab82cd80a52.png)'
- en: The Gradient, in blue, describes the direction of greatest increase in for the
    point marked by ‘x’. The red line describes, if you were to follow the gradient
    for some distance, where you would end up.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色的梯度描述了“x”标记的点的最大增加方向。红线描述了如果你沿着梯度走一段距离，你会到达哪里。
- en: So, to summarize that conceptually, **the gradient computes the direction of
    greatest increase, and how steep that increase is, by calculating the slope of
    the output relative to all inputs of a function.**
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，从概念上总结一下，**梯度计算了最大增加的方向，以及这种增加的陡峭程度，通过计算输出相对于函数所有输入的斜率。**
- en: Actual Gradients
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际梯度
- en: '[table of contents](#073c)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[目录](#073c)'
- en: In the previous example I computed what I referred to as a “simplified gradient”.
    I calculated the slope of z relative to a small change in x and a small change
    in y.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我计算了我称之为“简化梯度”的内容。我计算了 z 相对于 x 和 y 的小变化的斜率。
- en: '![](../Images/70b72e33bf806e3110eb0780d0626bf4.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70b72e33bf806e3110eb0780d0626bf4.png)'
- en: The ratios used to compute the pseudo gradient. the SlopeZvsX and SlopeZvsY
    were used to construct the blue line in the image.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 用于计算伪梯度的比率。 SlopeZvsX 和 SlopeZvsY 被用来构造图像中的蓝线。
- en: In reality, gradients are calculated using a **partial derivative**, meaning
    it’s a **derivative of the function with respect to a single variable (x or y).**
    If you’re unfamiliar with calculus you can watch a quick [video on derivatives](https://www.youtube.com/watch?v=rAof9Ld5sOg)
    and another on [partial derivatives](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivatives/v/partial-derivatives-introduction)
    to get up to speed on the math. However, regardless of the math, the concepts
    of the simplified gradient and actual gradient are identical.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，梯度是使用**偏导数**计算的，这意味着它是**相对于单个变量（x 或 y）的函数的导数。** 如果你对微积分不熟悉，你可以观看一个关于导数的
    [视频](https://www.youtube.com/watch?v=rAof9Ld5sOg) 和另一个关于 [偏导数](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivatives/v/partial-derivatives-introduction)
    的视频来快速了解数学。不过，无论数学如何，简化梯度和实际梯度的概念是相同的。
- en: '![](../Images/016c694498b40d881bc62b1a743ad4dd.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/016c694498b40d881bc62b1a743ad4dd.png)'
- en: This is the mathematical notation of a gradient of a function with an x and
    y dimension. The gradient is a vector made of the partial derivatives of the function
    with respect to each input to the function, represented as a vector. True gradients
    are both more accurate, and faster to compute than the “pseudo gradient” I calculated
    in previous examples. Conceptually, though, you’re still calculating the slope
    given a change in both x and y.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个具有 x 和 y 维度的函数的梯度的数学符号。梯度是一个由函数相对于每个输入的偏导数组成的向量，表示为一个向量。真实梯度比我在之前示例中计算的“伪梯度”更加准确且计算更快。从概念上讲，你仍然是在计算给定
    x 和 y 变化时的斜率。
- en: Gradients in a Simple Model
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单模型中的梯度
- en: '[table of contents](#073c)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[目录](#073c)'
- en: When researching machine learning you’ll find differentiability everywhere;
    differentiable loss functions, differentiable activation functions, etc. etc.
    The reason for this is, if everything's differentiable, you can quickly calculate
    the gradient (the direction in which you need to adjust inputs to functions to
    get a better output).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究机器学习时，你会发现到处都是可微性；可微损失函数、可微激活函数等等。原因是，如果一切都是可微的，你可以快速计算梯度（你需要调整输入以获得更好输出的方向）。
- en: For example, suppose we have a machine learning model with two parameters, x
    and y. We also have a function that tells us how well those parameters fit the
    model to a set of data. We can use gradients to try to find the best model parameters
    by taking a series of steps in the direction the gradient.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个具有两个参数 x 和 y 的机器学习模型。我们还有一个函数，告诉我们这些参数如何将模型拟合到一组数据。我们可以使用梯度通过在梯度方向上采取一系列步骤来尝试找到最佳模型参数。
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/97f7e11c596d5c00b2498ac5ca3db92b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97f7e11c596d5c00b2498ac5ca3db92b.png)'
- en: Iterative gradient search from x=0 and y=0 to a local maxima. the “x” points
    mark the location at each iteration. In this case, gradient optimization yielded
    a result of x=-3.104, y=0.789
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从 x=0 和 y=0 进行迭代梯度搜索到局部最大值。 “x” 点标记了每次迭代的位置。在这种情况下，梯度优化的结果是 x=-3.104，y=0.789。
- en: When people talk about training they sometimes refer to a “loss landscape”.
    **When you turn a problem into a machine learning problem, you are essentially
    creating a landscape of parameters which perform better, or worse than others**.
    You define what is good, and what is bad (the loss function) and a complex function
    with a set of differentiable parameters which can be adjusted to solve the problem
    (the machine learning model). These two, together, create a theoretical *loss
    landscape*, which one traverses through gradient descent (model training). **But**
    **What is Gradient Descent?**
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们谈论训练时，他们有时会提到“损失景观”。**当你将一个问题转化为机器学习问题时，你实际上是在创建一个参数的景观，这些参数表现得比其他参数好或差**。你定义什么是好，什么是坏（损失函数），以及一个复杂的函数，它具有一组可微分的参数，可以调整以解决问题（机器学习模型）。这两者共同创建了一个理论上的*损失景观*，通过梯度下降（模型训练）来遍历这个景观。**但**
    **什么是梯度下降？**
- en: A gradient, mathematically, is used to point towards a greater value in a function.
    In machine learning we usually optimize to reduce the error rate, so we want our
    loss function (how many errors we make) to be smaller, not bigger. The math is
    virtually identical though; you just turn a few +’s into -’s
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，梯度用于指向函数中的更大值。在机器学习中，我们通常优化以减少错误率，因此我们希望我们的损失函数（错误数量）更小，而不是更大。数学上几乎是相同的；你只需将一些
    + 变成 -。
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/89dfa12702e96055431f07410b11b948.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89dfa12702e96055431f07410b11b948.png)'
- en: Gradient descent, which is what is typically used in machine learning.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降，通常在机器学习中使用的方法。
- en: What Are Exploding and Vanishing Gradients?
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是梯度爆炸和梯度消失？
- en: '[table of contents](#073c)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[目录](#073c)'
- en: First, let’s look at a simplified neural network. We’re going to ignore the
    activation functions and bias and just think of a neural network as a series of
    weights. Under this set of simplifications, a neural network looks like this
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一个简化的神经网络。我们将忽略激活函数和偏置，仅将神经网络视为一系列权重。在这种简化的条件下，神经网络看起来像这样：
- en: '![](../Images/fd72b05b4b8a48c2d6061be8917b0e75.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd72b05b4b8a48c2d6061be8917b0e75.png)'
- en: an example of a small neural network which uses only weights. The value of Hidden
    1 is input1*w1 + input2*w2, which is equal to 0.5\. In general, a perceptron multiplies
    each input by it’s respective weight, and sums the values together.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个小型神经网络的例子，它仅使用权重。Hidden 1 的值是 input1*w1 + input2*w2，这等于 0.5。在一般情况下，感知器将每个输入乘以其各自的权重，然后将这些值加在一起。
- en: The issue of exploding and vanishing gradients comes into play when we have
    deeper neural networks. Imagine a network with several more layers multiplying
    and adding, multiplying and adding, etc.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有更深层的神经网络时，梯度爆炸和梯度消失的问题就会出现。想象一个网络，其中有几个更多的层在进行乘法和加法，不断地进行乘法和加法等等。
- en: '![](../Images/b80038af883d5b2289f8a42098998e2c.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b80038af883d5b2289f8a42098998e2c.png)'
- en: A snapshot of a deep portion of a neural network
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络深层部分的快照
- en: As you can see, the values quickly become large when you consistently multiply
    by numbers which are larger than 1\. With weights which are less than 1 the values
    would quickly shrink as they are multiplied by smaller and smaller values.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，当你持续乘以大于 1 的数字时，值会迅速增大。如果权重小于 1，值会因为乘以越来越小的值而迅速缩小。
- en: We can see that the values within the network can get large or small quickly
    depending on the weights within the model, but we can also imagine the values
    listed within each perceptron as a **change**, rather than a value.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，网络中的值会因为模型中的权重而迅速变大或变小，但我们也可以将每个感知器中的值看作**变化**，而不是具体的值。
- en: '![](../Images/b80038af883d5b2289f8a42098998e2c.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b80038af883d5b2289f8a42098998e2c.png)'
- en: Imagine the numbers within the perceptrons not as the value of the perceptron,
    but a **change** in the value of a perceptron. As you can see, because of repeated
    multiplication, the rate of change increases as one traverses deeper into the
    network.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下感知器中的数字不是感知器的值，而是感知器值的**变化**。正如你所看到的，由于重复的乘法，当你深入网络时，变化的速度会增加。
- en: 'The punchline is this: **Repeated multiplication can cause the rate of change
    (gradient) to get really big if the weights are larger than one, or really small
    if the weights are less than 1\.** In other words, the gradients can **explode
    or vanish.**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点是：**如果权重大于 1，重复的乘法会导致变化率（梯度）变得非常大；如果权重小于 1，变化率会变得非常小。** 换句话说，梯度可能会**爆炸或消失**。
- en: Why Are Exploding and Vanishing Gradients Bad?
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么梯度爆炸和梯度消失不好？
- en: '[table of contents](#073c)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[目录](#073c)'
- en: Big gradients, small gradients, so what?
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 大梯度，小梯度，那又如何？
- en: Small gradients move slowly, and have a tendency to get stuck in shallow local
    minima. As a result, having vanishing gradients can result in a lack of model
    improvement. On the other hand, exploding gradients move too quickly and can bounce
    around erratically, leading to an inability for the model to converge on a minima.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 小梯度移动缓慢，并且有可能陷入浅层的局部最小值。因此，梯度消失可能导致模型无法改进。另一方面，梯度爆炸移动过快，并可能出现不稳定的情况，从而导致模型无法收敛到最小值。
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/05c3f76705175abc05dd7cde1276a6a1.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05c3f76705175abc05dd7cde1276a6a1.png)'
- en: Three similarly shaped landscapes, one with vanishingly small gradients (left)
    one with reasonably sized gradients (middle) and one with exploding gradients
    (right). Vanishing gradients result in a lack of model improvement, and exploding
    gradients result in instability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 三种形状相似的景观，一种梯度几乎为零（左），一种梯度适中（中），一种梯度爆炸（右）。梯度消失导致模型改进不足，而梯度爆炸则导致不稳定性。
- en: This problem becomes even more pervasive when you consider that **not only does
    the larger model have a gradient, each individual perceptron has a gradient as
    well**. this means that different perceptrons within a network can have very different
    gradients. In theory, **a model can have vanishing and exploding gradients at
    the same time** in different parts of the model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当你考虑到**不仅仅是更大的模型有梯度，每个感知机也有自己的梯度**时，这个问题变得更加普遍。这意味着网络中的不同感知机可以有非常不同的梯度。理论上，**一个模型在不同的部分可以同时存在梯度消失和梯度爆炸**。
- en: How Do We Fix Exploding and Vanishing Gradients?
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何解决梯度爆炸和梯度消失的问题？
- en: '[table of contents](#073c)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[目录](#073c)'
- en: 1) Adjust the Learning Rate
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1) 调整学习率
- en: The learning rate gets multiplied by the gradient to control how far of a step
    gets taken along the gradient for each iteration. Smaller learning rates result
    in small steps, while large learning rates result in larger steps (I did something
    roughly equivalent in the previous example). This can be useful to a point, but
    the issue with vanishing and exploding gradients is that the gradients change
    throughout the model (some perceptrons have small changes, and others have large
    changes). So, while learning rate is a critical hyperparameter, it’s not usually
    considered as an approach for dealing with vanishing and exploding gradients specifically.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率通过梯度进行乘法运算，以控制每次迭代沿梯度方向的步长。较小的学习率会导致较小的步伐，而较大的学习率则会导致较大的步伐（我在之前的示例中做了大致等效的操作）。这在一定程度上可能有用，但梯度消失和梯度爆炸的问题在于梯度在模型中会发生变化（一些感知机的变化很小，其他的变化很大）。因此，虽然学习率是一个关键的超参数，但通常不被视为专门处理梯度消失和梯度爆炸的解决方法。
- en: '**2) Change Activation Functions**'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2) 更改激活函数**'
- en: For a variety of reasons, perceptrons don’t just multiply their inputs by some
    weight, add the results together, and spit out an output. Virtually all networks
    pass the result of this operation through a function called an activation function.
    Activation functions are all differentiable, and thus have a variety of characteristics
    in effecting the gradients throughout the depth of a network.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 出于各种原因，感知机并不会只是将其输入乘以某些权重，将结果加在一起，然后输出结果。几乎所有网络都会将这一操作的结果通过一个称为激活函数的函数。激活函数都是可微分的，因此在影响网络深度中的梯度方面具有多种特性。
- en: '**4) Change Model Architectures**'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4) 更改模型架构**'
- en: Simply having a shorter network can help. However, if that’s not an option,
    several network architectures were designed specifically to handle this problem.
    LSTMs were designed to be more robust than classical RNNs at dealing with exploding
    gradients, residual and skip connections are also designed to aid in solving this
    phenomenon.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地缩短网络可以有所帮助。然而，如果这不是一个选项，几种网络架构被专门设计来处理这个问题。LSTM被设计得比经典RNN在处理梯度爆炸方面更具鲁棒性，残差和跳跃连接也被设计用来帮助解决这种现象。
- en: 5) Use Batch Normalization
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**5) 使用批量归一化**'
- en: Batch normalization is a mainstay regularization strategy in machine learning.
    Instead of updating a model parameters by a single gradient, you update parameters
    by using the average gradient from a batch of examples.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化是机器学习中的一种主要正则化策略。它不是通过单一梯度来更新模型参数，而是通过使用一批样本的平均梯度来更新参数。
- en: '**6) Gradient Clipping**'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**6) 梯度裁剪**'
- en: You can just set a maximum allowable threshold for a gradient. If the gradient
    is larger than that, just set the magnitude of the gradient to be the threshold.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以设置梯度的最大允许阈值。如果梯度超过这个值，就将梯度的大小设为这个阈值。
- en: 7) Use Weight Regularizers
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7) 使用权重正则化
- en: L1 and L2 regularization are used to penalize models for having high weight
    values. Adding this form of regularization can reduce exploding gradients.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2正则化用于对高权重值的模型施加惩罚。添加这种形式的正则化可以减少梯度爆炸。
- en: 8) Truncate Context Windows
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8) 截断上下文窗口
- en: Whether you’re using a recurrent network, or some other network which employs
    a context window, reducing the size of that window can help minimize compounding
    gradient issues.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你使用的是递归网络还是其他使用上下文窗口的网络，缩小窗口的大小可以帮助最小化复合梯度问题。
- en: Conclusion
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: And that’s it! In this post we learned a bit about gradients and the math behind
    them. We learned you can use gradients to go up or down surfaces within some dimensional
    space, and how that’s used to optimize machine learning models. We learned that,
    as a model grows in size, the gradients can explode to large numbers, or vanish
    to very tiny numbers, both of which can ruin the models ability to learn. We also
    went through a list of approaches for correcting vanishing and exploding gradients.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！在这篇文章中，我们了解了梯度以及其背后的数学。我们了解到，你可以使用梯度在某些维度空间中上下移动表面，并且了解了如何利用这来优化机器学习模型。我们还了解到，随着模型规模的增大，梯度可能会爆炸成很大的数值，或消失成很小的数值，这两者都可能破坏模型的学习能力。我们还讨论了修正梯度消失和梯度爆炸的一些方法。
- en: Follow For More!
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关注以获取更多信息！
- en: In a future post I’ll be describing several landmark papers in the ML space,
    with an emphasis on practical and intuitive explanations. I also have posts on
    not-so commonly discussed ML concepts.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的文章中，我将描述一些机器学习领域的标志性论文，重点介绍实用和直观的解释。我还会有关于不常讨论的机器学习概念的文章。
