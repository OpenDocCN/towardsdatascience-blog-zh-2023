- en: All Languages Are NOT Created (Tokenized) Equal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/all-languages-are-not-created-tokenized-equal-cd87694a97c1](https://towardsdatascience.com/all-languages-are-not-created-tokenized-equal-cd87694a97c1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Language models cost much more in some languages than others
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@artfish?source=post_page-----cd87694a97c1--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----cd87694a97c1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cd87694a97c1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cd87694a97c1--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----cd87694a97c1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cd87694a97c1--------------------------------)
    ·12 min read·May 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5d82c8ce5ba774594d82455e802916d.png)'
  prefs: []
  type: TYPE_IMG
- en: “hey” translated to 52 different languages. The size of the text is scaled to
    corresponds to the number of tokens needed to represent the message in the corresponding
    language. Image created by author.
  prefs: []
  type: TYPE_NORMAL
- en: '*Original article was posted on my* [*blog*](https://www.artfish.ai/p/all-languages-are-not-created-tokenized)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models such as ChatGPT process and generate text sequences by
    first splitting the text into smaller units called **tokens**. In the image below,
    each colored block represents a unique token. Short or common words such as “you”,
    “say”, “loud”, and “always” are its own token, whereas longer or less common words
    such as “atrocious”, “precocious”, and “supercalifragilisticexpialidocious” are
    broken into smaller subwords.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17d355ca37bbda6bda1137a1956d5f38.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization of tokenization of a short text using [OpenAI’s tokenizer website](https://platform.openai.com/tokenizer).
    Screenshot taken by author.
  prefs: []
  type: TYPE_NORMAL
- en: This process of **tokenization** is not uniform across languages, leading to
    disparities in the number of tokens produced for equivalent expressions in different
    languages. For example, **a sentence in Burmese or Amharic may require 10x more
    tokens than a similar message in English.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77a9be17c8b6729603e33808832bd519.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of the same message translated into five languages and the corresponding
    number of tokens required to tokenize that message (using OpenAI’s tokenizer).
    The text comes from [Amazon’s MASSIVE dataset](https://www.amazon.science/blog/amazon-releases-51-language-dataset-for-language-understanding).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I explore the tokenization process and how it varies across
    different languages:'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of token distributions in a parallel dataset of short messages that
    have been translated into 52 different languages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some languages, such as Armenian or Burmese, require **9 to 10 times more tokens
    than English** to tokenize comparable messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The impact of this language disparity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**This phenomenon is not new to AI** — this is consistent with what we observe
    in Morse code and computer fonts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try it yourself!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Try out the exploratory dashboard I made, available on HuggingFace spaces](https://huggingface.co/spaces/yenniejun/tokenizers-languages).
    Here, you can compare the token lengths for different languages and for different
    tokenizers (which was not explored in this article, but which I explore the reader
    to do on their own).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef69e8025dc85f2213a7476d2a23f453.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of the language tokenizers dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[MASSIVE](https://arxiv.org/abs/2204.08582) is a parallel dataset [introduced
    by Amazon](https://github.com/alexa/massive) consisting of 1 million realistic,
    parallel short texts translated across 52 languages and 18 domains. I used the
    `dev` split of the dataset, which consists of **2033 texts translated into each
    of the languages**. The dataset is [available on HuggingFace](https://huggingface.co/datasets/AmazonScience/massive)
    and is licensed under the [CC BY 4.0 license](https://huggingface.co/datasets/AmazonScience/massive/blob/main/massive.py).'
  prefs: []
  type: TYPE_NORMAL
- en: A focus on OpenAI’s Tokenizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While many other language model tokenizers exist, this article mainly focuses
    on [OpenAI’s Byte Pair Encoding (BPE) tokenizer](https://platform.openai.com/tokenizer)
    (used by ChatGPT and GPT-4) for three main reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: First, [Denys Linkov’s article](https://denyslinkov.medium.com/why-is-gpt-3-15-77x-more-expensive-for-certain-languages-2b19a4adc4bc)
    compared several tokenizers and found that GPT-2’s tokenizer had the highest token
    length disparity among different languages. This prompted me to concentrate on
    OpenAI models, including GPT-2 and its successors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, since we lack insight into ChatGPT’s full training dataset, investigating
    OpenAI’s black box models and tokenizers help to better understand their behaviors
    and outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the widespread adoption of ChatGPT in various applications (from language
    learning platforms like [Duolingo](https://blog.duolingo.com/duolingo-max/) to
    social media apps like [Snapchat](https://newsroom.snap.com/say-hi-to-my-ai))
    highlights the importance of understanding tokenization nuances to ensure equitable
    language processing across diverse linguistic communities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To calculate the number of tokens a text contains, I use the `cl100k_base` tokenizer
    available on [tiktoken](https://github.com/openai/tiktoken), which is the BPE
    tokenizer used by OpenAI’s ChatGPT models (`gpt-3.5-turbo` and `gpt-4`).
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some languages consistently tokenize to longer lengths
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following distribution plot compares the distribution of token lengths for
    five languages. The curve for English is tall and narrow, meaning that English
    texts consistently tokenize to a smaller number of tokens. On the other hand,
    the curve for languages such as Hindi and Burmese are short and wide, meaning
    that these languages tokenize texts into many more tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad96619815dd74d0591c968737b50f6e.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of token lengths for all 2033 messages and 52 languages. Five of
    the languages have been bolded and colored; the rest are shown in gray. Figure
    created by author.
  prefs: []
  type: TYPE_NORMAL
- en: English has the shortest median token length
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For each language, I calculated the median token length for all of the texts
    in the dataset. The following chart compares a subset of the languages. English
    texts had the smallest median length of 7 tokens and Burmese texts had the largest
    median length of 72 tokens. Romance languages such as Spanish, French, and Portuguese
    tended to result in a similar number of tokens as English.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e09f188755d7a54c4697575672b3362a.png)'
  prefs: []
  type: TYPE_IMG
- en: A subset of the 52 languages and their median token length. Figure created by
    author.
  prefs: []
  type: TYPE_NORMAL
- en: As English had the shortest median token length, I calculated the ratio of the
    other languages’ median token length to that of English. Languages such as Hindi
    and Bengali (over 800 million people speak either of these languages) resulted
    in a median token length of about 5 times that of English. The ratio is 9 times
    that of English for Armenian and over 10 times that of English for Burmese. In
    other words, **to express the same sentiment, some languages require up to 10
    times more tokens**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db10fc30f4280e3a990c81316c801cba.png)'
  prefs: []
  type: TYPE_IMG
- en: A subset of the 52 languages and the ratio of that language’s median token length
    to that of English. Figure created by author.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implications of tokenization language disparity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Overall, requiring more tokens (to tokenize the same message in a different
    language) means:'
  prefs: []
  type: TYPE_NORMAL
- en: You’re limited by how much information you can put in the prompt (because the
    context window is fixed). As of March 2023, GPT-3 could take up to 4K tokens and
    GPT-4 could take up to 8K or 32K tokens in its input [[1](#footnote-1)]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It costs more money
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It takes longer to run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI’s models are increasingly being used in countries where English is not
    the dominant language. According to SimilarWeb.com, the United States only accounted
    for 10% of the traffic sent to ChatGPT in Jan-March 2023.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f74a6266c72c05e5887bf3aff7d588f.png)'
  prefs: []
  type: TYPE_IMG
- en: Top 5 countries sending the most traffic to chat.openai.com in Jan-March 2023\.
    Sourced from [similarweb.com](https://www.similarweb.com/website/chat.openai.com/#traffic)
    on May 2, 2023\. Screenshot taken by author.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, ChatGPT was used [in Pakistan to grant bail in a juvenile kidnapping
    case](https://interestingengineering.com/culture/pakistani-court-utilizes-chatgpt-4-to-grant-bail)
    and [in Japan for administrative tasks](https://www.japantimes.co.jp/news/2023/04/20/national/chatgpt-yokosuka-trial/).
    As ChatGPT and similar models are becoming increasingly integrated into products
    and services worldwide, it is crucial to understand and address such inequalities.
  prefs: []
  type: TYPE_NORMAL
- en: Language Disparity in Natural Language Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This digital divide in natural language processing (NLP) is an active area of
    research. 70% of research papers published in a computational linguistics conference
    only evaluated English.[2](#footnote-2) Multilingual models perform worse on several
    NLP tasks on low resource languages than on high resource languages such as English.[3](#footnote-3)
    According to [W3Techs](https://w3techs.com/) (World Wide Web Technology Surveys),
    English dominates more than half (55.6%) of the content on the Internet.[4](#footnote-4)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ce6f6662d13b419b39bfae2e6cb9442.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Percentages of websites using various content languages (as of April 30, 2023).
    Data source: [https://w3techs.com/technologies/overview/content_language.](https://w3techs.com/technologies/overview/content_language.)
    Figure created by author.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, English makes up [over 46% of the Common Crawl corpus](https://commoncrawl.github.io/cc-crawl-statistics/plots/languages)
    (billions of webpages from the Internet [crawled for over a decade](https://commoncrawl.org/the-data/)),
    versions of which have been used to train many large languages such as Google’s
    T5 and OpenAI’s GPT-3 (and likely ChatGPT and GPT-4). Common Crawl makes up 60%
    of GPT-3 training data.[5](#footnote-5)
  prefs: []
  type: TYPE_NORMAL
- en: Addressing the digital divide in NLP is crucial to ensure equitable language
    representation and performance in AI-driven technologies. Bridging this gap calls
    for a concerted effort from researchers, developers, and linguists to prioritize
    and invest in the development of low-resource languages, fostering a more inclusive
    and diverse linguistic landscape in the realm of natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Historical example: Representing Chinese Typography using Morse Code'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Such a disparity of technological costs for different languages is not new to
    AI or even to computing.
  prefs: []
  type: TYPE_NORMAL
- en: Over a hundred years ago, telegraphy, a revolutionary technology of its time
    (“the internet of its era”), faced language inequities similar to those we see
    in today’s large language models. Despite its promises of open exchange and collaboration,
    telegraphy exhibited discrepancies in speed and cost across languages. For instance,
    encoding and transmitting a message in Chinese (compared to an equivalent message
    in English) was
  prefs: []
  type: TYPE_NORMAL
- en: 2 times as expensive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Took 15–20 times longer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sound familiar?
  prefs: []
  type: TYPE_NORMAL
- en: Telegraphy was “designed first and foremostfor Western alphabetic languages,
    English above all.”[6](#footnote-6) Morse code assigned different lengths and
    costs to dots and dashes, resulting in a cost-efficient system for English. However,
    the Chinese language, which relies on ideograms, faced challenges in telegraphy.
    A Frenchman named Viguier devised a mapping system for Chinese characters to Morse
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, each Chinese ideogram was mapped to a four-digit code, which had
    to then be translated into Morse code. This was took a long time looking up the
    codes in the codebook (which lacked meaningful correlations) and was more costly
    to transmit (as each character was represented by four digits, and a single digit
    was more expensive to transmit than a single letter). This practice put the Chinese
    language at a disadvantage compared to other languages in terms of telegraphic
    speed and cost.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfb454208313c88edf79d3edbb9c3e1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Manuscript on left from Zhang Deyim *Dianxin xinfa* 電信新法, 1873\. Danish National
    Archives. [http://www5.kb.dk/permalink/2006/manus/350/eng/32/.](http://www5.kb.dk/permalink/2006/manus/350/eng/32/.)
    Red circle drawn in by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example: Inequity in representing fonts'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Initially, I tried to visualize all 52 languages in a single word cloud. I ended
    up with something like this, where a majority of the languages were not rendered
    properly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73a33e8723f379efc0adab7f8aa10f98.png)'
  prefs: []
  type: TYPE_IMG
- en: Word cloud visualizing “hey” in 52 languages. Many of the languages (including
    Arabic, Hindi, and Korean) cannot be rendered using a single font (depicted is
    the [default WordCloud font](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html)
    DroidSansMono). Size corresponds to the number of tokens required to represent
    “hey” in that language. Figure created by author.
  prefs: []
  type: TYPE_NORMAL
- en: This led me down a rabbit hole of trying to find a font that could render all
    of the language scripts. I went on Google Fonts to find this perfect font and
    found that one did not exist. Below is a screenshot showing how these 52 languages
    would render in 3 different fonts from Google Fonts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8322277624175d809f7cc33ec664149d.png)'
  prefs: []
  type: TYPE_IMG
- en: To generate the word cloud at the beginning of this article, I (ehm) manually
    downloaded the 17 font files necessary to render all of the language scripts and
    displayed words one at a time. While I got the desired effect, it was a lot more
    work than it would have been if, for example, all of my languages used the same
    script (such as the Latin alphabet).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I explored the language disparity in language models by looking
    at how they process text through tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: Using a dataset of parallel texts translated into 52 languages, I showed that
    some languages require up to 10 times more tokens to express the same message
    in English
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I shared a [dashboard where you can explore different languages and tokenizers](https://huggingface.co/spaces/yenniejun/tokenizers-languages)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I discussed the impacts of this disparity on certain languages in terms of performance,
    monetary cost, and time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I showed how this pattern of linguistic technological disparity is not new,
    comparing the phenomenon to the historical case of Chinese Morse code and telegraphy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language disparities in NLP tokenization reveal a pressing issue in AI: equity
    and inclusivity. As models like ChatGPT are predominantly trained on English,
    non-Indo-European and non-Latin script languages face barriers due to prohibitive
    tokenization costs. Addressing these disparities is essential to ensure a more
    inclusive and accessible future for artificial intelligence, ultimately benefiting
    diverse linguistic communities worldwide.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Thank you for reading this article! If you liked it, please check out similar
    articles on my* [*blog*](https://blog.yenniejun.com/)!'
  prefs: []
  type: TYPE_NORMAL
- en: APPENDIX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Byte-Pair Encoding Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the realm of natural language processing, tokenizers play a crucial role
    in enabling language models to process and understand text. Different models use
    different methods for tokenizing a sentence, such as splitting it into words,
    into characters, or into parts of words (also known as subwords; e.g. splitting
    “constantly” into “constant” and “ly”).
  prefs: []
  type: TYPE_NORMAL
- en: One common tokenization is called [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)
    (BPE). This is the encoding used by OpenAI for their ChatGPT models. BPE is meant
    to decompose rare words into meaningful subwords while keeping frequently used
    words intact. A comprehensive explanation of the BPE algorithm can be found on
    the [HuggingFace Transformers course](https://huggingface.co/docs/transformers/tokenizer_summary).
  prefs: []
  type: TYPE_NORMAL
- en: Deeper Dive into Token Distribution for Languages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I augmented Amazon’s MASSIVE dataset by using information about each of the
    52 languages using the infobox section of that language’s Wikipedia page, obtaining
    information such as writing script (e.g. Latin, Arabic alphabet) and main geographic
    region the language is predominant in (if relevant). I additionally use metadata
    from [The World Atlas of Language Structures](https://wals.info) to obtain information
    such as [language family](https://en.wikipedia.org/wiki/Language_family) (e.g.
    Indo-European, Sino-Tibetan).[7](#footnote-7)
  prefs: []
  type: TYPE_NORMAL
- en: Note that the following analyses in this article uphold the assumptions made
    by Wikipedia, The World Atlas of Language Structures, and by the Amazon MASSIVE
    dataset. Since I am not a linguistics expert, I had to assume that whatever on
    Wikipedia and the World Atlas were canonically accepted as correct with regards
    to dominant geographic region or language family.
  prefs: []
  type: TYPE_NORMAL
- en: Also, there are debates about what constitutes a language versus a dialect.
    For example, while languages such as Chinese and Arabic have different forms that
    people may not understand, they are still called single languages. On the other
    hand, Hindi and Urdu are very similar and are sometimes grouped together as one
    language called Hindustani. Because of these challenges, we need to be careful
    when deciding what counts as a language or a dialect.
  prefs: []
  type: TYPE_NORMAL
- en: '**Breakdown by language.** I chose the [12 most spoken languages](https://en.wikipedia.org/wiki/List_of_languages_by_total_number_of_speakers)
    (a combination of both first-language and second-language speakers).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e148df217ba668d4cbfec343071f951.png)'
  prefs: []
  type: TYPE_IMG
- en: Token distribution by language. Figure created by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Breakdown by language family.** Indo-European (e.g. Swedish, French), Austronesian
    languages (e.g. Indonesian, Tagalog), and Uralic languages (e.g. Hungarian, Finnish)
    resulted in shorter tokens. Dravidian languages (e.g. Tamil, Kannada) tended to
    have longer tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc49a50a083e6c4aca8a872d8e1a59d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Token distribution by language family. Figure created by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Breakdown by main geographic region.** Not all languages were specific to
    a single geographic region (such as Arabic, English, and Spanish, which are spread
    across many regions) — these languages were removed from this section. Languages
    spoken mostly in Europe tend to be shorter in token length, while languages spoken
    mostly in the Middle East, Central Asia, and the Horn of Africa tended to be longer
    in token length.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aeaff5dd8c825faf20eaf3c42c5154b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Token distribution by main geographic region. Figure created by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Breakdown by writing script.** Other than the Latin, Arabic, and Cyrillic
    alphabets, all other languages use their own unique script. While the latter combines
    many very different unique scripts (such as Korean, Hebrew, and Georgian scripts),
    these unique scripts definitely tokenize to longer values. Compared to Latin-based
    scripts, which tokenize to shorter values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55596b9c31ca8132f67d82c225d5ec00.png)'
  prefs: []
  type: TYPE_IMG
- en: Token distribution by writing script. Figure created by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'English almost always ranks #1'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For each text in the dataset, I ranked all languages based on number of tokens
    — the language with the least tokens was ranked #1 and the one with the most tokens
    was ranked #52\. Then, I plotted the distribution of each language’s *ranking*.
    Essentially, this should show how each language’s token length compares with the
    other languages in this dataset. In the below figure, I labeled a few of the languages
    (the other languages show up as gray lines in the background).'
  prefs: []
  type: TYPE_NORMAL
- en: 'While there were a few cases where some languages’ tokens were fewer than that
    of English (such as a few examples in Indonesian or Norwegian), English almost
    always ranked number one. Does this come as a surprise to anyone? What surprised
    me most was that there was no clear #2 or #3\. English language texts consistently
    produce the shortest tokens, and the ranking fluctuates a bit more for other languages.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7ee92a742558e62dd51567056c69201.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of token ranking with respect to other languages. Figure created
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying token distributions differences using Earth Mover’s Distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To quantify how different the token length distribution between two languages
    were, I calculated the [earth mover’s distance](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)
    (also known as the [Wasserstein distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wasserstein_distance.html))
    between two distributions. Essentially, this metric calculates the minimum amount
    of “work” required to transform one distribution into another. Larger values mean
    the distributions are farther apart (more different) while smaller values mean
    the distributions are quite similar.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a small subset of languages. Note that the distance says nothing about
    the length of the tokens, just how similar the distribution of token lengths are
    for two languages. For example, Arabic and Russian have similar distributions
    even though the languages themselves are not similar in a linguistic sense.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5574c2aa21645c2be176deaa6d8365c.png)'
  prefs: []
  type: TYPE_IMG
- en: Heatmap showing Earth Mover’s Distance among a subset of languages. Figure created
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: '[1](#footnote-anchor-1). OpenAI. [“Models”](https://platform.openai.com/docs/models).
    *OpenAI API*. [Archived](https://web.archive.org/web/20230317000210/https://platform.openai.com/docs/models)
    from the original on March 17, 2023\. Retrieved March 18, 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2](#footnote-anchor-2). Sebastian Ruder, Ivan Vulić, and Anders Søgaard. 2022.
    [Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research
    Manifold](https://aclanthology.org/2022.findings-acl.184). In *Findings of the
    Association for Computational Linguistics: ACL 2022*, pages 2340–2354, Dublin,
    Ireland. Association for Computational Linguistics.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3](#footnote-anchor-3). Shijie Wu and Mark Dredze. 2020. [Are All Languages
    Created Equal in Multilingual BERT?](https://aclanthology.org/2020.repl4nlp-1.16).
    In *Proceedings of the 5th Workshop on Representation Learning for NLP*, pages
    120–130, Online. Association for Computational Linguistics.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4](#footnote-anchor-4). [Usage statistics of content languages for websites”](https://w3techs.com/technologies/overview/content_language).
    [Archived](https://archive.ph/RzLBr) from the original on 30 April 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5](#footnote-anchor-5). Brown, Tom, et al. “Language models are few-shot learners.”
    *Advances in neural information processing systems* 33 (2020): 1877–1901.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6](#footnote-anchor-6). Jin Tsu. Kingdom of Characters: The Language Revolution
    That Made China Modern. New York: Riverhead Books, 2022 (p. 124).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7](#footnote-anchor-7). Dryer, Matthew S. & Haspelmath, Martin (eds.) 2013\.
    WALS Online (v2020.3) [Data set]. Zenodo. [https://doi.org/10.5281/zenodo.7385533.](https://doi.org/10.5281/zenodo.7385533.)
    Available online at [https://wals.info,](https://wals.info,) Accessed on 2023–04–30.'
  prefs: []
  type: TYPE_NORMAL
