- en: 'Unsupervised Learning with K-Means Clustering: Generate Color Palettes from
    Images'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/unsupervised-learning-with-k-means-clustering-generate-color-palettes-from-images-94bb8e6a1416](https://towardsdatascience.com/unsupervised-learning-with-k-means-clustering-generate-color-palettes-from-images-94bb8e6a1416)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A comprehensive guide to unsupervised ML and the K-Means algorithm with a demo
    of a clustering use case for grouping image pixels by color
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://nroy0110.medium.com/?source=post_page-----94bb8e6a1416--------------------------------)[![Nabanita
    Roy](../Images/83ab7766a28c79371ebf9517e1f273d2.png)](https://nroy0110.medium.com/?source=post_page-----94bb8e6a1416--------------------------------)[](https://towardsdatascience.com/?source=post_page-----94bb8e6a1416--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----94bb8e6a1416--------------------------------)
    [Nabanita Roy](https://nroy0110.medium.com/?source=post_page-----94bb8e6a1416--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----94bb8e6a1416--------------------------------)
    ·13 min read·Apr 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf06c65cce12f976b12ba19ea8fc1473.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Billy Huynh](https://unsplash.com/@billy_huy?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning is a method in which underlying patterns in data can be
    discovered without providing extra information (or labels/targets) to a machine
    learning algorithm. In this article, I have documented a pretty cool application
    of the K-Means clustering algorithm that I recently found out while reading some
    image-processing articles along with an introduction to unsupervised clustering
    approaches in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Key Takeaways from this Article:**'
  prefs: []
  type: TYPE_NORMAL
- en: '1️⃣ Unsupervised ML: Introduction, Classifications, and Applications'
  prefs: []
  type: TYPE_NORMAL
- en: 2️⃣ Comprehensive Understanding of KMeans Clustering
  prefs: []
  type: TYPE_NORMAL
- en: 3️⃣ A Step-by-Step K-Means Clustering Application using Scikit Learn Python
    Libary to Generate Color Palette from a Given Image
  prefs: []
  type: TYPE_NORMAL
- en: 4️⃣ Read and process Images using Pillow, Requests and Numpy
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Dive into Unsupervised Learning First
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning (ML), a technique in which machines can be trained to assimilate
    and learn from given data, can be broadly classified into supervised, unsupervised,
    and reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bc9157e3b35d7f16017be6ea7e05b91.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '**✅ *Supervised learning*** is where the machine is taught to learn by providing
    samples with metadata (which in ML terminology is called labels) to help the identification
    process. For example, in fraud detection use-cases, transactions are labeled fraud
    or genuine manually by analysts and then they are used to train an ML model. This
    ML model learns from examples of patterns or behaviors in fraudulent transactions
    and is enabled to apply the learning to a new transaction and assess if it matches
    the patterns of the fraud or genuine transactions that were used for learning.
    Then, the model associates the new transaction with either of the classes, fraud
    or genuine, depending on the identified patterns, and makes a prediction. Often,
    these predictions are associated with a confidence interval or probability to
    scores to indicate how well the pattern matched with the unseen sample. Higher
    scores indicate the ML model thinks that the prediction is likely to be correct.'
  prefs: []
  type: TYPE_NORMAL
- en: '**✅ *Unsupervised learning*** also finds such patterns hidden in the provided
    sample. The difference is that there are no labels/classes associated. Unsupervised
    techniques are great for exploring data and understanding behaviors that are otherwise
    difficult to recognize by humans from a huge dataset. *They are widely used for
    text categorization (like news articles), anomaly detection, satellite & spatial
    image processing, medical image analysis, customer segmentation, and recommendation
    engines to name a few.*'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning is used for three main tasks— *clustering, association,
    and dimensionality reduction.*
  prefs: []
  type: TYPE_NORMAL
- en: '***Clustering*** is a technique in which samples are grouped automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Association*** is a technique used to find relationships between features
    in a dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Dimensionality Reduction*** is a technique used to reduce the number of
    features in the dataset when it is high.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this article, I will focus on clustering as a grouping technique.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering could be performed for multiple applications, for example, assessing
    how similar or dissimilar are data-points from each other, how dense are the data
    points in a vector space, extracting topics, and so on. Primarily, there are four
    types of clustering techniques -
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b9ff0506563862bd8a58b3abf33dc44.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '***Centroid-based Clustering*** is where groups of data points are identified
    by the average distance from a centroid. There could be multiple centroids and
    an initial centroid is optimized over multiple passes. This technique is simple,
    efficient, and effective, however, it is sensitive to initial hyperparameter configurations.
    **K-Means clustering is the most-popular clustering algorithm that belongs to
    this school of techniques.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***Density-based Clustering*** groups high-density areas into clusters. These
    are useful for spatial data processing. For example, this technique could be used
    to locate areas with a high concentration of COVID-19-infected households, locate
    densely populated areas, or deforestation. **DBSCAN (Density-Based Spatial Clustering
    of Applications with Noise) is the most common algorithm that is used for detecting
    density-based clusters.** Usually, these algorithms ignore outliers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***Distribution-based Clustering*** is used for detecting if the data has a
    particular distribution (probability distribution e.g. Gaussian) embedded subsets
    of the dataset. This technique is recommended if a particular type of distribution
    is known to be embedded in the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***Hierarchical Clustering*** is used to detect if there are hierarchical relationships
    or taxonomies embedded in the data. This technique can be further classified into
    ‘top-down’ and ‘bottom-up’ approaches. **Dendrograms** are a data visualization
    technique that helps in the interpretation of the results of hierarchical clustering
    by creating taxonomy maps. Applications of hierarchical clustering are in understanding
    and charting evolution in life-sciences use-cases using DNA sequences, and tracking
    infected clusters during Covid 19, modeled into regions and drilled down into
    sub-regions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are other approaches and definitions to types of clustering including
    **Partitional/Exclusive Clustering** (where every data point is assigned to exactly
    one cluster), **Overlapping/Non-exclusive Clustering** (where a data point can
    be assigned to multiple clusters), **Fuzzy or Non-Fuzzy** Approaches as well as
    **Partial** (where a data point might not have any cluster assigned)or **Complete
    Clustering techniques** (where all data points have one or more clusters assigned).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, I will focus on elaborating more on the K-Means clustering
    technique, the scikit-learn implementation, and the pros-cons of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to K-Means Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-Means clustering is one of the most popular centroid-based clustering methods
    with partitioned clusters. The number of clusters is predefined, usually denoted
    by *k*. All data points are assigned to one and exactly one of these *k* clusters.
    Below is a demonstration of how (random) data points in a 2-dimensional space
    are clustered into 4 groups after 3 iterations (as set on the right-hand side).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04645a77bdb5ee6eb23a40e08a3e3927.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A KMeans Clustering Simulator by [METU — Middle East Technical University](https://www.metu.edu.tr/)
    | Image Source & Simulator Link: [https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/)'
  prefs: []
  type: TYPE_NORMAL
- en: Each cluster obtained from training a K-Means model contains a centroid (marked
    by the larger circle markers in the image above) and each data point is assigned
    to the cluster with the closest centroid. The centroid of a cluster is the mean
    of all the data points in that cluster. In short, the K-Means algorithm groups
    the data points into *k* clusters by minimizing the distance between the centroid
    and each data point in an iterative manner. Commonly, [Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance)
    is used to measure the distance between the centroid and the data point.
  prefs: []
  type: TYPE_NORMAL
- en: If you are familiar with the Cost Functions in ML, you would know that evaluation
    of this function estimates the error in the predictions made by an ML model, and
    enhancing a model’s performance incorporates strategies to minimize this function
    for optimal impact. In clustering, the cost function sums the distortions of the
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of K-Means clustering, the distortion is the squared distance (given
    that Euclidean Distance is being used) between all the points to their closest
    cluster center. To obtain optimal clusters, the distortion function needs to be
    minimized.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7c53036ed985aa61edeef71c3cec7c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Distortion Function | Image Source: Author | Reference: [http://cs229.stanford.edu/notes2020spring/cs229-notes7a.pdf](http://cs229.stanford.edu/notes2020spring/cs229-notes7a.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The K-Means algorithm minimizes *J* with respect to *c* keeping *µ* fixed (when
    the data points are (re)distributed to the fixed centroids based on the distance)
    and then minimizes J with respect to *µ* keeping *c* fixed (when the centroids
    are (re)updated after (re)distributing the data points based on the distance).
  prefs: []
  type: TYPE_NORMAL
- en: A variant of K-Means is K-Median which relies on the [Manhattan distance](https://wikipedia.org/wiki/Taxicab_geometry)
    from the centroid to a data point.
  prefs: []
  type: TYPE_NORMAL
- en: '**The K-Means Algorithm:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19d93d1f18899400d852df3e48e17053.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1:* Randomly initialize centroids for each of the *k* clusters'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2:* Assign each point to the closest centroid to group data points to
    the initial *k* clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 3:* Recompute the centroid by getting the average of all points in each
    of the *k* clusters. Since the centroids are recomputed and therefore have been
    updated, the data points are also reassigned to the closest centroid thereafter.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 4:* Recompute centroids and reassign data points to updated centroids
    *(i.e. repeat step 3)* until the points stop changing clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scikit-Learn’s K-Means Implementation:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/435d8bee27dd7ce1b3f6323f9394aa71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**K-Means implementation**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
    **in Scikit-Learn has the following key hyperparameters:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*n_clusters*: The number of clusters that the user has to provide'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*init*: The strategy to initialize the centroids. Even though they are randomly
    chosen, to speed up convergence and obtain optimal clusters, strategies like the
    K-means++ technique could be applied. Otherwise, ‘random’ uses randomly initiated
    clusters. K-Means++ selects a centroid at random and then places the remaining
    *k−1* centroids such that they are maximally far away from another. [Here’s the
    paper for delving further into K-Means++.](https://dl.acm.org/doi/10.5555/1283383.1283494)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*n_init*: Number of times the k-means algorithm is run with different centroid
    seeds. If this is set to 1, then the clusters might emerge highly imbalanced for
    sparse data. To mitigate this issue, *n_init* number of times the clustering algorithm
    is applied and the cluster distributions with the best inertia (performance evaluation
    metric for clustering) are returned.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*max_iter*: Maximum number of times the centroids can be recomputed. This comes
    in handy for large datasets where the time taken to process is high but a low
    max_iter can also result in sub-optimal clusters when the algorithm is terminated
    before convergence is reached.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*algorithm*: A choice between Lloyd or Elkan. The ‘Lloyd’s algorithm’ is generally
    used for K-Means. In Wikipedia, it is stated that this algorithm (by [Stuart P.
    Lloyd, Bell Labs, 1957](https://en.wikipedia.org/wiki/Lloyd%27s_algorithm)) is
    used for finding evenly spaced sets of points in subsets of [Euclidean spaces](https://en.wikipedia.org/wiki/Euclidean_space)
    (i.e. establish centroids) and partitions of these subsets into well-shaped and
    uniformly sized convex cells (assignment and clustering data points related to
    the centroids).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Now, let’s look at the key attributes available for training:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*cluster_centers_* : array of co-ordinates of centroids'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*labels_* : labels for each data point'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*inertia_*: sum of squared distances of samples to their closest cluster center,
    weighted by the sample weights if provided. This is often used for assessing how
    well the clusters are formed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that, unlike supervised approaches, the model is fitted on only the training
    data. Therefore, the *fit*, *fit_transform*, and *fit_predict* methods will only
    take one argument, that is, the dataset under observation.
  prefs: []
  type: TYPE_NORMAL
- en: A detailed overview of all the available hyperparameters, attributes, and methods
    is [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**✅ Why is the K-Means algorithm so popular?**'
  prefs: []
  type: TYPE_NORMAL
- en: Simple, fast, efficient, and explainable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scales easily for large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convergence for the common similarity measures like Euclidean distance, correlations,
    and cosine similarity is guaranteed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flexible and can be generalized to suit a variety of cluster sizes, shapes,
    and density
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**⚠️Challenges:**'
  prefs: []
  type: TYPE_NORMAL
- en: Manually set the number of clusters (*k*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Centroids are susceptible to outliers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K-means finds roughly circular clusters and performs poorly for varying sizes
    and densities. However, this can be eliminated by applying generalizing strategies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Color Palette Generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A typical clustering process includes:'
  prefs: []
  type: TYPE_NORMAL
- en: ➡️ preparing the data ➡️ vectorizing them (for example similarity scores embeddings
    for texts or RGB values in the case of images) ➡️ running the chosen clustering
    algorithm ➡️ interpreting the results ➡️ making adjustments by retraining the
    model with updated data or hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**💡 *In this project, given an image, the objective is to cluster the pixel
    RGB values, apply the clustering algorithm and obtain the cluster centers that
    represent the palette colors. Bonus: Sort palette colors & use them in Python’s
    data visualization libraries.***'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0c81ce5b5518b3362f04aec00ed6163.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s begin👍**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Get the image data**'
  prefs: []
  type: TYPE_NORMAL
- en: 'First and foremost, we need an image and get the RGB values for each pixel.
    To represent a color on the screen, each pixel actually consists of three color
    components: **red (R), green (G), and blue (B)**. These are frequently referred
    to as the pixel’s RGB value.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Some Notes:**'
  prefs: []
  type: TYPE_NORMAL
- en: 🔴RGB value of (255, 0, 0) is a red pixel
  prefs: []
  type: TYPE_NORMAL
- en: 🟢RGB value of (0, 255, 0) is a green pixel
  prefs: []
  type: TYPE_NORMAL
- en: 🔵RGB value of (0, 0, 255) is a blue pixel
  prefs: []
  type: TYPE_NORMAL
- en: ⚪RGB of (255, 255, 255) is a white pixel
  prefs: []
  type: TYPE_NORMAL
- en: ⚫RGB of (0, 0, 0) is a black pixel
  prefs: []
  type: TYPE_NORMAL
- en: Here is how it is done in Python using Pillow Python Library. In *line 5*, I
    read an image using ***Image.open()*** and in *line 15*, I converted the image
    into a [Numpy array](https://numpy.org/doc/stable/reference/generated/numpy.array.html)
    of RGB codes and finally, put the values into a [Pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)
    in *line 18*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: You can simply use the ***image_variable_name*** (img in my case) to view the
    image or use **display(img)** or **img.show()** to show the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0634a8b9eddfbf3315ef9e6b4561f84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how the RGB DataFrame turned out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a59006d249a30a8f5cc09e190dc384f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Besides, to read images from an URL, here’s the implementation where I used
    the [requests](https://pypi.org/project/requests/) library to fetch the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Next, follow the previous steps to get the RGB values and convert them to a
    DataFrame to proceed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2: Clustering using SkLearn’s K-Means Implementation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that I have the RGB values, I am using [K-Means clustering algorithm in
    scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).
    The parameters I have used are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n_cluster*: integer indicating the number of clusters. I have used 6\. This
    is also indicative of the number of colors in the palette since we will be picking
    out the cluster centers (~ RGB values) to represent the colors in the palette.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*random_state*: random seed for centroid initialization. I have used 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*init*: centroid initialization method. I have used k-mean++ which selects
    initial cluster centroids using sampling based on an empirical probability distribution
    of the points’ contribution to the overall inertia and then makes several trials
    at each sampling step to choose the best centroid among them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n_init*: Number of times the k-means algorithm is run with different centroid
    seeds. Since I selected *init*: *K-means++*, *n_init* is automatically set to
    1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides, I have used the defaults for *max_iter (300)* and *algorithm(Lloyd)*.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I used ***fit()*** method to fit the model on the RGB DataFrame in *line
    9*.
  prefs: []
  type: TYPE_NORMAL
- en: On successful training for the model, I accessed the *cluster_centers_* attribute*,*
    converted them to integers, and put them in *list*. This nested list is the palette’s
    color’s RGB values where each element is a list of R, G, and B values of the color
    it represents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3 (optional): Reorder the cluster centers i.e. your palette colors**'
  prefs: []
  type: TYPE_NORMAL
- en: This is an optional step where I have ordered the colors based on the Value
    in their HSV representatives where H — Hue, S — Saturation, and V — Value. By
    accessing the last element of the [h, s, v] list after conversion using [colorsys’s](https://docs.python.org/3/library/colorsys.html)
    rgb_to_hsv() method, I chose to order the list by the “Value” which describes
    the brightness or intensity of the color. It is expressed by an integer between
    0 to 100 (percent), where 0 is completely black, and 100 is the brightest and
    reveals the most color.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Here is a summary to compare the palette before and after sorting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06f4b38e027140ee70fe902282a01f76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: The detailed code is in the notebook but here below is how I used Plotly to
    generate the palette.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Cluster Analysis**'
  prefs: []
  type: TYPE_NORMAL
- en: In case you are curious about the pixels that were grouped together in the image,
    here’s how you can visualize them. Use the ***predict()*** method to predict the
    closest cluster to each sample in your data. Since this can be performed on your
    training data because we are not evaluating the performance, but rather assessing
    patterns in the data — the unlabelled training or unseen set, hence, to assign
    the data points in the RGB DataFrame, I passed it to the ***predict()*** in *line
    1*. Then, added the predicted array of cluster numbers back to the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: In *line 3*, I used the lambda function to map the individual R, G, and B values
    in each column of the RGB DataFrame and convert the RGB value to the corresponding
    Hex code since it is easier to plot using [Seaborn’s color_palette() method](https://seaborn.pydata.org/generated/seaborn.color_palette.html)
    in a single line of code. Below, is the function that is used to convert from
    RGB to Hex.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I wrapped the visualizing process of each cluster in a for loop (*lines
    4 to 6*) where for each cluster number, beginning from 0 to the palette size/number
    of clusters ([exclusive](https://www.geeksforgeeks.org/python-range-function/)),
    the corresponding Hex codes (up to 10 as specified with [:10])are shown using
    ***seaborn.color_palette()*** method in *line 6*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the color palette or cluster centers to compare & associate with the
    clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04488d97005621b1c43a6f8bc982c2d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Below are the colors in each cluster. *Note that the palette is sorted as shown
    before and they correspond to clusters 3, 0, 5, 2, 4, and 1, respectively.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/214f643ccb7c1de868daa3ddb43316b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5: Using Palette with the Popular Visualization Libraries**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, I converted the RGB values to a list of Hex values which can directly
    be passed as a list of colors in the visualization methods. Here’s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Seaborn**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here’s an example of using the palette on a Seaborn barplot. In Seaborn, we
    can set the palette as in *line 3* or directly pass the *list_hex*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9cf3f9daebb0da658e5058b719e359e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Plotly**'
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an example of using the palette on a Plotly barplot. I have used the
    *list_hex* for the parameter color_discrete_sequence as in *line 3.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Matplotlib**'
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an example of using the palette on a Matplotlib barplot where I passed
    the *list_hex* for the parameter color in *line 8*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16164744f9378348eabe81173227da41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Hope you enjoyed this clustering exercise 🙂. ***Here’s the complete*** [***Notebook***](https://github.com/royn5618/Medium_Blog_Codes/blob/master/color_palette_generator/color_palette_generator.ipynb)
    ***and a demo below!***
  prefs: []
  type: TYPE_NORMAL
- en: 'Video Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Clustering Algorithms](https://developers.google.com/machine-learning/clustering/clustering-algorithms)
    | Google'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[What is unsupervised learning? | IBM](https://www.ibm.com/topics/unsupervised-learning#:~:text=the%20next%20step-,What%20is%20unsupervised%20learning%3F,the%20need%20for%20human%20intervention)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The HSV Color Model in Graphic Design](https://www.lifewire.com/what-is-hsv-in-design-1078068)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Digital Image Basics](https://www.shsu.edu/~csc_dsb/DigitalImage/DigitalImages.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[What does the parameter n_init actually do?](https://stackoverflow.com/questions/46359490/python-scikit-learn-k-means-what-does-the-parameter-n-init-actually-do)
    | StackOverFlow'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[How Could One Implement the K-Means++ Algorithm?](https://stackoverflow.com/questions/5466323/how-could-one-implement-the-k-means-algorithm)
    | StackOverFlow'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Lecture 2 — The k-means clustering problem](https://cseweb.ucsd.edu/~dasgupta/291-unsup/lec2.pdf)
    | University of California San Diego'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Lecture 10: k-means clustering](http://www.cs.yale.edu/homes/el327/datamining2013aFiles/10_k_means_clustering.pdf)
    | Yale'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[CS229 Lecture notes | Andrew Ng](http://cs229.stanford.edu/notes2020spring/cs229-notes7a.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 🔍Looking for Supervised Machine Learning Articles?
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-simplified-explanation-of-supervised-machine-learning-for-electrical-and-electronics-engineers-6d533cdedc6d?source=post_page-----94bb8e6a1416--------------------------------)
    [## A Simplified Explanation of Supervised Machine Learning for Electrical and
    Electronics Engineers'
  prefs: []
  type: TYPE_NORMAL
- en: If you landed up in Electrical or Electronics engineering domain (like I was
    once myself), you are probably dealing…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/a-simplified-explanation-of-supervised-machine-learning-for-electrical-and-electronics-engineers-6d533cdedc6d?source=post_page-----94bb8e6a1416--------------------------------)
    [](/predicting-hazardous-seismic-bumps-using-supervised-classification-algorithms-part-i-2c5d21f379bc?source=post_page-----94bb8e6a1416--------------------------------)
    [## Predicting Hazardous Seismic Bumps Part I : EDA, Feature Engineering & Splitting
    Unbalanced Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: This article demonstrates exploratory data analysis (EDA), feature engineering,
    and splitting strategies for unbalanced…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/predicting-hazardous-seismic-bumps-using-supervised-classification-algorithms-part-i-2c5d21f379bc?source=post_page-----94bb8e6a1416--------------------------------)
    [](/predicting-hazardous-seismic-bumps-part-ii-training-supervised-classifier-models-and-8b9104b611b0?source=post_page-----94bb8e6a1416--------------------------------)
    [## Predicting Hazardous Seismic Bumps Part II: Training & Tuning Supervised ML
    Classifiers and Model…'
  prefs: []
  type: TYPE_NORMAL
- en: This article demonstrates predicting hazardous seismic bumps using binary classifiers,
    tuning model hyperparameters…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/predicting-hazardous-seismic-bumps-part-ii-training-supervised-classifier-models-and-8b9104b611b0?source=post_page-----94bb8e6a1416--------------------------------)
    [](/predicting-fake-news-using-nlp-and-machine-learning-scikit-learn-glove-keras-lstm-7bbd557c3443?source=post_page-----94bb8e6a1416--------------------------------)
    [## Predicting Fake News using NLP and Machine Learning | Scikit-Learn | GloVe
    | Keras | LSTM
  prefs: []
  type: TYPE_NORMAL
- en: towardsdatascience.com](/predicting-fake-news-using-nlp-and-machine-learning-scikit-learn-glove-keras-lstm-7bbd557c3443?source=post_page-----94bb8e6a1416--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Thanks for visiting!*'
  prefs: []
  type: TYPE_NORMAL
- en: '**My Links:** [Medium](https://medium.com/@nroy0110) | [LinkedIn](https://www.linkedin.com/in/nabanita-roy/)
    | [GitHub](https://github.com/royn5618)'
  prefs: []
  type: TYPE_NORMAL
