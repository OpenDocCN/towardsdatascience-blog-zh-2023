- en: 'BigQuery Best Practices: Unleash the Full Potential of Your Data Warehouse'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BigQuery 最佳实践：释放数据仓库的全部潜力
- en: 原文：[https://towardsdatascience.com/bigquery-best-practices-unleash-the-full-potential-of-your-data-warehouse-334a0a9adef2](https://towardsdatascience.com/bigquery-best-practices-unleash-the-full-potential-of-your-data-warehouse-334a0a9adef2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/bigquery-best-practices-unleash-the-full-potential-of-your-data-warehouse-334a0a9adef2](https://towardsdatascience.com/bigquery-best-practices-unleash-the-full-potential-of-your-data-warehouse-334a0a9adef2)
- en: Supercharge your BigQuery experience with these 6 best practices
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用这6个最佳实践超级提升你的 BigQuery 体验
- en: '[](https://niczky12.medium.com/?source=post_page-----334a0a9adef2--------------------------------)[![Bence
    Komarniczky](../Images/d4de94667bcac6d9001390515592eab9.png)](https://niczky12.medium.com/?source=post_page-----334a0a9adef2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----334a0a9adef2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----334a0a9adef2--------------------------------)
    [Bence Komarniczky](https://niczky12.medium.com/?source=post_page-----334a0a9adef2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://niczky12.medium.com/?source=post_page-----334a0a9adef2--------------------------------)[![Bence
    Komarniczky](../Images/d4de94667bcac6d9001390515592eab9.png)](https://niczky12.medium.com/?source=post_page-----334a0a9adef2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----334a0a9adef2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----334a0a9adef2--------------------------------)
    [Bence Komarniczky](https://niczky12.medium.com/?source=post_page-----334a0a9adef2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----334a0a9adef2--------------------------------)
    ·5 min read·Jun 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----334a0a9adef2--------------------------------)
    ·5分钟阅读·2023年6月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/597078e5659914a67c73196ce9a38721.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/597078e5659914a67c73196ce9a38721.png)'
- en: Photo by [charlesdeluvio](https://unsplash.com/@charlesdeluvio?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [charlesdeluvio](https://unsplash.com/@charlesdeluvio?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Google BigQuery, a powerful serverless data warehouse, has become a cornerstone
    of data analysis and machine learning pipelines for many organizations. To harness
    its full potential and ensure an efficient, cost-effective experience, it’s crucial
    to follow some best practices. In this article, we’ll delve into **six essential
    best practices that will help you optimize your BigQuery performance and usage**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Google BigQuery 是一个强大的无服务器数据仓库，已经成为许多组织数据分析和机器学习管道的基石。为了充分发挥其潜力并确保高效且具有成本效益的体验，遵循一些最佳实践至关重要。在本文中，我们将深入探讨**六个关键最佳实践，帮助你优化
    BigQuery 的性能和使用**。
- en: 1 Keep an Eye on Cost Estimates
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 关注成本估算
- en: BigQuery’s pricing model is based on the amount of data processed by your queries.
    Before running complex or large-scale queries, it’s wise to review the cost estimates
    to ensure you stay within your budget. By understanding the cost implications
    of your queries, you can make informed decisions and avoid unexpected billing
    surprises.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 的定价模型基于你查询处理的数据量。在运行复杂或大规模查询之前，明智的做法是查看成本估算，以确保你保持在预算范围内。通过了解查询的成本影响，你可以做出明智的决策，避免意外的账单惊喜。
- en: Never select more columns than needed. BigQuery will push down your selections
    to the very beginning of computing if it can, but if you do `select *` you’re
    shooting yourself in the foot as BigQuery would need to take all the columns for
    your query and that costs money.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 不要选择超过需要的列。如果可能的话，BigQuery 会将你的选择推到计算的最前面，但如果你使用`select *`，你就是在自掘坟墓，因为 BigQuery
    需要处理查询中的所有列，这会产生额外的费用。
- en: Before you press `RUN`, take a peak at the top right corner and save yourself
    some 💰.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在你按下 `RUN` 之前，瞄一眼右上角，为自己节省一些💰。
- en: '![](../Images/489339e51fadd22e5c2fed735b0ef4c8.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/489339e51fadd22e5c2fed735b0ef4c8.png)'
- en: You’ll be looking for the number of GB/MB processed by the query — screenshot
    by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要查看查询处理的 GB/MB 数量 — 作者截图
- en: 2 Sampling for Machine Learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 机器学习的抽样
- en: Machine Learning in BigQuery costs 50x more ($250/TB vs $5/TB) than querying
    so it is especially important to **reduce the amount of data you’re working with**.
    By sampling your data, you can significantly reduce the amount of data processed,
    leading to faster model training and reduced costs. Use BigQuery’s `FARM_FINGERPRINT()`
    function to generate reproducible random samples for your machine-learning pipelines.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 中的机器学习成本是查询的 50 倍（$250/TB 对比 $5/TB），因此**减少你处理的数据量**尤为重要。通过对数据进行抽样，你可以显著减少处理的数据量，从而加快模型训练并降低成本。使用
    BigQuery 的 `FARM_FINGERPRINT()` 函数生成可复现的随机样本，以用于你的机器学习管道。
- en: 'Check out this article on how to do ML in BigQuery and do reproducible sampling:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这篇文章了解如何在 BigQuery 中进行机器学习和可复现的抽样：
- en: '[](/end-to-end-bigquery-machine-learning-e7e6e2e83b34?source=post_page-----334a0a9adef2--------------------------------)
    [## End-to-End BigQuery Machine Learning'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/end-to-end-bigquery-machine-learning-e7e6e2e83b34?source=post_page-----334a0a9adef2--------------------------------)
    [## 端到端 BigQuery 机器学习'
- en: Use Google Cloud BigQuery to compete in a Kaggle competition
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Google Cloud BigQuery 参加 Kaggle 比赛
- en: towardsdatascience.com](/end-to-end-bigquery-machine-learning-e7e6e2e83b34?source=post_page-----334a0a9adef2--------------------------------)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/end-to-end-bigquery-machine-learning-e7e6e2e83b34?source=post_page-----334a0a9adef2--------------------------------)'
- en: 3 Utilize Parquet for Data Uploads
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 利用 Parquet 进行数据上传
- en: When uploading data to BigQuery, consider using the Parquet file format. Parquet
    is a columnar storage format optimized for analytical workloads, offering improved
    compression and encoding schemes. Uploading data in Parquet format can lead to
    **reduced headaches and faster load times** as all your column information is
    embedded in your files.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据上传到 BigQuery 时，考虑使用 Parquet 文件格式。Parquet 是一种列式存储格式，针对分析工作负载进行了优化，提供了更好的压缩和编码方案。以
    Parquet 格式上传数据可以**减少麻烦和加快加载时间**，因为所有列信息都嵌入在文件中。
- en: 'Here’s how you can load files into BigQuery and why parquet is faster:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何将文件加载到 BigQuery 以及为何 Parquet 更快：
- en: '[](/loading-files-into-bigquery-6de1ff63df35?source=post_page-----334a0a9adef2--------------------------------)
    [## Loading files into BigQuery'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/loading-files-into-bigquery-6de1ff63df35?source=post_page-----334a0a9adef2--------------------------------)
    [## 将文件加载到 BigQuery'
- en: Using Python to ingest Parquet and CSV files into GCP BigQuery
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 将 Parquet 和 CSV 文件导入 GCP BigQuery
- en: towardsdatascience.com](/loading-files-into-bigquery-6de1ff63df35?source=post_page-----334a0a9adef2--------------------------------)
    [](/load-files-faster-into-bigquery-94355c4c086a?source=post_page-----334a0a9adef2--------------------------------)
    [## Load files faster into BigQuery
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/loading-files-into-bigquery-6de1ff63df35?source=post_page-----334a0a9adef2--------------------------------)
    [](/load-files-faster-into-bigquery-94355c4c086a?source=post_page-----334a0a9adef2--------------------------------)
    [## 更快地将文件加载到 BigQuery'
- en: Benchmarking CSV, GZIP, AVRO and PARQUET file types for ingestion
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对 CSV、GZIP、AVRO 和 PARQUET 文件类型进行基准测试
- en: towardsdatascience.com](/load-files-faster-into-bigquery-94355c4c086a?source=post_page-----334a0a9adef2--------------------------------)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/load-files-faster-into-bigquery-94355c4c086a?source=post_page-----334a0a9adef2--------------------------------)'
- en: 4 Generate Arrays for More Powerful Experimentations
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 生成数组以进行更强大的实验
- en: BigQuery allows you to **generate arrays for quick and cheap experimentation
    by querying your data multiple times**. Use the `GENERATE_ARRAY()` and `UNNEST()`
    functions to cross-join your data for testing parameters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 允许你**通过多次查询数据生成数组以进行快速且廉价的实验**。使用 `GENERATE_ARRAY()` 和 `UNNEST()` 函数对数据进行交叉连接以测试参数。
- en: For example, if you have a TB of data, but you want to test 100 different parameters
    for an experiment, don’t query your data 100 times as that would mean you query
    100TB resulting in a bill of $500! Instead, **if you cross-join your data with
    your 100 parameters, you’ll only incur costs for querying that single TB**, so
    your bill is $5.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你有一 TB 的数据，但你想测试 100 个不同的实验参数，不要对数据进行 100 次查询，因为那样会查询 100TB 数据，账单将高达 500
    美元！相反，**如果你将数据与 100 个参数进行交叉连接，你只会为查询那一 TB 数据而产生费用**，所以账单是 5 美元。
- en: 'Here’s a made-up example for finding the optimal values for the `min_price`,
    `min_quantity`, and `max_delivery_time` parameters that maximize the profit margin
    while meeting customer expectations:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个虚拟的示例，用于寻找 `min_price`、`min_quantity` 和 `max_delivery_time` 参数的最佳值，以在满足客户期望的同时最大化利润：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And if you want a more sophisticated example, check this article out:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要一个更复杂的示例，可以查看这篇文章：
- en: '[](/advanced-random-sampling-in-bigquery-sql-7d4483b580bb?source=post_page-----334a0a9adef2--------------------------------)
    [## Advanced Random Sampling in BigQuery SQL'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/advanced-random-sampling-in-bigquery-sql-7d4483b580bb?source=post_page-----334a0a9adef2--------------------------------)
    [## BigQuery SQL 中的高级随机采样'
- en: Learn how to sample rows from BigQuery tables in a reproducible manner
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习如何以可重现的方式从 BigQuery 表中采样行
- en: towardsdatascience.com](/advanced-random-sampling-in-bigquery-sql-7d4483b580bb?source=post_page-----334a0a9adef2--------------------------------)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/advanced-random-sampling-in-bigquery-sql-7d4483b580bb?source=post_page-----334a0a9adef2--------------------------------)
- en: 5 Harness the Power of WITH Statements
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 利用 `WITH` 语句的强大功能
- en: BigQuery’s `WITH` statement, also known as **Common Table Expressions** (CTEs),
    can make your queries more readable and maintainable. By using `WITH` statements,
    you can **break down complex queries into smaller, reusable components, making
    it easier to understand and debug your queries**. Additionally, CTEs can improve
    performance by preventing the same subquery from being executed multiple times.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 的 `WITH` 语句，也称为**公共表表达式**（CTE），可以使你的查询更加可读和易于维护。通过使用 `WITH` 语句，你可以**将复杂的查询分解成更小的、可重用的组件，使理解和调试查询变得更容易**。此外，CTE
    还能通过防止相同的子查询被多次执行来提高性能。
- en: 'You can use as many of these as you like, and you can even create new CTEs
    from already-defined CTEs:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以根据需要使用这些功能，并且你甚至可以从已经定义的 CTE 创建新的 CTE：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 6 Leverage Arguments in Custom Functions
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 在自定义函数中利用参数
- en: When creating custom functions in BigQuery, make use of arguments to create
    reusable and flexible functions. **You can build modular and easily maintainable
    code by passing arguments to your functions**. This practice leads to cleaner
    and more efficient code, streamlining your development process.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BigQuery 中创建自定义函数时，利用参数来创建可重用和灵活的函数。**通过向函数传递参数，你可以构建模块化和易于维护的代码**。这种做法能够使代码更简洁、更高效，从而简化开发过程。
- en: 'Want to learn about BigQuery custom functions? Glad you asked, check this out:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解 BigQuery 自定义函数吗？很高兴你问了，看看这个：
- en: '[](/bigquery-udfs-complete-guide-181cbdaea55b?source=post_page-----334a0a9adef2--------------------------------)
    [## BigQuery UDFs Complete Guide'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/bigquery-udfs-complete-guide-181cbdaea55b?source=post_page-----334a0a9adef2--------------------------------)
    [## BigQuery UDFs 完整指南'
- en: Everything you need to know about Google Cloud BigQuery’s User-Defined Functions
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于 Google Cloud BigQuery 用户定义函数的所有信息
- en: towardsdatascience.com](/bigquery-udfs-complete-guide-181cbdaea55b?source=post_page-----334a0a9adef2--------------------------------)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/bigquery-udfs-complete-guide-181cbdaea55b?source=post_page-----334a0a9adef2--------------------------------)
- en: 'Want to deploy a bunch of these using CI/CD pipelines on GitHub? Look no more:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 想在 GitHub 上通过 CI/CD 管道部署这些功能？别再找了：
- en: '[](https://levelup.gitconnected.com/deploying-bigquery-custom-functions-with-github-actions-d76c118e0abf?source=post_page-----334a0a9adef2--------------------------------)
    [## Deploying BigQuery Custom Functions with GitHub Actions'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/deploying-bigquery-custom-functions-with-github-actions-d76c118e0abf?source=post_page-----334a0a9adef2--------------------------------)
    [## 使用 GitHub Actions 部署 BigQuery 自定义函数'
- en: Streamline your BigQuery custom functions deployment with the power of GitHub
    Actions automation.
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用 GitHub Actions 自动化来简化你的 BigQuery 自定义函数部署。
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/deploying-bigquery-custom-functions-with-github-actions-d76c118e0abf?source=post_page-----334a0a9adef2--------------------------------)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/deploying-bigquery-custom-functions-with-github-actions-d76c118e0abf?source=post_page-----334a0a9adef2--------------------------------)
- en: Happy Querying!
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询愉快！
- en: Mastering these best practices will help you take full advantage of BigQuery’s
    features while maintaining cost efficiency and improving performance. By monitoring
    cost estimates, using data sampling with `FARM_FINGERPRINT()`, embracing Parquet,
    generating arrays, utilizing `WITH` statements, and employing arguments in custom
    functions, you'll be well-equipped to tackle any BigQuery challenge with confidence.
    Happy querying! 💻
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握这些最佳实践将帮助你充分利用 BigQuery 的功能，同时保持成本效益并提升性能。通过监控成本估算、使用 `FARM_FINGERPRINT()`
    进行数据采样、使用 Parquet 格式、生成数组、利用 `WITH` 语句以及在自定义函数中使用参数，你将能够自信地应对任何 BigQuery 挑战。查询愉快！💻
