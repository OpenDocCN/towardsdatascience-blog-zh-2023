- en: Understanding DeepMind Matrix Multiplication
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解DeepMind矩阵乘法
- en: 原文：[https://towardsdatascience.com/understanding-deepmind-matrix-multiplication-c8dc49687ce7](https://towardsdatascience.com/understanding-deepmind-matrix-multiplication-c8dc49687ce7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-deepmind-matrix-multiplication-c8dc49687ce7](https://towardsdatascience.com/understanding-deepmind-matrix-multiplication-c8dc49687ce7)
- en: DeepMind matrix multiplications on NVIDIA V100, Tesla T4, and a look at FBHHRBNRSSSHK
    — which is not me typing random letters!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DeepMind矩阵乘法在NVIDIA V100、Tesla T4上的表现，以及FBHHRBNRSSSHK——这可不是我在随意输入字母！
- en: '[](https://stefanobosisio1.medium.com/?source=post_page-----c8dc49687ce7--------------------------------)[![Stefano
    Bosisio](../Images/450d904024a4cbf1adf8a625886d852e.png)](https://stefanobosisio1.medium.com/?source=post_page-----c8dc49687ce7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c8dc49687ce7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c8dc49687ce7--------------------------------)
    [Stefano Bosisio](https://stefanobosisio1.medium.com/?source=post_page-----c8dc49687ce7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://stefanobosisio1.medium.com/?source=post_page-----c8dc49687ce7--------------------------------)[![Stefano
    Bosisio](../Images/450d904024a4cbf1adf8a625886d852e.png)](https://stefanobosisio1.medium.com/?source=post_page-----c8dc49687ce7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c8dc49687ce7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c8dc49687ce7--------------------------------)
    [Stefano Bosisio](https://stefanobosisio1.medium.com/?source=post_page-----c8dc49687ce7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c8dc49687ce7--------------------------------)
    ·7 min read·Feb 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c8dc49687ce7--------------------------------)
    ·7分钟阅读·2023年2月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a8a82776e50742a320323c87c1b96767.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8a82776e50742a320323c87c1b96767.png)'
- en: Image by [Ivan Diaz](https://unsplash.com/@ivvndiaz) on [Unsplash](https://unsplash.com/photos/Z-PU9Lv441Y)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[伊凡·迪亚兹](https://unsplash.com/@ivvndiaz)提供，发布于[Unsplash](https://unsplash.com/photos/Z-PU9Lv441Y)
- en: 'In the previous [post](/understanding-deepmind-and-strassen-algorithms-9bdb3d8b6ea6),
    we learned the maths behind the Strassen algorithm and wrote a Python code to
    run-test it against different matrices sizes. Moreover, we’ve learned that the
    Holy Grail of linear algebra is an optimization algorithm for matrix multiplication.
    Normally, we would think of a matrix multiplication code as three for-loops:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的[帖子](/understanding-deepmind-and-strassen-algorithms-9bdb3d8b6ea6)中，我们学习了Strassen算法背后的数学，并编写了Python代码以在不同矩阵大小下进行测试。此外，我们了解到线性代数的圣杯是矩阵乘法的优化算法。通常，我们会把矩阵乘法代码看作是三个for循环：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Thus the computational complexity is *O(n³).* Strassen has improved this calculation,
    finding the following relationships:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，计算复杂度是*O(n³)*。Strassen改进了这一计算，找到了以下关系：
- en: '![](../Images/05de81f0850b2f05079d9623f5729882.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05de81f0850b2f05079d9623f5729882.png)'
- en: 'Fig.1: Strassen algorithm as proposed in his paper “Gaussian Elimination is
    not Optimal”. [Image by the author]'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Strassen算法，如其论文“高斯消去法不是最优的”中提出的。[作者提供的图片]
- en: This algorithm is applied to block matrices and the total complexity is reduced
    to *O(n²·⁸⁰⁸).* Although 2.808 may seem a very little improvement, we saw that
    for square matrices of size 4096 the standard numpy `matmult` takes about 454.37
    +/- 6.27 s, while Strassen takes 31.57 +/- 1.01, which is a difference of about
    one order of magnitude.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法应用于块矩阵，总复杂度降至*O(n²·⁸⁰⁸)*。虽然2.808可能看起来改进很小，但我们看到对于4096大小的方阵，标准numpy `matmult`大约需要454.37
    +/- 6.27秒，而Strassen需要31.57 +/- 1.01秒，差异约为一个数量级。
- en: 'We saw that the matrix multiplication problem can be reduced to a tensor product,
    with the *tensoring* operation:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到矩阵乘法问题可以简化为张量积，通过*张量*操作：
- en: '![](../Images/12d14eff7c20296a202a8ca28f375be5.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12d14eff7c20296a202a8ca28f375be5.png)'
- en: 'Fig.2: Definition of the matrix multiplication tensor as a triad, as defined
    in Deep Mind paper. [Image by the author].'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：矩阵乘法张量的三元组定义，如Deep Mind论文中所定义的。[作者提供的图片]
- en: Fig.2 reports exactly the matrix multiplication, expressed as *a triad*, namely
    three elements. *The minimal number of triads defines the minimal number of operations
    to compute the product matrix.* This minimal number is the tensor’s *rank* *R(t)*.
    Working on the tensor rank is an efficient way to find new multiplication algorithms,
    as explained in the DeepMind papers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2准确报告了矩阵乘法，表示为*三元组*，即三个元素。*最小三元组数定义了计算乘积矩阵所需的最小操作数。* 这个最小数就是张量的*秩* *R(t)*。研究张量秩是一种有效的方式来寻找新的乘法算法，如DeepMind的论文中所述。
- en: 'DeepMind has shown in these years how mathematical problems, from theoretical
    to application, can be tackled with a machine learning approach, using the reinforcement
    learning (RL) approach. Also during this time they’ve adapted AlphaZero to find
    the best strategy for multiplying matrices, and the result is AlphaTensor. I think
    it’s worth defining right now what we can appreciate in this paper:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind这些年来展示了如何通过机器学习方法、特别是强化学习（RL）方法，解决从理论到应用的数学问题。在此期间，他们还调整了AlphaZero，以寻找矩阵乘法的最佳策略，结果就是AlphaTensor。我认为现在定义一下我们可以从这篇论文中欣赏到的内容是值得的：
- en: DeepMind proved again that RL can be a formidable ally for tackling complicated
    mathematical problems;
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DeepMind再次证明了RL可以成为解决复杂数学问题的强大助手；
- en: AlphaTensor has found an acceptable algorithm that can be even better than Strassen’s
    one, to multiply 4 x 4 and 5 x 5 block matrices;
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AlphaTensor找到了一个可以比Strassen算法更好的算法，用于4 x 4和5 x 5块矩阵的乘法；
- en: Moreover, AlphaTensor can find an optimal solution for specific hardware requirements.
    As they show in the paper, there can be algorithms specifically tuned for TPUs
    and GPUs (V100 in the paper case);
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，AlphaTensor可以为特定的硬件需求找到最优解。正如他们在论文中所示，可能会有专门针对TPU和GPU（论文中的V100）的算法；
- en: Although these may not be the best results, mathematicians can now have a completely
    new set of equations for the matrix multiplication problem that can be a new starting
    point for finding new optimal solutions.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管这些可能不是最佳结果，但数学家们现在可以拥有一套全新的矩阵乘法方程，这可以作为寻找新的最优解的起点。
- en: Testing AlphaTensor on a V100 GPU
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在V100 GPU上测试AlphaTensor
- en: Luckily for us, DeepMind provides on its [GitHub the implementation for AlphaTensor,](https://github.com/deepmind/alphatensor)
    ready to be tested and all written in JAX. The code is tailored for multiplying
    4 x 4 block matrices, with a total of 47 multiplications, already reported in
    the tensor notations, as you can see [here](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/factorizations.py#L21).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，DeepMind在其[GitHub上提供了AlphaTensor的实现，](https://github.com/deepmind/alphatensor)可以进行测试，全部用JAX编写。代码专为4
    x 4块矩阵的乘法设计，共有47次乘法，已在张量表示中报告，具体可见[这里](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/factorizations.py#L21)。
- en: 'The benchmark is run on a TPU and V100 GPU and they’re testing matrices multiplications
    for the following [sizes](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/run_gpu_benchmark.py#L67):
    8192, 10240, 12288, 14336, 16384, 18432, 20480 for the [standard](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/utils.py#L155)
    `[jax.numpy.dot](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/utils.py#L155)`[multiplication](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/utils.py#L155),
    the Strassen algorithm and the AlphaTensor one.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试在TPU和V100 GPU上进行，他们测试了以下[尺寸](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/run_gpu_benchmark.py#L67)的矩阵乘法：8192,
    10240, 12288, 14336, 16384, 18432, 20480，对于[标准](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/utils.py#L155)的`[jax.numpy.dot](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/utils.py#L155)`[乘法](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/utils.py#L155)，Strassen算法和AlphaTensor算法。
- en: 'I forked the code and added two tiny modifications:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我分叉了代码，并进行了两个小的修改：
- en: I am printing the timings for each approach
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我正在打印每种方法的时间
- en: Since we are running the multiplication 10 times for each algorithm, *I modified
    the* [*average number in the benchmark functions to 10*](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/utils.py#L227)*,
    rather than keeping it as 20*.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们对每个算法进行10次乘法运算，*我将基准函数中的* [*平均次数修改为10*](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/utils.py#L227)*，而不是保持为20*。
- en: 'Having free access to the Google Cloud Console (still relying on $300 credits)
    I have created a Virtual Machine in GCP Compute Engine to test AlphaTensor, with
    the following specifics:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在拥有Google Cloud Console的免费访问权限（仍然依赖$300积分）的情况下，我在GCP Compute Engine中创建了一个虚拟机以测试AlphaTensor，具体如下：
- en: Running region `europe-west4`
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行区域为`europe-west4`
- en: Selected GPU machine with 1 NVIDIA V100 GPU
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择了带有1 NVIDIA V100 GPU的GPU机器
- en: Automatic selection for the CPU platform
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动选择CPU平台
- en: '`n1-standard-4` machine type (4 vCPU and 15 GB RAM)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n1-standard-4`机器类型（4 vCPU和15 GB RAM）'
- en: 'I changed the OS image to: Operating System `Deep Learning on Linux` and Version
    `Debian 10 based Deep Learning VM for TensorFlow Enterprise 1.15 with CUDA 11.0
    M100` and disk size `50GB`'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我将操作系统镜像更改为：操作系统`Deep Learning on Linux`和版本`基于Debian 10的Deep Learning VM用于TensorFlow
    Enterprise 1.15，带有CUDA 11.0 M100`，磁盘大小为`50GB`
- en: The total cost is $1.94 per hour — so be careful and do not let this machine
    run indefinitely.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 总成本为每小时$1.94 — 因此要小心，不要让这台机器无限期运行。
- en: Once the machine is created, you can directly access with SSH and download the
    repo with `git clone https://github.com/Steboss/alphatensor.git` . You’ll have
    to set up the Python environment and install `jax` with `pip3 install -r alphatensor/benchmarking/requirements.txt
    -f [https://storage.googleapis.com/jax-releases/jax_cuda_releases.html](https://storage.googleapis.com/jax-releases/jax_cuda_releases.html)`
    . Finally, you can run the test with `python alphatensor.benchmarking.run_gpu_benchmark`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了机器，你可以直接通过SSH访问并使用`git clone https://github.com/Steboss/alphatensor.git`下载代码库。你需要设置Python环境并使用`pip3
    install -r alphatensor/benchmarking/requirements.txt -f [https://storage.googleapis.com/jax-releases/jax_cuda_releases.html](https://storage.googleapis.com/jax-releases/jax_cuda_releases.html)`安装`jax`。最后，你可以通过`python
    alphatensor.benchmarking.run_gpu_benchmark`运行测试。
- en: '![](../Images/d477e22f2c6e2f2a3f7f93cf70703b46.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d477e22f2c6e2f2a3f7f93cf70703b46.png)'
- en: 'Fig.3: Comparison between Jax matrix multiplication, Strassen algorithm and
    AlphaTensor on V100 GPU. [Image by the author].'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Jax矩阵乘法、Strassen算法与AlphaTensor在V100 GPU上的比较。 [图片来源：作者]。
- en: Fig.3 reports the performance time with respect to the matrix size for each
    algorithm. We can see that for *small matrices* size, 8192 and 10240, the Strassen
    achieves an *improvement of about 6.5%* with respect to standard JAX, comparable
    with the AlphaTensor improvement of about 8.5%. Excellent results are achieved
    for *bigger matrices* so that for 18432 square matrices Strassen improves the
    calculation by *15%* (7.37 +/- 0.01) and AlphaTensor achieves an improvement of
    the *16%* (7.31 +/- 0.01) compared to JAX (8.53 +/- 0.01).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图3显示了每个算法相对于矩阵大小的性能时间。我们可以看到，对于*小矩阵*尺寸，8192和10240，Strassen相对于标准JAX实现了*约6.5%的提升*，与AlphaTensor约8.5%的提升相当。对于*大矩阵*，取得了优异的结果，因此对于18432的方阵，Strassen的计算提升了*15%*（7.37
    +/- 0.01），而AlphaTensor相对于JAX（8.53 +/- 0.01）达到了*16%*的提升（7.31 +/- 0.01）。
- en: What if I don’t have free access to V100?
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果我没有免费访问V100的权限怎么办？
- en: Another test I’ve done was on Google Colab. In this case, we can rely on a Tesla
    T4 GPU. Although the algorithm has been tested on V100, it’s worth investigating
    its transferability and comparing the results. Similarly to the V100 test, I’ve
    replicated these calculations on a Google Colab notebook, [removing these lines](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/run_gpu_benchmark.py#L53:L55)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我还在Google Colab上进行了另一个测试。在这种情况下，我们可以依赖Tesla T4 GPU。虽然算法已经在V100上测试过，但值得调查其可迁移性并比较结果。与V100测试类似，我在Google
    Colab笔记本上复制了这些计算，[去除了这些行](https://github.com/deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/run_gpu_benchmark.py#L53:L55)
- en: '![](../Images/0f1cd3d44638901ea2a35bd6389526e4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f1cd3d44638901ea2a35bd6389526e4.png)'
- en: 'Fig.4: Comparison between Jax matrix multiplication, Strassen algorithm and
    AlphaTensor on Tesla T4\. [Image by the author].'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Jax矩阵乘法、Strassen算法与AlphaTensor在Tesla T4上的比较。 [图片来源：作者]。
- en: 'As you can see we have more variability in the results, especially for matrices
    of size 16384 we can see that all the algorithms achieve the same performance
    timings. This is not accurate, as it may be due to some downtime that we can’t
    manage on Google Colab. Tab.1 summarises all the findings on Tesla T4:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们在结果中有更多的变化，特别是在 16384 大小的矩阵中，我们可以看到所有算法实现了相同的性能时序。这并不准确，因为这可能是由于我们在
    Google Colab 上无法管理的一些停机时间。表1 总结了在 Tesla T4 上的所有发现：
- en: 'Tab.1: Comparison of performance timings on a Tesla T4 for JAX, Strassen and
    AlphaTensor matrix multiplication algorithm.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在 Tesla T4 上对 JAX、Strassen 和 AlphaTensor 矩阵乘法算法的性能时序比较。
- en: Sizes 12288 and 16384 are tricky points, where we don’t have a real improvement
    with respect to JAX standard multiplication. On the other side, we can see that
    we have an improvement for very large matrices, at 18432 Strassen achieves a 20%
    speedup and Alphatensor 22%.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尺寸 12288 和 16384 是棘手的点，在这些点上我们相对于 JAX 标准乘法没有实际改进。另一方面，我们可以看到对于非常大的矩阵有改进，在 18432
    时，Strassen 实现了 20% 的加速，AlphaTensor 实现了 22% 的加速。
- en: Is this the end of the story?
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这是故事的结局吗？
- en: 'Just a few days after [DeepMind’s paper](https://www.nature.com/articles/s41586-022-05172-4)
    was published, Manuel Kauers and Jakob Moosbauer wrote a great paper reply, proposing
    the [FBHHRBNRSSSHK-Algorithm](https://arxiv.org/abs/2210.04045). This algorithm
    starts from DeepMind findings and improves the calculation over 4x4 matrices using
    47 multiplications, rather than 49 as AlphaTensor found, and 5x5 matrices to 95
    multiplications (rather than the 96 proposed by AlphaTensor). This is great news
    and it shows how humans can collaborate productively with ML algorithms. After
    this reply, Kauers and Moosbaur published a fantastic mathematical paper called
    “[Flip Graphs for Matrix Multiplication](https://arxiv.org/abs/2212.01175)”. In
    this paper the authors showed the technique they found to further improve matrix
    multiplications. In particular, the core part of the technique is to start from
    known schemes for matrix multiplications and group them in a graph:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 就在 [DeepMind 论文](https://www.nature.com/articles/s41586-022-05172-4) 发布几天后，Manuel
    Kauers 和 Jakob Moosbauer 写了一篇精彩的论文回复，提出了 [FBHHRBNRSSSHK-算法](https://arxiv.org/abs/2210.04045)。该算法基于
    DeepMind 的发现，使用 47 次乘法改进了 4x4 矩阵的计算，而不是 AlphaTensor 发现的 49 次乘法，5x5 矩阵的乘法次数减少到
    95 次（而不是 AlphaTensor 提出的 96 次）。这是一个好消息，表明人类可以与 ML 算法有效合作。在这篇回复之后，Kauers 和 Moosbauer
    发表了一篇出色的数学论文，名为 “[矩阵乘法的翻转图](https://arxiv.org/abs/2212.01175)”。在这篇论文中，作者展示了他们找到的进一步改进矩阵乘法的技术。特别是，这项技术的核心部分是从已知的矩阵乘法方案开始，并将其分组在一个图中：
- en: We define a graph whose vertices are correct matrix multiplication schemes and
    where there is an edge from one scheme to another if the second can be obtained
    from the first by some kind of transformation. We consider two transformations.
    One is called a flip and turns a given scheme to a different one with the same
    number of multiplications, and the other is called a reduction and turns a given
    scheme to one with a smaller number of multiplications
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们定义了一个图，其中的顶点是正确的矩阵乘法方案，如果第二个方案可以通过某种变换从第一个方案获得，则在两个方案之间存在一条边。我们考虑了两种变换。一种叫做翻转，将给定方案转变为一个不同的方案，乘法次数相同，另一种叫做归约，将给定方案转变为一个乘法次数更少的方案。
- en: 'The navigation across all the matrix multiplication schemes is done through
    a random walk. Then, the flip can be done using the following idea:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所有矩阵乘法方案之间的导航是通过随机游走完成的。然后，翻转可以使用以下想法进行：
- en: The idea is to subtract something from one rank-one tensor and add it to one
    of the others.
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个想法是从一个秩-1张量中减去某些东西，然后将其添加到其他张量中。
- en: '![](../Images/b8378ec40828afe2dbaa1bdc7dea14ec.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8378ec40828afe2dbaa1bdc7dea14ec.png)'
- en: 'Fig.5: Example of flip graph, that shows how all the algorithms live as nodes
    in the graph, flips are undirected edges and reductions transformations are directed
    edges.[Image by Manuel Kauers and Jakob Moosbauer].'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：翻转图示例，展示了所有算法如何作为图中的节点存在，翻转是无向边，归约变换是有向边。[图片来源：Manuel Kauers 和 Jakob Moosbauer]。
- en: Fig.5 reports the paper’s image, where the authors have depicted all the known
    schemes and how they are interlinked with each other through flip and reduction
    transformations. Thus, this is not the end of the story, but it’s another great
    starting point, which will bring more and more efficient algorithms for matrix
    multiplication.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图5 显示了论文中的图像，作者描绘了所有已知方案以及它们如何通过翻转和归约变换相互连接。因此，这并不是故事的结局，而是另一个很好的起点，将带来越来越高效的矩阵乘法算法。
- en: Conclusions
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Today we’ve concluded the review of the DeepMind paper “Discovering faster matrix
    multiplication algorithms with reinforcement learning”. This paper has brought
    a lot of interest in the matrix multiplication field and, obviously, lots of questions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们结束了对DeepMind论文《利用强化学习发现更快的矩阵乘法算法》的评审。这篇论文在矩阵乘法领域引起了极大的兴趣，并且显然带来了许多问题。
- en: 'We defined 4 main points that we can draw from the paper:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从论文中得出了4个主要观点：
- en: RL is a formidable tool that can help us in tackling mathematical problems
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习是一个强大的工具，可以帮助我们解决数学问题。
- en: AlphaTensor has found new formulations for multiplying 5x5 and 4x4 matrices
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AlphaTensor 已经找到了乘法 5x5 和 4x4 矩阵的新公式。
- en: AlphaTensor can find optimal solutions for specific hardware (e.g. GPU or TPU)
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AlphaTensor 可以为特定硬件（例如 GPU 或 TPU）找到最佳解决方案。
- en: This is a great starting point for new research on the matrix multiplication
    problem
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是新研究矩阵乘法问题的一个很好的起点。
- en: Then, we run AlphaTensor on an NVIDIA V100 GPU and Tesla T4\. Although some
    ups and downs, we can see that overall AlphaTensor improves the calculation, with
    an improvement of up to 16% on V100 and 22% on Tesla T4 — although the algorithm
    is not optimized for such a GPU.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在NVIDIA V100 GPU 和 Tesla T4上运行了AlphaTensor。尽管有一些起伏，我们可以看到总体上AlphaTensor提高了计算效率，在V100上提高了多达16%，在Tesla
    T4上提高了22% —— 尽管该算法并未针对这种GPU进行优化。
- en: Finally, we saw that this is not the end of the story, but a beautiful start
    for a new store. An example is given by the [FBHHRBNRSSSHK-Algorithm](https://arxiv.org/abs/2210.04045)
    which proves how the DeepMind solution can be further exploited with a pure mathematical
    formalism to find new and more efficient matrix multiplication techniques.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看到这不仅仅是故事的结束，而是一个美好的新开始。一个例子是[FBHHRBNRSSSHK-算法](https://arxiv.org/abs/2210.04045)，它证明了如何通过纯数学形式进一步利用DeepMind的解决方案，找到新的、更高效的矩阵乘法技术。
- en: 'Support my writing:'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持我的写作：
- en: '[](https://stefanobosisio1.medium.com/membership?source=post_page-----c8dc49687ce7--------------------------------)
    [## Join Medium with my referral link - Stefano Bosisio'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://stefanobosisio1.medium.com/membership?source=post_page-----c8dc49687ce7--------------------------------)
    [## 通过我的推荐链接加入Medium - Stefano Bosisio'
- en: Read every story from Stefano Bosisio (and thousands of other writers on Medium).
    Why supporting me? 1) Articles on AI…
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读Stefano Bosisio的每个故事（以及Medium上成千上万其他作家的故事）。为什么支持我？1）关于AI的文章……
- en: stefanobosisio1.medium.com](https://stefanobosisio1.medium.com/membership?source=post_page-----c8dc49687ce7--------------------------------)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: stefanobosisio1.medium.com](https://stefanobosisio1.medium.com/membership?source=post_page-----c8dc49687ce7--------------------------------)
- en: '*Please, feel free to send me an email for questions or comments at: stefanobosisio1@gmail.com
    or directly here in Medium.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果有任何问题或意见，请随时发邮件至：stefanobosisio1@gmail.com，或者直接在Medium上联系我。*'
