- en: 'T5: Text-to-Text Transformers (Part Two)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'T5: 文本到文本的变换器（第二部分）'
- en: 原文：[https://towardsdatascience.com/t5-text-to-text-transformers-part-two-837ba23a9eb4](https://towardsdatascience.com/t5-text-to-text-transformers-part-two-837ba23a9eb4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/t5-text-to-text-transformers-part-two-837ba23a9eb4](https://towardsdatascience.com/t5-text-to-text-transformers-part-two-837ba23a9eb4)
- en: Optimal transfer learning for large language models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型的最佳迁移学习
- en: '[](https://wolfecameron.medium.com/?source=post_page-----837ba23a9eb4--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----837ba23a9eb4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----837ba23a9eb4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----837ba23a9eb4--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----837ba23a9eb4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----837ba23a9eb4--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----837ba23a9eb4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----837ba23a9eb4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----837ba23a9eb4--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----837ba23a9eb4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----837ba23a9eb4--------------------------------)
    ·14 min read·Jul 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----837ba23a9eb4--------------------------------)
    ·阅读时间14分钟·2023年7月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/16eb0b06a8e38a4121e6c959124618c5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16eb0b06a8e38a4121e6c959124618c5.png)'
- en: (Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/text?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片来源于[Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)于[Unsplash](https://unsplash.com/s/photos/text?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: 'The proposal of [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [5] led to the popularization of transfer learning approaches for natural language
    processing (NLP). Due to the widespread availability of unlabeled text on the
    internet, we could easily *(i)* pre-train large transformer models over large
    amounts of raw text and *(ii)* fine-tune these models to accurately solve downstream
    tasks. This approach was incredibly effective, but its newfound popularity led
    many alternative methods and modifications to be proposed. With all these new
    methods becoming available, one could easily begin to wonder: *What are the best
    practices for transfer learning in NLP?*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [5] 的提出促使了自然语言处理（NLP）领域中迁移学习方法的普及。由于互联网上大量未标记文本的广泛存在，我们可以轻松地*(i)* 在大量原始文本上预训练大型变换器模型，以及*(ii)*
    微调这些模型以准确解决下游任务。这种方法非常有效，但其新兴的受欢迎程度导致许多替代方法和改进被提出。随着这些新方法的出现，人们不禁开始想：*NLP中迁移学习的最佳实践是什么？*'
- en: This question was answered by analysis performed with the unified text-to-text
    transformer (T5) model. T5 reformulates all tasks (during both pre-training and
    fine-tuning) with a text-to-text format, meaning that the model receives textual
    input and produces textual output. Using this unified format, T5 can analyze various
    different transfer learning settings, allowing many approaches to be compared.
    In a [previous newsletter](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part),
    we learned about the format, architecture, and overall approach of the T5 model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题通过对统一的文本到文本变换器（T5）模型的分析得到了回答。T5在预训练和微调期间都将所有任务重新格式化为文本到文本的格式，这意味着模型接收文本输入并生成文本输出。利用这种统一格式，T5可以分析各种不同的迁移学习设置，从而允许比较多种方法。在[之前的通讯](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part)中，我们了解了T5模型的格式、架构和整体方法。
- en: In this newsletter, we will outline the analysis performed by T5, including
    an empirical comparison different pre-training objectives, architectures, model/data
    scales, and training approaches for transfer learning in NLP. Within [1], each
    of these options are studied one-at-a-time to determine their impact on T5’s performance.
    By studying this analysis, we arrive at a set of best practices, which (when combined
    together) produce the state-of-the-art T5 framework that can solve language understanding
    tasks with incredibly high accuracy.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本期通讯中，我们将概述 T5 进行的分析，包括不同预训练目标、架构、模型/数据规模和 NLP 领域迁移学习的训练方法的实证比较。在[1]中，这些选项被逐一研究，以确定它们对
    T5 性能的影响。通过研究这些分析，我们得出了一套最佳实践，这些实践（当结合在一起时）产生了最先进的 T5 框架，能够以极高的准确性解决语言理解任务。
- en: '![](../Images/f405573770f7dcd828722a57a401b9aa.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f405573770f7dcd828722a57a401b9aa.png)'
- en: (from [1])
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Preliminaries
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础知识
- en: We already covered the motivation and basics of the T5 architecture. Check out
    that post at the link [here](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part).
    We can quickly cover these ideas here as well. The proposal of [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [5] popularized the [transfer learning](https://cameronrwolfe.substack.com/i/108182616/what-is-transfer-learning)
    paradigm (i.e., pre-training a model over some separate dataset, then fine-tuning
    on a target dataset) for NLP. However, BERT’s effectiveness led many researchers
    to focus on this topic and propose various modifications and improvements. The
    idea of T5 is to *(i)* convert all language tasks into a unified, text-to-text
    format (see figure below) and *(ii)* study a bunch of different settings for transfer
    learning in NLP to deduce the techniques that work best.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经覆盖了 T5 架构的动机和基础知识。请查看链接中的帖子 [这里](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part)。我们也可以快速回顾这些思想。
    [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [5] 的提案普及了 [迁移学习](https://cameronrwolfe.substack.com/i/108182616/what-is-transfer-learning)
    范式（即，在某些独立数据集上预训练模型，然后在目标数据集上进行微调）用于 NLP。然而，BERT 的有效性使许多研究人员将重点放在这个话题上，并提出各种修改和改进。T5
    的想法是 *(i)* 将所有语言任务转换为统一的文本到文本格式（见下图），并 *(ii)* 研究各种不同的 NLP 迁移学习设置，以推断出最有效的技术。
- en: '![](../Images/8c1579aef03f0f44be25910121043e47.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c1579aef03f0f44be25910121043e47.png)'
- en: (from [1])
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Language Modeling vs. Denoising
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言建模 vs. 去噪
- en: 'Initial transfer learning approaches in NLP leveraged a [causal language modeling](https://cameronrwolfe.substack.com/i/85568430/language-modeling)
    objective [6] for pre-training. However, denoising (also called [masked language
    modeling](https://cameronrwolfe.substack.com/i/76273144/training-bert), or MLM)
    objectives were subsequently shown to perform better [5]. Given a set of textual
    tokens to be passed as input to some model, MLM operates by:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 初期的 NLP 迁移学习方法利用了 [因果语言建模](https://cameronrwolfe.substack.com/i/85568430/language-modeling)
    目标 [6] 进行预训练。然而，随后显示去噪（也称为 [掩码语言建模](https://cameronrwolfe.substack.com/i/76273144/training-bert)，或
    MLM）目标表现更好 [5]。给定一组要作为输入传递给某些模型的文本标记，MLM 通过以下方式操作：
- en: Randomly (uniformly) selecting 15% of the tokens
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机（均匀）选择 15% 的标记
- en: Replacing 90% of selected tokens with a `[MASK]` token
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用一个 `[MASK]` 标记替换 90% 的选择标记
- en: Replacing 10% of selected tokens with a random token
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用一个随机标记替换 10% 的选择标记
- en: Training the model to predict/classify each `[MASK]` token
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型预测/分类每个 `[MASK]` 标记
- en: The percentage of tokens that are uniformly selected is called the “corruption
    rate”. Within T5, we will see a few different variants of this denoising objective,
    but the basic idea remains the same.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 被均匀选择的标记的百分比称为“损坏率”。在 T5 中，我们将看到这种去噪目标的几个不同变体，但基本想法保持不变。
- en: '*“All of our objectives ingest a sequence of token IDs corresponding to a tokenized
    span of text from our unlabeled text data set. The token sequence is processed
    to produce a (corrupted) input sequence and a corresponding target. Then, the
    model is trained as usual with maximum likelihood to predict the target sequence.”*
    — from [1]'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“我们所有的目标都接收来自我们未标记文本数据集的一个标记化文本片段的标记 ID 序列。标记序列被处理以生成一个（损坏的）输入序列和相应的目标。然后，模型像往常一样通过最大似然法来预测目标序列。”*
    — 来自 [1]'
- en: Benchmarks and Evaluation
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准测试和评估
- en: T5 attempts to derive a set of best practices for transfer learning in NLP.
    To determine which techniques work best, however, T5 is evaluated on a variety
    of different tasks and natural language benchmarks. All of these task are solved
    using T5’s [text-to-text format](https://cameronrwolfe.substack.com/i/108182616/text-to-text-framework).
    See Section 2.3 in [1] for a full description of these tasks. A brief summary
    is provided below.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: T5 尝试推导出 NLP 中迁移学习的最佳实践集。然而，为了确定哪些技术效果最佳，T5 在各种任务和自然语言基准上进行了评估。所有这些任务都是使用 T5
    的 [文本到文本格式](https://cameronrwolfe.substack.com/i/108182616/text-to-text-framework)解决的。有关这些任务的完整描述，请参见
    [1] 的第 2.3 节。下面提供了简要总结。
- en: '[GLUE](https://gluebenchmark.com/) and [SuperGLUE](https://super.gluebenchmark.com/)
    [7, 8]: both benchmarks include many different tasks, such as [sentence acceptability
    judgement](https://nyu-mll.github.io/CoLA/), [sentiment analysis](https://huggingface.co/datasets/sst2),
    [paraphrasing](https://paperswithcode.com/dataset/mrpc), [sentence similarity](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts-benchmark),
    [natural language inference (NLI)](https://cims.nyu.edu/~sbowman/multinli/), [coreference
    resolution](https://paperswithcode.com/sota/natural-language-inference-on-wnli),
    [sentence completion](https://paperswithcode.com/dataset/copa), [word sense disambiguation](https://pilehvar.github.io/wic/),
    and [question answering](https://sheng-z.github.io/ReCoRD-explorer/). SuperGLUE
    is an improved and more difficult benchmark with a similar structure to GLUE.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GLUE](https://gluebenchmark.com/) 和 [SuperGLUE](https://super.gluebenchmark.com/)
    [7, 8]：这两个基准测试包含许多不同的任务，如 [句子接受度判断](https://nyu-mll.github.io/CoLA/)、[情感分析](https://huggingface.co/datasets/sst2)、[释义](https://paperswithcode.com/dataset/mrpc)、[句子相似度](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts-benchmark)、[自然语言推断（NLI）](https://cims.nyu.edu/~sbowman/multinli/)、[共指消解](https://paperswithcode.com/sota/natural-language-inference-on-wnli)、[句子完成](https://paperswithcode.com/dataset/copa)、[词义消歧](https://pilehvar.github.io/wic/)和
    [问答](https://sheng-z.github.io/ReCoRD-explorer/)。SuperGLUE 是一个改进且更具挑战性的基准测试，其结构类似于
    GLUE。'
- en: '[CNN + Daily Mail Abstractive Summarization](https://huggingface.co/datasets/cnn_dailymail)
    [9]: pairs news articles with a short, summarized sequence of text that captures
    the main highlights of the article.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CNN + Daily Mail 抽象总结](https://huggingface.co/datasets/cnn_dailymail) [9]：将新闻文章与一段简短的总结文本配对，捕捉文章的主要亮点。'
- en: '[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) [10]: a question answering
    dataset on Wikipedia articles, where the answer to each question is a segment
    of text from the related article.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) [10]：一个关于维基百科文章的问答数据集，其中每个问题的答案是相关文章中的一段文本。'
- en: Several translation datasets (e.g., English to German, French, and Romanian).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几个翻译数据集（例如，英语到德语、法语和罗马尼亚语）。
- en: Notably, all tasks in the GLUE and SuperGLUE benchmarks are concatenated together
    by T5, and fine-tuning is performed over all tasks at once.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，GLUE 和 SuperGLUE 基准测试中的所有任务都由 T5 连接在一起，并且在所有任务上同时进行微调。
- en: Other Important Ideas
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他重要观点
- en: Different Types of Transformer Architectures [[link](https://cameronrwolfe.substack.com/i/108182616/different-transformer-architectures)]
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的 Transformer 架构 [[link](https://cameronrwolfe.substack.com/i/108182616/different-transformer-architectures)]
- en: Language Modeling Basics [[link](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)]
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模基础 [[link](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)]
- en: Self-Attention [[link](https://twitter.com/cwolferesearch/status/1641932082283700226?s=20)]
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力 [[link](https://twitter.com/cwolferesearch/status/1641932082283700226?s=20)]
- en: What do we learn from T5?
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们从 T5 中学到了什么？
- en: As previously mentioned, the experiments in T5 attempt to discover best practices
    for transfer learning in NLP. To do this, a baseline approach is first proposed,
    then several aspects of this baseline (e.g., model architecture/size, dataset,
    and pre-training objective) are changed one-at-a-time to see what works best.
    This approach mimics a [coordinate descent](https://en.wikipedia.org/wiki/Coordinate_descent)
    strategy. We will first describe the baseline technique, then explain T5’s findings
    after testing a variety of different transfer learning settings.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，T5 的实验尝试发现 NLP 中迁移学习的最佳实践。为此，首先提出一种基线方法，然后逐一更改该基线的几个方面（例如，模型架构/大小、数据集和预训练目标），以查看哪些效果最佳。这种方法类似于
    [坐标下降](https://en.wikipedia.org/wiki/Coordinate_descent)策略。我们将首先描述基线技术，然后解释 T5
    在测试各种迁移学习设置后的发现。
- en: T5 Baseline Model
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: T5 基线模型
- en: '![](../Images/1164abd82c0351276b47aebc6c0ff224.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1164abd82c0351276b47aebc6c0ff224.png)'
- en: (from [11])
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [11]）
- en: '**the model.** The T5 baseline architecture uses a standard, [encoder-decoder
    transformer architecture](https://newsletter.artofsaience.com/p/vision-transformers-from-idea-to);
    see above. Both the encoder and decoder are structured similarly to [BERTBase](https://huggingface.co/bert-base-uncased).
    Although many modern approaches for NLP use “single stack” transformer architecture
    (e.g., [encoder-only architecture](https://cameronrwolfe.substack.com/i/76273144/transformer-encoders)
    for BERT or [decoder-only architecture](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers)
    for most language models), T5 chooses to avoid these architectures. Interestingly,
    authors in [1] find that the encoder-decoder architecture achieves impressive
    results on both generative and classification tasks. Encoder-only models are not
    considered in [1] due to the fact that they are specialized for token/span prediction
    and don’t solve generative tasks well.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型**。T5基础架构使用标准的[编码器-解码器变换器架构](https://newsletter.artofsaience.com/p/vision-transformers-from-idea-to)；见上文。编码器和解码器的结构与[BERTBase](https://huggingface.co/bert-base-uncased)类似。尽管许多现代NLP方法使用“单堆栈”变换器架构（例如，[仅编码器架构](https://cameronrwolfe.substack.com/i/76273144/transformer-encoders)用于BERT或[仅解码器架构](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers)用于大多数语言模型），T5选择避免这些架构。有趣的是，[1]中的作者发现编码器-解码器架构在生成和分类任务中都取得了令人印象深刻的结果。[1]中没有考虑仅编码器模型，因为它们专门用于标记/跨度预测，不能很好地解决生成任务。'
- en: '![](../Images/c6a34aec0e8b8a501f645c36308a5e95.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6a34aec0e8b8a501f645c36308a5e95.png)'
- en: (from [1])
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Compared to encoder-decoder architectures, decoder-only models are limited because
    they solely use [causal (or masked) self-attention](https://cameronrwolfe.substack.com/i/108182616/different-transformer-architectures);
    see above. Masked self-attention only considers preceding tokens when computing
    the representation for any given token in a sequence. However, there are certain
    cases in which we would like to perform fully-visible attention over an initial
    span or prefix of text, then generate output based on this prefix (e.g., translation
    tasks). Decoder-only models cannot handle such cases, as they perform causal self-attention
    across the entire input.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与编码器-解码器架构相比，仅解码器模型受到限制，因为它们仅使用[因果（或掩码）自注意力](https://cameronrwolfe.substack.com/i/108182616/different-transformer-architectures)；见上文。掩码自注意力在计算序列中任何给定标记的表示时只考虑前面的标记。然而，有些情况下我们希望对初始范围或文本前缀进行完全可见的注意力，然后基于这个前缀生成输出（例如，翻译任务）。仅解码器模型无法处理这些情况，因为它们对整个输入进行因果自注意力。
- en: '**training T5.** The T5 model is pre-trained on a total of 34B tokens from
    the [C4 corpus](https://cameronrwolfe.substack.com/i/108182616/how-is-t-studied).
    For comparison, BERT is trained over 137B tokens, while RoBERTa is trained over
    2.2T tokens [5, 12]. Inspired by the MLM objective from BERT, T5 is pre-trained
    using a slightly modified denoising objective that:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练T5**。T5模型在[C4语料库](https://cameronrwolfe.substack.com/i/108182616/how-is-t-studied)上进行了34B标记的预训练。作为对比，BERT在137B标记上进行训练，而RoBERTa在2.2T标记上进行训练[5,
    12]。受BERT中的MLM目标启发，T5使用略微修改的去噪目标进行预训练：'
- en: Randomly selects 15% of tokens in the input sequence
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择输入序列中的15%标记
- en: Replaces all consecutive spans of selected tokens with a single
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用单一标记替换所有连续选择的标记范围
- en: ”sentinel” token
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “哨兵”标记
- en: Gives each sentinel token an ID that is unique to the current input sequence
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给每个哨兵标记一个在当前输入序列中唯一的ID
- en: Constructs a target using all selected tokens, separated by the sentinel tokens
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有选择的标记构造目标，用哨兵标记分隔
- en: Although this task seems a bit complex, we can see an illustration of how it
    works on a short input sequence below.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个任务看起来有点复杂，但我们可以在下面看到它在短输入序列上的工作示例。
- en: '![](../Images/fc6d45fea45d7059ee02c3f765550acc.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc6d45fea45d7059ee02c3f765550acc.png)'
- en: (from [1])
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: By replacing entire spans of masked tokens with a single sentinel token, we
    reduce the computational cost of pre-training, as we tend to operate over shorter
    input and target sequences.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过用单一的哨兵标记替换整个掩码标记的范围，我们降低了预训练的计算成本，因为我们通常在较短的输入和目标序列上进行操作。
- en: '**fine-tuning.** After pre-training has been performed, T5 is separately fine-tuned
    on each downstream task prior to being evaluated. Due to the text-to-text format
    used by T5, both pre-training and fine-tuning use the same [maximum likelihood
    objective](https://cameronrwolfe.substack.com/i/85568430/language-modeling)! In
    other words, we just formulate the correct answer as a textual sequence (during
    both pre-training and fine-tuning) and train the model to output the correct textual
    sequence.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**微调。** 在完成预训练后，T5 会在每个下游任务上单独进行微调，然后再进行评估。由于 T5 使用文本到文本的格式，预训练和微调都使用相同的 [最大似然目标](https://cameronrwolfe.substack.com/i/85568430/language-modeling)！换句话说，我们只需将正确答案表述为文本序列（在预训练和微调过程中），然后训练模型输出正确的文本序列。'
- en: '**how does the baseline perform?** As shown in the table below, the baseline
    T5 model performs similarly to prior models like BERT, even though these models
    are not directly comparable (i.e., the baseline T5 model uses 25% of the compute
    used by BERTBase). Plus, we see that pre-training provides a huge benefit on most
    tasks. The exception to this rule is translation tasks, where performance is similar
    both with and without pre-training.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**基线表现如何？** 如下表所示，基线 T5 模型的表现与之前的模型（如 BERT）相似，尽管这些模型并不可直接比较（即，基线 T5 模型使用的计算量是
    BERTBase 的 25%）。此外，我们看到预训练在大多数任务上提供了巨大的好处。这个规则的例外是翻译任务，在这些任务中，预训练与未预训练的表现相似。'
- en: '![](../Images/bd2737cdb0b6a47d562d90be471046cc.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd2737cdb0b6a47d562d90be471046cc.png)'
- en: (from [1])
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Searching for a better approach…
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找更好的方法…
- en: After testing the baseline architecture and training approach, authors in [1]
    modify one aspect of this approach at a time, such as the underlying architecture,
    pre-training objective, or fine-tuning strategy. By testing these different transfer
    learning variants, we can find an approach that consistently works best across
    different language understanding tasks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试基线架构和训练方法后，[1] 的作者逐次修改这种方法的一个方面，例如底层架构、预训练目标或微调策略。通过测试这些不同的迁移学习变体，我们可以找到在不同语言理解任务中
    consistently 表现最佳的方法。
- en: '![](../Images/4501632abb16a5694ee835b61fef66df.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4501632abb16a5694ee835b61fef66df.png)'
- en: (from [1])
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**the architecture.** To study the impact of architecture choice on transfer
    learning results, we can test different [variants of the transformer architecture](https://cameronrwolfe.substack.com/i/108182616/different-transformer-architectures).
    The architectures tested in [1] include the normal encoder-decoder architecture,
    the decoder-only architecture, and a prefix language model, which performs fully-visible
    attention over a fixed prefix within a sequence then generates output using causal
    self-attention; see above. The main difference between these architectures is
    the type of masking used within their self-attention mechanisms.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构。** 为了研究架构选择对迁移学习结果的影响，我们可以测试不同的 [变体变换器架构](https://cameronrwolfe.substack.com/i/108182616/different-transformer-architectures)。在
    [1] 中测试的架构包括普通的编码器-解码器架构、仅解码器架构以及一个前缀语言模型，它在序列中对固定前缀执行完全可见的注意力，然后使用因果自注意力生成输出；见上文。这些架构之间的主要区别在于它们的自注意力机制中使用的掩蔽类型。'
- en: '![](../Images/3374e6d2feb1f559969604c21e1685c6.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3374e6d2feb1f559969604c21e1685c6.png)'
- en: (from [1])
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: When several different architectures are tested (using both causal language
    modeling and denoising objectives for pre-training), we see that the encoder-decoder
    transformer architecture (with a denoising objective) performs the best, leading
    this architecture to be used in the remainder of experiments. Relative to other
    models, this encoder-decoder variant has 2P parameters in total but the same computational
    cost as a decoder-only model with P parameters. To reduce the total number of
    parameters to P, we can share parameters between the encoder and decoder, which
    is found to perform quite well.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当测试几种不同的架构（使用因果语言建模和去噪目标进行预训练）时，我们发现编码器-解码器变换器架构（具有去噪目标）表现最佳，因此在剩下的实验中采用了这种架构。相对于其他模型，这种编码器-解码器变体总共有
    2P 个参数，但计算成本与具有 P 个参数的仅解码器模型相同。为了将总参数数量减少到 P，我们可以在编码器和解码器之间共享参数，发现这种方法表现非常好。
- en: '**the pre-training objective.** At first, T5 is trained using three different
    types of pre-training objectives. The first is a BERT-style MLM objective. The
    other objectives are a deshuffling [3] strategy (i.e., the model tries to put
    a shuffled sentence back into the correct order) and a prefix-based language modeling
    objective [2]. In the latter, the text is separated into two spans, where the
    first span is passed as input to the encoder and the second span is predicted
    by the decoder (i.e., recall that we are using an encoder-decoder transformer).
    The performance of models trained with these objectives is compared below, where
    we see that denoising objectives clearly outperform other strategies.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**预训练目标。** 最初，T5使用三种不同类型的预训练目标进行训练。第一种是BERT风格的MLM目标。其他目标是一个去洗牌策略（即模型尝试将打乱的句子恢复到正确的顺序）和一个基于前缀的语言建模目标。在后者中，文本被分成两个跨度，第一个跨度作为输入传递给编码器，第二个跨度由解码器预测（即回忆一下我们使用的是编码器-解码器转换器）。下文比较了这些目标训练的模型的表现，我们可以看到去噪目标明显优于其他策略。'
- en: '![](../Images/66118100ba35a924d0d85038d805a7a4.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66118100ba35a924d0d85038d805a7a4.png)'
- en: (from [1])
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: From here, authors in [1] test several modifications to the BERT-style MLM objective
    [4], as shown in the table below.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，[1]中的作者测试了对BERT风格MLM目标的几种修改，如下表所示。
- en: '![](../Images/37c7525e991bc22e87aec755725e910b.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37c7525e991bc22e87aec755725e910b.png)'
- en: (from [1])
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: Each of these variants tend to perform similarly; see below. However, by selecting
    pre-training objectives that replace entire spans of corrupted tokens with single
    sentinel tokens and only attempting to predict corrupted tokens within the target,
    we can minimize the computational cost of pre-training. As such, the baseline
    strategy of masking entire spans of consecutive tokens is efficient because it
    produces shorter target sequences.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变体的表现趋于相似；见下文。然而，通过选择替换整个损坏令牌跨度为单个哨兵令牌的预训练目标，并且仅尝试预测目标中的损坏令牌，我们可以最小化预训练的计算成本。因此，遮蔽整个连续令牌跨度的基线策略是高效的，因为它产生了更短的目标序列。
- en: '![](../Images/2e84b4d88b27544df2239fd4afbf1c40.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e84b4d88b27544df2239fd4afbf1c40.png)'
- en: (from [1])
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: Authors in [1] test different corruption rates, finding that the corruption
    rate doesn’t significantly impact results and that a setting of 15% works well.
    An alternative pre-training objective that explicitly selects spans of tokens
    for corruption (i.e., the baseline approach selects tokens uniformly instead of
    as a span, then combines consecutive tokens together) is also found to perform
    similarly to the baseline. A schematic of the different pre-training objectives
    tested in [1] is provided below.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 的作者测试了不同的损坏率，发现损坏率对结果没有显著影响，并且15%的设置效果良好。还发现一种替代的预训练目标，即明确选择令牌跨度进行损坏（即基线方法选择令牌时采用均匀分布而不是跨度，然后将连续的令牌组合在一起），其表现与基线方法相似。下图展示了[1]中测试的不同预训练目标的示意图。'
- en: '![](../Images/2d0823916554a44745ff5a6c11a8f1cd.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d0823916554a44745ff5a6c11a8f1cd.png)'
- en: (from [1])
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: Many different strategies are studied, but the main takeaways here are *(i)*
    denoising objectives work best, *(ii)* variants of denoising objectives perform
    similarly, and *(iii)* strategies that minimize the length of the target are most
    computationally efficient.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 研究了许多不同的策略，但这里的主要结论是 *(i)* 去噪目标效果最好，*(ii)* 去噪目标的变体表现相似，以及 *(iii)* 最小化目标长度的策略在计算上最为高效。
- en: '**data and model size.** Finally, the impact of scale on T5 quality is studied.
    First, T5 is pre-trained with several different datasets, including one that is
    not filtered, a news-specific dataset, a dataset that mimics [GPT-2’s WebText
    corpus](https://cameronrwolfe.substack.com/i/85568430/language-models-are-unsupervised-multitask-learners-gpt),
    and a few variants of the Wikipedia corpus. T5’s performance after being pre-trained
    on each of these datasets is shown below.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据和模型规模。** 最后，研究了规模对T5质量的影响。首先，T5使用几个不同的数据集进行预训练，包括一个未过滤的数据集，一个新闻特定数据集，一个模仿
    [GPT-2 的 WebText 语料库](https://cameronrwolfe.substack.com/i/85568430/language-models-are-unsupervised-multitask-learners-gpt)的数据集，以及几个维基百科语料库的变体。T5在每个数据集上进行预训练后的表现如下面所示。'
- en: '![](../Images/6c5ee2dd55d190a687d0503720236cbb.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c5ee2dd55d190a687d0503720236cbb.png)'
- en: (from [1])
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: We see here that *(i)* not filtering the pre-training corpus is incredibly detrimental
    and *(ii)* pre-training on domain-specific corpora can be helpful in some cases.
    For example, pre-training on the news-based corpus yields the best performance
    on [ReCoRD](https://sheng-z.github.io/ReCoRD-explorer/), a reading comprehension
    dataset based on news articles.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到 *(i)* 不过滤预训练语料库是极其有害的，*(ii)* 在特定领域语料库上进行预训练在某些情况下是有帮助的。例如，在基于新闻的语料库上进行预训练在
    [ReCoRD](https://sheng-z.github.io/ReCoRD-explorer/) 这个基于新闻文章的阅读理解数据集上表现最佳。
- en: “The main lesson behind these findings is that pre-training on in-domain unlabeled
    data can improve performance on downstream tasks. This is unsurprising but also
    unsatisfying if our goal is to pre-train a model that can rapidly adapt to language
    tasks from arbitrary domains.” *— from [1]*
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “这些发现背后的主要教训是，在领域内未标记的数据上进行预训练可以提高下游任务的性能。这并不令人意外，但如果我们的目标是预训练一个能够快速适应来自任意领域的语言任务的模型，这种情况就显得不那么令人满意。”
    *— 来源 [1]*
- en: Going further, T5 is pre-trained using truncated versions of the C4 corpus with
    varying sizes. From these experiments, we learn that more data is (unsurprisingly)
    better. Looping through a smaller version of the dataset multiple times during
    pre-training cause overfitting and damage downstream performance; see below.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步说，T5 使用不同大小的 C4 语料库的截断版本进行预训练。从这些实验中，我们了解到更多的数据（并不令人意外）更好。在预训练期间多次循环通过较小版本的数据集会导致过拟合，并损害下游性能；见下文。
- en: '![](../Images/dbdc4a5171a4c55177aef1a419b10f0a.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbdc4a5171a4c55177aef1a419b10f0a.png)'
- en: (from [1])
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: （来源 [1]）
- en: 'To scale up the T5 model, authors test the following modifications:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展 T5 模型，作者测试了以下修改：
- en: '`4X` more training iterations (or 4`X` larger batch size)'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`4X` 更多训练迭代（或 4`X` 更大的批次大小）'
- en: '`2X` more training iterations and 2`X` larger model'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`2X` 更多训练迭代和 2`X` 更大的模型'
- en: '`4X` larger model'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`4X` 更大的模型'
- en: Train an ensemble of 4 encoder-decoder transformers
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个由 4 个编码器-解码器变换器组成的集合
- en: Here, both pre-training and fine-tuning steps are increased for simplicity.
    The results of these experiments are shown below.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，为了简化，预训练和微调步骤都增加了。这些实验的结果如下所示。
- en: '![](../Images/fd40ae8bbd39863009892cf3d60815b2.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd40ae8bbd39863009892cf3d60815b2.png)'
- en: (from [1])
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: （来源 [1]）
- en: These results roughly correspond with what we would expect. Increasing training
    time (or batch size) improves performance. Combining this with a larger model
    yields a further benefit compared to increasing training iterations or batch size
    alone. In other words, *increasing the amount of pre-training data and the model
    size is complementary in terms of improving performance*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果大致符合我们的预期。增加训练时间（或批次大小）可以提高性能。将此与更大的模型结合起来，相比单独增加训练迭代或批次大小，能带来进一步的好处。换句话说，*增加预训练数据量和模型大小在提升性能方面是互补的*。
- en: “The bitter lesson of machine learning research argues that general methods
    that can leverage additional computation ultimately win out against methods that
    rely on human expertise” *— from [1]*
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “机器学习研究的痛苦教训认为，可以利用额外计算的通用方法最终会胜过依赖人类专业知识的方法” *— 来源 [1]*
- en: '**other stuff.** T5 is also fine-tuned using different multi-task training
    strategies. Overall, these models are found to perform slightly worse than those
    fine-tuned separately for each task. However, strategies do exist to minimize
    the performance gap between task-specific fine-tuning and multi-task learning.
    For more information, check out the overview [here](https://www.ruder.io/multi-task/).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他内容。** T5 也使用不同的多任务训练策略进行了微调。总体而言，这些模型的表现略逊于那些针对每个任务单独微调的模型。然而，确实存在策略来最小化任务特定微调和多任务学习之间的性能差距。有关更多信息，请查看
    [此处](https://www.ruder.io/multi-task/) 的概述。'
- en: Many fine-tuning approaches for deep neural nets only train a subset of model
    parameters (e.g., “freeze” early layers and fine-tune only the last few layers
    in the model). Authors in [1] try several techniques for fine-tuning T5 in this
    manner (e.g., via [adapter layers](https://medium.com/dair-ai/adapters-a-compact-and-extensible-transfer-learning-method-for-nlp-6d18c2399f62)
    or gradual unfreezing [6]), but these methods are outperformed by fine-tuning
    the full model end-to-end; see below.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 许多深度神经网络的微调方法仅训练模型参数的子集（例如，“冻结”早期层并仅微调模型中的最后几层）。[1] 的作者尝试了几种这种微调 T5 的技术（例如，通过
    [适配器层](https://medium.com/dair-ai/adapters-a-compact-and-extensible-transfer-learning-method-for-nlp-6d18c2399f62)
    或逐步解冻 [6]），但这些方法被端到端微调完整模型所超越；见下文。
- en: '![](../Images/1440ddf98e99d73602f1670d655f3113.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1440ddf98e99d73602f1670d655f3113.png)'
- en: (from [1])
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: 'T5: Putting it all together!'
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'T5: 把一切整合在一起！'
- en: Now that we have gone over the entire experimental analysis from [1], we have
    a better picture of different options for transfer learning in NLP and what works
    best! Below, we will go over the main takeaways from this analysis that comprise
    the official transfer learning framework used by T5\. This approach was found
    to perform quite well when compared to various alternatives.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了[1]中的整个实验分析，我们对NLP中的迁移学习不同选项以及什么效果最好有了更清晰的认识！下面，我们将讨论这一分析的主要要点，这些要点构成了T5所使用的官方迁移学习框架。与各种替代方案相比，这种方法被发现表现相当不错。
- en: '**baseline settings.** First, let’s recall T5’s baseline architecture. It is
    an encoder-decoder transformer that is trained using the unified [text-to-text
    format](https://cameronrwolfe.substack.com/i/108182616/text-to-text-framework).
    After pre-training with a denoising objective, the model is separately fine-tuned
    on each downstream task before evaluation. Notably, the final T5 model is fine-tuned
    separately for each task in the GLUE and SuperGLUE benchmarks, as training over
    all tasks together yields slightly lower performance (assuming we take necessary
    steps to avoid overfitting).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**基线设置。** 首先，让我们回顾一下T5的基线架构。它是一个编码器-解码器变换器，通过统一的[文本到文本格式](https://cameronrwolfe.substack.com/i/108182616/text-to-text-framework)进行训练。在进行去噪预训练后，模型会在每个下游任务上单独进行微调，然后再进行评估。值得注意的是，最终的T5模型在GLUE和SuperGLUE基准测试中对每个任务进行了单独的微调，因为对所有任务同时训练的效果稍微差一点（假设我们采取必要的步骤以避免过拟合）。'
- en: '**pre-training.** Instead of uniformly selecting tokens, the final T5 methodology
    performs span corruption (i.e., selecting entire spans of tokens for corruption
    at once) with an average length of three. Still, 15% of tokens are selected for
    corruption. This objective performs slightly better than the baseline and yields
    shorter target sequence lengths. Additionally, T5 mixes unsupervised pre-training
    updates with multi-task, supervised updates. The ratio between the number of unsupervised
    and supervised updates depends on the size of the model being used (i.e., larger
    models need more unsupervised updates to avoid overfitting).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**预训练。** 最终的T5方法并不是均匀选择标记，而是进行跨度破坏（即一次选择整个标记跨度进行破坏），平均跨度长度为三。然而，15%的标记仍然会被选择进行破坏。这个目标略好于基线，并产生更短的目标序列长度。此外，T5将无监督预训练更新与多任务监督更新混合使用。无监督更新与监督更新的比例取决于所使用的模型大小（即，大型模型需要更多无监督更新以避免过拟合）。'
- en: '**amount of training.** Additional pre-training is helpful to the performance
    of T5\. Specifically, both increasing the batch size and number of training iterations
    benefits T5’s performance. As such, the final T5 model is pre-trained over 1T
    tokens in total. This is much larger than the baseline’s 34B tokens during pre-training
    but still far short of RoBERTa [12], which is pre-trained on over 2.2T tokens.
    Pre-training is performed over the generic, filtered C4 dataset, as task-specific
    pre-training does not yield a consistent benefit across different tasks.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练量。** 额外的预训练对T5的性能有帮助。具体来说，增加批量大小和训练迭代次数都能提升T5的性能。因此，最终的T5模型在总共预训练了1T标记。这比基线的34B标记要大得多，但仍远远低于在2.2T标记上进行预训练的RoBERTa[12]。预训练是在通用的过滤C4数据集上进行的，因为特定任务的预训练在不同任务中没有一致的好处。'
- en: '**model scale.** Using larger models is helpful, but sometimes a smaller model
    may make more sense (e.g., when you have limited compute available for inference).
    For this reason, five different sizes of T5 models are released with anywhere
    from 220M to 11B parameters. Thus, T5 is actually a suite of different models!
    We can gain access to any of these models at the link [here](https://huggingface.co/docs/transformers/model_doc/t5).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型规模。** 使用更大的模型是有帮助的，但有时较小的模型可能更合适（例如，当你在推理时计算资源有限）。因此，T5发布了五种不同规模的模型，参数从220M到11B不等。因此，T5实际上是一套不同的模型！我们可以通过链接[这里](https://huggingface.co/docs/transformers/model_doc/t5)访问这些模型。'
- en: Closing Remarks
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. You can also check out my [other
    writings](https://medium.com/@wolfecameron) on medium! If you liked it, please
    follow me on [twitter](https://twitter.com/cwolferesearch) or subscribe to my
    [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/), where
    I help readers build a deeper understanding of topics in AI research via understandable
    overviews of popular papers.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)， [Rebuy](https://www.rebuyengine.com/)
    的AI总监。我研究深度学习的实证和理论基础。你还可以查看我在medium上的 [其他文章](https://medium.com/@wolfecameron)！如果你喜欢这篇文章，请在
    [twitter](https://twitter.com/cwolferesearch) 上关注我，或订阅我的 [Deep (Learning) Focus
    新闻通讯](https://cameronrwolfe.substack.com/)，在这里我通过对流行论文的易懂概述帮助读者建立对AI研究主题的深入理解。
- en: Bibliography
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Raffel, Colin, et al. “Exploring the limits of transfer learning with a
    unified text-to-text transformer.” *The Journal of Machine Learning Research*
    21.1 (2020): 5485–5551.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Raffel, Colin, 等. “探索统一文本到文本转换器的迁移学习极限。” *机器学习研究杂志* 21.1 (2020): 5485–5551。'
- en: '[2] Liu, Peter J., et al. “Generating wikipedia by summarizing long sequences.”
    *arXiv preprint arXiv:1801.10198* (2018).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Liu, Peter J., 等. “通过总结长序列生成维基百科。” *arXiv 预印本 arXiv:1801.10198* (2018)。'
- en: '[3] Liu, Peter J., Yu-An Chung, and Jie Ren. “Summae: Zero-shot abstractive
    text summarization using length-agnostic auto-encoders.” *arXiv preprint arXiv:1910.00998*
    (2019).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Liu, Peter J., Yu-An Chung 和 Jie Ren. “Summae: 使用长度无关的自编码器进行零样本抽象文本总结。”
    *arXiv 预印本 arXiv:1910.00998* (2019)。'
- en: '[4] Song, Kaitao, et al. “Mass: Masked sequence to sequence pre-training for
    language generation.” *arXiv preprint arXiv:1905.02450* (2019).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Song, Kaitao, 等. “Mass: 用于语言生成的掩码序列到序列预训练。” *arXiv 预印本 arXiv:1905.02450*
    (2019)。'
- en: '[5] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Devlin, Jacob, 等. “Bert: 用于语言理解的深度双向转换器预训练。” *arXiv 预印本 arXiv:1810.04805*
    (2018)。'
- en: '[6] Howard, Jeremy, and Sebastian Ruder. “Universal language model fine-tuning
    for text classification.” *arXiv preprint arXiv:1801.06146* (2018).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Howard, Jeremy 和 Sebastian Ruder. “通用语言模型微调用于文本分类。” *arXiv 预印本 arXiv:1801.06146*
    (2018)。'
- en: '[7] Wang, Alex, et al. “GLUE: A multi-task benchmark and analysis platform
    for natural language understanding.” *arXiv preprint arXiv:1804.07461* (2018).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Wang, Alex, 等. “GLUE: 一个多任务基准和自然语言理解分析平台。” *arXiv 预印本 arXiv:1804.07461*
    (2018)。'
- en: '[8] Wang, Alex, et al. “Superglue: A stickier benchmark for general-purpose
    language understanding systems.” *Advances in neural information processing systems*
    32 (2019).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Wang, Alex, 等. “Superglue: 一个更具粘性的通用语言理解系统基准。” *神经信息处理系统进展* 32 (2019)。'
- en: '[9] Hermann, Karl Moritz, et al. “Teaching machines to read and comprehend.”
    *Advances in neural information processing systems* 28 (2015).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Hermann, Karl Moritz, 等. “教机器阅读和理解。” *神经信息处理系统进展* 28 (2015)。'
- en: '[10] Rajpurkar, Pranav, et al. “Squad: 100,000+ questions for machine comprehension
    of text.” *arXiv preprint arXiv:1606.05250* (2016).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Rajpurkar, Pranav, 等. “Squad: 100,000+ 个用于机器文本理解的问题。” *arXiv 预印本 arXiv:1606.05250*
    (2016)。'
- en: '[11] Vaswani, Ashish, et al. “Attention is all you need.” *Advances in neural
    information processing systems* 30 (2017).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Vaswani, Ashish, 等. “注意力机制就是你所需要的一切。” *神经信息处理系统进展* 30 (2017)。'
- en: '[12] Liu, Yinhan, et al. “Roberta: A robustly optimized bert pretraining approach.”
    *arXiv preprint arXiv:1907.11692* (2019).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Liu, Yinhan, 等. “Roberta: 一种稳健优化的BERT预训练方法。” *arXiv 预印本 arXiv:1907.11692*
    (2019)。'
