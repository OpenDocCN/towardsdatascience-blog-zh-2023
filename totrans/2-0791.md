# 使用瓶颈适配器进行高效模型微调

> 原文：[https://towardsdatascience.com/efficient-model-fine-tuning-with-bottleneck-adapter-5162fcec3909](https://towardsdatascience.com/efficient-model-fine-tuning-with-bottleneck-adapter-5162fcec3909)

## 如何使用瓶颈适配器微调基于Transformer的模型

[](https://medium.com/@marcellusruben?source=post_page-----5162fcec3909--------------------------------)[![Ruben Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----5162fcec3909--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5162fcec3909--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5162fcec3909--------------------------------) [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----5162fcec3909--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5162fcec3909--------------------------------) ·阅读时间 14 分钟·2023年11月22日

--

![](../Images/b79802cb5528fbfe5bb1a2f50166d41b.png)

照片来源：Karolina Grabowska: [https://www.pexels.com/photo/set-of-modern-port-adapters-on-black-surface-4219861/](https://www.pexels.com/photo/set-of-modern-port-adapters-on-black-surface-4219861/)

微调是我们为了在特定任务中从深度学习模型中获得更好性能时可以做的最常见的事情之一。我们需要微调模型的时间通常与模型的大小成正比：模型越大，微调所需的时间就越长。

我们可以达成一致的是，如今，基于Transformer的深度学习模型正变得越来越复杂。总体来说，这是一个值得关注的好现象，但它有一个警告：它们往往拥有庞大的参数量。因此，微调大型模型变得越来越难以管理，我们需要一种更高效的方法来进行微调。

在本文中，我们将讨论一种称为瓶颈适配器的高效微调方法。虽然您可以将这种方法应用于任何深度学习模型，但我们将重点关注其在基于Transformer的模型中的应用。

本文的结构如下：首先，我们将对一个特定数据集进行正常的BERT模型微调。然后，我们将借助`adapter-transformers`库将一些瓶颈适配器插入到BERT模型中，以查看它们如何帮助我们使微调过程更高效。

在我们微调模型之前，让我们先介绍一下我们将使用的数据集。

# 关于数据集

我们即将使用的数据集包含[**从Reddit收集的与心理健康相关的不同类型文本**](https://huggingface.co/datasets/mrjunos/depression-reddit-cleaned)（许可协议为CC-BY-4.0）。该数据集适用于文本分类任务，我们可以预测给定文本是否包含抑郁情绪。让我们看一下它的样本。

[PRE0]

如你所见，数据集非常简单，因为我们只有两个字段：一个是文本，另一个是标签。标签本身只有两个可能的值：如果文本包含抑郁情绪则为1，否则为0。我们的任务是微调一个预训练的BERT模型，以预测每个文本的情感。

总共有大约7731个文本，我们将使用其中6500个进行训练，其余1231个用于微调过程中的验证。

让我们创建一个数据加载器，在微调过程中批量加载数据集，我们将在下一节中看到：

[PRE1]

现在我们有了数据，可以开始讨论本文的主要话题。然而，如果我们已经熟悉普通微调的标准过程，那么理解瓶颈适配器的概念会更容易。

因此，在下一节中，我们将从普通微调过程的概念开始，然后扩展到瓶颈适配器的应用。

我们将使用`adapter-transformers`库来进行普通微调和基于适配器的微调。这个库是著名的HuggingFace的`transformers`库的直接分支，这意味着它包含了`transformers`的所有功能，并增加了几个模型类和方法，以便我们可以轻松地将适配器应用到模型中。

你可以使用以下命令安装`adapter-transformers`：

[PRE2]

现在让我们开始讨论普通微调的常规过程。

# 普通BERT微调

微调是深度学习中的一种常见技术，旨在从预训练模型中获得在特定数据和/或任务上的更好性能。其主要思想很简单：我们获取预训练模型的权重，然后基于新的领域特定数据更新这些权重。

![](../Images/0e2d0faa76ec52acef09f4a5e5226d0c.png)

普通微调过程。图片由作者提供。

普通微调的常规过程如下。

首先，我们选择一个预训练模型，在我们的情况下是BERT-base模型。顺便提一下，我们在这篇文章中不会专注于BERT，但如果你对BERT不熟悉并想了解更多，可以查看我关于BERT的文章：

[](/text-classification-with-bert-in-pytorch-887965e5820f?source=post_page-----5162fcec3909--------------------------------) [## 使用BERT进行PyTorch中的文本分类

### 如何利用Hugging Face的预训练BERT模型来分类新闻文章的文本

[towardsdatascience.com](/text-classification-with-bert-in-pytorch-887965e5820f?source=post_page-----5162fcec3909--------------------------------)

简而言之，BERT-base 包含 12 层 Transformer 编码器。在微调过程中，我们需要在最后一层上添加一个线性层，作为分类器。由于我们数据集中的标签仅包含两个可能的值，因此我们的线性层的输出也将是两个。

[PRE3]

![](../Images/87b1418a06ca1585d96c7a2b9555b198.png)

BERT 架构。图片由作者提供。

现在我们已经定义了我们的模型，我们需要创建微调脚本。以下是对模型进行微调的代码片段。

[PRE4]

我们将对我们的 BERT 模型进行大约 10 个周期的微调，学习率设置为 10e-7。 我在 T4 GPU 上使用批量大小为 2 进行了模型微调。以下是训练和验证准确度的快照。

[PRE5]

就这样！我们在数据集上使用 BERT 达到了 97.3% 的验证准确率。然后我们可以继续使用微调后的模型对未见数据进行预测。

总体来说，如果我们的模型具有“小”数量的参数，正常的微调不会成为问题，如上所示的 BERT 模型。让我们检查一下我们的 BERT-base 模型的总参数数量。

[PRE6]

这个模型总共有接近 1.1 亿个参数。虽然看起来很多，但与现在的大多数大型语言模型相比，这仍然不算什么，因为它们可能有数十亿个参数。如果你也注意到，可训练参数的数量与我们 BERT 模型的总参数数量相同。这意味着在正常的微调过程中，我们会更新 BERT 模型的所有参数的权重。

借助 T4 GPU 和我们的训练数据集仅包含 6500 条数据，我们幸运地只需大约 12 分钟每个周期来更新所有权重。现在想象一下，如果我们使用更大的模型和数据集，进行正常微调的计算时间将会非常昂贵。

此外，正常的微调通常与所谓的灾难性遗忘风险相关，如果我们在选择学习率时不小心，或者当我们尝试在多个任务/数据集上微调预训练模型时。灾难性遗忘指的是当我们在新任务上微调预训练模型时，它会“遗忘”其训练过的任务。

因此，我们确实需要一种更高效的程序来进行微调过程。这就是我们可以使用不同类型的高效微调方法的地方，其中瓶颈适配器就是其中之一。

# **瓶颈适配器**的工作原理

适配器的主要思想是，我们引入一小部分层，并将其放置在预训练模型的原始架构中。在微调过程中，我们冻结预训练模型的所有参数，因此，只有这些附加子集层的权重会被更新。

瓶颈适配器特指一种由两个普通前馈层组成的适配器，前后可选地添加归一化层。一个前馈层的功能是缩小输出，而另一个是放大输出。这就是为什么它被称为瓶颈适配器的原因。

![](../Images/1fd52a6f95d490e9e5b4240441622681.png)

常见的瓶颈适配器。图片由作者提供。

你可以将这个适配器应用于任何深度学习模型，但如前所述，我们将重点关注其在基于Transformer的模型上的应用。

基于Transformer的模型通常由多个Transformer层堆叠组成。例如，本文使用的基于BERT的模型有12个Transformer编码器层堆叠。每个堆叠包括以下组件：

![](../Images/c91ba9179360f97f8dd2a6368efdcab5.png)

Transformer编码器堆叠。图片由作者提供。

我们可以将瓶颈适配器放入这个堆叠的几种不同方式。然而，有两种常见的配置：一种是Pfeiffer提出的，另一种是Houlsby提出的。

Pfeiffer提出的瓶颈适配器插入在最后的规范层之后，而Houlsby提出的瓶颈适配器插入在两个不同的位置：一个在多头注意力层之后，另一个在前馈层之后，如下图所示：

![](../Images/a783afeb3413b95c3f3f9c8cfc4dc457.png)

Pfeiffer和Houlsby适配器配置的区别。图片由作者提供。

由于我们的BERT-base模型有12个Transformer编码器层堆叠，因此如果使用Pfeiffer配置，我们将有12个瓶颈适配器：每个堆叠一个适配器。同时，如果使用Houlsby配置，我们将有24个瓶颈适配器：每个堆叠两个适配器。

尽管Pfeiffer配置相比于Houlsby配置参数更少，但在8个不同任务中的表现相当。

现在的问题是：这个瓶颈适配器是如何让微调过程更高效的？

如前所述，我们在微调过程中冻结预训练模型的权重，只更新适配器的权重。这意味着我们可以显著加快微调过程，接下来的部分会展示这一点。实验也表明，使用适配器进行微调的性能通常与普通微调相当。

同时，假设我们想用相同的预训练模型处理两个不同的数据集。我们可以使用一个模型，通过两个不同的适配器在不同的数据集上进行微调，从而避免灾难性遗忘的风险，而不是拥有两个在不同数据集上微调的模型。

![](../Images/62629350bae3af2e6b01722fad762b61.png)

图片由作者提供。

使用这种方法，我们节省了大量存储空间。例如，一个单独的BERT-base模型的大小是440 MB，如果我们有两个模型，则为880 MB。与此同时，如果我们有一个带有两个适配器的BERT-base模型，大小大约为450 MB，因为瓶颈适配器只占用少量内存。

# 瓶颈适配器实现

在本节中，我们将实现Pfeiffer版本的瓶颈适配器。为此，我们只需要更改模型架构的脚本，而与微调过程和数据加载相关的脚本保持不变。

让我们使用Pfeiffer的适配器定义模型架构。

[PRE7]

如你所见，实现适配器版本的模型非常简单：

+   使用`AdapterConfig.load('pfeiffer')`定义我们想要应用的适配器配置。如果你想使用Houlsby配置，只需将其更改为`'houlsby'`。

+   使用`add_adapter()`方法将适配器插入到我们的BERT模型中。常见做法是根据任务或数据集为适配器命名，以便我们希望模型进行微调。

+   使用`train_adapter()`方法冻结预训练模型的所有权重。

+   使用`add_classification_head()`方法在BERT模型上添加一个线性层，作为预测头。常见做法是为预测头取与适配器相同的名称。

+   激活我们的适配器和预测头，以确保它们在每次前向传递中都被使用，使用`set_active_adapters()`方法。

现在，让我们检查在添加适配器后参数的总数和可训练参数的比例：

[PRE8]

带有适配器的模型参数比我们原始的BERT-base模型多，但只有1.35%是可训练的，因为我们只会更新适配器的权重。

现在是训练模型的时候了。由于适配器的权重是随机初始化的，因此这次我们将使用略高的学习率。我们还将训练该模型10个时期。如果一切顺利，你将获得类似如下的输出：

[PRE9]

如你所见，带有适配器的模型性能与完全微调版本的模型相当。而且，完成一个时期所需的时间比完全微调快4.5分钟。

既然我们已经训练好了它，我们可以保存适配器。

[PRE10]

然后我们可以加载适配器并进行推理，如下所示：

[PRE11]

# 结论

在本文中，我们已经看到瓶颈适配器在大型模型的微调过程中是如何有帮助的。使用瓶颈适配器，我们能够加快微调速度，同时保持模型的最终性能。这些适配器也有助于避免通常与微调模型相关的灾难性遗忘风险。此外，这些适配器不会占用大量内存空间。

我希望这篇文章对你在使用瓶颈适配器时有所帮助。如果你想查看本文中实现的所有代码，可以通过[**这个笔记本**](https://github.com/marcellusruben/medium-resources/blob/main/Bottleneck_Adapters/Bottleneck_Adapters_Medium.ipynb)访问。

# 数据集参考

+   [https://huggingface.co/datasets/mrjunos/depression-reddit-cleaned](https://huggingface.co/datasets/mrjunos/depression-reddit-cleaned)
