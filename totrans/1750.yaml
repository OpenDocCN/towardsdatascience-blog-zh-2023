- en: PyTorch Image Classification Tutorial for Beginners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pytorch-image-classification-tutorial-for-beginners-94ea13f56f2](https://towardsdatascience.com/pytorch-image-classification-tutorial-for-beginners-94ea13f56f2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fine-tuning pre-trained Deep Learning models in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie?source=post_page-----94ea13f56f2--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----94ea13f56f2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----94ea13f56f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----94ea13f56f2--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----94ea13f56f2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----94ea13f56f2--------------------------------)
    ·22 min read·May 9, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f968859c4ae9f104408eddb0177d11ab.png)'
  prefs: []
  type: TYPE_IMG
- en: “Not sure if this is supposed to be a lion or a cheetah…”
  prefs: []
  type: TYPE_NORMAL
- en: This practical tutorial shows you how to classify images using a pre-trained
    Deep Learning model with the PyTorch framework.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between this beginner-friendly image classification tutorial
    to others is that we are not building and training the Deep neural network from
    scratch. In practice, only a few people train neural networks from scratch. Instead,
    most Deep Learning practitioners use a pre-trained model and fine-tune it to a
    new task.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, only a few people train neural networks from scratch.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The specific problem setting is to build a binary image classification model
    to classify images of cheetahs and lions based on a small dataset. For this purpose,
    we will fine-tune a pre-trained image classification model using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/70915a988ae6fb23d0bca9760fa94edc.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample images from the dataset [1].
  prefs: []
  type: TYPE_NORMAL
- en: 'This tutorial follows a basic Machine Learning workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Prepare and explore data](#758f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Build a baseline](#9377)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Run experiments](#a980)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Make predictions](#52fc)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can follow along in [my related Kaggle Notebook](https://www.kaggle.com/iamleonie/pytorch-image-classification-tutorial-for-beginner).
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites and Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ideally, you should have some familiarity with Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'As this is a practical tutorial, we will only cover how to build an image classification
    model at a high level. We will not cover a lot of theory, such as how convolutional
    layers or backpropagation work. I will mark sections where you can dig deeper
    once you feel comfortable with this topic with this sign: ⚒️'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to supplement this guide with some theoretical background information,
    I recommend the free Kaggle Learn courses on [Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning)
    and [Computer Vision](https://www.kaggle.com/learn/computer-vision).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by importing PyTorch and other relevant libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The essential libraries are PyTorch (version 1.13.0) for deep learning, OpenCV
    (version 4.5.4) for image processing, and Albumentations (version 1.3.0) for data
    augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Prepare and Explore Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step is to become familiar with the data. For this tutorial, we will
    keep the exploratory data analysis step short.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will load the data. The example dataset [1] has two folders with images
    — one folder for each class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9d20055393d1e5aa66c4f43970b2317.png)'
  prefs: []
  type: TYPE_IMG
- en: Example dataset [1] for binary image classification.
  prefs: []
  type: TYPE_NORMAL
- en: The following code goes through all subfolders and creates a Pandas dataframe
    containing the file name and its label.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Insert your data here!* — To follow along in this article, your dataset should
    look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/034de54983eaa8ef3e481109edfeb194.png)'
  prefs: []
  type: TYPE_IMG
- en: Example dataset [1] for binary image classification. Insert your data here.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have about 170 photographs: roughly 85 lions and 85 cheetahs (see remark
    in [1]). This is a very small but balanced dataset. It’s perfect for fine-tuning!'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3cc4c584f515ba83009a692a57ec0241.png)'
  prefs: []
  type: TYPE_IMG
- en: Class distribution of sample dataset for image classification plotted with seaborn
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a feeling for the dataset, it is always a good idea to plot a few samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/70915a988ae6fb23d0bca9760fa94edc.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample images from the dataset [1].
  prefs: []
  type: TYPE_NORMAL
- en: By exploring a dataset like this, you can gain some insights. E.g. as you can
    see here, the images are not limited to the animals but also to statues.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go any further, let’s split the dataset into training and testing
    data. The training data will be used to build our model, and the test data will
    be a hold-out dataset to evaluate the final model’s performance on unseen data.
    In this example, we will set 10% of the data aside for testing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/47d794f1876c7aa60f46a35c06c9603b.png)'
  prefs: []
  type: TYPE_IMG
- en: Splitting the data into training and testing datasets (Inspired by [scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html))
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Build a Baseline'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will build a baseline. A baseline consists of three key components:'
  prefs: []
  type: TYPE_NORMAL
- en: A [data pipeline for loading images](#d7e6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [model](#e194) with [loss function and optimizer](#9994)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [training pipeline](#8867), including a [cross-validation strategy](#bfd5)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will go through each component and finally [wrap it up](#8d70)
    nicely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because training a Deep Learning model includes a lot of experimentation, we
    want to be able to switch out specific parts of the code quickly. Thus, we will
    try to make the following code as modular as possible and work with a configuration
    for tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We will add the configurable parameters as we go along.
  prefs: []
  type: TYPE_NORMAL
- en: Build a data pipeline for loading images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, you must build a pipeline to load, preprocess and feed your images to
    the neural network in batches (instead of all at once). PyTorch provides two core
    classes you can use for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Dataset` class: Loads and preprocesses the dataset. You will need to customize
    this class for your purpose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dataloader` class: Loads batches of data samples to the neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, you need to customize the `Dataset` class. Its key components are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Constructor: to load the dataset as, e.g., Pandas Dataframe'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__len__()`: to get the length of the dataset. This usually will require minimal
    adjustments related to how you pass in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__getitem__()`: to get a sample from the dataset by index. This is usually
    the part where you modify most of the code depending on any preprocessing you
    want to do.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Below you can find a template to customize the `Dataset` class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: When loading your dataset, you can also perform any required preprocessing,
    such as transforms or image standardization. This happens in `__getitem__()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we first load the image from the root directory (`cfg.root_dir`)
    with OpenCV and convert it to the RGB color space. Then we will apply basic transforms:
    Resize the image (`cfg.image_size`) and convert the image from a NumPy array to
    a tensor. Finally, we will normalize the values of the image to be in the [0,
    1] range by dividing the values by 255.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need a `Dataloader` to feed the samples of the `Dataset` to the neural
    network in batches because we (probably) don’t have enough RAM to feed all the
    images to the model at once.
  prefs: []
  type: TYPE_NORMAL
- en: You need to provide the `Dataloader` the instance of the `Dataset` you want
    to navigate, the size of the batches (`cfg.batch_size`), and the information on
    whether to shuffle the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The batch size should be fixed throughout the training and not be tuned [2].
    Because the training speed is related to the batch size, we want to use the biggest
    batch size possible. Start with a batch size of 32 and then increase it in powers
    of two (64, 128, etc.) until you get a memory error, and then use the last batch
    size.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you iterate over the `Dataloader`, it will give you batches of samples
    from the customized `Dataset`. Let’s retrieve the first batch for a sanity check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `Dataloader` returns the image batch and a label batch. The `image_batch`
    is a tensor of the shape `(32, 3, 256, 256)`. This is a batch of 32 (`batch_size`)
    images with the shape `(3, 256, 256)` (`color_channels, image_height, image_width`).
    The `label_batch` is a tensor of the shape `(32)`. These are the corresponding
    labels to the 32 images.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ddbe050da4919b3d8f12a78c7e466dd0.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output of the Dataloader with customized Dataset
  prefs: []
  type: TYPE_NORMAL
- en: This section explained how to build a data pipeline. In a later section (see
    [Setup a training pipeline](#8867)), we will use the `Dataset` and `Dataloader`
    to create separate pipelines for training, validation, and testing.
  prefs: []
  type: TYPE_NORMAL
- en: Before we train the model, we need to split the training data again into a training
    and a validation dataset. Training the model on a dataset and then evaluating
    the model on the same data is a methodological mistake because the model just
    needs to memorize the labels of the seen samples. Thus, instead of generalizing,
    the model will overfit to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid overfitting, let’s randomly partition the training data into training
    and validation sets with the `train_test_split()` function for now. This section
    will later be replaced with a [cross-validation strategy](#bfd5).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5e5fed82448ece523ca25ca82c849770.png)'
  prefs: []
  type: TYPE_IMG
- en: Splitting the training data again into training and validation (Inspired by
    [scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html))
  prefs: []
  type: TYPE_NORMAL
- en: 'With this split, we can now create `Datasets` and `Dataloaders` for the training
    and validation data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Prepare the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the part where we would learn about building a neural network in PyTorch.
    When I started learning about Deep Learning, I thought building neural networks
    was an important part of training Deep Learning models. But the reality is that
    this is what researchers do for us. We, the practitioners, get to lean back and
    use the final models for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The researchers try different model architectures, such as convolutional neural
    networks (CNNs), and usually train image classification models on large baseline
    datasets, such as ImageNet [3]. We call these models **backbones**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/71e4d33bf3881fa1181e6f58a6a1e940.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Expectation vs. reality: In practice, only a few people train neural networks
    from scratch for image classification'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a pre-trained neural network works so well because the first few
    layers often learn general features (such as edge detection).
  prefs: []
  type: TYPE_NORMAL
- en: '*⚒️ Of course, you should understand how neural networks work in general, including
    backpropagation, and how different layers, such as convolutional layers, work.
    However, to follow along in this practical tutorial, you don’t need to understand
    these details right now. Once you have finished this tutorial, you can fill in
    some theoretical gaps with the free Kaggle Learn courses on* [*Deep Learning*](https://www.kaggle.com/learn/intro-to-deep-learning)
    *and* [*Computer Vision*](https://www.kaggle.com/learn/computer-vision)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fantastic backbones and where to find them** — Now, which of these pre-trained
    models should you choose, and where do you get these from?'
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will use `[timm](https://timm.fast.ai/)` — a Deep Learning
    library containing a collection of state-of-the-art computer vision models created
    by [Ross Wightman](https://twitter.com/wightmanr) — to get pre-trained models.
    (You can use `torchvision.models` for pre-trained models, but I personally find
    it easier to switch out backbones during experimentation with `timm`.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'There is a lot to unpack in this little piece of code. Let’s go step-by-step:'
  prefs: []
  type: TYPE_NORMAL
- en: '`backbone = ''resnet18''`— In this example, we use a ResNet [5] with 18 layers.
    ResNet stands for Residual Network, and it is a type of CNN using so-called residual
    blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: ⚒️*We will skip over the details of ResNet and residual blocks. If you are interested
    in the technical details,* [*you can dig deeper into this post, for example.*](/introduction-to-resnets-c0a830a288a4)
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many different models in the ResNet family, such as ResNet18, ResNet34,
    etc., where the number stands for how many layers the network has. As a (very
    rough) rule of thumb: The higher the number of layers, the better the performance.
    You can print `timm.list_models(''*resnet*'')` to see what other models are available.'
  prefs: []
  type: TYPE_NORMAL
- en: ⚒️ *Learn about different backbones for computer vision/image classification
    like ResNet, DenseNet, and EfficientNet.*
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained = True` — This means we want the weights of the model trained on
    ImageNet [3]. If this is set to `False`, you will only get the model''s architecture
    without the weights [6].'
  prefs: []
  type: TYPE_NORMAL
- en: '`num_classes = cfg.n_classes` — Because the model was pre-trained on ImageNet
    [3], you will get a classifier with the 1000 classes that are in ImageNet. Thus,
    you need to remove the ImageNet classifier and define how many classes you have
    in your problem [6]. If you set `num_classes = 0`, you will get the model without
    a classifier [6].'
  prefs: []
  type: TYPE_NORMAL
- en: To check output size, you can pass in a sample batch `X` with 3 channels of
    random values in the image size [6].
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: It will output `torch.Size([1, cfg.n_classes])` [6].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c2041f5ab5aaf83088a4affb21f9796.png)'
  prefs: []
  type: TYPE_IMG
- en: Model inputs and outputs
  prefs: []
  type: TYPE_NORMAL
- en: Prepare loss function and optimizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, to train a model, there are two key ingredients:'
  prefs: []
  type: TYPE_NORMAL
- en: a loss function (criterion),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an optimization algorithm (optimizer), and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: optionally a learning rate scheduler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function** — Common loss functions are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary cross-entropy (BCE) loss** for binary classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical cross-entropy** loss for multi-class classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean squared loss** for regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although we have a binary classification problem, you can also use categorical
    cross-entropy loss. If you like, you can switch out the loss function with BCE.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Optimizer —** The optimization algorithm minimizes the loss function (in
    our case, the cross-entropy loss). There are many different optimizers available.
    Let’s use a popular one: [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[**Learning rate scheduler**](/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)—
    A learning rate scheduler adapts the value of the learning rate during the training
    process. Although you don’t have to use a learning rate scheduler, using one can
    help the algorithm converge faster. This is because if the learning rate stays
    constant, it can prevent you from finding the optimum if it is too large, and
    it can take too long to converge if it is too small.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many different learning rate schedulers available, but Kaggle Grandmasters
    recommend using **cosine decay as a learning rate scheduler for fine-tuning**
    [2].
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`T_max` defines the half period and should be equal to the maximum number of
    iterations (`np.ceil(len(train_dataloader.dataset) /cfg.batch_size)*cfg.epochs`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting learning rates will look as follows over the course of a training
    run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e89dff534f52a0dae5b40aa8617819a.png)'
  prefs: []
  type: TYPE_IMG
- en: Cosine decay learning rate scheduler
  prefs: []
  type: TYPE_NORMAL
- en: '**Metric** — While we’re at it, let’s also define a metric to evaluate the
    model’s overall performance. Again, there are many different metrics. For this
    example, we will use accuracy as the metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Don’t confuse the metric with the loss function. The loss function is used to
    optimize the learning function during training, while the metric measures the
    model’s performance after the training.
  prefs: []
  type: TYPE_NORMAL
- en: ⚒️ *Learn about different metrics and which ones are suited for which problems.*
  prefs: []
  type: TYPE_NORMAL
- en: Setup a training pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is probably, the most complex but also most interesting part of this tutorial.
    Are you ready?
  prefs: []
  type: TYPE_NORMAL
- en: A model is typically trained in iterations. One iteration is called an epoch.
    Training from scratch usually requires many epochs, while fine-tuning requires
    only a few (roughly 5 to 10) epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each epoch, the model is trained on the full training data and then validated
    on the full validation data. We will now define two functions: One function to
    train (`train_an_epoch()`) and one function to validate the model on an epoch
    (`validate_an_epoch()`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Below you can see the training function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s go through it step-by-step:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the model to the training mode. The model can also be in evaluation mode.
    This mode affects the behavior of the layers `[Dropout](https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html)`
    and `[BatchNorm](https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html)`
    in a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate over the training data in small batches. The samples and labels need
    to be moved to GPU if you use one for faster training (`cfg.device`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clear the last error gradient of the optimizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do a forward pass of the input through the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the loss for the model output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagate the error through the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the model to reduce the loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step the learning rate scheduler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the loss and metric for statistics. Because the predictions will be
    Tensors on the GPU, just like the inputs, we need to [detach the Tensor](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach)
    from the automatic differentiation graph and call the NumPy function to convert
    them to NumPy arrays
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we define the validation function as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s go through it step-by-step again:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the model to the evaluation mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate over the validation data in small batches. The samples and labels need
    to be moved to GPU if you use one for faster training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do a forward pass of the input through the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the loss and metric for statistics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At first glance, training and validating an epoch looks similar. Let’s look
    at a code comparison to make the differences clearer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3cc0433231a4710415b44ccf035e3136.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of side-by-side code comparison in BeyondCompare of training and
    validation code in PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the following differences:'
  prefs: []
  type: TYPE_NORMAL
- en: The model has to be in training or evaluation mode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For training the model, we need an optimizer and an optional scheduler. For
    validation, we only need the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gradient calculation is only active for training. For validation, we don’t
    need it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we are not yet done with the training pipeline. Earlier, we divided the
    training data into training and validation data. But partitioning the available
    data into two fixed sets limits the number of training samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we will use a cross-validation strategy by splitting the training
    data into *k* folds. The model is then trained in *k* separate iterations, in
    which the model is trained on *k*-1 folds and validated on one fold for each iteration
    while the folds switch at every iteration as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3862554f761e899f0916eceefae3063e.png)'
  prefs: []
  type: TYPE_IMG
- en: Splitting the training data again into training and validation (Inspired by
    [scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html))
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we are using `[StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)`
    to create the splits. You could use `KFold` instead but `StratifiedKFold` has
    the advantage that it preserves the class distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Adding data augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the difference between the training and validation metric is significant,
    this indicates that the model is overfitting to the training data. Overfitting
    occurs when a model is trained on only a few examples and learns irrelevant details
    or noise from the training data. This negatively affects the model’s performance
    when it’s presented with new examples. As a result, the model struggles to generalize
    on new images.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome overfitting during the training process, you can use data augmentation.
    Data augmentation generates additional training data by randomly transforming
    existing images. This technique exposes the model to more aspects of the data,
    helping it to generalize better.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use some prepared data augmentations from the `albumentations` package,
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Rotating images (`A.Rotate()`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal flipping (`A.HorizontalFlip()`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cutout [4] (`A.CoarseDropout()`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Earlier, we defined a basic transform to resize and convert the image to a tensor.
    We will continue to use it for the validation and testing datasets because they
    don’t need any augmentations. For the training dataset, we create a new transform
    `transform_soft` , which has the three above augmentations in addition to the
    resizing and conversion to tensor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You can control the percentage of images the augmentations are applied to with
    the parameter `p`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we visualize a few samples from the augmented dataset, we can see that the
    three augmentations are applied successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: Rotation in images 0, 1, 2, 4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal flip is difficult to detect if you don’t know the original image,
    but we can see that image 2 must be horizontally flipped
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cutout (coarse dropout) in images 1 and 4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/0ab00b43ca8d9fdcd569f85b8599abfe.png)'
  prefs: []
  type: TYPE_IMG
- en: Augmented training dataset
  prefs: []
  type: TYPE_NORMAL
- en: ⚒️ *Next, you can review and add other image augmentation techniques, e.g.,
    Mixup and Cutmix, to your pipeline.*
  prefs: []
  type: TYPE_NORMAL
- en: '[](/cutout-mixup-and-cutmix-implementing-modern-image-augmentations-in-pytorch-a9d7db3074ad?source=post_page-----94ea13f56f2--------------------------------)
    [## Cutout, Mixup, and Cutmix: Implementing Modern Image Augmentations in PyTorch'
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation techniques for Computer Vision implemented in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/cutout-mixup-and-cutmix-implementing-modern-image-augmentations-in-pytorch-a9d7db3074ad?source=post_page-----94ea13f56f2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have discussed each component of the baseline from the [data pipeline](#d7e6)
    to the [model](#e194) with [loss function and optimizer](#9994), to the [training
    pipeline](#8867), including a [cross-validation strategy](#bfd5), we can put it
    all together as shown in the image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/787eb3ca571d744451bf7876241a0a59.png)'
  prefs: []
  type: TYPE_IMG
- en: Flowchart of the baseline code
  prefs: []
  type: TYPE_NORMAL
- en: We will iterate over each fold of our [cross-validation strategy](#bfd5). Within
    each fold, we set up a [data pipeline](#d7e6) for the training and validation
    data and a [model](#e194) with [loss function and optimizer](#9994). Then for
    each epoch, we will [train and validate the model](#8867).
  prefs: []
  type: TYPE_NORMAL
- en: Before we touch anything, let’s set ourselves up for success and fix the random
    seeds to ensure **reproducible** results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will write a `fit()` function that fits the model for all epochs. The
    function iterates over the number of epochs, while the training and validation
    functions contain inner loops that iterate over the batches in the training and
    validation datasets, as discussed in the section about the [training pipeline](#8867).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9119aad99942cb843e6f9b296c2cb37a.png)'
  prefs: []
  type: TYPE_IMG
- en: Log of the fit function
  prefs: []
  type: TYPE_NORMAL
- en: 'For visualization purposes, we will also create plots of the loss and accuracy
    on the training and validation sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/811ac6b17726482a35bd9b79bdc5bf83.png)'
  prefs: []
  type: TYPE_IMG
- en: Plotted history of metric and loss over epochs
  prefs: []
  type: TYPE_NORMAL
- en: 'When we combine everything, it will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Run Experiments'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Science is an experimental science. Thus, the aim of this step is to find
    the best configuration of hyperparameters, data augmentations, model backbones,
    and cross-validation strategy that achieve the best performance (or whatever your
    objective may be — e.g., best trade-off between performance and inference time).
  prefs: []
  type: TYPE_NORMAL
- en: Setup experiment tracking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before jumping into this step, take a minute to think about how you will track
    your experiments. [Experiment tracking](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)
    can be as simple as writing everything down with pen and paper. Alternatively,
    you can track everything in a spreadsheet or even use an experiment tracking system
    to automate the whole process.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133?source=post_page-----94ea13f56f2--------------------------------)
    [## Intro to MLOps: Experiment Tracking for Machine Learning'
  prefs: []
  type: TYPE_NORMAL
- en: Why it matters and three different ways you can log and organize your ML experiments
    with pen and paper, spreadsheets…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133?source=post_page-----94ea13f56f2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are an absolute beginner, I recommend starting simple and tracking your
    experiments manually in a spreadsheet at first. Open an empty spreadsheet and
    create columns for all inputs, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: backbone,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: learning rate,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: epochs,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: augmentations, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: image size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and outputs, such as loss and metrics for training and validation, you want
    to track.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting spreadsheet could look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99a24e70879082b1d3cc3351300f1a67.png)'
  prefs: []
  type: TYPE_IMG
- en: Example spreadsheet to track experiments for beginners
  prefs: []
  type: TYPE_NORMAL
- en: ⚒️ *Once you feel comfortable with the Deep Learning techniques, you can level
    up by* [*implementing an experiment tracking system*](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)
    *into your pipeline to automate experiment tracking, such as* [*Weights & Biases*](https://wandb.ai/site)*,*
    [*Neptune*](https://neptune.ai/)*, or* [*MLFlow*](https://mlflow.org/docs/latest/tracking.html)*.*
  prefs: []
  type: TYPE_NORMAL
- en: Experimentation and hyperparameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you have an experiment tracking system let’s run some experiments.
    You can start by tweaking the following hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of training steps: range of 2 to 10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning rate: range of 0.0001 to 0.001'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image size: range of 128 to 1028'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Backbone: Try different backbones. First, try deeper models from the ResNet
    family (print `timm.list_models(''*resnet*'')` to see what other models are available),
    then try a different backbone family like `timm.list_models(''*densenet*'')` or
    `timm.list_models(''*efficientnet*'')`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ⚒️ *Once you feel comfortable with the Deep Learning techniques, you can level
    up by automating this step with* [*Optuna*](https://optuna.org/) *or* [*Weights
    & Biases*](https://wandb.ai/site)*.*
  prefs: []
  type: TYPE_NORMAL
- en: Now it’s your turn! — Tweak a few notches and see how the model's performance
    changes. Once you’re happy with the results, move on to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8096166e016abadf0db4531e0d946ea0.png)'
  prefs: []
  type: TYPE_IMG
- en: Example log of experiments
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Make Predictions (Inference)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Drum roll, please! Now that we have found the configuration that will give us
    the best model, we want to put it to good use.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s fine-tune the model with the optimal configuration on the full
    dataset to take advantage of every data sample. We don’t split the data into training
    and validation data in this step. Instead, we only have one big training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: But the rest of the training pipeline stays the same.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**Inference** — And finally, we will use the model to predict the hold-out
    test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Below you can see the results of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7c480d2b6e80753e441aa9fce32adc9.png)'
  prefs: []
  type: TYPE_IMG
- en: Predictions
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9166b757b2019b4c1d383c0d111beaa.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary and Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This tutorial showed you how to fine-tune a pre-trained image classification
    model for your specific task, evaluate it, and perform inference on unseen data
    using the PyTorch framework in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Once you feel comfortable, you can level up by reviewing the sections marked
    with ⚒️ to level up to an intermediate level.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----94ea13f56f2--------------------------------)
    [## Intermediate Deep Learning with Transfer Learning'
  prefs: []
  type: TYPE_NORMAL
- en: A practical guide for fine-tuning Deep Learning models for computer vision and
    natural language processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----94ea13f56f2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Enjoyed This Story?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----94ea13f56f2--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----94ea13f56f2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [MikołajFish99](https://www.kaggle.com/mikoajfish99) (2023). [Lions or
    Cheetahs — Image Classification](https://www.kaggle.com/datasets/mikoajfish99/lions-or-cheetahs-image-classification/code)
    in Kaggle Datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '**License:** According to the original image source [(Open Images Dataset V6](https://storage.googleapis.com/openimages/web/factsfigures.html))
    the annotations are licensed by Google LLC under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)
    license, and the images are listed as having a [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/)
    license.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note the original dataset contains 200 images, with 100 images of each class.
    But the dataset needed some cleaning, including removing images of other animals;
    thus, the final dataset is slightly smaller. To keep this tutorial short, we will
    skip the data cleaning process here.*'
  prefs: []
  type: TYPE_NORMAL
- en: Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If not otherwise stated, all images are created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Literature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[2] S. Bhutani with H20.ai (2023). [Best Practises for Training ML Models |
    @ChaiTimeDataScience #160](https://www.youtube.com/watch?v=_mzrfMA8Qx4) presented
    on YouTube in January 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009,
    June). Imagenet: A large-scale hierarchical image database. In *2009 IEEE conference
    on computer vision and pattern recognition* (pp. 248–255). Ieee.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] DeVries, T., & Taylor, G. W. (2017). Improved regularization of convolutional
    neural networks with cutout. *arXiv preprint arXiv:1708.04552*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] K. He, X. Zhang, S. Ren, & J. Sun (2016). Deep residual learning for image
    recognition. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition* (pp. 770–778).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] timmdocs (2022). [Pytorch Image Models (timm)](https://timm.fast.ai/) (accessed
    April 10th, 2023).'
  prefs: []
  type: TYPE_NORMAL
