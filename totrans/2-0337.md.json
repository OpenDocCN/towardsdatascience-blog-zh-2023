["```py\n# Deep Learning framework\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n# Audio processing\nimport torchaudio\nimport torchaudio.transforms as T\nimport librosa\n\n# Pre-trained image models\nimport timm\n```", "```py\nPATH = \"audio_example.wav\"\n\n# Load a sample audio file with torchaudio\noriginal_audio, sample_rate = torchaudio.load(PATH)\n\n# Load a sample audio file with librosa\noriginal_audio, sample_rate = librosa.load(PATH, \n                                           sr = None) # Gotcha: Set sr to None to get original sampling rate. Otherwise the default is 22050\n```", "```py\n# Play the audio in Jupyter notebook\nfrom IPython.display import Audio\n\nAudio(data = original_audio, rate = sample_rate)\n```", "```py\nimport librosa.display as dsp\ndsp.waveshow(original_audio, sr = sample_rate);\n```", "```py\nclass AudioDataset(Dataset):\n    def __init__(self, \n                df, \n                target_sample_rate= 32000, \n                audio_length\n                wave_transforms=None,\n                spec_transforms=None):\n        self.df = df\n        self.file_paths = df['file_path'].values\n        self.labels = df[['class_0', ..., 'class_N']].values\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = target_sample_rate * audio_length\n        self.wave_transforms = wave_transforms\n        self.spec_transforms = spec_transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n\n        # Load audio from file to waveform\n        audio, sample_rate = torchaudio.load(self.file_paths[index])\n\n        # Convert to mono\n        audio = torch.mean(audio, axis=0)\n\n        # Resample\n        if sample_rate != self.target_sample_rate:\n            resample = T.Resample(sample_rate, self.target_sample_rate)\n            audio = resample(audio)\n\n        # Adjust number of samples\n        if audio.shape[0] > self.num_samples:\n            # Crop\n            audio = audio[:self.num_samples]\n        elif audio.shape[0] < self.num_samples:\n            # Pad\n            audio = F.pad(audio, (0, self.num_samples - audio.shape[0]))\n\n        # Add any preprocessing you like here \n        # (e.g., noise removal, etc.)\n        ...\n\n        # Add any data augmentations for waveform you like here\n        # (e.g., noise injection, shifting time, changing speed and pitch)\n        ...\n\n        # Convert to Mel spectrogram\n        melspectrogram = T.MelSpectrogram(sample_rate = self.target_sample_rate, \n                                        n_mels = 128, \n                                        n_fft = 2048, \n                                        hop_length = 512)\n        melspec = melspectrogram(audio)\n\n        # Add any data augmentations for spectrogram you like here\n        # (e.g., Mixup, cutmix, time masking, frequency masking)\n        ...\n\n        return {\"image\": torch.stack([melspec]), \n                \"label\": torch.tensor(self.labels[index]).float()}\n```", "```py\nclass AudioDataset(Dataset):\n    def __init__(self, \n                df, \n                target_sample_rate= 32000, \n                audio_length):\n        self.df = df\n        self.file_paths = df['file_path'].values\n        self.labels = df[['class_0', ..., 'class_N']].values\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = target_sample_rate * audio_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n\n        # Load audio from file to waveform\n        audio, sample_rate = torchaudio.load(self.file_paths[index])\n\n        # Add any preprocessing you like here \n        # (e.g., converting to mono, resampling, adjusting size, noise removal, etc.)\n        ...\n\n        # Add any data augmentations for waveform you like here\n        # (e.g., noise injection, shifting time, changing speed and pitch)\n        wave_transforms = T.PitchShift(sample_rate, 4)\n        audio = wave_transforms(audio)\n\n        # Convert to Mel spectrogram\n        melspec = ...\n\n        # Add any data augmentations for spectrogram you like here\n        # (e.g., Mixup, cutmix, time masking, frequency masking)\n        spec_transforms = T.FrequencyMasking(freq_mask_param=80)\n        melspec = spec_transforms(melspec)\n\n        return {\"image\": torch.stack([melspec]), \n                \"label\": torch.tensor(self.labels[index]).float()}\n```", "```py\nclass AudioModel(nn.Module):\n    def __init__(self, \n                model_name = 'tf_efficientnet_b3_ns',\n                pretrained = True, \n                num_classes):\n        super(AudioModel, self).__init__()\n\n        self.model = timm.create_model(model_name, \n                                       pretrained = pretrained, \n                                       in_chans = 1)\n        self.in_features = self.model.classifier.in_features\n        self.model.classifier = nn.Sequential(\n              nn.Linear(self.in_features, num_classes)\n          )\n\n    def forward(self, images):\n        logits = self.model(images)\n        return logits\n```", "```py\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n                              T_max = ..., # Maximum number of iterations.\n                              eta_min = ...) # Minimum learning rate.\n```"]