["```py\ngit clone https://github.com/facebookresearch/llama.git\ncd llama \n```", "```py\nchmod +x download.sh\n```", "```py\nbash download.sh\n```", "```py\nconda create --name yourenvname python=3.10\n```", "```py\nconda activate yourenvname\n```", "```py\npip install -r requirements.txt \n```", "```py\npip install -e .\n```", "```py\nexport RANK=1\nexport WORLD_SIZE=0\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=12355\n```", "```py\ntorchrun --nproc-per-node=NUM_GPUS_YOU_HAVE your_script.py \\ \n         -- ckpt_dir /path/to/checkpoint \\ \n         -- tokenizer_path /path/to/tokenizer\n```", "```py\ntorchrun --nproc_per_node 1 example_text_completion.py \\\n    --ckpt_dir llama-2-7b/ \\\n    --tokenizer_path tokenizer.model \\\n    --max_seq_len 128 --max_batch_size 4\n```", "```py\n!pip install huggingface_hub ipywidgets\n```", "```py\nimport huggingface_hub\nhuggingface_hub.login()\n```"]