["```py\nimport numpy as np\nfrom sklearn.linear_model import Perceptron\n\nX = np.array([2, 3], [1, 4], [4, 1], [3, 2])\ny = np.array([1, 1, 0, 0])\n\nperceptron = Perceptron()\nperceptron.fit(X, y)\n```", "```py\nnew_data_point = np.array([[1, 2]])\nprediction = perceptron.predict(new_data_point)\nprint(prediction)\n```", "```py\ndef nand_gate(x1, x2):\n    w1, w2, b = -1, -1, 1.5\nreturn int(w1 * x1 + w2 * x2 + b > 0)\n\nbinary_inputs = [(0,0), (0,1), (1,0), (1,1)]\nfor A and B in binary_inputs:\nprint(f\"(A, B) --> nand_gate(A, B)\")\n```", "```py\n(0, 0) --> 1\n(0, 1) --> 1\n(1, 0) --> 1\n(1, 1) --> 0\n```", "```py\ndef nand_gate(inputs):\nassert len(inputs) > 1, \"At least two inputs are required.\"\n\n# Helper function to create a 2-input AND gate\ndef and_gate (x1, x2):\n    w1, w2, b = 1, 1, -1.5\nreturn int(w1 * x1 + w2 * x2 + b > 0)\n\nReduce the inputs to a single NAND output using the helper function.\nresult = and_gate(inputs[0], inputs[1])\nfor i in range (2, len (inputs)):\nresult = and_gate(result, inputs[i])\n  return 0 if result > 0 else 1\n\n# Example usage\ninputs = [(0, 0, 0, 0),\n          (0, 0, 0, 1),\n          (0, 0, 1, 0),\n          (0, 0, 1, 1),\n          (0, 1, 0, 0),\n          (0, 1, 0, 1),\n          (0, 1, 1, 1),\n          (1, 0, 0, 0),\n          (1, 0, 0, 1),\n          (1, 0, 1, 0),\n          (1, 0, 1, 1),\n          (1, 1, 0, 0),\n          (1, 1, 0, 1),\n          (1, 1, 1, 0),\n          (1, 1, 1, 1)]\n\nfor A0, A1, A2, and A3 inputs:\n  output = nand_gate((A0, A1, A2, A3))\n  print(f\"({A0}, {A1}, {A2}, {A3}) --> {output}\")\n```", "```py\ndef and_gate(x1, x2):\n    w1, w2, b = 1, 1, -1.5\n    return int(w1 * x1 + w2 * x2 + b > 0)\n\nbinary_inputs = [(0,0), (0,1), (1,0), (1,1)]\nfor A, B in binary_inputs:\n  print(f\"({A}, {B}) --> {and_gate(A, B)}\")\n```", "```py\n(0, 0) --> 0\n(0, 1) --> 0\n(1, 0) --> 0\n(1, 1) --> 1\n```", "```py\nA1A0 = [1, 0]\nB1B0 = [1, 1]\n\nP00 = and_gate(A1A0[1], B1B0[1])\nP01 = and_gate(A1A0[1], B1B0[0])\nP10 = and_gate(A1A0[0], B1B0[1])\nP11 = and_gate(A1A0[0], B1B0[0])\n\n# Implement a simple adder using perceptron-based logic gates\nresult = [P00, P01 ^ P10, (P01 & P10) ^ P11, P11]\nprint(result)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\n\nnew_data_point = np.array([[1, 2]])\nprob_prediction = log_reg.predict_proba(new_data_point)\nprint(prob_prediction)\n```", "```py\nimport numpy as np\nfrom sklearn.linear_model import Perceptron, LogisticRegression\n\n# Dataset\nX = np.array([[2, 3], [1, 4], [4, 1], [3, 2]])\ny = np.array([1, 1, 0, 0])\n\n# Train Perceptron\nperceptron = Perceptron()\nperceptron.fit(X, y)\n\n# Train Logistic Regression\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\n\n# New data point\nnew_data_point = np.array([[1, 2]])\n\n# Perceptron prediction\nperc_prediction = perceptron.predict(new_data_point)\nprint(\"Perceptron prediction:\", perc_prediction)\n\n# Logistic Regression prediction\nlog_reg_prediction = log_reg.predict(new_data_point)\nprint(\"Logistic Regression prediction:\", log_reg_prediction)\n\n# Logistic Regression probability prediction\nprob_prediction = log_reg.predict_proba(new_data_point)\nprint(\"Logistic Regression probability prediction:\", prob_prediction)\n```", "```py\nPerceptron prediction: [1]\nLogistic Regression prediction: [1]\nLogistic Regression probability prediction: [[0.33610873 0.66389127]]\n```"]