- en: The Three Essential Methods to Evaluate a New Language Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/the-three-essential-methods-to-evaluate-a-new-language-model-aa5c526bacfd](https://towardsdatascience.com/the-three-essential-methods-to-evaluate-a-new-language-model-aa5c526bacfd)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to check whether the newest, hottest Large Language Model (LLM) fits your
    needs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://heiko-hotz.medium.com/?source=post_page-----aa5c526bacfd--------------------------------)[![Heiko
    Hotz](../Images/d08394d46d41d5cd9e76557a463be95e.png)](https://heiko-hotz.medium.com/?source=post_page-----aa5c526bacfd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----aa5c526bacfd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----aa5c526bacfd--------------------------------)
    [Heiko Hotz](https://heiko-hotz.medium.com/?source=post_page-----aa5c526bacfd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----aa5c526bacfd--------------------------------)
    ¬∑6 min read¬∑Jul 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d44df1d02e0e695eedca24b1c6e2d15b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author (using Stable Diffusion)
  prefs: []
  type: TYPE_NORMAL
- en: What is this about?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'New LLMs are released every week, and if you‚Äôre like me, you might ask yourself:
    Does this one finally fit all the use cases I want to utilise an LLM for? In this
    tutorial, I will share the techniques that I use to evaluate new LLMs. I‚Äôll introduce
    three techniques I use regularly ‚Äî none of them are new (in fact, I will refer
    to blog posts that I have written previously), but by bringing them all together,
    I save a significant amount of time whenever a new LLM is released. I will demonstrate
    examples of testing on the new [OpenChat](https://huggingface.co/openchat/openchat_8192)
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: Why is this important?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to new LLMs, it‚Äôs important to understand their capabilities and
    limitations. Unfortunately, figuring out how to deploy the model and then systematically
    testing it can be a bit of a drag. This process is often manual and can consume
    a lot of time. However, with a standardised approach, we can iterate much faster
    and quickly determine whether a model is worth investing more time in, or if we
    should discard it. So, let‚Äôs get started.
  prefs: []
  type: TYPE_NORMAL
- en: '**Getting Started**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many ways to utilise an LLM, but when we distil the most common uses,
    they often pertain to open-ended tasks (e.g. generating text for a marketing ad),
    chatbot applications, and Retrieval Augmented Generation (RAG). Correspondingly,
    I employ relevant methods to test these capabilities in an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 0\. Deploying the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we get started with the evaluation, we first need to deploy the model.
    I have boilerplate code ready for this, where we can just swap out the model ID
    and the instance to which to deploy (I‚Äôm using Amazon SageMaker for model hosting
    in this example) and we‚Äôre good to go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It‚Äôs worth noting that we can utilise the new [Hugging Face LLM Inference Container
    for SageMaker](https://huggingface.co/blog/sagemaker-huggingface-llm), as the
    new OpenChat model is based on the LLAMA architecture, which is supported in this
    container.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Playground
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the notebook to test a few prompts can be burdensome, and it may also
    discourage non-technical users from experimenting with the model. A much more
    effective way to familiarise yourself with the model, and to encourage others
    to do the same, involves the construction of a playground. I have previously detailed
    how to easily create such a playground in this [blog post](/create-your-own-large-language-model-playground-in-sagemaker-studio-1be5846c5089).
    With the code from that blog post, we can get a playground up and running quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the playground is established, we can introduce some prompts to evaluate
    the model‚Äôs responses. I prefer using open-ended prompts, where I pose a question
    that requires some degree of common sense to answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '***How can I improve my time management skills?***'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c59792642c67fb692d5da39dac0ed64.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '***What if the Suez Canal had never been constructed?***'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/257c34f41bfc5060f0add201b72d2ee3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Both responses appear promising, suggesting that it could be worthwhile to invest
    additional time and resources in testing the OpenChat model.
  prefs: []
  type: TYPE_NORMAL
- en: 2.Chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second thing we want to explore is a model‚Äôs chatbot capabilities. Unlike
    the playground, where the model is consistently stateless, we want to understand
    its ability to ‚Äúremember‚Äù context within a conversation. In this [blog post](https://medium.com/mlearning-ai/unlocking-the-future-of-chatbots-with-falcon-hugging-face-and-amazon-sagemaker-cf6bd8aeba54),
    I described how to set up a chatbot using the Falcon model. It‚Äôs a simple plug-and-play
    operation, and by changing the SageMaker endpoint, we can direct it towards the
    new OpenChat model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs see how it fares:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d384127f6a6d619277833486c512d7bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The performance as a chatbot is quite impressive. There was an instance, however,
    where Openchat attempted to abruptly terminate the conversation, cutting off in
    mid-sentence. This occurrence is not rare, in fact. We don‚Äôt usually observe this
    with other chatbots because they employ specific stop words to compel the AI to
    cease text generation. The occurrence of this issue in my app is probably due
    to the implementation of stop words within my application.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond that, OpenChat has the capability to maintain context throughout a conversation,
    as well as to extract crucial information from a document. Impressive. üòä
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Retrieval Augmented Generation (RAG)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last task we want to test involves using LangChain for some RAG tasks.
    I‚Äôve found that RAG tasks can be quite challenging for open source models, often
    requiring me to write my own prompts and custom response parsers to achieve functionality.
    However, what I‚Äôd like to see is a model that operates optimally ‚Äúout of the box‚Äù
    for standard RAG tasks. This [blog post](https://medium.com/mlearning-ai/supercharging-large-language-models-with-langchain-1cac3c103b52)
    provides a few examples of such tasks. Let‚Äôs examine how well it performs. The
    question we‚Äôll be posing is:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Who is the prime minister of the UK? Where was he or she born? How far is
    their birth place from London?***'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3aaf72deeacd3075e4d82c4bfdd1f528.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: This is, without a doubt, the best performance I‚Äôve seen from an open-source
    model using the standard prompt from LangChain. This is probably unsurprising,
    considering OpenChat has been fine-tuned on ChatGPT conversations, and LangChain
    is tailored towards OpenAI models, particularly ChatGPT. Nonetheless, the model
    was capable of retrieving all three facts accurately using the tools at its disposal.
    The only shortcoming was that, in the end, the model failed to recognise that
    it possessed all the necessary information and could answer the user‚Äôs question.
    Ideally, it should have stated, ‚ÄúI now have the final answer,‚Äù and provided the
    user with the facts it had gathered.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dffb5124caaedb68a80ae033b5ab5478.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this blog post, I‚Äôve introduced you to three standard evaluation techniques
    that I use all the time to evaluate LLMs. We‚Äôve observed that the new OpenChat
    model performs exceptionally well on all these tasks. Surprisingly, it appears
    very promising as the underlying LLM for a RAG application, probably just requiring
    customised prompting to discern when it has arrived at the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs noteworthy that this isn‚Äôt a comprehensive evaluation, nor is it intended
    to be. Instead, it offers an indication of whether a particular model is worth
    investing more time in and conducting further, more intensive testing. It seems
    that OpenChat is definitely worth spending time on ü§ó
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to use all the tools, expand and customise them, and start evaluating
    the LLMs that pique your interest within minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Heiko Hotz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: üëã Follow me on [Medium](https://heiko-hotz.medium.com/) and [LinkedIn](https://www.linkedin.com/in/heikohotz/)
    to read more about Generative AI, Machine Learning, and Natural Language Processing.
  prefs: []
  type: TYPE_NORMAL
- en: üë• If you‚Äôre based in London join one of our [NLP London Meetups](https://www.meetup.com/nlp_london/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e787e5a8febdad2128756ce2a7e75fd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
