- en: Leverage LLMs Like GPT to Analyze Your Documents or Transcripts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/leverage-llms-like-gpt-to-analyze-your-documents-or-transcripts-c640a266ad52](https://towardsdatascience.com/leverage-llms-like-gpt-to-analyze-your-documents-or-transcripts-c640a266ad52)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Use prompt engineering to analyze your documents with langchain and openai in
    a ChatGPT-like way
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://konstantin-rink.medium.com/?source=post_page-----c640a266ad52--------------------------------)[![Konstantin
    Rink](../Images/41bfc069d7382a0fd56f081eea7eb2d9.png)](https://konstantin-rink.medium.com/?source=post_page-----c640a266ad52--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c640a266ad52--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c640a266ad52--------------------------------)
    [Konstantin Rink](https://konstantin-rink.medium.com/?source=post_page-----c640a266ad52--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c640a266ad52--------------------------------)
    ·6 min read·Mar 31, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7dd6687f2e85da1c40832aaf26002fc7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: (Original) photo by [Laura Rivera](https://unsplash.com/@laurar1vera?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/de/fotos/9ZQzrLWV52M?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT is definitely one of the most popular Large Language Models (LLMs).
    Since the release of its beta version at the end of 2022, everyone can use the
    convenient chat function to ask questions or interact with the language model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '**But what if we would like to ask ChatGPT questions about our own documents
    or about a podcast we just listened to?**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this article is to show you how to leverage LLMs like GPT to analyze
    our documents or transcripts and then ask questions and receive answers in a ChatGPT
    way about the content in the documents.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: tl;dr
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article uses OpenAI’s ChatGPT `gpt-3.5-turbo` model, which requires an
    [API key](https://platform.openai.com/account/api-keys).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `langchain` package, a framework built around LLMs, is used to load and
    process our documents (Prompt Engineering) and to interact with the model.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A colab notebook containing the whole code of the article can be found [here](https://colab.research.google.com/drive/1oG6TXgXJTd8qyCj_ncD0Uy1pGEDMlly3?usp=sharing).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prerequisites
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before writing all the code, we have to make sure that all the necessary packages
    are installed, API keys are created, and configurations set.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: API key
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make use of ChatGPT one needs to create an OpenAI API key first. The key
    can be created under this [link](https://platform.openai.com/account/api-keys)
    and then by clicking on the
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '`+ Create new secret key` button.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '**Nothing is free**: Generally OpenAI charges you for every 1,000 tokens. Tokens
    are the result of processed texts and can be words or chunks of characters. The
    prices per 1,000 tokens vary per model (e.g., $0.002 / 1K tokens for gpt-3.5-turbo).
    More details about the pricing options can be found [here](https://openai.com/pricing).'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The good thing is that OpenAI grants you a free trial usage of $18 without requiring
    any payment information. An overview of your current usage can be seen in your
    [account](https://platform.openai.com/account/usage).
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Installing the OpenAI package
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have to also install the official OpenAI package by running the following
    command
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Since OpenAI needs a (valid) API key, we will also have to set the key as a
    environment variable:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Installing the langchain package
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the tremendous rise of interest in Large Language Models (LLMs) in late
    2022 (release of Chat-GPT), a package named LangChain appeared [around the same
    time](https://github.com/hwchase17/langchain/graphs/commit-activity).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[LangChain](https://github.com/hwchase17/langchain) is a framework built around
    LLMs like ChatGPT. The aim of this package is to assist in the development of
    applications that combine LLMs with other sources of computation or knowledge.
    It covers the application areas like *Question Answering over specific documents*
    (**goal of this article**)*, Chatbots*, and *Agents*. More information can be
    found in the [documentation](https://langchain.readthedocs.io/en/latest/).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'The package can be installed with the following command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Prompt Engineering
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might be wondering what *Prompt Engineering* is. It is possible to [fine-tune](https://platform.openai.com/docs/guides/fine-tuning)
    GPT-3 by creating a custom model trained on the documents you would like to analyze.
    However, besides costs for training we would also need a lot of high-quality examples,
    ideally vetted by human experts (according to the [documentation](https://platform.openai.com/docs/guides/fine-tuning/general-best-practices)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: This would be overkill for just analyzing our documents or transcripts. So instead
    of training or fine-tuning a model, we pass the text (commonly referred to as
    prompt) that we would like to analyze to it. Producing or creating such high quality
    prompts is called *Prompt Engineering*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: A good article for further reading about Prompt Engineering can be
    found [here](https://docs.cohere.ai/docs/prompt-engineering)'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Loading the data
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on your use case, `langchain` offers you several “[loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)”
    like `Facebook Chat`, `PDF`, or `DirectoryLoader` to load or read your (unstructured)
    text (files). The package also comes with a `YoutubeLoader` to transcribe youtube
    videos.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The following examples focus on the `DirectoryLoader` and `YoutubeLoader`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Read text files with DirectoryLoader
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `DirectoryLoader` takes as a first argument the **path** and as a second
    a **pattern** to find the documents or document types we are looking for. In our
    case we would load all text files (*.txt*) in the same directory as the script.
    The `load_and_split` function then initiates the loading.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`DirectoryLoader` 的第一个参数是**路径**，第二个参数是**模式**，用以查找我们所需要的文档或文档类型。在我们的案例中，我们将加载与脚本位于同一目录下的所有文本文件（*.txt*）。`load_and_split`
    函数随后启动加载过程。'
- en: Even though we might only load one text document, it makes sense to do a splitting
    in case we have a large file and to avoid a `NotEnoughElementsException` (minimum
    four documents are needed). More Information can be found [here](https://github.com/hwchase17/langchain/issues/1793#issuecomment-1480765690).
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 即使我们可能只加载一个文本文档，但如果文件较大，进行拆分也是有意义的，以避免`NotEnoughElementsException`（需要至少四个文档）。更多信息可以在[这里](https://github.com/hwchase17/langchain/issues/1793#issuecomment-1480765690)找到。
- en: Transcribe youtube videos with YoutubeLoader
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 YoutubeLoader 转录 YouTube 视频
- en: LangChain comes with a YoutubeLoader module, which makes use of the `youtube_transcript_api`
    [package](https://pypi.org/project/youtube-transcript-api/). This module gathers
    the (generated) subtitles for a given video.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 提供了一个 YoutubeLoader 模块，该模块使用 `youtube_transcript_api` [包](https://pypi.org/project/youtube-transcript-api/)。这个模块会收集给定视频的（生成的）字幕。
- en: Not every video comes with its own subtitles. In these cases auto-generated
    subtitles are available. However, in some cases they have a bad quality. In these
    cases the usage of [Whisper](/transcribe-audio-files-with-openais-whisper-e973ae348aa7)
    to transcribe audio files could be an alternative.
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 并非每个视频都自带字幕。在这些情况下，可以使用自动生成的字幕。然而，在某些情况下，这些字幕质量较差。在这些情况下，使用[Whisper](/transcribe-audio-files-with-openais-whisper-e973ae348aa7)来转录音频文件可能是一个替代方案。
- en: 'The code below takes the **video id** and a **language** (default: en) as parameters.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码将**视频 ID**和**语言**（默认为 en）作为参数。
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Before we continue…
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在我们继续之前……
- en: In case you decide to go with **transcribed youtube videos**, consider a **proper
    cleaning** of, e.g., Latin1 characters (*\xa0*) **first**. I experienced in the
    *Question-Answering* part differences in the answers depending on which format
    of the same source I used.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定使用**转录的 YouTube 视频**，请考虑**首先进行适当的清理**，例如，清除 Latin1 字符（*\xa0*）。我在*问答*部分体验到的答案差异取决于我使用的相同来源的格式。
- en: Processing the data
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数据
- en: LLMs like GPT can only handle a certain [amount of tokens](https://platform.openai.com/docs/models/gpt-3-5).
    These limitations are important when working with large(r) documents. In general,
    there are three ways of dealing with these limitations. One is to make use of
    embeddings or `vector space engine`. A second way is to try out different chaining
    methods like `map-reduce` or `refine`. And a third one is a combination of both.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPT 这样的 LLMs 只能处理一定数量的[令牌](https://platform.openai.com/docs/models/gpt-3-5)。这些限制在处理大型（或更大）文档时尤为重要。通常，有三种方法来应对这些限制。一种是利用嵌入或`vector
    space engine`。第二种方法是尝试不同的链式方法，如`map-reduce`或`refine`。第三种方法是两者的结合。
- en: 'A great article that provides more details about the different chaining methods
    and the use of a vector space engine can be found [here](https://dagster.io/blog/chatgpt-langchain).
    Also keep in mind: The more tokens you use, the more you get charged.'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一篇提供有关不同链式方法和向量空间引擎使用的更多细节的精彩文章可以在[这里](https://dagster.io/blog/chatgpt-langchain)找到。还要记住：使用的令牌越多，收费也会越高。
- en: In the following we combine `embeddings` with the chaining method `stuff` which
    “stuffs” all documents in one single prompt.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们将`embeddings`与链式方法`stuff`结合起来，`stuff`会将所有文档“塞入”一个单一的提示中。
- en: First we ingest our transcript ( `docs`) into a vector space by using `OpenAIEmbeddings`.
    The embeddings are then stored in an in-memory embeddings database called [Chroma](https://github.com/chroma-core/chroma).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过使用`OpenAIEmbeddings`将转录文本（`docs`）导入向量空间。然后，这些嵌入被存储在一个称为[Chroma](https://github.com/chroma-core/chroma)的内存嵌入数据库中。
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: After that, we define the **model_name** we would like to use to analyze our
    data. In this case we choose `gpt-3.5-turbo`. A full list of available models
    can be found [here](https://platform.openai.com/docs/models/gpt-3-5). The **temperature**
    parameter defines the sampling temperature. Higher values lead to more random
    outputs, while lower values will make the answers more focused and deterministic.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们定义我们希望用于分析数据的**model_name**。在这种情况下，我们选择`gpt-3.5-turbo`。可以在[这里](https://platform.openai.com/docs/models/gpt-3-5)找到可用模型的完整列表。**temperature**参数定义了采样温度。较高的值会导致输出更随机，而较低的值则会使答案更加集中和确定。
- en: Last but not least we use the`RetrievalQA` (**Q**uestion/**A**nswer) [Retriever](https://blog.langchain.dev/retrieval/)
    and set the respective parameters (`llm`, `chain_type` , `retriever`).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Asking questions
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we are ready to ask the model questions about our documents. The code below
    shows how to define the query.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: What do to with incomplete answers?
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases you might experience incomplete answers. The answer text just
    stops after a few words.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The reason for an incomplete answer is most likely the token limitation. If
    the provided prompt is quite long, the model does not have that many tokens left
    to give an (full) answer. One way of handling this could be to switch to a different
    **chain-type** like `refine`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: However, I experienced that when using a different`chain_type`than *stuff* ,
    I get less concrete results. Another way of handling these issues is to rephrase
    the question and make it more concrete.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thanks to the LangChain package, one needs only a few lines of code to analyze
    LLMs like GPT text documents or transcripts. Since the package is relatively new,
    I expect many updates and code changes soon. That might affect the provided code
    snippets in this article.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In case you think about using LLM in your daily work or for a larger private
    project, you should focus on cleaning the data properly, optimizing the number
    of used tokens, and using best practices like setting budget limits or alarms.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed reading this article. A colab notebook with the source code
    can be found [here](https://colab.research.google.com/drive/1oG6TXgXJTd8qyCj_ncD0Uy1pGEDMlly3?usp=sharing).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Sources
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://www.pinecone.io/learn/langchain-intro/?source=post_page-----c640a266ad52--------------------------------)
    [## LangChain: Introduction and Getting Started | Pinecone'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: arge Language M odels (LLMs) entered the world stage with the release of OpenAI's
    GPT-3 in 2020 [1]. Since then…
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.pinecone.io](https://www.pinecone.io/learn/langchain-intro/?source=post_page-----c640a266ad52--------------------------------)
    [](https://dagster.io/blog/chatgpt-langchain?source=post_page-----c640a266ad52--------------------------------)
    [## Build a GitHub Support Bot with GPT3, LangChain, and Python | Dagster Blog
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: January 9, 2023 * 13 minute read * ChatGPT, ever heard of it? ChatGPT came out
    a few months ago and blew everyones'…
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: dagster.io](https://dagster.io/blog/chatgpt-langchain?source=post_page-----c640a266ad52--------------------------------)  [##
    Prompt Engineering
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Here, we discuss a few principles and techniques for writing prompts (inputs
    for our models) that will help you get the…
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: docs.cohere.ai](https://docs.cohere.ai/docs/prompt-engineering?source=post_page-----c640a266ad52--------------------------------)
    [](https://blog.langchain.dev/retrieval/?source=post_page-----c640a266ad52--------------------------------)
    [## Retrieval
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'TL;DR: We are adjusting our abstractions to make it easy for other retrieval
    methods besides the LangChain VectorDB…'
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: blog.langchain.dev](https://blog.langchain.dev/retrieval/?source=post_page-----c640a266ad52--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[blog.langchain.dev](https://blog.langchain.dev/retrieval/?source=post_page-----c640a266ad52--------------------------------)'
