# 测量新 Pandas 2.0 相对于 Polars 和 Datatable 的速度——仍然不够好

> 原文：[https://towardsdatascience.com/measuring-the-speed-of-new-pandas-2-0-against-polars-and-datatable-still-not-good-enough-e44dc78f6585](https://towardsdatascience.com/measuring-the-speed-of-new-pandas-2-0-against-polars-and-datatable-still-not-good-enough-e44dc78f6585)

## 尽管新的 PyArrow 后端为 Pandas 带来了令人兴奋的功能，但在速度方面仍然令人失望。

[](https://ibexorigin.medium.com/?source=post_page-----e44dc78f6585--------------------------------)[![Bex T.](../Images/516496f32596e8ad56bf07f178a643c6.png)](https://ibexorigin.medium.com/?source=post_page-----e44dc78f6585--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e44dc78f6585--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e44dc78f6585--------------------------------) [Bex T.](https://ibexorigin.medium.com/?source=post_page-----e44dc78f6585--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e44dc78f6585--------------------------------) ·7 分钟阅读·2023年3月29日

--

![](../Images/741230d22d003ff7576c9b047ca76d76.png)

作者提供的图像来自 Midjourney

自从第一次尝试用 `read_csv` 读取一个大小为一千兆字节的数据集，并意识到需要等待 - *喘息* - 五秒钟以来，人们一直在抱怨 Pandas 的速度。是的，我也是那些抱怨者之一。

五秒钟听起来可能不多，但当加载数据集本身需要这么长时间时，通常意味着后续操作也会需要这么久。而且，由于速度是快速、简便的数据探索中最重要的因素之一，这可能让你感到*非常*沮丧。

因此，PyData 的人员最近宣布了计划发布带有全新 PyArrow 后端的 Pandas 2.0。对于完全不了解的人，PyArrow 本身是一个设计用于高性能、内存高效处理数组的小巧库。

人们真心希望新的后端能带来比原生 Pandas 更显著的加速。本文将通过将 PyArrow 后端与两种最快的数据框库 Datatable 和 Polars 进行比较来测试这一希望的光芒。

## 难道这些人还没有做过这个吗？

既然 H20 目前已经运行了受欢迎的 [**Database-like Ops Benchmark**](https://h2oai.github.io/db-benchmark/) 来测量几乎 15 个库在三种不同数据集大小上的三个数据操作的计算速度，那做这个基准测试还有什么意义？我的基准测试不可能做到那么全面。

好吧，首先，这个基准测试没有包括带有 PyArrow 后端的 Pandas，并且最后一次更新是在 2021 年，那已经很久了。

其次，基准测试在一台拥有40个CPU核心、128 GB内存和20 GB GPU的超级机器上运行（[cuDF](https://github.com/rapidsai/cudf)，有人用过吗？）。普通用户通常无法使用这样的机器，因此比较在像我这样日常设备上的库之间的差异非常重要。它配备了一个拥有十几个核心的中等性能CPU和32 GB的内存。

最后，我提倡过程中的完全透明，所以我将详细解释基准测试代码，并将其作为GitHub Gist提供，供您在自己的机器上运行。

## 安装和设置

我们首先安装Pandas 2.0的RC（候选版本），以及最新版本的PyArrow、Datatable和Polars。

[PRE0]

[PRE1]

[PRE2]

[PRE3]

[PRE4]

[PRE5]

[PRE6]

[PRE7]

我使用NumPy和Faker库创建了一个合成数据集，以模拟人口普查数据集中的典型特征，并将其保存为CSV和Parquet格式。以下是文件路径：

[PRE8]

> 查看[这个 GitHub gist](https://gist.github.com/BexTuychiev/92a1fbbed96fa52cec47fe2cd725cf3e)以查看生成数据的代码。

有5000万行七个特征，使文件大小达到约2.5 GB。

## 基准测试结果

在展示代码之前，我们先看看好东西——基准测试结果：

![](../Images/2e45560dbd6baf259a966f20b883c57c.png)

作者提供的图片

一开始，我们可以看到PyArrow Pandas在所有类别中都排在最后（或在`groupby`中倒数第二）。

请不要将阅读和写入Parquet类别中的不存在的条形图误认为是0运行时间。这些操作在Datatable中不受支持。

在其他类别中，Datatable和Polars并列第一，Polars稍微占有优势。

写入CSV文件一直是Pandas的慢速过程，我猜新后端不足以改变这一点。

## 你应该切换吗？

所以，百万美元的问题是——你应该切换到更快的Polars还是Datatable？

答案是*我非常讨厌*的“这要看情况”。你是否愿意为了更快的速度牺牲Pandas近二十年的成熟度，以及说实话，愚蠢的容易和熟悉的语法？

在这种情况下，请记住，你花时间学习新库的语法可能会平衡其性能提升。

但是，如果你只是处理大量数据集，那么学习这些快速库中的任何一个可能从长远来看都值得付出努力。

如果你决定继续使用Pandas，请仔细阅读[Pandas用户指南的性能提升](https://pandas.pydata.org/docs/user_guide/enhancingperf.html)页面。它概述了一些技巧和窍门，以在不依赖第三方库的情况下为Pandas引擎增加额外的动力。

此外，如果你被困在一个大的CSV文件中，仍然想使用Pandas，你应该记住以下代码片段：

[PRE9]

它以Datatable的速度读取文件，将其转换为Pandas DataFrame几乎是瞬间完成的。

## 基准测试代码

好了，最后来看代码吧。

导入库后，首先要做的是定义一个DataFrame来存储基准测试结果。这将使绘图过程变得更简单。

[PRE10]

它有三列，一列用于任务名称，另一列用于库名称，还有一列用于存储运行时间。

然后，我们定义一个`timer`装饰器，执行以下任务：

1.  测量装饰函数的运行时间。

1.  提取函数的名称和其`library`参数的值。

1.  将运行时间、函数名称和库名称存储到传递的结果DataFrame中。

[PRE11]

这个想法是定义一个通用函数，如`read_csv`，用于读取三种库中的任意一个的CSV文件，可以通过像`library`这样的参数进行控制：

[PRE12]

注意我们是如何用`timer(results_df)`装饰函数的。

我们以类似的方式定义其他任务的函数（参见[the Gist](https://gist.github.com/BexTuychiev/dba8d1f876e1d601f530c0e8b16d5a85)中的函数体）：

[PRE13]

然后，我们为每个库运行这些函数：

[PRE14]

为了避免内存错误，我避免了循环，并在Jupyter Notebook中运行了三次基准测试，改变了`l`变量。

然后，我们使用可爱的Seaborn创建基准图形，展示以下简单的柱状图：

[PRE15]

![](../Images/2e45560dbd6baf259a966f20b883c57c.png)

图片由作者提供

## 事情正在发生变化

多年来，Pandas依赖于NumPy的肩膀，因为NumPy在流行度上迅猛增长。NumPy慷慨地借用了其快速计算和数组操作的功能。

但这种方法受限于NumPy对文本和缺失值的糟糕支持。Pandas不能使用本地Python数据类型如列表和字典，因为那样会在大规模上成为笑柄。

因此，Pandas在过去几年中已悄然远离NumPy。例如，它在2020年就引入了PyArrow数据类型用于字符串。它还使用了用其他语言编写的扩展，如C++和Rust，用于日期（含时区）或分类数据等复杂数据类型。

现在，Pandas 2.0拥有一个全面的后端，支持所有数据类型，基于Apache Arrow的PyArrow实现。除了明显的速度提升外，它还提供了更好的缺失值支持、互操作性和更广泛的数据类型支持。

因此，即使后端仍然比其他DataFrame库慢，我仍然热切期待其正式发布。感谢阅读！

这里有一些页面可以了解更多关于Pandas 2.0和PyArrow后端的信息：

+   [https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i)

+   [https://levelup.gitconnected.com/welcoming-pandas-2-0-194094e4275b](https://levelup.gitconnected.com/welcoming-pandas-2-0-194094e4275b)

+   [https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html](https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html)

喜欢这篇文章以及它那奇特的写作风格？想象一下，获取更多类似的文章，全部由一位才华横溢、迷人幽默的作者（对了，那就是我 :）。

只需4.99美元的会员费用，你将不仅能访问我的故事，还能获取来自 Medium 上最聪明、最杰出思想者的宝贵知识。如果你使用[我的推荐链接](https://ibexorigin.medium.com/membership)，你将获得我超级感激的心意和一个虚拟的击掌，以支持我的工作。

[](https://ibexorigin.medium.com/membership?source=post_page-----e44dc78f6585--------------------------------) [## 通过我的推荐链接加入 Medium — Bex T.

### 独享所有⚡高级⚡内容，并在 Medium 上无限畅游。通过购买我一杯…

ibexorigin.medium.com](https://ibexorigin.medium.com/membership?source=post_page-----e44dc78f6585--------------------------------) ![](../Images/a01b5e4fb641db5f35b8172a4388e821.png)
