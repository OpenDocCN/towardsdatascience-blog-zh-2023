["```py\nbase_model: codellama/CodeLlama-7b-hf\nbase_model_config: codellama/CodeLlama-7b-hf\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\nis_llama_derived_model: true\nhub_model_id: EvolCodeLlama-7b\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n  - path: mlabonne/Evol-Instruct-Python-1k\n    type: alpaca\ndataset_prepared_path: last_run_prepared\nval_set_size: 0.02\noutput_dir: ./qlora-out\n\nadapter: qlora\nlora_model_dir:\n\nsequence_len: 2048\nsample_packing: true\n\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_modules:\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project: axolotl\nwandb_entity:\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\n\ngradient_accumulation_steps: 1\nmicro_batch_size: 10\nnum_epochs: 3\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: true\nfp16: false\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nwarmup_steps: 100\neval_steps: 0.01\nsave_strategy: epoch\nsave_steps:\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n  bos_token: \"<s>\"\n  eos_token: \"</s>\"\n  unk_token: \"<unk>\"\n```", "```py\ngit clone https://github.com/OpenAccess-AI-Collective/axolotl\ncd axolotl\n\npip3 install -e .[flash-attn]\npip3 install -U git+https://github.com/huggingface/peft.git\n```", "```py\nwget https://gist.githubusercontent.com/mlabonne/8055f6335e2b85f082c8c75561321a66/raw/93915a9563fcfff8df9a81fc0cdbf63894465922/EvolCodeLlama-7b.yaml\n```", "```py\naccelerate launch scripts/finetune.py EvolCodeLlama-7b.yaml\n```", "```py\nwget https://gist.githubusercontent.com/mlabonne/a3542b0519708b8871d0703c938bba9f/raw/60abc5afc07f9d843bc23d56f4e0b7ab072c4a62/merge_peft.py\n```", "```py\npython merge_peft.py --base_model=codellama/CodeLlama-7b-hf --peft_model=./qlora-out --hub_id=EvolCodeLlama-7b\n```"]