- en: 'Practical Data Quality Auditing: A Comprehensive Guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/data-quality-auditing-a-comprehensive-guide-66b7bfe2aa1a](https://towardsdatascience.com/data-quality-auditing-a-comprehensive-guide-66b7bfe2aa1a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring how to leverage the Python eco-system for data quality auditing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://warsamewords.medium.com/?source=post_page-----66b7bfe2aa1a--------------------------------)[![Mohamed
    A. Warsame](../Images/181088ce4386d05a1609d352f0fba518.png)](https://warsamewords.medium.com/?source=post_page-----66b7bfe2aa1a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----66b7bfe2aa1a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----66b7bfe2aa1a--------------------------------)
    [Mohamed A. Warsame](https://warsamewords.medium.com/?source=post_page-----66b7bfe2aa1a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----66b7bfe2aa1a--------------------------------)
    ·8 min read·May 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d39ed09c1f0e1e25761929e54bf0277.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: You can’t manage what you can’t measure — Peter Drucker
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data quality auditing is an indispensable skill in our rapidly evolving, AI-empowered
    world. Just like crude oil needs refining, data also requires cleaning and processing
    to be useful. The old adage “*garbage in, garbage out*” remains as relevant today
    as it was in the early days of computing.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll explore how Python can help us ensure our datasets meet
    quality standards for successful projects. We’ll delve into Python libraries,
    code snippets, and examples that you can use in your own workflows.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table of Contents**:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding Data Quality and Its Dimensions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Validating Data Using Pydantic and pandas_dq
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Comparing Pydantic and pandas_dq
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exploring Accuracy and Consistency
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data Quality Auditing with pandas_dq
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Data Quality Audit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into tools and techniques, let’s first review the concept of data
    quality. According to a widely accepted industry definition, data quality refers
    to the degree to which a dataset is accurate, complete, timely, valid, unique
    in identifier attributes, and consistent.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3987d320e240e363cdb159bc93be492e.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Quality Dimensions. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Completeness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Completeness in data quality encompasses the **availability of all vital data**
    elements required to fulfill a specific objective. Take, for example, a customer
    database tailored for marketing purposes; it would be deemed incomplete if essential
    contact information such as phone numbers or email addresses were missing for
    certain customers.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure data completeness, organizations can employ data profiling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '*Data profiling is the systematic examination and assessment of datasets to
    uncover patterns, inconsistencies, and anomalies.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By scrutinizing the data meticulously, one can identify gaps, idiosyncrasies
    or missing values, enabling corrective measures such as sourcing the missing information
    or implementing robust data validation processes. The result is a more reliable,
    complete, and actionable dataset that empowers better decision-making, optimized
    marketing efforts, and ultimately, drive business success.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before thorough data profiling, **the first step** in any data quality
    audit is a review of the data dictionary: a concise, descriptive reference that
    defines the structure, attributes, and relationships of data elements within a
    dataset, serving as a guide for understanding and interpreting the data’s meaning
    and purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a52b4a0e32cbe392e3b92c7c2ba70e4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Dictionary Example. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: With a thorough review or creation of your data dictionary in hand, assessing
    completeness becomes a breeze when you leverage the power of low-code libraries
    such as Sweetviz, Missingno, or Pandas_DQ.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Personally, I gravitate towards the Pandas-Matplotlib-Seaborn combo, as it provides
    me with the flexibility to have full control over my output. This way, I can craft
    an engaging and visually appealing analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e96beac28e6a2d45e90dcd2a586bfade.png)'
  prefs: []
  type: TYPE_IMG
- en: Missing Values Plot. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Uniqueness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Uniqueness is a **data quality dimension** **that emphasizes the absence of
    duplicate** data in columns with uniqueness constraint. Each record should represent
    a unique entity without redundancy. For example, a user list should have unique
    IDs for each registered user; multiple records with the same ID indicate a lack
    of uniqueness.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the below example I’m mimicking a data integration step of merging two identically
    structured datasets. The Pandas concat function’s argument `verify_integrity`
    throws an error if uniqueness is violated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6c7460e20d506729bb0e8afe58a9c4a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Violation of Uniqueness. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, you would check the presence of duplication as part of your data quality
    audit.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Timeliness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Timeliness is **an aspect of data quality that focuses on the availability and
    cadence** of the data. Up-to-date and readily available data is essential for
    accurate analysis and decision-making. For example, a timely sales report should
    include the most recent data possible, not only data from several months prior.
    The dataset we have been using thus far for the examples doesn’t have a time dimension
    for us to explore cadence more deeply.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3fc703b047440c6251cc4c4459c4265.png)'
  prefs: []
  type: TYPE_IMG
- en: Timeliness example. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Validity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we transition to the concept of validity, one should recognize its role
    in ensuring that data adheres to the established rules, formats, and standards.
    Validity guarantees compliance with the schema, constraints, and data types designated
    for the dataset. We can use the powerful Python library Pydantic for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once tested with an example, we can run the entire dataset through a validation
    check which should print “no data validation issues” if successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can do the same with **pandas_dq**, using far less code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns an easy to read Pandas dataframe style report that details any
    validation issues encounter. I’ve provided an incorrect schema where `int64` variables
    have been reported to be `float64` variables. The library has correctly identified
    these:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/243d34c4251e64e403466702be1b249f.png)'
  prefs: []
  type: TYPE_IMG
- en: DataSchemaChecker output. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data type mismatch is rectified with a single line of code using the checker
    object created from the `DataSchemaChecker` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2570c256386716f0b4b9b214ab237044.png)'
  prefs: []
  type: TYPE_IMG
- en: DataSchemaChecker transform output. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Pydantic or pandas_dq?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are some differences between Pydantic and pandas_dq:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Declarative syntax**: arguably, Pydantic allows you to define the data schema
    and validation rules using a more concise and readable syntax. This can make it
    easier to understand and maintain your code. I find it super helpful to be able
    to define the ranges of possible values instead of merely the data type.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Built-in validation functions**: Pydantic provides various powerful built-in
    validation functions like `conint`, `condecimal`, and `constr`, which allow you
    to enforce constraints on your data without having to write custom validation
    functions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Comprehensive error handling**: When using Pydantic, if the input data does
    not conform to the defined schema, it raises a `ValidationError` with detailed
    information about the errors. This can help you easily identify issues with your
    data and take necessary action.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Serialization and deserialization**: Pydantic automatically handles serialization
    and deserialization of data, making it convenient to work with different data
    formats (like JSON) and convert between them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In conclusion, Pydantic offers a more concise, feature-rich, and user-friendly
    approach to data validation compared to the `DataSchemaChecker` class from pandas_dq.
  prefs: []
  type: TYPE_NORMAL
- en: Pydantic is likely a better choice for validating your data schema in a productionized
    environment. But if you just want to get up and running quickly with a prototype,
    you might prefer the low-code nature of the `DataSchemaChecker.`
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy & Consistency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are 2 further data quality dimensions which we haven’t explored up until
    now:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy** is a data quality dimension that addresses the correctness of
    data, ensuring it represents real-world situations without errors. For instance,
    an accurate customer database should contain correct and up-to-date addresses
    for all customers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency** **deals with the uniformity of data** across different sources
    or datasets within an organization. Data should be consistent in terms of format,
    units, and values. For example, a multinational company should report revenue
    data in a single currency to maintain consistency across its offices in various
    countries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can check all data quality issues present in a dataset using the *dq_report*
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'It detects the below data quality issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Strongly associated variables (multicollinearity)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Columns with no variance (redundant features)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asymmetrical data distributions (anomalies, outliers, ect.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrequent category occurrences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/11ff3091f550d83de10940d2205b79c9.png)'
  prefs: []
  type: TYPE_IMG
- en: DQ Report from pandas_dq libray. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performing data quality audits is crucial for maintaining high-quality datasets,
    which in turn drive better decision-making and business success. Python offers
    a wealth of libraries and tools that make the auditing process more accessible
    and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding and applying the concepts and techniques discussed in this
    article, you’ll be well-equipped to ensure your datasets meet the necessary quality
    standards for your projects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Link to full code:** [https://github.com/mohwarsame273/Medium-Articles/blob/main/DataQualityAudit.ipynb](https://github.com/mohwarsame273/Medium-Articles/blob/main/DataQualityAudit.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Pydantic (2023): Documentation [https://docs.pydantic.dev/](https://docs.pydantic.dev/)(accessed
    24\. April 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Pandas_dq (2023): Documentation [https://github.com/AutoViML/pandas_dq](https://github.com/AutoViML/pandas_dq)
    (accessed April 24, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Data Quality Dimensions (Enterprise Data Management Council EDM): [https://cdn.ymaws.com/edmcouncil.org/resource/resmgr/featured_documents/BP_DQ_Dimensions_Oct17.pdf](https://cdn.ymaws.com/edmcouncil.org/resource/resmgr/featured_documents/BP_DQ_Dimensions_Oct17.pdf)
    (accessed April 24, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Batini, C., Cappiello, C., Francalanci, C., & Maurino, A. (2009). Methodologies
    for data quality assessment and improvement. *ACM computing surveys (CSUR)*, *41*(3),
    pp.1–52.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Günther, L.C., Colangelo, E., Wiendahl, H.H. and Bauer, C., (2019). Data
    quality assessment for improved decision-making: a methodology for small and medium-sized
    enterprises. *Procedia Manufacturing*, *29*, pp.583–591.'
  prefs: []
  type: TYPE_NORMAL
