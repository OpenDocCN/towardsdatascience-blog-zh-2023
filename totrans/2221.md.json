["```py\n# Move to the airflow directory.\ncd airflow\n\n# Make expected directories and environment variables\nmkdir -p ./logs ./plugins\nsudo chmod 777 ./logs ./plugins\n\n# It will be used by Airflow to identify your user.\necho -e \"AIRFLOW_UID=$(id -u)\" > .env\n# This shows where the project root directory is located.\necho \"ML_PIPELINE_ROOT_DIR=/opt/airflow/dags\" >> .env\n```", "```py\n# Install dependencies.\nsudo apt install -y apache2-utils\npip install passlib\n\n# Create the credentials under the energy-forecasting name.\nmkdir ~/.htpasswd\nhtpasswd -sc ~/.htpasswd/htpasswd.txt energy-forecasting\n```", "```py\npoetry config repositories.my-pypi http://localhost\npoetry config http-basic.my-pypi energy-forecasting <password>\n```", "```py\ncat ~/.config/pypoetry/auth.toml\n```", "```py\n my-private-pypi:\n    image: pypiserver/pypiserver:latest\n    restart: always\n    ports:\n      - \"80:8080\"\n    volumes:\n      - ~/.htpasswd:/data/.htpasswd\n    command:\n      - run\n      - -P\n      - .htpasswd/htpasswd.txt\n      - --overwrite\n```", "```py\nrun -P .htpasswd/htpasswd.txt --overwrite\n```", "```py\nx-airflow-common:\n  &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.5.2}\n```", "```py\nversion: '3.8'\nx-airflow-common:\n  &airflow-common\n#  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.5.2}\n  build: .\n```", "```py\n# Go to the ./airflow directory.\ncd ./airflow\n\n# Initialize the Airflow database\ndocker compose up airflow-init\n\n# Start up all services\n# Note: You should set up the private PyPi server credentials before running this command.\ndocker compose --env-file .env up --build -d\n```", "```py\npoetry config repositories.my-pypi http://localhost\npoetry config http-basic.my-pypi energy-forecasting <password>\n```", "```py\n!/bin/bash\n\n# Build and publish the feature-pipeline, training-pipeline, and batch-prediction-pipeline packages.\n# This is done so that the pipelines can be run from the CLI.\n# The pipelines are executed in the feature-pipeline, training-pipeline, and batch-prediction-pipeline\n# directories, so we must change directories before building and publishing the packages.\n# The my-pypi repository must be defined in the project's poetry.toml file.\n\ncd feature-pipeline\npoetry build\npoetry publish -r my-pypi\n\ncd ../training-pipeline\npoetry build\npoetry publish -r my-pypi\n\ncd ../batch-prediction-pipeline\npoetry build\npoetry publish -r my-pypi\n```", "```py\npoetry build\npoetry publish -r my-pypi\n```", "```py\n feature_pipeline_metadata = run_feature_pipeline(\n        export_end_reference_datetime=\"{{ dag_run.logical_date }}\",\n    )\n```", "```py\n>> if_run_hyperparameter_tuning_branch\n>> [\n    if_run_hyperparameter_tuning_branch\n    >> Label(\"Run HPO\")\n    >> branch_run_hyperparameter_tuning_operator\n    >> last_sweep_metadata\n    >> upload_best_model_step,\n    if_run_hyperparameter_tuning_branch\n    >> Label(\"Skip HPO\")\n    >> branch_skip_hyperparameter_tuning_operator,\n]\n```", "```py\ndocker ps\n```", "```py\ndocker exec -it <container-id-of-airflow-webserver> sh\n# In this example, you did a backfill between 2023/04/11 00:00:00 and 2023/04/13 23:59:59.\nairflow dags backfill --start-date \"2023/04/11 00:00:00\" --end-date \"2023/04/13 23:59:59\" ml_pipeline\n```", "```py\ndocker exec -it <container-id-of-airflow-airflow-webserver> sh\nairflow tasks clear --start-date \"2023/04/11 00:00:00\" --end-date \"2023/04/13 23:59:59\" ml_pipeline\n```"]