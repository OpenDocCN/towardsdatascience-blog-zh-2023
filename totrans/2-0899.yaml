- en: Fine-Tune Your Own Llama 2 Model in a Colab Notebook
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨Colabç¬”è®°æœ¬ä¸­å¾®è°ƒä½ è‡ªå·±çš„Llama 2æ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32)
- en: A practical introduction to LLM fine-tuning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç”¨çš„LLMå¾®è°ƒä»‹ç»
- en: '[](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)[](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)[](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----df9823a04a32--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------)
    Â·12 min readÂ·Jul 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----df9823a04a32--------------------------------)
    Â·12åˆ†é’Ÿé˜…è¯»Â·2023å¹´7æœˆ25æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/deab49c0869889d83573906da9f2c40b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/deab49c0869889d83573906da9f2c40b.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: 'With the release of LLaMA v1, we saw a Cambrian explosion of fine-tuned models,
    including [Alpaca](https://github.com/tatsu-lab/stanford_alpaca), [Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.3),
    and [WizardLM](https://huggingface.co/WizardLM/WizardLM-13B-V1.1), among others.
    This trend encouraged different businesses to launch their own base models with
    licenses suitable for commercial use, such as [OpenLLaMA](https://github.com/openlm-research/open_llama),
    [Falcon](https://falconllm.tii.ae/), [XGen](https://github.com/salesforce/xgen),
    etc. The release of Llama 2 now combines the best elements from both sides: it
    offers a **highly efficient base model along with a more permissive license**.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€LLaMA v1çš„å‘å¸ƒï¼Œæˆ‘ä»¬è§è¯äº†å¾®è°ƒæ¨¡å‹çš„å¯’æ­¦çºªå¤§çˆ†å‘ï¼ŒåŒ…æ‹¬[Alpaca](https://github.com/tatsu-lab/stanford_alpaca)ã€[Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.3)å’Œ[WizardLM](https://huggingface.co/WizardLM/WizardLM-13B-V1.1)ç­‰ã€‚è¿™ä¸€è¶‹åŠ¿ä¿ƒä½¿ä¸åŒçš„å…¬å¸æ¨å‡ºäº†é€‚åˆå•†ä¸šä½¿ç”¨çš„åŸºç¡€æ¨¡å‹è®¸å¯è¯ï¼Œå¦‚[OpenLLaMA](https://github.com/openlm-research/open_llama)ã€[Falcon](https://falconllm.tii.ae/)å’Œ[XGen](https://github.com/salesforce/xgen)ç­‰ã€‚Llama
    2çš„å‘å¸ƒç°å°†ä¸¤è€…çš„æœ€ä½³å…ƒç´ ç»“åˆèµ·æ¥ï¼šæä¾›äº†**é«˜æ•ˆçš„åŸºç¡€æ¨¡å‹å’Œæ›´å®½æ¾çš„è®¸å¯è¯**ã€‚
- en: 'During the first half of 2023, the software landscape was significantly shaped
    by the **widespread use of APIs** (like OpenAI API) to create infrastructures
    based on Large Language Models (LLMs). Libraries such as [LangChain](https://python.langchain.com/docs/get_started/introduction.html)
    and [LlamaIndex](https://www.llamaindex.ai/) played a critical role in this trend.
    Moving into the latter half of the year, the process of **fine-tuning (or instruction
    tuning) these models is set to become a standard procedure** in the LLMOps workflow.
    This trend is driven by various factors: the potential for cost savings, the ability
    to process confidential data, and even the potential to develop models that exceed
    the performance of prominent models like ChatGPT and GPT-4 in certain specific
    tasks.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨2023å¹´ä¸ŠåŠå¹´ï¼Œè½¯ä»¶é¢†åŸŸå—åˆ°äº†**APIçš„å¹¿æ³›ä½¿ç”¨**ï¼ˆå¦‚OpenAI APIï¼‰çš„é‡å¤§å½±å“ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ„å»ºåŸºç¡€è®¾æ–½ã€‚[LangChain](https://python.langchain.com/docs/get_started/introduction.html)å’Œ[LlamaIndex](https://www.llamaindex.ai/)ç­‰åº“åœ¨è¿™ä¸€è¶‹åŠ¿ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚è¿›å…¥ä¸‹åŠå¹´ï¼Œ**å¾®è°ƒï¼ˆæˆ–æŒ‡ä»¤è°ƒä¼˜ï¼‰è¿™äº›æ¨¡å‹å°†æˆä¸ºLLMOpså·¥ä½œæµä¸­çš„æ ‡å‡†æµç¨‹**ã€‚è¿™ä¸€è¶‹åŠ¿å—åˆ°å¤šç§å› ç´ æ¨åŠ¨ï¼šèŠ‚çœæˆæœ¬çš„æ½œåŠ›ã€å¤„ç†æœºå¯†æ•°æ®çš„èƒ½åŠ›ï¼Œç”šè‡³æ˜¯å¼€å‘è¶…è¶ŠChatGPTå’ŒGPT-4ç­‰æ˜¾è‘—æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„è¡¨ç°çš„æ¨¡å‹çš„æ½œåŠ›ã€‚
- en: In this article, we will see why instruction tuning works and how to implement
    it in a Google Colab notebook to create your own Llama 2 model. As usual, the
    code is available on [Colab](https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing)
    and [GitHub](https://github.com/mlabonne/llm-course).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä¸ºä»€ä¹ˆæŒ‡ä»¤è°ƒä¼˜æœ‰æ•ˆï¼Œä»¥åŠå¦‚ä½•åœ¨Google Colabç¬”è®°æœ¬ä¸­å®ç°å®ƒï¼Œä»¥åˆ›å»ºä½ è‡ªå·±çš„Llama 2æ¨¡å‹ã€‚ä¸å¾€å¸¸ä¸€æ ·ï¼Œä»£ç å¯åœ¨[Colab](https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing)å’Œ[GitHub](https://github.com/mlabonne/llm-course)ä¸Šæ‰¾åˆ°ã€‚
- en: '**ğŸ”§** Background on fine-tuning LLMs'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ğŸ”§** å¾®è°ƒLLMsçš„èƒŒæ™¯'
- en: '![](../Images/a0c0ab9d87266afc92a4a41531b2f70f.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0c0ab9d87266afc92a4a41531b2f70f.png)'
- en: Image by author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: LLMs are pretrained on an extensive corpus of text. In the case of [Llama 2](https://arxiv.org/abs/2307.09288),
    we know very little about the composition of the training set, besides its length
    of 2 trillion tokens. In comparison, [BERT](https://arxiv.org/abs/1810.04805)
    (2018) was â€œonlyâ€ trained on the BookCorpus (800M words) and English Wikipedia
    (2,500M words). From experience, this is a **very costly and long process** with
    a lot of hardware issues. If you want to know more about it, I recommend reading
    [Metaâ€™s logbook](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf)
    about the pretraining of the OPT-175B model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLMsåœ¨å¤§é‡æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚ä»¥[Llama 2](https://arxiv.org/abs/2307.09288)ä¸ºä¾‹ï¼Œæˆ‘ä»¬å¯¹è®­ç»ƒé›†çš„ç»„æˆçŸ¥ä¹‹ç”šå°‘ï¼Œé™¤äº†å®ƒæœ‰2ä¸‡äº¿ä¸ªæ ‡è®°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ[BERT](https://arxiv.org/abs/1810.04805)ï¼ˆ2018å¹´ï¼‰â€œä»…â€åœ¨BookCorpusï¼ˆ8äº¿è¯ï¼‰å’Œè‹±è¯­ç»´åŸºç™¾ç§‘ï¼ˆ25äº¿è¯ï¼‰ä¸Šè¿›è¡Œè¿‡è®­ç»ƒã€‚ä»ç»éªŒæ¥çœ‹ï¼Œè¿™**æ˜¯ä¸€ä¸ªéå¸¸æ˜‚è´µä¸”æ¼«é•¿çš„è¿‡ç¨‹**ï¼Œå¹¶ä¸”æœ‰å¾ˆå¤šç¡¬ä»¶é—®é¢˜ã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šï¼Œæˆ‘æ¨èé˜…è¯»[Metaçš„æ—¥å¿—](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf)ï¼Œäº†è§£OPT-175Bæ¨¡å‹çš„é¢„è®­ç»ƒæƒ…å†µã€‚
- en: 'When the pretraining is complete, auto-regressive models like Llama 2 can **predict
    the next token** in a sequence. However, this does not make them particularly
    useful assistants since they donâ€™t reply to instructions. This is why we employ
    instruction tuning to align their answers with what humans expect. There are two
    main fine-tuning techniques:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å½“é¢„è®­ç»ƒå®Œæˆåï¼ŒåƒLlama 2è¿™æ ·çš„è‡ªå›å½’æ¨¡å‹å¯ä»¥**é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°**ã€‚ç„¶è€Œï¼Œè¿™å¹¶æ²¡æœ‰ä½¿å®ƒä»¬æˆä¸ºç‰¹åˆ«æœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå› ä¸ºå®ƒä»¬ä¸ä¼šå¯¹æŒ‡ä»¤ä½œå‡ºå›åº”ã€‚è¿™å°±æ˜¯æˆ‘ä»¬é‡‡ç”¨æŒ‡ä»¤è°ƒä¼˜æ¥ä½¿å®ƒä»¬çš„å›ç­”ä¸äººç±»æœŸæœ›å¯¹é½çš„åŸå› ã€‚ä¸»è¦æœ‰ä¸¤ç§å¾®è°ƒæŠ€æœ¯ï¼š
- en: '**Supervised Fine-Tuning** (SFT): Models are trained on a dataset of instructions
    and responses. It adjusts the weights in the LLM to minimize the difference between
    the generated answers and ground-truth responses, acting as labels.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç›‘ç£å¾®è°ƒ**ï¼ˆSFTï¼‰ï¼šæ¨¡å‹åœ¨ä¸€ç»„æŒ‡ä»¤å’Œå“åº”çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚å®ƒè°ƒæ•´LLMä¸­çš„æƒé‡ï¼Œä»¥æœ€å°åŒ–ç”Ÿæˆçš„ç­”æ¡ˆä¸çœŸå®å“åº”ä¹‹é—´çš„å·®å¼‚ï¼Œä½œä¸ºæ ‡ç­¾ã€‚'
- en: '**Reinforcement Learning from Human Feedback** (RLHF): Models learn by interacting
    with their environment and receiving feedback. They are trained to maximize a
    reward signal (using [PPO](https://arxiv.org/abs/1707.06347)), which is often
    derived from human evaluations of model outputs.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¥è‡ªäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ **ï¼ˆRLHFï¼‰ï¼šæ¨¡å‹é€šè¿‡ä¸ç¯å¢ƒäº’åŠ¨å¹¶æ¥æ”¶åé¦ˆæ¥å­¦ä¹ ã€‚å®ƒä»¬è¢«è®­ç»ƒä»¥æœ€å¤§åŒ–å¥–åŠ±ä¿¡å·ï¼ˆä½¿ç”¨[PPO](https://arxiv.org/abs/1707.06347)ï¼‰ï¼Œè¿™ä¸ªä¿¡å·é€šå¸¸æ¥æºäºäººç±»å¯¹æ¨¡å‹è¾“å‡ºçš„è¯„ä¼°ã€‚'
- en: In general, RLHF is shown to capture **more complex and nuanced** human preferences,
    but is also more challenging to implement effectively. Indeed, it requires careful
    design of the reward system and can be sensitive to the quality and consistency
    of human feedback. A possible alternative in the future is the [Direct Preference
    Optimization](https://arxiv.org/abs/2305.18290) (DPO) algorithm, which directly
    runs preference learning on the SFT model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼ŒRLHFè¢«è¯æ˜èƒ½å¤Ÿæ•æ‰åˆ°**æ›´å¤æ‚å’Œç»†è‡´çš„**äººç±»åå¥½ï¼Œä½†ä¹Ÿæ›´éš¾æœ‰æ•ˆå®æ–½ã€‚ç¡®å®ï¼Œå®ƒéœ€è¦ç²¾å¿ƒè®¾è®¡å¥–åŠ±ç³»ç»Ÿï¼Œå¹¶ä¸”å¯¹äººç±»åé¦ˆçš„è´¨é‡å’Œä¸€è‡´æ€§è¾ƒä¸ºæ•æ„Ÿã€‚æœªæ¥çš„ä¸€ä¸ªå¯èƒ½æ›¿ä»£æ–¹æ¡ˆæ˜¯[ç›´æ¥åå¥½ä¼˜åŒ–](https://arxiv.org/abs/2305.18290)ï¼ˆDPOï¼‰ç®—æ³•ï¼Œå®ƒç›´æ¥åœ¨SFTæ¨¡å‹ä¸Šè¿è¡Œåå¥½å­¦ä¹ ã€‚
- en: 'In our case, we will perform SFT, but this raises a question: why does fine-tuning
    work in the first place? As highlighted in the [Orca paper](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/orca.html),
    our understanding is that fine-tuning **leverages knowledge learned during the
    pretraining** process. In other words, fine-tuning will be of little help if the
    model has never seen the kind of data youâ€™re interested in. However, if thatâ€™s
    the case, SFT can be extremely performant.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†æ‰§è¡ŒSFTï¼Œä½†è¿™æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šä¸ºä»€ä¹ˆå¾®è°ƒåœ¨ç¬¬ä¸€æ—¶é—´ä¼šæœ‰æ•ˆï¼Ÿæ­£å¦‚[Orcaè®ºæ–‡](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/orca.html)ä¸­æ‰€å¼ºè°ƒçš„ï¼Œæˆ‘ä»¬çš„ç†è§£æ˜¯å¾®è°ƒ**åˆ©ç”¨äº†åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—çš„çŸ¥è¯†**ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœæ¨¡å‹ä»æœªè§è¿‡ä½ æ„Ÿå…´è¶£çš„æ•°æ®ç±»å‹ï¼Œå¾®è°ƒå°†æ— æµäºäº‹ã€‚ç„¶è€Œï¼Œå¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼ŒSFTå¯ä»¥è¡¨ç°å¾—æå…¶ä¼˜ç§€ã€‚
- en: For example, the [LIMA paper](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/lima.html)
    showed how you could outperform GPT-3 (DaVinci003) by fine-tuning a LLaMA (v1)
    model with 65 billion parameters on only 1,000 high-quality samples. The **quality
    of the instruction dataset is essential** to reach this level of performance,
    which is why a lot of work is focused on this issue (like [evol-instruct](https://arxiv.org/abs/2304.12244),
    Orca, or [phi-1](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/phi1.html)).
    Note that the size of the LLM (65b, not 13b or 7b) is also fundamental to leverage
    pre-existing knowledge efficiently.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œ[LIMAè®ºæ–‡](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/lima.html)å±•ç¤ºäº†å¦‚ä½•é€šè¿‡åœ¨ä»…1,000ä¸ªé«˜è´¨é‡æ ·æœ¬ä¸Šå¾®è°ƒä¸€ä¸ªå…·æœ‰65äº¿å‚æ•°çš„LLaMAï¼ˆv1ï¼‰æ¨¡å‹æ¥è¶…è¶ŠGPT-3ï¼ˆDaVinci003ï¼‰ã€‚**æŒ‡ä»¤æ•°æ®é›†çš„è´¨é‡è‡³å…³é‡è¦**ï¼Œä»¥è¾¾åˆ°è¿™ç§æ€§èƒ½æ°´å¹³ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆè®¸å¤šå·¥ä½œé›†ä¸­åœ¨è¿™ä¸ªé—®é¢˜ä¸Šï¼ˆå¦‚[evol-instruct](https://arxiv.org/abs/2304.12244)ã€Orcaæˆ–[phi-1](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/phi1.html)ï¼‰ã€‚è¯·æ³¨æ„ï¼ŒLLMçš„å¤§å°ï¼ˆ65bï¼Œè€Œä¸æ˜¯13bæˆ–7bï¼‰å¯¹æœ‰æ•ˆåˆ©ç”¨å·²æœ‰çŸ¥è¯†ä¹Ÿæ˜¯è‡³å…³é‡è¦çš„ã€‚
- en: 'Another important point related to the data quality is the **prompt template**.
    Prompts are comprised of similar elements: system prompt (optional) to guide the
    model, user prompt (required) to give the instruction, additional inputs (optional)
    to take into consideration, and the modelâ€™s answer (required). In the case of
    Llama 2, the authors used the following template for the **chat models**:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ•°æ®è´¨é‡ç›¸å…³çš„å¦ä¸€ä¸ªé‡è¦ç‚¹æ˜¯**æç¤ºæ¨¡æ¿**ã€‚æç¤ºç”±ç±»ä¼¼çš„å…ƒç´ ç»„æˆï¼šç”¨äºæŒ‡å¯¼æ¨¡å‹çš„ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºæä¾›æŒ‡ä»¤çš„ç”¨æˆ·æç¤ºï¼ˆå¿…éœ€ï¼‰ï¼Œéœ€è¦è€ƒè™‘çš„é¢å¤–è¾“å…¥ï¼ˆå¯é€‰ï¼‰ï¼Œä»¥åŠæ¨¡å‹çš„å›ç­”ï¼ˆå¿…éœ€ï¼‰ã€‚åœ¨Llama
    2çš„æƒ…å†µä¸‹ï¼Œä½œè€…ä½¿ç”¨äº†ä»¥ä¸‹**èŠå¤©æ¨¡å‹**çš„æ¨¡æ¿ï¼š
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There are other templates, like the ones from Alpaca and Vicuna, and their impact
    is not very clear. In this example, we will reformat our instruction dataset to
    follow Llama 2â€™s template. For the purpose of this tutorial, Iâ€™ve already done
    it using the excellent `[timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)`
    dataset. You can find it on Hugging Face under the name `[mlabonne/guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k)`.
    Note that you donâ€™t need to follow a specific prompt template if youâ€™re using
    the base Llama 2 model instead of the chat version.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å…¶ä»–æ¨¡æ¿ï¼Œæ¯”å¦‚æ¥è‡ªAlpacaå’ŒVicunaçš„æ¨¡æ¿ï¼Œå®ƒä»¬çš„å½±å“è¿˜ä¸å¤ªæ˜ç¡®ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†é‡æ–°æ ¼å¼åŒ–æˆ‘ä»¬çš„æŒ‡ä»¤æ•°æ®é›†ä»¥ç¬¦åˆLlama 2çš„æ¨¡æ¿ã€‚ä¸ºäº†æœ¬æ•™ç¨‹çš„ç›®çš„ï¼Œæˆ‘å·²ç»ä½¿ç”¨äº†ä¼˜ç§€çš„`[timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)`æ•°æ®é›†ã€‚ä½ å¯ä»¥åœ¨Hugging
    Faceä¸Šæ‰¾åˆ°å®ƒï¼Œåç§°ä¸º`[mlabonne/guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k)`ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯åŸºç¡€ç‰ˆLlama
    2æ¨¡å‹è€Œä¸æ˜¯èŠå¤©ç‰ˆæœ¬ï¼Œä½ ä¸éœ€è¦éµå¾ªç‰¹å®šçš„æç¤ºæ¨¡æ¿ã€‚
- en: ğŸ¦™ How to fine-tune Llama 2
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦™ å¦‚ä½•å¾®è°ƒLlama 2
- en: 'In this section, we will fine-tune a Llama 2 model with 7 billion parameters
    on a T4 GPU with high RAM using Google Colab (2.21 credits/hour). Note that a
    T4 only has 16 GB of VRAM, which is barely enough to **store Llama 2â€“7bâ€™s weights**
    (7b Ã— 2 bytes = 14 GB in FP16). In addition, we need to consider the overhead
    due to optimizer states, gradients, and forward activations (see [this excellent
    article](https://huggingface.co/docs/transformers/perf_train_gpu_one#anatomy-of-models-memory)
    for more information). This means that a full fine-tuning is not possible here:
    we need parameter-efficient fine-tuning (PEFT) techniques like [LoRA](https://arxiv.org/abs/2106.09685)
    or [QLoRA](https://arxiv.org/abs/2305.14314).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Google Colabï¼ˆ2.21ä¿¡ç”¨/å°æ—¶ï¼‰åœ¨é…å¤‡é«˜RAMçš„T4 GPUä¸Šå¾®è°ƒä¸€ä¸ªå…·æœ‰70äº¿å‚æ•°çš„Llama 2æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼ŒT4çš„VRAMä»…ä¸º16
    GBï¼Œè¿™å‹‰å¼ºè¶³å¤Ÿ**å­˜å‚¨Llama 2â€“7bçš„æƒé‡**ï¼ˆ7b Ã— 2å­—èŠ‚ = 14 GBçš„FP16ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è€ƒè™‘ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‰å‘æ¿€æ´»çš„å¼€é”€ï¼ˆæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è¿™ç¯‡ä¼˜ç§€çš„æ–‡ç« ](https://huggingface.co/docs/transformers/perf_train_gpu_one#anatomy-of-models-memory)ï¼‰ã€‚è¿™æ„å‘³ç€åœ¨è¿™é‡Œè¿›è¡Œå…¨é¢å¾®è°ƒæ˜¯ä¸å¯èƒ½çš„ï¼šæˆ‘ä»¬éœ€è¦åƒ[LoRA](https://arxiv.org/abs/2106.09685)æˆ–[QLoRA](https://arxiv.org/abs/2305.14314)è¿™æ ·çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯ã€‚
- en: To drastically reduce the VRAM usage, we must **fine-tune the model in 4-bit
    precision**, which is why weâ€™ll use QLoRA here. The good thing is that we can
    leverage the Hugging Face ecosystem with the `transformers`, `accelerate`, `peft`,
    `trl`, and `bitsandbytes` libraries. We'll do this in the following code based
    on Younes Belkada's [GitHub Gist](https://gist.github.com/younesbelkada/9f7f75c94bdc1981c8ca5cc937d4a4da).
    First, we install and load these libraries.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¤§å¹…å‡å°‘VRAMä½¿ç”¨é‡ï¼Œæˆ‘ä»¬å¿…é¡»**ç”¨4ä½ç²¾åº¦å¾®è°ƒæ¨¡å‹**ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨QLoRAçš„åŸå› ã€‚å¥½æ¶ˆæ¯æ˜¯æˆ‘ä»¬å¯ä»¥åˆ©ç”¨Hugging Faceç”Ÿæ€ç³»ç»Ÿä¸­çš„`transformers`ã€`accelerate`ã€`peft`ã€`trl`å’Œ`bitsandbytes`åº“ã€‚æˆ‘ä»¬å°†åœ¨ä»¥ä¸‹ä»£ç ä¸­è¿›è¡Œæ“ä½œï¼ŒåŸºäºYounes
    Belkadaçš„[GitHub Gist](https://gist.github.com/younesbelkada/9f7f75c94bdc1981c8ca5cc937d4a4da)ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å®‰è£…å¹¶åŠ è½½è¿™äº›åº“ã€‚
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Letâ€™s talk a bit about the parameters we can tune here. First, we want to load
    a `llama-2-7b-chat-hf` model (**chat** model) and train it on the `mlabonne/guanaco-llama2-1k`
    (1,000 samples), which will produce our fine-tuned model `llama-2-7b-miniguanaco`.
    If youâ€™re interested in how this dataset was created, you can check [this notebook](https://colab.research.google.com/drive/1Ad7a9zMmkxuXTOh1Z7-rNSICA4dybpM2?usp=sharing).
    Feel free to change it: there are many good datasets on the [Hugging Face Hub](https://huggingface.co/datasets),
    like `[databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)`.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç¨å¾®è®¨è®ºä¸€ä¸‹å¯ä»¥åœ¨è¿™é‡Œè°ƒæ•´çš„å‚æ•°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¸Œæœ›åŠ è½½ä¸€ä¸ª`llama-2-7b-chat-hf`æ¨¡å‹ï¼ˆ**chat** æ¨¡å‹ï¼‰ï¼Œå¹¶åœ¨`mlabonne/guanaco-llama2-1k`ï¼ˆ1,000
    ä¸ªæ ·æœ¬ï¼‰ä¸Šè®­ç»ƒå®ƒï¼Œè¿™å°†ç”Ÿæˆæˆ‘ä»¬å¾®è°ƒåçš„æ¨¡å‹`llama-2-7b-miniguanaco`ã€‚å¦‚æœä½ å¯¹å¦‚ä½•åˆ›å»ºè¿™ä¸ªæ•°æ®é›†æ„Ÿå…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹[è¿™ä¸ªç¬”è®°æœ¬](https://colab.research.google.com/drive/1Ad7a9zMmkxuXTOh1Z7-rNSICA4dybpM2?usp=sharing)ã€‚å¯ä»¥éšæ„æ›´æ”¹ï¼šåœ¨[Hugging
    Face Hub](https://huggingface.co/datasets)ä¸Šæœ‰å¾ˆå¤šå¥½çš„æ•°æ®é›†ï¼Œæ¯”å¦‚`[databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)`ã€‚
- en: QLoRA will use a rank of 64 with a scaling parameter of 16 (see [this article](https://rentry.org/llm-training#low-rank-adaptation-lora_1)
    for more information about LoRA parameters). Weâ€™ll load the Llama 2 model directly
    in 4-bit precision using the NF4 type and train it for one epoch. To get more
    information about the other parameters, check the [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments),
    [PeftModel](https://huggingface.co/docs/peft/package_reference/peft_model), and
    [SFTTrainer](https://huggingface.co/docs/trl/main/en/sft_trainer) documentation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA å°†ä½¿ç”¨ 64 çš„ç§©å’Œ 16 çš„ç¼©æ”¾å‚æ•°ï¼ˆæœ‰å…³ LoRA å‚æ•°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è¿™ç¯‡æ–‡ç« ](https://rentry.org/llm-training#low-rank-adaptation-lora_1)ï¼‰ã€‚æˆ‘ä»¬å°†ä½¿ç”¨
    NF4 ç±»å‹ä»¥ 4 ä½ç²¾åº¦ç›´æ¥åŠ è½½ Llama 2 æ¨¡å‹ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªå‘¨æœŸã€‚æœ‰å…³å…¶ä»–å‚æ•°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)ã€[PeftModel](https://huggingface.co/docs/peft/package_reference/peft_model)å’Œ[SFTTrainer](https://huggingface.co/docs/trl/main/en/sft_trainer)
    æ–‡æ¡£ã€‚
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can now load everything and start the fine-tuning process. Weâ€™re relying
    on multiple wrappers, so bear with me.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥åŠ è½½æ‰€æœ‰å†…å®¹å¹¶å¼€å§‹å¾®è°ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬ä¾èµ–å¤šä¸ªåŒ…è£…å™¨ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚
- en: First of all, we want to **load the dataset** we defined. Here, our dataset
    is already preprocessed but, usually, this is where you would reformat the prompt,
    filter out bad text, combine multiple datasets, etc.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬è¦**åŠ è½½æˆ‘ä»¬å®šä¹‰çš„æ•°æ®é›†**ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çš„æ•°æ®é›†å·²ç»è¿‡é¢„å¤„ç†ï¼Œä½†é€šå¸¸ï¼Œè¿™æ—¶ä½ ä¼šé‡æ–°æ ¼å¼åŒ–æç¤ºã€ç­›é€‰æ‰ä¸è‰¯æ–‡æœ¬ã€åˆå¹¶å¤šä¸ªæ•°æ®é›†ç­‰ã€‚
- en: Then, weâ€™re configuring `bitsandbytes` for 4-bit quantization.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æ­£åœ¨é…ç½® `bitsandbytes` ä»¥è¿›è¡Œ 4 ä½é‡åŒ–ã€‚
- en: Next, we're loading the Llama 2 model in 4-bit precision on a GPU with the corresponding
    tokenizer.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ç›¸åº”çš„åˆ†è¯å™¨å°† Llama 2 æ¨¡å‹ä»¥ 4 ä½ç²¾åº¦åŠ è½½åˆ° GPU ä¸Šã€‚
- en: Finally, we're loading configurations for QLoRA, regular training parameters,
    and passing everything to the `SFTTrainer`. The training can finally start!
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬æ­£åœ¨åŠ è½½ QLoRA é…ç½®ã€å¸¸è§„è®­ç»ƒå‚æ•°ï¼Œå¹¶å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™`SFTTrainer`ã€‚è®­ç»ƒå¯ä»¥æœ€ç»ˆå¼€å§‹ï¼
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/7a4459b066e68e7cd3002cd3113c8876.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a4459b066e68e7cd3002cd3113c8876.png)'
- en: Image by author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: 'The training can be very long, depending on the size of your dataset. Here,
    it took less than an hour on a T4 GPU. We can check the plots on tensorboard,
    as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¯èƒ½ä¼šå¾ˆé•¿æ—¶é—´ï¼Œè¿™å–å†³äºæ•°æ®é›†çš„å¤§å°ã€‚åœ¨è¿™é‡Œï¼ŒT4 GPU ä¸ŠèŠ±äº†ä¸åˆ°ä¸€ä¸ªå°æ—¶ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ tensorboard ä¸Šæ£€æŸ¥å›¾è¡¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/0961e701ded34f0fb82b7c66fcdad85f.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0961e701ded34f0fb82b7c66fcdad85f.png)'
- en: Image by author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Letâ€™s make sure that the model is behaving correctly. It would require a more
    exhaustive evaluation, but we can use the **text generation pipeline** to ask
    questions like â€œWhat is a large language model?â€ Note that Iâ€™m formatting the
    input to match Llama 2â€™s prompt template.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç¡®ä¿æ¨¡å‹è¡Œä¸ºæ­£ç¡®ã€‚è¿™éœ€è¦æ›´å…¨é¢çš„è¯„ä¼°ï¼Œä½†æˆ‘ä»¬å¯ä»¥ä½¿ç”¨**æ–‡æœ¬ç”Ÿæˆç®¡é“**æ¥æå‡ºè¯¸å¦‚â€œä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿâ€è¿™æ ·çš„é—®é¢˜ã€‚è¯·æ³¨æ„ï¼Œæˆ‘æ­£åœ¨æ ¼å¼åŒ–è¾“å…¥ä»¥åŒ¹é…
    Llama 2 çš„æç¤ºæ¨¡æ¿ã€‚
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The model outputs the following response:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¾“å‡ºäº†ä»¥ä¸‹å“åº”ï¼š
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: From experience, it is **very coherent** for a model with only 7 billion parameters.
    You can play with it and ask harder questions from evaluation datasets like [BigBench-Hard](https://github.com/suzgunmirac/BIG-Bench-Hard).
    Guanaco is an excellent dataset that has produced high-quality models in the past.
    You can train a Llama 2 model on the entire dataset using `[mlabonne/guanaco-llama2](https://huggingface.co/datasets/mlabonne/guanaco-llama2)`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ç»éªŒï¼Œå¯¹äºåªæœ‰ 70 äº¿å‚æ•°çš„æ¨¡å‹æ¥è¯´ï¼Œå®ƒ**éå¸¸ä¸€è‡´**ã€‚ä½ å¯ä»¥å°è¯•ä½¿ç”¨è¯„ä¼°æ•°æ®é›†ä¸­çš„æ›´éš¾é—®é¢˜ï¼Œå¦‚[BigBench-Hard](https://github.com/suzgunmirac/BIG-Bench-Hard)ã€‚Guanaco
    æ˜¯ä¸€ä¸ªä¼˜ç§€çš„æ•°æ®é›†ï¼Œä»¥å‰ç”Ÿäº§è¿‡é«˜è´¨é‡çš„æ¨¡å‹ã€‚ä½ å¯ä»¥ä½¿ç”¨`[mlabonne/guanaco-llama2](https://huggingface.co/datasets/mlabonne/guanaco-llama2)`åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒ
    Llama 2 æ¨¡å‹ã€‚
- en: 'How can we store our new `llama-2-7b-miniguanaco` model now? We need to merge
    the weights from LoRA with the base model. Unfortunately, as far as I know, there
    is no straightforward way to do it: we need to reload the base model in FP16 precision
    and use the `peft` library to merge everything. Alas, it also creates a problem
    with the VRAM (despite emptying it), so I recommend **restarting the notebook**,
    re-executing the three first cells, and then executing the next one. Please contact
    me if you know a fix!'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¦‚ä½•å­˜å‚¨æˆ‘ä»¬æ–°çš„ `llama-2-7b-miniguanaco` æ¨¡å‹ï¼Ÿæˆ‘ä»¬éœ€è¦å°† LoRA çš„æƒé‡ä¸åŸºç¡€æ¨¡å‹åˆå¹¶ã€‚ä¸å¹¸çš„æ˜¯ï¼Œæ®æˆ‘æ‰€çŸ¥ï¼Œæ²¡æœ‰ç®€å•çš„æ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼šæˆ‘ä»¬éœ€è¦ä»¥
    FP16 ç²¾åº¦é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ `peft` åº“æ¥åˆå¹¶æ‰€æœ‰å†…å®¹ã€‚å¯æƒœçš„æ˜¯ï¼Œè¿™ä¹Ÿä¼šå¼•å‘ VRAM çš„é—®é¢˜ï¼ˆå°½ç®¡å·²ç»æ¸…ç©ºï¼‰ï¼Œæ‰€ä»¥æˆ‘å»ºè®®**é‡å¯ç¬”è®°æœ¬**ï¼Œé‡æ–°æ‰§è¡Œå‰ä¸‰ä¸ªå•å…ƒæ ¼ï¼Œç„¶åæ‰§è¡Œä¸‹ä¸€ä¸ªå•å…ƒæ ¼ã€‚å¦‚æœä½ çŸ¥é“è§£å†³åŠæ³•ï¼Œè¯·è”ç³»æˆ‘ï¼
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Our weights are merged and we reloaded the tokenizer. We can now push everything
    to the Hugging Face Hub to save our model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æƒé‡å·²åˆå¹¶å¹¶é‡æ–°åŠ è½½äº†åˆ†è¯å™¨ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰å†…å®¹æ¨é€åˆ° Hugging Face Hub ä»¥ä¿å­˜æˆ‘ä»¬çš„æ¨¡å‹ã€‚
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can now use this model for inference by loading it like any other Llama
    2 model from the Hub. It is also possible to reload it for more fine-tuning â€”
    perhaps with another dataset?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä½ å¯ä»¥åƒåŠ è½½ Hub ä¸Šçš„å…¶ä»– Llama 2 æ¨¡å‹ä¸€æ ·ä½¿ç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œæ¨ç†ã€‚ä¹Ÿå¯ä»¥é‡æ–°åŠ è½½å®ƒä»¥è¿›è¡Œæ›´å¤šçš„å¾®è°ƒâ€”â€”ä¹Ÿè®¸ç”¨å¦ä¸€ä¸ªæ•°æ®é›†ï¼Ÿ
- en: If youâ€™re serious about fine-tuning models, **using a script** instead of a
    notebook is recommended. You can easily rent GPUs on Lambda Labs, Runpod, Vast.ai,
    for less than 0.3$/h. Once youâ€™re connected, you can install libraries, import
    your script, log in to Hugging Face and other tools (like Weights & Biases for
    logging your experiments), and start your fine-tuning.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è®¤çœŸè€ƒè™‘å¾®è°ƒæ¨¡å‹ï¼Œå»ºè®®**ä½¿ç”¨è„šæœ¬**è€Œä¸æ˜¯ç¬”è®°æœ¬ã€‚ä½ å¯ä»¥åœ¨ Lambda Labsã€Runpodã€Vast.ai ä¸Šä»¥ä½äº 0.3$/å°æ—¶çš„ä»·æ ¼è½»æ¾ç§Ÿç”¨
    GPUã€‚ä¸€æ—¦è¿æ¥ï¼Œä½ å¯ä»¥å®‰è£…åº“ã€å¯¼å…¥è„šæœ¬ã€ç™»å½• Hugging Face å’Œå…¶ä»–å·¥å…·ï¼ˆå¦‚ Weights & Biases ç”¨äºè®°å½•å®éªŒï¼‰ï¼Œç„¶åå¼€å§‹å¾®è°ƒã€‚
- en: The `trl` script is currently very limited, so I made my own based on the previous
    notebook. You can [**find it here on GitHub Gist**](https://gist.github.com/mlabonne/8eb9ad60c6340cb48a17385c68e3b1a5).
    If youâ€™re looking for a comprehensive solution, check out [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)
    from the OpenAccess AI Collective, which also natively handles multiple datasets,
    Deepspeed, Flash Attention, etc.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`trl` è„šæœ¬ç›®å‰éå¸¸æœ‰é™ï¼Œå› æ­¤æˆ‘åŸºäºä¹‹å‰çš„ç¬”è®°æœ¬åˆ¶ä½œäº†è‡ªå·±çš„ç‰ˆæœ¬ã€‚ä½ å¯ä»¥åœ¨[**GitHub Gist ä¸Šæ‰¾åˆ°å®ƒ**](https://gist.github.com/mlabonne/8eb9ad60c6340cb48a17385c68e3b1a5)ã€‚å¦‚æœä½ åœ¨å¯»æ‰¾å…¨é¢çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥æŸ¥çœ‹æ¥è‡ª
    OpenAccess AI Collective çš„ [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)ï¼Œå®ƒä¹ŸåŸç”Ÿæ”¯æŒå¤šä¸ªæ•°æ®é›†ã€Deepspeedã€Flash
    Attention ç­‰ã€‚'
- en: Conclusion
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we saw how to fine-tune a Llama 2 7b model using a Colab notebook.
    We introduced some necessary background on LLM training and fine-tuning, as well
    as important considerations related to instruction datasets. In the second section,
    we **successfully fine-tuned the Llama 2 model** with its native prompt template
    and custom parameters.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Colab ç¬”è®°æœ¬å¯¹ Llama 2 7b æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€äº› LLM è®­ç»ƒå’Œå¾®è°ƒçš„å¿…è¦èƒŒæ™¯ï¼Œä»¥åŠä¸æŒ‡ä»¤æ•°æ®é›†ç›¸å…³çš„é‡è¦è€ƒè™‘å› ç´ ã€‚åœ¨ç¬¬äºŒéƒ¨åˆ†ï¼Œæˆ‘ä»¬**æˆåŠŸåœ°å¾®è°ƒäº†
    Llama 2 æ¨¡å‹**ï¼Œä½¿ç”¨äº†å…¶åŸç”Ÿæç¤ºæ¨¡æ¿å’Œè‡ªå®šä¹‰å‚æ•°ã€‚
- en: These fine-tuned models can then be integrated into LangChain and other architectures
    as advantageous alternatives to the OpenAI API. Remember, in this new paradigm,
    instruction datasets are the new gold, and the quality of your model heavily depends
    on the data on which itâ€™s been fine-tuned. So, good luck with building high-quality
    datasets!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å¾®è°ƒåçš„æ¨¡å‹å¯ä»¥é›†æˆåˆ° LangChain å’Œå…¶ä»–æ¶æ„ä¸­ï¼Œä½œä¸º OpenAI API çš„æœ‰åˆ©æ›¿ä»£æ–¹æ¡ˆã€‚è®°ä½ï¼Œåœ¨è¿™ä¸ªæ–°èŒƒå¼ä¸­ï¼ŒæŒ‡ä»¤æ•°æ®é›†æ˜¯æ–°çš„é»„é‡‘ï¼Œä½ çš„æ¨¡å‹çš„è´¨é‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå…¶å¾®è°ƒçš„æ•°æ®ã€‚å› æ­¤ï¼Œç¥ä½ æ„å»ºé«˜è´¨é‡æ•°æ®é›†å¥½è¿ï¼
- en: If youâ€™re interested in more content about LLMs, follow me on Twitter [@maximelabonne](https://twitter.com/maximelabonne).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹æ›´å¤šå…³äº LLM çš„å†…å®¹æ„Ÿå…´è¶£ï¼Œå¯ä»¥åœ¨ Twitter ä¸Šå…³æ³¨æˆ‘ [@maximelabonne](https://twitter.com/maximelabonne)ã€‚
- en: References
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: 'Hugo Touvron, Thomas Scialom, et al. (2023). [Llama 2: Open Foundation and
    Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288).'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hugo Touvron, Thomas Scialom ç­‰äººï¼ˆ2023å¹´ï¼‰ã€‚[Llama 2: Open Foundation and Fine-Tuned
    Chat Models](https://arxiv.org/abs/2307.09288)ã€‚'
- en: Philipp Schmid, Omar Sanseviero, Pedro Cuenca, & Lewis Tunstall. Llama 2 is
    here â€” get it on Hugging Face. [https://huggingface.co/blog/llama2](https://huggingface.co/blog/llama2)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Philipp Schmid, Omar Sanseviero, Pedro Cuenca, & Lewis Tunstall. Llama 2 å·²ä¸Šçº¿â€”â€”åœ¨
    Hugging Face ä¸Šè·å–å®ƒã€‚ [https://huggingface.co/blog/llama2](https://huggingface.co/blog/llama2)
- en: 'Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
    Guestrin, Percy Liang, & Tatsunori B. Hashimoto. (2023). [Stanford Alpaca: An
    Instruction-following LLaMA model](https://crfm.stanford.edu/2023/03/13/alpaca.html).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç½—æ±‰Â·å¡”å¥¥é‡Œã€ä¼Šèæ©Â·å¤å°”æ‹‰è´¾å°¼ã€å¼ å¤©è‰ºã€æ‰¬Â·æœå¸ƒç“¦ã€æé›ªè¾°ã€å¡æ´›æ–¯Â·ç›–æ–¯ç‰¹æ—ã€ç€è¥¿Â·æ¢åŠè¾°é‡åšã€‚ (2023). [æ–¯å¦ç¦é˜¿å°”å¸•å¡ï¼šä¸€ç§éµå¾ªæŒ‡ä»¤çš„LLaMAæ¨¡å‹](https://crfm.stanford.edu/2023/03/13/alpaca.html)ã€‚
- en: 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, & Kristina Toutanova. (2019). [BERT:
    Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é›…å„å¸ƒÂ·å¾·å¤«æ—ã€æ˜ä¼ŸÂ·å¼ ã€è‚¯é¡¿Â·æåŠå…‹é‡Œæ–¯è’‚å¨œÂ·æ‰˜ç‰¹è¯ºå¨ƒã€‚ (2019). [BERTï¼šç”¨äºè¯­è¨€ç†è§£çš„æ·±åº¦åŒå‘å˜æ¢å™¨çš„é¢„è®­ç»ƒ](https://arxiv.org/abs/1810.04805)ã€‚
- en: 'Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, & Luke Zettlemoyer. (2023). [QLoRA:
    Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æå§†Â·å¾·ç‰¹é»˜æ–¯ã€é˜¿å°”è’‚å¤šç½—Â·å¸•å°¼å¥¥å°¼ã€é˜¿é‡ŒÂ·éœå°”èŒ¨æ›¼åŠå¢å…‹Â·æ³½ç‰¹å°”æ‘©è€¶ã€‚ (2023). [QLoRAï¼šé«˜æ•ˆå¾®è°ƒé‡åŒ– LLM](https://arxiv.org/abs/2305.14314)ã€‚
- en: Related articles
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›¸å…³æ–‡ç« 
- en: '[](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----df9823a04a32--------------------------------)
    [## 4-bit Quantization with GPTQ'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----df9823a04a32--------------------------------)
    [## ä½¿ç”¨ GPTQ è¿›è¡Œ 4 ä½é‡åŒ–'
- en: Quantize your own LLMs using AutoGPTQ
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ AutoGPTQ å¯¹è‡ªå·±çš„ LLM è¿›è¡Œé‡åŒ–
- en: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----df9823a04a32--------------------------------)
    [](/decoding-strategies-in-large-language-models-9733a8f70539?source=post_page-----df9823a04a32--------------------------------)
    [## Decoding Strategies in Large Language Models
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----df9823a04a32--------------------------------)
    [](/decoding-strategies-in-large-language-models-9733a8f70539?source=post_page-----df9823a04a32--------------------------------)
    [## å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§£ç ç­–ç•¥
- en: A Guide to Text Generation From Beam Search to Nucleus Sampling
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»æŸæœç´¢åˆ°æ ¸é‡‡æ ·çš„æ–‡æœ¬ç”ŸæˆæŒ‡å—
- en: towardsdatascience.com](/decoding-strategies-in-large-language-models-9733a8f70539?source=post_page-----df9823a04a32--------------------------------)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/decoding-strategies-in-large-language-models-9733a8f70539?source=post_page-----df9823a04a32--------------------------------)
- en: '*Learn more about machine learning and support my work with one click â€” become
    a Medium member here:*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*äº†è§£æ›´å¤šæœºå™¨å­¦ä¹ çŸ¥è¯†ï¼Œå¹¶é€šè¿‡ç‚¹å‡»æ”¯æŒæˆ‘çš„å·¥ä½œâ€”â€”æˆä¸º Medium ä¼šå‘˜ï¼š*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----df9823a04a32--------------------------------)
    [## Join Medium with my referral link - Maxime Labonne'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne/membership?source=post_page-----df9823a04a32--------------------------------)
    [## é€šè¿‡æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium - Maxime Labonne'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every storyâ€¦
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½œä¸º Medium ä¼šå‘˜ï¼Œä½ çš„ä¼šå‘˜è´¹çš„ä¸€éƒ¨åˆ†ä¼šæ”¯æŒä½ é˜…è¯»çš„ä½œè€…ï¼ŒåŒæ—¶ä½ å¯ä»¥å…¨é¢è®¿é—®æ¯ä¸€ä¸ªæ•…äº‹â€¦â€¦
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----df9823a04a32--------------------------------)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----df9823a04a32--------------------------------)
- en: '*If youâ€™re already a member, you can* [*follow me on Medium*](https://medium.com/@mlabonne)*.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ å·²ç»æ˜¯ä¼šå‘˜ï¼Œä½ å¯ä»¥* [*åœ¨ Medium ä¸Šå…³æ³¨æˆ‘*](https://medium.com/@mlabonne)*ã€‚*'
