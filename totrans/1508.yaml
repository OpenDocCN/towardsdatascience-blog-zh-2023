- en: Meta AI Introduces Revolutionary Image Segmentation Model Trained on 1 Billion
    Masks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/meta-ai-introduces-revolutionary-image-segmentation-model-trained-on-1-billion-masks-8f13c86a13a2](https://towardsdatascience.com/meta-ai-introduces-revolutionary-image-segmentation-model-trained-on-1-billion-masks-8f13c86a13a2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Segment Anything - Best DL Model for Image Segmentation.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gkeretchashvili?source=post_page-----8f13c86a13a2--------------------------------)[![Gurami
    Keretchashvili](../Images/4da78f113a0046c2deb8224e09dd9e3d.png)](https://medium.com/@gkeretchashvili?source=post_page-----8f13c86a13a2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8f13c86a13a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8f13c86a13a2--------------------------------)
    [Gurami Keretchashvili](https://medium.com/@gkeretchashvili?source=post_page-----8f13c86a13a2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8f13c86a13a2--------------------------------)
    ·7 min read·Apr 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After revolutionary step made by OpenAI’s ChatGPT in NLP, AI progression continues
    and Meta AI introduces astonishing progress in computer vision. Meta AI research
    team introduced the model called Segment Anything Model (SAM) and a dataset of
    1 Billion masks on 11 Million images. Segmentation of an image is identifying
    which image pixels belong to an object.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/971677e742403a8ddc0322de29fe513c.png)'
  prefs: []
  type: TYPE_IMG
- en: Demo of image segmentation by ai.facebook.com
  prefs: []
  type: TYPE_NORMAL
- en: 'The proposed project mainly includes three pillars: **Task**, **Model** and
    **Data**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Segment Anything Task**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main goal for Meta AI team was to create a promptable image segmentation
    model that would work with user input prompt as it is working with ChatGPT. Therefore,
    they came up with the solution to integrate user input with the image to produce
    segmentation masks. Segmentation prompt can be any information indicating what
    to segment in an image. For example, set of foreground or background point, a
    box, free-form text etc. So the model’s output is a valid segmentation mask given
    any user defined prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Segment Anything Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The promotable Segment Anything Model (SAM) has three components shown in the
    figure bellow.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8352e7a206b80c841e9ccd992a47ca50.png)'
  prefs: []
  type: TYPE_IMG
- en: Segment anything model workflow by ai.facebook.com
  prefs: []
  type: TYPE_NORMAL
- en: A high level of model architecture consists of *an image encoder*, *prompt encoder*,
    and *mask decoder*. For***the image encoder*** they have used MAE [1] pre-trained
    model that has Vision Transformer(ViT) [2] architecture. ViT models are state-of-the-art
    models in image classification and segmentation tasks. As for the prompts, they
    divided them into two types — one type of prompts is *sparse* such as points,
    boxes, and text and another type is *dense* such as masks. **The prompt encoder**
    step creates embeddings for each type of prompt. As for **the mask decoder**,
    it just maps image embeddings, prompt embeddings, and output tokens to a mask.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Segment Anything Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**3.1 Segment Anything Data Engine**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/53fda8ddeccfd49e5b6bf76d1ea3daf9.png)'
  prefs: []
  type: TYPE_IMG
- en: Garbage in garbage out (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The principle — garbage in garbage out — applies to the AI domain as well. If
    the input data is poor quality, a model-generated result will not be good as well.
    That is why, the Meta team tried to select high-quality images to train their
    model. The team has created a data engine to filter the raw image dataset. Creating
    a data engine is divided into three stages.
  prefs: []
  type: TYPE_NORMAL
- en: '*Manual stage*: Human professional annotators were involved to label masks
    on the image manually.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Semi-automatic stage*: They trained the model on annotated images and made
    an inference on the rest of the images. Then, human annotators were asked to label
    additional unlabeled objects that were not detected by the model or correct segments
    with low confidence scores.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Fully automatic stage*: This stage includes automatic mask generation and
    automatic filtering stage which tries to leave non-ambiguous masks and keep the
    masks based on confidence, stability, and size.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3.2 Segment Anything Dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Segment Anything Data Engine created a 1 Billion masks dataset (SA-1B) on
    11 Million diverse, high resolution (3300x4900 pixels on average) and licensed
    images. It is worth mentioning that 99.1% of masks were generated automatically,
    however the quality is so high because they are carefully selected.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion — Why is the model revolutionary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Meta AI team together with other Huge company teams are doing great progress
    in development of AI. The Segment Anything Model (SAM) has capabilities to power
    applications in numerous domains that require finding and segmenting any object
    in any image. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: SAM could be a component of a large multimodal model that integrated images,
    text, audio etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAM could enable selecting an object AR/VR domain based on a user’s gaze and
    then “lifting” it into 3D
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAM can improve creative applications such as extracting image regions for video
    editing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and many more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image Segmentation Demo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, I will try to use [official GitHub code](https://github.com/facebookresearch/segment-anything)
    to play with the algorithm using [Google Colab](https://colab.research.google.com/)
    and perform two types of segmentation on the image. First, I will do segmentation
    with user-defined prompt and second I will do fully automatic segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 1: Image segmentation using user-defined prompt'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Set up (import libraries and installations)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Helper functions to plot masks, point and boxes on the image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Input image (initial image to segment). Lets try to select the mask of a
    first bag of a groceries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c70fb14c5c02fd0df72e0e28e673e0fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Input image (image from Facebook research)
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Load the pretrained model called [*sam_vit_h_4b8939.pth*](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth)which
    is a default model. There are another lighter version of models such as[*sam_vit_l_0b3195.pth*](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth)
    and [*sam_vit_b_01ec64.pth*](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Visualize the point on the image(user prompt) which will help to identify
    our target object — first glossary bag.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1d8e8b196a4c7245e10b40bd73ddc4cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Input image with user prompt (image from Facebook research)
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Make a prediction to generate a mask of the object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 7\. Show top 3 generated mask. When *multimask_output=True*, the algorithm returns
    three mask. Later we can select the one with the highest score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/11c8884ed59aad93a3241f52717e0bc3.png)'
  prefs: []
  type: TYPE_IMG
- en: Prediction results (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The highlighted objects are the masks predicted by the model. As the result
    shows, the model generated three output masks with following prediction scores:
    mask1 — 0.990, Mask2 — 0.875 and Mask3 — 0.827\. We select mask1 which has the
    highest score. Voila!!!! Model’s prediction mask is out target object that we
    wanted to segment initially. The result is amazing, the model works quite well!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Fully Automatic Image segmentation — Cont.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Plotting function of segments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Generate masks automatedly
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Show the result
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1f467ce0e91dabed111231e051319569.png)'
  prefs: []
  type: TYPE_IMG
- en: Automatic segmentation result by SAM (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm identified 137 different objects (masks) using default parameters.
    Each mask contains information about segment area, bounding box coordinates, prediction
    score and stability score that could be used to filter out unwonted segments.
  prefs: []
  type: TYPE_NORMAL
- en: '*I hope you enjoyed it and now can start creating beautiful apps yourself.
    If you have any questions or would like to share your thoughts about this article,
    feel free to comment, I will be happy to answer.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*If you want to support my work directly and also get unlimited access on Medium
    articles, become a Medium member using my* [*referral link*](https://medium.com/@gkeretchashvili/membership)
    *here. Thank you a million times and have a nice day!*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gkeretchashvili/membership?source=post_page-----8f13c86a13a2--------------------------------)
    [## Join Medium with my referral link - Gurami Keretchashvili'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Gurami Keretchashvili (and thousands of other writers
    on Medium). Your membership fee directly…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@gkeretchashvili/membership?source=post_page-----8f13c86a13a2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross
    Girshick. Masked autoencoders are scalable vision learners. CVPR, 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:
    Transformers for image recognition at scale. ICLR, 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland,
    Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo,
    Piotr Dollar, Ross Girshick. Segment Anything, 2023'
  prefs: []
  type: TYPE_NORMAL
- en: My Previous articles about ML deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](/how-to-deploy-machine-learning-models-end-to-end-dog-breed-identification-project-5689457d8973?source=post_page-----8f13c86a13a2--------------------------------)
    [## How to Deploy Machine Learning models? End-to-End Dog Breed Identification
    Project!'
  prefs: []
  type: TYPE_NORMAL
- en: Simplest way to deploy your ML model on the web.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-deploy-machine-learning-models-end-to-end-dog-breed-identification-project-5689457d8973?source=post_page-----8f13c86a13a2--------------------------------)
    [](/how-to-deploy-machine-learning-models-601f8c13ff45?source=post_page-----8f13c86a13a2--------------------------------)
    [## How to Deploy Machine Learning Models
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to deploy machine learning models on the web
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-deploy-machine-learning-models-601f8c13ff45?source=post_page-----8f13c86a13a2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
