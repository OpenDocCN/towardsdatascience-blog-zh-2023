# 我不会签署“生存风险”新声明的四个理由

> 原文：[https://towardsdatascience.com/4-reasons-why-i-wont-sign-the-existential-risk-new-statement-ef658ec699ca](https://towardsdatascience.com/4-reasons-why-i-wont-sign-the-existential-risk-new-statement-ef658ec699ca)

## 意见

## 煽动恐惧是一场危险的游戏

[](https://rafebrena.medium.com/?source=post_page-----ef658ec699ca--------------------------------)[![Rafe Brena, Ph.D.](../Images/6bf622a8ce9b3d06d1cb989fd8d625c6.png)](https://rafebrena.medium.com/?source=post_page-----ef658ec699ca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ef658ec699ca--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ef658ec699ca--------------------------------) [Rafe Brena, Ph.D.](https://rafebrena.medium.com/?source=post_page-----ef658ec699ca--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----ef658ec699ca--------------------------------) ·阅读时间5分钟·2023年5月31日

--

![](../Images/b4e36f211c05017db500e8b00ea8f548.png)

图片由[Cash Macanaya](https://unsplash.com/@cashmacanaya?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

# 新声明是什么？

几周前，我发表了关于签署那份广为人知的[未来生命研究所公开信](https://futureoflife.org/open-letter/pause-giant-ai-experiments/?utm_source=pocket_saves)的赞成和反对论点——最终，我签署了它，尽管有一些保留意见。一些广播和电视主持人采访了我，解释了所有这些争论的根源。

最近，我收到了来自未来生命研究所（以下称FLI）的一封邮件，请我签署一份声明：这次是由人工智能安全中心（CAIS）发布的简短声明，集中在最近人工智能发展带来的生存威胁上。

声明内容如下：

> “**缓解人工智能带来的灭绝风险应成为全球优先事项，与大流行病和核战争等其他社会规模风险并列。**”

确实非常简洁；这有什么问题呢？

# 为什么这不可信？

如果之前的FLI声明存在弱点，这份声明不仅没有纠正这些弱点，反而进一步加重了这些问题，使我无法支持它。

特别是，我有以下四个异议，这些异议肯定会比声明本身长一些：

# 1. 它误置了人工智能的真正风险，并转移了注意力

新声明本质上是对AI的恐慌呼吁，不仅仅是对我们现在能看到的自然后果感到恐慌，而是对那些由随机人物提出的非常模糊的风险估计，如“10%的人类灭绝风险”感到恐慌。

真的吗？10%的人类灭绝风险？基于什么？调查对象没有被要求解释或说明他们的理由，但我怀疑许多人在思考类似“终结者”的场景。你知道，恐怖片的目的是吓唬你，所以你去看电影。但将这些信息转化为现实是不合理的。

对人类的所谓威胁假设了一种尚未解释的毁灭能力和一种代理——消灭人类的意愿。当设备没有任何情感时，它们怎么会想要杀死我们呢？机器并不“想要”这个或那个。

我们现在看到的AI的真正危险是非常不同的。其中之一是生成AI制造虚假声音、图片和视频的能力。你能想象如果你收到一个假冒的你女儿声音的电话，她要求你去救她，你会怎么做吗？

另一个是带有伪证的公共错误信息，比如伪造的视频。那个伪造的教皇相对无害，但很快，Twitter将被充斥着虚假的声明、关于从未发生过事件的图像等等。顺便问一下，你考虑过美国大选正在临近吗？

还有人工内容的利用，AI算法在互联网上挖掘这些内容以生成其“原创”图像和文本：人类的工作被拿走而没有任何经济补偿。在某些情况下，对人类工作的引用是明确的，比如“将此图像制作成X的风格”。

# 人类与机器不是框架AI风险的正确方式。

如果一个月前的FLI信中有“人类与机器”心态的建议，这次被明确指出了。“来自AI的灭绝，”他们称之为，毫不含糊。

在我们生活的现实世界里——而非末日题材的好莱坞电影——并不是机器对我们造成了伤害或威胁我们的存在：更像是一些人（偶然地，强大和富有的那些，大公司的老板）利用新兴的强大技术来增加他们的财富，往往是以无权者的代价：我们看到计算机生成图像的可用性如何缩小了像Fiverr这样的地方图形艺术家的小生意。

此外，高级机器智能试图推翻人类的假设也需要质疑；正如**斯蒂芬·平克**所写：

> “AI反乌托邦将狭隘的Alpha雄性心理投射到智能的概念上。他们假设超人类智能的机器人会发展出像推翻主人或统治世界这样的目标。”

著名的Meta AI研究负责人**扬·勒昆**宣称：

> “人类有各种各样的驱动因素使他们对彼此做坏事，比如自我保护的本能……这些驱动因素被编程到我们的大脑中，但完全没有理由去制造具有相同驱动因素的机器人。”

不，机器失控不会成为我们的统治者或灭绝我们：其他人类，即目前的统治者，将通过利用他们所拥有的经济和技术手段——包括适合的AI——来增强他们的统治。

# 3. 将AI与疫情和核战争进行比较是不成立的

我明白FLI提到疫情是为了将声明与我们刚刚经历的事情联系起来——这在许多人中留下了情感伤痕——但这并不是一个有效的比较。撇开一些[阴谋论](https://journals.sagepub.com/doi/pdf/10.1177/1368430220982068)不谈，我们走出的疫情并不是技术——疫苗才是。FLI如何假设灾难性AI会传播？通过传染吗？

当然，核弹是一项技术进步，但在核战争的情况下，我们清楚地知道核弹会如何以及为什么摧毁我们：这不是猜测，就像“流氓 AI”的情况一样。

# 4. 来自带来风险的公司领导正在签署这一声明

另一个引起我注意的事项是看到签署声明的人员名单，从Sam Altman开始。他是OpenAI的领导者，自2022年11月以来，ChatGPT使我们所经历的狂热AI竞赛启动。即使是强大的Google也难以跟上这场竞赛——微软的Satya Nadella难道没有说过他想要“让Google跳舞”吗？他达成了愿望，却以加速AI竞赛为代价。

我不理解那些在推动AI竞赛的公司领导者同时签署这个声明的逻辑。Altman可以说他对AI的发展非常担忧，但如果我们看到他的公司仍然全速前进，那么他的担忧看起来就显得毫无意义且不协调。我不打算对Altman的声明进行道德评判，但接受他的支持而不加质疑会削弱声明的有效性——尤其是考虑到对Altman的公司来说，领导竞赛对其财务底线至关重要。

# 最后的思考

并不是机器失控，而是资本主义垄断和专制政府对AI工具的使用可能对我们造成伤害。这不仅仅是在好莱坞的反乌托邦未来，而是在我们今天所处的现实世界中。

我不会支持这种由恐惧驱动的机器愿景，因为这终究是虚伪的，因为它是由那些试图转移其追求利润的公司提出的。这就是为什么我没有签署由FLI支持的新声明。

此外，我怀疑富有和有影响力的领导者可以允许自己审视虚构的威胁，因为他们不会担心像自由职业平面设计师的收入减少这样更“平凡”的现实威胁：他们非常清楚自己不会在月底面临经济困难，他们的孩子或孙子也不会。
