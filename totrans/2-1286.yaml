- en: Applying Hyperparameter Tuning To Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将超参数调整应用于神经网络
- en: 原文：[https://towardsdatascience.com/hyperparameter-tuning-neural-networks-101-ca1102891b27](https://towardsdatascience.com/hyperparameter-tuning-neural-networks-101-ca1102891b27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hyperparameter-tuning-neural-networks-101-ca1102891b27](https://towardsdatascience.com/hyperparameter-tuning-neural-networks-101-ca1102891b27)
- en: How you can improve the “learning” of neural networks through tuning hyperparameters
    with an example in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何通过调整超参数来提高神经网络的“学习”，并附带 Python 示例
- en: '[](https://medium.com/@egorhowell?source=post_page-----ca1102891b27--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----ca1102891b27--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ca1102891b27--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ca1102891b27--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----ca1102891b27--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----ca1102891b27--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----ca1102891b27--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ca1102891b27--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ca1102891b27--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----ca1102891b27--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ca1102891b27--------------------------------)
    ·9 min read·Nov 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ca1102891b27--------------------------------)
    ·9分钟阅读·2023年11月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6b66a379b90bb74a1caf0ca7ed45e511.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b66a379b90bb74a1caf0ca7ed45e511.png)'
- en: Neural-network icons created by Vectors Tank — Flaticon. neural-network icons.
    [https://www.flaticon.com/free-icons/neural](https://www.flaticon.com/free-icons/neural-network)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络图标由 Vectors Tank — Flaticon 创建。神经网络图标。[https://www.flaticon.com/free-icons/neural](https://www.flaticon.com/free-icons/neural-network)
- en: Background
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'In my previous post, we discussed how [***neural networks***](https://medium.com/gitconnected/intro-perceptron-architecture-neural-networks-101-2a487062810c)
    predict and learn from the data. There are two processes responsible for this:
    the [***forward pass***](/neural-networks-forward-pass-and-backpropagation-be3b75a1cfcc)
    and backward pass, also known as [***backpropagation***](https://en.wikipedia.org/wiki/Backpropagation).
    You can learn more about it here:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的帖子中，我们讨论了[***神经网络***](https://medium.com/gitconnected/intro-perceptron-architecture-neural-networks-101-2a487062810c)如何预测和从数据中学习。负责这一过程的有两个步骤：[***前向传播***](/neural-networks-forward-pass-and-backpropagation-be3b75a1cfcc)和后向传播，也称为[***反向传播***](https://en.wikipedia.org/wiki/Backpropagation)。你可以在这里了解更多：
- en: '[](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----ca1102891b27--------------------------------)
    [## Forward Pass & Backpropagation: Neural Networks 101'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----ca1102891b27--------------------------------)
    [## 前向传播与反向传播：神经网络 101'
- en: Explaining how neural networks “train” and “learn” patterns in data by hand
    and in code using PyTorch
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过手工和使用 PyTorch 代码解释神经网络如何“训练”和“学习”数据中的模式
- en: towardsdatascience.com](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----ca1102891b27--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----ca1102891b27--------------------------------)
- en: This post will dive into how we can optimise this “learning” and “training”
    process to increase the performance of our model. The areas we will cover are
    computational improvements and [***hyperparameter tuning***](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5)and
    how to implement it in PyTorch!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将深入探讨如何优化“学习”和“训练”过程，以提高模型的性能。我们将覆盖的领域包括计算改进和[***超参数调整***](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5)，以及如何在
    PyTorch 中实现！
- en: But, before all that good stuff, let’s quickly jog our memory about neural networks!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在所有这些好东西之前，让我们快速回顾一下神经网络的基本知识！
- en: 'Quick Recap: What are Neural Networks?'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速回顾：什么是神经网络？
- en: 'Neural networks are large mathematical expressions that try to find the “right”
    function that can map a set of inputs to their corresponding outputs. An example
    of a neural network is depicted below:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是大型数学表达式，试图找到能够将一组输入映射到其对应输出的“正确”函数。下面展示了一个神经网络的例子：
- en: '![](../Images/94cdac9897a5e852d9cd6edd425fcba0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94cdac9897a5e852d9cd6edd425fcba0.png)'
- en: A basic two-hidden multi-layer perceptron. Diagram by author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基本的双隐层多层感知器。图示由作者提供。
- en: 'Each hidden-layer neuron carries out the following computation:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个隐藏层神经元执行以下计算：
- en: '![](../Images/57f8921c8f7e374d7f075b2f4ae32e6b.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57f8921c8f7e374d7f075b2f4ae32e6b.png)'
- en: The process carried out inside each neuron. Diagram by author.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元内部进行的过程。图示由作者提供。
- en: '***Inputs:*** *These are the features of our dataset.*'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***输入：*** *这些是我们数据集的特征。*'
- en: '***Weights:*** *Coefficients that scale the inputs. The goal of the algorithm
    is to find the most optimal coefficients through* [***gradient descent***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c)*.*'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***权重：*** *用于缩放输入的系数。算法的目标是通过* [***梯度下降***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c)*找到最优系数。*'
- en: '***Linear Weighted Sum:*** *Sum up the products of the inputs and weights and
    add a bias/offset term,* ***b****.*'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***线性加权和：*** *将输入和权重的乘积求和，并加上一个偏置/偏移量项，* ***b****.*'
- en: '***Hidden Layer:*** *Multiple neurons are stored to learn patterns in the dataset.
    The superscript refers to the layer and the subscript to the number of neuron
    in that layer.*'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***隐藏层：*** *多个神经元用于学习数据集中的模式。上标表示层数，下标表示该层中的神经元编号。*'
- en: '***Arrows:*** *These are the weights for the network from their corresponding
    inputs, whether it be the features or the hidden layer outputs. I have omitted
    writing them explicitly on the diagram them for a cleaner plot.*'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***箭头：*** *这些是从相应输入（无论是特征还是隐藏层输出）到网络的权重。我在图示中省略了它们的明确标注，以保持图示的整洁。*'
- en: '[***ReLU Activation Function***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)***:***
    *The most popular* [***activation function***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)*as
    it is computationally efficient and intuitive. See more about it* [*here*](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)*.*'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***ReLU 激活函数***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)***:***
    *最受欢迎的* [***激活函数***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)*，因为它计算上高效且直观。有关更多信息，请*
    [*点击这里*](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)*。*'
- en: I have [linked a fantastic video here](https://www.youtube.com/watch?v=0QczhVg5HaI)
    explaining how neural networks can learn anything for some more context!
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我在这里[链接了一个精彩的视频](https://www.youtube.com/watch?v=0QczhVg5HaI)，解释了神经网络如何学习任何东西，以提供更多背景！
- en: 'If you want a more thorough introduction, check my previous post here:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要更全面的介绍，可以查看我之前的帖子：
- en: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ca1102891b27--------------------------------)
    [## Intro, Perceptron & Architecture: Neural Networks 101'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ca1102891b27--------------------------------)
    [## 介绍，感知器与架构：神经网络 101]'
- en: An introduction to Neural Networks and their building blocks
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络及其构建块简介
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ca1102891b27--------------------------------)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ca1102891b27--------------------------------)
- en: Computational Improvements
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算改进
- en: Perhaps the main optimisation technique that made neural networks so commonly
    accessible nowadays is parallel processing.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使神经网络如此普遍可用的主要优化技术可能是并行处理。
- en: The volume of data is also another main reason why neural networks are so effective
    in practice.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据量也是神经网络在实际应用中如此有效的主要原因之一。
- en: 'Each layer can be formulated as a large matrix with its associated inputs,
    weights, and biases. For example, consider this basic output from a neuron:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层都可以表示为一个大型矩阵，其包含相关的输入、权重和偏置。例如，考虑一个神经元的基本输出：
- en: '![](../Images/83f85cd34579fe553967381e057e723d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83f85cd34579fe553967381e057e723d.png)'
- en: Linear output from neuron. Equation by author in LaTeX.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的线性输出。公式由作者用 LaTeX 表示。
- en: 'Here ***x*** is the inputs, ***w*** is the weights, ***b*** is the bias and
    ***z*** is the final output. The above formula can be re-written in matrix form:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，***x*** 是输入，***w*** 是权重，***b*** 是偏置，***z*** 是最终输出。上述公式可以重写为矩阵形式：
- en: '![](../Images/a1cd2bd222767bd7c853cfa26a56e22c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1cd2bd222767bd7c853cfa26a56e22c.png)'
- en: Vectorised implementation for a neuron output. Equation by author in LaTeX.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元输出的向量化实现。公式由作者用 LaTeX 表示。
- en: Without this vectorised implementation, the runtime for training neural networks
    would be enormous as we would start to rely on loops. In that case, we would multiply
    each weight and input and then add them one after the other to get ***z***. Whereas
    vectorising the implementation, it can be done in one whole step.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有这种向量化实现，训练神经网络的运行时间将会非常巨大，因为我们需要依赖循环。在这种情况下，我们将逐个乘以每个权重和输入，然后相加得到***z***。而使用向量化实现，这可以在一个整体步骤中完成。
- en: There is a great article [linked here](/understand-vectorization-for-deep-learning-d712d260ab0f)
    and [a video here](https://www.youtube.com/watch?v=kkWRbIb42Ms) that compares
    the runtime for a vectorised approach vs using loops.
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里有一篇很好的文章[链接在此](/understand-vectorization-for-deep-learning-d712d260ab0f)和[一个视频在这里](https://www.youtube.com/watch?v=kkWRbIb42Ms)，比较了向量化方法与使用循环的运行时间。
- en: Most deep learning frameworks like [***PyTorch***](https://pytorch.org/) and
    [***TensorFlow***](https://www.tensorflow.org/) do this for you under the hood,
    so you don’t have to worry too much about it!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习框架，如[***PyTorch***](https://pytorch.org/)和[***TensorFlow***](https://www.tensorflow.org/)，在后台为你处理这些问题，所以你不需要太担心！
- en: Hyperparameters
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数
- en: Overview
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The search space for neural network architectures and parameters is unimaginably
    huge, if not infinite. Several libraries exist to help you tune the parameters
    such as [***hyperopt***](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5),
    [***optuna***](https://optuna.org/) or just regular [***sci-kit learn***](https://scikit-learn.org/stable/).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构和参数的搜索空间是难以想象的巨大，甚至可以说是无限的。存在一些库可以帮助你调整参数，比如[***hyperopt***](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5)、[***optuna***](https://optuna.org/)或普通的[***sci-kit
    learn***](https://scikit-learn.org/stable/)。
- en: There are also many more libraries out there, [see here for a list.](/10-hyperparameter-optimization-frameworks-8bc87bc8b7e3)
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 还有很多其他的库，[请查看此处的列表。](/10-hyperparameter-optimization-frameworks-8bc87bc8b7e3)
- en: They differ in their approach some use a simple grid search or random search,
    whereas others have more sophisticated methods such as [***Bayesian optimisation***](https://pub.towardsai.net/conditional-probability-and-bayes-theorem-simply-explained-788a6361f333)
    or even evolutionary algorithms like [***genetic algorithms***](/from-biology-to-computing-an-introduction-to-genetic-algorithms-b39476743483).
    One method is not better than the other, it all comes down to how you devise your
    search space and compute resources.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的处理方法不同，有的使用简单的网格搜索或随机搜索，而其他的则采用更复杂的方法，如[***贝叶斯优化***](https://pub.towardsai.net/conditional-probability-and-bayes-theorem-simply-explained-788a6361f333)或甚至像[***遗传算法***](/from-biology-to-computing-an-introduction-to-genetic-algorithms-b39476743483)这样的进化算法。一种方法并不优于另一种，最终取决于你如何设计搜索空间和计算资源。
- en: It is important to have some knowledge of your ideal parameters to not waste
    a significant amount of time searching over unnecessary values and converge quickly.
    Let’s now run through some of the main ones you ought to look at!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 了解你理想的参数是很重要的，以避免在不必要的值上浪费大量时间，并快速收敛。现在让我们来看看一些你应该关注的主要内容！
- en: Number of Hidden Layers
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐藏层数量
- en: There is something called the [***universal approximation theorem***](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
    that basically says a single hidden layer neural network can learn any function,
    provided it has enough neurons.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种叫做[***普遍逼近定理***](https://en.wikipedia.org/wiki/Universal_approximation_theorem)的理论，基本上说单层隐藏层神经网络可以学习任何函数，只要它有足够的神经元。
- en: However, having one layer with loads of neurons is not ideal, and it is better
    to have several layers with fewer neurons. The hypothesis for this being is that
    each layer learns something new and at a more granular level. Whereas with one
    hidden layer, the network is expected to learn every nuance of the dataset all
    at once.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，拥有一个层中大量神经元并不是理想的，最好还是有几个层，每层中神经元较少。这样做的假设是每层学习新的东西，且在更细粒度的水平上进行学习。相比之下，单层隐藏层网络需要一次性学习数据集的每一个细节。
- en: In general, a couple of hidden layers are often good enough. For example, on
    the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), a model with
    one hidden layer and a couple of hundred neurons has [97% accuracy](https://paperswithcode.com/sota/image-classification-on-mnist),
    but a network with two hidden layers and the same number of neurons has [98% accuracy](https://paperswithcode.com/sota/image-classification-on-mnist).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，几个隐藏层通常已经足够。例如，在[MNIST数据集](https://en.wikipedia.org/wiki/MNIST_database)上，一个具有一个隐藏层和几百个神经元的模型具有[97%准确率](https://paperswithcode.com/sota/image-classification-on-mnist)，但一个具有两个隐藏层且神经元数量相同的网络具有[98%准确率](https://paperswithcode.com/sota/image-classification-on-mnist)。
- en: The MNIST dataset contains many examples of handwritten digits.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MNIST数据集包含许多手写数字的例子。
- en: Of course, like any hyperparameter, we should apply some tuning process to iterate
    over a different number of layers in combination with other hyperparameters.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，像任何超参数一样，我们应该应用一些调优过程，通过不同的层数与其他超参数的组合进行迭代。
- en: Number of Neurons in Layers
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层中的神经元数量
- en: The input and output layers are pre-defined on the number of neurons they can
    have. The input layer size must equal the number of features in the dataset. If
    your dataset has 50 features, then your input layer will have 50 neurons. Similarly,
    the output layer needs to be appropriate for the problem. If you are predicting
    house prices, then the output layer will only have 1 neuron. However, if you are
    trying to classify single digits, like in the MNIST dataset, then you need 10
    output neurons.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层和输出层的神经元数量是预定义的。输入层的大小必须等于数据集中的特征数量。如果你的数据集有50个特征，那么你的输入层将有50个神经元。同样，输出层需要适合问题。如果你在预测房价，那么输出层将只有1个神经元。然而，如果你在尝试分类单个数字，如MNIST数据集中的情况，那么你需要10个输出神经元。
- en: For the hidden layers, you can go a bit wild! The search space of the number
    of neurons is massive. However, it’s best to overkill it slightly with how many
    you have and use a technique such as [***early stopping***](https://en.wikipedia.org/wiki/Early_stopping#:~:text=In%20machine%20learning%2C%20early%20stopping,training%20data%20with%20each%20iteration.)to
    prevent overfitting.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于隐藏层，你可以稍微放开一点！神经元数量的搜索空间是巨大的。然而，最好是略微过度使用神经元数量，并使用像[***早停***](https://en.wikipedia.org/wiki/Early_stopping#:~:text=In%20machine%20learning%2C%20early%20stopping,training%20data%20with%20each%20iteration.)这样的技术来防止过拟合。
- en: Another key idea is to ensure that each layer has enough neurons to have *representational
    power.* If you are trying to predict a 3D image, with 2 neurons you can only work
    in 2D, hence will lose some information about the signal.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键想法是确保每层都有足够的神经元以具有*表示能力*。如果你试图预测一个3D图像，2个神经元只能在2D中工作，因此会丢失一些关于信号的信息。
- en: Learning Rate
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率
- en: The learning rate determines how quickly the algorithm can converge to the optimal
    as it’s responsible for the step size used during backpropagation. It’s probably
    one of the most important hyperparameters for training neural networks. Too high
    and learning will diverge, too low, and the algorithm will take ages to converge.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率决定了算法收敛到最优解的速度，因为它负责反向传播过程中的步长。这可能是训练神经网络时最重要的超参数之一。太高会导致学习发散，太低则算法会需要很长时间才能收敛。
- en: I would typically just hyperparameter-tune my learning rate over a wide search
    space between ***0.001*** and ***1***, this is seen as the *traditional* learning
    rate that most commonly comes up in literature.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常会在***0.001***到***1***之间的广泛搜索空间中调整我的学习率，这被视为文献中最常见的*传统*学习率。
- en: 'One of the best ways to find the optimal learning rate is through [***learning
    rate scheduling***](/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1).
    The schedule reduces the learning rate as training progresses, so take smaller
    step sizes near and around the optimum. Let’s break down some of the common ones:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最佳学习率的最佳方法之一是通过[***学习率调度***](/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)。该计划随着训练的进展减少学习率，因此在接近最优点时采用更小的步长。让我们来详细分解一些常见的：
- en: '**Time Based Decay:**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于时间的衰减：**'
- en: The learning rate decreases at a certain rate overtime.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率随着时间的推移以一定的速度下降。
- en: '![](../Images/a3593cb7ee892ae8a131bf0a2e1d3468.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3593cb7ee892ae8a131bf0a2e1d3468.png)'
- en: Time based decay. Equation by author in LaTeX.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于时间的衰减。作者在LaTeX中的方程。
- en: Here, ***α*** is the learning rate, ***α_0*** is the initial learning rate**,
    *decay***is the decay rate and ***epoch*** is the number of iterations.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，***α*** 是学习率，***α_0*** 是初始学习率，*decay* 是衰减率，***epoch*** 是迭代次数。
- en: An epoch is one training cycle of a neural network using all of the training
    data.
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个 epoch 是神经网络使用所有训练数据进行的一个训练周期。
- en: '**Step Decay:**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**步长衰减：**'
- en: The learning rate is reduced by a certain factor after a given number of training
    epochs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率在经过一定数量的训练 epoch 后按某一因子减少。
- en: '![](../Images/e6ad17edcc7651647d45e3d1e108de32.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6ad17edcc7651647d45e3d1e108de32.png)'
- en: Step decay. Equation by author in LaTeX.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 步长衰减。作者在 LaTeX 中的方程式。
- en: Where ***factor*** is the factor that the learning rate will be reduced by and
    ***step*** is the number of epochs after which the learning rate should be reduced.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***factor*** 是学习率减少的因子，***step*** 是学习率应减少的 epoch 数量。
- en: '**Exponential Decay:**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**指数衰减：**'
- en: The learning rate will decrease exponentially at each epoch.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率将在每个 epoch 中以指数方式减少。
- en: '![](../Images/1bfad8027a68b154c55f4297e6c66676.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bfad8027a68b154c55f4297e6c66676.png)'
- en: Exponential decay. Equation by author in LaTeX.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 指数衰减。作者在 LaTeX 中的方程式。
- en: '**Others:**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他：**'
- en: There are also many other learning rate schedules such as *performance scheduling*,
    *1cycle scheduling*, and *power scheduling*. It is important to remember that
    one scheduling method is not better than another and it’s ideal to try several
    to determine which is the best fit for your model and data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他学习率调度方法，如 *性能调度*、*1cycle 调度* 和 *功率调度*。重要的是要记住，没有一种调度方法比另一种更好，最好尝试几种以确定哪一种最适合你的模型和数据。
- en: Batch Size
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量大小
- en: 'When training neural networks, there are three common variations of gradient
    descent, which are:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，常见的梯度下降变体有三种：
- en: '[**Batch Gradient Descent**](/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a)**:**
    Use the entire training dataset to compute the gradient of the loss function.
    This is the most robust but not computationally feasible for large datasets.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**批量梯度下降**](/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a)**:**
    使用整个训练数据集来计算损失函数的梯度。这是最稳健的方法，但对于大数据集来说，计算上不具可行性。'
- en: '[**Stochastic Gradient Descent**](https://realpython.com/gradient-descent-algorithm-python/#:~:text=Stochastic%20gradient%20descent%20is%20an,used%20in%20machine%20learning%20applications.)**:**
    Use a single data point to compute the gradient of the loss function. This method
    is the quickest but the estimate can be noisy and the convergence path slow.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**随机梯度下降**](https://realpython.com/gradient-descent-algorithm-python/#:~:text=Stochastic%20gradient%20descent%20is%20an,used%20in%20machine%20learning%20applications.)**:**
    使用单个数据点来计算损失函数的梯度。这种方法是最快的，但估计可能会有噪声，收敛路径也可能较慢。'
- en: '[**Mini-Batch Gradient Descent**](/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a)**:**
    Use a subset of the training dataset to compute the gradient of the loss function.
    The size of the batches varies and depends on the size of the dataset. This is
    the best of both worlds of both batch and stochastic gradient descent.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**小批量梯度下降**](/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a)**:**
    使用训练数据集的一个子集来计算损失函数的梯度。批量的大小有所变化，并取决于数据集的大小。这是批量和随机梯度下降的两者优点的结合。'
- en: The tricky part is finding the best batch size to carry out mini-batch gradient
    descent with. It’s recommended to use the largest batch sizes possible that can
    fit inside your computer’s GPU RAM as they parallelize the computation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于找到适合执行小批量梯度下降的最佳批量大小。建议使用尽可能大的批量大小，这些批量可以适配到计算机的 GPU 内存中，因为它们能并行计算。
- en: Number of Iterations
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代次数
- en: This is the number of epochs, which is the total forward and backward passes
    that we carry out for the neural network. In reality, it’s better to use early
    stopping and set the number of iterations arbitrarily high. This removes the chance
    of terminating learning too early.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 epoch 的数量，即我们为神经网络执行的所有前向和反向传播的总次数。实际上，最好使用提前停止，并将迭代次数设置得很高。这可以避免过早终止学习的可能性。
- en: Activation Functions
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'Most networks use the [***ReLU***](/breaking-linearity-with-relu-d2cfa7ebf264),
    primarily due to its computational efficiencies, but there are others out there.
    One of my previous posts summarised the main ones along with their pros and cons:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数网络使用 [***ReLU***](/breaking-linearity-with-relu-d2cfa7ebf264)，主要由于其计算效率，但也有其他激活函数。我之前的帖子总结了主要的激活函数及其优缺点：
- en: '[](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----ca1102891b27--------------------------------)
    [## Activation Functions & Non-Linearity: Neural Networks 101'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----ca1102891b27--------------------------------)
    [## 激活函数与非线性：神经网络101'
- en: Explaining why neural networks can learn (nearly) anything and everything
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释为什么神经网络可以学习（几乎）任何东西和一切
- en: towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----ca1102891b27--------------------------------)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----ca1102891b27--------------------------------)'
- en: The selected activation function is important for the output layer to ensure
    your prediction is in the right context of the problem. For example, if you are
    predicting probability, then you would use a [***sigmoid***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)
    activation function.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的激活函数对于输出层很重要，以确保你的预测符合问题的上下文。例如，如果你在预测概率，则应使用[***sigmoid***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)激活函数。
- en: However, in my opinion, testing different activation functions in the hidden
    layers won’t move the needle of performance that much compared to the other hyperparameters
    we have discussed above.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我看来，与我们之前讨论的其他超参数相比，在隐藏层中测试不同的激活函数对性能的影响不会太大。
- en: Other Hyperparameters
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他超参数
- en: 'There are also other hyperparameters that can tuned for the model:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他超参数可以调优：
- en: '[***Optimising algorithm***](/optimizers-for-training-neural-network-59450d71caf6)
    ***(cover in next article!)***'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***优化算法***](/optimizers-for-training-neural-network-59450d71caf6) ***(下篇文章中讨论！)***'
- en: '[***Regularisation methods***](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)
    ***(cover in future!)***'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***正则化方法***](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)
    ***(未来讨论！)***'
- en: '[***Weight initialisation***](https://www.deeplearning.ai/ai-notes/initialization/index.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***权重初始化***](https://www.deeplearning.ai/ai-notes/initialization/index.html)'
- en: '[***Loss function***](/loss-functions-and-their-use-in-neural-networks-a470e703f1e9)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***损失函数***](/loss-functions-and-their-use-in-neural-networks-a470e703f1e9)'
- en: Python Example
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python示例
- en: 'Below is some boilerplate code that carries out hyperparameter tuning for a
    neural network in PyTorch using hyperopt for the MNIST dataset:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些模板代码，使用hyperopt在PyTorch中对MNIST数据集进行神经网络超参数调优：
- en: GitHub Gist by author.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的GitHub Gist。
- en: 'The code is available on my GitHub here:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可在我的GitHub上找到：
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/hyperparam_tune.py?source=post_page-----ca1102891b27--------------------------------)
    [## Medium-Articles/Neural Networks/hyperparam_tune.py at main · egorhowell/Medium-Articles'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/hyperparam_tune.py?source=post_page-----ca1102891b27--------------------------------)
    [## Medium-Articles/Neural Networks/hyperparam_tune.py at main · egorhowell/Medium-Articles'
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on…
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我在我的中等博客/文章中使用的代码。通过创建一个账户来贡献于egorhowell/Medium-Articles的开发…
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/hyperparam_tune.py?source=post_page-----ca1102891b27--------------------------------)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/hyperparam_tune.py?source=post_page-----ca1102891b27--------------------------------)'
- en: Summary & Further Thoughts
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与进一步思考
- en: Neural networks have many hyperparameters and infinite architectures, this makes
    finding the best combination very difficult. Fortunately, packages such as ***optuna***
    and ***hyperpot*** exist that carry out this process for us in a smart way. The
    hyperparameters that are often best to tune are the number of hidden layers, the
    number of neurons, and the learning rate. These often give us the most ‘bang for
    our buck’ when developing neural net models. The number of epochs is made redundant
    through the use of early stopping, and the activation function chosen also often
    has a minimal effect on the performance. However, it is always important to consider
    what type of problem you are trying to solve when considering the structure of
    your input and output layers as well as the output layer’s activation function.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有很多超参数和无限的架构，这使得找到最佳组合非常困难。幸运的是，像***optuna***和***hyperpot***这样的包可以智能地为我们完成这个过程。通常最需要调整的超参数是隐藏层的数量、神经元的数量和学习率。这些通常在开发神经网络模型时能够带来最显著的效果。通过使用早期停止，训练轮数会变得冗余，而选择的激活函数也通常对性能影响很小。然而，在考虑输入和输出层的结构以及输出层的激活函数时，总是重要的要考虑你试图解决的是什么类型的问题。
- en: References & Further Reading
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料与进一步阅读
- en: '[*Andrej Karpathy Neural Network Course*](https://www.youtube.com/watch?v=i94OvYb6noo)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Andrej Karpathy 神经网络课程*](https://www.youtube.com/watch?v=i94OvYb6noo)'
- en: '[*PyTorch site*](https://pytorch.org/)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*PyTorch 网站*](https://pytorch.org/)'
- en: '[*Another example of training a neural network by hand*](/training-a-neural-network-by-hand-1bcac4d82a6e)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*另一例手动训练神经网络*](/training-a-neural-network-by-hand-1bcac4d82a6e)'
- en: '[*Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition.
    Aurélien Géron. September 2019\. Publisher(s): O’Reilly Media, Inc. ISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*.*'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*动手实践机器学习：Scikit-Learn、Keras 和 TensorFlow 第 2 版。奥雷利安·热龙。2019年9月。出版社：O’Reilly
    Media, Inc. ISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*'
- en: Another Thing!
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另一个事项！
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个免费的新闻通讯，[**Dishing the Data**](https://dishingthedata.substack.com/)，每周分享成为更好数据科学家的技巧。没有“空洞的内容”或“点击诱饵”，只有来自实践数据科学家的纯粹可操作见解。
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----ca1102891b27--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Dishing The Data | Egor Howell | Substack](https://newsletter.egorhowell.com/?source=post_page-----ca1102891b27--------------------------------)'
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何成为更好的数据科学家。点击阅读由 Egor Howell 编写的 Substack 出版物《Dishing The Data》…
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----ca1102891b27--------------------------------)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----ca1102891b27--------------------------------)'
- en: Connect With Me!
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与我联系！
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**YouTube**](https://www.youtube.com/@egorhowell)'
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Twitter**](https://twitter.com/EgorHowell)'
- en: '[**GitHub**](https://github.com/egorhowell)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GitHub**](https://github.com/egorhowell)'
