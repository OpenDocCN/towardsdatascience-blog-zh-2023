# 字节对编码初学者指南

> 原文：[https://towardsdatascience.com/byte-pair-encoding-for-beginners-708d4472c0c7](https://towardsdatascience.com/byte-pair-encoding-for-beginners-708d4472c0c7)

## 一份通俗易懂的BPE标记器指南

[](https://medium.com/@mina.ghashami?source=post_page-----708d4472c0c7--------------------------------)[![Mina Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----708d4472c0c7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----708d4472c0c7--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----708d4472c0c7--------------------------------) [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----708d4472c0c7--------------------------------)

·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----708d4472c0c7--------------------------------) ·6分钟阅读·2023年10月10日

--

![](../Images/82f1a7e623d9991a0f1419ff63b1a6c4.png)

图片由作者提供

在这篇文章中，我们将深入了解一种最著名的标记化算法，即字节对编码（BPE）。它被广泛应用于许多最先进的大型语言模型中，如BERT家族、BART和GPT家族。

让我们开始吧。

# 字节对编码（BPE）

字节对编码（BPE）是一种**基于语料库的子词标记化**算法。**它是**基于语料库的，因为它使用训练语料库来学习频繁的字符（或符号）并将其合并成一个符号。它也是一种**子词标记器**，因为它将文本分解为比（或等于）单词更小的单元。

下面的图像展示了句子“ it is raining”的子词标记化。请注意，虽然“it”和“is”是完整的单词标记；“rain”和“ing”是“raining”的子词。

![](../Images/b0b1349cb9ad950a80f5154d761c786f.png)

BPE算法有两个主要部分：标记学习器、标记分割器。

1- **标记学习器**：它处理一组文本并创建一个包含标记的词汇表。这些语料库作为训练语料库。

![](../Images/3723bdade2749ac3cd543e34ba112533.png)

标记学习器处理一组文本并建立一个词汇表 —— 图片由作者提供

2- **标记分割器**：它将一段文本（如句子）分割成标记。该文本是测试数据。我们利用从前一步中获得的学习，在这一步中对测试数据进行标记化。

![](../Images/489e8da9aee801fd298bbb188293b477.png)

标记分割器将句子转换为其标记 —— 图片由作者提供

值得一提的是，

> “字节对编码（BPE）（Gage，1994）是一种旧的数据压缩技术，它通过迭代替换序列中最频繁的字节对，用一个未使用的字节。”[1]

我们知道的当前BPE算法用于分词，适应了这个算法，但不是合并频繁的字节对，而是合并频繁的字符（或字符序列）。

算法的完整伪代码如下。让我们按照伪代码一步一步地学习这个算法。

![](../Images/0d2f448bd73522609ebb8c179a685cd4.png)

BPE伪代码 — 图像来源 [[1](https://arxiv.org/pdf/1508.07909.pdf)]

第一阶段如下图红框中突出显示：我们通过查看我们拥有的文档语料库来创建一个词汇表，按字符拆分它们，并将每个字符及其出现频率添加到词汇表中。

我们看到我们的文档语料库包含* [low, lower, newest, widest]*，每个的频率不同。我们使用*</w>*来表示单词的结束。

![](../Images/97914fa271d0cb192451e703f4fe7308.png)

第一步 — 图像来源 [[1](https://arxiv.org/pdf/1508.07909.pdf)] 由作者修改

在第二步中，对于*num_merge=10*次，我们依次调用三个函数：

1.  get_stats()：此函数返回一个字典（称为pairs），其中包含词汇表中每对字符及其出现频率。

1.  max()：此函数获取get_stats()返回的字符对，并返回出现频率最高的对。

1.  merge_vocab()：将字符对视为一个单位，将每个字符对的所有出现替换为一个单独的单位。它返回更新后的词汇表。

这三个函数一起获取语料库中最常见的字符对，将它们合并为一个符号并更新词汇表。我们将此过程重复num_merges=10次。

![](../Images/1387a6909484a49f7719af64a11af8ef.png)

第二步 — 图像来源 [[1](https://arxiv.org/pdf/1508.07909.pdf)] 由作者修改

让我们通过上述示例进行演练。我们的语料库如下：

![](../Images/a4760f88d9013a3118c11e3909f64aa6.png)

语料库示例 — 图像由作者提供

我们通过将所有单词拆分为字符来构建词汇表。在单词末尾添加_以表示*单词的结束*，然后将其拆分为字符。在伪代码中，他们使用</w>符号表示单词的结束，但在这里我们使用—。

![](../Images/06e11fa6bf80d9166ef81360391d7dd7.png)

语料库中的词汇表 — 图像由作者提供

接下来，我们需要计算语料库中每个字符的频率。为了更好地表示字符频率的语料库，我们将其表示如下：

![](../Images/6460a209518826129d21ec98c18bfe1e.png)

语料库表示 — 图像由作者提供

注意“l o w — 5”表示在“low-”这个词的上下文中，*l, o, w* 和 *—* 每个字符都重复了5次。

接下来，我们找出**哪个字符对最常相邻？** 我们看到“e”和“r”最常相邻，共出现8次。因此，我们将它们合并为一个符号“er”，将其添加到词汇表中，并用新符号更新语料库表示。

![](../Images/6df93d118636e4a285ca3fee763b103f.png)

合并“e”和“r”为“er” — 图像由作者提供

我们重复这个过程：**接下来哪两个符号最常紧挨在一起？** 我们发现“w”和“er”最常紧挨在一起出现 8 次。因此，我们将它们合并为一个符号“wer”*。

![](../Images/d38a5e955d772dd6f82edfc10a19113f.png)

合并“w”和“er”为“wer” — 图片来自作者

我们重复这个过程：接下来最常一起出现的两个符号是“wer”和“—”。我们将它们合并为“wer-”并更新语料库：

![](../Images/013a6c126c6d0abb66490a2cbcf82863.png)

合并“wer”和“—”为“wer-” — 图片来自作者

频率最高的两个符号是“l”和“o”，它们出现了 7 次。我们将它们合并为“lo”并更新词汇表。

![](../Images/54be5dce5941273762b583706ea8c9c4.png)

合并“l”和“o”为“lo” — 图片来自作者

到目前为止，我们已经进行了 4 次合并。我们将继续将符号合并，直到达到 10 次合并。之后，我们的词汇表就准备好了，可以使用 BPE 的分词算法对测试数据进行分词。**这就是分词算法的工作原理：**

**BPE 分词器算法：** 在测试数据上运行每个从训练数据中学习到的合并。这些已学习的合并是我们之前添加到词汇表中的符号。按我们从训练数据中学到的顺序逐一在测试数据上运行它们。

注意，字符对的频率在这里并不重要，也不需要额外的学习。只需扫描测试数据以查找已学得的词汇条目，并拆分序列。

## BPE Tokens 的属性

BPE 分词器的工作方式会导致生成的 tokens 常常是*频繁出现的词汇*和*频繁出现的子词*。频繁出现的子词通常是像*est*和*er*这样的词素。

> 词素是具有意义的最小语言单位。它们提供关于语言的意义和语法的线索。

需要注意的是，词汇表的大小可以通过控制合并操作的数量来配置。较小的词汇表会导致更频繁的子词单元。

# 总结

总之，字节对编码（BPE）算法在现代自然语言处理（NLP）中发挥了关键作用，通过高效地将文本拆分为子词单元。其 token 学习器建立了一个子词词汇表，而 token 分词器使用这个词汇表进行文本分词。尽管存在一些缺点，BPE 在各种 NLP 任务中已经证明了其高效性，并且仍然是许多当前大型语言模型（如 GPT-3、BERT 和 RoBERTa）的基础组件。

如果你有任何问题或建议，请随时与我联系：

电子邮件：mina.ghashami@gmail.com

LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)

# 参考文献

1.  [稀有词汇的神经机器翻译与子词单元](https://arxiv.org/pdf/1508.07909.pdf)

1.  [日语和韩语语音搜索](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)

1.  [子词正则化：通过多子词候选项提升神经网络翻译模型](https://arxiv.org/pdf/1804.10959.pdf)

1.  [https://smltar.com/tokenization.html](https://smltar.com/tokenization.html)

1.  [SentencePiece：一个简单且语言无关的子词分词器和去分词器，用于神经文本处理](https://aclanthology.org/D18-2012.pdf)
