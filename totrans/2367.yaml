- en: Why Do We Even Have Neural Networks?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-do-we-even-have-neural-networks-72410cb9348e](https://towardsdatascience.com/why-do-we-even-have-neural-networks-72410cb9348e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Alternatives to Neural Networks: Taylor Series & Fourier Series'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page-----72410cb9348e--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----72410cb9348e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72410cb9348e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72410cb9348e--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----72410cb9348e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72410cb9348e--------------------------------)
    ·7 min read·Dec 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25f1853ad21413f753796df43b84dfb0.png)'
  prefs: []
  type: TYPE_IMG
- en: ”[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)."
    title=”neural network icons” Neural network icons created by imaginationlol —
    Flaticon.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have recently been writing a series of articles explaining the key concepts
    behind modern-day neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Egor Howell](../Images/e969a9f3c3357e1c80dcd0092d9a1288.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Egor Howell](https://medium.com/@egorhowell?source=post_page-----72410cb9348e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@egorhowell/list/neural-networks-616db722dbbb?source=post_page-----72410cb9348e--------------------------------)9
    stories![](../Images/66f86f14ddf20d9da9cac53dee54bae3.png)![](../Images/d968b0bd4358bb0d60bd296aef08a720.png)![](../Images/30f3f4354836e6d3d56175fe20cf6b7c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: One reason why neural networks are so powerful and popular is that they exhibit
    the [***universal approximation theorem***](https://en.wikipedia.org/wiki/Universal_approximation_theorem)***.***
    This means that a neural network can “learn” any function no matter how complex.
  prefs: []
  type: TYPE_NORMAL
- en: '[***“Functions describe the world.”***](https://www.youtube.com/watch?v=PAZTIAfaNr8)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A function, ***f(x)***, takes some input, ***x***, and gives an output ***y***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c132aa6dab2a0b9da872eb5ead558c9.png)'
  prefs: []
  type: TYPE_IMG
- en: How a mathematical function works. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: This function defines the relationship between the input and output. In most
    cases, we have the inputs and the corresponding outputs with the goal of the neural
    network to learn, or approximate, the function that maps between them.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks were invented around the 1950s and 1960s. Yet, at that time
    there were other known universal approximaters out there. So, why do we even have
    neural networks …
  prefs: []
  type: TYPE_NORMAL
- en: Taylor Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [***Taylor Series***](https://en.wikipedia.org/wiki/Taylor_series) represents
    a function as an infinite sum of terms calculated from the values of its derivatives
    at a single point. In other words, it’s a sum of infinite polynomials to approximate
    a function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f4f8b3545ec098a95d29e2aa431229e.png)'
  prefs: []
  type: TYPE_IMG
- en: Taylor Series. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: The above expression represents a function ***f*(*x*)** as an infinite sum,
    where ***f^n*** is the ***n-th*** derivative or order of ***f*** at the point
    ***a***, and ***n*!** denotes the factorial of ***n***.
  prefs: []
  type: TYPE_NORMAL
- en: See [here](https://math.stackexchange.com/questions/218421/what-are-the-practical-applications-of-the-taylor-series)
    if you are interested in learning why we use Taylor Series. Long story short,
    they are used to make ugly functions nice to work with!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There exists a simplification of the Taylor Series called the [***Maclaurin
    series***](https://brilliant.org/wiki/maclaurin-series/) where ***a = 0***.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d32e8122e1d96f78970db6ccf3c91ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Maclaurin Series. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Where in this case ***a_0***, ***a_1***, etc are the coefficients for the corresponding
    polynomials. The goal of the Taylor and Maclaurin series is to find the best values
    of the coefficients to approximate a given target function.
  prefs: []
  type: TYPE_NORMAL
- en: '*Sounds familiar?*'
  prefs: []
  type: TYPE_NORMAL
- en: We can even express the Maclaurin series in matrix notation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01e86c2f0b9fc6ffa4bde3fb7eb0367b.png)'
  prefs: []
  type: TYPE_IMG
- en: Maclaurin Series in matrix notation. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: This is pretty much a single-layer neural network! ***a_0*** is the bias term,
    ***a_1*** to ***a_n*** are the weights, and ***x*** to ***x^n*** are our features.
  prefs: []
  type: TYPE_NORMAL
- en: I like to think of the Taylor series as (loosely) [**polynomial regression**](https://medium.com/analytics-vidhya/understanding-polynomial-regression-5ac25b970e18#:~:text=Polynomial%20Regression%20is%20a%20form,the%20method%20of%20least%20squares.)**!**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In machine learning problems, we don’t actually have the whole function but
    rather a sample of data points. This is where we would pass in the ***x^n*** features
    as Taylor features to a neural network to learn the coefficients using backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: One last interesting property relating the Taylor series to machine learning
    is gradient descent. The general gradient descent formula comes from applying
    the Taylor series to the loss function. See [here](https://www.reddit.com/r/MachineLearning/comments/o01ox7/d_unfair_comparison_neural_networks_vs_taylor/)
    for the proof of this concept.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fourier Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**The Fourier series**](https://www.youtube.com/watch?v=Zgcry0SPUY8) is very
    similar to the Taylor series, but instead uses sines and cosines waves instead
    of polynomials. It’s defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: Any periodic function can be decomposed into a sum of sine and cosine waves
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is a very simple statement but its implications are significant.
  prefs: []
  type: TYPE_NORMAL
- en: Supplemental Video on Fourier Series.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, shown below are the functions ***sin(2x)*** and ***cos(3x)***
    and their corresponding summation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a183f5e7a6b960ebee5a7c0da7b0f444.png)'
  prefs: []
  type: TYPE_IMG
- en: Example sine waves and their sum. Plot generated by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: The ***sin(2x)*** and ***cos(3x)*** functionsare simple functions yet their
    summation (red line) leads to a more complex pattern. This is the main idea behind
    the Fourier series using multiple simple functions to build a complex one.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most interesting results from the Fourier series is being able to
    construct a [***square wave***](https://en.wikipedia.org/wiki/Square_wave) by
    summing infinite sines ([***harmonics***](https://electronics.stackexchange.com/questions/32310/what-exactly-are-harmonics-and-how-do-they-appear))
    of different *odd number* (orders)frequencies and amplitudes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0371b9c3f215a468de4e9619c39073fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Summation of odd sine waves. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ec07ecb0dd2cbc496d12451f330adb28.png)'
  prefs: []
  type: TYPE_IMG
- en: Using sine waves to create a square wave. Plot generated by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: What’s amazing about this result is that we have generated a sharp and straight-line
    plot from smooth sine functions. This shows the true power of the Fourier series
    to construct any periodic function.
  prefs: []
  type: TYPE_NORMAL
- en: The Fourier series is often applied to time series to model complex [**seasonal**](/seasonality-of-time-series-5b45b4809acd?sk=aaa7e3e31c0d2ec75e151a95ecce3098)
    patterns. This is called [**harmonic regression**](https://medium.com/towards-data-science/take-your-forecasting-to-the-next-level-with-harmonic-regression-5a8515f63295?sk=8c5a869f9825ce001f337cf5f478338f).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'As declared earlier, the Fourier series states that any periodic function can
    be broken down into a sum of sine and cosine waves. Mathematically, this is written
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5d85fb61a2151e425aa2b03b2994768.png)'
  prefs: []
  type: TYPE_IMG
- en: Fourier series. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '***A_0:*** *average value of the given periodic function*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***A_n:*** *coefficients of the cosine components*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***B_n:*** *coefficients of the sine components*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***n:*** *the**order which is the frequency of the sine or cosine wave, this
    is referred to as the ‘*[***harmonics***](https://electronics.stackexchange.com/questions/32310/what-exactly-are-harmonics-and-how-do-they-appear)*’*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***P:*** *period of the function*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Likewise, with the Taylor series, our aim with the Fourier Series is to find
    the coefficients ***A_n*** and ***B_n*** to our features, which in this case is
    the sine and cosine function.
  prefs: []
  type: TYPE_NORMAL
- en: Why Use Neural Networks Then?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both the Taylor and Fourier series can be viewed as universal function approximators
    and they both predate the neural network. So, why on earth do we have neural networks?
  prefs: []
  type: TYPE_NORMAL
- en: Well, the answer is not straightforward as there are many intricacies between
    the three methods. I have been fairly liberal when describing how the Taylor and
    Fourier series work, otherwise this article would be very, very exhaustive.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s break down some reasons why the Taylor or Fourier series can’t replace
    a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Taylor Series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main issue with the Taylor series is that they approximate around a single
    point. They are estimating a function over one value and its local region. We
    want to know what the whole function looks like over a large range. This means
    the Taylor series (polynomials) fails to generalise outside the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Fourier Series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One issue with the Fourier series is that it needs to *see* the function it’s
    going to approximate. For example, in time series it’s used to find the complex
    seasonal pattern in the data. But, it knows what the data looks like. A neural
    network aims to *learn* this function.
  prefs: []
  type: TYPE_NORMAL
- en: However, the main problem is the complexity of the Fourier series. The number
    of coefficients increases exponentially with the number of variables in the function
    we are trying to estimate. However, for a neural network, this is not necessarily
    the case.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we have a function ***f(x)***, that we can approximate well with 100
    coefficients. Now suppose we want to approximate ***f(x,y).*** Instead of having
    100 coefficients, we now have 100² = 10,000\. For ***f(x,y,z***), we have 100³.
    And this process goes on, increasing exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: What I am describing here is the [***curse of dimensionality***](https://en.wikipedia.org/wiki/Curse_of_dimensionality)***.***
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks on the other hand can accurately model (some) of these high
    dimensional functions without increasing their input dimensions too much.
  prefs: []
  type: TYPE_NORMAL
- en: No Free Lunch Theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to mention that neural networks will not always be better than
    the Taylor series and Fourier series. The beauty of machine learning is that it
    is the science of maths. You have to play around when fitting your model to find
    the best one. It may well be that adding Taylor or Fourier features will improve
    it. However, it may also make it worse. The goal is to find the best one, but
    this is different for every dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Another Thing!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----72410cb9348e--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----72410cb9348e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Connect With Me!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**GitHub**](https://github.com/egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References and Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Forecasting: Principles and Practice:* [https://otexts.com/fpp2/](https://otexts.com/fpp3/arima.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Excellent video explaining a similar concept*](https://www.youtube.com/watch?v=TkwXa7Cvfr8&t=923s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Great discussion on reddit regarding Neural Networks vs Taylor Series vs
    Fourier Series.*](https://www.reddit.com/r/MachineLearning/comments/o01ox7/d_unfair_comparison_neural_networks_vs_taylor/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
