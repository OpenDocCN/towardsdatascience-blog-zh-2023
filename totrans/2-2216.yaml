- en: Unlock the Secret to Efficient Batch Prediction Pipelines Using Python, a Feature
    Store and GCS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489](https://towardsdatascience.com/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[THE FULL STACK 7-STEPS MLOPS FRAMEWORK](https://towardsdatascience.com/tagged/full-stack-mlops)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lesson 3: Batch Prediction Pipeline. Package Python Modules with Poetry'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)[](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)
    ·15 min read·May 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d8383467c77df456d69215c8e1509ca6.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial represents **lesson 3 out of a 7-lesson course** that will walk
    you step-by-step through how to **design, implement, and deploy an ML system**
    using **MLOps good practices**. During the course, you will build a production-ready
    model to forecast energy consumption levels for the next 24 hours across multiple
    consumer types from Denmark.
  prefs: []
  type: TYPE_NORMAL
- en: '*By the end of this course, you will understand all the fundamentals of designing,
    coding and deploying an ML system using a batch-serving architecture.*'
  prefs: []
  type: TYPE_NORMAL
- en: This course *targets mid/advanced machine learning engineers* who want to level
    up their skills by building their own end-to-end projects.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, certificates are everywhere. Building advanced end-to-end projects
    that you can later show off is the best way to get recognition as a professional
    engineer.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Table of Contents:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Course Introduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Course Lessons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lesson 3: Batch Prediction Pipeline. Package Python Modules with Poetry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lesson 3: Code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Course Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***At the end of this 7 lessons course, you will know how to:***'
  prefs: []
  type: TYPE_NORMAL
- en: design a batch-serving architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Hopsworks as a feature store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: design a feature engineering pipeline that reads data from an API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build a training pipeline with hyper-parameter tunning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use W&B as an ML Platform to track your experiments, models, and metadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: implement a batch prediction pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Poetry to build your own Python packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: deploy your own private PyPi server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: orchestrate everything with Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use the predictions to code a web app using FastAPI and Streamlit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Docker to containerize your code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Great Expectations to ensure data validation and integrity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: monitor the performance of the predictions over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: deploy everything to GCP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build a CI/CD pipeline using GitHub Actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If that sounds like a lot, don't worry. After you cover this course, you will
    understand everything I said before. Most importantly, you will know WHY I used
    all these tools and how they work together as a system.
  prefs: []
  type: TYPE_NORMAL
- en: '**If you want to get the most out of this course,** [**I suggest you access
    the GitHub repository**](https://github.com/iusztinpaul/energy-forecasting) **containing
    all the lessons'' code. This course is designed to read and replicate the code
    along the articles quickly.**'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the course, you will know how to implement the diagram below.
    Don't worry if something doesn't make sense to you. I will explain everything
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of the architecture you will build during the course [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: By the **end of Lesson 3**, you will know how to implement and integrate the
    **batch prediction pipeline** and **package all the Python modules using Poetry**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Course Lessons:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Batch Serving. Feature Stores. Feature Engineering Pipelines.](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Training Pipelines. ML Platforms. Hyperparameter Tuning.](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Batch Prediction Pipeline. Package Python Modules with Poetry.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Private PyPi Server. Orchestrate Everything with Airflow.](https://medium.com/towards-data-science/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Data Validation for Quality and Integrity using GE. Model Performance Continuous
    Monitoring.](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Consume and Visualize your Model’s Predictions using FastAPI and Streamlit.
    Dockerize Everything.](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Deploy All the ML Components to GCP. Build a CI/CD Pipeline Using Github Actions.](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[[Bonus] Behind the Scenes of an ‘Imperfect’ ML Project — Lessons and Insights](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you want to grasp this lesson fully, we recommend you check out our [previous
    lesson](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee),
    which talks about designing a training pipeline that uses a feature store and
    an ML platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee?source=post_page-----17a1462ca489--------------------------------)
    [## A Guide to Building Effective Training Pipelines for Maximum Results'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 2: Training Pipelines. ML Platforms. Hyperparameter Tuning.'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee?source=post_page-----17a1462ca489--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Data Source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used a free & open API that provides hourly energy consumption values for
    all the energy consumer types within Denmark [1].
  prefs: []
  type: TYPE_NORMAL
- en: They provide an intuitive interface where you can easily query and visualize
    the data. [You can access the data here](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1].
  prefs: []
  type: TYPE_NORMAL
- en: 'The data has 4 main attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hour UTC:** the UTC datetime when the data point was observed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Price Area:** Denmark is divided into two price areas: DK1 and DK2 — divided
    by the Great Belt. DK1 is west of the Great Belt, and DK2 is east of the Great
    Belt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumer Type:** The consumer type is the Industry Code DE35, owned and maintained
    by Danish Energy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total Consumption:** Total electricity consumption in kWh'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note:** The observations have a lag of 15 days! But for our demo use case,
    that is not a problem, as we can simulate the same steps as it would in real-time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot from our web app showing how we forecasted the energy consumption
    for area = 1 and consumer_type = 212 [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: 'The data points have an hourly resolution. For example: "2023–04–15 21:00Z",
    "2023–04–15 20:00Z", "2023–04–15 19:00Z", etc.'
  prefs: []
  type: TYPE_NORMAL
- en: We will model the data as multiple time series. Each unique **price area** and
    **consumer type tuple represents its** unique time series.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we will build a model that independently forecasts the energy consumption
    for the next 24 hours for every time series.
  prefs: []
  type: TYPE_NORMAL
- en: '*Check out the video below to better understand what the data looks like* 👇'
  prefs: []
  type: TYPE_NORMAL
- en: Course & data source overview [Video by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: '**Lesson 3: Batch Prediction Pipeline. Package Python Modules with Poetry.**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Goal of Lesson 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This lesson will teach you how to build the batch prediction pipeline. Also,
    it will show you how to package into Python PyPi modules, using Poetry, all the
    code from the pipelines we have done so far in Lessons 1, 2, and 3\. 👇
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** In the next lesson, we will upload these Python modules into our
    own private PyPi server and install them from Airflow.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d77eb4a5b81e825ecb286bea9157466.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of the final architecture with the Lesson 3 components highlighted in
    blue [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall from Lesson 1, a model can be deployed in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: batch mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: request-response (e.g., RESTful API or gRPC)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: streaming mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: embedded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This course will *deploy the model in batch mode*.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss strategies to transition from batch to other methods when building
    the web app. You will see how natural it is.
  prefs: []
  type: TYPE_NORMAL
- en: But, if you are eager to compare the batch mode with a request-response serving
    mode, [check out my 5-minute article that explains how to serve a model using
    the request-response methodology](https://faun.pub/key-concepts-for-model-serving-38ccbb2de372Q).
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the main steps of deploying a model in batch mode, aka building
    a batch prediction pipeline?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1:** You will load the features from the feature store in batch mode.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2:** You will load the trained model from the model registry (in our
    case, we use Hopsworks as a model registry).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3:** You will forecast the energy consumption levels for the next 24
    hours.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4:** You will save the predictions in a GCP bucket.'
  prefs: []
  type: TYPE_NORMAL
- en: After, various consumers will read the predictions from the GCP bucket and use
    them accordingly. In our case, we implemented a dashboard using FastAPI and Streamlit.
  prefs: []
  type: TYPE_NORMAL
- en: '*Often, your initial deployment strategy will be in batch mode.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Why?**'
  prefs: []
  type: TYPE_NORMAL
- en: Because doing so, you don't have to focus on restrictions such as latency and
    throughput. By saving your predictions into some storage, you can quickly make
    your model online.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, batch mode is the easiest and fastest way of deploying your model while
    preserving a good experience for the end user of the applications.
  prefs: []
  type: TYPE_NORMAL
- en: A model is online when an application can access the predictions in real-time.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the predictions are not made in real-time, only accessed in real-time
    (e.g., read from the storage).
  prefs: []
  type: TYPE_NORMAL
- en: '*The biggest downside* of using this method is that your predictions will have
    a degree of lag. For example, in our use case, you make and save the predictions
    for the next 24 hours. Let’s assume that 2 hours pass without any new predictions.
    Now, you have predictions only for the next 22 hours.'
  prefs: []
  type: TYPE_NORMAL
- en: Where the number of predictions that you have to store is reasonable, you can
    bypass this issue by making the predictions often. In our example, we will make
    the predictions hourly — our data has a resolution of 1 hour. Thus, we solved
    the lag issue by constantly making and storing new predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '*But here comes the second problem with the batch prediction strategy*. Suppose
    the set of predictions is large. For example, you want to predict the recommendations
    for 1 million users with a database of 100 million items. Then, computing the
    predictions very often will be highly costly.'
  prefs: []
  type: TYPE_NORMAL
- en: Then you have to consider using other serving methods strongly.
  prefs: []
  type: TYPE_NORMAL
- en: '***But here is the catch.***'
  prefs: []
  type: TYPE_NORMAL
- en: Your application probably won't start with a database of 1 million users and
    100 million items. That means you can safely begin using a batch mode architecture
    and gradually shift to other methodologies when it makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: That is what most people do!
  prefs: []
  type: TYPE_NORMAL
- en: To get an intuition on how to shift to other methods, [check out this article](https://medium.com/mlearning-ai/this-is-what-you-need-to-know-to-build-an-mlops-end-to-end-architecture-c0be1deaa3ce)
    to learn about a *standardized ML architecture* *suggested by* *Google Cloud.*
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical Concepts & Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**GCS:** GCS stands for Google Cloud Storage, which is Google''s storage solution
    within GCP. It is similar to AWS S3 if you are more familiar with it.'
  prefs: []
  type: TYPE_NORMAL
- en: You can write to GCS any file. In our course, we will write Pandas DataFrames
    as parquet files.
  prefs: []
  type: TYPE_NORMAL
- en: '**GCS vs. Redis:** We choose to write our predictions in GCS because of 4 main
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Easy to setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No maintenance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to the free tier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also use GCP to deploy the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redis is a popular choice for caching your predictions to be later accessed
    by various clients.
  prefs: []
  type: TYPE_NORMAL
- en: '*Why?*'
  prefs: []
  type: TYPE_NORMAL
- en: Because you can access the data at low latency, improving the users' experience.
  prefs: []
  type: TYPE_NORMAL
- en: It would have been a good choice, but we wanted to simplify things.
  prefs: []
  type: TYPE_NORMAL
- en: Also, it is good practice to write the predictions on GCS for long-term storage
    and cache them in Redis for real-time access.
  prefs: []
  type: TYPE_NORMAL
- en: '**Poetry:** Poetry is my favorite Python virtual environment manager. It is
    similar to Conda, venv, and Pipenv. In my opinion, it is superior because:'
  prefs: []
  type: TYPE_NORMAL
- en: It offers you a **.lock** file that reflects the versions of all your sub-dependencies.
    Thus, replicating code is extremely easy and safe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can quickly build your module directly using Poetry. No other setup is required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can quickly deploy your module to a PiPy server using Poetry. No other setup
    is required, and more…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lesson 3: Code'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[You can access the GitHub repository here.](https://github.com/iusztinpaul/energy-forecasting)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** All the installation instructions are in the READMEs of the repository.
    Here we will jump straight to the code.'
  prefs: []
  type: TYPE_NORMAL
- en: '*All the code within Lesson 3 is located under the* [***batch-prediction-pipeline***](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)*folder.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The files under the [**batch-prediction-pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)folderare
    structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c47f99f9476d03de8ade95a30b583daa.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot that shows the structure of the batch-prediction-pipeline folder
    [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: All the code is located under the [**batch_prediction_pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline/batch_prediction_pipeline)directory
    (note the "_" instead of "-")**.**
  prefs: []
  type: TYPE_NORMAL
- en: Directly storing credentials in your git repository is a huge security risk.
    That is why you will inject sensitive information using a **.env** file.
  prefs: []
  type: TYPE_NORMAL
- en: The **.env.default** is an example of all the variables you must configure.
    It is also helpful to store default values for attributes that are not sensitive
    (e.g., project name).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b4f40ca19a12ac8ff070610a8530d46.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot of the .env.default file [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: Prepare Credentials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, you have to create a **.env** filewhere you will add all our credentials.
  prefs: []
  type: TYPE_NORMAL
- en: I already showed you in [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how to set up your **.env** file. Also, I explained in [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how the variables from the **.env** file are loaded from your **ML_PIPELINE_ROOT_DIR**
    directory into a **SETTINGS** Python dictionary to be used throughout your code.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if you want to replicate what I have done, I strongly recommend checking
    out [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f).
  prefs: []
  type: TYPE_NORMAL
- en: '*If you only want a light read, you can completely skip the "****Prepare Credentials****"
    step.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Lesson 3, you will use two services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Hopsworks](https://www.hopsworks.ai/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[GCP — Cloud Storage](https://cloud.google.com/storage)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[***Hopsworks***](https://www.hopsworks.ai/) ***(free)***'
  prefs: []
  type: TYPE_NORMAL
- en: We already showed you in [Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how to set up the credentials for **Hopsworks**. Please visit the ["Prepare Credentials"
    section from Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f),
    where we showed you in detail how to set up the API KEY for Hopsworks.
  prefs: []
  type: TYPE_NORMAL
- en: '[***GCP — Cloud Storage***](https://cloud.google.com/storage) ***(free)***'
  prefs: []
  type: TYPE_NORMAL
- en: While replicating this course, you will stick to the *GCP — Cloud Storage free
    tier*. You can store up to 5GB for free in GCP — Cloud Storage, which is far more
    than enough for our use case.
  prefs: []
  type: TYPE_NORMAL
- en: This configuration step will be longer, but I promise that it is not complicated.
    By the way, you will learn the basics of using a cloud vendor such as GCP.
  prefs: []
  type: TYPE_NORMAL
- en: First, go to GCP and create a project called "**energy_consumption"** (or any
    other name)**.** Afterward, go to your GCP project's "Cloud Storage" section and
    create a **non-public bucket** called "**hourly-batch-predictions**"**.** Pick
    any region, but just be aware of it—[official docs about creating a bucket on
    GCP](https://cloud.google.com/storage/docs/creating-buckets) [2].
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE:** *You might need to pick different names due to constant changes to
    the platform’s rules*. That is not an issue, just call them as you wish and change
    them in the **.env** file: *GOOGLE_CLOUD_PROJECT* (ours “energy_consumption”)
    and *GOOGLE_CLOUD_BUCKET_NAME (*ours “hourly-batch-predictions”*)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0cd1bf7dff7a64180918b371d064e98.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of the GCP — Cloud Storage view, where you must create your bucket
    [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: Now you finished creating all your GCP resources. The last step is to create
    a way to have read & write access to the GCP bucket directly from your Python
    code.
  prefs: []
  type: TYPE_NORMAL
- en: You can easily do this using GCP *service accounts.* I don't want to hijack
    the whole article with GCP configurations. Thus, [this GCP official doc shows
    you how to create a service account](https://cloud.google.com/iam/docs/service-accounts-create)
    [3].
  prefs: []
  type: TYPE_NORMAL
- en: '*When creating the service account, be aware of one thing!*'
  prefs: []
  type: TYPE_NORMAL
- en: Service accounts have attached different roles. A role is a way to configure
    your service account with various permissions.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, you need to configure your service account to have read & write access
    the your "**hourly-batch-predictions**" bucket.
  prefs: []
  type: TYPE_NORMAL
- en: You can easily do that by choosing the "**Storage Object Admin**" role when
    creating your service account.
  prefs: []
  type: TYPE_NORMAL
- en: The final step is to find a way to authenticate with your newly created service
    account in your Python code.
  prefs: []
  type: TYPE_NORMAL
- en: You can easily do that by going to your service account and creating a JSON
    key. Again, [here are the official GCP docs that will show you how to create a
    JSON key for your service account](https://cloud.google.com/iam/docs/keys-create-delete)
    [4].
  prefs: []
  type: TYPE_NORMAL
- en: '*Again, keep in mind one thing!*'
  prefs: []
  type: TYPE_NORMAL
- en: When creating the JSON key, you will download a JSON file.
  prefs: []
  type: TYPE_NORMAL
- en: After you download your JSON file, put it in a safe place and go to your **.env**
    file. There, change the value of *GOOGLE_CLOUD_SERVICE_ACCOUNT_JSON_PATH*with
    your absolute path to the JSON file.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b4f40ca19a12ac8ff070610a8530d46.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot of the .env.default file [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE:** Remember to change the *GOOGLE_CLOUD_PROJECT* and *GOOGLE_CLOUD_BUCKET_NAME*
    variables with your names.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Congratulations! You are done configuring GCS — Cloud Storage.*'
  prefs: []
  type: TYPE_NORMAL
- en: Now you have created a GCP project and bucket. Also, you have read & write access
    using your Python code through your service account. You log in with your service
    account with the help of the JSON file.
  prefs: []
  type: TYPE_NORMAL
- en: If something isn't working, let me know in the comments below or directly on
    [LinkedIn](https://www.linkedin.com/feed/).
  prefs: []
  type: TYPE_NORMAL
- en: Batch Prediction Pipeline — Main Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you can see, the main function follows the 4 steps of a batch prediction
    pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Loads data from the Feature Store in batch mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loads the model from the model registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Makes the predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It saves the predictions to the GCS bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most of the function is log lines 😆
  prefs: []
  type: TYPE_NORMAL
- en: Along these 4 main steps, you must load all the parameters from the metadata
    generated by previous steps, such as the **feature_view_version** and **model_version.**
  prefs: []
  type: TYPE_NORMAL
- en: Also, you have to get a reference to the Hopsworks Feature store.
  prefs: []
  type: TYPE_NORMAL
- en: After, you go straight to the 4 main steps that we will detail later in the
    tutorial 👇
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Loading Data From the Feature Store In Batch Mode'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This step is similar to what we have done in [Lesson 2](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)
    when loading data for training.
  prefs: []
  type: TYPE_NORMAL
- en: But this time, instead of downloading the data from a training dataset, we directly
    ask for a batch of data between a datetime range, using the **get_batch_data()**
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Doing so allows us to time travel to our desired datetime range and ask for
    the features we need. This method makes batch inference extremely easy.
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to prepare the indexes of the DataFrame as expected by **sktime**
    and to split it between X and y.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** This is an autoregressive process: we learn from past values of y
    to predict future values of y ( y = energy consumption levels). Thus, we will
    use only X as input to the model. We will use y only for visualization purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Loading the Model From the Model Registry'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Loading a model from the Hopsworks model registry is extremely easy.
  prefs: []
  type: TYPE_NORMAL
- en: The function below has as a parameter a reference to the Hopsworks project and
    the version of the model we want to download.
  prefs: []
  type: TYPE_NORMAL
- en: Using these two variables, you get a reference to the model registry. Afterward,
    you get a reference to the model itself using its name. In this case, it is **best_model**.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you download the artifact/model and load it into memory.
  prefs: []
  type: TYPE_NORMAL
- en: The trick here is that your model is versioned. Thus, you always know what model
    you are using.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** We uploaded the **best_model** in the model registry using the training
    pipeline explained in [Lesson 2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee).
    The training pipeline also provides us with a metadata dictionary that contains
    the latest model_version.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Forecast Energy Consumption Levels for the Next 24 Hours'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Sktime** makes forecasting extremely easy. The key line from the snippet
    below is "**predictions = model.predict(X=X_forecast)"**, which forecasts the
    energy consumption values for the next 24 hours.'
  prefs: []
  type: TYPE_NORMAL
- en: The forecasting horizon of 24 hours was given when the model was trained. Thus,
    it already knows how many data points into the future to forecast.
  prefs: []
  type: TYPE_NORMAL
- en: Also, you have to prepare the exogenous variable **X_forecast**. In time series
    forecasting, an exogenous variable is a feature that you already know it will
    happen in the future. For example, a holiday. Thus, based on your training data
    X which contains all the area and consumer types IDs, you can generate the **X_forecast**
    variable by mapping the datetime range into the forecasting range.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Save the Predictions to the Bucket'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last component is the function that saves everything to the GCP bucket.
  prefs: []
  type: TYPE_NORMAL
- en: This step is relatively straightforward, and the hard part was to configure
    your bucket and access credentials.
  prefs: []
  type: TYPE_NORMAL
- en: We get a reference to the bucket, iterate through X, y & predictions and write
    them to the bucket as a blob.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** Besides the predictions, we also save X and y to have everything
    in one place to quickly access everything we need and nicely render them in the
    web app.'
  prefs: []
  type: TYPE_NORMAL
- en: To get a reference to the bucket, you have to access the settings you configured
    at the beginning of the tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, you create a GCS client with the project name and the JSON credentials
    file path. Afterward, you can quickly get a reference to your given bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a blob to a bucket is highly similar to writing a regular file.
  prefs: []
  type: TYPE_NORMAL
- en: You get a reference to the blob you want to write and open the resource with
    "**with blob.open("wb") as f**".
  prefs: []
  type: TYPE_NORMAL
- en: Note that you opened the blob in binary format.
  prefs: []
  type: TYPE_NORMAL
- en: You are writing the data in parquet format, as it is an excellent trade-off
    between storage size and writing & reading performance.
  prefs: []
  type: TYPE_NORMAL
- en: Package Python Modules with Poetry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Poetry](https://python-poetry.org/) makes the building process extremely easy.'
  prefs: []
  type: TYPE_NORMAL
- en: The first obvious step is to use Poetry as your virtual environment manager.
    That means you already have the "**pyproject.toml"** and "**poetry.lock**" files
    — we already provided these files for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, all you have to do is to go to your project at the same level as your
    Poetry files (the ones mentioned above — for example, go to your [batch-prediction-pipeline](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)
    directory) and run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will create a **dist** folder containing your package as a **wheel.** Now
    you can directly install your package using the wheel file or deploy it to a PyPi
    server.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy it, configure your PyPi server credentials with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, deploy it using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: And that was it. I was amazed at how easy Poetry can make this process.
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, building and deploying your Python package is a tedious and lengthy
    process.
  prefs: []
  type: TYPE_NORMAL
- en: In [Lesson 4](/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff),
    you will deploy your private PyPi server and deploy all the code you have written
    until this point using the commands I showed you above.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You finished the **third lesson** from the **Full Stack 7-Steps
    MLOps Framework** course.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have reached this far, you know how to:'
  prefs: []
  type: TYPE_NORMAL
- en: choose the right architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: access data from the feature store in batch mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: download your model from the model registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build an inference pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: save your predictions to GCS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you understand the power of using and implementing a batch prediction
    architecture, you can quickly serve models in real-time while paving your way
    for other fancier serving methods.
  prefs: []
  type: TYPE_NORMAL
- en: Check out [Lesson 4](/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)
    to learn about hosting your own private PyPi server and orchestrating all the
    pipelines using Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: '**Also,** [**you can access the GitHub repository here**](https://github.com/iusztinpaul/energy-forecasting)**.**'
  prefs: []
  type: TYPE_NORMAL
- en: 💡 My goal is to help machine learning engineers level up in designing and productionizing
    ML systems. Follow me on [LinkedIn](https://www.linkedin.com/in/pauliusztin/)
    or subscribe to my [weekly newsletter](https://pauliusztin.substack.com/) for
    more insights!
  prefs: []
  type: TYPE_NORMAL
- en: 🔥 If you enjoy reading articles like this and wish to support my writing, consider
    [becoming a Medium member](https://pauliusztin.medium.com/membership). By using
    [my referral link](https://pauliusztin.medium.com/membership), you can support
    me without any extra cost while enjoying limitless access to Medium’s rich collection
    of stories.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pauliusztin.medium.com/membership?source=post_page-----17a1462ca489--------------------------------)
    [## Join Medium with my referral link - Paul Iusztin'
  prefs: []
  type: TYPE_NORMAL
- en: 🤖 Join to get exclusive content about designing and building production-ready
    ML systems 🚀 Unlock full access to…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----17a1462ca489--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [Energy Consumption per DE35 Industry Code from Denmark API](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour),
    [Denmark Energy Data Service](https://www.energidataservice.dk/about/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Create buckets](https://cloud.google.com/storage/docs/creating-buckets),
    GCP Cloud Storage Docs'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [Create service accounts](https://cloud.google.com/iam/docs/service-accounts-create),
    GCP IAM Docs'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [Create and delete service account keys](https://cloud.google.com/iam/docs/keys-create-delete),
    GCP IAM Docs'
  prefs: []
  type: TYPE_NORMAL
