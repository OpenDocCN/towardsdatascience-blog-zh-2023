- en: Unlock the Secret to Efficient Batch Prediction Pipelines Using Python, a Feature
    Store and GCS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è§£é”ä½¿ç”¨ Pythonã€ç‰¹å¾å­˜å‚¨å’Œ GCS çš„é«˜æ•ˆæ‰¹é‡é¢„æµ‹ç®¡é“çš„ç§˜å¯†
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489](https://towardsdatascience.com/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489](https://towardsdatascience.com/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)
- en: '[THE FULL STACK 7-STEPS MLOPS FRAMEWORK](https://towardsdatascience.com/tagged/full-stack-mlops)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[å®Œæ•´å †æ ˆ 7 æ­¥ MLOps æ¡†æ¶](https://towardsdatascience.com/tagged/full-stack-mlops)'
- en: 'Lesson 3: Batch Prediction Pipeline. Package Python Modules with Poetry'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ 3ï¼šæ‰¹é‡é¢„æµ‹ç®¡é“ã€‚ä½¿ç”¨ Poetry æ‰“åŒ… Python æ¨¡å—
- en: '[](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)[](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)[](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)
    Â·15 min readÂ·May 12, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 15 åˆ†é’ŸÂ·2023å¹´5æœˆ12æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d8383467c77df456d69215c8e1509ca6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8383467c77df456d69215c8e1509ca6.png)'
- en: Photo by [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç”± [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    æ‹æ‘„ï¼Œæ¥è‡ª [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This tutorial represents **lesson 3 out of a 7-lesson course** that will walk
    you step-by-step through how to **design, implement, and deploy an ML system**
    using **MLOps good practices**. During the course, you will build a production-ready
    model to forecast energy consumption levels for the next 24 hours across multiple
    consumer types from Denmark.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹ä»£è¡¨äº†**ä¸€ä¸ªåŒ…å« 7 èŠ‚è¯¾ç¨‹ä¸­çš„ç¬¬ 3 èŠ‚**ï¼Œå°†é€æ­¥æŒ‡å¯¼ä½ å¦‚ä½•**è®¾è®¡ã€å®æ–½å’Œéƒ¨ç½² ML ç³»ç»Ÿ**ï¼Œå¹¶è¿ç”¨**MLOps çš„è‰¯å¥½å®è·µ**ã€‚åœ¨è¯¾ç¨‹ä¸­ï¼Œä½ å°†æ„å»ºä¸€ä¸ªç”Ÿäº§å°±ç»ªçš„æ¨¡å‹ï¼Œä»¥é¢„æµ‹æ¥è‡ªä¸¹éº¦çš„ä¸åŒæ¶ˆè´¹è€…ç±»å‹åœ¨æ¥ä¸‹æ¥çš„
    24 å°æ—¶å†…çš„èƒ½æºæ¶ˆè€—æ°´å¹³ã€‚
- en: '*By the end of this course, you will understand all the fundamentals of designing,
    coding and deploying an ML system using a batch-serving architecture.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨æœ¬è¯¾ç¨‹ç»“æŸæ—¶ï¼Œä½ å°†ç†è§£ä½¿ç”¨æ‰¹é‡æœåŠ¡æ¶æ„è®¾è®¡ã€ç¼–ç å’Œéƒ¨ç½² ML ç³»ç»Ÿçš„æ‰€æœ‰åŸºç¡€çŸ¥è¯†ã€‚*'
- en: This course *targets mid/advanced machine learning engineers* who want to level
    up their skills by building their own end-to-end projects.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è¯¾ç¨‹*é¢å‘ä¸­çº§/é«˜çº§æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ*ï¼Œä»–ä»¬å¸Œæœ›é€šè¿‡æ„å»ºè‡ªå·±çš„ç«¯åˆ°ç«¯é¡¹ç›®æ¥æå‡æŠ€èƒ½ã€‚
- en: Nowadays, certificates are everywhere. Building advanced end-to-end projects
    that you can later show off is the best way to get recognition as a professional
    engineer.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè¯ä¹¦æ— å¤„ä¸åœ¨ã€‚æ„å»ºé«˜çº§ç«¯åˆ°ç«¯é¡¹ç›®ï¼Œå¹¶åœ¨ä¹‹åå±•ç¤ºå‡ºæ¥ï¼Œæ˜¯è·å¾—ä¸“ä¸šå·¥ç¨‹å¸ˆè®¤å¯çš„æœ€ä½³æ–¹å¼ã€‚
- en: 'Table of Contents:'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›®å½•ï¼š
- en: Course Introduction
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ä»‹ç»
- en: Course Lessons
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹å†…å®¹
- en: Data Source
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®æ¥æº
- en: 'Lesson 3: Batch Prediction Pipeline. Package Python Modules with Poetry.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ 3ï¼šæ‰¹é‡é¢„æµ‹ç®¡é“ã€‚ä½¿ç”¨ Poetry æ‰“åŒ… Python æ¨¡å—ã€‚
- en: 'Lesson 3: Code'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ 3ï¼šä»£ç 
- en: Conclusion
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: References
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: Course Introduction
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ä»‹ç»
- en: '***At the end of this 7 lessons course, you will know how to:***'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '***åœ¨è¿™ 7 èŠ‚è¯¾ç¨‹ç»“æŸæ—¶ï¼Œä½ å°†å­¦ä¼šï¼š***'
- en: design a batch-serving architecture
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¾è®¡ä¸€ä¸ªæ‰¹é‡æœåŠ¡æ¶æ„
- en: use Hopsworks as a feature store
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Hopsworks ä½œä¸ºç‰¹å¾å­˜å‚¨
- en: design a feature engineering pipeline that reads data from an API
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¾è®¡ä¸€ä¸ªä» API è¯»å–æ•°æ®çš„ç‰¹å¾å·¥ç¨‹ç®¡é“
- en: build a training pipeline with hyper-parameter tunning
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªå¸¦æœ‰è¶…å‚æ•°è°ƒä¼˜çš„è®­ç»ƒç®¡é“
- en: use W&B as an ML Platform to track your experiments, models, and metadata
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ W&B ä½œä¸º ML å¹³å°æ¥è·Ÿè¸ªä½ çš„å®éªŒã€æ¨¡å‹å’Œå…ƒæ•°æ®
- en: implement a batch prediction pipeline
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ç°ä¸€ä¸ªæ‰¹é‡é¢„æµ‹ç®¡é“
- en: use Poetry to build your own Python packages
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Poetry æ„å»ºä½ è‡ªå·±çš„ Python åŒ…
- en: deploy your own private PyPi server
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éƒ¨ç½²ä½ è‡ªå·±çš„ç§æœ‰ PyPi æœåŠ¡å™¨
- en: orchestrate everything with Airflow
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Airflow åè°ƒä¸€åˆ‡
- en: use the predictions to code a web app using FastAPI and Streamlit
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é¢„æµ‹æ¥ç¼–å†™ä¸€ä¸ªä½¿ç”¨ FastAPI å’Œ Streamlit çš„ Web åº”ç”¨
- en: use Docker to containerize your code
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Docker å¯¹ä»£ç è¿›è¡Œå®¹å™¨åŒ–
- en: use Great Expectations to ensure data validation and integrity
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Great Expectations ç¡®ä¿æ•°æ®éªŒè¯å’Œå®Œæ•´æ€§
- en: monitor the performance of the predictions over time
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éšæ—¶é—´ç›‘æ§é¢„æµ‹çš„æ€§èƒ½
- en: deploy everything to GCP
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰å†…å®¹éƒ¨ç½²åˆ° GCP
- en: build a CI/CD pipeline using GitHub Actions
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ GitHub Actions æ„å»º CI/CD ç®¡é“
- en: If that sounds like a lot, don't worry. After you cover this course, you will
    understand everything I said before. Most importantly, you will know WHY I used
    all these tools and how they work together as a system.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¿™äº›å¬èµ·æ¥å¾ˆå¤šï¼Œä¸ç”¨æ‹…å¿ƒã€‚å®Œæˆè¿™é—¨è¯¾ç¨‹åï¼Œä½ ä¼šç†è§£æˆ‘ä¹‹å‰è¯´çš„æ‰€æœ‰å†…å®¹ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä½ å°†çŸ¥é“**ä¸ºä»€ä¹ˆ**æˆ‘ä½¿ç”¨äº†è¿™äº›å·¥å…·ä»¥åŠå®ƒä»¬å¦‚ä½•ä½œä¸ºä¸€ä¸ªç³»ç»ŸååŒå·¥ä½œã€‚
- en: '**If you want to get the most out of this course,** [**I suggest you access
    the GitHub repository**](https://github.com/iusztinpaul/energy-forecasting) **containing
    all the lessons'' code. This course is designed to read and replicate the code
    along the articles quickly.**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚æœä½ æƒ³ä»è¿™é—¨è¯¾ç¨‹ä¸­è·å¾—æœ€å¤§æ”¶ç›Šï¼Œ** [**æˆ‘å»ºè®®ä½ è®¿é—®åŒ…å«æ‰€æœ‰è¯¾ç¨‹ä»£ç çš„ GitHub ä»“åº“**](https://github.com/iusztinpaul/energy-forecasting)
    **ã€‚è¿™é—¨è¯¾ç¨‹çš„è®¾è®¡ç›®çš„æ˜¯å¿«é€Ÿé˜…è¯»å’Œå¤ç°æ–‡ç« ä¸­çš„ä»£ç ã€‚**'
- en: By the end of the course, you will know how to implement the diagram below.
    Don't worry if something doesn't make sense to you. I will explain everything
    in detail.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯¾ç¨‹ç»“æŸæ—¶ï¼Œä½ å°†çŸ¥é“å¦‚ä½•å®ç°ä¸‹é¢çš„å›¾ç¤ºã€‚ä¸è¦æ‹…å¿ƒå¦‚æœæœ‰äº›åœ°æ–¹ä¸å¤ªæ˜ç™½ã€‚æˆ‘ä¼šè¯¦ç»†è§£é‡Šæ‰€æœ‰å†…å®¹ã€‚
- en: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
- en: Diagram of the architecture you will build during the course [Image by the Author].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†åœ¨è¯¾ç¨‹ä¸­æ„å»ºçš„æ¶æ„å›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: By the **end of Lesson 3**, you will know how to implement and integrate the
    **batch prediction pipeline** and **package all the Python modules using Poetry**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**ç¬¬ 3 è¯¾ç»“æŸæ—¶**ï¼Œä½ å°†å­¦ä¼šå¦‚ä½•å®ç°å’Œé›†æˆ**æ‰¹é‡é¢„æµ‹ç®¡é“**ä»¥åŠ**ä½¿ç”¨ Poetry æ‰“åŒ…æ‰€æœ‰ Python æ¨¡å—**ã€‚
- en: 'Course Lessons:'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹å†…å®¹ï¼š
- en: '[Batch Serving. Feature Stores. Feature Engineering Pipelines.](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ‰¹é‡æœåŠ¡ã€‚ç‰¹å¾å­˜å‚¨ã€‚ç‰¹å¾å·¥ç¨‹ç®¡é“ã€‚](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
- en: '[Training Pipelines. ML Platforms. Hyperparameter Tuning.](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[è®­ç»ƒç®¡é“ã€‚ML å¹³å°ã€‚è¶…å‚æ•°è°ƒæ•´ã€‚](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
- en: '**Batch Prediction Pipeline. Package Python Modules with Poetry.**'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ‰¹é‡é¢„æµ‹ç®¡é“ã€‚ä½¿ç”¨ Poetry æ‰“åŒ… Python æ¨¡å—ã€‚**'
- en: '[Private PyPi Server. Orchestrate Everything with Airflow.](https://medium.com/towards-data-science/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç§æœ‰ PyPi æœåŠ¡å™¨ã€‚ä½¿ç”¨ Airflow åè°ƒä¸€åˆ‡ã€‚](https://medium.com/towards-data-science/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)'
- en: '[Data Validation for Quality and Integrity using GE. Model Performance Continuous
    Monitoring.](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨ GE è¿›è¡Œæ•°æ®éªŒè¯ä»¥ç¡®ä¿è´¨é‡å’Œå®Œæ•´æ€§ã€‚æ¨¡å‹æ€§èƒ½æŒç»­ç›‘æ§ã€‚](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
- en: '[Consume and Visualize your Modelâ€™s Predictions using FastAPI and Streamlit.
    Dockerize Everything.](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨ FastAPI å’Œ Streamlit æ¶ˆè´¹å’Œå¯è§†åŒ–æ¨¡å‹é¢„æµ‹ã€‚å¯¹ä¸€åˆ‡è¿›è¡Œ Docker åŒ–ã€‚](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
- en: '[Deploy All the ML Components to GCP. Build a CI/CD Pipeline Using Github Actions.](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å°†æ‰€æœ‰ ML ç»„ä»¶éƒ¨ç½²åˆ° GCPã€‚ä½¿ç”¨ GitHub Actions æ„å»º CI/CD ç®¡é“ã€‚](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
- en: '[[Bonus] Behind the Scenes of an â€˜Imperfectâ€™ ML Project â€” Lessons and Insights](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[é¢å¤–å†…å®¹] â€˜ä¸å®Œç¾â€™ ML é¡¹ç›®çš„å¹•å â€” æ•™è®­ä¸è§è§£](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
- en: 'If you want to grasp this lesson fully, we recommend you check out our [previous
    lesson](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee),
    which talks about designing a training pipeline that uses a feature store and
    an ML platform:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³å…¨é¢æŒæ¡æœ¬èŠ‚å†…å®¹ï¼Œæˆ‘ä»¬å»ºè®®ä½ æŸ¥çœ‹æˆ‘ä»¬çš„[ä¸Šä¸€ç¯‡è¯¾ç¨‹](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)ï¼Œå…¶ä¸­è®²è¿°äº†è®¾è®¡ä¸€ä¸ªä½¿ç”¨ç‰¹å¾å­˜å‚¨å’ŒMLå¹³å°çš„è®­ç»ƒç®¡é“ï¼š
- en: '[](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee?source=post_page-----17a1462ca489--------------------------------)
    [## A Guide to Building Effective Training Pipelines for Maximum Results'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee?source=post_page-----17a1462ca489--------------------------------)
    [## å»ºç«‹æœ‰æ•ˆè®­ç»ƒç®¡é“ä»¥è·å¾—æœ€ä½³ç»“æœæŒ‡å—'
- en: 'Lesson 2: Training Pipelines. ML Platforms. Hyperparameter Tuning.'
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¬¬2èŠ‚ï¼šè®­ç»ƒç®¡é“ã€‚MLå¹³å°ã€‚è¶…å‚æ•°è°ƒæ•´ã€‚
- en: towardsdatascience.com](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee?source=post_page-----17a1462ca489--------------------------------)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee?source=post_page-----17a1462ca489--------------------------------)
- en: Data Source
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®æ¥æº
- en: We used a free & open API that provides hourly energy consumption values for
    all the energy consumer types within Denmark [1].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå…è´¹çš„å¼€æ”¾APIï¼Œæä¾›ä¸¹éº¦æ‰€æœ‰èƒ½æºæ¶ˆè´¹è€…ç±»å‹çš„æ¯å°æ—¶èƒ½æºæ¶ˆè€—å€¼[1]ã€‚
- en: They provide an intuitive interface where you can easily query and visualize
    the data. [You can access the data here](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬æä¾›äº†ä¸€ä¸ªç›´è§‚çš„ç•Œé¢ï¼Œæ‚¨å¯ä»¥è½»æ¾æŸ¥è¯¢å’Œå¯è§†åŒ–æ•°æ®ã€‚[æ‚¨å¯ä»¥åœ¨è¿™é‡Œè®¿é—®æ•°æ®](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1]ã€‚
- en: 'The data has 4 main attributes:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®æœ‰4ä¸ªä¸»è¦å±æ€§ï¼š
- en: '**Hour UTC:** the UTC datetime when the data point was observed.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°æ—¶UTCï¼š** è§‚å¯Ÿåˆ°æ•°æ®ç‚¹çš„UTCæ—¥æœŸæ—¶é—´ã€‚'
- en: '**Price Area:** Denmark is divided into two price areas: DK1 and DK2 â€” divided
    by the Great Belt. DK1 is west of the Great Belt, and DK2 is east of the Great
    Belt.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä»·æ ¼åŒºåŸŸï¼š** ä¸¹éº¦è¢«åˆ†ä¸ºä¸¤ä¸ªä»·æ ¼åŒºåŸŸï¼šDK1å’ŒDK2â€”â€”ç”±å¤§è´å°”ç‰¹åˆ†éš”ã€‚DK1ä½äºå¤§è´å°”ç‰¹ä»¥è¥¿ï¼ŒDK2ä½äºå¤§è´å°”ç‰¹ä»¥ä¸œã€‚'
- en: '**Consumer Type:** The consumer type is the Industry Code DE35, owned and maintained
    by Danish Energy.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¶ˆè´¹è€…ç±»å‹ï¼š** æ¶ˆè´¹è€…ç±»å‹æ˜¯å·¥ä¸šä»£ç DE35ï¼Œç”±ä¸¹éº¦èƒ½æºå…¬å¸æ‹¥æœ‰å’Œç»´æŠ¤ã€‚'
- en: '**Total Consumption:** Total electricity consumption in kWh'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ€»æ¶ˆè€—ï¼š** æ€»ç”µåŠ›æ¶ˆè€—ï¼ˆä»¥åƒç“¦æ—¶ä¸ºå•ä½ï¼‰'
- en: '**Note:** The observations have a lag of 15 days! But for our demo use case,
    that is not a problem, as we can simulate the same steps as it would in real-time.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** è§‚å¯Ÿæ•°æ®æœ‰15å¤©çš„å»¶è¿Ÿï¼ä½†å¯¹äºæˆ‘ä»¬çš„æ¼”ç¤ºç”¨ä¾‹ï¼Œè¿™ä¸æ˜¯é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥æ¨¡æ‹Ÿå®æ—¶ä¸­çš„ç›¸åŒæ­¥éª¤ã€‚'
- en: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
- en: A screenshot from our web app showing how we forecasted the energy consumption
    for area = 1 and consumer_type = 212 [Image by the Author].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç½‘é¡µåº”ç”¨ç¨‹åºçš„æˆªå›¾ï¼Œæ˜¾ç¤ºäº†æˆ‘ä»¬å¦‚ä½•é¢„æµ‹åŒºåŸŸ=1å’Œæ¶ˆè´¹è€…ç±»å‹=212çš„èƒ½æºæ¶ˆè€—[ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: 'The data points have an hourly resolution. For example: "2023â€“04â€“15 21:00Z",
    "2023â€“04â€“15 20:00Z", "2023â€“04â€“15 19:00Z", etc.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç‚¹å…·æœ‰æ¯å°æ—¶åˆ†è¾¨ç‡ã€‚ä¾‹å¦‚ï¼šâ€œ2023â€“04â€“15 21:00Zâ€ï¼Œâ€œ2023â€“04â€“15 20:00Zâ€ï¼Œâ€œ2023â€“04â€“15 19:00Zâ€ç­‰ã€‚
- en: We will model the data as multiple time series. Each unique **price area** and
    **consumer type tuple represents its** unique time series.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ•°æ®å»ºæ¨¡ä¸ºå¤šä¸ªæ—¶é—´åºåˆ—ã€‚æ¯ä¸ªç‹¬ç‰¹çš„**ä»·æ ¼åŒºåŸŸ**å’Œ**æ¶ˆè´¹è€…ç±»å‹å…ƒç»„ä»£è¡¨å…¶**ç‹¬ç‰¹çš„æ—¶é—´åºåˆ—ã€‚
- en: Thus, we will build a model that independently forecasts the energy consumption
    for the next 24 hours for every time series.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œç‹¬ç«‹é¢„æµ‹æ¯ä¸ªæ—¶é—´åºåˆ—çš„æœªæ¥24å°æ—¶èƒ½æºæ¶ˆè€—ã€‚
- en: '*Check out the video below to better understand what the data looks like* ğŸ‘‡'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*æŸ¥çœ‹ä¸‹é¢çš„è§†é¢‘ï¼Œä»¥æ›´å¥½åœ°äº†è§£æ•°æ®çš„æ ·å­* ğŸ‘‡'
- en: Course & data source overview [Video by the Author].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹å’Œæ•°æ®æ¥æºæ¦‚è§ˆ[ä½œè€…æä¾›çš„è§†é¢‘]ã€‚
- en: '**Lesson 3: Batch Prediction Pipeline. Package Python Modules with Poetry.**'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç¬¬3èŠ‚ï¼šæ‰¹é‡é¢„æµ‹ç®¡é“ã€‚ä½¿ç”¨Poetryæ‰“åŒ…Pythonæ¨¡å—ã€‚**'
- en: The Goal of Lesson 3
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬3èŠ‚çš„ç›®æ ‡
- en: This lesson will teach you how to build the batch prediction pipeline. Also,
    it will show you how to package into Python PyPi modules, using Poetry, all the
    code from the pipelines we have done so far in Lessons 1, 2, and 3\. ğŸ‘‡
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è¯¾ç¨‹å°†æ•™ä½ å¦‚ä½•æ„å»ºæ‰¹é‡é¢„æµ‹ç®¡é“ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨Poetryå°†æˆ‘ä»¬åœ¨ç¬¬1ã€ç¬¬2å’Œç¬¬3èŠ‚ä¸­å®Œæˆçš„æ‰€æœ‰ç®¡é“ä»£ç æ‰“åŒ…æˆPython PyPiæ¨¡å—ã€‚ğŸ‘‡
- en: '**Note:** In the next lesson, we will upload these Python modules into our
    own private PyPi server and install them from Airflow.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠè¿™äº›Pythonæ¨¡å—ä¸Šä¼ åˆ°æˆ‘ä»¬è‡ªå·±çš„ç§æœ‰PyPiæœåŠ¡å™¨ï¼Œå¹¶ä»Airflowä¸­å®‰è£…å®ƒä»¬ã€‚'
- en: '![](../Images/7d77eb4a5b81e825ecb286bea9157466.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d77eb4a5b81e825ecb286bea9157466.png)'
- en: Diagram of the final architecture with the Lesson 3 components highlighted in
    blue [Image by the Author].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæ¶æ„å›¾ï¼Œå…¶ä¸­ç¬¬ 3 è¯¾çš„ç»„ä»¶ä»¥è“è‰²çªå‡ºæ˜¾ç¤º [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: 'If you recall from Lesson 1, a model can be deployed in the following ways:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è®°å¾—ç¬¬ 1 è¯¾ï¼Œæ¨¡å‹å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿›è¡Œéƒ¨ç½²ï¼š
- en: batch mode
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¹å¤„ç†æ¨¡å¼
- en: request-response (e.g., RESTful API or gRPC)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯·æ±‚-å“åº”ï¼ˆä¾‹å¦‚ï¼ŒRESTful API æˆ– gRPCï¼‰
- en: streaming mode
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æµå¼æ¨¡å¼
- en: embedded
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åµŒå…¥å¼
- en: This course will *deploy the model in batch mode*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è¯¾ç¨‹å°†*ä»¥æ‰¹å¤„ç†æ¨¡å¼éƒ¨ç½²æ¨¡å‹*ã€‚
- en: We will discuss strategies to transition from batch to other methods when building
    the web app. You will see how natural it is.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è®¨è®ºåœ¨æ„å»º web åº”ç”¨ç¨‹åºæ—¶å¦‚ä½•ä»æ‰¹å¤„ç†è¿‡æ¸¡åˆ°å…¶ä»–æ–¹æ³•ã€‚ä½ ä¼šå‘ç°è¿™éå¸¸è‡ªç„¶ã€‚
- en: But, if you are eager to compare the batch mode with a request-response serving
    mode, [check out my 5-minute article that explains how to serve a model using
    the request-response methodology](https://faun.pub/key-concepts-for-model-serving-38ccbb2de372Q).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œå¦‚æœä½ æ¸´æœ›æ¯”è¾ƒæ‰¹å¤„ç†æ¨¡å¼ä¸è¯·æ±‚-å“åº”æœåŠ¡æ¨¡å¼ï¼Œ[å¯ä»¥æŸ¥çœ‹æˆ‘å†™çš„ 5 åˆ†é’Ÿæ–‡ç« ï¼Œè§£é‡Šå¦‚ä½•ä½¿ç”¨è¯·æ±‚-å“åº”æ–¹æ³•æœåŠ¡æ¨¡å‹](https://faun.pub/key-concepts-for-model-serving-38ccbb2de372Q)ã€‚
- en: '**What are the main steps of deploying a model in batch mode, aka building
    a batch prediction pipeline?**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‰¹å¤„ç†æ¨¡å¼ä¸‹éƒ¨ç½²æ¨¡å‹çš„ä¸»è¦æ­¥éª¤æ˜¯ä»€ä¹ˆï¼Œä¹Ÿå°±æ˜¯æ„å»ºæ‰¹é‡é¢„æµ‹ç®¡é“çš„æ­¥éª¤ï¼Ÿ**'
- en: '**Step 1:** You will load the features from the feature store in batch mode.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1ï¼š** ä½ å°†ä»ç‰¹å¾åº“ä¸­ä»¥æ‰¹å¤„ç†æ¨¡å¼åŠ è½½ç‰¹å¾ã€‚'
- en: '**Step 2:** You will load the trained model from the model registry (in our
    case, we use Hopsworks as a model registry).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 2ï¼š** ä½ å°†ä»æ¨¡å‹æ³¨å†Œè¡¨ä¸­åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ Hopsworks ä½œä¸ºæ¨¡å‹æ³¨å†Œè¡¨ï¼‰ã€‚'
- en: '**Step 3:** You will forecast the energy consumption levels for the next 24
    hours.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 3ï¼š** ä½ å°†é¢„æµ‹æœªæ¥ 24 å°æ—¶çš„èƒ½æºæ¶ˆè€—æ°´å¹³ã€‚'
- en: '**Step 4:** You will save the predictions in a GCP bucket.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 4ï¼š** ä½ å°†æŠŠé¢„æµ‹ç»“æœä¿å­˜åˆ° GCP å­˜å‚¨æ¡¶ä¸­ã€‚'
- en: After, various consumers will read the predictions from the GCP bucket and use
    them accordingly. In our case, we implemented a dashboard using FastAPI and Streamlit.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹åï¼Œå„ç§æ¶ˆè´¹è€…å°†ä» GCP å­˜å‚¨æ¡¶ä¸­è¯»å–é¢„æµ‹å¹¶ç›¸åº”ä½¿ç”¨å®ƒä»¬ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ FastAPI å’Œ Streamlit å®ç°äº†ä¸€ä¸ªä»ªè¡¨æ¿ã€‚
- en: '*Often, your initial deployment strategy will be in batch mode.*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*é€šå¸¸ï¼Œä½ çš„åˆå§‹éƒ¨ç½²ç­–ç•¥å°†æ˜¯æ‰¹å¤„ç†æ¨¡å¼ã€‚*'
- en: '**Why?**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆï¼Ÿ**'
- en: Because doing so, you don't have to focus on restrictions such as latency and
    throughput. By saving your predictions into some storage, you can quickly make
    your model online.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºè¿™æ ·åšï¼Œä½ ä¸å¿…å…³æ³¨å»¶è¿Ÿå’Œååé‡ç­‰é™åˆ¶ã€‚é€šè¿‡å°†é¢„æµ‹ä¿å­˜åˆ°æŸäº›å­˜å‚¨ä¸­ï¼Œä½ å¯ä»¥å¿«é€Ÿä½¿æ¨¡å‹ä¸Šçº¿ã€‚
- en: Thus, batch mode is the easiest and fastest way of deploying your model while
    preserving a good experience for the end user of the applications.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ‰¹å¤„ç†æ¨¡å¼æ˜¯éƒ¨ç½²æ¨¡å‹çš„æœ€ç®€å•å’Œæœ€å¿«çš„æ–¹å¼ï¼ŒåŒæ—¶ä¿æŒåº”ç”¨ç¨‹åºæœ€ç»ˆç”¨æˆ·çš„è‰¯å¥½ä½“éªŒã€‚
- en: A model is online when an application can access the predictions in real-time.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å½“åº”ç”¨ç¨‹åºå¯ä»¥å®æ—¶è®¿é—®é¢„æµ‹æ—¶ï¼Œæ¨¡å‹æ˜¯åœ¨çº¿çš„ã€‚
- en: Note that the predictions are not made in real-time, only accessed in real-time
    (e.g., read from the storage).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œé¢„æµ‹ä¸æ˜¯å®æ—¶ç”Ÿæˆçš„ï¼Œä»…åœ¨å®æ—¶è®¿é—®ï¼ˆä¾‹å¦‚ï¼Œä»å­˜å‚¨è¯»å–ï¼‰ä¸­è¿›è¡Œã€‚
- en: '*The biggest downside* of using this method is that your predictions will have
    a degree of lag. For example, in our use case, you make and save the predictions
    for the next 24 hours. Letâ€™s assume that 2 hours pass without any new predictions.
    Now, you have predictions only for the next 22 hours.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ­¤æ–¹æ³•çš„*æœ€å¤§ç¼ºç‚¹*æ˜¯ä½ çš„é¢„æµ‹å°†æœ‰ä¸€å®šçš„æ»åã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬çš„ç”¨ä¾‹ä¸­ï¼Œä½ ç”Ÿæˆå¹¶ä¿å­˜æœªæ¥ 24 å°æ—¶çš„é¢„æµ‹ã€‚å‡è®¾ç»è¿‡ 2 å°æ—¶æ²¡æœ‰æ–°çš„é¢„æµ‹ï¼Œç°åœ¨ä½ åªæœ‰æœªæ¥
    22 å°æ—¶çš„é¢„æµ‹ã€‚
- en: Where the number of predictions that you have to store is reasonable, you can
    bypass this issue by making the predictions often. In our example, we will make
    the predictions hourly â€” our data has a resolution of 1 hour. Thus, we solved
    the lag issue by constantly making and storing new predictions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ å¿…é¡»å­˜å‚¨çš„é¢„æµ‹æ•°é‡æ˜¯åˆç†çš„æ—¶ï¼Œä½ å¯ä»¥é€šè¿‡é¢‘ç¹ç”Ÿæˆé¢„æµ‹æ¥ç»•è¿‡è¿™ä¸ªé—®é¢˜ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†æ¯å°æ—¶ç”Ÿæˆé¢„æµ‹â€”â€”æˆ‘ä»¬çš„æ•°æ®åˆ†è¾¨ç‡ä¸º 1 å°æ—¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡ä¸æ–­ç”Ÿæˆå’Œå­˜å‚¨æ–°é¢„æµ‹æ¥è§£å†³å»¶è¿Ÿé—®é¢˜ã€‚
- en: '*But here comes the second problem with the batch prediction strategy*. Suppose
    the set of predictions is large. For example, you want to predict the recommendations
    for 1 million users with a database of 100 million items. Then, computing the
    predictions very often will be highly costly.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä½†è¿™é‡Œå‡ºç°äº†æ‰¹é‡é¢„æµ‹ç­–ç•¥çš„ç¬¬äºŒä¸ªé—®é¢˜*ã€‚å‡è®¾é¢„æµ‹é›†åˆå¾ˆå¤§ã€‚ä¾‹å¦‚ï¼Œä½ æƒ³é¢„æµ‹ 100 ä¸‡ç”¨æˆ·çš„æ¨èï¼Œè€Œæ•°æ®åº“ä¸­æœ‰ 1 äº¿ä¸ªé¡¹ç›®ã€‚é‚£ä¹ˆï¼Œé¢‘ç¹è®¡ç®—é¢„æµ‹å°†æ˜¯éå¸¸æ˜‚è´µçš„ã€‚'
- en: Then you have to consider using other serving methods strongly.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä½ å¿…é¡»å¼ºçƒˆè€ƒè™‘ä½¿ç”¨å…¶ä»–æœåŠ¡æ–¹æ³•ã€‚
- en: '***But here is the catch.***'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä½†è¿™é‡Œæœ‰ä¸€ä¸ªé™·é˜±ã€‚***'
- en: Your application probably won't start with a database of 1 million users and
    100 million items. That means you can safely begin using a batch mode architecture
    and gradually shift to other methodologies when it makes sense.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çš„åº”ç”¨ç¨‹åºå¯èƒ½ä¸ä¼šä¸€å¼€å§‹å°±æœ‰ 100 ä¸‡ç”¨æˆ·å’Œ 1 äº¿æ¡æ•°æ®ã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥å®‰å…¨åœ°ä»æ‰¹å¤„ç†æ¨¡å¼æ¶æ„å¼€å§‹ï¼Œå¹¶åœ¨æœ‰å¿…è¦æ—¶é€æ­¥è½¬å‘å…¶ä»–æ–¹æ³•ã€‚
- en: That is what most people do!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å¤§å¤šæ•°äººåšçš„äº‹æƒ…ï¼
- en: To get an intuition on how to shift to other methods, [check out this article](https://medium.com/mlearning-ai/this-is-what-you-need-to-know-to-build-an-mlops-end-to-end-architecture-c0be1deaa3ce)
    to learn about a *standardized ML architecture* *suggested by* *Google Cloud.*
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£å¦‚ä½•è½¬å‘å…¶ä»–æ–¹æ³•ï¼Œ[è¯·æŸ¥çœ‹è¿™ç¯‡æ–‡ç« ](https://medium.com/mlearning-ai/this-is-what-you-need-to-know-to-build-an-mlops-end-to-end-architecture-c0be1deaa3ce)ï¼Œäº†è§£*Google
    Cloud* *å»ºè®®çš„* *æ ‡å‡†åŒ– ML æ¶æ„*ã€‚
- en: Theoretical Concepts & Tools
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†è®ºæ¦‚å¿µä¸å·¥å…·
- en: '**GCS:** GCS stands for Google Cloud Storage, which is Google''s storage solution
    within GCP. It is similar to AWS S3 if you are more familiar with it.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**GCSï¼š** GCS ä»£è¡¨ Google Cloud Storageï¼Œæ˜¯ Google åœ¨ GCP ä¸­çš„å­˜å‚¨è§£å†³æ–¹æ¡ˆã€‚å¦‚æœä½ æ›´ç†Ÿæ‚‰ AWS S3ï¼Œå®ƒç±»ä¼¼äºæ­¤ã€‚'
- en: You can write to GCS any file. In our course, we will write Pandas DataFrames
    as parquet files.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å‘ GCS å†™å…¥ä»»ä½•æ–‡ä»¶ã€‚åœ¨æˆ‘ä»¬çš„è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°† Pandas DataFrames å†™å…¥ parquet æ–‡ä»¶ã€‚
- en: '**GCS vs. Redis:** We choose to write our predictions in GCS because of 4 main
    reasons:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**GCS ä¸ Redisï¼š** æˆ‘ä»¬é€‰æ‹©å°†é¢„æµ‹ç»“æœå†™å…¥ GCS ä¸»è¦æœ‰ 4 ä¸ªåŸå› ï¼š'
- en: Easy to setup
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ˜“äºè®¾ç½®
- en: No maintenance
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ— éœ€ç»´æŠ¤
- en: Access to the free tier
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¿é—®å…è´¹å±‚
- en: We will also use GCP to deploy the code.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ GCP æ¥éƒ¨ç½²ä»£ç ã€‚
- en: Redis is a popular choice for caching your predictions to be later accessed
    by various clients.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Redis æ˜¯ç¼“å­˜é¢„æµ‹ç»“æœä»¥ä¾¿åç»­ç”±å„ç§å®¢æˆ·ç«¯è®¿é—®çš„çƒ­é—¨é€‰æ‹©ã€‚
- en: '*Why?*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸ºä»€ä¹ˆï¼Ÿ*'
- en: Because you can access the data at low latency, improving the users' experience.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºä½ å¯ä»¥ä»¥ä½å»¶è¿Ÿè®¿é—®æ•°æ®ï¼Œä»è€Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚
- en: It would have been a good choice, but we wanted to simplify things.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ¥è¿™æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œä½†æˆ‘ä»¬æƒ³è¦ç®€åŒ–äº‹æƒ…ã€‚
- en: Also, it is good practice to write the predictions on GCS for long-term storage
    and cache them in Redis for real-time access.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå°†é¢„æµ‹ç»“æœå†™å…¥ GCS ä»¥ä¾¿é•¿æœŸå­˜å‚¨ï¼Œå¹¶åœ¨ Redis ä¸­ç¼“å­˜ä»¥ä¾›å®æ—¶è®¿é—®ä¹Ÿæ˜¯ä¸€ç§è‰¯å¥½çš„åšæ³•ã€‚
- en: '**Poetry:** Poetry is my favorite Python virtual environment manager. It is
    similar to Conda, venv, and Pipenv. In my opinion, it is superior because:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**Poetryï¼š** Poetry æ˜¯æˆ‘æœ€å–œæ¬¢çš„ Python è™šæ‹Ÿç¯å¢ƒç®¡ç†å·¥å…·ã€‚å®ƒç±»ä¼¼äº Condaã€venv å’Œ Pipenvã€‚ä¾æˆ‘çœ‹ï¼Œå®ƒæ›´ä¼˜ï¼Œå› ä¸ºï¼š'
- en: It offers you a **.lock** file that reflects the versions of all your sub-dependencies.
    Thus, replicating code is extremely easy and safe.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒæä¾›äº†ä¸€ä¸ª**.lock**æ–‡ä»¶ï¼Œåæ˜ äº†æ‰€æœ‰å­ä¾èµ–é¡¹çš„ç‰ˆæœ¬ã€‚å› æ­¤ï¼Œå¤åˆ¶ä»£ç éå¸¸ç®€å•å’Œå®‰å…¨ã€‚
- en: You can quickly build your module directly using Poetry. No other setup is required.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ Poetry å¿«é€Ÿæ„å»ºä½ çš„æ¨¡å—ã€‚æ— éœ€å…¶ä»–è®¾ç½®ã€‚
- en: You can quickly deploy your module to a PiPy server using Poetry. No other setup
    is required, and moreâ€¦
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä½¿ç”¨ Poetry å¿«é€Ÿå°†æ¨¡å—éƒ¨ç½²åˆ° PiPy æœåŠ¡å™¨ã€‚æ— éœ€å…¶ä»–è®¾ç½®ï¼Œè¿˜æœ‰æ›´å¤šâ€¦â€¦
- en: 'Lesson 3: Code'
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ 3 è¯¾ï¼šä»£ç 
- en: '[You can access the GitHub repository here.](https://github.com/iusztinpaul/energy-forecasting)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä½ å¯ä»¥åœ¨è¿™é‡Œè®¿é—® GitHub ä»“åº“ã€‚](https://github.com/iusztinpaul/energy-forecasting)'
- en: '**Note:** All the installation instructions are in the READMEs of the repository.
    Here we will jump straight to the code.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** æ‰€æœ‰å®‰è£…è¯´æ˜éƒ½åœ¨ä»“åº“çš„ README æ–‡ä»¶ä¸­ã€‚è¿™é‡Œæˆ‘ä»¬å°†ç›´æ¥è·³åˆ°ä»£ç ã€‚'
- en: '*All the code within Lesson 3 is located under the* [***batch-prediction-pipeline***](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)*folder.*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç¬¬ 3 è¯¾ä¸­çš„æ‰€æœ‰ä»£ç éƒ½ä½äº* [***batch-prediction-pipeline***](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)*æ–‡ä»¶å¤¹ä¸­ã€‚*'
- en: 'The files under the [**batch-prediction-pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)folderare
    structured as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[**batch-prediction-pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)
    æ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶ç»“æ„å¦‚ä¸‹ï¼š'
- en: '![](../Images/c47f99f9476d03de8ade95a30b583daa.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c47f99f9476d03de8ade95a30b583daa.png)'
- en: A screenshot that shows the structure of the batch-prediction-pipeline folder
    [Image by the Author].
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç¤ºæ‰¹é‡é¢„æµ‹ç®¡é“æ–‡ä»¶å¤¹ç»“æ„çš„å±å¹•æˆªå›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: All the code is located under the [**batch_prediction_pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline/batch_prediction_pipeline)directory
    (note the "_" instead of "-")**.**
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ä»£ç éƒ½ä½äº [**batch_prediction_pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline/batch_prediction_pipeline)
    ç›®å½•ä¸‹ï¼ˆæ³¨æ„â€œ_â€è€Œä¸æ˜¯â€œ-â€ï¼‰ã€‚
- en: Directly storing credentials in your git repository is a huge security risk.
    That is why you will inject sensitive information using a **.env** file.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´æ¥åœ¨ä½ çš„ git ä»“åº“ä¸­å­˜å‚¨å‡­æ®æ˜¯ä¸€ä¸ªå·¨å¤§çš„å®‰å…¨é£é™©ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ å°†ä½¿ç”¨**.env**æ–‡ä»¶æ¥æ³¨å…¥æ•æ„Ÿä¿¡æ¯ã€‚
- en: The **.env.default** is an example of all the variables you must configure.
    It is also helpful to store default values for attributes that are not sensitive
    (e.g., project name).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**.env.default**æ˜¯ä½ å¿…é¡»é…ç½®çš„æ‰€æœ‰å˜é‡çš„ç¤ºä¾‹ã€‚å®ƒè¿˜å¯ä»¥å¸®åŠ©å­˜å‚¨é‚£äº›ä¸æ•æ„Ÿçš„å±æ€§çš„é»˜è®¤å€¼ï¼ˆä¾‹å¦‚ï¼Œé¡¹ç›®åç§°ï¼‰ã€‚'
- en: '![](../Images/1b4f40ca19a12ac8ff070610a8530d46.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b4f40ca19a12ac8ff070610a8530d46.png)'
- en: A screenshot of the .env.default file [Image by the Author].
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: .env.defaultæ–‡ä»¶çš„æˆªå›¾[å›¾ç‰‡ç”±ä½œè€…æä¾›]ã€‚
- en: Prepare Credentials
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡†å¤‡å‡­æ®
- en: First of all, you have to create a **.env** filewhere you will add all our credentials.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ª**.env**æ–‡ä»¶ï¼Œåœ¨å…¶ä¸­æ·»åŠ æˆ‘ä»¬æ‰€æœ‰çš„å‡­æ®ã€‚
- en: I already showed you in [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how to set up your **.env** file. Also, I explained in [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how the variables from the **.env** file are loaded from your **ML_PIPELINE_ROOT_DIR**
    directory into a **SETTINGS** Python dictionary to be used throughout your code.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å·²ç»åœ¨[ç¬¬1è¯¾](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ä¸­å‘ä½ å±•ç¤ºäº†å¦‚ä½•è®¾ç½®ä½ çš„**.env**æ–‡ä»¶ã€‚åŒæ—¶ï¼Œæˆ‘åœ¨[ç¬¬1è¯¾](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ä¸­è§£é‡Šäº†å¦‚ä½•å°†**.env**æ–‡ä»¶ä¸­çš„å˜é‡ä»**ML_PIPELINE_ROOT_DIR**ç›®å½•åŠ è½½åˆ°**SETTINGS**
    Pythonå­—å…¸ä¸­ï¼Œä»¥ä¾¿åœ¨ä»£ç ä¸­ä½¿ç”¨ã€‚
- en: Thus, if you want to replicate what I have done, I strongly recommend checking
    out [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¦‚æœä½ æƒ³å¤åˆ¶æˆ‘æ‰€åšçš„ï¼Œæˆ‘å¼ºçƒˆå»ºè®®æŸ¥çœ‹[ç¬¬1è¯¾](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ã€‚
- en: '*If you only want a light read, you can completely skip the "****Prepare Credentials****"
    step.*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ åªæ˜¯æƒ³è½»æ¾é˜…è¯»ï¼Œå¯ä»¥å®Œå…¨è·³è¿‡â€œ****å‡†å¤‡å‡­æ®****â€æ­¥éª¤ã€‚*'
- en: 'In Lesson 3, you will use two services:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬3è¯¾ä¸­ï¼Œä½ å°†ä½¿ç”¨ä¸¤ä¸ªæœåŠ¡ï¼š
- en: '[Hopsworks](https://www.hopsworks.ai/)'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Hopsworks](https://www.hopsworks.ai/)'
- en: '[GCP â€” Cloud Storage](https://cloud.google.com/storage)'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[GCP â€” Cloud Storage](https://cloud.google.com/storage)'
- en: '[***Hopsworks***](https://www.hopsworks.ai/) ***(free)***'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[***Hopsworks***](https://www.hopsworks.ai/) ***(å…è´¹)***'
- en: We already showed you in [Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how to set up the credentials for **Hopsworks**. Please visit the ["Prepare Credentials"
    section from Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f),
    where we showed you in detail how to set up the API KEY for Hopsworks.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»åœ¨[ç¬¬1è¯¾](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ä¸­å‘ä½ å±•ç¤ºäº†å¦‚ä½•è®¾ç½®**Hopsworks**çš„å‡­æ®ã€‚è¯·è®¿é—®[ç¬¬1è¯¾ä¸­çš„â€œå‡†å¤‡å‡­æ®â€éƒ¨åˆ†](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬è¯¦ç»†å±•ç¤ºäº†å¦‚ä½•è®¾ç½®Hopsworksçš„API
    KEYã€‚
- en: '[***GCP â€” Cloud Storage***](https://cloud.google.com/storage) ***(free)***'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[***GCP â€” Cloud Storage***](https://cloud.google.com/storage) ***(å…è´¹)***'
- en: While replicating this course, you will stick to the *GCP â€” Cloud Storage free
    tier*. You can store up to 5GB for free in GCP â€” Cloud Storage, which is far more
    than enough for our use case.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤åˆ¶æœ¬è¯¾ç¨‹æ—¶ï¼Œä½ å°†åšæŒä½¿ç”¨*GCP â€” Cloud Storage å…è´¹å±‚*ã€‚ä½ å¯ä»¥åœ¨GCP â€” Cloud Storageä¸­å…è´¹å­˜å‚¨æœ€å¤š5GBï¼Œè¿™å¯¹æˆ‘ä»¬çš„ä½¿ç”¨æƒ…å†µç»°ç»°æœ‰ä½™ã€‚
- en: This configuration step will be longer, but I promise that it is not complicated.
    By the way, you will learn the basics of using a cloud vendor such as GCP.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé…ç½®æ­¥éª¤ä¼šç¨é•¿ä¸€äº›ï¼Œä½†æˆ‘ä¿è¯å®ƒå¹¶ä¸å¤æ‚ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œä½ å°†å­¦ä¹ ä½¿ç”¨åƒGCPè¿™æ ·çš„äº‘æœåŠ¡æä¾›å•†çš„åŸºç¡€çŸ¥è¯†ã€‚
- en: First, go to GCP and create a project called "**energy_consumption"** (or any
    other name)**.** Afterward, go to your GCP project's "Cloud Storage" section and
    create a **non-public bucket** called "**hourly-batch-predictions**"**.** Pick
    any region, but just be aware of itâ€”[official docs about creating a bucket on
    GCP](https://cloud.google.com/storage/docs/creating-buckets) [2].
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®¿é—®GCPå¹¶åˆ›å»ºä¸€ä¸ªåä¸ºâ€œ**energy_consumption**â€ï¼ˆæˆ–å…¶ä»–ä»»æ„åç§°ï¼‰çš„é¡¹ç›®**ã€‚** éšåï¼Œå‰å¾€GCPé¡¹ç›®çš„â€œCloud Storageâ€éƒ¨åˆ†ï¼Œåˆ›å»ºä¸€ä¸ªåä¸ºâ€œ**hourly-batch-predictions**â€çš„**éå…¬å¼€å­˜å‚¨æ¡¶**ã€‚**
    é€‰æ‹©ä»»ä½•åŒºåŸŸï¼Œä½†è¯·æ³¨æ„è¿™ä¸€ç‚¹â€”[åˆ›å»ºGCPå­˜å‚¨æ¡¶çš„å®˜æ–¹æ–‡æ¡£](https://cloud.google.com/storage/docs/creating-buckets)
    [2]ã€‚
- en: '**NOTE:** *You might need to pick different names due to constant changes to
    the platformâ€™s rules*. That is not an issue, just call them as you wish and change
    them in the **.env** file: *GOOGLE_CLOUD_PROJECT* (ours â€œenergy_consumptionâ€)
    and *GOOGLE_CLOUD_BUCKET_NAME (*ours â€œhourly-batch-predictionsâ€*)*.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** *ç”±äºå¹³å°è§„åˆ™çš„ä¸æ–­å˜åŒ–ï¼Œä½ å¯èƒ½éœ€è¦é€‰æ‹©ä¸åŒçš„åç§°*ã€‚è¿™ä¸æ˜¯é—®é¢˜ï¼Œåªéœ€æ ¹æ®ä½ çš„éœ€æ±‚å‘½åï¼Œå¹¶åœ¨**.env**æ–‡ä»¶ä¸­è¿›è¡Œæ›´æ”¹ï¼š*GOOGLE_CLOUD_PROJECT*ï¼ˆæˆ‘ä»¬çš„â€œenergy_consumptionâ€ï¼‰å’Œ*GOOGLE_CLOUD_BUCKET_NAME*ï¼ˆæˆ‘ä»¬çš„â€œhourly-batch-predictionsâ€ï¼‰ã€‚'
- en: '![](../Images/f0cd1bf7dff7a64180918b371d064e98.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0cd1bf7dff7a64180918b371d064e98.png)'
- en: Screenshot of the GCP â€” Cloud Storage view, where you must create your bucket
    [Image by the Author].
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: GCP â€” Cloud Storage è§†å›¾çš„æˆªå›¾ï¼Œä½ éœ€è¦åœ¨å…¶ä¸­åˆ›å»ºä½ çš„æ¡¶ [å›¾ç‰‡ç”±ä½œè€…æä¾›]ã€‚
- en: Now you finished creating all your GCP resources. The last step is to create
    a way to have read & write access to the GCP bucket directly from your Python
    code.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»å®Œæˆäº†æ‰€æœ‰ GCP èµ„æºçš„åˆ›å»ºã€‚æœ€åä¸€æ­¥æ˜¯åˆ›å»ºä¸€ç§æ–¹å¼ï¼Œä»¥ä¾¿é€šè¿‡ä½ çš„ Python ä»£ç ç›´æ¥è®¿é—® GCP æ¡¶çš„è¯»å†™æƒé™ã€‚
- en: You can easily do this using GCP *service accounts.* I don't want to hijack
    the whole article with GCP configurations. Thus, [this GCP official doc shows
    you how to create a service account](https://cloud.google.com/iam/docs/service-accounts-create)
    [3].
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡ GCP *æœåŠ¡è´¦æˆ·*è½»æ¾åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘ä¸æƒ³æŠŠæ•´ç¯‡æ–‡ç« éƒ½æŒ¤å äº GCP é…ç½®ã€‚å› æ­¤ï¼Œ[è¿™ä»½ GCP å®˜æ–¹æ–‡æ¡£å±•ç¤ºäº†å¦‚ä½•åˆ›å»ºæœåŠ¡è´¦æˆ·](https://cloud.google.com/iam/docs/service-accounts-create)
    [3]ã€‚
- en: '*When creating the service account, be aware of one thing!*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*åˆ›å»ºæœåŠ¡è´¦æˆ·æ—¶ï¼Œè¯·æ³¨æ„ä¸€ä»¶äº‹ï¼*'
- en: Service accounts have attached different roles. A role is a way to configure
    your service account with various permissions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æœåŠ¡è´¦æˆ·å…·æœ‰ä¸åŒçš„è§’è‰²ã€‚è§’è‰²æ˜¯ä¸€ç§é…ç½®æœåŠ¡è´¦æˆ·æƒé™çš„æ–¹å¼ã€‚
- en: Thus, you need to configure your service account to have read & write access
    the your "**hourly-batch-predictions**" bucket.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½ éœ€è¦é…ç½®ä½ çš„æœåŠ¡è´¦æˆ·ä»¥æ‹¥æœ‰å¯¹ä½ çš„â€œ**hourly-batch-predictions**â€æ¡¶çš„è¯»å†™è®¿é—®æƒé™ã€‚
- en: You can easily do that by choosing the "**Storage Object Admin**" role when
    creating your service account.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡åœ¨åˆ›å»ºæœåŠ¡è´¦æˆ·æ—¶é€‰æ‹©â€œ**Storage Object Admin**â€è§’è‰²æ¥è½»æ¾åšåˆ°è¿™ä¸€ç‚¹ã€‚
- en: The final step is to find a way to authenticate with your newly created service
    account in your Python code.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€æ­¥æ˜¯æ‰¾åˆ°ä¸€ç§æ–¹æ³•ï¼Œåœ¨ä½ çš„ Python ä»£ç ä¸­ä½¿ç”¨æ–°åˆ›å»ºçš„æœåŠ¡è´¦æˆ·è¿›è¡Œèº«ä»½éªŒè¯ã€‚
- en: You can easily do that by going to your service account and creating a JSON
    key. Again, [here are the official GCP docs that will show you how to create a
    JSON key for your service account](https://cloud.google.com/iam/docs/keys-create-delete)
    [4].
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡è®¿é—®ä½ çš„æœåŠ¡è´¦æˆ·å¹¶åˆ›å»ºä¸€ä¸ª JSON å¯†é’¥æ¥è½»æ¾åšåˆ°è¿™ä¸€ç‚¹ã€‚å†æ¬¡ï¼Œ[è¿™é‡Œæ˜¯å®˜æ–¹ GCP æ–‡æ¡£ï¼Œå®ƒä¼šå‘Šè¯‰ä½ å¦‚ä½•ä¸ºä½ çš„æœåŠ¡è´¦æˆ·åˆ›å»º JSON å¯†é’¥](https://cloud.google.com/iam/docs/keys-create-delete)
    [4]ã€‚
- en: '*Again, keep in mind one thing!*'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*å†æ¬¡ï¼Œè¯·è®°ä½ä¸€ä»¶äº‹ï¼*'
- en: When creating the JSON key, you will download a JSON file.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»º JSON å¯†é’¥æ—¶ï¼Œä½ å°†ä¸‹è½½ä¸€ä¸ª JSON æ–‡ä»¶ã€‚
- en: After you download your JSON file, put it in a safe place and go to your **.env**
    file. There, change the value of *GOOGLE_CLOUD_SERVICE_ACCOUNT_JSON_PATH*with
    your absolute path to the JSON file.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹è½½ JSON æ–‡ä»¶åï¼Œå°†å…¶æ”¾åœ¨å®‰å…¨çš„åœ°æ–¹ï¼Œå¹¶è½¬åˆ°ä½ çš„**.env**æ–‡ä»¶ã€‚åœ¨é‚£é‡Œï¼Œå°†*GOOGLE_CLOUD_SERVICE_ACCOUNT_JSON_PATH*çš„å€¼æ›´æ”¹ä¸º
    JSON æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ã€‚
- en: '![](../Images/1b4f40ca19a12ac8ff070610a8530d46.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b4f40ca19a12ac8ff070610a8530d46.png)'
- en: A screenshot of the .env.default file [Image by the Author].
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**.env.default** æ–‡ä»¶çš„æˆªå›¾ [å›¾ç‰‡ç”±ä½œè€…æä¾›]ã€‚'
- en: '**NOTE:** Remember to change the *GOOGLE_CLOUD_PROJECT* and *GOOGLE_CLOUD_BUCKET_NAME*
    variables with your names.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** è®°å¾—å°†*GOOGLE_CLOUD_PROJECT*å’Œ*GOOGLE_CLOUD_BUCKET_NAME*å˜é‡æ›´æ”¹ä¸ºä½ çš„åç§°ã€‚'
- en: '*Congratulations! You are done configuring GCS â€” Cloud Storage.*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ­å–œï¼ä½ å·²ç»å®Œæˆäº† GCS â€” Cloud Storage çš„é…ç½®ã€‚*'
- en: Now you have created a GCP project and bucket. Also, you have read & write access
    using your Python code through your service account. You log in with your service
    account with the help of the JSON file.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»åˆ›å»ºäº†ä¸€ä¸ª GCP é¡¹ç›®å’Œæ¡¶ã€‚æ­¤å¤–ï¼Œä½ å¯ä»¥é€šè¿‡æœåŠ¡è´¦æˆ·ä½¿ç”¨ä½ çš„ Python ä»£ç è¿›è¡Œè¯»å†™è®¿é—®ã€‚ä½ ä½¿ç”¨ JSON æ–‡ä»¶çš„å¸®åŠ©ç™»å½•åˆ°æœåŠ¡è´¦æˆ·ã€‚
- en: If something isn't working, let me know in the comments below or directly on
    [LinkedIn](https://www.linkedin.com/feed/).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæœ‰ä»€ä¹ˆé—®é¢˜ï¼Œè¯·åœ¨ä¸‹é¢çš„è¯„è®ºä¸­å‘Šè¯‰æˆ‘ï¼Œæˆ–ç›´æ¥åœ¨[LinkedIn](https://www.linkedin.com/feed/)ä¸Šè”ç³»æˆ‘ã€‚
- en: Batch Prediction Pipeline â€” Main Function
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰¹é‡é¢„æµ‹ç®¡é“ â€” ä¸»åŠŸèƒ½
- en: 'As you can see, the main function follows the 4 steps of a batch prediction
    pipeline:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œä¸»åŠŸèƒ½éµå¾ªæ‰¹é‡é¢„æµ‹ç®¡é“çš„å››ä¸ªæ­¥éª¤ï¼š
- en: Loads data from the Feature Store in batch mode.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»ç‰¹å¾å­˜å‚¨ä¸­ä»¥æ‰¹å¤„ç†æ¨¡å¼åŠ è½½æ•°æ®ã€‚
- en: Loads the model from the model registry.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»æ¨¡å‹æ³¨å†Œè¡¨ä¸­åŠ è½½æ¨¡å‹ã€‚
- en: Makes the predictions.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿›è¡Œé¢„æµ‹ã€‚
- en: It saves the predictions to the GCS bucket.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†é¢„æµ‹ç»“æœä¿å­˜åˆ° GCS æ¡¶ä¸­ã€‚
- en: Most of the function is log lines ğŸ˜†
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§éƒ¨åˆ†åŠŸèƒ½éƒ½æ˜¯æ—¥å¿—è¡Œ ğŸ˜†
- en: Along these 4 main steps, you must load all the parameters from the metadata
    generated by previous steps, such as the **feature_view_version** and **model_version.**
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™å››ä¸ªä¸»è¦æ­¥éª¤ä¸­ï¼Œä½ å¿…é¡»ä»ä¹‹å‰æ­¥éª¤ç”Ÿæˆçš„å…ƒæ•°æ®ä¸­åŠ è½½æ‰€æœ‰å‚æ•°ï¼Œä¾‹å¦‚**feature_view_version**å’Œ**model_version**ã€‚
- en: Also, you have to get a reference to the Hopsworks Feature store.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œä½ éœ€è¦è·å¾—å¯¹ Hopsworks ç‰¹å¾å­˜å‚¨çš„å¼•ç”¨ã€‚
- en: After, you go straight to the 4 main steps that we will detail later in the
    tutorial ğŸ‘‡
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä½ ç›´æ¥è¿›å…¥æˆ‘ä»¬å°†åœ¨åé¢çš„æ•™ç¨‹ä¸­è¯¦ç»†ä»‹ç»çš„å››ä¸ªä¸»è¦æ­¥éª¤ ğŸ‘‡
- en: 'Step 1: Loading Data From the Feature Store In Batch Mode'
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 1ï¼šä»¥æ‰¹å¤„ç†æ¨¡å¼ä»ç‰¹å¾å­˜å‚¨åŠ è½½æ•°æ®
- en: This step is similar to what we have done in [Lesson 2](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)
    when loading data for training.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€æ­¥ç±»ä¼¼äºæˆ‘ä»¬åœ¨[ç¬¬2è¯¾](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)ä¸­åŠ è½½è®­ç»ƒæ•°æ®æ—¶æ‰€åšçš„ã€‚
- en: But this time, instead of downloading the data from a training dataset, we directly
    ask for a batch of data between a datetime range, using the **get_batch_data()**
    method.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬ä¸æ˜¯ä»è®­ç»ƒæ•°æ®é›†ä¸­ä¸‹è½½æ•°æ®ï¼Œè€Œæ˜¯ç›´æ¥è¯·æ±‚ä¸€ä¸ªæ—¥æœŸæ—¶é—´èŒƒå›´å†…çš„æ•°æ®æ‰¹æ¬¡ï¼Œä½¿ç”¨**get_batch_data()**æ–¹æ³•ã€‚
- en: Doing so allows us to time travel to our desired datetime range and ask for
    the features we need. This method makes batch inference extremely easy.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·åšå…è®¸æˆ‘ä»¬æ—¶é—´æ—…è¡Œåˆ°æ‰€éœ€çš„æ—¥æœŸæ—¶é—´èŒƒå›´ï¼Œå¹¶è¯·æ±‚æ‰€éœ€çš„ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•ä½¿æ‰¹é‡æ¨ç†å˜å¾—éå¸¸ç®€å•ã€‚
- en: The last step is to prepare the indexes of the DataFrame as expected by **sktime**
    and to split it between X and y.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€æ­¥æ˜¯æŒ‰ç…§**sktime**çš„é¢„æœŸå‡†å¤‡DataFrameçš„ç´¢å¼•ï¼Œå¹¶å°†å…¶åˆ†å‰²ä¸ºXå’Œyã€‚
- en: '**Note:** This is an autoregressive process: we learn from past values of y
    to predict future values of y ( y = energy consumption levels). Thus, we will
    use only X as input to the model. We will use y only for visualization purposes.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** è¿™æ˜¯ä¸€ä¸ªè‡ªå›å½’è¿‡ç¨‹ï¼šæˆ‘ä»¬ä»è¿‡å»çš„yå€¼ä¸­å­¦ä¹ ä»¥é¢„æµ‹æœªæ¥çš„yå€¼ï¼ˆy = èƒ½æºæ¶ˆè€—æ°´å¹³ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åªä½¿ç”¨Xä½œä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚æˆ‘ä»¬å°†ä»…å°†yç”¨äºå¯è§†åŒ–ç›®çš„ã€‚'
- en: 'Step 2: Loading the Model From the Model Registry'
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ­¥ï¼šä»æ¨¡å‹æ³¨å†Œè¡¨åŠ è½½æ¨¡å‹
- en: Loading a model from the Hopsworks model registry is extremely easy.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Hopsworksæ¨¡å‹æ³¨å†Œè¡¨åŠ è½½æ¨¡å‹éå¸¸ç®€å•ã€‚
- en: The function below has as a parameter a reference to the Hopsworks project and
    the version of the model we want to download.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„å‡½æ•°æœ‰ä¸€ä¸ªå‚æ•°æ˜¯å¯¹Hopsworksé¡¹ç›®çš„å¼•ç”¨å’Œæˆ‘ä»¬è¦ä¸‹è½½çš„æ¨¡å‹ç‰ˆæœ¬ã€‚
- en: Using these two variables, you get a reference to the model registry. Afterward,
    you get a reference to the model itself using its name. In this case, it is **best_model**.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸¤ä¸ªå˜é‡ï¼Œä½ å¯ä»¥è·å¾—å¯¹æ¨¡å‹æ³¨å†Œè¡¨çš„å¼•ç”¨ã€‚ä¹‹åï¼Œé€šè¿‡æ¨¡å‹çš„åç§°ï¼Œä½ å¯ä»¥è·å¾—å¯¹æ¨¡å‹æœ¬èº«çš„å¼•ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒæ˜¯**best_model**ã€‚
- en: Finally, you download the artifact/model and load it into memory.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä½ ä¸‹è½½å·¥ä»¶/æ¨¡å‹å¹¶å°†å…¶åŠ è½½åˆ°å†…å­˜ä¸­ã€‚
- en: The trick here is that your model is versioned. Thus, you always know what model
    you are using.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„è¯€çªæ˜¯ä½ çš„æ¨¡å‹æ˜¯æœ‰ç‰ˆæœ¬æ§åˆ¶çš„ã€‚å› æ­¤ï¼Œä½ æ€»æ˜¯çŸ¥é“ä½ ä½¿ç”¨çš„æ˜¯å“ªä¸ªæ¨¡å‹ã€‚
- en: '**Note:** We uploaded the **best_model** in the model registry using the training
    pipeline explained in [Lesson 2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee).
    The training pipeline also provides us with a metadata dictionary that contains
    the latest model_version.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** æˆ‘ä»¬ä½¿ç”¨åœ¨[ç¬¬2è¯¾](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)ä¸­è§£é‡Šçš„è®­ç»ƒç®¡é“ä¸Šä¼ äº†**best_model**åˆ°æ¨¡å‹æ³¨å†Œè¡¨ã€‚è®­ç»ƒç®¡é“è¿˜æä¾›äº†ä¸€ä¸ªåŒ…å«æœ€æ–°model_versionçš„å…ƒæ•°æ®å­—å…¸ã€‚'
- en: 'Step 3: Forecast Energy Consumption Levels for the Next 24 Hours'
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰æ­¥ï¼šé¢„æµ‹æœªæ¥24å°æ—¶çš„èƒ½æºæ¶ˆè€—æ°´å¹³
- en: '**Sktime** makes forecasting extremely easy. The key line from the snippet
    below is "**predictions = model.predict(X=X_forecast)"**, which forecasts the
    energy consumption values for the next 24 hours.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sktime**ä½¿é¢„æµ‹å˜å¾—æå…¶ç®€å•ã€‚ä¸‹é¢ä»£ç ç‰‡æ®µä¸­çš„å…³é”®è¡Œæ˜¯â€œ**predictions = model.predict(X=X_forecast)**â€ï¼Œå®ƒé¢„æµ‹äº†æœªæ¥24å°æ—¶çš„èƒ½æºæ¶ˆè€—å€¼ã€‚'
- en: The forecasting horizon of 24 hours was given when the model was trained. Thus,
    it already knows how many data points into the future to forecast.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹è®­ç»ƒæ—¶è®¾ç½®äº†24å°æ—¶çš„é¢„æµ‹æ—¶é—´èŒƒå›´ã€‚å› æ­¤ï¼Œå®ƒå·²ç»çŸ¥é“æœªæ¥è¦é¢„æµ‹å¤šå°‘ä¸ªæ•°æ®ç‚¹ã€‚
- en: Also, you have to prepare the exogenous variable **X_forecast**. In time series
    forecasting, an exogenous variable is a feature that you already know it will
    happen in the future. For example, a holiday. Thus, based on your training data
    X which contains all the area and consumer types IDs, you can generate the **X_forecast**
    variable by mapping the datetime range into the forecasting range.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œä½ éœ€è¦å‡†å¤‡å¤–ç”Ÿå˜é‡**X_forecast**ã€‚åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­ï¼Œå¤–ç”Ÿå˜é‡æ˜¯ä½ å·²ç»çŸ¥é“æœªæ¥ä¼šå‘ç”Ÿçš„ç‰¹å¾ã€‚ä¾‹å¦‚ï¼ŒèŠ‚å‡æ—¥ã€‚å› æ­¤ï¼ŒåŸºäºä½ çš„è®­ç»ƒæ•°æ®Xï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰åŒºåŸŸå’Œæ¶ˆè´¹è€…ç±»å‹IDï¼Œä½ å¯ä»¥é€šè¿‡å°†æ—¥æœŸæ—¶é—´èŒƒå›´æ˜ å°„åˆ°é¢„æµ‹èŒƒå›´æ¥ç”Ÿæˆ**X_forecast**å˜é‡ã€‚
- en: 'Step 4: Save the Predictions to the Bucket'
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬å››æ­¥ï¼šå°†é¢„æµ‹ç»“æœä¿å­˜åˆ°å­˜å‚¨æ¡¶
- en: The last component is the function that saves everything to the GCP bucket.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ä¸ªç»„ä»¶æ˜¯å°†æ‰€æœ‰å†…å®¹ä¿å­˜åˆ°GCPå­˜å‚¨æ¡¶çš„å‡½æ•°ã€‚
- en: This step is relatively straightforward, and the hard part was to configure
    your bucket and access credentials.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€æ­¥ç›¸å¯¹ç®€å•ï¼Œéš¾ç‚¹åœ¨äºé…ç½®ä½ çš„å­˜å‚¨æ¡¶å’Œè®¿é—®å‡­è¯ã€‚
- en: We get a reference to the bucket, iterate through X, y & predictions and write
    them to the bucket as a blob.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è·å–å¯¹å­˜å‚¨æ¡¶çš„å¼•ç”¨ï¼Œéå†Xã€yå’Œé¢„æµ‹ï¼Œå°†å®ƒä»¬ä½œä¸ºblobå†™å…¥å­˜å‚¨æ¡¶ã€‚
- en: '**Note:** Besides the predictions, we also save X and y to have everything
    in one place to quickly access everything we need and nicely render them in the
    web app.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** é™¤äº†é¢„æµ‹ç»“æœå¤–ï¼Œæˆ‘ä»¬è¿˜ä¿å­˜Xå’Œyï¼Œä»¥ä¾¿å°†æ‰€æœ‰å†…å®¹é›†ä¸­åœ¨ä¸€ä¸ªåœ°æ–¹ï¼Œæ–¹ä¾¿å¿«é€Ÿè®¿é—®æ‰€æœ‰æ‰€éœ€å†…å®¹ï¼Œå¹¶åœ¨ç½‘é¡µåº”ç”¨ä¸­æ¼‚äº®åœ°å‘ˆç°å®ƒä»¬ã€‚'
- en: To get a reference to the bucket, you have to access the settings you configured
    at the beginning of the tutorial.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è·å–æ¡¶çš„å¼•ç”¨ï¼Œä½ å¿…é¡»è®¿é—®åœ¨æ•™ç¨‹å¼€å§‹æ—¶é…ç½®çš„è®¾ç½®ã€‚
- en: As you can see, you create a GCS client with the project name and the JSON credentials
    file path. Afterward, you can quickly get a reference to your given bucket.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œä½ åˆ›å»ºäº†ä¸€ä¸ªGCSå®¢æˆ·ç«¯ï¼Œå¹¶æŒ‡å®šäº†é¡¹ç›®åç§°å’ŒJSONå‡­æ®æ–‡ä»¶è·¯å¾„ã€‚ä¹‹åï¼Œä½ å¯ä»¥å¿«é€Ÿè·å–ä½ æŒ‡å®šæ¡¶çš„å¼•ç”¨ã€‚
- en: Writing a blob to a bucket is highly similar to writing a regular file.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: å°†blobå†™å…¥æ¡¶ä¸å†™å…¥æ™®é€šæ–‡ä»¶éå¸¸ç›¸ä¼¼ã€‚
- en: You get a reference to the blob you want to write and open the resource with
    "**with blob.open("wb") as f**".
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è·å–ä½ è¦å†™å…¥çš„blobçš„å¼•ç”¨ï¼Œå¹¶ä½¿ç”¨"**with blob.open("wb") as f**"æ‰“å¼€èµ„æºã€‚
- en: Note that you opened the blob in binary format.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ä½ ä»¥äºŒè¿›åˆ¶æ ¼å¼æ‰“å¼€äº†blobã€‚
- en: You are writing the data in parquet format, as it is an excellent trade-off
    between storage size and writing & reading performance.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†æ•°æ®ä»¥parquetæ ¼å¼å†™å…¥ï¼Œå› ä¸ºå®ƒåœ¨å­˜å‚¨å¤§å°å’Œå†™å…¥&è¯»å–æ€§èƒ½ä¹‹é—´æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æŠ˜è¡·ã€‚
- en: Package Python Modules with Poetry
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Poetryæ‰“åŒ…Pythonæ¨¡å—
- en: '[Poetry](https://python-poetry.org/) makes the building process extremely easy.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[Poetry](https://python-poetry.org/)ä½¿æ„å»ºè¿‡ç¨‹å˜å¾—æå…¶ç®€å•ã€‚'
- en: The first obvious step is to use Poetry as your virtual environment manager.
    That means you already have the "**pyproject.toml"** and "**poetry.lock**" files
    â€” we already provided these files for you.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¾è€Œæ˜“è§ï¼Œå°±æ˜¯ä½¿ç”¨Poetryä½œä¸ºä½ çš„è™šæ‹Ÿç¯å¢ƒç®¡ç†å™¨ã€‚è¿™æ„å‘³ç€ä½ å·²ç»æ‹¥æœ‰äº†"**pyproject.toml**"å’Œ"**poetry.lock**"æ–‡ä»¶â€”â€”æˆ‘ä»¬å·²ç»ä¸ºä½ æä¾›äº†è¿™äº›æ–‡ä»¶ã€‚
- en: 'Now, all you have to do is to go to your project at the same level as your
    Poetry files (the ones mentioned above â€” for example, go to your [batch-prediction-pipeline](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)
    directory) and run:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä½ åªéœ€è¿›å…¥ä¸ä½ çš„Poetryæ–‡ä»¶ï¼ˆä¸Šè¿°æ–‡ä»¶ï¼‰åŒä¸€å±‚çº§çš„é¡¹ç›®ç›®å½•ï¼ˆä¾‹å¦‚ï¼Œè¿›å…¥ä½ çš„[batch-prediction-pipeline](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)ç›®å½•ï¼‰å¹¶è¿è¡Œï¼š
- en: '[PRE0]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This will create a **dist** folder containing your package as a **wheel.** Now
    you can directly install your package using the wheel file or deploy it to a PyPi
    server.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†åˆ›å»ºä¸€ä¸ªåŒ…å«ä½ çš„åŒ…çš„**dist**æ–‡ä»¶å¤¹ä½œä¸º**wheel**ã€‚ç°åœ¨ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨wheelæ–‡ä»¶å®‰è£…ä½ çš„åŒ…ï¼Œæˆ–å°†å…¶éƒ¨ç½²åˆ°PyPiæœåŠ¡å™¨ä¸Šã€‚
- en: 'To deploy it, configure your PyPi server credentials with the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¿›è¡Œéƒ¨ç½²ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹é…ç½®ä½ çš„PyPiæœåŠ¡å™¨å‡­æ®ï¼š
- en: '[PRE1]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, deploy it using the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œéƒ¨ç½²ï¼š
- en: '[PRE2]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And that was it. I was amazed at how easy Poetry can make this process.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ã€‚æˆ‘å¯¹Poetryå¦‚ä½•ç®€åŒ–è¿™ä¸ªè¿‡ç¨‹æ„Ÿåˆ°æƒŠè®¶ã€‚
- en: Otherwise, building and deploying your Python package is a tedious and lengthy
    process.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: å¦åˆ™ï¼Œæ„å»ºå’Œéƒ¨ç½²ä½ çš„PythonåŒ…æ˜¯ä¸€ä¸ªç¹çä¸”æ¼«é•¿çš„è¿‡ç¨‹ã€‚
- en: In [Lesson 4](/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff),
    you will deploy your private PyPi server and deploy all the code you have written
    until this point using the commands I showed you above.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[ç¬¬4è¯¾](/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)ä¸­ï¼Œä½ å°†éƒ¨ç½²ä½ çš„ç§äººPyPiæœåŠ¡å™¨ï¼Œå¹¶ä½¿ç”¨æˆ‘åœ¨ä¸Šé¢å±•ç¤ºçš„å‘½ä»¤éƒ¨ç½²åˆ°ç›®å‰ä¸ºæ­¢ä½ ç¼–å†™çš„æ‰€æœ‰ä»£ç ã€‚
- en: Conclusion
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Congratulations! You finished the **third lesson** from the **Full Stack 7-Steps
    MLOps Framework** course.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ ï¼ä½ å®Œæˆäº†**Full Stack 7-Steps MLOps Framework**è¯¾ç¨‹çš„**ç¬¬ä¸‰è¯¾**ã€‚
- en: 'If you have reached this far, you know how to:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å·²ç»èµ°åˆ°è¿™ä¸€æ­¥ï¼Œä½ çŸ¥é“å¦‚ä½•ï¼š
- en: choose the right architecture
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€‰æ‹©æ­£ç¡®çš„æ¶æ„
- en: access data from the feature store in batch mode
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»¥æ‰¹é‡æ¨¡å¼ä»ç‰¹å¾å­˜å‚¨ä¸­è®¿é—®æ•°æ®
- en: download your model from the model registry
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»æ¨¡å‹æ³¨å†Œä¸­å¿ƒä¸‹è½½ä½ çš„æ¨¡å‹
- en: build an inference pipeline
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„å»ºæ¨ç†ç®¡é“
- en: save your predictions to GCS
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†ä½ çš„é¢„æµ‹ç»“æœä¿å­˜åˆ°GCS
- en: Now that you understand the power of using and implementing a batch prediction
    architecture, you can quickly serve models in real-time while paving your way
    for other fancier serving methods.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ äº†è§£äº†ä½¿ç”¨å’Œå®ç°æ‰¹é‡é¢„æµ‹æ¶æ„çš„å¼ºå¤§åŠŸèƒ½ï¼Œä½ å¯ä»¥å¿«é€Ÿå®æ—¶æä¾›æ¨¡å‹ï¼ŒåŒæ—¶ä¸ºå…¶ä»–æ›´é«˜çº§çš„æœåŠ¡æ–¹æ³•é“ºå¹³é“è·¯ã€‚
- en: Check out [Lesson 4](/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)
    to learn about hosting your own private PyPi server and orchestrating all the
    pipelines using Airflow.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[ç¬¬4è¯¾](/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)ï¼Œäº†è§£å¦‚ä½•æ‰˜ç®¡ä½ è‡ªå·±çš„ç§äººPyPiæœåŠ¡å™¨ä»¥åŠå¦‚ä½•ä½¿ç”¨Airflowç¼–æ’æ‰€æœ‰ç®¡é“ã€‚
- en: '**Also,** [**you can access the GitHub repository here**](https://github.com/iusztinpaul/energy-forecasting)**.**'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦å¤–ï¼Œ** [**ä½ å¯ä»¥åœ¨è¿™é‡Œè®¿é—®GitHubä»“åº“**](https://github.com/iusztinpaul/energy-forecasting)**ã€‚**'
- en: ğŸ’¡ My goal is to help machine learning engineers level up in designing and productionizing
    ML systems. Follow me on [LinkedIn](https://www.linkedin.com/in/pauliusztin/)
    or subscribe to my [weekly newsletter](https://pauliusztin.substack.com/) for
    more insights!
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ æˆ‘çš„ç›®æ ‡æ˜¯å¸®åŠ©æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆåœ¨è®¾è®¡å’Œç”Ÿäº§åŒ–æœºå™¨å­¦ä¹ ç³»ç»Ÿæ–¹é¢æå‡æ°´å¹³ã€‚å…³æ³¨æˆ‘åœ¨ [LinkedIn](https://www.linkedin.com/in/pauliusztin/)
    æˆ–è®¢é˜…æˆ‘çš„ [æ¯å‘¨é€šè®¯](https://pauliusztin.substack.com/)ä»¥è·å–æ›´å¤šè§è§£ï¼
- en: ğŸ”¥ If you enjoy reading articles like this and wish to support my writing, consider
    [becoming a Medium member](https://pauliusztin.medium.com/membership). By using
    [my referral link](https://pauliusztin.medium.com/membership), you can support
    me without any extra cost while enjoying limitless access to Mediumâ€™s rich collection
    of stories.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ å¦‚æœä½ å–œæ¬¢é˜…è¯»è¿™æ ·çš„æ–‡ç« å¹¶å¸Œæœ›æ”¯æŒæˆ‘çš„å†™ä½œï¼Œå¯ä»¥è€ƒè™‘ [æˆä¸º Medium ä¼šå‘˜](https://pauliusztin.medium.com/membership)ã€‚é€šè¿‡ä½¿ç”¨
    [æˆ‘çš„æ¨èé“¾æ¥](https://pauliusztin.medium.com/membership)ï¼Œä½ å¯ä»¥åœ¨æ²¡æœ‰é¢å¤–æˆæœ¬çš„æƒ…å†µä¸‹æ”¯æŒæˆ‘ï¼ŒåŒæ—¶äº«å— Medium
    ä¸°å¯Œæ•…äº‹çš„æ— é™åˆ¶è®¿é—®æƒé™ã€‚
- en: '[](https://pauliusztin.medium.com/membership?source=post_page-----17a1462ca489--------------------------------)
    [## Join Medium with my referral link - Paul Iusztin'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pauliusztin.medium.com/membership?source=post_page-----17a1462ca489--------------------------------)
    [## ä½¿ç”¨æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium - Paul Iusztin]'
- en: ğŸ¤– Join to get exclusive content about designing and building production-ready
    ML systems ğŸš€ Unlock full access toâ€¦
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ğŸ¤– åŠ å…¥ä»¥è·å–æœ‰å…³è®¾è®¡å’Œæ„å»ºç”Ÿäº§å°±ç»ªçš„æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„ç‹¬å®¶å†…å®¹ ğŸš€ è§£é”å®Œæ•´è®¿é—®æƒé™â€¦
- en: pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----17a1462ca489--------------------------------)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----17a1462ca489--------------------------------)
- en: References
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: '[1] [Energy Consumption per DE35 Industry Code from Denmark API](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour),
    [Denmark Energy Data Service](https://www.energidataservice.dk/about/)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [ä¸¹éº¦ API çš„ DE35 è¡Œä¸šä»£ç èƒ½æºæ¶ˆè€—](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)ï¼Œ[ä¸¹éº¦èƒ½æºæ•°æ®æœåŠ¡](https://www.energidataservice.dk/about/)'
- en: '[2] [Create buckets](https://cloud.google.com/storage/docs/creating-buckets),
    GCP Cloud Storage Docs'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [åˆ›å»ºå­˜å‚¨æ¡¶](https://cloud.google.com/storage/docs/creating-buckets)ï¼ŒGCP Cloud
    Storage æ–‡æ¡£'
- en: '[3] [Create service accounts](https://cloud.google.com/iam/docs/service-accounts-create),
    GCP IAM Docs'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [åˆ›å»ºæœåŠ¡è´¦æˆ·](https://cloud.google.com/iam/docs/service-accounts-create)ï¼ŒGCP
    IAM æ–‡æ¡£'
- en: '[4] [Create and delete service account keys](https://cloud.google.com/iam/docs/keys-create-delete),
    GCP IAM Docs'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [åˆ›å»ºå’Œåˆ é™¤æœåŠ¡è´¦æˆ·å¯†é’¥](https://cloud.google.com/iam/docs/keys-create-delete)ï¼ŒGCP
    IAM æ–‡æ¡£'
