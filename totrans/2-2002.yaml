- en: The CLIP Foundation Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/the-clip-foundation-model-7770858b487d](https://towardsdatascience.com/the-clip-foundation-model-7770858b487d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[ğŸš€Saschaâ€™s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learning Transferable Visual Models From Natural Language Supervision by A.
    Radford et. al.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)
    Â·8 min readÂ·Aug 26, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: In this article we are going through the paper behind CLIP (**C**ontrastive
    **L**anguage-**I**mage **P**re-Training). We will extract key concepts and break
    them down to make them easy to understand. Further, images and data graphs are
    annotated to clarify doubts.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60cfe45b459db912d2688dab7c9b3d31.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: Image created from [publication](https://arxiv.org/abs/2103.00020) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '**Paper:** [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020),
    Alec Radford et. al., 26 Feb. 2021'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/OpenAI/CLIP) â€” [Blog Post](https://openai.com/research/clip)
    â€” [Hugging Face](https://huggingface.co/docs/transformers/model_doc/clip)'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** multi-modal deep learning, computer vision, natural language
    processing, foundation models, representation learning'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    â€” [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    â€” [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    â€” [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Outline
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Context & Background
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸èƒŒæ™¯
- en: Method
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ–¹æ³•
- en: Experiments
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®éªŒ
- en: Further Readings & Resources
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»ä¸èµ„æº
- en: Context & Background
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸èƒŒæ™¯
- en: CLIP (**C**ontrastive **L**anguage-**I**mage **P**re-Training) is a multi-modal
    model that learns the correspondence between natural language and images. It is
    trained on 400 million text-images pairs collected from the internet. As we will
    discover later in this article, CLIP has strong zero-shot performance, meaning
    it performs well on downstream tasks different to those it was trained on, without
    performing any fine-tuning.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CLIPï¼ˆ**å¯¹æ¯”** **è¯­**è¨€-**å›¾**åƒ **é¢„**è®­ç»ƒï¼‰æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹ï¼Œå­¦ä¹ è‡ªç„¶è¯­è¨€ä¸å›¾åƒä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚å®ƒåœ¨äº’è”ç½‘ä¸Šæ”¶é›†çš„4äº¿å¯¹æ–‡æœ¬-å›¾åƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚æ­£å¦‚æˆ‘ä»¬åœ¨æœ¬æ–‡åé¢å°†æ·±å…¥æ¢è®¨çš„ï¼ŒCLIPå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œè¿™æ„å‘³ç€å®ƒåœ¨ä¸è®­ç»ƒæ—¶ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•å¾®è°ƒã€‚
- en: 'CLIP aims to:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: CLIPçš„ç›®æ ‡æ˜¯ï¼š
- en: Apply the success of large-scale pre-training techniques known from natural
    language processing (e.g. GPT family, [T5](https://arxiv.org/pdf/1910.10683.pdf)
    and [BERT](https://arxiv.org/abs/1810.04805)) to computer vision.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆå¦‚GPTç³»åˆ—ã€[T5](https://arxiv.org/pdf/1910.10683.pdf) å’Œ [BERT](https://arxiv.org/abs/1810.04805)ï¼‰ä¸­å·²çŸ¥çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæŠ€æœ¯çš„æˆåŠŸåº”ç”¨äºè®¡ç®—æœºè§†è§‰ã€‚
- en: Enable flexible zero-shot capabilities by using natural language instead of
    a fixed set class labels.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨è‡ªç„¶è¯­è¨€è€Œä¸æ˜¯å›ºå®šçš„ç±»æ ‡ç­¾ï¼Œå®ç°çµæ´»çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚
- en: Why is this a big deal you might ask yourself? First of all, many computer vision
    models are trained on crowd-sourced labeled datasets. These datasets often contain
    hundreds of thousands samples. Some exceptions are in the region of single or
    double digit million samples. As you can imagine it is a very time consuming and
    costly process. Datasets for natural language models on the other hand are usually
    several orders of magnitudes larger and are scraped from the internet. Secondly,
    if an object detection model has been trained on certain classes and you want
    to add an extra class, you would need to label this new class in your data and
    retrain the model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦å‘¢ï¼Ÿé¦–å…ˆï¼Œè®¸å¤šè®¡ç®—æœºè§†è§‰æ¨¡å‹æ˜¯åŸºäºä¼—åŒ…æ ‡æ³¨çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒçš„ã€‚è¿™äº›æ•°æ®é›†é€šå¸¸åŒ…å«æ•°åä¸‡çš„æ ·æœ¬ã€‚ä¸€äº›ä¾‹å¤–æƒ…å†µæ˜¯åœ¨åƒä¸‡çº§åˆ«çš„æ ·æœ¬æ•°é‡ã€‚å¯ä»¥æƒ³è±¡ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸è€—æ—¶å’Œæ˜‚è´µçš„è¿‡ç¨‹ã€‚å¦ä¸€æ–¹é¢ï¼Œè‡ªç„¶è¯­è¨€æ¨¡å‹çš„æ•°æ®é›†é€šå¸¸å¤§å‡ ä¸ªæ•°é‡çº§ï¼Œå¹¶ä¸”ä»äº’è”ç½‘ä¸ŠæŠ“å–ã€‚å…¶æ¬¡ï¼Œå¦‚æœä¸€ä¸ªç›®æ ‡æ£€æµ‹æ¨¡å‹å·²ç»åœ¨æŸäº›ç±»åˆ«ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè€Œä½ æƒ³æ·»åŠ ä¸€ä¸ªé¢å¤–çš„ç±»åˆ«ï¼Œä½ éœ€è¦åœ¨æ•°æ®ä¸­æ ‡æ³¨è¿™ä¸ªæ–°ç±»åˆ«å¹¶é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚
- en: CLIPâ€™s ability to combine natural language and image features in combination
    with its zero-shot performance has led to a wide adoption in many other popular
    foundation models such as [UnCLIP](https://arxiv.org/abs/2204.06125), [EVA](https://arxiv.org/abs/2211.07636),
    [SAM](https://arxiv.org/abs/2304.02643), [Stable Diffusion](https://arxiv.org/abs/2204.06125),
    [GLIDE](https://arxiv.org/abs/2112.10741) or [VQGAN-CLIP,](https://arxiv.org/abs/2204.08583)
    to name a few.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: CLIPç»“åˆè‡ªç„¶è¯­è¨€å’Œå›¾åƒç‰¹å¾çš„èƒ½åŠ›ï¼ŒåŠ ä¸Šå…¶é›¶æ ·æœ¬æ€§èƒ½ï¼Œå·²ç»å¯¼è‡´è®¸å¤šå…¶ä»–æµè¡ŒåŸºç¡€æ¨¡å‹çš„å¹¿æ³›é‡‡ç”¨ï¼Œä¾‹å¦‚[UnCLIP](https://arxiv.org/abs/2204.06125)ã€[EVA](https://arxiv.org/abs/2211.07636)ã€[SAM](https://arxiv.org/abs/2304.02643)ã€[ç¨³å®šæ‰©æ•£](https://arxiv.org/abs/2204.06125)ã€[GLIDE](https://arxiv.org/abs/2112.10741)æˆ–[VQGAN-CLIP](https://arxiv.org/abs/2204.08583)ç­‰ã€‚
- en: Method
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¹æ³•
- en: Now letâ€™s dive into the method of CLIP. The image bellow depicted in Fig.1 shows
    the architecture of CLIP and the process of how it is trained
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨CLIPçš„æ–¹æ³•ã€‚ä¸‹é¢çš„å›¾1å±•ç¤ºäº†CLIPçš„æ¶æ„ä»¥åŠå…¶è®­ç»ƒè¿‡ç¨‹ã€‚
- en: '![](../Images/2e39e17e5f547917a216759063155235.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e39e17e5f547917a216759063155235.png)'
- en: Fig. 1 â€” CLIPâ€™s Architecture and training process. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1 â€” CLIPçš„æ¶æ„å’Œè®­ç»ƒè¿‡ç¨‹ã€‚[å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2103.00020) + [Sascha Kirch](https://medium.com/@SaschaKirch)çš„æ³¨é‡Š
- en: The model architecture consists of two encoder models, one for each modality.
    For the text encoder a transformer was used while the image encoder uses either
    a version of ResNet or [ViT (Vision Transformer)](https://arxiv.org/abs/2010.11929).
    A learned linear transformation, one for each modality, transforms the features
    into embeddings of matching size. Finally, the cosine similarity is calculated
    between each of the embeddings of opposing modality and is scaled by a learned
    temperature scalar. During training, the cosine similarity between matching pairs
    is maximized while it is minimized for incorrect pairs, hence the term â€œcontrastiveâ€
    in the frameworkâ€™s name.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ¶æ„ç”±ä¸¤ä¸ªç¼–ç å™¨æ¨¡å‹ç»„æˆï¼Œæ¯ä¸ªæ¨¡æ€ä¸€ä¸ªã€‚å¯¹äºæ–‡æœ¬ç¼–ç å™¨ä½¿ç”¨äº†ä¸€ä¸ªå˜æ¢å™¨ï¼Œè€Œå›¾åƒç¼–ç å™¨åˆ™ä½¿ç”¨äº†[ViT (Vision Transformer)](https://arxiv.org/abs/2010.11929)æˆ–æŸä¸ªç‰ˆæœ¬çš„ResNetã€‚ä¸€ä¸ªé’ˆå¯¹æ¯ä¸ªæ¨¡æ€çš„å­¦ä¹ çº¿æ€§å˜æ¢å°†ç‰¹å¾è½¬æ¢ä¸ºåŒ¹é…å¤§å°çš„åµŒå…¥å‘é‡ã€‚æœ€åï¼Œè®¡ç®—å¯¹ç«‹æ¨¡æ€çš„æ¯å¯¹åµŒå…¥å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªå­¦ä¹ åˆ°çš„æ¸©åº¦æ ‡é‡è¿›è¡Œç¼©æ”¾ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŒ¹é…å¯¹ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦è¢«æœ€å¤§åŒ–ï¼Œè€Œä¸æ­£ç¡®å¯¹çš„ç›¸ä¼¼åº¦è¢«æœ€å°åŒ–ï¼Œå› æ­¤æ¡†æ¶åç§°ä¸­ä½¿ç”¨äº†â€œå¯¹æ¯”â€ä¸€è¯ã€‚
- en: There are some subtleties that are crucial for the success, beside the large
    dataset of course. First, the contrastive learning approach strongly depends on
    the batch size N. The more negative samples are provided along the correct ones,
    the stronger the learning signal. CLIP was trained on a batch size of 32,768,
    which is quite large. Second, CLIP does not learn a match of the exact wording,
    but an easier proxy task to only learn the text as a whole, also called bag of
    words (BoW).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å¤§æ•°æ®é›†ä¹‹å¤–ï¼Œè¿˜æœ‰ä¸€äº›å¯¹æˆåŠŸè‡³å…³é‡è¦çš„ç»†èŠ‚ã€‚é¦–å…ˆï¼Œå¯¹æ¯”å­¦ä¹ æ–¹æ³•å¼ºçƒˆä¾èµ–äºæ‰¹æ¬¡å¤§å°Nã€‚æä¾›çš„è´Ÿæ ·æœ¬è¶Šå¤šï¼Œå­¦ä¹ ä¿¡å·è¶Šå¼ºã€‚CLIPçš„è®­ç»ƒæ‰¹æ¬¡å¤§å°ä¸º32,768ï¼Œè¿™ç›¸å½“å¤§ã€‚å…¶æ¬¡ï¼ŒCLIPå¹¶ä¸æ˜¯å­¦ä¹ ç²¾ç¡®çš„æªè¾åŒ¹é…ï¼Œè€Œæ˜¯ä¸€ä¸ªæ›´ç®€å•çš„ä»£ç†ä»»åŠ¡ï¼Œåªéœ€å­¦ä¹ æ•´ä½“æ–‡æœ¬ï¼Œä¹Ÿç§°ä¸ºè¯è¢‹ï¼ˆBoWï¼‰ã€‚
- en: '**Fun Fact:** The version of CLIP using a ResNet50x64 as image encoder was
    trained for 18 days on 592 V100 GPUS and while the version with the ViT model
    was trained for 12 days on 256 V100 GPUS. In other words, **over 29 years** and
    **over 8 years** on a single GPU respectively (ignoring the fact a different batch
    size would be used).'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æœ‰è¶£çš„äº‹å®ï¼š** ä½¿ç”¨ResNet50x64ä½œä¸ºå›¾åƒç¼–ç å™¨çš„CLIPç‰ˆæœ¬åœ¨592å°V100 GPUä¸Šè®­ç»ƒäº†18å¤©ï¼Œè€Œä½¿ç”¨ViTæ¨¡å‹çš„ç‰ˆæœ¬åˆ™åœ¨256å°V100
    GPUä¸Šè®­ç»ƒäº†12å¤©ã€‚æ¢å¥è¯è¯´ï¼Œ**åœ¨å•ä¸ªGPUä¸Šåˆ†åˆ«è¶…è¿‡29å¹´**å’Œ**è¶…è¿‡8å¹´**ï¼ˆå¿½ç•¥ä½¿ç”¨ä¸åŒæ‰¹æ¬¡å¤§å°çš„äº‹å®ï¼‰ã€‚'
- en: 'Once the model is trained it can be used to perform object classification on
    a images. The question is: how to perform classification using a model that has
    not been trained to classify images nor does input class labels but text prompts?
    Fig 2\. shows how:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå®ƒå¯ä»¥ç”¨äºå¯¹å›¾åƒè¿›è¡Œå¯¹è±¡åˆ†ç±»ã€‚é—®é¢˜æ˜¯ï¼šå¦‚ä½•ä½¿ç”¨ä¸€ä¸ªæ²¡æœ‰è¢«è®­ç»ƒæ¥åˆ†ç±»å›¾åƒæˆ–è¾“å…¥ç±»åˆ«æ ‡ç­¾è€Œæ˜¯æ–‡æœ¬æç¤ºçš„æ¨¡å‹è¿›è¡Œåˆ†ç±»ï¼Ÿå›¾ 2\. æ˜¾ç¤ºäº†å¦‚ä½•æ“ä½œï¼š
- en: '![](../Images/75d808b0af753bd63ddcea1a3c929526.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75d808b0af753bd63ddcea1a3c929526.png)'
- en: Fig. 2 â€” CLIPâ€™s Architecture for image classification. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2 â€” CLIPçš„å›¾åƒåˆ†ç±»æ¶æ„ã€‚[å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2103.00020) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    çš„æ³¨é‡Š
- en: A class label can be seen as a text prompt formed by a single word. To tell
    the model, which classes are available for the classification task, a set of N
    classes is input into the model. This is a huge advantage compared to classification
    models trained on a fixed set of labels. We can now either input 3 classes or
    100; itâ€™s our choice. As we will see later, to improve the performance of CLIP,
    the class label is transformed into a prompt to provide further context to the
    model. Each prompt is then fed to the text encoder and is then transformed into
    an embedding vector.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«æ ‡ç­¾å¯ä»¥è§†ä¸ºç”±å•ä¸ªè¯å½¢æˆçš„æ–‡æœ¬æç¤ºã€‚ä¸ºäº†å‘ŠçŸ¥æ¨¡å‹å¯ç”¨äºåˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«ï¼Œä¸€ç»„Nä¸ªç±»åˆ«è¢«è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚è¿™ç›¸æ¯”äºåœ¨å›ºå®šæ ‡ç­¾é›†ä¸Šè®­ç»ƒçš„åˆ†ç±»æ¨¡å‹å…·æœ‰å·¨å¤§ä¼˜åŠ¿ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥è¾“å…¥3ä¸ªç±»åˆ«æˆ–100ä¸ªç±»åˆ«ï¼›è¿™ç”±æˆ‘ä»¬å†³å®šã€‚æ­£å¦‚æˆ‘ä»¬åé¢å°†çœ‹åˆ°çš„ï¼Œä¸ºäº†æé«˜CLIPçš„æ€§èƒ½ï¼Œç±»åˆ«æ ‡ç­¾ä¼šè¢«è½¬åŒ–ä¸ºæç¤ºï¼Œä»¥æä¾›æ›´å¤šä¸Šä¸‹æ–‡ã€‚æ¯ä¸ªæç¤ºéšåè¢«è¾“å…¥åˆ°æ–‡æœ¬ç¼–ç å™¨ä¸­ï¼Œå¹¶è½¬æ¢ä¸ºåµŒå…¥å‘é‡ã€‚
- en: The input image is fed into the image encoder to obtain the embedding vector.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥å›¾åƒè¢«è¾“å…¥åˆ°å›¾åƒç¼–ç å™¨ä¸­ï¼Œä»¥è·å–åµŒå…¥å‘é‡ã€‚
- en: Then the cosine similarity is calculated for each pair of text and image embeddings.
    A Softmax is applied on the obtained similarity values to form a probability distribution.
    Finally, the value with the highest probability is selected as the final prediction.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè®¡ç®—æ¯å¯¹æ–‡æœ¬å’Œå›¾åƒåµŒå…¥å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚å¯¹è·å¾—çš„ç›¸ä¼¼åº¦å€¼åº”ç”¨Softmaxä»¥å½¢æˆæ¦‚ç‡åˆ†å¸ƒã€‚æœ€åï¼Œé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å€¼ä½œä¸ºæœ€ç»ˆé¢„æµ‹ã€‚
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Paper Walkthroughs by Sascha Kirch
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----7770858b487d--------------------------------)7
    stories![â€œDDPMâ€Šâ€”â€ŠDenoising Diffusion Probabilistic Models â€œ paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![â€œDepth Anythingâ€
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Experiments and Ablations
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CLIP paper presents a vast number of experiments and ablations. Here we
    will cover five, from which I think are important to understand the success of
    CLIP. Upfront the take aways (as formulated by the authors of CLIP) and then we
    will dive into the details:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Efficiency:** CLIP is much more efficient at zero-shot transfer
    than our image caption baseline'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Text Input Format:** Prompt engineering and ensembling improve zero-shot
    performance'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Zero-Shot Performance:** Zero-shot CLIP is competitive with fully super-vised
    baseline'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Few-Shot Performance:** Zero-shot CLIP outperforms few-shot linear probes'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Distribution Shift:** Zero-shot CLIP is much more robust to distribution
    shift than standard ImageNet models'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training Efficiency
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During training, the image encoder and the text encoder are trained jointly,
    meaning with a single training objective and at the same time. Not only does CLIP
    perform a contrastive learning scheme, but the text prompts are compared as a
    whole against a given image, hence the order of words does not matter. It is simply
    a â€œbag of wordsâ€. The phrase â€œmy name is Saschaâ€ results in the same embedding
    as â€œSascha name is myâ€.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Predicting a bag of words instead of the correct words and its position in a
    phrase is a much easier proxy objective. Fig 3\. bellow shows the zero-shot accuracy
    on ImageNet over the number of training samples of the initial transformer model
    trained to predict exact words, the initial transformer model trained to predict
    a bag of words and the CLIP model that performs contrastive learning using bag
    of words.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: â€œCLIP is much more efficient at zero-shot transfer than our image caption baselineâ€
    â€” CLIP Authors
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/a8a3d4fc5a28d633b7e25e4922e1738b.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Fig. 3 â€” Zero-shot efficiency. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Text Input Format
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen in Fig. 2, to perform object classification, the class label
    has been converted into a text prompt. Of course, this was not by chance, because
    CLIP would be totally fine with a single word. It was done to leverage the descriptiveness
    of language and to provide context to resolve possible ambiguities. Letâ€™s take
    the word â€œboxerâ€ for example. It could be a type of dog or a type of athlete.
    The authors of CLIP have shown that the format of the text prompt matters a lot
    and can boost the performance as well increase the efficiency.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨å›¾2ä¸­çœ‹åˆ°çš„ï¼Œä¸ºäº†è¿›è¡Œå¯¹è±¡åˆ†ç±»ï¼Œç±»åˆ«æ ‡ç­¾è¢«è½¬æ¢ä¸ºæ–‡æœ¬æç¤ºã€‚å½“ç„¶ï¼Œè¿™ä¸æ˜¯å¶ç„¶çš„ï¼Œå› ä¸ºCLIPå¯¹å•è¯æ˜¯å®Œå…¨é€‚åº”çš„ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†åˆ©ç”¨è¯­è¨€çš„æè¿°æ€§ï¼Œå¹¶æä¾›ä¸Šä¸‹æ–‡ä»¥è§£å†³å¯èƒ½çš„æ­§ä¹‰ã€‚ä»¥â€œboxerâ€è¿™ä¸ªè¯ä¸ºä¾‹ï¼Œå®ƒå¯èƒ½æ˜¯æŸç§çŠ¬ç±»æˆ–æŸç§è¿åŠ¨å‘˜ã€‚CLIPçš„ä½œè€…å±•ç¤ºäº†æ–‡æœ¬æç¤ºçš„æ ¼å¼éå¸¸é‡è¦ï¼Œå®ƒå¯ä»¥æå‡æ€§èƒ½å¹¶æé«˜æ•ˆç‡ã€‚
- en: â€œPrompt engineering and ensembling improve zero-shot performanceâ€ â€” CLIP Authors
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œæç¤ºå·¥ç¨‹å’Œé›†æˆæå‡äº†é›¶æ ·æœ¬æ€§èƒ½â€ â€” CLIPä½œè€…
- en: '![](../Images/65a283f3d219f0792194ca18b19fe04f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65a283f3d219f0792194ca18b19fe04f.png)'
- en: Fig. 4â€” Prompt engineering and ensembling vs. contextless class names. [Image
    Source](https://arxiv.org/abs/2103.00020) + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4â€” æç¤ºå·¥ç¨‹å’Œé›†æˆä¸æ— ä¸Šä¸‹æ–‡ç±»åˆ«åç§°ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2103.00020) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    æ³¨é‡Š
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----7770858b487d--------------------------------)
    [## Get an email whenever Sascha Kirch publishes ğŸš€'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----7770858b487d--------------------------------)
    [## è®¢é˜…Sascha Kirchçš„æ›´æ–° ğŸš€'
- en: Get an email whenever Sascha Kirch publishes ğŸš€ Looking to learn more about deep
    learning or simply stay up to dateâ€¦
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¢é˜…Sascha Kirchçš„æ›´æ–° ğŸš€ æƒ³äº†è§£æ›´å¤šæ·±åº¦å­¦ä¹ å†…å®¹æˆ–ä¿æŒæœ€æ–°åŠ¨æ€â€¦â€¦
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----7770858b487d--------------------------------)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----7770858b487d--------------------------------)
- en: '**Zero-Shot Performance**'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**é›¶æ ·æœ¬æ€§èƒ½**'
- en: In another experiment, the authors compared the zero-shot image classification
    performance of CLIP against a model that was trained specifically on the dataset
    under comparison.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¦ä¸€ä¸ªå®éªŒä¸­ï¼Œä½œè€…å°†CLIPçš„é›¶æ ·æœ¬å›¾åƒåˆ†ç±»æ€§èƒ½ä¸ä¸“é—¨åœ¨æ¯”è¾ƒæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚
- en: â€œZero-shot CLIP is competitive with fully super-vised baselineâ€ â€” CLIP Authors
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œé›¶æ ·æœ¬CLIPä¸å®Œå…¨ç›‘ç£åŸºçº¿å…·æœ‰ç«äº‰åŠ›â€ â€” CLIPä½œè€…
- en: '![](../Images/d363b6bca65a2bcc47ea077a5930f453.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d363b6bca65a2bcc47ea077a5930f453.png)'
- en: Fig. 5â€” Zero-Shot CLIP vs. Supervised baseline. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by author
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5â€” é›¶æ ·æœ¬CLIP vs. ç›‘ç£åŸºçº¿ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2103.00020) + ä½œè€…æ³¨é‡Š
- en: '**Few-Shot Performance**'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å°‘æ ·æœ¬æ€§èƒ½**'
- en: While zero-shot predictors are not fine-tuned on the downstream task, few shot
    detectors are. The authors experimented with multiple publicly available pre-trained
    models and compared their few-shot performance on 20 different datasets against
    zero-shot and few-shot CLIP. The few-shot models have been fine-tuned on 1, 2,
    4, 8 and 16 examples per class.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡é›¶æ ·æœ¬é¢„æµ‹å™¨æ²¡æœ‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä½†å°‘æ ·æœ¬æ£€æµ‹å™¨åˆ™è¿›è¡Œäº†å¾®è°ƒã€‚ä½œè€…å¯¹å¤šä¸ªå…¬å¼€çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œäº†å®éªŒï¼Œå¹¶å°†å®ƒä»¬åœ¨20ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„å°‘æ ·æœ¬æ€§èƒ½ä¸é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬CLIPè¿›è¡Œäº†æ¯”è¾ƒã€‚å°‘æ ·æœ¬æ¨¡å‹åœ¨æ¯ä¸ªç±»åˆ«çš„1ã€2ã€4ã€8å’Œ16ä¸ªç¤ºä¾‹ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚
- en: Interestingly, zero-shot CLIP performs roughly as good as 4-shot CLIP.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œé›¶æ ·æœ¬CLIPçš„è¡¨ç°å¤§è‡´ä¸4æ ·æœ¬CLIPç›¸å½“ã€‚
- en: If comparing CLIP to other models, one must consider that the publicly available
    models under comparison (i.e. [BiT](https://arxiv.org/abs/1912.11370), [SimCLR](https://arxiv.org/abs/2006.10029)
    and ResNet) have been pre-trained on different and smaller datasets as the CLIP
    model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå°†CLIPä¸å…¶ä»–æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œå¿…é¡»è€ƒè™‘åˆ°æ‰€æ¯”è¾ƒçš„å…¬å¼€æ¨¡å‹ï¼ˆå³ [BiT](https://arxiv.org/abs/1912.11370)ã€[SimCLR](https://arxiv.org/abs/2006.10029)
    å’Œ ResNetï¼‰æ˜¯åœ¨ä¸åŒä¸”è¾ƒå°çš„æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œè€Œä¸æ˜¯CLIPæ¨¡å‹ã€‚
- en: â€œZero-shot CLIP outperforms few-shot linear probesâ€ â€” CLIP Authors
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œé›¶æ ·æœ¬CLIPä¼˜äºå°‘æ ·æœ¬çº¿æ€§æ¢é’ˆâ€ â€” CLIPä½œè€…
- en: '![](../Images/e65731d84d6a2d8069f1b9a9bb13bea0.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e65731d84d6a2d8069f1b9a9bb13bea0.png)'
- en: Fig. 6â€” Few-shot performance. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6â€” å°‘æ ·æœ¬æ€§èƒ½ã€‚ [å›¾ç‰‡æ¥æº](https://arxiv.org/abs/2103.00020) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    æ³¨é‡Š
- en: '**Distribution Shift**'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**åˆ†å¸ƒè½¬ç§»**'
- en: Generally speaking, a modelâ€™s robustness towards distribution shifts refers
    to its capability to perform as good on data of a different data distribution
    as on the data distribution of the data it was trained on. Ideally, it would perform
    equally well. In reality, its performance drops.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œæ¨¡å‹å¯¹åˆ†å¸ƒå˜åŒ–çš„é²æ£’æ€§æŒ‡çš„æ˜¯å®ƒåœ¨ä¸åŒæ•°æ®åˆ†å¸ƒçš„æ•°æ®ä¸Šè¡¨ç°å¾—ä¸åœ¨è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸Šçš„è¡¨ç°ä¸€æ ·å¥½ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œå®ƒçš„è¡¨ç°åº”å½“ä¸€æ ·å¥½ã€‚ä½†åœ¨ç°å®ä¸­ï¼Œå…¶æ€§èƒ½ä¼šä¸‹é™ã€‚
- en: The robustness of zero-shot CLIP has been compared to a ResNet101 ImageNet model.
    Both models are evaluated on natural distribution shifts of ImageNet, as depicted
    in Fig. 7.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: é›¶-shot CLIPçš„é²æ£’æ€§å·²ä¸ResNet101 ImageNetæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚è¿™ä¸¤ç§æ¨¡å‹åœ¨ImageNetçš„è‡ªç„¶åˆ†å¸ƒå˜åŒ–ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¦‚å›¾7æ‰€ç¤ºã€‚
- en: â€œZero-shot CLIP is much more robust to distribution shift than standard ImageNet
    modelsâ€ â€” CLIP Authors
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œé›¶-shot CLIPå¯¹åˆ†å¸ƒå˜åŒ–çš„é²æ£’æ€§è¿œè¶…æ ‡å‡†ImageNetæ¨¡å‹â€ â€” CLIPä½œè€…
- en: '![](../Images/65795f889f88a6bd09c103faa794ad03.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65795f889f88a6bd09c103faa794ad03.png)'
- en: Fig. 7 â€” Distribution shift. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7 â€” åˆ†å¸ƒå˜åŒ–ã€‚ [å›¾åƒæ¥æº](https://arxiv.org/abs/2103.00020) + [Sascha Kirch](https://medium.com/@SaschaKirch)çš„æ³¨é‡Š
- en: Further Readings & Resources
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»ä¸èµ„æº
- en: As mentioned at the beginning of this article, CLIP has been widely adopted
    by a vast number of projects.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æœ¬æ–‡å¼€å¤´æ‰€æåˆ°çš„ï¼ŒCLIPå·²è¢«å¹¿æ³›åº”ç”¨äºå¤§é‡é¡¹ç›®ä¸­ã€‚
- en: 'Following a list of papers using CLIP:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä½¿ç”¨CLIPçš„è®ºæ–‡åˆ—è¡¨ï¼š
- en: '[[UnCLIP] Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125)'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[UnCLIP] ä½¿ç”¨CLIPæ½œåœ¨å˜é‡è¿›è¡Œåˆ†å±‚æ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆ](https://arxiv.org/abs/2204.06125)'
- en: '[[EVA] Exploring the Limits of Masked Visual Representation Learning at Scale](https://arxiv.org/abs/2211.07636)'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[EVA] æ¢ç´¢å¤§è§„æ¨¡é®è”½è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„æé™](https://arxiv.org/abs/2211.07636)'
- en: '[[SAM] Segment Anything](https://arxiv.org/abs/2304.02643)'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[SAM] åˆ†å‰²ä»»æ„å†…å®¹](https://arxiv.org/abs/2304.02643)'
- en: '[[Stable Diffusion] High-Resolution Image Synthesis with Latent Diffusion Models](http://arxiv.org/abs/2112.10752)'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[ç¨³å®šæ‰©æ•£] ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆ](http://arxiv.org/abs/2112.10752)'
- en: '[[GLIDE](https://arxiv.org/abs/2112.10741)] Towards Photorealistic Image Generation
    and Editing with Text-Guided Diffusion Models'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[GLIDE](https://arxiv.org/abs/2112.10741)] æœç€æ–‡æœ¬å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œé€¼çœŸçš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘'
- en: '[[VQGAN-CLIP] Open Domain Image Generation and Editing with Natural Language
    Guidance](https://arxiv.org/abs/2204.08583)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[VQGAN-CLIP] ä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡å¯¼çš„å¼€æ”¾é¢†åŸŸå›¾åƒç”Ÿæˆå’Œç¼–è¾‘](https://arxiv.org/abs/2204.08583)'
- en: 'And a list of repositories if you want to dive into the implementation and
    test it yourself:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³æ·±å…¥äº†è§£å®ç°å¹¶è‡ªè¡Œæµ‹è¯•ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›åº“åˆ—è¡¨ï¼š
- en: '[Official Repo by OpenAI](https://github.com/openai/CLIP)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAIå®˜æ–¹åº“](https://github.com/openai/CLIP)'
- en: '[Python Notebook to play around with CLIP](https://github.com/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pythonç¬”è®°æœ¬ä»¥ä¾¿æ“ä½œCLIP](https://github.com/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb)'
- en: '[OpenCLIP: Open Source Implementation of CLIP](https://github.com/mlfoundations/open_clip)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenCLIPï¼šCLIPçš„å¼€æºå®ç°](https://github.com/mlfoundations/open_clip)'
