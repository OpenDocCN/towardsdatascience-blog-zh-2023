- en: The CLIP Foundation Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-clip-foundation-model-7770858b487d](https://towardsdatascience.com/the-clip-foundation-model-7770858b487d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[🚀Sascha’s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learning Transferable Visual Models From Natural Language Supervision by A.
    Radford et. al.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)
    ·8 min read·Aug 26, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: In this article we are going through the paper behind CLIP (**C**ontrastive
    **L**anguage-**I**mage **P**re-Training). We will extract key concepts and break
    them down to make them easy to understand. Further, images and data graphs are
    annotated to clarify doubts.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60cfe45b459db912d2688dab7c9b3d31.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: Image created from [publication](https://arxiv.org/abs/2103.00020) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '**Paper:** [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020),
    Alec Radford et. al., 26 Feb. 2021'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/OpenAI/CLIP) — [Blog Post](https://openai.com/research/clip)
    — [Hugging Face](https://huggingface.co/docs/transformers/model_doc/clip)'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** multi-modal deep learning, computer vision, natural language
    processing, foundation models, representation learning'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    — [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Outline
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Context & Background
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 背景与背景
- en: Method
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法
- en: Experiments
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实验
- en: Further Readings & Resources
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步阅读与资源
- en: Context & Background
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景与背景
- en: CLIP (**C**ontrastive **L**anguage-**I**mage **P**re-Training) is a multi-modal
    model that learns the correspondence between natural language and images. It is
    trained on 400 million text-images pairs collected from the internet. As we will
    discover later in this article, CLIP has strong zero-shot performance, meaning
    it performs well on downstream tasks different to those it was trained on, without
    performing any fine-tuning.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP（**对比** **语**言-**图**像 **预**训练）是一个多模态模型，学习自然语言与图像之间的对应关系。它在互联网上收集的4亿对文本-图像数据上进行训练。正如我们在本文后面将深入探讨的，CLIP具有强大的零样本性能，这意味着它在与训练时不同的下游任务上表现良好，而无需进行任何微调。
- en: 'CLIP aims to:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP的目标是：
- en: Apply the success of large-scale pre-training techniques known from natural
    language processing (e.g. GPT family, [T5](https://arxiv.org/pdf/1910.10683.pdf)
    and [BERT](https://arxiv.org/abs/1810.04805)) to computer vision.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将自然语言处理（如GPT系列、[T5](https://arxiv.org/pdf/1910.10683.pdf) 和 [BERT](https://arxiv.org/abs/1810.04805)）中已知的大规模预训练技术的成功应用于计算机视觉。
- en: Enable flexible zero-shot capabilities by using natural language instead of
    a fixed set class labels.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用自然语言而不是固定的类标签，实现灵活的零样本能力。
- en: Why is this a big deal you might ask yourself? First of all, many computer vision
    models are trained on crowd-sourced labeled datasets. These datasets often contain
    hundreds of thousands samples. Some exceptions are in the region of single or
    double digit million samples. As you can imagine it is a very time consuming and
    costly process. Datasets for natural language models on the other hand are usually
    several orders of magnitudes larger and are scraped from the internet. Secondly,
    if an object detection model has been trained on certain classes and you want
    to add an extra class, you would need to label this new class in your data and
    retrain the model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这很重要呢？首先，许多计算机视觉模型是基于众包标注的数据集进行训练的。这些数据集通常包含数十万的样本。一些例外情况是在千万级别的样本数量。可以想象，这是一个非常耗时和昂贵的过程。另一方面，自然语言模型的数据集通常大几个数量级，并且从互联网上抓取。其次，如果一个目标检测模型已经在某些类别上进行训练，而你想添加一个额外的类别，你需要在数据中标注这个新类别并重新训练模型。
- en: CLIP’s ability to combine natural language and image features in combination
    with its zero-shot performance has led to a wide adoption in many other popular
    foundation models such as [UnCLIP](https://arxiv.org/abs/2204.06125), [EVA](https://arxiv.org/abs/2211.07636),
    [SAM](https://arxiv.org/abs/2304.02643), [Stable Diffusion](https://arxiv.org/abs/2204.06125),
    [GLIDE](https://arxiv.org/abs/2112.10741) or [VQGAN-CLIP,](https://arxiv.org/abs/2204.08583)
    to name a few.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP结合自然语言和图像特征的能力，加上其零样本性能，已经导致许多其他流行基础模型的广泛采用，例如[UnCLIP](https://arxiv.org/abs/2204.06125)、[EVA](https://arxiv.org/abs/2211.07636)、[SAM](https://arxiv.org/abs/2304.02643)、[稳定扩散](https://arxiv.org/abs/2204.06125)、[GLIDE](https://arxiv.org/abs/2112.10741)或[VQGAN-CLIP](https://arxiv.org/abs/2204.08583)等。
- en: Method
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: Now let’s dive into the method of CLIP. The image bellow depicted in Fig.1 shows
    the architecture of CLIP and the process of how it is trained
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入探讨CLIP的方法。下面的图1展示了CLIP的架构以及其训练过程。
- en: '![](../Images/2e39e17e5f547917a216759063155235.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e39e17e5f547917a216759063155235.png)'
- en: Fig. 1 — CLIP’s Architecture and training process. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 — CLIP的架构和训练过程。[图片来源](https://arxiv.org/abs/2103.00020) + [Sascha Kirch](https://medium.com/@SaschaKirch)的注释
- en: The model architecture consists of two encoder models, one for each modality.
    For the text encoder a transformer was used while the image encoder uses either
    a version of ResNet or [ViT (Vision Transformer)](https://arxiv.org/abs/2010.11929).
    A learned linear transformation, one for each modality, transforms the features
    into embeddings of matching size. Finally, the cosine similarity is calculated
    between each of the embeddings of opposing modality and is scaled by a learned
    temperature scalar. During training, the cosine similarity between matching pairs
    is maximized while it is minimized for incorrect pairs, hence the term “contrastive”
    in the framework’s name.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构由两个编码器模型组成，每个模态一个。对于文本编码器使用了一个变换器，而图像编码器则使用了[ViT (Vision Transformer)](https://arxiv.org/abs/2010.11929)或某个版本的ResNet。一个针对每个模态的学习线性变换将特征转换为匹配大小的嵌入向量。最后，计算对立模态的每对嵌入向量之间的余弦相似度，并通过一个学习到的温度标量进行缩放。在训练过程中，匹配对之间的余弦相似度被最大化，而不正确对的相似度被最小化，因此框架名称中使用了“对比”一词。
- en: There are some subtleties that are crucial for the success, beside the large
    dataset of course. First, the contrastive learning approach strongly depends on
    the batch size N. The more negative samples are provided along the correct ones,
    the stronger the learning signal. CLIP was trained on a batch size of 32,768,
    which is quite large. Second, CLIP does not learn a match of the exact wording,
    but an easier proxy task to only learn the text as a whole, also called bag of
    words (BoW).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了大数据集之外，还有一些对成功至关重要的细节。首先，对比学习方法强烈依赖于批次大小N。提供的负样本越多，学习信号越强。CLIP的训练批次大小为32,768，这相当大。其次，CLIP并不是学习精确的措辞匹配，而是一个更简单的代理任务，只需学习整体文本，也称为词袋（BoW）。
- en: '**Fun Fact:** The version of CLIP using a ResNet50x64 as image encoder was
    trained for 18 days on 592 V100 GPUS and while the version with the ViT model
    was trained for 12 days on 256 V100 GPUS. In other words, **over 29 years** and
    **over 8 years** on a single GPU respectively (ignoring the fact a different batch
    size would be used).'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**有趣的事实：** 使用ResNet50x64作为图像编码器的CLIP版本在592台V100 GPU上训练了18天，而使用ViT模型的版本则在256台V100
    GPU上训练了12天。换句话说，**在单个GPU上分别超过29年**和**超过8年**（忽略使用不同批次大小的事实）。'
- en: 'Once the model is trained it can be used to perform object classification on
    a images. The question is: how to perform classification using a model that has
    not been trained to classify images nor does input class labels but text prompts?
    Fig 2\. shows how:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，它可以用于对图像进行对象分类。问题是：如何使用一个没有被训练来分类图像或输入类别标签而是文本提示的模型进行分类？图 2\. 显示了如何操作：
- en: '![](../Images/75d808b0af753bd63ddcea1a3c929526.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75d808b0af753bd63ddcea1a3c929526.png)'
- en: Fig. 2 — CLIP’s Architecture for image classification. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 — CLIP的图像分类架构。[图片来源](https://arxiv.org/abs/2103.00020) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    的注释
- en: A class label can be seen as a text prompt formed by a single word. To tell
    the model, which classes are available for the classification task, a set of N
    classes is input into the model. This is a huge advantage compared to classification
    models trained on a fixed set of labels. We can now either input 3 classes or
    100; it’s our choice. As we will see later, to improve the performance of CLIP,
    the class label is transformed into a prompt to provide further context to the
    model. Each prompt is then fed to the text encoder and is then transformed into
    an embedding vector.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 类别标签可以视为由单个词形成的文本提示。为了告知模型可用于分类任务的类别，一组N个类别被输入到模型中。这相比于在固定标签集上训练的分类模型具有巨大优势。我们现在可以输入3个类别或100个类别；这由我们决定。正如我们后面将看到的，为了提高CLIP的性能，类别标签会被转化为提示，以提供更多上下文。每个提示随后被输入到文本编码器中，并转换为嵌入向量。
- en: The input image is fed into the image encoder to obtain the embedding vector.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像被输入到图像编码器中，以获取嵌入向量。
- en: Then the cosine similarity is calculated for each pair of text and image embeddings.
    A Softmax is applied on the obtained similarity values to form a probability distribution.
    Finally, the value with the highest probability is selected as the final prediction.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算每对文本和图像嵌入向量的余弦相似度。对获得的相似度值应用Softmax以形成概率分布。最后，选择概率最高的值作为最终预测。
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Paper Walkthroughs by Sascha Kirch
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----7770858b487d--------------------------------)7
    stories![“DDPM — Denoising Diffusion Probabilistic Models “ paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![“Depth Anything”
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Experiments and Ablations
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CLIP paper presents a vast number of experiments and ablations. Here we
    will cover five, from which I think are important to understand the success of
    CLIP. Upfront the take aways (as formulated by the authors of CLIP) and then we
    will dive into the details:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Efficiency:** CLIP is much more efficient at zero-shot transfer
    than our image caption baseline'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Text Input Format:** Prompt engineering and ensembling improve zero-shot
    performance'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Zero-Shot Performance:** Zero-shot CLIP is competitive with fully super-vised
    baseline'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Few-Shot Performance:** Zero-shot CLIP outperforms few-shot linear probes'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Distribution Shift:** Zero-shot CLIP is much more robust to distribution
    shift than standard ImageNet models'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training Efficiency
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During training, the image encoder and the text encoder are trained jointly,
    meaning with a single training objective and at the same time. Not only does CLIP
    perform a contrastive learning scheme, but the text prompts are compared as a
    whole against a given image, hence the order of words does not matter. It is simply
    a “bag of words”. The phrase “my name is Sascha” results in the same embedding
    as “Sascha name is my”.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Predicting a bag of words instead of the correct words and its position in a
    phrase is a much easier proxy objective. Fig 3\. bellow shows the zero-shot accuracy
    on ImageNet over the number of training samples of the initial transformer model
    trained to predict exact words, the initial transformer model trained to predict
    a bag of words and the CLIP model that performs contrastive learning using bag
    of words.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: “CLIP is much more efficient at zero-shot transfer than our image caption baseline”
    — CLIP Authors
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/a8a3d4fc5a28d633b7e25e4922e1738b.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Fig. 3 — Zero-shot efficiency. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Text Input Format
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen in Fig. 2, to perform object classification, the class label
    has been converted into a text prompt. Of course, this was not by chance, because
    CLIP would be totally fine with a single word. It was done to leverage the descriptiveness
    of language and to provide context to resolve possible ambiguities. Let’s take
    the word “boxer” for example. It could be a type of dog or a type of athlete.
    The authors of CLIP have shown that the format of the text prompt matters a lot
    and can boost the performance as well increase the efficiency.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在图2中看到的，为了进行对象分类，类别标签被转换为文本提示。当然，这不是偶然的，因为CLIP对单词是完全适应的。这样做是为了利用语言的描述性，并提供上下文以解决可能的歧义。以“boxer”这个词为例，它可能是某种犬类或某种运动员。CLIP的作者展示了文本提示的格式非常重要，它可以提升性能并提高效率。
- en: “Prompt engineering and ensembling improve zero-shot performance” — CLIP Authors
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “提示工程和集成提升了零样本性能” — CLIP作者
- en: '![](../Images/65a283f3d219f0792194ca18b19fe04f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65a283f3d219f0792194ca18b19fe04f.png)'
- en: Fig. 4— Prompt engineering and ensembling vs. contextless class names. [Image
    Source](https://arxiv.org/abs/2103.00020) + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图4— 提示工程和集成与无上下文类别名称。 [图片来源](https://arxiv.org/abs/2103.00020) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    注释
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----7770858b487d--------------------------------)
    [## Get an email whenever Sascha Kirch publishes 🚀'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----7770858b487d--------------------------------)
    [## 订阅Sascha Kirch的更新 🚀'
- en: Get an email whenever Sascha Kirch publishes 🚀 Looking to learn more about deep
    learning or simply stay up to date…
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 订阅Sascha Kirch的更新 🚀 想了解更多深度学习内容或保持最新动态……
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----7770858b487d--------------------------------)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----7770858b487d--------------------------------)
- en: '**Zero-Shot Performance**'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**零样本性能**'
- en: In another experiment, the authors compared the zero-shot image classification
    performance of CLIP against a model that was trained specifically on the dataset
    under comparison.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个实验中，作者将CLIP的零样本图像分类性能与专门在比较数据集上训练的模型进行了比较。
- en: “Zero-shot CLIP is competitive with fully super-vised baseline” — CLIP Authors
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “零样本CLIP与完全监督基线具有竞争力” — CLIP作者
- en: '![](../Images/d363b6bca65a2bcc47ea077a5930f453.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d363b6bca65a2bcc47ea077a5930f453.png)'
- en: Fig. 5— Zero-Shot CLIP vs. Supervised baseline. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by author
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图5— 零样本CLIP vs. 监督基线。 [图片来源](https://arxiv.org/abs/2103.00020) + 作者注释
- en: '**Few-Shot Performance**'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**少样本性能**'
- en: While zero-shot predictors are not fine-tuned on the downstream task, few shot
    detectors are. The authors experimented with multiple publicly available pre-trained
    models and compared their few-shot performance on 20 different datasets against
    zero-shot and few-shot CLIP. The few-shot models have been fine-tuned on 1, 2,
    4, 8 and 16 examples per class.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管零样本预测器没有在下游任务上进行微调，但少样本检测器则进行了微调。作者对多个公开的预训练模型进行了实验，并将它们在20个不同数据集上的少样本性能与零样本和少样本CLIP进行了比较。少样本模型在每个类别的1、2、4、8和16个示例上进行了微调。
- en: Interestingly, zero-shot CLIP performs roughly as good as 4-shot CLIP.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，零样本CLIP的表现大致与4样本CLIP相当。
- en: If comparing CLIP to other models, one must consider that the publicly available
    models under comparison (i.e. [BiT](https://arxiv.org/abs/1912.11370), [SimCLR](https://arxiv.org/abs/2006.10029)
    and ResNet) have been pre-trained on different and smaller datasets as the CLIP
    model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将CLIP与其他模型进行比较，必须考虑到所比较的公开模型（即 [BiT](https://arxiv.org/abs/1912.11370)、[SimCLR](https://arxiv.org/abs/2006.10029)
    和 ResNet）是在不同且较小的数据集上进行预训练的，而不是CLIP模型。
- en: “Zero-shot CLIP outperforms few-shot linear probes” — CLIP Authors
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “零样本CLIP优于少样本线性探针” — CLIP作者
- en: '![](../Images/e65731d84d6a2d8069f1b9a9bb13bea0.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e65731d84d6a2d8069f1b9a9bb13bea0.png)'
- en: Fig. 6— Few-shot performance. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图6— 少样本性能。 [图片来源](https://arxiv.org/abs/2103.00020) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    注释
- en: '**Distribution Shift**'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**分布转移**'
- en: Generally speaking, a model’s robustness towards distribution shifts refers
    to its capability to perform as good on data of a different data distribution
    as on the data distribution of the data it was trained on. Ideally, it would perform
    equally well. In reality, its performance drops.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，模型对分布变化的鲁棒性指的是它在不同数据分布的数据上表现得与在训练数据分布上的表现一样好。理想情况下，它的表现应当一样好。但在现实中，其性能会下降。
- en: The robustness of zero-shot CLIP has been compared to a ResNet101 ImageNet model.
    Both models are evaluated on natural distribution shifts of ImageNet, as depicted
    in Fig. 7.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot CLIP的鲁棒性已与ResNet101 ImageNet模型进行了比较。这两种模型在ImageNet的自然分布变化上进行了评估，如图7所示。
- en: “Zero-shot CLIP is much more robust to distribution shift than standard ImageNet
    models” — CLIP Authors
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “零-shot CLIP对分布变化的鲁棒性远超标准ImageNet模型” — CLIP作者
- en: '![](../Images/65795f889f88a6bd09c103faa794ad03.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65795f889f88a6bd09c103faa794ad03.png)'
- en: Fig. 7 — Distribution shift. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图7 — 分布变化。 [图像来源](https://arxiv.org/abs/2103.00020) + [Sascha Kirch](https://medium.com/@SaschaKirch)的注释
- en: Further Readings & Resources
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读与资源
- en: As mentioned at the beginning of this article, CLIP has been widely adopted
    by a vast number of projects.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本文开头所提到的，CLIP已被广泛应用于大量项目中。
- en: 'Following a list of papers using CLIP:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用CLIP的论文列表：
- en: '[[UnCLIP] Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125)'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[UnCLIP] 使用CLIP潜在变量进行分层文本条件图像生成](https://arxiv.org/abs/2204.06125)'
- en: '[[EVA] Exploring the Limits of Masked Visual Representation Learning at Scale](https://arxiv.org/abs/2211.07636)'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[EVA] 探索大规模遮蔽视觉表示学习的极限](https://arxiv.org/abs/2211.07636)'
- en: '[[SAM] Segment Anything](https://arxiv.org/abs/2304.02643)'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[SAM] 分割任意内容](https://arxiv.org/abs/2304.02643)'
- en: '[[Stable Diffusion] High-Resolution Image Synthesis with Latent Diffusion Models](http://arxiv.org/abs/2112.10752)'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[稳定扩散] 使用潜在扩散模型进行高分辨率图像合成](http://arxiv.org/abs/2112.10752)'
- en: '[[GLIDE](https://arxiv.org/abs/2112.10741)] Towards Photorealistic Image Generation
    and Editing with Text-Guided Diffusion Models'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[GLIDE](https://arxiv.org/abs/2112.10741)] 朝着文本引导的扩散模型进行逼真的图像生成和编辑'
- en: '[[VQGAN-CLIP] Open Domain Image Generation and Editing with Natural Language
    Guidance](https://arxiv.org/abs/2204.08583)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[VQGAN-CLIP] 使用自然语言指导的开放领域图像生成和编辑](https://arxiv.org/abs/2204.08583)'
- en: 'And a list of repositories if you want to dive into the implementation and
    test it yourself:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解实现并自行测试，以下是一些库列表：
- en: '[Official Repo by OpenAI](https://github.com/openai/CLIP)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI官方库](https://github.com/openai/CLIP)'
- en: '[Python Notebook to play around with CLIP](https://github.com/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python笔记本以便操作CLIP](https://github.com/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb)'
- en: '[OpenCLIP: Open Source Implementation of CLIP](https://github.com/mlfoundations/open_clip)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenCLIP：CLIP的开源实现](https://github.com/mlfoundations/open_clip)'
