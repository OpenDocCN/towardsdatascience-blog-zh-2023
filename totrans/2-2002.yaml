- en: The CLIP Foundation Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/the-clip-foundation-model-7770858b487d](https://towardsdatascience.com/the-clip-foundation-model-7770858b487d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[üöÄSascha‚Äôs Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learning Transferable Visual Models From Natural Language Supervision by A.
    Radford et. al.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)
    ¬∑8 min read¬∑Aug 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In this article we are going through the paper behind CLIP (**C**ontrastive
    **L**anguage-**I**mage **P**re-Training). We will extract key concepts and break
    them down to make them easy to understand. Further, images and data graphs are
    annotated to clarify doubts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60cfe45b459db912d2688dab7c9b3d31.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created from [publication](https://arxiv.org/abs/2103.00020) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: '**Paper:** [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020),
    Alec Radford et. al., 26 Feb. 2021'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/OpenAI/CLIP) ‚Äî [Blog Post](https://openai.com/research/clip)
    ‚Äî [Hugging Face](https://huggingface.co/docs/transformers/model_doc/clip)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** multi-modal deep learning, computer vision, natural language
    processing, foundation models, representation learning'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    ‚Äî [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    ‚Äî [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    ‚Äî [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    ‚Äî [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    ‚Äî [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Outline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Context & Background
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Readings & Resources
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Context & Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CLIP (**C**ontrastive **L**anguage-**I**mage **P**re-Training) is a multi-modal
    model that learns the correspondence between natural language and images. It is
    trained on 400 million text-images pairs collected from the internet. As we will
    discover later in this article, CLIP has strong zero-shot performance, meaning
    it performs well on downstream tasks different to those it was trained on, without
    performing any fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'CLIP aims to:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply the success of large-scale pre-training techniques known from natural
    language processing (e.g. GPT family, [T5](https://arxiv.org/pdf/1910.10683.pdf)
    and [BERT](https://arxiv.org/abs/1810.04805)) to computer vision.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable flexible zero-shot capabilities by using natural language instead of
    a fixed set class labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is this a big deal you might ask yourself? First of all, many computer vision
    models are trained on crowd-sourced labeled datasets. These datasets often contain
    hundreds of thousands samples. Some exceptions are in the region of single or
    double digit million samples. As you can imagine it is a very time consuming and
    costly process. Datasets for natural language models on the other hand are usually
    several orders of magnitudes larger and are scraped from the internet. Secondly,
    if an object detection model has been trained on certain classes and you want
    to add an extra class, you would need to label this new class in your data and
    retrain the model.
  prefs: []
  type: TYPE_NORMAL
- en: CLIP‚Äôs ability to combine natural language and image features in combination
    with its zero-shot performance has led to a wide adoption in many other popular
    foundation models such as [UnCLIP](https://arxiv.org/abs/2204.06125), [EVA](https://arxiv.org/abs/2211.07636),
    [SAM](https://arxiv.org/abs/2304.02643), [Stable Diffusion](https://arxiv.org/abs/2204.06125),
    [GLIDE](https://arxiv.org/abs/2112.10741) or [VQGAN-CLIP,](https://arxiv.org/abs/2204.08583)
    to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let‚Äôs dive into the method of CLIP. The image bellow depicted in Fig.1 shows
    the architecture of CLIP and the process of how it is trained
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e39e17e5f547917a216759063155235.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 1 ‚Äî CLIP‚Äôs Architecture and training process. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: The model architecture consists of two encoder models, one for each modality.
    For the text encoder a transformer was used while the image encoder uses either
    a version of ResNet or [ViT (Vision Transformer)](https://arxiv.org/abs/2010.11929).
    A learned linear transformation, one for each modality, transforms the features
    into embeddings of matching size. Finally, the cosine similarity is calculated
    between each of the embeddings of opposing modality and is scaled by a learned
    temperature scalar. During training, the cosine similarity between matching pairs
    is maximized while it is minimized for incorrect pairs, hence the term ‚Äúcontrastive‚Äù
    in the framework‚Äôs name.
  prefs: []
  type: TYPE_NORMAL
- en: There are some subtleties that are crucial for the success, beside the large
    dataset of course. First, the contrastive learning approach strongly depends on
    the batch size N. The more negative samples are provided along the correct ones,
    the stronger the learning signal. CLIP was trained on a batch size of 32,768,
    which is quite large. Second, CLIP does not learn a match of the exact wording,
    but an easier proxy task to only learn the text as a whole, also called bag of
    words (BoW).
  prefs: []
  type: TYPE_NORMAL
- en: '**Fun Fact:** The version of CLIP using a ResNet50x64 as image encoder was
    trained for 18 days on 592 V100 GPUS and while the version with the ViT model
    was trained for 12 days on 256 V100 GPUS. In other words, **over 29 years** and
    **over 8 years** on a single GPU respectively (ignoring the fact a different batch
    size would be used).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Once the model is trained it can be used to perform object classification on
    a images. The question is: how to perform classification using a model that has
    not been trained to classify images nor does input class labels but text prompts?
    Fig 2\. shows how:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75d808b0af753bd63ddcea1a3c929526.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2 ‚Äî CLIP‚Äôs Architecture for image classification. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: A class label can be seen as a text prompt formed by a single word. To tell
    the model, which classes are available for the classification task, a set of N
    classes is input into the model. This is a huge advantage compared to classification
    models trained on a fixed set of labels. We can now either input 3 classes or
    100; it‚Äôs our choice. As we will see later, to improve the performance of CLIP,
    the class label is transformed into a prompt to provide further context to the
    model. Each prompt is then fed to the text encoder and is then transformed into
    an embedding vector.
  prefs: []
  type: TYPE_NORMAL
- en: The input image is fed into the image encoder to obtain the embedding vector.
  prefs: []
  type: TYPE_NORMAL
- en: Then the cosine similarity is calculated for each pair of text and image embeddings.
    A Softmax is applied on the obtained similarity values to form a probability distribution.
    Finally, the value with the highest probability is selected as the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Paper Walkthroughs by Sascha Kirch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----7770858b487d--------------------------------)7
    stories![‚ÄúDDPM‚Ää‚Äî‚ÄäDenoising Diffusion Probabilistic Models ‚Äú paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![‚ÄúDepth Anything‚Äù
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Experiments and Ablations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CLIP paper presents a vast number of experiments and ablations. Here we
    will cover five, from which I think are important to understand the success of
    CLIP. Upfront the take aways (as formulated by the authors of CLIP) and then we
    will dive into the details:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Efficiency:** CLIP is much more efficient at zero-shot transfer
    than our image caption baseline'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Text Input Format:** Prompt engineering and ensembling improve zero-shot
    performance'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Zero-Shot Performance:** Zero-shot CLIP is competitive with fully super-vised
    baseline'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Few-Shot Performance:** Zero-shot CLIP outperforms few-shot linear probes'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Distribution Shift:** Zero-shot CLIP is much more robust to distribution
    shift than standard ImageNet models'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training Efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During training, the image encoder and the text encoder are trained jointly,
    meaning with a single training objective and at the same time. Not only does CLIP
    perform a contrastive learning scheme, but the text prompts are compared as a
    whole against a given image, hence the order of words does not matter. It is simply
    a ‚Äúbag of words‚Äù. The phrase ‚Äúmy name is Sascha‚Äù results in the same embedding
    as ‚ÄúSascha name is my‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting a bag of words instead of the correct words and its position in a
    phrase is a much easier proxy objective. Fig 3\. bellow shows the zero-shot accuracy
    on ImageNet over the number of training samples of the initial transformer model
    trained to predict exact words, the initial transformer model trained to predict
    a bag of words and the CLIP model that performs contrastive learning using bag
    of words.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúCLIP is much more efficient at zero-shot transfer than our image caption baseline‚Äù
    ‚Äî CLIP Authors
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/a8a3d4fc5a28d633b7e25e4922e1738b.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 3 ‚Äî Zero-shot efficiency. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: Text Input Format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen in Fig. 2, to perform object classification, the class label
    has been converted into a text prompt. Of course, this was not by chance, because
    CLIP would be totally fine with a single word. It was done to leverage the descriptiveness
    of language and to provide context to resolve possible ambiguities. Let‚Äôs take
    the word ‚Äúboxer‚Äù for example. It could be a type of dog or a type of athlete.
    The authors of CLIP have shown that the format of the text prompt matters a lot
    and can boost the performance as well increase the efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúPrompt engineering and ensembling improve zero-shot performance‚Äù ‚Äî CLIP Authors
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/65a283f3d219f0792194ca18b19fe04f.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4‚Äî Prompt engineering and ensembling vs. contextless class names. [Image
    Source](https://arxiv.org/abs/2103.00020) + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----7770858b487d--------------------------------)
    [## Get an email whenever Sascha Kirch publishes üöÄ'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Sascha Kirch publishes üöÄ Looking to learn more about deep
    learning or simply stay up to date‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----7770858b487d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Zero-Shot Performance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In another experiment, the authors compared the zero-shot image classification
    performance of CLIP against a model that was trained specifically on the dataset
    under comparison.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúZero-shot CLIP is competitive with fully super-vised baseline‚Äù ‚Äî CLIP Authors
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/d363b6bca65a2bcc47ea077a5930f453.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 5‚Äî Zero-Shot CLIP vs. Supervised baseline. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Few-Shot Performance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While zero-shot predictors are not fine-tuned on the downstream task, few shot
    detectors are. The authors experimented with multiple publicly available pre-trained
    models and compared their few-shot performance on 20 different datasets against
    zero-shot and few-shot CLIP. The few-shot models have been fine-tuned on 1, 2,
    4, 8 and 16 examples per class.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, zero-shot CLIP performs roughly as good as 4-shot CLIP.
  prefs: []
  type: TYPE_NORMAL
- en: If comparing CLIP to other models, one must consider that the publicly available
    models under comparison (i.e. [BiT](https://arxiv.org/abs/1912.11370), [SimCLR](https://arxiv.org/abs/2006.10029)
    and ResNet) have been pre-trained on different and smaller datasets as the CLIP
    model.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúZero-shot CLIP outperforms few-shot linear probes‚Äù ‚Äî CLIP Authors
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e65731d84d6a2d8069f1b9a9bb13bea0.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 6‚Äî Few-shot performance. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: '**Distribution Shift**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generally speaking, a model‚Äôs robustness towards distribution shifts refers
    to its capability to perform as good on data of a different data distribution
    as on the data distribution of the data it was trained on. Ideally, it would perform
    equally well. In reality, its performance drops.
  prefs: []
  type: TYPE_NORMAL
- en: The robustness of zero-shot CLIP has been compared to a ResNet101 ImageNet model.
    Both models are evaluated on natural distribution shifts of ImageNet, as depicted
    in Fig. 7.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúZero-shot CLIP is much more robust to distribution shift than standard ImageNet
    models‚Äù ‚Äî CLIP Authors
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/65795f889f88a6bd09c103faa794ad03.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 7 ‚Äî Distribution shift. [Image Source](https://arxiv.org/abs/2103.00020)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: Further Readings & Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned at the beginning of this article, CLIP has been widely adopted
    by a vast number of projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following a list of papers using CLIP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[[UnCLIP] Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[[EVA] Exploring the Limits of Masked Visual Representation Learning at Scale](https://arxiv.org/abs/2211.07636)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[[SAM] Segment Anything](https://arxiv.org/abs/2304.02643)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[[Stable Diffusion] High-Resolution Image Synthesis with Latent Diffusion Models](http://arxiv.org/abs/2112.10752)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[[GLIDE](https://arxiv.org/abs/2112.10741)] Towards Photorealistic Image Generation
    and Editing with Text-Guided Diffusion Models'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[[VQGAN-CLIP] Open Domain Image Generation and Editing with Natural Language
    Guidance](https://arxiv.org/abs/2204.08583)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'And a list of repositories if you want to dive into the implementation and
    test it yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Official Repo by OpenAI](https://github.com/openai/CLIP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Notebook to play around with CLIP](https://github.com/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenCLIP: Open Source Implementation of CLIP](https://github.com/mlfoundations/open_clip)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
