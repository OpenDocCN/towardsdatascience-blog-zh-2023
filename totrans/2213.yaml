- en: 'Unlock the Power of Audio Data: Advanced Transcription and Diarization with
    Whisper, WhisperX, and PyAnnotate'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281](https://towardsdatascience.com/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Streamline Audio Analysis with State-of-the-Art Speech Recognition and Speaker
    Attribution Technologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisroque?source=post_page-----ed9424307281--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----ed9424307281--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ed9424307281--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ed9424307281--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----ed9424307281--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ed9424307281--------------------------------)
    ·9 min read·Apr 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our fast-paced world, we generate enormous amounts of audio data. Think about
    your favorite podcast or conference calls at work. The data is already rich in
    its raw form; we, as humans, can understand it. Even so, we could go further and,
    for example, convert it into a written format to search for it later.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand the task at hand, we are introducing two concepts. The
    first is transcription, simply converting spoken language into text. A second
    one that we explore in this article is diarization. Diarization helps us give
    additional structure to the unstructured content. In this case, we are interested
    in attributing specific speech segments to different speakers.
  prefs: []
  type: TYPE_NORMAL
- en: With the above context, we address both tasks by using different tools. We use
    Whisper, a general-purpose speech recognition model developed by OpenAI. It was
    training on a diverse dataset of audio samples, and the researchers developed
    it to perform multiple tasks. Secondly, we use PyAnnotate, a library for speaker
    diarization. Finally, we use WhisperX, a research project that helps combine the
    two while solving some limitations of Whisper.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20859fc38b69b3c538c6e1338a5e81ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Speak to the machines ([source](https://unsplash.com/photos/6dDHofabCQ8)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This article belongs to “Large Language Models Chronicles: Navigating the NLP
    Frontier”, a new weekly series of articles that will explore how to leverage the
    power of large models for various NLP tasks. By diving into these cutting-edge
    technologies, we aim to empower developers, researchers, and enthusiasts to harness
    the potential of NLP and unlock new possibilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Articles published so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Summarizing the latest Spotify releases with ChatGPT](https://medium.com/towards-data-science/summarizing-the-latest-spotify-releases-with-chatgpt-553245a6df88)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Master Semantic Search at Scale: Index Millions of Documents with Lightning-Fast
    Inference Times using FAISS and Sentence Transformers](https://medium.com/towards-data-science/master-semantic-search-at-scale-index-millions-of-documents-with-lightning-fast-inference-times-fa395e4efd88)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As always, the code is available on my [Github](https://github.com/luisroque/large_laguage_models).
  prefs: []
  type: TYPE_NORMAL
- en: 'Whisper: A General-Purpose Speech Recognition Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whisper is a general-purpose speech recognition model performing very well in
    various speech-processing tasks. It is reasonably robust at multilingual speech
    recognition, speech translation, spoken language identification, and voice activity
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: At the core of Whisper lies a Transformer sequence-to-sequence model. The model
    jointly represents various speech-processing tasks as a sequence of tokens to
    be predicted by the decoder. The model can replace multiple stages of a traditional
    speech-processing pipeline by employing special tokens as task specifiers or classification
    targets. We can think of it as a meta-model for speech-processing tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Whisper comes in five model sizes, targeting edge devices or large computing
    machines. It allows users to select the appropriate model for their use case and
    the capacity of their systems. Note that the English-only versions of some models
    perform better for English use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyAnnotate: Speaker Diarization Library'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Speaker diarization is the process of identifying and segmenting speech by different
    speakers. The task can be beneficial, for example, when we are analyzing data
    from a call center, and we want to separate the customer and the agent’s voices.
    Companies can then use it to improve customer service and ensure company policy
    compliance.
  prefs: []
  type: TYPE_NORMAL
- en: PyAnnotate is a Python library specifically designed to support this task. The
    process is relatively simple. It preprocesses the data, allowing us to extract
    features from the raw audio file. Next, it produces clusters of similar speech
    segments based on the extracted features. Finally, it attributes the generated
    clusters to individual speakers.
  prefs: []
  type: TYPE_NORMAL
- en: 'WhisperX: Long-Form Audio Transcription with Voice Activity Detection and Forced
    Phoneme Alignment'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the previous sections, Whisper is a large-scale and weakly supervised
    model trained to perform several tasks in the speech-processing field. While it
    performs well in different domains and even in different languages it falls short
    when it comes to long audio transcription. The limitation comes from the fact
    that the training procedure uses a sliding window approach, which can result in
    drifting or even hallucination. In addition, it has severe limitations when it
    comes to aligning the transcription with the audio timestamps. This is particularly
    important to us when performing the speaker diarization.
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle these limitations, an Oxford research group is actively developing
    WhisperX. The Arxiv pre-print paper was published last month. It uses Voice Activity
    Detection (VAD), which detects the presence or absence of human speech and pre-segments
    the input audio file. It then cuts and merges these segments into windows of approximately
    30 seconds by defining the boundaries on the regions where there is a low probability
    of speech (yield from the voice model). This step has an additional advantage:
    it allows using batch transcriptions with Whisper. It increases performance while
    reducing the probability of drifting or hallucination we discussed above. The
    final step is called forced alignment. WhisperX uses a phoneme model to align
    the transcription with the audio. Phoneme-based Automatic Speech Recognition (ASR)
    recognizes the smallest unit of speech, e.g., the element “g” in “big.” This post-processing
    operation aligns the generated transcription with the audio timestamps at the
    word level.'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating WhisperX, Whisper, and PyAnnotate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we integrate WhisperX, Whisper, and PyAnnotate to create our
    own ASR system. We designed our approach to handle long-form audio transcriptions
    while being able to segment the speech and attribute a specific speaker to each
    segment. In addition, it reduces the probability of hallucination, increases inference
    efficiency, and ensures proper alignment between the transcription and the audio.
    Let’s build a pipeline to perform the different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We start with transcription, converting the speech recognized from the audio
    file into written text. The `transcribe`function loads a Whisper model specified
    by `model_name` and transcribes the audio file. It then returns a dictionary containing
    the transcript segments and language code. OpenAI designed Whisper also to perform
    language detection, being a multilanguage model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we align the transcript segments using the `align_segments` function.
    As we discussed previously, this step is essencial for accurate speaker diarization,
    as it ensures that each segment corresponds to the correct speaker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With the transcript segments aligned, we can now perform speaker diarization.
    We use the `diarize` function, which leverages the PyAnnotate library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After diarization, we assign speakers to each transcript segment using the
    `assign_speakers` function. It is the final step in our pipeline and completes
    our process of transforming the raw audio file into a transcript with speaker
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we combine all the steps into a single `transcribe_and_diarize` function.
    This function returns a list of dictionaries representing each transcript segment,
    including the start and end times, spoken text, and speaker identifier. Note that
    you need a Hugging Face API token to run the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Assessing the Performance of the Integrated ASR System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s start by testing our pipeline with a short audio clip that I recorded
    myself. There are two speakers in the video that we need to identify. Also, notice
    the several hesitations in the speech of one of the speakers, making it hard to
    transcribe. We will use the *base* model from Whisper to assess its capabilities.
    For better accuracy, you can use the *medium* or *large* ones. The transcription
    is presented below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Segment 1:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Start time: 0.95'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'End time: 2.44'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Speaker: SPEAKER_01'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Transcript: What TV show are you watching?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Segment 2:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Start time: 3.56'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'End time: 5.40'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Speaker: SPEAKER_00'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Transcript: Currently I’m watching This Is Us.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Segment 3:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Start time: 6.18'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'End time: 6.93'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Speaker: SPEAKER_01'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Transcript: What is it about?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Segment 4:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Start time: 8.30'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'End time: 15.44'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Speaker: SPEAKER_00'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Transcript: It is about the life of a family throughout several generations.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Segment 5:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Start time: 15.88'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'End time: 21.42'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Speaker: SPEAKER_00'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Transcript: And you can somehow live their lives through the series.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Segment 6:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Start time: 22.34'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'End time: 23.55'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Speaker: SPEAKER_01'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Transcript: What will be the next one?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Segment 7:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Start time: 25.48'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'End time: 28.81'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Speaker: SPEAKER_00'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Transcript: Maybe beef I’ve been hearing very good things about it.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Execution time for base: 8.57 seconds'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Memory usage for base: 3.67GB'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our approach achieves its main goals with the transcription above. First, notice
    that the transcription is accurate and that we could ignore the speech hesitations
    successfully. We produced text with the correct syntax, which helps readability.
    The segments were well separated and correctly aligned with the audio timestamps.
    Finally, the speaker diarization was also executed adequately, with the two speakers
    attributed accurately to each speech segment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important aspect is the computation efficiency of the various models
    on long-format audio when running inference on CPU and GPU. We selected an audio
    file of around 30 minutes. Below, you can find the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d22c0e5b35642a811e90fae90ad26fc3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Execution time for the various models using CPU and GPU (Image by
    Author).'
  prefs: []
  type: TYPE_NORMAL
- en: The main takeaway is that these models are very heavy and need to be more efficient
    to run at scale. For 30 minutes of video, we take around 70–75 minutes to run
    the transcriptions on the CPU and approximately 15 minutes on GPU. Also, remember
    that we need about 10GB of VRAM to run the large model. We should expect these
    results since the models are still in the research phase.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article provides a comprehensive step-by-step guide to analyzing audio
    data using state-of-the-art speech recognition and speaker diarization technologies.
    We introduced Whisper, PyAnnotate, and WhisperX, forming a powerful integrated
    ASR system together — our approach produces promising results when working with
    long-form audio transcriptions. It also solves the main limitations of Whisper,
    hallucinating on long-form audio transcriptions, ensuring alignment between transcription
    and audio, accurately segmenting the speech, and attributing speakers to each
    segment.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, the computational efficiency of these models remains a challenge,
    particularly for long-format audio and when running inference on limited hardware.
    Even so, the integration of Whisper, WhisperX, and PyAnnotate demonstrates the
    potential of these tools to transform the way we process and analyze audio data,
    unlocking new possibilities for applications across various industries and use
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: About me
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serial entrepreneur and leader in the AI space. I develop AI products for businesses
    and invest in AI-focused startups.
  prefs: []
  type: TYPE_NORMAL
- en: '[Founder @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
  prefs: []
  type: TYPE_NORMAL
