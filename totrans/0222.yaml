- en: A Quick Guide on Normalization for Your NLP Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-quick-guide-on-normalization-for-your-nlp-model-2dbd7d2d42a7](https://towardsdatascience.com/a-quick-guide-on-normalization-for-your-nlp-model-2dbd7d2d42a7)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Accelerate your model convergence and stabilize the training process with normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vuphuongthao9611?source=post_page-----2dbd7d2d42a7--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page-----2dbd7d2d42a7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2dbd7d2d42a7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2dbd7d2d42a7--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page-----2dbd7d2d42a7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2dbd7d2d42a7--------------------------------)
    ·7 min read·Sep 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/664add61750b752d6992bc44d3645704.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mattia Bericchia](https://unsplash.com/@mattiabericchia?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Efficiently training deep learning models is challenging. The problem becomes
    more difficult with the recent growth of NLP models’ size and architecture complexity.
    To handle billions of parameters, more optimizations are proposed for faster convergence
    and stable training. One of the most remarkable techniques is normalization.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will learn about some normalization techniques, how they
    work, and how they can be used for NLP deep models.
  prefs: []
  type: TYPE_NORMAL
- en: Why not BatchNorm?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BatchNorm [2] is an early normalization technique proposed to solve internal
    covariate shifts.
  prefs: []
  type: TYPE_NORMAL
- en: To explain in simple terms, an internal covariate shift occurs when there is
    a change in the layer’s input data distribution. When the neural networks are
    forced to fit different data distributions, the gradient update changes dramatically
    between batches. Therefore, the models take longer to adjust, learn the correct
    weights and converge. The problem gets worse as the model size grows.
  prefs: []
  type: TYPE_NORMAL
- en: Initial solutions include using a small learning rate (so the impact of data
    distribution shifting is minor) and careful weight initialization. BatchNorm solved
    the problem effectively by normalizing the input on the feature dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07a089aad68ef457d36037c70e0e8a1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Batch Norm (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The technique helps speed up the convergence significantly and allows a higher
    learning rate as the model becomes less sensitive to outliers. However, it still
    has some drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Small batch size:** BatchNorm relies on batch data to compute the feature’s
    mean and standard deviation. When the batch size is small, the mean and variance
    can no longer represent the population. Therefore, online learning is impossible
    with BatchNorm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequence input:** In BatchNorm, each input sample’s normalization depends
    on other samples from the same batch. This does not work so well with sequence
    data. For example, we have 2 training samples with different lengths ***(a1, a2,..,
    a10)*** and ***(b1,b2,…,b20)***. Is it okay if token ***b11*** is normalized along
    with a padding token ***a11***? At the inference step, if we have a sequence of
    length 30 ***(c1, c2,.., c30)***, how do we get the mean and variance to normalize
    token ***c21***? This is the crucial reason BatchNorm is not suitable for NLP
    tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelization**: It''s difficult to parallelize batch-normalized models.
    Since there is dependence between elements (mean and variance), we need to synchronize
    them between devices. NLP models, such as Transformers, suffer from this setting
    due to their large-scale setup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is where LayerNorm [1] came in. Proposed in 2016, LayerNorm has dethroned
    BatchNorm and steadily become the most popular normalization technique among the
    research community.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2faf64e6af8978334d16ffd7b80d04a.png)'
  prefs: []
  type: TYPE_IMG
- en: Normalization Technique Usage ([Source](https://paperswithcode.com/method/layer-normalization))
  prefs: []
  type: TYPE_NORMAL
- en: So what is Layer Norm, and why it’s so effective?
  prefs: []
  type: TYPE_NORMAL
- en: Why LayerNorm works so well?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In contrast to BatchNorm, LayerNorm normalizes the inputs on each data sample
    dimension. So, in a training batch of n samples ***(x1, x2,.., xn)***, the normalization
    is done on each ***xi*** independently.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da3e48d88a2cf730fea695cb0f856f00.png)'
  prefs: []
  type: TYPE_IMG
- en: Layer Norm (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: LayerNorm has been widely used in many state-of-the-art language models, such
    as BERT [5] and BLOOM [6]. Why does it work so well?
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we need to mention a few advantages of LayerNorm compared to BatchNorm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequence data:** The technique introduces no dependency between training
    examples. Therefore, we can safely normalize sequence inputs without worrying
    about the different characteristics between training samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible batch size:** Since the normalization is done on each sample, batch
    size is no longer a problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training and Testing:** Unlike BatchNorm, LayerNorm does not need to keep
    the moving mean or variance of the population. Therefore, it performs the exact
    computation at training and inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelization*:*** There is no dependency between training samples, so
    we can train the model on different devices without the need for synchronization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Secondly, we will discuss one of the critical reasons normalization techniques
    help us so much with training stabilization: their invariance under weights and
    input transformation. Invariance means the normalization technique result does
    not get affected by input transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: Common known transformations are re-scaling and re-centering. Re-centering invariance
    makes the model insensitive to random noise in both weights and data. Meanwhile,
    re-scaling invariance keeps the output resilient to arbitrary scaling of inputs
    and weights.
  prefs: []
  type: TYPE_NORMAL
- en: It is much easier to understand the invariance characteristic by building a
    layer normalization function and trying it on different transformations to see
    how the mean, variance and result change.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: I got the following result after re-scaling and re-centering the matrix weight
    (w) and dataset (x).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/deec85212d0042c359fc372672c7324c.png)'
  prefs: []
  type: TYPE_IMG
- en: Layer Normalization results with different transformations (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the normalized result remains the same with weight re-scaling
    and re-center. For dataset transformation, the technique is invariant to re-scaling
    but not re-centering.
  prefs: []
  type: TYPE_NORMAL
- en: It’s pretty amazing how LayerNorm can keep the output resilient to such transformation.
    How can it do that? Let’s delve into the mathematical details to grasp better
    what is happening under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Things get a bit intimidating here, so feel free to skip this section. However,
    I bet this will give you a strong intuition of how normalization works.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical proof of Layer Norm invariance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a neural network with weight ***W***, input ***x***, bias ***b*** and
    activation function ***f***. The neural network output is ***y = f(Wx + b)***.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then LayerNorm can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cafd2765079df98ddcb13375251eddfa.png)'
  prefs: []
  type: TYPE_IMG
- en: The mathematical proof of LayerNorm variance can be shown as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Weight matrix re-scaling invariance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/053d0d711d617510f3d464b1de7d7fd1.png)'
  prefs: []
  type: TYPE_IMG
- en: Weight matrix re-scaling invariance (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Weight matrix re-centering invariance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/00e0fbc3290c2f8133a3992e637422e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Weight matrix re-centering invariance (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Dataset re-scaling invariance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/7169c039922d4cab5ff0f6afb0fa6bef.png)'
  prefs: []
  type: TYPE_IMG
- en: Dataset rep-scaling invariance (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: So that’s the proof of LayerNorm invariance to transformations. However, not
    all invariance is necessary. RMSNorm [3] is a younger sibling of LayerNorm with
    only re-scaling invariance characteristics. However, it has become a favoured
    choice for recent LLM architectures like Llama [4].
  prefs: []
  type: TYPE_NORMAL
- en: What makes RMSNorm more superior?
  prefs: []
  type: TYPE_NORMAL
- en: What is RMSNorm?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RMSNorm was published in 2019, with RMS stands for “root mean square”. Despite
    LayerNorm accelerating convergence, the authors pointed out that it consumes more
    time for each training step.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56a9cc92261dc4a7cc1e7459850c6826.png)'
  prefs: []
  type: TYPE_IMG
- en: Training procedure of a GRU-based RNNSearch [3]
  prefs: []
  type: TYPE_NORMAL
- en: The author also argued that the mean normalization of LayerNorm has an insignificant
    impact on the final performance and, hence, can be removed for computation efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Given that hypothesis, RMSNorm is proposed to focus on the re-scaling invariance
    and using root mean square regularization as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a7e814d0daf545bf29ef6dee6bd14d0.png)'
  prefs: []
  type: TYPE_IMG
- en: RMSNorm (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: As we skip the mean computation, the normalization becomes simpler. With such
    elegant optimization, the author observed speedups of 7%-64% across different
    implementations without performance degradation!
  prefs: []
  type: TYPE_NORMAL
- en: One of the experiments is the WMT14 English-German translation task with a GRU-based
    RNNSearch using BLEU score metric. LayerNorm and RMSNorm both achieved similar
    scores on the test. But RMSNorm was much faster with 25% less training time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/342b4b5a636d4a6a696a1580552c2bb8.png)'
  prefs: []
  type: TYPE_IMG
- en: BLEU score on test14 and test17 [3]
  prefs: []
  type: TYPE_NORMAL
- en: A closer look at the validation score during training also suggests RMSNorm
    performance is comparable to LayerNorm during all training stages. This supports
    the initial argument that re-centering invariance is insignificant and RMSNorm
    is more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/933cfe0838f456dd2071ad863223973a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*SacreBLEU score on newstest2013 for the RNNSearch [3]*'
  prefs: []
  type: TYPE_NORMAL
- en: Another intriguing metric is hidden vectors’ mean and standard deviation at
    different token positions. While the baseline’s mean and variance vary greatly,
    using LayerNorm and RMSNorm helps remarkably stabilize the distribution output.
    This is a strong proof of how vital normalization is to NLP models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2844353cab639fc74bc7cb108ce7da46.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Mean (M) and standard deviation (S) statistics estimated for tokens at specific
    positions [3]*'
  prefs: []
  type: TYPE_NORMAL
- en: I hope you find this article helpful in understanding each normalization technique’s
    pros and cons for the NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: See you in the next post!
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization.
    arXiv preprint arXiv:1607.06450, 2016.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep
    network training by reducing internal covariate shift. ICML, 2015.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Zhang, Biao, and Rico Sennrich. “Root mean square layer normalization.”
    *Advances in Neural Information Processing Systems* 32 (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.”
    *arXiv preprint arXiv:2302.13971* (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Scao, Teven Le, et al. “Bloom: A 176b-parameter open-access multilingual
    language model.” *arXiv preprint arXiv:2211.05100* (2022).'
  prefs: []
  type: TYPE_NORMAL
