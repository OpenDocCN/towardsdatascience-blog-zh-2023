["```py\n[([1,1], -1), ([2,1], -1), ([2,2], -1), ([1,2], -1), ([1,3], -1)]\n\nThe moves on this trajectory are:\n\n[0,1]->[1,1] - reward = -1\n[1,1]->[2,1] - reward = -1\n[2,1]->[2,2] - reward = -1\n[2,2]->[1,2] - reward = -1\n[1,2]-> Exit - reward = -1\n```", "```py\n[0,1] - return = -5\n[1,1] - return = -4\n[2,1] - return = -3\n[2,2] - return = -2\n[1,2] - return = -1\n```", "```py\n1\\. [0,1]->[1,1] - reward = -1\n2\\. [1,1]->[2,1] - reward = -1\n3\\. [2,1]->[2,2] - reward = -1\n4\\. [2,2]->[2,1] - reward = -1\n5\\. [2,1]->[2,2] - reward = -1\n6\\. [2,2]->[1,2] - reward = -1\n7\\. [1,2]-> Exit - reward = -1\n```", "```py\n1\\. [0,1] - return = -7\n2\\. [1,1] - return = -6\n3\\. [2,1] - return = -5\n4\\. [2,2] - return = -4\n5\\. [2,1]\n6\\. [2,2]\n7\\. [1,2] - return = -1\n```", "```py\n''' calculate state values using First-Visit Monte Carlo '''\n\n# keep a count of the visits to each state                            \ntotal_state_visits = np.zeros((env.height,env.width))                    ❶\n\n# the total returns for each state\ntotal_state_returns = np.zeros((env.height,env.width))\n\nfor episode in range(max_episodes):                                      ❷\n\n  state_rewards = single_episode(env)                                    ❸\n  state_returns = rewards_to_returns(state_rewards)                      ❹\n  episode_returns,episode_visits = get_first_visit_return(state_returns) ❺\n\n  # add the states that were visited to the total visits\n  total_state_visits += episode_visits\n\n  # add the return for each state visited to the total for each state\n  total_state_returns += episode_returns\n\nreturns = (total_state_returns/total_state_visits)                       ❻\n```", "```py\n''' calculate state-action values using First-Visit Monte Carlo '''\n\n# keep a count of the visits to each action\nvisits = np.zeros((env.height,env.width,len(Actions)))                    ❶\n\n# the average returns for each action\nreturns = np.zeros((env.height,env.width,len(Actions)))\n\nfor episode in tqdm(range(max_episodes)):                                 ❷\n\n  action_rewards = single_episode(env,policy)                             ❸\n  action_returns = rewards_to_returns(action_rewards)                     ❹\n  episode_returns,episode_visits = get_first_visit_return(action_returns) ❺\n\n  # add the episode returns to the total returns\n  returns += episode_returns\n\n  # increment the count of any states that have been visited\n  visits += episode_visits\n\navg_returns = (returns/visits)                                            ❻\n```", "```py\n# probability of selecting a random action\np = np.random.random()\n\n# if the probability is less than epsilon then a random action \n# is chosen from the state's available actions\nif p < epsilon:\n    action = env.action_space.sample()\nelse:                \n    # get the policy's action in the current state\n    action = policy.get_action(env.x,env.y) \n```", "```py\n[**< Part 3: Policy and Value Iteration**](/policy-and-value-iteration-78501afb41d2)\n```"]