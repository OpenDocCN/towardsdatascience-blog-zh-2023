- en: Convergence in Probability or Distribution
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¦‚ç‡æ”¶æ•›æˆ–åˆ†å¸ƒæ”¶æ•›
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/convergence-in-probability-or-distribution-1766e08125cd](https://towardsdatascience.com/convergence-in-probability-or-distribution-1766e08125cd)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/convergence-in-probability-or-distribution-1766e08125cd](https://towardsdatascience.com/convergence-in-probability-or-distribution-1766e08125cd)
- en: What is the difference between the two?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿™ä¸¤è€…ä¹‹é—´æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
- en: '[](https://r-shuo-wang.medium.com/?source=post_page-----1766e08125cd--------------------------------)[![Shuo
    Wang](../Images/17a7299c0a36d9a4c0d07ebfc9d5c282.png)](https://r-shuo-wang.medium.com/?source=post_page-----1766e08125cd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1766e08125cd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1766e08125cd--------------------------------)
    [Shuo Wang](https://r-shuo-wang.medium.com/?source=post_page-----1766e08125cd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://r-shuo-wang.medium.com/?source=post_page-----1766e08125cd--------------------------------)[![Shuo
    Wang](../Images/17a7299c0a36d9a4c0d07ebfc9d5c282.png)](https://r-shuo-wang.medium.com/?source=post_page-----1766e08125cd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1766e08125cd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1766e08125cd--------------------------------)
    [Shuo Wang](https://r-shuo-wang.medium.com/?source=post_page-----1766e08125cd--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1766e08125cd--------------------------------)
    Â·6 min readÂ·Sep 4, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1766e08125cd--------------------------------)
    Â·6 min readÂ·2023å¹´9æœˆ4æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/38ca6ca8cbc2275769040cd2e887056b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38ca6ca8cbc2275769040cd2e887056b.png)'
- en: Image by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: During your study of statistics, have you encountered the concepts of convergence
    in probability and convergence in distribution? Have you ever pondered why these
    concepts were introduced in the first place? If you have, then this story aims
    to help you answer some of those questions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½ å­¦ä¹ ç»Ÿè®¡å­¦çš„è¿‡ç¨‹ä¸­ï¼Œä½ æ˜¯å¦é‡åˆ°è¿‡æ¦‚ç‡æ”¶æ•›å’Œåˆ†å¸ƒæ”¶æ•›çš„æ¦‚å¿µï¼Ÿä½ æ˜¯å¦æ›¾æ€è€ƒè¿‡è¿™äº›æ¦‚å¿µæœ€åˆæ˜¯ä¸ºä½•è¢«å¼•å…¥çš„ï¼Ÿå¦‚æœæœ‰çš„è¯ï¼Œè¿™ä¸ªæ•…äº‹æ—¨åœ¨å¸®åŠ©ä½ å›ç­”ä¸€äº›è¿™äº›é—®é¢˜ã€‚
- en: '**Convergence in Probability**'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ¦‚ç‡æ”¶æ•›**'
- en: 'Letâ€™s begin by delving into the concept of convergence in probability, as it
    is the more straightforward concept to grasp. Imagine we have a sequence of random
    variables: *X1*, *X2*, â€¦, *Xn*, and as we let n approach infinity, if the probability
    that *Xn* is very close to x approaches 1, we can conclude that *Xn* converges
    to x in probability.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆ**æ·±å…¥**äº†è§£æ¦‚ç‡æ”¶æ•›çš„æ¦‚å¿µï¼Œå› ä¸ºå®ƒæ˜¯æ›´å®¹æ˜“ç†è§£çš„æ¦‚å¿µã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªéšæœºå˜é‡åºåˆ—ï¼š*X1*ã€*X2*ã€â€¦ã€*Xn*ï¼Œå½“ n è¶‹è¿‘äºæ— ç©·å¤§æ—¶ï¼Œå¦‚æœ
    *Xn* å¾ˆæ¥è¿‘ x çš„æ¦‚ç‡è¶‹è¿‘äº 1ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œ*Xn* åœ¨æ¦‚ç‡ä¸Šæ”¶æ•›äº xã€‚
- en: '![](../Images/25a2ce3fc83c323a08fb4a0a483aa329.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25a2ce3fc83c323a08fb4a0a483aa329.png)'
- en: Why is it defined in this manner? The rationale behind this definition stems
    from the fact that, regardless of how large n becomes, Xn will never precisely
    equal *x* (the constant). The most we can ascertain is to specify how close *Xn*
    must be to *x* in terms of the probability that *Xn* falls within a certain interval
    around *x*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¿™æ ·å®šä¹‰ï¼Ÿè¿™ç§å®šä¹‰çš„åˆç†æ€§æºäºè¿™æ ·ä¸€ä¸ªäº‹å®ï¼šæ— è®º n å¤šå¤§ï¼Œ*Xn* æ°¸è¿œä¸ä¼šç²¾ç¡®ç­‰äº *x*ï¼ˆå¸¸é‡ï¼‰ã€‚æˆ‘ä»¬èƒ½åšçš„æœ€å¤šæ˜¯ç¡®å®š *Xn* å¿…é¡»åœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ¥è¿‘
    *x*ï¼Œå³ *Xn* è½åœ¨ *x* å‘¨å›´æŸä¸ªåŒºé—´çš„æ¦‚ç‡ã€‚
- en: Hence, our definition asserts that as n approaches infinity, the likelihood
    of *Xn* differing from *x* by an amount greater than Îµ diminishes to an infinitesimal
    level, ultimately approaching zero. Moreover, Îµ can be arbitrarily small.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬çš„å®šä¹‰å£°ç§°ï¼Œå½“ n è¶‹è¿‘äºæ— ç©·å¤§æ—¶ï¼Œ*Xn* ä¸ *x* ä¹‹é—´çš„å·®å¼‚å¤§äº Îµ çš„å¯èƒ½æ€§ä¼šé™ä½åˆ°ä¸€ä¸ªå¾®å°çš„æ°´å¹³ï¼Œæœ€ç»ˆæ¥è¿‘äºé›¶ã€‚æ­¤å¤–ï¼ŒÎµ å¯ä»¥ä»»æ„å°ã€‚
- en: An illustrative example of convergence in probability would be the concept of
    the sample mean. Consider the scenario where we repeatedly draw n samples from
    a normal distribution with a mean of 0 and a standard deviation of 0.1\. If we
    calculate the sample mean of these n samples, this resulting sample mean becomes
    a random variable denoted as Xn and possesses its own distribution.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¦‚ç‡æ”¶æ•›çš„ä¾‹å­æ˜¯æ ·æœ¬å‡å€¼çš„æ¦‚å¿µã€‚è€ƒè™‘è¿™æ ·ä¸€ä¸ªåœºæ™¯ï¼Œæˆ‘ä»¬ä»å‡å€¼ä¸º 0ã€æ ‡å‡†å·®ä¸º 0.1 çš„æ­£æ€åˆ†å¸ƒä¸­åå¤æŠ½å– n ä¸ªæ ·æœ¬ã€‚å¦‚æœæˆ‘ä»¬è®¡ç®—è¿™äº› n ä¸ªæ ·æœ¬çš„æ ·æœ¬å‡å€¼ï¼Œé‚£ä¹ˆå¾—åˆ°çš„æ ·æœ¬å‡å€¼æˆä¸ºä¸€ä¸ªéšæœºå˜é‡ï¼Œè®°ä½œ
    *Xn*ï¼Œå¹¶ä¸”å…·æœ‰å…¶è‡ªå·±çš„åˆ†å¸ƒã€‚
- en: 'The question then arises: What is the nature of this distribution? When n=1,
    the sample mean is simply equivalent to the individual sample itself, and its
    distribution mirrors the population distribution, specifically the normal distribution
    with a mean of 0 and a standard deviation of 0.1.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼šè¿™ä¸ªåˆ†å¸ƒçš„æ€§è´¨æ˜¯ä»€ä¹ˆï¼Ÿå½“ n=1 æ—¶ï¼Œæ ·æœ¬å‡å€¼å®é™…ä¸Šç­‰åŒäºå•ä¸ªæ ·æœ¬æœ¬èº«ï¼Œå…¶åˆ†å¸ƒåæ˜ äº†æ€»ä½“åˆ†å¸ƒï¼Œç‰¹åˆ«æ˜¯å‡å€¼ä¸º 0ã€æ ‡å‡†å·®ä¸º 0.1 çš„æ­£æ€åˆ†å¸ƒã€‚
- en: But what if *n=1000*? Intuitively, in such cases, we would expect the sample
    mean calculated to be very close to the population mean, which is 0\. It is reasonable
    to assume that when we repeatedly draw 1000 samples and calculate the sample mean,
    the values might cluster around numbers like 0.001, 0.002, -0.001, and so on,
    without significant fluctuations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœ *n=1000* å‘¢ï¼Ÿç›´è§‚ä¸Šï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šæœŸæœ›è®¡ç®—å‡ºçš„æ ·æœ¬å‡å€¼éå¸¸æ¥è¿‘æ€»ä½“å‡å€¼ï¼Œå³ 0ã€‚å¯ä»¥åˆç†å‡è®¾ï¼Œå½“æˆ‘ä»¬åå¤æŠ½å– 1000 ä¸ªæ ·æœ¬å¹¶è®¡ç®—æ ·æœ¬å‡å€¼æ—¶ï¼Œæ•°å€¼å¯èƒ½ä¼šèšé›†åœ¨
    0.001ã€0.002ã€-0.001 ç­‰é™„è¿‘ï¼Œæ²¡æœ‰æ˜¾è‘—æ³¢åŠ¨ã€‚
- en: What if n=1,000,000? In this scenario, itâ€™s highly probable that the sample
    means would be extremely close to 0, with any deviation from this value being
    minuscule.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ n=1,000,000 å‘¢ï¼Ÿåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ ·æœ¬å‡å€¼éå¸¸å¯èƒ½æ¥è¿‘ 0ï¼Œä»»ä½•åç¦»è¿™ä¸ªå€¼çš„æƒ…å†µéƒ½æå…¶å¾®å°ã€‚
- en: This is precisely the essence of convergence in probability. As n increases,
    the distribution of the random variable *Xn* becomes progressively narrower, ultimately
    converging towards a single value.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯æ¦‚ç‡æ”¶æ•›çš„æœ¬è´¨ã€‚éšç€ n çš„å¢åŠ ï¼Œéšæœºå˜é‡ *Xn* çš„åˆ†å¸ƒå˜å¾—è¶Šæ¥è¶Šçª„ï¼Œæœ€ç»ˆæ”¶æ•›åˆ°ä¸€ä¸ªå•ä¸€çš„å€¼ã€‚
- en: '![](../Images/c98a39e7d9b0895e295064f66edcbf4f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c98a39e7d9b0895e295064f66edcbf4f.png)'
- en: The sampling distribution of the sample mean is visualized in a sequence of
    histograms, showcasing samples drawn from a normal distribution for different
    sample sizes, specifically [1, 10, 100, 1000]. Image by author.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ ·æœ¬å‡å€¼çš„æŠ½æ ·åˆ†å¸ƒé€šè¿‡ä¸€ç³»åˆ—ç›´æ–¹å›¾è¿›è¡Œäº†å¯è§†åŒ–ï¼Œå±•ç¤ºäº†ä»æ­£æ€åˆ†å¸ƒä¸­æŠ½å–çš„æ ·æœ¬ï¼Œæ ·æœ¬é‡åˆ†åˆ«ä¸º[1, 10, 100, 1000]ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'This phenomenon occurs not only with samples drawn from a normal distribution
    but also with the binomial distribution. When we draw n samples from a binomial
    distribution with 1 trial and a probability of success of 0.5, we observe a convergence
    pattern very similar to the previous example:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç°è±¡ä¸ä»…å‘ç”Ÿåœ¨ä»æ­£æ€åˆ†å¸ƒä¸­æŠ½å–çš„æ ·æœ¬ä¸­ï¼Œä¹Ÿä¼šåœ¨äºŒé¡¹åˆ†å¸ƒä¸­å‡ºç°ã€‚å½“æˆ‘ä»¬ä»ä¸€ä¸ªè¯•éªŒä¸”æˆåŠŸæ¦‚ç‡ä¸º 0.5 çš„äºŒé¡¹åˆ†å¸ƒä¸­æŠ½å– n ä¸ªæ ·æœ¬æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ”¶æ•›æ¨¡å¼ä¸å‰é¢çš„ä¾‹å­éå¸¸ç›¸ä¼¼ï¼š
- en: '![](../Images/fe260b5d0ec160f3075f62b4e23200fd.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe260b5d0ec160f3075f62b4e23200fd.png)'
- en: The sampling distribution of the sample mean is visualized in a sequence of
    histograms, showcasing samples drawn from a binomial distribution for different
    sample sizes, specifically [1, 10, 100, 1000]. Image by author.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ ·æœ¬å‡å€¼çš„æŠ½æ ·åˆ†å¸ƒé€šè¿‡ä¸€ç³»åˆ—ç›´æ–¹å›¾è¿›è¡Œäº†å¯è§†åŒ–ï¼Œå±•ç¤ºäº†ä»äºŒé¡¹åˆ†å¸ƒä¸­æŠ½å–çš„æ ·æœ¬ï¼Œæ ·æœ¬é‡åˆ†åˆ«ä¸º[1, 10, 100, 1000]ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Regardless of how tightly we attempt to constrain the sample mean within a specific
    interval, we can always identify a sufficiently large n where the probability
    of the sample mean falling within that interval approaches 100%.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºæˆ‘ä»¬å¦‚ä½•åŠªåŠ›å°†æ ·æœ¬å‡å€¼çº¦æŸåœ¨ä¸€ä¸ªç‰¹å®šçš„åŒºé—´å†…ï¼Œæˆ‘ä»¬æ€»èƒ½æ‰¾åˆ°ä¸€ä¸ªè¶³å¤Ÿå¤§çš„ nï¼Œä½¿å¾—æ ·æœ¬å‡å€¼è½å…¥è¯¥åŒºé—´çš„æ¦‚ç‡æ¥è¿‘ 100%ã€‚
- en: Convergence in Distribution
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒæ”¶æ•›
- en: Conversely, itâ€™s important to note that not every sequence of random variables
    converges to a single number in probability. In many cases, a sequence of random
    variables does not converge to a specific number but instead converges to a random
    variable with its own distinct distribution. In such instances, we refer to this
    behavior as convergence in distribution.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¹¶ä¸æ˜¯æ¯ä¸€ä¸ªéšæœºå˜é‡åºåˆ—éƒ½åœ¨æ¦‚ç‡ä¸Šæ”¶æ•›åˆ°ä¸€ä¸ªå•ä¸€çš„æ•°å­—ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œéšæœºå˜é‡åºåˆ—ä¸ä¼šæ”¶æ•›åˆ°ç‰¹å®šçš„æ•°å­—ï¼Œè€Œæ˜¯æ”¶æ•›åˆ°ä¸€ä¸ªå…·æœ‰è‡ªå·±ç‹¬ç‰¹åˆ†å¸ƒçš„éšæœºå˜é‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†è¿™ç§è¡Œä¸ºç§°ä¸ºåˆ†å¸ƒæ”¶æ•›ã€‚
- en: '![](../Images/d5c675de23c01ecad4885ea5694a0169.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5c675de23c01ecad4885ea5694a0169.png)'
- en: CDFn(t) represents the cumulative distribution function of the random variable
    Xn within the given sequence, while CDF(t) signifies the cumulative distribution
    function of a random variable X.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: CDFn(t) è¡¨ç¤ºç»™å®šåºåˆ—ä¸­éšæœºå˜é‡ Xn çš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼Œè€Œ CDF(t) è¡¨ç¤ºéšæœºå˜é‡ X çš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚
- en: The definition states that when considering a sequence of random variables *Xn*,
    the cumulative distribution function (CDF) of the random variables in this sequence
    converges to the CDF of a random variable *X* as *n* approaches infinity.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰æŒ‡å‡ºï¼Œå½“è€ƒè™‘ä¸€ä¸ªéšæœºå˜é‡åºåˆ— *Xn* æ—¶ï¼Œè¯¥åºåˆ—ä¸­çš„éšæœºå˜é‡çš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰ä¼šéšç€ *n* çš„å¢åŠ è€Œæ”¶æ•›åˆ°éšæœºå˜é‡ *X* çš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚
- en: 'An illustrative example of this concept is the standardized sample mean. Below,
    you will find the definition of the standardized sample mean:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¦‚å¿µçš„ä¸€ä¸ªè¯´æ˜æ€§ä¾‹å­æ˜¯æ ‡å‡†åŒ–æ ·æœ¬å‡å€¼ã€‚ä¸‹é¢ï¼Œä½ ä¼šæ‰¾åˆ°æ ‡å‡†åŒ–æ ·æœ¬å‡å€¼çš„å®šä¹‰ï¼š
- en: '![](../Images/783c3a10e68ad323288653e2b3fd0ac3.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/783c3a10e68ad323288653e2b3fd0ac3.png)'
- en: Z represents the standardized sample mean, where n is the number of samples
    drawn, X_bar is the sample mean, ğœ‡ is the population mean, and ğœ is the population
    standard deviation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Z ä»£è¡¨æ ‡å‡†åŒ–æ ·æœ¬å‡å€¼ï¼Œå…¶ä¸­ n æ˜¯æŠ½å–çš„æ ·æœ¬æ•°é‡ï¼ŒX_bar æ˜¯æ ·æœ¬å‡å€¼ï¼Œğœ‡ æ˜¯æ€»ä½“å‡å€¼ï¼Œğœ æ˜¯æ€»ä½“æ ‡å‡†å·®ã€‚
- en: 'When you have drawn n samples from the population, you can obtain the standardized
    sample mean by following these steps: calculate the sample mean from the n samples,
    subtract the population mean from it, multiply the result by the square root of
    the sample size, and then divide by the population standard deviation.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ ä»æ€»ä½“ä¸­æŠ½å– n ä¸ªæ ·æœ¬æ—¶ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤è·å¾—æ ‡å‡†åŒ–æ ·æœ¬å‡å€¼ï¼šè®¡ç®—è¿™ n ä¸ªæ ·æœ¬çš„å‡å€¼ï¼Œä»ä¸­å‡å»æ€»ä½“å‡å€¼ï¼Œå°†ç»“æœä¹˜ä»¥æ ·æœ¬å¤§å°çš„å¹³æ–¹æ ¹ï¼Œç„¶åé™¤ä»¥æ€»ä½“æ ‡å‡†å·®ã€‚
- en: Interestingly, while the sample mean itself converges in probability to the
    population mean, the standardized sample mean converges in distribution to a random
    variable with normal distribution with a mean of zero and a standard deviation
    of one.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œè™½ç„¶æ ·æœ¬å‡å€¼æœ¬èº«åœ¨æ¦‚ç‡ä¸Šæ”¶æ•›åˆ°æ€»ä½“å‡å€¼ï¼Œä½†æ ‡å‡†åŒ–æ ·æœ¬å‡å€¼åœ¨åˆ†å¸ƒä¸Šæ”¶æ•›åˆ°å‡å€¼ä¸ºé›¶ã€æ ‡å‡†å·®ä¸ºä¸€çš„æ­£æ€åˆ†å¸ƒéšæœºå˜é‡ã€‚
- en: Intuitively, we can conceptualize the standardized sample mean as a rescaled
    version of the sample mean. Referring back to the previous illustrations of the
    sample mean convergence, we observe that its distribution gradually resembles
    that of a normal distribution as the sample size increases, only becoming progressively
    narrower. By rescaling the sample mean through multiplication by the square root
    of the sample size, we effectively broaden the distribution, allowing it to maintain
    the shape of a normal distribution.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å°†æ ‡å‡†åŒ–æ ·æœ¬å‡å€¼æ¦‚å¿µåŒ–ä¸ºæ ·æœ¬å‡å€¼çš„é‡æ–°ç¼©æ”¾ç‰ˆæœ¬ã€‚å›åˆ°ä¹‹å‰æ ·æœ¬å‡å€¼æ”¶æ•›çš„å›¾ç¤ºï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å…¶åˆ†å¸ƒéšç€æ ·æœ¬å¤§å°çš„å¢åŠ é€æ¸ç±»ä¼¼äºæ­£æ€åˆ†å¸ƒï¼Œåªæ˜¯é€æ¸å˜å¾—æ›´åŠ ç‹­çª„ã€‚é€šè¿‡å°†æ ·æœ¬å‡å€¼ä¹˜ä»¥æ ·æœ¬å¤§å°çš„å¹³æ–¹æ ¹ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°æ‰©å±•äº†åˆ†å¸ƒï¼Œä½¿å…¶ä¿æŒæ­£æ€åˆ†å¸ƒçš„å½¢çŠ¶ã€‚
- en: 'The visualization below illustrates the convergence of the standardized sample
    mean for samples drawn from both a normal distribution and a binomial distribution:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„å¯è§†åŒ–å›¾ç¤ºå±•ç¤ºäº†æ ‡å‡†åŒ–æ ·æœ¬å‡å€¼åœ¨ä»æ­£æ€åˆ†å¸ƒå’ŒäºŒé¡¹åˆ†å¸ƒä¸­æŠ½å–æ ·æœ¬çš„æ”¶æ•›æƒ…å†µï¼š
- en: '![](../Images/7d2bb75aef9c7328ab9db039869502e5.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d2bb75aef9c7328ab9db039869502e5.png)'
- en: The sampling distribution of the standardized sample mean is visualized in a
    sequence of histograms, showcasing samples drawn from a normal distribution for
    different sample sizes, specifically [1, 10, 100, 1000]. Image by author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†åŒ–æ ·æœ¬å‡å€¼çš„æŠ½æ ·åˆ†å¸ƒé€šè¿‡ä¸€ç³»åˆ—ç›´æ–¹å›¾è¿›è¡Œå¯è§†åŒ–ï¼Œå±•ç¤ºäº†ä¸åŒæ ·æœ¬å¤§å°ä¸‹ä»æ­£æ€åˆ†å¸ƒä¸­æŠ½å–çš„æ ·æœ¬ï¼Œå…·ä½“ä¸º [1, 10, 100, 1000]ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '![](../Images/43c73f6a84450ebbdb75202679d98652.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43c73f6a84450ebbdb75202679d98652.png)'
- en: The sampling distribution of the standardized sample mean is visualized in a
    sequence of histograms, showcasing samples drawn from a binomial distribution
    for different sample sizes, specifically [1, 10, 100, 1000]. Image by author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†åŒ–æ ·æœ¬å‡å€¼çš„æŠ½æ ·åˆ†å¸ƒé€šè¿‡ä¸€ç³»åˆ—ç›´æ–¹å›¾è¿›è¡Œå¯è§†åŒ–ï¼Œå±•ç¤ºäº†ä¸åŒæ ·æœ¬å¤§å°ä¸‹ä»äºŒé¡¹åˆ†å¸ƒä¸­æŠ½å–çš„æ ·æœ¬ï¼Œå…·ä½“ä¸º [1, 10, 100, 1000]ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Why do We Care?
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ºä»€ä¹ˆè¦å…³æ³¨è¿™ä¸ªï¼Ÿ
- en: 'In a sense, there exists only one overarching form of convergence: convergence
    by distribution. We can regard convergence by probability as a particular instance
    of convergence by distribution, wherein the final distribution becomes degenerate
    and converges to a single value. But why is this distinction significant?'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œå­˜åœ¨ä¸€ç§ç»ˆæçš„æ”¶æ•›å½¢å¼ï¼šæŒ‰åˆ†å¸ƒæ”¶æ•›ã€‚æˆ‘ä»¬å¯ä»¥å°†æŒ‰æ¦‚ç‡æ”¶æ•›è§†ä¸ºæŒ‰åˆ†å¸ƒæ”¶æ•›çš„ä¸€ç§ç‰¹ä¾‹ï¼Œå…¶ä¸­æœ€ç»ˆçš„åˆ†å¸ƒå˜å¾—é€€åŒ–å¹¶æ”¶æ•›åˆ°ä¸€ä¸ªå•ä¸€å€¼ã€‚ä½†ä¸ºä»€ä¹ˆè¿™ç§åŒºåˆ†å¾ˆé‡è¦å‘¢ï¼Ÿ
- en: First and foremost, the observation that the sample mean converges to the true
    population mean is what enables us to estimate that population mean. This estimation
    process is a common practice in a wide array of real-life situations. For instance,
    whenever we make generalizations like â€œneighbors are nosy,â€ we are implicitly
    relying on the idea that our limited sampling, derived from our own experiences,
    converges to the actual population mean. This principle is known as the Weak Law
    of Large Numbers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæ ·æœ¬å‡å€¼æ”¶æ•›äºçœŸå®æ€»ä½“å‡å€¼çš„è§‚å¯Ÿä½¿æˆ‘ä»¬èƒ½å¤Ÿä¼°è®¡è¯¥æ€»ä½“å‡å€¼ã€‚è¿™ç§ä¼°è®¡è¿‡ç¨‹åœ¨å„ç§å®é™…æƒ…å†µä¸­éƒ½æ˜¯å¸¸è§çš„ã€‚ä¾‹å¦‚ï¼Œæ¯å½“æˆ‘ä»¬åšå‡ºè¯¸å¦‚â€œé‚»å±…å¾ˆçˆ±æ‰“å¬â€çš„æ¦‚æ‹¬æ—¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯ä¾èµ–äºè¿™æ ·çš„è§‚ç‚¹ï¼šæˆ‘ä»¬æœ‰é™çš„æ ·æœ¬æ¥è‡ªæˆ‘ä»¬è‡ªå·±çš„ç»éªŒï¼Œæœ€ç»ˆä¼šæ”¶æ•›åˆ°å®é™…çš„æ€»ä½“å‡å€¼ã€‚è¿™ä¸ªåŸç†è¢«ç§°ä¸ºå¤§æ•°æ³•åˆ™ã€‚
- en: Even more crucial is the observation that the standardized sample mean converges
    to a normal distribution. It is this fact that empowers us to conduct hypothesis
    tests and make informed assessments regarding the likelihood that a particular
    set of observations arises from mere chance or an underlying causal process. This
    phenomenon is more formally recognized as the Central Limit Theorem.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´ä¸ºå…³é”®çš„æ˜¯æ ‡å‡†åŒ–æ ·æœ¬å‡å€¼æ”¶æ•›äºæ­£æ€åˆ†å¸ƒçš„è§‚å¯Ÿã€‚æ­£æ˜¯è¿™ä¸ªäº‹å®ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¿›è¡Œå‡è®¾æ£€éªŒï¼Œå¹¶å¯¹ç‰¹å®šè§‚å¯Ÿç»“æœæ˜¯ç”±äºå¶ç„¶æ€§è¿˜æ˜¯æ½œåœ¨å› æœè¿‡ç¨‹åšå‡ºæ˜æ™ºçš„è¯„ä¼°ã€‚è¿™ä¸€ç°è±¡æ›´æ­£å¼åœ°è¢«ç§°ä¸ºä¸­å¿ƒæé™å®šç†ã€‚
- en: '**Links**'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**é“¾æ¥**'
- en: '[Notebook for convergence illustration](https://github.com/swang225/meinyenura/blob/main/python/research/notebook/sample_mean_std_sample_mean_convergence.ipynb)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ”¶æ•›æ€§è¯´æ˜ç¬”è®°æœ¬](https://github.com/swang225/meinyenura/blob/main/python/research/notebook/sample_mean_std_sample_mean_convergence.ipynb)'
