- en: 'Make Python Faster by Caching Functions: Memoization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/make-python-faster-by-caching-functions-memoization-4fca250ab5f6](https://towardsdatascience.com/make-python-faster-by-caching-functions-memoization-4fca250ab5f6)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: PYTHON PROGRAMMING
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The article discusses memoization using the Python standard library. The functools.lru_cache
    decorator makes this so simple!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@nyggus?source=post_page-----4fca250ab5f6--------------------------------)[![Marcin
    Kozak](../Images/d7faf62e48ed81dab5d8ad92819fff54.png)](https://medium.com/@nyggus?source=post_page-----4fca250ab5f6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4fca250ab5f6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4fca250ab5f6--------------------------------)
    [Marcin Kozak](https://medium.com/@nyggus?source=post_page-----4fca250ab5f6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4fca250ab5f6--------------------------------)
    ·11 min read·Nov 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3774b4417d263b6337423d06fdb6a85.png)'
  prefs: []
  type: TYPE_IMG
- en: You can request Python to remember what functions have returned already — and
    to use it. Photo by [Kelly Sikkema](https://unsplash.com/@kellysikkema?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'We all know Python can be slow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/pythoniq/the-speed-of-python-it-aint-that-bad-9f703dd2924e?source=post_page-----4fca250ab5f6--------------------------------)
    [## The Speed of Python: It Ain’t That Bad!'
  prefs: []
  type: TYPE_NORMAL
- en: I hear all the time that Python is way too slow. Is it?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/pythoniq/the-speed-of-python-it-aint-that-bad-9f703dd2924e?source=post_page-----4fca250ab5f6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: What usually takes most time in Python is calling functions and class methods
    that run expensive processes. Imagine for a second that you need to run such a
    function twice for the same arguments; it will need two times as much time even
    though you both calls lead to the very same output. Is it possible to just remember
    this output and use it once more whenever it’s needed?
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes, you can! It’s called [memoization](https://en.wikipedia.org/wiki/Memoization),
    and it’s a common term in programming. You could implement your own memoization
    techniques, but the truth is, you don’t have to. Python offers you a powerful
    memoization tool, and it does so in the standard library: the `functools.lru_cache`
    decorator.'
  prefs: []
  type: TYPE_NORMAL
- en: Although often very efficient, memoization if often omitted in Python textbooks,
    even those that describe profiling and memory savings during coding. The books
    that do mention memoization in Python include [*Serious Python*](https://nostarch.com/seriouspython)
    by Julien Danjou, [*Fluent Python*, *2nd ed*.](https://www.oreilly.com/library/view/fluent-python-2nd/9781492056348/)
    by Luciano Ramalho, [*Functional Python Programming, 3rd ed.*](https://www.packtpub.com/product/functional-python-programming-third-edition/9781803232577)
    by Steven F. Lott.
  prefs: []
  type: TYPE_NORMAL
- en: 'This article shows two things: how simple it is to use memoization in the Python
    standard library (so, using `functools.lru_cache`), and how powerful this technique
    can be. It’s not only milk and honey, however. That’s why we’ll also discuss issues
    you can encounter when using the `functools.lru_cache` caching tool.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Caching using the functools module`'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`functools.lru_cache`'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Python offers various memoization tools, but today, we’re talking about the
    one that is part of the Python standard library: `functools.lru_cache`.'
  prefs: []
  type: TYPE_NORMAL
- en: The LRU caching strategy stands for *Least Recently Used*. In this strategy,
    the least recently used item is removed from the cache when the size of the cache
    has been exceeded.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use it for a function, decorate it with `@functools.lru_cache`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `foo()` does not take any arguments, which basically means it will
    be run only once during a session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, after having been called three times, the function was run only once
    — but we got the same output (`10`) three times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the real power of function memoization comes with functions that
    take arguments. Such a function is run for each new set of arguments, but whenever
    the same arguments have been used before, the function is not run but the remembered
    output is taken as the function output. Consider the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the type of `sum_of_powers`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'So its type is not `function` but `lru_cache_wrapper`. Let’s see the function
    in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: So, the last two calls of the function did not actually run it — but thanks
    to memoization, the function returned the output anyway.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the `maxsize` argument to `lru_cache` to indicate how many items
    are being kept in the cache. The default value is `128`. As Luciano Ramalho explains
    in his [*Fluent Python. 2nd ed*](https://www.oreilly.com/library/view/fluent-python-2nd/9781492056348/)*.*
    book, for optimum performance you should use `maxsize` values that are the power
    of `2`. When `maxsize` is `None`, the the cache can keep any number of objects
    more precisely, as many as memory allows. Be cautious, as this may cause the memory
    run out.
  prefs: []
  type: TYPE_NORMAL
- en: You can use one more argument, that is, `typed`, which is a Boolean object with
    the default value of `False`. When it’s `True`, the same value of different types
    would be kept separately. So, an integer value `1` and a float value of `1.0`
    would be kept as separate items. When `typed=False` (the default), they would
    be kept as one item.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s a way to learn how caching worked for a function decorated with `functools.lru_cache`,
    thanks to a `cache_info` attribute added to the function. More precisely, it’s
    a method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And this is how the `cache_info()` works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This means the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hits=2` → the cache has been used twice; indeed, we used the function twice
    for `x=(1, 1, 2)` and `pow=2` and twice for `x=(1, 1, 2)` and `pow=3`, but the
    cache was used once per each value of `x`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`misses=2` → this output represents the two calls to the function with new
    argument values, so when cache was not used. The more misses-to-hits ratio, the
    less useful caching is because the function is often used with new values of arguments
    and seldom with the argument values that have been already used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxsize=128` → the size of the cache; we’ll discuss in below.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`currsize=2` → the number of elements already being kept in the cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a real life project in which you want to use caching, it’s good to experiment
    with it, and the `cache_info()` method can be very helpful with that. Remember
    that caching shouldn’t be used automatically for any function; it may lead to
    time losses rather than gains, as caching itself needs some time.
  prefs: []
  type: TYPE_NORMAL
- en: '`functools.cache`'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As of Python 3.9, the `functools` module offers also [the](https://docs.python.org/3.9/library/functools.html#functools.cache)
    `[cache](https://docs.python.org/3.9/library/functools.html#functools.cache)`
    [decorator](https://docs.python.org/3.9/library/functools.html#functools.cache).
    It’s just a wrapper around `functools.lru_cache` in which `maxsize=None`. So,
    decorating with `functools.cache` is equivalent to decorating with `functools.lru_cache(maxsize=None)`.
    To be honest, I don’t think it’s a necessary amendment — the standard library
    doesn’t need, in my opinion, such overly simplified wrappers. What’s wrong with
    using `functools.lru_cache(maxsize=None)`?
  prefs: []
  type: TYPE_NORMAL
- en: Comments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Use for class methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Function caching is considered to be a functional-programming tool, but it
    can just as well be used for class methods. Unlike Python functions, a Python
    class can have state, which is why it’s simple to implement individual caching
    within a class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is a very simplified caching approach, and one that would not work after
    changing the `self.letter` attribute in the class’s instance. It’s easy to solve
    this issue, however, for instance by making `self.cache` a dictionary of dictionaries
    being assigned to keys being letters. I’ll leave you the implementation as an
    exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'This was an example how to do it manually, but we can use the `functools.lru_cache`
    decorator as well. Doing so as simple as for a function: enough to decorate the
    method inside the class definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this works just like with functions, so don’t hesitate to use
    the `functools.lru_cache` decorator also for class methods when you see such a
    need.
  prefs: []
  type: TYPE_NORMAL
- en: Only hashable arguments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Standard caching employs mapping using a hash table. This means that it wouldn’t
    work for functions with unhashable arguments. Thus, for example, you cannot use
    caching for objects of the following types: lists, sets and dictionaries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s analyze the above function, `sum_of_powers()`. Its `x` argument has a
    general type of `Sequence[float]`. `collections.Sequence` does not say anything
    being hashable or not; and indeed, some types that follow this protocol are hashable,
    such as `tuple`, but others aren’t, such as `list`. Note:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Just so you know, static type checkers can point you out a call of a caching-decorated
    function that uses an unhashable argument. This call is fine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/869711fbc572a9dc49e05152110f663d.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot from Visual Studio Code. No static errors when the function is
    called with a tuple, which is hashable. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'but this one isn’t:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4db0874895e429e27c5bec92f4f38973.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot from Visual Studio Code. [Mypy](https://mypy.readthedocs.io/en/stable/)
    points out a static error indicating an attempt to cache a function called with
    a list, which is unhashable. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Pure and non-pure functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have heard that memoization should be used solely for so-called pure
    functions, that is, functions without side effects. A side effect is something
    that’s done in the background; for example, a global variable is being changed,
    data are being read from a database, information is being logged.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, non-pure functions should not be cached indeed, but there are a few
    exceptions. For example, imagine a function that generates a report and saves
    it to a file, based on the provided values of the function’s arguments. Caching
    the function will mean generating the report for these argument values only once
    instead of every time the function is called with these values. In general, costs
    of I/O operations can be saved thanks to caching. In this case, however, we must
    be sure that the side effect of such a function doesn’t differ for its subsequent
    calls. This could happen, for example, when a function would overwrite the file
    with a new file with the very same contents — but the time of this operations
    matters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, we have observed caching of functions with side effects. We did so
    above, when we cached functions that printed a message to the console. As you
    can recall, the functions printed messages only during the first call but not
    when the cache was used. This is exactly what we’re talking about: When the side
    effect doesn’t matter, you can use caching; when it does, you shouldn’t as you
    risk that the side effect will be observed only during the first call of the function
    but not during the subsequent ones.'
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I will not present you the example typically presented for caching, that is,
    a function calculating factorials. Instead, I will use a very simple function
    that creates a simple dictionary using a dict comprehension.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run this script for various `n` values, which is used as an argument
    of the two benchmarked functions. It’s easy to guess the hypothesis behind the
    experiment: the bigger the value of `n`, that is, the larger the dictionary created
    by the two functions, the bigger the caching effect should be.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll see if this is the case soon. We will run the script for the following
    `n` values: `1`, `10`, `100`, `1000`, `10_000` and `100_000`. The number of runs
    needs to be adjusted so that the tests don’t take too much time — but you can
    run the experiments yourself with more runs; to do so, change this fragment: `Number=int(1_000_000
    / n)`.'
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering why I did not create a loop to run all the benchmarks in
    one run of the script. This is because each experiment should be independent;
    otherwise, we could risk that the results of the subsequent experiments would
    be biased. It’s always best to run benchmarks individually, without combining
    them in the same run of a script.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script prints the best results for each function and one more metrics:
    how many times the cached function was faster than the non-cached one. Below,
    I will show only this ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the performance of caching is amazing, but at some point (here,
    for `n` of `10_000`) its performance drops significantly; for even bigger `n`,
    it drops dramatically. Can we fix that?
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, we can’t. The only change we can make in this function is choosing
    `maxsize`, which would be of no use here as the cache keeps only one element:
    the dictionary for the given value of `n`.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, I am afraid we cannot improve the performance of `functools.lru_cache`
    in the case of so large items being kept in cache. As you’ll see below, this is
    not an issue of the caching technique, but of caching itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our performance experiment taught us two significant things:'
  prefs: []
  type: TYPE_NORMAL
- en: Caching using `functools.lru_cache` can help increase performance, even a lot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching very large objects can decrease performance as compared to caching smaller
    objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s implement our own simple caching and check out if the same situation happens
    for it. This will be an overly simplified caching tool, something I wouldn’t decide
    to use in a real life project.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'and here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: They differ a little bit from the previous ones, but the trend is the same.
    This shows it’s not the strategy that `functools.lru_cache` uses that was the
    reason behind this significant drop in performance. Most likely, it’s the size
    of the output (here, the dictionary).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Caching is a fantastic tool, one that can help improve performance, even a lot.
    A great thing is, it doesn’t take much time to learn how to use it, and using
    it is simple, too.
  prefs: []
  type: TYPE_NORMAL
- en: The Python standard library offers caching via the `functools.lru_cache` decorator.
    Oftentimes, it’ll be exactly what you need, but sometimes its limitations can
    make it impossible to use it. The most significant one seems to be inability to
    use unhashable arguments. The other one is that caching should be used only for
    pure functions; nonetheless, it’s not always the case, as we discussed above.
  prefs: []
  type: TYPE_NORMAL
- en: We’re not done with caching. In this article, we discussed the Python standard
    library tool, but in a future article, we’ll discuss PyPi caching tools, such
    as [cache3](https://github.com/StKali/cache3) and [cachetools](https://github.com/tkem/cachetools/).
    We’ll compare their performance with that of `functools.lru_cache`, and we will
    consider their pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks for reading. If you enjoyed this article, you may also enjoy other articles
    I wrote; you will see them [here](https://medium.com/@nyggus). And if you want
    to join Medium, please use my referral link below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@nyggus/membership?source=post_page-----4fca250ab5f6--------------------------------)
    [## Join Medium with my referral link - Marcin Kozak'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@nyggus/membership?source=post_page-----4fca250ab5f6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
