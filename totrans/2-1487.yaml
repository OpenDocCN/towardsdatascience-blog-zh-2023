- en: Mastering Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mastering-logistic-regression-3e502686f0ae](https://towardsdatascience.com/mastering-logistic-regression-3e502686f0ae)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From theory to implementation in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roiyeho?source=post_page-----3e502686f0ae--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----3e502686f0ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e502686f0ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e502686f0ae--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----3e502686f0ae--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e502686f0ae--------------------------------)
    ·17 min read·May 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/054e0f77d6a401cd71794ce2aa09c550.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Gerd Altmann](https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1044090)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1044090)
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression is one of the most common machine learning algorithms. It
    can be used to predict the probability of an event occurring, such as whether
    an incoming email is spam or not, or whether a tumor is malignant or not, based
    on a given labeled data set.
  prefs: []
  type: TYPE_NORMAL
- en: Due to its simplicity, logistic regression is often used as a baseline against
    which other, more complex models are evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: The model has the word “logistic” in its name, since it uses the **logistic
    function** (sigmoid) to convert a linear combination of the input features into
    probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: It also has the word “regression” in its name, since its output is a continuous
    value between 0 and 1, although it is typically used as a **binary classifier**
    by choosing a threshold value (usually 0.5) and classifying inputs with probability
    greater than the threshold as the positive class, and those below the threshold
    as the negative class.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we will discuss the logistic regression model in depth, implement
    it from scratch in Python, and then show its implementation in Scikit-Learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Background: Binary Classification Problems'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that in [supervised machine learning](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)
    problems, we are given a training set of *n* labeled samples: *D* = {(**x**₁,
    *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ* is a *m*-dimensional
    vector that contains the **features** of sample *i*, and *yᵢ* represents the **label**
    of that sample. Our goal is to build a model whose predictions are as close as
    possible to the true labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In **classification problems**, the label *yᵢ* can take one of *k* values,
    representing the *k* classes to which the samples belong. More specifically, in
    **binary classification problems**, the label *yᵢ* can assume only two values:
    0 (representing the negative class) and 1 (representing the positive class).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we distinguish between two types of classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deterministic classifiers** output a **hard label** for each sample, without
    providing probability estimates for the classes. Examples for such classifiers
    include [perceptrons](https://medium.com/towards-data-science/perceptrons-the-first-neural-network-model-8b3ee4513757),
    [K-nearest neighbors](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad),
    and SVMs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Probabilistic classifiers** output probability estimates for the classes,
    and then assign a label to the given sample based on these probabilities (typically
    the label of the class with the highest probability). Examples for such classifiers
    include logistic regression, naïve Bayes classifiers, and [neural networks](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)
    that use sigmoid or softmax in the output layer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Logistic Regression Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression is a probabilistic classifier that handles binary classification
    problems. Given a sample (**x**, *y*), it outputs a probability *p* that the sample
    belongs to the positive class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d45b6f548b6b589d1f8c9f1dfa22a0cf.png)'
  prefs: []
  type: TYPE_IMG
- en: If this probability is higher than some threshold value (typically chosen as
    0.5), then the sample is classified as 1, otherwise it is classified as 0.
  prefs: []
  type: TYPE_NORMAL
- en: How does the model estimate the probability *p*?
  prefs: []
  type: TYPE_NORMAL
- en: The basic assumption in logistic regression is that the **log-odds of the event
    that the sample belongs to the positive class is a linear combination of its features**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Log-odds** (also called **logit**) is the logarithm of the **odds ratio**,
    which is the ratio between the probability that the sample belongs to the positive
    class and the probability that it belongs to the negative class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b2d5596a24be64140c99eb6ac1765e9.png)'
  prefs: []
  type: TYPE_IMG
- en: The log-odds (logit) function
  prefs: []
  type: TYPE_NORMAL
- en: We assume here that the base of the logarithm is *e* (i.e., natural logarithm),
    although other bases can be used as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph of the logit function is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff6827dbd135cac0ec6872fce8bcb001.png)'
  prefs: []
  type: TYPE_IMG
- en: The logit function
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen, the logit function maps probability values in (0, 1) into real
    numbers in (-∞, +∞).
  prefs: []
  type: TYPE_NORMAL
- en: In logistic regression, we assume that the log odds is a linear combination
    of the features, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e62fbedc12d1be120fa4777b42f1994.png)'
  prefs: []
  type: TYPE_IMG
- en: where **w** = (*w*₀, …, *wₘ*) are the **parameters** (or weights) of the model.
    The parameter *w*₀ is often called the **intercept** (or bias).
  prefs: []
  type: TYPE_NORMAL
- en: 'The points for which *p* = 0.5 (i.e., the log odds is equal to 0) define the
    separating hyperplane between the two classes, whose equation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74c8e107254ec62915b2af1181a5a591.png)'
  prefs: []
  type: TYPE_IMG
- en: The equation of the separating hyperplane
  prefs: []
  type: TYPE_NORMAL
- en: 'The weight vector **w** is orthogonal to this hyperplane. Every example above
    the hyperplane (**w***ᵗ***x** > 0**)** is classified as a positive example, whereas
    every example below the hyperplane (**w***ᵗ***x** < 0**)** is classified as a
    negative example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e991e170dd4d8e1a5f2f0cf77f34ed67.png)'
  prefs: []
  type: TYPE_IMG
- en: This makes logistic regression a **linear classifier**, since it assumes that
    the boundary between the classes is a linear surface. Other linear classifiers
    include [perceptrons](https://medium.com/towards-data-science/perceptrons-the-first-neural-network-model-8b3ee4513757)
    and SVMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can find a direct correlation between *p* and the parameters **w**, by exponentiating
    both sides of the log-odds equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8abd4f8da7b78d14e45f5650c5af6124.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *σ* is the **sigmoid function** (also known as the **logistic function**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89eae5c5e6e3b27204838eb5b3481c13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sigmoid function is used to convert the log-odds (**w***ᵗ***x**) into probabilities.
    It has a characteristic “S” or shaped curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd1a3ef14df25d51c82d947e9a36b49d.png)'
  prefs: []
  type: TYPE_IMG
- en: The sigmoid function
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen, the function maps real numbers in (-∞, +∞) into probability
    values in (0, 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'The sigmoid function has some nice mathematical properties that will be useful
    later:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15b985b012b1fa23f822043d0c1c5d57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram summarizes the computational process of logistic regression
    starting from the inputs until the final prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72f9ffae8042d4954afdd5fed718c560.png)'
  prefs: []
  type: TYPE_IMG
- en: The logistic regression model
  prefs: []
  type: TYPE_NORMAL
- en: Log Loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our goal is to find the parameters **w** that will make the model’s predictions
    *p* = *σ*(**w***ᵗ***x**) as close as possible to the true labels *y*. To that
    end, we need to define a **loss function** that will measure how far our model’s
    predictions are from the true labels. This function needs to be differentiable,
    so it can be optimized using techniques such as gradient descent (for more information
    on loss functions in machine learning see [this article](https://medium.com/towards-data-science/loss-functions-in-machine-learning-9977e810ac02)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function used by logistic regression is called **log loss** (or **logistic
    loss**). It is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/204fff1b75019188c1b25ebf1249e8e7.png)'
  prefs: []
  type: TYPE_IMG
- en: The log loss function
  prefs: []
  type: TYPE_NORMAL
- en: How did we get to this function? The derivation of this function is based on
    the [maximum likelihood principle](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43).
    More specifically, we can show that log loss is the negative log likelihood under
    the assumption that the labels have a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution)
    (i.e., a probability distribution of a binary random variable that takes 1 with
    probability *p* and 0 with probability 1 − *p*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, we will show that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/615eb356f5860b57e640f5c6bb4f2830.png)'
  prefs: []
  type: TYPE_IMG
- en: where *P*(*y*|*p*) is the probability of getting the label *y* given the model’s
    prediction *p* (i.e., the likelihood of the data given our model).
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof:**'
  prefs: []
  type: TYPE_NORMAL
- en: Given a model of the data (the labels) as a Bernoulli distribution with parameter
    *p*, the probability that a sample belongs to the positive class is simply *p*,
    i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0844535fa1311bfd6b66aeb64a22be3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the probability that the sample belongs to the negative class is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23c8ef1dc747aca60c605bbf04954e39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can write these two equations more compactly as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c658cf69f1834acc67a0262ac647d9d5.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Explanation*: when *y* = 1, *pʸ* = *p* and (1 − *p*)¹⁻*ʸ* = 1, therefore *P*(*y*|*p*)
    = *p*. Similarly, when *y* = 0, *pʸ* = 1 and (1 − *p*)¹⁻*ʸ* = 1 − *p*, therefore
    *P*(*y*|*p*) = 1 − *p*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore the log likelihood of the data given the model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf2cbf2a69b101936b74249690535fb9.png)'
  prefs: []
  type: TYPE_IMG
- en: The log loss is exactly the negative of this function. Therefore, maximizing
    the log likelihood is equivalent to minimizing the log loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows the log loss when *y* = 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8be4e4573f31964eee4dae08c1f55a63.png)'
  prefs: []
  type: TYPE_IMG
- en: The log loss equals to 0 only in case of a perfect prediction (*p* = 1 and *y*
    = 1, or *p* = 0 and *y* = 0), and approaches infinity as the prediction gets worse
    (i.e., when *y* = 1 and *p* → 0 or *y* = 0 and *p* → 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'The **cost function** calculates the average loss over the whole data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51b377c12bd97944e1ff47659d7bf32b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This function can be written in a vectorized form as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec54b26bd9532f213ddddc0f5e561e14.png)'
  prefs: []
  type: TYPE_IMG
- en: where **y** = (*y*₁, …, *yₙ*) is a vector that contains the labels of all the
    training samples, and **p** = (*p*₁, …, *pₙ*) is a vector that contains the predicted
    probabilities of the model for all the training samples.
  prefs: []
  type: TYPE_NORMAL
- en: This cost function is convex, i.e., it has a single global minimum. However,
    there is no closed-form solution for finding the optimal **w*** (due to the non-linearities
    introduced by the log function). Therefore, we need to use iterative optimization
    methods such as gradient descent in order to find the minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gradient descent is an iterative approach for finding a minimum of a function,
    where we take small steps in the opposite direction of the gradient in order to
    get closer to the minimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f419ed93df7d803b49270ddeb7f6070b.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient descent
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use gradient descent to find the minimum of the cost *J*(**w**),
    we need to compute its partial derivatives with respect to each one of the weights.
    The partial derivative of *J*(**w**) with respect to a given weight *wⱼ* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/551631cc09e61f7ca4198ddf6e64e665.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Proof**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21277b342cb8e559ec2d9f9bc2be4ac1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the gradient vector can be written in a vectorized form as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ebd8e83e32c3a1a60e778f6a507f6e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the gradient descent update rule is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de25b46c2bab6095aa6077f02fa573a3.png)'
  prefs: []
  type: TYPE_IMG
- en: where *α* is a learning rate that controls the step size (0 < *α* < 1).
  prefs: []
  type: TYPE_NORMAL
- en: Note that whenever you use gradient descent, you must make sure that your data
    set is **normalized** (otherwise gradient descent may take steps of different
    sizes in different directions, which will make it unstable).
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now implement the logistic regression model in Python from scratch,
    including the cost function and gradient computation, optimizing the model using
    gradient descent, evaluation of the model, and plotting the final decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: For the demonstration we will use the [Iris data set](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)
    (BSD license). The original data set contains 150 samples of Iris flowers that
    belong to one of three species (setosa, versicolor and virginica). We will change
    it into a binary classification problem by using only the first two types of flowers
    (setosa and versicolor). In addition, we will use only the first two features
    of each flower (sepal width and sepal length).
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Data Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s first import the required libraries and fix the random seed in order
    to get reproducible results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we load the data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2d9937474c68acf2acaafacf7bfedfa7.png)'
  prefs: []
  type: TYPE_IMG
- en: The Iris data set
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen, the data set is linearly separable, therefore logistic regression
    should be able to find the boundary between the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to add a column of ones to the features matrix *X* in order to
    represent the bias (*w*₀):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We now split the data set into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Model Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are now ready to implement the logistic regression model. We start by defining
    a helper function to compute the sigmoid function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, we implement the cost function that returns the cost of a logistic regression
    model with parameters **w** on a given data set (*X*, **y**), and also its gradient
    with respect to **w**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are using the vectorized forms of the cost and the gradient functions
    that have been shown previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sanity check this function, let’s compute the cost and gradient of the model
    on some random weight vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output we get is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Gradient Descent Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now implement gradient descent in order to find the optimal **w*** that
    minimizes the cost function on a given training set. The algorithm will run at
    most *max_iter* passes over the training set (defaults to 5000), unless the cost
    has not decreased by at least *tol* since the previous iteration (defaults to
    0.0001), in which case the training stops immediately.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Normally at this point you would have to normalize your data set, since gradient
    descent does not work well with features that have different scales. In our specific
    data set normalization is not necessary since the ranges of the two features are
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now call this function to optimize our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm converges after 1,413 iterations and the optimal **w*** we get
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There are other solvers you can use for the optimization which are often faster
    than gradient descent, such as conjugate gradient (CG) and truncated Newton (TNC).
    See [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)
    for more details on how to use these optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Model for Predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have found the optimal parameters of the model, we can use it for
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s write a function that gets a matrix of new samples *X* and returns
    their probabilities of belonging to the positive class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The function computes the predictions of the model by simply taking the sigmoid
    of *Xᵗ***w** (which computes *σ*(**w***ᵗ***x**) for every row **x** in the matrix).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s find the probability that a sample located at (6, 2) belongs
    to the versicolor class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This sample has 89.52% chance of being a versicolor flower. This makes sense
    since this sample is located well within the area of the versicolor flowers far
    from the border between the classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the probability that a sample located at (5.5, 3) belongs
    to the versicolor class is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This time the probability is much lower (only 56.44%), since this sample is
    close to the border between the classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s write another function that returns the predicted class labels instead
    of probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The function simply predicts 1 whenever the probability of belonging to the
    positive class is at least 0.5, and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s test this function with the samples from above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As expected, both of the samples are classified as 1.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, let’s write a function to compute the accuracy of the model on a given
    data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The function first finds the predicted labels of the model on the given data
    set *X,* and compares them to the true labels **y**. The accuracy is then measured
    as the mean number of correct classifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d624d81c65fa15a038fde0246258dfba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s use this function to find the accuracy of our model on the training and
    the test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the scores are very high since the data set is linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to accuracy, there are other important metrics that are used to
    evaluate classification models such as precision, recall and F1 score. These metrics
    will be discussed in a future post.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the Decision Boundary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, since our data set is two-dimensional, we can plot the boundary line
    between the classes that was found by our model. To that end, we first need to
    find the equation of this line.
  prefs: []
  type: TYPE_NORMAL
- en: The boundary line is defined by the points for which the prediction of our model
    is exactly 0.5, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27dadcb4ae17a4836c653fc811826e38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sigmoid function is equal to 0.5 when its input is equal to 0, therefore
    we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2bbc7ec897ddce1dd5749879782b3261.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After rearranging the terms we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99d2bf0c245062e5b1f58b661b4ec241.png)'
  prefs: []
  type: TYPE_IMG
- en: 'That is, the slope of the boundary line is -*w*₁/*w*₂ and its intercept is
    -*w*₀/*w*₂. We can now write a function that plots this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5514bd18eafdf2330842ddd847eb5671.png)'
  prefs: []
  type: TYPE_IMG
- en: The decision boundary between the classes
  prefs: []
  type: TYPE_NORMAL
- en: We can see that only one sample was misclassified by our model. Training the
    model for more iterations (around 200,000) would have found a line that perfectly
    separates the two classes. With a fixed step size, the optimal convergence rate
    of gradient descent is very slow. This can be improved by using an adaptive learning
    rate (e.g., using more aggressive step sizes to compensate for the rapidly vanishing
    gradients).
  prefs: []
  type: TYPE_NORMAL
- en: The LogisticRegression Class in Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although implementing logistic regression from scratch had its own educational
    merits, a more practical choice would be to use the ready-made [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
    class from Scikit-Learn. This class uses more efficient solvers than the plain
    vanilla gradient descent, and it also provides additional options such as regularization
    and early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: 'The important hyperparameters of this class are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*penalty* — specifies the type of regularization to apply. Can be one of the
    options: None, ‘l2’ (the default), ‘l1’ and ‘elasticnet’. See [this article](https://medium.com/@roiyeho/regularization-19b1879415a1)
    for more information on regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*tol* — the tolerance for the stopping criterion (defaults to 0.0001).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*C* — the inverse of the regularization coefficient (defaults to 1.0). Smaller
    values specify stronger regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*solver* — the algorithm to use for the optimization. Can take one of the options:
    ‘lbfgs’ (the default), ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’.
    Read the documentation for more information on these optimizers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*max_iter* — the maximum number of iterations for the solvers to converge (defaults
    to 100)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*multi_class* — how to handle multi-class classification problems. Can take
    one of the options: ‘ovr’ (One Vs. Rest, i.e., a binary classifier is built for
    each class against all the other ones), ‘multinomial’ (uses multinomial logistic
    regression) or ‘auto’ (the default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When using the LogisticRegression class, you do not need to manually add a
    column of ones to the design matrix *X*, since this is done automatically by Scikit-Learn.
    Therefore, before building the model, we will split the original data (without
    the extra column of ones) into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s create an instance of LogisticRegression with its default settings, and
    fit it to the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we evaluate the model on the training and the test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This time we get perfect scores both on the training and the test sets. We
    can also check how many iterations were needed for convergence by querying the
    *n_iter_* attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Only 15 iterations were needed for convergence! Clearly, the solver used by
    LogisticRegression ([L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)
    by default) is more efficient than our implementation of gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot the decision boundary found by the model as we did previously.
    However, this time the optimal coefficients are stored in two different attributes
    of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '*coef_* is an array that contains all the weights except for the intercept'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*intercept_* is the intercept term (*w*₀)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, we need to concatenate these two attributes into one array before
    calling the *plot_decision_boundary*() function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b18f099a77da55cd7e524d6175341179.png)'
  prefs: []
  type: TYPE_IMG
- en: The decision boundary found by LogisticRegression
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the line found by LogisticRegression perfectly separates the two
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s summarize the pros and cons of logistic regression as compared to other
    classification models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**:'
  prefs: []
  type: TYPE_NORMAL
- en: When the data is linearly separable, the algorithm is guaranteed to find a separating
    hyperplane between the classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides class probability estimates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not suffer from overfitting (but usually underfits the data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly interpretable (the weight associated with each feature represents its
    importance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly scalable (requires a number of parameters linear in the number of features)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle redundant features (by assigning them weights close to 0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has a small number of hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**:'
  prefs: []
  type: TYPE_NORMAL
- en: Can find only linear decision boundaries between classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually outperformed by more complex models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports only binary classification, but can be extended to multi-class. The
    extension of logistic regression to multi-class problems (called **multinomial
    logistic regression** or **softmax regression**) is covered in [this article](/deep-dive-into-softmax-regression-62deea103cb8).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cannot deal with missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code examples of this article can be found on my github: [https://github.com/roiyeho/medium/tree/main/logistic_regression](https://github.com/roiyeho/medium/tree/main/logistic_regression)'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
