- en: Create your own Generative AI Text-to-Image API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建你自己的生成 AI 文本到图像 API
- en: 原文：[https://towardsdatascience.com/create-your-own-generative-ai-text-to-image-api-548c07a4d839](https://towardsdatascience.com/create-your-own-generative-ai-text-to-image-api-548c07a4d839)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/create-your-own-generative-ai-text-to-image-api-548c07a4d839](https://towardsdatascience.com/create-your-own-generative-ai-text-to-image-api-548c07a4d839)
- en: Turn your ramblings into masterpieces, on demand
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将你的随想转化为杰作，按需制作
- en: '[](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)[![Omer
    Mahmood](../Images/0c87da4134bea397c77bc4ba6640e34b.png)](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)[](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)
    [Omer Mahmood](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)[![奥默尔·马赫穆德](../Images/0c87da4134bea397c77bc4ba6640e34b.png)](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)[](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)
    [奥默尔·马赫穆德](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)
    ·17 min read·Apr 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)
    ·17分钟阅读·2023年4月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/738244045935fe00f21f0a7c31c3f546.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/738244045935fe00f21f0a7c31c3f546.png)'
- en: Image generated by the author using Midjourney on General Commercial Terms.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 Midjourney 根据一般商业条款生成。
- en: The TL;DR
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TL;DR
- en: Recent advances in Generative AI have led to the launch of a whole host of services
    such as DALL-E 2, Midjourney and Stability AI that have the potential to drastically
    change the way we approach content creation.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成 AI 的最新进展导致了一系列服务的推出，如 DALL-E 2、Midjourney 和 Stability AI，它们有潜力彻底改变我们对内容创作的方式。
- en: In this post I show you how to build and serve your very own, high performance,
    text-to-image service over an API. Based on [Stable Diffusion](https://github.com/CompVis/stable-diffusion)
    via [HuggingFace](https://medium.com/towards-data-science/whats-hugging-face-122f4e7eb11a),
    using Vertex AI Workbench and Endpoints.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将展示如何通过 API 构建并提供你自己高性能的文本到图像服务。基于 [稳定扩散](https://github.com/CompVis/stable-diffusion)
    通过 [HuggingFace](https://medium.com/towards-data-science/whats-hugging-face-122f4e7eb11a)，使用
    Vertex AI Workbench 和 Endpoints。
- en: 🚣🏼 How we got here
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🚣🏼 我们是如何到达这里的
- en: 'As George Lawton mentions in his [article](https://www.techtarget.com/searchenterpriseai/definition/generative-AI):
    “Generative AI is a type of artificial intelligence technology that can produce
    various types of content including text, imagery, audio and synthetic data. The
    recent buzz around generative AI has been driven by the simplicity of new user
    interfaces for creating high-quality text, graphics and videos in a matter of
    seconds.”[2]'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如乔治·劳顿在他的 [文章](https://www.techtarget.com/searchenterpriseai/definition/generative-AI)
    中提到的：“生成 AI 是一种人工智能技术，能够生成各种类型的内容，包括文本、图像、音频和合成数据。最近围绕生成 AI 的热议是由于新用户界面的简便性，这些界面可以在几秒钟内创建高质量的文本、图形和视频。”[2]
- en: Machine Learning is nothing new, in fact it’s been around in some shape or form
    since the 1960s[1]. “But it was not until 2014, with the introduction of [generative
    adversarial networks](https://en.wikipedia.org/wiki/Generative_adversarial_network)
    (GANs), a type of machine learning algorithm, that generative AI could create
    convincingly authentic images, videos and audio of real people.”[2]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习并不是什么新鲜事，事实上，自1960年代以来，它以某种形式存在。“但直到2014年，随着 [生成对抗网络](https://en.wikipedia.org/wiki/Generative_adversarial_network)（GANs）的引入，一种机器学习算法，生成
    AI 才得以创造令人信服的真实人物图像、视频和音频。”[2]
- en: Combined with the power of Large Language Models (LLMs) that can take a user
    prompt in natural language describing something and then produce photorealistic
    images, we’ve come a very long way in a short period of time. The first to do
    this was [OpenAI’s DALL·E](https://openai.com/product/dall-e-2), in April 2022,
    followed by Disco Diffusion in August 2022, which was eventually succeeded by
    Stable Diffusion. In parallel to all these offerings, there is a company called
    [Midjourney](https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F) that has
    built a very popular model that is used by interacting with a Discord bot.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 结合能够接收自然语言提示并生成照片级真实图像的大型语言模型（LLMs）的能力，我们在短时间内取得了巨大的进步。第一个做到这一点的是[OpenAI的DALL·E](https://openai.com/product/dall-e-2)，于2022年4月推出，随后是2022年8月的Disco
    Diffusion，最终被稳定扩散所取代。与这些产品并行的是一家名为[Midjourney](https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F)的公司，它开发了一个非常受欢迎的模型，通过与Discord机器人互动使用。
- en: Since then progress made in the state of the art has been quite astounding.
    The screenshot below shows what was accomplished in just the space of a few months
    — two different models were provided with the same prompt, but the contrast between
    early and later models is stark!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，艺术状态的进展令人惊叹。下面的截图展示了仅在几个月的时间内所取得的成就——两个不同的模型提供了相同的提示，但早期模型和后期模型之间的对比非常明显！
- en: '![](../Images/c0322ec3ed14c73845d0dc9300b08599.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0322ec3ed14c73845d0dc9300b08599.png)'
- en: '*Figure 1: Advances in Generative AI Art, images produced by Disco Diffusion
    and Midjourney.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1：生成性人工智能艺术的进展，由Disco Diffusion和Midjourney生成的图像。*'
- en: Fast forward to 2023, Midjourney just released version 5 on March 15th. This
    model has very high Coherency, excels at interpreting natural language prompts,
    is higher resolution, and supports advanced features like repeating patterns[4].
    Stability AI has also released Stable Diffusion 2.1\. The pace of advancement
    for these models moves at a blistering pace!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 快进到2023年，Midjourney于3月15日刚刚发布了第5版。这个模型具有非常高的连贯性，擅长解释自然语言提示，分辨率更高，并支持像重复图案这样的高级功能[4]。Stability
    AI也发布了稳定扩散2.1。这些模型的发展速度非常迅猛！
- en: 'As described in a recent [white paper](https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html)
    published by Deloitte: “It feels like we may only be just beginning to see the
    impact of what Generative AI models are capable of. Although early traction has
    been through consumer releases, which could be era-defining, Generative AI also
    has the potential to add contextual awareness and human-like decision-making to
    nearly all facets of life.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如德勤最近发布的[白皮书](https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html)中描述的：“我们感觉可能只是刚刚开始看到生成性人工智能模型所能带来的影响。虽然早期的吸引力主要来自消费者发布，这可能会成为定义时代的里程碑，但生成性人工智能也有潜力为几乎所有生活领域增加情境意识和类人决策能力。
- en: As such, Generative AI has attracted interest from traditional (e.g., Venture
    Capital (VC), Mergers & Acquisitions (M&A)) and emerging (e.g., ecosystem partnerships)
    sources. In 2022 alone, venture capital firms invested more than $2B, and technology
    leaders made significant investments, such as Microsoft’s $10B stake in OpenAI
    and Google’s $300M stake in Anthropic.”[3]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，生成性人工智能吸引了来自传统（例如风险投资（VC），并购（M&A））和新兴（例如生态系统合作伙伴）来源的兴趣。仅在2022年，风险投资公司就投资了超过20亿美元，科技领袖也进行了重要投资，如微软对OpenAI的100亿美元投资和谷歌对Anthropic的3亿美元投资。”[3]
- en: 🧐 What’s Stable Diffusion?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🧐 什么是稳定扩散？
- en: Stable Diffusion is a method used in AI text-to-image generation that uses a
    diffusion model to create images from text descriptions. The diffusion model starts
    with a random image and then gradually adds noise to it, step by step. The noise
    is added in a controlled way, so that the image remains recognisable.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散是一种用于人工智能文本生成图像的方法，它利用扩散模型从文本描述中创建图像。扩散模型从一张随机图像开始，然后逐步向其中添加噪声。噪声以受控的方式添加，使得图像仍然可以辨认。
- en: 'At each diffusion step, the image becomes more refined, with the details becoming
    clearer. This process continues for several diffusion steps until the generated
    image is considered “stable”, meaning we’ve hit a point where further iterations
    aren’t likely to improve it. The process is shown below:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个扩散步骤中，图像变得更加精细，细节变得更加清晰。这个过程会持续几个扩散步骤，直到生成的图像被认为“稳定”，这意味着我们已经达到了一个进一步迭代不会改善图像的点。过程如下所示：
- en: '![](../Images/ce6279d504e7f24b30e95752aabbdd18.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce6279d504e7f24b30e95752aabbdd18.png)'
- en: '*Figure 2: Process of Stable Diffusion, image credit:* [*Benlisquare*](https://en.wikipedia.org/wiki/Stable_Diffusion#/media/File:X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png)*,
    shared under licence:* [*CC BY-SA 4.0*](http://creativecommons.org/licenses/by-nc/4.0/)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2：Stable Diffusion 过程，图像来源：* [*Benlisquare*](https://en.wikipedia.org/wiki/Stable_Diffusion#/media/File:X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png)*，根据许可共享：*
    [*CC BY-SA 4.0*](http://creativecommons.org/licenses/by-nc/4.0/)'
- en: Diffusion models are trained on a dataset of images and text descriptions, and
    it learns to associate the text descriptions with the images that match them.
    When you give the diffusion model a new text description, it uses its knowledge
    to generate an image that matches the description.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型在一个图像和文本描述的数据集上进行训练，它学会将文本描述与匹配的图像关联起来。当你给扩散模型一个新的文本描述时，它会利用其知识生成一个匹配描述的图像。
- en: The main advantage of Stable Diffusion is that it is very fast. It can generate
    an image from a text description in just a few seconds. This is much faster than
    other methods, such as GANs. Our modern contenders (e.g. Midjourney, DALL E, etc.)
    all use some variant of the Stable Diffusion technique.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion 的主要优势在于它非常快速。它可以在几秒钟内从文本描述生成图像。这比其他方法（如 GANs）快得多。我们的现代竞争者（例如
    Midjourney、DALL E 等）都使用某种变体的 Stable Diffusion 技术。
- en: But Stable Diffusion is not perfect. Although its application has improved in
    leaps and bounds over the past year or so, sometimes the images produced can be
    distorted or lack detail. This is probably because models like these hallucinate
    or “guess” certain details of what an image should look like, based on the data
    they are trained on. As training datasets and model algorithms improve this will
    become less of an issue.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但 Stable Diffusion 并不完美。虽然在过去一年中它的应用有了飞跃式的进步，但有时生成的图像可能会失真或缺乏细节。这可能是因为像这样的模型会根据训练数据来“猜测”图像应该是什么样子。随着训练数据集和模型算法的改进，这个问题将会减少。
- en: 👷🏾‍♀️ Let’s get started!
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 👷🏾‍♀️ 开始吧！
- en: 'In this post I’m going to show you how to take some code to:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将向你展示如何使用一些代码来：
- en: '**Experiment with the Stable Diffusion** model interactively and generate some
    cool art!'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**互动地实验 Stable Diffusion** 模型，生成一些酷炫的艺术作品！'
- en: '**Serve the model via an endpoint** using Vertex AI.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**通过端点服务模型** 使用 Vertex AI。'
- en: '**Create a simple RESTful API** using FLASK.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用 FLASK 创建一个简单的 RESTful API**。'
- en: Sure you could just use an existing, consumer facing model like those we mentioned
    in the previous section, but where would the fun be in that? 😜
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以直接使用我们在前面章节中提到的现有的面向消费者的模型，但那样会没有乐趣啊？😜
- en: ➡️ Feel free to jump to the part you find most interesting!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 随意跳到你感兴趣的部分！
- en: 👾 I’ll pop any code snippets (not already linked) in a github repo, link at
    the end under Useful Resources
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 👾 我会把任何代码片段（如果没有链接的话）放在一个 github 仓库中，链接见最后的有用资源部分
- en: 👩🏻‍💻 1\. Experimenting with Stable Diffusion
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 👩🏻‍💻 1. 实验 Stable Diffusion
- en: When you start working on a new machine learning problem, notebooks provide
    a super agile way to test models and iterate fast. Maybe you like running [Jupyter](https://jupyter.org/)
    in a local environment, using a [Kaggle Kernel](https://www.kaggle.com/code),
    or my personal favourite, [Colab](https://colab.research.google.com/). With tools
    like these, creating and experimenting with machine learning is becoming increasingly
    accessible.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始处理一个新的机器学习问题时，笔记本提供了一种非常灵活的方式来测试模型和快速迭代。也许你喜欢在本地环境中运行 [Jupyter](https://jupyter.org/)，使用
    [Kaggle Kernel](https://www.kaggle.com/code)，或者我个人最喜欢的 [Colab](https://colab.research.google.com/)。有了这些工具，创建和实验机器学习变得越来越可及。
- en: We’re going to create a quick notebook that will introduce you to the Stable
    Diffusion model
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个快速的笔记本，向你介绍 Stable Diffusion 模型
- en: 💨 See the Resources section to download the notebook file if you don’t have
    time (or want) to build it step-by-step!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 💨 如果你没有时间（或不想）一步步构建，可以查看资源部分下载笔记本文件！
- en: '**Prerequisites:**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**前提条件：**'
- en: Google account to sign into Colab and save notebooks
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于登录 Colab 和保存笔记本的 Google 账户
- en: HuggingFace account and an API token (free)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuggingFace 账户和 API 令牌（免费）
- en: 1\. Head over to [https://colab.research.google.com/](https://colab.research.google.com/).
    This will create a new Python notebook for you and save it to your Google Drive.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 访问 [https://colab.research.google.com/](https://colab.research.google.com/)。这将为你创建一个新的
    Python 笔记本，并将其保存到你的 Google Drive 中。
- en: 2\. Colab is made available for free, but you’ll have to pay if you want access
    to more memory and compute resources on which to run your notebook. We’ll just
    use what’s available for free, the first thing you’ll need to do is configure
    the notebook runtime to include a GPU accelerator. Using a GPU will speed up the
    time it takes for the model to generate an image aka. perform inference. The free
    tier provides an NVIDIA Tesla T4 GPU with 12GB of memory (which is just enough
    for what we want to do). From the file menu select *Runtime -> Change Runtime
    Type, and select Hardware accelerator = GPU*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 2. Colab 免费提供，但如果你需要更多的内存和计算资源来运行你的笔记本，你需要付费。我们将只使用免费的资源，首先你需要配置笔记本运行时以包含 GPU
    加速器。使用 GPU 会加快模型生成图像的时间，也就是执行推理。免费层提供了 12GB 内存的 NVIDIA Tesla T4 GPU（这对我们要做的事情正好足够）。从文件菜单中选择
    *运行时 -> 更改运行时类型*，然后选择硬件加速器 = GPU。
- en: '![](../Images/f221b1b327a28f16c52af8d6322c8787.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f221b1b327a28f16c52af8d6322c8787.png)'
- en: '*Figure 3: Colab Notebook, Change notebook runtime.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3：Colab 笔记本，更改笔记本运行时。*'
- en: 'Then you’ll need to connect to your Runtime so we can execute any code we write
    in the notebook. Click on *Connect/Reconnect -> Connect to a hosted runtime*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你需要连接到你的运行时，以便我们可以执行笔记本中编写的任何代码。点击 *连接/重新连接 -> 连接到托管运行时*：
- en: '![](../Images/ba22e38c451cbb8a3098582d01982b16.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba22e38c451cbb8a3098582d01982b16.png)'
- en: '*Figure 4: Colab Notebook, “+ Code” button to add cells, and Connecting to
    a hosted runtime.*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4：Colab 笔记本，“+ 代码”按钮添加单元格，以及连接到托管运行时。*'
- en: '3\. Once you’ve connected to your runtime, it’s time to enter some code! You
    can enter code in a single notebook “cell”, or if you want to execute steps separately,
    using the “+ Code” button to create a new “cell”. I find it useful to break up
    the code into cells so it’s easier to identify and debug issues. Each cell has
    a “play” button when you hover over, click it to run the code. You can run the
    following steps after typing or copy/pasting the code into a new cell:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 一旦你连接到你的运行时，就可以输入一些代码了！你可以在单个笔记本“单元格”中输入代码，或者如果你想单独执行步骤，可以使用“+ 代码”按钮创建一个新的“单元格”。我发现将代码分解成单元格很有用，这样更容易识别和调试问题。每个单元格在你悬停时都有一个“播放”按钮，点击它以运行代码。输入或复制粘贴代码到新的单元格中后，你可以运行以下步骤：
- en: 'a. Because we’re pulling the model from HuggingFace, we need to install some
    python libraries and authenticate using an API token:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: a. 由于我们需要从 HuggingFace 拉取模型，因此需要安装一些 Python 库并使用 API 令牌进行认证：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'b. Next we need to [authenticate using a token from you HuggingFace account](https://huggingface.co/docs/hub/security-tokens#how-to-manage-user-access-tokens),
    when you execute the code below, you will be prompted in your notebook to paste
    in your HuggingFace API token:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: b. 接下来，我们需要 [使用你 HuggingFace 账户的令牌进行认证](https://huggingface.co/docs/hub/security-tokens#how-to-manage-user-access-tokens)，当你执行以下代码时，你会在笔记本中被提示粘贴你的
    HuggingFace API 令牌：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'c. After successfully logging into your HuggingFace account, we’re going download
    the diffusers and transformers python libraries:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: c. 成功登录你的 HuggingFace 账户后，我们将下载 diffusers 和 transformers Python 库：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'd. We need to create a StableDiffusion model pipeline so we can basically pass
    the model some text and have it generate an image based on that prompt. You might
    notice that one of the parameters we’re passing is a path to a Stable Diffusion
    model hosted on HuggingFace. The examples in this post were tested using v1.5,
    you can try swapping for the latest (v2.1) at the time of writing:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: d. 我们需要创建一个 StableDiffusion 模型管道，以便我们可以将模型传递一些文本，并根据该提示生成图像。你可能会注意到我们传递的参数之一是
    HuggingFace 上托管的 Stable Diffusion 模型的路径。此帖子中的示例使用的是 v1.5，你可以尝试在写作时交换为最新的（v2.1）：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'e. Now for the fun bit, we’re going to generate some art! Load the torch python
    library, and then add a cell, replacing the string value for the prompt variable
    with whatever you want! I put in simple example to get you started:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: e. 现在进入有趣的部分，我们将生成一些艺术作品！加载 torch Python 库，然后添加一个单元格，将 prompt 变量的字符串值替换为你想要的任何内容！我提供了一个简单的示例以帮助你入门：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You should end up with something similar to the screenshot below:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到如下所示的结果：
- en: '![](../Images/e98a0eeada649dd901d490466eea6b7c.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e98a0eeada649dd901d490466eea6b7c.png)'
- en: '*Figure 5: Colab Notebook, image output by Stable Diffusion model.*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5：Colab 笔记本，Stable Diffusion 模型生成的图像输出。*'
- en: '**💡Tip:** if you find yourself running into ‘out of memory’ errors, you can
    periodically clear the GPU’s cache by adding this line of code above your text
    prompt:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**💡提示：** 如果你遇到‘内存不足’的错误，你可以通过在文本提示上方添加以下代码行定期清除 GPU 的缓存：'
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With just a few lines of code in a notebook, you can generate art from a text
    based prompt!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 🍦 2\. Serve the model via a Vertex AI Endpoint
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making production applications or training large models requires additional
    tooling to help you scale beyond just code in a notebook, and using a cloud service
    provider can help.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to package the Stable Diffusion model and host it on an endpoint
    that we can use to handle prediction requests from application code.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: You could use another public cloud service provider to achieve approximately
    the same results, but I’m going to go with Google Cloud Platform (GCP) and in
    particular it’s Vertex AI toolset given I’m most familiar with it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '**Prerequisites:**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: GCP account with billing enabled/free starter credits
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have some basic knowledge of GCP admin e.g. how to create a Project, VMs, storage
    buckets and other resources,
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Give a service account permissions via IAM,
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: download a service account key,
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use the Google Cloud SDK + a bit of Python.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1\. We’re going to use a pre-built notebook example: ([https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb))
    from the official repository of GCP Vertex AI samples on Github. **I’m not going
    to regurgitate the whole thing here**, I’ll just explain the most relevant parts
    and any hiccups I had to overcome to actually get it working…code examples are
    never as straightforward as they seem!'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: ⚠️ **Creating the user managed notebook and deploying the model to an endpoint
    in this example will incur costs**. You can get started with $300 of free credits
    when you create a GCP account for the first time, but the recommended hardware
    config (and any subsequent calls to the endpoint where your model is deployed)
    is likely to burn through those credits pretty quickly — you have been warned.
    I’ll share the actual costs I racked up in the Closing Thoughts section.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '2\. If this is the first time you’re using Vertex AI, sign into your GCP console
    and be sure to enable all of the necessary APIs from the Vertex AI dashboard:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5477d0fd316762a0569855edf4e1316.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6: Vertex AI dashboard in GCP, Enable All Recommended APIs button.*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Open the [notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb),
    just below its title, you should see some buttons including “Run in Colab” and
    “**Open in Vertex AI Workbench**”. You want to click the latter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 4\. You’ll then be asked to configure a VM instance on which to host a User
    Managed notebook. The example recommends using an NVIDIA A100 with 85GB RAM instance
    aka. “a2-highgpu-1g”. Inference (creating an image from the example in the notebook)
    was quick, returning an image from the Stable Diffusion model in about 11 seconds.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 然后你需要配置一个 VM 实例来托管用户管理的笔记本。示例建议使用带有 85GB RAM 的 NVIDIA A100 实例，也就是“a2-highgpu-1g”。推理（从笔记本中的示例创建图像）很快，大约
    11 秒钟就能从 Stable Diffusion 模型中返回图像。
- en: 5\. Once your instance has been created you can access it via *Workbench ->
    User Managed Notebooks.*
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 一旦你的实例创建完成，你可以通过 *Workbench -> 用户管理的笔记本* 访问它。
- en: '![](../Images/9f903e4a5efbd63cfc2ec27b1d02f917.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f903e4a5efbd63cfc2ec27b1d02f917.png)'
- en: '*Figure 7: Vertex AI Workbench, User-Managed Notebooks list.*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7：Vertex AI 工作台，用户管理的笔记本列表。*'
- en: Before you dive in and click *OPEN JUPYTERLAB*, make sure you give the Service
    Account for the VM your notebook is running on permissions to do things like create
    storage buckets and endpoints. **This will not be done for you automatically given
    its “User Managed”.**
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在你深入并点击 *OPEN JUPYTERLAB* 之前，确保给你的笔记本所在 VM 的服务账户授予权限，以便进行诸如创建存储桶和端点等操作。**由于它是“用户管理”的，这不会自动为你完成。**
- en: 6\. Click on the notebook name, in the Notebook details you will see the Owner
    or service account alias.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 点击笔记本名称，在笔记本详细信息中你将看到所有者或服务账户别名。
- en: '![](../Images/63111df16dbce5738f2d20cba3cb731a.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63111df16dbce5738f2d20cba3cb731a.png)'
- en: '*Figure 8: Vertex AI, Notebook details.*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8：Vertex AI，笔记本详细信息。*'
- en: 7\. Go to *IAM & Admin -> IAM, then c*lick *Grant Access*, paste or type in
    the service account alias for your notebook instance and to make life simple I
    gave it blanket “Editor” access. If you’re finicky, you can grant only the specific
    permissions needed by the steps in the dreambooth_diffusion example.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 7\. 转到 *IAM & Admin -> IAM*，然后点击 *授予访问权限*，粘贴或输入你的笔记本实例的服务账户别名，为了简化操作，我授予了“编辑者”访问权限。如果你很挑剔，你可以只授予
    `dreambooth_diffusion` 示例步骤所需的特定权限。
- en: '![](../Images/a28ceee2caddfcc72ef7537fb5b7a97d.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a28ceee2caddfcc72ef7537fb5b7a97d.png)'
- en: '*Figure 9: GCP IAM & Admin, IAM, Edit access/assign roles.*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9：GCP IAM & Admin, IAM, 编辑访问/分配角色。*'
- en: '8\. Now you should be ready to start running the dreambooth_diffusion.ipynb
    notebook code in Jupyterlab, open it from the Workbench (as shown in Step 5.).
    For whatever reason, the code from the example’s git repo was not copied to my
    notebook instance so I just opened a terminal and did a quick clone of the Github
    repo:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 8\. 现在你应该准备好在 Jupyterlab 中运行 `dreambooth_diffusion.ipynb` 笔记本代码了，从工作台中打开它（如步骤
    5 所示）。由于某种原因，示例的 git 仓库中的代码没有被复制到我的笔记本实例中，所以我只是打开了终端并快速克隆了 GitHub 仓库：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 9\. Once you have opened the dreambooth_diffusion.ipynb in Jupyterlab, you should
    be able to run the notebook cells without any major issues. The first part of
    the notebook runs through the steps to download the Stable Diffusion model and
    create an image from a prompt. The next step is to create a Vertex AI Endpoint
    and deploy the model there for serving.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 9\. 一旦你在 Jupyterlab 中打开了 `dreambooth_diffusion.ipynb`，你应该能够运行笔记本单元而不会遇到重大问题。笔记本的第一部分是通过步骤下载
    Stable Diffusion 模型并从提示中创建图像。下一步是创建 Vertex AI 端点并将模型部署到那里进行服务。
- en: '10\. Follow the steps in the notebook to:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 10\. 按照笔记本中的步骤进行：
- en: a. Create a custom TorchServe handler.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: a. 创建自定义 TorchServe 处理程序。
- en: b. Upload the model artifacts onto Google Cloud Storage.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: b. 将模型工件上传到 Google Cloud Storage。
- en: c. Create a Vertex AI model with the model artifacts and a prebuilt PyTorch
    container image.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: c. 使用模型工件和预构建的 PyTorch 容器镜像创建 Vertex AI 模型。
- en: d. Deploy the Vertex AI model onto an endpoint.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: d. 将 Vertex AI 模型部署到端点上。
- en: 'This should all go smoothly, provided you enabled the Vertex API earlier (see
    Step 2\. If you forgot!). For me, after creating the endpoint, the model deployment
    took about 30 minutes. When it’s ready to serve, you will see something like this
    under *Endpoints*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 只要你在之前启用了 Vertex API（参见步骤 2。如果你忘记了！），这一切应该都会顺利进行。对我而言，在创建端点之后，模型部署大约花费了 30 分钟。当它准备好提供服务时，你会在
    *端点* 下看到类似的内容：
- en: '![](../Images/c7daecac9d9e3b07f85e43677b8db55e.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7daecac9d9e3b07f85e43677b8db55e.png)'
- en: '*Figure 10: Vertex AI Endpoints.*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10：Vertex AI 端点。*'
- en: You are now ready to serve requests from your Stable Diffusion model!! At this
    point you can stop the notebook instance VM and delete the bucket you created
    if you want to save on costs. See the “Cleaning up” steps at the very end of dreambooth_diffusion.ipynb.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在准备好从你的 Stable Diffusion 模型中处理请求了！！此时，如果你想节省成本，可以停止笔记本实例 VM 并删除你创建的存储桶。请参见
    `dreambooth_diffusion.ipynb` 最后部分的“清理”步骤。
- en: 🧪 Test your Endpoint
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 🧪 测试你的端点
- en: To send a request to a Vertex AI endpoint, you will need to use an HTTP client
    library or a command-line tool that supports sending requests with the appropriate
    HTTP method and request parameters.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要向 Vertex AI 端点发送请求，你需要使用支持发送具有适当 HTTP 方法和请求参数的请求的 HTTP 客户端库或命令行工具。
- en: I carried out testing locally on my laptop. To do this you will need to download
    and install the [Vertex AI SDK for Python](https://cloud.google.com/python/docs/reference/aiplatform/latest/index.html#endpoints),
    then create and download a service key for authentication.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我在我的笔记本电脑上进行了本地测试。为此，你需要下载并安装 [Vertex AI Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest/index.html#endpoints)，然后创建并下载一个服务密钥用于认证。
- en: If you kept your notebook instance VM, you could use the same service account
    alias as before or just create a new one with permissions to, at a minimum, [get
    predictions from an endpoint](https://cloud.google.com/vertex-ai/docs/general/iam-permissions).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你保留了你的笔记本实例虚拟机，你可以使用之前的服务帐户别名，或者只需创建一个新的服务帐户，其权限至少包括[从端点获取预测](https://cloud.google.com/vertex-ai/docs/general/iam-permissions)。
- en: '1\. Go to *IAM & Admin -> Service Accounts*. Then click on the three dots under
    Action on the right of the service account alias you want to create keys for and
    then click *Manage Keys*:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 转到 *IAM & Admin -> 服务帐户*。然后点击服务帐户别名右侧操作下的三个点，然后点击 *管理密钥*：
- en: '![](../Images/855a1d349360219d1836d9c84000eb22.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/855a1d349360219d1836d9c84000eb22.png)'
- en: '*Figure 11: GCP IAM & Admin, Service accounts, manage keys for a service account.*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11：GCP IAM & Admin，服务帐户，管理服务帐户的密钥。*'
- en: 'then click *Add Key -> Create new Key* and download as in the recommended .JSON
    format:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后点击*添加密钥 -> 创建新密钥*，并按推荐的 .JSON 格式下载：
- en: '![](../Images/658dd6110b2eb15cc1c7f0f98d6f40c5.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/658dd6110b2eb15cc1c7f0f98d6f40c5.png)'
- en: '*Figure 12: GCP IAM & Admin, Service accounts, create key for a service account.*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12：GCP IAM & Admin，服务帐户，为服务帐户创建密钥。*'
- en: ⚠️ **Remember, a service account key file grants the same permissions in your
    GCP project as the service account itself.** Always be very careful with the file,
    delete it when it’s no longer required and never, ever, upload it to a Github
    repo, I’ve seen this happen way too many times!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️ **请记住，服务帐户密钥文件授予与你的 GCP 项目中服务帐户相同的权限。** 始终非常小心处理该文件，删除它时不要再使用，并且永远不要将其上传到
    Github 仓库，我见过太多这种情况！
- en: '2\. In a terminal window, or from wherever you’re going to run your code to
    test your endpoint, you’ll want to set a variable to point to the service key
    you downloaded. This will authenticate requests to your endpoint:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 在终端窗口中，或在你运行代码以测试端点的地方，你需要设置一个变量以指向你下载的服务密钥。这将对端点请求进行认证：
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '3\. Now just modify this python code snippet with your Project Name, Region,
    and Endpoint ID. It will pass a prompt to your endpoint and store the response
    i.e. image generated by Stable Diffusion in a JPEG file:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 现在只需用你的项目名称、区域和端点 ID 修改这个 Python 代码片段。它将向你的端点传递一个提示，并将响应（即由 Stable Diffusion
    生成的图像）存储在 JPEG 文件中：
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you’ve installed the Vertex AI for Python SDK, authenticated to GCP, and
    the endpoint is active, after a few seconds you should see the generated image
    file appear in your file system!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你安装了 Vertex AI Python SDK、进行了 GCP 认证，并且端点处于活动状态，几秒钟后你应该会看到生成的图像文件出现在你的文件系统中！
- en: 🎁 3\. Create a simple RESTful API using FLASK
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🎁 3\. 使用 FLASK 创建一个简单的 RESTful API
- en: At this point you could quite easily integrate the code to call your generative
    AI model endpoint with an existing app using the Vertex AI Python SDK.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可以很容易地将调用生成 AI 模型端点的代码集成到使用 Vertex AI Python SDK 的现有应用程序中。
- en: But, I did promise an API, so that is what I’m going to run through in this
    final part of the section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 但我确实承诺了一个 API，所以我将在这一部分的最后部分详细介绍它。
- en: '**Prerequisites:**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**前提条件：**'
- en: Install [Flask](https://pypi.org/project/Flask/) and [Pillow](https://pypi.org/project/Pillow/)
    python libraries.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 [Flask](https://pypi.org/project/Flask/) 和 [Pillow](https://pypi.org/project/Pillow/)
    Python 库。
- en: '[Download Postman](https://www.postman.com/downloads/?utm_source=postman-home)
    and install, it’s free! We’ll use it to simulate calls to our API.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[下载 Postman](https://www.postman.com/downloads/?utm_source=postman-home) 并安装，它是免费的！我们将用它来模拟对我们的
    API 的调用。'
- en: 1\. As in the previous section, make sure you have an environment variable from
    where you plan to run your code, that points to your service account key.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 与前一节一样，确保你有一个环境变量，指向你计划运行代码的地方，这个变量指向你的服务帐户密钥。
- en: '2\. Here is the code you need to create a simple RESTful API using the Flask
    web application framework. See the comments in the code for an explanation of
    what’s going on. We basically used the code from earlier to query the endpoint
    and wrapped it in an API call:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 这是你需要的代码，用于使用 Flask Web 应用程序框架创建一个简单的 RESTful API。请参见代码中的注释，了解正在发生的事情。我们基本上使用了之前的代码来查询端点，并将其封装在
    API 调用中：
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '3\. Run the resulting file, and you should have a Flask server hosting your
    API, ready to receive requests:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 运行生成的文件，你应该会有一个托管你 API 的 Flask 服务器，准备接收请求：
- en: '![](../Images/7a83c0c778babc2f9b0881ebd22530da.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a83c0c778babc2f9b0881ebd22530da.png)'
- en: '*Figure 13: MacOS terminal, running Flask server locally.*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13：MacOS 终端，本地运行 Flask 服务器。*'
- en: '4\. Next we’re going to fire up Postman, go to *File -> Import*, paste in the
    following cURL command (you might need to modify the server address if you have
    it configured differently to the default):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 接下来我们将启动 Postman，进入 *文件 -> 导入*，粘贴以下 cURL 命令（如果你的服务器地址配置与默认不同，你可能需要修改它）：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We could run this from a terminal and the response from our API would be saved
    as a JPEG image in our local file system. But to simulate an app, and because
    I didn’t have time to write a Discord bot or HTML front end… 😅
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从终端运行这个，API 的响应会被保存为本地文件系统中的 JPEG 图像。但为了模拟一个应用程序，而且因为我没有时间编写 Discord 机器人或
    HTML 前端… 😅
- en: '![](../Images/1ab9d69c5b3463030296060bc70e88b2.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ab9d69c5b3463030296060bc70e88b2.png)'
- en: '*Figure 14: Postman, API request example, image response generated by Stable
    Diffusion model.*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14：Postman，API 请求示例，Stable Diffusion 模型生成的图像响应。*'
- en: If all has gone well, you’ll end up with a nicely generated image, served from
    your very own API. You can modify the prompt under the Body tab, creativity is
    your only limit!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你将得到一个漂亮生成的图像，通过你自己的 API 提供服务。你可以在“Body”标签下修改提示，创造力是唯一的限制！
- en: '🗑️ **Done experimenting?** You can go back to *Vertex AI -> Endpoints*, select
    the endpoint, un-deploy the model, go back up a level to the endpoint and delete
    it (hint: triple dots menu at the end of each line item). Double check you spun
    down or deleted anything else related to this post so you’re not consuming resources.
    You can also revoke the service account key you created just to be safe.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 🗑️ **实验完成了吗？** 你可以返回到 *Vertex AI -> 端点*，选择端点，撤销模型部署，返回上一层删除它（提示：每行末尾的三点菜单）。仔细检查你是否关闭或删除了与这篇文章相关的任何其他内容，以避免消耗资源。你也可以撤销你创建的服务账户密钥，以确保安全。
- en: ⚠️ **Final warning —** this is clearly not production ready code. If you were
    to make your API public facing there’s work to be done around authentication,
    amongst other things that are out of scope for this post! Relatively speaking
    the cost per request (or compute hours) for our endpoint is also much more expensive
    than say Midjourney or DALL-E 2, so probably not a viable thing for you to launch
    as a service.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️ **最后警告 —** 这显然不是生产就绪的代码。如果你打算让你的 API 对外公开，还有很多工作需要做，比如认证等，这些超出了本文的范围！相对而言，我们的端点每次请求的成本（或计算时间）也比
    Midjourney 或 DALL-E 2 高得多，因此可能不适合作为服务上线。
- en: 🏆 Wrap up and closing thoughts
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🏆 总结和结束思考
- en: I had **a lot** of fun writing this post, and learnt loads about the generative
    AI space, Stable Diffusion and what it means to package it up into something loosely
    resembling one of the popular consumer facing services available today. I salute
    those dev teams on the bleeding edge of this technology, it’s such an exciting
    space to be in at the moment!
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我在写这篇文章时**非常**开心，并且学到了很多关于生成式 AI、Stable Diffusion 以及将其打包成类似今天流行的消费者服务的内容。我向那些站在这项技术最前沿的开发团队致敬，这确实是一个令人兴奋的领域！
- en: 💸 Costs
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💸 成本
- en: I was a little nervous about running large GPU attached instances. If you’re
    a long time public cloud user, you will know how easy it is to overspend without
    the right checks and balances in place.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我有点担心运行大型 GPU 附加实例。如果你是长期使用公共云的用户，你会知道没有适当的检查和制衡机制，很容易超支。
- en: At the end of writing this post, I had a look at the Billing section in my GCP
    console to see what the damage was…
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在写完这篇文章的最后，我查看了 GCP 控制台中的账单部分，看看花了多少钱…
- en: '![](../Images/f0e6619fbe9fabd43cfb448f83477b81.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0e6619fbe9fabd43cfb448f83477b81.png)'
- en: '*Figure 15: GCP Billing, Reports.*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15：GCP 账单，报告。*'
- en: During the two days that I was using GCP, with a project created exclusively
    for this post, I spent about USD $40\. You can see the breakdown in the screenshot
    above.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 GCP 的两天里，专门为这篇文章创建的项目中，我花费了大约 40 美元。你可以在上面的截图中看到详细分解。
- en: Most of the cost was related to inference (see the Vertex AI service line item).
    This is the cost in ‘compute hours’ for making my endpoint available to generate
    images using the Stable Diffusion model I deployed there.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分费用与推理相关（参见 Vertex AI 服务条目）。这是使我的端点能够使用我部署的 Stable Diffusion 模型生成图像的“计算小时”费用。
- en: The endpoint used the “Vertex AI:Online/Batch Prediction NVIDIA Tesla P100 GPU
    running in Americas for AI Platform SKU”, with some other costs for running a
    Predefined instance. We could have trimmed the fat by choosing a cheaper GPU family,
    but the trade off would be an increased time period to get back a generated image.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 端点使用了“Vertex AI:在线/批处理预测 NVIDIA Tesla P100 GPU 运行于美洲的 AI 平台 SKU”，还包括运行预定义实例的一些其他费用。我们本可以通过选择更便宜的
    GPU 家族来减少开支，但这将导致生成图像的时间延长。
- en: Comparing this to a commercially available solution, the Midjourney [basic plan](https://docs.midjourney.com/docs/plans)
    costs USD $10 on a monthly subscription. For that you get 3.3 fast GPU hrs/month,
    then it costs another $4/hr beyond that. In our case, I left the endpoint up for
    about 24 hours and spent $30 with no limitations on concurrency or jobs waiting.
    Again there’s a tradeoff between a fully managed service like Midjourney whose
    engineers are always making improvements to the model vs. iterating and deploying
    your own models on scalable cloud infrastructure.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 与商业上可用的解决方案相比，Midjourney 的 [基础计划](https://docs.midjourney.com/docs/plans) 每月订阅费用为
    10 美元。这个计划包括每月 3.3 小时的快速 GPU 计算，超出部分费用为每小时 4 美元。在我们的案例中，我让端点运行了大约 24 小时，花费了 30
    美元，没有并发或作业等待的限制。再次说明，与 Midjourney 这样的完全托管服务相比，工程师们始终在改进模型，而选择在可扩展的云基础设施上迭代和部署自己的模型之间存在权衡。
- en: '**The important point to note here is that you’re not paying per prediction
    request, but for how long your endpoint is up and the size/type of instance it
    runs on.**'
  id: totrans-154
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**这里需要注意的重要一点是，你支付的不是每次预测请求的费用，而是你的端点运行的时间以及它所运行的实例的大小/类型。**'
- en: The cost to test the model and actually deploy the endpoint via a User Managed
    notebook came in at just under USD $10 (see Compute Engine & Notebooks line items).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 测试模型和通过用户管理的笔记本实际部署端点的费用不到 10 美元（参见计算引擎和笔记本条目）。
- en: So there you have it, in this post I gave a brief overview of generative AI
    and in particular its use for creating art using the Stable Diffusion technique.
    We then dived into some code examples to demonstrate the steps needed to generate
    some art using the Stable Diffusion model and indeed how to deploy it to an endpoint
    and consume it via an API. **I hope you enjoyed this post, and I’ll see you in
    the next!** 👋🏼
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，本文简要概述了生成式人工智能，特别是使用 Stable Diffusion 技术创建艺术的应用。接着，我们深入探讨了一些代码示例，以展示使用 Stable
    Diffusion 模型生成艺术的步骤，以及如何将其部署到端点并通过 API 使用。**希望你喜欢这篇文章，我们下次见！** 👋🏼
- en: The figures in this post were created by the author, unless otherwise stated.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中的数据由作者创建，除非另有说明。
- en: 📇 References
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📇 参考资料
- en: '[1] Machine Learning, History and relationships to other fields: [https://en.wikipedia.org/wiki/Machine_learning#History_and_relationships_to_other_fields](https://en.wikipedia.org/wiki/Machine_learning#History_and_relationships_to_other_fields)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 机器学习，历史及其与其他领域的关系: [https://en.wikipedia.org/wiki/Machine_learning#History_and_relationships_to_other_fields](https://en.wikipedia.org/wiki/Machine_learning#History_and_relationships_to_other_fields)'
- en: '[2] What is generative AI? Everything you need to know, George Lawton: [https://www.techtarget.com/searchenterpriseai/definition/generative-AI](https://www.techtarget.com/searchenterpriseai/definition/generative-AI)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 什么是生成式人工智能？你需要知道的一切，George Lawton: [https://www.techtarget.com/searchenterpriseai/definition/generative-AI](https://www.techtarget.com/searchenterpriseai/definition/generative-AI)'
- en: '[3] The implications of Generative AI for businesses — A new frontier in Artificial
    Intelligence, Deloitte LLP: [https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html](https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 生成式人工智能对企业的影响——人工智能的新前沿，Deloitte LLP: [https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html](https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html)'
- en: '[4] Midjourney docs, Version: [https://docs.midjourney.com/docs/model-versions#:~:text=Current%20Model,places%2C%20objects%2C%20and%20more](https://docs.midjourney.com/docs/model-versions#:~:text=Current%20Model,places%2C%20objects%2C%20and%20more).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Midjourney 文档，版本: [https://docs.midjourney.com/docs/model-versions#:~:text=Current%20Model,places%2C%20objects%2C%20and%20more](https://docs.midjourney.com/docs/model-versions#:~:text=Current%20Model,places%2C%20objects%2C%20and%20more)。'
- en: 📚 Useful Resources
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📚 有用资源
- en: 'You can find the completed notebook (StableDiffusion_Hugging_Face.ipynb) from
    “1\. Experimenting with Stable Diffusion” and python code from “2\. Serve the
    model via an API -> endpoint” here: [https://github.com/omermx/medium_text_to_image_api](https://github.com/omermx/medium_text_to_image_api)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以从“1. 实验稳定扩散”和“2. 通过 API 提供模型 -> 端点”中找到完整的笔记本（StableDiffusion_Hugging_Face.ipynb）和
    Python 代码，链接如下：[https://github.com/omermx/medium_text_to_image_api](https://github.com/omermx/medium_text_to_image_api)
- en: 'You can swap out the version Stable Diffusion I used in my examples (1.5) with
    the latest at the time of writing (2.1), here on Hugging Face: [https://huggingface.co/stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将我在示例中使用的稳定扩散版本（1.5）替换为写作时的最新版本（2.1），请访问 Hugging Face：[https://huggingface.co/stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)
