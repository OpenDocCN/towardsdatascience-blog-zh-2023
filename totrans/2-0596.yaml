- en: Create your own Generative AI Text-to-Image API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/create-your-own-generative-ai-text-to-image-api-548c07a4d839](https://towardsdatascience.com/create-your-own-generative-ai-text-to-image-api-548c07a4d839)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Turn your ramblings into masterpieces, on demand
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)[![Omer
    Mahmood](../Images/0c87da4134bea397c77bc4ba6640e34b.png)](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)[](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)
    [Omer Mahmood](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)
    ¬∑17 min read¬∑Apr 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/738244045935fe00f21f0a7c31c3f546.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using Midjourney on General Commercial Terms.
  prefs: []
  type: TYPE_NORMAL
- en: The TL;DR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recent advances in Generative AI have led to the launch of a whole host of services
    such as DALL-E 2, Midjourney and Stability AI that have the potential to drastically
    change the way we approach content creation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this post I show you how to build and serve your very own, high performance,
    text-to-image service over an API. Based on [Stable Diffusion](https://github.com/CompVis/stable-diffusion)
    via [HuggingFace](https://medium.com/towards-data-science/whats-hugging-face-122f4e7eb11a),
    using Vertex AI Workbench and Endpoints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üö£üèº How we got here
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As George Lawton mentions in his [article](https://www.techtarget.com/searchenterpriseai/definition/generative-AI):
    ‚ÄúGenerative AI is a type of artificial intelligence technology that can produce
    various types of content including text, imagery, audio and synthetic data. The
    recent buzz around generative AI has been driven by the simplicity of new user
    interfaces for creating high-quality text, graphics and videos in a matter of
    seconds.‚Äù[2]'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning is nothing new, in fact it‚Äôs been around in some shape or form
    since the 1960s[1]. ‚ÄúBut it was not until 2014, with the introduction of [generative
    adversarial networks](https://en.wikipedia.org/wiki/Generative_adversarial_network)
    (GANs), a type of machine learning algorithm, that generative AI could create
    convincingly authentic images, videos and audio of real people.‚Äù[2]
  prefs: []
  type: TYPE_NORMAL
- en: Combined with the power of Large Language Models (LLMs) that can take a user
    prompt in natural language describing something and then produce photorealistic
    images, we‚Äôve come a very long way in a short period of time. The first to do
    this was [OpenAI‚Äôs DALL¬∑E](https://openai.com/product/dall-e-2), in April 2022,
    followed by Disco Diffusion in August 2022, which was eventually succeeded by
    Stable Diffusion. In parallel to all these offerings, there is a company called
    [Midjourney](https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F) that has
    built a very popular model that is used by interacting with a Discord bot.
  prefs: []
  type: TYPE_NORMAL
- en: Since then progress made in the state of the art has been quite astounding.
    The screenshot below shows what was accomplished in just the space of a few months
    ‚Äî two different models were provided with the same prompt, but the contrast between
    early and later models is stark!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0322ec3ed14c73845d0dc9300b08599.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1: Advances in Generative AI Art, images produced by Disco Diffusion
    and Midjourney.*'
  prefs: []
  type: TYPE_NORMAL
- en: Fast forward to 2023, Midjourney just released version 5 on March 15th. This
    model has very high Coherency, excels at interpreting natural language prompts,
    is higher resolution, and supports advanced features like repeating patterns[4].
    Stability AI has also released Stable Diffusion 2.1\. The pace of advancement
    for these models moves at a blistering pace!
  prefs: []
  type: TYPE_NORMAL
- en: 'As described in a recent [white paper](https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html)
    published by Deloitte: ‚ÄúIt feels like we may only be just beginning to see the
    impact of what Generative AI models are capable of. Although early traction has
    been through consumer releases, which could be era-defining, Generative AI also
    has the potential to add contextual awareness and human-like decision-making to
    nearly all facets of life.'
  prefs: []
  type: TYPE_NORMAL
- en: As such, Generative AI has attracted interest from traditional (e.g., Venture
    Capital (VC), Mergers & Acquisitions (M&A)) and emerging (e.g., ecosystem partnerships)
    sources. In 2022 alone, venture capital firms invested more than $2B, and technology
    leaders made significant investments, such as Microsoft‚Äôs $10B stake in OpenAI
    and Google‚Äôs $300M stake in Anthropic.‚Äù[3]
  prefs: []
  type: TYPE_NORMAL
- en: üßê What‚Äôs Stable Diffusion?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stable Diffusion is a method used in AI text-to-image generation that uses a
    diffusion model to create images from text descriptions. The diffusion model starts
    with a random image and then gradually adds noise to it, step by step. The noise
    is added in a controlled way, so that the image remains recognisable.
  prefs: []
  type: TYPE_NORMAL
- en: 'At each diffusion step, the image becomes more refined, with the details becoming
    clearer. This process continues for several diffusion steps until the generated
    image is considered ‚Äústable‚Äù, meaning we‚Äôve hit a point where further iterations
    aren‚Äôt likely to improve it. The process is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce6279d504e7f24b30e95752aabbdd18.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2: Process of Stable Diffusion, image credit:* [*Benlisquare*](https://en.wikipedia.org/wiki/Stable_Diffusion#/media/File:X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png)*,
    shared under licence:* [*CC BY-SA 4.0*](http://creativecommons.org/licenses/by-nc/4.0/)'
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models are trained on a dataset of images and text descriptions, and
    it learns to associate the text descriptions with the images that match them.
    When you give the diffusion model a new text description, it uses its knowledge
    to generate an image that matches the description.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of Stable Diffusion is that it is very fast. It can generate
    an image from a text description in just a few seconds. This is much faster than
    other methods, such as GANs. Our modern contenders (e.g. Midjourney, DALL E, etc.)
    all use some variant of the Stable Diffusion technique.
  prefs: []
  type: TYPE_NORMAL
- en: But Stable Diffusion is not perfect. Although its application has improved in
    leaps and bounds over the past year or so, sometimes the images produced can be
    distorted or lack detail. This is probably because models like these hallucinate
    or ‚Äúguess‚Äù certain details of what an image should look like, based on the data
    they are trained on. As training datasets and model algorithms improve this will
    become less of an issue.
  prefs: []
  type: TYPE_NORMAL
- en: üë∑üèæ‚Äç‚ôÄÔ∏è Let‚Äôs get started!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this post I‚Äôm going to show you how to take some code to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment with the Stable Diffusion** model interactively and generate some
    cool art!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Serve the model via an endpoint** using Vertex AI.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create a simple RESTful API** using FLASK.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sure you could just use an existing, consumer facing model like those we mentioned
    in the previous section, but where would the fun be in that? üòú
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è Feel free to jump to the part you find most interesting!
  prefs: []
  type: TYPE_NORMAL
- en: üëæ I‚Äôll pop any code snippets (not already linked) in a github repo, link at
    the end under Useful Resources
  prefs: []
  type: TYPE_NORMAL
- en: üë©üèª‚Äçüíª 1\. Experimenting with Stable Diffusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you start working on a new machine learning problem, notebooks provide
    a super agile way to test models and iterate fast. Maybe you like running [Jupyter](https://jupyter.org/)
    in a local environment, using a [Kaggle Kernel](https://www.kaggle.com/code),
    or my personal favourite, [Colab](https://colab.research.google.com/). With tools
    like these, creating and experimenting with machine learning is becoming increasingly
    accessible.
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôre going to create a quick notebook that will introduce you to the Stable
    Diffusion model
  prefs: []
  type: TYPE_NORMAL
- en: üí® See the Resources section to download the notebook file if you don‚Äôt have
    time (or want) to build it step-by-step!
  prefs: []
  type: TYPE_NORMAL
- en: '**Prerequisites:**'
  prefs: []
  type: TYPE_NORMAL
- en: Google account to sign into Colab and save notebooks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HuggingFace account and an API token (free)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1\. Head over to [https://colab.research.google.com/](https://colab.research.google.com/).
    This will create a new Python notebook for you and save it to your Google Drive.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Colab is made available for free, but you‚Äôll have to pay if you want access
    to more memory and compute resources on which to run your notebook. We‚Äôll just
    use what‚Äôs available for free, the first thing you‚Äôll need to do is configure
    the notebook runtime to include a GPU accelerator. Using a GPU will speed up the
    time it takes for the model to generate an image aka. perform inference. The free
    tier provides an NVIDIA Tesla T4 GPU with 12GB of memory (which is just enough
    for what we want to do). From the file menu select *Runtime -> Change Runtime
    Type, and select Hardware accelerator = GPU*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f221b1b327a28f16c52af8d6322c8787.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3: Colab Notebook, Change notebook runtime.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then you‚Äôll need to connect to your Runtime so we can execute any code we write
    in the notebook. Click on *Connect/Reconnect -> Connect to a hosted runtime*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba22e38c451cbb8a3098582d01982b16.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4: Colab Notebook, ‚Äú+ Code‚Äù button to add cells, and Connecting to
    a hosted runtime.*'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Once you‚Äôve connected to your runtime, it‚Äôs time to enter some code! You
    can enter code in a single notebook ‚Äúcell‚Äù, or if you want to execute steps separately,
    using the ‚Äú+ Code‚Äù button to create a new ‚Äúcell‚Äù. I find it useful to break up
    the code into cells so it‚Äôs easier to identify and debug issues. Each cell has
    a ‚Äúplay‚Äù button when you hover over, click it to run the code. You can run the
    following steps after typing or copy/pasting the code into a new cell:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a. Because we‚Äôre pulling the model from HuggingFace, we need to install some
    python libraries and authenticate using an API token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'b. Next we need to [authenticate using a token from you HuggingFace account](https://huggingface.co/docs/hub/security-tokens#how-to-manage-user-access-tokens),
    when you execute the code below, you will be prompted in your notebook to paste
    in your HuggingFace API token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'c. After successfully logging into your HuggingFace account, we‚Äôre going download
    the diffusers and transformers python libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'd. We need to create a StableDiffusion model pipeline so we can basically pass
    the model some text and have it generate an image based on that prompt. You might
    notice that one of the parameters we‚Äôre passing is a path to a Stable Diffusion
    model hosted on HuggingFace. The examples in this post were tested using v1.5,
    you can try swapping for the latest (v2.1) at the time of writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'e. Now for the fun bit, we‚Äôre going to generate some art! Load the torch python
    library, and then add a cell, replacing the string value for the prompt variable
    with whatever you want! I put in simple example to get you started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You should end up with something similar to the screenshot below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e98a0eeada649dd901d490466eea6b7c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5: Colab Notebook, image output by Stable Diffusion model.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**üí°Tip:** if you find yourself running into ‚Äòout of memory‚Äô errors, you can
    periodically clear the GPU‚Äôs cache by adding this line of code above your text
    prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: With just a few lines of code in a notebook, you can generate art from a text
    based prompt!
  prefs: []
  type: TYPE_NORMAL
- en: üç¶ 2\. Serve the model via a Vertex AI Endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making production applications or training large models requires additional
    tooling to help you scale beyond just code in a notebook, and using a cloud service
    provider can help.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to package the Stable Diffusion model and host it on an endpoint
    that we can use to handle prediction requests from application code.
  prefs: []
  type: TYPE_NORMAL
- en: You could use another public cloud service provider to achieve approximately
    the same results, but I‚Äôm going to go with Google Cloud Platform (GCP) and in
    particular it‚Äôs Vertex AI toolset given I‚Äôm most familiar with it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prerequisites:**'
  prefs: []
  type: TYPE_NORMAL
- en: GCP account with billing enabled/free starter credits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have some basic knowledge of GCP admin e.g. how to create a Project, VMs, storage
    buckets and other resources,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Give a service account permissions via IAM,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: download a service account key,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use the Google Cloud SDK + a bit of Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1\. We‚Äôre going to use a pre-built notebook example: ([https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb))
    from the official repository of GCP Vertex AI samples on Github. **I‚Äôm not going
    to regurgitate the whole thing here**, I‚Äôll just explain the most relevant parts
    and any hiccups I had to overcome to actually get it working‚Ä¶code examples are
    never as straightforward as they seem!'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ö†Ô∏è **Creating the user managed notebook and deploying the model to an endpoint
    in this example will incur costs**. You can get started with $300 of free credits
    when you create a GCP account for the first time, but the recommended hardware
    config (and any subsequent calls to the endpoint where your model is deployed)
    is likely to burn through those credits pretty quickly ‚Äî you have been warned.
    I‚Äôll share the actual costs I racked up in the Closing Thoughts section.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. If this is the first time you‚Äôre using Vertex AI, sign into your GCP console
    and be sure to enable all of the necessary APIs from the Vertex AI dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5477d0fd316762a0569855edf4e1316.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6: Vertex AI dashboard in GCP, Enable All Recommended APIs button.*'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Open the [notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb),
    just below its title, you should see some buttons including ‚ÄúRun in Colab‚Äù and
    ‚Äú**Open in Vertex AI Workbench**‚Äù. You want to click the latter.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. You‚Äôll then be asked to configure a VM instance on which to host a User
    Managed notebook. The example recommends using an NVIDIA A100 with 85GB RAM instance
    aka. ‚Äúa2-highgpu-1g‚Äù. Inference (creating an image from the example in the notebook)
    was quick, returning an image from the Stable Diffusion model in about 11 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Once your instance has been created you can access it via *Workbench ->
    User Managed Notebooks.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f903e4a5efbd63cfc2ec27b1d02f917.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7: Vertex AI Workbench, User-Managed Notebooks list.*'
  prefs: []
  type: TYPE_NORMAL
- en: Before you dive in and click *OPEN JUPYTERLAB*, make sure you give the Service
    Account for the VM your notebook is running on permissions to do things like create
    storage buckets and endpoints. **This will not be done for you automatically given
    its ‚ÄúUser Managed‚Äù.**
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Click on the notebook name, in the Notebook details you will see the Owner
    or service account alias.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63111df16dbce5738f2d20cba3cb731a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8: Vertex AI, Notebook details.*'
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Go to *IAM & Admin -> IAM, then c*lick *Grant Access*, paste or type in
    the service account alias for your notebook instance and to make life simple I
    gave it blanket ‚ÄúEditor‚Äù access. If you‚Äôre finicky, you can grant only the specific
    permissions needed by the steps in the dreambooth_diffusion example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a28ceee2caddfcc72ef7537fb5b7a97d.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9: GCP IAM & Admin, IAM, Edit access/assign roles.*'
  prefs: []
  type: TYPE_NORMAL
- en: '8\. Now you should be ready to start running the dreambooth_diffusion.ipynb
    notebook code in Jupyterlab, open it from the Workbench (as shown in Step 5.).
    For whatever reason, the code from the example‚Äôs git repo was not copied to my
    notebook instance so I just opened a terminal and did a quick clone of the Github
    repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 9\. Once you have opened the dreambooth_diffusion.ipynb in Jupyterlab, you should
    be able to run the notebook cells without any major issues. The first part of
    the notebook runs through the steps to download the Stable Diffusion model and
    create an image from a prompt. The next step is to create a Vertex AI Endpoint
    and deploy the model there for serving.
  prefs: []
  type: TYPE_NORMAL
- en: '10\. Follow the steps in the notebook to:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Create a custom TorchServe handler.
  prefs: []
  type: TYPE_NORMAL
- en: b. Upload the model artifacts onto Google Cloud Storage.
  prefs: []
  type: TYPE_NORMAL
- en: c. Create a Vertex AI model with the model artifacts and a prebuilt PyTorch
    container image.
  prefs: []
  type: TYPE_NORMAL
- en: d. Deploy the Vertex AI model onto an endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'This should all go smoothly, provided you enabled the Vertex API earlier (see
    Step 2\. If you forgot!). For me, after creating the endpoint, the model deployment
    took about 30 minutes. When it‚Äôs ready to serve, you will see something like this
    under *Endpoints*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7daecac9d9e3b07f85e43677b8db55e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10: Vertex AI Endpoints.*'
  prefs: []
  type: TYPE_NORMAL
- en: You are now ready to serve requests from your Stable Diffusion model!! At this
    point you can stop the notebook instance VM and delete the bucket you created
    if you want to save on costs. See the ‚ÄúCleaning up‚Äù steps at the very end of dreambooth_diffusion.ipynb.
  prefs: []
  type: TYPE_NORMAL
- en: üß™ Test your Endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To send a request to a Vertex AI endpoint, you will need to use an HTTP client
    library or a command-line tool that supports sending requests with the appropriate
    HTTP method and request parameters.
  prefs: []
  type: TYPE_NORMAL
- en: I carried out testing locally on my laptop. To do this you will need to download
    and install the [Vertex AI SDK for Python](https://cloud.google.com/python/docs/reference/aiplatform/latest/index.html#endpoints),
    then create and download a service key for authentication.
  prefs: []
  type: TYPE_NORMAL
- en: If you kept your notebook instance VM, you could use the same service account
    alias as before or just create a new one with permissions to, at a minimum, [get
    predictions from an endpoint](https://cloud.google.com/vertex-ai/docs/general/iam-permissions).
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Go to *IAM & Admin -> Service Accounts*. Then click on the three dots under
    Action on the right of the service account alias you want to create keys for and
    then click *Manage Keys*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/855a1d349360219d1836d9c84000eb22.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11: GCP IAM & Admin, Service accounts, manage keys for a service account.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'then click *Add Key -> Create new Key* and download as in the recommended .JSON
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/658dd6110b2eb15cc1c7f0f98d6f40c5.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12: GCP IAM & Admin, Service accounts, create key for a service account.*'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ö†Ô∏è **Remember, a service account key file grants the same permissions in your
    GCP project as the service account itself.** Always be very careful with the file,
    delete it when it‚Äôs no longer required and never, ever, upload it to a Github
    repo, I‚Äôve seen this happen way too many times!
  prefs: []
  type: TYPE_NORMAL
- en: '2\. In a terminal window, or from wherever you‚Äôre going to run your code to
    test your endpoint, you‚Äôll want to set a variable to point to the service key
    you downloaded. This will authenticate requests to your endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '3\. Now just modify this python code snippet with your Project Name, Region,
    and Endpoint ID. It will pass a prompt to your endpoint and store the response
    i.e. image generated by Stable Diffusion in a JPEG file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you‚Äôve installed the Vertex AI for Python SDK, authenticated to GCP, and
    the endpoint is active, after a few seconds you should see the generated image
    file appear in your file system!
  prefs: []
  type: TYPE_NORMAL
- en: üéÅ 3\. Create a simple RESTful API using FLASK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point you could quite easily integrate the code to call your generative
    AI model endpoint with an existing app using the Vertex AI Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: But, I did promise an API, so that is what I‚Äôm going to run through in this
    final part of the section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prerequisites:**'
  prefs: []
  type: TYPE_NORMAL
- en: Install [Flask](https://pypi.org/project/Flask/) and [Pillow](https://pypi.org/project/Pillow/)
    python libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Download Postman](https://www.postman.com/downloads/?utm_source=postman-home)
    and install, it‚Äôs free! We‚Äôll use it to simulate calls to our API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1\. As in the previous section, make sure you have an environment variable from
    where you plan to run your code, that points to your service account key.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Here is the code you need to create a simple RESTful API using the Flask
    web application framework. See the comments in the code for an explanation of
    what‚Äôs going on. We basically used the code from earlier to query the endpoint
    and wrapped it in an API call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '3\. Run the resulting file, and you should have a Flask server hosting your
    API, ready to receive requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a83c0c778babc2f9b0881ebd22530da.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13: MacOS terminal, running Flask server locally.*'
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Next we‚Äôre going to fire up Postman, go to *File -> Import*, paste in the
    following cURL command (you might need to modify the server address if you have
    it configured differently to the default):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We could run this from a terminal and the response from our API would be saved
    as a JPEG image in our local file system. But to simulate an app, and because
    I didn‚Äôt have time to write a Discord bot or HTML front end‚Ä¶ üòÖ
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ab9d69c5b3463030296060bc70e88b2.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14: Postman, API request example, image response generated by Stable
    Diffusion model.*'
  prefs: []
  type: TYPE_NORMAL
- en: If all has gone well, you‚Äôll end up with a nicely generated image, served from
    your very own API. You can modify the prompt under the Body tab, creativity is
    your only limit!
  prefs: []
  type: TYPE_NORMAL
- en: 'üóëÔ∏è **Done experimenting?** You can go back to *Vertex AI -> Endpoints*, select
    the endpoint, un-deploy the model, go back up a level to the endpoint and delete
    it (hint: triple dots menu at the end of each line item). Double check you spun
    down or deleted anything else related to this post so you‚Äôre not consuming resources.
    You can also revoke the service account key you created just to be safe.'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ö†Ô∏è **Final warning ‚Äî** this is clearly not production ready code. If you were
    to make your API public facing there‚Äôs work to be done around authentication,
    amongst other things that are out of scope for this post! Relatively speaking
    the cost per request (or compute hours) for our endpoint is also much more expensive
    than say Midjourney or DALL-E 2, so probably not a viable thing for you to launch
    as a service.
  prefs: []
  type: TYPE_NORMAL
- en: üèÜ Wrap up and closing thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I had **a lot** of fun writing this post, and learnt loads about the generative
    AI space, Stable Diffusion and what it means to package it up into something loosely
    resembling one of the popular consumer facing services available today. I salute
    those dev teams on the bleeding edge of this technology, it‚Äôs such an exciting
    space to be in at the moment!
  prefs: []
  type: TYPE_NORMAL
- en: üí∏ Costs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I was a little nervous about running large GPU attached instances. If you‚Äôre
    a long time public cloud user, you will know how easy it is to overspend without
    the right checks and balances in place.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of writing this post, I had a look at the Billing section in my GCP
    console to see what the damage was‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0e6619fbe9fabd43cfb448f83477b81.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15: GCP Billing, Reports.*'
  prefs: []
  type: TYPE_NORMAL
- en: During the two days that I was using GCP, with a project created exclusively
    for this post, I spent about USD $40\. You can see the breakdown in the screenshot
    above.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the cost was related to inference (see the Vertex AI service line item).
    This is the cost in ‚Äòcompute hours‚Äô for making my endpoint available to generate
    images using the Stable Diffusion model I deployed there.
  prefs: []
  type: TYPE_NORMAL
- en: The endpoint used the ‚ÄúVertex AI:Online/Batch Prediction NVIDIA Tesla P100 GPU
    running in Americas for AI Platform SKU‚Äù, with some other costs for running a
    Predefined instance. We could have trimmed the fat by choosing a cheaper GPU family,
    but the trade off would be an increased time period to get back a generated image.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing this to a commercially available solution, the Midjourney [basic plan](https://docs.midjourney.com/docs/plans)
    costs USD $10 on a monthly subscription. For that you get 3.3 fast GPU hrs/month,
    then it costs another $4/hr beyond that. In our case, I left the endpoint up for
    about 24 hours and spent $30 with no limitations on concurrency or jobs waiting.
    Again there‚Äôs a tradeoff between a fully managed service like Midjourney whose
    engineers are always making improvements to the model vs. iterating and deploying
    your own models on scalable cloud infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: '**The important point to note here is that you‚Äôre not paying per prediction
    request, but for how long your endpoint is up and the size/type of instance it
    runs on.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The cost to test the model and actually deploy the endpoint via a User Managed
    notebook came in at just under USD $10 (see Compute Engine & Notebooks line items).
  prefs: []
  type: TYPE_NORMAL
- en: So there you have it, in this post I gave a brief overview of generative AI
    and in particular its use for creating art using the Stable Diffusion technique.
    We then dived into some code examples to demonstrate the steps needed to generate
    some art using the Stable Diffusion model and indeed how to deploy it to an endpoint
    and consume it via an API. **I hope you enjoyed this post, and I‚Äôll see you in
    the next!** üëãüèº
  prefs: []
  type: TYPE_NORMAL
- en: The figures in this post were created by the author, unless otherwise stated.
  prefs: []
  type: TYPE_NORMAL
- en: üìá References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Machine Learning, History and relationships to other fields: [https://en.wikipedia.org/wiki/Machine_learning#History_and_relationships_to_other_fields](https://en.wikipedia.org/wiki/Machine_learning#History_and_relationships_to_other_fields)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] What is generative AI? Everything you need to know, George Lawton: [https://www.techtarget.com/searchenterpriseai/definition/generative-AI](https://www.techtarget.com/searchenterpriseai/definition/generative-AI)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] The implications of Generative AI for businesses ‚Äî A new frontier in Artificial
    Intelligence, Deloitte LLP: [https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html](https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Midjourney docs, Version: [https://docs.midjourney.com/docs/model-versions#:~:text=Current%20Model,places%2C%20objects%2C%20and%20more](https://docs.midjourney.com/docs/model-versions#:~:text=Current%20Model,places%2C%20objects%2C%20and%20more).'
  prefs: []
  type: TYPE_NORMAL
- en: üìö Useful Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the completed notebook (StableDiffusion_Hugging_Face.ipynb) from
    ‚Äú1\. Experimenting with Stable Diffusion‚Äù and python code from ‚Äú2\. Serve the
    model via an API -> endpoint‚Äù here: [https://github.com/omermx/medium_text_to_image_api](https://github.com/omermx/medium_text_to_image_api)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can swap out the version Stable Diffusion I used in my examples (1.5) with
    the latest at the time of writing (2.1), here on Hugging Face: [https://huggingface.co/stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
