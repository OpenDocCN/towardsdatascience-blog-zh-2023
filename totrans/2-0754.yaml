- en: Dockerizing Apache Zeppelin and Apache Spark for Easy Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/dockerizing-apache-zeppelin-and-apache-spark-for-easy-deployment-1814d0c1c245](https://towardsdatascience.com/dockerizing-apache-zeppelin-and-apache-spark-for-easy-deployment-1814d0c1c245)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn How To Build a Portable and Scalable Data Analysis Environment with Docker-Compose
    And Volumes.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://anbento4.medium.com/?source=post_page-----1814d0c1c245--------------------------------)[![Antonello
    Benedetto](../Images/bf802bb46dce03dd3bd4e17c1dffe5b7.png)](https://anbento4.medium.com/?source=post_page-----1814d0c1c245--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1814d0c1c245--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1814d0c1c245--------------------------------)
    [Antonello Benedetto](https://anbento4.medium.com/?source=post_page-----1814d0c1c245--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1814d0c1c245--------------------------------)
    ·9 min read·Jan 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dcd662fabf5ce0f263fe35015b17886d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Tom Fisk](https://www.pexels.com/@tomfisk/) On [Pexels](https://www.pexels.com/photo/birds-eye-view-photo-of-freight-containers-2226458/)
  prefs: []
  type: TYPE_NORMAL
- en: On-Demand Courses | Recommended
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*A few of my readers have contacted me asking for on-demand courses to learn
    more about* ***Apache Spark with Python****. These are 3 great resources I would
    recommend:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Data Streaming With Apache Kafka & Apache Spark Nano-Degree (UDACITY)**](https://imp.i115008.net/zaX10r)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Data Engineering Nano-Degree (UDACITY)**](https://imp.i115008.net/zaX10r)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Spark And Python For Big Data With PySpark (UDEMY)**](https://click.linksynergy.com/deeplink?id=533LxfDBSaM&mid=39197&murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fspark-and-python-for-big-data-with-pyspark%2F)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Not a Medium member yet? Consider signing up with my* [*referral link*](https://anbento4.medium.com/membership)
    *to gain access to everything Medium has to offer for as little as $5 a month!*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker has revolutionised the way we deploy and manage our applications and
    data analysis tools. It allows for effortless scaling, and the ability to easily
    customise the services to suit specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, I will show you how to quickly deploy Apache Zeppelin and
    Apache Spark using a `docker-compose.yaml` file and take advantage of volumes
    to manage data dependencies among services.
  prefs: []
  type: TYPE_NORMAL
- en: '**The advantage of the solution presented below, is that does not require any
    additional instructions to be provided as part of a** `**Dockerfile**`**.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even people with limited Docker experience, will find this template as a valuable
    starting point to deploy *easy-to-maintain* containers.
  prefs: []
  type: TYPE_NORMAL
- en: With this method, you will be able to create a portable and scalable data analysis
    environment that requires less maintenance, and can be customised to use your
    preferred Spark image.
  prefs: []
  type: TYPE_NORMAL
- en: This makes it an excellent choice particularly for data engineers, data scientists
    and machine learning engineers who need to process large datasets, perform exploratory
    analysis and train their models.
  prefs: []
  type: TYPE_NORMAL
- en: '****Please Note:** In the sections that follow, I assume you have Docker installed
    on your machine (*otherwise you might find* [*the official docs*](https://docs.docker.com/engine/install/)
    *useful*) and are familiar with Docker basic commands to interact with containers
    (*otherwise refer to* [*this article*](/12-essential-docker-commands-you-should-know-c2d5a7751bb5)).**'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Docker-Compose File
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code (*also available on* [*GitHub*](https://github.com/anbento0490/docker/blob/master/zeppelin_spark/docker-compose.yaml)),
    is an example of `docker-compose.yaml` file that can be used to run **Apache Zeppelin**
    on top of **Apache Spark** standalone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'By running the command `docker-compose up -d`, this file will start three services:'
  prefs: []
  type: TYPE_NORMAL
- en: A `SPARK-MASTER` ( `version=3.1.1`) responsible for managing the Spark cluster.
    This service will be running at `port 7077` and have a UI available at [http://localhost:8080/](http://localhost:8080/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `SPARK-WORKER-1` that is indeed a worker node assigned with 2 cores and 4GB
    of memory, that connects to the `SPARK-MASTER` via port `7077` and has a UI reachable
    at [http://localhost:8081/](http://localhost:8080/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image used for both `SPARK-MASTER` and `SPARK-WORKER` is managed by [bde2020](https://github.com/big-data-europe/docker-spark)
    and downloadable from [DockerHub](https://hub.docker.com/r/bde2020/spark-master/tags).
  prefs: []
  type: TYPE_NORMAL
- en: An`ZEPPELIN` web-based notebook ( `version=10.1`) that is set to run code against
    the standalone `SPARK-MASTER`, through `SPARK-WORKER-1` and has a UI available
    at [http://localhost:8082/](http://localhost:8080/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To verify that all services are up and running, execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This command should return the `container ID`, `container name` and `current
    status`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Access Docker Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, assuming that all the services in the `docker-compose.yaml` file
    are running, their user interfaces should be accessible at the host addresses
    specified in the ports mapping configuration.
  prefs: []
  type: TYPE_NORMAL
- en: However, before checking how the UI looks like, let’s access the `SPARK-MASTER`
    and verify that it is indeed available at `port 7077` and PySpark commands can
    be executed against its cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To execute commands in the running`SPARK-MASTER` container, via bash shell,
    you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is an extremely useful command to troubleshoot the containers, check logs,
    and make changes to the container’s file system or running processes.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'At this stage, you should be able to launch a PySpark shell that connects to
    the `spark-master` service available at `port 7077` by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If the everything works without errors, you should see a Python shell prompt
    appearing and you can try running a few commands (*for example create a simple
    DF* ) against the cluster, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f512a3ebf623676a56ab68306f74e3a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Success! With the step above you verified that the `spark-master` service is
    reachable and able to run Spark applications in standalone mode (to exit from
    the interactive Python prompt, type `exit()` and then `CTRL + D`)
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, if you now navigate to host [http://localhost:8080/](http://localhost:8080/),
    you will notice that a `PySparkShell` application has appeared under the `Running
    Applications` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f785bea04ef0e015f7733eb3b34eff75.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The details of the job executed a moment ago to create the DF with PySpark,
    are also summarised by the `spark-worker` at host [http://localhost:8081/](http://localhost:8080/):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67a5ce824f27a639070f9aa363e50d14.png)'
  prefs: []
  type: TYPE_IMG
- en: Hurrah! Spark works!
  prefs: []
  type: TYPE_NORMAL
- en: However, the next step is for you to make sure that also the`ZEPPELIN` service
    can interact with the`SPARK-MASTER` and run commands against the same standalone
    cluster, built on top of `version=3.1.1`.
  prefs: []
  type: TYPE_NORMAL
- en: By navigating to host [http://localhost:8082/](http://localhost:8080/) , you
    can verify that `ZEPPELIN` is indeed running and new notebooks can be created
    ( *for instance, I created one named* `PySpark Project 1` ) and Spark commands
    be run.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3bce0c64f1d2fc68b98c7da6bc0b269.png)'
  prefs: []
  type: TYPE_IMG
- en: By default, code will be executed *in local* instead of against the standalone
    cluster, using Zeppelin’s standard interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: To change this behaviour, you need to get familiar with the concept of Docker
    *volumes* and *environment variables*.
  prefs: []
  type: TYPE_NORMAL
- en: '****Please note:** like with `SPARK-MASTER`, also `ZEPPELIN` container can
    be accessed by running `docker exec -it zeppelin /bin/bash`. The main path you
    might want to explore for `version=10.1` is `/opt/zeppelin/`.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Docker Volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker volumes are a way to store data outside of a container’s filesystem.
    This allows for data to be persisted even if the container is deleted or recreated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to the `docker-compose.yaml` file, you can verify that the following
    volumes got created when containers were run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: First of all, I created a so called *“named volume”* `spark_volume` under the
    `volumes` section and mapped it to the `/spark` folder of the `SPARK-MASTER` container’s
    filesystem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same `spark_volume` has been then mapped to the `/opt/zeppelin/spark` directory
    in the `ZEPPELIN` filesystem. This effectively creates a new folder `/spark` with
    all the content available in the original `SPARK-MASTER` filesystem. This folder,
    among the other files, includes the `SPARK-SUBMIT` executables, that are necessary
    to run a Spark application via Zeppelin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `/opt/zeppelin/notebook` directory has been mapped to the local `./zeppelin/notebook`
    folder in your local environment. This allows you to create and save notebooks
    in local, to avoid losing them every time the container are removed with `docker-compose
    down` or restarted with `docker restart <service-name>`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the `/opt/zeppelin/conf` directory has been mapped to the local `./zeppelin/conf`.
    This is necessary, as in the next step, you will have to set some environment
    variables at this location.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****Please note**: these are the four fundamental volumes, that are required
    to avoid both application / interpreter errors in Zeppelin notebooks. However
    you can create more if you wish.******'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see what environment variable are required and how to set them.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Docker Environment Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Environment variables are used to pass configuration options to a container.
    Looking at the `docker-compose.yaml` file, the most relevant environment variables
    to describe are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: First of all, for both `SPARK-WORKER` and `ZEPPELIN` services, the `SPARK_MASTER`
    variable has been set to `spark://spark-master:7077`, that is the indeed the URL
    of the `SPARK-MASTER` service. This allows both services to connect to the standalone
    Spark cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, in the `ZEPPELIN` service, the `SPARK_HOME` variable has been set to `/opt/zeppelin/spark`
    which, as you remember from the previous section, is the path to the folder created
    via `spark_volume` (*that contains the* `SPARK-SUBMIT` *executable*). ***Setting
    this variable correctly is key if you wish for a* `zeppelin` *application to run
    on top of Spark standalone.***
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the `SPARK-WORKER` service, I set `SPARK_WORKER_CORES=2`, which means that
    the Spark Worker will use 2 cores and `SPARK_WORKER_MEMORY=4g`, which means that
    the Spark Worker will have 4GB of memory available. You can of course tweak these
    parameters depending on your specific requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As far as services configuration is concerned, this is pretty much all you need
    to do. In fact, you could already go ahead and use Zeppelin to execute code (*written
    in Scala*), against the Spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: However, in order to avoid getting errors related to the PySpark interpreter
    in Zeppelin, you should also export two additional variables as part of the `zeppelin-env.sh`
    file, that for `version=10.1` has to be created and persisted under the `/opt/zeppelin/conf`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set the environment variables, first access the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'then run this bash script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Once created and populated with the `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON`
    variables, the presence of a `zeppelin-env.sh` will ensure that the PySpark interpreter
    in Zeppelin uses `python3`, while executing PySpark code and avoid clashes with
    default versions.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Execute Code In Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With both volumes and and environment variables in place, you can now test that
    Scala and Python commands are executed as expected in the Zeppelin notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: 'After saving recent changes to the `docker-compose` file, restart the `ZEPPELIN`
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Navigate to host [http://localhost:8082/](http://localhost:8080/) and create
    a new notebook or open an existing one ( *I am going to use* `PySpark Project
    1`). Now, try to execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point you can try to create a simple DF with PySpark, as I did on the
    the shell at Step 2\. The output should look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/147e28d3b40e2b87978210e7157d4d3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, notice how a `ZEPPELIN` application has been allocated resources on the
    Spark Cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8bf796da98e77dc8ac79ece4992c7b6d.png)'
  prefs: []
  type: TYPE_IMG
- en: Terrific work!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this tutorial, you have learnt how to deploy recent versions of **Apache
    Zeppelin** and **Apache Spark** via `docker-compose` file, without the need of
    any additional instructions usually provided via `Dockerfile`.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, you took advantage of **volumes** to manage and persist data dependencies
    and **environment variables** to customise the services. This makes it easy to
    create a portable and scalable data analysis environment that requires little
    or no maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: This solution, allows you to quickly spin up a **standalone Spark Cluster**,
    interacting with one or more Spark worker nodes to process large datasets, and
    use Apache Zeppelin as a web-based notebook to perform data exploration and visualisation.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you understood how to leverage environment variables to configure
    the services, and how to set up the **PySpark interpreter** in Zeppelin, avoiding
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, using `docker-compose` to deploy Apache Zeppelin and Apache Spark is
    an efficient and convenient way to manage data analysis environments. It allows
    for easy scaling, and the ability to customise the services to suit specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: This makes it an excellent choice for **data engineers**, **data scientists**
    and **machine learning engineers** who need to process large datasets, perform
    exploratory analysis and train their models.
  prefs: []
  type: TYPE_NORMAL
- en: Sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Docker Engine Installation Overview](https://docs.docker.com/engine/install/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Docker Docs | Volumes](https://docs.docker.com/storage/volumes/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Docker Docs | Environment Variables In Compose](https://docs.docker.com/compose/environment-variables/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DockerHub | bde2020 Images](https://hub.docker.com/r/bde2020/spark-master/tags)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[BDE Docker-Spark GitHub Repository](https://github.com/big-data-europe/docker-spark)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Spark interpreter For Apache Zeppelin (version 10.1)](https://zeppelin.apache.org/docs/latest/interpreter/spark.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using PySpark In Zeppelin With python3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[12 Essential Docker Commands You Should Know](/12-essential-docker-commands-you-should-know-c2d5a7751bb5)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
