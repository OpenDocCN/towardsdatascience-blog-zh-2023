- en: How I Leveraged Open Source LLMs to Achieve Massive Savings on a Large Compute
    Project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•åˆ©ç”¨å¼€æºLLMåœ¨å¤§å‹è®¡ç®—é¡¹ç›®ä¸­å®ç°å·¨é¢èŠ‚çœ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267](https://towardsdatascience.com/how-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267](https://towardsdatascience.com/how-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267)
- en: Unlocking Cost-Efficiency in Large Compute Projects with Open Source LLMs and
    GPU Rentals.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ©ç”¨å¼€æºLLMå’ŒGPUç§Ÿèµå®ç°å¤§å‹è®¡ç®—é¡¹ç›®çš„æˆæœ¬æ•ˆç›Šã€‚
- en: '[](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)[![Ryan
    Shrott](../Images/186524066383b4b02c994692aebb3ea5.png)](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)
    [Ryan Shrott](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)[![Ryan
    Shrott](../Images/186524066383b4b02c994692aebb3ea5.png)](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)
    [Ryan Shrott](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)
    Â·6 min readÂ·Aug 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)
    Â·é˜…è¯»æ—¶é—´6åˆ†é’ŸÂ·2023å¹´8æœˆ30æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/83382f63b21df0de2b0d47d33ad7212d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83382f63b21df0de2b0d47d33ad7212d.png)'
- en: Photo by [Alexander Grey](https://unsplash.com/@sharonmccutcheon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Alexander Grey](https://unsplash.com/@sharonmccutcheon?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: In the world of large language models (LLMs), the cost of computation can be
    a significant barrier, especially for extensive projects. I recently embarked
    on a project that required running 4,000,000 prompts with an average input length
    of 1000 tokens and an average output length of 200 tokens. Thatâ€™s nearly 5 billion
    tokens! The traditional approach of paying per token, as is common with models
    like GPT-3.5 and GPT-4, would have resulted in a hefty bill. However, I discovered
    that by leveraging open source LLMs, I could shift the pricing model to pay per
    hour of compute time, leading to substantial savings. This article will detail
    the approaches I took and compare and contrast each of them. Please note that
    while I share my experience with pricing, these are subject to change and may
    vary depending on your region and specific circumstances. The key takeaway here
    is the potential cost savings when leveraging open source LLMs and renting a GPU
    per hour, rather than the specific prices quoted.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸–ç•Œä¸­ï¼Œè®¡ç®—æˆæœ¬å¯èƒ½æ˜¯ä¸€ä¸ªé‡å¤§éšœç¢ï¼Œå°¤å…¶æ˜¯åœ¨å¤§å‹é¡¹ç›®ä¸­ã€‚æˆ‘æœ€è¿‘å¼€å§‹äº†ä¸€ä¸ªéœ€è¦è¿è¡Œ4,000,000ä¸ªæç¤ºçš„é¡¹ç›®ï¼Œå¹³å‡è¾“å…¥é•¿åº¦ä¸º1000ä¸ªtokensï¼Œå¹³å‡è¾“å‡ºé•¿åº¦ä¸º200ä¸ªtokensã€‚è¿™æ¥è¿‘50äº¿tokensï¼ä¼ ç»Ÿçš„æŒ‰tokenè®¡è´¹çš„æ–¹æ³•ï¼ˆå¦‚GPT-3.5å’ŒGPT-4æ‰€ç”¨çš„ï¼‰ä¼šå¯¼è‡´é«˜é¢è´¦å•ã€‚ç„¶è€Œï¼Œæˆ‘å‘ç°é€šè¿‡åˆ©ç”¨å¼€æºLLMï¼Œæˆ‘å¯ä»¥å°†å®šä»·æ¨¡å‹è½¬å˜ä¸ºæŒ‰å°æ—¶è®¡ç®—è®¡ç®—æ—¶é—´ï¼Œä»è€ŒèŠ‚çœå¤§é‡æˆæœ¬ã€‚æœ¬æ–‡å°†è¯¦ç»†è¯´æ˜æˆ‘é‡‡å–çš„æ–¹æ³•ï¼Œå¹¶å¯¹æ¯ç§æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚è¯·æ³¨æ„ï¼Œè™½ç„¶æˆ‘åˆ†äº«äº†å®šä»·ç»éªŒï¼Œä½†è¿™äº›è´¹ç”¨å¯èƒ½ä¼šå˜åŒ–ï¼Œå¹¶å¯èƒ½å› æ‚¨çš„åœ°åŒºå’Œå…·ä½“æƒ…å†µè€Œæœ‰æ‰€ä¸åŒã€‚å…³é”®ç‚¹æ˜¯ï¼Œåˆ©ç”¨å¼€æºLLMå’ŒæŒ‰å°æ—¶ç§ŸèµGPUçš„æˆæœ¬èŠ‚çœæ½œåŠ›ï¼Œè€Œä¸æ˜¯å…·ä½“çš„æŠ¥ä»·ã€‚
- en: ChatGPT API
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT API
- en: 'I conducted an initial test using GPT-3.5 and GPT-4 on a small subset of my
    prompt input data. Both models demonstrated commendable performance, but GPT-4
    consistently outperformed GPT-3.5 in a majority of the cases. To give you a sense
    of the cost, running all 4 million prompts using the Open AI API would look something
    like this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨ä¸€ä¸ªå°æ•°æ®å­é›†ä¸Šä½¿ç”¨äº†GPT-3.5å’ŒGPT-4è¿›è¡Œäº†åˆæ­¥æµ‹è¯•ã€‚ä¸¤ä¸ªæ¨¡å‹éƒ½è¡¨ç°å‡ºè‰²ï¼Œä½†GPT-4åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å§‹ç»ˆä¼˜äºGPT-3.5ã€‚ä¸ºäº†è®©ä½ äº†è§£æˆæœ¬ï¼Œé€šè¿‡Open
    AI APIè¿è¡Œå…¨éƒ¨400ä¸‡ä¸ªæç¤ºçš„æˆæœ¬å¤§è‡´å¦‚ä¸‹ï¼š
- en: '![](../Images/35209c957d0dc0f7f6be806ff60f6cd8.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35209c957d0dc0f7f6be806ff60f6cd8.png)'
- en: Total cost of running 4mm prompts with input length of 1000 tokens and 200 token
    output length
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œ400ä¸‡ä¸ªæç¤ºçš„æ€»æˆæœ¬ï¼Œè¾“å…¥é•¿åº¦ä¸º1000ä¸ªtokensï¼Œè¾“å‡ºé•¿åº¦ä¸º200ä¸ªtokens
- en: While GPT-4 did offer some performance benefits, the cost was disproportionately
    high compared to the incremental performance it added to my outputs. Conversely,
    GPT-3.5 Turbo, although more affordable, fell short in terms of performance, making
    noticeable errors on 2â€“3% of my prompt inputs. Given these factors, I wasnâ€™t prepared
    to invest $7,600 on a project that was essentially a personal endeavor.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ GPT-4 æä¾›äº†ä¸€äº›æ€§èƒ½ä¸Šçš„å¥½å¤„ï¼Œä½†ç›¸è¾ƒäºå…¶å¯¹æˆ‘çš„è¾“å‡ºæ‰€å¢åŠ çš„å¢é‡æ€§èƒ½ï¼Œæˆæœ¬å´è¿‡é«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå°½ç®¡ GPT-3.5 Turbo æ›´åŠ å®æƒ ï¼Œä½†åœ¨æ€§èƒ½ä¸Šæœ‰æ‰€æ¬ ç¼ºï¼Œåœ¨
    2â€“3% çš„æç¤ºè¾“å…¥ä¸­å‡ºç°æ˜æ˜¾é”™è¯¯ã€‚é‰´äºè¿™äº›å› ç´ ï¼Œæˆ‘ä¸æ„¿æ„åœ¨ä¸€ä¸ªæœ¬è´¨ä¸Šæ˜¯ä¸ªäººé¡¹ç›®çš„é¡¹ç›®ä¸ŠæŠ•èµ„ $7,600ã€‚
- en: Open-Source Models
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼€æºæ¨¡å‹
- en: 'Open-source models to the rescue! With open-source models, the pricing model
    is very different: **you only pay per hour of compute time**. Therefore, your
    goal is to maximize compute iterations per hour. Also, with solutions like Petals.ml,
    you can run your compute for free (with limitations)!'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¼€æºæ¨¡å‹æ¥æ•‘æ´ï¼ä½¿ç”¨å¼€æºæ¨¡å‹ï¼Œå®šä»·æ¨¡å¼éå¸¸ä¸åŒï¼š**ä½ åªéœ€æŒ‰è®¡ç®—æ—¶é—´çš„å°æ—¶æ”¶è´¹**ã€‚å› æ­¤ï¼Œä½ çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ¯å°æ—¶çš„è®¡ç®—è¿­ä»£æ¬¡æ•°ã€‚æ­¤å¤–ï¼Œåƒ Petals.ml
    è¿™æ ·çš„è§£å†³æ–¹æ¡ˆï¼Œä½ å¯ä»¥å…è´¹è¿è¡Œä½ çš„è®¡ç®—ï¼ˆæœ‰ä¸€å®šé™åˆ¶ï¼‰ï¼
- en: After trying various models on Hugging Face, I found that a fine-tuned version
    of LLama-2 70B called [Stable Beluga 2](https://huggingface.co/stabilityai/StableBeluga2)
    gave me excellent performance without any fine tuning needed! It performed better
    than GPT-3.5 Turbo, but slightly below GPT-4\. However, this model is still very
    large and requires a very beefy GPU. Itâ€™s often best practice to use the smallest
    model possible to complete your task efficiently. Therefore, I tried out the 7B
    version of Stable Beluga 2\. The performance was not good enough!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å°è¯•äº† Hugging Face ä¸Šçš„å„ç§æ¨¡å‹åï¼Œæˆ‘å‘ç°ä¸€ä¸ªåä¸º [Stable Beluga 2](https://huggingface.co/stabilityai/StableBeluga2)
    çš„ LLama-2 70B å¾®è°ƒç‰ˆæœ¬ç»™äº†æˆ‘å¾ˆå¥½çš„æ€§èƒ½ï¼Œæ— éœ€ä»»ä½•å¾®è°ƒï¼å®ƒçš„è¡¨ç°ä¼˜äº GPT-3.5 Turboï¼Œä½†ç•¥ä½äº GPT-4ã€‚ç„¶è€Œï¼Œè¿™ä¸ªæ¨¡å‹ä»ç„¶éå¸¸åºå¤§ï¼Œéœ€è¦éå¸¸å¼ºå¤§çš„
    GPUã€‚é€šå¸¸ï¼Œæœ€ä½³å®è·µæ˜¯ä½¿ç”¨å°½å¯èƒ½å°çš„æ¨¡å‹æ¥é«˜æ•ˆå®Œæˆä»»åŠ¡ã€‚å› æ­¤ï¼Œæˆ‘å°è¯•äº† Stable Beluga 2 çš„ 7B ç‰ˆæœ¬ã€‚æ€§èƒ½ä»ç„¶ä¸å¤Ÿå¥½ï¼
- en: Fine-tuning
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¾®è°ƒ
- en: 'To increase the performance for my task, I used GPT-4 and [Petals.ml](https://github.com/bigscience-workshop/petals)
    (Stable Beluga 2 70B) to generate a fine-tuning dataset. I generated 25K prompt
    completion pairs using Petals.ml and 2K using GPT-4 API. As a reminder Petals.ml
    allows you to run open source LLM models for free using bit-torrent technology.
    The inference time is not great though: it would have taken over a year to do
    4mm iterations.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æå‡ä»»åŠ¡æ€§èƒ½ï¼Œæˆ‘ä½¿ç”¨äº† GPT-4 å’Œ [Petals.ml](https://github.com/bigscience-workshop/petals)ï¼ˆStable
    Beluga 2 70Bï¼‰æ¥ç”Ÿæˆå¾®è°ƒæ•°æ®é›†ã€‚æˆ‘é€šè¿‡ Petals.ml ç”Ÿæˆäº† 25K ä¸ªæç¤ºå®Œæˆå¯¹ï¼Œå¹¶é€šè¿‡ GPT-4 API ç”Ÿæˆäº† 2K ä¸ªã€‚è¯·æ³¨æ„ï¼ŒPetals.ml
    å…è®¸ä½ ä½¿ç”¨ bit-torrent æŠ€æœ¯å…è´¹è¿è¡Œå¼€æº LLM æ¨¡å‹ã€‚ç„¶è€Œï¼Œæ¨ç†æ—¶é—´å¹¶ä¸ç†æƒ³ï¼šè¿›è¡Œ 4mm è¿­ä»£éœ€è¦è¶…è¿‡ä¸€å¹´ã€‚
- en: 'Also, Petals.ml hosts this model for free. Therefore, I utilized Petals.ml
    to generate 25,000 prompt completion pairs. I also used the GPT-4 API to generate
    an additional 2K prompt completion pairs. In hindsight, I probably overdid it
    on the training data. Iâ€™ve read reports showing that as few as 500 fine tuning
    samples can be enough to improve the performance of an LLM. Anyway, here is the
    total cost of running 2K prompts using Open AI API:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒPetals.ml å…è´¹æ‰˜ç®¡äº†è¿™ä¸ªæ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘åˆ©ç”¨ Petals.ml ç”Ÿæˆäº† 25,000 ä¸ªæç¤ºå®Œæˆå¯¹ã€‚ æˆ‘è¿˜ä½¿ç”¨äº† GPT-4 API ç”Ÿæˆäº†é¢å¤–çš„
    2K ä¸ªæç¤ºå®Œæˆå¯¹ã€‚äº‹åçœ‹æ¥ï¼Œæˆ‘å¯èƒ½åœ¨è®­ç»ƒæ•°æ®ä¸Šåšå¾—è¿‡å¤´äº†ã€‚æˆ‘é˜…è¯»è¿‡æŠ¥å‘Šæ˜¾ç¤ºï¼Œå°‘è‡³ 500 ä¸ªå¾®è°ƒæ ·æœ¬å°±è¶³ä»¥æå‡ LLM çš„æ€§èƒ½ã€‚æ— è®ºå¦‚ä½•ï¼Œè¿™é‡Œæ˜¯ä½¿ç”¨ Open
    AI API è¿è¡Œ 2K æç¤ºçš„æ€»æˆæœ¬ï¼š
- en: '![](../Images/80c052ebe7c81d606c55085777984208.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80c052ebe7c81d606c55085777984208.png)'
- en: Total cost of running 2K prompt completion pairs
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œ 2K ä¸ªæç¤ºå®Œæˆå¯¹çš„æ€»æˆæœ¬
- en: 'Thatâ€™s right: $84\. Can you guess what happened next? Using this newly acquired
    27K prompt completion pairs, I fine-tuned the 7B variant of the smallest LLama
    2 7B, and voila, this new model performed extremely well for my use case. The
    total fine-tuning cost was only $6.6 as I rented an A100 GPU for 6 hours. **In
    the end, my fine-tuned model performed better than GPT-3.5, and slightly worse
    than GPT-4.**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡é”™ï¼š$84ã€‚ä½ èƒ½çŒœåˆ°æ¥ä¸‹æ¥å‘ç”Ÿäº†ä»€ä¹ˆå—ï¼Ÿä½¿ç”¨è¿™ 27K ä¸ªæç¤ºå®Œæˆå¯¹ï¼Œæˆ‘å¯¹æœ€å°çš„ LLama 2 7B è¿›è¡Œäº†å¾®è°ƒï¼Œç»“æœï¼Œè¿™ä¸ªæ–°æ¨¡å‹åœ¨æˆ‘çš„ç”¨ä¾‹ä¸­è¡¨ç°æå…¶å‡ºè‰²ã€‚æ€»å¾®è°ƒæˆæœ¬ä»…ä¸º
    $6.6ï¼Œå› ä¸ºæˆ‘ç§Ÿç”¨äº†ä¸€ä¸ª A100 GPU 6 å°æ—¶ã€‚**æœ€ç»ˆï¼Œæˆ‘å¾®è°ƒåçš„æ¨¡å‹è¡¨ç°ä¼˜äº GPT-3.5ï¼Œç•¥é€Šäº GPT-4ã€‚**
- en: Inference
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: 'Now letâ€™s talk about inference compute costs. My model requires at least 20GB
    VRAM, so weâ€™ll require at least an RTX 3090\. There are four great options currently
    available for GPU rentals: AWS, LambdaLabs, [RunPod](https://runpod.io/?ref=lemrt56t)
    and [Vast.AI](https://cloud.vast.ai/?ref_id=79595). From my experience, Vast.AI
    has the best prices, but the worst reliability. AWS is super reliable but is more
    expensive than the other options. RunPod has great prices and a great UI. I personally
    wanted to keep my costs as low as possible, so I decided to go with Vast.AI for
    this project. Vast.AI is a peer-to-peer GPU rental service, so itâ€™s likely also
    the least secure. After scouring Vast.AI for the best price to performance ratio
    servers available, here are the best options I found:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è°ˆè°ˆæ¨æ–­è®¡ç®—æˆæœ¬ã€‚æˆ‘çš„æ¨¡å‹è‡³å°‘éœ€è¦20GBçš„VRAMï¼Œå› æ­¤æˆ‘ä»¬è‡³å°‘éœ€è¦RTX 3090ã€‚ç›®å‰ï¼Œæœ‰å››ä¸ªå¾ˆå¥½çš„GPUç§Ÿèµé€‰é¡¹ï¼šAWSã€LambdaLabsã€[RunPod](https://runpod.io/?ref=lemrt56t)å’Œ[Vast.AI](https://cloud.vast.ai/?ref_id=79595)ã€‚æ ¹æ®æˆ‘çš„ç»éªŒï¼ŒVast.AIçš„ä»·æ ¼æœ€å¥½ï¼Œä½†å¯é æ€§æœ€å·®ã€‚AWSéå¸¸å¯é ï¼Œä½†æ¯”å…¶ä»–é€‰é¡¹æ›´è´µã€‚RunPodä»·æ ¼ä¼˜è¶Šï¼Œç”¨æˆ·ç•Œé¢ä¹Ÿå¾ˆå¥½ã€‚æˆ‘ä¸ªäººæƒ³å°½é‡é™ä½æˆæœ¬ï¼Œæ‰€ä»¥å†³å®šä½¿ç”¨Vast.AIè¿›è¡Œè¿™ä¸ªé¡¹ç›®ã€‚Vast.AIæ˜¯ä¸€ä¸ªç‚¹å¯¹ç‚¹GPUç§ŸèµæœåŠ¡ï¼Œå› æ­¤å®ƒå¯èƒ½ä¹Ÿæ˜¯æœ€ä¸å®‰å…¨çš„ã€‚ç»è¿‡åœ¨Vast.AIä¸Šå¯»æ‰¾æœ€ä½³æ€§ä»·æ¯”çš„æœåŠ¡å™¨åï¼Œä»¥ä¸‹æ˜¯æˆ‘æ‰¾åˆ°çš„æœ€ä½³é€‰é¡¹ï¼š
- en: '![](../Images/387407f56c236b653480400d7dc503f7.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/387407f56c236b653480400d7dc503f7.png)'
- en: Vast.AI total project costs using fune tuned LLama2 7b model. The total cost
    is computed using the number of iterations per second, the price per hour of GPU
    rental, and the total iterations required.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Vast.AIä½¿ç”¨å¾®è°ƒLLama2 7bæ¨¡å‹çš„æ€»é¡¹ç›®æˆæœ¬ã€‚æ€»æˆæœ¬æ˜¯é€šè¿‡æ¯ç§’çš„è¿­ä»£æ¬¡æ•°ã€GPUç§Ÿèµçš„æ¯å°æ—¶ä»·æ ¼å’Œæ‰€éœ€çš„æ€»è¿­ä»£æ¬¡æ•°æ¥è®¡ç®—çš„ã€‚
- en: As shown above, the total cost of inference can be less than $99\. In my case,
    I spun up around 10 servers and computed the number of iterations per second I
    could yield with each. For this particular project, which leveraged a fine-tuned
    variant of Llama2â€“7B, the RTX A5000 was a great option in terms of price to performance.
    I should also mention that none of this would be possible without the INSANE speeds
    of the open-source library [VLLM](https://github.com/vllm-project/vllm). My compute
    is 100% powered by VLLM under the hood. **Without VLLM, the compute time would
    increase by 20 times!** You may also look at the total runtime and laugh â€” 638
    hours is 26 days; however, you could easily parallelize the task across multiple
    GPUs/servers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šæ‰€ç¤ºï¼Œæ¨æ–­çš„æ€»æˆæœ¬å¯ä»¥ä½äº$99ã€‚åœ¨æˆ‘çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘å¯åŠ¨äº†å¤§çº¦10å°æœåŠ¡å™¨ï¼Œå¹¶è®¡ç®—äº†æ¯å°æœåŠ¡å™¨æ¯ç§’èƒ½äº§ç”Ÿçš„è¿­ä»£æ¬¡æ•°ã€‚å¯¹äºè¿™ä¸ªç‰¹åˆ«çš„é¡¹ç›®ï¼Œåˆ©ç”¨äº†å¾®è°ƒçš„Llama2â€“7Bå˜ä½“ï¼ŒRTX
    A5000åœ¨æ€§ä»·æ¯”æ–¹é¢æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚æˆ‘è¿˜åº”è¯¥æåˆ°ï¼Œå¦‚æœæ²¡æœ‰å¼€æºåº“*[VLLM](https://github.com/vllm-project/vllm)*çš„**æå¿«é€Ÿåº¦**ï¼Œè¿™ä¸€åˆ‡éƒ½ä¸å¯èƒ½å®ç°ã€‚æˆ‘çš„è®¡ç®—å®Œå…¨ä¾èµ–äºVLLMçš„æ”¯æŒã€‚**æ²¡æœ‰VLLMï¼Œè®¡ç®—æ—¶é—´å°†å¢åŠ 20å€ï¼**
    ä½ ä¹Ÿå¯ä»¥çœ‹çœ‹æ€»è¿è¡Œæ—¶é—´å¹¶å‘ç¬‘â€”â€”638å°æ—¶å°±æ˜¯26å¤©ï¼›ç„¶è€Œï¼Œä½ å¯ä»¥è½»æ¾åœ°åœ¨å¤šä¸ªGPU/æœåŠ¡å™¨ä¹‹é—´å¹¶è¡Œå¤„ç†ä»»åŠ¡ã€‚
- en: Conclusion
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In conclusion, the utilization of open-source LLMs and the shift in the pricing
    model from paying per token to paying per hour of compute time resulted in substantial
    cost savings in this large compute project. The total cost, which included prompt
    generation, fine-tuning, and inference, was a mere $189.58\. This is a stark contrast
    to the costs associated with using models like GPT-4 or GPT-3.5 Turbo, which would
    have amounted to $167,810 and $7410.42 respectively.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œåˆ©ç”¨å¼€æºLLMä»¥åŠå°†å®šä»·æ¨¡å¼ä»æŒ‰ä»¤ç‰Œè®¡è´¹æ”¹ä¸ºæŒ‰è®¡ç®—æ—¶é—´è®¡è´¹ï¼Œä½¿å¾—åœ¨è¿™ä¸ªå¤§å‹è®¡ç®—é¡¹ç›®ä¸­å®ç°äº†æ˜¾è‘—çš„æˆæœ¬èŠ‚çœã€‚æ€»æˆæœ¬ï¼ŒåŒ…æ‹¬æç¤ºç”Ÿæˆã€å¾®è°ƒå’Œæ¨æ–­ï¼Œä»…ä¸º$189.58ã€‚è¿™ä¸ä½¿ç”¨åƒGPT-4æˆ–GPT-3.5
    Turboè¿™æ ·çš„æ¨¡å‹çš„è´¹ç”¨å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œåè€…çš„è´¹ç”¨åˆ†åˆ«ä¸º$167,810å’Œ$7410.42ã€‚
- en: The key takeaway from this project is the significant potential for cost reduction
    when leveraging open source LLMs. By renting a GPU per hour, the cost is tied
    to the compute time rather than the number of prompts, which can lead to massive
    savings, especially for extensive projects. This approach not only makes such
    projects more financially feasible but also opens up opportunities for individuals
    and smaller organizations to undertake large compute projects without incurring
    prohibitive costs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé¡¹ç›®çš„ä¸»è¦æ”¶è·æ˜¯åˆ©ç”¨å¼€æºLLMå¯ä»¥æ˜¾è‘—é™ä½æˆæœ¬ã€‚é€šè¿‡æŒ‰å°æ—¶ç§Ÿç”¨GPUï¼Œè´¹ç”¨ä¸è®¡ç®—æ—¶é—´æŒ‚é’©ï¼Œè€Œä¸æ˜¯æç¤ºçš„æ•°é‡ï¼Œè¿™å¯ä»¥å¸¦æ¥å·¨å¤§çš„èŠ‚çœï¼Œå°¤å…¶æ˜¯å¯¹äºå¤§å‹é¡¹ç›®ã€‚è¿™ç§æ–¹æ³•ä¸ä»…ä½¿è¿™äº›é¡¹ç›®åœ¨è´¢åŠ¡ä¸Šæ›´å…·å¯è¡Œæ€§ï¼Œè€Œä¸”ä¸ºä¸ªäººå’Œå°å‹ç»„ç»‡æä¾›äº†æ‰¿æ‹…å¤§å‹è®¡ç®—é¡¹ç›®çš„æœºä¼šï¼Œè€Œä¸ä¼šäº§ç”Ÿè¿‡é«˜çš„è´¹ç”¨ã€‚
- en: The success of this project underscores the value and potential of open-source
    models in the realm of large language models and AI and serves as a testament
    to the power of innovative, cost-effective solutions in tackling complex computational
    tasks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé¡¹ç›®çš„æˆåŠŸçªæ˜¾äº†å¼€æºæ¨¡å‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’ŒAIé¢†åŸŸçš„ä»·å€¼ä¸æ½œåŠ›ï¼Œè¯æ˜äº†åˆ›æ–°ä¸”å…·æœ‰æˆæœ¬æ•ˆç›Šçš„è§£å†³æ–¹æ¡ˆåœ¨è§£å†³å¤æ‚è®¡ç®—ä»»åŠ¡ä¸­çš„åŠ›é‡ã€‚
- en: Pricing Note
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®šä»·è¯´æ˜
- en: The pricing information provided in this article is based on my personal experience
    and is intended to serve as a general comparison. Prices may vary depending on
    your region and specific circumstances. **The key takeaway is the potential cost
    savings when leveraging open source LLMs and renting a GPU per hour, rather than
    the specific prices quoted.**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æä¾›çš„å®šä»·ä¿¡æ¯åŸºäºæˆ‘çš„ä¸ªäººç»éªŒï¼Œæ—¨åœ¨ä½œä¸ºä¸€èˆ¬æ¯”è¾ƒã€‚ä»·æ ¼å¯èƒ½ä¼šæ ¹æ®ä½ æ‰€åœ¨çš„åœ°åŒºå’Œå…·ä½“æƒ…å†µæœ‰æ‰€ä¸åŒã€‚**å…³é”®ç‚¹æ˜¯åˆ©ç”¨å¼€æºLLMå’ŒæŒ‰å°æ—¶ç§ŸèµGPUçš„æ½œåœ¨æˆæœ¬èŠ‚çœï¼Œè€Œä¸æ˜¯å…·ä½“çš„æŠ¥ä»·ã€‚**
- en: 'ğŸ“¢ Hey there! If you found this article helpful, please consider:'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“¢ å—¨ï¼å¦‚æœä½ è§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ï¼š
- en: ğŸ‘ Clapping 50 times (this helps a lot!)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘ é¼“æŒ50æ¬¡ï¼ˆè¿™å¾ˆæœ‰å¸®åŠ©ï¼ï¼‰
- en: âœï¸ Leaving a comment
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: âœï¸ ç•™ä¸‹è¯„è®º
- en: ğŸŒŸ Highlighting parts you found insightful
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŒŸ çªå‡ºä½ è§‰å¾—æœ‰æ´å¯ŸåŠ›çš„éƒ¨åˆ†
- en: ğŸ‘£ Following me
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘£ å…³æ³¨æˆ‘
- en: Any questions? ğŸ¤” Donâ€™t hesitate to ask. Supporting me this way is a free and
    easy way to show appreciation for my detailed tutorial articles! ğŸ˜Š
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä»»ä½•é—®é¢˜å—ï¼ŸğŸ¤” è¯·éšæ—¶æé—®ã€‚ä»¥è¿™ç§æ–¹å¼æ”¯æŒæˆ‘æ˜¯ä¸€ç§å…è´¹ä¸”ç®€å•çš„æ–¹å¼æ¥æ„Ÿè°¢æˆ‘çš„è¯¦ç»†æ•™ç¨‹æ–‡ç« ï¼ğŸ˜Š
- en: '**GPU rental recommendations:**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPUç§Ÿèµæ¨èï¼š**'
- en: 'Best overall: [RunPod.io GPU Rentals](https://runpod.io/?ref=lemrt56t)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä½³é€‰æ‹©ï¼š[RunPod.io GPU ç§Ÿèµ](https://runpod.io/?ref=lemrt56t)
- en: 'Cheapest: [Vast.AI](https://cloud.vast.ai/?ref_id=79595)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä¾¿å®œï¼š[Vast.AI](https://cloud.vast.ai/?ref_id=79595)
- en: '**Other Deep Learning Blogs**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…¶ä»–æ·±åº¦å­¦ä¹ åšå®¢**'
- en: '[Efficiently Serving Open Source LLMs](https://medium.com/p/5f0bf5d8fd59)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[é«˜æ•ˆæœåŠ¡å¼€æºLLM](https://medium.com/p/5f0bf5d8fd59)'
- en: '[Computer Vision by Andrew Ng â€” 11 Lessons Learned](https://medium.com/towards-data-science/efficiently-serving-open-source-llms-5f0bf5d8fd59)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[Andrew Ngçš„è®¡ç®—æœºè§†è§‰ â€” 11èŠ‚è¯¾å­¦åˆ°çš„çŸ¥è¯†](https://medium.com/towards-data-science/efficiently-serving-open-source-llms-5f0bf5d8fd59)'
- en: '[Deep Learning Specialization by Andrew Ng â€” 21 Lessons Learned](/deep-learning-specialization-by-andrew-ng-21-lessons-learned-15ffaaef627c)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[Andrew Ngçš„æ·±åº¦å­¦ä¹ ä¸“ä¿®è¯¾ç¨‹ â€” 21èŠ‚è¯¾å­¦åˆ°çš„çŸ¥è¯†](/deep-learning-specialization-by-andrew-ng-21-lessons-learned-15ffaaef627c)'
