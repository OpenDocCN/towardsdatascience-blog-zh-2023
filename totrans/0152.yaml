- en: 9 Tips for Training Models on your University’s HPC Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/9-tips-for-training-models-on-your-universitys-hpc-cluster-a703eb87f3d6](https://towardsdatascience.com/9-tips-for-training-models-on-your-universitys-hpc-cluster-a703eb87f3d6)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to effectively run and debug code in a resource-constrained environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://conorosullyds.medium.com/?source=post_page-----a703eb87f3d6--------------------------------)[![Conor
    O''Sullivan](../Images/2dc50a24edb12e843651d01ed48a3c3f.png)](https://conorosullyds.medium.com/?source=post_page-----a703eb87f3d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a703eb87f3d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a703eb87f3d6--------------------------------)
    [Conor O''Sullivan](https://conorosullyds.medium.com/?source=post_page-----a703eb87f3d6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a703eb87f3d6--------------------------------)
    ·6 min read·Mar 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5fc4bc25739d8d8186821bd8e9d602a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Martijn Baudoin](https://unsplash.com/ja/@martijnbaudoin?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Queue job, wait 24 hours, **cuda runtime error:** out of memory
  prefs: []
  type: TYPE_NORMAL
- en: Queue job, wait 24 hours, **FileNotFoundError:** No such file or directory
  prefs: []
  type: TYPE_NORMAL
- en: Queue job, wait 24 hours, **RuntimeError:** stack expects each tensor…
  prefs: []
  type: TYPE_NORMAL
- en: AHHHGH!!!
  prefs: []
  type: TYPE_NORMAL
- en: Debugging code on a high-performance computing (HPC) cluster can be incredibly
    frustrating. To make matters worse, at university you will be sharing resources
    with other students. Jobs will be added to a queue. You can wait hours before
    you even know if your code has errors.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve recently been training models on my university’s HPC. I’ve learned some
    things (the hard way). I want to share these tips and tricks to hopefully make
    your experience a bit smoother. I’ll keep things general so you can apply them
    to any system.
  prefs: []
  type: TYPE_NORMAL
- en: Do as much development on your personal machine as possible
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An HPC cluster has one objective — crunch the numbers. No fancy IDE, no debugger
    and no co-pilot. Do you really want to code your entire project using vim?
  prefs: []
  type: TYPE_NORMAL
- en: To avoid going insane (save that for your thesis), you should develop the code
    on your own machine. Train the model for at least **1 epoch** using a **small
    sample**. Include tests to make sure the data is loaded correctly and that all
    results are saved. It is no fun training a model for 50 epochs to find you forgot
    to save the best one (yes, I did this).
  prefs: []
  type: TYPE_NORMAL
- en: Keep the code simple
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any additional steps you include in the code will increase the chance of a failed
    run. You should only run the processes that need heavy computing power. For example,
    after the model has been trained you will want to evaluate it. This can be done
    on your local machine. If your test set is small enough even a CPU will do.
  prefs: []
  type: TYPE_NORMAL
- en: Do not hardcode any paths or variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When moving the code from your local machine to HPC, some things will inevitably
    have to change. For example, the file paths for loading data and saving results.
    I was developing on a Mac but the HPC has a Linux operating system. This meant
    I needed to change the device from “mps” to “cuda”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/049220951feba249272f10d2004a4fe4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: examples of variables to be updated (source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: Do not hardcode anything that will need to change. Use variables and define
    them right at the top of your script. This makes them easy to change and you will
    not forget anything. Trust me! You do not want to scroll through lines of code
    using vim.
  prefs: []
  type: TYPE_NORMAL
- en: Increase batch size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In **Figure 1** above, you can see one of the variables I included is **batch_size.**
    This is the number of samples that are loaded and used to update model parameters
    at each iteration. A larger batch size means you can process more samples in parallel
    on a GPU leading to faster training times.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing this on your local machine will quickly lead to a “**cuda runtime
    error:** out of memory”. In comparison, the HPC can handle a larger batch size.
    However, it is [important to not increase it too much](https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e)
    as this can have a negative effect on model accuracy. I simply doubled the batch
    size from 32 to 64.
  prefs: []
  type: TYPE_NORMAL
- en: Use system arguments for any experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As far as possible, you want to avoid editing code on the HPC. At the same time,
    to train the best model you will need to do experiments. To get around this, use
    system arguments. As seen in **Figure 2**, these allow you to update variables
    in your script at the command line.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69e87f0472b4305dd0f21d9519f6409b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: examples of system arguments (source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: The first one allows me to update the save path of the final model (**model_nam**e).
    The others allow me to **sample, scale** and **clean** the data in various ways
    (see **Figure 3**). You could even update the model architecture. For example,
    by passing in a list [x,y,z] that defines the number of nodes in the hidden layers
    of a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1739baee9a425eb964921541c87ec490.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: training a U-Net with 4 data cleaning experiments (source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: Include a sample argument
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **sample** system argument is particularly useful. The one I’ve included
    above is a binary flag. If set to true, the modelling code will be run using a
    subset of 1000 samples. You could also pass in the actual number of samples as
    an integer.
  prefs: []
  type: TYPE_NORMAL
- en: Often I schedule 2 jobs right after each other — the sampled and full dataset
    runs. The sampled run will usually complete within a few minutes (unless someone
    is hogging all the GPUs!!). This helps point out any pesky bugs. If something
    goes wrong, I have the opportunity to stop the full dataset run, correct it and
    kick it off again before the end of the day.
  prefs: []
  type: TYPE_NORMAL
- en: Be verbose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'No, I’m not talking about your friend majoring in finance. Your code should
    tell you as much as possible. This will help fix any bugs that do occur. Use those
    print statements! Some things I find useful to print on each run:'
  prefs: []
  type: TYPE_NORMAL
- en: The system device (i.e. cuda or CPU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Length of the dataset, training and validation sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sample of the data before and after any transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and validation loss for each epoch and batch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value for any system arguments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You never know what is going to go wrong. For machine learning, logical errors
    can be difficult to identify. Your code can run perfectly but produce a model
    that is absolute garbage. Having a record of the process will help you trace back
    to where an error is coming from. This is especially important if you want to
    run multiple experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Use a diff checker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Things will go wrong. Very wrong. I broke my own rules and made a few changes
    to the script on the HPC. Okay, I made many changes. I lost track of what I had
    done and the code would not run properly. This is where a [diff checker](https://www.diffchecker.com/text-compare/#editor)
    came in handy.
  prefs: []
  type: TYPE_NORMAL
- en: It will compare, line-by-line, the text in one document to another. I used it
    to compare the script on the HPC to one of my local machines. This pointed out
    what I had changed and I could immediately identify the problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfd059907e9e9f6d1e4e3d24fee2d307.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: pointing out the difference in HPC and local script (source: [diff
    checker](https://www.diffchecker.com/text-compare/#editor))'
  prefs: []
  type: TYPE_NORMAL
- en: Be realistic about your research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tips so far have lacked detail. There is a lot you can do to [speed up the
    training for specific model packages (e.g. XGBoost)](https://medium.com/towards-data-science/how-to-speed-up-xgboost-model-training-fcf4dc5dbe5f).
    [Keeping better track of your data and models](https://medium.com/@iamleonie/intro-to-mlops-data-and-model-versioning-fa623c220966)
    can also help avoid errors and confusion. But, really, there is only so much you
    can do. My final tip is to be realistic about what you can achieve with your available
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: In the era of LLMs, this may be disheartening. It seems like all the major breakthroughs
    are achieved with increasing computational power. You must consider that a model
    the size of GPT-3 is [estimated to cost **$450,000** to train](https://www.mosaicml.com/blog/gpt-3-quality-for-500k#).
    This rivals some departments' entire budgets! You simply cannot compete.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, you can still make a valuable contribution. Fine-tuning models do not need
    as many resources. Collecting and labelling data is often a more significant contribution
    than chasing SOTA on benchmarks. Every sub-domain to which you apply machine learning
    will come with its own challenges. Often a lack of resources will lead to more
    innovative solutions to these challenges. So be thankful for those long job queues…
  prefs: []
  type: TYPE_NORMAL
- en: Oh, who am I kidding!! Anyone want to buy me a GPU?
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed this article! You can support me by becoming one of my [**referred
    members**](https://conorosullyds.medium.com/membership) **:)**
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://conorosullyds.medium.com/membership?source=post_page-----a703eb87f3d6--------------------------------)
    [## Join Medium with my referral link — Conor O’Sullivan'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: conorosullyds.medium.com](https://conorosullyds.medium.com/membership?source=post_page-----a703eb87f3d6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '| [Twitter](https://twitter.com/conorosullyDS) | [YouTube](https://www.youtube.com/channel/UChsoWqJbEjBwrn00Zvghi4w)
    | [Newsletter](https://mailchi.mp/aa82a5ce1dc0/signup) — sign up for FREE access
    to a [Python SHAP course](https://adataodyssey.com/courses/shap-with-python/)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Michael Galarnyk](https://medium.com/u/c07aac64b6e1?source=post_page-----a703eb87f3d6--------------------------------)
    **How to Speed Up XGBoost Model Training** [https://medium.com/towards-data-science/how-to-speed-up-xgboost-model-training-fcf4dc5dbe5f](https://medium.com/towards-data-science/how-to-speed-up-xgboost-model-training-fcf4dc5dbe5f)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Leonie Monigatti](https://medium.com/u/3a38da70d8dc?source=post_page-----a703eb87f3d6--------------------------------)
    **Intro to MLOps: Data and Model Versionin**g [https://medium.com/@iamleonie/intro-to-mlops-data-and-model-versioning-fa623c220966](https://medium.com/@iamleonie/intro-to-mlops-data-and-model-versioning-fa623c220966)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Kevin Shen](https://medium.com/u/c1ed18ad484c?source=post_page-----a703eb87f3d6--------------------------------)
    **Effect of batch size on training dynamics** [https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e](https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e)'
  prefs: []
  type: TYPE_NORMAL
