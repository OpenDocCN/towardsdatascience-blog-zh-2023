# 使用 Athena 和 MySQL 构建批量数据管道

> 原文：[https://towardsdatascience.com/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c](https://towardsdatascience.com/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c)

## 初学者的端到端教程

[](https://mshakhomirov.medium.com/?source=post_page-----7e60575ff39c--------------------------------)[![💡Mike Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----7e60575ff39c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e60575ff39c--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e60575ff39c--------------------------------) [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----7e60575ff39c--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e60575ff39c--------------------------------) ·16 min 阅读·2023 年 10 月 20 日

--

![](../Images/368293b91e4bc0283007a555789b6479.png)

图片由 [Redd F](https://unsplash.com/@raddfilms?utm_source=medium&utm_medium=referral) 提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

在这个故事中，我将讲述一种非常流行的数据转换任务执行方式——批量数据处理。当我们需要以块状方式处理数据时，这种数据管道设计模式变得极其有用，非常适合需要调度的 ETL 作业。我将通过使用 MySQL 和 Athena 构建数据转换管道来展示如何实现这一目标。我们将使用基础设施即代码在云中部署它。

想象一下，你刚刚作为数据工程师加入了一家公司。他们的数据堆栈现代、事件驱动、成本效益高、灵活，并且可以轻松扩展以满足不断增长的数据资源。你数据平台中的外部数据源和数据管道由数据工程团队管理，使用具有 CI/CD GitHub 集成的灵活环境设置。

作为数据工程师，你需要创建一个业务智能仪表板，展示公司收入来源的地理分布，如下所示。原始支付数据存储在服务器数据库（MySQL）中。你想构建一个批量管道，从该数据库中每日提取数据，然后使用 AWS S3 存储数据文件，并使用 Athena 进行处理。

![](../Images/7dc86278ad5d6755486da64418c7b7bf.png)

收入仪表板。图像由作者提供。

## 批量数据管道

数据管道可以被视为一系列数据处理步骤。由于这些阶段之间的***逻辑数据流连接***，每个阶段生成的**输出**作为下一个阶段的**输入**。

> 只要在点 A 和点 B 之间进行数据处理，就存在数据管道。

数据管道可能因其概念和逻辑性质而有所不同。我之前在这里写过 [1]：

[数据管道设计模式](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----7e60575ff39c--------------------------------)

### 选择合适的架构及其示例

[数据管道设计模式](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----7e60575ff39c--------------------------------)

我们希望创建一个数据管道，在以下 **步骤** 中转换数据：

1\. 使用 Lambda 函数将数据从 MySQL 数据库表 `myschema.users` 和 `myschema.transactions` 提取到 S3 数据湖桶中。

2\. 添加一个具有 Athena 资源的状态机节点以启动执行 (`arn:aws:states:::athena:startQueryExecution.sync`) 并创建一个名为 `mydatabase` 的数据库

3\. 创建另一个数据管道节点以显示 Athena 数据库中的现有表。使用该节点的输出执行所需的数据转换。

如果表不存在，我们希望我们的管道在 Athena 中根据来自数据湖 S3 桶的数据创建它们。我们希望创建两个 **外部表**，数据来自 MySQL：

+   mydatabase.users (LOCATION ‘s3://<YOUR_DATALAKE_BUCKET>/data/myschema/users/’)

+   mydatabase.transactions (LOCATION ‘s3://<YOUR_DATALAKE_BUCKET>/data/myschema/transactions/’)

然后我们希望创建一个 **优化的 ICEBERG** 表：

+   mydatabase.user_transactions (‘table_type’=’ICEBERG’, ‘format’=’parquet’) 使用以下 SQL：

[PRE0]

+   我们还将使用 MERGE 来更新此表。

MERGE 是一种非常有用的 SQL 技巧，用于表中的增量更新。查看我之前的故事 [3] 以获取更高级的示例：

[高级 SQL 技巧](https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----7e60575ff39c--------------------------------)

### 从 1 到 10，你的数据仓库技能有多好？

[高级 SQL 技巧](https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----7e60575ff39c--------------------------------)

Athena 可以通过运行有吸引力的即席 SQL 查询来分析存储在 Amazon S3 中的结构化、非结构化和半结构化数据，无需管理基础设施。

> 我们不需要加载数据，这使得它成为我们任务的完美选择。

它可以轻松地与 Business Intelligence (BI) 解决方案如 QuickSight 集成以生成报告。

ICEBERG 是一种非常有用且高效的表格格式，多个独立程序可以同时且一致地处理相同的数据集 [2]。我之前在这里写过：

[介绍 Apache Iceberg 表](https://towardsdatascience.com/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----7e60575ff39c--------------------------------)

### 选择 Apache Iceberg 作为数据湖的几个有力理由

[介绍 Apache Iceberg 表](https://towardsdatascience.com/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----7e60575ff39c--------------------------------)

## MySQL 数据连接器

让我们创建一个 AWS Lambda 函数，它能够在 MySQL 数据库中执行 SQL 查询。

> 代码非常简单且通用。它可以在任何无服务器应用程序中与任何云服务提供商一起使用。

我们将使用它将收入数据提取到数据湖中。建议的 Lambda 文件夹结构如下所示：

[PRE1]

我们将通过 AWS Step Functions 将这个小服务集成到管道中，以便于 **编排和可视化**。

为了创建一个能够从 MySQL 数据库中提取数据的 Lambda 函数，我们需要先为我们的 Lambda 创建一个文件夹。首先创建一个名为 stack` 的新文件夹，然后在其中创建一个名为 `mysql_connector` 的文件夹：

[PRE2]

然后我们可以使用下面的代码（将数据库连接设置替换为你的设置）来创建 `app.py`：

[PRE3]

要使用 AWS CLI 部署我们的微服务，请在命令行中运行以下命令（假设你在 ./stack 文件夹中）：

[PRE4]

确保在运行下一部分之前 AWS Lambda 角色已经存在 ` — role arn:aws:iam::<your-aws-account-id>:role/my-lambda-role`。

[PRE5]

我们的 MySQL 实例必须具备 **S3 集成**，以便 **将数据导出到 S3** 桶。这可以通过运行以下 SQL 查询实现：

[PRE6]

## 如何创建 MySQL 实例

我们可以使用 CloudFormation 模板和基础设施即代码来创建 MySQL 数据库。考虑这个 AWS 命令：

[PRE7]

它将使用 `cfn_mysql.yaml` 模板文件来创建名为 MySQLDB 的 CloudFormation 堆栈。我之前在这里写过有关它的内容 [4]：

[](/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a?source=post_page-----7e60575ff39c--------------------------------) [## 使用 AWS CloudFormation 创建 MySQL 和 Postgres 实例

### 数据库从业人员的基础设施即代码

towardsdatascience.com](/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a?source=post_page-----7e60575ff39c--------------------------------)

我们的 `cfn_mysql.yaml` 应该如下所示：

[PRE8]

如果一切顺利，我们将看到 Amazon 账户中出现一个新的堆栈：

![](../Images/d6ff5754b68a1d8279412c0bb82d917b.png)

带有 MySQL 实例的 CloudFormation 堆栈。图片由作者提供。

现在我们可以在我们的数据管道中使用这个 MySQL 实例。我们可以在任何 SQL 工具中尝试我们的 SQL 查询，例如 SQL Workbench，以填充表数据。这些表将用于稍后使用 Athena 创建外部表，可以通过 SQL 创建：

[PRE9]

## 使用 Athena 处理数据

现在我们希望添加一个数据管道工作流，该工作流触发我们的 Lambda 函数以从 MySQL 提取数据，将其保存到数据湖中，然后在 Athena 中开始数据转换。

我们希望使用 MySQL 中的数据创建两个外部 Athena 表：

+   myschema.users

+   myschema.transactions

然后我们希望创建一个优化的 ICEBERG 表 **myschema.user_transactions**，将其连接到我们的 BI 解决方案。

我们希望使用 MERGE 语句将新数据插入到该表中。

[PRE10]

当新表准备好后，我们可以通过运行 `SELECT *` 来检查它：

![](../Images/0067053393a777846c94f0d3acd48e90.png)

mydatabase.user_transactions。图片由作者提供。

## 使用Step Functions（状态机）编排数据管道

在之前的步骤中，我们学习了如何分别部署数据管道的每一步并进行测试。在这一段中，我们将了解如何使用基础设施代码和管道编排工具如AWS Step Functions（状态机）创建一个完整的数据管道。当我们完成时，管道图将如下所示：

![](../Images/3d6fa66e126398bfe0bd267f57bf7059.png)

使用Step Functions进行数据管道编排。图像由作者提供。

数据管道编排是一种很好的数据工程技术，它为我们的数据管道增加了互动性。这个想法在我之前的一篇故事中已经解释过[5]：

[](/data-pipeline-orchestration-9887e1b5eb7a?source=post_page-----7e60575ff39c--------------------------------) [## 数据管道编排

### 数据管道管理得当可以简化部署并提高数据的可用性和可访问性……

[towardsdatascience.com](/data-pipeline-orchestration-9887e1b5eb7a?source=post_page-----7e60575ff39c--------------------------------)

要部署完整的**编排器解决方案**，包括所有必要的资源，我们可以使用CloudFormation（基础设施即代码）。考虑下面这个可以在`/stack`文件夹中从命令行运行的脚本。确保<YOUR_S3_BUCKET>存在，并将其替换为您的实际S3桶：

[PRE11]

它将使用stack.yaml创建一个名为BatchETLpipeline的CloudFormation堆栈。它将打包我们的Lambda函数，创建一个包并将其上传到S3桶中。如果该桶不存在，它将创建它。然后将部署管道。

[PRE12]

如果一切顺利，我们的新数据管道的堆栈将被部署：

![](../Images/2b97e39b295eadfc8d179a499197fad0.png)

BatchETLpipeline堆栈和资源。图像由作者提供。

如果我们点击状态机资源，然后点击‘编辑’，我们将看到我们的ETL管道作为图形展示：

![](../Images/cb9292383481ab8b5951980275a644a9.png)

批量数据管道的工作流工作室。图像由作者提供。

现在我们可以执行管道以运行所有必要的数据转换步骤。点击‘开始执行’。

![](../Images/3d6fa66e126398bfe0bd267f57bf7059.png)

成功执行。图像由作者提供。

现在我们可以将我们的Athena表连接到我们的**BI解决方案**。连接我们最终的Athena数据集`mydataset.user_transactions`以创建仪表盘。

![](../Images/3427a98727ee0bbf52ef9d94f868f567.png)

连接Quicksight中的数据集。图像由作者提供。

我们只需调整几个设置，使我们的仪表盘看起来像这样：

![](../Images/2fc0d03bb10de92d3b88a7f4a7a3b3ed.png)

Quicksight仪表盘。图像由作者提供。

我们希望使用`dt`作为维度，`total_cost_usd`作为指标。我们还可以为每个`user_id`设置一个拆分维度。

## 结论

批处理数据管道很受欢迎，因为历史上工作负载主要是批处理型的数据环境。我们刚刚建立了一个 ETL 数据管道，从 MySQL 中提取数据并在数据湖中转换。该模式最适用于数据集不大且需要持续处理的情况，因为 Athena 根据扫描的数据量收费。这种方法在将数据转换为列式格式如 Parquet 或 ORC 时表现良好，结合几个小文件成较大的文件，或进行分桶和添加分区。我以前在我的一个故事中写过这些大数据文件格式[6]。

[](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----7e60575ff39c--------------------------------) [## 大数据文件格式解析

### Parquet 与 ORC 与 AVRO 与 JSON。该选择哪一个，如何使用它们？

towardsdatascience.com](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----7e60575ff39c--------------------------------)

我们学习了如何使用 Step Functions 来编排数据管道，视觉化数据流从源头到最终用户，并使用基础设施即代码进行部署。这个设置使得我们可以对数据管道应用 CI/CD 技术[7]。

希望这个教程对你有帮助。如果你有任何问题，请告诉我。

## 推荐阅读

[1] [https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](/data-pipeline-design-patterns-100afa4b93e3)

[2] [https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009](https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009)

[3] [https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488](https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488)

[4] [https://medium.com/towards-data-science/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a](https://medium.com/towards-data-science/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a)

[5] [https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a](https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a)

[6] [https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9](https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9)

[7] [https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)
