["```py\npip install CFXplorer\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom cfxplorer import Focus\nfrom sklearn.datasets import make_classification\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.tree import DecisionTreeClassifier\n```", "```py\ndef generate_example_data(rows: int = 1000):\n    \"\"\"\n    Generate random data with a binary target variable and 10 features.\n\n    Args:\n        rows (int): The number of rows in the generated dataset.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the randomly generated data.\n\n    \"\"\"\n    X, y = make_classification(\n        n_samples=rows, n_features=10, n_classes=2, random_state=42\n    )\n\n    return train_test_split(X, y, test_size=0.2, random_state=42)\n```", "```py\ndef standardize_features(x_train, x_test):\n    \"\"\"\n    Standardizes the features of the input data using Min-Max scaling.\n\n    Args:\n        x_train (pandas.DataFrame or numpy.ndarray): The training data.\n        x_test (pandas.DataFrame or numpy.ndarray): The test data.\n\n    Returns:\n        tuple: A tuple containing two pandas DataFrames.\n            - The first DataFrame contains the standardized features of the training data.\n            - The second DataFrame contains the standardized features of the test data.\n    \"\"\"\n    # Create a MinMaxScaler object\n    scaler = MinMaxScaler(feature_range=(0, 1))\n\n    # Fit and transform the data to perform feature scaling\n    scaler = scaler.fit(x_train)\n    scaled_x_train = scaler.transform(x_train)\n    scaled_x_test = scaler.transform(x_test)\n\n    # Create a new DataFrame with standardized features\n    standardized_train = pd.DataFrame(scaled_x_train)\n    standardized_test = pd.DataFrame(scaled_x_test)\n\n    return standardized_train, standardized_test\n```", "```py\ndef train_decision_tree_model(X_train, y_train):\n    \"\"\"\n    Train a decision tree model using scikit-learn.\n\n    Args:\n        X_train (array-like or sparse matrix of shape (n_samples, n_features)): The training input samples.\n        y_train (array-like of shape (n_samples,)): The target values for training.\n\n    Returns:\n        sklearn.tree.DecisionTreeClassifier: The trained decision tree model.\n\n    \"\"\"\n    # Create and train the decision tree model\n    model = DecisionTreeClassifier(random_state=42)\n    model.fit(X_train, y_train)\n\n    return model\n```", "```py\n X_train, X_test, y_train, y_test = generate_example_data(1000)\n  X_train, X_test = standardize_features(X_train, X_test)\n  model = train_decision_tree_model(X_train, y_train)\n```", "```py\nfocus = Focus(\n      num_iter=1000,\n      distance_function=\"cosine\",\n  )\n```", "```py\ndistance_function: str, optional (default=\"euclidean\")\n    Distance function - one of followings;\n        - \"euclidean\"\n        - \"cosine\"\n        - \"l1\"\n        - \"mahalabobis\"\n\noptimizer: Keras optimizer, optional (default=tf.keras.optimizers.Adam())\n    Optimizer for gradient decent\n\nsigma: float, optional (default=10.0)\n    Sigma hyperparameter value for hinge loss\n\ntemperature: float, optional (default=1.0)\n    Temperature hyperparameter value for hinge loss\n\ndistance_weight: float, optional (default=0.01)\n    Weight hyperparameter for distance loss\n\nlr: float, optional (default=0.001)\n    Learning rate for gradient descent optimization\n\nnum_iter: int, optional (default=100)\n    Number of iterations for gradient descent optimization\n\ndirection: str, optional (default=\"both\")\n    Direction of perturbation (e.g. both, positive and negative)\n\nhyperparameter_tuning: bool, optional (default=False)\n    if True, generate method returns unchanged_ever and mean_distance\n\nverbose: int, optional (default=1)\n    Verbosity mode.\n        - 0: silent\n        - else: print current number of iterations\n```", "```py\nperturbed_feats = focus.generate(model, X_test, X_train)\n```", "```py\ndef plot_pca(plot_df, focus_plot_df):\n    \"\"\"\n    Plots the PCA-transformed features and corresponding predictions before and after applying FOCUS.\n\n    Args:\n        plot_df (pandas.DataFrame): A DataFrame containing the PCA-transformed features and\n            predictions before applying FOCUS.\n        focus_plot_df (pandas.DataFrame): A DataFrame containing the PCA-transformed features and\n            predictions after applying FOCUS.\n\n    Returns:\n        None: This function displays the plot but does not return any value.\n    \"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n    sns.scatterplot(\n        data=focus_plot_df, x=\"pca1\", y=\"pca2\", hue=\"predictions\", ax=axes[0]\n    )\n    axes[0].set_title(\"After applying FOCUS\")\n    sns.scatterplot(data=plot_df, x=\"pca1\", y=\"pca2\", hue=\"predictions\", ax=axes[1])\n    axes[1].set_title(\"Before applying FOCUS\")\n    fig.suptitle(\"Prediction Before and After FOCUS comparison\")\n    plt.show()\n\nplot_df, focus_plot_df = prepare_plot_df(model, X_test, perturbed_feats)\nplot_pca(plot_df, focus_plot_df)\n```", "```py\nimport optuna\nimport tensorflow as tf\n\nfrom cfxplorer import Focus\n\ndef objective(trial):\n    \"\"\"\n    This function is an objective function for\n    hyperparameter tuning using optuna.\n    It explores the hyperparameter sets and evaluates the result on a\n    given model and dataset\n\n    Mean distance and number of unchanged instances are\n    used for the evaluation.\n\n    Args:\n    trial (optuna.Trial):\n    Object that contains information about the current trial,\n    including hyperparameters.\n\n    Returns:\n    Mean CFE distance + number of unchanged instances squared -\n    This is the objective function for hyperparameter optimization\n\n    * Note: typically we want to minimise a number of unchanged first,\n        so penalising the score by having squared number.\n    Also, to not distort this objective,\n    having the mean distance divided by 100.\n    \"\"\"\n    X_train, X_test, y_train, y_test = generate_example_data(1000)\n    X_train, X_test = standardize_features(X_train, X_test)\n    model = train_decision_tree_model(X_train, y_train)\n\n    focus = Focus(\n        num_iter=1000,\n        distance_function=\"euclidean\",\n        sigma=trial.suggest_int(\"sigma\", 1, 20, step=1.0),\n        temperature=0,  # DT models do not use temperature\n        distance_weight=round(\n            trial.suggest_float(\"distance_weight\", 0.01, 0.1, step=0.01), 2\n        ),\n        lr=round(trial.suggest_float(\"lr\", 0.001, 0.01, step=0.001), 3),\n        optimizer=tf.keras.optimizers.Adam(),\n        hyperparameter_tuning=True,\n        verbose=0,\n    )\n\n    best_perturb, unchanged_ever, cfe_distance = focus.generate(model, X_test)\n\n    print(f\"Unchanged: {unchanged_ever}\")\n    print(f\"Mean distance: {cfe_distance}\")\n\n    return cfe_distance / 100 + pow(unchanged_ever, 2)\n```", "```py\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=100)\n\n    print(f\"Number of finished trials: {len(study.trials)}\")\n\n    trial = study.best_trial\n\n    print(\"Best trial:\")\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n```"]