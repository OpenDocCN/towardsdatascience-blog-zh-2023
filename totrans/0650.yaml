- en: Data Modelling For Data Engineers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/data-modelling-for-data-engineers-93d058efa302](https://towardsdatascience.com/data-modelling-for-data-engineers-93d058efa302)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The definitive guide for beginners
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----93d058efa302--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----93d058efa302--------------------------------)[](https://towardsdatascience.com/?source=post_page-----93d058efa302--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----93d058efa302--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----93d058efa302--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----93d058efa302--------------------------------)
    ·9 min read·Dec 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38909e6454cd7c166ad2897c1828b42d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sebastian Svenson](https://unsplash.com/@sebastiansvenson?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Data modelling is an essential part of data engineering. In this story, I would
    like to talk about different data models, the role of SQL in data transformation
    and the data enrichment process. SQL is a powerful tool that helps to manipulate
    data. With data transformation pipelines we can transform and enrich data loaded
    into our data platform. We will discuss various methods of data manipulation,
    scheduling and incremental table updates. In order to make this process efficient
    we would want to know a few essential things about data modelling first.
  prefs: []
  type: TYPE_NORMAL
- en: What is data modelling?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **data model** aims to organise elements of your data and standardise how
    the data elements relate to one another.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Models** ensure the quality of the data, semantic configurations and
    consistency in naming conventions. It helps to design the database ***conceptually***
    and create logical connections between data elements, i.e. primary and foreign
    keys, tables, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Good and thorough **data model design** is crucial if you need the most reliable
    and cost-effective data transformation for your data platform. It guarantees that
    the data is processed without delays and unnecessary steps.
  prefs: []
  type: TYPE_NORMAL
- en: Companies use a procedure known as **dimensional data modelling** to process
    data. **Source** — **Production** — **Analytics** level split between schemas
    (datasets) enables effective data governance and makes sure our data is ready
    for business intelligence and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Any measurable information is being stored in **fact tables**, i.e. ***transactions,
    sessions, requests, etc.***
  prefs: []
  type: TYPE_NORMAL
- en: '**Foreign keys** are used in the fact tables, and they are connected to Dimension
    Tables. **Dimension Tables** have descriptive data that is linked to the Fact
    Table, i.e. ***brand, product type/code, country, etc.***'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensions and Facts** based on business requirements are being tied into
    the **Schema**.'
  prefs: []
  type: TYPE_NORMAL
- en: The two most popular schema types are **Star and Snowflake.** Not to say that
    these are the most frequent questions during data engineering job interviews [1].
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-engineering-interview-questions-fdef62e46505?source=post_page-----93d058efa302--------------------------------)
    [## Data Engineering Interview Questions'
  prefs: []
  type: TYPE_NORMAL
- en: Tips to prepare for a job interview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-engineering-interview-questions-fdef62e46505?source=post_page-----93d058efa302--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Star Schema**: The Star Schema is the Schema with a **highly denormalised**
    structure. This is very common in modern data warehouses where data is repeated
    in many tables. Each Dimension represents one Dimension Table. Dimension Tables
    have foreign keys that are joined with the Fact Tables to get the results. Data
    redundancy is high, but due to the simplicity of the schema design, it can offer
    performance benefits. It is more denormalised; It means fewer joins between tables,
    and therefore, it is optimised for querying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9b14f90ff13ea78dd0d641f374a68a32.png)'
  prefs: []
  type: TYPE_IMG
- en: Star Schema example. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Snowflake Schema**: Snowflake schema extends Star schema with more normalised
    dimensional tables. This Schema uses less disk space as all the tables are normalised
    and split down into further dimension tables. It is easy to add Dimensions to
    this Schema, and the data redundancy is also less because of the complex Schema
    design.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8adb908d8436beaba878067695fffce1.png)'
  prefs: []
  type: TYPE_IMG
- en: Snowflake Schema example. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Galaxy Schema**: Similar to the schemas stated above, a galaxy schema has
    several fact tables from **more than one-dimensional models**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational, Dimensional and Multi-dimensional data modelling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pure **relational data modelling** is often seen in OLTP data pipelines and
    relational (transactional) databases feeding data into the application. In this
    scenario data transaction speed and size is crucial. In this case, we would want
    to apply a highly-normalised data schema design. Data is always up-to-date but
    running reports might become a challenging task due to a high number of joins
    on various dimension tables.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensional modelling** as a process aims to ensure the most efficient data
    storage structure and faster data transformation for large amounts of data. The
    reason we would want to do this is simple — a more performant data platform with
    easy management and data governance. For instance, Snowflake schema helps to reduce
    data redundancy which is opposite to Star schema and is more suitable for OLAP
    analytics and reporting (no need to perform expensive joins).'
  prefs: []
  type: TYPE_NORMAL
- en: The **Multi-Dimensional Data Model** represents data in **cubes**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/238f2155f5f94f08b9e98346270ad4b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Data points (values) in a three-dimensional data cube. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: It adds complexity but also makes data structure more efficient for complex
    reporting. Data is stored in aggregated cubes (tables) which adds more capabilities
    for users to perform robust OLAP data transformations, i.e. pivoting, drill-down
    features, and advanced dashboarding with multiple dimensions simultaneously. Often
    it is not necessary to aggregate and preserve the data. We can define the data
    model as a **cube** using SQL with ROLLUP [2].
  prefs: []
  type: TYPE_NORMAL
- en: The ROLLUP function is used to perform aggregation at multiple levels. This
    is useful when you have to work with dimension graphs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/866e98ef7b2cc52b52fefcdbf67e3bd9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Consider this SQL below. It provides input data and a required SQL transformation
    to create a multi-dimensional data model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Any dataset we create as a custom SQL query can be considered as a dimensional
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----93d058efa302--------------------------------)
    [## Advanced SQL techniques for beginners'
  prefs: []
  type: TYPE_NORMAL
- en: On a scale from 1 to 10 how good are your data warehousing skills?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----93d058efa302--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Dimensional and multi-dimensional models are very often seen in data warehouse
    solutions. The dimensional data model makes schemas simple and more clear for
    business users.
  prefs: []
  type: TYPE_NORMAL
- en: How to create a dimensional model?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To introduce a dimensional model element into our data platform design we create
    dimensional models. It can be a schema or a database or even a part of our data
    platform in the data lake that can describe various business processes defined
    by many fact tables. Let’s consider a single business process with two fact tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we would want to **identify a business process** we need to transform
    into our data model and define the **granularity** of each fact table. Let’s imagine
    it’s an in-app store purchase with the following data in fact table rows for a
    payment transaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Granularity** refers to the level of information stored in our table. For
    example, in our example, the in-app payments are recorded with each transaction;
    hence, the granularity is **transaction-level** which can be easily transformed
    into **daily**. It is important that the fact tables in our dimensional model
    are consistent with the granularity specified. It means we would want to load
    atomic data into the same dimensional structures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically we would want to **transform and incrementally update** data like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to define **the dimensions** for each fact table. In our case,
    we have ***payment transactions*** as facts. These **facts** can contain such
    useful data as the total number of items sold, currency exchange rates, total
    revenue, tax, etc. Further **dimensions** will provide context for our in-app
    purchase data, i.e. **time, product and user** dimension data. Each dimension
    can help to answer important business questions. *For instance, we might want
    to know which is the best-selling product or what was the average spend among
    users.*
  prefs: []
  type: TYPE_NORMAL
- en: Each in-app payment transaction will have foreign keys describing fact-to-dimension
    relationships and point to a relevant product, time of purchase or user profile.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we would want to define **hierarchies. Hierarchies** help business users
    to understand different levels of **granularity** of our dataset. It also helps
    with navigation and advanced reporting features, i.e. drill down. For example,
    the product dimension might have a **hierarchy** that goes from `category` to
    `type` to `id` to `sub_id`.
  prefs: []
  type: TYPE_NORMAL
- en: We would want to use some calculations in our model called **Measures**. Measures
    are what we calculate using our fact table to generate data insights. It can be
    total `revenue`, `net revenue`, `tax amount`, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this multi-dimensional model businesses could answer some crucial questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What was our total revenue for each product in the last year or month?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users with what reputation level spend more?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which product was the best-selling one?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How revenue this year can be compared to the same period last year?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we would want to verify our data model with users.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of Dimensional Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Flexibility.** A dimensional modelling framework extends the data warehousing
    process. The design is highly adaptable to accommodate new business requirements
    or to alter the central repository. To represent updated business processes, new
    entities may be added to the model or the layout of existing ones might be adjusted.
    Dimensional models can handle these changes with ease. More columns can be added
    to dimension tables without disrupting current business intelligence applications
    that use the original tables.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-dimensional Analytics:** Like it was demonstrated in the previous example
    we can use extra dimensions to create better graphs and more complex charts. Hierarchies
    help to understand data and analyse it simultaneously with drill-down features
    added to reporting dashboards. It is optimised for business intelligence (BI)
    needs and is easy to understand. Modern BI tools have built-in AI capabilities
    natively designed to understand dimension structure and data [3].'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/artificial-intelligence-in-analytics-f11d2deafdf0?source=post_page-----93d058efa302--------------------------------)
    [## Artificial Intelligence in Analytics'
  prefs: []
  type: TYPE_NORMAL
- en: AI-powered Business Intelligence. A hype or reality?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/artificial-intelligence-in-analytics-f11d2deafdf0?source=post_page-----93d058efa302--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved query speed:** Multi-dimensional tables are often denormalised and
    bring all the performance benefits of star schema design. Opposite to Snowflake
    schema, it enables **faster data retrieval** with fewer `join` operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of dimensional data modelling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While dimensional modelling is a strong technique for OLAP purposes, it does
    have certain limits, and in some cases, it may not be the best option.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data changes frequently:** As dimensional datasets are often denormalised
    data redundancy is high here. Denormalised datasets might struggle with the requirements
    of real-time modifications. An OLTP approach may be more suited to such a scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unstructured data:** Dimensional data modelling won’t perform well on **sparse
    and unstructured data.** Wherever we have limited fact data that can be tied to
    dimensions it might be inefficient, i.e. documents and text data. Document oriented
    database solution might be a better choice in this case.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Storage:** Denormalised datasets require more storage space compared to OLTP
    schemas.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because of the benefits it provides, dimensional modelling is still the most
    often used data modelling approach for developing business data warehouses. It
    yields better reporting capabilities with multi-dimensional analysis, optimised
    query performance with fewer joins and faster data retrieval. Dimensional modelling
    makes the data warehousing design process more flexible and highly adaptable.
    It also has a certain limitation which makes it incompatible with unstructured
    datasets and OLTP pipelines due to a high requirement for data redundancy of the
    latter. Overall multi-domensional modelling is a great technique for OLAP and
    reporting which enables better query performance.
  prefs: []
  type: TYPE_NORMAL
- en: Recommended read
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [https://medium.com/towards-data-science/data-engineering-interview-questions-fdef62e46505](https://medium.com/towards-data-science/data-engineering-interview-questions-fdef62e46505)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488](https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [https://medium.com/towards-data-science/artificial-intelligence-in-analytics-f11d2deafdf0](https://medium.com/towards-data-science/artificial-intelligence-in-analytics-f11d2deafdf0)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [https://towardsdatascience.com/modern-data-engineering-e202776fb9a9](/modern-data-engineering-e202776fb9a9)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2](/how-to-become-a-data-engineer-c0319cb226c2)'
  prefs: []
  type: TYPE_NORMAL
