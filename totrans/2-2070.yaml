- en: The Transformer Architecture of GPT Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT模型的Transformer架构
- en: 原文：[https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b](https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b](https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b)
- en: Learn the details of the Transformer architecture
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解Transformer架构的详细信息
- en: '[](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)
    ·22 min read·Jul 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)
    ·阅读时间22分钟·2023年7月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b27fc545e177f9112abf27cfac0ceebb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b27fc545e177f9112abf27cfac0ceebb.png)'
- en: Photo by [fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In 2017, authors from Google published a paper called [Attention is All You
    Need](https://arxiv.org/abs/1706.03762) in which they introduced the Transformer
    architecture. This new architecture achieved unparalleled success in language
    translation tasks, and the paper quickly became essential reading for anyone immersed
    in the area. Like many others, when I read the paper for the first time, I could
    see the value of its innovative ideas, but I didn’t realize just how disruptive
    the paper would be to other areas under the broader umbrella of AI. Within a few
    years, researchers adapted the Transformer architecture to many tasks other than
    language translation, including image classification, image generation, and protein
    folding problems. In particular, the Transformer architecture revolutionized text
    generation and paved the way for GPT models and the exponential growth we’re currently
    experiencing in AI.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，谷歌的作者们发布了一篇名为[《Attention is All You Need》](https://arxiv.org/abs/1706.03762)的论文，在其中引入了Transformer架构。这一新架构在语言翻译任务中取得了前所未有的成功，该论文很快成为任何从事这一领域的必读文献。像许多人一样，当我第一次阅读这篇论文时，我能看出其创新思想的价值，但没有意识到这篇论文对更广泛的AI领域会产生如此巨大的影响。在短短几年内，研究人员将Transformer架构应用于语言翻译之外的许多任务，包括图像分类、图像生成和蛋白质折叠问题。特别是，Transformer架构革新了文本生成，为GPT模型和我们当前在AI领域经历的指数级增长铺平了道路。
- en: 'Given how pervasive Transformer models are these days, both in the industry
    and academia, understanding the details of how they work is an important skill
    for every AI practitioner. This article will focus mostly on the architecture
    of GPT models, which are built using a subset of the original Transformer architecture,
    but it will also cover the original Transformer at the end. For the model code,
    I’ll start from the most clearly written implementation I have found for the original
    Transformer: [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)
    from Harvard University. I’ll keep the parts that are relevant to a GPT transformer,
    and remove the parts that aren’t. Along the way, I’ll avoid making any unnecessary
    changes to the code, so that you can easily compare the GPT-like version of the
    code with the original and understand the differences.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 Transformer 模型在当前行业和学术界的广泛应用，理解它们的工作细节是每位 AI 从业者的重要技能。本文将主要关注 GPT 模型的架构，这些模型是使用原始
    Transformer 架构的一个子集构建的，但最后也会涉及原始 Transformer。关于模型代码，我将从我找到的最清晰的原始 Transformer
    实现开始：[哈佛大学的注释 Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)。我将保留与
    GPT transformer 相关的部分，移除不相关的部分。在此过程中，我会避免对代码做任何不必要的修改，以便你可以轻松地将 GPT 类似版本的代码与原始代码进行比较，理解它们的差异。
- en: This article is intended for experienced data scientists and machine learning
    engineers. In particular, I assume that you’re well-versed in tensor algebra,
    that you’ve implemented neural networks from scratch, and that you’re comfortable
    with Python. In addition, even though I’ve done my best to make this article stand
    on its own, you’ll have an easier time understanding it if you’ve read my previous
    article on [How GPT models work](https://bea.stollnitz.com/blog/how-gpt-works-technical/).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文面向经验丰富的数据科学家和机器学习工程师。特别是，我假设你对张量代数非常熟悉，已经从头实现了神经网络，并且对 Python 使用自如。此外，尽管我尽力使这篇文章独立完整，如果你读过我之前关于
    [GPT 模型如何工作的文章](https://bea.stollnitz.com/blog/how-gpt-works-technical/)，理解起来会更容易。
- en: The code in this post can be found in the associated [project on GitHub](https://github.com/bstollnitz/gpt-transformer).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中的代码可以在相关的 [GitHub 项目](https://github.com/bstollnitz/gpt-transformer) 中找到。
- en: '**How to invoke our GPT Transformer**'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**如何调用我们的 GPT Transformer**'
- en: Before we dive into how to construct a GPT model, let’s start by understanding
    how we’ll invoke it. We’ll assume for the moment that we have a working GPT model,
    and focus on how to prepare the input, call the model, and interpret the output.
    The general idea is to provide a few words as input to kick-start the generation,
    and return text that is likely to follow that input. For example, if we give a
    GPT model the input “A long time ago”, the model might return as output “in a
    galaxy far, far away”.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解如何构建 GPT 模型之前，先来理解如何调用它。我们暂时假设已经有一个可工作的 GPT 模型，重点讨论如何准备输入、调用模型以及解读输出。总体思路是提供几个单词作为输入来启动生成，并返回可能跟随该输入的文本。例如，如果我们给
    GPT 模型输入“很久以前”，模型可能会返回“在一个遥远的星系”。
- en: Let’s take a look at the code we’ll use to invoke our model, passing the input
    “A long time ago” and generating 10 new tokens. I’ve used comments to show the
    shape of each tensor. I’ll explain more of the details after the code.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看用于调用模型的代码，传入输入“很久以前”并生成 10 个新词。我使用了注释来展示每个张量的形状。代码之后我会解释更多细节。
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Since we begin with the string “A long time ago”, you might be inclined to think
    that a Transformer receives a string as input. However, just like other neural
    networks, a Transformer requires numerical inputs, so the input string must first
    be converted into a sequence of numbers. We do that conversion in the `tokenize`
    function using a *tokenizer* (`tiktoken` from OpenAI, in our example), which breaks
    up the text into chunks of a few letters, and assigns a number called a *token*
    to each unique chunk. To get the correct input for our Transformer, we place the
    sequence of tokens in a tensor and expand it to include a batch dimension. That’s
    because, just like other types of neural networks, a Transformer can be trained
    most efficiently by using batches to take advantage of parallel computations on
    GPUs. Our example code is running inference on one sequence, so our `batch_size`
    is one, but you can experiment with larger numbers if you want to generate multiple
    sequences at once.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们从字符串“A long time ago”开始，你可能会倾向于认为 Transformer 接收字符串作为输入。然而，和其他神经网络一样，Transformer
    需要数值输入，因此输入字符串必须首先转换为一系列数字。我们在`tokenize`函数中使用一个*分词器*（在我们的示例中是来自 OpenAI 的`tiktoken`）进行转换，它将文本拆分为几字母的块，并为每个唯一的块分配一个称为*令牌*的数字。为了获得
    Transformer 的正确输入，我们将令牌序列放入一个张量中，并扩展它以包含一个批量维度。这是因为，和其他类型的神经网络一样，Transformer 最有效的训练方式是使用批量处理，以利用
    GPU 上的并行计算。我们的示例代码在一个序列上运行推断，因此我们的`batch_size`为一，但如果你想一次生成多个序列，可以尝试更大的数字。
- en: After we’ve tokenized our input, we create the Transformer model using the `make_model`
    function, which we’ll discuss later in detail. You might think that invoking the
    model would return several tokens as output, since this is the typical text generation
    scenario. However, the Transformer is only able to generate a single token each
    time it’s called. Since we want to generate many tokens, we use a `for` loop to
    call it multiple times, and in each iteration we append the newly generated token
    to the original sequence of tokens using `torch.cat`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在对输入进行分词后，我们使用`make_model`函数创建 Transformer 模型，稍后我们将详细讨论这个函数。你可能会认为调用模型会返回几个令牌作为输出，因为这通常是文本生成的场景。然而，Transformer
    每次只能生成一个令牌。由于我们希望生成多个令牌，我们使用`for`循环多次调用模型，在每次迭代中，我们使用`torch.cat`将新生成的令牌附加到原始令牌序列中。
- en: '![](../Images/2949d05852f9bb6bd22a324497cd76c5.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2949d05852f9bb6bd22a324497cd76c5.png)'
- en: 'GPT-style Transformer models typically have a well-defined token limit: for
    example, `gpt-35-turbo` (Chat GPT) has a limit of 4096 tokens, and `gpt-4-32k`
    has a limit of 32768 tokens. Since we pass to the Transformer model the concatenation
    of the input tokens and all output tokens generated so far, the token limit of
    the model refers to the total number of input plus output tokens. In our code,
    we define this token limit using the `block_size` constant, and deal with longer
    sequences of tokens by simply truncating them to the maximum supported length
    in the `limit_sequence_length` function.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 风格的 Transformer 模型通常有一个明确定义的令牌限制：例如，`gpt-35-turbo`（Chat GPT）的限制为 4096 个令牌，而`gpt-4-32k`的限制为
    32768 个令牌。由于我们将输入令牌和到目前为止生成的所有输出令牌连接传递给 Transformer 模型，模型的令牌限制指的是输入加输出令牌的总数。在我们的代码中，我们使用`block_size`常量定义这个令牌限制，并通过在`limit_sequence_length`函数中简单地将更长的令牌序列截断到最大支持长度来处理更长的序列。
- en: We invoke the Transformer model in the `generate_next_token` function by calling
    `model.decode` followed by `model.generator`, which correspond to the two major
    sections of the Transformer architecture. The decoding section expects a mask,
    which we create using the `subsequent_mask` function. We’ll analyze all of these
    functions in detail later in this article. The generation phase returns a sequence
    of probability distributions, and we select the last one (we’ll see why later),
    which we use to predict the next token. This distribution contain a probability
    value for each possible token, representing how likely it is for that token to
    come next in the sentence.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`generate_next_token`函数中通过调用`model.decode`和`model.generator`来调用 Transformer
    模型，这对应于 Transformer 架构的两个主要部分。解码部分需要一个掩码，我们使用`subsequent_mask`函数创建这个掩码。我们将在本文后面详细分析这些函数。生成阶段返回一个概率分布序列，我们选择最后一个（稍后我们将看到原因），用它来预测下一个令牌。这个分布包含每个可能的令牌的概率值，表示该令牌在句子中接下来出现的可能性。
- en: '![](../Images/6b990788b66e90c73a5781e85742a804.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b990788b66e90c73a5781e85742a804.png)'
- en: To make our example code simple and readable, we choose the token that has the
    highest probability in the output distribution (using `torch.argmax`). In actual
    GPT models, the next token is chosen by sampling from the probability distribution,
    which introduces some variability in the output that makes the text feel more
    natural. If you have access to the “Completions playground” in the Azure AI Studio,
    you may have noticed the “Temperature” and “Top probabilities” sliders, which
    give you some control over how this sampling is done.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化和提高示例代码的可读性，我们选择输出分布中概率最高的标记（使用 `torch.argmax`）。在实际的 GPT 模型中，下一个标记是通过从概率分布中进行采样来选择的，这会在输出中引入一些变异，使得文本感觉更加自然。如果你可以访问
    Azure AI Studio 的“Completions playground”，你可能会注意到“Temperature”和“Top probabilities”滑块，这些滑块可以让你控制采样的方式。
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this article we’ll be focusing on the code for the Transformer model architecture,
    rather than the code for training the model, since that’s where we’ll find the
    bulk of the Transformer’s innovations. I’ll give you some pointers to train the
    model at the end, in case you’re interested in extending this code to generate
    better results.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将重点关注 Transformer 模型架构的代码，而不是模型训练的代码，因为这正是 Transformer 创新大部分所在的地方。最后我会给出一些训练模型的指引，以防你有兴趣扩展这段代码以生成更好的结果。
- en: We now have a good understanding of the inputs and outputs of our Transformer
    model, and how we instantiate and invoke it. Next we’ll delve into the implementation
    details of the model itself.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对 Transformer 模型的输入和输出有了很好的理解，以及如何实例化和调用它。接下来，我们将深入探讨模型本身的实现细节。
- en: '**Overview of Transformer architecture**'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Transformer 架构概述**'
- en: 'Let’s get familiar with the high-level architecture of the GPT transformer:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们熟悉一下 GPT transformer 的高层架构：
- en: '![](../Images/3f05be2dbf9b40771180a956ce812d96.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f05be2dbf9b40771180a956ce812d96.png)'
- en: 'In this diagram, the data flows from the bottom to the top, as is traditional
    in Transformer illustrations. Initially, our input tokens undergo a couple of
    encoding steps: they’re encoded using an Embedding layer, followed by a Positional
    Encoding layer, and then the two encodings are added together. Next, our encoded
    inputs go through a sequence of *N* decoding steps, followed by a normalization
    layer. And finally, we send our decoded data through a linear layer and a softmax,
    ending up with a probability distribution that we can use to select the next token.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图示中，数据从下到上流动，这在 Transformer 插图中是传统的。最初，我们的输入标记经过几个编码步骤：首先使用嵌入层进行编码，然后是位置编码层，最后将这两种编码加在一起。接下来，我们的编码输入经过一系列
    *N* 解码步骤，然后是一个归一化层。最后，我们将解码后的数据通过一个线性层和一个 softmax，得到一个概率分布，用于选择下一个标记。
- en: In the sections that follow, we’ll take a closer look at each of the components
    in this architecture.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将仔细研究这个架构中的每一个组件。
- en: '**Embedding**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**嵌入层**'
- en: The Embedding layer turns each token in the input sequence into a vector of
    length `d_model`. The input of the Transformer consists of batches of sequences
    of tokens, and has shape `(batch_size, seq_len)`. The Embedding layer takes each
    token, which is a single number, calculates its embedding, which is a sequence
    of numbers of length `d_model`, and returns a tensor containing each embedding
    in place of the corresponding original token. Therefore, the output of this layer
    has shape `(batch_size, seq_len, d_model)`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层将输入序列中的每个标记转换为长度为 `d_model` 的向量。Transformer 的输入由标记序列的批次组成，形状为 `(batch_size,
    seq_len)`。嵌入层接受每个标记（一个单一的数字），计算其嵌入（一个长度为 `d_model` 的数字序列），并返回一个张量，用每个嵌入替代对应的原始标记。因此，这一层的输出形状为
    `(batch_size, seq_len, d_model)`。
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The purpose of using an embedding instead of the original token is to ensure
    that we have a similar mathematical vector representation for tokens that are
    semantically similar. For example, let’s consider the words “she” and “her”. These
    words are semantically similar, in the sense that they both refer to a woman or
    girl, but the corresponding tokens can be completely different (for example, when
    using OpenAI’s `tiktoken` tokenizer, “she” corresponds to token 7091, and “her”
    corresponds to token 372). The embeddings for these two tokens will start out
    being very different from one another as well, because the weights of the embedding
    layer are initialized randomly and learned during training. But if the two words
    frequently appear nearby in the training data, eventually the embedding representations
    will converge to be similar.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌入而不是原始标记的目的是为了确保对于语义相似的标记，我们有类似的数学向量表示。例如，考虑“she”和“her”这两个词。这两个词在语义上是相似的，因为它们都指代女性或女孩，但相应的标记可能完全不同（例如，当使用
    OpenAI 的`tiktoken`分词器时，“she”对应的标记是7091，而“her”对应的标记是372）。这两个标记的嵌入在开始时也会非常不同，因为嵌入层的权重是随机初始化的，并在训练过程中学习。但如果这两个词在训练数据中经常出现在一起，最终嵌入表示将趋于相似。
- en: '**Positional Encoding**'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**位置编码**'
- en: The Positional Encoding layer adds information about the absolute position and
    relative distance of each token in the sequence. Unlike recurrent neural networks
    (RNNs) or convolutional neural networks (CNNs), Transformers don’t inherently
    possess any notion of where in the sequence each token appears. Therefore, to
    capture the order of tokens in the sequence, Transformers rely on a Positional
    Encoding.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码层添加了每个标记在序列中的绝对位置和相对距离的信息。与递归神经网络（RNN）或卷积神经网络（CNN）不同，变换器本身并没有固有地认识到每个标记在序列中的位置。因此，为了捕捉序列中标记的顺序，变换器依赖于位置编码。
- en: There are many ways to encode the positions of tokens. For example, we could
    implement the Positional Encoding layer by using another embedding module (similar
    to the previous layer), if we pass the position of each token rather than the
    value of each token as input. Once again, we would start with the weights in this
    embedding chosen randomly. Then during the training phase, the weights would learn
    to capture the position of each token.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 编码标记位置的方法有很多。例如，我们可以通过使用另一个嵌入模块（类似于前面的层）来实现位置编码层，如果我们将每个标记的位置而不是每个标记的值作为输入。同样，我们会从随机选择的权重开始。然后在训练阶段，权重会学习捕捉每个标记的位置。
- en: 'The authors of [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)
    code decided to implement a more sophisticated algorithm that precomputes a representation
    for the positions of tokens in the sequence. Since we want to follow their code
    as closely as possible, we’ll use the same approach:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)的作者决定实现一个更复杂的算法，该算法预计算了序列中标记位置的表示。由于我们希望尽可能紧密地跟随他们的代码，我们将使用相同的方法：'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This positional encoding uses sines and cosines of varying frequencies to populate
    a `pe` tensor. For example, in the illustration below, the values in blue and
    red were calculated using sine waves of two different frequencies, and the values
    in orange and green were calculated using cosine waves of those same frequencies.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种位置编码使用不同频率的正弦和余弦函数来填充`pe`张量。例如，在下面的插图中，蓝色和红色的值是使用两种不同频率的正弦波计算得出的，而橙色和绿色的值是使用相同频率的余弦波计算得出的。
- en: '![](../Images/eddfe97cb8a32e8a6ace80d47d93f803.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eddfe97cb8a32e8a6ace80d47d93f803.png)'
- en: 'The values of the sine and cosine graphs end up populating the columns of the
    `pe` tensor, as shown below:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正弦和余弦图的值最终填充了`pe`张量的列，如下所示：
- en: '![](../Images/f02bcdd47bf836c66fb929b97e81bf5c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f02bcdd47bf836c66fb929b97e81bf5c.png)'
- en: Then during the “forward” phase, we receive the result `x` of the previous Embedding
    layer as input, and we return the sum of `x` and `pe`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在“前向”阶段，我们将前一个嵌入层的结果`x`作为输入，并返回`x`和`pe`的和。
- en: The main advantage of precomputing the values for the positional encoding (rather
    than using a trainable Embedding) is that our model ends up with fewer parameters
    to train. This reduction in parameters leads to improved training performance,
    which is tremendously important when working with large language models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 预计算位置编码的值（而不是使用可训练的嵌入）的主要优点是我们的模型训练参数减少。这种参数减少提高了训练性能，这在处理大型语言模型时至关重要。
- en: '**Decoder**'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**解码器**'
- en: 'As we saw in the diagrammatic overview of the Transformer architecture, the
    next stage after the Embedding and Positional Encoding layers is the Decoder module.
    The Decoder consists of *N* copies of a Decoder Layer followed by a Layer Norm.
    Here’s the `Decoder` class, which takes a single `DecoderLayer` instance as input
    to the class initializer:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 Transformer 架构的图示概述中看到的那样，嵌入和位置编码层之后的下一个阶段是解码器模块。解码器由*N*个解码器层副本组成，后面跟着一个层归一化。以下是`Decoder`类，它将单个`DecoderLayer`实例作为类初始化器的输入：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The “clones” function simply creates a PyTorch list containing *N* copies of
    a module:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: “克隆”函数简单地创建一个包含*N*个模块副本的 PyTorch 列表：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The Layer Norm takes an input of shape `(batch_size, seq_len, d_model)` and
    normalizes it over its last dimension. As a result of this step, each embedding
    distribution will start out as unit normal (centered around zero and with standard
    deviation of one). Then during training, the distribution will change shape as
    the parameters `a_2` and `b_2` are optimized for our scenario. You can learn more
    about Layer Norm in the [Layer Normalization](https://arxiv.org/abs/1607.06450)
    paper from 2016.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化接收形状为`(batch_size, seq_len, d_model)`的输入，并在其最后一个维度上进行归一化。经过这一步，每个嵌入分布将开始时为单位正态分布（以零为中心，标准差为一）。然后，在训练过程中，分布会随着参数`a_2`和`b_2`在我们场景中的优化而改变形状。你可以在[层归一化](https://arxiv.org/abs/1607.06450)论文中了解更多关于层归一化的内容，这篇论文来自2016年。
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `DecoderLayer` class that we clone has the following architecture:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们克隆的`DecoderLayer`类具有以下架构：
- en: '![](../Images/95fa0df26b006228e9d1be7650d21926.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95fa0df26b006228e9d1be7650d21926.png)'
- en: 'Here’s the corresponding code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对应的代码：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'At a high level, a `DecoderLayer` consists of two main steps: the attention
    step, which is responsible for the communication between tokens, and the feed
    forward step, which is responsible for the computation of the predicted tokens.
    Surrounding each of those steps, we have residual (or skip) connections, which
    are represented by the plus signs in the diagram. Residual connections provide
    an alternative path for the data to flow in the neural network, which allows skipping
    some layers. The data can flow through the layers within the residual connection,
    or it can go directly through the residual connection and skip the layers within
    it. In practice, residual connections are often used with deep neural networks,
    because they help the training to converge better. You can learn more about residual
    connections in the paper [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385),
    from 2015\. We implement these residual connections using the `SublayerConnection`
    module:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次看，`DecoderLayer` 包含两个主要步骤：注意力步骤，负责令牌之间的通信，以及前馈步骤，负责预测令牌的计算。在这些步骤周围，我们有残差（或跳跃）连接，用图中的加号表示。残差连接为数据在神经网络中的流动提供了替代路径，从而允许跳过某些层。数据可以通过残差连接中的层流动，也可以直接通过残差连接跳过其中的层。实际上，残差连接通常用于深度神经网络，因为它们有助于训练更好地收敛。你可以在论文[深度残差学习用于图像识别](https://arxiv.org/abs/1512.03385)中了解更多关于残差连接的内容，这篇论文来自2015年。我们使用`SublayerConnection`模块来实现这些残差连接：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The feed-forward step is implemented using two linear layers with a Rectified
    Linear Unit (ReLU) activation function in between:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈步骤通过两个线性层实现，中间夹一个 Rectified Linear Unit (ReLU) 激活函数：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The attention step is the most important part of the Transformer, so we’ll devote
    the next section to it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力步骤是 Transformer 中最重要的部分，因此我们将在下一节中专门讨论它。
- en: '**Masked multi-headed self-attention**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**掩蔽多头自注意力**'
- en: 'The multi-headed attention section in the previous diagram can be expanded
    into the following architecture:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 上图中的多头注意力部分可以扩展成以下架构：
- en: '![](../Images/608a3f4c3aec9470e1b387a9c6ca014a.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/608a3f4c3aec9470e1b387a9c6ca014a.png)'
- en: As the name implies, the multi-headed attention module processes several instances
    of attention computations in parallel, with some additional pre- and post-processing
    of the data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，多头注意力模块并行处理多个注意力计算实例，并对数据进行一些额外的前处理和后处理。
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The inputs to the multi-headed attention layer include three tensors called
    `query` (Q), `key` (K), and `value` (V). In our particular model, we pass the
    same tensor for all three of these parameters: the output `x` of the previous
    layer, which has shape `(batch_size, seq_len, d_model)` (this is why we call it
    *self*-attention). We pre-process these three tensors by first passing each through
    a linear layer, then splitting them into `h` attention heads of size `d_k` where
    `h*d_k = d_model`, resulting in tensors of shape `(batch_size, seq_len, h, d_k)`.
    Then we transpose dimensions 1 and 2 to produce tensors of shape `(batch_size,
    h, seq_len, d_k)`. Next we compute attention for each head, resulting in tensors
    of the same shape. And finally, our post-processing concatenates all the heads
    back into tensors of shape `(batch_size, seq_len, d_model)`, and passes them through
    one more linear layer. By using tensor operations to do all the attention computations
    in each head in parallel, we can take full advantage of the GPU.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力层的输入包括三个张量，分别称为`query` (Q)、`key` (K) 和 `value` (V)。在我们的特定模型中，我们将前一层的输出
    `x` 传递给这三个参数，它的形状为 `(batch_size, seq_len, d_model)`（这就是为什么我们称之为 *自注意力*）。我们通过先将每个张量传递通过一个线性层，然后将它们分割成
    `h` 个注意力头，每个注意力头的大小为 `d_k`，其中 `h*d_k = d_model`，得到形状为 `(batch_size, seq_len, h,
    d_k)` 的张量。然后我们交换维度 1 和 2，得到形状为 `(batch_size, h, seq_len, d_k)` 的张量。接下来，我们为每个头计算注意力，得到相同形状的张量。最后，我们的后处理将所有头连接回形状为
    `(batch_size, seq_len, d_model)` 的张量，并通过另一个线性层。通过使用张量操作在每个头中并行进行所有注意力计算，我们可以充分利用
    GPU。
- en: 'Attention is calculated using the following formula:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力的计算使用以下公式：
- en: '![](../Images/962b20dcb548b8b373b072c539af57d8.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/962b20dcb548b8b373b072c539af57d8.png)'
- en: 'Here’s the code that implements the formula:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是实现公式的代码：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: At a high level, the attention algorithm determines which tokens in the input
    sequence it should pay more attention to, and then uses that information to predict
    the next token. In the image below, darker shades of orange represent tokens that
    are more relevant in the prediction.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，注意力算法决定输入序列中哪些标记应该受到更多关注，然后使用这些信息来预测下一个标记。在下图中，深色的橙色表示在预测中更相关的标记。
- en: '![](../Images/decf787bcf0f9655c11266682370e0d0.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/decf787bcf0f9655c11266682370e0d0.png)'
- en: More specifically, attention actually predicts the next token for several portions
    of our input sequence. It looks at the first token and predicts what a second
    token might be, then it looks at the first and second tokens and predicts what
    a third token might be, and so on.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，注意力实际上预测了我们输入序列的多个部分的下一个标记。它查看第一个标记，并预测第二个标记是什么，然后它查看第一个和第二个标记，并预测第三个标记是什么，以此类推。
- en: '![](../Images/b433dbae627e98570b657cb710bc5c41.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b433dbae627e98570b657cb710bc5c41.png)'
- en: This seems a bit wasteful during inference because we’re only interested in
    the last prediction. However, this is extremely useful during training. If yougive
    the Transformer *n* tokens as input, it will be trained to receive inputs of lengths
    from *1* to *n-1*, so the model is better able to handle inputs of different lengths
    in the future.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，这似乎有些浪费，因为我们只对最后的预测感兴趣。然而，这在训练期间极为有用。如果你给 Transformer *n* 个标记作为输入，它将被训练接收从
    *1* 到 *n-1* 长度的输入，从而使模型在未来更好地处理不同长度的输入。
- en: 'The idea in the diagram above is represented by the `p_attn` tensor in the
    code. This tensor has shape `(batch_size, h, seq_len, seq_len)`, but let’s ignore
    the batch size and number of heads for now (each batch and each head work identically),
    and consider just one tensor slice of shape `(seq_len, seq_len)`. Each row in
    the `p_attn` tensor contains a probability distribution, indicating how interesting
    all other key tokens are to the query token corresponding to that row. The resulting
    tensor encapsulates all the values shown in the previous image:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 上图中的思想由代码中的 `p_attn` 张量表示。这个张量的形状为 `(batch_size, h, seq_len, seq_len)`，但我们暂时忽略批量大小和头的数量（每个批量和每个头的工作方式相同），只考虑一个形状为
    `(seq_len, seq_len)` 的张量切片。`p_attn` 张量中的每一行包含一个概率分布，表示其他所有键标记对该行对应的查询标记的相关性。结果张量封装了前一图像中显示的所有值：
- en: '![](../Images/8ea8ebdbee6193baf5ccf9b81eb3f21f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ea8ebdbee6193baf5ccf9b81eb3f21f.png)'
- en: You can see in the code exactly how this tensor is calculated. We first do a
    matrix multiplication between the query and the transposed key. If we ignore the
    batch size and number of heads, the query and key consist of a sequence of embeddings
    of shape `(seq_len, d_k)`, which are the result of sending the input `x` through
    different linear layers. When we multiply the query tensor of shape `(seq_len,
    d_k)` with the transposed key tensor of shape `(d_k, seq_len)`, we’re essentially
    doing a dot-product between each embedding in the query and all other embeddings
    in the key, ending up with a tensor `scores` of shape `(seq_len, seq_len)`. A
    large value of the dot product indicates that a particular embedding in the query
    has “taken an interest” in a particular embedding in the key, or in other words,
    the model has discovered an affinity between two positions in the input sequence.
    Roughly speaking, we now have a tensor that represents how “interesting” or “important”
    each token finds all other tokens in the sequence.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在代码中看到这个张量是如何计算的。我们首先对查询和转置的键进行矩阵乘法。如果忽略批量大小和头数，查询和键由形状为`(seq_len, d_k)`的嵌入序列组成，这些嵌入是将输入`x`通过不同线性层得到的。当我们将形状为`(seq_len,
    d_k)`的查询张量与形状为`(d_k, seq_len)`的转置键张量相乘时，我们实际上是在对查询中的每个嵌入和键中的所有其他嵌入进行点积，最终得到一个形状为`(seq_len,
    seq_len)`的`scores`张量。点积的较大值表示查询中的某个嵌入对键中的某个嵌入“感兴趣”，换句话说，模型发现了输入序列中两个位置之间的关联。大致而言，我们现在有了一个表示每个标记在序列中发现其他所有标记的“有趣”或“重要”的张量。
- en: 'The next step is to apply a mask to the `scores` tensor that causes the values
    in its upper triangle to be ignored (which is why we call it *masked* attention).
    We do this because in the GPT-style text generation scenario, the model looks
    only at past tokens when predicting the next token. We use the following code
    to define a mask that contains the value `True` in its diagonal and lower triangle,
    and `False` in its upper triangle:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是对`scores`张量应用掩码，使其上三角的值被忽略（这就是我们称之为*masked*注意力的原因）。我们这样做是因为在GPT风格的文本生成场景中，模型在预测下一个标记时仅查看过去的标记。我们使用以下代码定义一个掩码，它在对角线和下三角中包含`True`值，在上三角中包含`False`值：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We apply this mask to the `scores` tensor using the `masked_fill` function,
    replacing all the values in the upper-triangle with a negative number of very
    large magnitude.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`masked_fill`函数将这个掩码应用到`scores`张量中，用一个非常大的负数替换掉上三角的所有值。
- en: Last, we apply a softmax that converts each row in the tensor into a probability
    distribution. Remember the formula for softmax?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应用一个softmax，将张量中的每一行转换为概率分布。记得softmax的公式吗？
- en: '![](../Images/80756c319ea5c0a355c5493e03228324.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80756c319ea5c0a355c5493e03228324.png)'
- en: Since *e* raised to a negative power of very large magnitude is close to zero,
    all the values in the upper-triangle of the `p_attn` tensor essentially become
    zero. The remaining values (in the lower triangle and diagonal) become probabilities
    that sum to one in each row.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*e*的负幂非常大的情况下接近零，`p_attn`张量的上三角中的所有值本质上变为零。剩余的值（在下三角和对角线中）变成每行总和为一的概率。
- en: You may have noticed that in the code, when we multiplied the query and key
    tensors, we divided all the values in the resulting matrix by the square root
    of `d_k`. We did that to keep the variance close to one, which ensures that the
    softmax gives us probability values that are well distributed along the whole
    range, from zero to one. If we hadn’t done that, the distributions calculated
    by softmax could approach one-hot vectors, where one value is one and the others
    are all zero — which would make the output of the model seem predictable and robotic.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，在代码中，当我们将查询和键张量相乘时，我们将结果矩阵中的所有值除以`d_k`的平方根。这样做是为了保持方差接近于一，这样可以确保softmax给出的概率值在整个范围内（从零到一）分布良好。如果我们不这样做，softmax计算的分布可能接近于one-hot向量，其中一个值为一，其他值都为零——这会使模型的输出显得可预测且机械。
- en: 'At this point, we have a `p_attn` tensor containing probability distributions
    along its rows, representing how interesting tokens are to each other. The next
    step is to use this measure of interest to determine how much attention we should
    pay to each input token, while generating the output token. Naturally, we’ll pay
    more attention to the most interesting tokens. We generate the next token by multiplying
    our tensor of probabilities by our `value` tensor, which contains the input token
    embeddings `x` after applying a linear layer. The resulting tensor will contain
    a prediction for each token subsequence:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们有一个包含概率分布的`p_attn`张量，表示各标记之间的相互兴趣程度。下一步是利用这种兴趣程度来确定在生成输出标记时我们应该对每个输入标记给予多少关注。自然地，我们会对最感兴趣的标记给予更多关注。我们通过将概率张量与`value`张量相乘来生成下一个标记，`value`张量包含经过线性层处理后的输入标记嵌入`x`。结果张量将包含每个标记子序列的预测：
- en: '![](../Images/bdc02745464241ba99f093e8059aede5.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdc02745464241ba99f093e8059aede5.png)'
- en: 'Here’s the intuition for this diagram: for the input subsequence “A”, we pay
    full attention to the one and only input token, and might produce a next-token
    prediction such as “person”. For the input subsequence “A long”, our model has
    been trained to pay a bit more attention to the token “long” than to the token
    “A”, and might produce the next-token prediction “dress”. And so on. When doing
    inference we want to take into account the full input sequence “A long time ago”,
    so we only care about the last row in this diagram. We pay most attention to “ago”,
    we pay a little less attention to “long”, we pay the least attention to the other
    two tokens, and we produce a next-token prediction of “in”.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表的直观解释是：对于输入子序列“A”，我们对唯一的输入标记给予完全的关注，并可能产生下一个标记预测，比如“person”。对于输入子序列“A long”，我们的模型已经训练得更加关注标记“long”而非标记“A”，并可能产生下一个标记预测“dress”，依此类推。在推理时，我们希望考虑到完整的输入序列“A
    long time ago”，所以我们只关注这个图表中的最后一行。我们最关注“ago”，次之是“long”，对其他两个标记的关注最少，我们产生下一个标记预测“in”。
- en: After we’ve calculated attention for all the heads, and have concatenated the
    results back together, we have an output tensor of dimension `(batch_size, seq_len,
    d_model)`. This tensor contains the token predictions for each sub-sequence, and
    is almost ready to be returned to the user. But before we do that, we need one
    last step to finalize its shape and contents.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们计算了所有头的注意力，并将结果重新拼接在一起后，我们得到一个维度为`(batch_size, seq_len, d_model)`的输出张量。这个张量包含每个子序列的标记预测，几乎准备好返回给用户。但是在此之前，我们需要最后一步来最终确定其形状和内容。
- en: '**Generator**'
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**生成器**'
- en: 'The last step in our Transformer is the Generator, which consists of a linear
    layer and a softmax executed in sequence:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Transformer中的最后一步是生成器，它包括一个线性层和一个顺序执行的softmax：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The purpose of the linear layer is to convert the third dimension of our tensor
    from the internal-only `d_model` embedding dimension to the `vocab_size` dimension,
    which is understood by the code that calls our Transformer. The result is a tensor
    dimension of `(batch_size, seq_len, vocab_size)`. The purpose of the softmax is
    to convert the values in the third tensor dimension into a probability distribution.
    This tensor of probability distributions is what we return to the user.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 线性层的目的是将张量的第三维度从内部的`d_model`嵌入维度转换为`vocab_size`维度，这是调用我们Transformer的代码所理解的。结果是一个维度为`(batch_size,
    seq_len, vocab_size)`的张量。softmax的目的是将第三维度中的值转换为概率分布。这一概率分布张量就是我们返回给用户的内容。
- en: You might remember that at the very beginning of this article, we explained
    that the input to the Transformer consists of batches of sequences of tokens,
    of shape `(batch_size, seq_len)`. And now we know that the output of the Transformer
    consists of batches of sequences of probability distributions, of shape `(batch_size,
    seq_len, vocab_size)`. Each batch contains a distribution that predicts the token
    that follows the first input token, another distribution that predicts the token
    that follows the first and second input tokens, and so on. The very last probability
    distribution of each batch enables us to predict the token that follows the whole
    input sequence, which is what we care about when doing inference.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，在本文开头，我们解释了Transformer的输入由形状为`(batch_size, seq_len)`的标记序列批次组成。而现在我们知道Transformer的输出由形状为`(batch_size,
    seq_len, vocab_size)`的概率分布序列批次组成。每个批次包含一个预测第一个输入标记之后的标记的分布，另一个预测第一个和第二个输入标记之后的标记的分布，依此类推。每个批次的最后一个概率分布使我们能够预测整个输入序列之后的标记，这也是我们在做推理时所关心的。
- en: The Generator is the last piece of our Transformer architecture, so we’re ready
    to put it all together.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器是我们Transformer架构的最后一个部分，因此我们准备好将所有组件结合起来了。
- en: '**Putting it all together**'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**将所有组件结合起来**'
- en: 'We use the `DecoderModel` module to encapsulate the three main pieces of the
    Transformer architecture: the embeddings, the decoder, and the generator.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`DecoderModel`模块来封装Transformer架构的三个主要部分：嵌入层、解码器和生成器。
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Calling `decode` executes just the embeddings and decoder, so if we want to
    execute all steps of the Transformer, we need to call `decode` followed by `generator`.
    That’s exactly what we did in the `generate_next_token` function of the inference
    code that I showed at the beginning of this post.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`decode`仅执行嵌入层和解码器，所以如果我们想执行Transformer的所有步骤，我们需要调用`decode`，然后调用`generator`。这正是我们在本文开头展示的推理代码中的`generate_next_token`函数所做的。
- en: 'The inference code also calls a `make_model` function that returns an instance
    of `DecoderModel`. This function initializes all the components we’ve discussed
    so far, and puts them together according to the architecture diagram at the beginning
    of this post:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 推理代码还调用了一个`make_model`函数，该函数返回`DecoderModel`的一个实例。这个函数初始化了我们迄今为止讨论的所有组件，并按照本文开头的架构图将它们结合在一起：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We now have all the pieces needed to implement a GPT-style Transformer architecture!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经拥有了实现GPT风格Transformer架构所需的所有组件！
- en: We’ll finish with a few thoughts on training, and we’ll do a brief comparison
    between the full Transformer and the GPT-style subset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以一些关于训练的思考结束，并且对完整的Transformer与GPT风格子集进行简要比较。
- en: '**Training**'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练**'
- en: The code for training the GPT-style Transformer is the same as the code for
    training any other neural network — except that in our scenario, for each input
    sequence of tokens, we expect the output to be the sequence that begins one position
    to the right. For example, if we give it “A long time ago” as input, we expect
    the sampling of the probabilities returned by the model to produce “long time
    ago in”.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GPT风格的Transformer的代码与训练任何其他神经网络的代码相同——只是，在我们的场景中，对于每个输入的标记序列，我们期望输出是从右边一个位置开始的序列。例如，如果我们给它输入“A
    long time ago”，我们期望模型返回的概率采样会产生“long time ago in”。
- en: If you have experience training neural networks and want to train the Transformer
    on your own, you can reuse any code you’ve written in the past and adapt it to
    our scenario. If you need guidance, I recommend that you follow the code and explanations
    in [Part 2 of the Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/#part-2-model-training).
    Either way, you’ll need access to a fast GPU, locally or in the cloud. I’m partial
    to training in the cloud on Azure of course!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有训练神经网络的经验，并且希望自己训练Transformer，你可以重用你过去编写的任何代码，并将其调整到我们的场景中。如果你需要指导，我建议你参考[Annotated
    Transformer的第2部分](http://nlp.seas.harvard.edu/annotated-transformer/#part-2-model-training)中的代码和解释。无论如何，你都需要访问快速GPU，无论是本地还是云端。自然，我更倾向于在Azure云端进行训练！
- en: '**Comparison with full Transformer**'
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**与完整Transformer的比较**'
- en: Once you understand the architecture of the GPT-style Transformer, you’re a
    short step away from understanding the full Transformer as it’s presented in the
    [Attention is all you need](https://arxiv.org/abs/1706.03762) paper. Below you
    can see the diagram of the Transformer architecture presented in the paper, with
    the parts we covered in this post enclosed by an orange box.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了GPT风格Transformer的架构，你就离理解[Attention is all you need](https://arxiv.org/abs/1706.03762)论文中呈现的完整Transformer只差一步。下面你可以看到论文中呈现的Transformer架构图，我们在本文中覆盖的部分被橙色框圈出。
- en: '![](../Images/6fa18631106401a5fff1c12a1a4b3385.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6fa18631106401a5fff1c12a1a4b3385.png)'
- en: '*Illustration adapted from the* [*Attention is all you need*](https://arxiv.org/abs/1706.03762)
    *paper*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*插图改编自* [*Attention is all you need*](https://arxiv.org/abs/1706.03762) *论文*'
- en: The full Transformer has an encoder section (on the left) and a decoder section
    (on the right). The original intent of the paper was to present an architecture
    for machine translation. In that context, the encoder was used to process the
    input language, and the decoder was used to produce the output language.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的Transformer有一个编码器部分（在左侧）和一个解码器部分（在右侧）。论文的原始意图是提出一种用于机器翻译的架构。在这种背景下，编码器用于处理输入语言，而解码器用于生成输出语言。
- en: You can see that in addition to the masked multi-headed self-attention used
    in the GPT-style Transformer, the full Transformer has two other multi-headed
    attention blocks. The one in the encoder is not masked, which means that the `p_attn`
    tensor we saw in the attention section doesn’t have the values in its upper-triangular
    section zeroed out. That’s because in machine translation, generating a single
    output language token may require the model to pay attention to input language
    tokens in all sequence positions, including earlier and later positions. The additional
    multi-headed attention block in the decoder section is a “cross-attention” (as
    opposed to “self-attention”) layer which means that its key and value come from
    a different source than the query, as you can see in the diagram. That’s because
    the model needs to understand how much attention it should pay to each token in
    the input language, as it predicts tokens in the output language. The rest of
    the pieces of the diagram are similar to parts of the GPT-style Transformer, and
    have already been explained in this post.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，除了GPT风格Transformer中使用的掩码多头自注意力外，完整的Transformer还有另外两个多头注意力块。编码器中的那个未被掩码，这意味着我们在注意力部分看到的`p_attn`张量在其上三角区域的值没有被置零。这是因为在机器翻译中，生成一个输出语言的单一令牌可能需要模型关注输入语言中所有序列位置的令牌，包括早期和晚期位置。解码器部分的附加多头注意力块是一个“交叉注意力”（与“自注意力”相对）层，这意味着它的键和值来自不同于查询的源，如图所示。这是因为模型需要理解在预测输出语言中的令牌时，应该关注输入语言中的每个令牌的程度。图中其余部分类似于GPT风格Transformer的部分，并已在本文中解释过。
- en: '**Conclusion**'
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In this article, we discussed the architecture of a GPT-style Transformer model
    in detail, and covered the architecture of the original Transformer at a high
    level. Given the increasing popularity of GPT models in the industry, and how
    often variations of the original Transformer model come up in recent papers, I
    hope that you’ll find this knowledge useful in your job or education. If you want
    to go deeper, I encourage you to clone the associated [GitHub project](https://github.com/bstollnitz/gpt-transformer)
    and explore the code. Set breakpoints in any section of the code that isn’t clear
    to you, run it, and inspect its variables. If you have access to a GPU, write
    code that trains it and see the performance of its predictions improve.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们详细讨论了GPT风格的Transformer模型的架构，并在较高层次上覆盖了原始Transformer的架构。鉴于GPT模型在行业中的日益流行，以及最近论文中经常出现的原始Transformer模型的变体，我希望你能在工作或学习中找到这些知识的用处。如果你想深入了解，我鼓励你克隆相关的
    [GitHub项目](https://github.com/bstollnitz/gpt-transformer) 并探索代码。设置断点于你不清楚的代码部分，运行它，并检查其变量。如果你有GPU访问权限，可以编写代码训练模型，并查看其预测性能的提升。
- en: With your newly acquired knowledge, I hope that you can help the AI community
    demystify some of the misconceptions associated with these models. Many people
    in the general public assume that these models have a higher-level understanding
    of the world, which you now know is not the case. They’re simply composed of mathematical
    and statistical operations aimed at predicting the next token based on previous
    training data. Maybe future versions of generative models will have a better understanding
    of the world, and generate even better predictions? If so, I’ll be here to tell
    you all about it.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你运用新获得的知识，帮助人工智能社区揭开一些与这些模型相关的误解。许多人误以为这些模型对世界有更高层次的理解，而你现在知道事实并非如此。它们只是由数学和统计运算组成，旨在基于之前的训练数据预测下一个令牌。也许未来的生成模型会对世界有更好的理解，并生成更准确的预测？如果是这样，我会在这里告诉你所有的细节。
- en: '**Note**'
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**注意**'
- en: All images are by the author unless otherwise noted. You can use any of the
    original images in this blog post for any purpose, with attribution (a link to
    this article).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由作者提供，除非另有说明。你可以在这篇博客文章中出于任何目的使用原始图片，并需注明出处（即链接到本文）。
