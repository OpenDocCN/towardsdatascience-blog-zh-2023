- en: When Should You Fine-Tune LLMs?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你什么时候应该微调 LLM？
- en: 原文：[https://towardsdatascience.com/when-should-you-fine-tune-llms-2dddc09a404a](https://towardsdatascience.com/when-should-you-fine-tune-llms-2dddc09a404a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/when-should-you-fine-tune-llms-2dddc09a404a](https://towardsdatascience.com/when-should-you-fine-tune-llms-2dddc09a404a)
- en: There has been a flurry of exciting open-source LLMs which can be fine-tuned.
    But how does that compare to just using a closed API?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最近有一波令人兴奋的开源 LLM 可以进行微调。但这与使用闭源 API 比较如何呢？
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)
    ·7 min read·May 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)
    ·7 分钟阅读·2023年5月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: I get this question a lot — from folks on LinkedIn asking me questions on how
    to fine-tune open source models like LLaMA, companies trying to figure out the
    business case for selling LLM hosting and deployment solutions, and companies
    trying to capitalize on AI and LLMs applied to their products. But when I ask
    them why they don’t want to use a close-sourced model like ChatGPT — they don’t
    really have an answer. So I decided to write this article as someone who applies
    LLMs to solve business problems every day.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常被问到这个问题——LinkedIn 上的朋友们问我如何微调开源模型如 LLaMA，企业们试图弄清楚销售 LLM 托管和部署解决方案的商业案例，还有一些公司试图利用
    AI 和 LLM 应用于他们的产品。但当我问他们为什么不想使用像 ChatGPT 这样的闭源模型时，他们并没有真正的答案。所以我决定以一个每天应用 LLM
    解决业务问题的人的身份来写这篇文章。
- en: The Case For Closed APIs
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 闭源 API 的案例
- en: Have you tried implementing the ChatGPT API for your use case? Maybe you want
    to summarize documents or answer questions, or just want a chatbot on your website.
    More often than not, you will find that ChatGPT does a pretty good job at multiple
    language tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你尝试过为你的用例实现 ChatGPT API 吗？也许你想总结文档或回答问题，或者只是想在你的网站上拥有一个聊天机器人。往往你会发现 ChatGPT
    在多种语言任务上表现得相当不错。
- en: A common perception is that these models are too expensive. But at $0.002/1K
    tokens, I bet you could at least try this out on a few 100 samples and evaluate
    whether LLMs are the way to go or not for your particular application. In fact,
    at thousands of API calls per day or around that range, ChatGPT API works out
    much cheaper than hosting infrastructure for custom open-source models [as I have
    written about in this blog](/llm-economics-chatgpt-vs-open-source-dfc29f69fec1).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一个普遍的看法是这些模型太贵了。但以 $0.002/1K tokens 的价格，我敢打赌你至少可以在几百个样本上尝试一下，评估一下 LLM 是否适合你的具体应用。事实上，以每天数千次
    API 调用的频率，ChatGPT API 比托管自定义开源模型的基础设施便宜得多[正如我在这篇博客中所写的](/llm-economics-chatgpt-vs-open-source-dfc29f69fec1)。
- en: One argument is let’s say you want to answer questions on thousands or tens
    of thousands of documents. In this case, wouldn’t it be easier to just train or
    fine-tune an open source model on this data and ask the fine-tuned model questions
    about this data? Turns out this is not as simple as it sounds (for a variety of
    reasons that I will discuss below in the section on labeling data for fine-tuning).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个论点是，假设你想回答成千上万份文档中的问题。在这种情况下，是否不如直接在这些数据上训练或微调一个开源模型，然后让微调后的模型回答关于这些数据的问题更简单？事实证明，这并不像听起来那么简单（由于各种原因，我将在下文的微调数据标注部分讨论）。
- en: But there is an easy solution for ChatGPT to answer questions from context that
    contains thousands of documents. It is basically to store all these documents
    as small chunks of text in a database.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，ChatGPT 可以通过上下文回答包含数千份文档的问题，这里有一个简单的解决方案。基本上是将所有这些文档作为小块文本存储在数据库中。
- en: '![](../Images/346d372796c458d06e86bdaafa36b68d.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/346d372796c458d06e86bdaafa36b68d.png)'
- en: Offloading documents to a database for LLM Queries at Scale | Skanda Vivek
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 将文档卸载到数据库中以进行大规模 LLM 查询 | Skanda Vivek
- en: The problem of giving all the necessary information to the model to answer questions
    is now offloaded from the model architecture to a database, containing document
    chunks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有必要信息提供给模型以回答问题的任务现在从模型架构转移到了包含文档块的数据库中。
- en: The documents of relevance can then be found by computing similarities between
    the question and the document chunks. This is done typically by converting the
    chunks and question into word embedding vectors, and computing cosine similarities
    between chunks and question, and finally choosing only those chunks above a certain
    cosine similarity as relevant context.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的文档可以通过计算问题和文档块之间的相似性来找到。这通常通过将文档块和问题转换为词嵌入向量，计算块与问题之间的余弦相似度，最后只选择那些高于某个余弦相似度的块作为相关背景来完成。
- en: 'Finally, the question and context can be combined into a prompt as below, and
    fed into an LLM API like ChatGPT:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，问题和背景可以组合成一个如下的提示，并输入到像 ChatGPT 这样的 LLM API 中：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You might ask — why not feed the entire documents and question into the prompt
    instead of separating out into chunks? The reason we are separating documents
    into chunks and not combining all documents into a single prompt is because LLMs
    have certain limits on maximum input and output length. For ChatGPT, this is 4096
    tokens, or roughly 6–7 pages of text.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问——为什么不将整个文档和问题一起放入提示中，而是将其分成块？我们将文档分成块而不是将所有文档组合成一个提示的原因是 LLMs 对最大输入和输出长度有一定的限制。对于
    ChatGPT，这个限制是 4096 个令牌，或大约 6-7 页的文本。
- en: Offloading documents to a database and querying using closed LLM APIs might
    work well in cases where the answer is obviously present in these documents. But
    there are a number of cases in which these could yield sub-optimal results, such
    as if you want ChatGPT to give an expert perspective on a niche topic — for example,
    speculating on the financial outlook of the U.S. economy.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 将文档卸载到数据库并使用封闭的 LLM API 查询可能在答案显然存在于这些文档中的情况下效果很好。但也有一些情况可能会产生次优结果，例如，如果你希望
    ChatGPT 提供关于某个小众话题的专家观点——例如，预测美国经济的财务前景。
- en: Domain Specific LLMs
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 领域特定的 LLMs
- en: 'Ok, so you tried out ChatGPT or BARD — and you didn’t like it. The answers
    were a bit off. For example, you asked ChatGPT:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，你试过 ChatGPT 或 BARD——但你不喜欢。答案有些偏差。例如，你问了 ChatGPT：
- en: “How long is the U.S. economy likely to have inflation?”
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “美国经济可能会持续多久的通货膨胀？”
- en: '**And it returned:**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**然后它返回：**'
- en: As an AI language model, I cannot predict the future with certainty, especially
    with regards to complex economic systems. However, I can provide you with some
    information and perspectives that may help you understand the current situation.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 作为一个 AI 语言模型，我不能确定地预测未来，特别是涉及复杂经济系统时。然而，我可以提供一些信息和观点，可能有助于你理解当前的情况。
- en: ''
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Inflation is a complex phenomenon that can be caused by a variety of factors
    such as an increase in demand, supply constraints,….
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通货膨胀是一个复杂的现象，可能由多种因素引起，例如需求增加、供应限制等……
- en: Unfortunately that is not good enough for you. Of course you can feed ChatGPT
    some recent news about Fed Chairman Jerome Powell’s statements. But that does
    not give you the same rich domain experience that you would get if you spoke to
    — well, Jerome Powell, who else! Or another expert.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这对你来说还不够好。当然，你可以给 ChatGPT 提供一些关于美联储主席杰罗姆·鲍威尔最近声明的新闻。但这并不能给你与——嗯，杰罗姆·鲍威尔，其他人也可以！或其他专家交流时获得的丰富领域经验。
- en: Think about what it takes to be an expert in a certain field. While some amount
    of this is reading books on the topic, a lot is also interacting with subject
    matter experts in the field, and learning from experience. While ChatGPT has been
    trained on an incredible number of finance books, it probably hasn’t been trained
    by top financial experts or experts in other specific fields. So how would you
    make an LLM that is an “expert” in the finance sector? This is where fine-tuning
    comes in.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想成为某个领域专家所需的条件。虽然一部分是阅读相关书籍，但更多的是与领域内的专家互动，并从经验中学习。虽然 ChatGPT 已经接受了大量金融书籍的训练，但它可能没有经过顶级金融专家或其他特定领域专家的训练。那么，如何使
    LLM 成为金融领域的“专家”呢？这就是微调发挥作用的地方。
- en: Fine-Tuning LLMs
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调 LLMs
- en: 'Before I discuss fine-tuning LLMs, let’s talk about fine-tuning smaller language
    models like BERT, which was commonplace before LLMs. For models like BERT and
    RoBERTa, fine-tuning amounts to passing some context, and labels. Tasks are well-defined
    like extracting answers from contexts, or classifying emails as spam vs not spam.
    I’ve written a couple of blog posts on these that might be useful if you are interested
    in fine-tuning language models:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论微调 LLMs 之前，我们先聊聊像 BERT 这样的较小语言模型的微调，这在 LLMs 之前是很常见的。对于像 BERT 和 RoBERTa 这样的模型，微调就是传递一些上下文和标签。任务定义明确，比如从上下文中提取答案，或将电子邮件分类为垃圾邮件或非垃圾邮件。如果你对微调语言模型感兴趣，我写了一些可能对你有用的博客文章：
- en: '[](/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=post_page-----2dddc09a404a--------------------------------)
    [## Fine-Tune Transformer Models For Question Answering On Custom Data'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 微调 Transformer 模型以进行自定义数据上的问答](https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=post_page-----2dddc09a404a--------------------------------)
    [## 微调 Transformer 模型以进行自定义数据上的问答](https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=post_page-----2dddc09a404a--------------------------------)'
- en: A tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and
    obtaining significant performance boosts
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于如何在自定义数据上微调 Hugging Face RoBERTa QA 模型并获得显著性能提升的教程
- en: towardsdatascience.com](/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=post_page-----2dddc09a404a--------------------------------)
    [](/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1?source=post_page-----2dddc09a404a--------------------------------)
    [## Transformer Models For Custom Text Classification Through Fine-Tuning
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 通过微调定制文本分类的 Transformer 模型](https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=post_page-----2dddc09a404a--------------------------------)
    [## 微调 Transformer 模型以进行自定义文本分类](https://towardsdatascience.com/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1?source=post_page-----2dddc09a404a--------------------------------)'
- en: A tutorial on how to build a spam classifier (or any other classifier) by fine-tuning
    the DistilBERT model
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于如何通过微调 DistilBERT 模型来构建垃圾邮件分类器（或其他分类器）的教程
- en: towardsdatascience.com](/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1?source=post_page-----2dddc09a404a--------------------------------)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 通过微调定制文本分类的 Transformer 模型](https://towardsdatascience.com/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1?source=post_page-----2dddc09a404a--------------------------------)'
- en: However, the reason large language models (LLMs) are all the rage is because
    they can perform multiple tasks seamlessly by changing the way you frame prompts,
    and you have the experience similar to talking with a person at the other end.
    What we want now is to fine-tune that LLM to be an expert in a certain subject
    and engage in conversation like a “person.” This is quite different from fine-tuning
    a model like BERT on specific tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大型语言模型（LLMs）之所以备受关注，是因为它们能够通过改变提示的方式无缝地执行多个任务，你的体验类似于与另一端的人对话。我们现在希望的是将这些LLM微调为某个特定领域的专家，并像“人类”一样进行对话。这与在特定任务上微调像
    BERT 这样的模型截然不同。
- en: One of the earliest open-source breakthroughs was by a group of Stanford researchers
    that fine-tuned a 7B LLaMa model (released earlier in the year by Meta) which
    they called [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) for less
    than 600$ on 52K instructions. Soon after, the Vicuna team released a 13 Billion
    parameter model which achieves [90% of ChatGPT quality](https://lmsys.org/blog/2023-03-30-vicuna/).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最早的开源突破之一是由一组斯坦福研究人员完成的，他们微调了一个 7B LLaMa 模型（Meta 早些时候发布）并称之为 [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)，花费不到
    600 美元进行 52K 指令的训练。随后，Vicuna 团队发布了一个 130 亿参数的模型，达到了 [ChatGPT 90% 的质量](https://lmsys.org/blog/2023-03-30-vicuna/)。
- en: Very recently, the [MPT-7B transformer](https://www.mosaicml.com/blog/mpt-7b)
    was released that could ingest 65k tokens, 16X the input size of ChatGPT! The
    training was done from scratch over 9.5 days for 200k$. As an example for a domain
    specific LLM, Bloomberg released a GPT-like model [BloombergGPT](https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/),
    built for finance and also trained from scratch.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，[MPT-7B transformer](https://www.mosaicml.com/blog/mpt-7b)被发布，它能够处理65k个token，是ChatGPT输入大小的16倍！这次训练从零开始，耗时9.5天，成本为200k$。作为领域特定LLM的一个例子，Bloomberg发布了一个类似GPT的模型[BloombergGPT](https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/)，专为金融领域设计，也是在从零开始训练的情况下构建的。
- en: Recent advancements in training and fine-tuning open-source models are just
    the beginning for small and medium sized companies enriching their offerings through
    customized LLMs. So how do you decide when it makes sense to fine-tune or train
    entire domain specific LLMs?
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在训练和微调开源模型方面的进展只是小型和中型公司通过定制化LLM丰富其产品的开始。那么，你如何决定何时进行微调或训练整个领域特定的LLM呢？
- en: First off, it is important to clearly establish the limitations of closed-source
    LLM APIs in your domain and make the case for empowering customers to chat with
    an expert in that domain at a fraction of the cost. Fine-tuning a model is not
    very expensive for a hundred thousand instructions or so — but getting the right
    instructions requires careful thought. This is where you also need to be a bit
    bold — I can’t yet think of many areas where a fine-tuned model is shown to perform
    significantly better than ChatGPT on domain specific tasks, but I believe this
    is right around the corner, and any company that does this well will be rewarded.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，重要的是要清楚地确定闭源LLM API在你领域中的局限性，并说明让客户以更低成本与领域专家对话的必要性。对大约十万条指令进行微调并不昂贵——但获取正确的指令需要仔细思考。这也是你需要有一点大胆的地方——我目前还没想到很多领域中微调模型在领域特定任务上比ChatGPT表现得更好的情况，但我相信这很快就会出现，任何做得好的公司都将获得回报。
- en: Which brings me to the case for completely training an LLM from scratch. Yes
    this could easily cost upwards of hundreds of thousands of dollars, but if you
    make a solid case, investors would be glad to pitch in. In a recent [interview
    with IBM, Hugging Face CEO Clem Delangue](https://research.ibm.com/blog/generative-ai-dario-gil-think?sf177843602=1)
    commented that soon, customized LLMs could be as common place as proprietary codebases
    — and a significant component of what it takes to be competitive in an industry.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这就引出了从头开始训练LLM的理由。是的，这可能需要花费数十万美元，但如果你能提供充分的理由，投资者会很乐意出资。在最近的[IBM采访中，Hugging
    Face首席执行官Clem Delangue](https://research.ibm.com/blog/generative-ai-dario-gil-think?sf177843602=1)评论说，定制化LLM很快可能像专有代码库一样普遍，并成为在行业中具有竞争力的关键组成部分。
- en: Takeaways
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键要点
- en: 'LLMs applied to specific domains can be extremely valuable in the industry.
    There are **3 levels of increasing cost and customizability**:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于特定领域的大型语言模型（LLMs）在行业中可以非常有价值。它们有**3个逐步增加的成本和定制化水平**：
- en: '**Closed source APIs + Document Embedding Database:** This first solution is
    probably the easiest to get started off with, and considering the high quality
    of ChatGPT API — might even give you a good enough (if not the best) performance.
    And it’s cheap!'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**闭源API + 文档嵌入数据库：** 这一解决方案可能是最容易上手的，并且考虑到ChatGPT API的高质量——可能会提供足够好（如果不是最好的）性能。而且成本低廉！'
- en: '**Fine-tune LLMs:** Recent progress from fine-tuning LLaMA-like models has
    shown this costs **~500$** to get a baseline performance similar to ChatGPT in
    certain domains. Could be worthwhile if you had a database with ~50–100k instructions
    or conversations to fine-tune a baseline model.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调LLMs：** 最近对类似LLaMA模型的微调进展表明，这样的微调成本大约为**~500$**，可以在某些领域获得类似于ChatGPT的基线性能。如果你有一个包含~50–100k条指令或对话的数据库，微调一个基线模型可能是值得的。'
- en: '**Train from scratch:** As LLaMA and the more recent MPT-7B models have shown,
    this costs **~100–200k** and takes a week or two.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从零开始训练：** 如LLaMA和最近的MPT-7B模型所示，这种方式的成本大约为**~100–200k**，需要一到两周的时间。'
- en: Now that you have the knowledge — go forth and build your custom domain specific
    LLM applications!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经掌握了这些知识——那就去构建你的定制领域特定的LLM应用吧！
- en: '*If you like this post, follow me — I write on topics related to applying state-of-the-art
    NLP in real-world applications and, more generally, on the intersections between
    data and society.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，关注我——我写关于在实际应用中应用最先进的自然语言处理（NLP）技术的话题，更广泛地说，也涉及数据与社会之间的交集。*'
- en: '*Feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/skanda-vivek-01619311b/)*!*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*随时在* [*LinkedIn*](https://www.linkedin.com/in/skanda-vivek-01619311b/)*上与我联系！*'
- en: '*If you are not yet a Medium member and want to support writers like me, feel
    free to sign-up through my referral link:* [*https://skanda-vivek.medium.com/membership*](https://skanda-vivek.medium.com/membership)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你还不是Medium会员，并且想要支持像我这样的作者，请通过我的推荐链接注册：* [*https://skanda-vivek.medium.com/membership*](https://skanda-vivek.medium.com/membership)'
- en: '**Here are some related articles:**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**以下是一些相关的文章：**'
- en: '[LLM Economics: ChatGPT vs Open-Source](https://medium.com/towards-data-science/llm-economics-chatgpt-vs-open-source-dfc29f69fec1)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLM经济学：ChatGPT与开源](https://medium.com/towards-data-science/llm-economics-chatgpt-vs-open-source-dfc29f69fec1)'
- en: '[How Do You Build A ChatGPT-Powered App?](https://medium.com/geekculture/how-do-you-build-a-chatgpt-powered-app-89c83f3e2143)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[如何构建一个基于ChatGPT的应用？](https://medium.com/geekculture/how-do-you-build-a-chatgpt-powered-app-89c83f3e2143)'
- en: '[Extractive vs Generative Q&A — Which is better for your business?](https://medium.com/towards-data-science/extractive-vs-generative-q-a-which-is-better-for-your-business-5a8a1faab59a)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[提取式与生成式问答——哪种更适合你的业务？](https://medium.com/towards-data-science/extractive-vs-generative-q-a-which-is-better-for-your-business-5a8a1faab59a)'
- en: '[Fine-Tune Transformer Models For Question Answering On Custom Data](https://medium.com/towards-data-science/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[为定制数据微调Transformer模型以进行问答](https://medium.com/towards-data-science/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80)'
- en: '[Unleashing the Power of Generative AI For Your Customers](https://medium.com/geekculture/unleashing-the-power-of-generative-ai-for-your-customers-70297f1c9698)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[释放生成式AI为你的客户带来的力量](https://medium.com/geekculture/unleashing-the-power-of-generative-ai-for-your-customers-70297f1c9698)'
