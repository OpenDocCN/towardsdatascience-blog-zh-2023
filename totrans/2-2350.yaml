- en: When Should You Fine-Tune LLMs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/when-should-you-fine-tune-llms-2dddc09a404a](https://towardsdatascience.com/when-should-you-fine-tune-llms-2dddc09a404a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There has been a flurry of exciting open-source LLMs which can be fine-tuned.
    But how does that compare to just using a closed API?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----2dddc09a404a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2dddc09a404a--------------------------------)
    ·7 min read·May 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: I get this question a lot — from folks on LinkedIn asking me questions on how
    to fine-tune open source models like LLaMA, companies trying to figure out the
    business case for selling LLM hosting and deployment solutions, and companies
    trying to capitalize on AI and LLMs applied to their products. But when I ask
    them why they don’t want to use a close-sourced model like ChatGPT — they don’t
    really have an answer. So I decided to write this article as someone who applies
    LLMs to solve business problems every day.
  prefs: []
  type: TYPE_NORMAL
- en: The Case For Closed APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you tried implementing the ChatGPT API for your use case? Maybe you want
    to summarize documents or answer questions, or just want a chatbot on your website.
    More often than not, you will find that ChatGPT does a pretty good job at multiple
    language tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A common perception is that these models are too expensive. But at $0.002/1K
    tokens, I bet you could at least try this out on a few 100 samples and evaluate
    whether LLMs are the way to go or not for your particular application. In fact,
    at thousands of API calls per day or around that range, ChatGPT API works out
    much cheaper than hosting infrastructure for custom open-source models [as I have
    written about in this blog](/llm-economics-chatgpt-vs-open-source-dfc29f69fec1).
  prefs: []
  type: TYPE_NORMAL
- en: One argument is let’s say you want to answer questions on thousands or tens
    of thousands of documents. In this case, wouldn’t it be easier to just train or
    fine-tune an open source model on this data and ask the fine-tuned model questions
    about this data? Turns out this is not as simple as it sounds (for a variety of
    reasons that I will discuss below in the section on labeling data for fine-tuning).
  prefs: []
  type: TYPE_NORMAL
- en: But there is an easy solution for ChatGPT to answer questions from context that
    contains thousands of documents. It is basically to store all these documents
    as small chunks of text in a database.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/346d372796c458d06e86bdaafa36b68d.png)'
  prefs: []
  type: TYPE_IMG
- en: Offloading documents to a database for LLM Queries at Scale | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: The problem of giving all the necessary information to the model to answer questions
    is now offloaded from the model architecture to a database, containing document
    chunks.
  prefs: []
  type: TYPE_NORMAL
- en: The documents of relevance can then be found by computing similarities between
    the question and the document chunks. This is done typically by converting the
    chunks and question into word embedding vectors, and computing cosine similarities
    between chunks and question, and finally choosing only those chunks above a certain
    cosine similarity as relevant context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the question and context can be combined into a prompt as below, and
    fed into an LLM API like ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You might ask — why not feed the entire documents and question into the prompt
    instead of separating out into chunks? The reason we are separating documents
    into chunks and not combining all documents into a single prompt is because LLMs
    have certain limits on maximum input and output length. For ChatGPT, this is 4096
    tokens, or roughly 6–7 pages of text.
  prefs: []
  type: TYPE_NORMAL
- en: Offloading documents to a database and querying using closed LLM APIs might
    work well in cases where the answer is obviously present in these documents. But
    there are a number of cases in which these could yield sub-optimal results, such
    as if you want ChatGPT to give an expert perspective on a niche topic — for example,
    speculating on the financial outlook of the U.S. economy.
  prefs: []
  type: TYPE_NORMAL
- en: Domain Specific LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ok, so you tried out ChatGPT or BARD — and you didn’t like it. The answers
    were a bit off. For example, you asked ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: “How long is the U.S. economy likely to have inflation?”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**And it returned:**'
  prefs: []
  type: TYPE_NORMAL
- en: As an AI language model, I cannot predict the future with certainty, especially
    with regards to complex economic systems. However, I can provide you with some
    information and perspectives that may help you understand the current situation.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Inflation is a complex phenomenon that can be caused by a variety of factors
    such as an increase in demand, supply constraints,….
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unfortunately that is not good enough for you. Of course you can feed ChatGPT
    some recent news about Fed Chairman Jerome Powell’s statements. But that does
    not give you the same rich domain experience that you would get if you spoke to
    — well, Jerome Powell, who else! Or another expert.
  prefs: []
  type: TYPE_NORMAL
- en: Think about what it takes to be an expert in a certain field. While some amount
    of this is reading books on the topic, a lot is also interacting with subject
    matter experts in the field, and learning from experience. While ChatGPT has been
    trained on an incredible number of finance books, it probably hasn’t been trained
    by top financial experts or experts in other specific fields. So how would you
    make an LLM that is an “expert” in the finance sector? This is where fine-tuning
    comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before I discuss fine-tuning LLMs, let’s talk about fine-tuning smaller language
    models like BERT, which was commonplace before LLMs. For models like BERT and
    RoBERTa, fine-tuning amounts to passing some context, and labels. Tasks are well-defined
    like extracting answers from contexts, or classifying emails as spam vs not spam.
    I’ve written a couple of blog posts on these that might be useful if you are interested
    in fine-tuning language models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=post_page-----2dddc09a404a--------------------------------)
    [## Fine-Tune Transformer Models For Question Answering On Custom Data'
  prefs: []
  type: TYPE_NORMAL
- en: A tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and
    obtaining significant performance boosts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=post_page-----2dddc09a404a--------------------------------)
    [](/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1?source=post_page-----2dddc09a404a--------------------------------)
    [## Transformer Models For Custom Text Classification Through Fine-Tuning
  prefs: []
  type: TYPE_NORMAL
- en: A tutorial on how to build a spam classifier (or any other classifier) by fine-tuning
    the DistilBERT model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1?source=post_page-----2dddc09a404a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: However, the reason large language models (LLMs) are all the rage is because
    they can perform multiple tasks seamlessly by changing the way you frame prompts,
    and you have the experience similar to talking with a person at the other end.
    What we want now is to fine-tune that LLM to be an expert in a certain subject
    and engage in conversation like a “person.” This is quite different from fine-tuning
    a model like BERT on specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: One of the earliest open-source breakthroughs was by a group of Stanford researchers
    that fine-tuned a 7B LLaMa model (released earlier in the year by Meta) which
    they called [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) for less
    than 600$ on 52K instructions. Soon after, the Vicuna team released a 13 Billion
    parameter model which achieves [90% of ChatGPT quality](https://lmsys.org/blog/2023-03-30-vicuna/).
  prefs: []
  type: TYPE_NORMAL
- en: Very recently, the [MPT-7B transformer](https://www.mosaicml.com/blog/mpt-7b)
    was released that could ingest 65k tokens, 16X the input size of ChatGPT! The
    training was done from scratch over 9.5 days for 200k$. As an example for a domain
    specific LLM, Bloomberg released a GPT-like model [BloombergGPT](https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/),
    built for finance and also trained from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Recent advancements in training and fine-tuning open-source models are just
    the beginning for small and medium sized companies enriching their offerings through
    customized LLMs. So how do you decide when it makes sense to fine-tune or train
    entire domain specific LLMs?
  prefs: []
  type: TYPE_NORMAL
- en: First off, it is important to clearly establish the limitations of closed-source
    LLM APIs in your domain and make the case for empowering customers to chat with
    an expert in that domain at a fraction of the cost. Fine-tuning a model is not
    very expensive for a hundred thousand instructions or so — but getting the right
    instructions requires careful thought. This is where you also need to be a bit
    bold — I can’t yet think of many areas where a fine-tuned model is shown to perform
    significantly better than ChatGPT on domain specific tasks, but I believe this
    is right around the corner, and any company that does this well will be rewarded.
  prefs: []
  type: TYPE_NORMAL
- en: Which brings me to the case for completely training an LLM from scratch. Yes
    this could easily cost upwards of hundreds of thousands of dollars, but if you
    make a solid case, investors would be glad to pitch in. In a recent [interview
    with IBM, Hugging Face CEO Clem Delangue](https://research.ibm.com/blog/generative-ai-dario-gil-think?sf177843602=1)
    commented that soon, customized LLMs could be as common place as proprietary codebases
    — and a significant component of what it takes to be competitive in an industry.
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LLMs applied to specific domains can be extremely valuable in the industry.
    There are **3 levels of increasing cost and customizability**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Closed source APIs + Document Embedding Database:** This first solution is
    probably the easiest to get started off with, and considering the high quality
    of ChatGPT API — might even give you a good enough (if not the best) performance.
    And it’s cheap!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Fine-tune LLMs:** Recent progress from fine-tuning LLaMA-like models has
    shown this costs **~500$** to get a baseline performance similar to ChatGPT in
    certain domains. Could be worthwhile if you had a database with ~50–100k instructions
    or conversations to fine-tune a baseline model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train from scratch:** As LLaMA and the more recent MPT-7B models have shown,
    this costs **~100–200k** and takes a week or two.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you have the knowledge — go forth and build your custom domain specific
    LLM applications!
  prefs: []
  type: TYPE_NORMAL
- en: '*If you like this post, follow me — I write on topics related to applying state-of-the-art
    NLP in real-world applications and, more generally, on the intersections between
    data and society.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/skanda-vivek-01619311b/)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are not yet a Medium member and want to support writers like me, feel
    free to sign-up through my referral link:* [*https://skanda-vivek.medium.com/membership*](https://skanda-vivek.medium.com/membership)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Here are some related articles:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[LLM Economics: ChatGPT vs Open-Source](https://medium.com/towards-data-science/llm-economics-chatgpt-vs-open-source-dfc29f69fec1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[How Do You Build A ChatGPT-Powered App?](https://medium.com/geekculture/how-do-you-build-a-chatgpt-powered-app-89c83f3e2143)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Extractive vs Generative Q&A — Which is better for your business?](https://medium.com/towards-data-science/extractive-vs-generative-q-a-which-is-better-for-your-business-5a8a1faab59a)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Fine-Tune Transformer Models For Question Answering On Custom Data](https://medium.com/towards-data-science/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Unleashing the Power of Generative AI For Your Customers](https://medium.com/geekculture/unleashing-the-power-of-generative-ai-for-your-customers-70297f1c9698)'
  prefs: []
  type: TYPE_NORMAL
