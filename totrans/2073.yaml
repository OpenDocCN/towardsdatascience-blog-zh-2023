- en: 'The Ultimate Guide to Training BERT from Scratch: Final Act'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The Final Frontier: Building and Training Your BERT Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------)
    ·7 min read·Dec 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/548c617f9d3d11635cc30ccc45e1d106.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Rob Laughter](https://unsplash.com/@roblaughter?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This blog post concludes our series on training BERT from scratch. For context
    and a complete understanding, please refer to [Part I](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f),
    [Part II](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822),
    and [Part III](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)
    of the series.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When BERT burst onto the scene in 2018, it triggered a tsunami in the world
    of Natural Language Processing (NLP). Many consider this as the NLP’s own ImageNet
    moment, drawing parallels to the shift deep neural networks brought to computer
    vision and the broader field of machine learning back in 2012.
  prefs: []
  type: TYPE_NORMAL
- en: Five years down the line, the prophecy holds true. Transformer-based Large Language
    Models (LLMs) aren’t just the shiny new toy; they’re reshaping the landscape.
    From transforming how we work to revolutionizing how we access information, these
    models are core technology behind countless emerging startups aiming to harness
    their untapped potential.
  prefs: []
  type: TYPE_NORMAL
- en: This is the reason I decided to write this series of blog posts, diving into
    the world of BERT and how can you train your own model from scratch. The point
    isn’t just to get the job done — after all, you can easily find pre-trained BERT
    models on the Hugging Face Hub. The real magic lies in understanding the inner
    workings of this groundbreaking model and applying that knowledge to the current
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first post served as your entry ticket, introducing BERT’s core concepts,
    objectives, and potential applications. We even went through the fine-tuning process
    together, creating a question-answering system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----eab78b0657bb--------------------------------)
    [## The Ultimate Guide to Training BERT from Scratch: Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: 'Demystifying BERT: The definition and various applications of the model that
    changed the NLP landscape.'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----eab78b0657bb--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'The second installment acted as your insider’s guide to the often-overlooked
    realm of tokenizers — unpacking their role, showing how they convert words into
    numerical values, and guiding you through the process of training your own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----eab78b0657bb--------------------------------)
    [## The Ultimate Guide to Training BERT from Scratch: The Tokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: 'From Text to Tokens: Your Step-by-Step Guide to BERT Tokenization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----eab78b0657bb--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the third chapter, we tackled what I consider the most difficult step of
    the entire training pipeline: dataset preparation. We delved into the details
    of preparing your dataset for BERT’s specialized Masked ML and NSP tasks, a proxy
    it uses to understand language and context.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5?source=post_page-----eab78b0657bb--------------------------------)
    [## The Ultimate Guide to Training BERT from Scratch: Prepare the Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Preparation: Dive Deeper, Optimize your Process, and Discover How to Attack
    the Most Crucial Step'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5?source=post_page-----eab78b0657bb--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'And now, the curtain rises and drops on our final act: training. In this post,
    we’ll roll up our sleeves and train a new BERT model — one you can later fine-tune
    for your specific NLP needs. If you’ve already dipped your toes into deep learning
    model training, you should be able to follow this one without any issues. So,
    without further ado, let’s dive in!'
  prefs: []
  type: TYPE_NORMAL
- en: '[Learning Rate](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-training)
    is a newsletter for those who are curious about the world of ML and MLOps. If
    you want to learn more about topics like this subscribe [here](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-training).
    You’ll hear from me on the last Sunday of every month with updates and thoughts
    on the latest MLOps news and articles!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Training Loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern frameworks like PyTorch and TensorFlow have streamlined the process of
    building and training a deep neural network. The training loop is like choreography
    that passes the dataset features through the model to get a prediction, compares
    the model’s prediction to the labels (the ground truth) to calculate the loss,
    and finally uses that loss to update the weights of the model by propagating the
    error backward through the layers of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what this flow looks like in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In theory, it looks simple, wouldn’t you agree? Well, in practice, it gets simpler
    yet more powerful. We will use the Hugging Face `transformers` library to train
    our BERT encoder. The `Trainer` object handles the loop implementation for us,
    adding a lot of other features on top of that. Thus, we can turn our attention
    to other pieces of the code. So, let’s define our metric first.
  prefs: []
  type: TYPE_NORMAL
- en: 'On paper, the process seems really simple, don’t you think? The reality is
    even better: it’s not just simple but also incredibly robust, thanks to tools
    like the Hugging Face `transformers` library.'
  prefs: []
  type: TYPE_NORMAL
- en: Their `Trainer` object takes over the heavy lifting while tossing in a lot of
    additional features. Thus, we can turn our attention to other pieces of the code.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s define our metric next.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate our model’s performance during training, we need to keep our eyes
    on two key numbers: the loss and a metric that’s meaningful to us. The loss is
    what algorithms like gradient descent use to improve the model’s performance during
    back-propagation, while the metric is something that is more human-readable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the spirit of simplicity, we’ll opt for a straightforward measure: the model’s
    accuracy on both the Masked Language Modeling (MLM) and Next Sentence Prediction
    (NSP) tasks. Essentially, we’re interested in two things: how accurately our model
    fills in those masked words and how good is to recognize that two sentences logically
    follow one another.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we must say that this is not the best way to validate our model’s
    performance. For instance, picture the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: I am [MASK] at math.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How would you fill in the masked word? I am *great* at math, or I am *terrible*
    at math? Both of these predictions seem contextually valid, yet they completely
    change the meaning of the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: So, evaluating your model against a validation dataset that offers only one
    correct answer is not the best idea. This may not really test your model’s ability
    to understand language but to memorize. To mitigate this issue a bit, we can calculate
    another metric, like the perplexity of our model after training.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let’s evaluate the accuracy of the model by loading the corresponding
    module from the `transformers` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming you’ve been with me through this tutorial series, you’re now at a
    pivotal moment where you have prepared your dataset. There’s a common saying among
    data scientists: once your dataset is sculpted for the model, the bulk of your
    work is nearly complete. Thanks to the advancements in Deep Learning frameworks,
    what follows often feels like mere boilerplate code.'
  prefs: []
  type: TYPE_NORMAL
- en: So, with anticipation, let’s instantiate the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes the grand finale: launching the training script. First, we’ll define
    the training arguments, and then, with everything in place, we’ll launch the training
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Two hours and 20 epochs later, in the limited world of a Colab Notebook, I’ve
    hit 80% accuracy on the NSP task and 15% accuracy on the Masked ML task. For a
    model we’re training from scratch, these numbers are okay.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/686ea6f168d6cb72d37ecbb6983d1ae2.png)'
  prefs: []
  type: TYPE_IMG
- en: Training results — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s important to note that in most real-world scenarios, your starting
    line isn’t from scratch. Typically, you’d begin with a pre-trained model, which
    often leads to better results.
  prefs: []
  type: TYPE_NORMAL
- en: Also, there’s ample room to experiment and enhance performance. Tinkering with
    the hyperparameters of the training process, particularly the learning rate, can
    significantly boost your results, even when you’re building a model from the ground
    up.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve reached the final destination in our quest to train a BERT model from
    scratch. Throughout this journey, we saw why BERT is considered to be a seminal
    model in the area of NLP, and we delved into the intricacies of tokenizers, not
    only understanding their mechanisms but also mastering the art of training our
    own.
  prefs: []
  type: TYPE_NORMAL
- en: Next, our journey took us through the meticulous process of preparing a dataset
    for two distinct tasks running in tandem, the NSP and Masked ML tasks. Finally,
    we landed on the crucial step of launching a training script, using the `transformers`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: It’s been an long but fulfilling expedition, and I hope you also enjoyed it.
    You should be ready to apply this knowledge to your own activities and expand
    it to cover more ground. Until next time, happy coding!
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My name is [Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-intro),
    and I’m a machine learning engineer working for [HPE](https://www.hpe.com/us/en/home.html).
    I have designed and implemented AI and software solutions for major clients such
    as the European Commission, IMF, the European Central Bank, IKEA, Roblox and others.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in reading more posts about Machine Learning, Deep Learning,
    Data Science, and DataOps, follow me on [Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow),
    [LinkedIn](https://www.linkedin.com/in/dpoulopoulos/), or [@james2pl](https://twitter.com/james2pl)
    on Twitter.
  prefs: []
  type: TYPE_NORMAL
- en: Opinions expressed are solely my own and do not express the views or opinions
    of my employer.
  prefs: []
  type: TYPE_NORMAL
