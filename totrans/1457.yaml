- en: 'Machine Learning, Illustrated: Incremental Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/machine-learning-illustrated-incremental-machine-learning-4d73747dc60c](https://towardsdatascience.com/machine-learning-illustrated-incremental-machine-learning-4d73747dc60c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How models learn new information over time, maintaining and building upon previous
    knowledge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shreya.rao?source=post_page-----4d73747dc60c--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page-----4d73747dc60c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4d73747dc60c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4d73747dc60c--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----4d73747dc60c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4d73747dc60c--------------------------------)
    ·7 min read·Sep 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Welcome back to the Illustrated Machine Learning series. If you read the other
    [articles in the series](https://medium.com/@shreya.rao/list/machine-learning-illustrated-dfb4532491ff),
    you know the drill. We take a (*boring sounding*) machine learning concept and
    make it fun by illustrating it! This article will cover a concept called **Incremental
    Learning**, where machine learning models learn new information over time, maintaining
    and building upon previous knowledge. But before getting into that, let’s first
    talk about what the model building process looks like today.
  prefs: []
  type: TYPE_NORMAL
- en: We usually follow a process called **static learning** when building models.
    In this process, we train a model using the latest available data. We tweak and
    tune the model in the training process. And once we’re happy with its performance,
    we deploy it. This model is in production for a while. Then we notice that the
    model performance is getting worse over time. That’s when we throw away the existing
    model and build a new one using the latest available data. And we rinse and repeat
    this same process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07e8722405e2926bf8c48eebe746363e.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s illustrate this using a concrete example. Consider this hypothetical scenario.
    We started building a fraud model at the end of January 2023\. This model detects
    whether a credit card transaction is fraudulent or not. We train our model using
    all the credit card transaction data that we had from the past one-year period
    (January 2022 to December 2022) and use transaction data from this month (January
    2023) to test the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/993cd22d306048f39a9c11930a51c17e.png)'
  prefs: []
  type: TYPE_IMG
- en: At the end of next month we notice that the model isn’t doing too well against
    new data. So we built another model, but this time using data from the past one-year
    period (February 2022 to January 2023) to train it and then use the current month’s
    data (February 2023) to test it. And all data outside of these training and testing
    periods is thrown out.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e6e414b3ffc200024eca1265fc138ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Next month, we again notice that the model performance isn’t holding up against
    new data. And again, we build a new model using data from the past one-year period.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/198acd7c211542e6d589e5b9792b883c.png)'
  prefs: []
  type: TYPE_IMG
- en: And we keep repeating this process whenever we see a decline in model performance.
    This doesn't have to be after 1 month. It could be after 3 months, 6 months, or
    even a year.
  prefs: []
  type: TYPE_NORMAL
- en: '**And why do we do this batching of data?**'
  prefs: []
  type: TYPE_NORMAL
- en: 3 main reasons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concept drift: As time goes on, we see a phenomenon called concept drift, which
    means that what we’re trying to predict changes over time and using older data
    might sometimes be counterproductive.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Memory constraints: The larger our training set, the more memory it occupies.
    So we try to limit the data we input into the model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Time constraints: In the same vein as reason 2, the larger our training data,
    the longer it takes for our model to train. (Although this usually isn’t such
    a big concern for a lot of models we build. Where this might be problematic is
    in NLP models.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**But what if we don’t want to throw away all the old models and data?** Throwing
    away the old models means wasting all the knowledge that the old models have gathered
    so far. Ideally, we want to find a way to retain the previous knowledge, while
    gradually adding the information coming from new data. We want to preserve this
    ‘institutional knowledge’ because it is essential for adapting to slowly changing
    or recurring patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: '**And this is exactly what incremental learning does.**'
  prefs: []
  type: TYPE_NORMAL
- en: In incremental learning, the model learns and enhances its knowledge progressively,
    without forgetting previously acquired information. It grows with the data and
    becomes more refined over time.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this let’s go back to our fraud model example. We start the same
    way as we did in static learning. We build a model using data from the previous
    year, but when we get new data next month instead of building a new model from
    scratch, we just add our new month's data to the already existing model. And we
    repeat this process in 2 months, 3 months, and so on. So here we aren’t technically
    building new models but we are building new versions of the same model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15e361925e082f4e2240e842431111e9.png)'
  prefs: []
  type: TYPE_IMG
- en: This can be a good thing because a) It's an efficient use of resources because
    less data storage = more memory saving. Each iteration uses less memory, reducing
    costs. b) This is great for dynamic data, which is most data in the real world
    because we can continuously update predictions as and when we get new data instead
    of building a new model each time. c) The training procedure is carried out on
    a smaller portion of data, so it is much faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fraud detection actually serves as a great example of how incremental learning
    can be beneficial, exemplified by Mastercard’s real-time fraud detection system.
    With each transaction, Mastercard’s system examines more than 100 variables (such
    as transaction size, location, and merchant type) to gauge the likelihood of fraud.
    (source: [DataCamp](https://www.datacamp.com/blog/what-is-incremental-learning))
    The system uses incremental learning to adjust to changing patterns of fraudulent
    activity. In dynamic environments like financial fraud, where fraudsters constantly
    adapt their methods, the challenge of concept drift is significant. Therefore,
    our models must adapt swiftly to maintain their performance and effectively combat
    fraudsters and their evolving tactics.'
  prefs: []
  type: TYPE_NORMAL
- en: And the good news is — incremental learning has already been built into some
    of our favorite models like XGBoost and CatBoost. And it’s very easy to implement!
  prefs: []
  type: TYPE_NORMAL
- en: I explain the mathematical details behind [XGBoost](/xgboost-regression-explain-it-to-me-like-im-10-2cf324b0bbdb)
    and [CatBoost](/catboost-regression-break-it-down-for-me-16ed8c6c1eca) in my previous
    articles.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let's test static and incremental learning on real data to compare performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use this Credit Card Fraud [dataset](https://www.kaggle.com/datasets/kartik2112/fraud-detection?select=fraudTrain.csv)
    (CC0) to build the models. After some feature cleaning and selection, we end up
    with a dataset like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ac4e5692ae31cf11cc5c656517d84a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Where **is_fraud** is our target (y) column.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with static learning. We build the same 12 XGBoost models…
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '…over the following 12 one-year training periods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d1644fb3704136114a2801e5f698206.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At the end of every iteration or training period, we are shifting the training
    dates ahead by a month. Then we test each of these models over the following one-month
    test periods, which start right when their corresponding training periods ends:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ab05fb115abfbdbcd6971b03f36d127.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And then we record the 12 AUC scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We record these scores to compare them to the ones we’ll get from the incremental
    models. Remember the higher our AUC score, the better our model is performing.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now onto our incremental learning models. **We start by training the first model
    over the same one-year period as we did in static learning. This is our base model.**
    But for the next 11 models, we feed in the new months' data to the already existing
    model incrementally.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78080de785cd2b992f8e2c994a8311c5.png)'
  prefs: []
  type: TYPE_IMG
- en: So we’re continuing to train the model right where we left off each time.
  prefs: []
  type: TYPE_NORMAL
- en: The model looks pretty much the same as before except for one little change.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, we declare the previous model in the current xgboost model we’re building
    using the parameter`xgb_model`. By retaining the previously trained model, the
    training process becomes faster and more efficient, as the model doesn’t need
    to learn from scratch each time.
  prefs: []
  type: TYPE_NORMAL
- en: Then we test the 12 models using the same 12 one-month test periods as we did
    in the static models…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ab05fb115abfbdbcd6971b03f36d127.png)'
  prefs: []
  type: TYPE_IMG
- en: '…and record the AUC scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now for the fun part where we compare the performance of the two processes.
    Out of the 11 recorded AUC scores (because the first score is the same since we
    used the same training and testing data), **incremental learning had better AUC
    scores in 7 out of the 11 iterations**!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d91c1cb0aac3f4ca94852593f5283e2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Having said that there are a couple of caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is a concern with incremental learning because it relies on a continuous
    stream of data. The risk is that it might over-adjust its parameters based on
    recent data, which may not accurately represent the overall distribution. In contrast,
    state learning can consider the entire distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While incremental learning can handle evolving data, abrupt changes in data
    trends can pose a challenge. Thus, it may not be suitable for data that changes
    too drastically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incremental learning faces a phenomenon called **catastrophic forgetting**,
    where old knowledge is lost as new data is learned and it’s difficult to determine
    what specific information is forgotten.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although there are some considerations to keep in mind, it is beneficial to
    explore the integration of this approach into models by optimizing each version
    of the model. We can achieve this by fine-tuning parameters or refining our feature
    selection, in order to enhance the results even further.
  prefs: []
  type: TYPE_NORMAL
- en: And that is all on incremental learning! As always, let me know if you have
    any comments/questions/concerns, and feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/shreyarao24/)
    or email me at *shreya.statistics@gmail.com*.
  prefs: []
  type: TYPE_NORMAL
- en: Unless specified, all images are by the author.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
