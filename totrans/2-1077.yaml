- en: How I Built A Cascading Data Pipeline Based on AWS (Part 1)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何基于 AWS 构建级联数据管道（第 1 部分）
- en: 原文：[https://towardsdatascience.com/how-i-built-a-cascading-data-pipeline-based-on-aws-997b212a84d2](https://towardsdatascience.com/how-i-built-a-cascading-data-pipeline-based-on-aws-997b212a84d2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-i-built-a-cascading-data-pipeline-based-on-aws-997b212a84d2](https://towardsdatascience.com/how-i-built-a-cascading-data-pipeline-based-on-aws-997b212a84d2)
- en: Automatic, scalable, and powerful
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化、可扩展且强大
- en: '[](https://anzhemeng.medium.com/?source=post_page-----997b212a84d2--------------------------------)[![Memphis
    Meng](../Images/5a2b214eb5d5ab884b18224c471662c0.png)](https://anzhemeng.medium.com/?source=post_page-----997b212a84d2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----997b212a84d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----997b212a84d2--------------------------------)
    [Memphis Meng](https://anzhemeng.medium.com/?source=post_page-----997b212a84d2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://anzhemeng.medium.com/?source=post_page-----997b212a84d2--------------------------------)[![Memphis
    Meng](../Images/5a2b214eb5d5ab884b18224c471662c0.png)](https://anzhemeng.medium.com/?source=post_page-----997b212a84d2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----997b212a84d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----997b212a84d2--------------------------------)
    [Memphis Meng](https://anzhemeng.medium.com/?source=post_page-----997b212a84d2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----997b212a84d2--------------------------------)
    ·14 min read·Jul 31, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----997b212a84d2--------------------------------)
    ·14 分钟阅读·2023年7月31日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Today I’m going to share some experience of building a data engineering project
    that I always take pride in. You are going to learn the reasons behind why I used
    the tools and AWS components, and how I designed the architecture.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我要分享一些我引以为豪的数据工程项目的经验。你将了解到我为何使用这些工具和 AWS 组件，以及我如何设计架构。
- en: '![](../Images/4f95b8b2918bb62a2337706df55ff6e3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f95b8b2918bb62a2337706df55ff6e3.png)'
- en: Image by Author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '**Disclaimer**: The content of this text is inspired by my experience with
    an unnamed entity. However, certain critical commercial interests and details
    have intentionally been replaced with fictional data/codes or omitted, for the
    purpose of maintaining confidentiality and privacy. Therefore, the full and accurate
    extent of the actual commercial interests involved is reserved.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明**：本文本内容受到我与一个未命名实体的经验启发。然而，某些关键商业利益和细节已故意用虚构数据/代码替代或省略，以保持机密性和隐私。因此，实际涉及的商业利益的完整和准确程度保留。'
- en: Prerequisites
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前提条件
- en: Knowledge of Python
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 Python 的了解
- en: Understanding of AWS components, such as DynamoDB, Lambda serverless, SQS and
    CloudWatch
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 AWS 组件的了解，如 DynamoDB、Lambda 无服务器、SQS 和 CloudWatch
- en: Comfortable coding experience with [YAML](https://en.wikipedia.org/wiki/YAML)
    & [SAM CLI](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/install-sam-cli.html)
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 [YAML](https://en.wikipedia.org/wiki/YAML) 和 [SAM CLI](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/install-sam-cli.html)
    的舒适编码体验
- en: Background
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'Let’s say you are a data engineer and you need to constantly update the data
    in the warehouse. For example, you are responsible to sync up with the sales records
    of [Dunder Mifflin Paper Co.](https://dundermifflinpaper.com) on a regular basis.
    (I understand this is not a realistic scenario but have fun :) !) The data is
    sent to you via a vendor’s API and you are held accountable for making sure the
    information of the branches, employees (actually only salespersons are considered),
    and sales are up-to-date. The provided API has the following 3 paths:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一个数据工程师，需要不断更新数据仓库中的数据。例如，你需要定期与 [Dunder Mifflin Paper Co.](https://dundermifflinpaper.com)
    的销售记录同步。（我知道这不是一个现实的场景，但请尽情享受 :)！）数据通过供应商的 API 发送给你，你需要确保分支、员工（实际上只考虑销售人员）和销售的信息是最新的。提供的
    API 有以下 3 个路径：
- en: '`/branches` , accepting branch name as a query parameter for retrieving the
    metadata of a specified branch;'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`/branches` ，接受分支名称作为查询参数以检索指定分支的元数据；'
- en: '`/employees` , accepting branch ID as a query parameter for retrieving the
    information of all its employees of a certain branch, the response includes a
    key-value pair that indicates the employees’ occupations;'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`/employees` ，接受分支 ID 作为查询参数以检索某个分支所有员工的信息，响应中包括一个表示员工职业的键值对；'
- en: '`/sales` , accepting employee ID as a query parameter for retrieving the all-time
    sales records of a salesperson, the response includes a key-value pair that indicates
    when the transaction was complete.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`/sales`，接受员工ID作为查询参数来检索销售人员的所有销售记录，响应包括一个指示交易完成时间的键值对。'
- en: 'So generally speaking, the returns of API look like this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，API的返回结果如下：
- en: '`/branches` path:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`/branches` 路径：'
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`/employees` path:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`/employees` 路径：'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`/sales` path:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`/sales` 路径：'
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It’s expected that your final work will facilitate the data analysts who are
    able to retrieve data for their analyses only with SQL queries.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 期望你的最终工作将使数据分析师能够仅通过SQL查询来检索他们分析所需的数据。
- en: Ideas
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思路
- en: It’s quite an easy call to say that in the end, we are going to have 3 different
    places to respectively store the data of branches, salespersons, and sales. The
    data will be imported by accessing certain API paths respectively. However, due
    to the fact that the identifiers for all those entities are mostly automatically
    generated, it’s not likely for a practitioner to have the IDs beforehand. Instead,
    since it’s normally available for us to find the branch names so it’s plausible
    that we use the first path to grab the metadata of branches, as well as the IDs
    of its employees. And we can access the `/employees` path using the employee IDs
    and so can we do to the `/sales` path. ***That’s exactly why I call this pipeline
    cascading.***
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们将有3个不同的地方分别存储分支、销售人员和销售的数据，这一点说起来很简单。数据将通过访问特定的API路径来导入。然而，由于所有这些实体的标识符大多是自动生成的，实践者不大可能提前获得这些ID。相反，由于我们通常可以找到分支名称，因此可以使用第一个路径来获取分支的元数据及其员工的ID。然后，我们可以使用员工ID访问`/employees`路径，`/sales`路径也是如此。***这就是我称这个流程为级联的原因。***
- en: To assure our database is up-to-date most of the time, it is necessary to execute
    these operations frequently enough. But on the other, we are also obligated to
    take into consideration the cost and the potential API visit quotas. Hence, running
    it once an hour is proper though arguably not yet optimal.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们的数据库大部分时间是最新的，必须足够频繁地执行这些操作。但另一方面，我们也需要考虑成本和潜在的API访问配额。因此，每小时运行一次是适当的，尽管可以说尚未达到最佳。
- en: Last but not least, let’s discuss AWS. First of all, the codes executing those
    operations are going to be run by [**AWS Lambda**](https://aws.amazon.com/lambda/)
    because of its capacity of having 200+ AWS services and applications as its triggers,
    including [SQS](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-pipes-sqs.html)
    and [EventBridge](https://docs.aws.amazon.com/eventbridge/index.html). The data
    is going to be delivered via **SQS** as one of the most established messaging
    services provided by AWS. Finally, the information scraped from API is going to
    be stored in **DynamoDB**. To some experienced readers here, it’s probably confusing
    to leverage **DynamoDB** as the data warehousing tool since this is a NoSQL database
    service while data warehouses are generally incompatible with NoSQL databases.
    I’m surely aware of it, and the DynamoDB tables here will only be the staging
    ones as I can make use of its flexibility in key-value/document data model schemas
    before eventually converting JSON-formatted API retrievals into data warehouse
    records. Check out [**this article**](https://medium.com/plumbersofdatascience/a-real-time-streaming-channel-c8bbae187c83)
    if you are interested in the details of my implementation of DynamoDB-S3 loading.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，让我们讨论AWS。首先，执行这些操作的代码将由[**AWS Lambda**](https://aws.amazon.com/lambda/)运行，因为它能够使用200多个AWS服务和应用程序作为触发器，包括[SQS](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-pipes-sqs.html)和[EventBridge](https://docs.aws.amazon.com/eventbridge/index.html)。数据将通过**SQS**传递，作为AWS提供的最成熟的消息服务之一。最后，从API抓取的信息将存储在**DynamoDB**中。对于一些有经验的读者来说，利用**DynamoDB**作为数据仓库工具可能会令人困惑，因为这是一个NoSQL数据库服务，而数据仓库通常与NoSQL数据库不兼容。我确实意识到这一点，DynamoDB表在这里将仅作为临时存储表，因为我可以利用其在键值/文档数据模型模式中的灵活性，然后最终将JSON格式的API检索数据转换为数据仓库记录。如果你对我实现DynamoDB-S3加载的细节感兴趣，可以查看[**这篇文章**](https://medium.com/plumbersofdatascience/a-real-time-streaming-channel-c8bbae187c83)。
- en: Implementation
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: Here is the structure of my final work.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我最终工作的结构。
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There are 3 folders (*/branches*, */salespersons*, */sales*) respectively containing
    the codes of each lambda function. *Utils.py* is a Swiss-army-knife-like file
    where the functions, variables, and classes are globally applied. And template.yml
    is the AWS CloudFormation template that we will use to declare and deploy AWS
    resources to establish our data pipeline.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有3个文件夹（*/branches*，*/salespersons*，*/sales*），分别包含每个lambda函数的代码。*Utils.py* 是一个多功能文件，其中的函数、变量和类被全局应用。模板.yml
    是我们将用来声明和部署AWS资源以建立数据管道的AWS CloudFormation模板。
- en: 'Lambda_function.py in each folder is the entrance function of the code execution:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文件夹中的`Lambda_function.py`是代码执行的入口函数：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '/Service/config.py returns the environment variables input in the *template.yml*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: /Service/config.py 返回*template.yml*中输入的环境变量：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: /Service/service.py is where we actually wrangle the data. Basically speaking,
    the function is invoked by a trigger or two (time schedulers), before retrieving
    data from a data source (API or SQS queue). Data will be packaged in a set of
    key-value pairs and if in need, updated into its corresponding DynamoDB table
    before the function distributes the identifiers of its members (i.e., all employees
    in a branch, all sales records of a salesperson).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`/Service/service.py` 是我们实际处理数据的地方。基本上，该功能由一个或两个触发器（时间调度器）调用，在从数据源（API或SQS队列）中检索数据之前。数据将以键值对的形式打包，如有需要，更新到相应的DynamoDB表中，然后该功能分发其成员的标识符（即，分支中的所有员工，销售人员的所有销售记录）。'
- en: 'Take `/branches/service/service.py` as an example. Its functionality includes:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以 `/branches/service/service.py` 为例。它的功能包括：
- en: acquire all the data from the API `/branches` as soon as it is wakened up;
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦被唤醒，立即从API `/branches` 获取所有数据；
- en: check the existence and accuracy of the personal information of each salesperson
    in the DynamoDB data table, if it’s not up-to-date, upsert the data record;
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查DynamoDB数据表中每个销售人员的个人信息是否存在和准确，如果不更新，则插入数据记录；
- en: get all the IDs of their employees, and deliver them along with their branch
    ID as a message via an SQS queue to the tailing function (*/salespersons*).
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取所有员工的ID，并将其连同分支ID作为消息通过SQS队列传递给尾部函数（*/salespersons*）。
- en: 'In practice, the implementation will be like:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，实施方式如下：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the end, we need to prepare for the build and deployment:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要为构建和部署做准备：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Q&A
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问答
- en: Yes, I’m doing a Q&A session with myself. It’s usually helpful to challenge
    myself with “Why?” or “How?” when coding, by doing so I will have more confidence
    in solidifying each decision I made, as well as justifying every tool I used.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我正在进行自问自答的环节。在编码时，挑战自己“为什么？”或“怎么做？”通常是有帮助的，这样我会对自己所做的每个决定有更多信心，也能证明我使用的每个工具。
- en: a. How do I monitor the functions?
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: a. 我如何监控这些功能？
- en: I use [CloudWatch alarms](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html).
    CloudWatch alarms watch any available metric or the calculation of the metrics
    supported by AWS CloudWatch. They can conduct customized action(s) based on the
    metric or the calculation relative to the given threshold value within a specified
    period.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用[CloudWatch警报](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)。CloudWatch警报监视任何可用的指标或AWS
    CloudWatch支持的指标计算。它们可以根据给定阈值在指定时间内对指标或计算进行自定义操作。
- en: To me, it’s most pivotal to learn and relieve it as soon as an error happens.
    So I set up alarms towards all 3 functions with 1 error as the threshold, that’s
    to say, alarms will be pulled whenever there is an error. I want to recognize
    the errors without constantly keeping an eye on a CloudWatch dashboard, so the
    alarms’ actions are to push a notification to an SNS topic which forwards the
    alert to my email inbox.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，最关键的是在发生错误时立即学习和解决。因此，我为所有3个功能设置了以1个错误为阈值的警报，也就是说，只要出现错误就会触发警报。我希望在不不断盯着CloudWatch仪表板的情况下识别错误，因此警报的操作是将通知推送到SNS主题，该主题将警报转发到我的电子邮件收件箱。
- en: If you are working in a collaborative environment, I suggest you extend the
    visibility by sending it to a Slack channel, distributing it to all addresses
    in a distribution list, or including it in a shared mailbox.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个协作环境中工作，我建议你通过将其发送到Slack频道，分发到分发列表中的所有地址，或将其包含在共享邮箱中，以扩展可见性。
- en: b. Why do you define the keys of the tables as they are?
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: b. 你为什么按现在的方式定义表的键？
- en: It’s based on reality. Apparently, branches differentiate from each other so
    their IDs are sufficient to be the sole **hash key** in the branches’ table under
    the [1NF constraint](https://en.wikipedia.org/wiki/Database_normalization). By
    contrast, both salespersons and sales tables take extra keys to be normalized.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基于现实情况的。显然，分支之间有所不同，因此它们的 ID 足以作为分支表中的唯一**哈希键**，符合[1NF 约束](https://en.wikipedia.org/wiki/Database_normalization)。相比之下，销售人员和销售表则需要额外的键进行规范化。
- en: Because in reality, a branch is likely to have multiple employees in the book,
    while an employee is allowed to transition from one branch to another, the relationship
    between branches and employees, from the perspective of data schemas, is many-to-many.
    Also, it is because of it that only a combination of sale record ID + salesperson
    ID + branch ID (the branch when the transaction took place) shall point to an
    exact record in the sales table. The bottleneck is a document-based database like
    DynamoDB allows as many as 2 attributes to serve as keys, I picked the salesperson
    ID as the **sort key** over the branch ID in favor of the certainty between a
    sales-record-salesperson relationship. The variance between sales and branches
    is going to be explained in the following question.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在现实中，一个分支可能在记录中有多个员工，而员工允许从一个分支转移到另一个分支，从数据模式的角度来看，分支与员工之间的关系是多对多的。而且正因为如此，只有销售记录
    ID + 销售人员 ID + 分支 ID（交易发生时的分支）组合才能指向销售表中的一个确切记录。瓶颈在于像 DynamoDB 这样的文档数据库允许最多两个属性作为键，我选择了销售人员
    ID 作为**排序键**，而不是分支 ID，以确保销售记录与销售人员之间的关系。销售和分支之间的差异将在接下来的问题中解释。
- en: c. How do I establish the linkage between the sales and branches? And why?
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: c. 我如何建立销售和分支之间的联系？为什么？
- en: The data vendor falls short in including branch information in the sales records.
    The cookie cutter to take care of it is to attach the branch ID from the very
    top (branch collector function) down to the end. This manner, nevertheless, omits
    some extreme scenarios. For instance, [Jim Halpert](https://en.wikipedia.org/wiki/Jim_Halpert)
    placed a sale on his last day in the Scranton branch. owing to some technical
    issues, this record wasn’t appended to his sales record list or published on the
    API, until the second business day when his status had been preset to transfer
    as a Stamford worker.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 数据供应商在销售记录中未能包含分支信息。处理这个问题的万灵药是从最上层（分支收集器函数）到最底层附加分支 ID。然而，这种方式忽略了一些极端情况。例如，[吉姆·哈尔普特](https://en.wikipedia.org/wiki/Jim_Halpert)
    在他在斯克兰顿分支的最后一天进行了销售。由于一些技术问题，这条记录没有被添加到他的销售记录列表中，也没有发布到 API 上，直到第二个工作日他被预设为转移到斯坦福德的员工。
- en: It’s hard to sniff the mislabeling without any context especially when the root
    reason is from the vendor. From my experience, the debugging at this stage relies
    heavily on our feedback. This is why I let the branch ID in the sales table be
    a loose key-value pair; otherwise, it takes additional effort to remove and rewrite
    the item.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 没有上下文很难发现标签错误，尤其是当根本原因来自供应商时。从我的经验来看，此阶段的调试非常依赖于我们的反馈。这就是为什么我让销售表中的分支 ID 成为一个宽松的键值对；否则，需要额外的工作来删除和重写该项。
- en: d. How do I trigger the salespersons and sales collector functions?
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: d. 我如何触发销售人员和销售收集器函数？
- en: SQS queue is one of the invoking actions officially allowed by the Lambda function,
    and hence the natural choice to wake up these 2 functions since they are set to
    listen to queues already. I took a detour, to walk around the maximum visit cap
    imposed by the API owner. Should I let my functions pick up the messages and hit
    the API from the queues as soon as they come, there would be multiple functions
    processing the messages nearly at the same time, which rendered the pipelining
    architecture no longer functionable as it could easily exceed the API quota. With
    time schedulers set up every minute (I created 2 schedulers for each function),
    the processing frequency is declined from the millisecond level to the second
    level. In this way, the message traffic in the data pipeline is mitigated.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: SQS 队列是 Lambda 函数官方允许的触发操作之一，因此这是唤醒这两个函数的自然选择，因为它们已经设置为监听队列。我绕了一下路，绕过了 API 所有者施加的最大访问限制。如果我让我的函数在消息到达时立即从队列中获取并访问
    API，将会有多个函数几乎同时处理消息，这使得管道架构失效，因为它可能很容易超过 API 配额。通过设置每分钟的时间调度器（我为每个函数创建了两个调度器），处理频率从毫秒级下降到秒级。通过这种方式，数据管道中的消息流量得到了缓解。
- en: e. How do I avoid repetitive operations?
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: e. 如何避免重复操作？
- en: It is almost impossible to tell whether the data collected is up-to-date or
    not without actually visiting the API, the source of truth. So we can’t reduce
    API visits, but instead, can do what I do in the last question to lower the chance
    that the API visit quota is exceeded.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎不可能在不实际访问API（真相来源）的情况下判断收集的数据是否是最新的。因此，我们不能减少API访问次数，而是可以像我在上一个问题中所做的那样降低超出API访问配额的风险。
- en: If the destination of the dataflow is DynamoDB, it’s all set to fully upsert
    each record every single time when we receive it from the API. The horror is that
    our firehose stream from DynamoDB to S3 is short of bandwidth, which leads to
    a halt in transportation occasionally. In light of this fact, I insert a sanity
    check before an upsertion. This check compares each value of the record’s attributes
    with those recently withdrawn from the API. The existing record shall be overwritten
    unless it is totally unchanged.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据流的目标是DynamoDB，每次从API接收时都会完全更新每条记录。令人担忧的是，我们从DynamoDB到S3的火hose 流带宽不足，导致运输偶尔中断。鉴于这一情况，我在更新之前插入了一个合理性检查。此检查将记录的每个属性值与最近从API提取的值进行比较。除非记录完全没有变化，否则现有记录将被覆盖。
- en: 'Attached is the sanity check function:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 附件是合理性检查函数：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Deployment
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: In each folder, do
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个文件夹中，执行
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And go back to the parent folder:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后返回到上一级文件夹：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Final Work
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终工作
- en: '[My GitHub repo](https://github.com/MemphisMeng/Cascading-ETL-pipeline/tree/dev)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[我的 GitHub 仓库](https://github.com/MemphisMeng/Cascading-ETL-pipeline/tree/dev)'
- en: Credits
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 版权信息
- en: '[Diagrams as Code](https://diagrams.mingrammer.com/) for all the tools that
    support concise visualizations'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[作为代码的图表](https://diagrams.mingrammer.com/)提供了所有支持简洁可视化的工具'
- en: '[The Office](https://www.imdb.com/title/tt0386676/) for a brilliant show, the
    laughter it brings, and the inspiration'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《办公室》（The Office](https://www.imdb.com/title/tt0386676/)是一个精彩的节目，带来的欢笑和灵感。'
