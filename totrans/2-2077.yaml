- en: The Ultimate Preprocessing Pipeline for Your NLP Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你 NLP 模型的终极预处理流程
- en: 原文：[https://towardsdatascience.com/the-ultimate-preprocessing-pipeline-for-your-nlp-models-80afd92650fe](https://towardsdatascience.com/the-ultimate-preprocessing-pipeline-for-your-nlp-models-80afd92650fe)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-ultimate-preprocessing-pipeline-for-your-nlp-models-80afd92650fe](https://towardsdatascience.com/the-ultimate-preprocessing-pipeline-for-your-nlp-models-80afd92650fe)
- en: Get the most out of training NLP ML models by feeding the best possible input
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过提供最佳可能的输入，充分发挥 NLP 机器学习模型的效果。
- en: '[](https://rahulraj24.medium.com/?source=post_page-----80afd92650fe--------------------------------)[![Rahulraj
    Singh](../Images/8bfa5fdcb41c9c81c026b88744145b11.png)](https://rahulraj24.medium.com/?source=post_page-----80afd92650fe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----80afd92650fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----80afd92650fe--------------------------------)
    [Rahulraj Singh](https://rahulraj24.medium.com/?source=post_page-----80afd92650fe--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rahulraj24.medium.com/?source=post_page-----80afd92650fe--------------------------------)[![Rahulraj
    Singh](../Images/8bfa5fdcb41c9c81c026b88744145b11.png)](https://rahulraj24.medium.com/?source=post_page-----80afd92650fe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----80afd92650fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----80afd92650fe--------------------------------)
    [Rahulraj Singh](https://rahulraj24.medium.com/?source=post_page-----80afd92650fe--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----80afd92650fe--------------------------------)
    ·10 min read·May 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----80afd92650fe--------------------------------)
    ·阅读时间 10 分钟·2023 年 5 月 8 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e689a96f5fd638b621c932f2bb07d426.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e689a96f5fd638b621c932f2bb07d426.png)'
- en: Photo by [Cyrus Crossan](https://unsplash.com/@cys_escapes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Cyrus Crossan](https://unsplash.com/@cys_escapes?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
    上。
- en: If you have worked on a text summarization project before, you would have noticed
    the difficulty in seeing the results you expect to see. You have a notion in mind
    for how the algorithm should work and what sentences it should mark in the text
    summaries, but more often than not the algorithm sends out results that are “not-so-accurate”.
    Even more interesting is keyword extraction because all sorts of algorithms from
    topic modeling to vectorizing embeddings, are all really good but given a paragraph
    as an input the results they give out are again “not-so-accurate” because the
    most often occurring word is not always the most important word of the paragraph.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经参与过文本摘要项目，你可能会发现很难看到你期望的结果。你脑海中有一个关于算法如何工作的概念，以及它应该在文本摘要中标记哪些句子，但往往算法提供的结果是“不是很准确”的。更有趣的是关键词提取，因为从主题建模到向量化嵌入的各种算法都表现得很出色，但当输入一个段落时，它们给出的结果同样是“不是很准确”的，因为出现频率最高的词不一定是段落中最重要的词。
- en: Preprocessing and data cleaning requirements vary largely based on the use case
    you are trying to solve. I will attempt to create a generalized pipeline that
    should work well for all NLP models, but you will always need to tune the steps
    to achieve the best results for your use-case. In this story, I will focus on
    NLP models that solve for **topic modelling, keyword extraction, and text summarization**.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据预处理和清洗的需求在很大程度上取决于你要解决的具体问题。我将尝试创建一个通用的流程，应该适用于所有 NLP 模型，但你总是需要调整这些步骤，以获得最佳的结果。在这个故事中，我将重点关注解决
    **主题建模、关键词提取和文本摘要** 的 NLP 模型。
- en: '![](../Images/29dcaa4577d871de3734a6e3d9b5b855.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29dcaa4577d871de3734a6e3d9b5b855.png)'
- en: Preprocessing Pipeline | Image by Author
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理流程 | 作者图像
- en: The image above outlines the process we will be following to build the preprocessing
    NLP pipeline. The four steps mentioned above, are explained with code later and
    there is also a Jupyter notebook attached, that implements the complete pipeline
    together. The idea behind this pipeline is to highlight steps that will enhance
    the performance of machine learning algorithms that are going to be used on text
    data. This is a step between input data and model training.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图片概述了我们将要遵循的预处理NLP管道的过程。上述四个步骤将在后面用代码解释，同时还附有一个Jupyter笔记本，实现了完整的管道。这条管道的想法是突出那些能够提升将在文本数据上使用的机器学习算法性能的步骤。这是在输入数据和模型训练之间的一个步骤。
- en: 1\. Cleaning the text
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 清理文本
- en: The first step to structuring the pipeline is cleaning the input text data,
    which can consist of several steps based on the model you are trying to build
    and the results you desire. Machine learning algorithms (or largely all computer
    algorithms, rather every computer instruction) work on numbers, which is why building
    a model for text data is challenging. You are essentially asking the computer
    to learn and work on something it has never seen before and hence, it needs a
    bit more work.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化管道的第一步是清理输入文本数据，这可能包含几个步骤，具体取决于你试图构建的模型以及你期望的结果。机器学习算法（或者说几乎所有计算机算法，实际上是每一条计算机指令）都在数字上工作，这就是为什么为文本数据构建模型具有挑战性的原因。你实际上是在要求计算机学习和处理它从未见过的内容，因此需要多花一些功夫。
- en: In the section below, I give the first function of our pipeline to perform cleaning
    on the text data. There are numerous operations parts of the cleaning function,
    and I have explained them all in the comments of the code.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的部分中，我提供了我们管道的第一个功能来执行文本数据的清理。清理功能包含许多操作部分，我在代码的注释中解释了它们。
- en: To see the performance of this function, below is an input to the function and
    the output that it generates.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看该功能的性能，下面是函数的输入和它生成的输出。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As we observe in the output, the text is now clean of all HTML tags, it has
    converted emojis to their word forms and corrected the text for any punctuations
    and special characters. This text is now easier to deal with and in the next few
    steps, we will refine it even further.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在输出中所观察到的，文本现在已经清除了所有的HTML标签，将表情符号转换为其文字形式，并纠正了任何标点符号和特殊字符。现在处理这些文本更为简便，在接下来的几步中，我们将进一步优化它。
- en: 2\. Clustering to remove noise and boilerplate language from the data
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 聚类以去除数据中的噪声和模板语言
- en: The next step in our preprocessing pipeline is probably the most important and
    underrated activity for an NLP workflow. In the diagram below, you can see a rough
    representation of what the algorithm below is going to be doing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预处理管道中的下一步可能是最重要且被低估的NLP工作流程活动。在下面的图示中，你可以看到下面的算法将要执行的粗略表示。
- en: '![](../Images/e4c7ea409c9edc5430c927a7736721fb.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4c7ea409c9edc5430c927a7736721fb.png)'
- en: Representation of the noise removal process | Image by Author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声去除过程的表示 | 作者提供的图片
- en: So, why is removing noise important? It’s because this text is disguised inside
    the input but does not contain any useful information that would make the learning
    algorithm better. Documents like legal agreements, news articles, government contracts,
    etc. contain a lot of boilerplate text specific to the organization. Imagine creating
    a topic modeling project from a legal contract to understand the most important
    terms in a series of contracts, and the algorithm picks the jurisdiction explanation,
    and definitions of state laws as the most important parts of the contracts. Legal
    contracts contain numerous definitions of laws and arbitrations, but these are
    publicly available and therefore not specific to the contract at hand, making
    these predictions essentially useless. We need to extract information specific
    to that contract.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么去除噪声如此重要？因为这些文本隐藏在输入数据中，但不包含任何能够使学习算法更好的有用信息。像法律协议、新闻文章、政府合同等文件中包含了大量特定于组织的模板文本。假设你从法律合同中创建一个主题建模项目，以了解一系列合同中最重要的术语，而算法将管辖权解释和州法律定义作为合同中最重要的部分。法律合同包含了大量的法律和仲裁定义，但这些都是公开的，因此不特定于手头的合同，这使得这些预测实际上毫无用处。我们需要提取特定于该合同的信息。
- en: Removing boilerplate language from text data is challenging, but extremely important.
    Since this data is all clean text, it is hard to detect and remove. But, if not
    removed, it can significantly affect the model’s learning process.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从文本数据中去除模板语言是一项具有挑战性的工作，但却极为重要。由于这些数据都是干净的文本，因此很难检测和去除。但如果不去除，它会显著影响模型的学习过程。
- en: Let us now see the implementation of a function that will remove noise and boilerplate
    language from the input. This algorithm uses clustering to find repeatedly occurring
    sentences and words and removes them, with an assumption that something that is
    repeated more than a threshold number of times, is probably “noise”.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看一个去除输入噪声和模板语言的函数的实现。这个算法使用聚类来找出重复出现的句子和词汇，并将其删除，假设重复超过阈值次数的内容可能是“噪声”。
- en: Below, let us look at the results that this function would produce on a news
    article [3] that is given as input to the algorithm.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们来看看这个函数在作为算法输入的新闻文章 [3] 上会产生什么结果。
- en: '![](../Images/7eb62579100a9b7d34a58ee86894053b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7eb62579100a9b7d34a58ee86894053b.png)'
- en: Output from the boilerplate removal code | Image by Author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 去除模板代码的输出 | 图片由作者提供
- en: As you notice from the output image above, the text that was fed into the algorithm
    had alength of 7574 which was reduced to 892 by removing noise and boilerplate
    text. ***Boilerplate and noise removal resulted in reducing our input size by
    nearly 88%,*** which was essentially garbage that would have made its way into
    the ML algorithm. The resultant text is a cleaner, more meaningful, summarized
    form of the input text. **By removing noise, we are pointing our algorithm to
    concentrate on the important stuff only.**
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的输出图片中你可以看到，输入算法的文本长度为7574，通过去除噪声和模板文本减少到892。***去除模板和噪声将我们的输入大小减少了近88%，***
    这些原本会进入机器学习算法的垃圾数据。结果文本是更清晰、更有意义的摘要形式。**通过去除噪声，我们让算法专注于重要的内容。**
- en: 3\. POS tagging
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 词性标注
- en: POS, or parts-of-speech tagging is a process for assigning specific POS tags
    to every word of an input sentence. It reads and understands the words’ relationship
    with other words in the sentence and recognizes how the context of use for each
    word. These are grammatical categories like nouns, verbs, adjectives, pronouns,
    prepositions, adverbs, conjunctions, and interjections. This process is crucial
    because, for algorithms like sentiment analysis, text classification, information
    extraction, machine translation, or any other form of analysis, it is important
    to understand the context in which words are being used. The context can largely
    affect the natural language understanding (NLU) processes of algorithms.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注（POS），即为输入句子的每个词分配特定的词性标签的过程。它读取并理解词与句子中其他词的关系，并识别每个词的使用上下文。这些词性标签包括名词、动词、形容词、代词、介词、副词、连词和感叹词。这个过程至关重要，因为对于情感分析、文本分类、信息提取、机器翻译或任何其他形式的分析算法来说，了解词汇的使用上下文是非常重要的。上下文可以大大影响算法的自然语言理解（NLU）过程。
- en: 'Next, we will go through the final step of the preprocessing pipeline, which
    is converting the text to vector embeddings that will be used by the Machine Learning
    algorithm, later. But, before that let’s discuss two key topics: Lemmatization
    and Stemming.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进入预处理管道的最后一步，即将文本转换为向量嵌入，供机器学习算法使用。但是，在这之前，让我们讨论两个关键话题：词形还原和词干提取。
- en: Do you need Lemmatization (or) Stemming?
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你需要词形还原（Lemmatization）还是词干提取（Stemming）？
- en: Lemmatization and stemming are two commonly used techniques in NLP workflows
    that help in reducing inflected words to their base or root form. These are probably
    the most questioned actions as well, which is why it is worth understanding when
    to and when not to use either of these functions. The idea behind both lemmatization
    and stemming is the reduction of the dimensionality of the input feature space.
    This helps in improving the performance of ML models that will eventually read
    this data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原和词干提取是自然语言处理工作流中常用的两种技术，帮助将屈折变化的词汇还原到其基础或根本形式。这些操作可能是最受质疑的，因此了解何时使用和何时不使用这些功能是值得的。词形还原和词干提取的核心思想是减少输入特征空间的维度。这有助于提高最终读取这些数据的机器学习模型的性能。
- en: Stemming removes suffixes from words to bring them to their base form, while
    lemmatization uses a vocabulary and a form of morphological analysis to bring
    the words to their base form.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 词干提取通过去除单词的后缀来将其还原为基本形式，而词形还原则使用词汇表和形态分析的形式将单词还原为基本形式。
- en: Due to their functioning, lemmatization is generally more accurate than stemming
    but is computationally expensive. The trade-off between speed and accuracy for
    your specific use case should generally help answer which of the two methods to
    use.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其功能，词形还原通常比词干提取更准确，但计算开销较大。你特定用例中速度和准确性的权衡通常有助于决定使用哪种方法。
- en: 'Some important points to note about implementing lemmatization and stemming:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 实施词形还原和词干提取时需要注意的一些重要点：
- en: '**Lemmatization** preserves the semantics of the input text. Algorithms that
    are meant to work on **sentiment analysis**, might work well if the tense of words
    is needed for the model. Something that has happened in the past might have a
    different sentiment than the same thing happening in the present.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**词形还原**保留了输入文本的语义。针对**情感分析**的算法，如果模型需要单词的时态，可能会工作得很好。过去发生的事情可能与现在发生的相同事情具有不同的情感。'
- en: '**Stemming** is fast, but less accurate. In instances where you are trying
    to achieve **text classification**, where there are thousands of words that need
    to be put into categories, stemming might work better than lemmatization purely
    because of the speed.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**词干提取**速度快，但准确性较低。在尝试进行**文本分类**的情况下，其中有成千上万的单词需要分类时，词干提取可能比词形还原更有效，主要是因为其速度。'
- en: Like all approaches, it might be worth it to explore both in your use case and
    compare the performance of your model to see which works best.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像所有方法一样，可能值得在你的用例中探索两者，并比较模型的性能，以查看哪种效果最好。
- en: Additionally, some deep-learning models have the ability to automatically learn
    word representations which makes using either of these techniques, moot.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，一些深度学习模型具备自动学习词汇表示的能力，这使得使用这些技术变得无关紧要。
- en: 4\. Lemmatization and vector embedding
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 词形还原和向量嵌入
- en: The final step of this preprocessing workflow is the application of lemmatization
    and conversion of words to vector embeddings (because remember how machines work
    best with numbers and not words?). As I previously mentioned, lemmatization may
    or may not be needed for your use case based on the results you expect and the
    machine learning technique you will be using. For a more generalized approach,
    I have included it in my preprocessing pipeline.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 该预处理工作流程的最终步骤是应用词形还原和将单词转换为向量嵌入（因为要记住机器如何最好地处理数字而非单词吗？）。如前所述，词形还原可能根据你预期的结果和使用的机器学习技术，可能需要或不需要。为了更通用的方法，我在我的预处理管道中包含了它。
- en: The function written below will extract words from the POS-tagged input that
    is received, lemmatize every word and then apply vector embeddings to the lemmatized
    words. The comments further explain the individual steps involved.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下面编写的函数将从接收到的词性标注输入中提取单词，对每个单词进行词形还原，然后将向量嵌入应用于词形还原后的单词。注释进一步解释了每个步骤。
- en: This function will return a numpy array of shape (num_words, X) where *‘num_words’*
    represents the number of words in the input text and *‘X’* is the size of the
    vector embeddings.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将返回一个形状为 (num_words, X) 的 numpy 数组，其中 *‘num_words’* 表示输入文本中的单词数量，*‘X’* 是向量嵌入的大小。
- en: The vector-embedded words (numerical forms of words) should be the input that
    is fed into any machine learning algorithm. There could be instances of using
    deep learning models or several Large Language Models (LLMs) where vector embedding
    and lemmatization are not required because the algorithm is mature enough to build
    its own representation of the words. Therefore, this can be an optional step if
    you are working with any of these “self-learning” algorithms.
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 向量嵌入的单词（单词的数值形式）应作为输入提供给任何机器学习算法。在使用深度学习模型或多个大型语言模型（LLMs）时，向量嵌入和词形还原可能不需要，因为算法已经足够成熟，可以构建自己对单词的表示。因此，如果你使用这些“自学习”算法中的任何一种，这可以是一个可选步骤。
- en: Complete pipeline implementation
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整管道实现
- en: The four sections above are detailed individually on every part of our preprocessing
    pipeline, and attached below is the working notebook for running the preprocessing
    code.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 上述四个部分详细说明了我们预处理管道中的每个部分，下面附有用于运行预处理代码的工作笔记本。
- en: '[](https://github.com/rjrahul24/ai-with-python-series/blob/main/08.%20Natural%20Language%20Processing/Preprocessing%20Pipeline/NLP%20Preprocessing%20Pipeline%20.ipynb?source=post_page-----80afd92650fe--------------------------------)
    [## ai-with-python-series/NLP Preprocessing Pipeline'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/rjrahul24/ai-with-python-series/blob/main/08.%20Natural%20Language%20Processing/Preprocessing%20Pipeline/NLP%20Preprocessing%20Pipeline%20.ipynb?source=post_page-----80afd92650fe--------------------------------)
    [## ai-with-python-series/NLP 预处理管道'
- en: A Python Series of tutorials aimed at learning Artificial Intelligence concepts.
    This series of tutorials start from…
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一系列旨在学习人工智能概念的Python教程。该系列教程从…
- en: github.com](https://github.com/rjrahul24/ai-with-python-series/blob/main/08.%20Natural%20Language%20Processing/Preprocessing%20Pipeline/NLP%20Preprocessing%20Pipeline%20.ipynb?source=post_page-----80afd92650fe--------------------------------)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/rjrahul24/ai-with-python-series/blob/main/08.%20Natural%20Language%20Processing/Preprocessing%20Pipeline/NLP%20Preprocessing%20Pipeline%20.ipynb?source=post_page-----80afd92650fe--------------------------------)'
- en: I would like to bring to your notice the caveat that this implementation is
    not a one-shot solution to every NLP problem. The idea behind building a robust
    preprocessing pipeline is to create a workflow that is capable of feeding the
    best possible input into your machine-learning algorithm. The sequencing of the
    steps mentioned above should solve about 70% of your problem, and with fine-tuning
    specific to your use case, you should be able to establish the remainder.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我想提醒您，这种实现并不是对每个NLP问题的一次性解决方案。构建一个稳健的预处理管道的核心思想是创建一个能够将最佳输入提供给机器学习算法的工作流程。上述步骤的顺序应该能解决约70%的问题，通过针对特定用例的微调，您应该能够解决剩余部分。
- en: Conclusion
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: '![](../Images/2803fef3857a6e6d62a39c4fed8d6613.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2803fef3857a6e6d62a39c4fed8d6613.png)'
- en: Photo by [Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral)
    拍摄，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: I hope this story gave you a robust framework for your next Natural Language
    Processing (NLP) project. While, working with words is one of the toughest challenges
    in the AI space, proper cleaning, preprocessing, and preparation of data can ensure
    the machine’s learning process is smooth. As important as it is to properly implement
    the techniques mentioned in this story, **it is equally important to follow the
    sequence of preprocessing activities** highlighted here.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这个故事为您下一个自然语言处理（NLP）项目提供了一个稳健的框架。虽然处理词语是AI领域中最具挑战性的任务之一，但数据的适当清理、预处理和准备可以确保机器的学习过程顺畅。正如正确实施本文提到的技术一样，**遵循这里突出显示的预处理活动的顺序同样重要**。
- en: Every step is dependent on the success of the previous step.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每一步都依赖于前一步的成功。
- en: The boilerplate removal logic will not be able to accurately identify noise
    if the data that is fed into this algorithm contains a lot of stopwords or HTML
    tags. Similarly, POS tagging, lemmatizing, and vectorizing the entire text will
    make the compute extremely costly and seldom work like garbage-in-garbage-out.
    So, experiment with this pipeline and enjoy better results in your NLP setups.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入到该算法中的数据包含大量停用词或HTML标签，模板删除逻辑将无法准确识别噪音。类似地，对整个文本进行词性标注、词形还原和向量化会使计算成本极高，并且很少像垃圾进垃圾出那样有效。因此，请尝试这个管道，并在您的NLP设置中享受更好的结果。
- en: About Me
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于我
- en: I have been building products and solutions using AI for several years now.
    Prior to my current role at Bloomberg, I worked in the data and machine learning
    space at Microsoft, Tesla, and Johnson & Johnson. I hold a data science degree
    from Columbia University, where I was also involved in researching the responsible
    and ethical use of Artificial Intelligence. In addition to my work, I am also
    a published author of two books and online courses on Machine Learning and Data
    Science. I am constantly exploring ways to make a positive impact in the world
    by leveraging AI to solve complex problems while upholding ethical and responsible
    practices.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经使用AI构建产品和解决方案好几年了。在我目前在Bloomberg的职位之前，我曾在Microsoft、Tesla和Johnson & Johnson从事数据和机器学习工作。我拥有哥伦比亚大学的数据科学学位，同时也参与了人工智能负责任和伦理使用的研究。除了我的工作，我还是两本关于机器学习和数据科学的书籍及在线课程的作者。我不断探索通过利用AI解决复杂问题的方式，以在世界上产生积极影响，同时秉持伦理和负责任的实践。
- en: Feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/rjrahul24/)
    or [Twitter](https://twitter.com/rjrahul24).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 随时通过[LinkedIn](https://www.linkedin.com/in/rjrahul24/)或[Twitter](https://twitter.com/rjrahul24)与我联系。
- en: Here’s some more Data Science content you might like!
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这里有更多你可能感兴趣的数据科学内容！
- en: '[](/a-detailed-novice-introduction-to-natural-language-processing-nlp-90b7be1b7e54?source=post_page-----80afd92650fe--------------------------------)
    [## A Detailed, Novice Introduction to Natural Language Processing (NLP)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/a-detailed-novice-introduction-to-natural-language-processing-nlp-90b7be1b7e54?source=post_page-----80afd92650fe--------------------------------)
    [## 自然语言处理（NLP）的详细新手入门'
- en: The Ultimate Code-Based Guide to Getting Started with NLP in Python
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 《Python中开始使用NLP的终极代码指南》
- en: towardsdatascience.com](/a-detailed-novice-introduction-to-natural-language-processing-nlp-90b7be1b7e54?source=post_page-----80afd92650fe--------------------------------)
    [](/a-step-by-step-guide-to-speech-recognition-and-audio-signal-processing-in-python-136e37236c24?source=post_page-----80afd92650fe--------------------------------)
    [## A Step-by-Step Guide to Speech Recognition and Audio Signal Processing in
    Python
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/a-detailed-novice-introduction-to-natural-language-processing-nlp-90b7be1b7e54?source=post_page-----80afd92650fe--------------------------------)
    [](/a-step-by-step-guide-to-speech-recognition-and-audio-signal-processing-in-python-136e37236c24?source=post_page-----80afd92650fe--------------------------------)
    [## Python中语音识别和音频信号处理的逐步指南
- en: The Science of Teaching Human Vocabulary to Machines
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 教授机器人类词汇的科学
- en: towardsdatascience.com](/a-step-by-step-guide-to-speech-recognition-and-audio-signal-processing-in-python-136e37236c24?source=post_page-----80afd92650fe--------------------------------)
    [](/the-ultimate-guide-to-emotion-recognition-from-facial-expressions-using-python-64e58d4324ff?source=post_page-----80afd92650fe--------------------------------)
    [## The Ultimate Guide to Emotion Recognition from Facial Expressions using Python
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/a-step-by-step-guide-to-speech-recognition-and-audio-signal-processing-in-python-136e37236c24?source=post_page-----80afd92650fe--------------------------------)
    [](/the-ultimate-guide-to-emotion-recognition-from-facial-expressions-using-python-64e58d4324ff?source=post_page-----80afd92650fe--------------------------------)
    [## 使用Python从面部表情识别情感的终极指南
- en: Build a Face Emotion Recognition (FER) Algorithm that works on both, Images
    and Videos
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建一个可以处理图像和视频的面部情感识别（FER）算法
- en: towardsdatascience.com](/the-ultimate-guide-to-emotion-recognition-from-facial-expressions-using-python-64e58d4324ff?source=post_page-----80afd92650fe--------------------------------)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-ultimate-guide-to-emotion-recognition-from-facial-expressions-using-python-64e58d4324ff?source=post_page-----80afd92650fe--------------------------------)
- en: Resources
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[1] [https://dataaspirant.com/nlp-text-preprocessing-techniques-implementation-python/#t-1600081660732](https://dataaspirant.com/nlp-text-preprocessing-techniques-implementation-python/#t-1600081660732)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://dataaspirant.com/nlp-text-preprocessing-techniques-implementation-python/#t-1600081660732](https://dataaspirant.com/nlp-text-preprocessing-techniques-implementation-python/#t-1600081660732)'
- en: '[2] [https://www.exxactcorp.com/blog/Deep-Learning/text-preprocessing-methods-for-deep-learning](https://www.exxactcorp.com/blog/Deep-Learning/text-preprocessing-methods-for-deep-learning)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://www.exxactcorp.com/blog/Deep-Learning/text-preprocessing-methods-for-deep-learning](https://www.exxactcorp.com/blog/Deep-Learning/text-preprocessing-methods-for-deep-learning)'
- en: '[3] [https://www.nbcnews.com/news/us-news/oklahoma-sex-offender-suspected-killing-6-was-free-sex-crimes-charges-rcna82884](https://www.nbcnews.com/news/us-news/oklahoma-sex-offender-suspected-killing-6-was-free-sex-crimes-charges-rcna82884)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://www.nbcnews.com/news/us-news/oklahoma-sex-offender-suspected-killing-6-was-free-sex-crimes-charges-rcna82884](https://www.nbcnews.com/news/us-news/oklahoma-sex-offender-suspected-killing-6-was-free-sex-crimes-charges-rcna82884)'
- en: '[4] [https://www.nltk.org/book/ch05.html](https://www.nltk.org/book/ch05.html)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://www.nltk.org/book/ch05.html](https://www.nltk.org/book/ch05.html)'
- en: '[5] [https://www.turing.com/kb/guide-on-word-embeddings-in-nlp](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://www.turing.com/kb/guide-on-word-embeddings-in-nlp](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp)'
