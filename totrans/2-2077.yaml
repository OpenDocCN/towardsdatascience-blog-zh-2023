- en: The Ultimate Preprocessing Pipeline for Your NLP Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-ultimate-preprocessing-pipeline-for-your-nlp-models-80afd92650fe](https://towardsdatascience.com/the-ultimate-preprocessing-pipeline-for-your-nlp-models-80afd92650fe)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Get the most out of training NLP ML models by feeding the best possible input
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rahulraj24.medium.com/?source=post_page-----80afd92650fe--------------------------------)[![Rahulraj
    Singh](../Images/8bfa5fdcb41c9c81c026b88744145b11.png)](https://rahulraj24.medium.com/?source=post_page-----80afd92650fe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----80afd92650fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----80afd92650fe--------------------------------)
    [Rahulraj Singh](https://rahulraj24.medium.com/?source=post_page-----80afd92650fe--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----80afd92650fe--------------------------------)
    ·10 min read·May 8, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e689a96f5fd638b621c932f2bb07d426.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Cyrus Crossan](https://unsplash.com/@cys_escapes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: If you have worked on a text summarization project before, you would have noticed
    the difficulty in seeing the results you expect to see. You have a notion in mind
    for how the algorithm should work and what sentences it should mark in the text
    summaries, but more often than not the algorithm sends out results that are “not-so-accurate”.
    Even more interesting is keyword extraction because all sorts of algorithms from
    topic modeling to vectorizing embeddings, are all really good but given a paragraph
    as an input the results they give out are again “not-so-accurate” because the
    most often occurring word is not always the most important word of the paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing and data cleaning requirements vary largely based on the use case
    you are trying to solve. I will attempt to create a generalized pipeline that
    should work well for all NLP models, but you will always need to tune the steps
    to achieve the best results for your use-case. In this story, I will focus on
    NLP models that solve for **topic modelling, keyword extraction, and text summarization**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/29dcaa4577d871de3734a6e3d9b5b855.png)'
  prefs: []
  type: TYPE_IMG
- en: Preprocessing Pipeline | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The image above outlines the process we will be following to build the preprocessing
    NLP pipeline. The four steps mentioned above, are explained with code later and
    there is also a Jupyter notebook attached, that implements the complete pipeline
    together. The idea behind this pipeline is to highlight steps that will enhance
    the performance of machine learning algorithms that are going to be used on text
    data. This is a step between input data and model training.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Cleaning the text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step to structuring the pipeline is cleaning the input text data,
    which can consist of several steps based on the model you are trying to build
    and the results you desire. Machine learning algorithms (or largely all computer
    algorithms, rather every computer instruction) work on numbers, which is why building
    a model for text data is challenging. You are essentially asking the computer
    to learn and work on something it has never seen before and hence, it needs a
    bit more work.
  prefs: []
  type: TYPE_NORMAL
- en: In the section below, I give the first function of our pipeline to perform cleaning
    on the text data. There are numerous operations parts of the cleaning function,
    and I have explained them all in the comments of the code.
  prefs: []
  type: TYPE_NORMAL
- en: To see the performance of this function, below is an input to the function and
    the output that it generates.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As we observe in the output, the text is now clean of all HTML tags, it has
    converted emojis to their word forms and corrected the text for any punctuations
    and special characters. This text is now easier to deal with and in the next few
    steps, we will refine it even further.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Clustering to remove noise and boilerplate language from the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step in our preprocessing pipeline is probably the most important and
    underrated activity for an NLP workflow. In the diagram below, you can see a rough
    representation of what the algorithm below is going to be doing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4c7ea409c9edc5430c927a7736721fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Representation of the noise removal process | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: So, why is removing noise important? It’s because this text is disguised inside
    the input but does not contain any useful information that would make the learning
    algorithm better. Documents like legal agreements, news articles, government contracts,
    etc. contain a lot of boilerplate text specific to the organization. Imagine creating
    a topic modeling project from a legal contract to understand the most important
    terms in a series of contracts, and the algorithm picks the jurisdiction explanation,
    and definitions of state laws as the most important parts of the contracts. Legal
    contracts contain numerous definitions of laws and arbitrations, but these are
    publicly available and therefore not specific to the contract at hand, making
    these predictions essentially useless. We need to extract information specific
    to that contract.
  prefs: []
  type: TYPE_NORMAL
- en: Removing boilerplate language from text data is challenging, but extremely important.
    Since this data is all clean text, it is hard to detect and remove. But, if not
    removed, it can significantly affect the model’s learning process.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let us now see the implementation of a function that will remove noise and boilerplate
    language from the input. This algorithm uses clustering to find repeatedly occurring
    sentences and words and removes them, with an assumption that something that is
    repeated more than a threshold number of times, is probably “noise”.
  prefs: []
  type: TYPE_NORMAL
- en: Below, let us look at the results that this function would produce on a news
    article [3] that is given as input to the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7eb62579100a9b7d34a58ee86894053b.png)'
  prefs: []
  type: TYPE_IMG
- en: Output from the boilerplate removal code | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: As you notice from the output image above, the text that was fed into the algorithm
    had alength of 7574 which was reduced to 892 by removing noise and boilerplate
    text. ***Boilerplate and noise removal resulted in reducing our input size by
    nearly 88%,*** which was essentially garbage that would have made its way into
    the ML algorithm. The resultant text is a cleaner, more meaningful, summarized
    form of the input text. **By removing noise, we are pointing our algorithm to
    concentrate on the important stuff only.**
  prefs: []
  type: TYPE_NORMAL
- en: 3\. POS tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: POS, or parts-of-speech tagging is a process for assigning specific POS tags
    to every word of an input sentence. It reads and understands the words’ relationship
    with other words in the sentence and recognizes how the context of use for each
    word. These are grammatical categories like nouns, verbs, adjectives, pronouns,
    prepositions, adverbs, conjunctions, and interjections. This process is crucial
    because, for algorithms like sentiment analysis, text classification, information
    extraction, machine translation, or any other form of analysis, it is important
    to understand the context in which words are being used. The context can largely
    affect the natural language understanding (NLU) processes of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will go through the final step of the preprocessing pipeline, which
    is converting the text to vector embeddings that will be used by the Machine Learning
    algorithm, later. But, before that let’s discuss two key topics: Lemmatization
    and Stemming.'
  prefs: []
  type: TYPE_NORMAL
- en: Do you need Lemmatization (or) Stemming?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lemmatization and stemming are two commonly used techniques in NLP workflows
    that help in reducing inflected words to their base or root form. These are probably
    the most questioned actions as well, which is why it is worth understanding when
    to and when not to use either of these functions. The idea behind both lemmatization
    and stemming is the reduction of the dimensionality of the input feature space.
    This helps in improving the performance of ML models that will eventually read
    this data.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming removes suffixes from words to bring them to their base form, while
    lemmatization uses a vocabulary and a form of morphological analysis to bring
    the words to their base form.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Due to their functioning, lemmatization is generally more accurate than stemming
    but is computationally expensive. The trade-off between speed and accuracy for
    your specific use case should generally help answer which of the two methods to
    use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some important points to note about implementing lemmatization and stemming:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lemmatization** preserves the semantics of the input text. Algorithms that
    are meant to work on **sentiment analysis**, might work well if the tense of words
    is needed for the model. Something that has happened in the past might have a
    different sentiment than the same thing happening in the present.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Stemming** is fast, but less accurate. In instances where you are trying
    to achieve **text classification**, where there are thousands of words that need
    to be put into categories, stemming might work better than lemmatization purely
    because of the speed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Like all approaches, it might be worth it to explore both in your use case and
    compare the performance of your model to see which works best.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally, some deep-learning models have the ability to automatically learn
    word representations which makes using either of these techniques, moot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4\. Lemmatization and vector embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final step of this preprocessing workflow is the application of lemmatization
    and conversion of words to vector embeddings (because remember how machines work
    best with numbers and not words?). As I previously mentioned, lemmatization may
    or may not be needed for your use case based on the results you expect and the
    machine learning technique you will be using. For a more generalized approach,
    I have included it in my preprocessing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The function written below will extract words from the POS-tagged input that
    is received, lemmatize every word and then apply vector embeddings to the lemmatized
    words. The comments further explain the individual steps involved.
  prefs: []
  type: TYPE_NORMAL
- en: This function will return a numpy array of shape (num_words, X) where *‘num_words’*
    represents the number of words in the input text and *‘X’* is the size of the
    vector embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: The vector-embedded words (numerical forms of words) should be the input that
    is fed into any machine learning algorithm. There could be instances of using
    deep learning models or several Large Language Models (LLMs) where vector embedding
    and lemmatization are not required because the algorithm is mature enough to build
    its own representation of the words. Therefore, this can be an optional step if
    you are working with any of these “self-learning” algorithms.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Complete pipeline implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The four sections above are detailed individually on every part of our preprocessing
    pipeline, and attached below is the working notebook for running the preprocessing
    code.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/rjrahul24/ai-with-python-series/blob/main/08.%20Natural%20Language%20Processing/Preprocessing%20Pipeline/NLP%20Preprocessing%20Pipeline%20.ipynb?source=post_page-----80afd92650fe--------------------------------)
    [## ai-with-python-series/NLP Preprocessing Pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: A Python Series of tutorials aimed at learning Artificial Intelligence concepts.
    This series of tutorials start from…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/rjrahul24/ai-with-python-series/blob/main/08.%20Natural%20Language%20Processing/Preprocessing%20Pipeline/NLP%20Preprocessing%20Pipeline%20.ipynb?source=post_page-----80afd92650fe--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: I would like to bring to your notice the caveat that this implementation is
    not a one-shot solution to every NLP problem. The idea behind building a robust
    preprocessing pipeline is to create a workflow that is capable of feeding the
    best possible input into your machine-learning algorithm. The sequencing of the
    steps mentioned above should solve about 70% of your problem, and with fine-tuning
    specific to your use case, you should be able to establish the remainder.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/2803fef3857a6e6d62a39c4fed8d6613.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: I hope this story gave you a robust framework for your next Natural Language
    Processing (NLP) project. While, working with words is one of the toughest challenges
    in the AI space, proper cleaning, preprocessing, and preparation of data can ensure
    the machine’s learning process is smooth. As important as it is to properly implement
    the techniques mentioned in this story, **it is equally important to follow the
    sequence of preprocessing activities** highlighted here.
  prefs: []
  type: TYPE_NORMAL
- en: Every step is dependent on the success of the previous step.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The boilerplate removal logic will not be able to accurately identify noise
    if the data that is fed into this algorithm contains a lot of stopwords or HTML
    tags. Similarly, POS tagging, lemmatizing, and vectorizing the entire text will
    make the compute extremely costly and seldom work like garbage-in-garbage-out.
    So, experiment with this pipeline and enjoy better results in your NLP setups.
  prefs: []
  type: TYPE_NORMAL
- en: About Me
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I have been building products and solutions using AI for several years now.
    Prior to my current role at Bloomberg, I worked in the data and machine learning
    space at Microsoft, Tesla, and Johnson & Johnson. I hold a data science degree
    from Columbia University, where I was also involved in researching the responsible
    and ethical use of Artificial Intelligence. In addition to my work, I am also
    a published author of two books and online courses on Machine Learning and Data
    Science. I am constantly exploring ways to make a positive impact in the world
    by leveraging AI to solve complex problems while upholding ethical and responsible
    practices.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/rjrahul24/)
    or [Twitter](https://twitter.com/rjrahul24).
  prefs: []
  type: TYPE_NORMAL
- en: Here’s some more Data Science content you might like!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](/a-detailed-novice-introduction-to-natural-language-processing-nlp-90b7be1b7e54?source=post_page-----80afd92650fe--------------------------------)
    [## A Detailed, Novice Introduction to Natural Language Processing (NLP)'
  prefs: []
  type: TYPE_NORMAL
- en: The Ultimate Code-Based Guide to Getting Started with NLP in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-detailed-novice-introduction-to-natural-language-processing-nlp-90b7be1b7e54?source=post_page-----80afd92650fe--------------------------------)
    [](/a-step-by-step-guide-to-speech-recognition-and-audio-signal-processing-in-python-136e37236c24?source=post_page-----80afd92650fe--------------------------------)
    [## A Step-by-Step Guide to Speech Recognition and Audio Signal Processing in
    Python
  prefs: []
  type: TYPE_NORMAL
- en: The Science of Teaching Human Vocabulary to Machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-step-by-step-guide-to-speech-recognition-and-audio-signal-processing-in-python-136e37236c24?source=post_page-----80afd92650fe--------------------------------)
    [](/the-ultimate-guide-to-emotion-recognition-from-facial-expressions-using-python-64e58d4324ff?source=post_page-----80afd92650fe--------------------------------)
    [## The Ultimate Guide to Emotion Recognition from Facial Expressions using Python
  prefs: []
  type: TYPE_NORMAL
- en: Build a Face Emotion Recognition (FER) Algorithm that works on both, Images
    and Videos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-ultimate-guide-to-emotion-recognition-from-facial-expressions-using-python-64e58d4324ff?source=post_page-----80afd92650fe--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [https://dataaspirant.com/nlp-text-preprocessing-techniques-implementation-python/#t-1600081660732](https://dataaspirant.com/nlp-text-preprocessing-techniques-implementation-python/#t-1600081660732)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://www.exxactcorp.com/blog/Deep-Learning/text-preprocessing-methods-for-deep-learning](https://www.exxactcorp.com/blog/Deep-Learning/text-preprocessing-methods-for-deep-learning)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [https://www.nbcnews.com/news/us-news/oklahoma-sex-offender-suspected-killing-6-was-free-sex-crimes-charges-rcna82884](https://www.nbcnews.com/news/us-news/oklahoma-sex-offender-suspected-killing-6-was-free-sex-crimes-charges-rcna82884)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [https://www.nltk.org/book/ch05.html](https://www.nltk.org/book/ch05.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [https://www.turing.com/kb/guide-on-word-embeddings-in-nlp](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp)'
  prefs: []
  type: TYPE_NORMAL
