- en: DINO ‚Äî A Foundation Model for Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/dino-a-foundation-model-for-computer-vision-4cb08e821b18](https://towardsdatascience.com/dino-a-foundation-model-for-computer-vision-4cb08e821b18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[üöÄSascha‚Äôs Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Emerging Properties in Self-Supervised Vision Transformers by M. Caron et. al.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)
    ¬∑13 min read¬∑Sep 27, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: It is an exciting decade for computer vision. Great successes from the natural
    language domain are transferred to the vision domain including the introduction
    of the ViT (vision transformer) and lately large-scale self-supervised pre-training
    techniques have made headlines under the name of foundation models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today we are looking into a framework called DINO (self **DI**stillation, **N**O
    labels), a visual foundation model built on interesting properties of ViTs. It
    is also the predecessor of one of today‚Äôs best performing foundation models: [DINOv2](https://arxiv.org/abs/2304.07193).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/826ce8cc3feae5497593efe2c4e9631c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created from [publication](https://arxiv.org/abs/2104.14294) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: '**Paper:** [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294),
    by [Mathilde Caron](https://arxiv.org/search/cs?searchtype=author&query=Caron%2C+M)
    et.al., 29\. Apr. 2021'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/facebookresearch/dino) ‚Äî [Blog Post](https://ai.meta.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** foundation model, computer vision, vision transformer, knowledge
    distillation, similarity learning, self-supervised learning'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    ‚Äî [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    ‚Äî [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    ‚Äî [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    ‚Äî [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    ‚Äî [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    ‚Äî [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Outline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Context & Background
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ablations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Readings & Resources
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Context & Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The year is 2021, April to be precise. It has been four years since the release
    of the transformer model with [attention is all you need](https://arxiv.org/abs/1706.03762).
    Self-supervised pre-training is long being practiced in NLP by models such as
    [BERT](https://arxiv.org/pdf/1810.04805.pdf) and the term foundation model is
    not yet known for the next few months until the release of *‚Äú*[*on the opportunities
    and Risks of Foundation Models*](https://arxiv.org/abs/2108.07258)*‚Äù*. Six months
    earlier the [Vision transformer (ViT)](https://arxiv.org/abs/2010.11929) was first
    published on arxiv and it is still one month until ICLR 2021 where it will be
    presented.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let that sink in for a moment: ViT had its debut on arxiv.org in October 2020
    and was presented on ICLR2021 in May 2021\. DINO was released on arxiv in April
    2021\. So, one month before it was actually presented on a conference. This would
    mean they only had 5 months if they had started right away to come up with the
    project‚Äôs idea, compile a team, lay out the theoretical foundation, train the
    model, perform experiments and ablations, and write the paper. No wonder PhD students
    these days feel constantly anxious. At least that‚Äôs what‚Äôs happening to me sometimes
    *üòÖ*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While ViTs were very competitive with convolutional networks, they are demanding
    in terms of computational resources and amount of training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of DINO made a simple observation: the success of transformers
    in NLP was coupled with self-supervised pre-training and current self-supervised
    methods in the vision domain are built from convnets, like e.g. [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c).'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
    [## BYOL -The Alternative to Contrastive Self-Supervised Learning'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Analysis‚Äî Bootstrap Your Own Latent: A New Approach to Self-Supervised
    Learning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    and the [mean teacher](https://arxiv.org/abs/1703.01780), the authors came up
    with a framework to train a ViT in a self-supervised fashion and found:'
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised ViT features explicitly contain the scene layout and, in particular,
    object boundaries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Self-supervised ViT features perform particularly well with a basic nearest
    neighbors classifier (k-NN) without any fine-tuning, linear classifier nor data
    augmentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In contrast to BYOL and mean teacher, DINO implements a knowledge-distillation
    framework consisting of a student and teacher model that acts upon different views
    of the same image and adds extra measures to deal with inherent instabilities
    of similarity-learning approaches, where solutions often collapse.
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting finding of the underlying vision transformer architecture (ViT)
    is that when trained with unsupervised learning techniques its features contain
    explicit information about the semantic segmentation of an image. One can simply
    visualize the self-attention map of selected heads of the multi-head attention
    layer as shown in the video bellow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98c910b938eac30dc3417917e4e55d08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: Self-attention maps for selected heads. [Source](https://arxiv.org/abs/2104.14294)'
  prefs: []
  type: TYPE_NORMAL
- en: Let us unpack another layer of abstraction and let‚Äôs try to understand how DINO
    implements its framework, tackles instabilities and how it performs compared to
    previous methods!
  prefs: []
  type: TYPE_NORMAL
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Paper Walkthroughs by Sascha Kirch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----4cb08e821b18--------------------------------)7
    stories![‚ÄúDDPM‚Ää‚Äî‚ÄäDenoising Diffusion Probabilistic Models ‚Äú paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![‚ÄúDepth Anything‚Äù
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The DINO framework shares the same overall structure with other similarity-learning
    frameworks like [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    or the [mean teacher](https://arxiv.org/abs/1703.01780) but also with knowledge
    distillation. Let‚Äôs first have a look on how DINO does it and the differentiate
    between the other frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2dacf33beadadf8b5604e4842686f18f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2: DINO architecture. [Source](https://arxiv.org/abs/2104.14294) + annotations
    by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Networks and Update Rule**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs start from the middle. DINO implements two networks with the exact same
    architecture but a different set of weights. Those are the student and the teacher.
    The student is trained with back propagation and the teacher updates its weights
    with an exponential moving average of its own weights and those of the student.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2fe06104b0d6ee43b9013c3b398e4e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 1: Update rule of the teacher‚Äôs weights. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Backbones are either a ResNet50 or [DeiT](https://arxiv.org/abs/2012.12877)
    (which is a [ViT](https://arxiv.org/abs/2010.11929) adapted for knowledge distillation).
    An MLP-based projection head is connected to the backbone to reduce the dimensionality
    of the features, but is removed for inference.
  prefs: []
  type: TYPE_NORMAL
- en: '***Nice, but which model is used for inference: student or teacher?*** ‚Äî Well
    that‚Äôs a good question and funny enough not a single word is mentioned in the
    paper. Intuitively you might think the student, at least I did at first. But as
    we will see later, the teacher outperforms the student throughout the training.
    The only hint beside the better performance is that in the code implementation
    the teacher checkpoint is the default one for the evaluation of for example [video
    segmentation](https://github.com/facebookresearch/dino/blob/7c446df5b9f45747937fb0d72314eb9f7b66930a/eval_video_segmentation.py#L257C9-L257C9),
    [linear probing](https://github.com/facebookresearch/dino/blob/7c446df5b9f45747937fb0d72314eb9f7b66930a/eval_linear.py#L264C34-L264C34)
    and [k-NN](https://github.com/facebookresearch/dino/blob/7c446df5b9f45747937fb0d72314eb9f7b66930a/eval_knn.py#L203C32-L203C32).
    Since this parameter can be changed though, I cannot tell you with certainty.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inputs and Outputs**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From an input image *x* different views *x1* and *x2* are created by cropping
    and applying image augmentations like in [BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)
    (e.g. color jitter, Gaussian blur and solarization). The technique used for cropping
    is called [multi-crop](https://arxiv.org/abs/2006.09882) where multiple crops
    of different sizes are generated to save memory while providing more data. Small
    crops are called local views and consist of 96x96 pixels that are exclusively
    feed into the student. Larger crops are called global views and consists of 224x224
    pixels that are exclusively fed into the teacher. As we will see later in the
    ablation section, 2 global views and 10 local views have been used during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: The paper is a bit confusing regarding the multi-crop technique because
    neither the provided pseudo-code nor the architecture shown in Fig. 3 above reflect
    it. The pseudo code even suggests that x1 and x2 are feed into both, the student
    and the teacher like in BYOL, which is not the case when using multi-crop.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In contrast to similarity learning where the objective is to maximize the similarity
    of embeddings, DINO minimizes the cross-entropy between the teacher‚Äôs and the
    student‚Äôs output distribution. As indicated by the equation bellow, the cross-entropy
    is calculated for each pair of global and local views and is then summed up.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b999355c87ae211c90653c509cea3da3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 2: Optimization objective. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: '***And what do the models output?*** ‚Äî Like in similarity learning, the student
    and the teacher output an embedding for a given image, rather than a prediction
    score. Like in knowledge distillation, the output is transformed via a SoftMax
    transformation into a probability distribution. The SoftMax has a temperature
    parameter that controls the smoothing or sharpening of the resulting distribution.
    This temperature plays a crucial role in knowledge distillation because it allows
    to control the balance between transferring general knowledge and fine-grained
    details from a teacher network to a student network, making the distillation process
    more effective for different tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d9f933db5fb3f0e5dfba164632ebfcd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3: Effect of temperature value on the SoftMax output. Illustration by
    [Sascha Kirch](https://medium.com/@SaschaKirch) created with [this python notebook](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'I created a notebook for you so you can investigate the impact of the temperature
    on the resulting distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb?source=post_page-----4cb08e821b18--------------------------------)
    [## ML_Notebooks/Softmax_Temperature.ipynb at main ¬∑ sascha-kirch/ML_Notebooks'
  prefs: []
  type: TYPE_NORMAL
- en: Collection of machine learning related notebooks to share. ‚Äî ML_Notebooks/Softmax_Temperature.ipynb
    at main ¬∑‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb?source=post_page-----4cb08e821b18--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Avoiding Collapse**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned earlier, student and teacher have the exact same architecture.
    This kind of setup is unstable (if no counter measures are implemented) and might
    result in collapsing solutions, where all features are mapped to a certain region
    in the latent space, e.g. a single point in the worst case. BYOL addressed this
    issue with an extra prediction head for only one of the models introducing an
    asymmetry. Since DINO has symmetric models another trick is required: centering
    and sharpening. Both are applied to the teacher network only. Centering is a technique
    that prevents a single dimension in the latent space to dominate, by adding a
    bias term *c* to the teachers output *g(x) = g(x)+c*, where'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/268f8c0249d46fa219f3dcad096e9332.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 3: Update rule of the centering term. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: While centering has a positive effect, it also encourages the output to collapse
    into a uniform distribution. Sharpening has the opposite effect hence applying
    both balances their effect and stabilizes training. Sharpening is achieved by
    using a smaller temperature in the SoftMax (see Fig. 3) for the teacher as for
    the student.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid collapsing the hyperparameter *m* from equation 3 and the temperature
    of the teacher are crucial. In their ablation study in the appendix section the
    authors show that *m=0.9‚Ä¶0.999* works best and the temperature value is linearly
    increased from *0.04* to *0.07* during warm-up.
  prefs: []
  type: TYPE_NORMAL
- en: '**What does DINO do? Knowledge Distillation or Similarity Learning?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The answer is a little bit of both!
  prefs: []
  type: TYPE_NORMAL
- en: While knowledge distillation usually distils knowledge from an already trained,
    larger and more accurate teacher model into a smaller student model, it could
    also be seen as some sort of similarity learning because it encourages the student
    network to produce predictions that are similar to those of the teacher. In similarity
    learning, the two models are usually trained jointly and often align their latent
    space predictions rather than probability distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the authors of DINO phrase their objective as knowledge distillation,
    let‚Äôs have a look on some differences compared with ‚Äústandard‚Äù knowledge distillation:'
  prefs: []
  type: TYPE_NORMAL
- en: DINO‚Äôs teacher is not available a priori but ‚Äútrained‚Äù alongside the student.
    It can even be considered as a co-distillation since knowledge is also distilled
    from the student into the teacher.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DINO‚Äôs teacher and student are not acting on the same input but on different
    views of the image cropped to different sizes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DINO uses different temperatures in the SoftMax of both models to perform sharpening.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DINO calculates the cross-entropy over the temperature-scaled SoftMax of the
    embeddings rather than prediction scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'And how is it similar to knowledge distillation?:'
  prefs: []
  type: TYPE_NORMAL
- en: DINO consists of a student and a teacher network, where the teacher performs
    better than the student as we will see in the experiments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rather than maximizing a similarity metric, DINO minimizes the cross-entropy
    loss of a temperature scaled SoftMax output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----4cb08e821b18--------------------------------)
    [## Get an email whenever Sascha Kirch publishes üöÄ'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Sascha Kirch publishes üöÄ Looking to learn more about deep
    learning or simply stay up to date‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----4cb08e821b18--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The paper presents a vast number of experiments. They pre-train the model on
    ImageNet, a commonly used dataset in representation learning.
  prefs: []
  type: TYPE_NORMAL
- en: For the evaluation, common techniques usually either train a linear classifier
    on top of frozen features or fine-tune the model to new downstream tasks, where
    the parameters of the model are adapted.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of DINO claim that those techniques are very sensitive to hyperparameters
    which makes comparisons unfair and hard to reproduce. Hence, they propose to use
    a simple nearest neighbor clustering algorithm on the features of the pre-trained
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Linear and k-NN Classification on ImageNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a this experiment the models are tested on their image classification accuracy
    on ImageNet. A variety of self-supervised pre-trained models are tested with either
    a ResNet or a ViT backbone. The classification is done either with linear probing
    or k-NN clustering.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8411c4b87ac4d9fab26ed43dfe033c82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 1: Linear and k-NN classification on ImageNet. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: 'I guess the key take-aways are:'
  prefs: []
  type: TYPE_NORMAL
- en: K-NN performs better on ViT features than on ResNet features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decreasing patch size in the ViT has larger improvement as larger backbone,
    but at the cost of slower inference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Video Instance Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An important experiment has been the video segmentation task, since the paper
    is about the ViT‚Äôs capability to capture semantic segmentation in its features
    when trained with unsupervised methods. Or let‚Äôs say that‚Äôs what is claimed üòÅ
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9480e3a017e4d94de1b778797fc40226.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 2: Video Instance segmentation. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observing those results I am missing two further experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: It would be nice to see a comparison of a supervised ResNet50 and a self-supervised
    ResNet50 in the DINO framework to support their claim that the ViT is superior
    to the ResNet architecture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It would also be great to see the same set of ViT backbones for supervised as
    for self-supervised to see the impact on patch-size and model size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'But as I always say: asking questions is easy üòÅ In real-world projects the
    authors often face resource constraints and project deadlines so not every single
    little detail can be covered!'
  prefs: []
  type: TYPE_NORMAL
- en: Probing the Self-Attention Map
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this experiment the authors investigated the self-attention maps of different
    heads in the multi-head self-attention layers of the ViT. They visualize the attention
    maps from selected heads from the last layer of ViT-S/8, those of the learned
    [CLS] token to be precise.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/133db7e45053a51d9d4e14bf554844d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 4: Attention maps from selected heads. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Other Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In other experiments, DINO improved compared against the supervised baseline.
    Those tasks include image retrieval and copy detection.
  prefs: []
  type: TYPE_NORMAL
- en: Ablations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For their ablation study the authors experiment with the ViT-S model.
  prefs: []
  type: TYPE_NORMAL
- en: Importance of Patch Size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that a vision transformer inputs a patchified version of an input image,
    transforms each patch into a token and then applies a transformer with its self-attention
    mechanism. This was a trick by the authors of ViT to reduce the compute requirements
    for trading-off performance, making transformers applicable to image data.
  prefs: []
  type: TYPE_NORMAL
- en: DINO claims that smaller size of the patches increases the performance while
    decreasing the throughput (number of images that can be processed per second),
    which is exactly what ViT claims.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2103ddc8e040677295c3880a1ff19051.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 5: Impact of patch size on accuracy and throughput. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively I‚Äôd say it is no surprise since you increase the input resolution
    and you end up with more tokens to attend to, so you end up with a fine-grained
    attention map.
  prefs: []
  type: TYPE_NORMAL
- en: Different Teacher Update Rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The teacher in DINO is updated by calculating the exponential moving average
    from the updated student and the current teacher. This is the ‚Äúmomentum encoder‚Äù
    approach they refer to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a momentum encoder and plotting the accuracy of the teacher and the student
    during training, the teacher performs better throughout the entire process. From
    this we can hypothesize:'
  prefs: []
  type: TYPE_NORMAL
- en: the teacher can provide a strong learning signal to the student.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: an improving student improves the teacher due to the EMA update rule (co-distillation).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One can use the Teacher as final model which has better performance but the
    same architecture as the student, hence no change in compute requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/36bd26e2335351a2980f6f518e29211d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6: Teacher performance. [Source](https://arxiv.org/abs/2104.14294) + annotations
    by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: 'They also experiment with 3 other update rules: copying the weights from the
    student to the teacher, use the student weights from the previous iteration of
    the optimizer and use the student weights from the previous epoch.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Crop vs. Time and GPU Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, DINO inputs multiple cropped views of the same image and
    feeds the global views into the teacher and the local views into the student.
    In this ablation, the authors experiment with different amounts of local views
    and report the impact on performance, training time and peak memory per GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1150cc8dc59aeb8b950bce9b7b52a6be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3: Multi-Crop vs. Time and GPU Memory. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding Collapse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this ablation the authors evaluated the role of their stabilizing measures
    to avoid collapsing solutions: centering and sharpening.'
  prefs: []
  type: TYPE_NORMAL
- en: To do so, they decomposed the cross-entropy into an entropy term and a Kullback-Leibler
    (KL) divergence term. KL divergence is a measure of difference of two probability
    distributions. If KL is 0, two distributions are considered equal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The intuition behind this is the following: if the KL divergence of the output
    distribution of the teacher and the student is constant throughout the training,
    there is no learning signal for updating the weights of the student.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65a0d4da56fdaf6c580d5c58625de2aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 7: Analysis of collapsing solutions. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Effect of Batch Size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An interesting property is that DINO can be trained with small batch sizes without
    a large drop in performance. This was actually one of BYOL‚Äôs motivation, a paper
    DINO builds upon, to be less dependent on batch size compared to contrastive self-supervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e0fa8d51ff10f8d989b51eb03eb122c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 4: Batch size vs. accuracy. [Source](https://arxiv.org/abs/2104.14294)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive methods like CLIP and GLIP provide a lot of negative samples for
    a given positive sample to avoid collapsing solutions. The more negative samples
    per optimizer update step (hence per batch) the better it works.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, DINO is a knowledge-distillation framework. It is a visual foundation
    model that exploits interesting properties of ViTs and is the predecessor of one
    of today‚Äôs best-performing foundation models, DINOv2\. DINO‚Äòs framework consists
    of a student and teacher model that acts upon different views of the same image
    and adds extra measures to deal with inherent instabilities of similarity-learning
    approaches. The experiments show that DINO outperforms other self-supervised pre-trained
    models on various tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Further Readings & Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Papers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the meantime an improved version of DINO has been released:'
  prefs: []
  type: TYPE_NORMAL
- en: '[DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Blog post DINOv2 by Meta](https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Paper Walkthroughs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You might also like my other paper walkthroughs covering concepts we discussed
    in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----4cb08e821b18--------------------------------)
    [## The CLIP Foundation Model'
  prefs: []
  type: TYPE_NORMAL
- en: Paper Summary‚Äî Learning Transferable Visual Models From Natural Language Supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----4cb08e821b18--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----4cb08e821b18--------------------------------)
    [## GLIP: Introducing Language-Image Pre-Training to Object Detection'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Summary: Grounded Language-Image Pre-training'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----4cb08e821b18--------------------------------)
    [](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
    [## BYOL -The Alternative to Contrastive Self-Supervised Learning
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper Analysis‚Äî Bootstrap Your Own Latent: A New Approach to Self-Supervised
    Learning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?source=post_page-----4cb08e821b18--------------------------------)
    [](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?source=post_page-----4cb08e821b18--------------------------------)
    [## Segment Anything ‚Äî Promptable Segmentation of Arbitrary Objects
  prefs: []
  type: TYPE_NORMAL
- en: Paper Walkthrough ‚Äî Segment Anything
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?source=post_page-----4cb08e821b18--------------------------------)
  prefs: []
  type: TYPE_NORMAL
