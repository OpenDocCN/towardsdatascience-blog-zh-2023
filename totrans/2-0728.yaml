- en: Detection of Credit Card Fraud with an Autoencoder
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è‡ªç¼–ç å™¨æ£€æµ‹ä¿¡ç”¨å¡æ¬ºè¯ˆ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/detection-of-credit-card-fraud-with-an-autoencoder-9275854efd48](https://towardsdatascience.com/detection-of-credit-card-fraud-with-an-autoencoder-9275854efd48)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/detection-of-credit-card-fraud-with-an-autoencoder-9275854efd48](https://towardsdatascience.com/detection-of-credit-card-fraud-with-an-autoencoder-9275854efd48)
- en: A guide for the implementation of an anomaly detector
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°å¼‚å¸¸æ£€æµ‹å™¨çš„æŒ‡å—
- en: '[](https://tinztwinspro.medium.com/?source=post_page-----9275854efd48--------------------------------)[![Janik
    and Patrick Tinz](../Images/a08aa54f553f606ef5df86f9411c36ac.png)](https://tinztwinspro.medium.com/?source=post_page-----9275854efd48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9275854efd48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9275854efd48--------------------------------)
    [Janik and Patrick Tinz](https://tinztwinspro.medium.com/?source=post_page-----9275854efd48--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://tinztwinspro.medium.com/?source=post_page-----9275854efd48--------------------------------)[![Janik
    and Patrick Tinz](../Images/a08aa54f553f606ef5df86f9411c36ac.png)](https://tinztwinspro.medium.com/?source=post_page-----9275854efd48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9275854efd48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9275854efd48--------------------------------)
    [Janik and Patrick Tinz](https://tinztwinspro.medium.com/?source=post_page-----9275854efd48--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9275854efd48--------------------------------)
    Â·10 min readÂ·Jun 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9275854efd48--------------------------------)
    Â·10åˆ†é’Ÿé˜…è¯»Â·2023å¹´6æœˆ1æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ead0f960668e0c078768fd7ab039672b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ead0f960668e0c078768fd7ab039672b.png)'
- en: Photo by [Christiann Koepke](https://unsplash.com/@christiannkoepke?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç”± [Christiann Koepke](https://unsplash.com/@christiannkoepke?utm_source=medium&utm_medium=referral)
    æä¾›çš„ç…§ç‰‡ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Do you want to know how to create an **anomaly detector** using Python and TensorFlow?
    Then this article is for you. Credit card companies use anomaly detectors to detect
    fraudulent transactions. It is important to identify fraudulent transactions so
    that customers do not have to pay for something they did not buy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æƒ³çŸ¥é“å¦‚ä½•ä½¿ç”¨ Python å’Œ TensorFlow åˆ›å»ºä¸€ä¸ª**å¼‚å¸¸æ£€æµ‹å™¨**å—ï¼Ÿé‚£ä¹ˆè¿™ç¯‡æ–‡ç« é€‚åˆä½ ã€‚ä¿¡ç”¨å¡å…¬å¸ä½¿ç”¨å¼‚å¸¸æ£€æµ‹å™¨æ¥æ£€æµ‹æ¬ºè¯ˆäº¤æ˜“ã€‚è¯†åˆ«æ¬ºè¯ˆäº¤æ˜“å¾ˆé‡è¦ï¼Œä»¥ä¾¿å®¢æˆ·ä¸å¿…ä¸ºä»–ä»¬æ²¡æœ‰è´­ä¹°çš„ä¸œè¥¿ä»˜é’±ã€‚
- en: Many credit card transactions take place every day, but very few transactions
    are fraudulent. The fraudulent transactions are anomalies. The article presents
    an implementation of an autoencoder model to detect these fraudulent transactions.
    First, we define an anomaly and introduce different types of anomalies. Then we
    describe the implementation of the anomaly detector for credit card fraud detection.
    Letâ€™s begin!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å¤©éƒ½æœ‰å¤§é‡çš„ä¿¡ç”¨å¡äº¤æ˜“ï¼Œä½†åªæœ‰æå°‘æ•°äº¤æ˜“æ˜¯æ¬ºè¯ˆæ€§çš„ã€‚æ¬ºè¯ˆäº¤æ˜“å°±æ˜¯å¼‚å¸¸ã€‚æ–‡ç« å±•ç¤ºäº†ä¸€ä¸ªè‡ªç¼–ç å™¨æ¨¡å‹çš„å®ç°ï¼Œç”¨äºæ£€æµ‹è¿™äº›æ¬ºè¯ˆäº¤æ˜“ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰å¼‚å¸¸å¹¶ä»‹ç»ä¸åŒç±»å‹çš„å¼‚å¸¸ã€‚ç„¶åæˆ‘ä»¬æè¿°ç”¨äºä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹çš„å¼‚å¸¸æ£€æµ‹å™¨çš„å®ç°ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: Anomaly detection in general
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼‚å¸¸æ£€æµ‹æ¦‚è¿°
- en: An anomaly detection algorithm identifies novel and unexpected structures in
    acquired datasets. There are many definitions of an anomaly in the literature.
    We derive a definition for our use case.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¼‚å¸¸æ£€æµ‹ç®—æ³•è¯†åˆ«åœ¨è·å–çš„æ•°æ®é›†ä¸­å‡ºç°çš„æ–°é¢–å’Œæ„å¤–çš„ç»“æ„ã€‚æ–‡çŒ®ä¸­æœ‰å¾ˆå¤šå¼‚å¸¸çš„å®šä¹‰ã€‚æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„ç”¨ä¾‹æ¨å¯¼äº†ä¸€ä¸ªå®šä¹‰ã€‚
- en: Anomaly definition
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼‚å¸¸å®šä¹‰
- en: 'Chandola et al [1] describe anomalies as patterns in data that do not conform
    to a well-deï¬ned notion of normal behaviour. Another widely used definition comes
    from Hawkins. Hawkins [2] describes an outlier as an observation that deviates
    from other observations to such an extent that it is suspected to have been generated
    by some other mechanism. Concerning the definitions presented, two essential aspects
    should be noted (cf. [3]):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Chandola ç­‰äºº [1] å°†å¼‚å¸¸æè¿°ä¸ºæ•°æ®ä¸­ä¸ç¬¦åˆæ­£å¸¸è¡Œä¸ºå®šä¹‰çš„æ¨¡å¼ã€‚å¦ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„å®šä¹‰æ¥è‡ª Hawkinsã€‚Hawkins [2] å°†ç¦»ç¾¤ç‚¹æè¿°ä¸ºä¸€ä¸ªåç¦»å…¶ä»–è§‚æµ‹å€¼çš„ç¨‹åº¦ï¼Œä»¥è‡³äºæ€€ç–‘å®ƒæ˜¯ç”±å…¶ä»–æœºåˆ¶ç”Ÿæˆçš„ã€‚å…³äºæ‰€å‘ˆç°çš„å®šä¹‰ï¼Œéœ€è¦æ³¨æ„ä¸¤ä¸ªé‡è¦æ–¹é¢ï¼ˆå‚è§
    [3]ï¼‰ï¼š
- en: The distribution of the anomalies deviates strongly from the general distribution
    of the data.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¼‚å¸¸çš„åˆ†å¸ƒä¸æ•°æ®çš„ä¸€èˆ¬åˆ†å¸ƒæœ‰å¾ˆå¤§åç¦»ã€‚
- en: The majority of the data are normal observations, and the anomalies are only
    a small part.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°æ•°æ®æ˜¯æ­£å¸¸çš„è§‚æµ‹å€¼ï¼Œè€Œå¼‚å¸¸åªæ˜¯å…¶ä¸­çš„ä¸€å°éƒ¨åˆ†ã€‚
- en: '**We define an anomaly as follows:**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: An anomaly is an observation or a sequence of observations that differ significantly
    from the majority of the data in distribution.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Types of anomalies
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can basically distinguish three types of anomalies.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: A **punctual anomaly** or **point anomaly** is when an observation deviates
    significantly from the rest of the data [3] and only lasts for a short time [4].
    Fraudulent transactions can lead to point anomalies.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **collective anomaly** is a collection of observations that are abnormal compared
    to the rest of the data. Individual observations can appear as abnormal or as
    normal, only the occurrence in a group makes them appear abnormal [4]. You can
    only detect collective anomalies in data where the individual observations are
    related.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **contextual anomaly** describes an observation or several observations that
    appear abnormal in a specific context [3]. These anomalies, when considered globally,
    lie within the range of values valid for this variable [4].
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this article, we develop an autoencoder model that can only detect point
    anomalies. There are also more advanced Autoencoder models, such as GRU or LSTM
    Autoencoders, which include the temporal component in the data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two options for the output of anomaly detection methods:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**Anomaly Score:** Deviation of an observation from the expected value.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Binary Label:** Normal or abnormal observation (Label: 0 or 1).'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some algorithms directly have a binary label as output, and others calculate
    the label based on the anomaly score over a certain threshold. Thus, you can derive
    the label from the anomaly score. [4]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following, you see the function for the anomaly score (cf. [3]):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/748d372e3eac65db292330f274016c08.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: 'Function: Anomaly Score (Formula by authors)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: In the equation, *Î³* denotes the anomaly score, ***x_t*** an observation at
    time *t*. *n* is the number of observations, and *p* is the number of variables/features.
    You can convert the anomaly score into a binary label (normal or abnormal) by
    defining a threshold value *Î´ âˆˆ R*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e960fe6234deb833549ed2909e747967.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: 'Formula: Binary Label (Formula by authors)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation shows that you can adjust the binary label according to the threshold
    value *Î´*. The implementation in this article uses a binary label (0: no fraud
    and 1: fraud).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoder concept
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we look at the theory behind an autoencoder. Autoencoders are
    artificial neural networks. They are very often used in anomaly detection. They
    belong to the semi-supervised methods because you train them only with the normal
    state of the data. An autoencoder model tries to efficiently compress an input
    (encoding) and finally reconstruct this compression (decoding) so that the reconstruction
    matches the input data as closely as possible. The compressed layer is called
    the latent representation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'The network, therefore, consists of two sections (cf. [5], p. 499):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: an encoder function ***z*** *= g(****x****)* and
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a decoder function ***xâ€²*** *= f (****z****)*
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following figure shows the general structure of an autoencoder.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c4d22a8e0ea0fb884a9b5c277e977c3.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: How an autoencoder works (Image by authors)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, learning an exact reconstruction is not useful because we mainly
    want to approximate relevant structures in the data. Compression in encoding forces
    the autoencoder to learn useful features in the data. The autoencoder uses the
    difference between the input and the output as a reconstruction error. During
    training, we aim to minimise this error. You can use different error metrics as
    error functions, such as the mean squared error (MSE):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5de70035144db6990d4a7dadf64a1c2e.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: 'Formula: Mean Squared Error (MSE) (Formula by authors)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The equation shows the calculation of the reconstruction error (MSE) for an
    observation ***x_t*** . The choice of the error function and the architecture
    of the autoencoder depends on the particular application. For example, the encoder
    and decoder can consist of simple feedforward layers, LSTM/GRU layers or convolutional
    neural network (CNN) layers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Credit Card Fraud Detection Implementation
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use the [Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?resource=download)
    dataset (Licensed under [Database Contents License (DbCL) v1.0](https://opendatacommons.org/licenses/dbcl/1-0/))
    from Kaggle to create our anomaly detector. The dataset contains anonymised credit
    card transactions of European credit card customers from September 2013\. The
    data was anonymised using PCA. Letâ€™s start with the data preparation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we read the data and output the first five data points.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We see that there are 28 anonymised columns and two columns are not anonymised.
    The Time and Amount columns are not anonymised. There is also a Class column (0:
    normal transactions or 1: fraudulent transactions). In total, the dataset has
    31 columns.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Next, we check if the dataset contains missing values.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The dataset does not contain any missing values. Thatâ€™s great.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are interested in the distribution of the two classes (No Fraud and
    Fraud). For this, we calculate the percentage share of the respective class in
    the total data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We see that the proportion of fraudulent transactions is small. In supervised
    learning methods, unbalanced data is dangerous because these methods learn based
    on labels. In our use case, however, we use a semi-supervised approach. The training
    only takes place with data from normal transactions. For this reason, no balancing
    of the data is necessary for the autoencoder approach.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Next, we convert the feature Amount into a normally distributed log equivalent
    by logarithmising the feature. This conversion improves the training of the autoencoder.
    We also remove the Time feature. We split our data into training, validation and
    test data. Then we remove the fraudulent transactions from the training data because
    we train our autoencoder only with the normal transactions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹ç‰¹å¾Amountè¿›è¡Œå¯¹æ•°è½¬æ¢ï¼Œå°†å…¶è½¬æ¢ä¸ºæ­£æ€åˆ†å¸ƒçš„å¯¹æ•°ç­‰ä»·ç‰©ã€‚è¿™ç§è½¬æ¢æ”¹å–„äº†è‡ªåŠ¨ç¼–ç å™¨çš„è®­ç»ƒã€‚æˆ‘ä»¬è¿˜ç§»é™¤äº†Timeç‰¹å¾ã€‚æˆ‘ä»¬å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒæ•°æ®ã€éªŒè¯æ•°æ®å’Œæµ‹è¯•æ•°æ®ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»è®­ç»ƒæ•°æ®ä¸­ç§»é™¤æ¬ºè¯ˆäº¤æ˜“ï¼Œå› ä¸ºæˆ‘ä»¬åªç”¨æ­£å¸¸äº¤æ˜“æ¥è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ã€‚
- en: Then we scale our data with the `MinMaxScaler` from the [sklearn library](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).
    We train the scaler on the training data and then transform the training, validation
    and test data with this scaler. It is important to adjust the scaler only on the
    training data, otherwise information from the validation or test data would flow
    into the training. Now our data are ready for modelling.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ¥è‡ª[sklearnåº“](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)çš„`MinMaxScaler`å¯¹æ•°æ®è¿›è¡Œç¼©æ”¾ã€‚æˆ‘ä»¬åœ¨è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒç¼©æ”¾å™¨ï¼Œç„¶åç”¨è¿™ä¸ªç¼©æ”¾å™¨è½¬æ¢è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®ã€‚é‡è¦çš„æ˜¯ï¼Œåªåœ¨è®­ç»ƒæ•°æ®ä¸Šè°ƒæ•´ç¼©æ”¾å™¨ï¼Œå¦åˆ™éªŒè¯æˆ–æµ‹è¯•æ•°æ®çš„ä¿¡æ¯ä¼šæµå…¥è®­ç»ƒä¸­ã€‚ç°åœ¨æˆ‘ä»¬çš„æ•°æ®å·²ç»å‡†å¤‡å¥½è¿›è¡Œå»ºæ¨¡äº†ã€‚
- en: Modelling
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å»ºæ¨¡
- en: 'In this section, we implement the autoencoder model with TensorFlow. The following
    listing shows the implementation of the autoencoder:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨TensorFlowå®ç°äº†è‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹ã€‚ä»¥ä¸‹æ¸…å•å±•ç¤ºäº†è‡ªåŠ¨ç¼–ç å™¨çš„å®ç°ï¼š
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The code shows the implementation for the encoder and the decoder. The encoder
    compresses the data into two features. Then the autoencoder performs decoding
    based on these two features. In this context, the autoencoder tries to reconstruct
    the input. The autoencoder aims to reconstruct the input as well as possible.
    We use ELU as an activation function because it performed best in our tests. Now
    we can train our model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç å±•ç¤ºäº†ç¼–ç å™¨å’Œè§£ç å™¨çš„å®ç°ã€‚ç¼–ç å™¨å°†æ•°æ®å‹ç¼©æˆä¸¤ä¸ªç‰¹å¾ã€‚ç„¶åï¼Œè‡ªåŠ¨ç¼–ç å™¨æ ¹æ®è¿™ä¸¤ä¸ªç‰¹å¾æ‰§è¡Œè§£ç ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè‡ªåŠ¨ç¼–ç å™¨å°è¯•é‡æ„è¾“å…¥ã€‚è‡ªåŠ¨ç¼–ç å™¨çš„ç›®æ ‡æ˜¯å°½å¯èƒ½å‡†ç¡®åœ°é‡æ„è¾“å…¥ã€‚æˆ‘ä»¬ä½¿ç”¨ELUä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œå› ä¸ºå®ƒåœ¨æˆ‘ä»¬çš„æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ã€‚
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: First, we initialise our model with the number of features. Then we define our
    optimiser and set the learning rate. In our case, we use the optimiser Adam. Finally,
    we compile our model with the optimiser and the loss function. In our case we
    use the MSE as loss function. We also define an early stopping by stopping the
    training if the validation loss does not change in five consecutive epochs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ç”¨ç‰¹å¾æ•°é‡åˆå§‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ã€‚ç„¶åæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„ä¼˜åŒ–å™¨å¹¶è®¾ç½®å­¦ä¹ ç‡ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¼˜åŒ–å™¨Adamã€‚æœ€åï¼Œæˆ‘ä»¬ç”¨ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ç¼–è¯‘æˆ‘ä»¬çš„æ¨¡å‹ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨MSEä½œä¸ºæŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€ä¸ªæ—©åœæœºåˆ¶ï¼Œé€šè¿‡åœ¨éªŒè¯æŸå¤±åœ¨è¿ç»­äº”ä¸ªepochå†…æ²¡æœ‰å˜åŒ–æ—¶åœæ­¢è®­ç»ƒã€‚
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Once the autoencoder is trained and validated, we can try out the model on the
    test data. The autoencoder tries to reconstruct the input of the test data as
    well as possible. The autoencoder reconstructs a normal transaction well. On the
    other hand, it has to reconstruct fraudulent transactions poorly. We can calculate
    the error between the input and the reconstruction using the MSE. Fraudulent transactions
    produce a large MSE, and normal transactions produce a small MSE.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è‡ªåŠ¨ç¼–ç å™¨ç»è¿‡è®­ç»ƒå’ŒéªŒè¯ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æµ‹è¯•æ•°æ®ä¸Šå°è¯•æ¨¡å‹ã€‚è‡ªåŠ¨ç¼–ç å™¨å°è¯•å°½å¯èƒ½å‡†ç¡®åœ°é‡æ„æµ‹è¯•æ•°æ®çš„è¾“å…¥ã€‚è‡ªåŠ¨ç¼–ç å™¨èƒ½å¾ˆå¥½åœ°é‡æ„æ­£å¸¸äº¤æ˜“ã€‚å¦ä¸€æ–¹é¢ï¼Œå®ƒå¿…é¡»å¯¹æ¬ºè¯ˆäº¤æ˜“è¿›è¡Œå·®çš„é‡æ„ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨MSEè®¡ç®—è¾“å…¥å’Œé‡æ„ä¹‹é—´çš„è¯¯å·®ã€‚æ¬ºè¯ˆäº¤æ˜“ä¼šäº§ç”Ÿè¾ƒå¤§çš„MSEï¼Œè€Œæ­£å¸¸äº¤æ˜“ä¼šäº§ç”Ÿè¾ƒå°çš„MSEã€‚
- en: Evaluation
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: In the previous section, we trained our model. Now itâ€™s time to evaluate the
    model. First, we look at the distribution of the MSE (reconstruction error).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®­ç»ƒäº†æˆ‘ä»¬çš„æ¨¡å‹ã€‚ç°åœ¨æ˜¯æ—¶å€™è¯„ä¼°æ¨¡å‹äº†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æŸ¥çœ‹å‡æ–¹è¯¯å·®ï¼ˆMSEï¼Œé‡æ„è¯¯å·®ï¼‰çš„åˆ†å¸ƒã€‚
- en: '![](../Images/e87e0d2ac2f888d0999d18b8fc5e3fd4.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e87e0d2ac2f888d0999d18b8fc5e3fd4.png)'
- en: Distribution of the reconstruction loss (Image by authors)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: é‡æ„æŸå¤±çš„åˆ†å¸ƒï¼ˆå›¾ç‰‡æ¥è‡ªä½œè€…ï¼‰
- en: The plot shows that the MSE (on the x axis) is higher for fraudulent transactions
    than for normal transactions. However, some fraudulent transactions also have
    a similar MSE to normal transactions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾è¡¨æ˜¾ç¤ºï¼Œæ¬ºè¯ˆäº¤æ˜“çš„MSEï¼ˆxè½´ï¼‰é«˜äºæ­£å¸¸äº¤æ˜“ã€‚ç„¶è€Œï¼Œä¸€äº›æ¬ºè¯ˆäº¤æ˜“çš„MSEä¸æ­£å¸¸äº¤æ˜“ç›¸ä¼¼ã€‚
- en: 'In the following, you can see some metrics:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„å†…å®¹ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ä¸€äº›æŒ‡æ ‡ï¼š
- en: '![](../Images/310b3dfd36e95722991f82b53e3f635e.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/310b3dfd36e95722991f82b53e3f635e.png)'
- en: Evaluation metrics autoencoder model (Image by authors)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°æŒ‡æ ‡è‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹ï¼ˆå›¾ç‰‡æ¥è‡ªä½œè€…ï¼‰
- en: In anomaly detection, recall is an important metric. Our model achieves a recall
    of 87%. That is a good value for an anomaly detection model. In addition, the
    model has a precision of 72%, which is also well for such a model. The true positive
    rate (TPR) is 75.2%, that means our model detects fraudulent transactions at 75%.
    On the other hand, our model fails to detect 25% of fraudulent transactions (false
    negative rate (FNR)).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼‚å¸¸æ£€æµ‹ä¸­ï¼Œå¬å›ç‡æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†87%çš„å¬å›ç‡ã€‚è¿™å¯¹äºå¼‚å¸¸æ£€æµ‹æ¨¡å‹æ¥è¯´æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å€¼ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„ç²¾ç¡®åº¦ä¸º72%ï¼Œè¿™å¯¹äºè¿™æ ·çš„æ¨¡å‹ä¹Ÿå¾ˆä¸é”™ã€‚çœŸæ­£çš„æ­£ä¾‹ç‡ï¼ˆTPRï¼‰ä¸º75.2%ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„æ¨¡å‹ä»¥75%çš„å‡†ç¡®ç‡æ£€æµ‹åˆ°æ¬ºè¯ˆäº¤æ˜“ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æœªèƒ½æ£€æµ‹åˆ°25%çš„æ¬ºè¯ˆäº¤æ˜“ï¼ˆå‡é˜´æ€§ç‡ï¼ˆFNRï¼‰ï¼‰ã€‚
- en: The aim of further optimisation must be to minimise the false negative rate.
    However, we must not forget that we trained our model without ever seeing fraud!
    In this respect, its performance is decent. Nevertheless, we can still try to
    improve the model a little.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥ä¼˜åŒ–çš„ç›®æ ‡å¿…é¡»æ˜¯æœ€å°åŒ–å‡é˜´æ€§ç‡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¿…é¡»è®°ä½ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæ¨¡å‹æ—¶ä»æœªè§è¿‡æ¬ºè¯ˆï¼åœ¨è¿™æ–¹é¢ï¼Œå®ƒçš„è¡¨ç°è¿˜ç®—ä¸é”™ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥å°è¯•ç¨å¾®æ”¹è¿›æ¨¡å‹ã€‚
- en: Using our FNR, we could already see that the network was not able to generalise
    perfectly. To improve the performance of the model, we can use a different autoencoder
    model, e.g. a different number of neurons per layer, or a latent representation
    of three or four neurons. We did some tests and varied the number of neurons.
    As a result, the following model gave better results.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æˆ‘ä»¬çš„FNRï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç½‘ç»œæ— æ³•å®Œç¾åœ°è¿›è¡Œæ³›åŒ–ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸åŒçš„è‡ªç¼–ç å™¨æ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œæ¯å±‚çš„ç¥ç»å…ƒæ•°é‡ä¸åŒï¼Œæˆ–è€…ä½¿ç”¨ä¸‰æˆ–å››ä¸ªç¥ç»å…ƒçš„æ½œåœ¨è¡¨ç¤ºã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€äº›æµ‹è¯•å¹¶å˜åŒ–äº†ç¥ç»å…ƒçš„æ•°é‡ã€‚ç»“æœï¼Œä»¥ä¸‹æ¨¡å‹å–å¾—äº†æ›´å¥½çš„æ•ˆæœã€‚
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We have increased the neurons in the latent layer to four. The first layer of
    the encoder has 32 neurons, and the last layer of the decoder has 32 neurons.
    The output layer uses the ReLU activation function. These changes lead to better
    results on the test data. You can see the results below.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ½œåœ¨å±‚çš„ç¥ç»å…ƒå¢åŠ åˆ°å››ä¸ªã€‚ç¼–ç å™¨çš„ç¬¬ä¸€å±‚æœ‰32ä¸ªç¥ç»å…ƒï¼Œè§£ç å™¨çš„æœ€åä¸€å±‚æœ‰32ä¸ªç¥ç»å…ƒã€‚è¾“å‡ºå±‚ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ã€‚è¿™äº›å˜åŒ–åœ¨æµ‹è¯•æ•°æ®ä¸Šå¯¼è‡´äº†æ›´å¥½çš„ç»“æœã€‚ä½ å¯ä»¥åœ¨ä¸‹é¢çœ‹åˆ°è¿™äº›ç»“æœã€‚
- en: '![](../Images/42c99abff219b5f6ee56a0f177c0142c.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42c99abff219b5f6ee56a0f177c0142c.png)'
- en: Evaluation metrics improved autoencoder (Image by authors)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°æŒ‡æ ‡æ”¹è¿›çš„è‡ªç¼–ç å™¨ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: We improved the recall and precision by two per cent with the adjustments. In
    addition, we have a TPR of 78.5% compared to 75.2% before. The new model has an
    FNR of 21.5%. That means that more fraudulent transactions are detected.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡è°ƒæ•´å°†å¬å›ç‡å’Œç²¾ç¡®åº¦æé«˜äº†2ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„TPRä»ä¹‹å‰çš„75.2%æé«˜åˆ°78.5%ã€‚æ–°æ¨¡å‹çš„FNRä¸º21.5%ã€‚è¿™æ„å‘³ç€æ£€æµ‹åˆ°çš„æ¬ºè¯ˆäº¤æ˜“æ›´å¤šã€‚
- en: The results show that it is essential to try different model configurations
    to get the best possible model. In our use case, a larger autoencoder lead to
    better results. However, this does not always have to be the case. It is important
    to implement a model that works well for the use case.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœè¡¨æ˜ï¼Œå°è¯•ä¸åŒçš„æ¨¡å‹é…ç½®ä»¥è·å¾—æœ€ä½³æ¨¡å‹æ˜¯è‡³å…³é‡è¦çš„ã€‚åœ¨æˆ‘ä»¬çš„ç”¨ä¾‹ä¸­ï¼Œæ›´å¤§çš„è‡ªç¼–ç å™¨å¯¼è‡´äº†æ›´å¥½çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ€»æ˜¯å¦‚æ­¤ã€‚é‡è¦çš„æ˜¯å®æ–½ä¸€ä¸ªé€‚åˆç”¨ä¾‹çš„æ¨¡å‹ã€‚
- en: Implementing an autoencoder model is especially useful when you have few labels
    to test. An autoencoder only needs normal data and not anomalies for training.
    The presented approach is particularly suitable for use cases in which anomalies
    occur rarely, and only very few labels are available for these anomalies.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å®æ–½è‡ªç¼–ç å™¨æ¨¡å‹åœ¨æµ‹è¯•æ ‡ç­¾å¾ˆå°‘æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚è‡ªç¼–ç å™¨ä»…éœ€è¦æ­£å¸¸æ•°æ®è€Œä¸æ˜¯å¼‚å¸¸æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æ‰€æå‡ºçš„æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºå¼‚å¸¸å‘ç”Ÿè¾ƒå°‘ä¸”ä»…æœ‰æå°‘æ•°æ ‡ç­¾çš„ç”¨ä¾‹ã€‚
- en: Conclusion
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we showed the implementation of an anomaly detector for credit
    card fraud. First, we presented the basics of anomaly detection, followed by the
    intuition of autoencoders. An autoencoder compresses an input and tries to reconstruct
    it as well as possible. Moreover, an autoencoder only needs normal transactions
    for training. We then implemented an autoencoder with Tensorflow. The evaluation
    showed that the autoencoder performs well.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªç”¨äºä¿¡ç”¨å¡æ¬ºè¯ˆçš„å¼‚å¸¸æ£€æµ‹å™¨çš„å®ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†å¼‚å¸¸æ£€æµ‹çš„åŸºç¡€çŸ¥è¯†ï¼Œç„¶åæ˜¯è‡ªç¼–ç å™¨çš„ç›´è§‰ã€‚è‡ªç¼–ç å™¨å‹ç¼©è¾“å…¥å¹¶å°½å¯èƒ½é‡å»ºå®ƒã€‚æ­¤å¤–ï¼Œè‡ªç¼–ç å™¨ä»…éœ€è¦æ­£å¸¸äº¤æ˜“è¿›è¡Œè®­ç»ƒã€‚ç„¶åï¼Œæˆ‘ä»¬ç”¨Tensorflowå®ç°äº†ä¸€ä¸ªè‡ªç¼–ç å™¨ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè‡ªç¼–ç å™¨è¡¨ç°è‰¯å¥½ã€‚
- en: ğŸ‘‰ğŸ½ [**Join our free weekly Magic AI newsletter for the latest AI updates!**](https://magicai.tinztwins.de)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘‰ğŸ½ [**åŠ å…¥æˆ‘ä»¬çš„æ¯å‘¨å…è´¹Magic AIé€šè®¯ï¼Œè·å–æœ€æ–°çš„AIæ›´æ–°ï¼**](https://magicai.tinztwins.de)
- en: ğŸ‘‰ğŸ½ [**You can find all our Freebies on our digital products page!**](https://shop.tinztwins.de/)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘‰ğŸ½ [**ä½ å¯ä»¥åœ¨æˆ‘ä»¬çš„æ•°å­—äº§å“é¡µé¢æ‰¾åˆ°æ‰€æœ‰å…è´¹çš„èµ„æºï¼**](https://shop.tinztwins.de/)
- en: '[**Subscribe for free**](https://tinztwinspro.medium.com/subscribe) **to get
    notified when we publish a new story:**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[**å…è´¹è®¢é˜…**](https://tinztwinspro.medium.com/subscribe) **ä»¥ä¾¿åœ¨æˆ‘ä»¬å‘å¸ƒæ–°æ•…äº‹æ—¶å¾—åˆ°é€šçŸ¥ï¼š**'
- en: '[](https://tinztwinspro.medium.com/subscribe?source=post_page-----9275854efd48--------------------------------)
    [## Get an email whenever Janik and Patrick Tinz publishes.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[## è®¢é˜…æˆ‘ä»¬çš„é‚®ä»¶ï¼Œéšæ—¶è·å–Janikå’ŒPatrick Tinzå‘å¸ƒçš„å†…å®¹ã€‚](https://tinztwinspro.medium.com/subscribe?source=post_page-----9275854efd48--------------------------------)'
- en: Get an email whenever Janik and Patrick Tinz publishes. By signing up, you will
    create a Medium account if you donâ€™tâ€¦
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¢é˜…é‚®ä»¶ï¼Œéšæ—¶è·å–Janikå’ŒPatrick Tinzå‘å¸ƒçš„å†…å®¹ã€‚é€šè¿‡æ³¨å†Œï¼Œä½ å°†åˆ›å»ºä¸€ä¸ªMediumè´¦æˆ·ï¼Œå¦‚æœä½ è¿˜æ²¡æœ‰çš„è¯â€¦
- en: tinztwinspro.medium.com](https://tinztwinspro.medium.com/subscribe?source=post_page-----9275854efd48--------------------------------)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[tinztwinspro.medium.com](https://tinztwinspro.medium.com/subscribe?source=post_page-----9275854efd48--------------------------------)'
- en: Learn more about us on our [About page](https://medium.com/@tinztwinspro/about).
    Donâ€™t forget to follow us on [X](https://twitter.com/tinztwins). Thanks so much
    for reading. If you liked this article, feel free to share it. **Have a great
    day!**
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: äº†è§£æ›´å¤šå…³äºæˆ‘ä»¬çš„ä¿¡æ¯ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„ [å…³äºé¡µé¢](https://medium.com/@tinztwinspro/about)ã€‚ä¸è¦å¿˜è®°å…³æ³¨æˆ‘ä»¬ [X](https://twitter.com/tinztwins)ã€‚éå¸¸æ„Ÿè°¢é˜…è¯»ã€‚å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·éšæ„åˆ†äº«ã€‚**ç¥ä½ æœ‰ç¾å¥½çš„ä¸€å¤©ï¼**
- en: Sign up for a Medium membership using [our link](https://tinztwinspro.medium.com/membership)
    to read unlimited Medium stories.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ [æˆ‘ä»¬çš„é“¾æ¥](https://tinztwinspro.medium.com/membership) æ³¨å†ŒMediumä¼šå‘˜ï¼Œé˜…è¯»æ— é™åˆ¶çš„Mediumæ•…äº‹ã€‚
- en: References
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Varun Chandola, Arindam Banerjee and Vipin Kumar. â€œAnomaly detection: A
    surveyâ€. In: ACM computing surveys (CSUR) 41.3 (2009), p. 1â€“58.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Varun Chandola, Arindam Banerjee å’Œ Vipin Kumar. â€œå¼‚å¸¸æ£€æµ‹ï¼šç»¼è¿°â€ã€‚å‘è¡¨äºï¼šACMè®¡ç®—è°ƒæŸ¥ï¼ˆCSURï¼‰41.3
    (2009), ç¬¬1â€“58é¡µã€‚'
- en: '[2] Douglas M Hawkins. Identification of outliers. Bd. 11\. Springer, 1980.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Douglas M Hawkins. ã€Šç¦»ç¾¤ç‚¹è¯†åˆ«ã€‹ã€‚ç¬¬11å·ï¼ŒSpringerï¼Œ1980ã€‚'
- en: '[3] Mohammad Braei and Sebastian Wagner. â€œAnomaly detection in univariate time-series:
    A survey on the state-of-the-artâ€. In: arXiv preprint arXiv:2004.00433 (2020).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Mohammad Braei å’Œ Sebastian Wagner. â€œå•å˜é‡æ—¶é—´åºåˆ—ä¸­çš„å¼‚å¸¸æ£€æµ‹ï¼šå‰æ²¿ç»¼è¿°â€ã€‚å‘è¡¨äºï¼šarXivé¢„å°æœ¬ arXiv:2004.00433
    (2020)ã€‚'
- en: '[4] Andrew A Cook, GÃ¶ksel MÄ±sÄ±rlÄ± and Zhong Fan. â€œAnomaly detection for IoT
    time-series data: A surveyâ€. In: IEEE Internet of Things Journal 7.7 (2019), S.
    6481â€“6494.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Andrew A Cook, GÃ¶ksel MÄ±sÄ±rlÄ± å’Œ Zhong Fan. â€œç‰©è”ç½‘æ—¶é—´åºåˆ—æ•°æ®çš„å¼‚å¸¸æ£€æµ‹ï¼šç»¼è¿°â€ã€‚å‘è¡¨äºï¼šIEEEç‰©è”ç½‘æœŸåˆŠ
    7.7 (2019), ç¬¬6481â€“6494é¡µã€‚'
- en: '[5] Ian Goodfellow, Yoshua Bengio and Aaron Courville. Deep Learning. MIT Press,
    2016.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Ian Goodfellow, Yoshua Bengio å’Œ Aaron Courville. ã€Šæ·±åº¦å­¦ä¹ ã€‹ã€‚MITå‡ºç‰ˆç¤¾ï¼Œ2016ã€‚'
