- en: AI’s Sentence Embeddings, Demystified
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI的句子嵌入，揭密
- en: 原文：[https://towardsdatascience.com/ais-sentence-embeddings-demystified-7c9cce145dd2](https://towardsdatascience.com/ais-sentence-embeddings-demystified-7c9cce145dd2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ais-sentence-embeddings-demystified-7c9cce145dd2](https://towardsdatascience.com/ais-sentence-embeddings-demystified-7c9cce145dd2)
- en: 'Bridging the gap between computers and language: How AI Sentence Embeddings
    Revolutionize NLP'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弥合计算机与语言之间的差距：AI句子嵌入如何革新自然语言处理
- en: '[](https://medium.com/@dataemporium?source=post_page-----7c9cce145dd2--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----7c9cce145dd2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7c9cce145dd2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7c9cce145dd2--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----7c9cce145dd2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dataemporium?source=post_page-----7c9cce145dd2--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----7c9cce145dd2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7c9cce145dd2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7c9cce145dd2--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----7c9cce145dd2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7c9cce145dd2--------------------------------)
    ·10 min read·Aug 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----7c9cce145dd2--------------------------------)
    ·阅读时长10分钟·2023年8月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c42af0ad915c5feb7d8468cc3195ef2d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c42af0ad915c5feb7d8468cc3195ef2d.png)'
- en: Photo by [Steve Johnson](https://unsplash.com/@steve_j?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Steve Johnson](https://unsplash.com/@steve_j?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In this blog post, let’s demystify how computers understand sentences and documents.
    To kick this discussion off, we will rewind time beginning with the earliest methods
    of representing sentences using n-gram vectors and TF-IDF vectors. Later sections
    will discuss methods that aggregate word vectors from neural bag of words to the
    sentence transformers and language models we see today. There is a lot of fun
    technology to cover. Let’s begin our journey with the simple, elegant n-grams.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，让我们揭示计算机如何理解句子和文档。为了开始这个讨论，我们将回顾最早使用n-gram向量和TF-IDF向量表示句子的方法。后续章节将讨论从神经袋词到今天看到的句子变换器和语言模型的方法。还有很多有趣的技术要介绍。让我们从简单优雅的n-gram开始我们的旅程。
- en: 1\. N-gram vectors
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. N-gram向量
- en: Computers don’t understand words, but they do understand numbers. As such, we
    need to convert words and sentences into vectors when processing by a computer.
    One of the earliest representations of sentences as a vector can be traced back
    to a [1948 paper by Claude Shanon](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf),
    father of information theory. In this seminal work, sentences were represented
    as an n-gram vector of words. *What does this mean?*
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机不理解单词，但它们理解数字。因此，我们需要将单词和句子转换为向量，以便计算机处理。句子作为向量的最早表示之一可以追溯到[克劳德·香农于1948年的论文](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)，信息理论的奠基人。在这项开创性的工作中，句子被表示为一个单词的n-gram向量。*这意味着什么？*
- en: '![](../Images/adde47738399c0b060b5a05fc6910ffe.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adde47738399c0b060b5a05fc6910ffe.png)'
- en: 'Figure 1: Generating n-gram vector from a sentence. (image by author)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：从句子生成n-gram向量。（作者提供的图片）
- en: 'Consider the sentence “This is a good day”. We can break this sentence down
    into the following n-grams:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以句子“This is a good day”为例。我们可以将这个句子分解成以下n-gram：
- en: '**Unigrams**: This, is, a, good, day'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单字组**：This, is, a, good, day'
- en: '**Bigrams**: This is, is a, a good, good day'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双字组**：This is, is a, a good, good day'
- en: '**Trigrams**: this is a, is a good, a good day'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**三字组**：this is a, is a good, a good day'
- en: and much more …
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及更多内容……
- en: In general, a sentence can be broken down into its constituent n-grams, iterating
    from 1 to n. When constructing the vector, each number in this vector represents
    whether the n-gram was present in the sentence or not. Some methods instead could
    us the count of the n-gram present in the sentence. A sample vector representation
    of a sentence is shown above in *Figure 1*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个句子可以分解为其组成的 n-gram，从 1 到 n 迭代。在构建向量时，这个向量中的每个数字表示 n-gram 是否在句子中出现。一些方法则可以使用句子中
    n-gram 的出现次数。*图 1* 上展示了一个句子的示例向量表示。
- en: 2\. TF-IDF
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. TF-IDF
- en: Another early, yet popular method of representing sentences and documents involved
    determining TF-IDF vector of a sentence or the “Term Frequency — Inverse Document
    Frequency” vector. In this case, we would count the number of times a word appears
    in the sentence to determine the text frequency (TF). Then we would calculate
    the number of sentences containing this word to determine the inverse Document
    Frequency (IDF). Multiply these values gives the TF-IDF product for every word.
    Repeating this computation for every single word in the sentence allows us to
    generate the TF-IDF vector for that sentence. A sample TF-IDF representation of
    a sentence is shown in *Figure 2*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种早期且流行的句子和文档表示方法是确定句子的 TF-IDF 向量，即“词频—逆文档频率”向量。在这种情况下，我们会计算一个词在句子中出现的次数来确定文本频率
    (TF)。然后我们会计算包含该词的句子数量来确定逆文档频率 (IDF)。将这些值相乘得到每个词的 TF-IDF 乘积。对句子中每个单词重复这一计算，可以生成该句子的
    TF-IDF 向量。*图 2* 显示了句子的一个 TF-IDF 表示示例。
- en: '![](../Images/a4a440ddd75570d0adc296f3c8b05636.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4a440ddd75570d0adc296f3c8b05636.png)'
- en: 'Figure 2: Generating TF-IDF vector from a sentence. (image by author)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：从句子生成 TF-IDF 向量。（图片来源：作者）
- en: Hence, each sentence (document) is represented by a vector equal to the vocabulary
    of the language. And each position wold be the TF-IDF score for every word in
    that sentence. In case you are interested, TF-IDF was introduced in this [1972
    dissertation](https://www.emerald.com/insight/content/doi/10.1108/eb026526/full/html).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个句子（文档）由一个等于语言词汇表的向量表示。每个位置将是该句子中每个词的 TF-IDF 分数。如果你感兴趣的话，TF-IDF 是在这个 [1972
    年的论文](https://www.emerald.com/insight/content/doi/10.1108/eb026526/full/html)
    中引入的。
- en: 3\. Assessing early sentence vectors
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 评估早期句子向量
- en: Pros
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优点
- en: '**Simple**: Count based representations are very easy to understand and compute
    manually.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单**：基于计数的表示方法非常容易理解和手动计算。'
- en: '**Fixed length vectors**: Sentences with different number of words can be represented
    by vectors of the same length. This uniformity makes processing easier.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**固定长度向量**：具有不同单词数量的句子可以由相同长度的向量表示。这种统一性使得处理更加方便。'
- en: Cons
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: '**Sparsity**: The sentence vectors generated are large, of the order of the
    vocabulary size. For the n-grams case, the vector size is the number of possible
    n-grams for growing values of *n*. This value can get exceptionally large even
    when *n* is a modest size. Sentence similarities are typically computed by taking
    the distance between their vectors. With such large and sparse vectors, the distance
    between sentences A and B can become indistinguishable from another pair of sentences
    A and C. This means we cannot determine if sentence A is more similar to B or
    C as the number of dimensions becomes too large. This is issue is popularly known
    as the **Curse of Dimensionality** and plagues many fields of statistics and AI.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏性**：生成的句子向量较大，大小级别与词汇表的规模相当。对于 n-gram 的情况，向量的大小是可能的 n-gram 数量，随着 *n* 值的增长，这个值可能变得异常大，即使
    *n* 是一个适中的大小。句子相似性通常通过计算其向量之间的距离来评估。对于这样大的稀疏向量，句子 A 和 B 之间的距离可能与另一对句子 A 和 C 的距离难以区分。这意味着我们无法确定句子
    A 是否更类似于 B 还是 C，因为维度数量变得太大。这个问题通常被称为 **维度灾难**，困扰着许多统计学和人工智能领域。'
- en: Clearly, these sentence representations were getting out of hand. To handle
    this, we take a step back. Instead of representing sentences as vectors, *what
    happens when words themselves are represented as vectors?*
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这些句子表示变得难以处理。为了解决这个问题，我们退后一步。与其将句子表示为向量，不如 *当词本身被表示为向量时会发生什么？*
- en: 4\. Dense Word Representations
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 密集词表示
- en: A [2001 paper on neural probabilistic language models](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
    introduced the idea of representing words with dense & continuous vectors that
    are fixed length. This combated the curse of dimensionality by ensuring every
    word can be represented as a vector of tractable size (~64, 128 or 256), instead
    of being proportional to the vocabulary size (~100,000)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇[2001年的神经概率语言模型论文](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)提出了用稠密和连续的固定长度向量表示词的想法。这种方法通过确保每个词可以用一个可处理大小的向量（~64、128或256）表示，而不是与词汇表大小（~100,000）成比例，从而克服了维度灾难。
- en: '![](../Images/9db69c57c74cbf82d32ee605199e8eb9.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9db69c57c74cbf82d32ee605199e8eb9.png)'
- en: 'Figure 3: In 2001, this language model learned dense word representations during
    training ([image source](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf))'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：2001年，这种语言模型在训练期间学习了稠密的词表示（[图像来源](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)）
- en: In *Figure 3*, *C(.)* are vectors that each represent a single word. Ideally,
    we want the computer to have some semantic understanding of these words. One way
    to measure this is to ensure the vectors learned that correspond to similar words
    should be physically closer to each other in space as shown in *Figure 4*. The
    NLP space was geared towards improving these word embeddings.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3*中，*C(.)*是表示单个词的向量。理想情况下，我们希望计算机对这些词有一定的语义理解。一种衡量方法是确保学习到的对应于相似词的向量在空间中彼此更靠近，如*图4*所示。NLP领域致力于改进这些词嵌入。
- en: '![](../Images/68e681ab7694b422101565d5590bf4c2.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68e681ab7694b422101565d5590bf4c2.png)'
- en: 'Figure 4: This is an embedding space where each dot represents the vector representation
    of a word. The vector representations are good if similar words are closer to
    each other as shown here (image by author)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：这是一个嵌入空间，每个点代表一个词的向量表示。如果相似的词彼此靠得更近，则表示向量效果良好，如此图所示（图像由作者提供）
- en: 5\. Word2Vec
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. Word2Vec
- en: Once such advancement to further achieve the goal of learning word embeddings
    came with the [Word2Vec framework in 2013](https://arxiv.org/abs/1301.3781). Word
    vectors of higher quality than ever before were learned with very simple feed
    forward neural network architectures show in *Figure 5*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 实现进一步学习词嵌入的目标的一个进展是[2013年的Word2Vec框架](https://arxiv.org/abs/1301.3781)。通过非常简单的前馈神经网络架构，学习到了比以往更高质量的词向量，如*图5*所示。
- en: '![](../Images/31749ebf457fa5e7e39836c58c8e96b8.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31749ebf457fa5e7e39836c58c8e96b8.png)'
- en: 'Figure 5: The 2 Word2Vec architectures that learn effective continuous dense
    representations for every word ([image source](https://arxiv.org/abs/1301.3781))'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：2种Word2Vec架构学习了每个词的有效连续稠密表示（[图像来源](https://arxiv.org/abs/1301.3781)）
- en: For more information on Word2Vec and other similar architectures, [check out
    my blog on this topic where we explain the architectures](https://medium.com/towards-data-science/word2vec-glove-and-fasttext-explained-215a5cd4c06f).
    Essentially, by this point, words could efficiently be represented by fixed length
    vectors that could be processed by computers. *But what about sentences*, which
    is the topic of interest for this blog?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于Word2Vec和其他类似架构的信息，[请查看我关于此主题的博客，其中我们解释了这些架构](https://medium.com/towards-data-science/word2vec-glove-and-fasttext-explained-215a5cd4c06f)。本质上，到此为止，词可以通过固定长度的向量有效地表示，并且这些向量可以由计算机处理。*但是句子呢*，这是本博客关注的主题。
- en: 6\. Neural Bag of Words
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 神经词袋
- en: One simple approach is to just take the average of word vectors in a sentence.
    A simple approach that is shown in *Figure 6*. A sentence is essentially treated
    like a *bag of words* with no ordering or context information considered. From
    this fixed size vector, we can add feed forward layers to perform machine learning
    tasks such as sentiment analysis or topic classification among others. This has
    the pro of ensuring the resulting vector is smaller in size, yet preserves some
    meaning of its constituent words. However, its representation lacks understanding
    of context and grammar.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是对句子中的词向量取平均。这种方法在*图6*中展示。一个句子本质上被视为一个*词袋*，不考虑排序或上下文信息。从这个固定大小的向量中，我们可以添加前馈层以执行诸如情感分析或主题分类等机器学习任务。这种方法的优点是确保生成的向量尺寸更小，同时保留其组成词的一些意义。然而，它的表示缺乏对上下文和语法的理解。
- en: '![](../Images/6dd0ca9b6881d598c73da6a4862eb388.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6dd0ca9b6881d598c73da6a4862eb388.png)'
- en: 'Figure 6: The neural bag of words archtiecture simply averages word vectors
    to get the corresponding sentence vector (image by author)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：神经词袋架构简单地平均单词向量以获得对应的句子向量（作者提供的图片）
- en: There have been strides over the years to better preserve context and grammar
    in a sentence vector.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，已经取得了进展，以更好地保留句子向量中的上下文和语法。
- en: 7\. Convolution Neural Networks in NLP
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7\. 自然语言处理中的卷积神经网络
- en: Convolution neural networks (CNNs) allows sequences of different lengths to
    be represented with fixed length vectors using a combination of convolution and
    pooling operation. The convolution operation allows words in the sentence to gain
    context from surrounding words and hence encode meaning in the final vector. This
    was applied as early as [1989 in Time Delay Neural Networks for phoneme detection](https://www.cs.toronto.edu/~fritz/absps/waibelTDNN.pdf).
    The architecture of this network is shown in *Figure 7* where given an audio wave,
    we classify the phonetic sound spoken as “b”, “d” or “g”.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）允许不同长度的序列使用卷积和池化操作的组合表示为固定长度的向量。卷积操作使句子中的单词能够从周围的单词中获取上下文，从而在最终向量中编码意义。这在[1989年的时间延迟神经网络用于音素检测](https://www.cs.toronto.edu/~fritz/absps/waibelTDNN.pdf)中就已应用。该网络的架构如*图7*所示，其中，给定一个音频波形，我们将语音的音素分类为“b”、“d”或“g”。
- en: '![](../Images/8332afc1d25166a60aaad89f961750c6.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8332afc1d25166a60aaad89f961750c6.png)'
- en: 'Figure 7: Initial Time Delay Neural Network architecture that used convolution
    to process sequence information ([image source](https://www.cs.toronto.edu/~fritz/absps/waibelTDNN.pdf))'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：使用卷积处理序列信息的初始时间延迟神经网络架构（[图片来源](https://www.cs.toronto.edu/~fritz/absps/waibelTDNN.pdf)）
- en: 'Advancements of convolution in NLP were minimal after this seminal paper. However
    they made a resurgence in 2010s with improved performance and follow up research
    in the form of [Dynamic Convolution Neural Networks (DCNNs)](https://arxiv.org/pdf/1404.2188.pdf)
    for following reasons:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇开创性的论文之后，卷积在自然语言处理中的进展有限。然而，它们在2010年代得到了复兴，性能得到改善，并有跟进研究，如[动态卷积神经网络（DCNNs）](https://arxiv.org/pdf/1404.2188.pdf)，原因如下：
- en: '**More data available**: More data is needed for training larger models wit
    lots of parameters'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更多数据可用**：训练具有大量参数的大型模型需要更多的数据。'
- en: '**Software advancements**: These are the advancements in learning dense word
    representations discussed in previous paragraphs.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软件进展**：这些是前述段落中讨论的密集词表示学习的进展。'
- en: '**Hardware advancements**: The introduction of GPUs and cUDA to speed up the
    training of models and exploit parallelization .'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件进展**：引入了GPU和CUDA，以加速模型训练并利用并行化。'
- en: Dynamic Convolution Neural Network have a dynamic architecture depending on
    the sequence length. So even long sentences and documents can be processed appropriately
    without changing the configurations of the network.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 动态卷积神经网络具有根据序列长度动态变化的架构。因此，即使是长句子和文档也可以适当地处理，而无需更改网络的配置。
- en: '![](../Images/31ca0e4b58e95e2c79454d098f517de8.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31ca0e4b58e95e2c79454d098f517de8.png)'
- en: 'Figure 8: DCNN architecture that has a dynamic max-k pooling depending on the
    sentence input length ([image source](https://arxiv.org/pdf/1404.2188.pdf))'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：根据句子输入长度具有动态最大k池化的DCNN架构（[图片来源](https://arxiv.org/pdf/1404.2188.pdf)）
- en: For more information on these convolution architectures and how they structured
    the landscape of NLP, [check out this other medium blog on this topic](https://medium.com/@dataemporium/convolution-in-nlp-573d0329cc37).
    Like convolution, there have been architectures introduced that have seen more
    of the spot light in recent years. *What are they?*
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解有关这些卷积架构及其如何塑造自然语言处理领域的更多信息，[请查看这个关于该主题的其他 Medium 博客](https://medium.com/@dataemporium/convolution-in-nlp-573d0329cc37)。与卷积一样，近年来也出现了一些架构，它们获得了更多的关注。*它们是什么？*
- en: 8\. Recurrent Neural Networks
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8\. 循环神经网络
- en: Recurrent neural networks (RNNs) are feed forward networks rolled out over time.
    As such, they can process sequence data. While RNNs have been talked about since
    the 1980s, [this dissertation by Rumelhart being one of the first indirect mentions](https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf),
    they hadn’t been used in the early years because they were difficult to train.
    This changed with [Ilya Sutskever’s 2013 PhD thesis](https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)
    that showed how RNNs can be trained stably. Since this time, we have seen RNNs
    burst into the scene to process sequence data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络（RNNs）是时间上展开的前馈网络。因此，它们可以处理序列数据。尽管自1980年代以来就有人谈论RNN，[Rumelhart的这篇论文是最早的间接提及之一](https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf)，但早期并未使用，因为训练困难。这一情况随着[Ilya
    Sutskever的2013年博士论文](https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)的出现而改变，论文展示了如何稳定地训练RNN。从那时起，我们见证了RNN在处理序列数据方面的爆发性增长。
- en: They processed input words sequentially so every word would have context of
    the words that came before it. With the introduction of bidirectional RNNs, the
    context of the words could be understood based on the words that came bother before
    and after it’s position in the sentence.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 他们顺序处理输入单词，使每个单词都能参考前面的单词。引入双向RNN后，单词的上下文可以基于其在句子中前后的单词进行理解。
- en: With RNNs however, long sentence and document inputs may lead to vanishing and
    exploding gradients during the training of the network. This is where LSTM (Long
    Short Term Memory) networks shine with their ability to process long sequences.
    For more information on this topic, [check out my accompanying YouTube video](https://youtu.be/QciIcRxJvsM).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用RNN时，长句子和文档输入可能导致训练网络时梯度消失或爆炸。这正是LSTM（长短期记忆）网络展现其处理长序列能力的地方。有关这个主题的更多信息，[请查看我配套的YouTube视频](https://youtu.be/QciIcRxJvsM)。
- en: '![](../Images/77baf6de9426431566f7cee5eb9c18da.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77baf6de9426431566f7cee5eb9c18da.png)'
- en: 'Figure 9: LSTM Cells for better understanding long sentences ([image source](https://colah.github.io/posts/2015-08-Understanding-LSTMs/))'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: LSTM单元以更好地理解长句子 ([image source](https://colah.github.io/posts/2015-08-Understanding-LSTMs/))'
- en: However, an issue with these recurrent neural networks is they are so slow to
    train that they use a truncated version of backpropogation to learn parameters.
    This slowness is attributed to the fact that RNNs process data sequentially, one
    word at a time. And so they don’t make use of modern GPUs very well.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些递归神经网络的一个问题是训练速度非常慢，以至于它们使用了截断版本的反向传播来学习参数。这种慢速归因于RNN逐个处理数据的方式。因此，它们不能很好地利用现代GPU。
- en: 9\. Transformer Neural Networks
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9\. Transformer 神经网络
- en: The issue of slowness seen in RNNs was addressed with Transformer neural networks.
    In the [2017 paper “Attention is all you need”](https://arxiv.org/abs/1706.03762),
    the transformer architecture was used to solve sequence to sequence problems like
    Machine Translation, Question Answering, and more. The network consists of an
    encoder and decoder that comprises of attention units as shown in *Figure 10*.
    These attention units would allow the network to learn high quality vector representations
    of a word while accounting for long term dependencies. In addition to these benefits,
    it has an edge over RNNs as input words could now be processed faster in parallel.
    This leads to faster training than RNNs. For more information on transformers
    and how to build them yourself, [check out this playlist of videos](https://www.youtube.com/playlist?list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 神经网络解决了RNN的慢速问题。在[2017年的论文《Attention is all you need》](https://arxiv.org/abs/1706.03762)中，Transformer架构被用于解决诸如机器翻译、问答等序列到序列的问题。该网络由一个编码器和解码器组成，其中包含注意力单元，如*图
    10*所示。这些注意力单元使网络能够学习高质量的单词向量表示，同时考虑到长期依赖关系。除此之外，它还优于RNN，因为输入单词现在可以并行处理，从而实现比RNN更快的训练速度。有关Transformer及其构建方法的更多信息，[请查看这个视频播放列表](https://www.youtube.com/playlist?list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4)。
- en: '![](../Images/1d808e6bbe3bacb4794106514f4684e9.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d808e6bbe3bacb4794106514f4684e9.png)'
- en: 'Figure 10: The architecture of a transformer neural network ([image source](https://arxiv.org/abs/1706.03762))'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: Transformer 神经网络的架构 ([image source](https://arxiv.org/abs/1706.03762))'
- en: Despite these wonderful features, this transformer network has it’s limitations.
    We need to train the model from scratch every time a new problem needs to be learned.
    This requires a ton of tailored data for each task. For example, if I want to
    build a translator from English to Kannada, I might need tens of thousands of
    examples. This data can be difficult to come by for many tasks. *How do we address
    this issue?*
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些优秀的特性，但这种变换器网络也有其局限性。每当需要学习一个新问题时，我们需要从头开始训练模型。这需要为每个任务准备大量量身定制的数据。例如，如果我想建立一个从英语到卡纳达语的翻译器，我可能需要数万个例子。这些数据对许多任务来说可能很难获取。*我们如何解决这个问题？*
- en: 10\. BERT
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10\. BERT
- en: 'BERT stands for Bidirectional Encoder Representation from Transformers. As
    such, it is a stack of transformer encoders that is designed to handle language
    tasks. Many NLP tasks have the common element of “language understanding”. So
    if we have an AI that has some fundamental understanding of language, it shouldn’t
    require too much data to fine tune this model to understand language translation,
    or question answering or any other language task. Based on this philosophy, [BERT
    was introduced in 2019](https://arxiv.org/pdf/1810.04805.pdf) with the training
    phase is split into 2 parts:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 代表双向编码器表示来自变换器。作为这种设计，它是一组变换器编码器，旨在处理语言任务。许多自然语言处理任务都有“语言理解”这一共同元素。因此，如果我们有一个对语言有基本理解的
    AI，那么微调这个模型以理解语言翻译、问答或任何其他语言任务不应该需要太多的数据。基于这一理念，[BERT 于 2019 年推出](https://arxiv.org/pdf/1810.04805.pdf)，其训练阶段分为两个部分：
- en: '**pretraining**: The model learns from scratch the understanding of language'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练**：模型从头学习语言理解'
- en: '**fine tuning**: The model learns to solve a specific NLP task with less data
    requirements.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**：模型学会用较少的数据解决特定的自然语言处理任务。'
- en: '![](../Images/4a0082a8259bf5ef50d09197a56b390c.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a0082a8259bf5ef50d09197a56b390c.png)'
- en: 'Figure 11: Pretraining and fine tuning of BERT ([image source](https://arxiv.org/pdf/1810.04805.pdf))'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：BERT 的预训练和微调 ([图片来源](https://arxiv.org/pdf/1810.04805.pdf))
- en: Essentially, you can download a pretrained BERT model and fine tune it with
    the data you have, even if you don’t have much of it. Hence BERT leverages transfer
    learning to quickly train models to solve language problems. For more information
    on BERT and its training and inference, [check out this YouTube video](https://youtu.be/xI0HHN5XKDo).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，你可以下载一个预训练的 BERT 模型，并使用你拥有的数据对其进行微调，即使你没有太多数据。因此，BERT 利用迁移学习快速训练模型来解决语言问题。有关
    BERT 及其训练和推理的更多信息，[请查看这个 YouTube 视频](https://youtu.be/xI0HHN5XKDo)。
- en: However, BERT is a stack of transformer encoders. And transformer encoders internally
    learn high quality word embeddings. So coming back to the question again, *how
    do we derive sentence embeddings?*
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，BERT 是一组变换器编码器。而变换器编码器内部学习高质量的词嵌入。那么回到这个问题，*我们如何得到句子嵌入？*
- en: 11\. Sentence Transformers
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11\. 句子转换器
- en: The easiest way is to pass the sentence through BERT to generate high quality
    word embeddings. Then take the average of these to get a sentence. This is the
    most basic sentence transformer.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是将句子传递给 BERT 以生成高质量的词嵌入。然后对这些词嵌入取平均值以获得句子嵌入。这是最基本的句子转换器。
- en: '![](../Images/2d20b1f22bbc2fc26444cb48fd4021b7.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d20b1f22bbc2fc26444cb48fd4021b7.png)'
- en: 'Figure 12: A basic sentence transformer that takes in a sentence and generates
    a vector ([image source](https://arxiv.org/pdf/1908.10084.pdf))'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：一个基本的句子转换器，它接受一个句子并生成一个向量 ([图片来源](https://arxiv.org/pdf/1908.10084.pdf))
- en: 'Sentence transformers have a straightforward flow of passing in a sentence
    through BERT and a pooling layer to get a vector representation of that sentence.
    [But this leads to bad embeddings that even words than GloVe embeddings](https://arxiv.org/pdf/1908.10084.pdf).
    Instead, we fine tune this BERT architecture on one, or both, of the following
    tasks:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 句子转换器的流程非常简单，即将句子通过 BERT 和池化层，以获得该句子的向量表示。[但这会导致比 GloVe 嵌入更差的嵌入](https://arxiv.org/pdf/1908.10084.pdf)。因此，我们对
    BERT 架构进行微调，针对以下一个或两个任务：
- en: '**Natural Language Inference** [(NLI)](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.sbert.net%2Fdocs%2Fpretrained-models%2Fnli-models.html&link_redirector=1)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言推理** [(NLI)](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.sbert.net%2Fdocs%2Fpretrained-models%2Fnli-models.html&link_redirector=1)'
- en: '**Sentence Test Similarity** [(STS)](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.sbert.net%2Fdocs%2Fpretrained-models%2Fsts-models.html&link_redirector=1)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子测试相似性** [(STS)](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.sbert.net%2Fdocs%2Fpretrained-models%2Fsts-models.html&link_redirector=1)'
- en: The *Figure 13* below shows the fine tuning process for each.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下图*图 13*展示了每个部分的微调过程。
- en: '![](../Images/008214a995416a71ba5f85a0cb30a474.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/008214a995416a71ba5f85a0cb30a474.png)'
- en: 'Figure 13: Siamese network trained on NLI on the left and STS on the right
    ([image source](https://arxiv.org/pdf/1908.10084.pdf))'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：左侧是对NLI训练的Siamese网络，右侧是对STS训练的Siamese网络 ([图像来源](https://arxiv.org/pdf/1908.10084.pdf))
- en: Once one form of this architecture is trained, we can then use one of the BERT
    and pooling units for inference to convert a sentence into a vector representation
    that is of higher quality.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这种架构的某一种形式被训练好，我们就可以使用其中一个BERT和池化单元进行推理，将句子转换为更高质量的向量表示。
- en: Conclusion
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article we have seen the advancements in representing sentences as vectors.
    In the early days of NLP, we started with count based methods such as n-gram vectors
    and TF-IDF vectors. These methods however created sparse vectors that are difficult
    to process by computers. But with the introduction of dense vector representations
    in the early 2000s, increased data, hardware advancements, we have been able to
    train models that have a high quality understanding of words represented in vectors.
    This could also be used to create sentence vectors effectively as seen in the
    previous section on sentence transformers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们了解了将句子表示为向量的进展。在NLP的早期，我们开始使用基于计数的方法，如n-gram向量和TF-IDF向量。然而，这些方法产生了稀疏向量，计算机难以处理。但随着2000年代初期密集向量表示的引入、数据的增加以及硬件的进步，我们能够训练出具有高质量词向量理解的模型。这也可以有效地用于创建句子向量，如前面关于句子转换器的部分所示。
- en: Current language models are increasingly having a better understanding of words,
    sentences and documents. And I hope you are as excited as I am to learn more about
    the AI future.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的语言模型对单词、句子和文档的理解越来越好。我希望你和我一样，对学习更多关于AI未来的内容感到兴奋。
- en: '*Happy Learning!*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*快乐学习！*'
