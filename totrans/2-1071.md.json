["```py\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\n```", "```py\n# Set a random seed for reproducibility\nnp.random.seed(0)\n\n# Constants for the quadratic equation\na, b, c = 1, 2, 3\n\n# Create a DataFrame with a single feature\nn = 100  # number of samples\nx = np.linspace(-10, 10, n)  # feature values from -10 to 10\nnoise = np.random.normal(0, 10, n)  # some random noise\ny = a * x**2 + b * x + c + noise  # quadratic equation with noise\n\n# create a pandas dataframe\ndf = pd.DataFrame({'X': x, 'y': y})\n```", "```py\ndf = data.copy()\n\n# Create the figure\nfig = go.Figure()\n\n# Add scatter plot trace\nfig.add_trace(\n    go.Scatter(\n        x=df[\"X\"], \n        y=df[\"y\"], \n        mode=\"markers\", \n        marker=dict(opacity=0.7, size=6, color=\"red\",\n                    line=dict(color='Black', width=1))\n    )\n)\n\n# Update layout\nfig.update_layout(\n    title={\n        'text': \"Scatter plot of our sample data\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    xaxis=dict(\n        title=\"X\",\n        showline=True,  # Show the X axis line\n        tickmode='linear',  # Tick mode to be linear\n        # tick0=-10,  # Start tick at 0\n        dtick=1,  # Tick at every 1 unit\n        linecolor='black',  # X axis line color\n    ),\n    yaxis=dict(\n        title=\"y\",\n        showline=True,  # Show the Y axis line\n        linecolor='black',  # Y axis line color\n    ),\n    plot_bgcolor='white',  # Background color\n    width=900,\n    height=500\n)\n\n# Show the figure\nfig.show()\n```", "```py\ndef mean_squared_error(y, y_hat):\n\"\"\"\nReturns mean squared error for \ngiven actual values and predicted values\n\"\"\"\n  y = np.array(y)\n  y_hat = np.array(y_hat)\n  return np.mean((y-y_hat)**2)\n```", "```py\ndf = data.copy()\n\n# list of all features (only one i.e., 'X' in our case)\nfeatures = [\"X\"]\n\n# iterate over all features\nfor feature in features:\n  # initialize best_params dict\n  best_params = {\"feature\": None, # best feature\n                 \"split_value\": None, # best split value\n                 \"weighted_mse\": np.inf, # weighted mse of two branches\n                 \"curr_mse\": None, # mse at current node\n                 \"right_yhat\": None, # prediction at right branch\n                 \"left_yhat\": None # prediction at left branch\n                 }\n  # sort the df by current feature\n  df = df.sort_values(by=feature)\n\n  # compute the mse at current node\n  curr_yhat = np.mean(df['y'].mean())\n  curr_mse = mean_squared_error(df['y'], curr_yhat)\n  curr_mse = np.round(curr_mse, 3)\n\n  # iterate over all rows of sorted df\n  for i in range(1, len(df)):\n    # compute average of two consecutive rows\n    split_val = (df.iloc[i][feature] + df.iloc[i-1][feature]) / 2\n    split_val = np.round(split_val, 3)\n\n    # split the df into two partitions\n    left_branch = df[df[feature]<=split_val]\n    right_branch = df[df[feature]>split_val]\n\n    # compute the MSE of both partitions:\n\n    left_yhat = np.mean(left_branch[\"y\"]) # prediction will be average of target\n    left_yhat = np.round(left_yhat, 3)\n    left_mse = mean_squared_error(left_branch[\"y\"], left_yhat) # mse of left\n    left_mse = np.round(left_mse, 3)\n\n    right_yhat = np.mean(right_branch[\"y\"]) # prediction will be average of target\n    right_yhat = np.round(right_yhat, 3)\n    right_mse = mean_squared_error(right_branch[\"y\"], right_yhat) # mse of right\n    right_mse = np.round(right_mse, 3)\n\n    # compute weighted MSE\n    weighted_mse = ((len(left_branch) * left_mse) + (len(right_branch) * right_mse))/len(df)\n    weighted_mse = np.round(weighted_mse, 3)\n\n    # update best_params if weighted_mse is less than previously best_mse\n    if weighted_mse <= best_params[\"weighted_mse\"]:\n      best_params[\"weighted_mse\"] = weighted_mse\n      best_params[\"split_value\"] = split_val\n      best_params[\"feature\"] = feature\n      best_params[\"curr_mse\"] = curr_mse\n      best_params[\"right_yhat\"] = right_yhat\n      best_params[\"left_yhat\"] = left_yhat\n\nbest_params\n```", "```py\n# add vertical line to existing figure\nfig.add_vline(x=best_params[\"split_value\"], line_width=3, line_color=\"black\")\n\n# plot right branch split\nfig.add_shape(type=\"line\",\nx0=best_params[\"split_value\"], y0=best_params[\"right_yhat\"],\nx1=10, y1=best_params[\"right_yhat\"],  \nline=dict(color=\"gray\", width=3))\n\n# plot left branch split\nfig.add_shape(type=\"line\",\nx0=-10, y0=best_params[\"left_yhat\"],\nx1=best_params[\"split_value\"], y1=best_params[\"left_yhat\"],  \nline=dict(color=\"gray\", width=3))\n\n# fig.update_layout(title=\nfig.update_layout(\ntitle={\n    'text': f\"X<={np.round(best_params[\"split_value\"],2)}, MSE={np.round(best_params[\"curr_mse\"],2)}, samples={len(df)}, value={np.round(df[\"y\"].mean(),2)}\",\n    'y':0.9,\n    'x':0.5,\n    'xanchor': 'center',\n    'yanchor': 'top'},\n)\n\nfig.show()\n```", "```py\ndef get_best_params(df):\n\"\"\"Function to return best split\"\"\"\n    for feature in features:\n    # initialize best_params dict\n        best_params = {\"feature\": None, # best feature\n                 \"split_value\": None, # best split value\n                 \"weighted_mse\": np.inf, # weighted mse of two branches\n                 \"curr_mse\": None, # mse at current node\n                 \"right_yhat\": None, # prediction at right branch\n                 \"left_yhat\": None # prediction at left branch\n                 }\n        # sort the df by current feature\n        df = df.sort_values(by=feature)\n\n        # iterate over all rows of sorted df\n        for i in range(1, len(df)):\n            # compute average of two consecutive rows\n            split_val = (df.iloc[i][feature] + df.iloc[i-1][feature]) / 2\n            split_val = np.round(split_val, 3)\n\n            # split the df into two partitions\n            left_branch = df[df[feature]<=split_val]\n            right_branch = df[df[feature]>split_val]\n\n            # compute the MSE of both partitions:\n\n            left_yhat = np.mean(left_branch[\"y\"]) # prediction will be average of target\n            left_yhat = np.round(left_yhat, 3)\n            left_mse = mean_squared_error(left_branch[\"y\"], left_yhat) # mse of left\n            left_mse = np.round(left_mse, 3)\n\n            right_yhat = np.mean(right_branch[\"y\"]) # prediction will be average of target\n            right_yhat = np.round(right_yhat, 3)\n            right_mse = mean_squared_error(right_branch[\"y\"], right_yhat) # mse of right\n            right_mse = np.round(right_mse, 3)\n\n            # compute weighted MSE\n            weighted_mse = ((len(left_branch) * left_mse) + (len(right_branch) * right_mse))/len(df)\n            weighted_mse = np.round(weighted_mse, 3)\n\n            # update best_params if weighted_mse is less than previously best_mse\n            if weighted_mse <= best_params[\"weighted_mse\"]:\n                best_params[\"left_yhat\"] = left_yhat\n                best_params[\"right_yhat\"] = right_yhat\n                best_params[\"weighted_mse\"] = weighted_mse\n                best_params[\"split_value\"] = split_val\n                best_params[\"feature\"] = feature\n                best_params[\"curr_mse\"] = curr_mse\n\n    return best_params\n```", "```py\ndef build_tree(df, max_depth=3, curr_depth=0):\n    \"\"\"Function to build the regression tree recursively\"\"\"\n\n    if curr_depth>=max_depth:\n        prediction = np.round(np.mean(df[\"y\"]), 3)\n        print((\"--\" * curr_depth) + f\"Predict: {prediction}\")\n        return\n\n    best_params = get_best_params(df)\n\n    if best_params[\"feature\"] is None or best_params[\"split_value\"] is None:\n        prediction = np.round(np.mean(df[\"y\"]), 2)\n        print((\"--\" * curr_depth) + f\"Predict: {prediction}\")\n        return\n\n    feature = best_params[\"feature\"]\n    split_val = best_params[\"split_value\"]\n\n    # Print the current question (decision rule)\n    question = f\"{feature} <= {split_val}\"\n    mse = mean_squared_error(df[\"y\"], df[\"y\"].mean())\n    mse = np.round(mse, 3)\n    samples = len(df)\n    print((\"--\" * (curr_depth*2)) + \">\" + f\"{question}, mse: {mse}, samples: {samples}, value: {np.round(df[\"y\"].mean(),3)}\")\n\n    left_branch = df[df[feature]<=split_val]\n    right_branch = df[df[feature]>split_val]\n\n    # recursive calls for left and right subtrees\n    if not left_branch.empty:\n        print((\"--\" * curr_depth) + f\"Yes ->\")\n        build_tree(left_branch, curr_depth=curr_depth+1)\n\n    if not right_branch.empty:\n        print((\"--\" * curr_depth) + f\"No ->\")\n        build_tree(right_branch, curr_depth=curr_depth+1)\n```", "```py\nbuild_tree(data)\n```", "```py\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Fit the regression tree\nregressor = DecisionTreeRegressor(max_depth=3)\nX = np.array(data[\"X\"]).reshape(100, 1)\ny = np.array(data[\"y\"])\nregressor.fit(X, y)\n\n# Plot the tree\nplt.figure(figsize=(15, 8))\ntree.plot_tree(regressor, feature_names=['X'], filled=True)\nplt.show()\n```"]