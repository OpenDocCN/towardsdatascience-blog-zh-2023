["```py\n!pip install datasets\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"train\")\n\nprint(dataset[0])\n>>> {'sentence1': 'A plane is taking off.',\n 'An air plane is taking off.',\n 'similarity_score': 5.0}\n\nprint(dataset[1])\n>>> {'sentence1': 'A man is playing a large flute.',\n 'sentence2': 'A man is playing a flute.',\n 'similarity_score': 3.799999952316284}\n```", "```py\nsimilarity = [i['similarity_score'] for i in dataset]\nnormalized_similarity = [i/5.0 for i in similarity]\n```", "```py\n!pip install sentence-transformers\n```", "```py\nfrom sentence_transformers import SentenceTransformer, models\n\nword_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=128)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\nsts_bert_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n```", "```py\n!pip install transformers\n\nfrom transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nsentence_1 = [i['sentence1'] for i in dataset]\nsentence_2 = [i['sentence2'] for i in dataset]\ntext_cat = [[str(x), str(y)] for x,y in zip(sentence_1, sentence_2)][0]\n\ninput_data = tokenizer(text_cat, padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\")\noutput = sts_bert_model(input_data)\n\nprint(output['sentence_embedding'][0].size())\n>>> torch.Size([768])\n\nprint(output['sentence_embedding'][1].size())\n>>> torch.Size([768])\n```", "```py\nimport torch\n\nclass STSBertModel(torch.nn.Module):\n\n    def __init__(self):\n\n        super(STSBertModel, self).__init__()\n\n        word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=128)\n        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n        self.sts_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\n    def forward(self, input_data):\n\n        output = self.sts_model(input_data)\n\n        return output\n```", "```py\nclass DataSequence(torch.utils.data.Dataset):\n\n    def __init__(self, dataset):\n\n        similarity = [i['similarity_score'] for i in dataset]\n        self.label = [i/5.0 for i in similarity]\n        self.sentence_1 = [i['sentence1'] for i in dataset]\n        self.sentence_2 = [i['sentence2'] for i in dataset]\n        self.text_cat = [[str(x), str(y)] for x,y in zip(self.sentence_1, self.sentence_2)]\n\n    def __len__(self):\n\n        return len(self.text_cat)\n\n    def get_batch_labels(self, idx):\n\n        return torch.tensor(self.label[idx])\n\n    def get_batch_texts(self, idx):\n\n        return tokenizer(self.text_cat[idx], padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\")\n\n    def __getitem__(self, idx):\n\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n\n        return batch_texts, batch_y\n\ndef collate_fn(texts):\n\n  num_texts = len(texts['input_ids'])\n  features = list()\n  for i in range(num_texts):\n      features.append({'input_ids':texts['input_ids'][i], 'attention_mask':texts['attention_mask'][i]})\n\n  return features\n```", "```py\nclass CosineSimilarityLoss(torch.nn.Module):\n\n    def __init__(self,  loss_fct = torch.nn.MSELoss(), cos_score_transformation=torch.nn.Identity()):\n\n        super(CosineSimilarityLoss, self).__init__()\n        self.loss_fct = loss_fct\n        self.cos_score_transformation = cos_score_transformation\n        self.cos = torch.nn.CosineSimilarity(dim=1)\n\n    def forward(self, input, label):\n\n        embedding_1 = torch.stack([inp[0] for inp in input])\n        embedding_2 = torch.stack([inp[1] for inp in input])\n\n        output = self.cos_score_transformation(self.cos(embedding_1, embedding_2))\n\n        return self.loss_fct(output, label.squeeze())\n```", "```py\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndef model_train(dataset, epochs, learning_rate, bs):\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    model = STSBertModel()\n\n    criterion = CosineSimilarityLoss()\n    optimizer = Adam(model.parameters(), lr=learning_rate)\n\n    train_dataset = DataSequence(dataset)\n    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=bs, shuffle=True)\n\n    if use_cuda:\n        model = model.cuda()\n        criterion = criterion.cuda()\n\n    best_acc = 0.0\n    best_loss = 1000\n\n    for i in range(epochs):\n\n        total_acc_train = 0\n        total_loss_train = 0.0\n\n        for train_data, train_label in tqdm(train_dataloader):\n\n            train_data['input_ids'] = train_data['input_ids'].to(device)\n            train_data['attention_mask'] = train_data['attention_mask'].to(device)\n            del train_data['token_type_ids']\n\n            train_data = collate_fn(train_data)\n\n            output = [model(feature)['sentence_embedding'] for feature in train_data]\n\n            loss = criterion(output, train_label.to(device))\n            total_loss_train += loss.item()\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n        print(f'Epochs: {i + 1} | Loss: {total_loss_train / len(dataset): .3f}')\n        model.train()\n\n    return model\n\nEPOCHS = 8\nLEARNING_RATE = 1e-6\nBATCH_SIZE = 8\n\n# Train the model\ntrained_model = model_train(dataset, EPOCHS, LEARNING_RATE, BATCH_SIZE)\n```", "```py\n# Load test data\ntest_dataset = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"test\")\n\n# Prepare test data\nsentence_1_test = [i['sentence1'] for i in test_dataset]\nsentence_2_test = [i['sentence2'] for i in test_dataset]\ntext_cat_test = [[str(x), str(y)] for x,y in zip(sentence_1_test, sentence_2_test)]\n\n# Function to predict test data\ndef predict_sts(texts):\n\n  trained_model.to('cpu')\n  trained_model.eval()\n\n  test_input = tokenizer(texts, padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\")\n  test_input['input_ids'] = test_input['input_ids']\n  test_input['attention_mask'] = test_input['attention_mask']\n  del test_input['token_type_ids']\n\n  test_output = trained_model(test_input)['sentence_embedding']\n  sim = torch.nn.functional.cosine_similarity(test_output[0], test_output[1], dim=0).item()\n\n  return sim\n```", "```py\nprint(text_cat_test[420])\n>>> ['four children are playing on a trampoline.',\n 'Four kids are jumping on a trampoline.']\n\nprint(predict_sts(text_cat_test[420]))\n>>> 0.8608950972557068\n```", "```py\nprint(text_cat_test[245])\n>>> ['A man spins on a surf board.', \n'A man is putting barbecue sauce on chicken.']\n\nprint(predict_sts(text_cat_test[245]))\n>>> 0.05531075596809387\n```"]