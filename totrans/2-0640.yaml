- en: Data Engineering Interview Questions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/data-engineering-interview-questions-fdef62e46505](https://towardsdatascience.com/data-engineering-interview-questions-fdef62e46505)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tips to prepare for a job interview
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----fdef62e46505--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----fdef62e46505--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fdef62e46505--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fdef62e46505--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----fdef62e46505--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fdef62e46505--------------------------------)
    Â·20 min readÂ·Nov 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7bc9856e987821675426d58096e4630.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ignacio AmenÃ¡bar](https://unsplash.com/@amenabarladrondeguevara?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This story aims to shed some light on various data engineering interview scenarios
    and typical discussions. It covers almost every question you might be asked and
    I hope it will be useful for beginner and intermediate-level data practitioners
    during the job interview preparation. Throughout my almost fifteen-year career
    in analytics and data engineering, I interviewed many people and now I would like
    to share my observations with you.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Are data engineering interviews tough? No, not really if you understand what
    you are dealing with. Many companies have tech blogs where they describe their
    stacks and the tech they use. I would recommend doing some research beforehand.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering interviews are quite simple per se and the job is very rewarding.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The interviews are quite simple indeed as questions follow the same pattern
    typically. The number of **data platform types** [1] is limited to just four and
    that would define the answer helping you to pass. So if we know what we are engineering
    then itâ€™s not a very big task to answer interview questions correctly.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----fdef62e46505--------------------------------)
    [## Data Platform Architecture Types'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: How well does it answer your business needs? Dilemma of a choice.
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering (DE) interviews are easy to pass unless you are tasked with
    coding. This is a whole different story and usually, this would be the second
    part of the interview process. Below is my collection of DE interview questions
    and answers. Enjoy!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: What is your DE like on a day-to-day basis?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Usually hiring managers start the conversation with this simple question. Here
    we would want to demonstrate the abundance of enthusiasm and experience with various
    DE tools and frameworks. Provide some data pipeline examples to decorate your
    answer. It can be a couple of data pipelines you built or a full life cycle project
    with a data warehouse in the centre of this infrastructure. Donâ€™t call it a tutorial.
    It is always better to say something likeâ€¦
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: â€œâ€¦ a full-lifecycle project from requirements gathering to data pipeline design
    and go live.â€
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'It looks more professional and this is the impression you would want to create.
    Try to be concise but also be fluent in describing your day-to-day work. For example,
    you can say that you are a student, your main focus is data quality at the moment
    and you designed and built data pipelines to check data using row conditions in
    the first place before loading data into the data platform. Alternatively, you
    could mention that you know how to work with SDKs to load data into the data warehouse,
    etc. You can find some good examples in this article [2]:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----fdef62e46505--------------------------------)
    [## Python for Data Engineers'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Advanced ETL techniques for beginners
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: It is not very difficult. You can say that you have various data sources on
    the left-hand side and you can create data pipelines following this pattern below
    to integrate them into your data warehouse (DWH) solution.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a533b28c5e039871007b44ee56caff97.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Data warehouse exmaple. Image by author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this example of data loading into a BigQuery data warehouse using
    Pandas and google.cloud libraries [3]:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How do you create data pipelines?
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You would want to make clear that you are confident working with both third-party
    ETL tools (Fivetran, Stitch, etc.) and bespoke data connectors you can write yourself.
    A data pipeline is something that extracts, transforms and/or loads data from
    point A into the destination at point B [4].
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----fdef62e46505--------------------------------)
    [## Data pipeline design patterns'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right architecture with examples
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: So all you need is to demonstrate that you know how to do it following three
    main data pipeline design patterns â€” **batch** (aggregate and process in chunks),
    **streaming** (process and load record by record), **change data capture** (CDC,
    identify and capture changes at point A to process and load into B).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: CDC and streaming are closely connected.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For example, we can use MySQL binary log file to move data into our DWH solution
    in real time. It must be used with care and is not always the most cost-effective
    tool for data pipelines but it is worth mentioning this. Keep everything in order
    following the conceptual design diagram. It helps to explain many ETL things.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f319a0d445798a12e7ff1aa554120f2a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Conceptual data pipeline design. Image by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: What do you know about data platform design?
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a nutshell, there are four data platform architecture types that would define
    the selection of tools you might want to use while building a pipeline. This is
    the key to this question â€” it helps to choose the right DE tools and techniques.
    Data lakes, warehouses, and lake houses each have their benefits and serve each
    purpose. The fourth architecture type is **Data Mesh** where data management is
    decentralised. **Data Mesh** defines the state when we have different data domains
    (company departments) with their own teams and shared data resources. It might
    seem a bit more chaotic but many companies choose this model to reduce data bureaucracy.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Typically data warehouses offer better data governance compared to data lakes.
    It makes the data stack look modern and flexible due to built-in ANSI-SQL capabilities.
    The shift to a lake or data warehouse would depend primarily on the skillset of
    your users. The Data warehouse solution will enable more interactivity and narrow
    down our choice to a SQL-first product (Snowflake, BigQuery, etc.).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Data lakes are for users with programming skills and we would want to go for
    Python-first products like Databricks, Galaxy, Dataproc, EMR.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is data modeling?
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data modelling is an essential part of data engineering as data is being transformed
    using relationships between entities (tables, views, silos, data lakes). You would
    want to demonstrate that you understand how this process works in terms of **the
    conceptual** and **physical** design process. We always start with the concept
    of creating a model for our business process or a data transformation task. Then
    it is followed by a functional model which is a prototype and it aims to prove
    that our conceptual model works for this task. In the end, we will create a physical
    model which contains the final infrastructure including all required physical
    entities and objects. Itâ€™s good to say that it doesnâ€™t have to be SQL entities
    always. Conceptual data modelling might include all types of data platforms with
    semi-structured data files in the cloud storage. A good example would be a scenario
    when we need to prepare data in the data lake first and then use it to train the
    machine learning (ML) model. I previously wrote about it in this story:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/orchestrate-machine-learning-pipelines-with-aws-step-functions-d8216a899bd5?source=post_page-----fdef62e46505--------------------------------)
    [## Orchestrate Machine Learning Pipelines with AWS Step Functions'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Advanced-Data Engineering and ML Ops with Infrastructure as Code
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/orchestrate-machine-learning-pipelines-with-aws-step-functions-d8216a899bd5?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Itâ€™s always good to mention that you are familiar with **templating engines**
    such as **DBT** and **Dataform** that can be used for this task. Why? It helps
    a lot with data transformation **unit tests** [4] and data environments [5], prevents
    human errors and provides better deployment workflows. I previously wrote about
    it here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)
    [## Continuous Integration and Deployment for Data Platforms'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD for data engineers and ML Ops
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between Star and Snowflake schema?
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Very often job interviewers test your knowledge of data engineering design schemas.
    Try to be concise and say that Star schema is where we can take advantage of super
    large denormalised datasets connected to one fact table. Thatâ€™s why itâ€™s a Star
    database design pattern as it looks like a star. This is more suitable for data
    warehouse OLAP-style analytics pipelines. Data in those datasets is not always
    up-to-date but thatâ€™s fine because we need it to be materialised this way and
    we can update the required fields if needed.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Opposite to a Star schema Snowflake schema design has the same fact table in
    the center but it is linked with many other fact and dimension tables which are
    typically **denormalised.** This schema design is more suitable for OLTP data
    processing when data needs to be always up-to-date and individual rows can be
    pulled fast to use in the application.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: On a scale from 1 to 10 how good are your SQL skills?
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Make sure you can explain your answer. SQL is a natural dialect to model data
    transformation and create analytics datasets. Working confidently with incremental
    table updates gives you 6 out of 10 straight away. Consider this example below.
    It creates an **incremental** table using MERGE:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'I wrote about advanced techniques before. I think itâ€™s a good place to start
    the preparation [6]:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----fdef62e46505--------------------------------)
    [## Advanced SQL techniques for beginners'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: On a scale from 1 to 10 how good are your data warehousing skills?
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Running SQL unit tests for data transformation scripts and working with custom
    user-defined functions (UDF) [7] would grant you 9 out of 10.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: How do I get 10 out of 10 in SQL?
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It would be something very tricky and obviously related to your expert knowledge
    of a particular tool, i.e. converting a table into **an array of structs** and
    passing them to UDF.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: This is useful when you need to apply a user-defined function (UDF) with some
    complex logic to each row or table.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You can always consider your table as an array of TYPE STRUCT objects and then
    pass each one of them to UDF. It depends on your logic. For example, I use it
    in purchase stacking to calculate expire times:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: What is the difference between OLAP and OLTP?
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Online analytical processing (OLAP) and Online transactional processing (OLTP)
    are data processing systems designed for completely different purposes. OLAP aims
    to aggregate and store the data for analytical purposes such as reporting and
    large-scale data processing, Thatâ€™s why denormalised super big tables are seen
    very often here. OLTP processing is different in the way we process data â€” it
    would have a single transaction focus and require lightning-fast data processing.
    Good examples are in-app purchases, managing user accounts and updating store
    content. Data for OLTP is stored in indexed tables connected using the Snowflake
    pattern where dimension tables are mostly normalised.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: What data engineering frameworks do you know?
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We canâ€™t know everything. I interviewed a lot of people and itâ€™s not necessary
    to have experience with all data engineering tools and frameworks. You can name
    a few: **Python ETL (PETL), Bonobo, Apache Airflow, Bubbles, Kestra, Luigi** and
    I previously wrote about the ETL frameworks explosion we witnessed during the
    past couple of years.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[](/modern-data-engineering-e202776fb9a9?source=post_page-----fdef62e46505--------------------------------)
    [## Modern Data Engineering'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Platform Specific Tools and Advanced Techniques
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/modern-data-engineering-e202776fb9a9?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: We donâ€™t need to be super experienced with all frameworks but demonstrating
    confidence is a must.
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In order to demonstrate confidence with various data tools we would want to
    learn at least one or two and then use the basic principles (data engineering
    principles). Using this approach we can answer almost every DE question:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Why did you do it this way? â€” I got this from basic principles.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Having said this it would be just fine to learn a few things from Apache Airflow
    and demonstrate it with a simple pipeline example. For example, we can run ml_engine_training_op
    after we export data into the cloud storage (bq_export_op) and make this workflow
    run daily or weekly.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d08e679e0eaf09488813ce519b4a635.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: ML model training using Airflow. Image by author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Consider this example below.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '*It creates a simple data pipeline graph to export data into a cloud storage
    bucket and then trains the ML model using MLEngineTrainingOperator.*'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: What would you use to orchestrate your data pipelines?
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is important to differentiate the ETL frameworks we can use for data transformation
    and the frameworks we use to orchestrate our data pipelines. You can mention a
    few: **Airflow, Prefect, Dagster, Kestra, Argo, Luigi**. These are the most popular
    ones at the moment. These are open-source projects free to use. However, a good
    answer should indicate that you are capable of performing data pipeline orchestration
    using your own bespoke tools. If you like AWS you can deploy and orchestrate data
    pipelines using CloudFormation (Infrastructure as code) and Step Functions. I
    previously wrote about it here [9]:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-pipeline-orchestration-9887e1b5eb7a?source=post_page-----fdef62e46505--------------------------------)
    [## Data Pipeline Orchestration'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Data pipeline management done right simplifies deployment and increases the
    availability and accessibility of data forâ€¦
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-pipeline-orchestration-9887e1b5eb7a?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: In fact, we donâ€™t even need Step Functions here as it would be a very platform-specific
    choice. We could use platform-agnostic Terraform (Infrastructure as code) and
    Serverless to deploy microservices with required data pipelines orchestrating
    logic.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: What is your programming language?
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The answer to this question depends on the company stack. Long story short,
    you wonâ€™t miss it if you answer Python. This one is a coding absolute in DE and
    data science because of its simplicity and the numerous libraries and open-source
    data tools available in the market.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: However, donâ€™t limit yourself with to Python.
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is always good to be familiar with other languages, i.e. JAVA, JavaScript,
    Scala, Spark and R. R for example is good for data science and is very popular
    among scholars and universities. It is always good to mention Spark. Itâ€™s not
    a language (framework) but it became very popular due to its great scalability
    and capabilities for large-scale data processing [8].
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: You might not know Spark but if you know Python then you can always use a Spark
    API connector (PySpark).
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is *args and **kwargs?
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Typically, it would be the next one if you named Python in the previous question.
    Answering a question about function arguments is the most common one I ask during
    job interviews. You would want to be ready to answer it and maybe even impress
    your interviewer with a few lines of code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How good are you with CLI tools and shell scripting?
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cloud vendor command-line tools are based on REST API and enable data engineers
    with a powerful command-line interface to communicate with cloud services endpoints
    to describe and modify resources. Data engineers use CLI tools with bash scripting
    to chain commands. It helps to create powerful scripts and interact with cloud
    services with ease. Consider this example below. It will invoke the AWS Lambda
    function called pipeline-manager:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can create something even more powerful to deploy our serverless microservices.
    Consider this example below. It will check if the storage bucket for the lambda
    package exists, upload and deploy our ETL service as a Lambda Function [10]:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€äº›æ›´å¼ºå¤§çš„å·¥å…·æ¥éƒ¨ç½²æˆ‘ä»¬çš„æ— æœåŠ¡å™¨å¾®æœåŠ¡ã€‚è€ƒè™‘ä¸‹é¢è¿™ä¸ªä¾‹å­ã€‚å®ƒå°†æ£€æŸ¥lambdaåŒ…çš„å­˜å‚¨æ¡¶æ˜¯å¦å­˜åœ¨ï¼Œä¸Šä¼ å¹¶éƒ¨ç½²æˆ‘ä»¬çš„ETLæœåŠ¡ä½œä¸ºä¸€ä¸ªLambdaå‡½æ•°[10]ï¼š
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How do you deploy your data pipelines?
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ å¦‚ä½•éƒ¨ç½²ä½ çš„æ•°æ®ç®¡é“ï¼Ÿ
- en: There are no right or wrong answers but if you say â€œ I manually create pipeline
    steps and then deploy them in the cloud using vendorâ€™s consoleâ€¦â€ that wouldnâ€™t
    be the best answer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰å¯¹é”™ä¹‹åˆ†ï¼Œä½†å¦‚æœä½ è¯´â€œæˆ‘æ‰‹åŠ¨åˆ›å»ºç®¡é“æ­¥éª¤ï¼Œç„¶ååœ¨äº‘ä¸­ä½¿ç”¨ä¾›åº”å•†çš„æ§åˆ¶å°è¿›è¡Œéƒ¨ç½²â€¦â€¦â€é‚£å°±ä¸æ˜¯æœ€ä½³ç­”æ¡ˆã€‚
- en: Now the good answer would be to mention scripts. This tells the interviewer
    that you are an intermediate user familiar with shell scripting at a minimum.
    You would want to say that whatever you deploy, can be deployed using bash scripts
    and CLI tools. All major cloud vendors have their command line tools and you would
    want to be at least familiar with one of them.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¥½çš„ç­”æ¡ˆæ˜¯æåˆ°è„šæœ¬ã€‚è¿™å‘Šè¯‰é¢è¯•å®˜ä½ æ˜¯ä¸€ä¸ªä¸­çº§ç”¨æˆ·ï¼Œè‡³å°‘ç†Ÿæ‚‰Shellè„šæœ¬ã€‚ä½ ä¼šæƒ³è¯´ï¼Œæ— è®ºä½ éƒ¨ç½²ä»€ä¹ˆï¼Œéƒ½å¯ä»¥ä½¿ç”¨bashè„šæœ¬å’ŒCLIå·¥å…·è¿›è¡Œéƒ¨ç½²ã€‚æ‰€æœ‰ä¸»è¦çš„äº‘ä¾›åº”å•†éƒ½æœ‰ä»–ä»¬çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œä½ è‡³å°‘è¦å¯¹å…¶ä¸­ä¸€ç§å·¥å…·æœ‰æ‰€äº†è§£ã€‚
- en: The optimal way which is often considered as best practice is to deploy your
    pipelines using Infrastructure as code and CI/CD tools [11].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä¼˜çš„æ–¹æ³•ï¼Œé€šå¸¸è¢«è®¤ä¸ºæ˜¯æœ€ä½³å®è·µï¼Œæ˜¯ä½¿ç”¨åŸºç¡€è®¾æ–½å³ä»£ç å’ŒCI/CDå·¥å…·æ¥éƒ¨ç½²ä½ çš„ç®¡é“[11]ã€‚
- en: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)
    [## Continuous Integration and Deployment for Data Platforms'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)
    [## æ•°æ®å¹³å°çš„æŒç»­é›†æˆå’Œéƒ¨ç½²'
- en: CI/CD for data engineers and ML Ops
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®å·¥ç¨‹å¸ˆå’Œæœºå™¨å­¦ä¹ è¿ç»´çš„CI/CD
- en: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)'
- en: How good are you with Data Science?
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ åœ¨æ•°æ®ç§‘å­¦æ–¹é¢çš„æ°´å¹³å¦‚ä½•ï¼Ÿ
- en: As a data engineer you donâ€™t need to know all the intricacies of data science
    model training and hypertuning but remember that a good data scientist must be
    a good data engineer. Doesnâ€™t have to be vice versa but it is always good to demonstrate
    at least some knowledge of basic data science algorithms. For example, you can
    mention that you know how to create linear and logistic regression models. One
    creates quantitative output (a predicted number) when the other one returns a
    simple answer â€” â€œyesâ€ or â€œnoâ€ (1/0). In fact, all major data science models can
    be easily trained using SQL inside your data warehouse solution.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæ•°æ®å·¥ç¨‹å¸ˆï¼Œä½ ä¸éœ€è¦äº†è§£æ•°æ®ç§‘å­¦æ¨¡å‹è®­ç»ƒå’Œè¶…å‚æ•°è°ƒä¼˜çš„æ‰€æœ‰ç»†èŠ‚ï¼Œä½†è®°ä½ä¸€ä¸ªå¥½çš„æ•°æ®ç§‘å­¦å®¶å¿…é¡»æ˜¯ä¸€ä¸ªå¥½çš„æ•°æ®å·¥ç¨‹å¸ˆã€‚ä¸å¿…å®Œå…¨ç›¸åï¼Œä½†å±•ç¤ºè‡³å°‘ä¸€äº›åŸºæœ¬çš„æ•°æ®ç§‘å­¦ç®—æ³•çŸ¥è¯†æ€»æ˜¯æœ‰ç›Šçš„ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥æåˆ°ä½ çŸ¥é“å¦‚ä½•åˆ›å»ºçº¿æ€§å›å½’å’Œé€»è¾‘å›å½’æ¨¡å‹ã€‚çº¿æ€§å›å½’ç”Ÿæˆå®šé‡è¾“å‡ºï¼ˆé¢„æµ‹çš„æ•°å­—ï¼‰ï¼Œè€Œé€»è¾‘å›å½’åˆ™è¿”å›ç®€å•çš„ç­”æ¡ˆâ€”â€”â€œæ˜¯â€æˆ–â€œå¦â€ï¼ˆ1/0ï¼‰ã€‚äº‹å®ä¸Šï¼Œæ‰€æœ‰ä¸»è¦çš„æ•°æ®ç§‘å­¦æ¨¡å‹éƒ½å¯ä»¥åœ¨ä½ çš„æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆä¸­ä½¿ç”¨SQLè½»æ¾è®­ç»ƒã€‚
- en: Letâ€™s imagine our use case is churn prediction.
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬çš„ç”¨ä¾‹æ˜¯å®¢æˆ·æµå¤±é¢„æµ‹ã€‚
- en: 'Consider **BigQuery ML** where we can create a logistic regression like so:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥**BigQuery ML**ä¸ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥åƒè¿™æ ·åˆ›å»ºä¸€ä¸ªé€»è¾‘å›å½’æ¨¡å‹ï¼š
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: What do you know about data quality and data reliability?
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ å¯¹æ•°æ®è´¨é‡å’Œæ•°æ®å¯é æ€§äº†è§£å¤šå°‘ï¼Ÿ
- en: This is always a good question because you might be asked about possible ways
    to ensure data quality in your data platform. It is one of the data engineerâ€™s
    daily routine jobs to improve data pipelines in terms of data accuracy. Data engineers
    connect data sources and deploy pipelines where data must be extracted and then
    very often it has to be transformed according to business requirements.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼Œå› ä¸ºä½ å¯èƒ½ä¼šè¢«é—®åŠåœ¨æ•°æ®å¹³å°ä¸­ç¡®ä¿æ•°æ®è´¨é‡çš„å¯èƒ½æ–¹æ³•ã€‚è¿™æ˜¯æ•°æ®å·¥ç¨‹å¸ˆæ—¥å¸¸å·¥ä½œçš„ä¸€éƒ¨åˆ†ï¼Œæ¶‰åŠæé«˜æ•°æ®ç®¡é“çš„æ•°æ®å‡†ç¡®æ€§ã€‚æ•°æ®å·¥ç¨‹å¸ˆè¿æ¥æ•°æ®æºå¹¶éƒ¨ç½²ç®¡é“ï¼Œå…¶ä¸­æ•°æ®å¿…é¡»è¢«æå–ï¼Œç„¶åæ ¹æ®ä¸šåŠ¡éœ€æ±‚ç»å¸¸éœ€è¦è¿›è¡Œè½¬æ¢ã€‚
- en: We would want to make sure that all required fields exist (data quality) and
    no data is missing (reliability).
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„å­—æ®µå­˜åœ¨ï¼ˆæ•°æ®è´¨é‡ï¼‰ä¸”æ²¡æœ‰æ•°æ®ä¸¢å¤±ï¼ˆå¯é æ€§ï¼‰ã€‚
- en: How do we do it? Itâ€™s always good to mention self-fixing pipelines and that
    you know how to deploy them. Data engineers can deploy data quality pipelines
    in a similar way they deploy ETL pipelines. To put it simply, you would want to
    use row conditions for one dataset and based on the outcome deploy a fixing step,
    i.e. extract missing data and load it.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼Ÿæ€»æ˜¯æåˆ°è‡ªä¿®å¤ç®¡é“æ˜¯å¥½çš„ï¼Œå¹¶ä¸”ä½ çŸ¥é“å¦‚ä½•éƒ¨ç½²å®ƒä»¬ã€‚æ•°æ®å·¥ç¨‹å¸ˆå¯ä»¥ä»¥ç±»ä¼¼äºéƒ¨ç½²ETLç®¡é“çš„æ–¹å¼éƒ¨ç½²æ•°æ®è´¨é‡ç®¡é“ã€‚ç®€å•æ¥è¯´ï¼Œä½ éœ€è¦ä¸ºä¸€ä¸ªæ•°æ®é›†ä½¿ç”¨è¡Œæ¡ä»¶ï¼Œå¹¶æ ¹æ®ç»“æœéƒ¨ç½²ä¿®å¤æ­¥éª¤ï¼Œå³æå–ç¼ºå¤±çš„æ•°æ®å¹¶åŠ è½½å®ƒã€‚
- en: Using row conditions for your datasets aims to ensure data quality.
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¡Œæ¡ä»¶æ¥ç¡®ä¿æ•°æ®é›†çš„æ•°æ®è´¨é‡ã€‚
- en: All data quality checks can be scheduled as scripts and if any of them fail
    to meet certain conditions then we can send an email notification. Itâ€™s worth
    saying that modern data warehouse solutions allow SQL scripts to do such checks
    but it doesnâ€™t have to be limited to SQL. Any data check script can be run on
    data in the data lake or anywhere else. It just depends on the type of our data
    platform. Good coding skills are a must in this case so we would want to demonstrate
    that we know how to create a simple patrol application that can scan our data
    depending on where it is located physically.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰çš„æ•°æ®è´¨é‡æ£€æŸ¥éƒ½å¯ä»¥ä½œä¸ºè„šæœ¬è¿›è¡Œè°ƒåº¦ï¼Œå¦‚æœä»»ä½•ä¸€ä¸ªæ£€æŸ¥æœªèƒ½æ»¡è¶³ç‰¹å®šæ¡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥å‘é€ç”µå­é‚®ä»¶é€šçŸ¥ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œç°ä»£æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆå…è®¸ä½¿ç”¨SQLè„šæœ¬è¿›è¡Œè¿™äº›æ£€æŸ¥ï¼Œä½†è¿™ä¸ä¸€å®šè¦å±€é™äºSQLã€‚ä»»ä½•æ•°æ®æ£€æŸ¥è„šæœ¬éƒ½å¯ä»¥åœ¨æ•°æ®æ¹–æˆ–å…¶ä»–åœ°æ–¹çš„æ•°æ®ä¸Šè¿è¡Œã€‚è¿™å–å†³äºæˆ‘ä»¬æ•°æ®å¹³å°çš„ç±»å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè‰¯å¥½çš„ç¼–ç æŠ€èƒ½æ˜¯å¿…éœ€çš„ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å±•ç¤ºæˆ‘ä»¬çŸ¥é“å¦‚ä½•åˆ›å»ºä¸€ä¸ªç®€å•çš„å·¡æ£€åº”ç”¨ç¨‹åºï¼Œå¯ä»¥æ ¹æ®æ•°æ®çš„å®é™…ä½ç½®æ‰«ææ•°æ®ã€‚
- en: The SQL-based answer is also good but it would be more suitable for the Data
    Developer role as SQL is often considered the main data querying dialect in analytics.
    Consider this example below. It will use SQL with row conditions to check if there
    are any records with NULL `payment_date`. It will also check for duplicates.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºSQLçš„ç­”æ¡ˆä¹Ÿå¾ˆå¥½ï¼Œä½†å®ƒæ›´é€‚åˆæ•°æ®å¼€å‘äººå‘˜è§’è‰²ï¼Œå› ä¸ºSQLé€šå¸¸è¢«è®¤ä¸ºæ˜¯åˆ†æä¸­çš„ä¸»è¦æ•°æ®æŸ¥è¯¢æ–¹è¨€ã€‚è€ƒè™‘ä¸‹é¢çš„ä¾‹å­ã€‚å®ƒå°†ä½¿ç”¨SQLå’Œè¡Œæ¡ä»¶æ¥æ£€æŸ¥æ˜¯å¦æœ‰NULL
    `payment_date`çš„è®°å½•ã€‚å®ƒè¿˜ä¼šæ£€æŸ¥é‡å¤è®°å½•ã€‚
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As a result BigQuery will send an automated email containing the alert:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯BigQueryå°†å‘é€åŒ…å«è­¦æŠ¥çš„è‡ªåŠ¨ç”µå­é‚®ä»¶ï¼š
- en: '![](../Images/333076df44ae7cc144559f8d8999a013.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/333076df44ae7cc144559f8d8999a013.png)'
- en: Email alert. Image by author.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: é‚®ä»¶è­¦æŠ¥ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: What algorithm would you use to extract or process a very large dataset?
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ ä¼šä½¿ç”¨ä»€ä¹ˆç®—æ³•æ¥æå–æˆ–å¤„ç†éå¸¸å¤§çš„æ•°æ®é›†ï¼Ÿ
- en: This question might be a trap if you had previous questions about data transformation
    with Python. If you like Python then you are probably a big fan of the Pandas
    library and you probably already mentioned this during the interview. Well, this
    is the kind of question where you wouldnâ€™t want to use Pandas. The thing is that
    Pandas doesnâ€™t work with big datasets very well, especially with data transformation.
    You will always be limited to your machineâ€™s memory while running data transformations
    in the Pandas data frame.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä¹‹å‰æœ‰å…³äºä½¿ç”¨Pythonè¿›è¡Œæ•°æ®è½¬æ¢çš„é—®é¢˜ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½æ˜¯ä¸ªé™·é˜±ã€‚å¦‚æœä½ å–œæ¬¢Pythonï¼Œé‚£ä¹ˆä½ å¯èƒ½æ˜¯Pandasåº“çš„å¿ å®ç²‰ä¸ï¼Œå¹¶ä¸”ä½ å¯èƒ½å·²ç»åœ¨é¢è¯•ä¸­æåˆ°è¿‡ã€‚å®é™…ä¸Šï¼Œè¿™ç§é—®é¢˜ä¸­ä½ ä¸å¸Œæœ›ä½¿ç”¨Pandasã€‚é—®é¢˜åœ¨äºPandaså¤„ç†å¤§æ•°æ®é›†æ—¶æ•ˆæœä¸å¥½ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®è½¬æ¢æ–¹é¢ã€‚ä½ åœ¨Pandasæ•°æ®æ¡†ä¸­è¿è¡Œæ•°æ®è½¬æ¢æ—¶ï¼Œæ€»æ˜¯ä¼šå—åˆ°æœºå™¨å†…å­˜çš„é™åˆ¶ã€‚
- en: The right answer would be to mention that if memory is limited then you would
    find a scalable solution for this task. This can be a simple Python **generator**
    and, yes, it can take a lot of time but at least it wonâ€™t fail.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£ç¡®çš„ç­”æ¡ˆåº”è¯¥æåˆ°ï¼Œå¦‚æœå†…å­˜æœ‰é™ï¼Œä½ ä¼šæ‰¾åˆ°ä¸€ä¸ªå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚è¿™å¯ä»¥æ˜¯ä¸€ä¸ªç®€å•çš„Python **ç”Ÿæˆå™¨**ï¼Œæ˜¯çš„ï¼Œå®ƒå¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œä½†è‡³å°‘ä¸ä¼šå¤±è´¥ã€‚
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The optimal answer should include transforming the data using distributed computing
    and ideally some tool that is fast for this purpose and scales well. Spark or
    HIVE-based tools might be a good choice.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä¼˜çš„ç­”æ¡ˆåº”è¯¥åŒ…æ‹¬ä½¿ç”¨åˆ†å¸ƒå¼è®¡ç®—æ¥è½¬æ¢æ•°æ®ï¼Œå¹¶ä¸”ç†æƒ³æƒ…å†µä¸‹ï¼Œä½¿ç”¨ä¸€äº›å¿«é€Ÿä¸”æ‰©å±•æ€§å¥½çš„å·¥å…·ã€‚Sparkæˆ–åŸºäºHIVEçš„å·¥å…·å¯èƒ½æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚
- en: When would you use Hadoop in your pipelines?
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ ä¼šåœ¨ç®¡é“ä¸­ä½•æ—¶ä½¿ç”¨Hadoopï¼Ÿ
- en: You would want to mention that Hadoop is an open-source Big Data processing
    framework developed by Apache Foundation and it brings all the benefits of distributed
    data processing. Thatâ€™s why it became so popular in data pipelines processing
    large volumes of data. It has its own intrinsic components that aim to ensure
    data quality (HDFS â€” Hadoop Distributed Data System) and scalability (MapReduce).
    Even if you donâ€™t have experience with Hadoop it should be enough just to mention
    these things as there are a lot of tools built on top of Apache Hadoop, i.e. **Apache
    Pig** (a programming platform that executes Hadoop jobs in MapReduce) or **Apache
    Hive** â€” a data warehouse project where we can use standard SQL dialect to process
    data stored in databases and file systems that integrate with Hadoop.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: How would you approach a big data migration project?
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'During the job interview, you might be asked this question as interviewers
    would want to understand your experience regarding data migration and approach
    to data validation when it is complete. Here I would recommend starting with business
    requirements. It might be cost-effectiveness, data governance or overall database
    performance. Depending on these requirements we can select the optimal solution
    as a destination point for our migration project. For example, if your current
    data platform is built on a data lake and there are a lot of business stakeholders
    who want to access the data then your choice should be between ANSI-SQL data warehouse
    solutions where we can offer better data governance and granular access controls.
    On the opposite, if our data warehouse solution has cost-effectiveness issues
    related to data storage then migrating or archiving to datalake might be a good
    option. I previously wrote about it here:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/geekculture/data-warehouse-dba-tasks-i-do-daily-32d9f823d799?source=post_page-----fdef62e46505--------------------------------)
    [## Data Warehouse DBA Tasks I Do Daily'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Activity And Managing Resources Like a Pro Or â€œMy tableâ€¦ Where has
    it gone?â€
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/geekculture/data-warehouse-dba-tasks-i-do-daily-32d9f823d799?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Once the migration is complete we would want to validate the data. Data consistency
    is the top priority for data engineers and you would want to demonstrate that
    you know how to validate that no data is lost when the migration is complete.
    For instance, we could calculate the total number of records per partition in
    the data warehouse and then compare it against the number of records in data lake
    partitions. `count(*)` is the least expensive operation but it is very effective
    for data validation and can be run fast. In fact in many DWH solutions `count(*)`
    is free.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Which ETL tools do you know and how is it different from ELT?
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Answering this question we would want to demonstrate that we know how to extract,
    transform and load the data not only with third-party tools but also by writing
    our own bespoke data connectors and loaders.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: You can start with a quick note that there are managed solutions like Fivetran,
    Stitch, etc. that help with ETL. Donâ€™t forget to mention their pricing models
    that often are based on the number of records processed.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: You donâ€™t need third-party ETL tools when you know how to code.
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Donâ€™t be shy about saying this phrase. It is fairly easy to create your own
    ETL tool and then load the data into the DWH solution of your choice. Consider
    one of my previous articles where I extract millions of rows of data from MySQL
    or Postgres databases as an example. It explains how to create a robust data connector
    and extract data in chunks in a memory-efficient manner [12]. Things like this
    were designed to be serverless and can be easily deployed and scheduled in the
    cloud.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[](/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c?source=post_page-----fdef62e46505--------------------------------)
    [## Building a Batch Data Pipeline with Athena and MySQL'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: An End-To-End Tutorial for Beginners
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: We can even create our own bespoke data loading manager if we need to prepare
    and transform data before loading it into the DWH destination using cloud SDKs.
    Itâ€™s a fairly complex application but itâ€™s worth learning it. Here is the tutorial.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: â€œWhat is your approach to â€¦?â€ type of question is very common in data engineering
    interviews. Be ready to answer these scenario questions. During the interview,
    you can be asked to design a pipeline or a data platform. Thatâ€™s all it is if
    you take a look at the wider picture. Every data platform has its own business
    and functional requirements â€” itâ€™s always good to put this phrase in and then
    mention that you would select data tools based on these requirements. Decorate
    your answers with data pipeline examples and you will most definitely pass.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Recommended read
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://medium.com/towards-data-science/python-for-data-engineers-f3d5db59b6dd](https://medium.com/towards-data-science/python-for-data-engineers-f3d5db59b6dd)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [https://towardsdatascience.com/modern-data-engineering-e202776fb9a9](/modern-data-engineering-e202776fb9a9)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [https://medium.com/towards-data-science/unit-tests-for-sql-scripts-with-dependencies-in-dataform-847133b803b7](https://medium.com/towards-data-science/unit-tests-for-sql-scripts-with-dependencies-in-dataform-847133b803b7)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
- en: '[6] [https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488](/advanced-sql-techniques-for-beginners-211851a28488)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488](/advanced-sql-techniques-for-beginners-211851a28488)'
- en: '[7] [https://cloud.google.com/bigquery/docs/user-defined-functions](https://cloud.google.com/bigquery/docs/user-defined-functions)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://cloud.google.com/bigquery/docs/user-defined-functions](https://cloud.google.com/bigquery/docs/user-defined-functions)'
- en: '[8] [https://spark.apache.org/](https://spark.apache.org/)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [https://spark.apache.org/](https://spark.apache.org/)'
- en: '[9] [https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a](https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a](https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a)'
- en: '[10] [https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2](/how-to-become-a-data-engineer-c0319cb226c2)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] [https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2](/how-to-become-a-data-engineer-c0319cb226c2)'
- en: '[11] [https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] [https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
- en: '[12] [https://medium.com/towards-data-science/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d](https://medium.com/towards-data-science/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] [https://medium.com/towards-data-science/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d](https://medium.com/towards-data-science/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d)'
