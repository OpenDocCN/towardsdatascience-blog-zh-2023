- en: Data Engineering Interview Questions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据工程面试问题
- en: 原文：[https://towardsdatascience.com/data-engineering-interview-questions-fdef62e46505](https://towardsdatascience.com/data-engineering-interview-questions-fdef62e46505)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-engineering-interview-questions-fdef62e46505](https://towardsdatascience.com/data-engineering-interview-questions-fdef62e46505)
- en: Tips to prepare for a job interview
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备面试的技巧
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----fdef62e46505--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----fdef62e46505--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fdef62e46505--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fdef62e46505--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----fdef62e46505--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----fdef62e46505--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----fdef62e46505--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fdef62e46505--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fdef62e46505--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----fdef62e46505--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fdef62e46505--------------------------------)
    ·20 min read·Nov 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fdef62e46505--------------------------------)
    ·20 min 阅读·2023年11月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a7bc9856e987821675426d58096e4630.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7bc9856e987821675426d58096e4630.png)'
- en: Photo by [Ignacio Amenábar](https://unsplash.com/@amenabarladrondeguevara?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Ignacio Amenábar](https://unsplash.com/@amenabarladrondeguevara?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This story aims to shed some light on various data engineering interview scenarios
    and typical discussions. It covers almost every question you might be asked and
    I hope it will be useful for beginner and intermediate-level data practitioners
    during the job interview preparation. Throughout my almost fifteen-year career
    in analytics and data engineering, I interviewed many people and now I would like
    to share my observations with you.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故事旨在揭示各种数据工程面试场景和典型讨论。它涵盖了你可能会被问到的几乎所有问题，希望对初学者和中级数据从业者在求职面试准备期间有所帮助。在我将近十五年的分析和数据工程职业生涯中，我面试了很多人，现在我想与大家分享我的观察。
- en: Are data engineering interviews tough? No, not really if you understand what
    you are dealing with. Many companies have tech blogs where they describe their
    stacks and the tech they use. I would recommend doing some research beforehand.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程面试难吗？不，如果你了解你在处理什么的话，并不难。许多公司都有技术博客，描述他们的技术栈和使用的技术。我建议提前做一些研究。
- en: Data engineering interviews are quite simple per se and the job is very rewarding.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据工程面试本身其实很简单，这项工作也非常有回报。
- en: The interviews are quite simple indeed as questions follow the same pattern
    typically. The number of **data platform types** [1] is limited to just four and
    that would define the answer helping you to pass. So if we know what we are engineering
    then it’s not a very big task to answer interview questions correctly.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 面试确实很简单，因为问题通常遵循相同的模式。**数据平台类型** [1] 的数量仅限于四种，这将定义答案，帮助你通过面试。所以，如果我们知道我们在工程什么，那么正确回答面试问题就不是一项很大的任务。
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----fdef62e46505--------------------------------)
    [## Data Platform Architecture Types'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----fdef62e46505--------------------------------)
    [## 数据平台架构类型'
- en: How well does it answer your business needs? Dilemma of a choice.
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它如何满足你的业务需求？选择的两难困境。
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----fdef62e46505--------------------------------)
- en: Data engineering (DE) interviews are easy to pass unless you are tasked with
    coding. This is a whole different story and usually, this would be the second
    part of the interview process. Below is my collection of DE interview questions
    and answers. Enjoy!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程（DE）面试很容易通过，除非你被要求编写代码。这通常是面试过程的第二部分。以下是我收集的数据工程面试问题及答案。希望你喜欢！
- en: What is your DE like on a day-to-day basis?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你日常的DE工作是怎样的？
- en: Usually hiring managers start the conversation with this simple question. Here
    we would want to demonstrate the abundance of enthusiasm and experience with various
    DE tools and frameworks. Provide some data pipeline examples to decorate your
    answer. It can be a couple of data pipelines you built or a full life cycle project
    with a data warehouse in the centre of this infrastructure. Don’t call it a tutorial.
    It is always better to say something like…
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通常招聘经理会用这个简单的问题开始谈话。在这里，我们希望展示对各种DE工具和框架的丰富热情和经验。提供一些数据管道的示例来装饰你的回答。可以是你构建的几个数据管道，或者以数据仓库为中心的全生命周期项目。不要称其为教程。最好说点像这样的话…
- en: “… a full-lifecycle project from requirements gathering to data pipeline design
    and go live.”
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “… 一个从需求收集到数据管道设计和上线的全生命周期项目。”
- en: 'It looks more professional and this is the impression you would want to create.
    Try to be concise but also be fluent in describing your day-to-day work. For example,
    you can say that you are a student, your main focus is data quality at the moment
    and you designed and built data pipelines to check data using row conditions in
    the first place before loading data into the data platform. Alternatively, you
    could mention that you know how to work with SDKs to load data into the data warehouse,
    etc. You can find some good examples in this article [2]:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来更专业，这也是你希望传达的印象。尽量简洁，但也要流利地描述你的日常工作。例如，你可以说你是一个学生，目前主要关注数据质量，并且你设计并构建了数据管道，以便在将数据加载到数据平台之前，首先使用行条件检查数据。或者，你可以提到你知道如何使用SDK将数据加载到数据仓库等。你可以在这篇文章中找到一些好的示例[2]：
- en: '[](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----fdef62e46505--------------------------------)
    [## Python for Data Engineers'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----fdef62e46505--------------------------------)
    [## 数据工程师的Python'
- en: Advanced ETL techniques for beginners
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面向初学者的高级ETL技术
- en: towardsdatascience.com](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----fdef62e46505--------------------------------)
- en: It is not very difficult. You can say that you have various data sources on
    the left-hand side and you can create data pipelines following this pattern below
    to integrate them into your data warehouse (DWH) solution.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不太困难。你可以说，你在左侧有各种数据源，可以按照下面的模式创建数据管道，将它们集成到你的数据仓库（DWH）解决方案中。
- en: '![](../Images/a533b28c5e039871007b44ee56caff97.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a533b28c5e039871007b44ee56caff97.png)'
- en: Data warehouse exmaple. Image by author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库示例。图片由作者提供
- en: 'Consider this example of data loading into a BigQuery data warehouse using
    Pandas and google.cloud libraries [3]:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑这个将数据加载到BigQuery数据仓库的示例，使用了Pandas和google.cloud库[3]：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How do you create data pipelines?
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你是如何创建数据管道的？
- en: You would want to make clear that you are confident working with both third-party
    ETL tools (Fivetran, Stitch, etc.) and bespoke data connectors you can write yourself.
    A data pipeline is something that extracts, transforms and/or loads data from
    point A into the destination at point B [4].
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要明确表示你对使用第三方ETL工具（如Fivetran、Stitch等）和自己编写的定制数据连接器都很有信心。数据管道是指将数据从A点提取、转换和/或加载到B点的过程[4]。
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----fdef62e46505--------------------------------)
    [## Data pipeline design patterns'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----fdef62e46505--------------------------------)
    [## 数据管道设计模式'
- en: Choosing the right architecture with examples
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的架构及示例
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----fdef62e46505--------------------------------)
- en: So all you need is to demonstrate that you know how to do it following three
    main data pipeline design patterns — **batch** (aggregate and process in chunks),
    **streaming** (process and load record by record), **change data capture** (CDC,
    identify and capture changes at point A to process and load into B).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你需要做的就是展示你知道如何按照三种主要数据管道设计模式来操作——**批处理**（分块汇总和处理）、**流处理**（逐条处理和加载记录）、**变更数据捕获**（CDC，在A点识别和捕获变化，以处理和加载到B点）。
- en: CDC and streaming are closely connected.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: CDC和流处理是紧密相关的。
- en: For example, we can use MySQL binary log file to move data into our DWH solution
    in real time. It must be used with care and is not always the most cost-effective
    tool for data pipelines but it is worth mentioning this. Keep everything in order
    following the conceptual design diagram. It helps to explain many ETL things.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用 MySQL 二进制日志文件实时将数据移动到我们的 DWH 解决方案中。它必须谨慎使用，并不总是数据管道中最具成本效益的工具，但值得一提。按照概念设计图保持一切有序。这有助于解释许多
    ETL 相关的事项。
- en: '![](../Images/f319a0d445798a12e7ff1aa554120f2a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f319a0d445798a12e7ff1aa554120f2a.png)'
- en: Conceptual data pipeline design. Image by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 概念数据管道设计。图像由作者提供
- en: What do you know about data platform design?
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你对数据平台设计了解多少？
- en: In a nutshell, there are four data platform architecture types that would define
    the selection of tools you might want to use while building a pipeline. This is
    the key to this question — it helps to choose the right DE tools and techniques.
    Data lakes, warehouses, and lake houses each have their benefits and serve each
    purpose. The fourth architecture type is **Data Mesh** where data management is
    decentralised. **Data Mesh** defines the state when we have different data domains
    (company departments) with their own teams and shared data resources. It might
    seem a bit more chaotic but many companies choose this model to reduce data bureaucracy.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，有四种数据平台架构类型，这将定义你在构建管道时可能想要使用的工具。这是问题的关键——它帮助选择合适的数据工程工具和技术。数据湖、数据仓库和数据湖屋各有其优点并服务于不同的目的。第四种架构类型是**数据网格**，其中数据管理是分散的。**数据网格**定义了当我们拥有不同的数据领域（公司部门）及其自身团队和共享数据资源时的状态。它可能看起来有些混乱，但许多公司选择这种模型来减少数据官僚主义。
- en: Typically data warehouses offer better data governance compared to data lakes.
    It makes the data stack look modern and flexible due to built-in ANSI-SQL capabilities.
    The shift to a lake or data warehouse would depend primarily on the skillset of
    your users. The Data warehouse solution will enable more interactivity and narrow
    down our choice to a SQL-first product (Snowflake, BigQuery, etc.).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，与数据湖相比，数据仓库提供更好的数据治理。由于内置 ANSI-SQL 能力，它使数据栈看起来现代且灵活。迁移到数据湖或数据仓库主要取决于用户的技能水平。数据仓库解决方案将提供更多的互动性，并将我们的选择缩小到
    SQL 优先的产品（如 Snowflake、BigQuery 等）。
- en: Data lakes are for users with programming skills and we would want to go for
    Python-first products like Databricks, Galaxy, Dataproc, EMR.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据湖适用于具备编程技能的用户，我们倾向于选择以 Python 为优先的产品，如 Databricks、Galaxy、Dataproc、EMR。
- en: What is data modeling?
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是数据建模？
- en: 'Data modelling is an essential part of data engineering as data is being transformed
    using relationships between entities (tables, views, silos, data lakes). You would
    want to demonstrate that you understand how this process works in terms of **the
    conceptual** and **physical** design process. We always start with the concept
    of creating a model for our business process or a data transformation task. Then
    it is followed by a functional model which is a prototype and it aims to prove
    that our conceptual model works for this task. In the end, we will create a physical
    model which contains the final infrastructure including all required physical
    entities and objects. It’s good to say that it doesn’t have to be SQL entities
    always. Conceptual data modelling might include all types of data platforms with
    semi-structured data files in the cloud storage. A good example would be a scenario
    when we need to prepare data in the data lake first and then use it to train the
    machine learning (ML) model. I previously wrote about it in this story:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据建模是数据工程的一个重要部分，因为数据是通过实体（表、视图、孤岛、数据湖）之间的关系进行转化的。你需要展示你对**概念**和**物理**设计过程的理解。我们总是从为我们的业务流程或数据转化任务创建一个模型的概念开始。然后是功能模型，即原型，旨在证明我们的概念模型适用于该任务。最后，我们将创建一个物理模型，其中包含最终的基础设施，包括所有所需的物理实体和对象。值得一提的是，这并不总是
    SQL 实体。概念数据建模可能包括所有类型的数据平台以及云存储中的半结构化数据文件。一个好的例子是，当我们需要先在数据湖中准备数据，然后用它来训练机器学习（ML）模型时。我之前在这篇文章中写过：
- en: '[](https://pub.towardsai.net/orchestrate-machine-learning-pipelines-with-aws-step-functions-d8216a899bd5?source=post_page-----fdef62e46505--------------------------------)
    [## Orchestrate Machine Learning Pipelines with AWS Step Functions'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pub.towardsai.net/orchestrate-machine-learning-pipelines-with-aws-step-functions-d8216a899bd5?source=post_page-----fdef62e46505--------------------------------)
    [## 使用 AWS Step Functions 编排机器学习管道]'
- en: Advanced-Data Engineering and ML Ops with Infrastructure as Code
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级数据工程和基础设施即代码的机器学习操作
- en: pub.towardsai.net](https://pub.towardsai.net/orchestrate-machine-learning-pipelines-with-aws-step-functions-d8216a899bd5?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/orchestrate-machine-learning-pipelines-with-aws-step-functions-d8216a899bd5?source=post_page-----fdef62e46505--------------------------------)'
- en: 'It’s always good to mention that you are familiar with **templating engines**
    such as **DBT** and **Dataform** that can be used for this task. Why? It helps
    a lot with data transformation **unit tests** [4] and data environments [5], prevents
    human errors and provides better deployment workflows. I previously wrote about
    it here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 提到你熟悉**模板引擎**如**DBT**和**Dataform**是很好的，这些工具可以用于这项任务。为什么？它们对于数据转换**单元测试**[4]和数据环境[5]非常有帮助，防止人为错误，并提供更好的部署工作流。我之前在这里写过：
- en: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)
    [## Continuous Integration and Deployment for Data Platforms'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)
    [## 数据平台的持续集成和部署'
- en: CI/CD for data engineers and ML Ops
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据工程师和机器学习操作的CI/CD
- en: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)'
- en: What is the difference between Star and Snowflake schema?
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 星型模式和雪花模式有什么区别？
- en: Very often job interviewers test your knowledge of data engineering design schemas.
    Try to be concise and say that Star schema is where we can take advantage of super
    large denormalised datasets connected to one fact table. That’s why it’s a Star
    database design pattern as it looks like a star. This is more suitable for data
    warehouse OLAP-style analytics pipelines. Data in those datasets is not always
    up-to-date but that’s fine because we need it to be materialised this way and
    we can update the required fields if needed.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 求职面试官很常测试你的数据工程设计模式知识。尽量简明扼要地说明星型模式是我们可以利用超级大型非规范化数据集与一个事实表连接的地方。这就是为什么它被称为星型数据库设计模式，因为它看起来像一颗星。这更适用于数据仓库的OLAP风格分析管道。这些数据集中的数据并不总是最新的，但没关系，因为我们需要以这种方式物化数据，如果需要，我们可以更新所需的字段。
- en: Opposite to a Star schema Snowflake schema design has the same fact table in
    the center but it is linked with many other fact and dimension tables which are
    typically **denormalised.** This schema design is more suitable for OLTP data
    processing when data needs to be always up-to-date and individual rows can be
    pulled fast to use in the application.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与星型模式相对，雪花模式设计的中心是相同的事实表，但它与许多其他事实表和维度表连接，这些表通常是**非规范化的**。这种模式设计更适合于OLTP数据处理，当数据需要始终保持最新状态时，能够快速提取单独的行以供应用程序使用。
- en: On a scale from 1 to 10 how good are your SQL skills?
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在1到10的范围内，你的SQL技能有多好？
- en: 'Make sure you can explain your answer. SQL is a natural dialect to model data
    transformation and create analytics datasets. Working confidently with incremental
    table updates gives you 6 out of 10 straight away. Consider this example below.
    It creates an **incremental** table using MERGE:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你能解释你的答案。SQL是一种自然的方言，用于建模数据转换和创建分析数据集。自信地处理增量表更新可以直接给你6分。请考虑下面的例子。它使用MERGE创建一个**增量**表：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'I wrote about advanced techniques before. I think it’s a good place to start
    the preparation [6]:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前写过高级技巧。我认为这是开始准备的好地方[6]：
- en: '[](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----fdef62e46505--------------------------------)
    [## Advanced SQL techniques for beginners'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----fdef62e46505--------------------------------)
    [## 高级SQL技巧入门'
- en: On a scale from 1 to 10 how good are your data warehousing skills?
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在1到10的范围内，你的数据仓库技能有多好？
- en: towardsdatascience.com](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----fdef62e46505--------------------------------)'
- en: Running SQL unit tests for data transformation scripts and working with custom
    user-defined functions (UDF) [7] would grant you 9 out of 10.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据转换脚本运行SQL单元测试和处理自定义用户定义函数（UDF）[7]将使你获得9分。
- en: How do I get 10 out of 10 in SQL?
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在SQL中获得10分？
- en: It would be something very tricky and obviously related to your expert knowledge
    of a particular tool, i.e. converting a table into **an array of structs** and
    passing them to UDF.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是非常棘手的，显然与对特定工具的专业知识相关，即将表转换为**结构体数组**并将其传递给 UDF。
- en: This is useful when you need to apply a user-defined function (UDF) with some
    complex logic to each row or table.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当你需要对每一行或表应用具有复杂逻辑的用户定义函数（UDF）时，这很有用。
- en: 'You can always consider your table as an array of TYPE STRUCT objects and then
    pass each one of them to UDF. It depends on your logic. For example, I use it
    in purchase stacking to calculate expire times:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将你的表视为一组 TYPE STRUCT 对象，然后将每个对象传递给 UDF。这取决于你的逻辑。例如，我在购买堆叠中使用它来计算过期时间：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: What is the difference between OLAP and OLTP?
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OLAP 和 OLTP 的区别是什么？
- en: Online analytical processing (OLAP) and Online transactional processing (OLTP)
    are data processing systems designed for completely different purposes. OLAP aims
    to aggregate and store the data for analytical purposes such as reporting and
    large-scale data processing, That’s why denormalised super big tables are seen
    very often here. OLTP processing is different in the way we process data — it
    would have a single transaction focus and require lightning-fast data processing.
    Good examples are in-app purchases, managing user accounts and updating store
    content. Data for OLTP is stored in indexed tables connected using the Snowflake
    pattern where dimension tables are mostly normalised.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在线分析处理（OLAP）和在线事务处理（OLTP）是为完全不同目的设计的数据处理系统。OLAP 旨在聚合和存储数据，以便用于分析，如报告和大规模数据处理，这也是为什么在这里经常可以看到非规范化的超大表。OLTP
    处理则不同，它专注于单个事务，并要求数据处理速度非常快。好的例子包括应用内购买、管理用户账户和更新商店内容。OLTP 的数据存储在使用雪花模式连接的索引表中，其中维度表大多是规范化的。
- en: What data engineering frameworks do you know?
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你知道哪些数据工程框架？
- en: 'We can’t know everything. I interviewed a lot of people and it’s not necessary
    to have experience with all data engineering tools and frameworks. You can name
    a few: **Python ETL (PETL), Bonobo, Apache Airflow, Bubbles, Kestra, Luigi** and
    I previously wrote about the ETL frameworks explosion we witnessed during the
    past couple of years.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能知道所有事情。我面试了很多人，拥有所有数据工程工具和框架的经验并不是必要的。你可以列举一些：**Python ETL (PETL)、Bonobo、Apache
    Airflow、Bubbles、Kestra、Luigi**，我之前写过关于过去几年我们见证的 ETL 框架爆炸的文章。
- en: '[](/modern-data-engineering-e202776fb9a9?source=post_page-----fdef62e46505--------------------------------)
    [## Modern Data Engineering'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/modern-data-engineering-e202776fb9a9?source=post_page-----fdef62e46505--------------------------------)
    [## 现代数据工程'
- en: Platform Specific Tools and Advanced Techniques
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平台特定工具和高级技术
- en: towardsdatascience.com](/modern-data-engineering-e202776fb9a9?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/modern-data-engineering-e202776fb9a9?source=post_page-----fdef62e46505--------------------------------)
- en: We don’t need to be super experienced with all frameworks but demonstrating
    confidence is a must.
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们不需要对所有框架都非常熟悉，但展示自信是必须的。
- en: 'In order to demonstrate confidence with various data tools we would want to
    learn at least one or two and then use the basic principles (data engineering
    principles). Using this approach we can answer almost every DE question:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示对各种数据工具的自信，我们应该至少学习一两个，然后使用基本原则（数据工程原则）。采用这种方法，我们可以回答几乎所有数据工程问题：
- en: Why did you do it this way? — I got this from basic principles.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你为什么这样做？——我从基本原则中得到了这个。
- en: Having said this it would be just fine to learn a few things from Apache Airflow
    and demonstrate it with a simple pipeline example. For example, we can run ml_engine_training_op
    after we export data into the cloud storage (bq_export_op) and make this workflow
    run daily or weekly.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这，学习 Apache Airflow 的一些内容并通过简单的管道示例来展示它是完全可以的。例如，我们可以在将数据导出到云存储（bq_export_op）后运行
    ml_engine_training_op，并使这个工作流每天或每周运行一次。
- en: '![](../Images/9d08e679e0eaf09488813ce519b4a635.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d08e679e0eaf09488813ce519b4a635.png)'
- en: ML model training using Airflow. Image by author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Airflow 进行 ML 模型训练。图片由作者提供。
- en: Consider this example below.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请看下面的这个例子。
- en: '*It creates a simple data pipeline graph to export data into a cloud storage
    bucket and then trains the ML model using MLEngineTrainingOperator.*'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*它创建一个简单的数据管道图，将数据导出到云存储桶中，然后使用 MLEngineTrainingOperator 训练 ML 模型。*'
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: What would you use to orchestrate your data pipelines?
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你会使用什么来协调你的数据管道？
- en: 'It is important to differentiate the ETL frameworks we can use for data transformation
    and the frameworks we use to orchestrate our data pipelines. You can mention a
    few: **Airflow, Prefect, Dagster, Kestra, Argo, Luigi**. These are the most popular
    ones at the moment. These are open-source projects free to use. However, a good
    answer should indicate that you are capable of performing data pipeline orchestration
    using your own bespoke tools. If you like AWS you can deploy and orchestrate data
    pipelines using CloudFormation (Infrastructure as code) and Step Functions. I
    previously wrote about it here [9]:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 区分用于数据转换的 ETL 框架和用于编排数据管道的框架非常重要。你可以提到一些：**Airflow、Prefect、Dagster、Kestra、Argo、Luigi**。这些是目前最受欢迎的开源项目，可以免费使用。然而，一个好的回答应该表明你能够使用自己的定制工具进行数据管道编排。如果你喜欢
    AWS，你可以使用 CloudFormation（基础设施即代码）和 Step Functions 部署和编排数据管道。我之前在这里写过关于它的内容[9]：
- en: '[](/data-pipeline-orchestration-9887e1b5eb7a?source=post_page-----fdef62e46505--------------------------------)
    [## Data Pipeline Orchestration'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-pipeline-orchestration-9887e1b5eb7a?source=post_page-----fdef62e46505--------------------------------)
    [## 数据管道编排'
- en: Data pipeline management done right simplifies deployment and increases the
    availability and accessibility of data for…
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正确进行数据管道管理可以简化部署，并提高数据的可用性和可访问性...
- en: towardsdatascience.com](/data-pipeline-orchestration-9887e1b5eb7a?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/data-pipeline-orchestration-9887e1b5eb7a?source=post_page-----fdef62e46505--------------------------------)'
- en: In fact, we don’t even need Step Functions here as it would be a very platform-specific
    choice. We could use platform-agnostic Terraform (Infrastructure as code) and
    Serverless to deploy microservices with required data pipelines orchestrating
    logic.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们在这里甚至不需要 Step Functions，因为这将是一个非常平台特定的选择。我们可以使用平台无关的 Terraform（基础设施即代码）和
    Serverless 部署带有所需数据管道编排逻辑的微服务。
- en: What is your programming language?
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你的编程语言是什么？
- en: The answer to this question depends on the company stack. Long story short,
    you won’t miss it if you answer Python. This one is a coding absolute in DE and
    data science because of its simplicity and the numerous libraries and open-source
    data tools available in the market.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个问题的回答取决于公司技术栈。长话短说，如果你回答 Python，你不会错。这在 DE 和数据科学中是绝对必要的，因为它的简单性以及市场上众多的库和开源数据工具。
- en: However, don’t limit yourself with to Python.
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不过，不要仅仅局限于 Python。
- en: It is always good to be familiar with other languages, i.e. JAVA, JavaScript,
    Scala, Spark and R. R for example is good for data science and is very popular
    among scholars and universities. It is always good to mention Spark. It’s not
    a language (framework) but it became very popular due to its great scalability
    and capabilities for large-scale data processing [8].
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉其他语言总是有益的，例如 JAVA、JavaScript、Scala、Spark 和 R。例如，R 在数据科学中表现优异，并且在学术界和大学中非常受欢迎。提到
    Spark 也是有益的。它不是一种语言（框架），但由于其卓越的可扩展性和处理大规模数据的能力，它变得非常流行[8]。
- en: You might not know Spark but if you know Python then you can always use a Spark
    API connector (PySpark).
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可能不知道 Spark，但如果你知道 Python，那么你可以使用 Spark API 连接器（PySpark）。
- en: What is *args and **kwargs?
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 *args 和 **kwargs？
- en: 'Typically, it would be the next one if you named Python in the previous question.
    Answering a question about function arguments is the most common one I ask during
    job interviews. You would want to be ready to answer it and maybe even impress
    your interviewer with a few lines of code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果你在前一个问题中提到 Python，下一步就是这个。回答关于函数参数的问题是我在面试中最常问的问题之一。你需要准备好回答它，也许还会用几行代码给面试官留下深刻印象：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How good are you with CLI tools and shell scripting?
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你对 CLI 工具和 shell 脚本的掌握如何？
- en: 'Cloud vendor command-line tools are based on REST API and enable data engineers
    with a powerful command-line interface to communicate with cloud services endpoints
    to describe and modify resources. Data engineers use CLI tools with bash scripting
    to chain commands. It helps to create powerful scripts and interact with cloud
    services with ease. Consider this example below. It will invoke the AWS Lambda
    function called pipeline-manager:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 云供应商的命令行工具基于 REST API，使数据工程师能够通过强大的命令行界面与云服务端点进行通信，以描述和修改资源。数据工程师使用 CLI 工具与
    bash 脚本链式命令。这有助于创建强大的脚本，并轻松与云服务进行交互。考虑下面的示例。它将调用名为 pipeline-manager 的 AWS Lambda
    函数：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can create something even more powerful to deploy our serverless microservices.
    Consider this example below. It will check if the storage bucket for the lambda
    package exists, upload and deploy our ETL service as a Lambda Function [10]:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一些更强大的工具来部署我们的无服务器微服务。考虑下面这个例子。它将检查lambda包的存储桶是否存在，上传并部署我们的ETL服务作为一个Lambda函数[10]：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How do you deploy your data pipelines?
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你如何部署你的数据管道？
- en: There are no right or wrong answers but if you say “ I manually create pipeline
    steps and then deploy them in the cloud using vendor’s console…” that wouldn’t
    be the best answer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 没有对错之分，但如果你说“我手动创建管道步骤，然后在云中使用供应商的控制台进行部署……”那就不是最佳答案。
- en: Now the good answer would be to mention scripts. This tells the interviewer
    that you are an intermediate user familiar with shell scripting at a minimum.
    You would want to say that whatever you deploy, can be deployed using bash scripts
    and CLI tools. All major cloud vendors have their command line tools and you would
    want to be at least familiar with one of them.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，好的答案是提到脚本。这告诉面试官你是一个中级用户，至少熟悉Shell脚本。你会想说，无论你部署什么，都可以使用bash脚本和CLI工具进行部署。所有主要的云供应商都有他们的命令行工具，你至少要对其中一种工具有所了解。
- en: The optimal way which is often considered as best practice is to deploy your
    pipelines using Infrastructure as code and CI/CD tools [11].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最优的方法，通常被认为是最佳实践，是使用基础设施即代码和CI/CD工具来部署你的管道[11]。
- en: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)
    [## Continuous Integration and Deployment for Data Platforms'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)
    [## 数据平台的持续集成和部署'
- en: CI/CD for data engineers and ML Ops
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据工程师和机器学习运维的CI/CD
- en: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----fdef62e46505--------------------------------)'
- en: How good are you with Data Science?
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你在数据科学方面的水平如何？
- en: As a data engineer you don’t need to know all the intricacies of data science
    model training and hypertuning but remember that a good data scientist must be
    a good data engineer. Doesn’t have to be vice versa but it is always good to demonstrate
    at least some knowledge of basic data science algorithms. For example, you can
    mention that you know how to create linear and logistic regression models. One
    creates quantitative output (a predicted number) when the other one returns a
    simple answer — “yes” or “no” (1/0). In fact, all major data science models can
    be easily trained using SQL inside your data warehouse solution.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据工程师，你不需要了解数据科学模型训练和超参数调优的所有细节，但记住一个好的数据科学家必须是一个好的数据工程师。不必完全相反，但展示至少一些基本的数据科学算法知识总是有益的。例如，你可以提到你知道如何创建线性回归和逻辑回归模型。线性回归生成定量输出（预测的数字），而逻辑回归则返回简单的答案——“是”或“否”（1/0）。事实上，所有主要的数据科学模型都可以在你的数据仓库解决方案中使用SQL轻松训练。
- en: Let’s imagine our use case is churn prediction.
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们假设我们的用例是客户流失预测。
- en: 'Consider **BigQuery ML** where we can create a logistic regression like so:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以**BigQuery ML**为例，我们可以像这样创建一个逻辑回归模型：
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: What do you know about data quality and data reliability?
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你对数据质量和数据可靠性了解多少？
- en: This is always a good question because you might be asked about possible ways
    to ensure data quality in your data platform. It is one of the data engineer’s
    daily routine jobs to improve data pipelines in terms of data accuracy. Data engineers
    connect data sources and deploy pipelines where data must be extracted and then
    very often it has to be transformed according to business requirements.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的问题，因为你可能会被问及在数据平台中确保数据质量的可能方法。这是数据工程师日常工作的一部分，涉及提高数据管道的数据准确性。数据工程师连接数据源并部署管道，其中数据必须被提取，然后根据业务需求经常需要进行转换。
- en: We would want to make sure that all required fields exist (data quality) and
    no data is missing (reliability).
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们需要确保所有必需的字段存在（数据质量）且没有数据丢失（可靠性）。
- en: How do we do it? It’s always good to mention self-fixing pipelines and that
    you know how to deploy them. Data engineers can deploy data quality pipelines
    in a similar way they deploy ETL pipelines. To put it simply, you would want to
    use row conditions for one dataset and based on the outcome deploy a fixing step,
    i.e. extract missing data and load it.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何做到这一点？总是提到自修复管道是好的，并且你知道如何部署它们。数据工程师可以以类似于部署ETL管道的方式部署数据质量管道。简单来说，你需要为一个数据集使用行条件，并根据结果部署修复步骤，即提取缺失的数据并加载它。
- en: Using row conditions for your datasets aims to ensure data quality.
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用行条件来确保数据集的数据质量。
- en: All data quality checks can be scheduled as scripts and if any of them fail
    to meet certain conditions then we can send an email notification. It’s worth
    saying that modern data warehouse solutions allow SQL scripts to do such checks
    but it doesn’t have to be limited to SQL. Any data check script can be run on
    data in the data lake or anywhere else. It just depends on the type of our data
    platform. Good coding skills are a must in this case so we would want to demonstrate
    that we know how to create a simple patrol application that can scan our data
    depending on where it is located physically.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的数据质量检查都可以作为脚本进行调度，如果任何一个检查未能满足特定条件，我们可以发送电子邮件通知。值得一提的是，现代数据仓库解决方案允许使用SQL脚本进行这些检查，但这不一定要局限于SQL。任何数据检查脚本都可以在数据湖或其他地方的数据上运行。这取决于我们数据平台的类型。在这种情况下，良好的编码技能是必需的，因此我们需要展示我们知道如何创建一个简单的巡检应用程序，可以根据数据的实际位置扫描数据。
- en: The SQL-based answer is also good but it would be more suitable for the Data
    Developer role as SQL is often considered the main data querying dialect in analytics.
    Consider this example below. It will use SQL with row conditions to check if there
    are any records with NULL `payment_date`. It will also check for duplicates.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 基于SQL的答案也很好，但它更适合数据开发人员角色，因为SQL通常被认为是分析中的主要数据查询方言。考虑下面的例子。它将使用SQL和行条件来检查是否有NULL
    `payment_date`的记录。它还会检查重复记录。
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As a result BigQuery will send an automated email containing the alert:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是BigQuery将发送包含警报的自动电子邮件：
- en: '![](../Images/333076df44ae7cc144559f8d8999a013.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/333076df44ae7cc144559f8d8999a013.png)'
- en: Email alert. Image by author.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 邮件警报。图像由作者提供。
- en: What algorithm would you use to extract or process a very large dataset?
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你会使用什么算法来提取或处理非常大的数据集？
- en: This question might be a trap if you had previous questions about data transformation
    with Python. If you like Python then you are probably a big fan of the Pandas
    library and you probably already mentioned this during the interview. Well, this
    is the kind of question where you wouldn’t want to use Pandas. The thing is that
    Pandas doesn’t work with big datasets very well, especially with data transformation.
    You will always be limited to your machine’s memory while running data transformations
    in the Pandas data frame.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前有关于使用Python进行数据转换的问题，这个问题可能是个陷阱。如果你喜欢Python，那么你可能是Pandas库的忠实粉丝，并且你可能已经在面试中提到过。实际上，这种问题中你不希望使用Pandas。问题在于Pandas处理大数据集时效果不好，尤其是在数据转换方面。你在Pandas数据框中运行数据转换时，总是会受到机器内存的限制。
- en: The right answer would be to mention that if memory is limited then you would
    find a scalable solution for this task. This can be a simple Python **generator**
    and, yes, it can take a lot of time but at least it won’t fail.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的答案应该提到，如果内存有限，你会找到一个可扩展的解决方案。这可以是一个简单的Python **生成器**，是的，它可能需要很长时间，但至少不会失败。
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The optimal answer should include transforming the data using distributed computing
    and ideally some tool that is fast for this purpose and scales well. Spark or
    HIVE-based tools might be a good choice.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最优的答案应该包括使用分布式计算来转换数据，并且理想情况下，使用一些快速且扩展性好的工具。Spark或基于HIVE的工具可能是一个不错的选择。
- en: When would you use Hadoop in your pipelines?
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你会在管道中何时使用Hadoop？
- en: You would want to mention that Hadoop is an open-source Big Data processing
    framework developed by Apache Foundation and it brings all the benefits of distributed
    data processing. That’s why it became so popular in data pipelines processing
    large volumes of data. It has its own intrinsic components that aim to ensure
    data quality (HDFS — Hadoop Distributed Data System) and scalability (MapReduce).
    Even if you don’t have experience with Hadoop it should be enough just to mention
    these things as there are a lot of tools built on top of Apache Hadoop, i.e. **Apache
    Pig** (a programming platform that executes Hadoop jobs in MapReduce) or **Apache
    Hive** — a data warehouse project where we can use standard SQL dialect to process
    data stored in databases and file systems that integrate with Hadoop.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: How would you approach a big data migration project?
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'During the job interview, you might be asked this question as interviewers
    would want to understand your experience regarding data migration and approach
    to data validation when it is complete. Here I would recommend starting with business
    requirements. It might be cost-effectiveness, data governance or overall database
    performance. Depending on these requirements we can select the optimal solution
    as a destination point for our migration project. For example, if your current
    data platform is built on a data lake and there are a lot of business stakeholders
    who want to access the data then your choice should be between ANSI-SQL data warehouse
    solutions where we can offer better data governance and granular access controls.
    On the opposite, if our data warehouse solution has cost-effectiveness issues
    related to data storage then migrating or archiving to datalake might be a good
    option. I previously wrote about it here:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/geekculture/data-warehouse-dba-tasks-i-do-daily-32d9f823d799?source=post_page-----fdef62e46505--------------------------------)
    [## Data Warehouse DBA Tasks I Do Daily'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Activity And Managing Resources Like a Pro Or “My table… Where has
    it gone?”
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/geekculture/data-warehouse-dba-tasks-i-do-daily-32d9f823d799?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Once the migration is complete we would want to validate the data. Data consistency
    is the top priority for data engineers and you would want to demonstrate that
    you know how to validate that no data is lost when the migration is complete.
    For instance, we could calculate the total number of records per partition in
    the data warehouse and then compare it against the number of records in data lake
    partitions. `count(*)` is the least expensive operation but it is very effective
    for data validation and can be run fast. In fact in many DWH solutions `count(*)`
    is free.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Which ETL tools do you know and how is it different from ELT?
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Answering this question we would want to demonstrate that we know how to extract,
    transform and load the data not only with third-party tools but also by writing
    our own bespoke data connectors and loaders.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这个问题时，我们想要展示的是我们不仅知道如何使用第三方工具提取、转换和加载数据，还能通过编写自己的定制数据连接器和加载器来完成。
- en: You can start with a quick note that there are managed solutions like Fivetran,
    Stitch, etc. that help with ETL. Don’t forget to mention their pricing models
    that often are based on the number of records processed.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以先简单提到一些托管解决方案，比如 Fivetran、Stitch 等，它们有助于 ETL。别忘了提到它们的定价模型，通常是基于处理的记录数量。
- en: You don’t need third-party ETL tools when you know how to code.
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当你知道如何编码时，你不需要第三方 ETL 工具。
- en: Don’t be shy about saying this phrase. It is fairly easy to create your own
    ETL tool and then load the data into the DWH solution of your choice. Consider
    one of my previous articles where I extract millions of rows of data from MySQL
    or Postgres databases as an example. It explains how to create a robust data connector
    and extract data in chunks in a memory-efficient manner [12]. Things like this
    were designed to be serverless and can be easily deployed and scheduled in the
    cloud.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 不要害怕说出这句话。创建自己的 ETL 工具然后将数据加载到你选择的 DWH 解决方案中是相当简单的。考虑我之前的一篇文章，其中我从 MySQL 或 Postgres
    数据库中提取了数百万行数据作为例子。它解释了如何创建一个强大的数据连接器并以内存高效的方式分块提取数据 [12]。这类东西设计成无服务器的，可以轻松地在云中部署和调度。
- en: '[](/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c?source=post_page-----fdef62e46505--------------------------------)
    [## Building a Batch Data Pipeline with Athena and MySQL'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c?source=post_page-----fdef62e46505--------------------------------)
    [## 使用 Athena 和 MySQL 构建批量数据管道'
- en: An End-To-End Tutorial for Beginners
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初学者的端到端教程
- en: towardsdatascience.com](/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c?source=post_page-----fdef62e46505--------------------------------)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c?source=post_page-----fdef62e46505--------------------------------)
- en: We can even create our own bespoke data loading manager if we need to prepare
    and transform data before loading it into the DWH destination using cloud SDKs.
    It’s a fairly complex application but it’s worth learning it. Here is the tutorial.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要在将数据加载到 DWH 目标之前进行准备和转换，我们甚至可以创建自己的定制数据加载管理器。这是一个相当复杂的应用，但值得学习。这里是教程。
- en: Conclusion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: “What is your approach to …?” type of question is very common in data engineering
    interviews. Be ready to answer these scenario questions. During the interview,
    you can be asked to design a pipeline or a data platform. That’s all it is if
    you take a look at the wider picture. Every data platform has its own business
    and functional requirements — it’s always good to put this phrase in and then
    mention that you would select data tools based on these requirements. Decorate
    your answers with data pipeline examples and you will most definitely pass.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: “你对……的处理方法是什么？”这类问题在数据工程面试中非常常见。准备好回答这些场景问题。在面试中，你可能会被要求设计一个管道或数据平台。如果你从更广泛的角度看待问题，就是这样。每个数据平台都有其业务和功能需求——总是好在回答中提到这一点，然后提到你会根据这些需求选择数据工具。用数据管道的例子来装饰你的答案，你一定会通过面试。
- en: Recommended read
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐阅读
- en: '[1] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
- en: '[2] [https://medium.com/towards-data-science/python-for-data-engineers-f3d5db59b6dd](https://medium.com/towards-data-science/python-for-data-engineers-f3d5db59b6dd)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://medium.com/towards-data-science/python-for-data-engineers-f3d5db59b6dd](https://medium.com/towards-data-science/python-for-data-engineers-f3d5db59b6dd)'
- en: '[3] [https://towardsdatascience.com/modern-data-engineering-e202776fb9a9](/modern-data-engineering-e202776fb9a9)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://towardsdatascience.com/modern-data-engineering-e202776fb9a9](/modern-data-engineering-e202776fb9a9)'
- en: '[4] [https://medium.com/towards-data-science/unit-tests-for-sql-scripts-with-dependencies-in-dataform-847133b803b7](https://medium.com/towards-data-science/unit-tests-for-sql-scripts-with-dependencies-in-dataform-847133b803b7)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://medium.com/towards-data-science/unit-tests-for-sql-scripts-with-dependencies-in-dataform-847133b803b7](https://medium.com/towards-data-science/unit-tests-for-sql-scripts-with-dependencies-in-dataform-847133b803b7)'
- en: '[5] [https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
- en: '[6] [https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488](/advanced-sql-techniques-for-beginners-211851a28488)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488](/advanced-sql-techniques-for-beginners-211851a28488)'
- en: '[7] [https://cloud.google.com/bigquery/docs/user-defined-functions](https://cloud.google.com/bigquery/docs/user-defined-functions)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://cloud.google.com/bigquery/docs/user-defined-functions](https://cloud.google.com/bigquery/docs/user-defined-functions)'
- en: '[8] [https://spark.apache.org/](https://spark.apache.org/)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [https://spark.apache.org/](https://spark.apache.org/)'
- en: '[9] [https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a](https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a](https://medium.com/towards-data-science/data-pipeline-orchestration-9887e1b5eb7a)'
- en: '[10] [https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2](/how-to-become-a-data-engineer-c0319cb226c2)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] [https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2](/how-to-become-a-data-engineer-c0319cb226c2)'
- en: '[11] [https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] [https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](https://medium.com/towards-data-science/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
- en: '[12] [https://medium.com/towards-data-science/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d](https://medium.com/towards-data-science/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] [https://medium.com/towards-data-science/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d](https://medium.com/towards-data-science/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d)'
