["```py\npip install bertopic datasets\n```", "```py\nfrom datasets import load_dataset\n\nfrom bertopic import BERTopic\nfrom bertopic.representation import KeyBERTInspired\n\n# We select a subsample of 5000 abstracts from ArXiv\ndataset = load_dataset(\"CShorten/ML-ArXiv-Papers\")[\"train\"]\ndocs = dataset[\"abstract\"][:5_000]\n\n# We define a number of topics that we know are in the documents\nzeroshot_topic_list = [\"Clustering\", \"Topic Modeling\", \"Large Language Models\"]\n\n# We fit our model using the zero-shot topics\n# and we define a minimum similarity. For each document,\n# if the similarity does not exceed that value, it will be used\n# for clustering instead.\ntopic_model = BERTopic(\n    embedding_model=\"thenlper/gte-small\", \n    min_topic_size=15,\n    zeroshot_topic_list=zeroshot_topic_list,\n    zeroshot_min_similarity=.85,\n    representation_model=KeyBERTInspired()\n)\ntopics, probs = topic_model.fit_transform(docs)\n```", "```py\ntopic_model.get_topic_info()\n```", "```py\nfrom bertopic import BERTopic\n\n# Merge 3 pre-trained BERTopic models\nmerged_model = BERTopic.merge_models(\n    [topic_model_1, topic_model_2, topic_model_3]\n)\n```", "```py\nfrom bertopic import BERTopic\nfrom bertopic.representation import KeyBERTInspired\nfrom datasets import load_dataset\n\n# Prepare documents\nall_docs = load_dataset(\"CShorten/ML-ArXiv-Papers\")[\"train\"][\"abstract\"][:20_000]\ndoc_chunks = [all_docs[i:i+5000] for i in range(0, len(all_docs), 5000)]\n\n# Base Model\nrepresentation_model = KeyBERTInspired()\nbase_model = BERTopic(representation_model=representation_model, min_topic_size=15).fit(doc_chunks[0])\n\n# Iteratively add small and newly trained models\nfor docs in doc_chunks[1:]:\n    new_model = BERTopic(representation_model=representation_model, min_topic_size=15).fit(docs)\n    updated_model = BERTopic.merge_models([base_model, new_model])\n\n    # Let's print the newly discover topics\n    nr_new_topics = len(set(updated_model.topics_)) - len(set(base_model.topics_))\n    new_topics = list(updated_model.topic_labels_.values())[-nr_new_topics:]\n    print(\"The following topics are newly found:\")\n    print(f\"{new_topics}\\n\")\n\n    # Update the base model\n    base_model = updated_model\n```", "```py\npip install llama-cpp-python\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\nwget https://huggingface.co/TheBloke/zephyr-7B-alpha-GGUF/resolve/main/zephyr-7b-alpha.Q4_K_M.gguf\n```", "```py\nfrom bertopic import BERTopic\nfrom bertopic.representation import LlamaCPP\n\n# Use llama.cpp to load in a 4-bit quantized version of Zephyr 7B Alpha\n# and truncate each document to 50 words\nrepresentation_model = LlamaCPP(\n    \"zephyr-7b-alpha.Q4_K_M.gguf\",\n    tokenizer=\"whitespace\",\n    doc_length=50\n)\n\n# Create our BERTopic model\ntopic_model = BERTopic(representation_model=representation_model, verbose=True)\n```"]