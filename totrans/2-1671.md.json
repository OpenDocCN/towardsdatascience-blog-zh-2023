["```py\npip install spacy\n```", "```py\npython -m spacy download en_core_web_sm\n```", "```py\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndoc = nlp(\"This is a sentence.\")\n```", "```py\nfor token in doc: \n  print(token.text)\n```", "```py\nThis \nis \na \nsentence\n```", "```py\nfor token in doc: \n  print(token.text, token.pos_)\n```", "```py\nThis DET \nis VERB \na DET \nsentence NOUN\n```", "```py\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n```", "```py\nfor token in doc:\n    print(token.text, token.dep_, token.head.text)\n```", "```py\nThis nsubj is\nis ROOT is\na attr a\nsentence dobj is\n```", "```py\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nwith open(\"large_dataset.txt\") as f:\n    for doc in nlp.pipe(f):\n        # Process the doc here\n        print(doc)\n```", "```py\nimport spacy\n\nnlp = spacy.blank(\"en\")  # Create a blank English model\n\n# Load the training data and configuration\ntrain_data = [(\"This is a text.\", {\"entities\": [(4, 10, \"ENTITY\")]})]\nconfig = {\"iterations\": 100}\n\n# Train the model\nnlp.train(train_data, config)\n\n# Save the trained model to a directory\nnlp.to_disk(\"/model_output_dir\")\n```", "```py\nimport spacy\n\nnlp = spacy.blank(\"en\")  # Create a blank English model\n\n# Load the training data and configuration\ntrain_data = [(\"This is a positive text.\", {\"cats\": {\"POSITIVE\": 1}}),\n              (\"This is a negative text.\", {\"cats\": {\"NEGATIVE\": 1}})]\n```", "```py\npython -m spacy download en_core_web_lg\n```", "```py\nimport spacy \n\nnlp = spacy.load(\"en_core_web_lg\") \ndoc = nlp(\"This is a sentence.\") \nfor token in doc: \n  print(token.text, '\\n', token.vector)\n```", "```py\nThis [  1.6849     1.9826    -0.77743   ...    -2.9454    -0.83337 ]\nis [ 1.4750e+00  6.0078e+00  1.1205e+00 ... -8.5967e-01  9.7466e+00]\na [ -9.3629     9.2761    -7.2708     ...    -6.816      3.5737  ]\nsentence [-2.7653e+00 -7.9512e-01  ... 1.0124e+00  1.7035e-01]\n. [-0.076454 -4.6896   -4.0431  ...    -0.52699  -1.3622  ]\n```", "```py\nprint(len(token.vector))\n# 300\n```", "```py\npython -m spacy download en_core_web_md\n```", "```py\nimport spacy\n\nnlp = spacy.load(\"en_core_web_md\")\n\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n```", "```py\nApple ORG\nU.K. GPE\n$1 billion MONEY\n```", "```py\nimport spacy\n\n# Load the spaCy model with the \"en_core_web_md\" model\nnlp = spacy.load(\"en_core_web_md\")\n\n# Process a text\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion. The startup specializes in machine learning and artificial intelligence.\")\n\n# Iterate over the named entities in the document\nfor ent in doc.ents:\n    # Print the text and label of the entity\n    print(ent.text, ent.label_)\n\n# Extract the first noun phrase in the text\nfirst_noun_phrase = doc[0:2]\nprint(first_noun_phrase.text)\n\n# Get the part-of-speech tags for the tokens in the text\npos_tags = [token.pos_ for token in doc]\nprint(pos_tags)\n\n# Extract the dependencies of the tokens in the text\ndependencies = [(token.text, token.dep_, token.head.text) for token in doc]\nprint(dependencies)\n\n# Get the lemma of the first token in the text\nfirst_token_lemma = doc[0].lemma_\nprint(first_token_lemma)\n```", "```py\nApple ORG\nU.K. GPE\n$1 billion MONEY\nApple \n['PROPN', 'AUX', 'VERB', 'VERB', 'ADP', 'VERB', 'ADP', 'NOUN', 'NUM', 'NOUN']\n[('Apple', 'nsubj', 'is'), ('is', 'aux', 'looking'), ('looking', 'ROOT', 'looking'), ('at', 'prep', 'looking'), ('buying', 'pcomp', 'at'), ('U.K.', 'compound', 'startup'), ('startup', 'dobj', 'buying'), ('for', 'prep', 'buying'), ('$1', 'nummod', 'billion'), ('billion', 'pobj', 'for')]\napple\n```"]