- en: Non-Negative Matrix Factorization (NMF) for Dimensionality Reduction in Image
    Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa](https://towardsdatascience.com/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Discussing theory and implementation with Python and Scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rukshanpramoditha.medium.com/?source=post_page-----8450f4cae8fa--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----8450f4cae8fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8450f4cae8fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8450f4cae8fa--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----8450f4cae8fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8450f4cae8fa--------------------------------)
    ·9 min read·May 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85529f41284031529617ac661c1a911b.png)'
  prefs: []
  type: TYPE_IMG
- en: Original image by [an_photos](https://pixabay.com/users/an_photos-3160435/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4503287)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4503287)
    (Slightly edited by author)
  prefs: []
  type: TYPE_NORMAL
- en: I have already discussed different types of dimensionality reduction techniques
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal Component Analysis (PCA)**, **Factor Analysis (FA)**, **Linear
    Discriminant Analysis (LDA)**, **Autoencoders (AEs)**, and **Kernel PCA** are
    the most popular ones.'
  prefs: []
  type: TYPE_NORMAL
- en: Non-Negative Matrix Factorization (NMF or NNMF) is also a *linear* dimensionality
    reduction technique that can be used to reduce the dimensionality of the feature
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: All dimensionality reduction techniques fall under the category of unsupervised
    machine learning in which we can reveal hidden patterns and important relationships
    in the data without requiring labels.
  prefs: []
  type: TYPE_NORMAL
- en: So, dimensionality reduction algorithms deal with unlabeled data. When training
    such an algorithm, the **fit()** method only needs the feature matrix, **X** as
    the input and it does not require the label column, **y**.
  prefs: []
  type: TYPE_NORMAL
- en: As its name implies, non-negative matrix factorization (NMF) needs the feature
    matrix to be non-negative.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this non-negativity constraint, the usage of NMF is limited to data
    with non-negative values such as image data (pixel values always lie between 0
    and 255, hence there are no negative values in image data!).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The maths behind non-negative matrix factorization (NMF)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-negative matrix factorization comes from linear algebra. In simple words,
    it is the process of decomposing a matrix into the product of two small matrices.
  prefs: []
  type: TYPE_NORMAL
- en: To be more precise,
  prefs: []
  type: TYPE_NORMAL
- en: Non-negative matrix factorization (NMF) is the process of decomposing a non-negative
    feature matrix, V (nxp) into a product of two non-negative matrices called W (nxd)
    and H (dxp). All three matrices should contain non-negative elements.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/358168c09c0d66c5abebf748a85dccbc.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Non-negative matrix factorization equation** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The product of **W** and **H** matrices only gives an approximation to the matrix
    **V**. So, you should expect some information loss when applying NMF.
  prefs: []
  type: TYPE_NORMAL
- en: '**V (*n x p*):** Represents the **feature matrix** where ***n*** is the number
    of observations (samples) and ***p*** is the number of features (variables). This
    is the data matrix we decompose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**W (*n x d*):** Represents the **transformed data matrix** after applying
    NMF. We can use this transformed matrix in place of the original feature matrix,
    **V**. So, **W** is the most important output of NMF. It is obtained by calling
    Scikit-learn NMF’s ***fit_transform()*** method. ***n*** is the number of observations
    (samples) and ***d*** is the number of latent factors or components. In other
    words, ***d*** describes the amount of dimensionality that we want to keep. It
    is actually a hyperparameter that we need to specify in the Scikit-learn NMF’s
    ***n_components*** argument. This is an integer value that should be less than
    the number of features, ***p*** and greater than 0\. Selecting the right value
    for ***d*** is a real challenge when performing NMF. We need to consider the balance
    between the amount of information and the number of components that we want to
    keep.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**H (*d x p*):** Represents the **factorization matrix**. ***d***and***p***have
    the same definitions above. This matrix is not very important. However, this can
    be obtained by calling the Scikit-learn NMF’s ***components_*** attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Python implementation of non-negative matrix factorization (NMF)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Python, NMF is implemented by using Scikit-learn’s **NMF()** class. As you
    already know, Scikit-learn is the Python machine learning library.
  prefs: []
  type: TYPE_NORMAL
- en: All you need to do is to import the **NMF()** class and create an instance of
    it by specifying the required arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Important arguments of NMF() class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**n_components:** An integer that defines the number of components or latent
    factors or the amount of dimensionality that we want to keep. The most important
    hyperparameter! The value is less than the number of original features and greater
    than 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**init:** A method of the initialization process. The output returned by the
    NMF model will significantly vary depending on the ***init*** method you choose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**random_state:** Used when the initialization method is*‘nndsvdar’* or *‘random’*.
    Use an integer to get the same results across different executions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note:** There are many arguments in the **NMF()** class. If we do not specify
    them, they take their default values when calling the **NMF()** function. To learn
    more about those arguments, refer to the Scikit-learn documentation.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Important methods of NMF() class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**fit(V):** Learns an NMF model from the feature matrix, **V**. No transformation
    is applied here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**fit_transform(V):** Learns an NMF model from the feature matrix, **V** and
    returns the transformed data matrix, **W**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**transform(V):** Returns the transformed data matrix, **W** after fitting
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**inverse_transform(W):** Transforms (recovers) the data matrix, **W** back
    to the original space. Very useful for visualization purposes!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Important attributes of NMF() class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**components_:** Returns the factorization matrix, **H**. This matrix is not
    very important.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**reconstruction_err_:** Returns the beta divergence as a float that measures
    the distance between **V** and the product of **WH**. The solver tries to minimize
    this error during the training process. Analyzing this error by setting different
    values for **n_components** is a great way of choosing the right number of components,
    ***d***.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing dimensionality in image data using non-negative matrix factorization
    (NMF)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll use the MNIST digits dataset for this task. We’ll perform NMF on MNIST
    data to reduce dimensionality by choosing different numbers of components and
    then we compare each output with the original one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Load the MNIST dataset using Scikit-learn'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The MNIST digits dataset can be loaded as follows using Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2f44f63943b088f8f64b391b263cc7b0.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is loaded as a Pandas data frame. The shape is (70000, 784). There
    are 70000 observations (images) in the dataset. Each observation has 784 features
    (pixel values). The size of an image is 28 x 28\. When loading the MNIST dataset
    in this way, each image is represented as a 1D array that contains 784 (28 x 28)
    elements. This is the format we need for this task and no further modification
    is required for the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can load the MNIST dataset using Keras. There, you’ll get
    a 28 x 28 2D array for each image instead of a 1D array. You can learn more [here](https://medium.com/data-science-365/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7).
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Visualize a sample of the original images'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we’ll visualize a sample of the first five images in the MNIST dataset.
    This sample can be used to compare with the outputs of the NMF model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c357bf92bfaeb8b4b60e96e3cb71fc3b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**A sample of the original MNIST digits** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Apply NMF with 9 components (d = 9)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/096b088205f681e74188f35702370083.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Now, the new dimensionality is 9\. The original dimensionality was 784\. Therefore,
    the dimensionality has been significantly reduced!
  prefs: []
  type: TYPE_NORMAL
- en: To get the shape of **V**, **W** and **H** matrices, we can run the following
    code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/51e05bace4cf767e10c2a85a36655106.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: To get the reconstruction error or beta divergence between **V** and the product
    of **WH**, we can run the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9ab242413756e1abfbae0d230cb07702.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The reconstruction error is very high. This is because we have selected only
    9 components out of 784\. We can verify this by visualizing the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**NMF output: 9 components or d = 9**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29926404b025345a3b801d18f0945c92.png)'
  prefs: []
  type: TYPE_IMG
- en: '**NMF output: 9 components or d = 9** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The digits are not clear. You can compare this output with the sample of the
    original images.
  prefs: []
  type: TYPE_NORMAL
- en: I have run the NMF algorithm by choosing 100, 225 and 784 components. Here are
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: '**NMF output: 100 components or d = 100**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/402b335ec56660d1248012423af4fc9c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**NMF output: 100 components or d = 100** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The reconstruction error is 174524.20.
  prefs: []
  type: TYPE_NORMAL
- en: '**NMF output: 225 components or d = 225**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d021e74fcabce9a614b98ff2ffa141ae.png)'
  prefs: []
  type: TYPE_IMG
- en: '**NMF output: 225 components or d = 225** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The reconstruction error is 104024.62.
  prefs: []
  type: TYPE_NORMAL
- en: '**NMF output: 784 components or d = 784 (all components)**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63e27a47d157922b94d3d8dab6489bfc.png)'
  prefs: []
  type: TYPE_IMG
- en: '**NMF output: 784 components or d = 784** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The reconstruction error is 23349.67.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the number of components is increased when running non-negative matrix
    factorization (NMF), the images are getting clear and the reconstruction error
    is getting low.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By looking at the output and reconstruction error, a right value for ***d***
    can be chosen. For this, you need to run the NMF algorithm a few times which may
    be time-consuming depending on the computer resources you have.
  prefs: []
  type: TYPE_NORMAL
- en: With d = 784 (all components), you will ***still*** get a reconstruction error
    of 23349.67 instead of zero.
  prefs: []
  type: TYPE_NORMAL
- en: It is obvious that the product of W and H matrices only gives a non-negative
    matrix approximation to the feature matrix, V.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Can we run NMF with a negative matrix?***'
  prefs: []
  type: TYPE_NORMAL
- en: The answer is ***no***. If you try to use NMF with a feature matrix that has
    negative values, you will get the following **ValueError!**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/578974f3013d9327d3dbcbdc476501b9.png)'
  prefs: []
  type: TYPE_IMG
- en: '**ValueError!** (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: You can’t break the non-negativity constraint when running non-negative matrix
    factorization (NMF). The feature matrix should always contain non-negative elements.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is the end of today’s article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Please let me know if you’ve any questions or feedback.**'
  prefs: []
  type: TYPE_NORMAL
- en: Other matrix decomposition methods you might be interested in
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[**Eigendecomposition**](https://medium.com/data-science-365/eigendecomposition-of-a-covariance-matrix-with-numpy-c953334c965d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Singular value decomposition**](/singular-value-decomposition-vs-eigendecomposition-for-dimensionality-reduction-fc0d9ac24a8e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read next (Recommended)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[**PCA and Dimensionality Reduction Special Collection**](https://rukshanpramoditha.medium.com/list/pca-and-dimensionality-reduction-special-collection-146045a5acb5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**How RGB and Grayscale Images Are Represented in NumPy Arrays**](/exploring-the-mnist-digits-dataset-7ff62631766a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Acquire, Understand and Prepare the MNIST Dataset**](https://medium.com/data-science-365/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How about an AI course?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[**Neural Networks and Deep Learning Course**](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join my private list of emails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Never miss a great story from me again. By* [***subscribing to my email list***](https://rukshanpramoditha.medium.com/subscribe)*,
    you will directly receive my stories as soon as I publish them.*'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you so much for your continuous support! See you in the next article.
    Happy learning to everyone!
  prefs: []
  type: TYPE_NORMAL
- en: MNIST dataset info
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Citation:** Deng, L., 2012\. The mnist database of handwritten digit images
    for machine learning research. **IEEE Signal Processing Magazine**, 29(6), pp.
    141–142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Source:** [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**License:** *Yann LeCun* (Courant Institute, NYU) and *Corinna Cortes* (Google
    Labs, New York) hold the copyright of the MNIST dataset which is available under
    the *Creative Commons Attribution-ShareAlike 4.0 International License* ([**CC
    BY-SA**](https://creativecommons.org/licenses/by-sa/4.0/)). You can learn more
    about different dataset license types [here](https://rukshanpramoditha.medium.com/dataset-and-software-license-types-you-need-to-consider-d20965ca43dc#6ade).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Designed and written by:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Rukshan Pramoditha](https://medium.com/u/f90a3bb1d400?source=post_page-----8450f4cae8fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: '**2023–05–06**'
  prefs: []
  type: TYPE_NORMAL
