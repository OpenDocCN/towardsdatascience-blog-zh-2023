- en: Is it Time to Start Talking About Prompt Architecture in LLMs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/prompt-architecture-bd8a07117dab](https://towardsdatascience.com/prompt-architecture-bd8a07117dab)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From prompt engineering to prompt architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://donatoriccio.medium.com/?source=post_page-----bd8a07117dab--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----bd8a07117dab--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bd8a07117dab--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bd8a07117dab--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----bd8a07117dab--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bd8a07117dab--------------------------------)
    ·7 min read·Oct 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f118c2e6e264afa9690d8ddeaca23aba.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author. (AI generated)
  prefs: []
  type: TYPE_NORMAL
- en: Summarize.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It started with a single word.
  prefs: []
  type: TYPE_NORMAL
- en: Not happy with the results, we tried again.
  prefs: []
  type: TYPE_NORMAL
- en: Summarize the most important points of the article*.*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Prompt engineering teaches us that more specific prompts are better.
  prefs: []
  type: TYPE_NORMAL
- en: Identify the three most important arguments made in the article and evaluate
    the strength of the author’s argument based on the evidence provided. Are there
    any points where you feel the argument could be stronger or more convincing?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Over time, we learned to include more details to guide our favorite LLMs to
    provide the best answers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4eafc6b535fb758a0ee39002ebc806d7.png)'
  prefs: []
  type: TYPE_IMG
- en: A recent prompt architecture called Least to Most prompting. [1]
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering techniques are becoming more complex and elaborate systems,
    sometimes made of many components. The definition of *prompt engineering* might
    be limiting in defining such intricate systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I want to propose a more accurate label for multi-component
    systems that interface with LLMs:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Prompt Architecture.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The history of prompt engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modern language models have developed an impressive capacity to take on novel
    tasks after seeing only a couple of examples. This ability is called **in-context
    learning,** and it’s the main reason why we prompt engineering works so well.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers think in-context learning works because pretraining teaches the
    model the general skills needed for language tasks. Then, at test time, it just
    has to recognize the pattern and apply its skills. Bigger models do this even
    better, making them surprisingly adaptable at various natural language tasks.
    [2]
  prefs: []
  type: TYPE_NORMAL
- en: In the past, you’d need thousands of labeled examples to fine-tune a language
    model for a new task. But with in-context learning, you can give the model the
    task description in its context window, and it can figure out the new task. We
    call this **zero-shot learning.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00cf2a5bef153e79f78e55e93bad8e05.png)'
  prefs: []
  type: TYPE_IMG
- en: In few-shot prompting, some examples of the desired output are added to the
    prompt. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Few-shot learning** works by providing a few examples to the model in its
    context. The model then adapts at test time to recognize and continue the pattern.
    No weight updates are needed. This rapid adaptation ability improves as models
    grow in size, allowing large models like GPT-3 to learn new tasks from just a
    few examples. The model can generalize pretty well after seeing just a handful
    of examples.'
  prefs: []
  type: TYPE_NORMAL
- en: These techniques offer a general framework for interacting with LLMs more thoughtfully.
  prefs: []
  type: TYPE_NORMAL
- en: '**Role prompting** is a prompt engineering technique that I’m sure you’ve tried
    at least once in your ChatGPT experience, used for more specific tasks. Here,
    the AI system is assigned a specific role at the start of the prompt. This additional
    information provides context that can enhance the model’s comprehension and lead
    to more effective responses. [3]'
  prefs: []
  type: TYPE_NORMAL
- en: The prompt is initiated with a directive designating the AI’s role, and then
    continues with a question or task that the AI should respond to within the framework
    of the assigned role.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6da585282c768e6eec0cc0489f1c14cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Role prompting. Image by author. [Prompt source.](https://github.com/f/awesome-chatgpt-prompts)
  prefs: []
  type: TYPE_NORMAL
- en: Providing context through role prompting assists the AI in understanding and
    answering appropriately. It guides the model to respond as expected from someone
    with expertise in a particular domain. For example, you ask prompt the model to
    be a doctor to get a more medically relevant response.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/648d13c73972d75a2bafd06f04e631ff.png)'
  prefs: []
  type: TYPE_IMG
- en: One of my favorite prompts. Image by author. [Prompt source.](https://www.engraved.blog/building-a-virtual-machine-inside/)
  prefs: []
  type: TYPE_NORMAL
- en: Enabling complex reasoning in Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs still struggle with logical reasoning and multi-step problem-solving. **Chain
    of Thought** prompting is a technique to get these models to show their work and
    reason through problems step-by-step.
  prefs: []
  type: TYPE_NORMAL
- en: Demonstrating the desired reasoning process prompts the model to replicate that
    logical thought process on novel problems. **CoT** improves performance on multi-step
    reasoning tasks like math and logic puzzles that usually confuse these models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6503911fd27c8b36a9a3a2d59ea0d765.png)'
  prefs: []
  type: TYPE_IMG
- en: Chain of Thought prompting. [4]
  prefs: []
  type: TYPE_NORMAL
- en: Recent work has advanced prompt engineering into systems with multiple elements
    and inference phases.
  prefs: []
  type: TYPE_NORMAL
- en: Here is where we cross the line between prompt engineering and prompt architecture.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/293c0f6019424ae6f15e36a70f7b9365.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-consistency prompt architecture. [5]
  prefs: []
  type: TYPE_NORMAL
- en: Complex reasoning tasks typically admit multiple valid reasoning paths that
    lead to the correct solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Self-consistency prompting** first samples a diverse set of candidate outputs
    from the model, generating multiple possible reasoning paths. It then aggregates
    the answers and chooses the most common answer among the final answer set. If
    different reasoning paths lead to the same definitive answer, there is greater
    confidence that the answer is correct. [5]'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/395f63339339f8db9265d59f46dd4ac3.png)'
  prefs: []
  type: TYPE_IMG
- en: Before answering a physics question, the model asks itself about physics principles.[6]
  prefs: []
  type: TYPE_NORMAL
- en: '**Step-back prompting** further develops the idea of solving a problem by decomposing
    it in intermediate steps. It’s a prompt architecture that improves reasoning capabilities
    by having the mode*l take a step back* to formulate an abstract version of the
    question before attempting to answer it.'
  prefs: []
  type: TYPE_NORMAL
- en: Step-back prompting first asks the LLM a more general question about key ideas.
    The LLM answers with core facts and concepts. With this broad knowledge, the LLM
    then uses the specific original question to give the final response. Tests across
    benchmarks show stepping back helps large models reason better and make fewer
    mistakes. [6]
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c66c8223f8af9d86dddb649a920862be.png)'
  prefs: []
  type: TYPE_IMG
- en: Chain of Verification prompt architecture. [7]
  prefs: []
  type: TYPE_NORMAL
- en: '**Chain of Verification** (CoVe) is a prompt architecture that seeks to reduce
    hallucinations in LLMs. CoVe first has the model generate an initial response
    to a query, which may contain inaccuracies or hallucinations. Next, we prompt
    the LLM to create a set of verification questions to fact-check potential errors
    in its initial response. The LLM then answers these verification questions independently,
    without conditioning on the original response to avoid repeating hallucinations.
    The goal is to generate a revised, verified response, incorporating the verification
    question-answer pairs to correct any inconsistencies with the initial response.
    [7]'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt architectures for autonomous agents and advanced applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt architecture also enables complex applications that are impossible with
    engineering a single prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96fa2942932682636daf5a091e13a946.png)'
  prefs: []
  type: TYPE_IMG
- en: ReAct prompt architecture. [8]
  prefs: []
  type: TYPE_NORMAL
- en: In **ReAct** prompting, the model mixes thoughts and actions to solve complex
    tasks. Thoughts are plans and reason steps that resemble human reasoning. Actions
    gather information externally through APIs or environments. Observations return
    relevant information. ReAct also increases model interpretability by exposing
    thought processes to assess reasoning correctness. Humans can also edit thoughts
    to control model behavior. [8]
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Right Prompt Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a conversational chatbot, more straightforward prompt engineering techniques
    are often sufficient as a first try. If those fail, you can escalate to methods
    like **Step Back** or **Self-Consistency** prompting to improve reasoning without
    adding too much complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Look into **CoVe** if building an application where reducing hallucinations
    is your priority, or even more advanced methods like **Zero Resource Hallucination
    Prevention.** [9] However, **CoVe** a multi-step interaction that might be tedious
    for a chat. In this case, **Retrieval Augmented Generation (RAG)** may be a better
    option to reduce hallucinations. [10]
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98bd0e7da95925ced4a35a83b3fbdc13.png)'
  prefs: []
  type: TYPE_IMG
- en: High-level overview of RAG. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: These methods reveal exciting insights about LLMs, but RAG better suits real
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that building an application with advanced prompt architectures will
    be more expensive since each query will use more tokens to generate the final
    response.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you aim to build autonomous agents — intelligent, goal-oriented systems —
    try ReAct prompting. React allows the LLM to interact with the world by mixing
    thoughts and actions.
  prefs: []
  type: TYPE_NORMAL
- en: Models are growing more capable of solving complex tasks without help. Prompts
    will become even more complex, enabling more advanced use cases for large language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Experience lets you develop intuition for which techniques work best in different
    situations.
  prefs: []
  type: TYPE_NORMAL
- en: The future of interacting with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced prompt architectures elevate LLMs to perform tasks impossible with
    just one inference.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt architecture** is also an exciting way to understand what’s inside
    LLMs beyond improving their practical use. Some are too complicated or expensive
    for practical, real-world applications anyway.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt architecture lets us peek inside the black box of LLMs.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Prompt architecture is not an evolution of prompt engineering — **it’s a radically
    different technique.**
  prefs: []
  type: TYPE_NORMAL
- en: While **prompt engineering** uses a single inference step that anyone can perform
    in a chat interface, **prompt architecture** requires multiple inferences and
    logical steps that often need complex code to be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Looking into both reveals new capabilities for large language models.
  prefs: []
  type: TYPE_NORMAL
- en: In my view, the distinction is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you enjoyed this article, join* [***Text Generation***](https://textgeneration.substack.com/)
    *— our newsletter has two weekly posts with the latest insights on Generative
    AI and Large Language Models.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, you can find me on* [***LinkedIn***](https://www.linkedin.com/in/driccio/)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [[2205.10625v3] Least-to-Most Prompting Enables Complex Reasoning in Large
    Language Models (arxiv.org)](https://arxiv.org/abs/2205.10625v3)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [[2205.11916] Large Language Models are Zero-Shot Reasoners (arxiv.org)](https://arxiv.org/abs/2205.11916)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [[2308.07702] Better Zero-Shot Reasoning with Role-Play Prompting (arxiv.org)](https://arxiv.org/abs/2308.07702)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [[2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language
    Models (arxiv.org)](https://arxiv.org/abs/2201.11903)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [[2203.11171] Self-Consistency Improves Chain of Thought Reasoning in Language
    Models (arxiv.org)](https://arxiv.org/abs/2203.11171)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] [[2310.06117] Take a Step Back: Evoking Reasoning via Abstraction in Large
    Language Models (arxiv.org)](https://arxiv.org/abs//2310.06117)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] [[2309.11495] Chain-of-Verification Reduces Hallucination in Large Language
    Models (arxiv.org)](https://arxiv.org/abs/2309.11495)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] [[2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models
    (arxiv.org)](https://arxiv.org/abs/2210.03629)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] [[2309.02654] Zero-Resource Hallucination Prevention for Large Language
    Models (arxiv.org)](https://arxiv.org/abs/2309.02654)'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] [What is retrieval-augmented generation? | IBM Research Blog](https://research.ibm.com/blog/retrieval-augmented-generation-RAG)'
  prefs: []
  type: TYPE_NORMAL
