- en: PyTorch Introduction — Building your First Linear Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pytorch-introduction-building-your-first-linear-model-d868a8681a41](https://towardsdatascience.com/pytorch-introduction-building-your-first-linear-model-d868a8681a41)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how to build your first PyTorch model, by using the “magical” Linear layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ivopbernardo.medium.com/?source=post_page-----d868a8681a41--------------------------------)[![Ivo
    Bernardo](../Images/39887b6f3e63a67c0545e87962ad5df0.png)](https://ivopbernardo.medium.com/?source=post_page-----d868a8681a41--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d868a8681a41--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d868a8681a41--------------------------------)
    [Ivo Bernardo](https://ivopbernardo.medium.com/?source=post_page-----d868a8681a41--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d868a8681a41--------------------------------)
    ·8 min read·Dec 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48de4c355c69f4666a6e07d34fb89e52.png)'
  prefs: []
  type: TYPE_IMG
- en: Regression Model — Image generated by AI
  prefs: []
  type: TYPE_NORMAL
- en: In my last blog post, we’ve learned [how to work with PyTorch tensors](https://medium.com/towards-data-science/pytorch-introduction-tensors-and-tensor-calculations-412ff818bd5b?sk=2cf4d44549664fc647baa3455e9d78e8),
    the most important object in the PyTorch library. Tensors are the backbone of
    deep learning models so naturally we can use them to fit simpler machine learning
    models to our datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Although PyTorch is known for its Deep Learning capabilities, we are also able
    to fit simple linear models using the framework — and this is actually one of
    the best ways to get familiar with the `torch` API!
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, we’re going to continue with the PyTorch introduction series
    by checking how we can develop a simple linear regression using the `torch` library.
    In the process, we’ll learn about `torch` Optimizers, Weights and other parameters
    of our learning model, something that will be extremely useful for more complex
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start!
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading and Processing Data**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this blog post, we’ll use the song popularity dataset where we’ll want
    to predict the popularity of a certain song based on some song features. Let’s
    take a peek at the head of the dataset below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d4eab71b89cb0b1002c96e186c73e9d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Song Popularity Feature Columns — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the features of this dataset include interesting metrics about each
    song, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: a level of song “energy”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a label encoding of the key (for example, A, B, C, D, etc.) of the song
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song loudness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song tempo.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our goal is to use these features to predict the song popularity, an index
    ranging from 0 to 100\. In the examples we show above, we are aiming to predict
    the following song popularity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f50d07927df9b45944546fbada9d76d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Song Popularity Target Column— Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using `sklearn`, we are going to use PyTorch modules to predict this
    continuous variable. The good part of learning how to fit linear regressions in
    `pytorch`? The knowledge we’re going to gather can be applied with other complex
    models such as deep-layer neural networks!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by preparing our dataset and subset the features and target first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll use`train_test_split` to divide the data into train and test. We’ll perform
    this transformation before transforming our data into`tensors` as `sklearn` ‘s
    method will automatically turn the data into `pandas` or `numpy` format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With `X_train` , `X_test` , `y_train` and `y_test` created, we can now transform
    our data into `torch.tensor` — doing that is easy by passing our data through
    the `torch.Tensor` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Our objects are now in`torch.Tensor` format, the format expected by the `nn.Module`.
    Çet’s see `X_train` below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/404cbdaf0a5afb1ec7400d029ee70130.png)'
  prefs: []
  type: TYPE_IMG
- en: X_train tensor — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Cool — so we have our train and test data in `tensor`format. We’re ready to
    create our first `torch` model, something that we will do next!
  prefs: []
  type: TYPE_NORMAL
- en: Building our Linear Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll train our model using a `LinearRegressionModel class`that inherits from
    the `nn.Module` parent. The `nn.Module` class is Pytorch’s base class for all
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this class, we only need a single argument when creating an object — the
    `optimizer`. We’re doing this as we’ll want to test different optimizers during
    the training process. In the code above, let’s zoom in on the weight initialization
    right after `# Initialize Weights and Bias`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A linear regression is a very simple function consisting of the formula `y
    = b0 + b1x1 + ... bnxn` where:'
  prefs: []
  type: TYPE_NORMAL
- en: y is equal to the target we want to predict
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b0 is equal to the bias term.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b1, …, bn are equal to the weights of the model (how much each variable should
    weight in the final decision and if it contributes negatively or positively).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: x1, …, xn are the values of the features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea behind the `nn.Parameter` is to initialize *b0* (the bias being initialized
    in `self.bias`) and the *b1, … , bn* (the weights being initialized in `self.weights`
    ). We are initializing 13 weights, given that we have 13 features in our training
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: As we’re dealing with a linear regression there’s only a single value for the
    bias, so we just initialize a single random scalar ([check my first post if this
    name sounds alien to you!](https://medium.com/towards-data-science/pytorch-introduction-tensors-and-tensor-calculations-412ff818bd5b?sk=2cf4d44549664fc647baa3455e9d78e8)).
    Also, notice that we’re initializing these parameters randomly using `torch.randn`
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, our goal is to optimize these weights via backpropagation — for that we
    need to setup our linear layer, consisting of the regression formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `trainModel` method will help us perform backpropagation and weight adjustment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this method, we can choose between using between Stochastic Gradient Descent
    (SGD) or Adaptive Moment Estimation (*Adam*) optimizers. More importantly, let’s
    zoom in on what happens between each epoch (a pass on the entire dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This code is extremely important in the context of Neural Networks. It consists
    of the training process of a typical `torch` model:'
  prefs: []
  type: TYPE_NORMAL
- en: We set the model in training mode using `self.train()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we pass the data through the model using `self(X_train)` — this will pass
    the data through the forward layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss_fn` calculates the loss on the training data. Our loss is `torch.L1Loss`
    that consists of the Mean Absolute Error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizer.zero_grad()` sets the gradients to zero (they accumulate every epoch,
    so we want them to start clean in every pass).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss.backward()` calculates the gradient for every weight with respect to
    the loss function. This is the step where our weights are optimized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we update the model’s parameters using `optimizer.step()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The final touch is revealing how our model will be evaluated using the `evaluate`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This code performs a calculation of the loss in the test set. Also, we’ll use
    this method to print our loss in both train and test sets for every 10 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Having our model ready, let’s train it on our data and visualize the learning
    curves for train and test!
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s use the code we’ve built to train our model and see the training process
    — First, we’ll train a model using `Adam` optimizer and `0.001` learning rate
    for 500 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here I’m training a model using the `adam` optimizer for 200 epochs. The overview
    of the train and test loss is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/170b4bd07e394c19b1882ff88d07f4eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Train and Test evolution for the first 150 Epochs — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also plot the training and test loss throughout the epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02396ed220643923d7f33fb675b4ad9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Train and Test Loss — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Our loss is still a bit high (on the last epoch, an MAE of around 21) as a linear
    regression may not be able to solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s just fit a model with `SGD` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e636b24a94e0bdea6de6243f45ff609c.png)'
  prefs: []
  type: TYPE_IMG
- en: Train and Test Loss for SGD Model — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Interesting — train and test loss do not improve! This happens because SGD is
    very sensitive to feature scaling and may have trouble calculating the gradient
    with features that have a different scale. As a challenge, try to scale the features
    and check the results with `SGD` . After scaling, you will also notice a stabler
    behavior on the `Adam` optimizer model!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for taking the time to read this post! In this blog post, we’ve checked
    how we can use `torch` to train a simple linear regression model. While PyTorch
    is famous for it’s deep learning (more layers and complex functions), learning
    simple models is a great way to play around the framework. Also, this is an excellent
    use case to get familiar with the concept of “loss” function and gradients.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve also seen how the `SGD` and `Adam` optimizers work, particularly how sensitive
    they are to unscaled features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, I want you to retain the process that can be expanded to other types
    of models, functions and processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train()` to set the model into training mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass the data through the model using `torch.model`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `nn.L1Loss()` for regression problems. Other loss functions are available
    [here](https://pytorch.org/docs/stable/nn.html#loss-functions).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizer.zero_grad()` sets the gradients to zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss.backward()` calculates the gradient for every weight with respect to
    the loss function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `optimizer.step()` to update the weights of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See you on the next PyTorch post! I also recommend that you visit [PyTorch Zero
    to Mastery Course](https://www.learnpytorch.io/01_pytorch_workflow/), an amazing
    free resource that inspired the methodology behind this post.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to join my newly created YouTube Channel — the [Data Journey](https://www.youtube.com/@TheDataJourney42).
  prefs: []
  type: TYPE_NORMAL
- en: '*The dataset used in this blog post is available on the Kaggle Platform and
    was extracted using Spotify Official APP (*[https://www.kaggle.com/datasets/yasserh/song-popularity-dataset/data)](https://www.kaggle.com/datasets/yasserh/song-popularity-dataset/data).
    *The dataset is under license CC0: Public Domain*'
  prefs: []
  type: TYPE_NORMAL
