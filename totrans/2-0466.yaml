- en: Can an LLM Replace a FinTech Manager? Comprehensive Guide to Develop a GPU-Free
    AI Tool for Corporate Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/can-an-llm-replace-a-fintech-manager-comprehensive-guide-to-develop-a-gpu-free-ai-tool-for-corpo-ce04e12138e8](https://towardsdatascience.com/can-an-llm-replace-a-fintech-manager-comprehensive-guide-to-develop-a-gpu-free-ai-tool-for-corpo-ce04e12138e8)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[HANDS-ON TUTORIALS](https://towardsdatascience.com/tagged/hands-on-tutorials)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Develop your own zero-cost LLM wrapper to unlock corporate context locally
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)[![Gerasimos
    Plegas „ÄΩÔ∏è](../Images/6743a25790f55689dcb1c4a3819f0663.png)](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)
    [Gerasimos Plegas „ÄΩÔ∏è](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)
    ¬∑9 min read¬∑Dec 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*‚ÄúIn solitude the mind gains strength and learns to lean upon itself‚Äù* | Laurence
    Sterne'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd49c1d0f607e3cd2d6cf273079ba9f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Daniel Eliashevskyi](https://unsplash.com/@deni_eliash?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/black-flat-screen-computer-monitor-beside-black-computer-keyboard-aTg26S0_OC0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs been no more than a year now, where GPT *stardust* ‚ú® covered almost any
    sector globally. More and more experts, from any field, crave to utilise Large
    Language Models (LLM) in order to optimise their workflow. Evidently, the corporate
    world could not be absent from this new trend‚Äôs safari. The future promises unprecedented
    possibilities, yet wrapped in the suited‚Ä¶ cost.
  prefs: []
  type: TYPE_NORMAL
- en: The scope of this project is to demonstrate an end-to-end solution for leveraging
    LLMs, in a way that mitigates the privacy and cost concerns. We will utilise [**LLMWare**](https://github.com/llmware-ai/llmware?ref=hackernoon.com),
    an open-source framework for industrial-grade enterprise LLM apps development,
    the Retrieval Augmented Generation (**RAG**) method [1], and [**BLING**](https://huggingface.co/collections/llmware/bling-models-6553c718f51185088be4c91a)
    ‚Äî a newly introduced collection of open-source small models, solely run on CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Concept
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After successfully predicting Jrue Holiday‚Äôs üèÄ [transfer](https://medium.com/towards-data-science/can-a-data-scientist-replace-a-nba-scout-ml-app-development-for-best-transfer-suggestion-f07066c2773)
    to Milwaukee Bucks, Data Corp took on a new project: assisting a FinTech SME to
    optimise its decision-making with AI. That is, to build a tool that will manipulate
    the millions(!) of proprietary docs, query state-of-the-art GPT like models and
    provide Managers with concise, optimal information. That‚Äôs all very well, but
    it comes with two major pitfalls:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security**: Querying a commercial LLM model (i.e. GPT-4) essentially means
    sharing proprietary information over the internet (*how about all those millions
    of docs?*). A data breach would compromise the firm‚Äôs integrity for sure.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Cost**: An automated tool like the above will foster the Managers‚Äô productivity,
    but *there is no free lunch.* The anticipated daily queries might count up to
    hundreds and given the ‚ÄòGPU-thirsty‚Äô LLMs, the aggregated cost might easily get
    out of control.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The above limitations led me to a tricky alternative:'
  prefs: []
  type: TYPE_NORMAL
- en: '*How about developing a custom tool that will consume proprietary knowledge
    and leverage LLM models, but be able to run locally (on-premise) with almost zero
    cost?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To better communicate the outcomes, a couple of assumptions were made:'
  prefs: []
  type: TYPE_NORMAL
- en: '#1: The firm is specialised in the Asset Management sub-domain; therefore,
    we will query about a relevant topic: e.g. asset **criticality**.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2: For the sake of simplicity, we will use a small number of docs (3) to represent
    the firm‚Äôs proprietary sources. `doc_1` describes term ‚Äòcriticality‚Äô partially
    , `doc_2` includes ‚Äòcritical‚Äô lemma but with irrelevant meaning & `doc_3` is totally
    out of context.'
  prefs: []
  type: TYPE_NORMAL
- en: To accomplish the mission, we have to extract the best possible context, regarding
    the subject term ‚Äòcredibility‚Äô. Then, for validation purposes, we will make a
    direct comparison of that with the respective answer of OpenAI‚Äôs ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Modus Operandi
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Familiarisation with key **concepts**, such as the RAG and BLING models‚Äô utilisation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Environment** setup and testing to run the code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tool development,** includingvector database initialisation, embedding model
    selection, semantic query toward the effective RAG.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Benchmarking** the result against OpenAI Models; comparing with GPT-3.5-turbo
    output.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Key-Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into the implementation, it is essential to familiarise ourselves
    with the basics.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The original docs‚Äô text has to be transformed into a vector representation,
    which is essential for a relevancy search to be performed. In a nutshell, this
    kind of element enables ML models to find similarities between them, hence better
    understand the prevailing relationships between the original data (i.e. words).
    This transformation is done by making use of an embedding model [2].
  prefs: []
  type: TYPE_NORMAL
- en: RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Retrieval Augmented Generation constitutes a combined method of the broad Natural
    Language Processing (NLP) field, which operates in a dual way:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieves** pieces of information from relevant artifacts (documents) and'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generates** responses based on that.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its implementation refers to cases, like the one implemented herein, where the
    user wants to retrieve data outside a foundation model and then add it to the
    context to amplify their prompts [3].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6db7e6363cbc88608c1ec93de1d90379.png)'
  prefs: []
  type: TYPE_IMG
- en: Workflow Image by author
  prefs: []
  type: TYPE_NORMAL
- en: As shown above, the user query evokes the retrieval of relevant context by the
    knowledge base ‚Äî this is the RAG model‚Äôs job. The context, then, enhances the
    original prompt and the now *augmented* prompt feeds the foundation LLM model.
  prefs: []
  type: TYPE_NORMAL
- en: BLING Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is all about a handy collection of open-source small [models](https://huggingface.co/collections/llmware/bling-models-6553c718f51185088be4c91a)
    (1B-3B parameter) optimised for RAG implementation, whose purpose is to run on
    CPU-based infrastructure. They are developed by HuggingFace for knowledge-intensive
    industries (i.e. financial, legal, regulatory etc) and their implementation refers
    to cases of sensitive data that must be closely guarded‚Äî actually our enterprise
    case!
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the basic tech nuances behind our tool, let‚Äôs get coding!
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Environment Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to establish the requisite environment to run
    the code. In a CLI (Command Line Interface), simply run the following code snippets:'
  prefs: []
  type: TYPE_NORMAL
- en: Install [transformers](https://pypi.org/project/transformers/) ‚Äî an open-source
    toolkit of pretrained models that perform tasks on several modalities combined.
    It provides APIs to quickly download and use them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install the [llmware](https://pypi.org/project/llmware/) framework ‚Äî an enterprise-grade
    LLM-based development framework with tools and fine-tuned models, including the
    retrieval library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: install.py
  prefs: []
  type: TYPE_NORMAL
- en: Install [Docker](https://www.docker.com/products/docker-desktop/) ‚Äî a containerisation
    software that will let us run a compose file from LLMWare, including all the necessary
    tools, like the [Milvus](https://milvus.io) vector database & [MongoDB](https://www.mongodb.com).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: docker_compose.bash
  prefs: []
  type: TYPE_NORMAL
- en: Import the libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: import.py
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm the setup, you may follow the quick start guide from [LLMWare](https://github.com/llmware-ai/llmware?ref=hackernoon.com)
    or opt to directly run the following code (*for now, let that run entirely - we
    are going to explain everything later on)*:'
  prefs: []
  type: TYPE_NORMAL
- en: test_query.py
  prefs: []
  type: TYPE_NORMAL
- en: An output, including query results, indicates that you have successfully set
    up your machine. Let‚Äôs move on to the next part!
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Tool Development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, let‚Äôs proceed with the fun part--Hands-on!
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 - Vector Database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First things first, we have to create a database which will host our enterprise
    texts. These informational pieces are represented in the multi-dimensional space
    as vectors (embeddings) and the databases capable of storing them belong to the
    family of vector databases [4].
  prefs: []
  type: TYPE_NORMAL
- en: '[Milvus](https://milvus.io) is the open-source solution we are going to utilise,
    which will help us execute semantic queries on the firm‚Äôs docs. All we have to
    do is create a folder and move those docs in there. Then just copy the `samples_path`
    in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: vector_db.py
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 - Embedding Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As anticipated, the embeddings take position‚Ä¶ Since we apply our tool to the
    Asset Management domain, we may opt to choose a relevant embedding model. Hopefully,
    HuggingFace provides a purpose-build one, named: `[industry-bert-asset-management](https://huggingface.co/llmware/industry-bert-asset-management-v0.1)`.
    It is a BERT-based Industry domain Transformer designed for embeddings in the
    asset management industry.'
  prefs: []
  type: TYPE_NORMAL
- en: embeddings.py
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 - Semantic Query
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we construct our semantic query and pass it through the vector database,
    requesting for 2 resutls. For simplicity reasons (*see Assumption #1*), the query
    to be performed is: ‚ÄúWhat is defined as criticality?‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: semantic_search.py
  prefs: []
  type: TYPE_NORMAL
- en: Step 4- BLING Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After getting the source data, we will pass it to the selected BLING models.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we set a variable, `embedded_text`, to store the final concatenated
    text, derived from the items in the `query_res` list. Next, we instantiate a Prompt
    Object from LLMWare (`prompter`) to overcome the strict prompting structure. Then,
    we check all the relevant HuggingFace `models` to achieve the best possible performance.
  prefs: []
  type: TYPE_NORMAL
- en: rag.py
  prefs: []
  type: TYPE_NORMAL
- en: 'For better clarity, I hereby depict a condensed version of the output including
    the model-answer pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '***Note****: Your results may vary due to the stochastic nature of the models.*'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At first, it is wise to show what the involved docs state about the ‚Äòcriticality‚Äô
    term. According to Assumption #2, only `doc_1` is relevant and says‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that, it is easy to conclude that not all models adequately performed,
    except for the `*llmware/bling-falcon-1b-0.1*`, which encapsulates both the mathematical
    assessment (aka product) and the prioritisation semantics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Benchmarking against GPT-3.5-turbo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After setting up our own OpenAI key, we can use a `prompter` with any desired
    model (in our case, GPT-3.5) and add the `query_results`. In this way, we are
    ready to query the LLM with the sources and query string.
  prefs: []
  type: TYPE_NORMAL
- en: validate_gpt.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The answer is very good and when compared with our tool‚Äôs output, the deduction
    is more than pleasing. Particularly, even though our implementation lacked a brief
    referral to *asset importance* and *resources*, it managed to include the *mathematical*
    assessment (aka product) and *prioritisation* semantics, both derived from proprietary
    docs. In other words, that means that our modest model of 1B parameters, locally
    run on a rather humble laptop, managed to compete with an original LLM of 20B
    parameters!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once again, it was a delightful journey‚Ä¶ Starting with zero-cost tools and leveraging
    state-of-the-art open-source frameworks like that of LLMWare, we managed to easily
    develop a privacy-first AI tool for context analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Though we utilised small LLMs of even **1/20** of the GPT-3.5-turbo model, with
    **no GPU** engagement at all, the output was remarkable! We dare to claim that
    our tool managed to retrieve the most essential pieces from the corporate docs
    and ‚Äúmindfully‚Äù combined them with the LLM response.
  prefs: []
  type: TYPE_NORMAL
- en: But, most importantly, this attempt sets the foundation for overcoming GPU-related
    costs and privacy concerns, when manipulating third-party commercial LLM solutions.
    If something is to be taken for granted, that is the fact that firms ‚Äî even the
    small ones ‚Äî can significantly benefit from similar implementations. Running on-prem,
    potentially with proprietary GPUs for an extra boost, they can optimise their
    operations and catch up with this new LLM hype.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6070896875d8d42a3057e87345994031.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Andy Holmes](https://unsplash.com/@andyjh07?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/black-flat-screen-computer-monitor-xA26xebY3dw?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading & have a Merry Christmas! Should any questions arise,
    feel free to leave a comment below or reach out to me on [ùïè](https://twitter.com/MPlegas)
    / [LinkedIn](https://www.linkedin.com/in/gerasimosplegas). In any case‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: 'Enjoy your holiday, clone the [repo](https://github.com/makispl/bling-rag-llm-projects)
    and hire the next‚Ä¶ #LLM üòâ'
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] [https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://www.cloudflare.com/en-gb/learning/ai/what-are-embeddings/](https://www.cloudflare.com/en-gb/learning/ai/what-are-embeddings/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, P. Lewis
    et al, 2020, [arXiv:2005.11401](https://arxiv.org/abs/2005.11401)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [https://www.pinecone.io/learn/vector-database/](https://www.pinecone.io/learn/vector-database/)'
  prefs: []
  type: TYPE_NORMAL
