- en: Can an LLM Replace a FinTech Manager? Comprehensive Guide to Develop a GPU-Free
    AI Tool for Corporate Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM能否取代金融科技经理？开发无GPU AI工具进行企业分析的综合指南
- en: 原文：[https://towardsdatascience.com/can-an-llm-replace-a-fintech-manager-comprehensive-guide-to-develop-a-gpu-free-ai-tool-for-corpo-ce04e12138e8](https://towardsdatascience.com/can-an-llm-replace-a-fintech-manager-comprehensive-guide-to-develop-a-gpu-free-ai-tool-for-corpo-ce04e12138e8)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/can-an-llm-replace-a-fintech-manager-comprehensive-guide-to-develop-a-gpu-free-ai-tool-for-corpo-ce04e12138e8](https://towardsdatascience.com/can-an-llm-replace-a-fintech-manager-comprehensive-guide-to-develop-a-gpu-free-ai-tool-for-corpo-ce04e12138e8)
- en: '[HANDS-ON TUTORIALS](https://towardsdatascience.com/tagged/hands-on-tutorials)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[实践教程](https://towardsdatascience.com/tagged/hands-on-tutorials)'
- en: Develop your own zero-cost LLM wrapper to unlock corporate context locally
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发你自己的零成本LLM包装器，以在本地解锁企业上下文
- en: '[](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)[![Gerasimos
    Plegas 〽️](../Images/6743a25790f55689dcb1c4a3819f0663.png)](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)
    [Gerasimos Plegas 〽️](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)[![Gerasimos
    Plegas 〽️](../Images/6743a25790f55689dcb1c4a3819f0663.png)](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)
    [Gerasimos Plegas 〽️](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)
    ·9 min read·Dec 20, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)
    ·9分钟阅读·2023年12月20日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*“In solitude the mind gains strength and learns to lean upon itself”* | Laurence
    Sterne'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*“在孤独中，心灵获得力量，学会依赖自己”* | 劳伦斯·斯特恩'
- en: '![](../Images/bd49c1d0f607e3cd2d6cf273079ba9f3.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd49c1d0f607e3cd2d6cf273079ba9f3.png)'
- en: Photo by [Daniel Eliashevskyi](https://unsplash.com/@deni_eliash?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/black-flat-screen-computer-monitor-beside-black-computer-keyboard-aTg26S0_OC0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Daniel Eliashevskyi](https://unsplash.com/@deni_eliash?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)提供，来源于[Unsplash](https://unsplash.com/photos/black-flat-screen-computer-monitor-beside-black-computer-keyboard-aTg26S0_OC0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: It’s been no more than a year now, where GPT *stardust* ✨ covered almost any
    sector globally. More and more experts, from any field, crave to utilise Large
    Language Models (LLM) in order to optimise their workflow. Evidently, the corporate
    world could not be absent from this new trend’s safari. The future promises unprecedented
    possibilities, yet wrapped in the suited… cost.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 过去不到一年，GPT *星尘* ✨几乎涵盖了全球的各个领域。越来越多的专家，无论来自哪个领域，都渴望利用大型语言模型（LLM）来优化他们的工作流程。显然，企业界也不能缺席这一新趋势的探索。未来承诺着前所未有的可能性，但这些都伴随着适当的…成本。
- en: The scope of this project is to demonstrate an end-to-end solution for leveraging
    LLMs, in a way that mitigates the privacy and cost concerns. We will utilise [**LLMWare**](https://github.com/llmware-ai/llmware?ref=hackernoon.com),
    an open-source framework for industrial-grade enterprise LLM apps development,
    the Retrieval Augmented Generation (**RAG**) method [1], and [**BLING**](https://huggingface.co/collections/llmware/bling-models-6553c718f51185088be4c91a)
    — a newly introduced collection of open-source small models, solely run on CPU.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的范围是演示如何利用LLM的端到端解决方案，以减轻隐私和成本问题。我们将使用[**LLMWare**](https://github.com/llmware-ai/llmware?ref=hackernoon.com)，一个用于工业级企业LLM应用开发的开源框架，检索增强生成（**RAG**）方法[1]，以及[**BLING**](https://huggingface.co/collections/llmware/bling-models-6553c718f51185088be4c91a)——一组新推出的开源小模型，完全依赖CPU运行。
- en: Concept
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概念
- en: 'After successfully predicting Jrue Holiday’s 🏀 [transfer](https://medium.com/towards-data-science/can-a-data-scientist-replace-a-nba-scout-ml-app-development-for-best-transfer-suggestion-f07066c2773)
    to Milwaukee Bucks, Data Corp took on a new project: assisting a FinTech SME to
    optimise its decision-making with AI. That is, to build a tool that will manipulate
    the millions(!) of proprietary docs, query state-of-the-art GPT like models and
    provide Managers with concise, optimal information. That’s all very well, but
    it comes with two major pitfalls:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功预测Jrue Holiday的 🏀 [转会](https://medium.com/towards-data-science/can-a-data-scientist-replace-a-nba-scout-ml-app-development-for-best-transfer-suggestion-f07066c2773)
    到密尔沃基雄鹿后，Data Corp 开始了一个新项目：协助一家金融科技中小企业优化其决策过程。也就是说，构建一个工具来处理数百万份专有文档，查询先进的GPT类模型，并为经理提供简洁、优化的信息。这一切很好，但有两个主要陷阱：
- en: '**Security**: Querying a commercial LLM model (i.e. GPT-4) essentially means
    sharing proprietary information over the internet (*how about all those millions
    of docs?*). A data breach would compromise the firm’s integrity for sure.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安全性**: 查询商业LLM模型（即GPT-4）本质上意味着通过互联网共享专有信息（*那那些数百万份文档怎么办？*）。数据泄露无疑会损害公司的完整性。'
- en: '**Cost**: An automated tool like the above will foster the Managers’ productivity,
    but *there is no free lunch.* The anticipated daily queries might count up to
    hundreds and given the ‘GPU-thirsty’ LLMs, the aggregated cost might easily get
    out of control.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**成本**: 像上面提到的自动化工具将提高经理们的生产力，但*天下没有免费的午餐*。预计的每日查询可能达到数百次，考虑到‘GPU-饥渴’的LLM，累积的成本可能很容易失控。'
- en: 'The above limitations led me to a tricky alternative:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 上述限制促使我选择了一个棘手的替代方案：
- en: '*How about developing a custom tool that will consume proprietary knowledge
    and leverage LLM models, but be able to run locally (on-premise) with almost zero
    cost?*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*如何开发一个能够处理专有知识并利用LLM模型的定制工具，但能够在本地（内部部署）运行，几乎不花费成本？*'
- en: 'To better communicate the outcomes, a couple of assumptions were made:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地传达结果，做出了几个假设：
- en: '#1: The firm is specialised in the Asset Management sub-domain; therefore,
    we will query about a relevant topic: e.g. asset **criticality**.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '#1: 公司专注于资产管理子领域，因此我们将查询相关话题：例如资产**criticality**。'
- en: '#2: For the sake of simplicity, we will use a small number of docs (3) to represent
    the firm’s proprietary sources. `doc_1` describes term ‘criticality’ partially
    , `doc_2` includes ‘critical’ lemma but with irrelevant meaning & `doc_3` is totally
    out of context.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '#2: 为了简化起见，我们将使用少量文档（3）来代表公司的专有来源。`doc_1` 部分描述了术语“criticality”，`doc_2` 包含了“critical”的词条，但意义不相关，`doc_3`
    完全不相关。'
- en: To accomplish the mission, we have to extract the best possible context, regarding
    the subject term ‘credibility’. Then, for validation purposes, we will make a
    direct comparison of that with the respective answer of OpenAI’s ChatGPT.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成任务，我们必须提取有关主题术语“credibility”的最佳上下文。然后，为了验证，我们将直接将其与OpenAI的ChatGPT的相应答案进行比较。
- en: Modus Operandi
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作模式
- en: Familiarisation with key **concepts**, such as the RAG and BLING models’ utilisation.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 熟悉关键**概念**，如RAG和BLING模型的应用。
- en: '**Environment** setup and testing to run the code.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**环境**设置和测试以运行代码。'
- en: '**Tool development,** includingvector database initialisation, embedding model
    selection, semantic query toward the effective RAG.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**工具开发**，包括向量数据库初始化、嵌入模型选择、针对有效RAG的语义查询。'
- en: '**Benchmarking** the result against OpenAI Models; comparing with GPT-3.5-turbo
    output.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基准测试**结果与OpenAI模型；与GPT-3.5-turbo输出进行比较。'
- en: 1\. Key-Concepts
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 关键概念
- en: Before diving into the implementation, it is essential to familiarise ourselves
    with the basics.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入实施之前，熟悉基础知识是必要的。
- en: Embeddings
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入
- en: The original docs’ text has to be transformed into a vector representation,
    which is essential for a relevancy search to be performed. In a nutshell, this
    kind of element enables ML models to find similarities between them, hence better
    understand the prevailing relationships between the original data (i.e. words).
    This transformation is done by making use of an embedding model [2].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文档的文本必须转化为向量表示，这对执行相关性搜索至关重要。简而言之，这种元素使得机器学习模型能够找到它们之间的相似性，从而更好地理解原始数据（即单词）之间的关系。这种转换是通过使用嵌入模型[2]来完成的。
- en: RAG
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG
- en: 'Retrieval Augmented Generation constitutes a combined method of the broad Natural
    Language Processing (NLP) field, which operates in a dual way:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）是自然语言处理（NLP）领域的一个综合方法，以双重方式运作：
- en: '**Retrieves** pieces of information from relevant artifacts (documents) and'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索**相关文档中的信息'
- en: '**Generates** responses based on that.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成**基于这些信息的响应。'
- en: Its implementation refers to cases, like the one implemented herein, where the
    user wants to retrieve data outside a foundation model and then add it to the
    context to amplify their prompts [3].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其实现涉及到用户希望从基础模型之外检索数据的情况，然后将其添加到上下文中以增强他们的提示[3]。
- en: '![](../Images/6db7e6363cbc88608c1ec93de1d90379.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6db7e6363cbc88608c1ec93de1d90379.png)'
- en: Workflow Image by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流图片由作者提供
- en: As shown above, the user query evokes the retrieval of relevant context by the
    knowledge base — this is the RAG model’s job. The context, then, enhances the
    original prompt and the now *augmented* prompt feeds the foundation LLM model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，用户查询触发了知识库的相关上下文检索——这是RAG模型的工作。然后，这些上下文增强了原始提示，现已*增强*的提示则输入到基础LLM模型中。
- en: BLING Models
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BLING模型
- en: It is all about a handy collection of open-source small [models](https://huggingface.co/collections/llmware/bling-models-6553c718f51185088be4c91a)
    (1B-3B parameter) optimised for RAG implementation, whose purpose is to run on
    CPU-based infrastructure. They are developed by HuggingFace for knowledge-intensive
    industries (i.e. financial, legal, regulatory etc) and their implementation refers
    to cases of sensitive data that must be closely guarded— actually our enterprise
    case!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这完全是一个便捷的开源小型[模型](https://huggingface.co/collections/llmware/bling-models-6553c718f51185088be4c91a)（1B-3B
    参数），经过优化以实现RAG（检索增强生成）实施，旨在运行在基于CPU的基础设施上。它们由HuggingFace开发，针对知识密集型行业（即金融、法律、监管等），其实现涉及必须严格保护的敏感数据——这实际上是我们的企业案例！
- en: Now that we understand the basic tech nuances behind our tool, let’s get coding!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了工具背后的基本技术细节，让我们开始编程吧！
- en: 2\. Environment Setup
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 环境设置
- en: 'In this section, we are going to establish the requisite environment to run
    the code. In a CLI (Command Line Interface), simply run the following code snippets:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将建立运行代码所需的环境。在命令行界面（CLI）中，只需运行以下代码片段：
- en: Install [transformers](https://pypi.org/project/transformers/) — an open-source
    toolkit of pretrained models that perform tasks on several modalities combined.
    It provides APIs to quickly download and use them.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装[transformers](https://pypi.org/project/transformers/) —— 一个开源的预训练模型工具包，能够在多个模态上执行任务。它提供了快速下载和使用模型的API。
- en: Install the [llmware](https://pypi.org/project/llmware/) framework — an enterprise-grade
    LLM-based development framework with tools and fine-tuned models, including the
    retrieval library.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装[llmware](https://pypi.org/project/llmware/)框架 —— 一个企业级基于LLM的开发框架，提供工具和微调模型，包括检索库。
- en: install.py
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: install.py
- en: Install [Docker](https://www.docker.com/products/docker-desktop/) — a containerisation
    software that will let us run a compose file from LLMWare, including all the necessary
    tools, like the [Milvus](https://milvus.io) vector database & [MongoDB](https://www.mongodb.com).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装[Docker](https://www.docker.com/products/docker-desktop/) —— 一个容器化软件，允许我们从LLMWare运行一个compose文件，包括所有必要的工具，如[Milvus](https://milvus.io)向量数据库和[MongoDB](https://www.mongodb.com)。
- en: docker_compose.bash
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: docker_compose.bash
- en: Import the libraries
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入库
- en: import.py
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: import.py
- en: 'To confirm the setup, you may follow the quick start guide from [LLMWare](https://github.com/llmware-ai/llmware?ref=hackernoon.com)
    or opt to directly run the following code (*for now, let that run entirely - we
    are going to explain everything later on)*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认设置，你可以按照[LLMWare](https://github.com/llmware-ai/llmware?ref=hackernoon.com)的快速入门指南，或选择直接运行以下代码（*现在，让它完全运行
    - 我们稍后会解释一切*）：
- en: test_query.py
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: test_query.py
- en: An output, including query results, indicates that you have successfully set
    up your machine. Let’s move on to the next part!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，包括查询结果，表明你已经成功设置了机器。让我们继续下一部分吧！
- en: 3\. Tool Development
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 工具开发
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, let’s proceed with the fun part--Hands-on!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入有趣的部分——动手实践吧！
- en: Step 1 - Vector Database
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1 - 向量数据库
- en: First things first, we have to create a database which will host our enterprise
    texts. These informational pieces are represented in the multi-dimensional space
    as vectors (embeddings) and the databases capable of storing them belong to the
    family of vector databases [4].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须创建一个数据库来存储我们的企业文本。这些信息在多维空间中表示为向量（嵌入），能够存储它们的数据库属于向量数据库的范畴[4]。
- en: '[Milvus](https://milvus.io) is the open-source solution we are going to utilise,
    which will help us execute semantic queries on the firm’s docs. All we have to
    do is create a folder and move those docs in there. Then just copy the `samples_path`
    in the following snippet:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[Milvus](https://milvus.io) 是我们将要使用的开源解决方案，它将帮助我们对公司的文档执行语义查询。我们只需创建一个文件夹并将这些文档移动到那里。然后只需复制以下代码片段中的
    `samples_path`：'
- en: vector_db.py
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: vector_db.py
- en: Step 2 - Embedding Model
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步 - 嵌入模型
- en: 'As anticipated, the embeddings take position… Since we apply our tool to the
    Asset Management domain, we may opt to choose a relevant embedding model. Hopefully,
    HuggingFace provides a purpose-build one, named: `[industry-bert-asset-management](https://huggingface.co/llmware/industry-bert-asset-management-v0.1)`.
    It is a BERT-based Industry domain Transformer designed for embeddings in the
    asset management industry.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，嵌入将被定位……由于我们将工具应用于资产管理领域，我们可以选择一个相关的嵌入模型。希望 HuggingFace 提供了一个专门构建的模型，名为：`[industry-bert-asset-management](https://huggingface.co/llmware/industry-bert-asset-management-v0.1)`。这是一个基于
    BERT 的行业领域 Transformer，专为资产管理行业中的嵌入设计。
- en: embeddings.py
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: embeddings.py
- en: Step 3 - Semantic Query
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步 - 语义查询
- en: 'Next, we construct our semantic query and pass it through the vector database,
    requesting for 2 resutls. For simplicity reasons (*see Assumption #1*), the query
    to be performed is: “What is defined as criticality?”'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们构造我们的语义查询，并将其传递通过向量数据库，要求返回 2 个结果。为了简化起见（*参见假设 #1*），要执行的查询是：“什么被定义为关键性？”'
- en: semantic_search.py
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: semantic_search.py
- en: Step 4- BLING Models
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四步 - BLING 模型
- en: After getting the source data, we will pass it to the selected BLING models.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 获取源数据后，我们将把它传递给选择的 BLING 模型。
- en: Initially, we set a variable, `embedded_text`, to store the final concatenated
    text, derived from the items in the `query_res` list. Next, we instantiate a Prompt
    Object from LLMWare (`prompter`) to overcome the strict prompting structure. Then,
    we check all the relevant HuggingFace `models` to achieve the best possible performance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们设置一个变量 `embedded_text` 来存储从 `query_res` 列表中的项拼接得到的最终文本。接下来，我们从 LLMWare
    实例化一个 Prompt 对象（`prompter`），以克服严格的提示结构。然后，我们检查所有相关的 HuggingFace `models` 以实现最佳性能。
- en: rag.py
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: rag.py
- en: 'For better clarity, I hereby depict a condensed version of the output including
    the model-answer pairs:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚，我在此描述了一个精简版本的输出，包括模型-回答对：
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***Note****: Your results may vary due to the stochastic nature of the models.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意***：由于模型的随机性质，您的结果可能会有所不同。'
- en: 4\. Validation
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 验证
- en: 'At first, it is wise to show what the involved docs state about the ‘criticality’
    term. According to Assumption #2, only `doc_1` is relevant and says…'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '起初，明智的做法是展示涉及的文档对“关键性”术语的说明。根据假设 #2，只有 `doc_1` 是相关的，并且说……'
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Given that, it is easy to conclude that not all models adequately performed,
    except for the `*llmware/bling-falcon-1b-0.1*`, which encapsulates both the mathematical
    assessment (aka product) and the prioritisation semantics:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，很容易得出结论，除了 `*llmware/bling-falcon-1b-0.1*`，其他模型的表现均不尽如人意，该模型包含了数学评估（即产品）和优先级语义：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Benchmarking against GPT-3.5-turbo
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与 GPT-3.5-turbo 的基准测试
- en: After setting up our own OpenAI key, we can use a `prompter` with any desired
    model (in our case, GPT-3.5) and add the `query_results`. In this way, we are
    ready to query the LLM with the sources and query string.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好我们的 OpenAI 密钥后，我们可以使用任何所需的模型（在我们的例子中是 GPT-3.5）和添加 `query_results` 的 `prompter`。这样，我们就可以准备好用源和查询字符串对
    LLM 进行查询。
- en: validate_gpt.py
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: validate_gpt.py
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The answer is very good and when compared with our tool’s output, the deduction
    is more than pleasing. Particularly, even though our implementation lacked a brief
    referral to *asset importance* and *resources*, it managed to include the *mathematical*
    assessment (aka product) and *prioritisation* semantics, both derived from proprietary
    docs. In other words, that means that our modest model of 1B parameters, locally
    run on a rather humble laptop, managed to compete with an original LLM of 20B
    parameters!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 答案非常好，与我们工具的输出相比，推导结果令人非常满意。特别是，尽管我们的实现缺少了对*资产重要性*和*资源*的简要参考，但它成功地包含了*数学*评估（也就是产品）和*优先级*语义，这些都源于专有文档。换句话说，这意味着我们那只有1B参数、在相当普通的笔记本电脑上本地运行的简陋模型，竟然能够与一个20B参数的原始LLM进行竞争！
- en: Conclusion
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Once again, it was a delightful journey… Starting with zero-cost tools and leveraging
    state-of-the-art open-source frameworks like that of LLMWare, we managed to easily
    develop a privacy-first AI tool for context analysis.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 再一次，这是一次愉快的旅程……从零成本工具开始，利用最先进的开源框架如LLMWare，我们轻松开发了一个隐私优先的AI工具用于上下文分析。
- en: Though we utilised small LLMs of even **1/20** of the GPT-3.5-turbo model, with
    **no GPU** engagement at all, the output was remarkable! We dare to claim that
    our tool managed to retrieve the most essential pieces from the corporate docs
    and “mindfully” combined them with the LLM response.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用了甚至是**1/20** GPT-3.5-turbo模型的小型LLM，且**完全不使用GPU**，但输出结果还是非常出色！我们敢于声称，我们的工具成功地从公司文档中提取了最重要的信息，并“有意识地”将其与LLM的响应相结合。
- en: But, most importantly, this attempt sets the foundation for overcoming GPU-related
    costs and privacy concerns, when manipulating third-party commercial LLM solutions.
    If something is to be taken for granted, that is the fact that firms — even the
    small ones — can significantly benefit from similar implementations. Running on-prem,
    potentially with proprietary GPUs for an extra boost, they can optimise their
    operations and catch up with this new LLM hype.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但最重要的是，这次尝试为克服与GPU相关的成本和隐私问题奠定了基础，尤其是在处理第三方商业LLM解决方案时。如果有任何事情是理所当然的，那就是企业——即使是小企业——可以从类似的实现中获得显著的好处。通过在本地运行，可能还使用专有GPU以获得额外的提升，它们可以优化其运营并赶上这一新的LLM热潮。
- en: '![](../Images/6070896875d8d42a3057e87345994031.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6070896875d8d42a3057e87345994031.png)'
- en: Photo by [Andy Holmes](https://unsplash.com/@andyjh07?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/black-flat-screen-computer-monitor-xA26xebY3dw?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Andy Holmes](https://unsplash.com/@andyjh07?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)提供，刊登在[Unsplash](https://unsplash.com/photos/black-flat-screen-computer-monitor-xA26xebY3dw?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)上
- en: Thank you for reading & have a Merry Christmas! Should any questions arise,
    feel free to leave a comment below or reach out to me on [𝕏](https://twitter.com/MPlegas)
    / [LinkedIn](https://www.linkedin.com/in/gerasimosplegas). In any case…
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，祝您圣诞快乐！如果有任何问题，请随时在下面留言或通过[𝕏](https://twitter.com/MPlegas) / [LinkedIn](https://www.linkedin.com/in/gerasimosplegas)与我联系。无论如何……
- en: 'Enjoy your holiday, clone the [repo](https://github.com/makispl/bling-rag-llm-projects)
    and hire the next… #LLM 😉'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '享受假期，克隆[repo](https://github.com/makispl/bling-rag-llm-projects)，并招聘下一个…… #LLM
    😉'
- en: '**References**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] [https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)'
- en: '[2] [https://www.cloudflare.com/en-gb/learning/ai/what-are-embeddings/](https://www.cloudflare.com/en-gb/learning/ai/what-are-embeddings/)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://www.cloudflare.com/en-gb/learning/ai/what-are-embeddings/](https://www.cloudflare.com/en-gb/learning/ai/what-are-embeddings/)'
- en: '[3] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, P. Lewis
    et al, 2020, [arXiv:2005.11401](https://arxiv.org/abs/2005.11401)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, P. Lewis
    et al, 2020, [arXiv:2005.11401](https://arxiv.org/abs/2005.11401)'
- en: '[4] [https://www.pinecone.io/learn/vector-database/](https://www.pinecone.io/learn/vector-database/)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://www.pinecone.io/learn/vector-database/](https://www.pinecone.io/learn/vector-database/)'
