- en: Can an LLM Replace a FinTech Manager? Comprehensive Guide to Develop a GPU-Free
    AI Tool for Corporate Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMèƒ½å¦å–ä»£é‡‘èç§‘æŠ€ç»ç†ï¼Ÿå¼€å‘æ— GPU AIå·¥å…·è¿›è¡Œä¼ä¸šåˆ†æçš„ç»¼åˆæŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/can-an-llm-replace-a-fintech-manager-comprehensive-guide-to-develop-a-gpu-free-ai-tool-for-corpo-ce04e12138e8](https://towardsdatascience.com/can-an-llm-replace-a-fintech-manager-comprehensive-guide-to-develop-a-gpu-free-ai-tool-for-corpo-ce04e12138e8)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/can-an-llm-replace-a-fintech-manager-comprehensive-guide-to-develop-a-gpu-free-ai-tool-for-corpo-ce04e12138e8](https://towardsdatascience.com/can-an-llm-replace-a-fintech-manager-comprehensive-guide-to-develop-a-gpu-free-ai-tool-for-corpo-ce04e12138e8)
- en: '[HANDS-ON TUTORIALS](https://towardsdatascience.com/tagged/hands-on-tutorials)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[å®è·µæ•™ç¨‹](https://towardsdatascience.com/tagged/hands-on-tutorials)'
- en: Develop your own zero-cost LLM wrapper to unlock corporate context locally
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼€å‘ä½ è‡ªå·±çš„é›¶æˆæœ¬LLMåŒ…è£…å™¨ï¼Œä»¥åœ¨æœ¬åœ°è§£é”ä¼ä¸šä¸Šä¸‹æ–‡
- en: '[](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)[![Gerasimos
    Plegas ã€½ï¸](../Images/6743a25790f55689dcb1c4a3819f0663.png)](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)
    [Gerasimos Plegas ã€½ï¸](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)[![Gerasimos
    Plegas ã€½ï¸](../Images/6743a25790f55689dcb1c4a3819f0663.png)](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)
    [Gerasimos Plegas ã€½ï¸](https://medium.com/@gerasimos_plegas?source=post_page-----ce04e12138e8--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)
    Â·9 min readÂ·Dec 20, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce04e12138e8--------------------------------)
    Â·9åˆ†é’Ÿé˜…è¯»Â·2023å¹´12æœˆ20æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*â€œIn solitude the mind gains strength and learns to lean upon itselfâ€* | Laurence
    Sterne'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*â€œåœ¨å­¤ç‹¬ä¸­ï¼Œå¿ƒçµè·å¾—åŠ›é‡ï¼Œå­¦ä¼šä¾èµ–è‡ªå·±â€* | åŠ³ä¼¦æ–¯Â·æ–¯ç‰¹æ©'
- en: '![](../Images/bd49c1d0f607e3cd2d6cf273079ba9f3.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd49c1d0f607e3cd2d6cf273079ba9f3.png)'
- en: Photo by [Daniel Eliashevskyi](https://unsplash.com/@deni_eliash?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/black-flat-screen-computer-monitor-beside-black-computer-keyboard-aTg26S0_OC0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Daniel Eliashevskyi](https://unsplash.com/@deni_eliash?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)æä¾›ï¼Œæ¥æºäº[Unsplash](https://unsplash.com/photos/black-flat-screen-computer-monitor-beside-black-computer-keyboard-aTg26S0_OC0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: Itâ€™s been no more than a year now, where GPT *stardust* âœ¨ covered almost any
    sector globally. More and more experts, from any field, crave to utilise Large
    Language Models (LLM) in order to optimise their workflow. Evidently, the corporate
    world could not be absent from this new trendâ€™s safari. The future promises unprecedented
    possibilities, yet wrapped in the suitedâ€¦ cost.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‡å»ä¸åˆ°ä¸€å¹´ï¼ŒGPT *æ˜Ÿå°˜* âœ¨å‡ ä¹æ¶µç›–äº†å…¨çƒçš„å„ä¸ªé¢†åŸŸã€‚è¶Šæ¥è¶Šå¤šçš„ä¸“å®¶ï¼Œæ— è®ºæ¥è‡ªå“ªä¸ªé¢†åŸŸï¼Œéƒ½æ¸´æœ›åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ä¼˜åŒ–ä»–ä»¬çš„å·¥ä½œæµç¨‹ã€‚æ˜¾ç„¶ï¼Œä¼ä¸šç•Œä¹Ÿä¸èƒ½ç¼ºå¸­è¿™ä¸€æ–°è¶‹åŠ¿çš„æ¢ç´¢ã€‚æœªæ¥æ‰¿è¯ºç€å‰æ‰€æœªæœ‰çš„å¯èƒ½æ€§ï¼Œä½†è¿™äº›éƒ½ä¼´éšç€é€‚å½“çš„â€¦æˆæœ¬ã€‚
- en: The scope of this project is to demonstrate an end-to-end solution for leveraging
    LLMs, in a way that mitigates the privacy and cost concerns. We will utilise [**LLMWare**](https://github.com/llmware-ai/llmware?ref=hackernoon.com),
    an open-source framework for industrial-grade enterprise LLM apps development,
    the Retrieval Augmented Generation (**RAG**) method [1], and [**BLING**](https://huggingface.co/collections/llmware/bling-models-6553c718f51185088be4c91a)
    â€” a newly introduced collection of open-source small models, solely run on CPU.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬é¡¹ç›®çš„èŒƒå›´æ˜¯æ¼”ç¤ºå¦‚ä½•åˆ©ç”¨LLMçš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œä»¥å‡è½»éšç§å’Œæˆæœ¬é—®é¢˜ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[**LLMWare**](https://github.com/llmware-ai/llmware?ref=hackernoon.com)ï¼Œä¸€ä¸ªç”¨äºå·¥ä¸šçº§ä¼ä¸šLLMåº”ç”¨å¼€å‘çš„å¼€æºæ¡†æ¶ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆ**RAG**ï¼‰æ–¹æ³•[1]ï¼Œä»¥åŠ[**BLING**](https://huggingface.co/collections/llmware/bling-models-6553c718f51185088be4c91a)â€”â€”ä¸€ç»„æ–°æ¨å‡ºçš„å¼€æºå°æ¨¡å‹ï¼Œå®Œå…¨ä¾èµ–CPUè¿è¡Œã€‚
- en: Concept
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚å¿µ
- en: 'After successfully predicting Jrue Holidayâ€™s ğŸ€ [transfer](https://medium.com/towards-data-science/can-a-data-scientist-replace-a-nba-scout-ml-app-development-for-best-transfer-suggestion-f07066c2773)
    to Milwaukee Bucks, Data Corp took on a new project: assisting a FinTech SME to
    optimise its decision-making with AI. That is, to build a tool that will manipulate
    the millions(!) of proprietary docs, query state-of-the-art GPT like models and
    provide Managers with concise, optimal information. Thatâ€™s all very well, but
    it comes with two major pitfalls:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆåŠŸé¢„æµ‹Jrue Holidayçš„ ğŸ€ [è½¬ä¼š](https://medium.com/towards-data-science/can-a-data-scientist-replace-a-nba-scout-ml-app-development-for-best-transfer-suggestion-f07066c2773)
    åˆ°å¯†å°”æ²ƒåŸºé›„é¹¿åï¼ŒData Corp å¼€å§‹äº†ä¸€ä¸ªæ–°é¡¹ç›®ï¼šååŠ©ä¸€å®¶é‡‘èç§‘æŠ€ä¸­å°ä¼ä¸šä¼˜åŒ–å…¶å†³ç­–è¿‡ç¨‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ„å»ºä¸€ä¸ªå·¥å…·æ¥å¤„ç†æ•°ç™¾ä¸‡ä»½ä¸“æœ‰æ–‡æ¡£ï¼ŒæŸ¥è¯¢å…ˆè¿›çš„GPTç±»æ¨¡å‹ï¼Œå¹¶ä¸ºç»ç†æä¾›ç®€æ´ã€ä¼˜åŒ–çš„ä¿¡æ¯ã€‚è¿™ä¸€åˆ‡å¾ˆå¥½ï¼Œä½†æœ‰ä¸¤ä¸ªä¸»è¦é™·é˜±ï¼š
- en: '**Security**: Querying a commercial LLM model (i.e. GPT-4) essentially means
    sharing proprietary information over the internet (*how about all those millions
    of docs?*). A data breach would compromise the firmâ€™s integrity for sure.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å®‰å…¨æ€§**: æŸ¥è¯¢å•†ä¸šLLMæ¨¡å‹ï¼ˆå³GPT-4ï¼‰æœ¬è´¨ä¸Šæ„å‘³ç€é€šè¿‡äº’è”ç½‘å…±äº«ä¸“æœ‰ä¿¡æ¯ï¼ˆ*é‚£é‚£äº›æ•°ç™¾ä¸‡ä»½æ–‡æ¡£æ€ä¹ˆåŠï¼Ÿ*ï¼‰ã€‚æ•°æ®æ³„éœ²æ— ç–‘ä¼šæŸå®³å…¬å¸çš„å®Œæ•´æ€§ã€‚'
- en: '**Cost**: An automated tool like the above will foster the Managersâ€™ productivity,
    but *there is no free lunch.* The anticipated daily queries might count up to
    hundreds and given the â€˜GPU-thirstyâ€™ LLMs, the aggregated cost might easily get
    out of control.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æˆæœ¬**: åƒä¸Šé¢æåˆ°çš„è‡ªåŠ¨åŒ–å·¥å…·å°†æé«˜ç»ç†ä»¬çš„ç”Ÿäº§åŠ›ï¼Œä½†*å¤©ä¸‹æ²¡æœ‰å…è´¹çš„åˆé¤*ã€‚é¢„è®¡çš„æ¯æ—¥æŸ¥è¯¢å¯èƒ½è¾¾åˆ°æ•°ç™¾æ¬¡ï¼Œè€ƒè™‘åˆ°â€˜GPU-é¥¥æ¸´â€™çš„LLMï¼Œç´¯ç§¯çš„æˆæœ¬å¯èƒ½å¾ˆå®¹æ˜“å¤±æ§ã€‚'
- en: 'The above limitations led me to a tricky alternative:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°é™åˆ¶ä¿ƒä½¿æˆ‘é€‰æ‹©äº†ä¸€ä¸ªæ£˜æ‰‹çš„æ›¿ä»£æ–¹æ¡ˆï¼š
- en: '*How about developing a custom tool that will consume proprietary knowledge
    and leverage LLM models, but be able to run locally (on-premise) with almost zero
    cost?*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚ä½•å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†ä¸“æœ‰çŸ¥è¯†å¹¶åˆ©ç”¨LLMæ¨¡å‹çš„å®šåˆ¶å·¥å…·ï¼Œä½†èƒ½å¤Ÿåœ¨æœ¬åœ°ï¼ˆå†…éƒ¨éƒ¨ç½²ï¼‰è¿è¡Œï¼Œå‡ ä¹ä¸èŠ±è´¹æˆæœ¬ï¼Ÿ*'
- en: 'To better communicate the outcomes, a couple of assumptions were made:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ä¼ è¾¾ç»“æœï¼Œåšå‡ºäº†å‡ ä¸ªå‡è®¾ï¼š
- en: '#1: The firm is specialised in the Asset Management sub-domain; therefore,
    we will query about a relevant topic: e.g. asset **criticality**.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '#1: å…¬å¸ä¸“æ³¨äºèµ„äº§ç®¡ç†å­é¢†åŸŸï¼Œå› æ­¤æˆ‘ä»¬å°†æŸ¥è¯¢ç›¸å…³è¯é¢˜ï¼šä¾‹å¦‚èµ„äº§**criticality**ã€‚'
- en: '#2: For the sake of simplicity, we will use a small number of docs (3) to represent
    the firmâ€™s proprietary sources. `doc_1` describes term â€˜criticalityâ€™ partially
    , `doc_2` includes â€˜criticalâ€™ lemma but with irrelevant meaning & `doc_3` is totally
    out of context.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '#2: ä¸ºäº†ç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å°‘é‡æ–‡æ¡£ï¼ˆ3ï¼‰æ¥ä»£è¡¨å…¬å¸çš„ä¸“æœ‰æ¥æºã€‚`doc_1` éƒ¨åˆ†æè¿°äº†æœ¯è¯­â€œcriticalityâ€ï¼Œ`doc_2` åŒ…å«äº†â€œcriticalâ€çš„è¯æ¡ï¼Œä½†æ„ä¹‰ä¸ç›¸å…³ï¼Œ`doc_3`
    å®Œå…¨ä¸ç›¸å…³ã€‚'
- en: To accomplish the mission, we have to extract the best possible context, regarding
    the subject term â€˜credibilityâ€™. Then, for validation purposes, we will make a
    direct comparison of that with the respective answer of OpenAIâ€™s ChatGPT.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®Œæˆä»»åŠ¡ï¼Œæˆ‘ä»¬å¿…é¡»æå–æœ‰å…³ä¸»é¢˜æœ¯è¯­â€œcredibilityâ€çš„æœ€ä½³ä¸Šä¸‹æ–‡ã€‚ç„¶åï¼Œä¸ºäº†éªŒè¯ï¼Œæˆ‘ä»¬å°†ç›´æ¥å°†å…¶ä¸OpenAIçš„ChatGPTçš„ç›¸åº”ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒã€‚
- en: Modus Operandi
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ“ä½œæ¨¡å¼
- en: Familiarisation with key **concepts**, such as the RAG and BLING modelsâ€™ utilisation.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç†Ÿæ‚‰å…³é”®**æ¦‚å¿µ**ï¼Œå¦‚RAGå’ŒBLINGæ¨¡å‹çš„åº”ç”¨ã€‚
- en: '**Environment** setup and testing to run the code.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç¯å¢ƒ**è®¾ç½®å’Œæµ‹è¯•ä»¥è¿è¡Œä»£ç ã€‚'
- en: '**Tool development,** includingvector database initialisation, embedding model
    selection, semantic query toward the effective RAG.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å·¥å…·å¼€å‘**ï¼ŒåŒ…æ‹¬å‘é‡æ•°æ®åº“åˆå§‹åŒ–ã€åµŒå…¥æ¨¡å‹é€‰æ‹©ã€é’ˆå¯¹æœ‰æ•ˆRAGçš„è¯­ä¹‰æŸ¥è¯¢ã€‚'
- en: '**Benchmarking** the result against OpenAI Models; comparing with GPT-3.5-turbo
    output.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åŸºå‡†æµ‹è¯•**ç»“æœä¸OpenAIæ¨¡å‹ï¼›ä¸GPT-3.5-turboè¾“å‡ºè¿›è¡Œæ¯”è¾ƒã€‚'
- en: 1\. Key-Concepts
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. å…³é”®æ¦‚å¿µ
- en: Before diving into the implementation, it is essential to familiarise ourselves
    with the basics.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥å®æ–½ä¹‹å‰ï¼Œç†Ÿæ‚‰åŸºç¡€çŸ¥è¯†æ˜¯å¿…è¦çš„ã€‚
- en: Embeddings
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åµŒå…¥
- en: The original docsâ€™ text has to be transformed into a vector representation,
    which is essential for a relevancy search to be performed. In a nutshell, this
    kind of element enables ML models to find similarities between them, hence better
    understand the prevailing relationships between the original data (i.e. words).
    This transformation is done by making use of an embedding model [2].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æ¡£çš„æ–‡æœ¬å¿…é¡»è½¬åŒ–ä¸ºå‘é‡è¡¨ç¤ºï¼Œè¿™å¯¹æ‰§è¡Œç›¸å…³æ€§æœç´¢è‡³å…³é‡è¦ã€‚ç®€è€Œè¨€ä¹‹ï¼Œè¿™ç§å…ƒç´ ä½¿å¾—æœºå™¨å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿæ‰¾åˆ°å®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£åŸå§‹æ•°æ®ï¼ˆå³å•è¯ï¼‰ä¹‹é—´çš„å…³ç³»ã€‚è¿™ç§è½¬æ¢æ˜¯é€šè¿‡ä½¿ç”¨åµŒå…¥æ¨¡å‹[2]æ¥å®Œæˆçš„ã€‚
- en: RAG
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG
- en: 'Retrieval Augmented Generation constitutes a combined method of the broad Natural
    Language Processing (NLP) field, which operates in a dual way:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„ä¸€ä¸ªç»¼åˆæ–¹æ³•ï¼Œä»¥åŒé‡æ–¹å¼è¿ä½œï¼š
- en: '**Retrieves** pieces of information from relevant artifacts (documents) and'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ£€ç´¢**ç›¸å…³æ–‡æ¡£ä¸­çš„ä¿¡æ¯'
- en: '**Generates** responses based on that.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç”Ÿæˆ**åŸºäºè¿™äº›ä¿¡æ¯çš„å“åº”ã€‚'
- en: Its implementation refers to cases, like the one implemented herein, where the
    user wants to retrieve data outside a foundation model and then add it to the
    context to amplify their prompts [3].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶å®ç°æ¶‰åŠåˆ°ç”¨æˆ·å¸Œæœ›ä»åŸºç¡€æ¨¡å‹ä¹‹å¤–æ£€ç´¢æ•°æ®çš„æƒ…å†µï¼Œç„¶åå°†å…¶æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­ä»¥å¢å¼ºä»–ä»¬çš„æç¤º[3]ã€‚
- en: '![](../Images/6db7e6363cbc88608c1ec93de1d90379.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6db7e6363cbc88608c1ec93de1d90379.png)'
- en: Workflow Image by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥ä½œæµå›¾ç‰‡ç”±ä½œè€…æä¾›
- en: As shown above, the user query evokes the retrieval of relevant context by the
    knowledge base â€” this is the RAG modelâ€™s job. The context, then, enhances the
    original prompt and the now *augmented* prompt feeds the foundation LLM model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šæ‰€ç¤ºï¼Œç”¨æˆ·æŸ¥è¯¢è§¦å‘äº†çŸ¥è¯†åº“çš„ç›¸å…³ä¸Šä¸‹æ–‡æ£€ç´¢â€”â€”è¿™æ˜¯RAGæ¨¡å‹çš„å·¥ä½œã€‚ç„¶åï¼Œè¿™äº›ä¸Šä¸‹æ–‡å¢å¼ºäº†åŸå§‹æç¤ºï¼Œç°å·²*å¢å¼º*çš„æç¤ºåˆ™è¾“å…¥åˆ°åŸºç¡€LLMæ¨¡å‹ä¸­ã€‚
- en: BLING Models
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BLINGæ¨¡å‹
- en: It is all about a handy collection of open-source small [models](https://huggingface.co/collections/llmware/bling-models-6553c718f51185088be4c91a)
    (1B-3B parameter) optimised for RAG implementation, whose purpose is to run on
    CPU-based infrastructure. They are developed by HuggingFace for knowledge-intensive
    industries (i.e. financial, legal, regulatory etc) and their implementation refers
    to cases of sensitive data that must be closely guardedâ€” actually our enterprise
    case!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å®Œå…¨æ˜¯ä¸€ä¸ªä¾¿æ·çš„å¼€æºå°å‹[æ¨¡å‹](https://huggingface.co/collections/llmware/bling-models-6553c718f51185088be4c91a)ï¼ˆ1B-3B
    å‚æ•°ï¼‰ï¼Œç»è¿‡ä¼˜åŒ–ä»¥å®ç°RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰å®æ–½ï¼Œæ—¨åœ¨è¿è¡Œåœ¨åŸºäºCPUçš„åŸºç¡€è®¾æ–½ä¸Šã€‚å®ƒä»¬ç”±HuggingFaceå¼€å‘ï¼Œé’ˆå¯¹çŸ¥è¯†å¯†é›†å‹è¡Œä¸šï¼ˆå³é‡‘èã€æ³•å¾‹ã€ç›‘ç®¡ç­‰ï¼‰ï¼Œå…¶å®ç°æ¶‰åŠå¿…é¡»ä¸¥æ ¼ä¿æŠ¤çš„æ•æ„Ÿæ•°æ®â€”â€”è¿™å®é™…ä¸Šæ˜¯æˆ‘ä»¬çš„ä¼ä¸šæ¡ˆä¾‹ï¼
- en: Now that we understand the basic tech nuances behind our tool, letâ€™s get coding!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬äº†è§£äº†å·¥å…·èƒŒåçš„åŸºæœ¬æŠ€æœ¯ç»†èŠ‚ï¼Œè®©æˆ‘ä»¬å¼€å§‹ç¼–ç¨‹å§ï¼
- en: 2\. Environment Setup
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. ç¯å¢ƒè®¾ç½®
- en: 'In this section, we are going to establish the requisite environment to run
    the code. In a CLI (Command Line Interface), simply run the following code snippets:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å»ºç«‹è¿è¡Œä»£ç æ‰€éœ€çš„ç¯å¢ƒã€‚åœ¨å‘½ä»¤è¡Œç•Œé¢ï¼ˆCLIï¼‰ä¸­ï¼Œåªéœ€è¿è¡Œä»¥ä¸‹ä»£ç ç‰‡æ®µï¼š
- en: Install [transformers](https://pypi.org/project/transformers/) â€” an open-source
    toolkit of pretrained models that perform tasks on several modalities combined.
    It provides APIs to quickly download and use them.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®‰è£…[transformers](https://pypi.org/project/transformers/) â€”â€” ä¸€ä¸ªå¼€æºçš„é¢„è®­ç»ƒæ¨¡å‹å·¥å…·åŒ…ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæ¨¡æ€ä¸Šæ‰§è¡Œä»»åŠ¡ã€‚å®ƒæä¾›äº†å¿«é€Ÿä¸‹è½½å’Œä½¿ç”¨æ¨¡å‹çš„APIã€‚
- en: Install the [llmware](https://pypi.org/project/llmware/) framework â€” an enterprise-grade
    LLM-based development framework with tools and fine-tuned models, including the
    retrieval library.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®‰è£…[llmware](https://pypi.org/project/llmware/)æ¡†æ¶ â€”â€” ä¸€ä¸ªä¼ä¸šçº§åŸºäºLLMçš„å¼€å‘æ¡†æ¶ï¼Œæä¾›å·¥å…·å’Œå¾®è°ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬æ£€ç´¢åº“ã€‚
- en: install.py
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: install.py
- en: Install [Docker](https://www.docker.com/products/docker-desktop/) â€” a containerisation
    software that will let us run a compose file from LLMWare, including all the necessary
    tools, like the [Milvus](https://milvus.io) vector database & [MongoDB](https://www.mongodb.com).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®‰è£…[Docker](https://www.docker.com/products/docker-desktop/) â€”â€” ä¸€ä¸ªå®¹å™¨åŒ–è½¯ä»¶ï¼Œå…è®¸æˆ‘ä»¬ä»LLMWareè¿è¡Œä¸€ä¸ªcomposeæ–‡ä»¶ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¿…è¦çš„å·¥å…·ï¼Œå¦‚[Milvus](https://milvus.io)å‘é‡æ•°æ®åº“å’Œ[MongoDB](https://www.mongodb.com)ã€‚
- en: docker_compose.bash
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: docker_compose.bash
- en: Import the libraries
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¼å…¥åº“
- en: import.py
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: import.py
- en: 'To confirm the setup, you may follow the quick start guide from [LLMWare](https://github.com/llmware-ai/llmware?ref=hackernoon.com)
    or opt to directly run the following code (*for now, let that run entirely - we
    are going to explain everything later on)*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®è®¤è®¾ç½®ï¼Œä½ å¯ä»¥æŒ‰ç…§[LLMWare](https://github.com/llmware-ai/llmware?ref=hackernoon.com)çš„å¿«é€Ÿå…¥é—¨æŒ‡å—ï¼Œæˆ–é€‰æ‹©ç›´æ¥è¿è¡Œä»¥ä¸‹ä»£ç ï¼ˆ*ç°åœ¨ï¼Œè®©å®ƒå®Œå…¨è¿è¡Œ
    - æˆ‘ä»¬ç¨åä¼šè§£é‡Šä¸€åˆ‡*ï¼‰ï¼š
- en: test_query.py
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: test_query.py
- en: An output, including query results, indicates that you have successfully set
    up your machine. Letâ€™s move on to the next part!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºï¼ŒåŒ…æ‹¬æŸ¥è¯¢ç»“æœï¼Œè¡¨æ˜ä½ å·²ç»æˆåŠŸè®¾ç½®äº†æœºå™¨ã€‚è®©æˆ‘ä»¬ç»§ç»­ä¸‹ä¸€éƒ¨åˆ†å§ï¼
- en: 3\. Tool Development
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. å·¥å…·å¼€å‘
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, letâ€™s proceed with the fun part--Hands-on!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¿›å…¥æœ‰è¶£çš„éƒ¨åˆ†â€”â€”åŠ¨æ‰‹å®è·µå§ï¼
- en: Step 1 - Vector Database
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤1 - å‘é‡æ•°æ®åº“
- en: First things first, we have to create a database which will host our enterprise
    texts. These informational pieces are represented in the multi-dimensional space
    as vectors (embeddings) and the databases capable of storing them belong to the
    family of vector databases [4].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»åˆ›å»ºä¸€ä¸ªæ•°æ®åº“æ¥å­˜å‚¨æˆ‘ä»¬çš„ä¼ä¸šæ–‡æœ¬ã€‚è¿™äº›ä¿¡æ¯åœ¨å¤šç»´ç©ºé—´ä¸­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆåµŒå…¥ï¼‰ï¼Œèƒ½å¤Ÿå­˜å‚¨å®ƒä»¬çš„æ•°æ®åº“å±äºå‘é‡æ•°æ®åº“çš„èŒƒç•´[4]ã€‚
- en: '[Milvus](https://milvus.io) is the open-source solution we are going to utilise,
    which will help us execute semantic queries on the firmâ€™s docs. All we have to
    do is create a folder and move those docs in there. Then just copy the `samples_path`
    in the following snippet:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[Milvus](https://milvus.io) æ˜¯æˆ‘ä»¬å°†è¦ä½¿ç”¨çš„å¼€æºè§£å†³æ–¹æ¡ˆï¼Œå®ƒå°†å¸®åŠ©æˆ‘ä»¬å¯¹å…¬å¸çš„æ–‡æ¡£æ‰§è¡Œè¯­ä¹‰æŸ¥è¯¢ã€‚æˆ‘ä»¬åªéœ€åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹å¹¶å°†è¿™äº›æ–‡æ¡£ç§»åŠ¨åˆ°é‚£é‡Œã€‚ç„¶ååªéœ€å¤åˆ¶ä»¥ä¸‹ä»£ç ç‰‡æ®µä¸­çš„
    `samples_path`ï¼š'
- en: vector_db.py
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: vector_db.py
- en: Step 2 - Embedding Model
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ­¥ - åµŒå…¥æ¨¡å‹
- en: 'As anticipated, the embeddings take positionâ€¦ Since we apply our tool to the
    Asset Management domain, we may opt to choose a relevant embedding model. Hopefully,
    HuggingFace provides a purpose-build one, named: `[industry-bert-asset-management](https://huggingface.co/llmware/industry-bert-asset-management-v0.1)`.
    It is a BERT-based Industry domain Transformer designed for embeddings in the
    asset management industry.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼ŒåµŒå…¥å°†è¢«å®šä½â€¦â€¦ç”±äºæˆ‘ä»¬å°†å·¥å…·åº”ç”¨äºèµ„äº§ç®¡ç†é¢†åŸŸï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸€ä¸ªç›¸å…³çš„åµŒå…¥æ¨¡å‹ã€‚å¸Œæœ› HuggingFace æä¾›äº†ä¸€ä¸ªä¸“é—¨æ„å»ºçš„æ¨¡å‹ï¼Œåä¸ºï¼š`[industry-bert-asset-management](https://huggingface.co/llmware/industry-bert-asset-management-v0.1)`ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäº
    BERT çš„è¡Œä¸šé¢†åŸŸ Transformerï¼Œä¸“ä¸ºèµ„äº§ç®¡ç†è¡Œä¸šä¸­çš„åµŒå…¥è®¾è®¡ã€‚
- en: embeddings.py
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: embeddings.py
- en: Step 3 - Semantic Query
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰æ­¥ - è¯­ä¹‰æŸ¥è¯¢
- en: 'Next, we construct our semantic query and pass it through the vector database,
    requesting for 2 resutls. For simplicity reasons (*see Assumption #1*), the query
    to be performed is: â€œWhat is defined as criticality?â€'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ„é€ æˆ‘ä»¬çš„è¯­ä¹‰æŸ¥è¯¢ï¼Œå¹¶å°†å…¶ä¼ é€’é€šè¿‡å‘é‡æ•°æ®åº“ï¼Œè¦æ±‚è¿”å› 2 ä¸ªç»“æœã€‚ä¸ºäº†ç®€åŒ–èµ·è§ï¼ˆ*å‚è§å‡è®¾ #1*ï¼‰ï¼Œè¦æ‰§è¡Œçš„æŸ¥è¯¢æ˜¯ï¼šâ€œä»€ä¹ˆè¢«å®šä¹‰ä¸ºå…³é”®æ€§ï¼Ÿâ€'
- en: semantic_search.py
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: semantic_search.py
- en: Step 4- BLING Models
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬å››æ­¥ - BLING æ¨¡å‹
- en: After getting the source data, we will pass it to the selected BLING models.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–æºæ•°æ®åï¼Œæˆ‘ä»¬å°†æŠŠå®ƒä¼ é€’ç»™é€‰æ‹©çš„ BLING æ¨¡å‹ã€‚
- en: Initially, we set a variable, `embedded_text`, to store the final concatenated
    text, derived from the items in the `query_res` list. Next, we instantiate a Prompt
    Object from LLMWare (`prompter`) to overcome the strict prompting structure. Then,
    we check all the relevant HuggingFace `models` to achieve the best possible performance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬è®¾ç½®ä¸€ä¸ªå˜é‡ `embedded_text` æ¥å­˜å‚¨ä» `query_res` åˆ—è¡¨ä¸­çš„é¡¹æ‹¼æ¥å¾—åˆ°çš„æœ€ç»ˆæ–‡æœ¬ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä» LLMWare
    å®ä¾‹åŒ–ä¸€ä¸ª Prompt å¯¹è±¡ï¼ˆ`prompter`ï¼‰ï¼Œä»¥å…‹æœä¸¥æ ¼çš„æç¤ºç»“æ„ã€‚ç„¶åï¼Œæˆ‘ä»¬æ£€æŸ¥æ‰€æœ‰ç›¸å…³çš„ HuggingFace `models` ä»¥å®ç°æœ€ä½³æ€§èƒ½ã€‚
- en: rag.py
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: rag.py
- en: 'For better clarity, I hereby depict a condensed version of the output including
    the model-answer pairs:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´æ¸…æ¥šï¼Œæˆ‘åœ¨æ­¤æè¿°äº†ä¸€ä¸ªç²¾ç®€ç‰ˆæœ¬çš„è¾“å‡ºï¼ŒåŒ…æ‹¬æ¨¡å‹-å›ç­”å¯¹ï¼š
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***Note****: Your results may vary due to the stochastic nature of the models.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ³¨æ„***ï¼šç”±äºæ¨¡å‹çš„éšæœºæ€§è´¨ï¼Œæ‚¨çš„ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚'
- en: 4\. Validation
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. éªŒè¯
- en: 'At first, it is wise to show what the involved docs state about the â€˜criticalityâ€™
    term. According to Assumption #2, only `doc_1` is relevant and saysâ€¦'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 'èµ·åˆï¼Œæ˜æ™ºçš„åšæ³•æ˜¯å±•ç¤ºæ¶‰åŠçš„æ–‡æ¡£å¯¹â€œå…³é”®æ€§â€æœ¯è¯­çš„è¯´æ˜ã€‚æ ¹æ®å‡è®¾ #2ï¼Œåªæœ‰ `doc_1` æ˜¯ç›¸å…³çš„ï¼Œå¹¶ä¸”è¯´â€¦â€¦'
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Given that, it is easy to conclude that not all models adequately performed,
    except for the `*llmware/bling-falcon-1b-0.1*`, which encapsulates both the mathematical
    assessment (aka product) and the prioritisation semantics:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºæ­¤ï¼Œå¾ˆå®¹æ˜“å¾—å‡ºç»“è®ºï¼Œé™¤äº† `*llmware/bling-falcon-1b-0.1*`ï¼Œå…¶ä»–æ¨¡å‹çš„è¡¨ç°å‡ä¸å°½å¦‚äººæ„ï¼Œè¯¥æ¨¡å‹åŒ…å«äº†æ•°å­¦è¯„ä¼°ï¼ˆå³äº§å“ï¼‰å’Œä¼˜å…ˆçº§è¯­ä¹‰ï¼š
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Benchmarking against GPT-3.5-turbo
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ GPT-3.5-turbo çš„åŸºå‡†æµ‹è¯•
- en: After setting up our own OpenAI key, we can use a `prompter` with any desired
    model (in our case, GPT-3.5) and add the `query_results`. In this way, we are
    ready to query the LLM with the sources and query string.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¾ç½®å¥½æˆ‘ä»¬çš„ OpenAI å¯†é’¥åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•æ‰€éœ€çš„æ¨¡å‹ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯ GPT-3.5ï¼‰å’Œæ·»åŠ  `query_results` çš„ `prompter`ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥å‡†å¤‡å¥½ç”¨æºå’ŒæŸ¥è¯¢å­—ç¬¦ä¸²å¯¹
    LLM è¿›è¡ŒæŸ¥è¯¢ã€‚
- en: validate_gpt.py
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: validate_gpt.py
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The answer is very good and when compared with our toolâ€™s output, the deduction
    is more than pleasing. Particularly, even though our implementation lacked a brief
    referral to *asset importance* and *resources*, it managed to include the *mathematical*
    assessment (aka product) and *prioritisation* semantics, both derived from proprietary
    docs. In other words, that means that our modest model of 1B parameters, locally
    run on a rather humble laptop, managed to compete with an original LLM of 20B
    parameters!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆéå¸¸å¥½ï¼Œä¸æˆ‘ä»¬å·¥å…·çš„è¾“å‡ºç›¸æ¯”ï¼Œæ¨å¯¼ç»“æœä»¤äººéå¸¸æ»¡æ„ã€‚ç‰¹åˆ«æ˜¯ï¼Œå°½ç®¡æˆ‘ä»¬çš„å®ç°ç¼ºå°‘äº†å¯¹*èµ„äº§é‡è¦æ€§*å’Œ*èµ„æº*çš„ç®€è¦å‚è€ƒï¼Œä½†å®ƒæˆåŠŸåœ°åŒ…å«äº†*æ•°å­¦*è¯„ä¼°ï¼ˆä¹Ÿå°±æ˜¯äº§å“ï¼‰å’Œ*ä¼˜å…ˆçº§*è¯­ä¹‰ï¼Œè¿™äº›éƒ½æºäºä¸“æœ‰æ–‡æ¡£ã€‚æ¢å¥è¯è¯´ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬é‚£åªæœ‰1Bå‚æ•°ã€åœ¨ç›¸å½“æ™®é€šçš„ç¬”è®°æœ¬ç”µè„‘ä¸Šæœ¬åœ°è¿è¡Œçš„ç®€é™‹æ¨¡å‹ï¼Œç«Ÿç„¶èƒ½å¤Ÿä¸ä¸€ä¸ª20Bå‚æ•°çš„åŸå§‹LLMè¿›è¡Œç«äº‰ï¼
- en: Conclusion
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Once again, it was a delightful journeyâ€¦ Starting with zero-cost tools and leveraging
    state-of-the-art open-source frameworks like that of LLMWare, we managed to easily
    develop a privacy-first AI tool for context analysis.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å†ä¸€æ¬¡ï¼Œè¿™æ˜¯ä¸€æ¬¡æ„‰å¿«çš„æ—…ç¨‹â€¦â€¦ä»é›¶æˆæœ¬å·¥å…·å¼€å§‹ï¼Œåˆ©ç”¨æœ€å…ˆè¿›çš„å¼€æºæ¡†æ¶å¦‚LLMWareï¼Œæˆ‘ä»¬è½»æ¾å¼€å‘äº†ä¸€ä¸ªéšç§ä¼˜å…ˆçš„AIå·¥å…·ç”¨äºä¸Šä¸‹æ–‡åˆ†æã€‚
- en: Though we utilised small LLMs of even **1/20** of the GPT-3.5-turbo model, with
    **no GPU** engagement at all, the output was remarkable! We dare to claim that
    our tool managed to retrieve the most essential pieces from the corporate docs
    and â€œmindfullyâ€ combined them with the LLM response.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»¬ä½¿ç”¨äº†ç”šè‡³æ˜¯**1/20** GPT-3.5-turboæ¨¡å‹çš„å°å‹LLMï¼Œä¸”**å®Œå…¨ä¸ä½¿ç”¨GPU**ï¼Œä½†è¾“å‡ºç»“æœè¿˜æ˜¯éå¸¸å‡ºè‰²ï¼æˆ‘ä»¬æ•¢äºå£°ç§°ï¼Œæˆ‘ä»¬çš„å·¥å…·æˆåŠŸåœ°ä»å…¬å¸æ–‡æ¡£ä¸­æå–äº†æœ€é‡è¦çš„ä¿¡æ¯ï¼Œå¹¶â€œæœ‰æ„è¯†åœ°â€å°†å…¶ä¸LLMçš„å“åº”ç›¸ç»“åˆã€‚
- en: But, most importantly, this attempt sets the foundation for overcoming GPU-related
    costs and privacy concerns, when manipulating third-party commercial LLM solutions.
    If something is to be taken for granted, that is the fact that firms â€” even the
    small ones â€” can significantly benefit from similar implementations. Running on-prem,
    potentially with proprietary GPUs for an extra boost, they can optimise their
    operations and catch up with this new LLM hype.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æœ€é‡è¦çš„æ˜¯ï¼Œè¿™æ¬¡å°è¯•ä¸ºå…‹æœä¸GPUç›¸å…³çš„æˆæœ¬å’Œéšç§é—®é¢˜å¥ å®šäº†åŸºç¡€ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç¬¬ä¸‰æ–¹å•†ä¸šLLMè§£å†³æ–¹æ¡ˆæ—¶ã€‚å¦‚æœæœ‰ä»»ä½•äº‹æƒ…æ˜¯ç†æ‰€å½“ç„¶çš„ï¼Œé‚£å°±æ˜¯ä¼ä¸šâ€”â€”å³ä½¿æ˜¯å°ä¼ä¸šâ€”â€”å¯ä»¥ä»ç±»ä¼¼çš„å®ç°ä¸­è·å¾—æ˜¾è‘—çš„å¥½å¤„ã€‚é€šè¿‡åœ¨æœ¬åœ°è¿è¡Œï¼Œå¯èƒ½è¿˜ä½¿ç”¨ä¸“æœ‰GPUä»¥è·å¾—é¢å¤–çš„æå‡ï¼Œå®ƒä»¬å¯ä»¥ä¼˜åŒ–å…¶è¿è¥å¹¶èµ¶ä¸Šè¿™ä¸€æ–°çš„LLMçƒ­æ½®ã€‚
- en: '![](../Images/6070896875d8d42a3057e87345994031.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6070896875d8d42a3057e87345994031.png)'
- en: Photo by [Andy Holmes](https://unsplash.com/@andyjh07?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/black-flat-screen-computer-monitor-xA26xebY3dw?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Andy Holmes](https://unsplash.com/@andyjh07?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)æä¾›ï¼ŒåˆŠç™»åœ¨[Unsplash](https://unsplash.com/photos/black-flat-screen-computer-monitor-xA26xebY3dw?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)ä¸Š
- en: Thank you for reading & have a Merry Christmas! Should any questions arise,
    feel free to leave a comment below or reach out to me on [ğ•](https://twitter.com/MPlegas)
    / [LinkedIn](https://www.linkedin.com/in/gerasimosplegas). In any caseâ€¦
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼Œç¥æ‚¨åœ£è¯å¿«ä¹ï¼å¦‚æœæœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶åœ¨ä¸‹é¢ç•™è¨€æˆ–é€šè¿‡[ğ•](https://twitter.com/MPlegas) / [LinkedIn](https://www.linkedin.com/in/gerasimosplegas)ä¸æˆ‘è”ç³»ã€‚æ— è®ºå¦‚ä½•â€¦â€¦
- en: 'Enjoy your holiday, clone the [repo](https://github.com/makispl/bling-rag-llm-projects)
    and hire the nextâ€¦ #LLM ğŸ˜‰'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 'äº«å—å‡æœŸï¼Œå…‹éš†[repo](https://github.com/makispl/bling-rag-llm-projects)ï¼Œå¹¶æ‹›è˜ä¸‹ä¸€ä¸ªâ€¦â€¦ #LLM
    ğŸ˜‰'
- en: '**References**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‚è€ƒæ–‡çŒ®**'
- en: '[1] [https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)'
- en: '[2] [https://www.cloudflare.com/en-gb/learning/ai/what-are-embeddings/](https://www.cloudflare.com/en-gb/learning/ai/what-are-embeddings/)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://www.cloudflare.com/en-gb/learning/ai/what-are-embeddings/](https://www.cloudflare.com/en-gb/learning/ai/what-are-embeddings/)'
- en: '[3] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, P. Lewis
    et al, 2020, [arXiv:2005.11401](https://arxiv.org/abs/2005.11401)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, P. Lewis
    et al, 2020, [arXiv:2005.11401](https://arxiv.org/abs/2005.11401)'
- en: '[4] [https://www.pinecone.io/learn/vector-database/](https://www.pinecone.io/learn/vector-database/)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://www.pinecone.io/learn/vector-database/](https://www.pinecone.io/learn/vector-database/)'
