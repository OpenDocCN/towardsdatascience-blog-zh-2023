- en: The Only Guide You Need to Understand Regression Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-only-guide-you-need-to-understand-regression-trees-4964992a07a8](https://towardsdatascience.com/the-only-guide-you-need-to-understand-regression-trees-4964992a07a8)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Complete Guide to Decision Trees with a Step-by-Step Implementation from Scratch
    and Hands-On Example Using Scikit-Learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dmnkplzr.medium.com/?source=post_page-----4964992a07a8--------------------------------)[![Dominik
    Polzer](../Images/7e48cd15df31a0ab961391c0d57521de.png)](https://dmnkplzr.medium.com/?source=post_page-----4964992a07a8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4964992a07a8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4964992a07a8--------------------------------)
    [Dominik Polzer](https://dmnkplzr.medium.com/?source=post_page-----4964992a07a8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4964992a07a8--------------------------------)
    ·25 min read·Apr 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c91b3953a804aad1f6ca79fb81508c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Build a tree - Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Table of Content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Introduction](#b3c0)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Decision trees for regression: the theory behind them](#8935)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[From theory to practice — Decision Trees from scratch](#6edb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Hands-On Example — Implementation from scratch vs. Scikit-learn DecisionTree](#72e7)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](#0ce8)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[References](#9542)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Appendix / Code](#e952)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision Trees have been around since the 1960s. Despite being one of the simplest
    Machine Learning algorithms, they have proven to be highly effective in solving
    problems. One of their greatest advantages is their ease of interpretation, making
    them highly accessible to those without a technical background. In many industries,
    Data Scientists still have to build trust for Machine Learning use cases. Explainable
    baseline models like Decision Trees can help reduce the skepticism somewhat. If
    someone wanted to make the effort, they could even trace the branches of the learned
    tree and try to find patterns they already know about the problem.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, we quickly reach the limits of simple decision trees for
    complex problems. Theoretically, we can model any (complex) distribution of the
    data with appropriately sized trees, but the models often do not generalize well
    enough when applied to new data — they overfit the train dataset. Yet, decision
    trees have always played an important role in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Some weaknesses of Decision Trees have been gradually solved or at least mitigated
    over time by the progress made with Tree Ensembles. In Tree Ensembles, we do not
    learn one decision tree, but a whole series of trees and finally combine them
    into an ensemble. Nowadays we distinguish between bagging and boosting algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In Bagging, multiple decision trees are trained on different bootstrap samples
    (randomly selected subsets with replacement) of the training data. Each decision
    tree is trained independently, and the final prediction is made by averaging the
    predictions of all the individual trees. The **bagging** approach and in particular
    the **Random Forest** algorithm was developed by Leo Breiman.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Boosting, decision trees are trained sequentially, where each tree is trained
    to correct the errors made by the previous tree. The training data is weighted,
    with more weight given to the misclassified samples from the previous tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if random forest still plays an important role, today it is mostly **boosting**
    algorithms that perform best in data science competitions and often outperform
    bagging methods. Among the best known boosting algorithms are **AdaBoost, XGBoost,
    LightGBM and CatBoost**. Since 2016, their growth in popularity has continued
    relentlessly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5faf6e506a5d37c156699d3171a81381.png)'
  prefs: []
  type: TYPE_IMG
- en: Type of tree-based algorithms — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'While the concept of decision trees has been known and actively applied for
    several decades, boosting approaches are relatively "new" and only gradually gained
    importance with the release of XGBoost in 2014:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f255246a04156f5e320db518fb9d9b8c.png)'
  prefs: []
  type: TYPE_IMG
- en: The evolution of tree based algorithms — Image by the author (inspired by (Chow,
    2021; Swalin, 2019))
  prefs: []
  type: TYPE_NORMAL
- en: Just a few months after the initial release of the concept behind XGBoost, the
    Higgs Boson Challenge on Kaggle was won with it. XGBoost is based on a number
    of concepts that add up to an extremely effective algorithm. The core of XGBoost
    is of course, the principle of gradient boosting, but XGBoost is much more. XGBoost
    includes various optimization techniques, which makes XGBoost extremely efficient
    and fast in the training process. Especially for small to medium sized structured
    datasets, gradient boosting framerworks like XGBoost, LightGBM and CatBoost continue
    to play a mayor role.
  prefs: []
  type: TYPE_NORMAL
- en: It is not just my opinion. A good indicator are Kaggle competitions and their
    winning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In the article "The State of Competitive Machine Learning", [](https://www.google.com/url?q=http%3A%2F%2Fmlcontests.com%2F&sa=D&ust=1680021204969306&usg=AOvVaw2gAknvaBr5xG8QHfDVyQIM)
    [mlcontests.com](https://mlcontests.com/) evaluated more than 200 data competitions
    in 2022 on Kaggle and other competition platforms. According to the report, gradient-boosted
    decision trees (GBDT) are still the go-to approach for tabular data use cases
    in 2022 and could manage to win most of the competitions in this area. (Carlens,
    n.d.)
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the good performance that gradient boosting algorithms are showing
    again and again, the biggest advantage of decision trees or tree ensembles is
    speed. In general, gradient-boosting frameworks are faster in training compared
    to NNs, which can be an important factor in many real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: Often the data set is not clear at the beginning of an ML project. A major part
    of the work is the compilation of the dataset and extraction of relevant features.
    If we change the dataset, add a new column, or just slightly change the way we
    convert categorical values to numerical ones, we need a measure of whether we
    have improved or worsened the overall process by doing so. In this process, we
    may train models several hundred times. A faster training time can thus decisively
    influence the time for the entire development process of ML use cases.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d29ccfc4e9cb40d8de9e9d3aeb81bc3c.png)'
  prefs: []
  type: TYPE_IMG
- en: ML projects are iterative not linear — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: The following figure shows the individual steps along the ML pipeline. If we
    change one small thing in the process before we train the model, we have to re-evaluate
    the whole process and the resulting models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ac161e3da9040b785648e2458ac3421.png)'
  prefs: []
  type: TYPE_IMG
- en: ML pipeline — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**Content of the article:**'
  prefs: []
  type: TYPE_NORMAL
- en: This article is intended to lay the foundation to dive into various types of
    tree-based ensemble algorithms which are all based on Decision Trees. The concept
    of decision trees is very intuitive and easy to understand. At first glance somewhat
    more complex are XGBoost, CatBoostc and LightGBM. But if you take a closer look,
    XGBoost is just a combination of different concepts, which are again easy to understand
    each by itself.
  prefs: []
  type: TYPE_NORMAL
- en: Once you understand the random forest and gradient boosting frameworks, you
    will be able to solve a wide range of data science problems. From classifications
    to regression to anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: It's kind of absurd that knowledge about Deep Learning frameworks like Pytorch,
    TensorFlow, and Co plays such a central role in almost every Data Science job
    posting. In many areas, you will spend most of your time collecting data, preparing
    data, and extracting features. If you have the right feature set, the model creation
    itself is often quite straightforward. If you deal mainly with tabular data, you
    will probably get quite far with bagging and boosting algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to download the code used in the article all at once and use it
    for reference, you can find the code snippets used on [github](https://github.com/polzerdo55862/decision-tree-from-scratch).
    You can also find the code for the decision tree algorithm that we will build
    in this article in the appendix, at the bottom of this article.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Decision Trees for Regression: The theory behind it'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are among the simplest machine learning algorithms. The way they
    work is relatively easy to explain.
  prefs: []
  type: TYPE_NORMAL
- en: We, as humans, try to solve complex problems by breaking them down into relatively
    simple yes or no decisions. When we want to buy a new car, we browse all the car
    websites we can find. After a while, we get a feeling for how much a certain car
    make and model should cost. We get a feeling for how big the cost difference is
    between luxury brands and cheaper manufacturers and how much more we have to pay
    for a 150 hp engine compared to a smaller 100 hp engine and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Step by step, our brain memorizes certain ballpark values for certain combinations
    of features. When we then stop at a car dealer and go through the features of
    a car one by one, it is as if we are moving down a decision tree until we arrive
    at what we think is a fair price.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82efa6269742ac738614158a1bea8b91.png)'
  prefs: []
  type: TYPE_IMG
- en: A simple Regression Tree that predicts car prices — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '***Note:*** *Before we go into how Decision trees are built, it is important
    to mention that there are different Decision Tree algorithms. Some popular ones
    are ID3, C4.5, C5.0 and CART (Google Developers, 2017). Scikit-learns implementation
    is based on CART, which was first published in 1984 by Leo Breiman et al. Leo
    Breiman was an American statistician who shaped the approach of "bagging", developed
    the random forest and thus contributed significantly to the further development
    of tree-based algorithms.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**How do we start build the tree?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the Decision Tree building process, we need to answer three questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Which feature do we start with?** - We split the data set at each node along
    one dimension. For the example, we use the feature x_1 for splitting. Since we
    don''t want to choose just random features, we search in advance for the feature
    where splitting the data set offers the greatest added value. (In this context
    we often speak about the so-called information gain. We can define the information
    gain in different ways, in regression we often use the *squared error*).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**What is the best threshold to split the data?** - Similar to the first step,
    when we choose a feature, we still need to know the threshold we want to use to
    split the data set. So in other words, at what position along the dimension we
    want to split the data set.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**When do we stop splitting the data set?** - If we do not stop the splitting
    process at some point, the decision tree will continue until there is only one
    sample point in each leaf node. To avoid overfitting, we need some criteria to
    determine how far to split the data set and when to stop the splitting process
    so that the model does not become unnecessarily complex.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/065ee3908c081a52d5a95a63e112f58c.png)'
  prefs: []
  type: TYPE_IMG
- en: 3 questions we must answer — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Let's use the example of price prediction for cars again. First, we need to
    select one of the features and split the data set. We choose a feature and a threshold
    and split the dataset into a left and a right part and calculate the average price.
    That gives us a first node. If we stopped now, we would have a minimalistic decision
    tree with only one level - a so-called decision stump.
  prefs: []
  type: TYPE_NORMAL
- en: However, we do not want to start with a random split, but with the "best possible"
    split.
  prefs: []
  type: TYPE_NORMAL
- en: '**But how is the “best” split defined?**'
  prefs: []
  type: TYPE_NORMAL
- en: We need to define a metrics, that helps us to evaluate how good a split performs.
  prefs: []
  type: TYPE_NORMAL
- en: A often-used loss function in regression problems to assess how well a model
    performs is the **mean absolute error** or the **mean squared error**. Usually,
    we can choose between different metrics. To get an idea how scikit-learn is calculating
    the performance of each split, we can simply have a look into the documentation
    or directly in the source code.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to access the source code is via the code editor.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have not yet installed scikit, you can do so with pip via:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: I use Visual Studio Code for most of my projects. If I create a new notebook
    or Python file and import the corresponding modules, Visual Studio provides us
    a direct link to the source code behind it. In the picture on the left side, you
    can see how the whole thing looks like in Visual Studio Code.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bebe17511d12d3dee41c0768ed1f0356.png)'
  prefs: []
  type: TYPE_IMG
- en: Sceenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: Create a new file, in my case "**tree_algorithms.py**" and import the regression
    tree module "**sklearn.tree.DecisionTreeRegressor**".
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By pressing "**Ctrl**" and clicking on the according module you will be redirected
    directly to the corresponding part of the source code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alternatively, you can find the source code in the documentation of scikit-learn.
    On the right you can see how the whole thing looks on *scikit-learn.org*. Each
    class and function has also a link to the source code on Github.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we dive into the source code of DecisionTreeRegressor, we see that it is
    defined as a class that is initiated with the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will gradually go into what the individual hyperparameters do. What we are
    interested in for the start is the splitting criterion, i.e. how the decision
    tree determines how it splits the data set during building.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7cdac0f9233af5fa7f938241bb48d5eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Sceenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: The code also contains a short description of which criterias we can select.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn lets us choose between:'
  prefs: []
  type: TYPE_NORMAL
- en: squared_error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: friedmann_mse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: aboslute_error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: poisson
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The default value is "squared_error". The documentation describes it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“squared_error” is equal to variance reduction and minimizes the L2 loss using
    the mean of each terminal node*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So we are trying to minimize the *Mean Squared Error* in the terminal nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Let's imagine we have a simple two-dimensional dataset with only x_1 as the
    single input feature and y as the target variable. For this simple example, we
    don't need to decide which feature is best to split the dataset because we only
    have one in the dataset. So at the root node, we use x_1 to split the dataset
    in half.
  prefs: []
  type: TYPE_NORMAL
- en: In the following figure, you can find a simple 2 dimensional dataset. The two
    halves of the dataset are our child nodes. At the moment we perform the first
    split, the two child nodes are the leaf nodes or terminal nodes (nodes that are
    not further split).
  prefs: []
  type: TYPE_NORMAL
- en: In the case shown, we divide the data set at x_1=2.
  prefs: []
  type: TYPE_NORMAL
- en: '**What value would the tree now predict if we used it to make prediction?**'
  prefs: []
  type: TYPE_NORMAL
- en: We have to define a value for each terminal node, which then represents the
    possible prediction values of the decision tree. We calculate this prediction
    value y_pred in the simplest way, we calculate the average of the data points
    in the left node (**here:** y_pred_left = 9) and the right node (**here:** y_pred_right
    = 5).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f44d0ed1c9312ea353eb85c46274d50.png)'
  prefs: []
  type: TYPE_IMG
- en: Split the data set — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**How do I find the best threshold for splitting the data set?**'
  prefs: []
  type: TYPE_NORMAL
- en: In the example shown, we have chosen x_1 = 2 as the threshold. But is this the
    optimal scenario?
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the performance of the split, we calculate the residuals, i.e.,
    the difference between y_predict and the y for each sample. More precisely, we
    calculate the L2 loss function, the sum of squared residuals.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e67e847ae54396e72a2a87b971520693.png)'
  prefs: []
  type: TYPE_IMG
- en: How to calculate the prediction value for the left and right leaf — Image by
    the author
  prefs: []
  type: TYPE_NORMAL
- en: To get a value for the performance of the stump, we calculate the deviations
    (l2 loss) for both sides separately and then calculate a weighted overall loss
    by including the number of samples in both halves.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do that over and over again, for different thresholds (see image). In our
    example, the weighted squared error gets minimal when we choose x_1 = 5 as the
    splitting threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28b23a80e1b1b4e7c2a14ebe07460772.png)'
  prefs: []
  type: TYPE_IMG
- en: MSE calculation for parent and child nodes — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**How does our algorithm find the smallest error?**'
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree does this in a very simple way, it defines an iterative approach,
    that tries different values as thresholds. Therefore, we define a list of possible
    thresholds/splitting values and calculate the mean squared error for each of the
    possible thresholds in the list.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1 - Define a list with possible thresholds for the splitting:** We are
    defining all possible splitting values by sorting the values and calculating the
    rolling average. So if x_1 = 2 and x_2 = 3 then the first threshold in the list
    of possible thresholds is 2.5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 2:** In the next step, we need to find the threshold that minimizes
    the squared error when building a node. We start iterating over all thresholds,
    splitting the data set, and calculating the MSE for the right and left nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b8acba7d78ed71a2b7d8400cbc68e7e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Iterative calculation of Mean Squared Error for different thesholds — Image
    by the author
  prefs: []
  type: TYPE_NORMAL
- en: Lets try it with a real world data set.
  prefs: []
  type: TYPE_NORMAL
- en: Load a real world data set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate the steps just described on a real data set, we download the
    Automobile Data Set from UCIMachineLearning. The dataset includes a bunch of attributes
    like horsepower, dimensions, fuel type etc. which describe a car in detail. The
    target variable we are interested in is the price. [(](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FY7wEQt&sa=D&ust=1680021204980533&usg=AOvVaw3AthCMTcfOQu5mgN3xM1an)*UCI
    Machine Learning Repository: Automobile Data Set*, n.d.) *[License:* [*CC0: Public
    Domain*](https://creativecommons.org/publicdomain/zero/1.0/)*]*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Afterwards, we perform exactly the steps just described. The following code
    snippet uses the selected feature **selected_feature** and the defined threshold
    **threshold** to split the data set (X_parent, y_parent).
  prefs: []
  type: TYPE_NORMAL
- en: The plot shows the samples of the left and right child nodes and the average
    of the observations. If we stopped now, the child nodes would be the leaf nodes
    of the tree and the predicted value of the tree would be represented by the calculated
    mean of the two halves.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a69dc5d600ff12c0e63645acb7ea1fdc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Since we don't want to split the dataset anywhere, but at the "best" point,
    we do this iteratively as described above. We use the node plot class to calculate
    the residuals for a number of thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ff243bc068186eb7b3324729231efc9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: We get the squared error of the parent and child nodes for each possible split.
    For decision trees, we are searching for the maximal **information gain**. Using
    the squared error as a splitting criterion, we calculate the information gain
    simply as the difference between the MSE of the parent node and the weighted MSE
    of the child nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In the chart, the Information Gain maximum lies at 120 hs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c9eb27909468b0d2a75e8db5f03ee65.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: In the Decision Tree, we perform the steps just described again and again.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: The decision tree is counted among the greedy algorithms. Greedy algorithms
    gradually select the sequential state that promises the best result during selection.
    Previous and subsequent decisions are not taken into account. when making this
    decision.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**When does the procedure end?**'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees do not make many assumptions while training a model. Linear Regression,
    for example, is just the opposite, while the linear regression algorithm trains
    a model, it allows only one possible shape of the model, a straight line or a
    planar plane in space. Thus, when we use Linear Regression as a learning algorithm,
    we directly make the assumption that our problem follows a linear behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3ce75013d8d045de69aced1652fe9b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Parametric (Linear Regression) vs. nonparametric model (Regression Tree) — Image
    by the author
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees, on the other hand, are very flexible in their learning process.
    Such models are called "nonparametric models". Models are called non-parametric
    when their number of parameters is not determined in advance. Linear regression
    has a well-defined number of parameters, the slope and the offset. This significantly
    limits the degree of freedom in the training process. (Géron, 2022)
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees thus tend to overfit. To avoid that, we need to introduce hyperparameters
    that limit the freedom of the training process, so-called regularization hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'A frequently used regularization parameter is **max_depth**, i.e. the maximum
    depth of the tree. Others are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**min_samples_split** (the minimum number of samples a node needs to be split)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**min_samples_leaf** (the minimum number of samples each leaf node must have)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**min_weight_fraction_leaf** (similar to min_samples_leaf, but instead of a
    number we define a fraction of the whole dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_leaf_nodes** (maximum number of leaf nodes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_features** (the maximum number of features evaluated at each split).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the actual model building, we can still prune the tree to avoid unnecessary
    complexity of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is pruning?**'
  prefs: []
  type: TYPE_NORMAL
- en: This technique involves growing the tree to its full size and then removing
    branches or subtrees that do not improve the accuracy of the tree on a validation
    dataset. This is done by calculating the change in error before and after pruning
    a subtree and comparing it to a threshold value. If the change in error is not
    significant, the subtree is pruned. I don’t want to go further into this for the
    moment, as I will leave it out for the following simple example.
  prefs: []
  type: TYPE_NORMAL
- en: In the following, I’ll show you how to build a basic version of a regression
    tree from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. From theory to practice - Decision Tree from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To be able to use the regression tree in a flexible way, we put the code into
    a new module. We create a new Python file, where we put all the code concerning
    our algorithm and the learning process. In it, we define a new class called "RegressionTree"
    and initialize it with the hyperparameters that serve as constraints for the training
    process.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, one of the biggest challenges in working with decision
    trees is the risk of overfitting. To mitigate this risk and ensure that our model
    generalizes well to new data, we introduce regularisation parameters that guide
    and stop the learning process at a certain point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The regulation parameters (or stopping criteria) that we use in our simplified
    version of Decision Trees are the following two:'
  prefs: []
  type: TYPE_NORMAL
- en: '**min_samples_split**'
  prefs: []
  type: TYPE_NORMAL
- en: defines the maximum number of samples that a node needs in order to be split
    further. A suitable value depends on the type and size of the dataset. If chosen
    correctly, it can prevent overfitting. In scikit-learn, the default value is set
    to 2 samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_depth**'
  prefs: []
  type: TYPE_NORMAL
- en: the maximum depth determines how many levels the tree can have at most. If another
    stopping criterion such as *min_sample_split* prevents further growth of the tree
    on all branches before this depth is reached, it may not even reach this tree
    size. Scikit-learn sets the default value to “None”, so the maximum depth is not
    limited by default.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-learn includes a few additional stopping parameters such as **min_samples_leaf,
    min_weighted_fraction, max_leaf_nodes, or max_features**, which are also not set
    by default and I will ignore for now.
  prefs: []
  type: TYPE_NORMAL
- en: A function that we need for every regressor is the fit function, which starts
    the training process. Input variables are a multi-dimensional array (X) with the
    input features. y is a one-dimensional array and describes the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to our regressor (**RegressionTree**), we define a second class
    (**Node**) through which we set and store the parameters that each node of the
    tree has.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The fit function uses the helper function **_grow_tree(x, y)** to grow the tree
    piece by piece until one of the stopping criteria is reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before splitting the node, we check if one of the stopping criteria is met.
    In the simplified example, we only have two stopping criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '**(1) depth >= self.max_depth:** Is the maximum depth of the tree reached?'
  prefs: []
  type: TYPE_NORMAL
- en: '**(2) n_samples < self.min_samples_split:** Is the number of samples in the
    node greater than **min_samples_split**?'
  prefs: []
  type: TYPE_NORMAL
- en: If either condition is true, the node is a terminal node and the only thing
    we need to calculate is the mean (**np.mean(y)**).
  prefs: []
  type: TYPE_NORMAL
- en: If neither of the two conditions is true, we split the data set further. We
    first define which features we consider for the split. In our simplified case
    we do not limit the columns we use for the split. We use all features in X (**feat_idxs**).
  prefs: []
  type: TYPE_NORMAL
- en: For the actual split, we define another helper function **_best_split** which
    we pass the x and y values of the node we are looking at.
  prefs: []
  type: TYPE_NORMAL
- en: I'll go into more detail about what **_best_split** does in a moment, but as
    the name implies **_best_split** returns us the "best" split, in the form of the
    selected feature for the split (**best_features**) and the threshold at which
    we split the dataset (**best_threshold**).
  prefs: []
  type: TYPE_NORMAL
- en: We use this information to actually split the dataset and store it as a node
    of our tree.
  prefs: []
  type: TYPE_NORMAL
- en: Before we jump out of the function we call **_grow_tree** again for both halves
    of the branch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The only question that remains unanswered is how the algorithm figures out what
    the best split would be.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, we calculate a so-called information gain. In our case,
    we define the information gain as a reduction of the mean squared error. The errors
    or residuals for the node itself and the resulting child nodes is calculated as
    the difference between the average value of the target variable y in each node
    and the actual y values of the samples in the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The function goes through each feature one by one.
  prefs: []
  type: TYPE_NORMAL
- en: We compute a set of possible thresholds for each feature as a moving average
    of all observations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we iterate over each threshold in the list, split the dataset and calculate
    a weighted mean squared error of the child nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Afterwards, we check if the calculated MSE is the smallest MSE calculated so
    far, if yes, we save the feature_idx and threshold as optimal. (in **best_feature_idxs**
    and **best_thresholds**)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Two functions which we have already used several times in the above sections
    are **_split** and **_squared_error**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The only thing we still need is a **predict()** function. For this we use **_traverse_tree**.
  prefs: []
  type: TYPE_NORMAL
- en: Using a loop function we go through the just built tree one by one. If we reach
    a leaf node, **_traverse_tree** returns the stored node value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it, the complete *decision tree regressor* is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Hands-On Example — Implementation from scratch vs. Scikit-learn DecisionTree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Load the data set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the test, we use the dataset already used as an example earlier, the automobile
    dataset. First, we load the dataset from uci.edu. Then we select a few attributes
    for the first simple test. For the following example, I choose:'
  prefs: []
  type: TYPE_NORMAL
- en: Wheel Base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Width
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for the input vector X.
  prefs: []
  type: TYPE_NORMAL
- en: Since the attribute “make” contains strings, we transform it into numeric features
    using OneHot Encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Train the model**'
  prefs: []
  type: TYPE_NORMAL
- en: After the input and output variables are defined, we try to run a first test
    with our algorithm to see if we can actually use it for predictions.
  prefs: []
  type: TYPE_NORMAL
- en: First, we split the dataset into a train and a test data set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ea6d2000d02b7cf85522ed1b8befb7aa.png)'
  prefs: []
  type: TYPE_IMG
- en: MSE of our “decision tree from scratch”— Screenshot by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Compare it to the existing scikit-learn library**'
  prefs: []
  type: TYPE_NORMAL
- en: For comparison, we now try the same with the "DecisionTreeRegressor" library
    from scikit-learn. Our trained model performs exactly the same in this case. I
    won’t go into whether the result is good or bad here. If you want to know how
    to tune a regression model and find the model with the best performance, you can
    find a more detailed explanation of suitable methods in one of my previous articles
    ([here](https://medium.com/towards-data-science/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3)
    or [here](https://medium.com/towards-data-science/a-step-by-step-introduction-to-bayesian-hyperparameter-optimization-94a623062fc)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/27891be9517a5e05166cee93702c5619.png)'
  prefs: []
  type: TYPE_IMG
- en: MSE of scikit-learn decision tree — Screenshot by author
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Decision Tree is the basis for a number of outstanding algorithms such as
    Random Forest, XGBoost, LightGBM and CatBoost. The concepts behind them are very
    intuitive and generally easy to understand, at least as long as you try to understand
    the individual subconcepts piece by piece. With this article, you have taken a
    good first step by understanding the core of every tree ensemble algorithm, the
    decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: I plan to publish more articles about each concept that makes gradient-boosting
    frameworks so efficient.
  prefs: []
  type: TYPE_NORMAL
- en: '**Enjoyed the story?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*If you enjoyed reading and want to learn more about Machine Learning concepts,
    algorithms and applications, you can find a list with* [*all of my related articles.*](https://dmnkplzr.medium.com/list/e83997daeb7a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*If you don’t want to miss a new article, you can* [*subscribe for free*](https://dmnkplzr.medium.com/subscribe)
    *to get notified whenever I publish a new story.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Become a Medium member to read more stories from other writers and me. You
    can support me by using my* [*referral link*](https://medium.com/@dmnkplzr/membership)
    *when you sign up. I’ll receive a commission at no extra cost to you.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feel free to reach out to me on [LinkedIn](https://www.linkedin.com/in/polzerdo/)
    if you have any questions!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 6\. References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Carlens, H. (n.d.).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205052647&usg=AOvVaw2Pb5R1CPby6AraUz7keuav)
    [*The State of Competitive Machine Learning*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205053169&usg=AOvVaw02b_PEzqLVenZl73R1RWw-)[.
    ML Contests. Retrieved March 17, 2023, from https://mlcontests.com/state-of-competitive-machine-learning-2022/](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205053558&usg=AOvVaw070Krotnte0a5TdQv6FjZ8)[https://archive.ics.uci.edu/ml/datasets/automobile](https://www.google.com/url?q=https%3A%2F%2Farchive.ics.uci.edu%2Fml%2Fdatasets%2Fautomobile&sa=D&ust=1680021205053857&usg=AOvVaw2ehc0ELTUM0LQFHfPnPMNX)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chow, R. (2021, August 31).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205054283&usg=AOvVaw3iOkiDt58NURkVrRBdafx7)
    [*Decision Tree and Random Forest Algorithms: Decision Drivers*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205054645&usg=AOvVaw2CdIsxVMJJZXXrylsvekOy)[.
    History of Data Science. https://www.historyofdatascience.com/decision-tree-and-random-forest-algorithms-decision-drivers/](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205055068&usg=AOvVaw0I4ihT8MdGNEkPME96zZEH)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Géron, A. (2022).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205055543&usg=AOvVaw2qodvWBxf0OfnVTygMXkQQ)
    [*HANDS-ON MACHINE LEARNING WITH SCIKIT-LEARN, KERAS, AND TENSORFLOW concepts,
    tools, and techniques to build intelligent systems*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205055825&usg=AOvVaw0BB-492v7QeyrV6ajrJ35G)
    [(Third edition). O’Reilly Media, Inc.](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205056144&usg=AOvVaw21SJNw4GTfvgq7lPaxbny7)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Google Developers (Director). (2017, September 13).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205056524&usg=AOvVaw0sn3PWrGmEkNvyLwwGzsVF)
    [*Let’s Write a Decision Tree Classifier from Scratch—Machine Learning Recipes
    #8*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205056810&usg=AOvVaw1vfMxM9QW4YiXmrKThlU88)[.
    https://www.youtube.com/watch?v=LDRbO9a6XPU](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205057100&usg=AOvVaw3qIenpzMJ2DXQf4v8G4t5g)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Swalin, A. (2019, June 11).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205057661&usg=AOvVaw0Hx2RqIiROY9Gw9jSd-s6B)
    [*CatBoost vs. Light GBM vs. XGBoost*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205057965&usg=AOvVaw16zH0uhPbzR4Fn-98cq3jH)[.
    Medium. https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205058282&usg=AOvVaw3F9COP_67yKpUAwzl5-vV1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[*UCI Machine Learning Repository: Automobile Data Set*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205058732&usg=AOvVaw0lCQ-tx1LhGwFzHMkPH8K7)[.
    (n.d.). Retrieved February 24, 2023, from https://archive.ics.uci.edu/ml/datasets/automobile](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205059072&usg=AOvVaw2NJyPeMqBT1B0qE6gA2aGI)'
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression Tree from Scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
