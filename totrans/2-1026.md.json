["```py\nimport random\nfrom collections import deque, namedtuple\n\nimport gymnasium as gym\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm.auto import tqdm\n\nn_episodes = 1000 # play 1000 games\neps = 0.4 # exploration rate, probability of choosing random action\neps_decay = 0.95 # eps gets multiplied by this number each epoch...\nmin_eps = 0.1 # ...until this minimum eps is reached\ngamma = 0.95 # discount\nmax_memory_size = 10000 # size of the replay memory\nbatch_size = 16 # batch size of the neural network training\nmin_length = 160 # minimum length of the replay memory for training, before it reached this length, no gradient updates happen\nmemory_parts = [\"state\", \"action\", \"next_state\", \"reward\", \"done\"] # nice names for the part of replay memory, otherweise the names are 0-5\n```", "```py\nMemory = namedtuple(\"Memory\", memory_parts) # a single entry of the memory replay\n\nclass ReplayMemory:\n    def __init__(self, max_length=None):\n        self.max_length = max_length\n        self.memory = deque(maxlen=max_length)\n\n    def store(self, data):\n        self.memory.append(data)\n\n    def _sample(self, k):\n        return random.sample(self.memory, k)\n\n    def structured_sample(self, k):\n        batch = self._sample(k)\n        result = {}\n        for i, part in enumerate(memory_parts):\n            result[part] = np.array([row[i] for row in batch])\n\n        return result\n\n    def __len__(self):\n        return len(self.memory)\n```", "```py\nr = ReplayMemory(max_length=3)\n\nr.store((\"a\", \"b\", \"c\", \"e\", \"f\"))\nr.store((1, 2, 3, 4, 5))\nr.store((6, 7, 8, 9, 0))\n\nprint(r.structured_sample(2)) # get 2 random sampples from the replay memory\n\n# Output (for me):\n# {\n#   'state': array([1, 6]),\n#   'action': array([2, 7]),\n#   'next_state': array([3, 8]),\n#   'reward': array([4, 9]),\n#   'done': array([5, 0])\n# }\n```", "```py\nr.store((0, 0, 0, 0, 0))\n\nprint(r.memory)\n\n# Output:\n# deque([(1, 2, 3, 4, 5), (6, 7, 8, 9, 0), (0, 0, 0, 0, 0)], maxlen=3)\n# no more letters in here!\n```", "```py\nmodel = tf.keras.Sequential(\n    [\n        tf.keras.layers.Dense(16, input_shape=(4,), activation=\"relu\"), # state consists of 4 floats\n        tf.keras.layers.Dense(16, activation=\"relu\"),\n        tf.keras.layers.Dense(16, activation=\"relu\"),\n        tf.keras.layers.Dense(16, activation=\"relu\"),\n        tf.keras.layers.Dense(2, activation=\"linear\"), # 2 actions: go left or go right\n    ]\n)\nmodel.compile(\n    loss=tf.keras.losses.MeanSquaredError(),\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n)\n```", "```py\nenv = gym.make(\"CartPole-v1\")\nreplay_memory = ReplayMemory(max_length=max_memory_size)\n\nfor episode in tqdm(range(n_episodes)): # tqdm makes a nice proress bar\n    state, _ = env.reset()\n    done = False\n\n    while not done:\n        if random.random() < eps:\n            action = env.action_space.sample() # random action\n        else:\n            action = model.predict(state[np.newaxis, :], verbose=False).argmax() # best action according to the model\n\n        next_state, reward, done, _, _ = env.step(action)\n        memory = Memory(state, action, next_state, reward, done)\n        replay_memory.store(memory)\n\n        if len(replay_memory) >= min_length:\n            batch = replay_memory.structured_sample(batch_size) # get samples from the replay memory\n\n            target_batch = batch[\"reward\"] + gamma * model.predict(batch[\"next_state\"], verbose=False).max(axis=1) * (1 - batch[\"done\"]) # R(s, a) + γ·maxₐ N(s') if not a terminal state, otherwise R(s, a)\n            targets = model.predict(batch[\"state\"], verbose=False)\n            targets[range(batch_size), batch[\"action\"]] = target_batch # set the target for the action that was done and leave the outputs of other 3 actions as they are\n\n            model.fit(batch[\"state\"], targets, verbose=False, batch_size=batch_size) # train for one epoch\n\n        state = next_state\n\n    eps = max(min_eps, eps * eps_decay)\n```", "```py\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\nenv.reset()\ndone = False\n\nwhile not done:\n    env.render()\n    action = env.action_space.sample()\n    _, _, done, _, _ = env.step(action)\n\nenv.close()\n```", "```py\nmodel = tf.keras.models.load_model(\"PATH/TO/MODEL/230\") # the model is in my Github, https://github.com/Garve/towards_data_science/blob/main/A%20Tutorial%20on%20Deep%20Q-Learning/230.zip\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\nstate, _ = env.reset()\ndone = False\ntotal_reward = 0\n\nwhile not done and total_reward < 500: # force end the game after 500 time steps because the model is too good!\n    env.render()\n    action = model.predict(state[np.newaxis, :], verbose=False).argmax(axis=1)[0]\n    state, reward, done, _, _ = env.step(action)\n    total_reward += reward\n\nenv.close()\n```"]