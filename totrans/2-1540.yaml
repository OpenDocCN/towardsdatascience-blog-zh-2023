- en: Multi-Layer Neural Networks for Neurodegenerative Disorder Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multi-layer-neural-network-for-neurodegenerative-disorder-classification-27c0a5efc766](https://towardsdatascience.com/multi-layer-neural-network-for-neurodegenerative-disorder-classification-27c0a5efc766)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/b8e6cdffb84e992c90f9e59b694fb71b.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jon Tyson](https://unsplash.com/es/@jontyson?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step guide on developing a Multi-Layer Neural Network for classifying
    Magnetic Resonance images of Alzheimer's disease
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://luca-zammataro.medium.com/?source=post_page-----27c0a5efc766--------------------------------)[![Luca
    Zammataro](../Images/3ad63b538b138c13e915489f404c8be0.png)](https://luca-zammataro.medium.com/?source=post_page-----27c0a5efc766--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27c0a5efc766--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27c0a5efc766--------------------------------)
    [Luca Zammataro](https://luca-zammataro.medium.com/?source=post_page-----27c0a5efc766--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27c0a5efc766--------------------------------)
    ·23 min read·Mar 3, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In December 2019, with the support of *Medium* and *Toward Data Science*, I
    started writing a series of articles and Python tutorials explaining how Machine
    Learning (ML) works and how it should be applied to biomedical data. The idea
    of constructing ML algorithms from scratch, without the support of frameworks
    like *TensorFlow* or *Scikit-learn*, was intentional and animated by the intent
    of explaining the base of ML, with a particular benefit for the reader interested
    in a deep understanding of all the concepts. Moreover, the over-cited frameworks
    are potent tools that require special attention to parameter settings. For this
    reason, an in-depth study of their bases can explain to the reader how to make
    the most of their potential.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Over time, I realized that *diagnostic imaging* could represent the biomedical
    sector primarily benefiting from AI's help. If nothing else, it is a sector that
    offers many hints for optimizing ML algorithms. Indeed, diagnostic imaging is
    the medical field where variability and difficulty of definition collide with
    the need to abstract, broadly speaking, the nature of a pathological process.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, more efficient AI systems flank physicians and scientists in challenging
    tasks, like the early diagnosis of disease based on imaging. More often, AI''s
    contribution is not only limited to helping with the prompt diagnosis of an illness
    but can proficiently reveal what underlies a pathology process and when a tissue
    or an organ starts to degenerate. Consider the case of Alzheimer''s disease (AD),
    a chronic neuro disease causing neuron degeneration and brain tissue loss (McKhann
    et al., 1984): the cost of caring for AD patients tends to rise, and getting a
    prompt diagnosis is becoming necessary.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，更高效的人工智能系统辅助医生和科学家完成挑战性任务，例如基于影像的早期疾病诊断。人工智能的贡献不仅限于帮助快速诊断疾病，还可以熟练揭示病理过程的根本原因以及组织或器官何时开始退化。以阿尔茨海默病（AD）为例，这是一种导致神经退化和脑组织丧失的慢性神经疾病（McKhann等人，1984年）：照顾AD患者的成本趋于上升，因此及时诊断变得越来越必要。
- en: In this article, I wish to bring to your attention the construction of a Multi-Layer
    Perceptron (MLP Neural Network) that we will use as an image classification system
    for Alzheimer's Disease Magnetic Resonance Imaging data (ADMRI) from Kaggle. In
    my previous articles, I described other straightforward approaches for image detection
    and outcome prediction based on Logistic Regression (LR). In those articles, we
    have seen how successfully classify data from MNIST digits handwritten or Zalando,
    with an accuracy of 97%. Still, these approaches could not be so efficient when
    the structure of a training set reaches a certain complexity, as in the case of
    the ADMRI.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我希望引起您的注意，我们将构建一个多层感知器（MLP神经网络），作为阿尔茨海默病磁共振成像数据（ADMRI）图像分类系统，数据来自Kaggle。在我之前的文章中，我描述了基于逻辑回归（LR）的图像检测和结果预测的其他直接方法。在那些文章中，我们看到如何成功地对MNIST手写数字或Zalando数据进行分类，准确率达到97%。然而，当训练集的结构达到一定复杂性时，例如ADMRI，这些方法可能不会那么高效。
- en: For the code implemented in this article, we will use Python. I recommend using
    [*Jupyter notebook*](https://jupyter.org/install) and a Python version ≥ 3.8\.
    The code requires a minimum of optimized calculations, especially for the linear
    algebra involved. We will use packages like *Pandas*, *NumPy*, *matplotlib*, and
    *SciPy,* part of [SciPy.org](http://scipy.org/), a Python-based open-source software
    ecosystem for mathematics, science, and engineering. Also, we will import *matplotlib,*
    a Python data visualization library. Moreover, an object "*opt"* from the *scipy.optimize*
    will be created to make optimizations to the Gradient. The last library is "pickle,"
    fundamental for opening files in pickle format.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本文中实现的代码，我们将使用Python。我建议使用[*Jupyter notebook*](https://jupyter.org/install)和Python版本≥3.8。代码需要最低限度的优化计算，特别是涉及线性代数的部分。我们将使用如*Pandas*、*NumPy*、*matplotlib*和*SciPy*等包，这些包是[
    SciPy.org](http://scipy.org/)的一部分，是一个基于Python的数学、科学和工程的开源软件生态系统。此外，我们还将导入*matplotlib*，一个Python数据可视化库。此外，还将创建一个*opt*对象，来自*scipy.optimize*，用于对梯度进行优化。最后一个库是“pickle”，它对于打开pickle格式的文件至关重要。
- en: 'Suggestion:'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 建议：
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For a better understanding of the concepts, such as the sigmoidal function,
    Cost Function, and Gradient Descent in Linear and Logistic Regressions, I suggest
    the reader read my previous articles on these arguments first:'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了更好地理解概念，例如sigmoidal函数、成本函数和线性与逻辑回归中的梯度下降，我建议读者首先阅读我之前关于这些主题的文章：
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1\. Linear Regression with one or more variables](/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9);'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[1\. 一元或多元线性回归](/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9)；'
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[2\. Logistic Regression for malignancy prediction in cancer](https://medium.com/towards-data-science/logistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184);'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[2\. 用于癌症恶性预测的逻辑回归](https://medium.com/towards-data-science/logistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184)；'
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[3\. Detecting Melanoma with One-vs-All](/detecting-melanoma-with-one-vs-all-f84bc6dd0479);'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[3\. 使用One-vs-All检测黑色素瘤](/detecting-melanoma-with-one-vs-all-f84bc6dd0479)；'
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[4\. One-vs-All Logistic Regression for Image Recognition in Python](/one-vs-all-logistic-regression-for-image-recognition-in-python-5d290ce64ad7).'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[4\. Python中用于图像识别的One-vs-All逻辑回归](/one-vs-all-logistic-regression-for-image-recognition-in-python-5d290ce64ad7)。'
- en: ''
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These are fundamental preparatory concepts that will be taken for granted here.
    Moreover, many concepts here will be briefly mentioned, then wait to start reading
    this article before reading the others in the suggested numerical order.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些是这里将理所当然地使用的基本准备概念。此外，许多概念将在这里简要提及，因此在阅读这些建议的数字顺序的其他文章之前，请等待开始阅读本文。
- en: Datasets.
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集。
- en: 'The Alzheimer''s Disease Magnetic Resonance Imaging data (ADMRI) is downloadable
    from [Kaggle](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset).
    The data contains two folders: the first contains 33984 augmented MRI images,
    and the latter includes the originals. The data has four classes of images in
    training and testing sets: 1) *Non-Demented, 2) Moderate Demented, 3) Mild Demented,
    and 4) Very Mild Demented.* Each picture comprises ~ 200 x 200 pixels, corresponding
    to many features (40000).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 阿尔茨海默病磁共振成像数据（ADMRI）可以从[Kaggle](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset)下载。数据包含两个文件夹：第一个包含33984张增强的MRI图像，第二个包含原始图像。数据有四类图片在训练和测试集中：1)
    *非痴呆，2) 中度痴呆，3) 轻度痴呆，4) 极轻度痴呆*。每张图片包含约200 x 200像素，对应许多特征（40000）。
- en: 'For this exercise, I have provided a transformed version of the same dataset
    you can download from the following [link](https://www.kaggle.com/datasets/lucazammataro/augmented-alzheimer-mri-dataset-lowres-50x50).
    This "light" version contains the entire augmented dataset reduced to a 50 x 50
    resolution. The pickle file that you will get after unzipping is a *pandas dataframe*
    composed of two columns, one with the 2500 features characterizing each image
    and the other containing the corresponding labels, here defined:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我提供了相同数据集的转换版本，你可以从以下[链接](https://www.kaggle.com/datasets/lucazammataro/augmented-alzheimer-mri-dataset-lowres-50x50)下载。这“轻量”版本包含了缩减到50
    x 50分辨率的整个增强数据集。解压后得到的pickle文件是一个*pandas dataframe*，由两列组成，一列包含每张图片的2500个特征，另一列包含相应的标签，定义如下：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'First, we need to upload all the necessary libraries for opening the dataset
    and displaying images:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要上传所有必要的库来打开数据集和显示图像：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, open the pickle file, and discover the contained dataframe, typing the
    following code in a Jupyter notebook cell:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，打开pickle文件，发现包含的数据框，输入以下代码到Jupyter notebook单元格中：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The dataframe we call "ALZ" here has all the pictures and their relative labels.
    Typing "ALZ" in a new cell will show the data structure:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里称之为“ALZ”的数据框包含了所有图片及其相应标签。在新单元格中输入“ALZ”将显示数据结构：
- en: '![](../Images/b10247f4015da2557b34866f8a1cc8c5.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b10247f4015da2557b34866f8a1cc8c5.png)'
- en: Figure 1\. The training dataset structure (image by the author)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 训练数据集结构（图片由作者提供）
- en: 'Now, transform each data frame''s column into two distinct *numpy* objects,
    *X,* the training dataset, and *y,* the labels vector*. (*Transforming data into
    numpy objects is necessary for linear algebra calculations):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将每个数据框的列转换为两个不同的*numpy*对象，*X*为训练数据集，*y*为标签向量*。（*将数据转换为numpy对象是线性代数计算所必需的*）：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The *X* vector contains 33,984 items, each containing a vector of 2500 grayscale
    values. The numpy *shape* method shows the X structure:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*X*向量包含33,984个项目，每个项目包含一个2500灰度值的向量。numpy的*shape*方法显示X的结构：'
- en: '![](../Images/f0895939d76115c8ff8dc330726a5e90.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0895939d76115c8ff8dc330726a5e90.png)'
- en: 'Manipulating *X*, we can access the single items using their index; for example,
    to access the first 250 values of the first image (index = 0), type:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 操作*X*时，我们可以通过索引访问单个项目；例如，要访问第一张图片（索引=0）的前250个值，输入：
- en: '![](../Images/05bb898dce13dfbc463cc9fa2d42bc04.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05bb898dce13dfbc463cc9fa2d42bc04.png)'
- en: Figure 2\. Displaying the X[0] content (the first 250 values) (image by the
    author)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 显示X[0]内容（前250个值）（图片由作者提供）
- en: 'Each pixel corresponds to a specific gray value in a 0–255 range, where 0 is
    black, and 255 is white. Now that we can access the X vector, the *matplot* function
    *imshow()* will display a grayscale picture of 2500 pixels framed in a 2D representation
    of 50 x 50 pixels:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 每个像素对应0–255范围内的特定灰度值，其中0是黑色，255是白色。现在我们可以访问X向量，*matplot*函数*imshow()*将显示一个2500像素的灰度图像，框架为50
    x 50像素的2D表示：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Running the code, image number #12848 appears as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码时，图像编号#12848显示如下：
- en: '![](../Images/f15831bca5811592bb39dbbe7cf91ab9.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f15831bca5811592bb39dbbe7cf91ab9.png)'
- en: 'Figure 3\. A grayscale image from the Augmented Alzheimer MRI Dataset representing
    image #12848 corresponds to Label 2: Moderate Demented. (image elaborated from
    [https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset))'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 来自增强阿尔茨海默MRI数据集的灰度图像，表示图像#12848对应于标签2：中度痴呆。（图片来源于[https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset)）
- en: 'Instead of a one-by-one procedure, we prefer an easy function for randomly
    displaying fifty images picked from the dataset. Here is the code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更愿意使用一个简单的函数随机显示从数据集中挑选的五十张图像，而不是逐一操作。以下是代码：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Run the *plotSampleRandomly()* function passing as arguments X and y:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`plotSampleRandomly()`函数，传入参数X和y：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is in Figure 4:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输出见图4：
- en: '![](../Images/2a2fc41a94a83b38ebf5fe41611273d5.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a2fc41a94a83b38ebf5fe41611273d5.png)'
- en: 'Figure 4\. Visualizes randomly picked images from the Alzheimer''s Disease
    Magnetic Resonance Imaging dataset. Each tile represents a Magnetic Resonance
    Image accompanied by its identification number and label (f.i. #10902; y:2, which
    means that image number 10902 corresponds to a "Moderate Demented" diagnosis),
    images elaborated from [https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 可视化从阿尔茨海默病磁共振成像数据集中随机挑选的图像。每个瓦片代表一幅磁共振图像，附有其识别编号和标签（例如，#10902; y:2，表示图像编号10902对应于“中度痴呆”诊断），这些图像来自[https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset)
- en: Also, I have produced a light version of the original Alzheimer’s Disease Magnetic
    Resonance Imaging dataset, downloadable from this Kaggle [link](https://www.kaggle.com/datasets/lucazammataro/alzheimer-original-dataset-lowres-version).
    It is possible using this dataset for testing.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我还制作了原始阿尔茨海默病磁共振成像数据集的简化版本，可以从这个Kaggle [link](https://www.kaggle.com/datasets/lucazammataro/alzheimer-original-dataset-lowres-version)下载。可以使用此数据集进行测试。
- en: '**Importantly: the only aim of this exercise is to expose basic concepts on
    the application of MLP Neural Network to images of Alzheimer’s Disease Magnetic
    Resonance, and it is intended ONLY for experimental purposes, not for clinical
    use.**'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要的是：本练习的唯一目的是展示将MLP神经网络应用于阿尔茨海默病磁共振图像的基本概念，并且仅用于实验目的，不用于临床使用。**'
- en: The Logistic Unit model
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Logistic Unit模型
- en: 'Let us start with some historical definitions: with Neural Networks (NN), we
    define a class of AI algorithms that withdraw inspiration from biological neuronal
    cells'' networks. They were born with the specific aim of trying to mimic human
    brain functions. The assumption of their functioning has its foundations, precisely
    in studies conducted eighty years ago by eminent neuroscientists. In 1943, Warren
    McCulloch and Walter Pitts published a pivotal work entitled *"A Logical Calculus
    of Ideas Immanent in Nervous Activity,"* describing, for the first time, a computational
    model of how networks of biological neurons work together to accomplish complex
    tasks.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些历史定义开始：在神经网络（NN）中，我们定义了一类从生物神经细胞网络中汲取灵感的AI算法。它们诞生的具体目的是试图模拟人脑功能。其运作假设的基础，正是在八十年前由著名神经科学家进行的研究中建立的。1943年，沃伦·麦卡洛克和沃尔特·皮茨发表了一项具有重大意义的工作，题为*"神经活动中固有思想的逻辑演算"*，首次描述了生物神经网络如何协同工作以完成复杂任务的计算模型。
- en: Starting from that year, one behind the other, many NN models followed. For
    example, in 1957, Frank Rosenblatt defined the concept of *Perceptron,* a sort
    of artificial neuron able to classify the linear relationship between inputs and
    outputs. We can undoubtedly say that Artificial Intelligence was born at that
    moment. Still, despite the interest in all these models dramatically growing,
    huge hardware limitations of the time relegated Artificial Intelligence to the
    world of hopes and dreams rather than the real one. We had to wait until 1986
    when D. E. Rumelhart published a paper introducing the concept of *backpropagation
    training,* thus a new algorithm that could find the minimum of a Cost Function,
    automatizing the calculation of the Cost Function's derivative, in a few words,
    Rumelhart had invented the *Gradient Descent.*
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从那一年开始，许多NN模型接踵而至。例如，1957年，弗兰克·罗森布拉特定义了*感知器*的概念，这是一种能够对输入和输出之间的线性关系进行分类的人工神经元。我们可以毫无疑问地说，人工智能在那一刻诞生了。然而，尽管对这些模型的兴趣大幅增长，但当时的巨大硬件限制使得人工智能仍停留在希望和梦想的世界中，而非现实世界。我们不得不等到1986年，D.
    E. Rumelhart发表了一篇论文，引入了*反向传播训练*的概念，从而提出了一种能够找到成本函数最小值的新算法，自动化计算成本函数的导数，简而言之，Rumelhart发明了*梯度下降*。
- en: When we implement a Neural Network, in its modern meaning, we refer to a simplified
    model of what a group of biological neurons does. In this simplification, a neuron
    is just a **logistic unit**, connected with various dendrites to other similar
    input units, and produces an output value through its axon.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们实现神经网络时，以现代意义上的神经网络，我们指的是对一组生物神经元所做的简化模型。在这种简化中，神经元只是一个**逻辑单元**，通过各种树突连接到其他类似的输入单元，并通过其轴突产生输出值。
- en: '![](../Images/4f9c9a099f0265d51a2cd96b631e528f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f9c9a099f0265d51a2cd96b631e528f.png)'
- en: Figure 5\. The Logistic Unit model (image by the author)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 逻辑单元模型（作者提供的图片）
- en: 'The logistic unit model simplifies the biological neuron. The body (the blue
    circle in Figure 5) integrates values from the input neurons (the X vector), including
    the basis neuron, colored red, connected to the logistic unit through wires, and
    produces an output value through its axon. The output corresponds to the hypothesis
    model *hθ.* A way to represent hθ is the *g(z)* function, a sigmoid function (or
    logistic function), thus non-linear, whose formula can be written like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑单元模型简化了生物神经元。体部（图5中的蓝色圆圈）整合来自输入神经元（X向量）的值，包括基础神经元（红色），通过电缆连接到逻辑单元，并通过其轴突产生输出值。该输出对应于假设模型*hθ*。表示*hθ*的一种方式是*g(z)*函数，它是一个
    sigmoid 函数（或逻辑函数），因此是非线性的，其公式可以写成这样：
- en: '![](../Images/a76e172d7b37bcbb8c879c06f938a021.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a76e172d7b37bcbb8c879c06f938a021.png)'
- en: 'The g(z) function uses the product of the translated *θ* *vector* with the
    ***X*** *vector* (we will call this product ***z***) as an argument and can be
    defined as:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: g(z)函数使用翻译后的*θ* *向量*与***X*** *向量*的乘积（我们称这个乘积为***z***）作为参数，可以定义为：
- en: '![](../Images/6c437414bd8785b65f8a08a73f7fbd65.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c437414bd8785b65f8a08a73f7fbd65.png)'
- en: Suppose the X vectorassumes 0 or 1 values, g(z), when z = [θ₀X₀ + θ₁X₁ + θ₂X₂],calculates
    the probability that the output can be 0 or 1\. The sigmoid logistic function
    is also called the *activation function,* a definition deriving from the physiological
    function of each biological neuron, which is activated based on the type of received
    input. This input is then algebraically multiplied by the *θ vector,* the weight
    vector in the logistic unit. As for all the regression models (linear or logistic),
    the logistic unit has a *Cost function*, which can register how far we are from
    the minimum of the *hθ* and help us find the best θ.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 X 向量取值为 0 或 1，g(z) 当 z = [θ₀X₀ + θ₁X₁ + θ₂X₂] 时，计算输出可能为 0 或 1 的概率。Sigmoid
    逻辑函数也称为*激活函数*，这个定义源于每个生物神经元的生理功能，它根据接收到的输入类型被激活。然后，这个输入与*θ 向量*（逻辑单元中的权重向量）代数地相乘。对于所有回归模型（线性或逻辑），逻辑单元都有一个*成本函数*，它可以记录我们距离*hθ*的最小值有多远，并帮助我们找到最佳的
    θ。
- en: But before affording the Neural Network's Cost function, let us understand how
    a Neural Network works with some intuitions and two practical examples. This Neural
    Network we will describe is deliberately simple; it does not have hidden layers,
    composed only of a logistic unit and a couple of input neurons.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 但在理解神经网络的成本函数之前，让我们通过一些直觉和两个实际示例来了解神经网络是如何工作的。我们将描述的这个神经网络是故意简单的；它没有隐藏层，仅由一个逻辑单元和几个输入神经元组成。
- en: 'Simple Neural Network Intuition I: the AND gate'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单神经网络直觉 I：AND 门
- en: 'Imagine, for instance, that we want our Neural Network learns the logic gate
    AND:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们希望我们的神经网络学习逻辑门 AND：
- en: '![](../Images/e2ffddf3ab7b0910843ea1e67c33e762.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2ffddf3ab7b0910843ea1e67c33e762.png)'
- en: Figure 6a. Application of AND gate to a simple Neural Network (image by the
    author)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图6a. 将 AND 门应用于简单神经网络（作者提供的图片）
- en: 'Figure 6a represents an implementation of the AND gate into the logistic unit,
    which exploits three *θ* values that we have deliberately assigned: *θ* vector
    is, indeed, equal to the triad θ₀ *=* -3, θ₁ *=* +2, and θ₂*=*+2, (values in green).
    The table portion in red represents the AND truth table. The two AND operands
    are X1 and X2, and they can assume values equal to zero or one, while X0 is the
    input neuron representing the bias, and its value is equal to 1 as default. The
    output function Y corresponds to the hypothesis model hθ, implemented by the function
    g(z). Depending on which is *z*, the function *g(z)* returns a value approximated
    to 0 or 1, deriving from the *z* position mapped to the sigmoid.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图6a展示了将AND门实现到逻辑单元中的示例，它利用了我们故意指定的三个*θ*值：*θ*向量确实等于三元组θ₀*=*-3，θ₁*=*+2，θ₂*=*+2（绿色值）。红色的表格部分表示AND真值表。两个AND操作数是X1和X2，它们可以取0或1的值，而X0是表示偏置的输入神经元，默认值为1。输出函数Y对应于假设模型hθ，由函数g(z)实现。根据*z*的不同，函数*g(z)*返回接近0或1的值，这取决于*z*位置映射到的sigmoid。
- en: '![](../Images/f142e2e8c366459e1ce573696f1def16.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f142e2e8c366459e1ce573696f1def16.png)'
- en: 'Figure 6b: Magnification of the logistic function for the AND gate (image by
    the author)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图6b：AND门逻辑函数的放大图（作者提供的图像）
- en: 'For example, for X1 = 0 and X2 = 0, g(z) will equal 0.05 since the only value
    of X vector ≠ 0 is X0, thus the value stored in the bias neuron (-3). Indeed,
    the value returned by g(-3)=0.05 is approximatively close to zero when mapped
    into the sigmoid. Conversely, when X1 = 1 and X2 = 1 (the only condition where
    X1 AND X2 is true), g(z) is equal to g(-3+2+2) = g(1), that, mapped into the sigmoid,
    returns 0.73, a value abundantly beyond the 0.5 thresholds and that approximates
    1\. In all the other two remaining cases, that are X1=0, X2=1, and X1=1, X2=0,
    g(z) is equal to g(-1)=0.27, a value that is again near 0 (Figure 6b). The g(z)
    output in all four cases (yellow in Figure 6a) returns the expected outcome for
    the AND gate. The goodness of the sigmoid logistic function is that it can smoothly
    calculate the probability 0/1 of any combination of signals coming from the input
    neurons, activating itself physiologically as a biological neuron would do. Moreover,
    the sigmoid function will help guarantee that the *Gradient Descent* algorithm
    for seeking *θ* values will converge to the global minimum, avoiding problems
    with the non-convex *Cost function* with many local optima (see [Logistic Regression
    for malignancy prediction in cancer](https://medium.com/towards-data-science/logistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184)
    for a detailed discussion on issues related to non-convex Cost function). The
    python implementation for the sigmoid logistic function is the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于X1 = 0和X2 = 0，g(z)将等于0.05，因为X向量中唯一不等于0的值是X0，即存储在偏置神经元中的值（-3）。实际上，g(-3)=0.05映射到sigmoid时接近于零。相反，当X1
    = 1和X2 = 1（唯一使X1 AND X2为真的条件）时，g(z)等于g(-3+2+2) = g(1)，映射到sigmoid时返回0.73，这个值远远超过0.5的阈值，接近1。在其他两个剩余情况下，即X1=0，X2=1和X1=1，X2=0，g(z)等于g(-1)=0.27，这个值再次接近0（图6b）。所有四种情况下的g(z)输出（图6a中的黄色）返回了AND门的预期结果。sigmoid逻辑函数的优点是它可以平滑地计算输入神经元信号组合的0/1概率，像生物神经元一样生理地激活。此外，sigmoid函数将有助于保证*梯度下降*算法寻求*θ*值时收敛到全局最小值，避免处理具有多个局部最优的非凸*成本函数*的问题（详见[癌症恶性预测的逻辑回归](https://medium.com/towards-data-science/logistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184)以获取有关非凸成本函数问题的详细讨论）。sigmoid逻辑函数的Python实现如下：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Please copy and paste it into a new notebook cell. You can run the function,
    passing to it various *z* values as an argument, and see what output it produces,
    for instance:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请复制并粘贴到新的笔记本单元中。你可以运行该函数，将不同的*z*值作为参数传递给它，并查看它产生的输出，例如：
- en: '![](../Images/415bfd171f27ec75154f5eb97d05a00e.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/415bfd171f27ec75154f5eb97d05a00e.png)'
- en: 'Or is it possible to implement an excellent function for visualizing g(z) and
    its output like this:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 或者是否可以实现一个出色的函数来可视化g(z)及其输出，如下所示：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For example, running plotSigmoid, the output for z = 1, that is 0.73, will
    be displayed as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，运行plotSigmoid，对于z = 1，即0.73，将显示如下：
- en: '![](../Images/0bddf5a3371b6ff3aac56ff61d42de42.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0bddf5a3371b6ff3aac56ff61d42de42.png)'
- en: (image by the author)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: （作者提供的图像）
- en: 'Simple Neural Network Intuition II: the OR gate'
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单神经网络直观 II：OR门
- en: As for the AND gate, we can play with another example that involves the OR gate.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 关于AND门，我们可以用一个涉及OR门的其他示例来进行实验。
- en: '![](../Images/196c488d17377c02a3f87248db4bd0d2.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/196c488d17377c02a3f87248db4bd0d2.png)'
- en: Figure 7a. Application of OR gate to a Logistic Unit (image by the author)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7a：OR 门在逻辑单元中的应用（图片由作者提供）
- en: 'By switching the θ value for the bias neuron X0 from -3 to -1 and redoing all
    the calculations, the output now reflects the OR gate: we have three couples of
    X1 and X2 inputs that are mapped in the g(z) sigmoid function, producing values
    approximating > 0.5, close to 1.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将偏置神经元 X0 的θ值从 -3 更改为 -1 并重新进行所有计算，输出现在反映了 OR 门：我们有三个 X1 和 X2 输入对，它们被映射到 g(z)
    sigmoid 函数中，产生接近 > 0.5、接近 1 的值。
- en: '![](../Images/f269d3655e2c2215eb7ea1bdd8629c1f.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f269d3655e2c2215eb7ea1bdd8629c1f.png)'
- en: 'Figure 7b: Magnification of the logistic function for the OR gate (image by
    the author)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7b：OR 门的逻辑函数放大图（图片由作者提供）
- en: The Multi-Layer Perceptron (MLP) model
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多层感知机（MLP）模型
- en: A Multi-Layer Neural Network consists of one input layer, one or more hidden
    layers, and one output layer. All the layers, but not the output layer, embodies
    a bias neuron, and all the units are fully connected. The number of hidden layers
    differentiates between a classic MLP and a Deep Learning Neural Network. Thanks
    to the nature of their architecture, Neural Networks can help us to formulate
    complex non-linear hypotheses.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 多层神经网络由一个输入层、一个或多个隐藏层以及一个输出层组成。所有层（但输出层除外）都包含一个偏置神经元，所有单元完全连接。隐藏层的数量区别于经典的 MLP
    和深度学习神经网络。由于其架构的特性，神经网络可以帮助我们提出复杂的非线性假设。
- en: '![](../Images/f2a3a47656b42d422d653bd375e3f336.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2a3a47656b42d422d653bd375e3f336.png)'
- en: 'Figure 8: A Multi-Layer Neural Network (image by the author)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：多层神经网络（图片由作者提供）
- en: Observing the MLP's model representation, we notice that the calculations at
    the base of the MLP functioning are similar to those done for the Logistic Regression.
    This observation confirms that an MLP is nothing else than a "super" logistic
    regressor. The structure (Figure 8) shows three layers of neurons. The first layer
    is the input layer, composed of the *X vector*. The second layer consists of hidden
    nodes which activate based on the product of *X* times θ. The outcome of these
    activations becomes the input for the third layer, the output layer. The third
    layer comprises activation units, too. The only difference is that its outcome
    should correspond to the hypothesis model *hθ*. And here, we have to use the conditional
    verb because the heart of the matter in Neural Networks is precisely trying to
    minimize the error between the hypothesis and the predicted outcome deriving from
    the X times θ products.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 观察 MLP 的模型表示，我们发现 MLP 运作的基础计算与逻辑回归中的计算类似。这一观察确认了 MLP 实质上是一个“超级”逻辑回归器。结构（图 8）显示了三个神经层。第一层是输入层，由*X
    向量*组成。第二层由基于*X* 乘以 θ 的结果激活的隐藏节点组成。这些激活的结果成为第三层，即输出层的输入。第三层也包括激活单元。唯一的区别是，它的结果应对应于假设模型
    *hθ*。在这里，我们必须使用条件动词，因为神经网络的核心问题正是尝试最小化假设与来自 X 乘以 θ 产品的预测结果之间的误差。
- en: '![](../Images/80017a601e9f5611a3e59ea3a8889ecd.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80017a601e9f5611a3e59ea3a8889ecd.png)'
- en: 'Figure 9: Model representation of the Multi-Layer Neural Network. Calculations
    for each activation unit are color-coded based on the colored disks representing
    the hidden-level units in Figure 8\. (image by the author)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：多层神经网络的模型表示。每个激活单元的计算根据图 8 中表示隐藏层单元的彩色圆盘进行颜色编码。（图片由作者提供）
- en: 'The minimization process is embodied in the so-called training phase. It consists
    of finding those θ values (weights) for which *Xθ* will fit the *hθ* requirements
    through the *Gradient Descent*. We have to multiply a θ matrix with dimension
    *Sj* times *(n + 1)*, where *Sj* is the number of the activation nodes for *J*
    layers:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化过程体现在所谓的训练阶段。它包括找到那些θ值（权重），使得*Xθ*能够通过*梯度下降*符合*hθ*的要求。我们必须将一个维度为*Sj*乘以*(n
    + 1)*的θ矩阵相乘，其中*Sj*是*J*层的激活节点数量：
- en: '![](../Images/09e5baf77e7dc830098a42feac4fa6a3.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09e5baf77e7dc830098a42feac4fa6a3.png)'
- en: (image by the author)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由作者提供）
- en: and,
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 和，
- en: '![](../Images/9b0e48c8eb4a8ef5e78d28a2b525402d.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b0e48c8eb4a8ef5e78d28a2b525402d.png)'
- en: (image by the author)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由作者提供）
- en: Implementation of an MLP Neural Networks for augmented Alzheimer MRI classification
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 MLP 神经网络以增强阿尔茨海默病 MRI 分类
- en: 'We are going to create a one-layer MLP. The number of units for the hidden
    layer is calculated as 2/3 of the input units plus the number of the output units.
    This setting warrants the training success, despite other architectures using
    more o fewer activation units in the hidden layer. The number of input units depends
    on the number of pixels composing a single image in the dataset, and the number
    of output units corresponds to the category labels. The entire algorithm is structured
    into three main functions:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个单层 MLP。隐藏层的单元数计算为输入单元的 2/3 加上输出单元的数量。这个设置保证了训练成功，尽管其他架构在隐藏层使用了更多或更少的激活单元。输入单元的数量取决于数据集中单个图像的像素数量，而输出单元的数量对应于类别标签。整个算法分为三个主要函数：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: These three main functions embody other functions we will describe at the appropriate
    time.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个主要函数包含其他函数，我们将在适当的时候进行描述。
- en: The NN presented here is a *Forward Propagation MLP*; thus, the input data is
    fed through the network forward to generate the output. The hidden layer processes
    the data and moves to the successive layer. During forward propagation, the activations
    occur at each hidden and output layer node. The activation function is the calculation
    of the weighted sum. Based on the weighted sum, the activation function is applied
    to make the neural network flow non-linearly using bias.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里介绍的 NN 是一个 *前向传播 MLP*；因此，输入数据通过网络向前传播以生成输出。隐藏层处理数据并移动到下一个层。在前向传播过程中，激活发生在每个隐藏层和输出层节点上。激活函数是加权和的计算。基于加权和，激活函数被应用于使神经网络通过使用偏置非线性流动。
- en: Initialization
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化
- en: 'After uploading the dataset, define *X* and *y* vectors:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 上传数据集后，定义 *X* 和 *y* 向量：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The *X* vector will contains 33,984 items, each containing a vector of 2500
    grayscale values. The *init* function uses the method *X.shape* for initializing
    the NN architecture based on the X size:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*X* 向量将包含 33,984 个项目，每个项目包含一个 2500 灰度值的向量。*init* 函数使用方法 *X.shape* 根据 X 的大小初始化
    NN 架构：'
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The parameters *I, H1, and O* correspond to the size of the input, hidden, and
    output layers; *ini_nn_params* contains an "unrolled" version of the *θ* vectors
    (*Th1* for the input layer and *Th2* for the hidden layer). Using an unrolled
    flattened vector will reduce the complexity of handling too many vectors, making
    the code more readable. The *init* function accepts the vectors *X* and *y* as
    arguments but needs the sub-function *thetasRandomInit* nested inside itself,which
    randomizes *Th1* and *Th2*. (These must be randomly generated before the training
    phase).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 *I, H1, 和 O* 对应于输入层、隐藏层和输出层的大小；*ini_nn_params* 包含 *θ* 向量的“展开”版本（*Th1* 对于输入层，*Th2*
    对于隐藏层）。使用展开的扁平化向量将减少处理过多向量的复杂性，使代码更易读。*init* 函数接受向量 *X* 和 *y* 作为参数，但需要子函数 *thetasRandomInit*
    嵌套在内部，用于随机化 *Th1* 和 *Th2*。（这些必须在训练阶段之前随机生成）。
- en: 'The *thetasRandomInit* function can be implemented as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*thetasRandomInit* 函数可以如下实现：'
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The W matrix, which contains a random values vector, is implemented based on
    an *e_init* value, set to 0.12 as default. Please copy and paste the previous
    code into a new cell and run it. For calling *init* and defining the NN parameters
    *ini_nn_params, I, H1, O,* type as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: W 矩阵包含一个随机值向量，它是基于一个 *e_init* 值实现的，默认设置为 0.12。请将之前的代码复制粘贴到一个新单元格中并运行。调用 *init*
    函数并定义 NN 参数 *ini_nn_params, I, H1, O* 类型如下：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The init function will initialize the four variables and will produce the output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: init 函数将初始化这四个变量，并生成输出：
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Which describes the NN structure, comprising 2500 input units, 1670 hidden units,
    and four output units.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数描述了 NN 的结构，包括 2500 个输入单元，1670 个隐藏单元，以及四个输出单元。
- en: Training
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: 'For training the NN with the dataset of Alzheimer’s MRIs we uploaded (*X* and
    *y* vectors), we need to create a *training* function that is just a “wrapping”
    code passing the *nnCostFunction* to the *opt* object:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用我们上传的 Alzheimer’s MRIs 数据集（*X* 和 *y* 向量）训练 NN，我们需要创建一个 *training* 函数，这只是一个将
    *nnCostFunction* 传递给 *opt* 对象的“包装”代码：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The function *training* uses the *scipy* object *opt* to minimize the *Gradient
    Descent* of the Neural Network’s Cost Function (*nnCostFunction*). The function
    establishes how many iterations are necessary to accomplish the minimization goal
    through the “maxiter” parameter. As specified among the argument list of *opt.minimize*,
    the method used in this case is the TNC (Truncated Newton) which can minimize
    a scalar function, like *nnCostFunction*, of one or more variables. Finally, *training*
    will return the two *θ* vectors, *Th1* and *Th2,* trained and ready to be used
    in the testing phase.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 *training* 使用 *scipy* 对象 *opt* 来最小化神经网络成本函数 (*nnCostFunction*) 的 *梯度下降*。该函数通过
    “maxiter” 参数确定实现最小化目标所需的迭代次数。正如 *opt.minimize* 的参数列表中所指定的，这里使用的方法是 TNC（截断牛顿法），它可以最小化标量函数，如
    *nnCostFunction*，无论是单变量还是多变量。最后，*training* 将返回两个经过训练并准备好用于测试阶段的 *θ* 向量，*Th1* 和
    *Th2*。
- en: The *nnCostFunction* represents the most complex routine presented here, the
    heart of the whole algorithm. Now we are going to explain what it does in more
    detail. I suggest considering the schemas in Figure 8 and the calculations in
    Figure 9 while you’re studying the *nnCostFunction* code.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*nnCostFunction* 代表了这里展示的最复杂的例程，是整个算法的核心。现在我们将更详细地解释它的作用。我建议在学习 *nnCostFunction*
    代码时考虑图 8 中的模式和图 9 中的计算。'
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The function *nnCostFunction()* accepts as arguments *nn_params,* which isthe
    *ini_nn_params* we have created before and that contains an unrolled version of
    the *θ* vectors; *I, H1, O, (*respectively the size of the input, hidden and output
    layers*),* the two vectors *X, y,* and *lambd,* which corresponds to the regularization
    parameter *lambda.* The function return J and the *θ gradient vectors.*
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 *nnCostFunction()* 接受参数 *nn_params*，即我们之前创建的 *ini_nn_params*，它包含了 *θ* 向量的展开版本；*I,
    H1, O*（分别是输入层、隐藏层和输出层的大小）；两个向量 *X, y*；以及对应于正则化参数 *lambda* 的 *lambd*。该函数返回 J 和
    *θ 梯度向量*。
- en: First, the *nnCostFunction* reshapes *ini_nn_params* back into the parameters
    *Th1* and *Th2*, considering the original size of these two vectors.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，*nnCostFunction* 将 *ini_nn_params* 重新调整为参数 *Th1* 和 *Th2*，考虑到这两个向量的原始大小。
- en: 'The code needs to create a setup of the output layer, making a vector *Y* composed
    of *n-label-columns * n-X_samples* initially set to zero and then filled with
    1’s in a manner to codify the output in this way:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码需要创建输出层的设置，首先生成一个由 *n-label-columns * n-X_samples* 组成的向量 *Y*，初始值为零，然后用 1 填充，以这种方式对输出进行编码：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The new codification is then attributed to the *y* vector.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 新的编码随后被赋值给 *y* 向量。
- en: 3\. Initialize the *Cost J* and the *θ* vectors with zeros for the *Gradient
    Descent.*
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 用零初始化 *成本 J* 和 *θ* 向量，用于 *梯度下降*。
- en: '4\. Prepare all the vectors for the “Forward Propagation.” Six new vectors
    are generated here: *a1, z2, a2, z3, a3, and h*. The vector *a1* equals *X* (the
    input matrix), with a column of 1’s added (bias units) as the first column. The
    vector *z2* equals the product of *a1* and *θ1 (Th1).* The vector *a2* is created
    by adding a column of bias units after applying the sigmoid function to z2\. The
    vector *z3* equals the product of *a2* and *θ1 (Th2).* Finally, the vector z3
    is copied into the vector *h,* representing the hypothesis *hθ.* This passage,
    in appearance, looks not so important, but for better readability is essential
    to duplicate *z3* in *h*.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 为 “前向传播” 准备所有向量。这里生成了六个新向量：*a1, z2, a2, z3, a3* 和 *h*。向量 *a1* 等于 *X*（输入矩阵），在第一列添加了一列
    1（偏置单元）。向量 *z2* 等于 *a1* 和 *θ1 (Th1)* 的乘积。向量 *a2* 是在对 *z2* 应用 sigmoid 函数后添加了一列偏置单元创建的。向量
    *z3* 等于 *a2* 和 *θ1 (Th2)* 的乘积。最后，向量 *z3* 被复制到向量 *h* 中，表示假设 *hθ*。这个步骤表面上看似不重要，但为了更好的可读性，复制
    *z3* 到 *h* 是必要的。
- en: '5\. This part of the function implements the *Cost Function* that we want to
    be regularized. Recall the formula for the Logistic Regression Cost Function is:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 该部分函数实现了我们希望被正则化的 *成本函数*。请回顾逻辑回归成本函数的公式：
- en: '![](../Images/9551b290ed9653b47b9a8f82e0d82784.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9551b290ed9653b47b9a8f82e0d82784.png)'
- en: 'Figure 10: Logistic Regression Cost Function. (image by the author)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：逻辑回归成本函数。（作者提供的图片）
- en: 'And that the regularized version is:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化版本是：
- en: '![](../Images/d3a35c3698cd85275c4be73626ed0dea.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3a35c3698cd85275c4be73626ed0dea.png)'
- en: 'Figure 11: Regularized Logistic Regression Cost Function. (image by the author)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：正则化逻辑回归成本函数。（作者提供的图片）
- en: This implementation optimizes the regularization by multiplying *lambda* times
    the *probability p* (not present in the formula); *p* is calculated as the sum
    of the power of *θ1* and *θ2.*
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 该实现通过将 *lambda* 乘以 *probability p*（在公式中未出现）来优化正则化；*p* 计算为 *θ1* 和 *θ2* 的幂的总和。
- en: 6\. This part of the code will perform the Forward Propagation. In doing this
    task, the code must create a new vector, *d3,* for collecting the difference between
    *a3* and *y;* Here, *z2* is manipulated, adding a column of 1s, while a new vector,
    *d2,* will contain the product of *d3* and *Th2* times the output of the *Gradient
    Descent*, applied to *z2.*
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 这部分代码将执行前向传播。在执行此任务时，代码必须创建一个新的向量 *d3*，用于收集 *a3* 和 *y* 之间的差异；在这里，*z2* 被处理，添加了一列1，而新的向量
    *d2* 将包含 *d3* 和 *Th2* 的乘积，以及 *Gradient Descent* 的输出，应用于 *z2*。
- en: 'The *Gradient Descent* function is implemented as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*Gradient Descent* 函数的实现如下：'
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To run the *training* function, please invoke it, specifying as arguments the
    *ini_nn_params*, *I*, *H1*, *O*, *X,* and, *y*:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 *training* 函数，请调用它，并指定 *ini_nn_params*、*I*、*H1*、*O*、*X* 和 *y* 作为参数：
- en: '[PRE19]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Note:'
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：
- en: ''
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The training can take twelve hours of calculation. A couple of trained *θ*
    vectors are available as files at these two links: [Theta1](https://www.kaggle.com/datasets/lucazammataro/tds-mlp-nn?select=ALZ.50.df_theta_1.pickle),
    [Theta2](https://www.kaggle.com/datasets/lucazammataro/tds-mlp-nn?select=ALZ.50.df_theta_2.pickle)'
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 训练可能需要十二小时的计算。两个训练好的 *θ* 向量可以通过这两个链接获得：[Theta1](https://www.kaggle.com/datasets/lucazammataro/tds-mlp-nn?select=ALZ.50.df_theta_1.pickle)，[Theta2](https://www.kaggle.com/datasets/lucazammataro/tds-mlp-nn?select=ALZ.50.df_theta_2.pickle)
- en: 'After the download, you can upload the two files by typing:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 下载后，你可以通过输入以下命令上传这两个文件：
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Testing
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试
- en: 'The testing function serves to test the NN training:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 测试函数用于测试神经网络训练：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The function is straightforward and conceptually retraces the Forward Propagation
    path. After applying the 1s bias column to *X*, *a2* loads the sigmoid values
    of *X* times *θ1*, and only after adding the 1s bias column to *a2*, *a3* loads
    the sigmoid values of *X* times *θ2*. The parameter *pred* (prediction) equals
    the max value of *a3*. Finally, the function returns the percentage of successful
    outcome matches between the *pred* and *y* vectors.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数非常简单，并且在概念上重新追溯前向传播路径。在将1的偏置列应用于 *X* 后，*a2* 加载 *X* 乘以 *θ1* 的 sigmoid 值，并且只有在将1的偏置列添加到
    *a2* 后，*a3* 才加载 *X* 乘以 *θ2* 的 sigmoid 值。参数 *pred*（预测）等于 *a3* 的最大值。最后，函数返回 *pred*
    和 *y* 向量之间成功结果匹配的百分比。
- en: 'Before running the code, upload the testing version of the same dataset:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行代码之前，上传测试版本的相同数据集：
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To run the *testing*, please invoke it, specifying as arguments the two trained
    *θ* vectors*, X* and *Y*:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 *testing*，请调用它，并指定两个训练好的 *θ* 向量、*X* 和 *Y* 作为参数：
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The accuracy for the Alzheimer MRI dataset, with this NN architecture, is **95%**
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 该神经网络架构在阿尔茨海默 MRI 数据集上的准确率为 **95%**
- en: 'Seeing the NN in action would be great, showing some concrete results. The
    following function, which represents a modified version of *plotSampleRandomly,*
    the function we used to randomly display MRIs from the dataset, suits us. It takes
    vectors *X* and *y* as arguments plus the vector *pred* created during the testing
    phase:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 看到神经网络的实际运行效果将非常棒，展示一些具体结果。以下函数表示 *plotSampleRandomly* 的修改版，即我们用来随机显示数据集中 MRI
    图像的函数，适合我们。它接受向量 *X* 和 *y* 作为参数，还有在测试阶段创建的向量 *pred*：
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'And run the function as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 并按如下方式运行函数：
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'A possible output of *plotSamplesRandomlyWithPrediction* could be:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*plotSamplesRandomlyWithPrediction* 的一个可能输出如下：'
- en: '![](../Images/3945417eabebfcd3c4deee498f09ea34.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3945417eabebfcd3c4deee498f09ea34.png)'
- en: Figure 12\. Visualizes randomly picked predictions of the original Alzheimer
    dataset. Images elaborated from [https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图12\. 可视化原始阿尔茨海默数据集中随机挑选的预测结果。图像来源于 [https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset)
- en: Conclusions
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Neural Networks are an excellent solution for classifying complex images, especially
    when these come from neuroimaging. In other articles dedicated to a similar argument,
    we have seen how poorly contrasted images may present difficulties in classification.
    The Augmented Alzheimer MRI dataset provided by Kaggle shows some advantages since
    each image appears well contrasted. However, the complexity offered by the pattern
    diversities characterizing each pathological class is high and only sometimes
    identifiable at a glance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是分类复杂图像的绝佳解决方案，尤其是当这些图像来自神经成像时。在其他专门讨论类似问题的文章中，我们看到对比度较差的图像可能在分类中遇到困难。Kaggle
    提供的增强型阿尔茨海默病 MRI 数据集展示了一些优势，因为每张图像的对比度良好。然而，各病理类别特征的模式多样性所带来的复杂性很高，并且仅在偶尔的情况下才能一眼识别。
- en: This article aimed to identify a computational and relatively fast technology
    for classification and pattern prediction with a testing accuracy of ~ 95%. Tools
    like this offer new opportunities for quick diagnosis and a clever way to extract
    essential features in pathology.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章旨在确定一种计算快速且相对高效的分类和模式预测技术，测试准确率约为 ~ 95%。此类工具为快速诊断提供了新的机会，并且在病理学中提供了一种巧妙的方法来提取关键特征。
- en: Again the article was an occasion to explain in detail the mechanisms underlying
    the construction and the training process of Multi-Layer Neural Networks. Some
    intuitions have been unveiled regarding the Neural Network computational process
    in logic gates and the strong correlation with Logistic Regression, demonstrating
    how Neural Networks are, to all effects, a super Logistic Regression model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章再次提供了详细解释构建和训练多层神经网络机制的机会。揭示了一些关于神经网络在逻辑门中的计算过程及其与逻辑回归的强相关性的直觉，展示了神经网络实际上是一个超级逻辑回归模型。
- en: Extending the code described in the article may increase the number of hidden
    layers. For editorial reasons, I didn’t share these code extensions, and increasing
    the hidden layers in this specific application will not enrich the accuracy percentage;
    instead, it will make it worse. Anyway, code for two-hidden and three-hidden layers
    is available for those interested.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展文章中描述的代码可能会增加隐藏层的数量。由于编辑原因，我没有分享这些代码扩展，增加此特定应用中的隐藏层不会提高准确性百分比；相反，会使其变得更差。无论如何，有兴趣的人可以获得两层和三层隐藏层的代码。
- en: References
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: The Augmented Alzheimer MRI Dataset from [Kaggle](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset)
    is available under [GNU Lesser General Public License](https://www.gnu.org/licenses/lgpl-3.0.html).
    It represents a branch of the [Kaggle Alzheimer’s Dataset ( 4 class of Images
    )](https://www.kaggle.com/datasets/tourist55/alzheimers-dataset-4-class-of-images)
    available under [Open Data Commons Open Database License (ODbL) v1.0](https://opendatacommons.org/licenses/odbl/1-0/)
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 来自 [Kaggle](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset)
    的增强型阿尔茨海默病 MRI 数据集采用 [GNU 较小公共许可证](https://www.gnu.org/licenses/lgpl-3.0.html)。它代表了
    [Kaggle 阿尔茨海默病数据集（4 类图像）](https://www.kaggle.com/datasets/tourist55/alzheimers-dataset-4-class-of-images)
    的一个分支，采用 [开放数据公共开放数据库许可证（ODbL）v1.0](https://opendatacommons.org/licenses/odbl/1-0/)
- en: '[Luca Zammataro, **Linear Regression with one or more variables**, Towards
    Data Science, 2019](/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9)'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[卢卡·扎马塔罗，**一个或多个变量的线性回归**，《数据科学前沿》，2019](/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9)'
- en: '[Luca Zammataro, **Logistic Regression for malignancy prediction in cancer**,
    Towards Data Science, 2019](https://medium.com/towards-data-science/logistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184)'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[卢卡·扎马塔罗，**癌症恶性预测的逻辑回归**，《数据科学前沿》，2019](https://medium.com/towards-data-science/logistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184)'
- en: '[Luca Zammataro, **Detecting Melanoma with One-vs-All**, Towards Data Science,
    2020](/detecting-melanoma-with-one-vs-all-f84bc6dd0479)'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[卢卡·扎马塔罗，**通过一对多检测黑色素瘤**，《数据科学前沿》，2020](/detecting-melanoma-with-one-vs-all-f84bc6dd0479)'
- en: '[Luca Zammataro, **One-vs-All Logistic Regression for Image Recognition in
    Python**, Towards Data Science, 202](/one-vs-all-logistic-regression-for-image-recognition-in-python-5d290ce64ad7)2'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[卢卡·扎马塔罗，**用于图像识别的 Python 中的一对多逻辑回归**，《数据科学前沿》，202](/one-vs-all-logistic-regression-for-image-recognition-in-python-5d290ce64ad7)2'
- en: '[Andrew NG, **Machine Learning** | Coursera](https://www.coursera.org/learn/machine-learning)'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[安德鲁·恩吉，**机器学习** | Coursera](https://www.coursera.org/learn/machine-learning)'
- en: 'Chris Albon, **Machine Learning with Python Cookbook**, O''Reilly, ISBN-13:
    978–1491989388.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克里斯·阿尔邦，**Python 机器学习烹饪书**，O'Reilly，ISBN-13：978–1491989388。
- en: 'Aurélien Géron, **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow:
    Concepts, Tools, and Techniques to Build Intelligent Systems** 2nd Edition, O’Reilly,
    ISBN-10: 1492032646'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Aurélien Géron, **《动手学机器学习：使用 Scikit-Learn、Keras 和 TensorFlow 构建智能系统的概念、工具和技术》**
    第2版，O’Reilly，ISBN-10: 1492032646'
- en: '[Wen, J. et al.: Convolutional Neural Networks for Classification of Alzheimer’s
    Disease: Overview and Reproducible Evaluation](https://arxiv.org/pdf/1904.07773.pdf)'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Wen, J. 等人：用于阿尔茨海默病分类的卷积神经网络：概述与可重复评估](https://arxiv.org/pdf/1904.07773.pdf)'
