- en: Multi-Layer Neural Networks for Neurodegenerative Disorder Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multi-layer-neural-network-for-neurodegenerative-disorder-classification-27c0a5efc766](https://towardsdatascience.com/multi-layer-neural-network-for-neurodegenerative-disorder-classification-27c0a5efc766)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/b8e6cdffb84e992c90f9e59b694fb71b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jon Tyson](https://unsplash.com/es/@jontyson?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step guide on developing a Multi-Layer Neural Network for classifying
    Magnetic Resonance images of Alzheimer's disease
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://luca-zammataro.medium.com/?source=post_page-----27c0a5efc766--------------------------------)[![Luca
    Zammataro](../Images/3ad63b538b138c13e915489f404c8be0.png)](https://luca-zammataro.medium.com/?source=post_page-----27c0a5efc766--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27c0a5efc766--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27c0a5efc766--------------------------------)
    [Luca Zammataro](https://luca-zammataro.medium.com/?source=post_page-----27c0a5efc766--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27c0a5efc766--------------------------------)
    ·23 min read·Mar 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In December 2019, with the support of *Medium* and *Toward Data Science*, I
    started writing a series of articles and Python tutorials explaining how Machine
    Learning (ML) works and how it should be applied to biomedical data. The idea
    of constructing ML algorithms from scratch, without the support of frameworks
    like *TensorFlow* or *Scikit-learn*, was intentional and animated by the intent
    of explaining the base of ML, with a particular benefit for the reader interested
    in a deep understanding of all the concepts. Moreover, the over-cited frameworks
    are potent tools that require special attention to parameter settings. For this
    reason, an in-depth study of their bases can explain to the reader how to make
    the most of their potential.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, I realized that *diagnostic imaging* could represent the biomedical
    sector primarily benefiting from AI's help. If nothing else, it is a sector that
    offers many hints for optimizing ML algorithms. Indeed, diagnostic imaging is
    the medical field where variability and difficulty of definition collide with
    the need to abstract, broadly speaking, the nature of a pathological process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, more efficient AI systems flank physicians and scientists in challenging
    tasks, like the early diagnosis of disease based on imaging. More often, AI''s
    contribution is not only limited to helping with the prompt diagnosis of an illness
    but can proficiently reveal what underlies a pathology process and when a tissue
    or an organ starts to degenerate. Consider the case of Alzheimer''s disease (AD),
    a chronic neuro disease causing neuron degeneration and brain tissue loss (McKhann
    et al., 1984): the cost of caring for AD patients tends to rise, and getting a
    prompt diagnosis is becoming necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I wish to bring to your attention the construction of a Multi-Layer
    Perceptron (MLP Neural Network) that we will use as an image classification system
    for Alzheimer's Disease Magnetic Resonance Imaging data (ADMRI) from Kaggle. In
    my previous articles, I described other straightforward approaches for image detection
    and outcome prediction based on Logistic Regression (LR). In those articles, we
    have seen how successfully classify data from MNIST digits handwritten or Zalando,
    with an accuracy of 97%. Still, these approaches could not be so efficient when
    the structure of a training set reaches a certain complexity, as in the case of
    the ADMRI.
  prefs: []
  type: TYPE_NORMAL
- en: For the code implemented in this article, we will use Python. I recommend using
    [*Jupyter notebook*](https://jupyter.org/install) and a Python version ≥ 3.8\.
    The code requires a minimum of optimized calculations, especially for the linear
    algebra involved. We will use packages like *Pandas*, *NumPy*, *matplotlib*, and
    *SciPy,* part of [SciPy.org](http://scipy.org/), a Python-based open-source software
    ecosystem for mathematics, science, and engineering. Also, we will import *matplotlib,*
    a Python data visualization library. Moreover, an object "*opt"* from the *scipy.optimize*
    will be created to make optimizations to the Gradient. The last library is "pickle,"
    fundamental for opening files in pickle format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suggestion:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For a better understanding of the concepts, such as the sigmoidal function,
    Cost Function, and Gradient Descent in Linear and Logistic Regressions, I suggest
    the reader read my previous articles on these arguments first:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1\. Linear Regression with one or more variables](/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9);'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[2\. Logistic Regression for malignancy prediction in cancer](https://medium.com/towards-data-science/logistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184);'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[3\. Detecting Melanoma with One-vs-All](/detecting-melanoma-with-one-vs-all-f84bc6dd0479);'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[4\. One-vs-All Logistic Regression for Image Recognition in Python](/one-vs-all-logistic-regression-for-image-recognition-in-python-5d290ce64ad7).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These are fundamental preparatory concepts that will be taken for granted here.
    Moreover, many concepts here will be briefly mentioned, then wait to start reading
    this article before reading the others in the suggested numerical order.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Datasets.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Alzheimer''s Disease Magnetic Resonance Imaging data (ADMRI) is downloadable
    from [Kaggle](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset).
    The data contains two folders: the first contains 33984 augmented MRI images,
    and the latter includes the originals. The data has four classes of images in
    training and testing sets: 1) *Non-Demented, 2) Moderate Demented, 3) Mild Demented,
    and 4) Very Mild Demented.* Each picture comprises ~ 200 x 200 pixels, corresponding
    to many features (40000).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exercise, I have provided a transformed version of the same dataset
    you can download from the following [link](https://www.kaggle.com/datasets/lucazammataro/augmented-alzheimer-mri-dataset-lowres-50x50).
    This "light" version contains the entire augmented dataset reduced to a 50 x 50
    resolution. The pickle file that you will get after unzipping is a *pandas dataframe*
    composed of two columns, one with the 2500 features characterizing each image
    and the other containing the corresponding labels, here defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we need to upload all the necessary libraries for opening the dataset
    and displaying images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, open the pickle file, and discover the contained dataframe, typing the
    following code in a Jupyter notebook cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataframe we call "ALZ" here has all the pictures and their relative labels.
    Typing "ALZ" in a new cell will show the data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b10247f4015da2557b34866f8a1cc8c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. The training dataset structure (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, transform each data frame''s column into two distinct *numpy* objects,
    *X,* the training dataset, and *y,* the labels vector*. (*Transforming data into
    numpy objects is necessary for linear algebra calculations):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The *X* vector contains 33,984 items, each containing a vector of 2500 grayscale
    values. The numpy *shape* method shows the X structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0895939d76115c8ff8dc330726a5e90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Manipulating *X*, we can access the single items using their index; for example,
    to access the first 250 values of the first image (index = 0), type:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05bb898dce13dfbc463cc9fa2d42bc04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Displaying the X[0] content (the first 250 values) (image by the
    author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Each pixel corresponds to a specific gray value in a 0–255 range, where 0 is
    black, and 255 is white. Now that we can access the X vector, the *matplot* function
    *imshow()* will display a grayscale picture of 2500 pixels framed in a 2D representation
    of 50 x 50 pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the code, image number #12848 appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f15831bca5811592bb39dbbe7cf91ab9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3\. A grayscale image from the Augmented Alzheimer MRI Dataset representing
    image #12848 corresponds to Label 2: Moderate Demented. (image elaborated from
    [https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of a one-by-one procedure, we prefer an easy function for randomly
    displaying fifty images picked from the dataset. Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the *plotSampleRandomly()* function passing as arguments X and y:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is in Figure 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a2fc41a94a83b38ebf5fe41611273d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4\. Visualizes randomly picked images from the Alzheimer''s Disease
    Magnetic Resonance Imaging dataset. Each tile represents a Magnetic Resonance
    Image accompanied by its identification number and label (f.i. #10902; y:2, which
    means that image number 10902 corresponds to a "Moderate Demented" diagnosis),
    images elaborated from [https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset)'
  prefs: []
  type: TYPE_NORMAL
- en: Also, I have produced a light version of the original Alzheimer’s Disease Magnetic
    Resonance Imaging dataset, downloadable from this Kaggle [link](https://www.kaggle.com/datasets/lucazammataro/alzheimer-original-dataset-lowres-version).
    It is possible using this dataset for testing.
  prefs: []
  type: TYPE_NORMAL
- en: '**Importantly: the only aim of this exercise is to expose basic concepts on
    the application of MLP Neural Network to images of Alzheimer’s Disease Magnetic
    Resonance, and it is intended ONLY for experimental purposes, not for clinical
    use.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Logistic Unit model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us start with some historical definitions: with Neural Networks (NN), we
    define a class of AI algorithms that withdraw inspiration from biological neuronal
    cells'' networks. They were born with the specific aim of trying to mimic human
    brain functions. The assumption of their functioning has its foundations, precisely
    in studies conducted eighty years ago by eminent neuroscientists. In 1943, Warren
    McCulloch and Walter Pitts published a pivotal work entitled *"A Logical Calculus
    of Ideas Immanent in Nervous Activity,"* describing, for the first time, a computational
    model of how networks of biological neurons work together to accomplish complex
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Starting from that year, one behind the other, many NN models followed. For
    example, in 1957, Frank Rosenblatt defined the concept of *Perceptron,* a sort
    of artificial neuron able to classify the linear relationship between inputs and
    outputs. We can undoubtedly say that Artificial Intelligence was born at that
    moment. Still, despite the interest in all these models dramatically growing,
    huge hardware limitations of the time relegated Artificial Intelligence to the
    world of hopes and dreams rather than the real one. We had to wait until 1986
    when D. E. Rumelhart published a paper introducing the concept of *backpropagation
    training,* thus a new algorithm that could find the minimum of a Cost Function,
    automatizing the calculation of the Cost Function's derivative, in a few words,
    Rumelhart had invented the *Gradient Descent.*
  prefs: []
  type: TYPE_NORMAL
- en: When we implement a Neural Network, in its modern meaning, we refer to a simplified
    model of what a group of biological neurons does. In this simplification, a neuron
    is just a **logistic unit**, connected with various dendrites to other similar
    input units, and produces an output value through its axon.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f9c9a099f0265d51a2cd96b631e528f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. The Logistic Unit model (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The logistic unit model simplifies the biological neuron. The body (the blue
    circle in Figure 5) integrates values from the input neurons (the X vector), including
    the basis neuron, colored red, connected to the logistic unit through wires, and
    produces an output value through its axon. The output corresponds to the hypothesis
    model *hθ.* A way to represent hθ is the *g(z)* function, a sigmoid function (or
    logistic function), thus non-linear, whose formula can be written like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a76e172d7b37bcbb8c879c06f938a021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The g(z) function uses the product of the translated *θ* *vector* with the
    ***X*** *vector* (we will call this product ***z***) as an argument and can be
    defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c437414bd8785b65f8a08a73f7fbd65.png)'
  prefs: []
  type: TYPE_IMG
- en: Suppose the X vectorassumes 0 or 1 values, g(z), when z = [θ₀X₀ + θ₁X₁ + θ₂X₂],calculates
    the probability that the output can be 0 or 1\. The sigmoid logistic function
    is also called the *activation function,* a definition deriving from the physiological
    function of each biological neuron, which is activated based on the type of received
    input. This input is then algebraically multiplied by the *θ vector,* the weight
    vector in the logistic unit. As for all the regression models (linear or logistic),
    the logistic unit has a *Cost function*, which can register how far we are from
    the minimum of the *hθ* and help us find the best θ.
  prefs: []
  type: TYPE_NORMAL
- en: But before affording the Neural Network's Cost function, let us understand how
    a Neural Network works with some intuitions and two practical examples. This Neural
    Network we will describe is deliberately simple; it does not have hidden layers,
    composed only of a logistic unit and a couple of input neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simple Neural Network Intuition I: the AND gate'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine, for instance, that we want our Neural Network learns the logic gate
    AND:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2ffddf3ab7b0910843ea1e67c33e762.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6a. Application of AND gate to a simple Neural Network (image by the
    author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6a represents an implementation of the AND gate into the logistic unit,
    which exploits three *θ* values that we have deliberately assigned: *θ* vector
    is, indeed, equal to the triad θ₀ *=* -3, θ₁ *=* +2, and θ₂*=*+2, (values in green).
    The table portion in red represents the AND truth table. The two AND operands
    are X1 and X2, and they can assume values equal to zero or one, while X0 is the
    input neuron representing the bias, and its value is equal to 1 as default. The
    output function Y corresponds to the hypothesis model hθ, implemented by the function
    g(z). Depending on which is *z*, the function *g(z)* returns a value approximated
    to 0 or 1, deriving from the *z* position mapped to the sigmoid.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f142e2e8c366459e1ce573696f1def16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6b: Magnification of the logistic function for the AND gate (image by
    the author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, for X1 = 0 and X2 = 0, g(z) will equal 0.05 since the only value
    of X vector ≠ 0 is X0, thus the value stored in the bias neuron (-3). Indeed,
    the value returned by g(-3)=0.05 is approximatively close to zero when mapped
    into the sigmoid. Conversely, when X1 = 1 and X2 = 1 (the only condition where
    X1 AND X2 is true), g(z) is equal to g(-3+2+2) = g(1), that, mapped into the sigmoid,
    returns 0.73, a value abundantly beyond the 0.5 thresholds and that approximates
    1\. In all the other two remaining cases, that are X1=0, X2=1, and X1=1, X2=0,
    g(z) is equal to g(-1)=0.27, a value that is again near 0 (Figure 6b). The g(z)
    output in all four cases (yellow in Figure 6a) returns the expected outcome for
    the AND gate. The goodness of the sigmoid logistic function is that it can smoothly
    calculate the probability 0/1 of any combination of signals coming from the input
    neurons, activating itself physiologically as a biological neuron would do. Moreover,
    the sigmoid function will help guarantee that the *Gradient Descent* algorithm
    for seeking *θ* values will converge to the global minimum, avoiding problems
    with the non-convex *Cost function* with many local optima (see [Logistic Regression
    for malignancy prediction in cancer](https://medium.com/towards-data-science/logistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184)
    for a detailed discussion on issues related to non-convex Cost function). The
    python implementation for the sigmoid logistic function is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Please copy and paste it into a new notebook cell. You can run the function,
    passing to it various *z* values as an argument, and see what output it produces,
    for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/415bfd171f27ec75154f5eb97d05a00e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Or is it possible to implement an excellent function for visualizing g(z) and
    its output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, running plotSigmoid, the output for z = 1, that is 0.73, will
    be displayed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0bddf5a3371b6ff3aac56ff61d42de42.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Simple Neural Network Intuition II: the OR gate'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As for the AND gate, we can play with another example that involves the OR gate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/196c488d17377c02a3f87248db4bd0d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7a. Application of OR gate to a Logistic Unit (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'By switching the θ value for the bias neuron X0 from -3 to -1 and redoing all
    the calculations, the output now reflects the OR gate: we have three couples of
    X1 and X2 inputs that are mapped in the g(z) sigmoid function, producing values
    approximating > 0.5, close to 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f269d3655e2c2215eb7ea1bdd8629c1f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7b: Magnification of the logistic function for the OR gate (image by
    the author)'
  prefs: []
  type: TYPE_NORMAL
- en: The Multi-Layer Perceptron (MLP) model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Multi-Layer Neural Network consists of one input layer, one or more hidden
    layers, and one output layer. All the layers, but not the output layer, embodies
    a bias neuron, and all the units are fully connected. The number of hidden layers
    differentiates between a classic MLP and a Deep Learning Neural Network. Thanks
    to the nature of their architecture, Neural Networks can help us to formulate
    complex non-linear hypotheses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f2a3a47656b42d422d653bd375e3f336.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: A Multi-Layer Neural Network (image by the author)'
  prefs: []
  type: TYPE_NORMAL
- en: Observing the MLP's model representation, we notice that the calculations at
    the base of the MLP functioning are similar to those done for the Logistic Regression.
    This observation confirms that an MLP is nothing else than a "super" logistic
    regressor. The structure (Figure 8) shows three layers of neurons. The first layer
    is the input layer, composed of the *X vector*. The second layer consists of hidden
    nodes which activate based on the product of *X* times θ. The outcome of these
    activations becomes the input for the third layer, the output layer. The third
    layer comprises activation units, too. The only difference is that its outcome
    should correspond to the hypothesis model *hθ*. And here, we have to use the conditional
    verb because the heart of the matter in Neural Networks is precisely trying to
    minimize the error between the hypothesis and the predicted outcome deriving from
    the X times θ products.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80017a601e9f5611a3e59ea3a8889ecd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Model representation of the Multi-Layer Neural Network. Calculations
    for each activation unit are color-coded based on the colored disks representing
    the hidden-level units in Figure 8\. (image by the author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The minimization process is embodied in the so-called training phase. It consists
    of finding those θ values (weights) for which *Xθ* will fit the *hθ* requirements
    through the *Gradient Descent*. We have to multiply a θ matrix with dimension
    *Sj* times *(n + 1)*, where *Sj* is the number of the activation nodes for *J*
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09e5baf77e7dc830098a42feac4fa6a3.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: and,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b0e48c8eb4a8ef5e78d28a2b525402d.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of an MLP Neural Networks for augmented Alzheimer MRI classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to create a one-layer MLP. The number of units for the hidden
    layer is calculated as 2/3 of the input units plus the number of the output units.
    This setting warrants the training success, despite other architectures using
    more o fewer activation units in the hidden layer. The number of input units depends
    on the number of pixels composing a single image in the dataset, and the number
    of output units corresponds to the category labels. The entire algorithm is structured
    into three main functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: These three main functions embody other functions we will describe at the appropriate
    time.
  prefs: []
  type: TYPE_NORMAL
- en: The NN presented here is a *Forward Propagation MLP*; thus, the input data is
    fed through the network forward to generate the output. The hidden layer processes
    the data and moves to the successive layer. During forward propagation, the activations
    occur at each hidden and output layer node. The activation function is the calculation
    of the weighted sum. Based on the weighted sum, the activation function is applied
    to make the neural network flow non-linearly using bias.
  prefs: []
  type: TYPE_NORMAL
- en: Initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After uploading the dataset, define *X* and *y* vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The *X* vector will contains 33,984 items, each containing a vector of 2500
    grayscale values. The *init* function uses the method *X.shape* for initializing
    the NN architecture based on the X size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The parameters *I, H1, and O* correspond to the size of the input, hidden, and
    output layers; *ini_nn_params* contains an "unrolled" version of the *θ* vectors
    (*Th1* for the input layer and *Th2* for the hidden layer). Using an unrolled
    flattened vector will reduce the complexity of handling too many vectors, making
    the code more readable. The *init* function accepts the vectors *X* and *y* as
    arguments but needs the sub-function *thetasRandomInit* nested inside itself,which
    randomizes *Th1* and *Th2*. (These must be randomly generated before the training
    phase).
  prefs: []
  type: TYPE_NORMAL
- en: 'The *thetasRandomInit* function can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The W matrix, which contains a random values vector, is implemented based on
    an *e_init* value, set to 0.12 as default. Please copy and paste the previous
    code into a new cell and run it. For calling *init* and defining the NN parameters
    *ini_nn_params, I, H1, O,* type as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The init function will initialize the four variables and will produce the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Which describes the NN structure, comprising 2500 input units, 1670 hidden units,
    and four output units.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For training the NN with the dataset of Alzheimer’s MRIs we uploaded (*X* and
    *y* vectors), we need to create a *training* function that is just a “wrapping”
    code passing the *nnCostFunction* to the *opt* object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The function *training* uses the *scipy* object *opt* to minimize the *Gradient
    Descent* of the Neural Network’s Cost Function (*nnCostFunction*). The function
    establishes how many iterations are necessary to accomplish the minimization goal
    through the “maxiter” parameter. As specified among the argument list of *opt.minimize*,
    the method used in this case is the TNC (Truncated Newton) which can minimize
    a scalar function, like *nnCostFunction*, of one or more variables. Finally, *training*
    will return the two *θ* vectors, *Th1* and *Th2,* trained and ready to be used
    in the testing phase.
  prefs: []
  type: TYPE_NORMAL
- en: The *nnCostFunction* represents the most complex routine presented here, the
    heart of the whole algorithm. Now we are going to explain what it does in more
    detail. I suggest considering the schemas in Figure 8 and the calculations in
    Figure 9 while you’re studying the *nnCostFunction* code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The function *nnCostFunction()* accepts as arguments *nn_params,* which isthe
    *ini_nn_params* we have created before and that contains an unrolled version of
    the *θ* vectors; *I, H1, O, (*respectively the size of the input, hidden and output
    layers*),* the two vectors *X, y,* and *lambd,* which corresponds to the regularization
    parameter *lambda.* The function return J and the *θ gradient vectors.*
  prefs: []
  type: TYPE_NORMAL
- en: First, the *nnCostFunction* reshapes *ini_nn_params* back into the parameters
    *Th1* and *Th2*, considering the original size of these two vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code needs to create a setup of the output layer, making a vector *Y* composed
    of *n-label-columns * n-X_samples* initially set to zero and then filled with
    1’s in a manner to codify the output in this way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The new codification is then attributed to the *y* vector.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Initialize the *Cost J* and the *θ* vectors with zeros for the *Gradient
    Descent.*
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Prepare all the vectors for the “Forward Propagation.” Six new vectors
    are generated here: *a1, z2, a2, z3, a3, and h*. The vector *a1* equals *X* (the
    input matrix), with a column of 1’s added (bias units) as the first column. The
    vector *z2* equals the product of *a1* and *θ1 (Th1).* The vector *a2* is created
    by adding a column of bias units after applying the sigmoid function to z2\. The
    vector *z3* equals the product of *a2* and *θ1 (Th2).* Finally, the vector z3
    is copied into the vector *h,* representing the hypothesis *hθ.* This passage,
    in appearance, looks not so important, but for better readability is essential
    to duplicate *z3* in *h*.'
  prefs: []
  type: TYPE_NORMAL
- en: '5\. This part of the function implements the *Cost Function* that we want to
    be regularized. Recall the formula for the Logistic Regression Cost Function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9551b290ed9653b47b9a8f82e0d82784.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Logistic Regression Cost Function. (image by the author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'And that the regularized version is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3a35c3698cd85275c4be73626ed0dea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Regularized Logistic Regression Cost Function. (image by the author)'
  prefs: []
  type: TYPE_NORMAL
- en: This implementation optimizes the regularization by multiplying *lambda* times
    the *probability p* (not present in the formula); *p* is calculated as the sum
    of the power of *θ1* and *θ2.*
  prefs: []
  type: TYPE_NORMAL
- en: 6\. This part of the code will perform the Forward Propagation. In doing this
    task, the code must create a new vector, *d3,* for collecting the difference between
    *a3* and *y;* Here, *z2* is manipulated, adding a column of 1s, while a new vector,
    *d2,* will contain the product of *d3* and *Th2* times the output of the *Gradient
    Descent*, applied to *z2.*
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Gradient Descent* function is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the *training* function, please invoke it, specifying as arguments the
    *ini_nn_params*, *I*, *H1*, *O*, *X,* and, *y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Note:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The training can take twelve hours of calculation. A couple of trained *θ*
    vectors are available as files at these two links: [Theta1](https://www.kaggle.com/datasets/lucazammataro/tds-mlp-nn?select=ALZ.50.df_theta_1.pickle),
    [Theta2](https://www.kaggle.com/datasets/lucazammataro/tds-mlp-nn?select=ALZ.50.df_theta_2.pickle)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'After the download, you can upload the two files by typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The testing function serves to test the NN training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The function is straightforward and conceptually retraces the Forward Propagation
    path. After applying the 1s bias column to *X*, *a2* loads the sigmoid values
    of *X* times *θ1*, and only after adding the 1s bias column to *a2*, *a3* loads
    the sigmoid values of *X* times *θ2*. The parameter *pred* (prediction) equals
    the max value of *a3*. Finally, the function returns the percentage of successful
    outcome matches between the *pred* and *y* vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before running the code, upload the testing version of the same dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the *testing*, please invoke it, specifying as arguments the two trained
    *θ* vectors*, X* and *Y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy for the Alzheimer MRI dataset, with this NN architecture, is **95%**
  prefs: []
  type: TYPE_NORMAL
- en: 'Seeing the NN in action would be great, showing some concrete results. The
    following function, which represents a modified version of *plotSampleRandomly,*
    the function we used to randomly display MRIs from the dataset, suits us. It takes
    vectors *X* and *y* as arguments plus the vector *pred* created during the testing
    phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'And run the function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'A possible output of *plotSamplesRandomlyWithPrediction* could be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3945417eabebfcd3c4deee498f09ea34.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12\. Visualizes randomly picked predictions of the original Alzheimer
    dataset. Images elaborated from [https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural Networks are an excellent solution for classifying complex images, especially
    when these come from neuroimaging. In other articles dedicated to a similar argument,
    we have seen how poorly contrasted images may present difficulties in classification.
    The Augmented Alzheimer MRI dataset provided by Kaggle shows some advantages since
    each image appears well contrasted. However, the complexity offered by the pattern
    diversities characterizing each pathological class is high and only sometimes
    identifiable at a glance.
  prefs: []
  type: TYPE_NORMAL
- en: This article aimed to identify a computational and relatively fast technology
    for classification and pattern prediction with a testing accuracy of ~ 95%. Tools
    like this offer new opportunities for quick diagnosis and a clever way to extract
    essential features in pathology.
  prefs: []
  type: TYPE_NORMAL
- en: Again the article was an occasion to explain in detail the mechanisms underlying
    the construction and the training process of Multi-Layer Neural Networks. Some
    intuitions have been unveiled regarding the Neural Network computational process
    in logic gates and the strong correlation with Logistic Regression, demonstrating
    how Neural Networks are, to all effects, a super Logistic Regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Extending the code described in the article may increase the number of hidden
    layers. For editorial reasons, I didn’t share these code extensions, and increasing
    the hidden layers in this specific application will not enrich the accuracy percentage;
    instead, it will make it worse. Anyway, code for two-hidden and three-hidden layers
    is available for those interested.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Augmented Alzheimer MRI Dataset from [Kaggle](https://www.kaggle.com/datasets/uraninjo/augmented-alzheimer-mri-dataset)
    is available under [GNU Lesser General Public License](https://www.gnu.org/licenses/lgpl-3.0.html).
    It represents a branch of the [Kaggle Alzheimer’s Dataset ( 4 class of Images
    )](https://www.kaggle.com/datasets/tourist55/alzheimers-dataset-4-class-of-images)
    available under [Open Data Commons Open Database License (ODbL) v1.0](https://opendatacommons.org/licenses/odbl/1-0/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Luca Zammataro, **Linear Regression with one or more variables**, Towards
    Data Science, 2019](/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Luca Zammataro, **Logistic Regression for malignancy prediction in cancer**,
    Towards Data Science, 2019](https://medium.com/towards-data-science/logistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Luca Zammataro, **Detecting Melanoma with One-vs-All**, Towards Data Science,
    2020](/detecting-melanoma-with-one-vs-all-f84bc6dd0479)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Luca Zammataro, **One-vs-All Logistic Regression for Image Recognition in
    Python**, Towards Data Science, 202](/one-vs-all-logistic-regression-for-image-recognition-in-python-5d290ce64ad7)2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Andrew NG, **Machine Learning** | Coursera](https://www.coursera.org/learn/machine-learning)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chris Albon, **Machine Learning with Python Cookbook**, O''Reilly, ISBN-13:
    978–1491989388.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Aurélien Géron, **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow:
    Concepts, Tools, and Techniques to Build Intelligent Systems** 2nd Edition, O’Reilly,
    ISBN-10: 1492032646'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Wen, J. et al.: Convolutional Neural Networks for Classification of Alzheimer’s
    Disease: Overview and Reproducible Evaluation](https://arxiv.org/pdf/1904.07773.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
