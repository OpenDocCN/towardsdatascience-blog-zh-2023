# 从头开始的朴素贝叶斯分类器，使用Python

> 原文：[https://towardsdatascience.com/naive-bayes-classifier-from-scratch-with-python-942708211470](https://towardsdatascience.com/naive-bayes-classifier-from-scratch-with-python-942708211470)

## 从理论到实践，运用贝叶斯定理

[](https://piero-paialunga.medium.com/?source=post_page-----942708211470--------------------------------)[![Piero Paialunga](../Images/de2185596a49484698733e85114dd1ff.png)](https://piero-paialunga.medium.com/?source=post_page-----942708211470--------------------------------)[](https://towardsdatascience.com/?source=post_page-----942708211470--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----942708211470--------------------------------) [Piero Paialunga](https://piero-paialunga.medium.com/?source=post_page-----942708211470--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----942708211470--------------------------------) ·阅读时间10分钟·2023年1月4日

--

![](../Images/0861ef7d4e87fc5bedc5552305cb25bc.png)

由[Joel Abraham](https://unsplash.com/@joe_27?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)拍摄，图片来源于[Unsplash](https://unsplash.com/photos/8RRYJg26Wr4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

数学和物理充满了定理、方程、原理、公理和推论。当我开始学习物理时，我记得我达到了所有课程都具有相同结构的阶段：

A. 定义**基本假设**

B. 使用数学构建下一个“**砖块**”

C. **一块块地叠加**，直到所有部分汇聚成一个优雅、美丽的世界模型

让我们从我学习过的第一门物理课程开始：**微积分**。

1\. 你从**集合**和**数字**的基本假设开始。你开始定义自然数、整数、实数和复数。

2\. 从这里开始，你定义的**函数**不过是从空间A（假设是N维实数空间）到空间B（假设是1维实数空间）的**映射**。

3\. 然后你开始**研究**函数。于是你开始分析它们的最小值、最大值和鞍点。你偶然（*哦！*）了解了**“导数”**的概念。

4\. 然后你看看如何**积分**一个函数，那是**导数**的反过程。

5\. 然后你将这些与**微分方程**结合起来。

不要误解我：这个过程是**惊人的**。我喜欢看到人类逻辑能带你多远。我喜欢看到非常复杂的自然事件可以通过从非常简单的概念开始逐步推导出更深的含义。

另一个宝贵的科学课程是，最初应用于**“A”**的定理，也可以应用于“B”、“C”、“D”和“E”。

令人着迷的方面在于，领域“A”和其他领域（B、C、D和E）不必相关。

可能**最好的例子**就是**贝叶斯定理**。

贝叶斯定理在某种程度上是一个基础且显而易见的概念。令人难以置信的酷事是，我们可以通过强调贝叶斯定理背后的理念并将其应用于机器学习来构建一些非常有趣的算法。这个工具被称为**朴素贝叶斯分类器**。

在这篇文章中：

1.  我们将简要介绍**贝叶斯定理**。我们将解释它是什么，为什么重要，以及它如何应用于**机器学习**。

1.  我们将看到贝叶斯定理在一个虚构的**分类任务**中的应用。

1.  我们将看到一个升级版的贝叶斯定理，使用所谓的**高斯朴素贝叶斯分类器**。

我非常兴奋。让我们开始吧！

# 1\. 关于贝叶斯定理

有一个关于贝叶斯定理的定义一直深深地印在我的脑海里：

> “贝叶斯定理是这样的定理，它证明了仅仅因为你的车是蓝色的，并不意味着全世界的车都是蓝色的**但**如果全世界的车都是蓝色的**那么**你的车必须是蓝色的。”

如果你阅读了这篇[文章](https://medium.com/towards-data-science/from-theory-to-practice-with-bayesian-neural-network-using-python-9262b611b825)关于贝叶斯神经网络，你可能会认识到这与之前的定义相同，但我发誓我没有因为**懒惰**而重复使用同一定义！我的意思是……我**确实很懒**，但这不是原因。

我使用那个定义是因为它帮助我们理解**事件A在事件B的条件下**的发生概率与**事件B在事件A的条件下**的发生概率是不一样的。

让我做一个新的例子来克服我的懒惰。

假设我们有两个碗

![](../Images/2bf3a6d955208812a1c85bdc730b0ca1.png)

现在，假设这两个碗里满是**篮球**和**足球**（称为**足球**，不是*足球*）**球**。

![](../Images/afcf95d180c1f284ac39be7064deb4aa.png)

作者图片

现在，假设我问你这个问题：

> “**知道**我从**蓝色**碗里挑了一个球，挑到一个**足球**球的概率是多少。”

好吧，答案很简单。概率是**1**，因为在**蓝色碗**里，我只有足球球。现在假设我随机挑选一个碗（挑选白色碗和挑选蓝色碗的概率都是0.5）。让我问你这个问题：

> “知道我挑了一个足球，选择这个球来自蓝色碗的概率是多少？”

如你所见，问题有点类似，但事件A**（我挑了一个足球）**和事件B**（我挑了蓝色碗）**的顺序正好相反。

现在，我想你可能已经猜到答案并不相同，因为我可能从白色碗中而不是蓝色碗中提取了足球。我也认为你可能猜到这个概率仍然很高，因为我在白色碗中只有一个足球。

让我们在**Python**中进行实验。

我可以逐行详细解释，但我觉得那样很无聊。我所做的仅仅是设置碗和球的情况，就像之前一样，然后运行概率实验**N**次。在这N次迭代结束时，我们将得到N个结果。然后我们需要将概率计算为**频率主义**概率。在这种情况下，我们将计算：

![](../Images/f459332f8822862dff2f032a5591fc02.png)

图片由作者提供

现在我们知道，如果我们在相同的实验中运行N = 无限次，我们将收敛到分析结果，因此我们将迭代地增加N，看看它是否收敛到任何结果。

如你所见，我们进行了20,50,70,…, 1M次迭代。

这个结果是什么？

![](../Images/4acb4532e62a463ddd147c98b424ee0f.png)

图片由作者提供

嗯，看来确实存在一个分析结果，对吗？让我们来看看。

所以，贝叶斯定理告诉我们：

![](../Images/fa169e5da4f72824d0578b42efb59a8c.png)

图片由作者提供

因此，提取到的足球**来自** **蓝色碗**的概率等于提取一个足球**从蓝色碗（***注意区别！这是在你选择了蓝色碗的前提下提取一个足球的概率***)**的概率乘以从**蓝色碗**提取的概率，除以提取**足球**的概率。

我们知道：

![](../Images/892214e9bd65def6210abb0c518d9a89.png)

图片由作者提供

因为蓝色碗中只有足球。我们还假设我们以相等的概率选择两个碗中的一个。

所以我们有：

![](../Images/b85721a4bdb0281539ddc20e47419030.png)

图片由作者提供

现在，提取足球的概率是从白色碗中提取足球的概率和从蓝色碗中提取足球的概率之和。例如：

![](../Images/275360dfbea41ddf2cf3724a1f3ba07b.png)

图片由作者提供

所以我们有：

![](../Images/5a1be32fe9e16aca21b91b0fb4c868e5.png)

图片由作者提供

因为：

![](../Images/9bb8fb6dd963118c53306a973f8fdbdc.png)

图片由作者提供

所以我们有：

![](../Images/d766e8f77c3353591b0f6a136adec5f0.png)

图片由作者提供

如果我们在之前绘制的图上绘制5/6，我们会得到：

![](../Images/e0e6ea52c1030a0858d9948170b1778f.png)

图片由作者提供

我们的数学计算是正确的 😄

好吧，这时我相信你们都在想：

> “这与机器学习有什么关系？”

让我们来看看 😏

# 2\. 朴素贝叶斯分类器

朴素贝叶斯分类器是**朴素**地将**贝叶斯**定理应用于机器学习**分类器**的过程：就是这么简单。

假设我们有一个特定的二分类问题（类别1和类别2）。你有N个实例，每个实例都有其标签Y。

所谓的**先验概率**定义如下：

![](../Images/460ebdeba045701844437988f87531ad.png)![](../Images/b18459c6bed99eb5547dc9465b76c857.png)

图片由作者提供

现在我们真正想知道的是：给定一个特定的实例，**那个**实例属于类别1的概率是多少？属于类别2的概率是多少？

所以我们感兴趣的是，给定一个实例x：

![](../Images/4b2b4c09fb6a56faf101fa56aaa206b1.png)

图片由作者提供

在二元数据集中，我们知道这两个数量的总和是1，因此实际上我们只需要其中一个。现在这个数量可能看起来有些神秘，但另一个：

![](../Images/1bdd6071c51410a6d69c978afbdc80cb.png)

图片由作者提供

可以非常容易地计算（记住碗/球的例子！）。

计算P(x)的概率也非常简单，就像我们计算P(class 1)和P(class 2)一样。

所以我们实际上可以计算：

![](../Images/b2b0804ee2a11f067702b9768a38167c.png)

图片由作者提供

这一切看起来都很好。现在我想你可能已经理解了我们为什么称之为**朴素**。它之所以朴素，是因为它不过是计数出现次数并使用贝叶斯逻辑来推断预测。

让我们将其应用到一个玩具数据集上。我们有这两个类别和两个特征，是使用[sklearn.datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)库生成的。

太棒了。

所以：

![](../Images/9797c30b7c7147eed0a7fd9ebc110595.png)

图片由作者提供

因为我们有两个维度。

**给定类别0**，我们可以轻松计算给定区域（2D点周围的小区域）的概率。

所以：

1.  我们知道如何计算**后验**概率，即P(Y|X)

1.  我们有我们的Y和X集合

1.  我们可以将规则应用于**训练集**，并且可以**预测**测试集的结果。

让我们开始吧！

1.  **训练-测试分割**：

2\. **导入**和**拟合朴素贝叶斯分类器**：

请注意，我们对数据（x）添加了一个偏置。这是因为我们使用的贝叶斯分类器将x_1特征视为**分类**特征，而我们将这些特征视为数值特征。一种更严格的方法是使用*LabelEncoder()*特征，但在这个特定情况下，它们是完全等价的。

3\. **测试性能**：

在这个非常简单的玩具示例中，性能显然是（几乎）完美的。

# 3\. 高斯朴素贝叶斯分类器

你可能已经听够了我讲的内容，我对此感到抱歉。

我只是想通过谈论高斯朴素贝叶斯来结束这篇文章。

在前面的例子中，我们将特征视为**分类**的。但如果它们不是呢？

好吧，我们必须假设一个特定的可能性分布：**P(Y|X)**。

![](../Images/8522e72ad44166c55c56dd2c7b450e92.png)

图片由作者提供

所以它叫做**高斯**朴素贝叶斯，因为如上所见，似然被认为是高斯的。均值（mu_y）和方差（sigma_y平方）是通过计算类别的均值和方差得到的。

请注意，现在我们不再需要在数据中添加偏差。

这些是结果

# 4. 总结

在这篇文章中，我们：

1.  **承认贝叶斯定理的存在**。我们对它进行了直观的介绍，并进行了一个非常简单的受控案例来查看它是如何工作的。

1.  **了解贝叶斯定理如何应用于机器学习**。什么是Y，什么是X，我们如何将它们放入贝叶斯公式中以在**分类任务**中进行一些预测。

1.  我们自己创建了一个数据集，并使用**Categorial Naive Bayes**类（来自**sklearn**）进行了一项玩具分类任务。它几乎完美地工作，但有一个问题，就是只能处理分类特征。

1.  我们了解了如何将分类朴素贝叶斯扩展为**Gaussian Naive Bayes**。正如我们所见，Gaussian Naive Bayes使用高斯似然来工作。这种高斯似然也适用于非分类特征。

贝叶斯定理在机器学习中以多种方式应用，如贝叶斯神经网络或贝叶斯岭回归。我觉得这是一个很酷的入门示例，展示了贝叶斯定理在分类问题中的应用。我写这篇文章非常开心，希望你们喜欢它！🥰

# 5. 结论

如果你喜欢这篇文章，想了解更多关于机器学习的内容，或者只是想问我一些问题，你可以：

A. 在 [**Linkedin**](https://www.linkedin.com/in/pieropaialunga/) 上关注我，我会发布我的所有故事。

B. 订阅我的 [**新闻通讯**](https://piero-paialunga.medium.com/subscribe)。它将让你了解新故事，并给你机会通过短信与我联系，获取你可能有的所有更正或疑问。

C. 成为 [**会员**](https://piero-paialunga.medium.com/membership)，这样你就没有“每月最大故事数”的限制，可以阅读我（和其他数千名机器学习和数据科学顶级作者）关于最新技术的所有文章。
