- en: Theoretical Deep Dive Into Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/theoretical-deep-dive-into-linear-regression-e53c579aef5b](https://towardsdatascience.com/theoretical-deep-dive-into-linear-regression-e53c579aef5b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[EXPLAINABLE AI](https://medium.com/tag/explainable-ai)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learn about why linear regression is how it is, and how to naturally extend
    it in various ways
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)[![Dr.
    Robert K√ºbler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)
    [Dr. Robert K√ºbler](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)
    ¬∑10 min read¬∑Jun 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b8ab36b31e37bf56824bc5f7b55d548.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Erik van Dijk](https://unsplash.com/@erikvandijk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Most aspiring data science bloggers do it: write an introductory article about
    linear regression ‚Äî and it is a natural choice since this is one of the first
    models we learn when entering the field. While these articles are great for beginners,
    most do not go deep enough to satisfy senior data scientists.'
  prefs: []
  type: TYPE_NORMAL
- en: So, let me guide you through some unsung, yet refreshing details about linear
    regression that will make you a better data scientist (and give you bonus points
    during interviews).
  prefs: []
  type: TYPE_NORMAL
- en: '*This article is quite math-heavy, so in order to follow, it is beneficial
    to have some solid foundation with probabilities and calculus.*'
  prefs: []
  type: TYPE_NORMAL
- en: The Data Generation Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I‚Äôm a big fan of thinking about the data generation process when modeling.
    People who dealt with Bayesian modeling know what I mean, but for the others:
    Imagine you have a dataset (*X*, *y*) consisting of samples (*x*, *y*). Given
    *x*, how to get to a target *y*?'
  prefs: []
  type: TYPE_NORMAL
- en: '*Let us assume that we have* n *data points and that each* x *has* k *components/features.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For a linear model with the parameters ***w*‚ÇÅ, ‚Ä¶, *w‚Çñ* (coefficients)*, b*
    (intercept)*, œÉ* (noise)**, the assumption is that the data generation process
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute *¬µ* = *w*‚ÇÅ*x*‚ÇÅ + *w*‚ÇÇ*x*‚ÇÇ + ‚Ä¶ + *w‚Çñx‚Çñ* + *b.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Roll a random *y* ~ *N*(*¬µ, œÉ¬≤*). This is independent of other randomly generated
    numbers. *Alternatively:* Roll *Œµ* ~ *N*(0*, œÉ¬≤*) and output *y* = *¬µ* + *Œµ***.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That‚Äôs it already. These simple two lines are equivalent to the most important
    linear regression assumptions that people like to explain at great length, namely
    **linearity, homoscedasticity, and independence of errors.**
  prefs: []
  type: TYPE_NORMAL
- en: From step 1\. of the process, you can also see that we model the expectation
    *¬µ* with the typical linear equation *w*‚ÇÅ*x*‚ÇÅ + *w*‚ÇÇ*x*‚ÇÇ + ‚Ä¶ + *w‚Çñx‚Çñ* + *b* rather
    than the actual target. We know that we will not hit the target anyway, so we
    settle for the mean of the distribution that generates *y* instead.
  prefs: []
  type: TYPE_NORMAL
- en: Extensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Generalized Linear Models.** We are not forced to use a normal distribution
    for the generation process. If we deal with a dataset that **only contains positive
    targets**, it might be beneficial to assume that a **Poisson** **distribution**
    Poi(*¬µ*) is used instead of a normal distribution. This gives you **Poisson regression**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If our dataset only has the targets 0 and 1, use a **Bernoulli distribution**
    Ber(*p*), where *p* = sigmoid(*¬µ*), et voil√†: you got **logistic regression**.'
  prefs: []
  type: TYPE_NORMAL
- en: Only numbers between 0, 1, ‚Ä¶, *n*? Use a **binomial distribution** to get [**binomial
    regression**](https://en.wikipedia.org/wiki/Binomial_regression).
  prefs: []
  type: TYPE_NORMAL
- en: 'The list goes on and on. Long story short:'
  prefs: []
  type: TYPE_NORMAL
- en: Think about which distribution could have generated the labelsyou observe in
    the data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What Are We Actually Minimizing?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ok, so we decided on a model now. How do we train it now? How do we learn the
    parameters? Of course, you know: we minimized the (mean) squared error. But why?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The secret is that you just do a plan **maximum likelihood estimate** using
    the generation process we described before. The labels that we observe are *y*‚ÇÅ,
    *y*‚ÇÇ, ‚Ä¶, *y‚Çô*, all of them independently generated via a normal distribution with
    means *¬µ*‚ÇÅ, *¬µ*‚ÇÇ, ‚Ä¶, *¬µ‚Çô.* What is the likelihood to see these *y*‚Äôs? It is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb050a922b3f6db694c93cb8bcc19a2f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: We now want to find the parameters (that are hidden in the *¬µ·µ¢*‚Äôs) to **maximize**
    this term. This is equivalent to minimizing the mean squared error, as you can
    see.
  prefs: []
  type: TYPE_NORMAL
- en: Extensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Unequal Variances**. In fact, *œÉ* does not have to be constant. You can have
    a different *œÉ·µ¢* for each observation in your dataset. Then, you would minimize'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/71c6b101588ff794cbf66d13b9fa9aa9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: instead, which is **least squares with sample weights *s*.** Modeling libraries
    typically allow you to set these weights. In scikit-learn, for example, you can
    set the `sample_weight` keyword in the `fit` function.
  prefs: []
  type: TYPE_NORMAL
- en: This way, you can put more emphasis on certain observations by increasing the
    corresponding *s*. This is equivalent to decreasing the variance *œÉ¬≤*, i.e. you
    are more confident that the error for this observation is smaller. This method
    is also called [**weighted least squares**](https://en.wikipedia.org/wiki/Weighted_least_squares).
  prefs: []
  type: TYPE_NORMAL
- en: '**Variances Depending on The Input.** You can even say that the variance is
    also dependent on the input *x*. In this case, you get the interesting loss function
    that is also called **variance attenuation**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d4d422fccae6230d67479541e8be6ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The whole derivation process is outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f?source=post_page-----e53c579aef5b--------------------------------)
    [## Get Uncertainty Estimates in Regression Neural Networks for Free'
  prefs: []
  type: TYPE_NORMAL
- en: Given the right loss function, a standard neural network can output uncertainty
    as well
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f?source=post_page-----e53c579aef5b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Regularization.** Instead of only maximizing the likelihood of the observed
    labels*y*‚ÇÅ, *y*‚ÇÇ, ‚Ä¶, *y‚Çô,* you can take a **Bayesian standpoint** and **maximize
    the a posteriori likelihood**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ec367c68abfd26a1b36928a4056662b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Here, *p*(*y* | *w*) is the likelihood function from above. We have to decide
    on a probability density for *p*(*w*), a so-called **prior or a prior distribution**.
    If we say that the parameters are independently normally distributed around 0,
    i.e. *w·µ¢* ~ *N*(0, *ŒΩ¬≤*), then we end up with **L2 regularization, i.e. ridge
    regression**. For a Laplace distribution, we recover **L1 regularization, i.e.
    LASSO**.
  prefs: []
  type: TYPE_NORMAL
- en: Why is that? Let‚Äôs use the normal distribution as an example. We have
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc7b382737b4d8a13915832e1561e80f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: so together with our formula for *p*(*y* | *w*) from above, we have to maximize
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a2d7162b93c1510637fec06a34b3e4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: which means that we have to **minimize the mean squared error plus some regularization
    hyperparameter time the L2 norm of *w*.**
  prefs: []
  type: TYPE_NORMAL
- en: '*Note that we dropped the denominator* p*(*y*) from the Bayes formula since
    it does not depend on* w*, so we can ignore it for optimization.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can use any other prior distribution for your parameters to create more
    interesting regularizations. You can even say that your parameters *w* are normally
    distributed but **correlated** with some correlation matrix Œ£*.*
  prefs: []
  type: TYPE_NORMAL
- en: '*Let us assume that Œ£ is* positive-definite*, i.e. we are in the non-degenerate
    case. Otherwise, there is no density* p*(*w*).*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you do the math, you will find out that we then have to optimize
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb5af9d7a702a085ca22cf56f5785b6e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'for some matrix Œì. **Note: Œì is invertible and we have Œ£‚Åª¬π = Œì·µÄŒì.** This is
    also called **Tikhonov regularization**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hint:** start with the fact that'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81a3a8873f6d531a2ebb9bd177187282.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: and remember that positive-definite matrices can be [decomposed into a product
    of some invertible matrix and its transpose](https://en.wikipedia.org/wiki/Definite_matrix#Decomposition).
  prefs: []
  type: TYPE_NORMAL
- en: Minimize The Loss Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Great, so we defined our model and know what we want to optimize. But how can
    we optimize it, i.e. learn the best parameters that minimize the loss function?
    And when is there a unique solution? Let‚Äôs find out.
  prefs: []
  type: TYPE_NORMAL
- en: Ordinary Least Squares
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us assume that we do not regularize and don‚Äôt use sample weights. Then,
    the MSE can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7af931d447d5df14f791c8484c9a3d4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: This is quite abstract, so let us write it differently as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a5cea791903d850c7d7ff4bd115adab.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Using [matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector_identities),
    you can take the derivative of this function with respect to *w* (we assume that
    the bias term *b* is included there)*.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5820d6701156de11010d68969e10f50.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: If you set this gradient to zero, you end up with
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dacab3b7869c79f9cab2f4442e478c9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: If the (*n* √ó *k*)-matrix *X* has a rank of *k*, so does the (*k* √ó *k*)-matrix
    *X*·µÄ*X,* i.e. it is invertible*. Why?* It follows from [rank(*X*) *=* rank(*X*·µÄ*X*)](https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Properties)*.*
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we get the **unique solution**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa182f0811b2619d41bc9dbac897f904.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '***Note:*** *Software packages do not optimize like this but instead use gradient
    descent or other iterative techniques because it is faster. Still, the formula
    is nice and gives us some high-level insights about the problem.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But is this really a minimum? We can find out by computing the Hessian, which
    is *X*·µÄ*X.* The matrix is positive-semidefinite since *w*·µÄ*X*·µÄ*Xw = |Xw|¬≤* ‚â• 0
    for any *w*. It is even **strictly** positive-definite since *X*·µÄ*X* is invertible,
    i.e. 0 is not an eigenvector, so our optimal *w* is indeed minimizing our problem.
  prefs: []
  type: TYPE_NORMAL
- en: Perfect Multicollinearity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: That was the friendly case. But what happens if *X* has a rank smaller than
    *k*? This might happen if we have two features in our dataset where one is a multiple
    of the other, e.g. we use the features *height (in m)* and *height (in cm)* in
    our dataset. Then we have *height (in cm) = 100 * height (in m).*
  prefs: []
  type: TYPE_NORMAL
- en: It can also happen if we one-hot encode categorical data and do not drop one
    of the columns. For example, if we have a feature *color* in our dataset that
    can be red, green, or blue, then we can one-hot encode and end up with three columns
    *color_red, color_green,* and *color_blue*. For these features, we have *color_red
    + color_green + color_blue =* 1, which induces perfect multicollinearity as well.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, the rank of *X*·µÄ*X* is also smaller than *k*, so this matrix
    is not invertible.
  prefs: []
  type: TYPE_NORMAL
- en: End of story.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Or not? Actually, no, because it can mean two things: (*X*·µÄ*X*)*w = X*·µÄ*y*
    has'
  prefs: []
  type: TYPE_NORMAL
- en: no solution or
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: infinitely many solutions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It turns out that in our case, we can obtain one solution using the [Moore-Penrose
    inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse). This means
    that we are in the case of infinitely many solutions, all of them giving us the
    same (training) mean squared error loss.
  prefs: []
  type: TYPE_NORMAL
- en: If we denote the Moore-Penrose inverse of *A* by *A*‚Å∫, we can solve the linear
    system of equations as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b99b7b2b70d821c22f23e667fd03c984.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: To get the other infinitely many solutions, just add the null space of *X*·µÄ*X*
    to this specific solution.
  prefs: []
  type: TYPE_NORMAL
- en: Minimization With Tikhonov Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that we could add a prior distribution to our weights. We then had to
    minimize
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67f72d916860474636a7287cef747b8e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: for some invertible matrix Œì. Following the same steps as in ordinary least
    squares, i.e. taking the derivative with respect to *w* and setting the result
    to zero, the solution is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad542ac73d8a2a760adf430eb09b0d4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The neat part:'
  prefs: []
  type: TYPE_NORMAL
- en: X·µÄX + Œì·µÄŒì is always invertible!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let us find out why. It suffices to show that the null space of *X*·µÄ*X* + Œì·µÄŒì
    is only {0}. So, let us take a *w* with (*X*·µÄ*X* + Œì·µÄŒì)*w* = 0\. Now, our goal
    is to show that *w* = 0.
  prefs: []
  type: TYPE_NORMAL
- en: From (*X*·µÄ*X* + Œì·µÄŒì)*w* = 0 it follows that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfbadd99aed70af03cecda5cacf7730a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: which in turn implies |Œì*w*| = 0 ‚Üí Œì*w =* 0*.* SinceŒì is invertible, *w* has
    to be 0\. Using the same calculation, we can see that the Hessian is also positive-definite.
  prefs: []
  type: TYPE_NORMAL
- en: Nice, so Tikhonov regularization automatically helps make the solution unique!
    Since ridge regression is a special case of Tikhonov regression (for Œì = Œª*I‚Çñ,
    I‚Çñ* is the *k*-dimensional identity matrix), the same holds there.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Sample Weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a last point, let us also add sample weights to the Tikhonov regularization.
    Adding sample weights is equivalent to minimizing
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e85c7dc51fbd41a64ec7c105af11ba2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: For some diagonal matrix *S* with positive diagonal entries *s·µ¢.* Minimizing
    is as straightforward as in the case of ordinary least squares. The result is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b9517176c23bddef1d77cbfd7797681.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** The Hessian is also positive-definite.'
  prefs: []
  type: TYPE_NORMAL
- en: Homework for you
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assume that for the Tikhonov regularization, we do not impose that the weights
    should be centered around 0, but some other point *w*‚ÇÄ. Show that the optimization
    problem becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/177006d6d3b5ae0ee092be0de5fd1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: and that the solution is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04adac9e8b0679dd816373b0e33fc549.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: This is the most general form of Tikhov regularization. Some people prefer to
    define *P* := *S*¬≤, *Q* := Œì·µÄŒì, as [done here](https://en.wikipedia.org/wiki/Ridge_regression#Generalized_Tikhonov_regularization).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I took you on a journey through several advanced aspects of
    linear regression. By adopting a generative view, we could see that generalized
    linear models just differ from the *normal* linear models only in the type of
    distribution that is used to sample the target *y*.
  prefs: []
  type: TYPE_NORMAL
- en: Then we have seen that minimizing the mean squared error is equivalent to maximizing
    the likelihood of the observed values. If we impose a prior normal distribution
    on the learnable parameters, we end up with Tikhonov (and L2 as a special case)
    regularization. We can use different prior distributions such as the Laplace distribution
    as well, but then there are no closed solution formulas anymore. Still, convex
    programming approaches also let you find the best parameters.
  prefs: []
  type: TYPE_NORMAL
- en: As a last step, we found a lot of direct solution formulas for each minimization
    problem considered. These formulas are usually not used in practice for large
    datasets, but we could see that the solutions are always unique. And we also learned
    to do some calculus on the way. üòâ
  prefs: []
  type: TYPE_NORMAL
- en: I hope that you learned something new, interesting, and valuable today. Thanks
    for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '*If you have any questions, write me on* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*!*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And if you want to dive deeper into the world of algorithms, give my new publication
    **All About Algorithms** a try! I‚Äôm still searching for writers!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/all-about-algorithms?source=post_page-----e53c579aef5b--------------------------------)
    [## All About Algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: From intuitive explanations to in-depth analysis, algorithms come to life with
    examples, code, and awesome‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/all-about-algorithms?source=post_page-----e53c579aef5b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
