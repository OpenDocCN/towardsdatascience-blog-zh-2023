- en: 'Decision Trees: Introduction & Intuition'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/decision-trees-introduction-intuition-dac9592f4b7f](https://towardsdatascience.com/decision-trees-introduction-intuition-dac9592f4b7f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Making data-informed decisions with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)
    ·10 min read·Feb 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13f81f75cc709437ff2192216ceadf8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [niko photos](https://unsplash.com/@niko_photos?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) peppered
    with thinking emojis.
  prefs: []
  type: TYPE_NORMAL
- en: This is the first article in a series on Decision Trees. In this post, I introduce
    decision trees and describe how to *grow* them using data. The post concludes
    with example Python code showing how to create and use a decision tree to help
    make medical prognoses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key Points:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision Trees are a widely-used and intuitive machine learning technique used
    to solve prediction problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can *grow* decision trees from data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter tuning can be used to help avoid the *overfitting* problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What are Decision Trees?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Decision trees** are a **widely-used and intuitive machine learning technique**.
    Typically, they are **used to solve prediction problems**. For example, predicting
    tomorrow’s weather forecast or estimating an individual''s probability of developing
    heart disease.'
  prefs: []
  type: TYPE_NORMAL
- en: They work through a series of yes-no questions, which are used to narrow down
    possible choices and arrive at an outcome. A simple example of a decision tree
    is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19d95fea62f210ec0bc0b398fdea68a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Example decision tree to predict whether I will drink tea or coffee. Image by
    author.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the above figure, a decision tree consists of nodes connected by
    directed edges. Each node in a decision tree corresponds to a conditional statement
    based on a predictor variable.
  prefs: []
  type: TYPE_NORMAL
- en: At the top of the decision tree shown above is the **root node**, which **sets
    the initial splitting of data** records. Here we evaluate whether it is after
    4 PM or not. Each possible response (yes or no) follows a different path in our
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: If yes, we follow the left branch and end up at a **leaf node** (also called
    the terminal node). **No further splits are required to determine the outcome
    at this type of node**. In this case, we go with tea over coffee so we can get
    to bed at a reasonable hour.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, if it is 4 PM or earlier, we follow the right branch and end up
    at a so-called **splitting node**. These nodes **further split data records**
    based on conditional statements. From here, we evaluate whether the hours of sleep
    from last night were more than 6 hours. If yes, we go with tea again, but if no,
    we go with coffee ☕️.
  prefs: []
  type: TYPE_NORMAL
- en: Using Decision Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In practice, we often don’t use decision trees like we did just now *(*i.e.
    looking at a decision tree and following along for a particular data record).
    Rather, have a computer evaluate data for us. All we have to do is give the computer
    the data it needs in the form of a table.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of this is shown below. Here we have tabular data with two variables:
    time of day and hours of sleep from the previous night (blue columns). Then using
    the decision tree above, we can assign an appropriate caffeinated beverage to
    each record (green column).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5fcefe3250fcb94b3929efba85702486.png)'
  prefs: []
  type: TYPE_IMG
- en: Example table of input data and the resulting decision tree prediction. Image
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Graphical View of a Decision Tree**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way to think about decision trees is **graphically**. (*This is personally
    the intuition I carry around for decision trees.)*
  prefs: []
  type: TYPE_NORMAL
- en: Imagine we take the two predictor variables from the example decision tree and
    visualize them on a 2D plot. We can then represent the decision tree splits as
    lines that divide our plot into different sections. This then allows us to identify
    the beverage choice by simply looking at which quadrant a data point lies.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, this is all a decision tree is doing. **Partitioning the predictor
    space into sections and assigning a label (or probability) to each section**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1dca41b0e04b37add727a37582954df4.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphical view of decision tree predictions for tea or coffee example. Image
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to Grow a Decision Tree?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are an intuitive way to partition data. However, it may not be
    easy to draw out an appropriate decision tree by hand using data. In such cases,
    **we can use machine learning** strategies to learn the “best” decision tree for
    a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Data can be used to grow decision trees in an optimization process called **training**.
    Training requires a training dataset consisting of predictor variables pre-labeled
    with target values.
  prefs: []
  type: TYPE_NORMAL
- en: A standard strategy for training a decision tree uses something called **Greedy
    Search**. This is a popular technique in optimization, where we simplify a more
    complicated optimization problem by finding *locally* optimal solutions instead
    of *globally* optimal ones. (*I give an intuition for greedy search in a previous
    article on* [*causal discovery*](https://medium.com/towards-data-science/causal-discovery-6858f9af6dcb)*.)*
  prefs: []
  type: TYPE_NORMAL
- en: In the case of decision trees, the Greedy Search determines the gain from each
    possible splitting option and then chooses the one that provides the greatest
    gain [1,2]. Here “**gain**” is determined by the **split criterion**, which can
    be based on a few different quantities, e.g. **Gini impurity**, **information
    gain**, **mean squared error (MSE)**, among others. This process is repeated recursively
    until the decision tree is fully grown.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if using Gini impurity, data records are recursively split into
    two groups such that the **weighted average impurity of the resulting groups is
    minimized**. This splitting procedure can continue until all data partitions are
    *pure*, meaning all data records in a given partition corresponds to a single
    target value.
  prefs: []
  type: TYPE_NORMAL
- en: Although this implies decision trees can be *perfect* estimators, such an approach
    would result in **overfitting**. The trained **decision tree would not perform
    well on data sufficiently different than the training dataset**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyperparameter Tuning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way to combat the overfitting problem is hyperparameter tuning. **Hyperparameters**
    are **values that constrain the growth of a decision tree**.
  prefs: []
  type: TYPE_NORMAL
- en: Common decision tree hyperparameters are the maximum number of splits, minimum
    leaf size, and the number of splitting variables. The **key result** of setting
    decision tree hyperparameters is to **limit the tree’s size**, which can help
    avoid overfitting and improve generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: '**Alternative Training Strategies**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the training process I have described above is widely-used for decision
    trees, there are alternative approaches we can use.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pruning** — One such approach is called pruning [3]. In a sense, pruning
    is the opposite of growing a decision tree. Instead of starting from a root node
    and recursively adding nodes, we start with a fully grown tree and iteratively
    remove nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: While the pruning process can be done in multiple ways, it commonly will drop
    nodes that do not significantly increase model error. This is an alternative way
    to avoid overfitting in lieu of hyperparameter tuning to limit tree growth [3].
  prefs: []
  type: TYPE_NORMAL
- en: '**Maximum Likelihood** — We can train a decision tree using the maximum likelihood
    framework [4]. While this approach is less well-known, it sits on a strong theoretical
    framework. It allows us to use information criteria such as AIC and BIC to objectively
    optimize the number of parameters in the tree and its performance, which helps
    side-step the need for extensive hyperparameter tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example code: Sepsis Survival Prediction Using a Decision Tree'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, with a basic understanding of decision trees and how we can develop one
    from data, let’s dive into a concrete example using Python. Here we will use a
    dataset from the [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/Sepsis+survival+minimal+clinical+records)
    to train a decision tree to predict whether a patient will survive based on their
    age, sex, and number of sepsis episodes they’ve experienced [5,6].
  prefs: []
  type: TYPE_NORMAL
- en: For the decision tree training, we will use the sklearn Python library [7].
    The code for this example is freely available in the [GitHub repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree).
  prefs: []
  type: TYPE_NORMAL
- en: We start by importing some helpful libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we load our data from a .csv file and do some data preparation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fea3c309833e691f553f7aaf79ce4e0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Histograms for each variable in dataset. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Notice in the bottom-right histogram we have many more alive records than dead.
    This is called an **imbalanced dataset**. For a simple decision tree classifier,
    learning from imbalanced data can lead to the **decision tree *over-predicting*
    the majority class**.
  prefs: []
  type: TYPE_NORMAL
- en: To handle this situation, we can over-sample the minority class to make our
    data more balanced. One way to do this is using a technique called **Synthetic
    Minority Over-sampling Technique** (**SMOTE)**. While I will leave further details
    of SMOTE for a future article, for now, it will suffice to say this helps us balance
    our data and improve our decision tree model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/045d096e7d85b27e2f9ba1701d3cb90b.png)'
  prefs: []
  type: TYPE_IMG
- en: Outcome histogram after SMOTE. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The final step for our data preparation is to split our resampled data into
    training and testing datasets. The **training data** will be used to **grow the
    decision tree,** and **testing data** will be used to **evaluate its performance**.
    Here we use an 80–20 train-test split.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now with our training data, we can create our decision tree. Sklearn makes this
    super easy, with just two lines of code, we have a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a look at the result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cad6d35f2ad5a40779dfc1f54b3ee1e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Fully grown decision tree. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Needless to say, this is a very big decision tree, which can make interpreting
    the results difficult. However, let’s put that point aside for now and evaluate
    the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: For evaluating performance, we use a **confusion matrix**, which **displays
    the number of true positives (TP), true negatives (TN), false positives (FP),
    and false negatives (FN)**.
  prefs: []
  type: TYPE_NORMAL
- en: I won’t get into a discussion of confusion matrices here, but for now what **we
    want** is for the **on-diagonal numbers to be big** and the **off-diagonal terms
    to be small**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/847c78b12ad89d7bf2bf0e5f6bc326b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrices for fully-grown decision tree. (Left) Training data set.
    (Right) Testing dataset. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can take the numbers from our confusion matrices and compute three different
    performance metrics: precision, recall, and f1-score**.** Briefly, **precision**
    = TP / (TP + FP), **recall** = TP / (TP + FN), and the **f1-score** is the harmonic
    mean of precision and recall.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/867103da82f0cb2b241406a704a3ed82.png)'
  prefs: []
  type: TYPE_IMG
- en: Three performance metrics for fully-grown decision tree. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The code to generate these results is given below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Hyperparameter Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the decision tree has decent performance on the data used here, there
    is still the lingering issue of **interpretability**. Looking at the decision
    tree displayed previously, it would be challenging for a clinician to extract
    any meaningful insights from the decision tree’s logic.
  prefs: []
  type: TYPE_NORMAL
- en: This is where hyperparameter tuning can help. To do this with sklearn, we can
    simply add input arguments to our decision tree training step.
  prefs: []
  type: TYPE_NORMAL
- en: Here we will try setting the max_depth = 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s take a look at the resulting decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9b0eaf963d0a43c1b277088afe9ea3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Tuned decision tree so that max_depth=3\. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Since we constrained the max depth of the tree, we can plainly see what splits
    are happening here.
  prefs: []
  type: TYPE_NORMAL
- en: We again evaluate the model’s performance using confusion matrices and the same
    three performance metrics as before.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9540e446233230f2be271a1a4f1045aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrices for hyperparameter tuned decision tree. (Left) Training data
    set. (Right) Testing dataset. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0246f2318fcfd809c78fd1598db48ef0.png)'
  prefs: []
  type: TYPE_IMG
- en: 3 performance metrics for hyperparameter tuned decision tree. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Although it may seem the fully-grown tree is preferable to the hyperparameter-tuned
    one, this goes back to the discussion on overfitting. Yes, the fully-grown tree
    performance is better on the current data, but I would not expect this to be the
    case for other data.
  prefs: []
  type: TYPE_NORMAL
- en: Put another way, **although the simpler decision tree has worse performance
    here, it will likely generalize better than the fully-grown tree.**
  prefs: []
  type: TYPE_NORMAL
- en: This hypothesis can be tested by applying each model to the other two datasets
    available in the GitHub [repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree?source=post_page-----dac9592f4b7f--------------------------------)
    [## YouTube-Blog/decision-tree/decision_tree at main · ShawhinT/YouTube-Blog'
  prefs: []
  type: TYPE_NORMAL
- en: Codes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/decision-tree/decision_tree
    at main ·…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree?source=post_page-----dac9592f4b7f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Decision Tree Ensembles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While hyperparameter tuning can improve the generalizability of a decision tree,
    it still leaves something to be desired in regard to performance. In our example
    above, after hyperparameter tuning, the decision tree still mislabelled the training
    data **35%** of the time, which is a big deal when talking about life and death
    (*like with the example here*).
  prefs: []
  type: TYPE_NORMAL
- en: A popular solution to this problem is to use an **ensemble of trees rather than
    a single decision tree to make predictions.** These are called **decision tree
    ensembles** and will be the topic of the [next article](https://medium.com/towards-data-science/10-decision-trees-are-better-than-1-719406680564)
    in this series.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/10-decision-trees-are-better-than-1-719406680564?source=post_page-----dac9592f4b7f--------------------------------)
    [## 10 Decision Trees are Better Than 1'
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down bagging, boosting, Random Forest, and AdaBoost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/10-decision-trees-are-better-than-1-719406680564?source=post_page-----dac9592f4b7f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Connect**: [My website](https://shawhintalebi.com/) | [Book a call](https://calendly.com/shawhintalebi)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Socials**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) ☕️'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/subscribe?source=post_page-----dac9592f4b7f--------------------------------)
    [## Get FREE access to every new story I write'
  prefs: []
  type: TYPE_NORMAL
- en: Get FREE access to every new story I write P.S. I do not share your email with
    anyone By signing up, you will create a…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----dac9592f4b7f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[1] [*Classification and Regression Trees by* Breiman et al.](https://doi.org/10.1201/9781315139470)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Decision trees: a recent overview by Kotsiantis, S. B.](https://link.springer.com/article/10.1007/s10462-011-9272-4)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [A comparative analysis of methods for pruning decision trees by Esposito
    et al.](https://doi.org/10.1109/34.589207)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [Maximum likelihood regression trees by Su et al.](https://doi.org/10.1198/106186004X2165)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/census+income)
    [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School
    of Information and Computer Science. (CC BY 4.0)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] [Survival prediction of patients with sepsis from age, sex, and septic
    episode number alone by Chicco & Jurman](https://www.nature.com/articles/s41598-020-73558-3)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] [Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html),
    Pedregosa *et al.*, JMLR 12, pp. 2825–2830, 2011.'
  prefs: []
  type: TYPE_NORMAL
