- en: 'Private GPT: Fine-Tune LLM on Enterprise Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 私有GPT：在企业数据上微调LLM
- en: 原文：[https://towardsdatascience.com/private-gpt-fine-tune-llm-on-enterprise-data-7e663d808e6a](https://towardsdatascience.com/private-gpt-fine-tune-llm-on-enterprise-data-7e663d808e6a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/private-gpt-fine-tune-llm-on-enterprise-data-7e663d808e6a](https://towardsdatascience.com/private-gpt-fine-tune-llm-on-enterprise-data-7e663d808e6a)
- en: Doing cool things with data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用数据做一些酷炫的事情
- en: '[](https://priya-dwivedi.medium.com/?source=post_page-----7e663d808e6a--------------------------------)[![Priya
    Dwivedi](../Images/73087cb699750466312cc4752e2044d4.png)](https://priya-dwivedi.medium.com/?source=post_page-----7e663d808e6a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e663d808e6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e663d808e6a--------------------------------)
    [Priya Dwivedi](https://priya-dwivedi.medium.com/?source=post_page-----7e663d808e6a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://priya-dwivedi.medium.com/?source=post_page-----7e663d808e6a--------------------------------)[![Priya
    Dwivedi](../Images/73087cb699750466312cc4752e2044d4.png)](https://priya-dwivedi.medium.com/?source=post_page-----7e663d808e6a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e663d808e6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e663d808e6a--------------------------------)
    [Priya Dwivedi](https://priya-dwivedi.medium.com/?source=post_page-----7e663d808e6a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e663d808e6a--------------------------------)
    ·9 min read·Jul 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----7e663d808e6a--------------------------------)
    ·阅读时间9分钟·2023年7月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b1c22ad1ee60a9ba1c68c1dc31d44dc6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1c22ad1ee60a9ba1c68c1dc31d44dc6.png)'
- en: Photo by [Robynne Hu](https://unsplash.com/@robynnexy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/technology?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Robynne Hu](https://unsplash.com/@robynnexy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，来源于[Unsplash](https://unsplash.com/s/photos/technology?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: In the era of big data and advanced artificial intelligence, language models
    have emerged as formidable tools capable of processing and generating human-like
    text. Large Language Models like ChatGPT are general-purpose bots capable of having
    conversations on many topics. However, LLMs can also be fine-tuned on domain-specific
    data making them more accurate and on-point on domain-specific enterprise questions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据和先进人工智能的时代，语言模型已成为能够处理和生成类人文本的强大工具。像ChatGPT这样的大型语言模型是能够在许多主题上进行对话的通用机器人。然而，LLM也可以在特定领域的数据上进行微调，使它们在特定领域的企业问题上更加准确和切题。
- en: 'Many industries and applications will require a fine-tuned LLMs. Reasons include:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 许多行业和应用将需要经过微调的LLM。原因包括：
- en: Better performance from a chatbot trained on specific data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从在特定数据上训练的聊天机器人中获得更好的性能
- en: OpenAI models like chatgpt are a black box and companies may be hesitant to
    share their confidential data over an API
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像ChatGPT这样的OpenAI模型是一个黑箱，公司可能会犹豫通过API分享他们的机密数据。
- en: ChatGPT API costs may be prohibitive for large applications
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGPT API的费用对于大型应用程序可能会很高。
- en: The challenge with fine-tuning an LLM is that the process is unknown and the
    computational resources required to train a billion-parameter model without optimizations
    can be prohibitive.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLM的挑战在于该过程尚不明确，而且训练一个没有优化的十亿参数模型所需的计算资源可能非常昂贵。
- en: Fortunately, a lot of research has been done on training techniques that allow
    us now to fine-tune LLMs on smaller GPUs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，很多关于训练技术的研究已经完成，这使我们现在能够在较小的GPU上微调LLM。
- en: In this blog, we will cover some of the techniques used for fine-tuning LLMs.
    We will train [Falcon 7B model](https://huggingface.co/blog/falcon) on finance
    data on a Colab GPU! The techniques used here are general and can be applied to
    other bigger models like MPT-7B and MPT-30B.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本博客中，我们将介绍一些用于微调LLM的技术。我们将使用Colab GPU在金融数据上训练[Falcon 7B模型](https://huggingface.co/blog/falcon)！这里使用的技术是通用的，可以应用于其他更大的模型，如MPT-7B和MPT-30B。
- en: At [Deep Learning Analytics](https://deeplearninganalytics.org/), we have been
    building custom machine-learning models for the last 6 years. [Reach out to us](https://deeplearninganalytics.org/contact-us/)
    if you are interested in fine-tuning a LLM for your application.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在[深度学习分析](https://deeplearninganalytics.org/)中，我们过去6年一直在构建定制的机器学习模型。如果你对为你的应用程序微调一个LLM感兴趣，[联系我们](https://deeplearninganalytics.org/contact-us/)。
- en: QLoRA
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QLoRA
- en: QLoRA, which stands for “Quantized Low-Rank Adaptation,” presents an approach
    that combines quantization and low-rank adaptation to achieve efficient fine-tuning
    of AI models. Both these terms are explained in more detail below.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA，即“量化低秩适配”，呈现了一种结合量化和低秩适配的方法，以实现 AI 模型的高效微调。以下将详细解释这两个术语。
- en: QLoRA reduces the memory required for fine-tuning LLM, without any drop in performance
    with respect to a standard 16-bit model fine-tuned model. This method enables
    a 7 billion parameter model to be fine-tuned on a 16GB GPU, a 33 billion parameter
    model to be fine-tuned on a single 24GB GPU and a 65 billion parameter model to
    be fine-tuned on a single 46GB GPU.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA 减少了微调 LLM 所需的内存，同时在性能上与标准的 16 位模型微调模型没有任何下降。这种方法使得 70 亿参数的模型可以在 16GB GPU
    上微调，330 亿参数的模型可以在单个 24GB GPU 上微调，以及 650 亿参数的模型可以在单个 46GB GPU 上微调。
- en: This implies most companies can now have fine-tuned LLMs or on-prem models for
    a small cost.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着大多数公司现在可以以较小的成本拥有微调后的 LLM 或本地模型。
- en: 'QLoRA is composed of two techniques:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA 由两种技术组成：
- en: Lora — Low-Rank Adaptation
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LoRA — 低秩适配
- en: Quantization
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 量化
- en: LoRA
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA
- en: LoRA, which stands for Low-Rank Adapters (LoRA), are small sets of trainable
    parameters, injected into each layer of the Transformer architecture while fine-tuning.
    While original model weights are frozen and not updated, these small sets of injected
    weights are updated during fine-tuning. This greatly reduces the number of trainable
    parameters for downstream tasks. Gradients during stochastic gradient descent
    are passed through the frozen pre-trained model weights to the adapter. Thus,
    only these adapters, with a small memory footprint, are updated during the time
    of training.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA，即低秩适配器（LoRA），是小型可训练参数集，注入到 Transformer 架构的每一层中进行微调。在微调过程中，原始模型权重被冻结且未更新，这些注入的权重在微调时会更新。这大大减少了下游任务的可训练参数数量。在随机梯度下降期间，梯度通过被冻结的预训练模型权重传递到适配器。因此，只有这些具有小内存占用的适配器在训练过程中被更新。
- en: Quantization
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化
- en: Next, we will discuss Quantization. To understand quantization, first, we need
    to know the different data types that are generally used to store model weights.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论量化。要理解量化，首先需要了解通常用于存储模型权重的不同数据类型。
- en: FP32 — Full Precision weights which take 4 bytes of memory
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP32 — 完整精度权重，占用 4 字节内存
- en: FP16/BF16 — Half precision weights which take 2 bytes of memory
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP16/BF16 — 半精度权重，占用 2 字节内存
- en: INT8 —Data-type consists of an 8-bit representation that can store 2⁸ different
    values (between [0, 255] or [-128, 127] for signed integers)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: INT8 — 数据类型包括一个 8 位表示，可以存储 2⁸ 个不同的值（在 [0, 255] 或 [-128, 127] 的范围内用于有符号整数）
- en: FP8 and FP4, which stand for Floating-Point 8-bit and 4-bit precision, respectively.
    They are part of the mini floats family of floating point values.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP8 和 FP4，分别代表浮点 8 位和 4 位精度。它们是小型浮点值家族的一部分。
- en: Using quantization, the model needs much smaller memory than the memory needed
    to store the original model. For example, an 8-bit quantized model would require
    only 1/4th of the model size, as compared to a model stored in a 32-bit datatype.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用量化，模型需要的内存比存储原始模型所需的内存要小得多。例如，一个 8 位量化模型只需原始模型大小的 1/4。
- en: But how is it possible to store the original 32-bit weight in 8-bit data types
    like INT8 or FP8?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何将原始的 32 位权重存储在像 INT8 或 FP8 这样的 8 位数据类型中呢？
- en: Quantization means “rounding” off values, from one data type to another. It
    works with squeezing larger values into data types with less number of bits, but
    with a small loss of precision.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 量化意味着将值从一种数据类型“舍入”到另一种数据类型。它通过将较大的值压缩到具有较少位数的数据类型中来工作，但会有少量的精度损失。
- en: In the case of QLoRA, model weights are stored in a 4-bit floating point data
    type. But, in order to minimize the overall loss in accuracy, all the matrix operations
    are performed in a 16-bit floating point. Once the calculation is done, weights
    are stored/quantized back, before they are stored again.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 QLoRA 的情况下，模型权重存储在 4 位浮点数据类型中。但为了尽量减少总体精度损失，所有矩阵运算都在 16 位浮点中执行。计算完成后，权重会被存储/量化回来，然后再存储。
- en: To summarize, QLORA uses one storage data type (typically 4-bit NormalFloat)
    and one computation data type (16-bit BrainFloat). The storage data type is dequantized
    to the computation data type for performing the forward and backward pass. The
    weights are decompressed only when they are needed, therefore the memory usage
    stays low during training and inference. More details about these could be found
    here. [4]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，QLORA使用一种存储数据类型（通常是4位NormalFloat）和一种计算数据类型（16位BrainFloat）。存储数据类型在进行前向和后向传递时被反量化为计算数据类型。权重仅在需要时才被解压，因此在训练和推理过程中内存使用保持低位。更多详细信息可以在这里找到。[4]
- en: What about the accuracy impacts of QLoRA fine tuning?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 那么QLoRA微调的准确性影响如何？
- en: QLoRA tuning is shown to match 16-bit fine-tuning methods in a wide range of
    experiments. In addition, the Guanaco models, which use QLoRA fine-tuning for
    LLaMA models on the [OpenAssistant dataset (OASST1)](https://huggingface.co/datasets/OpenAssistant/oasst1),
    are state-of-the-art chatbot systems and are close to ChatGPT on the Vicuna benchmark.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA调优在各种实验中显示出与16位微调方法相匹配的效果。此外，使用QLoRA微调LLaMA模型的Guanaco模型在[OpenAssistant数据集（OASST1）](https://huggingface.co/datasets/OpenAssistant/oasst1)上是最先进的聊天机器人系统，并且在Vicuna基准测试中接近ChatGPT。
- en: HuggingFace Support for fine-tuning
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HuggingFace对微调的支持
- en: HuggingFace has released several libraries that can be used to fine-tune LLMs
    easily.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace发布了几个可以轻松微调LLM的库。
- en: 'These include:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包括：
- en: '[PEFT Library](https://huggingface.co/blog/peft): HuggingFace has released
    a library on Parameter Efficient Fine Tuning (PEFT) which has support for LORA.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PEFT库](https://huggingface.co/blog/peft)：HuggingFace发布了一个关于参数高效微调（PEFT）的库，该库支持LORA。'
- en: Quantization Support — Many models can be loaded in 8-bit and 4-bit precision
    using bitsandbytes module. The basic way to load a model in 4bit is to pass the
    argument load_in_4bit=True when calling the from_pretrained method.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化支持——许多模型可以使用bitsandbytes模块以8位和4位精度加载。以4位加载模型的基本方法是调用from_pretrained方法时传递参数load_in_4bit=True。
- en: Accelerate library — Accelerate library has many features to make it easy to
    reduce the memory requirements of models
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Accelerate库——Accelerate库具有许多功能，能够轻松减少模型的内存需求
- en: '[Supervised Fine-Tuning Trainer](https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTTrainer)
    — The SFT trainer is the trainer class for supervised fine-tuning of Large LLMs.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[监督微调训练器](https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTTrainer)——SFT训练器是大规模LLM的监督微调训练类。'
- en: Now we combine all the techniques here to train the Falcon 7B model on a finance
    dataset.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们结合所有技术在金融数据集上训练Falcon 7B模型。
- en: '**Training Falcon 7B on Alpaca Finance Dataset**'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**在Alpaca Finance数据集上训练Falcon 7B**'
- en: We were successfully able to fine-tune the recently released Falcon-7B model,
    on [**Alpaca-Finance**](https://huggingface.co/datasets/gbharti/finance-alpaca)
    dataset, on Google Colab. This dataset consisted of around 70K finance data points.
    This is an open source data available on HuggingFace dataset hub and can be loaded
    directly from the hub.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地在Google Colab上微调了最近发布的Falcon-7B模型，使用[**Alpaca-Finance**](https://huggingface.co/datasets/gbharti/finance-alpaca)数据集。这个数据集包含了大约70K的金融数据点。这是一个开源数据，存储在HuggingFace数据集中心，并可以直接从中心加载。
- en: 'Data is finance questions with answers. A snapshot of the data is shown below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是金融问题及其答案。数据快照如下：
- en: '![](../Images/a4846bbf9fefbf62bf8874eefc9a2359.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4846bbf9fefbf62bf8874eefc9a2359.png)'
- en: Alpaca Finance Dataset. Image by Author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Alpaca Finance数据集。图片来源：作者
- en: The code for this fine-tuning is shared [here](https://colab.research.google.com/drive/17Uyb3Q5_RwlpBqpk4XMRQmp7mfV9AdNE?authuser=2#scrollTo=mNnkgBq7Q3EU).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个微调的代码可以在[这里](https://colab.research.google.com/drive/17Uyb3Q5_RwlpBqpk4XMRQmp7mfV9AdNE?authuser=2#scrollTo=mNnkgBq7Q3EU)查看。
- en: First, we load a pre-trained Falcon-7B model from a hugging-face. We use AutoModelForCausalLM
    , generally used to load auto-regressive language models like GPT. Notice, we
    set the storage type to 4-bit and the computation type to FP-16.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从HuggingFace加载一个预训练的Falcon-7B模型。我们使用AutoModelForCausalLM，通常用于加载像GPT这样的自回归语言模型。注意，我们将存储类型设置为4位，将计算类型设置为FP-16。
- en: '![](../Images/dc2a510ec25294ab5ec1f2455e8d7c43.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc2a510ec25294ab5ec1f2455e8d7c43.png)'
- en: Code Snippet for loading Falcon 7B model. Image by Author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 加载Falcon 7B模型的代码片段。图片来源：作者
- en: Now, we’re gonna create Adapters (Remember ! Extra layers added to Transformers
    modules, that will hold our fine-tunes weights). We will add them to linear layers
    as well as query value pairs of Transformers modules, as suggested, for best accuracy
    of the fine-tuned model. Below, one can notice the same in the target_modules
    parameter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建 Adapters（记住！额外的层添加到 Transformers 模块中，将保存我们的微调权重）。我们将根据建议，将它们添加到线性层以及
    Transformers 模块的查询值对中，以获得最佳的微调模型精度。下面可以在 target_modules 参数中注意到这一点。
- en: '![](../Images/bed89bcadf6207c61921b0076adca415.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bed89bcadf6207c61921b0076adca415.png)'
- en: PEFT Configuration. Image by Author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT 配置。图片由作者提供
- en: Finally, we will bring everything together, while initializing the SFTTrainer
    class. We also provide our data, appropriate token, formatting function and max_seq_length
    to it.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将整合一切，同时初始化 SFTTrainer 类。我们还会提供我们的数据、适当的令牌、格式化函数和 max_seq_length。
- en: The model takes about an hour to train. I used WandB for logging. More details
    are in the Colab Notebook [here](https://colab.research.google.com/drive/17Uyb3Q5_RwlpBqpk4XMRQmp7mfV9AdNE?authuser=2#scrollTo=pq5bgHt9vwvH).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练大约需要一个小时。我使用了 WandB 进行日志记录。更多细节请查看 Colab Notebook [这里](https://colab.research.google.com/drive/17Uyb3Q5_RwlpBqpk4XMRQmp7mfV9AdNE?authuser=2#scrollTo=pq5bgHt9vwvH)。
- en: The training loss was quite low after 100 steps.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失在 100 步后相当低。
- en: '![](../Images/dad08719d055447de7c695fa84fd4fe9.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dad08719d055447de7c695fa84fd4fe9.png)'
- en: Train Loss — Finance Alpaca Dataset. Image by Author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失 — 财务 Alpaca 数据集。图片由作者提供
- en: Inference with Alpaca Finance Dataset
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Alpaca Finance 数据集进行推理
- en: The Colab Notebook also has the inference code. I found the model did well in
    responding to a question. On my question regarding income needed to retire, it
    had a long generation. It covered some rules of thumb regarding income from 401K
    and other pension plans. It emphasized the need to know the retirement income
    to come up with an answer and finally suggested I talk to a financial planner!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Colab Notebook 还包含推理代码。我发现模型在回应问题时表现良好。对于我关于退休所需收入的问题，它有一个较长的生成。它涵盖了一些关于 401K
    和其他养老金计划的经验法则。它强调了了解退休收入以给出答案的必要性，最后建议我咨询财务规划师！
- en: The response is not as clean as chatGPT would give but good to see that a 7B
    model can be trained and run quite easily.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 响应不如 chatGPT 那样干净，但很高兴看到一个 7B 模型可以相对容易地训练和运行。
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Another question I tried is around portfolio diversification. Again the quality
    of response from a small model surprised me!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试的另一个问题是关于投资组合多样化的。再次，来自一个小模型的响应质量让我感到惊讶！
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Conclusion
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: It has become easier to fine-tune LLMs on custom datasets which can give people
    access to their own “private GPT” model. The custom models can be locally hosted
    on a commercial GPU and have a ChatGPT like interface.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在自定义数据集上微调LLMs变得更加容易，这可以让人们使用自己的“私有 GPT”模型。这些自定义模型可以在商业 GPU 上本地托管，并具有类似 ChatGPT
    的界面。
- en: '[Deep Learning Analytics](http://deeplearninganalytics.org/) is a trusted provider
    of custom machine learning models tailored to diverse use cases. With a global
    client base, we have collaborated with organizations to develop bespoke solutions
    aligned with their unique requirements. If you’re seeking to leverage the power
    of Language Models (LLMs) for your application, we invite you to connect with
    us. Reach out to our team at [info@deeplearninganalytics.org](mailto:info@deeplearninganalytics.org)
    to explore the possibilities. We look forward to discussing your project further.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[深度学习分析](http://deeplearninganalytics.org/)是一个值得信赖的定制机器学习模型提供商，适用于多种使用场景。我们拥有全球客户基础，与组织合作开发符合其独特需求的定制解决方案。如果您希望利用语言模型（LLMs）的力量进行应用，欢迎与我们联系。请通过
    [info@deeplearninganalytics.org](mailto:info@deeplearninganalytics.org) 与我们的团队联系，探索更多可能性。我们期待进一步讨论您的项目。'
- en: References
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [https://huggingface.co/blog/falcon](https://huggingface.co/blog/falcon)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://huggingface.co/blog/falcon](https://huggingface.co/blog/falcon)'
- en: '[2] [https://huggingface.co/blog/hf-bitsandbytes-integration](https://huggingface.co/blog/hf-bitsandbytes-integration)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://huggingface.co/blog/hf-bitsandbytes-integration](https://huggingface.co/blog/hf-bitsandbytes-integration)'
- en: '[3] [https://huggingface.co/blog/peft](https://huggingface.co/blog/peft)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://huggingface.co/blog/peft](https://huggingface.co/blog/peft)'
- en: '[4] [https://huggingface.co/blog/4bit-transformers-bitsandbytes#qlora-paper-a-new-way-of-democratizing-quantized-large-transformer-models](https://huggingface.co/blog/4bit-transformers-bitsandbytes#qlora-paper-a-new-way-of-democratizing-quantized-large-transformer-models)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://huggingface.co/blog/4bit-transformers-bitsandbytes#qlora-paper-a-new-way-of-democratizing-quantized-large-transformer-models](https://huggingface.co/blog/4bit-transformers-bitsandbytes#qlora-paper-a-new-way-of-democratizing-quantized-large-transformer-models)'
- en: '[5] [https://www.promptengineering.org/exploring-qloras-potential-for-accessibility-and-innovation/](https://www.promptengineering.org/exploring-qloras-potential-for-accessibility-and-innovation/)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://www.promptengineering.org/exploring-qloras-potential-for-accessibility-and-innovation/](https://www.promptengineering.org/exploring-qloras-potential-for-accessibility-and-innovation/)'
- en: '[6] [https://blog.gopenai.com/paper-review-qlora-efficient-finetuning-of-quantized-llms-a3c857cd0cca](https://blog.gopenai.com/paper-review-qlora-efficient-finetuning-of-quantized-llms-a3c857cd0cca)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://blog.gopenai.com/paper-review-qlora-efficient-finetuning-of-quantized-llms-a3c857cd0cca](https://blog.gopenai.com/paper-review-qlora-efficient-finetuning-of-quantized-llms-a3c857cd0cca)'
- en: '[7] QLoRA paper -: [https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] QLoRA 论文 -: [https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)'
- en: '[8] LoRA paper -: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] LoRA 论文 -: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)'
- en: '[9] [https://huggingface.co/docs/trl/main/en/sft_trainer](https://huggingface.co/docs/trl/main/en/sft_trainer)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [https://huggingface.co/docs/trl/main/en/sft_trainer](https://huggingface.co/docs/trl/main/en/sft_trainer)'
