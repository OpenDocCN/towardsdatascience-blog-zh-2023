- en: 'Private GPT: Fine-Tune LLM on Enterprise Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/private-gpt-fine-tune-llm-on-enterprise-data-7e663d808e6a](https://towardsdatascience.com/private-gpt-fine-tune-llm-on-enterprise-data-7e663d808e6a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Doing cool things with data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://priya-dwivedi.medium.com/?source=post_page-----7e663d808e6a--------------------------------)[![Priya
    Dwivedi](../Images/73087cb699750466312cc4752e2044d4.png)](https://priya-dwivedi.medium.com/?source=post_page-----7e663d808e6a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e663d808e6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e663d808e6a--------------------------------)
    [Priya Dwivedi](https://priya-dwivedi.medium.com/?source=post_page-----7e663d808e6a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e663d808e6a--------------------------------)
    ·9 min read·Jul 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1c22ad1ee60a9ba1c68c1dc31d44dc6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Robynne Hu](https://unsplash.com/@robynnexy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/technology?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the era of big data and advanced artificial intelligence, language models
    have emerged as formidable tools capable of processing and generating human-like
    text. Large Language Models like ChatGPT are general-purpose bots capable of having
    conversations on many topics. However, LLMs can also be fine-tuned on domain-specific
    data making them more accurate and on-point on domain-specific enterprise questions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Many industries and applications will require a fine-tuned LLMs. Reasons include:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Better performance from a chatbot trained on specific data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI models like chatgpt are a black box and companies may be hesitant to
    share their confidential data over an API
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT API costs may be prohibitive for large applications
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The challenge with fine-tuning an LLM is that the process is unknown and the
    computational resources required to train a billion-parameter model without optimizations
    can be prohibitive.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, a lot of research has been done on training techniques that allow
    us now to fine-tune LLMs on smaller GPUs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: In this blog, we will cover some of the techniques used for fine-tuning LLMs.
    We will train [Falcon 7B model](https://huggingface.co/blog/falcon) on finance
    data on a Colab GPU! The techniques used here are general and can be applied to
    other bigger models like MPT-7B and MPT-30B.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: At [Deep Learning Analytics](https://deeplearninganalytics.org/), we have been
    building custom machine-learning models for the last 6 years. [Reach out to us](https://deeplearninganalytics.org/contact-us/)
    if you are interested in fine-tuning a LLM for your application.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: QLoRA
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: QLoRA, which stands for “Quantized Low-Rank Adaptation,” presents an approach
    that combines quantization and low-rank adaptation to achieve efficient fine-tuning
    of AI models. Both these terms are explained in more detail below.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: QLoRA reduces the memory required for fine-tuning LLM, without any drop in performance
    with respect to a standard 16-bit model fine-tuned model. This method enables
    a 7 billion parameter model to be fine-tuned on a 16GB GPU, a 33 billion parameter
    model to be fine-tuned on a single 24GB GPU and a 65 billion parameter model to
    be fine-tuned on a single 46GB GPU.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: This implies most companies can now have fine-tuned LLMs or on-prem models for
    a small cost.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'QLoRA is composed of two techniques:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Lora — Low-Rank Adaptation
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Quantization
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LoRA
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LoRA, which stands for Low-Rank Adapters (LoRA), are small sets of trainable
    parameters, injected into each layer of the Transformer architecture while fine-tuning.
    While original model weights are frozen and not updated, these small sets of injected
    weights are updated during fine-tuning. This greatly reduces the number of trainable
    parameters for downstream tasks. Gradients during stochastic gradient descent
    are passed through the frozen pre-trained model weights to the adapter. Thus,
    only these adapters, with a small memory footprint, are updated during the time
    of training.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Quantization
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we will discuss Quantization. To understand quantization, first, we need
    to know the different data types that are generally used to store model weights.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: FP32 — Full Precision weights which take 4 bytes of memory
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FP16/BF16 — Half precision weights which take 2 bytes of memory
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: INT8 —Data-type consists of an 8-bit representation that can store 2⁸ different
    values (between [0, 255] or [-128, 127] for signed integers)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FP8 and FP4, which stand for Floating-Point 8-bit and 4-bit precision, respectively.
    They are part of the mini floats family of floating point values.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using quantization, the model needs much smaller memory than the memory needed
    to store the original model. For example, an 8-bit quantized model would require
    only 1/4th of the model size, as compared to a model stored in a 32-bit datatype.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: But how is it possible to store the original 32-bit weight in 8-bit data types
    like INT8 or FP8?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Quantization means “rounding” off values, from one data type to another. It
    works with squeezing larger values into data types with less number of bits, but
    with a small loss of precision.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: In the case of QLoRA, model weights are stored in a 4-bit floating point data
    type. But, in order to minimize the overall loss in accuracy, all the matrix operations
    are performed in a 16-bit floating point. Once the calculation is done, weights
    are stored/quantized back, before they are stored again.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, QLORA uses one storage data type (typically 4-bit NormalFloat)
    and one computation data type (16-bit BrainFloat). The storage data type is dequantized
    to the computation data type for performing the forward and backward pass. The
    weights are decompressed only when they are needed, therefore the memory usage
    stays low during training and inference. More details about these could be found
    here. [4]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: What about the accuracy impacts of QLoRA fine tuning?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: QLoRA tuning is shown to match 16-bit fine-tuning methods in a wide range of
    experiments. In addition, the Guanaco models, which use QLoRA fine-tuning for
    LLaMA models on the [OpenAssistant dataset (OASST1)](https://huggingface.co/datasets/OpenAssistant/oasst1),
    are state-of-the-art chatbot systems and are close to ChatGPT on the Vicuna benchmark.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: HuggingFace Support for fine-tuning
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HuggingFace has released several libraries that can be used to fine-tune LLMs
    easily.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'These include:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PEFT Library](https://huggingface.co/blog/peft): HuggingFace has released
    a library on Parameter Efficient Fine Tuning (PEFT) which has support for LORA.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization Support — Many models can be loaded in 8-bit and 4-bit precision
    using bitsandbytes module. The basic way to load a model in 4bit is to pass the
    argument load_in_4bit=True when calling the from_pretrained method.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accelerate library — Accelerate library has many features to make it easy to
    reduce the memory requirements of models
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Supervised Fine-Tuning Trainer](https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTTrainer)
    — The SFT trainer is the trainer class for supervised fine-tuning of Large LLMs.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we combine all the techniques here to train the Falcon 7B model on a finance
    dataset.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Falcon 7B on Alpaca Finance Dataset**'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We were successfully able to fine-tune the recently released Falcon-7B model,
    on [**Alpaca-Finance**](https://huggingface.co/datasets/gbharti/finance-alpaca)
    dataset, on Google Colab. This dataset consisted of around 70K finance data points.
    This is an open source data available on HuggingFace dataset hub and can be loaded
    directly from the hub.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Data is finance questions with answers. A snapshot of the data is shown below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4846bbf9fefbf62bf8874eefc9a2359.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: Alpaca Finance Dataset. Image by Author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The code for this fine-tuning is shared [here](https://colab.research.google.com/drive/17Uyb3Q5_RwlpBqpk4XMRQmp7mfV9AdNE?authuser=2#scrollTo=mNnkgBq7Q3EU).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: First, we load a pre-trained Falcon-7B model from a hugging-face. We use AutoModelForCausalLM
    , generally used to load auto-regressive language models like GPT. Notice, we
    set the storage type to 4-bit and the computation type to FP-16.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc2a510ec25294ab5ec1f2455e8d7c43.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Code Snippet for loading Falcon 7B model. Image by Author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’re gonna create Adapters (Remember ! Extra layers added to Transformers
    modules, that will hold our fine-tunes weights). We will add them to linear layers
    as well as query value pairs of Transformers modules, as suggested, for best accuracy
    of the fine-tuned model. Below, one can notice the same in the target_modules
    parameter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bed89bcadf6207c61921b0076adca415.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: PEFT Configuration. Image by Author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will bring everything together, while initializing the SFTTrainer
    class. We also provide our data, appropriate token, formatting function and max_seq_length
    to it.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: The model takes about an hour to train. I used WandB for logging. More details
    are in the Colab Notebook [here](https://colab.research.google.com/drive/17Uyb3Q5_RwlpBqpk4XMRQmp7mfV9AdNE?authuser=2#scrollTo=pq5bgHt9vwvH).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: The training loss was quite low after 100 steps.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dad08719d055447de7c695fa84fd4fe9.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: Train Loss — Finance Alpaca Dataset. Image by Author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Inference with Alpaca Finance Dataset
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Colab Notebook also has the inference code. I found the model did well in
    responding to a question. On my question regarding income needed to retire, it
    had a long generation. It covered some rules of thumb regarding income from 401K
    and other pension plans. It emphasized the need to know the retirement income
    to come up with an answer and finally suggested I talk to a financial planner!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The response is not as clean as chatGPT would give but good to see that a 7B
    model can be trained and run quite easily.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Another question I tried is around portfolio diversification. Again the quality
    of response from a small model surprised me!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Conclusion
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It has become easier to fine-tune LLMs on custom datasets which can give people
    access to their own “private GPT” model. The custom models can be locally hosted
    on a commercial GPU and have a ChatGPT like interface.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[Deep Learning Analytics](http://deeplearninganalytics.org/) is a trusted provider
    of custom machine learning models tailored to diverse use cases. With a global
    client base, we have collaborated with organizations to develop bespoke solutions
    aligned with their unique requirements. If you’re seeking to leverage the power
    of Language Models (LLMs) for your application, we invite you to connect with
    us. Reach out to our team at [info@deeplearninganalytics.org](mailto:info@deeplearninganalytics.org)
    to explore the possibilities. We look forward to discussing your project further.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [https://huggingface.co/blog/falcon](https://huggingface.co/blog/falcon)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://huggingface.co/blog/hf-bitsandbytes-integration](https://huggingface.co/blog/hf-bitsandbytes-integration)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [https://huggingface.co/blog/peft](https://huggingface.co/blog/peft)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [https://huggingface.co/blog/4bit-transformers-bitsandbytes#qlora-paper-a-new-way-of-democratizing-quantized-large-transformer-models](https://huggingface.co/blog/4bit-transformers-bitsandbytes#qlora-paper-a-new-way-of-democratizing-quantized-large-transformer-models)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://huggingface.co/blog/4bit-transformers-bitsandbytes#qlora-paper-a-new-way-of-democratizing-quantized-large-transformer-models](https://huggingface.co/blog/4bit-transformers-bitsandbytes#qlora-paper-a-new-way-of-democratizing-quantized-large-transformer-models)'
- en: '[5] [https://www.promptengineering.org/exploring-qloras-potential-for-accessibility-and-innovation/](https://www.promptengineering.org/exploring-qloras-potential-for-accessibility-and-innovation/)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://www.promptengineering.org/exploring-qloras-potential-for-accessibility-and-innovation/](https://www.promptengineering.org/exploring-qloras-potential-for-accessibility-and-innovation/)'
- en: '[6] [https://blog.gopenai.com/paper-review-qlora-efficient-finetuning-of-quantized-llms-a3c857cd0cca](https://blog.gopenai.com/paper-review-qlora-efficient-finetuning-of-quantized-llms-a3c857cd0cca)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://blog.gopenai.com/paper-review-qlora-efficient-finetuning-of-quantized-llms-a3c857cd0cca](https://blog.gopenai.com/paper-review-qlora-efficient-finetuning-of-quantized-llms-a3c857cd0cca)'
- en: '[7] QLoRA paper -: [https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] QLoRA 论文 -: [https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)'
- en: '[8] LoRA paper -: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] LoRA 论文 -: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)'
- en: '[9] [https://huggingface.co/docs/trl/main/en/sft_trainer](https://huggingface.co/docs/trl/main/en/sft_trainer)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [https://huggingface.co/docs/trl/main/en/sft_trainer](https://huggingface.co/docs/trl/main/en/sft_trainer)'
