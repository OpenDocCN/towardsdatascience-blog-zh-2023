- en: 'Unlocking MLOps using Airflow: A Comprehensive Guide to ML System Orchestration'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Airflow 解锁 MLOps：ML 系统编排的全面指南
- en: 原文：[https://towardsdatascience.com/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff](https://towardsdatascience.com/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff](https://towardsdatascience.com/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)
- en: '[THE FULL STACK 7-STEPS MLOPS FRAMEWORK](https://towardsdatascience.com/tagged/full-stack-mlops)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[完整的 7 步 MLOps 框架](https://towardsdatascience.com/tagged/full-stack-mlops)'
- en: 'Lesson 4: Private PyPi Server. Orchestrate Everything with Airflow.'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 4 节：私人 PyPi 服务器。用 Airflow 编排一切。
- en: '[](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)
    ·17 min read·May 23, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)
    ·17 分钟阅读·2023年5月23日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3c440d7e58f12e1041fcde014eea4fda.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c440d7e58f12e1041fcde014eea4fda.png)'
- en: Photo by [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This tutorial represents **lesson 4 out of a 7-lesson course** that will walk
    you step-by-step through how to **design, implement, and deploy an ML system**
    using **MLOps good practices**. During the course, you will build a production-ready
    model to forecast energy consumption levels for the next 24 hours across multiple
    consumer types from Denmark.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程代表**7 节课程中的第 4 节课**，将一步一步指导你如何**设计、实现和部署一个 ML 系统**，并运用**MLOps 好实践**。在课程中，你将构建一个准备投入生产的模型，以预测丹麦多个消费者类型在接下来的
    24 小时内的能源消耗水平。
- en: '*By the end of this course, you will understand all the fundamentals of designing,
    coding and deploying an ML system using a batch-serving architecture.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本课程结束时，你将理解如何使用批量服务架构设计、编码和部署 ML 系统的所有基础知识。*'
- en: This course *targets mid/advanced machine learning engineers* who want to level
    up their skills by building their own end-to-end projects.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本课程*针对中级/高级机器学习工程师*，希望通过构建自己的端到端项目来提升技能。
- en: '*Nowadays, certificates are everywhere. Building advanced end-to-end projects
    that you can later show off is the best way to get recognition as a professional
    engineer.*'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如今，证书到处都是。构建可以展示的高级端到端项目是获得专业工程师认可的最佳方式。*'
- en: 'Table of Contents:'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录：
- en: Course Introduction
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程介绍
- en: Course Lessons
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程内容
- en: Data Source
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源
- en: 'Lesson 4: Private PyPi Server. Orchestrate Everything with Airflow.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 4 节：私人 PyPi 服务器。用 Airflow 编排一切。
- en: 'Lesson 4: Code'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 4 节：代码
- en: Conclusion
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结论
- en: References
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考资料
- en: Course Introduction
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 课程介绍
- en: '***At the end of this 7 lessons course, you will know how to:***'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '***在这个 7 节课程结束时，你将知道如何：***'
- en: design a batch-serving architecture
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计批量服务架构
- en: use Hopsworks as a feature store
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Hopsworks 作为特征存储
- en: design a feature engineering pipeline that reads data from an API
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计一个从 API 读取数据的特征工程管道
- en: build a training pipeline with hyper-parameter tunning
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个包含超参数调整的训练管道
- en: use W&B as an ML Platform to track your experiments, models, and metadata
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 W&B 作为 ML 平台跟踪你的实验、模型和元数据
- en: implement a batch prediction pipeline
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个批量预测管道
- en: use Poetry to build your own Python packages
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Poetry 构建你自己的 Python 包
- en: deploy your own private PyPi server
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署你自己的私人 PyPi 服务器
- en: orchestrate everything with Airflow
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Airflow 协调一切
- en: use the predictions to code a web app using FastAPI and Streamlit
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预测来编写一个使用 FastAPI 和 Streamlit 的 web 应用程序
- en: use Docker to containerize your code
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker 对代码进行容器化
- en: use Great Expectations to ensure data validation and integrity
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Great Expectations 确保数据的验证和完整性
- en: monitor the performance of the predictions over time
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控预测性能的变化情况
- en: deploy everything to GCP
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有内容部署到 GCP
- en: build a CI/CD pipeline using GitHub Actions
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GitHub Actions 构建 CI/CD 管道
- en: If that sounds like a lot, don't worry. After you cover this course, you will
    understand everything I said before. Most importantly, you will know WHY I used
    all these tools and how they work together as a system.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这听起来很多，不要担心。在你完成这门课程后，你将理解我之前说的所有内容。最重要的是，你将知道我为什么使用这些工具，以及它们如何作为一个系统协同工作。
- en: '**If you want to get the most out of this course,** [**I suggest you access
    the GitHub repository**](https://github.com/iusztinpaul/energy-forecasting) **containing
    all the lessons'' code. This course is designed to read and replicate the code
    along the articles quickly.**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你想从这门课程中获得最大的收益，** [**我建议你访问包含所有课程代码的 GitHub 仓库**](https://github.com/iusztinpaul/energy-forecasting)
    **。这门课程旨在让你快速阅读和复制文章中的代码。**'
- en: By the end of the course, you will know how to implement the diagram below.
    Don't worry if something doesn't make sense to you. I will explain everything
    in detail.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 到课程结束时，你将知道如何实现下图所示的内容。如果有什么不明白的地方，请不要担心。我会详细解释一切。
- en: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
- en: Diagram of the architecture you will build during the course [Image by the Author].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 课程中你将构建的架构图 [作者提供的图片]。
- en: By the **end of Lesson 4**, you will know how to host your PiPy repository and
    orchestrate the three pipelines using Airflow. You will learn how to schedule
    the pipelines to create hourly forecasts.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 到**第 4 节课结束时**，你将知道如何托管你的 PyPi 仓库，并使用 Airflow 协调三个管道。你将学习如何调度管道以创建每小时的预测。
- en: 'Course Lessons:'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 课程内容：
- en: '[Batch Serving. Feature Stores. Feature Engineering Pipelines.](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[批量服务。特征存储。特征工程管道。](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
- en: '[Training Pipelines. ML Platforms. Hyperparameter Tuning.](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[训练管道。ML 平台。超参数调整。](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
- en: '[Batch Prediction Pipeline. Package Python Modules with Poetry.](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[批量预测管道。使用 Poetry 打包 Python 模块。](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
- en: '**Private PyPi Server. Orchestrate Everything with Airflow.**'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**私有 PyPi 服务器。使用 Airflow 协调一切。**'
- en: '[Data Validation for Quality and Integrity using GE. Model Performance Continuous
    Monitoring.](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 GE 进行数据验证以确保质量和完整性。模型性能持续监控。](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
- en: '[Consume and Visualize your Model’s Predictions using FastAPI and Streamlit.
    Dockerize Everything.](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 FastAPI 和 Streamlit 消费和可视化你的模型预测。对一切进行 Docker 化。](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
- en: '[Deploy All the ML Components to GCP. Build a CI/CD Pipeline Using Github Actions.](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[将所有 ML 组件部署到 GCP。使用 Github Actions 构建 CI/CD 管道。](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
- en: '[[Bonus] Behind the Scenes of an ‘Imperfect’ ML Project — Lessons and Insights](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[额外内容] ‘不完美’ ML 项目的幕后 — 经验教训和见解](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
- en: 'If you want to grasp this lesson fully, we recommend you check out [Lesson
    1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f),
    [Lesson 2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee),
    and [Lesson 3](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489),
    which explain in detail the implementation of the pipelines that you will orchestrate
    in this article:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想全面掌握这节课，我们建议你查看[课程 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)，[课程
    2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)
    和[课程 3](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)，这些课程详细解释了你将在本文中协调的管道实现：
- en: '[Feature Engineering Pipeline](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[特征工程管道](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
- en: '[Training Pipeline](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[训练管道](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
- en: '[Batch Prediction Pipeline](/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[批量预测管道](/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
- en: Data Source
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据源
- en: We used a free & open API that provides hourly energy consumption values for
    all the energy consumer types within Denmark [1].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个免费且开放的 API，该 API 提供了丹麦所有能源消费类型的每小时能源消耗值[1]。
- en: They provide an intuitive interface where you can easily query and visualize
    the data. [You can access the data here](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 它们提供了一个直观的界面，你可以轻松查询和可视化数据。[你可以在这里访问数据](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1]。
- en: 'The data has 4 main attributes:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据具有 4 个主要属性：
- en: '**Hour UTC:** the UTC datetime when the data point was observed.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小时 UTC：** 数据点观察时的 UTC 日期时间。'
- en: '**Price Area:** Denmark is divided into two price areas: DK1 and DK2 — divided
    by the Great Belt. DK1 is west of the Great Belt, and DK2 is east of the Great
    Belt.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价格区域：** 丹麦被划分为两个价格区域：DK1 和 DK2——由大贝尔特海峡划分。DK1 位于大贝尔特海峡西侧，DK2 位于大贝尔特海峡东侧。'
- en: '**Consumer Type:** The consumer type is the Industry Code DE35, owned and maintained
    by Danish Energy.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费类型：** 消费类型是由丹麦能源公司拥有和维护的行业代码 DE35。'
- en: '**Total Consumption:** Total electricity consumption in kWh'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总消耗：** 总电力消耗（单位：千瓦时）'
- en: '**Note:** The observations have a lag of 15 days! But for our demo use case,
    that is not a problem, as we can simulate the same steps as it would in real-time.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 观察数据有 15 天的滞后！但对于我们的演示用例来说，这不是问题，因为我们可以模拟与实时相同的步骤。'
- en: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
- en: A screenshot from our web app showing how we forecasted the energy consumption
    for area = 1 and consumer_type = 212 [Image by the Author].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络应用程序的屏幕截图，展示了我们如何预测区域 = 1 和消费类型 = 212 的能源消耗 [作者提供的图片]。
- en: 'The data points have an hourly resolution. For example: "2023–04–15 21:00Z",
    "2023–04–15 20:00Z", "2023–04–15 19:00Z", etc.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点具有每小时的分辨率。例如：“2023–04–15 21:00Z”，“2023–04–15 20:00Z”，“2023–04–15 19:00Z”等。
- en: We will model the data as multiple time series. Each unique **price area** and
    **consumer type tuple represents its** unique time series.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据建模为多个时间序列。每个唯一的**价格区域**和**消费类型**组合表示一个唯一的时间序列。
- en: Thus, we will build a model that independently forecasts the energy consumption
    for the next 24 hours for every time series.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将建立一个独立预测未来 24 小时每个时间序列的能源消耗的模型。
- en: '*Check out the video below to better understand what the data looks like* 👇'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*查看下面的视频，以更好地理解数据的样子* 👇'
- en: Course & data source overview [Video by the Author].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 课程与数据源概述 [作者提供的视频]。
- en: 'Lesson 4: **Private PyPi Server. Orchestrate Everything with Airflow.**'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 课程 4：**私人 PyPi 服务器。用 Airflow 协调一切。**
- en: The goal of Lesson 4
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 4 节课的目标
- en: This lesson will teach you how to use Airflow to orchestrate the three pipelines
    you have implemented so far.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这节课将教你如何使用 Airflow 来协调你迄今为止实现的三个管道。
- en: Also, to run the code inside Airflow, you will learn to host your PiPy repository
    and deploy the pipelines as 3 different Python modules. Later you will install
    your modules inside Airflow directly from your PiPy repository.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，要运行Airflow中的代码，你将学习如何托管你的PiPy仓库，并将管道部署为3个不同的Python模块。之后，你将直接从你的PiPy仓库中安装这些模块到Airflow中。
- en: '![](../Images/0867f47c56b09ac9cb2dddc9a885283f.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0867f47c56b09ac9cb2dddc9a885283f.png)'
- en: Diagram of the final architecture with the Lesson 4 components highlighted in
    blue [Image by the Author].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 第4课组件用蓝色高亮的最终架构图 [作者提供的图片]。
- en: By orchestrating everything using Airflow, you will automate your entire process.
    Instead of running manually 10 different scripts, you will hit once a "Run" button
    to run the whole code.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用Airflow编排所有内容，你将自动化整个过程。你不再需要手动运行10个不同的脚本，而只需点击一次“运行”按钮即可运行整个代码。
- en: Also, connecting all the steps together in a programmatic way is less prone
    to bugs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，以编程方式将所有步骤连接起来的程序更不容易出现错误。
- en: '**Why?**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么？**'
- en: Because every script needs its configurations. For example, the batch prediction
    pipeline needs the feature view version (data version) and the model version as
    input.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每个脚本都需要自己的配置。例如，批量预测管道需要特征视图版本（数据版本）和模型版本作为输入。
- en: This information is generated as metadata from previous scripts. When you run
    everything manually, you can easily copy the wrong version. But when you wrap
    up everything inside a single DAG, you have to build it once, and afterward, it
    will work all the time.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息是从之前的脚本生成的元数据。当你手动运行一切时，容易复制错误的版本。但当你将所有内容封装在一个DAG中时，你只需构建一次，之后它将始终正常工作。
- en: 'Also, by using Airflow, you can:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过使用Airflow，你可以：
- en: schedule the pipeline to run periodically (you will run it hourly);
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期调度管道运行（你将每小时运行一次）；
- en: configure your entire process using Airflow Variables;
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Airflow变量配置整个过程；
- en: monitor the logs of every task.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控每个任务的日志。
- en: Here is an overview of what you will build in Airflow 👇
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是你将在Airflow中构建的概览 👇
- en: Theoretical Concepts & Tools
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理论概念与工具
- en: '**Airflow:** Airflow is oneof the most popular orchestration tools out there.
    The project was developed at Airbnb but is now open source under the Apache License.
    That means that you can modify and host it yourself for free. Airflow lets you
    build, schedule and monitor DAGs.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**Airflow：** Airflow是最受欢迎的编排工具之一。这个项目最初在Airbnb开发，但现在在Apache许可证下开源。这意味着你可以免费修改和托管它。Airflow允许你构建、调度和监控DAG。'
- en: '**DAG (Directed Acyclic Graph):** A DAG is a graph with no loops, meaning the
    logic flow can go only one way.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**DAG（有向无环图）：** DAG是一种没有循环的图，这意味着逻辑流只能朝一个方向进行。'
- en: '**PyPi Registry:** A PiPy registry is a server where you can host various Python
    modules. When you run "**pip install <your_package>**", pip knows how to look
    at the official PyPi repository for your package and install it. Hosting your
    own PyPi registry will behave precisely the same, but you must configure pip to
    know how to access it. Only people with access to your PyPi server can install
    packages from it.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyPi注册表：** PiPy注册表是一个可以托管各种Python模块的服务器。当你运行“**pip install <your_package>**”时，pip知道如何查看官方PyPi仓库中的你的包并安装它。托管自己的PyPi注册表的行为完全相同，但你必须配置pip以知道如何访问它。只有访问你PyPi服务器的人才能从中安装包。'
- en: 'Lesson 4: Code'
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4课：代码
- en: '[You can access the GitHub repository here.](https://github.com/iusztinpaul/energy-forecasting)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[你可以在这里访问GitHub仓库。](https://github.com/iusztinpaul/energy-forecasting)'
- en: '**Note:** All the installation instructions are in the READMEs of the repository.
    Here you will jump straight to the code.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 所有的安装说明都在仓库的README文件中。这里你将直接跳转到代码部分。'
- en: '*All the code within Lesson 4 is located under the* [***airflow***](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)
    *folder.*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*第4课中的所有代码都位于* [***airflow***](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)
    *文件夹下。*'
- en: 'The files under the [**airflow**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)
    folderare structured as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[**airflow**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)
    文件夹下的文件结构如下：'
- en: '![](../Images/626fcaf985c055b03a8e886cca969cbc.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/626fcaf985c055b03a8e886cca969cbc.png)'
- en: A screenshot that shows the structure of the airflow folder [Image by the Author].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 显示airflow文件夹结构的截图 [作者提供的图片]。
- en: All the code is located under the [**dags**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)
    directory**.** Every DAG will have its own Python file.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码都位于[**dags**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)
    目录下**。** 每个DAG都有其自己的Python文件。
- en: The Docker files will help you quickly host Airflow and the PiPy repository.
    I will explain them in detail later.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Docker文件将帮助你快速托管Airflow和PiPy仓库。我会在后面详细解释。
- en: Directly storing credentials in your git repository is a huge security risk.
    That is why you will inject sensitive information using a **.env** file.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 直接将凭证存储在你的git仓库中是一个巨大的安全风险。这就是为什么你将使用**.env**文件来注入敏感信息。
- en: The **.env.default** is an example of all the variables you must configure.
    It is also helpful to store default values for attributes that are not sensitive
    (e.g., project name).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**.env.default**是你必须配置的所有变量的示例。它也有助于存储不敏感的属性的默认值（例如，项目名称）。'
- en: '![](../Images/5e1d209fa47b2b18eea853fa1a9848b8.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e1d209fa47b2b18eea853fa1a9848b8.png)'
- en: A screenshot of the .env.default file [Image by the Author].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: .env.default文件的截图 [作者提供的图片]。
- en: Prepare Credentials
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备凭证
- en: As Lesson 4 talks about orchestrating the code from all the other lessons, if
    you want to reproduce the code, you need to check how to set up the 3 pipelines
    from [Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f),
    [Lesson 2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee),
    and [Lesson 3](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于第4节课讨论了如何协调其他所有课程中的代码，如果你想重现代码，你需要检查如何设置[第1节](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)、[第2节](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)和[第3节](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)中的3个管道。
- en: These three lessons will show you how to set up all the necessary tools and
    services. Also, it will show you how to create and complete the required .env
    file that contains all the credentials.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这三节课将展示如何设置所有必要的工具和服务。还会展示如何创建并完成包含所有凭证的所需.env文件。
- en: '![](../Images/5e1d209fa47b2b18eea853fa1a9848b8.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e1d209fa47b2b18eea853fa1a9848b8.png)'
- en: A screenshot of the .env.default file [Image by the Author].
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: .env.default文件的截图 [作者提供的图片]。
- en: '*The only thing to be careful of is* 👇'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*唯一需要注意的是* 👇'
- en: This time you have to place the **.env** that contains your credentials under
    the [**airflow/dags**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)folder**.**
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这次你需要将包含凭证的**.env**文件放置在[**airflow/dags**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)文件夹下**。**
- en: We have set up a default value of **/opt/airflow/dags** for the **ML_PIPELINE_ROOT_DIR**
    environment variable inside the docker-compose.yaml file. Thus, when running the
    pipelines inside Airflow, it will know to load the **.env** file from **/opt/airflow/dags**
    by default.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在docker-compose.yaml文件中为**ML_PIPELINE_ROOT_DIR**环境变量设置了默认值**/opt/airflow/dags**。因此，在Airflow内部运行管道时，它将默认从**/opt/airflow/dags**加载**.env**文件。
- en: Also, note that there is another **.env** file under the /[**airflow**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)folder.
    This one doesn't contain your custom credentials, but Airflow needs some custom
    configurations. This is what it looks like 👇
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，请注意在/ [**airflow**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)文件夹下还有另一个**.env**文件。这个文件不包含你的自定义凭证，但Airflow需要一些自定义配置。它的样子如下
    👇
- en: '![](../Images/262b1e1f5c8889636694e184cec858b4.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/262b1e1f5c8889636694e184cec858b4.png)'
- en: A screenshot of the .env file from the /airflow folder [Image by the Author].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: /airflow文件夹中的.env文件截图 [作者提供的图片]。
- en: I explained how to complete this **.env** file in the [README.md](https://github.com/iusztinpaul/energy-forecasting/tree/main#usage)
    of the repository. But as a side note, **AIRFLOW_UID** represents your computer's
    USER ID, and you know what **ML_PIPELINE_ROOT_DIR** is.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我在仓库的[README.md](https://github.com/iusztinpaul/energy-forecasting/tree/main#usage)中解释了如何完成这个**.env**文件。但作为附注，**AIRFLOW_UID**代表你计算机的用户ID，而你知道**ML_PIPELINE_ROOT_DIR**是什么。
- en: I just wanted to show you that you can override the default value for **ML_PIPELINE_ROOT_DIR**
    here. Note that this path will be used inside the Docker container, hence the
    path that starts with **/opt/**.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是想向你展示你可以在这里覆盖**ML_PIPELINE_ROOT_DIR**的默认值。请注意，这个路径将在Docker容器内使用，因此路径以**/opt/**开头。
- en: '[PRE0]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Setup Private PyPi Server
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置私人PyPi服务器
- en: You can easily host a PiPy server using [this repository](https://github.com/pypiserver/pypiserver).
    But let me explain how we did it in our setup.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 [这个仓库](https://github.com/pypiserver/pypiserver) 容易地托管一个 PiPy 服务器。不过让我解释一下我们在设置中是如何做的。
- en: The first step is to create a set of credentials that you will need to connect
    to the PyPi server.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建一组凭证，这些凭证是你连接到 PyPi 服务器所需的。
- en: '[PRE1]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The PyPi repository will know to load the credentials from the **~/.htpasswd/htpasswd.txt**
    file.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: PyPi 仓库将知道从 **~/.htpasswd/htpasswd.txt** 文件中加载凭证。
- en: 'Now, you will add the new private PyPi repository to Poetry. To configure Poetry,
    you need to specify the URL of the server, the name of the server and the username
    & password to use to authenticate (which are the ones you configured one step
    before):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你将把新的私有 PyPi 仓库添加到 Poetry 中。要配置 Poetry，你需要指定服务器的 URL、服务器名称以及用于认证的用户名和密码（这些是你之前配置的）：
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In our example:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中：
- en: '**name of the server:** my-pypy'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器名称：** my-pypy'
- en: '**URL:** [http://localhost](http://localhost)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**URL：** [http://localhost](http://localhost)'
- en: '**username:** energy-forecasting'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户名：** energy-forecasting'
- en: '**password:** <password>'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密码：** <password>'
- en: 'Check if your credentials are set correctly in your Poetry **auth.toml** file:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 检查你的 Poetry **auth.toml** 文件中的凭证设置是否正确：
- en: '[PRE3]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So, you finished preparing the username and password that will be loaded by
    your PyPi repository to authenticate. Also, you configured Poetry to be aware
    of your PyPi server.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经准备好了用户名和密码，这些将被你的 PyPi 仓库用于认证。同时，你也配置了 Poetry 以识别你的 PyPi 服务器。
- en: Now, let's see how to run the PyPi server.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何运行 PyPi 服务器。
- en: The [pyserver code](https://github.com/pypiserver/pypiserver) you will be using
    is already dockerized.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用的 [pyserver 代码](https://github.com/pypiserver/pypiserver) 已经进行了 Docker 化。
- en: To simplify things, we added the PyPi server as an additional service to the
    docker-compose.yaml file that runs the Airflow application.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们将 PyPi 服务器作为额外的服务添加到运行 Airflow 应用程序的 docker-compose.yaml 文件中。
- en: To better understand the docker-compose.yaml file check [Airflow's official
    documentation](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)
    [2] and [our README.md](https://github.com/iusztinpaul/energy-forecasting/tree/main#the-pipeline).
    But be careful to use the docker-compose.yaml file from our repository as we modified
    the original one, as you will see below.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 docker-compose.yaml 文件，请查看 [Airflow 官方文档](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)
    [2] 和 [我们的 README.md](https://github.com/iusztinpaul/energy-forecasting/tree/main#the-pipeline)。但请注意使用我们仓库中的
    docker-compose.yaml 文件，因为我们修改了原始文件，正如下文所示。
- en: 'Scroll at the bottom of the [**airflow/docker-compose.yaml**](https://github.com/iusztinpaul/energy-forecasting/blob/main/airflow/docker-compose.yaml)
    file, and you will see:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动到 [**airflow/docker-compose.yaml**](https://github.com/iusztinpaul/energy-forecasting/blob/main/airflow/docker-compose.yaml)
    文件的底部，你会看到：
- en: '[PRE4]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This code uses the PyPi server''slatest image, exposes the server under the
    **80 port**, loads the **~/.htpasswd** folder that contains your credentials as
    a volume and runs the server with the following command:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用了 PyPi 服务器的最新镜像，将服务器暴露在 **80 端口**，将包含你凭证的 **~/.htpasswd** 文件夹作为卷加载，并用以下命令运行服务器：
- en: '[PRE5]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '"-P .htpasswd/htpasswd.txt" explicitly tells the server what credentials to
    use.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"-P .htpasswd/htpasswd.txt" 明确告诉服务器使用哪些凭证。'
- en: '"— overwrite" states that if a new module with the same version is deployed,
    it will overwrite the last one.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"— overwrite" 表示如果部署了相同版本的新模块，它将覆盖上一个版本。'
- en: Thats it! When you run the Airflow application, you automatically start the
    PyPi server.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！当你运行 Airflow 应用程序时，你会自动启动 PyPi 服务器。
- en: '**Note:** In a production environment, you will likely host the PyPi server
    on a different server than Airflow. The steps are identical except for adding
    everything in a single docker-compose.yaml file. In this tutorial, we wanted to
    make everything easy to run.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 在生产环境中，你可能会将 PyPi 服务器托管在与 Airflow 不同的服务器上。步骤是相同的，只是将所有内容添加到单个 docker-compose.yaml
    文件中。在本教程中，我们希望一切运行起来更加简单。'
- en: Customize Airflow Docker File
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义 Airflow Docker 文件
- en: Because you have to run all the code in Python 3.9, you have to inherit the
    default **apache/airflow:2.5.2** Airflow Docker image and add some extra dependencies.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你必须在 Python 3.9 中运行所有代码，因此你需要继承默认的 **apache/airflow:2.5.2** Airflow Docker
    镜像，并添加一些额外的依赖项。
- en: 'This is what is going on in the Docker file below:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Docker 文件中的内容：
- en: inherit **apache/airflow:2.5.2**
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 继承 **apache/airflow:2.5.2**
- en: switch to the root user to install system dependencies
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 切换到 root 用户以安装系统依赖项
- en: install Python 3.9 dependencies needed to install packages from the private
    PyPi server
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装从私有PyPi服务器安装包所需的Python 3.9依赖项
- en: switch back to the default user
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 切换回默认用户
- en: 'Because we switched:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们切换了：
- en: '[PRE6]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 到：
- en: '[PRE7]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Docker will know to use your custom image instead of **apache/airflow:2.5.2
    when running docker-compose**.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Docker会知道在运行docker-compose时使用你的自定义镜像，而不是**apache/airflow:2.5.2**。
- en: Run Airflow
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行Airflow
- en: 'Now that you understand how to prepare the credentials and how the Docker files
    work, go to the [./airflow](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)
    directory and run:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你了解了如何准备凭据以及Docker文件如何工作，前往[./airflow](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)目录并运行：
- en: '[PRE8]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Check out the ***Usage*** *section* of the GitHub repository for more info.](https://github.com/iusztinpaul/energy-forecasting/tree/main#-usage-)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看***使用*** *部分* 获取更多信息。](https://github.com/iusztinpaul/energy-forecasting/tree/main#-usage-)'
- en: 'After you finish your Airflow setup, you can access Airflow at **127.0.0.1:8080**
    with the default credentials:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 完成Airflow设置后，你可以使用默认凭据在**127.0.0.1:8080**访问Airflow：
- en: 'username: airflow'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '用户名: airflow'
- en: 'password: airflow'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '密码: airflow'
- en: '![](../Images/76a9df4ab40a4c9b99fb00b950b8b489.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76a9df4ab40a4c9b99fb00b950b8b489.png)'
- en: Screenshot of the Airflow login page [Image by the Author].
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow登录页面的截图 [作者提供的图片]。
- en: Deploy Modules to Private PyPi Server
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模块部署到私有PyPi服务器
- en: 'Remember that you added to Poetry your new PyPi server using the following
    commands:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 记住你使用以下命令将新的PyPi服务器添加到Poetry中：
- en: '[PRE9]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, using **my-pypi** as an identifier, you can quickly push new packages to
    your PyPi repository.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用**my-pypi**作为标识符，你可以快速将新包推送到你的PyPi仓库。
- en: 'Using the [deploy/ml-pipeline.sh](https://github.com/iusztinpaul/energy-forecasting/blob/main/deploy/ml-pipeline.sh)
    shell script, you can build & deploy all the 3 pipelines using solely Poetry:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[deploy/ml-pipeline.sh](https://github.com/iusztinpaul/energy-forecasting/blob/main/deploy/ml-pipeline.sh)脚本，你可以仅使用Poetry构建和部署所有3个管道：
- en: '[PRE10]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As you can see, we iteratively go to the folders of the 3 pipelines and run:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们迭代地进入3个管道的文件夹并运行：
- en: '[PRE11]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Poetry uses these two commands to look for the **pyproject.toml** and **poetry.lock**
    files inside the folders and knows how to build the package.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Poetry使用这两个命令在文件夹中查找**pyproject.toml**和**poetry.lock**文件，并知道如何构建包。
- en: Afterward, based on the generated **wheel** file, running **"poetry publish
    -r my-pypi",** you push itto your **my-pipy** repository.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据生成的**wheel**文件，运行**"poetry publish -r my-pypi"**，你将其推送到你的**my-pipy**仓库。
- en: Remember that you labeled your PyPi server as **my-pipy**.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 记住你将你的PyPi服务器标记为**my-pipy**。
- en: You are done. You have your own PyPi repository.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了。你拥有了自己的PyPi仓库。
- en: In future sections, I will show you how to install packages from your private
    PyPi repository.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我将向你展示如何从你的私有PyPi仓库中安装包。
- en: '**Note:** You used Poetry just to build & deploy the modules. Airflow will
    use pip to install them from your PiPy repository.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 你只使用Poetry来构建和部署模块。Airflow将使用pip从你的PyPi仓库中安装这些模块。'
- en: Define the DAG Object
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义DAG对象
- en: Your dag is defined under the [**airflow/dags/ml_pipeline_dag.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/airflow/dags/ml_pipeline_dag.py)
    file.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你的dag定义在[**airflow/dags/ml_pipeline_dag.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/airflow/dags/ml_pipeline_dag.py)文件中。
- en: Using the ***API of Airflow 2.0***, you can define a DAG using the **dag()**
    Python decorator.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用***Airflow 2.0的API***，你可以使用**dag()** Python 装饰器定义一个DAG。
- en: Your dag will be defined inside the **ml_pipeline()** function, which is called
    at the end of the file. Also, Airflow knows to load all the DAGs defined under
    the [airflow/dags](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)
    directory.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你的dag将在**ml_pipeline()**函数中定义，该函数在文件末尾调用。此外，Airflow知道加载[airflow/dags](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)目录下定义的所有DAG。
- en: 'The DAG has the following properties:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: DAG具有以下属性：
- en: '**dag_id**: the ID of the DAG'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dag_id**: DAG的ID'
- en: '**schedule:** itdefines how often the DAG runs'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**schedule:** 它定义了DAG运行的频率'
- en: '**start_date:** when should the DAG start running based on the given schedule'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**start_date:** 根据给定的时间表，DAG应该何时开始运行'
- en: '**catchup:** automatically backfill between [start_date, present]'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**catchup:** 自动填补[start_date, 现在]之间的时间'
- en: '**tags:** tags 😄'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tags:** 标签 😄'
- en: '**max_active_runs:** how many instances of this DAG can run in parallel'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_active_runs:** 此DAG可以并行运行的实例数'
- en: Define the Tasks
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义任务
- en: The code below might look long, but you can easily read it once you understand
    the main ideas.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码可能看起来很长，但一旦你理解了主要思路，它很容易阅读。
- en: Inside a DAG, you have defined multiple tasks. A task is a single logic unit/step
    that performs a specific operation.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在DAG中，你定义了多个任务。一个任务是一个单独的逻辑单元/步骤，执行特定的操作。
- en: 'The tasks are defined similarly to the DAG: a function + a decorator. Every
    task has its function and decorator.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 任务的定义类似于DAG：一个函数+一个装饰器。每个任务都有其函数和装饰器。
- en: '***Note:*** *This a simple reminder that we used the API for Airflow 2.0, not
    1.0.*'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意：*** *这是一个简单的提醒，我们使用的是Airflow 2.0的API，而不是1.0的。*'
- en: In our case, a task will represent a main pipeline script. For example, the
    feature engineering pipeline will run inside a single task.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，一个任务将代表一个主管道脚本。例如，特征工程管道将在一个单独的任务中运行。
- en: You will use a DAG to glue all your scripts under a single "program", where
    every script has a 1:1 representation with a task.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用DAG将所有脚本粘合在一个“程序”下，每个脚本与一个任务有1:1的对应关系。
- en: '![](../Images/832932dc0905d7afa066c5026aa25207.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/832932dc0905d7afa066c5026aa25207.png)'
- en: Visual representation of the "ml_pipeline" (see the Youtube video for a better
    view) [Image by the Author].
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '"ml_pipeline"的视觉表示（请参见YouTube视频以获取更好的视图） [作者提供的图像]。'
- en: As you can see inside every task, you just import and call the function from
    its own module… and maybe add some additional logs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在每个任务中看到的，你只需从其模块中导入并调用函数……并可能添加一些额外的日志。
- en: The key step in defining a task is in the arguments of the **task.virtualenv()**
    Python decorator.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 定义任务的关键步骤在于**task.virtualenv()** Python装饰器的参数中。
- en: For every task, this specific decorator will create a different Python virtual
    environment inside which it will install all the given requirements.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个任务，这个特定的装饰器将创建一个不同的Python虚拟环境，在其中安装所有给定的需求。
- en: '**Note:** **172.17.0.1** is the IP address of your private PyPi repository.
    Remember that you host your PyPi repository using docker-compose under the same
    network as Airflow. **172.17.0.1** is the bridge IP address accessible by every
    Docker container inside the **default Docker** network. Thus, the Airflow container
    can access the PyPi server container using the bridge IP address.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** **172.17.0.1** 是你私有PyPi仓库的IP地址。记住，你通过docker-compose在与Airflow相同的网络下托管你的PyPi仓库。**172.17.0.1**
    是每个Docker容器在**default Docker**网络内可以访问的桥接IP地址。因此，Airflow容器可以通过桥接IP地址访问PyPi服务器容器。'
- en: 'As you can see in the **requirements** argument, we defined the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在**requirements**参数中所示，我们定义了以下内容：
- en: '"**— trusted-host 172.17.0.1**": As the PyPi server is not secured with HTTPS,
    you must explicitly say that you trust this source.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"**— trusted-host 172.17.0.1**": 由于PyPi服务器没有用HTTPS保护，你必须明确表示你信任这个来源。'
- en: '"**— extra-index-url http://172.17.0.1**": Tell Pip to also look at this PyPi
    repository when searching for a Python package. Note that Pip will still look
    in the official PyPi repository in addition to yours.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"**— extra-index-url http://172.17.0.1**": 告诉Pip在搜索Python包时也查看这个PyPi仓库。请注意，Pip在搜索时仍会查看官方PyPi仓库。'
- en: '"**<your_python_packages>**": After the two lines described above, you can
    add any Python package. But note that you installed **feature_pipeline**, **training_pipeline**,
    and **batch_prediction_pipeline** asPython packages you built and deployed using
    Poetry.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"**<your_python_packages>**": 在上述两行之后，你可以添加任何Python包。但请注意，你已经安装了**feature_pipeline**、**training_pipeline**和**batch_prediction_pipeline**作为你用Poetry构建和部署的Python包。'
- en: 'The other arguments aren''t that interesting, but let me explain them:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 其他参数并不是那么有趣，但让我来解释一下：
- en: '**task_id=" <task_id>":** The unique ID of a task.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**task_id=" <task_id>":** 任务的唯一ID。'
- en: '**python_version=" 3.9"**: When I was writing this course, Hopsworks worked
    only with Python 3.9, so we had to enforce this version of Python.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**python_version=" 3.9"**: 当我写这门课程时，Hopsworks只支持Python 3.9，所以我们必须强制使用这个版本的Python。'
- en: '**multiple_outputs=True**: The task returns a Python dictionary.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**multiple_outputs=True**: 任务返回一个Python字典。'
- en: '**system_site_packages=True:** Install default system packages.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**system_site_packages=True:** 安装默认的系统包。'
- en: '***Important***'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '***重要***'
- en: 'Notethat almost every task returns a dictionary of metadata that contains information
    such as:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，几乎每个任务都会返回一个包含信息的元数据字典，例如：
- en: the date range when the data was extracted,
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据提取的日期范围，
- en: the version of the feature group, feature view, etc.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征组、特征视图等的版本。
- en: the version of the sweep,
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sweep的版本。
- en: the version of the model, etc.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的版本等。
- en: This information is essential to be passed between tasks. For example, the **create_feature_view**
    task needs to know what version of the **feature_group** to use to create the
    next feature view. Also, when running **batch_predict**, you have to know the
    version of the feature view and model to use to generate the predictions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息在任务之间传递是至关重要的。例如，**create_feature_view** 任务需要知道使用哪个版本的 **feature_group**
    来创建下一个特征视图。此外，在运行 **batch_predict** 时，你需要知道要使用哪个版本的特征视图和模型来生成预测。
- en: One interesting task is **task.branch(task_id=" if_run_hyperparameter_tuning_branching")**,
    which defines an if-else logic between whether to run the hyperparameter tuning
    logic or not.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的任务是 **task.branch(task_id=" if_run_hyperparameter_tuning_branching")**，它定义了一个是否运行超参数调整逻辑的
    if-else 逻辑。
- en: This special type of task returns a list of **task_ids** that will be executed
    next. For example, if it returns **["branch_run_hyperparameter_tuning"],** it
    will run only the task with the **task_id =** **branch_run_hyperparameter_tuning**.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特殊类型的任务返回一个 **task_ids** 列表，这些任务将会被执行。例如，如果它返回 **["branch_run_hyperparameter_tuning"]**，则仅运行
    **task_id =** **branch_run_hyperparameter_tuning** 的任务。
- en: As you can see below, two empty operators (tasks) are defined with the task_ids
    used inside the **task.branch()** logic. It is a common pattern suggested by Airflow
    to use a set of empty operators (no operation) when choosing between multiple
    branches.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，定义了两个空的操作符（任务），并在 **task.branch()** 逻辑中使用了 task_ids。这是 Airflow 建议的一种常见模式，当在多个分支之间进行选择时，使用一组空操作符（无操作）。
- en: Connect the Tasks into a DAG
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将任务连接成一个DAG
- en: Now that you defined all the tasks, the final step is to connect them into a
    DAG. You have to perform this step so Airflow knows in what order to run every
    task.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经定义了所有的任务，最后一步是将它们连接成一个 DAG。你必须执行这一步，以便 Airflow 知道每个任务的运行顺序。
- en: Basically, here you will define the logic graph.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这里你将定义逻辑图。
- en: '**#1.** The first step is to ***determine the set of variables*** that you
    will use to configure the DAG, such as **days_delay, days_export, feature_group_version,
    etc.** You can access these variables from the “Admin -> Variables” panel of Airflow.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1.** 第一步是 ***确定你将用于配置DAG的变量集合***，如 **days_delay, days_export, feature_group_version,
    等**。你可以从 Airflow 的“Admin -> Variables”面板中访问这些变量。'
- en: Note that you have to add them using the blue plus button explicitly.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你必须显式地使用蓝色加号按钮添加它们。
- en: '![](../Images/c016988f56a6918bcf17af862524385d.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c016988f56a6918bcf17af862524385d.png)'
- en: Screenshot of the Variables Airflow panel [Image by the Author].
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 Airflow 面板的截图 [作者提供的图片]。
- en: '**#2.** The second step is to ***call the tasks with the right parameters***.
    As you can see, because of the Airflow 2.0 API, this step is just like calling
    a bunch of Python functions in a specific order.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**#2.** 第二步是 ***调用具有正确参数的任务***。如你所见，由于 Airflow 2.0 API，这一步就像按特定顺序调用一系列 Python
    函数一样。'
- en: '**Note:** A dependency in the graph is automatically created if the output
    of a function is added as an input to another function.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 如果一个函数的输出作为输入添加到另一个函数中，则图中的依赖关系会自动创建。'
- en: It is essential to highlight how we passed the metadata of every pipeline element
    to the next ones. In doing so, we enforce the following scripts to use the correct
    data and model versions.
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 强调如何将每个管道元素的元数据传递到下一个元素是至关重要的。这样，我们强制执行以下脚本，以使用正确的数据和模型版本。
- en: 'I also want to emphasize the following piece of code:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我还想强调以下这段代码：
- en: '[PRE12]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**“{{ dag_run.logical_date }}"** is a template variable injected by Airflow
    that reflects the logical date when the DAG is run. Not the current date. By doing
    so, using the Airflow backfill features, you can easily use this as a datetime
    reference to backfill in a given time window. Now you can easily manipulate your
    extraction window''s starting and ending points.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**“{{ dag_run.logical_date }}"** 是 Airflow 注入的模板变量，反映了 DAG 运行时的逻辑日期，而不是当前日期。通过这样做，利用
    Airflow 回填功能，你可以轻松地将其作为日期时间参考来回填给定时间窗口中的数据。现在你可以轻松地操作提取窗口的起始和结束点。'
- en: For example, if you want to run the DAG to backfill the energy consumption predictions
    between 10 and 11 May 2023, you will run the Airflow backfill logic with the "10
    May 2023 00:00 am date".
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想运行 DAG 以回填 2023年5月10日至11日的能源消耗预测，你将使用“2023年5月10日 00:00 am”日期运行 Airflow
    回填逻辑。
- en: '**#3\.** The last step is to enforce a specific ***DAG structure*** using the
    "***>>"*** operator.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**#3\.** 最后一步是使用 "***>>"*** 操作符来强制执行特定的 ***DAG 结构***。'
- en: '"**A >> B >> C**" means run A, then B, then C.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '"**A >> B >> C**" 意味着先运行 A，然后 B，再运行 C。'
- en: 'The only trickier piece of code is this one:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一稍微复杂一点的代码是这个：
- en: '[PRE13]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ', where based on the **branch** operator, the DAG will either run the **branch_run_hyperparameter_tuning_operator**
    or **branch_skip_hyperparameter_tuning_operator** branches of the DAG.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ，其中根据**branch**运算符，DAG 将运行**branch_run_hyperparameter_tuning_operator**或**branch_skip_hyperparameter_tuning_operator**分支。
- en: Read more about branching in Airflow [here](https://docs.astronomer.io/learn/airflow-branch-operator)
    [3].
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读有关 Airflow 中分支的更多信息[这里](https://docs.astronomer.io/learn/airflow-branch-operator)
    [3]。
- en: In English, it will run hyper optimization tunning or skip it, as shown in the
    image below — I know the image is quite small. Check the video for a better view.
    👇
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 以英语运行时，它将进行超参数优化或跳过，如下图所示——我知道图片比较小。查看视频以获取更清晰的视图。 👇
- en: '![](../Images/832932dc0905d7afa066c5026aa25207.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/832932dc0905d7afa066c5026aa25207.png)'
- en: Screenshot of the ml_pipeline DAG [Image by the Author].
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ml_pipeline DAG 截图 [作者提供的图片]。
- en: That is it! You orchestrated all the 3 pipelines using Airflow. Congrats!
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你使用 Airflow 协调了所有3个管道。恭喜！
- en: Run the ML Pipeline DAG
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行 ML Pipeline DAG
- en: This step is easy.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步很简单。
- en: Just go to your **ml_pipeline** DAG and click the play button.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 只需进入你的**ml_pipeline** DAG 并点击播放按钮。
- en: '![](../Images/8797258b1819ae7d4eab3b52f7bb272e.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8797258b1819ae7d4eab3b52f7bb272e.png)'
- en: Screenshot of the ml_pipeline DAG view [Image by the Author].
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ml_pipeline DAG 视图截图 [作者提供的图片]。
- en: Backfill using Airflow
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Airflow 进行回填
- en: 'Find your **airflow-webserver** docker container ID:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 查找你的**airflow-webserver** docker 容器 ID：
- en: '[PRE14]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Start a shell inside the **airflow-webserver** container and run **airflow
    dags backfill** as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在**airflow-webserver**容器内启动 shell 并运行**airflow dags backfill**，如下所示：
- en: '[PRE15]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If you want to clear the tasks and rerun them, run these commands:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想清除任务并重新运行它们，请运行以下命令：
- en: '[PRE16]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Conclusion
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Congratulations! You finished the **fourth lesson** from the **Full Stack 7-Steps
    MLOps Framework** course.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你完成了**第四课**来自**全栈7步MLOps框架**课程。
- en: 'If you have reached this far, you know how to:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经看到这里，你知道如何：
- en: Host your own PyPi server
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 托管自己的 PyPi 服务器
- en: Build & deploy your Python modules using Poetry
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Poetry 构建和部署 Python 模块
- en: Orchestrate multiple pipelines using Airflow
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Airflow 协调多个管道
- en: Now that you understand the power of using an orchestrator such as Airflow,
    you can build robust production-ready pipelines that you can quickly schedule,
    configure, and monitor.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了使用像 Airflow 这样的协调工具的强大功能，你可以构建强大的生产就绪管道，并快速调度、配置和监控。
- en: Check out [Lesson 5](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)
    to learn how to use Great Expectations to validate the integrity and quality of
    your data. Also, you will understand how to implement a monitoring component on
    top of your ML system.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第5课](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)了解如何使用
    Great Expectations 验证数据的完整性和质量。此外，你将了解如何在机器学习系统上实现监控组件。
- en: '**Also,** [**you can access the GitHub repository here**](https://github.com/iusztinpaul/energy-forecasting)**.**'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '**此外，** [**你可以在这里访问 GitHub 仓库**](https://github.com/iusztinpaul/energy-forecasting)**。**'
- en: 💡 My goal is to help machine learning engineers level up in designing and productionizing
    ML systems. Follow me on [LinkedIn](https://www.linkedin.com/in/pauliusztin/)
    or subscribe to my [weekly newsletter](https://pauliusztin.substack.com/) for
    more insights!
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 我的目标是帮助机器学习工程师提升设计和生产机器学习系统的能力。关注我在[LinkedIn](https://www.linkedin.com/in/pauliusztin/)或订阅我的[每周通讯](https://pauliusztin.substack.com/)以获取更多见解！
- en: 🔥 If you enjoy reading articles like this and wish to support my writing, consider
    [becoming a Medium member](https://pauliusztin.medium.com/membership). By using
    [my referral link](https://pauliusztin.medium.com/membership), you can support
    me without any extra cost while enjoying limitless access to Medium’s rich collection
    of stories.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥 如果你喜欢阅读这样的文章并希望支持我的写作，考虑[成为 Medium 会员](https://pauliusztin.medium.com/membership)。通过使用[我的推荐链接](https://pauliusztin.medium.com/membership)，你可以在不增加任何额外费用的情况下支持我，同时享受
    Medium 丰富故事的无限访问权限。
- en: '[](https://pauliusztin.medium.com/membership?source=post_page-----880aa9be8cff--------------------------------)
    [## Join Medium with my referral link - Paul Iusztin'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pauliusztin.medium.com/membership?source=post_page-----880aa9be8cff--------------------------------)
    [## 使用我的推荐链接加入 Medium - Paul Iusztin'
- en: 🤖 Join to get exclusive content about designing and building production-ready
    ML systems 🚀 Unlock full access to…
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 🤖 加入以获取有关设计和构建生产就绪机器学习系统的独家内容 🚀 解锁全部访问权限…
- en: pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----880aa9be8cff--------------------------------)
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----880aa9be8cff--------------------------------)'
- en: References
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[1] [Energy Consumption per DE35 Industry Code from Denmark API](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour),
    [Denmark Energy Data Service](https://www.energidataservice.dk/about/)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [丹麦 API 每小时 DE35 行业代码的能耗](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)，[丹麦能源数据服务](https://www.energidataservice.dk/about/)'
- en: '[2] [Running Airflow in Docker](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html),
    Airflow Documentation'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [在 Docker 中运行 Airflow](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)，Airflow
    文档'
- en: '[3] [Branching in Airflow](https://docs.astronomer.io/learn/airflow-branch-operator),
    Airflow Documentation on Astronomer'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [Airflow 中的分支](https://docs.astronomer.io/learn/airflow-branch-operator)，Astronomer
    上的 Airflow 文档'
