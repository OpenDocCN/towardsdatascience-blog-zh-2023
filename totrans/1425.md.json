["```py\n%matplotlib qt\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(0)\nn = 100\nX_ = np.random.uniform(3, 10, n).reshape(-1, 1)\nbeta_0 = 2\nbeta_1 = 2\ntrue_y = beta_1 * X_ + beta_0\nnoise = np.random.randn(n, 1) * 0.5 # change the scale to reduce/increase noise\ny = true_y + noise\n\nfig, axes = plt.subplots(1, 2, squeeze=False, sharex=True, sharey=True, figsize=(18, 8))\naxes[0, 0].plot(X_, y, \"o\", label=\"Input data\")\naxes[0, 0].plot(X_, true_y, '--', label='True linear relation')\naxes[0, 0].set_xlim(0, 11)\naxes[0, 0].set_ylim(0, 30)\naxes[0, 0].legend()\n\n# f_0 is a column of 1s\n# f_1 is the column of x1\nX = np.c_[np.ones((n, 1)), X_]\n\nbeta_OLS_scratch = np.linalg.inv(X.T @ X) @ X.T @ y\nlr = LinearRegression(\n    fit_intercept=False, # do not fit intercept independantly, since we added the 1 column for this purpose\n).fit(X, y)\n\nnew_X = np.linspace(0, 15, 50).reshape(-1, 1)\nnew_X = np.c_[np.ones((50, 1)), new_X]\nnew_y_OLS_scratch = new_X @ beta_OLS_scratch \nnew_y_lr = lr.predict(new_X)\naxes[0, 1].plot(X_, y, 'o', label='Input data')\naxes[0, 1].plot(new_X[:, 1], new_y_OLS_scratch, '-o', alpha=0.5,  label=r\"OLS scratch solution\")\naxes[0, 1].plot(new_X[:, 1], new_y_lr, '-*', alpha=0.5, label=r\"sklearn.lr OLS solution\")\naxes[0, 1].legend()\nfig.tight_layout()\nprint(beta_OLS_scratch)\nprint(lr.coef_)\n```", "```py\n[[2.12458946]\n [1.99549536]]\n[[2.12458946 1.99549536]]\n```", "```py\nimport numpy as np\nfrom sklearn.kernel_ridge import KernelRidge\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nX = np.sort(5 * np.random.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - np.random.rand(16))\n\n# Create a test dataset\nX_test = np.arange(0, 5, 0.01)[:, np.newaxis]\n\n# Fit the KernelRidge model with an RBF kernel\nkr = KernelRidge(\n    kernel='rbf', # use RBF kernel\n    alpha=1, # regularization \n    gamma=1, # scale for rbf\n)\nkr.fit(X, y)\n\ny_rbf = kr.predict(X_test)\n\n# Plot the results\nfig, ax = plt.subplots()\nax.scatter(X, y, color='darkorange', label='Data')\nax.plot(X_test, y_rbf, color='navy', lw=2, label='RBF Kernel Ridge Regression')\nax.set_title('Kernel Ridge Regression with RBF Kernel')\nax.legend() \n```", "```py\n%matplotlib qt\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(0)\nn = 100\nX_ = np.random.uniform(3, 10, n).reshape(-1, 1)\nbeta_0 = 2\nbeta_1 = 2\ntrue_y = beta_1 * X_ + beta_0\nnoise = np.random.randn(n, 1) * 0.5 # change the scale to reduce/increase noise\ny = true_y + noise\n\nfig, axes = plt.subplots(1, 2, squeeze=False, sharex=True, sharey=True, figsize=(18, 8))\naxes[0, 0].plot(X_, y, \"o\", label=\"Input data\")\naxes[0, 0].plot(X_, true_y, '--', label='True linear relation')\naxes[0, 0].set_xlim(0, 11)\naxes[0, 0].set_ylim(0, 30)\naxes[0, 0].legend()\n\n# f_0 is a column of 1s\n# f_1 is the column of x1\nX = np.c_[np.ones((n, 1)), X_]\n\nbeta_OLS_scratch = np.linalg.inv(X.T @ X) @ X.T @ y\nlr = LinearRegression(\n    fit_intercept=False, # do not fit intercept independantly, since we added the 1 column for this purpose\n).fit(X, y)\n\nnew_X = np.linspace(0, 15, 50).reshape(-1, 1)\nnew_X = np.c_[np.ones((50, 1)), new_X]\nnew_y_OLS_scratch = new_X @ beta_OLS_scratch \nnew_y_lr = lr.predict(new_X)\naxes[0, 1].plot(X_, y, 'o', label='Input data')\naxes[0, 1].plot(new_X[:, 1], new_y_OLS_scratch, '-o', alpha=0.5,  label=r\"OLS scratch solution\")\naxes[0, 1].plot(new_X[:, 1], new_y_lr, '-*', alpha=0.5, label=r\"sklearn.lr OLS solution\")\naxes[0, 1].legend()\nfig.tight_layout()\nprint(beta_OLS_scratch)\nprint(lr.coef_)\n```"]