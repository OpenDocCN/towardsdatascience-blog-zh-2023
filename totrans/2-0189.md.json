["```py\n# Listing 1\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import multivariate_normal\nimport torch\nimport torch.nn as nn\nfrom numpy import linalg as LA\nfrom sklearn.preprocessing import MinMaxScaler\nimport random\n%matplotlib inline\n\nnp.random.seed(1)\nmu = [0, 0]\nSigma = [[1, 1],\n         [1, 2.5]]\n\n# X is the design matrix and each row of X is an example\nX = np.random.multivariate_normal(mu, Sigma, 10000)\nX = np.concatenate([X, X[:, 0].reshape(len(X), 1)], axis=1)\nX[:, 2] = X[:, 1] / 2\nX = (X - X.mean(axis=0))\nx, y, z = X.T\n```", "```py\n# Listing 2\nfig = plt.figure(figsize=(10, 10))\nax1 = fig.add_subplot(111, projection='3d')\n\nax1.scatter(x, y, z, color = 'blue')\n\nax1.view_init(20, 185)\nax1.set_xlabel(\"$x_1$\", fontsize=20)\nax1.set_ylabel(\"$x_2$\", fontsize=20)\nax1.set_zlabel(\"$x_3$\", fontsize=20)\nax1.set_xlim([-5, 5])\nax1.set_ylim([-7, 7])\nax1.set_zlim([-4, 4])\nplt.show()\n```", "```py\npca = PCA(n_components=3)\npca.fit(X)\n```", "```py\n# Each row gives one of the principal components (eigenvectors)\npca.components_\n```", "```py\narray([[-0.38830581, -0.824242  , -0.412121  ],\n       [-0.92153057,  0.34731128,  0.17365564],\n       [ 0\\.        , -0.4472136 ,  0.89442719]])\n```", "```py\npca.explained_variance_\n```", "```py\narray([3.64826952e+00, 5.13762062e-01, 3.20547162e-32])\n```", "```py\n# Listing 3\nv1 = pca.components_[0]\nv2 = pca.components_[1]\nv3 = pca.components_[2]\n\nfig = plt.figure(figsize=(10, 10))\nax1 = fig.add_subplot(111, projection='3d')\n\nax1.scatter(x, y, z, color = 'blue', alpha= 0.1)\nax1.plot([0, v1[0]], [0, v1[1]], [0, v1[2]],\n         color=\"black\", zorder=6)\nax1.plot([0, v2[0]], [0, v2[1]], [0, v2[2]],\n         color=\"black\", zorder=6)\nax1.plot([0, v3[0]], [0, v3[1]], [0, v3[2]],\n         color=\"black\", zorder=6)\n\nax1.scatter(x, y, z, color = 'blue', alpha= 0.1)\nax1.plot([0, 7*v1[0]], [0, 7*v1[1]], [0, 7*v1[2]],\n         color=\"gray\", zorder=5)\nax1.plot([0, 5*v2[0]], [0, 5*v2[1]], [0, 5*v2[2]],\n         color=\"gray\", zorder=5)\nax1.plot([0, 3*v3[0]], [0, 3*v3[1]], [0, 3*v3[2]],\n         color=\"gray\", zorder=5)\n\nax1.text(v1[0], v1[1]-0.2, v1[2], \"$\\mathregular{v}_1$\",\n         fontsize=20, color='red', weight=\"bold\",\n         style=\"italic\", zorder=9)\nax1.text(v2[0], v2[1]+1.3, v2[2], \"$\\mathregular{v}_2$\",\n         fontsize=20, color='red', weight=\"bold\",\n         style=\"italic\", zorder=9)\nax1.text(v3[0], v3[1], v3[2], \"$\\mathregular{v}_3$\", fontsize=20,\n         color='red', weight=\"bold\", style=\"italic\", zorder=9)\n\nax1.view_init(20, 185)\nax1.set_xlabel(\"$x_1$\", fontsize=20, zorder=2)\nax1.set_ylabel(\"$x_2$\", fontsize=20)\nax1.set_zlabel(\"$x_3$\", fontsize=20)\nax1.set_xlim([-5, 5])\nax1.set_ylim([-7, 7])\nax1.set_zlim([-4, 4])\n\nplt.show()\n```", "```py\n# Listing 4\n\n# Z* = UΣ\npca.transform(X)\n```", "```py\n([[ 3.09698570e+00, -3.75386182e-01, -2.06378618e-17],\n       [-9.49162774e-01, -7.96300950e-01, -5.13280752e-18],\n       [ 1.79290419e+00, -1.62352748e+00,  2.41135694e-18],\n       ...,\n       [ 2.14708946e+00, -6.35303400e-01,  4.34271577e-17],\n       [ 1.25724271e+00,  1.76475781e+00, -1.18976523e-17],\n       [ 1.64921984e+00, -3.71612351e-02, -5.03148111e-17]])\n```", "```py\n# Listing 5\n\nfig = plt.figure(figsize=(8, 6))\nplt.scatter(pca.transform(X)[:,0], pca.transform(X)[:,1])\nplt.axis('equal')\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\nplt.xlabel(\"$v_1$\", fontsize=20)\nplt.ylabel(\"$v_2$\", fontsize=20)\nplt.xlim([-8.5, 8.5])\nplt.ylim([-4, 4])\nplt.show()\n```", "```py\n# Listing 6\n\nU, s, VT = LA.svd(X)\nprint(\"U=\", np.round(U, 4))\nprint(\"Diagonal of elements of Σ=\", np.round(s, 4))\nprint(\"V^T=\", np.round(VT, 4))\n```", "```py\nU= [[ 1.620e-02 -5.200e-03  1.130e-02 ... -2.800e-03 -2.100e-02 -6.200e-03]\n [-5.000e-03 -1.110e-02  9.895e-01 ...  1.500e-03 -3.000e-04  1.100e-03]\n [ 9.400e-03 -2.270e-02  5.000e-04 ... -1.570e-02  1.510e-02 -7.100e-03]\n ...\n [ 1.120e-02 -8.900e-03 -1.800e-03 ...  9.998e-01  2.000e-04 -1.000e-04]\n [ 6.600e-03  2.460e-02  1.100e-03 ...  1.000e-04  9.993e-01 -0.000e+00]\n [ 8.600e-03 -5.000e-04 -1.100e-03 ... -1.000e-04 -0.000e+00  9.999e-01]]\nDiagonal of elements of Σ= [190.9949  71.6736   0\\.    ]\nV^T= [[-0.3883 -0.8242 -0.4121]\n [-0.9215  0.3473  0.1737]\n [ 0\\.     -0.4472  0.8944]]\n```", "```py\n# Listing 7\n\nk = 2\nSigma = np.zeros((X.shape[0], X.shape[1]))\nSigma[:min(X.shape[0], X.shape[1]),\n      :min(X.shape[0], X.shape[1])] = np.diag(s)\n\nX2 = U[:, :k] @ Sigma[:k, :k]  @ VT[:k, :]\nX2\n```", "```py\narray([[-0.85665, -2.68304, -1.34152],\n       [ 1.10238,  0.50578,  0.25289],\n       [ 0.79994, -2.04166, -1.02083],\n       ...,\n       [-0.24828, -1.99037, -0.99518],\n       [-2.11447, -0.42335, -0.21168],\n       [-0.60616, -1.37226, -0.68613]])\n```", "```py\n# Listing 8\n\n# each row of Z*=U_k Σ_k gives the coordinate of projection of the \n# same row of X onto a rank-k subspace\nU[:, :k] @ Sigma[:k, :k] \n```", "```py\narray([[ 3.0969857 , -0.37538618],\n       [-0.94916277, -0.79630095],\n       [ 1.79290419, -1.62352748],\n       ...,\n       [ 2.14708946, -0.6353034 ],\n       [ 1.25724271,  1.76475781],\n       [ 1.64921984, -0.03716124]])\n```", "```py\n# Listing 9\n\nseed = 9 \nnp.random.seed(seed)\ntorch.manual_seed(seed)\nnp.random.seed(seed)\n\nclass Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        ## encoder \n        self.encoder = nn.Linear(3, 2, bias=False)\n\n        ## decoder \n        self.decoder = nn.Linear(2, 3, bias=False)\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return encoded, decoded\n\n# initialize the NN\nmodel1 = Autoencoder().double()\nprint(model1)\n```", "```py\n# Listing 10\n\n# specify the quadratic loss function\nloss_func = nn.MSELoss()\n\n# Define the optimizer\noptimizer = torch.optim.Adam(model1.parameters(), lr=0.001)\n```", "```py\nX_train = torch.from_numpy(X) \n```", "```py\n# Listing 11\n\ndef train(model, loss_func, optimizer, n_epochs, X_train):\n    model.train()\n    for epoch in range(1, n_epochs + 1):\n        optimizer.zero_grad()\n        encoded, decoded = model(X_train)\n        loss = loss_func(decoded, X_train)\n        loss.backward()\n        optimizer.step()\n\n        if epoch % int(0.1*n_epochs) == 0:\n            print(f'epoch {epoch} \\t Loss: {loss.item():.4g}')\n    return encoded, decoded\n\nencoded, decoded = train(model1, loss_func, optimizer, 3000, X_train)\n```", "```py\nepoch 300   Loss: 0.4452\nepoch 600   Loss: 0.1401\nepoch 900   Loss: 0.05161\nepoch 1200   Loss: 0.01191\nepoch 1500   Loss: 0.003353\nepoch 1800   Loss: 0.0009412\nepoch 2100   Loss: 0.0002304\nepoch 2400   Loss: 4.509e-05\nepoch 2700   Loss: 6.658e-06\nepoch 3000   Loss: 7.02e-07\n```", "```py\nencoded = encoded.detach().numpy()\ndecoded = decoded.detach().numpy()\n```", "```py\n# Z* values. Each row gives the coordinates of one of the \n# projected data points\nZstar = encoded\nZstar\n```", "```py\narray([[ 2.57510917, -3.13073321],\n       [-0.20285442,  1.38040138],\n       [ 2.39553775, -1.16300036],\n       ...,\n       [ 2.0265917 , -1.99727172],\n       [-0.18811382, -2.15635479],\n       [ 1.26660007, -1.74235118]])\n```", "```py\n# Listing 12\n\n# Each row of W^[2] gives the wights of one of the neurons in the\n# output layer\nW2 = model1.decoder.weight\nW2 = W2.detach().numpy()\nW2\n```", "```py\narray([[ 0.77703505,  0.91276084],\n       [-0.72734132,  0.25882988],\n       [-0.36143178,  0.13109568]])\n```", "```py\n# Each row of Pstar (or column of W2) is one of the basis vectors\nWstar = W2.T\nWstar\n```", "```py\narray([[ 0.77703505, -0.72734132, -0.36143178],\n       [ 0.91276084,  0.25882988,  0.13109568]])\n```", "```py\nw1 = Wstar[0]\nw2 = Wstar[1]\n\n# p1 and p2 are not orthogonal since thier inner product is not zero\nnp.dot(w1, w2)\n```", "```py\n0.47360735759\n```", "```py\n# X2 = Zstar @ Pstar\nZstar @ Wstar\n```", "```py\narray([[-0.8566606 , -2.68331059, -1.34115189],\n       [ 1.10235133,  0.50483352,  0.25428269],\n       [ 0.7998756 , -2.04339283, -1.0182878 ],\n       ...,\n       [-0.24829863, -1.99097748, -0.99430834],\n       [-2.11440724, -0.42130609, -0.21469848],\n       [-0.60615728, -1.37222311, -0.68620423]])\n```", "```py\n# Listing 13\n\nfig = plt.figure(figsize=(18, 14))\nplt.subplots_adjust(wspace = 0.01)\norigin = [0], [0], [0] \n\nax1 = fig.add_subplot(121, projection='3d')\nax2 = fig.add_subplot(122, projection='3d')\nax1.set_aspect('auto')\nax2.set_aspect('auto')\ndef plot_view(ax, view1, view2):\n    ax.scatter(x, y, z, color = 'blue', alpha= 0.1)\n    # Principal components \n    ax.plot([0, pca.components_[0,0]], [0, pca.components_[0,1]],\n            [0, pca.components_[0,2]],\n            color=\"black\", zorder=5)\n    ax.plot([0, pca.components_[1,0]], [0, pca.components_[1,1]],\n            [0, pca.components_[1,2]],\n            color=\"black\", zorder=5)\n\n    ax.text(pca.components_[0,0], pca.components_[0,1],\n            pca.components_[0,2]-0.5, \"$\\mathregular{v}_1$\",\n            fontsize=18, color='black', weight=\"bold\",\n            style=\"italic\")\n    ax.text(pca.components_[1,0], pca.components_[1,1]+0.7,\n            pca.components_[1,2], \"$\\mathregular{v}_2$\",\n            fontsize=18, color='black', weight=\"bold\",\n            style=\"italic\")\n\n    # New basis found by autoencoder\n    ax.plot([0, w1[0]], [0, w1[1]], [0, w1[2]],\n             color=\"darkred\", zorder=5)\n    ax.plot([0, w2[0]], [0, w2[1]], [0, w2[2]],\n             color=\"darkred\", zorder=5)\n\n    ax.text(w1[0], w1[1]-0.2, w1[2]+0.1,\n            \"$\\mathregular{w}_1$\", fontsize=18, color='darkred',\n            weight=\"bold\", style=\"italic\")\n    ax.text(w2[0], w2[1], w2[2]+0.3,\n            \"$\\mathregular{w}_2$\", fontsize=18, color='darkred',\n            weight=\"bold\", style=\"italic\")\n\n    ax.view_init(view1, view2)\n    ax.set_xlabel(\"$x_1$\", fontsize=20, zorder=2)\n    ax.set_ylabel(\"$x_2$\", fontsize=20)\n    ax.set_zlabel(\"$x_3$\", fontsize=20)\n    ax.set_xlim([-3, 5])\n    ax.set_ylim([-5, 5])\n    ax.set_zlim([-4, 4])\nplot_view(ax1, 25, 195)\nplot_view(ax2, 0, 180)\nplt.show()\n```", "```py\n# Listing 14\n\n# This is not the right way to plot the projected data points in\n# a 2d space since {w1, w2} is not an orthogonal basis\n\nfig = plt.figure(figsize=(8, 8))\nplt.scatter(Zstar[:, 0], Zstar[:, 1])\ni= 6452\nplt.scatter(Zstar[i, 0], Zstar[i, 1], color='red', s=60)\nplt.axis('equal')\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\nplt.xlabel(\"$z_1$\", fontsize=20)\nplt.ylabel(\"$z_2$\", fontsize=20)\nplt.xlim([-9,9])\nplt.ylim([-9,9])\nplt.show()\n```", "```py\n# Listing 15\n\nw1_V = np.array([np.dot(w1, v1), np.dot(w1, v2)])\nw2_V = np.array([np.dot(w2, v1), np.dot(w2, v2)])\nP_W = np.array([w1_V, w2_V]).T\n\nZstar_V = np.zeros((Zstar.shape[0], Zstar.shape[1]))\n\nfor i in range(len(Zstar_B)):\n    Zstar_V[i] = P_W @ Zstar[i]\n\nfig = plt.figure(figsize=(8, 6))\nplt.scatter(Zstar_V[:, 0], Zstar_V[:, 1])\nplt.axis('equal')\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\nplt.scatter(Zstar_V[i, 0], Zstar_V[i, 1], color='red', s=60)\nplt.quiver(0, 0, w1_V[0], w1_V[1], color=['black'], width=0.007,\n           angles='xy', scale_units='xy', scale=1)\nplt.quiver(0, 0, w2_V[0], w2_V[1], color=['black'], width=0.007,\n           angles='xy', scale_units='xy', scale=1)\nplt.text(w1_V[0]+0.1, w2_V[1]-0.2, \"$[\\mathregular{w}_1]_V$\",\n         weight=\"bold\", style=\"italic\", color='black',\n         fontsize=20)\nplt.text(w2_V[0]-2.25, w2_V[1]+0.1, \"$[\\mathregular{w}_2]_V$\",\n         weight=\"bold\", style=\"italic\", color='black',\n         fontsize=20)\n\nplt.xlim([-8.5, 8.5])\nplt.xlabel(\"$v_1$\", fontsize=20)\nplt.ylabel(\"$v_2$\", fontsize=20)\nplt.show()\n```", "```py\n# listing 16\n\nnp.random.seed(0)\nn = 90\ntheta = np.sort(np.random.uniform(0, 2*np.pi, n))\ncolors = np.linspace(1, 15, num=n)\n\nx1 = np.sqrt(2) * np.cos(theta)\nx2 = np.sqrt(2) * np.sin(theta)\nX_circ = np.array([x1, x2]).T\n\nfig = plt.figure(figsize=(8, 6))\nplt.axis('equal')\nplt.scatter(X_circ[:,0], X_circ[:,1], c=colors, cmap=plt.cm.jet)\n\nplt.xlabel(\"$x_1$\", fontsize= 18)\nplt.ylabel(\"$x_2$\", fontsize= 18)\n\nplt.show()\n```", "```py\n# Listing 17\n\npca = PCA(n_components=2, random_state = 1)\npca.fit(X_circ)\n\nfig = plt.figure(figsize=(8, 6))\nplt.axis('equal')\nplt.scatter(X_circ[:,0], X_circ[:,1], c=colors,\n            cmap=plt.cm.jet)\nplt.quiver(0, 0, pca.components_[0,0], pca.components_[0,1],\n           color=['black'], width=0.01, angles='xy',\n           scale_units='xy', scale=1.5)\nplt.quiver(0, 0, pca.components_[1,0], pca.components_[1,1],\n           color=['black'], width=0.01, angles='xy',\n           scale_units='xy', scale=1.5)\nplt.plot([-2*pca.components_[0,0], 2*pca.components_[0,0]],\n         [-2*pca.components_[0,1], 2*pca.components_[0,1]],\n         color='gray')\nplt.text(0.5*pca.components_[0,0], 0.8*pca.components_[0,1], \n         \"$\\mathregular{v}_1$\", color='black', fontsize=20)\nplt.text(0.8*pca.components_[1,0], 0.8*pca.components_[1,1],\n         \"$\\mathregular{v}_2$\", color='black', fontsize=20)\nplt.show()\n```", "```py\n# Listing 18\n\nprojected_points = pca.transform(X_circ)[:,0]\nfig = plt.figure(figsize=(16, 2))\nframe = plt.gca()\nplt.scatter(projected_points, [0]*len(projected_points),\n            c=colors, cmap=plt.cm.jet, alpha =0.7)\nplt.axhline(y=0, color='grey')\nplt.xlabel(\"$v_1$\", fontsize=18)\n#plt.xlim([-1.6, 1.7])\nframe.axes.get_yaxis().set_visible(False)\nplt.show()\n```", "```py\n# Listing 19\n\nseed = 3\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nnp.random.seed(seed)\n\nclass Autoencoder(nn.Module):\n    def __init__(self, in_shape, enc_shape):\n        super(Autoencoder, self).__init__()\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(in_shape, 64),\n            nn.ReLU(True),\n            nn.Dropout(0.1),\n            nn.Linear(64, 32),\n            nn.ReLU(True),\n            nn.Dropout(0.1),\n            nn.Linear(32, enc_shape),\n        )\n\n        #Decoder\n        self.decoder = nn.Sequential(\n            nn.BatchNorm1d(enc_shape),\n            nn.Linear(enc_shape, 32),\n            nn.ReLU(True),\n            nn.Dropout(0.1),\n            nn.Linear(32, 64),\n            nn.ReLU(True),\n            nn.Dropout(0.1),\n            nn.Linear(64, in_shape)\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return encoded, decoded\n\nmodel2 = Autoencoder(in_shape=2, enc_shape=1).double()\nprint(model2)\n```", "```py\nloss_func = nn.MSELoss()\noptimizer = torch.optim.Adam(model2.parameters())\n```", "```py\nX_circ_scaled = MinMaxScaler().fit_transform(X_circ)\nX_circ_train = torch.from_numpy(X_circ_scaled)\n```", "```py\n# Listing 20\n\ndef train(model, loss_func, optimizer, n_epochs, X_train):\n    model.train()\n    for epoch in range(1, n_epochs + 1):\n        optimizer.zero_grad()\n        encoded, decoded = model(X_train)\n        loss = loss_func(decoded, X_train)\n        loss.backward()\n        optimizer.step()\n\n        if epoch % int(0.1*n_epochs) == 0:\n            print(f'epoch {epoch} \\t Loss: {loss.item():.4g}')\n    return encoded, decoded\n\nencoded, decoded = train(model2, loss_func, optimizer, 5000, X_circ_train)\n```", "```py\nepoch 500   Loss: 0.01391\nepoch 1000   Loss: 0.005599\nepoch 1500   Loss: 0.007459\nepoch 2000   Loss: 0.005192\nepoch 2500   Loss: 0.005775\nepoch 3000   Loss: 0.005295\nepoch 3500   Loss: 0.005112\nepoch 4000   Loss: 0.004366\nepoch 4500   Loss: 0.003526\nepoch 5000   Loss: 0.003085\n```", "```py\nencoded = encoded.detach().numpy()\n\nfig = plt.figure(figsize=(16, 2))\nframe = plt.gca()\nplt.scatter(encoded.flatten(), [0]*len(encoded.flatten()),\n            c=colors, cmap=plt.cm.jet, alpha =0.7)\nplt.axhline(y=0, color='grey')\nplt.xlabel(\"$z_1$\", fontsize=18)\nframe.axes.get_yaxis().set_visible(False)\nplt.show()\n```"]