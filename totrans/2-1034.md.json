["```py\ndef measure_perf(\n    prompt: str, model: AutoModelForCausalLM, tokenizer: AutoTokenizer\n) -> Tuple[float, float, torch.Tensor]:\n    \"\"\"\n    Measures memory consumption and inference execution time for a given model and prompt.\n\n    Args:\n        prompt: Text to be used as input for the model.\n        model: Pretrained model used for inference.\n        tokenizer: Pretrained tokenizer used to encode the prompt.\n\n    Returns:\n        Peak memory consumption in GB, execution time in seconds, and output tensor from the model.\n    \"\"\"\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n\n    start_time = time.time()\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n    outputs = model.generate(input_ids, max_length=100)\n\n    end_time = time.time()\n\n    peak_mem = torch.cuda.max_memory_allocated()\n    peak_mem_consumption = peak_mem / 1e9  # convert bytes to GB\n\n    exec_time = end_time - start_time\n\n    return peak_mem_consumption, exec_time, outputs\n```", "```py\ndef plot_results(\n    mem_consumptions: List[float], execution_times: List[float], dir: str = \"plots\"\n) -> None:\n    \"\"\"\n    Plots memory consumption and execution times.\n\n    Args:\n        mem_consumptions: List of memory consumption data in GB.\n        execution_times: List of execution time data.\n        dir: Destination dir for the plot.\n    \"\"\"\n    os.makedirs(dir, exist_ok=True)\n\n    fig, ax1 = plt.subplots()\n\n    color = \"tab:red\"\n    ax1.set_xlabel(\"Runs\")\n    ax1.set_ylabel(\"GPU Memory Consumption (GB)\", color=color)\n    ax1.plot(mem_consumptions, color=color)\n    ax1.tick_params(axis=\"y\", labelcolor=color)\n    ax1.yaxis.get_major_formatter().set_useOffset(False)\n\n    ax2 = ax1.twinx()\n    color = \"tab:blue\"\n    ax2.set_ylabel(\"Execution time (s)\", color=color)\n    ax2.plot(execution_times, color=color)\n    ax2.tick_params(axis=\"y\", labelcolor=color)\n    ax2.yaxis.get_major_formatter().set_useOffset(False)\n\n    fig.tight_layout()\n    plt.title(\"GPU Memory Consumption and Execution Time for Each Run\")\n    fig.subplots_adjust(top=0.88)\n    plt.savefig(f\"{dir}/falcon_memory_time.png\")\n```", "```py\nmodel_path = \"tiiuae/falcon-40b-instruct\"\nconfig = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    config=config,\n    trust_remote_code=True,\n    load_in_4bit=True,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n```", "```py\nruns = 5\nmem_consumptions = []\nexecution_times = []\n\nfor i in range(runs):\n    prompts = [\n        \"Write a story about a magical object that grants wishes, but with unpredictable consequences.\",\n        \"Describe your ideal vacation destination and why it is your top choice.\",\n        \"Write a persuasive argument for or against the use of smartphones in classrooms.\",\n        \"Invent a new flavor of ice cream and describe its taste, ingredients, and unique qualities.\",\n        \"What are the steps to perform an A/B test?\",\n    ]\n\n    mem_consumption, exec_time, outputs = measure_perf(prompts[i], model, tokenizer)\n    mem_consumptions.append(mem_consumption)\n    execution_times.append(exec_time)\n    print(tokenizer.decode(outputs[0]))\n\nplot_results(mem_consumptions, execution_times)\n```"]