["```py\nKINESIS_STACK=YourRedshiftDataStream\nENV=staging\naws \\\ncloudformation deploy \\\n--template-file kinesis-data-stream.yaml \\\n--stack-name $KINESIS_STACK \\\n--capabilities CAPABILITY_IAM \\\n--parameter-overrides \\\n\"Environment\"=$ENV\n```", "```py\nAWSTemplateFormatVersion: 2010-09-09\nDescription: >\n  Firehose resources relating to statistics generation.\n  Repository - https://github.com/your_repository.\n\nParameters:\n  Environment:\n    AllowedValues:\n      - staging\n      - production\n    Description: Target environment\n    Type: String\n    Default: 'staging'\n\nResources:\n  MyKinesisStream:\n    Type: AWS::Kinesis::Stream\n    Properties: \n      Name: !Sub 'your-data-stream-${Environment}'\n      RetentionPeriodHours: 24 \n      StreamModeDetails:\n        StreamMode: ON_DEMAND\n      # ShardCount: 1\n      Tags: \n        -\n          Key: Environment\n          Value: Production\n```", "```py\n.\n├── app.py\n├── config\n│   └── staging.yaml\n├── env.json\n└── requirements.txt\n```", "```py\n# Make sure boto3 is installed locally, i.e. pip install boto3\nimport json\nimport random\nimport boto3\n\nkinesis_client = boto3.client('kinesis', region_name='eu-west-1')\n# Constants:\nSTREAM_NAME = \"your-data-stream-staging\"\n\ndef lambda_handler(event, context):\n    processed = 0\n    print(STREAM_NAME)\n    try:\n        print('Trying to send events to Kinesis...')\n        for i in range(0, 5):\n            data = get_data()\n            print(i, \" : \", data)\n            kinesis_client.put_record(\n                StreamName=STREAM_NAME,\n\n                Data=json.dumps(data),\n                PartitionKey=\"partitionkey\")\n            processed += 1\n    except Exception as e:\n        print(e)\n    message = 'Successfully processed {} events.'.format(processed)\n    return {\n        'statusCode': 200,\n        'body': { 'lambdaResult': message }\n    }\n\n```", "```pypython\n# Helpers:\ndef get_data():\n    return {\n        'event_time': datetime.now().isoformat(),\n        'event_name': random.choice(['JOIN', 'LEAVE', 'OPEN_CHAT', 'SUBSCRIBE', 'SEND_MESSAGE']),\n        'user': round(random.random() * 100)}\n```", "```py\npip install python-lambda-local\ncd stack\npython-lambda-local -e events_connector/env.json -f lambda_handler events_connector/app.py event.json --timeout 10000\n# -e is for environment variables if you choose to use them.\n#  event.json - sample JSON event to invoke our Lambda with.\n```", "```py\n# staging.yaml\nKinesis:\n  DataStreamNsme: your-data-stream-staging\n```", "```py\nrequests==2.28.1\npyyaml==6.0\nboto3==boto3-1.26.90\npython-lambda-local==0.1.13\n```", "```py\n cd stack\npip install -r events_connector/requirements.txt\n```", "```py\nPROFILE=your-aws-profile\nSTACK_NAME=YourStackNameLive\nLAMBDA_BUCKET=your-lambdas-bucket.aws # Make sure it exists\n\ndate\n\nTIME=`date +\"%Y%m%d%H%M%S\"`\n\nbase=${PWD##*/}\nzp=$base\".zip\"\necho $zp\n\nrm -f $zp\n\npip install --target ./package -r requirements.txt\n\ncd package\nzip -r ../${base}.zip .\n\ncd $OLDPWD\n\nzip -r $zp ./events_connector -x __pycache__ \n\naws --profile $PROFILE s3 cp ./${base}.zip s3://${LAMBDA_BUCKET}/events_connector/${base}${TIME}.zip\n\naws --profile $PROFILE \\\ncloudformation deploy \\\n--template-file stack.yaml \\\n--stack-name $STACK_NAME \\\n--capabilities CAPABILITY_IAM \\\n--parameter-overrides \\\n\"StackPackageS3Key\"=\"events_connector/${base}${TIME}.zip\" \\\n\"Environment\"=\"staging\" \\\n\"Testing\"=\"false\"\n```", "```py\nAWSTemplateFormatVersion: 2010-09-09\nParameters:\n  DatabaseName:\n    Description: The name of the first database in the Amazon Redshift Serverless environment.\n    Type: String\n    Default: dev\n    MaxLength: 127\n    AllowedPattern: '[a-zA-Z][a-zA-Z_0-9+.@-]*'\n  AdminUsername:\n    Description: The administrator's user name for Redshift Serverless Namespace being created.\n    Type: String\n    Default: admin\n    AllowedPattern: '[a-zA-Z][a-zA-Z_0-9+.@-]*'\n  AdminUserPassword:\n    Description: The password associated with admin user.\n    Type: String\n    NoEcho: 'true'\n    Default: Admin123\n    MinLength: 8\n    MaxLength: 64\n    # AllowedPattern: '^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)[^\\x00-\\x20\\x22\\x27\\x2f\\x40\\x5c\\x7f-\\uffff]+'\n  NamespaceName:\n    Description: A unique identifier that defines the Namespace.\n    Default: rswg\n    Type: String\n    MinLength: 3\n    MaxLength: 64\n    AllowedPattern: '^[a-z0-9-]+$'\n  WorkgroupName:\n    Description: A unique identifier that defines the Workspace.\n    Default: redshiftworkgroup\n    Type: String\n    MinLength: 3\n    MaxLength: 64\n    AllowedPattern: '^[a-z0-9-]*$'\n  BaseRPU:\n    Description: Base RPU for Redshift Serverless Workgroup.\n    Type: Number\n    MinValue: 8\n    MaxValue: 512\n    Default: 8\n    AllowedValues: [8,16,32,40,48,56,64,72,80,88,96,104,112,120,128,136,144,152,160,168,176,184,192,200,208,216,224,232,240,248,256,264,272,280,288,296,304,312,320,328,336,344,352,360,368,376,384,392,400,408,416,424,432,440,448,456,464,472,480,488,496,504,512]\n  PubliclyAccessible:\n    Description: Redshift Serverless instance to be publicly accessible.\n    Type: String\n    Default: true\n    AllowedValues:\n      - true\n      - false\n\n  SubnetId:\n    Description: You must have at least three subnets, and they must span across three Availability Zones\n    Type: List<AWS::EC2::Subnet::Id>\n  SecurityGroupIds:\n    Description: The list of SecurityGroupIds in your Virtual Private Cloud (VPC).\n    Type: List<AWS::EC2::SecurityGroup::Id>\n  LogExportsList:\n    Description: Provide comma seperate values from list \"userlog\",\"connectionlog\",\"useractivitylog\".  E.g userlog,connectionlog,useractivitylog.  If left blank, LogExport is turned off.\n    Type: CommaDelimitedList \n    Default: userlog,connectionlog,useractivitylog\n  EnhancedVpcRouting:\n    Description: The value that specifies whether to enable enhanced virtual private cloud (VPC) routing, which forces Amazon Redshift Serverless to route traffic through your VPC.\n    Type: String\n    AllowedValues:\n      - true\n      - false\n    Default: false    \nMetadata:\n  'AWS::CloudFormation::Interface':\n    ParameterGroups:\n      - Label:\n          default: Namespace parameters\n        Parameters:\n          - NamespaceName\n          - DatabaseName\n          - AdminUsername\n          - AdminUserPassword\n          - IAMRole\n          - LogExportsList          \n      - Label:\n          default: Workgroup parameters\n        Parameters:\n            - WorkgroupName\n            - BaseRPU\n            - PubliclyAccessible\n            - SubnetId\n            - SecurityGroupIds\n            - EnhancedVpcRouting            \n    ParameterLabels:\n      DatabaseName:\n        default: \"Database Name\"\n      AdminUsername:\n        default: \"Admin User Name\"\n      AdminUserPassword:\n        default: \"Admin User Password\"\n      NamespaceName:\n        default: \"Namespace\"\n      WorkgroupName:\n        default: \"Workgroup\"\n      BaseRPU:\n        default: \"Base RPU\"\n      PubliclyAccessible:\n        default: \"Publicly accessible\"\n      SubnetId:\n        default: \"Subnet Ids (Select 3 Subnet Ids spanning 3 AZs)\"\n      SecurityGroupIds:\n        default: \"Security Group Id\"\n      IAMRole:\n        default: \"Associate IAM Role\"\n      EnhancedVpcRouting:\n        default: \"Enhanced VPC Routing\"  \n      LogExportsList:\n        default: \"Log Export List\"\nResources:\n  RedshiftAccessRole:\n    Type: AWS::IAM::Role\n    Properties:\n      ManagedPolicyArns: \n          - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\n          - arn:aws:iam::aws:policy/AmazonRedshiftAllCommandsFullAccess\n\n      AssumeRolePolicyDocument:\n        Version: 2012-10-17\n        Statement:\n          -\n            Effect: Allow\n            Principal:\n              Service:\n                - redshift.amazonaws.com\n            Action:\n              - sts:AssumeRole\n  RedshiftRolePolicy:\n    Type: AWS::IAM::Policy\n    Properties:\n      PolicyName: RedshiftRolePolicy\n      PolicyDocument:\n        Version: 2012-10-17\n        Statement:\n          -\n            Effect: Allow\n            Action: s3:ListAllMyBuckets\n            Resource: arn:aws:s3:::*\n          -\n            Effect: Allow\n            Action:\n              - 's3:Get*'\n              - 's3:List*'\n            Resource: '*'\n          -\n            Effect: Allow\n            Action: cloudwatch:*\n            Resource: \"*\"\n          -\n            Effect: Allow\n            Action: kinesis:*\n            Resource: \"*\"\n      Roles:\n        - !Ref RedshiftAccessRole\n  RedshiftServerlessNamespace:\n    DependsOn: RedshiftAccessRole\n    Type: 'AWS::RedshiftServerless::Namespace'\n    Properties:\n      AdminUsername:\n        Ref: AdminUsername\n      AdminUserPassword:\n        Ref: AdminUserPassword\n      DbName:\n        Ref: DatabaseName\n      NamespaceName:\n        Ref: NamespaceName\n      IamRoles:\n        - !GetAtt [ RedshiftAccessRole, Arn ]\n      LogExports:\n        Ref: LogExportsList        \n  RedshiftServerlessWorkgroup:\n    Type: 'AWS::RedshiftServerless::Workgroup'\n    Properties:\n      WorkgroupName:\n        Ref: WorkgroupName\n      NamespaceName:\n        Ref: NamespaceName\n      BaseCapacity:\n        Ref: BaseRPU\n      PubliclyAccessible:\n        Ref: PubliclyAccessible\n      SubnetIds:\n        Ref: SubnetId\n      SecurityGroupIds:\n        Ref: SecurityGroupIds\n      EnhancedVpcRouting:\n        Ref: EnhancedVpcRouting        \n    DependsOn:\n      - RedshiftServerlessNamespace\nOutputs:\n  ServerlessNamespace:\n    Description: Name of the namespace\n    Value: !Ref NamespaceName\n  ServerlessWorkgroup:\n    Description: Name of the workgroup\n    Value: !Ref WorkgroupName\n```", "```py\nSTACK=YourRedshiftServerless\nSUBNETID=subnet-1,subnet-2,subnet-3\nSECURITYGROUPIDS=sg-your-security-group\naws \\\ncloudformation deploy \\\n--template-file redshift-serverless.yaml \\\n--stack-name $STACK \\\n--capabilities CAPABILITY_IAM \\\n--parameter-overrides \\\n\"SubnetId\"=$SUBNETID \\\n\"SecurityGroupIds\"=$SECURITYGROUPIDS\n```", "```py\nCREATE EXTERNAL SCHEMA kinesis_data\nFROM KINESIS\nIAM_ROLE 'arn:aws:iam::123456789:role/rs3-RedshiftAccessRole-1TU31HQNXM0EK';\n;\nCREATE MATERIALIZED VIEW \"your-stream-view\" AUTO REFRESH YES AS\n    SELECT approximate_arrival_timestamp,\n           partition_key,\n           shard_id,\n           sequence_number,\n           refresh_time,\n           JSON_PARSE(kinesis_data) as payload\n      FROM kinesis_data.\"your-data-stream-staging\";\n;\n```", "```py\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": \"s3:ListAllMyBuckets\",\n            \"Resource\": \"arn:aws:s3:::*\",\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Action\": [\n                \"s3:Get*\",\n                \"s3:List*\"\n            ],\n            \"Resource\": \"*\",\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Action\": \"cloudwatch:*\",\n            \"Resource\": \"*\",\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Action\": \"kinesis:*\",\n            \"Resource\": \"*\",\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\n```"]