["```py\nimport numpy as np\nimport pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load your dataset here; X should contain the features, and y should contain the target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the XGBoost classifier\nxgb_model = XGBClassifier()\n\n# Define the hyperparameters and their respective search space\nparam_grid = {\n    'learning_rate': [0.1, 0.01, 0.001],\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7],\n    'min_child_weight': [1, 3, 5]\n}\n\n# Create the GridSearchCV object\ngrid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=5)\n\n# Perform hyperparameter tuning on the training data\ngrid_search.fit(X_train, y_train)\n\n# Get the best hyperparameters and the corresponding model\nbest_params = grid_search.best_params_\nbest_model = grid_search.best_estimator_\n\n# Make predictions on the test set using the best model\ny_pred = best_model.predict(X_test)\n\n# Evaluate the model performance\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Test Accuracy:\", accuracy)\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport optuna\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n# Define the objective function for Optuna optimization\ndef objective(trial):\n    param = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=100),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n        'max_depth': trial.suggest_int('max_depth', 3, 10)\n    }\n\n    # Create an XGBoost classifier with the current hyperparameters\n    xgb_model = xgb.XGBClassifier(**param)\n\n    # Train and evaluate the classifier using cross-validation\n    f1_scores = []\n    for train_index, val_index in skf.split(X_train, y_train):\n        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n\n        xgb_model.fit(X_train_fold, y_train_fold)\n        y_pred_fold = xgb_model.predict(X_val_fold)\n        f1_scores.append(f1_score(y_val_fold, y_pred_fold))\n\n    return np.mean(f1_scores)\n\n# Load your dataset here; X should contain the features, and y should contain the target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a StratifiedKFold object for cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create an Optuna study and optimize the objective function\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\n# Get the best hyperparameters and the corresponding model\nbest_params = study.best_params\nbest_model = xgb.XGBClassifier(**best_params)\nbest_model.fit(X_train, y_train)\n\n# Make predictions on the test set using the best model\ny_pred = best_model.predict(X_test)\n\n# Evaluate the model performance on the test set\nf1_test = f1_score(y_test, y_pred)\n\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Test F1 Score:\", f1_test)\n```"]