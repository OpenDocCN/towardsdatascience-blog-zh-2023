- en: arXiv Keyword Extraction and Analysis Pipeline with KeyBERT and Taipy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/arxiv-keyword-extraction-and-analysis-pipeline-with-keybert-and-taipy-2972e81d9fa4](https://towardsdatascience.com/arxiv-keyword-extraction-and-analysis-pipeline-with-keybert-and-taipy-2972e81d9fa4)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Build a keyword analysis Python application comprising a frontend user interface
    and backend pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://kennethleungty.medium.com/?source=post_page-----2972e81d9fa4--------------------------------)[![Kenneth
    Leung](../Images/2514dffb34529d6d757c0c4ec5f98334.png)](https://kennethleungty.medium.com/?source=post_page-----2972e81d9fa4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2972e81d9fa4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2972e81d9fa4--------------------------------)
    [Kenneth Leung](https://kennethleungty.medium.com/?source=post_page-----2972e81d9fa4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2972e81d9fa4--------------------------------)
    ·12 min read·Apr 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/177bb91daf175985fe9b1a3705329593.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Marylou Fortier](https://unsplash.com/@rylouma?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/heNLI144X7Y?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: As the amount of textual data from sources like social media, customer reviews,
    and online platforms grows exponentially, we must be able to make sense of this
    unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: Keyword extraction and analysis are powerful natural language processing (NLP)
    techniques that enable us to achieve that.
  prefs: []
  type: TYPE_NORMAL
- en: '**Keyword extraction** involves automatically identifying and extracting the
    most relevant words from a given text, while **keyword analysis** involves analyzing
    the keywords to gain insights into the underlying patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: In this step-by-step guide, we explore building a keyword extraction and analysis
    pipeline and web app on arXiv abstracts using the powerful tools of **KeyBERT**
    and **Taipy**.
  prefs: []
  type: TYPE_NORMAL
- en: Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***(1)*** [*Context*](#3ed3)***(2)*** [*Tools Overview*](#e50e)***(3)*** [*Step-by-Step
    Guide*](#b70c)***(4)*** [*Wrapping it up*](#db84)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Here is the accompanying [GitHub repo](https://github.com/kennethleungty/Keyword-Analysis-with-KeyBERT-and-Taipy)
    for this article.
  prefs: []
  type: TYPE_NORMAL
- en: (1) Context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the rapid progress in artificial intelligence (AI) and machine learning
    research, keeping track of the many papers published daily can be challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding such research, [arXiv](https://arxiv.org/) is undoubtedly one of the
    leading sources of information. arXiv (pronounced ‘archive’) is an open-access
    archive hosting a vast collection of scientific papers covering various disciplines
    like computer science, mathematics, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a3babb0255822cfb690da00dd8c6e31.png)'
  prefs: []
  type: TYPE_IMG
- en: arXiv screenshot | Image used under [CC 2.0](https://ccnull.de/foto/arxivorg-logo-under-magnifying-glass/1014135)
    license
  prefs: []
  type: TYPE_NORMAL
- en: One of the key features of arXiv is that it provides abstracts for each paper
    uploaded to its platform. These abstracts are an ideal data source as they are
    concise, rich in technical vocabulary, and contain domain-specific terminology.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we will utilize the latest batches of arXiv abstracts as the text data
    to work on in this project.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to create a web application (comprising a frontend interface and
    backend pipeline) where users can view the keywords and key phrases of arXiv abstracts
    based on specific input values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bbf1043e0ffbbffc66e2b0d81daae13.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of the completed application user interface | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: (2) Tools Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three main tools that we will use in this project:'
  prefs: []
  type: TYPE_NORMAL
- en: arXiv API Python wrapper
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KeyBERT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taipy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (i) arXiv API Python wrapper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The arXiv website offers public API access to maximize its openness and interoperability.
    For example, to retrieve the text abstracts as part of our Python workflow, we
    can use the [**Python wrapper for the arXiv API**](https://github.com/lukasschwab/arxiv.py).
  prefs: []
  type: TYPE_NORMAL
- en: The arXiv API Python wrapper provides a set of functions for searching the database
    for papers that match specific criteria, such as author, keyword, category, and
    more.
  prefs: []
  type: TYPE_NORMAL
- en: It also lets users retrieve detailed metadata about each paper, such as the
    title, abstract, authors, and publication date.
  prefs: []
  type: TYPE_NORMAL
- en: (ii) KeyBERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KeyBERT (from the terms ‘keyword’ and ‘BERT’) is a Python library that provides
    an easy-to-use interface for using [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))
    embeddings and cosine similarity to extract the words in a document most representative
    of the document itself.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9734dda5a53b0f87dec849acc0bbf8b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of how KeyBERT works | Image used under [MIT License](https://github.com/MaartenGr/KeyBERT/blob/master/LICENSE)
  prefs: []
  type: TYPE_NORMAL
- en: The biggest strength of KeyBERT is its flexibility. It allows users to easily
    modify the underlying settings (e.g., parameters, embeddings, tokenization) to
    experiment and fine-tune the keywords obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this project, we will be tuning the following set of parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of the top keywords to be returned
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word n-gram range (i.e., minimum and maximum n-gram length)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diversification algorithm ([Max Sum Distance](/keyword-extraction-with-bert-724efca412ea#:~:text=Maximal%20Marginal%20Relevance-,Max%20Sum%20Similarity,-The%20maximum%20sum)
    or [Maximal Marginal Relevance](/keyword-extraction-with-bert-724efca412ea#:~:text=in%20your%20document.-,Maximal%20Marginal%20Relevance,-The%20final%20method))
    that determines how the similarity of extracted keywords is defined
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of candidates (if Max Sum Distance is set)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diversity value (if Maximal Marginal Relevance is set)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both diversification algorithms (Max Sum Distance and Maximal Marginal Relevance)
    share the same basic idea of balancing two objectives: Retrieve results that are
    highly relevant to the query and yet are diverse in their content to avoid redundancy
    amongst each other.'
  prefs: []
  type: TYPE_NORMAL
- en: (iii) Taipy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Taipy](https://www.taipy.io/) is an open-source Python application builder
    that quickly lets developers and data scientists turn data and machine learning
    algorithms into complete web applications.'
  prefs: []
  type: TYPE_NORMAL
- en: While designed to be a low-code library, Taipy also provides a high level of
    user customization. Therefore, it is well-suited for wide-ranging use cases, from
    simple dashboarding to production-ready industrial applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d3d3885aaae12d080ffe003c7a77e85.png)'
  prefs: []
  type: TYPE_IMG
- en: Taipy components | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two key components of Taipy: Taipy GUI and Taipy Core.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Taipy GUI**: A simple graphical user interface builder enabling us to easily
    create an interactive frontend app interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Taipy Core**: A modern backend framework that lets us efficiently build and
    execute pipelines and scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While we can use Taipy GUI or Taipy Core independently, combining both allows
    us to build powerful applications efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Step-by-Step Guide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier in the [Context](#3ed3) section, we will build a web app
    that extracts and analyzes keywords of selected arXiv abstracts.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram illustrates how the data and tools are integrated.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3daa3594d8ca067ec33f8d2fe438e50.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of project | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Let us get started with the steps to create the above pipeline and web application
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 — Initial Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by pip installing the necessary Python libraries with corresponding
    versions shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[arvix](https://pypi.org/project/arxiv/) 1.4.3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[keybert](https://pypi.org/project/keybert/) 0.7.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[pandas](https://pypi.org/project/pandas/) 1.5.3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[taipy](https://pypi.org/project/taipy/) 2.2.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step 2 — Setup Configuration File
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As numerous parameters will be used, saving them inside a separate configuration
    file is ideal. The following YAML file `config.yml` contains the initial set of
    configuration parameter values.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the configuration file set up, we can then easily import these parameter
    values into our other Python scripts with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Step 3 — Build Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this step, we will create a series of Python functions that form vital components
    of the pipeline. We create a new Python file `functions.py` to store these functions.
  prefs: []
  type: TYPE_NORMAL
- en: (3.1) Retrieve and Save arXiv Abstracts and Metadata
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first function to add into `functions.py` is one for retrieving text abstracts
    from the arXiv database using the arXiv API Python wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we write a function to store the abstract texts and corresponding metadata
    in a pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: (3.2) Process Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the data processing step, we have the following function to parse the abstract
    publication date into the appropriate format while creating new empty columns
    to store keywords.
  prefs: []
  type: TYPE_NORMAL
- en: (3.3) Run KeyBERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We next create a function to run the `KeyBert` class from the KeyBERT library.
    The `KeyBERT` class is a minimal method for keyword extraction with BERT and is
    the easiest way for us to get started.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different methods for generating the BERT embeddings (e.g., [Flair](https://github.com/flairNLP/),
    [Huggingface Transformers](https://github.com/huggingface/transformers), and [spaCy](https://nightly.spacy.io/)).
    In this case, we will use [sentence-transformers](https://www.sbert.net/) as recommended
    by the KeyBERT creator.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we will use the default`[all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)`
    model as it provides a good balance of speed and quality.
  prefs: []
  type: TYPE_NORMAL
- en: The following function extracts the keywords from each abstract iteratively
    and saves them in the new DataFrame columns created in the previous step.
  prefs: []
  type: TYPE_NORMAL
- en: (3.4) Get Keywords Value Counts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we create a function that generates a value count of the keywords so
    that we can plot the keyword frequencies in a chart later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4 — Setup Taipy Core: Backend Config'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To orchestrate and link the backend pipeline flow, we will leverage the capabilities
    of Taipy Core.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taipy Core offers an open-source framework to create, manage, and execute our
    data pipelines easily and efficiently. It has four fundamental concepts: **Data
    Nodes, Tasks, Pipelines, and Scenarios**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3fc4bb3f3728edad5077f91638991d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Four fundamental concepts in Taipy Core | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: To set up the backend, we will use **configuration objects** (from the `Config`
    class) to model and define the characteristics and desired behavior of the abovementioned
    concepts.
  prefs: []
  type: TYPE_NORMAL
- en: (4.1) Data Nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with most data science projects, we start by handling the data. In Taipy
    Core, we use **Data Nodes** to define the data we will work with.
  prefs: []
  type: TYPE_NORMAL
- en: We can think of Data Nodes as Taipy’s representation of data variables. However,
    instead of storing the data directly, Data Nodes contain a set of instructions
    on how to retrieve the data needed.
  prefs: []
  type: TYPE_NORMAL
- en: Data Nodes can read and write a wide range of data types, such as Python objects
    (e.g., `str`, `int`, `list`, `dict`, `DataFrame`, etc.), Pickle files, CSVs, SQL
    databases, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `Config.configure_data_node()` function, we define the Data Nodes
    for the keyword parameters based on the values from the configuration file in
    [Step 2](#e847).
  prefs: []
  type: TYPE_NORMAL
- en: The `id` parameter sets the name of the Data Node, while the `default_data`
    parameter defines the default values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We next include the configuration objects for the five sets of data along the
    pipeline, as illustrated below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad04a4bebcbf9175e231f2a733ab274a.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of five Data Nodes along pipeline | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code defines the five configuration objects:'
  prefs: []
  type: TYPE_NORMAL
- en: (4.2) Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tasks in Taipy can be thought of as Python functions. We can define the configuration
    object for Tasks using the `Config.configure_task()`.
  prefs: []
  type: TYPE_NORMAL
- en: We need to set five Task configuration objects corresponding to the five functions
    built in [Step 3](#124f).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/035776668d4c7670d9d28dedca15f3db.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of the five Tasks | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The `input` and `output` parameters refer to the input and output Data Nodes,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in `task_process_data_cfg`, the input is the Data Node for the
    raw pandas DataFrame containing the arXiv search results, while the output is
    the Data Node for the DataFrame storing processed data.
  prefs: []
  type: TYPE_NORMAL
- en: The `skippable` parameter, when set to True, indicates that the Task can be
    skipped if no changes have been made to the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the flowchart of the Data Nodes and Tasks we have defined so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/927e3dd829a3b8d3bb622dceb60d5b51.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Nodes and Tasks flowchart | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: (4.3) Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **Pipeline** is a series of Tasks that will be executed automatically by Taipy.
    It is a configuration object comprising a sequence of Task configuration objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we will allocate the five Tasks into two Pipelines (one for data
    preparation and one for keyword analysis) as illustrated below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d8dd694b913ad44a1067a4b388a8536.png)'
  prefs: []
  type: TYPE_IMG
- en: Tasks within the two pipelines | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the following code to define our two Pipeline configs:'
  prefs: []
  type: TYPE_NORMAL
- en: As with all configuration objects, we assign a name to these Pipeline configurations
    using the `id` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: (4.4) Scenarios
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this project, we aim to create an application that reflects the updated set
    of keywords (and corresponding analysis) based on changes made to input parameters
    (e.g., N-gram length).
  prefs: []
  type: TYPE_NORMAL
- en: For that to happen, we leverage the powerful concept of **Scenarios**. Taipy
    Scenarios provide the framework for running Pipelines under different conditions,
    such as when the user modifies the input parameters or data.
  prefs: []
  type: TYPE_NORMAL
- en: Scenarios also allow us to save the outputs from the different inputs for easy
    comparison within the same app interface.
  prefs: []
  type: TYPE_NORMAL
- en: Since we expect to do a straightforward sequential run of the Pipelines, we
    can place both Pipeline configs into the one Scenario configuration object.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 — Setup Taipy GUI (Frontend)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us now switch gears and explore the frontend aspects of our application.
    Taipy GUI provides Python classes that make it easy to create powerful web app
    interfaces with text and graphical elements.
  prefs: []
  type: TYPE_NORMAL
- en: Pages are the basis for the user interface, and they hold text, images, or controls
    that display information in the application through visual elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two pages to create: **(i)** a keyword analysis dashboard page and
    **(ii)** a data viewer page to display the keywords DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: (5.1) Data Viewer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Taipy GUI can be considered an **augmented** Markdown, meaning we can use the
    Markdown syntax to build our frontend interface.
  prefs: []
  type: TYPE_NORMAL
- en: We start with the simple frontend page displaying the DataFrame of the extracted
    arXiv abstract data. The page is set up in a Python script (named `data_viewer_md.py`)
    and storing the Markdown in a variable (called `data_page)`.
  prefs: []
  type: TYPE_NORMAL
- en: The basic syntax for creating Taipy constructs in Markdown is using text fragments
    in the generic format of `<|...|...|>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the above Markdown, we pass our DataFrame object `df` along with `table`,
    which indicates a **table** element. With just these few lines of code, we get
    an output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55985bde47e7b8cf96f7881a80dfb119.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of the Data Viewer page | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: (5.2) Keyword Analysis Dashboard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now move to the main dashboard page of the application, where we can make
    changes to the parameters and visualize the keywords obtained. The visual elements
    will be contained within a Python script (named `analysis_md.py`)
  prefs: []
  type: TYPE_NORMAL
- en: This page has numerous components, so let’s take it one step at a time. First,
    we instantiate the parameter values upon the loading of the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define the input segment of the page where users can make changes
    to parameters and scenarios. This segment will be saved in a variable called `input_page`,
    and will eventually look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9352eb0112b9821e49a6eeca02d3be8.png)'
  prefs: []
  type: TYPE_IMG
- en: Input segment of the Keyword Analysis page | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We create a seven-column layout in the Markdown so that the input fields (e.g.,
    text input, number input, dropdown menu selector) and buttons can be organized
    neatly.
  prefs: []
  type: TYPE_NORMAL
- en: We will explain the callback functions in the `on_change` and `on_action` parameters
    for the elements above, so there is no need to worry about them for now.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: After that, we define the output segment, where the frequency table and chart
    of the keywords based on the input parameters will be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ffaf72b0ce02d879c3a4ad3980820f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Output segment of the Keyword Analysis page | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We will define the chart properties in addition to specifying the Markdown of
    the output segment in the variable `output_page`.
  prefs: []
  type: TYPE_NORMAL
- en: And in the last line above, we combine both input and output segments into a
    single variable called `analysis_page`.
  prefs: []
  type: TYPE_NORMAL
- en: (5.3) Main Landing Page
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One last bit before our frontend interface is complete. Now that we have both
    pages ready, we shall display them on our main landing page.
  prefs: []
  type: TYPE_NORMAL
- en: The main page is defined within `main.py`, which is the script that will be
    run when the application is launched. The aim is to create a functional menu bar
    on the main page for users to toggle between the pages.
  prefs: []
  type: TYPE_NORMAL
- en: From the above code, we can see the state functionality of Taipy in action,
    where the page is rendered based on the selected page in the session state.
  prefs: []
  type: TYPE_NORMAL
- en: Step 6— Linking Backend and Frontend with Scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, our frontend interface and backend pipeline have been set up
    successfully. However, we have yet to link both of them together.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, we will need to create the **Scenarios** component so that
    variations in the input parameters are processed in the pipeline, and the output
    is reflected in the dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: The added benefit of Scenarios is that every input-output set can be saved so
    that users can refer back to these previous configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will define four functions to set up the Scenarios component, which will
    be stored in the `analysis_md.py` script:'
  prefs: []
  type: TYPE_NORMAL
- en: (6.1) Update Chart
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This function updates the keywords DataFrame, frequency count table, and corresponding
    bar chart based on the input parameters of the selected Scenario stored in the
    session state.
  prefs: []
  type: TYPE_NORMAL
- en: (6.2) Submit Scenario
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This function registers the updated set of input parameters the user has modified
    as a scenario and passes the values through the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: (6.3) Create Scenario
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This function saves a scenario that has been executed so that it can be easily
    recreated and referred to again from the dropdown menu of created Scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: (6.4) Synchronize GUI and Core
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This function retrieves input parameters from a Scenario selected from the dropdown
    menu of saved Scenarios and displays the resulting output in the frontend GUI.
  prefs: []
  type: TYPE_NORMAL
- en: Step 7— Launching the Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last step, we wrap up by completing the code in `main.py` so that the
    Taipy launches and runs correctly when the script is executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The above code does the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate Taipy Core
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setup scenario creation and execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve keywords DataFrame and frequency count table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launch Taipy GUI (with the specified pages)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we can run `python main.py` in the Command Line, and the application
    we have built will be accessible on `localhost:8020`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bbf1043e0ffbbffc66e2b0d81daae13.png)'
  prefs: []
  type: TYPE_IMG
- en: Frontend interface of completed application | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: (4) Wrapping it up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The keywords associated with a document offer concise and comprehensive indications
    of its subject matter, highlighting the most important themes, concepts, ideas,
    or arguments contained therein.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we explored how to extract and analyze keywords of arXiv abstracts
    using KeyBERT and Taipy. We also discovered how to deliver these capabilities
    as a web application comprising a frontend user interface and a backend pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to check out the codes in the accompanying [**GitHub repo**](https://github.com/kennethleungty/Keyword-Analysis-with-KeyBERT-and-Taipy).
  prefs: []
  type: TYPE_NORMAL
- en: Before you go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I welcome you to **join me on a journey of data science discovery!** Follow
    this [Medium](https://kennethleungty.medium.com/) page and visit my [GitHub](https://github.com/kennethleungty)
    to stay updated with more engaging and practical content. Meanwhile, have fun
    building your keyword extraction and analysis pipeline with KeyBERT and Taipy!
  prefs: []
  type: TYPE_NORMAL
- en: '[](/when-ai-goes-astray-high-profile-machine-learning-mishaps-in-the-real-world-26bd58692195?source=post_page-----2972e81d9fa4--------------------------------)
    [## When AI Goes Astray: High-Profile Machine Learning Mishaps in the Real World'
  prefs: []
  type: TYPE_NORMAL
- en: A tour of infamous machine learning blunders and failures that caught the world’s
    attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/when-ai-goes-astray-high-profile-machine-learning-mishaps-in-the-real-world-26bd58692195?source=post_page-----2972e81d9fa4--------------------------------)
    [](https://medium.datadriveninvestor.com/how-to-web-scrape-wikipedia-using-llm-agents-f0dba8400692?source=post_page-----2972e81d9fa4--------------------------------)
    [## How to Web Scrape Wikipedia with LLM Agents
  prefs: []
  type: TYPE_NORMAL
- en: Simple guide to using LangChain Agents and Tools with OpenAI’s LLMs and Function
    Calling for web scraping of Wikipedia
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.datadriveninvestor.com](https://medium.datadriveninvestor.com/how-to-web-scrape-wikipedia-using-llm-agents-f0dba8400692?source=post_page-----2972e81d9fa4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
