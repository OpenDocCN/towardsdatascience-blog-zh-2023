- en: A Step-by-step Guide to Solving 4 Real-life Problems With Transformers and Hugging
    Face
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一步步解决4个实际问题的指南：使用变压器和 Hugging Face
- en: 原文：[https://towardsdatascience.com/4-real-life-problems-solved-using-transformers-and-hugging-face-a-complete-guide-e45fe698cc4d](https://towardsdatascience.com/4-real-life-problems-solved-using-transformers-and-hugging-face-a-complete-guide-e45fe698cc4d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/4-real-life-problems-solved-using-transformers-and-hugging-face-a-complete-guide-e45fe698cc4d](https://towardsdatascience.com/4-real-life-problems-solved-using-transformers-and-hugging-face-a-complete-guide-e45fe698cc4d)
- en: Understand Transformers and harness their power to solve real-life problems
    in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解变压器并利用它们的力量在 Python 中解决实际问题
- en: '[](https://zoumanakeita.medium.com/?source=post_page-----e45fe698cc4d--------------------------------)[![Zoumana
    Keita](../Images/34a15c1d03687816dbdbc065f5719f80.png)](https://zoumanakeita.medium.com/?source=post_page-----e45fe698cc4d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e45fe698cc4d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e45fe698cc4d--------------------------------)
    [Zoumana Keita](https://zoumanakeita.medium.com/?source=post_page-----e45fe698cc4d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://zoumanakeita.medium.com/?source=post_page-----e45fe698cc4d--------------------------------)[![Zoumana
    Keita](../Images/34a15c1d03687816dbdbc065f5719f80.png)](https://zoumanakeita.medium.com/?source=post_page-----e45fe698cc4d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e45fe698cc4d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e45fe698cc4d--------------------------------)
    [Zoumana Keita](https://zoumanakeita.medium.com/?source=post_page-----e45fe698cc4d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e45fe698cc4d--------------------------------)
    ·11 min read·Jan 31, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----e45fe698cc4d--------------------------------)
    ·阅读时间11分钟·2023年1月31日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/429c05a1b2db762b98d5755b81d78326.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/429c05a1b2db762b98d5755b81d78326.png)'
- en: Image by [Aditya Vyas](https://unsplash.com/@aditya1702) on [Unsplash](https://unsplash.com/photos/B9MULm2UZIk)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Aditya Vyas](https://unsplash.com/@aditya1702) 在 [Unsplash](https://unsplash.com/photos/B9MULm2UZIk)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In the field of Natural Language Processing (NLP), researchers have made significant
    contributions over the past decades, resulting in innovative advancements in various
    domains. Some examples of NLP in practice are provided below:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）领域，研究人员在过去几十年里做出了显著贡献，带来了各种领域的创新进展。以下是 NLP 在实践中的一些例子：
- en: '**Siri**, a personal assistant developed by Apple, can assist users with tasks
    like setting alarms, sending texts, and answering questions.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Siri**，由苹果公司开发的个人助理，可以帮助用户完成设置闹钟、发送短信和回答问题等任务。'
- en: In the **medical field**, NLP is being utilized to speed up drug discovery.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**医疗领域**，NLP 正被用来加速药物发现过程。
- en: Additionally, NLP is also being used to bridge language barriers through **translation**.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，NLP 还被用于通过**翻译**来弥合语言障碍。
- en: The purpose of this article is to discuss Transformers, an extremely powerful
    model in Natural Language Processing. It will begin by highlighting the advantages
    of Transformers over recurrent neural networks, furthering your comprehension
    of the model. Then, it will provide practical examples of using Huggingface transformers
    in real-world scenarios
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是讨论变压器，这是一种在自然语言处理领域中极为强大的模型。它将首先强调变压器相对于循环神经网络的优势，以加深你对该模型的理解。接着，将提供使用
    Huggingface 变压器在实际场景中的应用实例。
- en: Recurrent Network — the shinning era before Transformers
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环网络——变压器之前的辉煌时代
- en: Before delving into the fundamental idea of transformers, it’s important to
    gain a basic understanding of recurrent models, including their limitations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解变压器的基本概念之前，了解循环模型及其局限性是非常重要的。
- en: Recurrent networks use an encoder-decoder structure and are typically used for
    tasks involving input and output sequences in a specific order. Some of the most
    common applications of recurrent networks include machine translation and modeling
    time series data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 循环网络使用编码器-解码器结构，通常用于处理按特定顺序输入和输出序列的任务。循环网络的一些最常见的应用包括机器翻译和时间序列数据建模。
- en: Challenges with recurrent networks
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环网络的挑战
- en: As an example, let’s take a French sentence and translate it into English. The
    encoder receives the original French sentence as input, and the decoder produces
    the translated output.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们以一句法语句子为例，将其翻译成英语。编码器接收原始法语句子作为输入，解码器生成翻译输出。
- en: '![](../Images/8c8639426bdc40dfafc77c5b86aa8163.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c8639426bdc40dfafc77c5b86aa8163.png)'
- en: Simple illustration of the recurrent network (By Author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 递归网络的简单示意图（作者提供）
- en: The encoder processes the input French sentence word by word, and the decoder
    generates word embeddings in the same sequence, making them time-consuming to
    train.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器逐词处理输入的法语句子，而解码器以相同的顺序生成单词嵌入，这使得训练非常耗时。
- en: The hidden state of the current word is dependent on the hidden states of the
    previous words, making it impossible to perform parallel computation, regardless
    of the computational power available.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前词的隐藏状态依赖于前面词的隐藏状态，这使得无法进行并行计算，无论计算能力多么强大。
- en: Sequence-to-sequence neural networks are prone to experiencing exploding gradients
    when the network is too large, resulting in poor performance
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到序列的神经网络在网络过大时容易遇到梯度爆炸的问题，导致性能不佳。
- en: Another type of recurrent network, Long Short-Term Memory (LSTM) networks, were
    developed to address the issue of vanishing gradients, but they are even slower
    than traditional sequence models.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种递归网络——长短期记忆（LSTM）网络，被开发出来解决梯度消失的问题，但它们的速度甚至比传统序列模型还慢。
- en: Wouldn’t it be beneficial to have a model that combines the advantages of recurrent
    networks and enables parallel computation?
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结合递归网络的优点并实现并行计算的模型难道不是很有用吗？
- en: ''
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Here is where transformers come in handy.**'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**这就是变换器派上用场的地方。**'
- en: What are transformers in NLP?
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，变换器是什么？
- en: In 2017, Google Brain introduced Transformers, a new, powerful neural network
    architecture, in their renowned research paper “Attention is all you need.” It
    is based on the attention mechanism rather than the sequential computation found
    in recurrent networks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，Google Brain 在其著名的研究论文《Attention is all you need》中介绍了变换器，这是一种新型强大的神经网络架构。它基于注意力机制，而不是递归网络中的序列计算。
- en: What are the main components of transformers?
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器的主要组件是什么？
- en: 'Like recurrent networks, transformers also consist of two main components:
    an encoder and a decoder, each incorporating a self-attention mechanism. The following
    section provides an overall understanding of the primary elements of each component
    of transformers'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 像递归网络一样，变换器也由两个主要组件组成：编码器和解码器，每个组件都包含一个自注意力机制。以下部分提供了对变换器每个组件主要元素的总体理解。
- en: '![](../Images/d25a9de3087b7b52d0c3fcd3f74227cd.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d25a9de3087b7b52d0c3fcd3f74227cd.png)'
- en: General [Architecture of Transformers](https://arxiv.org/abs/1706.03762) (adapted
    by Author)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一般 [变换器架构](https://arxiv.org/abs/1706.03762)（作者改编）
- en: Input sentence preprocessing stage
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入句子预处理阶段
- en: 'This section involves two primary steps: (1) creating the embeddings of the
    input sentence, and (2) calculating the positional vector of each word in the
    input sentence. These computations are carried out in the same way for both the
    input sentence (prior to the encoder block) and the output sentence (before the
    decoder block).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分包括两个主要步骤：（1）创建输入句子的嵌入，和（2）计算输入句子中每个单词的位置信息。这些计算在输入句子（编码器块之前）和输出句子（解码器块之前）中以相同的方式进行。
- en: '**Embedding of the input data**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入数据的嵌入**'
- en: Before creating the embeddings of the input data, we begin by tokenizing it,
    then creating the embedding for each individual word without considering their
    relationship within the sentence.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建输入数据的嵌入之前，我们首先对其进行分词，然后为每个单词创建嵌入，而不考虑它们在句子中的关系。
- en: '**Positional encoding**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置编码**'
- en: The tokenization process eliminates any sense of connections that existed in
    the input sentence. The positional encoding aims to restore the original cyclical
    nature by creating a context vector for each word.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 分词过程消除了输入句子中存在的任何连接感。位置编码旨在通过为每个单词创建上下文向量来恢复原始的循环特性。
- en: Encoder bloc
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器块
- en: 'As a result of the previous step, we obtain a combination of two vectors for
    each word: (1) the embedding and (2) its context vector. These vectors are combined
    to create a single vector for each word, which is then sent to the encoder.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前面的步骤，我们为每个单词得到两个向量的组合：（1）嵌入向量和（2）上下文向量。这些向量被结合起来，为每个单词创建一个单一向量，然后送到编码器中。
- en: '**Multi-head attention**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**多头注意力**'
- en: As previously stated, all sense of relationship is lost. The purpose of the
    attention layer is to identify the contextual connections between different words
    in the input sentence. This step ultimately results in the creation of an attention
    vector for each word.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，所有的关系感都丧失了。注意力层的目的是识别输入句子中不同单词之间的上下文连接。这个步骤最终会为每个单词创建一个注意力向量。
- en: '**Position-wise feed-forward net (FFN)**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置相关前馈网络（FFN）**'
- en: In this step, a feed-forward neural network is applied to each attention vector
    to change them into a format that can be used by the following multi-head attention
    layer in the decoder.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，应用前馈神经网络于每个注意力向量，将其转换为可以由解码器中的下一个多头注意力层使用的格式。
- en: Decoder block
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器块
- en: 'The decoder block is made up of three main layers: a masked multi-head attention
    layer, a multi-head attention layer, and a position-wise feed-forward network.
    The last two layers are similar to those in the encoder.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器块由三个主要层组成：一个被遮蔽的多头注意力层、一个多头注意力层和一个位置相关的前馈网络。后两个层与编码器中的层类似。
- en: 'The decoder is used during training and takes two inputs: the attention vectors
    of the input sentence being translated and the corresponding target sentences
    in English.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器在训练过程中使用，它接受两个输入：正在翻译的输入句子的注意力向量和对应的英文目标句子。
- en: '**So, what is the masked multi-head attention layer responsible for?**'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**那么，被遮蔽的多头注意力层负责什么呢？**'
- en: During the generation of the next English word, the network is allowed to use
    all the words from the French word. However, when dealing with a given word in
    the target sequence (English translation), the network only has to access the
    previous words, because making the next ones available will lead the network to
    “cheat” and not make any effort to learn properly. Here is where the masked multi-head
    attention layer has all its benefits. It masks those next words by transforming
    them into zeros so that they can’t be used by the attention network.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成下一个英文单词时，网络可以使用来自法语单词的所有单词。然而，当处理目标序列中的某个单词（英文翻译）时，网络只需访问之前的单词，因为让下一个单词可用会导致网络“作弊”，而不去真正努力学习。这就是被遮蔽的多头注意力层发挥作用的地方。它通过将这些下一个单词转换为零来遮蔽它们，以便注意力网络不能使用这些单词。
- en: The result of the masked multi-head attention layer passes through the rest
    of the layers in order to predict the next word by generating a probability score.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 被遮蔽的多头注意力层的结果通过其余的层，以生成概率分数来预测下一个单词。
- en: Transfer Learning in NLP
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理中的迁移学习
- en: 'Training deep neural networks such as transformers from scratch is not an easy
    task, and might present the following challenges: (1) finding the required amount
    of data for the target problem can be time-consuming, and (2) getting the necessary
    computation resources like GPUs to train such deep networks can be very costly.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 训练从头开始的深度神经网络（如变换器）不是一件容易的事，并且可能会面临以下挑战：（1）寻找目标问题所需的数据量可能非常耗时，以及（2）获得必要的计算资源，如
    GPU，以训练如此深的网络可能会非常昂贵。
- en: Image building a model from scratch to translate ***Mandingo language into Wolof,***
    which arebothlow resources languages***.*** Gathering data related to those languages
    is costly. Instead of going through all these challenges, one can re-use pre-trained
    deep-neural networks as the starting point for training the new model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下从头开始构建一个模型，以将***曼丁哥语翻译成沃洛夫语***，这两种语言都是低资源语言***。*** 收集与这些语言相关的数据是昂贵的。与其经历所有这些挑战，不如重新使用经过预训练的深度神经网络作为训练新模型的起点。
- en: Such models have been trained on a huge corpus of data, made available by someone
    else (moral person, organization, etc.), and evaluated to work very well on language
    translation tasks such as French to English.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型已经在一个巨大的数据语料库上进行了训练，由他人（道德人士、组织等）提供，并在语言翻译任务上（如法语到英语）评估效果非常好。
- en: But what do you mean by re-use deep-neural networks?
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但是，你所说的重新使用深度神经网络是什么意思？
- en: The re-use of the model involves choosing the pre-trained model that is similar
    to your use case, refining the input-output pair data of your target task, and
    retraining the higher layers of the pre-trained model by using your data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 重新使用模型涉及选择一个与您的用例相似的预训练模型，细化目标任务的输入输出对数据，并使用您的数据对预训练模型的高层进行再训练。
- en: 'The introduction of transformers has led to the development of state-of-the-art
    transfer learning models such as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器的引入导致了最先进的迁移学习模型的发展，如：
- en: BERT short for **B**idirectional **E**ncoder **R**epresentations from **T**ransformers
    was developed by Google researchers in 2018\. It helps to solve the most common
    language tasks such as named entity recognition, sentiment analysis, question-answering,
    text summarization, etc.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT，全称是 **B**idirectional **E**ncoder **R**epresentations from **T**ransformers，由
    Google 研究人员于 2018 年开发。它有助于解决最常见的语言任务，如命名实体识别、情感分析、问答、文本总结等。
- en: GPT3 (Generative Pre-Training-3), proposed by OpenAI researchers. It is a multi-layer
    transformer, mainly used to generate any type of text. GPT models are capable
    of producing human-like text responses to a given question.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT3（生成预训练变换器 3），由 OpenAI 研究人员提出。它是一个多层 transformer，主要用于生成各种类型的文本。GPT 模型能够生成类似人类的文本响应来回答给定的问题。
- en: An introduction to Hugging Face Transformers
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hugging Face Transformers 介绍
- en: 'Hugging Face is an AI community and Machine Learning platform created in 2016
    by Julien Chaumond, Clément Delangue, and Thomas Wolf. It aims to democratize
    NLP by providing Data Scientists, AI practitioners, and Engineers immediate access
    to over 20000 pre-trained models based on the state-of-the-art transformer architecture.
    These models can be applied to:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 是一个人工智能社区和机器学习平台，由 Julien Chaumond、Clément Delangue 和 Thomas Wolf
    于 2016 年创建。它的目标是通过为数据科学家、人工智能从业者和工程师提供对 20000 多个基于最先进的 transformer 架构的预训练模型的即时访问，来实现自然语言处理的民主化。这些模型可以应用于：
- en: Text in over 100 languages, for performing tasks such as classification, information
    extraction, question answering, generation, generation, and translation.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100 多种语言的文本，用于执行分类、信息提取、问答、生成、生成和翻译等任务。
- en: Speech, for tasks such as object audio classification and speech recognition.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音，用于对象音频分类和语音识别等任务。
- en: Vision for object detection, image classification, and segmentation.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于对象检测、图像分类和分割的视觉。
- en: 'Hugging Face transformers also provides almost 2000 data sets and layered APIs
    allowing programmers to easily interact with those models using the three most
    popular deep learning libraries: Pytorch, Tensorflow, and Jax.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face transformers 还提供了近 2000 个数据集和分层 API，使程序员可以使用三种最流行的深度学习库：Pytorch、Tensorflow
    和 Jax，轻松与这些模型进行交互。
- en: Other components of the Hugging Face transformers are the [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines).
    These are objects that abstract the complexity of the code from the library. They
    make it easy to use all these models for inference.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face transformers 的其他组件是 [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)。这些对象抽象了库中的代码复杂性，使得使用所有这些模型进行推理变得简单。
- en: Hugging Face Tutorial
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hugging Face 教程
- en: 'Now that you have a better understanding of transformers, and the Hugging Face
    platform, we will walk you through the following real-world scenarios: sequence
    classification, language translation, text generation, question answering, named
    entity recognition, and text summarization.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对 transformers 和 Hugging Face 平台有了更好的理解，我们将带你了解以下实际场景：序列分类、语言翻译、文本生成、问答、命名实体识别和文本总结。
- en: Prerequisites
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前提条件
- en: 'The first step is to install the transformers library as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是按照如下步骤安装 transformers 库：
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will be using the Internet News and Consumer Engagement data set from [Kaggle](https://www.kaggle.com/datasets/szymonjanowski/internet-articles-data-with-users-engagement).
    This data set is made freely availably by [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)
    and was created to predict the popularity of an article before its publication.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将使用来自 [Kaggle](https://www.kaggle.com/datasets/szymonjanowski/internet-articles-data-with-users-engagement)
    的互联网新闻和消费者互动数据集。该数据集由 [CC0: 公共领域](https://creativecommons.org/publicdomain/zero/1.0/)
    免费提供，旨在预测文章在发布前的受欢迎程度。'
- en: For simplicity’s sake, the tutorial will be using only three examples from the
    data, and the analysis is based on the description column.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化教程，本教程将仅使用数据中的三个示例，分析基于描述列。
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/8bd3cd5bb79bd7e0bd60b35fd7b967ea.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bd3cd5bb79bd7e0bd60b35fd7b967ea.png)'
- en: Original news in English (Image by Author)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 英文原新闻（图片由作者提供）
- en: Language Translation
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言翻译
- en: MariamMT is an efficient Machine Translation framework. It uses the [MarianNMT](https://marian-nmt.github.io/)
    engine under the hood, which is purely developed in C++ by Microsoft and many
    academic institutions such as the University of Edinburgh, and Adam Mickiewicz
    University in Poznań. The same engine is currently behind the [Microsoft Translator](http://translator.microsoft.com/)
    service.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: MarianMT是一个高效的机器翻译框架。它使用了[MarianNMT](https://marian-nmt.github.io/)引擎，该引擎完全由微软和许多学术机构（如爱丁堡大学和波兹南的亚当·密茨凯维奇大学）用C++开发。目前，同一引擎也用于[微软翻译器](http://translator.microsoft.com/)服务。
- en: The [NLP group from the University of Helsinki](https://blogs.helsinki.fi/language-technology/)
    open-sourced multiple translation models on Hugging Face Transformers and they
    are all in the following format `Helsinki-NLP/opus-mt-{src}-{tgt}`where `{src}`
    and `{tgt}`correspond respectively to the source and target languages.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[赫尔辛基大学的NLP小组](https://blogs.helsinki.fi/language-technology/)在Hugging Face
    Transformers上开源了多个翻译模型，它们的格式都是`Helsinki-NLP/opus-mt-{src}-{tgt}`，其中`{src}`和`{tgt}`分别对应源语言和目标语言。'
- en: So, in our case, the source language is English (en) and the target language
    is French (fr)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在我们的案例中，源语言是英语（en），目标语言是法语（fr）。
- en: MarianMT is one of those models previously trained using Marian on parallel
    data collected at [Opus](https://opus.nlpl.eu/).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: MarianMT是那些使用Marian在[Opus](https://opus.nlpl.eu/)上收集的平行数据上进行过训练的模型之一。
- en: 'MarianMT requires `sentencepiece` in addition to Transformers:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MarianMT除了Transformers外还需要`sentencepiece`：
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Select the pre-trained model, get the tokenizer and load the pre-trained model
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择预训练模型，获取分词器并加载预训练模型。
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Add the special token `>>{tgt}<<`in front of each source (English) text with
    the help of the following function.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下函数在每个源（英语）文本前添加特殊标记`>>{tgt}<<`。
- en: Implement the batch translation logic with the help of the following function,
    a batch being a list of texts to be translated.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下函数实现批量翻译逻辑，批量是要翻译的文本列表。
- en: '![](../Images/f9f8f7fef13b97168fc274c8e6575fca.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9f8f7fef13b97168fc274c8e6575fca.png)'
- en: Translated texts
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译文本
- en: Zero-shot classification
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本分类
- en: Most of the time, training a Machine Learning model requires all the candidate
    labels/targets to be known beforehand, meaning that if your training labels are
    **science**, **politics**, or **education**, you will not be able to predict the
    **healthcare** label unless you retrain your model, taking into consideration
    that label and the corresponding input data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，训练机器学习模型需要事先知道所有候选标签/目标，这意味着如果你的训练标签是**科学**、**政治**或**教育**，你将无法预测**医疗**标签，除非重新训练你的模型，考虑到该标签和相应的输入数据。
- en: This powerful approach makes it possible to predict the target of a text in
    about 15 languages without having seen any of the candidate labels. We can use
    this model by simply loading it from the hub.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种强大的方法使得我们可以在大约15种语言中预测文本的目标，而无需看到任何候选标签。我们只需从中心加载模型即可使用它。
- en: The goal here is to try to classify the category of each of the previous descriptions,
    whether it is ***tech****,* ***politics****,* ***security,***or***finance***.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是尝试对每个先前描述的类别进行分类，无论是***技术***、***政治***、***安全***还是***金融***。
- en: Import the pipeline module
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入管道模块
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define candidate labels. These correspond to what we want to predict: ***tech****,*
    ***politics****,* ***business,***or***finance***'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义候选标签。这些标签对应于我们想要预测的内容：***技术***、***政治***、***商业***或***金融***。
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Define the classifier with the multi-lingual option
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多语言选项定义分类器
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Implement the prediction logic using this helper function.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这个辅助函数实现预测逻辑。
- en: Run the predictions on the first and the last descriptions
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对第一个和最后一个描述进行预测
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/b623f00081e60981cd526c2504213bf0.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b623f00081e60981cd526c2504213bf0.png)'
- en: Original textual description
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文本描述
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/793ab28e5f86f849926116e2cb691e7f.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/793ab28e5f86f849926116e2cb691e7f.png)'
- en: Text predicted to be mainly about finance (Image by Author)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为主要涉及金融的文本（作者提供的图像）
- en: This previous result shows that the text is overall about finance at 81%.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果显示文本总体上是关于金融的，比例为81%。
- en: 'For the last description, we get the following result:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对最后一个描述，我们得到以下结果：
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/7bc527410c6eaba16da38414e18b7c31.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bc527410c6eaba16da38414e18b7c31.png)'
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/a8f6409eb75d5aa89b802f3ebb35ddd4.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8f6409eb75d5aa89b802f3ebb35ddd4.png)'
- en: Text predicted to be mainly about tech (Image by Author)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为主要涉及技术的文本（作者提供的图像）
- en: This previous result shows that the text is overall about tech at 95%.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的结果显示文本总体上关于技术，可信度为95%。
- en: Sentiment classification
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 情感分类
- en: Most models performing sentiment classification require proper training. The
    hugging Face pipeline module makes it easy to run sentiment analysis predictions
    by using a specific model available on the hub by specifying its name.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数进行情感分类的模型需要适当的训练。Hugging Face 的 pipeline 模块通过指定在中心可用的特定模型的名称，使情感分析预测变得简单。
- en: Load the pipeline module
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载 pipeline 模块
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Chose the task to perform and load the corresponding model. Here, we want to
    perform sentiment classification, using distill BERT base model.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择要执行的任务并加载相应的模型。在这里，我们想要执行情感分类，使用 distill BERT base 模型。
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The model is ready! Let's analyze the underlying sentiments behind the last
    two sentences.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型已经准备好了！让我们分析最后两句话背后的潜在情感。
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/b745e22c0ab4a2fdf757d446d8dc82e3.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b745e22c0ab4a2fdf757d446d8dc82e3.png)'
- en: Sentiment scores (Image by Author)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 情感评分（作者提供的图片）
- en: The model predicted the first text to have a negative sentiment with 96% of
    confidence, and the second one is predicted with positive sentiment at 52% of
    confidence.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型预测第一段文本的情感为负面，置信度为96%，第二段则预测为正面，置信度为52%。
- en: Question Answering
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问答
- en: Imagine dealing with a report much longer than the one about Apple. And, all
    you are interested in is the date of the event being mentioned. Instead of reading
    the whole report to find the key information, we can use a question-answering
    model from Hugging Face that will provide the answer we are interested in.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下处理一份比苹果公司报告长得多的报告。你唯一感兴趣的只是提到事件的日期。与其阅读整个报告来找到关键信息，我们可以使用 Hugging Face 的问答模型，它将提供我们感兴趣的答案。
- en: This can be done by providing the model with proper context (Apple’s report)and
    the question we are interested in finding the answer to.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过向模型提供适当的上下文（苹果公司的报告）以及我们感兴趣的问题来实现。
- en: Import the question-answering class and tokenizer from transformers
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 transformers 导入问答类和 tokenizer
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Instantiate the model using its name and its tokenizer.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型名称及其 tokenizer 实例化模型。
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Request the model by asking the question and specifying the context.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过提出问题并指定上下文来请求模型。
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Get the result of the model
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取模型结果
- en: '[PRE19]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/15915fd60f52c1f919d5e604f43d34c0.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15915fd60f52c1f919d5e604f43d34c0.png)'
- en: Question Answering Result (Image by Author)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 问答结果（作者提供的图片）
- en: The model answered that Apple’s event is on September 10th with high confidence
    of 97%. It even specifies where the answer is in the text by providing the starting
    and ending locations.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 模型回答说苹果公司的事件是在 9 月 10 日，置信度为97%。它甚至通过提供开始和结束位置来指定答案在文本中的位置。
- en: Conclusion
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we’ve covered the evolution of natural language technology
    from recurrent networks to transformers and how Hugging Face has democratized
    the use of NLP through its platform.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了自然语言技术从递归网络到 transformers 的演变，以及 Hugging Face 如何通过其平台使 NLP 使用变得民主化。
- en: If you are still hesitant about using transformers, we believe it is time to
    give them a try and add value to your business cases.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然犹豫是否使用 transformers，我们认为是时候尝试一下，给你的业务案例增添价值。
- en: Also, If you like reading my stories and wish to support my writing, consider
    [becoming a Medium member](https://zoumanakeita.medium.com/membership). With a
    $ 5-a-month commitment, you unlock unlimited access to stories on Medium.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢阅读我的故事并希望支持我的写作，可以考虑[成为 Medium 会员](https://zoumanakeita.medium.com/membership)。通过每月
    $5 的承诺，你可以无限制地访问 Medium 上的故事。
- en: Feel free to follow me on [Medium](https://zoumanakeita.medium.com/), [Twitter](https://twitter.com/zoumana_keita_),
    and [YouTube](https://www.youtube.com/channel/UC9xKdy8cz6ZuJU5FTNtM_pQ), or say
    Hi on [LinkedIn](https://www.linkedin.com/in/zoumana-keita/). It is always a pleasure
    to discuss AI, ML, Data Science, NLP, and MLOps stuff!
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎在[Medium](https://zoumanakeita.medium.com/)、[Twitter](https://twitter.com/zoumana_keita_)
    和[YouTube](https://www.youtube.com/channel/UC9xKdy8cz6ZuJU5FTNtM_pQ)上关注我，或者在[LinkedIn](https://www.linkedin.com/in/zoumana-keita/)上打个招呼。讨论
    AI、ML、数据科学、NLP 和 MLOps 话题总是很愉快！
- en: The article [When Should You Consider Using Datatable Instead of Pandas to Process
    Large Data?](/when-should-you-consider-using-datatable-instead-of-pandas-to-process-large-data-29a4245f67c6)
    could be a good next step.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 文章[你应该考虑使用 Datatable 而不是 Pandas 来处理大数据吗？](/when-should-you-consider-using-datatable-instead-of-pandas-to-process-large-data-29a4245f67c6)可能是一个好的下一步。
