- en: Writing a book on NLP is a bit like solving a complex data science project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¼–å†™å…³äºè‡ªç„¶è¯­è¨€å¤„ç†çš„ä¹¦ç±æœ‰ç‚¹åƒè§£å†³ä¸€ä¸ªå¤æ‚çš„æ•°æ®ç§‘å­¦é¡¹ç›®
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/writing-a-book-on-nlp-is-a-bit-like-solving-a-complex-data-science-project-c0848f975ca](https://towardsdatascience.com/writing-a-book-on-nlp-is-a-bit-like-solving-a-complex-data-science-project-c0848f975ca)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/writing-a-book-on-nlp-is-a-bit-like-solving-a-complex-data-science-project-c0848f975ca](https://towardsdatascience.com/writing-a-book-on-nlp-is-a-bit-like-solving-a-complex-data-science-project-c0848f975ca)
- en: An interview with Lewis Tunstall, co-author of the book- [**Natural Language
    Processing with Transformers**](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹åˆ˜æ˜“æ–¯Â·å¦æ–¯ç‰¹å°”çš„è®¿è°ˆï¼Œä»–æ˜¯ã€Š[**è‡ªç„¶è¯­è¨€å¤„ç†ä¸å˜å‹å™¨**](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)ã€‹ä¸€ä¹¦çš„åˆè‘—è€…
- en: '[](https://pandeyparul.medium.com/?source=post_page-----c0848f975ca--------------------------------)[![Parul
    Pandey](../Images/760b72a4feacfad6fc4224835c2e1f19.png)](https://pandeyparul.medium.com/?source=post_page-----c0848f975ca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0848f975ca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c0848f975ca--------------------------------)
    [Parul Pandey](https://pandeyparul.medium.com/?source=post_page-----c0848f975ca--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pandeyparul.medium.com/?source=post_page-----c0848f975ca--------------------------------)[![å¸•é²å°”Â·æ½˜è¿ª](../Images/760b72a4feacfad6fc4224835c2e1f19.png)](https://pandeyparul.medium.com/?source=post_page-----c0848f975ca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0848f975ca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c0848f975ca--------------------------------)
    [Parul Pandey](https://pandeyparul.medium.com/?source=post_page-----c0848f975ca--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c0848f975ca--------------------------------)
    Â·7 min readÂ·Feb 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c0848f975ca--------------------------------)
    Â·7åˆ†é’Ÿé˜…è¯»Â·2023å¹´2æœˆ6æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*A series of interviews highlighting the incredible work of writers in the
    space of data science and their path of writing.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸€ç³»åˆ—è®¿è°ˆï¼Œçªæ˜¾æ•°æ®ç§‘å­¦é¢†åŸŸä½œå®¶çš„éå‡¡å·¥ä½œåŠå…¶å†™ä½œå†ç¨‹ã€‚*'
- en: '![](../Images/5fddda751df11beee48e146f3e6a365f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fddda751df11beee48e146f3e6a365f.png)'
- en: Photo courtesy of Lewis Tunstall
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±åˆ˜æ˜“æ–¯Â·å¦æ–¯ç‰¹å°”æä¾›
- en: â€œIn fiction, the language and the senses it evokes are important, whereas in
    technical writing, the content, and the information it conveys, are important.â€
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œåœ¨å°è¯´ä¸­ï¼Œè¯­è¨€å’Œå¼•å‘çš„æ„Ÿå®˜ä½“éªŒå¾ˆé‡è¦ï¼Œè€Œåœ¨æŠ€æœ¯å†™ä½œä¸­ï¼Œå†…å®¹å’Œä¼ è¾¾çš„ä¿¡æ¯æ‰æ˜¯é‡è¦çš„ã€‚â€
- en: â€• **Krista Van Laan,** [**The Insiderâ€™s Guide to Technical Writing**](https://www.goodreads.com/work/quotes/20300600)
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€• **å…‹é‡Œæ–¯å¡”Â·èŒƒÂ·å…°**ï¼Œ[**ã€ŠæŠ€æœ¯å†™ä½œå†…å¹•æŒ‡å—ã€‹**](https://www.goodreads.com/work/quotes/20300600)
- en: '*Last edited on Feb 6, 2023*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ€åç¼–è¾‘äº2023å¹´2æœˆ6æ—¥*'
- en: Being a writer myself, I have a keen interest in uncovering the narratives behind
    the books we read, especially in the machine learning realm. These writers possess
    an uncanny ability to translate the complexities of AI into words that are both
    informative and interesting is truly remarkable. It is my goal, through a series
    of interviews, to bring their stories to the forefront and shed light on the story
    of some of the well-known authors in the field of Artificial Intelligence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€åä½œå®¶ï¼Œæˆ‘å¯¹æ­ç¤ºæˆ‘ä»¬é˜…è¯»çš„ä¹¦ç±èƒŒåçš„æ•…äº‹å……æ»¡äº†æµ“åšçš„å…´è¶£ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸã€‚è¿™äº›ä½œå®¶å…·æœ‰å°†äººå·¥æ™ºèƒ½çš„å¤æ‚æ€§è½¬åŒ–ä¸ºæ—¢ä¿¡æ¯ä¸°å¯Œåˆå¼•äººå…¥èƒœçš„æ–‡å­—çš„éå‡¡èƒ½åŠ›ï¼Œå®åœ¨æ˜¯ä»¤äººæƒŠå¹ã€‚æˆ‘çš„ç›®æ ‡æ˜¯é€šè¿‡ä¸€ç³»åˆ—è®¿è°ˆï¼ŒæŠŠä»–ä»¬çš„æ•…äº‹å±•ç°å‡ºæ¥ï¼Œæ­ç¤ºä¸€äº›åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„çŸ¥åä½œè€…çš„æ•…äº‹ã€‚
- en: 'Meet the Author: Lewis Tunstall'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®¤è¯†ä½œè€…ï¼šåˆ˜æ˜“æ–¯Â·å¦æ–¯ç‰¹å°”
- en: '**Lewis Tunstall** is an accomplished machine learning engineer currently working
    at Hugging Face. He has extensive experience in building machine learning applications
    for startups and enterprises, with a focus on the areas of NLP, topological data
    analysis, and time series. With a PhD in theoretical physics, Lewis has had the
    opportunity to hold research positions in various countries, including Australia,
    the USA, and Switzerland. His current work focuses on developing innovative tools
    for the NLP community and empowering individuals with the knowledge and skills
    to use them effectively.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**åˆ˜æ˜“æ–¯Â·å¦æ–¯ç‰¹å°”** æ˜¯ä¸€ä½æ°å‡ºçš„æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œç›®å‰åœ¨Hugging Faceå·¥ä½œã€‚ä»–åœ¨ä¸ºåˆåˆ›ä¼ä¸šå’Œå¤§å‹ä¼ä¸šæ„å»ºæœºå™¨å­¦ä¹ åº”ç”¨æ–¹é¢æ‹¥æœ‰ä¸°å¯Œçš„ç»éªŒï¼Œä¸“æ³¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€æ‹“æ‰‘æ•°æ®åˆ†æå’Œæ—¶é—´åºåˆ—ç­‰é¢†åŸŸã€‚æ‹¥æœ‰ç†è®ºç‰©ç†å­¦åšå£«å­¦ä½çš„åˆ˜æ˜“æ–¯æ›¾æœ‰æœºä¼šåœ¨æ¾³å¤§åˆ©äºšã€ç¾å›½å’Œç‘å£«ç­‰å¤šä¸ªå›½å®¶æ‹…ä»»ç ”ç©¶èŒä½ã€‚ä»–ç›®å‰çš„å·¥ä½œé›†ä¸­åœ¨ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ç¤¾åŒºå¼€å‘åˆ›æ–°å·¥å…·ï¼Œå¹¶èµ‹äºˆä¸ªäººæœ‰æ•ˆä½¿ç”¨è¿™äº›å·¥å…·çš„çŸ¥è¯†å’ŒæŠ€èƒ½ã€‚'
- en: Lewis is the co-author of the book -â€ Natural Language Processing with Transformersâ€
    along with [**Leandro von Werra**](https://twitter.com/lvwerra?s=20&t=XOZTW3iOxNZoAopPR5LRsw)and[**Thomas
    Wolf**](https://twitter.com/Thom_Wolf)**.** The book is a comprehensive guide
    to the latest advancements in the field of NLP and is a great resource for anyone
    looking to gain a deeper understanding of NLP and how it can be applied to real-world
    problems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ˜æ˜“æ–¯æ˜¯ä¹¦ç±ã€Šè‡ªç„¶è¯­è¨€å¤„ç†ä¸ Transformersã€‹çš„åˆè‘—è€…ä¹‹ä¸€ï¼Œä¸[**Leandro von Werra**](https://twitter.com/lvwerra?s=20&t=XOZTW3iOxNZoAopPR5LRsw)å’Œ[**Thomas
    Wolf**](https://twitter.com/Thom_Wolf)**åˆä½œ**ã€‚è¿™æœ¬ä¹¦æ˜¯è¯¥é¢†åŸŸæœ€æ–°è¿›å±•çš„å…¨é¢æŒ‡å—ï¼Œæ˜¯ä»»ä½•å¸Œæœ›æ·±å…¥ç†è§£è‡ªç„¶è¯­è¨€å¤„ç†ä»¥åŠå¦‚ä½•å°†å…¶åº”ç”¨äºç°å®ä¸–ç•Œé—®é¢˜çš„äººçš„ç»ä½³èµ„æºã€‚
- en: '[](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/?source=post_page-----c0848f975ca--------------------------------)
    [## Natural Language Processing with Transformers, Revised Edition'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/?source=post_page-----c0848f975ca--------------------------------)
    [## è‡ªç„¶è¯­è¨€å¤„ç†ä¸ Transformers, ä¿®è®¢ç‰ˆ'
- en: Since their introduction in 2017, transformers have quickly become the dominant
    architecture for achievingâ€¦
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ª2017å¹´å¼•å…¥ä»¥æ¥ï¼Œtransformers å·²è¿…é€Ÿæˆä¸ºå®ç°...
- en: learning.oreilly.com](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/?source=post_page-----c0848f975ca--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[learning.oreilly.com](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/?source=post_page-----c0848f975ca--------------------------------)'
- en: '**Q: How did the idea of this book originate?**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®: è¿™æœ¬ä¹¦çš„æ„æ€æ˜¯å¦‚ä½•äº§ç”Ÿçš„ï¼Ÿ**'
- en: '*Lewis:* Although we began the book in 2020, its origin story really began
    in 2019 when Leandro and I first started working with Transformer models. At the
    time, Jay Alammarâ€™s amazing [blog posts](https://jalammar.github.io/) and [The
    Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
    by Sasha Rush were among the few written resources available to understand how
    these models work. These articles were (and are!) great for developing understanding,
    but we felt there was a gap in guiding people on how to apply Transformers to
    industrial use cases. So in 2020, we had the somewhat foolhardy idea to combine
    the knowledge weâ€™d learned from our jobs as a book. My wife suggested that we
    contact Thomas to see if heâ€™d be interested in being a co-author, and to our great
    surprise, he agreed!'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*åˆ˜æ˜“æ–¯:* å°½ç®¡æˆ‘ä»¬åœ¨2020å¹´å¼€å§‹äº†è¿™æœ¬ä¹¦ï¼Œä½†å®ƒçš„èµ·æºæ•…äº‹å®é™…ä¸Šå¯ä»¥è¿½æº¯åˆ°2019å¹´ï¼Œå½“æ—¶Leandroå’Œæˆ‘é¦–æ¬¡å¼€å§‹ä½¿ç”¨ Transformer
    æ¨¡å‹ã€‚å½“æ—¶ï¼ŒJay Alammar çš„ç²¾å½©[åšå®¢æ–‡ç« ](https://jalammar.github.io/)å’Œ Sasha Rush çš„[ã€Šæ³¨é‡Š Transformerã€‹](http://nlp.seas.harvard.edu/2018/04/03/attention.html)æ˜¯ç†è§£è¿™äº›æ¨¡å‹å¦‚ä½•å·¥ä½œçš„ä¸ºæ•°ä¸å¤šçš„ä¹¦é¢èµ„æºã€‚è¿™äº›æ–‡ç« ï¼ˆå¹¶ä¸”ä»ç„¶æ˜¯ï¼ï¼‰éå¸¸æœ‰åŠ©äºç†è§£ï¼Œä½†æˆ‘ä»¬è§‰å¾—åœ¨æŒ‡å¯¼äººä»¬å¦‚ä½•å°†
    Transformer åº”ç”¨äºå·¥ä¸šç”¨ä¾‹æ–¹é¢å­˜åœ¨ä¸€ä¸ªç©ºç™½ã€‚å› æ­¤ï¼Œåœ¨2020å¹´ï¼Œæˆ‘ä»¬æœ‰äº†ä¸€ä¸ªæœ‰äº›å†’å¤±çš„æƒ³æ³•ï¼Œå°†æˆ‘ä»¬ä»å·¥ä½œä¸­å­¦åˆ°çš„çŸ¥è¯†ç»“åˆæˆä¸€æœ¬ä¹¦ã€‚æˆ‘çš„å¦»å­å»ºè®®æˆ‘ä»¬è”ç³»
    Thomas çœ‹ä»–æ˜¯å¦æœ‰å…´è¶£æˆä¸ºåˆè‘—è€…ï¼Œä»¤æˆ‘ä»¬æƒŠè®¶çš„æ˜¯ï¼Œä»–åŒæ„äº†ï¼'
- en: '**Q: Could you summarize the main points covered in the book for the readers?**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®: èƒ½å¦ä¸ºè¯»è€…æ€»ç»“ä¸€ä¸‹ä¹¦ä¸­æ¶µç›–çš„ä¸»è¦å†…å®¹ï¼Ÿ**'
- en: '*Lewis:* As you might expect from the title, this book is about applying Transformer
    models to NLP tasks. Most chapters are structured around a single use case youâ€™re
    likely to encounter in the industry. The book covers core applications such as
    text classification, named entity recognition, and question answering. We take
    a lot of inspiration from the fantastic [fast.ai](http://fast.ai/) course (which
    is how I got started with deep learning!), so the book is written in a hands-on
    style, emphasising solving real-world problems with code. In the early chapters,
    we introduce the concepts of self-attention and transfer learning, which underpins
    the success of Transformers.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*åˆ˜æ˜“æ–¯:* æ­£å¦‚æ ‡é¢˜æ‰€ç¤ºï¼Œè¿™æœ¬ä¹¦æ˜¯å…³äºå°† Transformer æ¨¡å‹åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„ã€‚å¤§å¤šæ•°ç« èŠ‚å›´ç»•ç€ä½ åœ¨è¡Œä¸šä¸­å¯èƒ½é‡åˆ°çš„å•ä¸€ç”¨ä¾‹è¿›è¡Œç»“æ„åŒ–ã€‚ä¹¦ä¸­æ¶µç›–äº†æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«å’Œé—®ç­”ç­‰æ ¸å¿ƒåº”ç”¨ã€‚æˆ‘ä»¬ä»ç²¾å½©çš„[fast.ai](http://fast.ai/)è¯¾ç¨‹ä¸­æ±²å–äº†å¾ˆå¤šçµæ„Ÿï¼ˆè¿™ä¹Ÿæ˜¯æˆ‘å¼€å§‹æ·±åº¦å­¦ä¹ çš„é€”å¾„ï¼ï¼‰ï¼Œå› æ­¤æœ¬ä¹¦é‡‡ç”¨äº†åŠ¨æ‰‹å®è·µçš„é£æ ¼ï¼Œå¼ºè°ƒé€šè¿‡ä»£ç è§£å†³ç°å®ä¸–ç•Œçš„é—®é¢˜ã€‚åœ¨æ—©æœŸç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œè¿ç§»å­¦ä¹ çš„æ¦‚å¿µï¼Œè¿™äº›æ¦‚å¿µæ˜¯
    Transformer æˆåŠŸçš„åŸºç¡€ã€‚'
- en: The main advice Iâ€™d suggest to new writers is to find co-authors or colleagues
    who can deeply critique your ideas and writing.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘å¯¹æ–°ä½œè€…çš„ä¸»è¦å»ºè®®æ˜¯æ‰¾åˆ°å¯ä»¥æ·±åº¦æ‰¹è¯„ä½ çš„æƒ³æ³•å’Œå†™ä½œçš„åˆè‘—è€…æˆ–åŒäº‹ã€‚
- en: The latter part of the book dives into more advanced topics, such as optimising
    Transformers for production environments and handling scenarios where you have
    little labelled data (i.e. every data scientistâ€™s nightmare ğŸ˜ƒ. One of my favourite
    chapters is about training a GPT-2 scale model from scratch, including how to
    create a large-scale corpus and train on distributed infrastructure! The book
    concludes with an eye towards the future by highlighting some of the exciting
    recent developments involving Transformers and other modalities like images and
    audio.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹¦çš„ååŠéƒ¨åˆ†æ·±å…¥æ¢è®¨äº†æ›´é«˜çº§çš„ä¸»é¢˜ï¼Œå¦‚ä¸ºç”Ÿäº§ç¯å¢ƒä¼˜åŒ– Transformers å’Œå¤„ç†æ ‡æ³¨æ•°æ®ç¨€å°‘çš„åœºæ™¯ï¼ˆå³æ¯ä¸ªæ•°æ®ç§‘å­¦å®¶çš„å™©æ¢¦ ğŸ˜ƒï¼‰ã€‚æˆ‘æœ€å–œæ¬¢çš„ç« èŠ‚ä¹‹ä¸€æ˜¯å…³äºä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ª
    GPT-2 è§„æ¨¡æ¨¡å‹ï¼ŒåŒ…æ‹¬å¦‚ä½•åˆ›å»ºä¸€ä¸ªå¤§è§„æ¨¡è¯­æ–™åº“å¹¶åœ¨åˆ†å¸ƒå¼åŸºç¡€è®¾æ–½ä¸Šè¿›è¡Œè®­ç»ƒï¼ä¹¦çš„ç»“å°¾ç€çœ¼äºæœªæ¥ï¼Œé€šè¿‡çªå‡ºä¸€äº›æ¶‰åŠ Transformers å’Œå…¶ä»–æ¨¡å¼ï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰çš„æ¿€åŠ¨äººå¿ƒçš„æœ€æ–°è¿›å±•æ¥ä½œç»“ã€‚
- en: '**Q: You co-authored the book with Leandro von Werra and Thomas Wolf. How is
    it to author a book with multiple writers?**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®ï¼šä½ ä¸ Leandro von Werra å’Œ Thomas Wolf å…±åŒç¼–å†™äº†è¿™æœ¬ä¹¦ã€‚ä¸å¤šä½ä½œè€…ä¸€èµ·ç¼–å†™ä¹¦ç±æ˜¯ä»€ä¹ˆæ„Ÿè§‰ï¼Ÿ**'
- en: Writing a book on NLP is a bit like solving a complex data science project.
    Among various challenges, you need to design the use case and story for each chapter,
    find appropriate data and models, and make sure that you can keep code complexity
    to a minimum because reading long blocks of code is no fun at all. And just like
    any data science project, some chapters involved running dozens of experiments.
    For all these challenges, I found writing the book with Leandro and Thomas to
    be an extremely valuable experience! In particular, having co-authors with whom
    you can brainstorm ideas or sanity-check your code was especially helpful. Of
    course, one challenge that arises with multiple authors is trying to keep the
    same writing style throughout the book, and we were lucky to have great editors
    at Oâ€™Reilly to help us achieve this.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å†™ä¸€æœ¬å…³äº NLP çš„ä¹¦æœ‰ç‚¹åƒè§£å†³ä¸€ä¸ªå¤æ‚çš„æ•°æ®ç§‘å­¦é¡¹ç›®ã€‚åœ¨å„ç§æŒ‘æˆ˜ä¸­ï¼Œä½ éœ€è¦ä¸ºæ¯ä¸ªç« èŠ‚è®¾è®¡ç”¨ä¾‹å’Œæ•…äº‹ï¼Œæ‰¾åˆ°åˆé€‚çš„æ•°æ®å’Œæ¨¡å‹ï¼Œå¹¶ç¡®ä¿ä»£ç å¤æ‚æ€§ä¿æŒåœ¨æœ€ä½é™åº¦ï¼Œå› ä¸ºé˜…è¯»é•¿å—ä»£ç ä¸€ç‚¹ä¹Ÿä¸æœ‰è¶£ã€‚è€Œä¸”å°±åƒä»»ä½•æ•°æ®ç§‘å­¦é¡¹ç›®ä¸€æ ·ï¼Œä¸€äº›ç« èŠ‚æ¶‰åŠåˆ°è¿›è¡Œå‡ åæ¬¡å®éªŒã€‚é¢å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘å‘ç°ä¸
    Leandro å’Œ Thomas ä¸€èµ·å†™ä¹¦æ˜¯ä¸€ä¸ªæå…¶å®è´µçš„ç»å†ï¼ç‰¹åˆ«æ˜¯ï¼Œä¸èƒ½å¤Ÿå¤´è„‘é£æš´æˆ–æ£€æŸ¥ä»£ç çš„åˆè‘—è€…ä¸€èµ·å·¥ä½œå°¤å…¶æœ‰å¸®åŠ©ã€‚å½“ç„¶ï¼Œå¤šä½ä½œè€…é¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ä¿æŒå…¨ä¹¦çš„å†™ä½œé£æ ¼ä¸€è‡´ï¼Œæˆ‘ä»¬å¾ˆå¹¸è¿èƒ½æœ‰
    Oâ€™Reilly çš„ä¼˜ç§€ç¼–è¾‘å¸®åŠ©æˆ‘ä»¬å®ç°è¿™ä¸€ç‚¹ã€‚
- en: '**Q: Who do you think is the target audience for the book?**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®ï¼šä½ è®¤ä¸ºè¿™æœ¬ä¹¦çš„ç›®æ ‡è¯»è€…æ˜¯è°ï¼Ÿ**'
- en: We wrote this book for data scientists and machine learning/software engineers
    who may have heard about the recent breakthroughs involving Transformers but still
    need an in-depth guide to help them adapt these models to their own use cases.
    In other words, people like myself about 1â€“2 years ago! We envision our book will
    be most helpful to industry practitioners or those hoping to break into NLP from
    a nearby field (e.g. a software engineer who wants to build machine learning-powered
    applications).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å†™è¿™æœ¬ä¹¦æ˜¯ä¸ºäº†æ•°æ®ç§‘å­¦å®¶å’Œæœºå™¨å­¦ä¹ /è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œä»–ä»¬å¯èƒ½å¬è¯´è¿‡æœ€è¿‘æ¶‰åŠ Transformers çš„çªç ´ï¼Œä½†ä»éœ€è¦ä¸€ä¸ªæ·±å…¥çš„æŒ‡å—æ¥å¸®åŠ©ä»–ä»¬å°†è¿™äº›æ¨¡å‹é€‚åº”åˆ°è‡ªå·±çš„ç”¨ä¾‹ä¸­ã€‚æ¢å¥è¯è¯´ï¼Œå°±æ˜¯åƒæˆ‘è‡ªå·±å¤§çº¦1-2å¹´å‰ä¸€æ ·çš„äººï¼æˆ‘ä»¬è®¾æƒ³è¿™æœ¬ä¹¦å¯¹è¡Œä¸šä»ä¸šè€…æˆ–å¸Œæœ›ä»ç›¸å…³é¢†åŸŸï¼ˆä¾‹å¦‚ï¼Œæƒ³è¦æ„å»ºæœºå™¨å­¦ä¹ é©±åŠ¨åº”ç”¨çš„å·¥ç¨‹å¸ˆï¼‰è½¬å‘è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„äººä¼šæœ€æœ‰å¸®åŠ©ã€‚
- en: '**Q: What, according to you, is the best way to make the most out of this book-
    read first and code later or code along?**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®ï¼šä½ è®¤ä¸ºæ€æ ·æ‰èƒ½æœ€å¤§åŒ–åˆ©ç”¨è¿™æœ¬ä¹¦â€”â€”å…ˆè¯»åå†™ä»£ç ï¼Œè¿˜æ˜¯è¾¹è¯»è¾¹å†™ä»£ç ï¼Ÿ**'
- en: We actually wrote the whole book using Jupyter notebooks and a tool called [fastdoc](https://github.com/fastai/fastdoc),
    so every line of code can be executed in the accompanying notebooks that we provide
    on [GitHub](https://github.com/nlp-with-transformers/notebooks). I suggest having
    the book chapter and accompanying code side-by-side, so you can quickly experiment
    with the inputs and outputs while youâ€™re reading. We are also planning some live
    events around the bookâ€™s release, so stay tuned on the [bookâ€™s website](https://nlp-with-transformers.github.io/website/)
    to find out when these events will happen.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®é™…ä¸Šä½¿ç”¨ Jupyter notebooks å’Œä¸€ä¸ªåä¸º [fastdoc](https://github.com/fastai/fastdoc)
    çš„å·¥å…·å†™äº†æ•´æœ¬ä¹¦ï¼Œå› æ­¤æ¯ä¸€è¡Œä»£ç éƒ½å¯ä»¥åœ¨æˆ‘ä»¬æä¾›çš„ [GitHub](https://github.com/nlp-with-transformers/notebooks)
    é™„å¸¦çš„ notebooks ä¸­æ‰§è¡Œã€‚æˆ‘å»ºè®®å°†ä¹¦çš„ç« èŠ‚å’Œé™„å¸¦çš„ä»£ç æ”¾åœ¨ä¸€èµ·ï¼Œè¿™æ ·ä½ å¯ä»¥åœ¨é˜…è¯»æ—¶å¿«é€Ÿå®éªŒè¾“å…¥å’Œè¾“å‡ºã€‚æˆ‘ä»¬è¿˜è®¡åˆ’åœ¨ä¹¦å‘å¸ƒæ—¶ä¸¾åŠä¸€äº›ç›´æ’­æ´»åŠ¨ï¼Œè¯·å…³æ³¨
    [ä¹¦ç±å®˜ç½‘](https://nlp-with-transformers.github.io/website/) ä»¥äº†è§£è¿™äº›æ´»åŠ¨çš„æ—¶é—´ã€‚
- en: '**Q: What advice would you give a new writer, someone just starting?**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®ï¼šä½ ä¼šç»™åˆšå¼€å§‹å†™ä½œçš„æ–°ä½œè€…ä»€ä¹ˆå»ºè®®ï¼Ÿ**'
- en: Hehe, my first piece of advice would be to ask whether youâ€™re really sure you
    want to write a book. My second piece of advice would be donâ€™t write a book just
    after youâ€™ve had a baby â€¦ in the middle of a pandemic ğŸ˜ƒ
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å‘µå‘µï¼Œæˆ‘çš„ç¬¬ä¸€æ¡å»ºè®®æ˜¯é—®è‡ªå·±æ˜¯å¦çœŸçš„ç¡®å®šæƒ³å†™ä¸€æœ¬ä¹¦ã€‚æˆ‘çš„ç¬¬äºŒæ¡å»ºè®®æ˜¯ä¸è¦åœ¨åˆšåˆšæœ‰äº†å®å®ä¹‹åâ€¦â€¦åœ¨ç–«æƒ…æœŸé—´å†™ä¹¦ ğŸ˜ƒ
- en: Writing a book on NLP is a bit like solving a complex data science project.
    Among various challenges, you need to design the use case and story for each chapter,
    find appropriate data and models, and make sure that you can keep code complexity
    to a minimum because reading long blocks of code is no fun at all.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å†™ä¸€æœ¬å…³äºNLPçš„ä¹¦æœ‰ç‚¹åƒè§£å†³ä¸€ä¸ªå¤æ‚çš„æ•°æ®ç§‘å­¦é¡¹ç›®ã€‚åœ¨å„ç§æŒ‘æˆ˜ä¸­ï¼Œä½ éœ€è¦ä¸ºæ¯ä¸€ç« è®¾è®¡ç”¨ä¾‹å’Œæ•…äº‹ï¼Œæ‰¾åˆ°åˆé€‚çš„æ•°æ®å’Œæ¨¡å‹ï¼Œå¹¶ç¡®ä¿ä»£ç å¤æ‚æ€§ä¿æŒåœ¨æœ€ä½ï¼Œå› ä¸ºé˜…è¯»é•¿ç¯‡ä»£ç ä¸€ç‚¹ä¹Ÿä¸æœ‰è¶£ã€‚
- en: Jokes aside, the main advice Iâ€™d suggest to new writers is to find co-authors
    or colleagues who can deeply critique your ideas and writing. We were fortunate
    to have experienced writers like AurÃ©lien Geron and Hamel Husain provide us with
    detailed comments on our drafts, and their feedback significantly improved the
    book. This type of feedback is invaluable because, as a writer, you sometimes
    forget which aspects were challenging to master the first time you learned a subject.
    In a curious twist of fate, AurÃ©lienâ€™s written the foreword to our book!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´æ­£ç»çš„ï¼Œæˆ‘ç»™æ–°ä½œè€…çš„ä¸»è¦å»ºè®®æ˜¯æ‰¾åˆ°å¯ä»¥æ·±å…¥æ‰¹è¯„ä½ æƒ³æ³•å’Œå†™ä½œçš„åˆè‘—è€…æˆ–åŒäº‹ã€‚æˆ‘ä»¬å¾ˆå¹¸è¿åœ°æœ‰åƒAurÃ©lien Geronå’ŒHamel Husainè¿™æ ·çš„ç»éªŒä¸°å¯Œçš„ä½œè€…æä¾›äº†è¯¦ç»†çš„è¯„è®ºï¼Œä»–ä»¬çš„åé¦ˆæ˜¾è‘—æé«˜äº†ä¹¦ç±è´¨é‡ã€‚è¿™ç§åé¦ˆéå¸¸å®è´µï¼Œå› ä¸ºä½œä¸ºä¸€ä¸ªä½œè€…ï¼Œä½ æœ‰æ—¶ä¼šå¿˜è®°æœ€åˆå­¦ä¹ ä¸€ä¸ªä¸»é¢˜æ—¶å“ªäº›æ–¹é¢å¾ˆéš¾æŒæ¡ã€‚å·§åˆçš„æ˜¯ï¼ŒAurÃ©lienä¸ºæˆ‘ä»¬çš„ä¹¦å†™äº†å‰è¨€ï¼
- en: '**Q: Who is your favourite book and author (in technical or non-technical space)?**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®ï¼šä½ æœ€å–œæ¬¢çš„ä¹¦å’Œä½œè€…æ˜¯è°ï¼ˆæŠ€æœ¯æˆ–éæŠ€æœ¯é¢†åŸŸï¼‰ï¼Ÿ**'
- en: Ooh, this is a tricky question! Iâ€™m an avid reader of science fiction, and [**The
    Three-Body Problem**](https://en.wikipedia.org/wiki/The_Three-Body_Problem_(novel))
    **by Liu Cixin** is one of my favourite series because it provides a really novel
    solution to the **Fermi paradox** â€” i.e. why is there no observable evidence of
    alien civilisations, given the large number of stars with Earth-like planets?
    I also enjoy reading books and biographies about the history of scientific fields
    like physics and computer science. One recent favourite of mine is [**The Man
    from the Future** by Ananyo Bhattacharya](https://www.amazon.in/Man-Future-Visionary-Life-Neumann/dp/0241398851),
    which is about the life of the extraordinary John von Neumann. Although I had
    encountered some of von Neumannâ€™s work as a physics student (he wrote a masterpiece
    on quantum mechanics), I was not fully aware of how vast his contributions were
    to mathematics and science at large. The book does a wonderful job of describing
    his work and the fascinating history around it â€” highly recommended!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ï¼Œè¿™æ˜¯ä¸ªæ£˜æ‰‹çš„é—®é¢˜ï¼æˆ‘æ˜¯ç§‘å¹»å°è¯´çš„å¿ å®è¯»è€…ï¼Œ[**ã€Šä¸‰ä½“ã€‹**](https://en.wikipedia.org/wiki/The_Three-Body_Problem_(novel))
    **ç”±åˆ˜æ…ˆæ¬£** ç¼–å†™ï¼Œæ˜¯æˆ‘æœ€å–œæ¬¢çš„ç³»åˆ—ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒæä¾›äº†å¯¹**è´¹ç±³æ‚–è®º**çš„ä¸€ä¸ªå…¨æ–°è§£å†³æ–¹æ¡ˆâ€”â€”å³ä¸ºä»€ä¹ˆæ²¡æœ‰å¯è§‚å¯Ÿåˆ°çš„å¤–æ˜Ÿæ–‡æ˜è¯æ®ï¼Œå°½ç®¡æœ‰å¤§é‡ç±»åœ°è¡Œæ˜Ÿçš„æ’æ˜Ÿï¼Ÿæˆ‘ä¹Ÿå–œæ¬¢é˜…è¯»å…³äºç‰©ç†å­¦å’Œè®¡ç®—æœºç§‘å­¦ç­‰ç§‘å­¦é¢†åŸŸå†å²çš„ä¹¦ç±å’Œä¼ è®°ã€‚æˆ‘æœ€è¿‘çš„ä¸€ä¸ªæœ€çˆ±æ˜¯
    [**ã€Šæœªæ¥ä¹‹äººã€‹ç”±Ananyo Bhattacharyaç¼–å†™**](https://www.amazon.in/Man-Future-Visionary-Life-Neumann/dp/0241398851)ï¼Œè¿™æœ¬ä¹¦è®²è¿°äº†æ°å‡ºçš„çº¦ç¿°Â·å†¯Â·è¯ºä¾æ›¼çš„ç”Ÿæ´»ã€‚è™½ç„¶ä½œä¸ºç‰©ç†å­¦å­¦ç”Ÿæˆ‘é‡åˆ°è¿‡å†¯Â·è¯ºä¾æ›¼çš„ä¸€äº›å·¥ä½œï¼ˆä»–å†™äº†å…³äºé‡å­åŠ›å­¦çš„æ°ä½œï¼‰ï¼Œä½†æˆ‘æ²¡æœ‰å®Œå…¨æ„è¯†åˆ°ä»–å¯¹æ•°å­¦å’Œç§‘å­¦é¢†åŸŸçš„å¹¿æ³›è´¡çŒ®ã€‚è¿™æœ¬ä¹¦éå¸¸å‡ºè‰²åœ°æè¿°äº†ä»–çš„å·¥ä½œåŠå…¶è¿·äººçš„å†å²â€”â€”å¼ºçƒˆæ¨èï¼
- en: ğŸ‘‰ **Are you looking forward to connecting with Lewis? Follow him on** [**Twitter**](https://twitter.com/_lewtun)**.**
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘‰ **ä½ æƒ³ä¸Lewiså»ºç«‹è”ç³»å—ï¼Ÿè¯·å…³æ³¨ä»–çš„** [**Twitter**](https://twitter.com/_lewtun)**.**
- en: ğŸ‘‰ **Read other interviews in this series:**
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘‰ **é˜…è¯»æœ¬ç³»åˆ—çš„å…¶ä»–è®¿è°ˆï¼š**
- en: '[](/dont-just-take-notes-turn-them-into-articles-and-share-them-with-others-72aa43b83e29?source=post_page-----c0848f975ca--------------------------------)
    [## Donâ€™t just take notes â€” turn them into articles and share them with others'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/dont-just-take-notes-turn-them-into-articles-and-share-them-with-others-72aa43b83e29?source=post_page-----c0848f975ca--------------------------------)
    [## ä¸è¦åªæ˜¯åšç¬”è®° â€” æŠŠå®ƒä»¬å˜æˆæ–‡ç« å¹¶ä¸ä»–äººåˆ†äº«'
- en: An interview with Alexey Grigorev, author of the book- Machine Learning Bookcamp.
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸€æ¬¡ä¸Alexey Grigorevçš„è®¿è°ˆï¼Œä»–æ˜¯ã€Šæœºå™¨å­¦ä¹ ä¹¦è¥ã€‹çš„ä½œè€…ã€‚
- en: towardsdatascience.com](/dont-just-take-notes-turn-them-into-articles-and-share-them-with-others-72aa43b83e29?source=post_page-----c0848f975ca--------------------------------)
    [](/you-do-not-become-better-by-employing-fancy-techniques-but-by-working-on-the-fundamentals-17d5c471c69c?source=post_page-----c0848f975ca--------------------------------)
    [## You do not become better by employing fancy techniques but by working on the
    fundamentals
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä¸è¦åªæ˜¯åšç¬”è®°ï¼ŒæŠŠå®ƒä»¬è½¬åŒ–ä¸ºæ–‡ç« å¹¶ä¸ä»–äººåˆ†äº«](https://towardsdatascience.com/dont-just-take-notes-turn-them-into-articles-and-share-them-with-others-72aa43b83e29?source=post_page-----c0848f975ca--------------------------------)
    [](/you-do-not-become-better-by-employing-fancy-techniques-but-by-working-on-the-fundamentals-17d5c471c69c?source=post_page-----c0848f975ca--------------------------------)
    [## ä½ å¹¶ä¸ä¼šé€šè¿‡ä½¿ç”¨èŠ±å“¨çš„æŠ€å·§å˜å¾—æ›´å¥½ï¼Œè€Œæ˜¯é€šè¿‡ä¸“æ³¨äºåŸºç¡€'
- en: 'An interview with Radek Osmulski, author of the book- Meta Learning: How To
    Learn Deep Learning And Thrive In Theâ€¦'
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'ä¸Radek Osmulskiçš„è®¿è°ˆï¼Œä»–æ˜¯ã€ŠMeta Learning: How To Learn Deep Learning And Thrive
    In Theâ€¦ã€‹ä¸€ä¹¦çš„ä½œè€…'
- en: towardsdatascience.com](/you-do-not-become-better-by-employing-fancy-techniques-but-by-working-on-the-fundamentals-17d5c471c69c?source=post_page-----c0848f975ca--------------------------------)
    [](/publishing-is-powerful-as-it-serves-as-a-catalyst-for-scope-and-writing-decisions-713306e8a0d?source=post_page-----c0848f975ca--------------------------------)
    [## Publishing Is Powerful as It Serves as a Catalyst for Scope and Writing Decisions
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä½ å¹¶ä¸ä¼šé€šè¿‡ä½¿ç”¨èŠ±å“¨çš„æŠ€å·§å˜å¾—æ›´å¥½ï¼Œè€Œæ˜¯é€šè¿‡ä¸“æ³¨äºåŸºç¡€](https://towardsdatascience.com/you-do-not-become-better-by-employing-fancy-techniques-but-by-working-on-the-fundamentals-17d5c471c69c?source=post_page-----c0848f975ca--------------------------------)
    [](/publishing-is-powerful-as-it-serves-as-a-catalyst-for-scope-and-writing-decisions-713306e8a0d?source=post_page-----c0848f975ca--------------------------------)
    [## å‘å¸ƒæ˜¯å¼ºå¤§çš„ï¼Œå› ä¸ºå®ƒä½œä¸ºèŒƒå›´å’Œå†™ä½œå†³ç­–çš„å‚¬åŒ–å‰‚'
- en: An interview with Christoph Molnar, author of the book- Interpretable Machine
    Learning
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸Christoph Molnarçš„è®¿è°ˆï¼Œä»–æ˜¯ã€ŠInterpretable Machine Learningã€‹ä¸€ä¹¦çš„ä½œè€…
- en: towardsdatascience.com](/publishing-is-powerful-as-it-serves-as-a-catalyst-for-scope-and-writing-decisions-713306e8a0d?source=post_page-----c0848f975ca--------------------------------)
    ![](../Images/90c29813c4ef86e9865b1cc09aca3796.png)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[å‘å¸ƒæ˜¯å¼ºå¤§çš„ï¼Œå› ä¸ºå®ƒä½œä¸ºèŒƒå›´å’Œå†™ä½œå†³ç­–çš„å‚¬åŒ–å‰‚](https://towardsdatascience.com/publishing-is-powerful-as-it-serves-as-a-catalyst-for-scope-and-writing-decisions-713306e8a0d?source=post_page-----c0848f975ca--------------------------------)
    ![](../Images/90c29813c4ef86e9865b1cc09aca3796.png)'
