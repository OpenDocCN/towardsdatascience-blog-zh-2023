- en: Advanced Data Preparation Using Custom Transformers in Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/advanced-data-preparation-using-custom-transformers-in-scikit-learn-5e58d2713ac5](https://towardsdatascience.com/advanced-data-preparation-using-custom-transformers-in-scikit-learn-5e58d2713ac5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Go beyond “beginner mode” and take full advantage of scikit-learn’s more powerful
    capabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mattchapmanmsc?source=post_page-----5e58d2713ac5--------------------------------)[![Matt
    Chapman](../Images/7511deb8d9ed408ece21031f6614c532.png)](https://medium.com/@mattchapmanmsc?source=post_page-----5e58d2713ac5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5e58d2713ac5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5e58d2713ac5--------------------------------)
    [Matt Chapman](https://medium.com/@mattchapmanmsc?source=post_page-----5e58d2713ac5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5e58d2713ac5--------------------------------)
    ·10 min read·Jun 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d8d8b06eb619180898fe4e29f587be9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Daniel K Cheung](https://unsplash.com/@danielkcheung) on [Unsplash](https://unsplash.com/photos/cPF2nlWcMY4)
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-Learn provides many useful tools for data preparation, but sometimes
    the pre-built options aren’t enough.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I’ll show you how to create advanced data preparation workflows
    using custom Transformers. If you’ve been using scikit-learn for a while and want
    to level-up your skills, learning about Transformers is an excellent way to advance
    beyond “beginner mode” and learn about some of the more advanced capabilities
    required in modern Data Science teams.
  prefs: []
  type: TYPE_NORMAL
- en: If the topic sounds a bit advanced, don’t worry — this article is packed full
    of examples which will help you feel confident with both the code and the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll start with a brief overview of scikit-learn’s `Transformer` class and
    then walk through two ways to build customised Transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a `FunctionTransformer`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Writing a custom `Transformer` from scratch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Transformers: The best way to preprocess data in scikit-learn'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Transformer` is one of the central building blocks of scikit-learn. It’s
    so foundational, in fact, that chances are you’ve already been using one without
    even realising.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, a `Transformer` is any object with the `fit()` and `transform()`
    methods. In plain English, that means a Transformer is a class (i.e. a reusable
    chunk of code) that takes your raw dataset as an input and returns a transformed
    version of that dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7dc632c664aee56d6b737766b6bc2db0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, scikit-learn `Transformers` are NOT the same as the “transformers”
    used in Large Language Models (LLMs) like BERT and GPT-4, or the models available
    through the HuggingFace `transformers` library. In the context of LLMs, a “transformer”
    (lower-case ‘t’) is a deep learning model; a scikit-learn `Transformer` (upper-case
    ‘T’) is a completely different (and much simpler) entity. You can think of it
    simply as a tool for preprocessing data in a typical ML workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you import scikit-learn, you get automatic access to a bunch of pre-built
    Transformers designed for common ML data preprocessing tasks like imputing missing
    values, rescaling features and one-hot encoding. Some of the most popular Transformers
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn.impute.SimpleImputer` - a Transformer that will replace missing values
    in your dataset'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sklearn.preprocessing.MinMaxScaler` - a Transformer that can rescale the numerical
    features in your dataset'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sklearn.preprocessing.OneHotEncoder` - a Transformer for one-hot encoding
    categorical features'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using a scikit-learn `sklearn.pipeline.Pipeline`, you can even chain together
    multiple Transformers to build multi-step data preparation workflows, in preparation
    for subsequent ML modelling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03fbf1b205ea500b325e3e7fbfed50d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re not familiar with Pipelines or ColumnTransformers, they’re a great
    way to simplify your ML code, and you read more about them in my previous article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/simplify-your-data-preparation-with-these-4-lesser-known-scikit-learn-classes-70270c94569f?source=post_page-----5e58d2713ac5--------------------------------)
    [## Simplify Your Data Preparation With These 4 Lesser-Known Scikit-Learn Classes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Forget train_test_split: Pipeline, ColumnTransformer, FeatureUnion and FunctionTransformer
    are indispensable even if…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/simplify-your-data-preparation-with-these-4-lesser-known-scikit-learn-classes-70270c94569f?source=post_page-----5e58d2713ac5--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: What’s wrong with the pre-built scikit-learn Transformers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nothing at all!
  prefs: []
  type: TYPE_NORMAL
- en: If you’re working with simple datasets and performing standard data preparation
    steps, chances are that scikit-learn’s pre-built transformers will be perfectly
    adequate. There’s no need to reinvent the wheel by writing custom ones from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: But — and let’s be honest — when are datasets ever really simple in real life?
  prefs: []
  type: TYPE_NORMAL
- en: '(Spoiler: never.)'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re working with real-world data or need to implement some juicy preprocessing
    method, chances are that scikit-learn’s built-in Transformers won’t always be
    adequate. Sooner or later, you’re going to need to implement custom data transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, scikit-learn provides a few ways to extend its basic Transformer functionalities
    and build more customised Transformers. To showcase how these work, I’ll be using
    the canonical Titanic Survival Prediction dataset. Even on this supposedly “simple”
    dataset, you’ll find that there’s plenty of opportunity for getting creative with
    your data preparation. And, as I’ll show, custom Transformers are the ideal tool
    for the task.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let’s load the dataset and split it into training and testing subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5216168cd8901e8c31de6dc524c6e539.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author. [Titanic dataset](https://www.openml.org/search?type=data&sort=runs&id=40945&status=active)
    available a CC0 public domain license.
  prefs: []
  type: TYPE_NORMAL
- en: Because this article focuses on how to build **customised** Transformers, I
    won’t go into detail on the standard preprocessing steps which can be easily applied
    to this dataset using scikit-learn’s in-built Transformers (e.g. one-hot encoding
    categorical variables like `sex` using a `OneHotEncoder`, or replacing missing
    values using a `SimpleImputer`).
  prefs: []
  type: TYPE_NORMAL
- en: Instead, I’ll focus on how to incorporate more complex preprocessing steps which
    cannot be implemented using “off-the-shelf” Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: One such step involves extracting each passenger’s title (e.g. Mr, Mrs, Master)
    from the `name` field. Why might we want to do this? Well, if we know that each
    passenger’s title contains an indication of their class/age/sex, and we assume
    that these factors influenced passengers’ ability to get on lifeboats, it’s reasonable
    to hypothesise that titles might be informative about survival chances. For example,
    a passenger with a “Master” title (indicating that they are a child) might be
    more likely to survive than a passenger with a “Mr” title (indicating that they
    are an adult).
  prefs: []
  type: TYPE_NORMAL
- en: The problem, of course, is that there’s no in-built scikit-learn class which
    can do something as specific as extracting the title from the `name` field. To
    extract the titles, we need to build a custom Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. FunctionTransformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The quickest way to build a custom Transformer is through using the `FunctionTransformer`
    class, which allows you to create Transformers directly from normal Python functions.
  prefs: []
  type: TYPE_NORMAL
- en: To use a `FunctionTransformer`, you start by defining a function which takes
    an input dataset `X`, performs the desired transformation, and returns a transformed
    version of `X`. Then, wrap your function in a `FunctionTransformer`, and scikit-learn
    will create a customised Transformer which implements your function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here’s a function that can extract a passenger’s Title from the
    `name` field of our Titanic dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, wrapping the function in a `FunctionTransformer` turned it into
    a scikit-learn Transformer, giving it the `.fit()` and `.transform()` methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then incorporate this Transformer into our data preparation pipeline
    alongside any additional preprocessing steps/transformers we want to include:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a966e4f57584d07d7af04bfd212711ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to define a more complex function/Transformer that takes additional
    arguments, you can pass these to the function by incorporating them into the `kw_args`
    argument of `FunctionTransformer`. For example, let’s define another function
    which identifies whether each passenger is from an upper-class/professional background,
    based on their title:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7dd3fd7c128c72d53e24e9a5b3068bde.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, using `FunctionTransformer` is a really simple way to incorporate
    these complex preprocessing steps into a Pipeline without fundamentally changing
    the structure of our code.
  prefs: []
  type: TYPE_NORMAL
- en: '1.1 The limitations of FunctionTransformer: Stateful transformations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`FunctionTransformer` is a powerful and elegant solution, but it’s only suitable
    when you want to apply ***stateless*** transformations (i.e. rule-based transformations
    which are not dependent on prior values computed from the training data). If you
    want to define a custom Transformer that can transform testing datasets based
    on the values observed in the training dataset, you can’t use a `FunctionTransformer`,
    and you’ll need to take a different approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If that sounds a bit confusing, take a minute to reconsider the function we
    just wrote to extract passenger’s titles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The function is ***stateless*** because it has no memory of the past; it does
    not use any pre-computed values during the operation. Each time we call this function,
    it will be applied from scratch as if it were being done for the very first time.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **stateful** function, by contrast, retains information from previous operations
    and uses this when implementing the current operation. To illustrate this distinction,
    here are two functions that replace missing values in our dataset with a mean
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first function is a *stateless* function because no prior information is
    used in the transformation; the mean is only calculated using the dataset `X`
    which is passed to the function.
  prefs: []
  type: TYPE_NORMAL
- en: The second is a *stateful* function which uses `column1_mean_train` (i.e. the
    mean value of `column1` from the training set `X_train`) to replace missing values
    in `X`.
  prefs: []
  type: TYPE_NORMAL
- en: The distinction between stateless and stateful transformation might seem a bit
    abstruse, but it’s an incredibly important concept in ML tasks where we have separate
    training and testing datasets. Whenever we want to replace missing values, scale
    features or perform one-hot encoding on our testing datasets, we want these transformations
    to be based on the values observed in the training dataset. In other words, we
    want our Transformer to be ***fit*** to the training set. Using the example of
    imputing missing values with the mean, we would want the “mean” value to be the
    mean value of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with using `FunctionTransformer` is that it can’t be used to implement
    stateful transformations. Even though a `Transformer` created with `FunctionTransformer`
    *technically* has the `.fit()` method, [calling it won’t do anything](https://stackoverflow.com/a/62225211),
    and so we can’t really “fit” this Transformer to the training data. Why? Because
    the transformations in a `FunctionTransformer`-created Transformer are always
    dependent on the function’s input value `X`. Our `Transformer` will always re-calculate
    the values using the dataset it is passed; it has no way of imputing/transforming
    with a pre-calculated value.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 An example to illustrate the limitations of FunctionTransformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate this, here’s an example where I try to “fit” a FunctionTransformer-based
    Transformer to a training set and then transform the testing set using this supposedly
    “fitted” transformer. As you can see, the missing values in the testing set are
    not replaced with the mean value from the training set; they are recalculated
    based on the testing set. In other words, the Transformer was unable to apply
    a stateful transformation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/648add161d7746faa7471bc53ccb2e89.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author, showing a missing value in the third row of the testing set
    in the column ‘Age’
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7973d4ccbc7bcff0978c0416e85a741a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author. The missing value in the third row was replaced with the mean
    of the testing set, not the mean of the training set, illustrating the inability
    of FuntionTransformer to produce Transformers capable of stateful transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'If this all sounds a bit confusing, don’t sweat it. The key takeaway message
    is: if you want to define a custom Transformer that can preprocess testing datasets
    based on the values observed in the training dataset, you can’t use a `FunctionTransformer`,
    and you’ll need to take a different approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Create a custom Transformer from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One alternative approach is to define a new Transformer class which inherits
    from a class found in the `sklearn.base` module: `TransformerMixin`. This new
    class will then function as a Transformer, and is suitable for applying both stateless
    *and* stateful transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how we’d take our `extract_title` code snippet and turn it into a Transformer
    using this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a966e4f57584d07d7af04bfd212711ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we achieve the exact same transformation as we did when constructing
    our Transformer using FunctionTransformer.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Passing arguments to a custom Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you need to pass data to your custom Transformer, simply define an `__init__()`
    method before defining the `fit()` and `transform()` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7dd3fd7c128c72d53e24e9a5b3068bde.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'And there you have it: two methods for building customised Transformers in
    scikit-learn. The ability to use these methods is an incredibly valuable one,
    and something which I regularly use in my day-to-day job as a Data Scientist.
    I hope you’ve found it useful.'
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this article and you’d like to get further tips and insights on
    working in Data Science, consider following me here on Medium or [LinkedIn](https://www.linkedin.com/in/matt-chapman-ba8488118/).
    If you’d like to get unlimited access to all of my stories (and the rest of Medium.com),
    you can sign up via my [referral link](https://medium.com/@mattchapmanmsc/membership)
    for $5 per month. It adds no extra cost to you vs. signing up via the general
    signup page, and helps to support my writing as I get a small commission.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
