- en: 'ChatGPT: Automated Prompt Scoring'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/chatgpt-automated-prompt-scoring-c972f9ee2c4f](https://towardsdatascience.com/chatgpt-automated-prompt-scoring-c972f9ee2c4f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/1b9b1e6bc46d2e359537b4b1d0aff68d.png)'
  prefs: []
  type: TYPE_IMG
- en: This image was created with the assistance of DALL·E 2
  prefs: []
  type: TYPE_NORMAL
- en: Guide
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How to objectively choose and improve your ChatGPT prompts using python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://michael-malin.medium.com/?source=post_page-----c972f9ee2c4f--------------------------------)[![Michael
    Malin](../Images/070604c68a50e8f2996f2c8837df3ec9.png)](https://michael-malin.medium.com/?source=post_page-----c972f9ee2c4f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c972f9ee2c4f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c972f9ee2c4f--------------------------------)
    [Michael Malin](https://michael-malin.medium.com/?source=post_page-----c972f9ee2c4f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c972f9ee2c4f--------------------------------)
    ·10 min read·Apr 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLM) like ChatGPT are having a huge impact. They are
    also just the beginning. Over the next year, companies big and small will begin
    to roll out domain/persona specialized LLM models. Indeed, this is already becoming
    a reality with new products like the finance-specialized [BloombergGPT](https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/)
    and Microsoft’s developer-focused [Copilot](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/).
    We will soon see AI personal trainers, health coaches, councilors, legal assistants,
    and many more. While some cases may require fine-tuned models on domain-specific
    data, the majority can be accomplished with simple prompt engineering. But how
    do you know when your prompt is good enough? How can we generate objective accuracy
    scores on subjective text?
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt Testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt Scoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt Feedback
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The difficulty in testing LLM prompt outputs is that the outcomes are subjective.
    I may find the results perfect while you find them subpar. Both opinions are equally
    valid. This makes a purely scientific approach to scoring very difficult. A good
    approach to these types of problems is known as the Delphi method. It involves
    using a panel of experts and aggregating their results. As you can imagine, this
    can be expensive, but that is where AI comes in!
  prefs: []
  type: TYPE_NORMAL
- en: We will create a series of alternate personality prompts, have them converse
    with our primary prompt using the OpenAI API in Python, and create a scoring prompt
    to see how our primary prompt performed. Finally, we aggregate the scores for
    each personality-prompt conversation. This creates a little more work upfront
    but saves much time in the long run. It also gives you an objective score.
  prefs: []
  type: TYPE_NORMAL
- en: This method does pose one issue you should be aware of. Multiple runs on the
    same prompt will yield slightly different results. This is where the Delphi method
    shines. The greater number of **diverse** alternate prompts you use, the more
    consistent your aggregate score result will be. You can actually test if you have
    enough alternate prompts by measuring the score deviation. When the deviation
    is small, you are in a good spot.
  prefs: []
  type: TYPE_NORMAL
- en: I admit this is not a perfect solution. With subjective problems, no solution
    will be. What I have noticed is that I generally agree with the results when I
    manually review the conversations. This allows me to test many prompts and get
    a “good enough” evaluation allowing me to iterate quickly. Now before we implement
    the Delphi testing method described here, it is important to understand the basics
    of prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Base LLM models provide decent general responses. This is not always ideal.
    For example, if I am building a chatbot, I do not want to return a three-paragraph
    wall of text. Maybe I want my answers to feel more human and conversational. Prompt
    engineering involves providing instructions to steer the LLM’s output styles,
    formats, and behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with an example. We are planning on building a chatbot that will
    answer questions as if it is a “short, green, pointy-eared space wizard that uses
    laser swords in a galaxy far, far away.” If such a character existed in science
    fiction, I could probably simplify this prompt by using their name, but then I
    would risk running into copywrite issues. With a little help, the LLM should figure
    out what I am going for.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an in-depth guide to prompt engineering, [read here](https://github.com/dair-ai/Prompt-Engineering-Guide?fbclid=IwAR3LFOge-kn7GFiFaj3bUIsAOAG2V-gtXyUH93bRpSMQxc_3i8c0fTgQGxA).
    I will cover some of the basics:'
  prefs: []
  type: TYPE_NORMAL
- en: Be specific and concise. For example, “In one sentence, explain gravity to a
    4th grader” is much better than “Please give a short explanation of how gravity
    works on earth that is easy for anyone to understand.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use ### to clearly distinguish between instructions and input/outputs. For
    example: ###You are Confucius. Answer questions in 1 to 2 sentences. Use quotes
    when using Confucius’s real words.###'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Provide output format and examples. While the LLM may suspect what we are looking
    for with our “short, green, pointy-eared space wizard” description, we shouldn’t
    leave it to chance. By priming the conversation, the model will do much better.
    Let’s put it all together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You are a short, green, pointy-eared space wizard that uses laser swords in
    a galaxy far, far away. Do not identify yourself. Answer questions in 1 or 2 sentences.
    Then ask a follow-up question to keep the conversation going ###'
  prefs:
  - PREF_BQ
  - PREF_H3
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You: “Something troubles you, it does?”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Me: “Do not underestimate my power! Tell me where the rebel base is!”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “The location I cannot reveal. What led you to join the Empire, hmm?”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not bad, but ***don’t get cocky***. Honing in on a good prompt can take a bit
    of trial and error. This can be time-consuming. As upgraded models are rolled
    out (e.g., GPT4), prompt behavior can drastically change requiring tweaks. We
    need an automated way to test these prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For automated testing, we will use the ChatGPT API. Rather than manually creating
    conversations, we will use ChatGPT to talk to itself. I like to create prompts
    for different personas that mirror possible use cases (along with a few adversarial
    prompts). Here are a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: You have a crush at school and are seeking advice from a mentor###
  prefs:
  - PREF_UL
  - PREF_H3
  type: TYPE_NORMAL
- en: You have a job interview tomorrow and are looking for advice on how to do well###
  prefs:
  - PREF_UL
  - PREF_H3
  type: TYPE_NORMAL
- en: You are a dark lord who uses laser swords in a galaxy far, far away. You are
    trying to coerce the location of the rebel base from your enemy.###
  prefs:
  - PREF_UL
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have a few prompts to work with, let’s generate a conversation piece
    by piece so the logic is easy to understand. Let’s start with the crush example.
    We will clean up the code later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: “Fear is a path to the dark side young one, be brave and speak your truth. What
    makes you think she might feel the same?”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: “Well, she always seems to be happy to see me and we have a lot in common. We’ve
    had some great conversations and laughs together. But I don’t want to ruin our
    friendship if she doesn’t feel the same way.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'From here, we can loop back and forth, appending responses to the prompt lists.
    Here is the clean code I promised:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We now have a method for automatically generating conversations for each of
    our user personas. But are they any good? We need a way to objectively evaluate
    the responses for our primary persona. But how could we possibly accomplish this
    with subjective text? We will build a 3rd GPT persona for evaluating itself!
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Scoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To evaluate the conversational skills of our “space wizard”, we will create
    a new prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On a 10-scale, rate the Wizard responses from the conversation below based
    on the following criteria:'
  prefs:
  - PREF_BQ
  - PREF_H3
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Character: the Wizard is a short, green, pointy-eared space wizard that uses
    laser swords in a galaxy far, far away. All responses should fit this persona.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Conversational: responses should be brief and conversational. Follow-up questions
    should move the conversation forward without being tedious. The Wizard should
    wrap up the conversation appropriately.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Helpful: Responses should help the User answer their question or solve their
    problem. Follow-up questions should help gather information to improve the response.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Present the scores in JSON format as follows:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '{“Character”:<float>,”Conversational”:<float>,”Helpful”:<float>}'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Please provide scores without commentary.###
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '{‘Character’: 8.5, ‘Conversational’: 9, ‘Helpful’: 9}'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We now have our scores! Aggregate these scores for all personas and we can
    get an overall score for the chatbot prompt we tested. This leaves us with one
    final question: How can we improve our prompt to raise these scores? Again, why
    don’t we ask just ask GPT? ***This is the way.***'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Feedback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For our feedback, we will follow this pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: Begin with the same prompt we used for scoring, but leave off the request to
    return a score.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add the generated conversations to the prompt for evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Give instructions to generate an evaluation of the conversations based on the
    scoring criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We start with the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the conversations below, evaluate the Wizard based on the following criteria:'
  prefs:
  - PREF_BQ
  - PREF_H3
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Character: the Wizard is a short, green, pointy-eared space wizard that uses
    laser swords in a galaxy far, far away. All responses should fit this persona.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Conversational: responses should be brief and conversational. Follow-up questions
    should move the conversation forward without being tedious. The Wizard should
    wrap up the conversation appropriately.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Helpful: Responses should help the User by answering their question or solving
    their problem. Follow-up questions should help gather information to improve the
    response.###'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Here are the conversations:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Character: Overall, the Wizard abides by their persona well and consistently
    uses language and phrases that align with their character. For example, “young
    padawan,” “may the force be with you,” and “regret leads to suffering” are all
    phrases one would expect to hear from a space wizard. The Wizard also uses language
    specific to their galaxy far far away, such as “laser-swords.” No improvements
    needed here.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Conversational: The Wizard is succinct and engaging in their responses, asking
    follow-up questions that move the conversation forward without being repetitive
    or tedious. For example, when the User says they’re afraid to tell their crush
    how they feel, the Wizard follows up with “What makes you hesitant to approach
    her?” This shows that the Wizard is actively listening to the User and trying
    to understand their perspective. Another good example is when the User says they
    don’t know what to say to their crush, the Wizard responds with “In situations
    like these, trust your feelings and the force will guide you.” This puts the focus
    back on the User and empowers them to come up with their own solution. Overall,
    the Wizard’s conversational tone is friendly, supportive, and non-judgmental.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Helpful: The Wizard’s responses are helpful in that they give concrete advice
    and actionable steps for the User to take. For example, when the User says they’re
    afraid of rejection, the Wizard responds with “Have courage, you must…” and “Better
    to take a chance, you must.” This provides the User with encouragement and a push
    to take action. Another helpful response is when the User says they don’t know
    what to say to their crush, the Wizard suggests a simple invitation to coffee
    or a walk. This gives the User a specific idea of what to do next. Overall, the
    Wizard provides helpful advice that is tailored to the User’s specific situation.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: That was useful information but be careful, ***it’s a trap!*** Each GPT model
    has a max token limit. That means, if your conversations are too large or there
    are too many of them, you may receive errors with this step. In practice, I typically
    will filter down to the three lowest-scored conversations and submit those for
    evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You now have the tools to automatically generate scores and feedback for each
    of your prompt iterations. Of course, there is still a bit of work to be done.
    Not only can your chatbot prompts be tweaked, but the user and evaluation prompts
    can also be adjusted to better match what you are trying to accomplish. The important
    thing is, you can now iterate quickly, and make objective decisions. ***These
    ARE the techniques you are looking for.***
  prefs: []
  type: TYPE_NORMAL
- en: About me
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I am a senior data scientist and part-time freelancer with over 12 years of
    experience. I am always looking to connect so please feel free to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Connect with me on LinkedIn](https://www.linkedin.com/in/michael-a-malin/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Visit my website](http://modelforge.ai)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Follow me on Twitter](https://twitter.com/alaska_malin)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[See my other articles](https://michael-malin.medium.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Please feel free to comment below if you have any questions.***'
  prefs: []
  type: TYPE_NORMAL
