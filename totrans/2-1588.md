# 那么，为什么我们应该关心推荐系统呢？特邀：对汤普森采样的简要介绍

> 原文：[https://towardsdatascience.com/now-why-should-we-care-about-recommendation-systems-ft-a-soft-introduction-to-thompson-sampling-b9483b43f262](https://towardsdatascience.com/now-why-should-we-care-about-recommendation-systems-ft-a-soft-introduction-to-thompson-sampling-b9483b43f262)

## 正在进行的推荐系统系列

[](https://irenechang1510.medium.com/?source=post_page-----b9483b43f262--------------------------------)[![Irene Chang](../Images/02280890ed87239c75cbcbfa7c5d686c.png)](https://irenechang1510.medium.com/?source=post_page-----b9483b43f262--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b9483b43f262--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b9483b43f262--------------------------------) [Irene Chang](https://irenechang1510.medium.com/?source=post_page-----b9483b43f262--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b9483b43f262--------------------------------) ·阅读时间12分钟·2023年11月7日

--

![](../Images/a8dea9c6cebe66313b191c252f95b74c.png)

图片由 [Myke Simon](https://unsplash.com/@myke_simon?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

今天我发现自己再次陷入了相同的情境，连续第100...01天，一边浏览Netflix寻找观看的节目，一边拿着晚餐盒子吃饭。我的推荐内容中充斥着过多的亚洲浪漫和美国成长题材的建议，可能是基于我一个月或两个月前观看过的这些类别的某几部剧。 “这里没什么好看的…”–我一边阅读完所有简介一边叹了口气，自信地觉得自己能预测剧情的发展。我掏出了另一个备用的娱乐选项Tiktok，同时潜意识里想着我可能需要*不感兴趣*一些视频，而*喜欢*、*保存*其他视频，以便…推荐算法今天给我推送一些新的内容流。

推荐系统（RecSys）可以被认为是一个已经非常成熟的算法，它已经深深植入我们的日常生活中，以至于在1到Chat-GPT的尺度上，它在学术界和非学术界都感觉像是80年代的趋势。然而，它绝不是一个近乎完美的算法。操作推荐应用程序所面临的伦理、社会、技术和法律挑战从未成为研究的前沿（就像大多数其他技术产品一样……）。例如，选择性群体的不公平和隐私侵犯是围绕RecSys的热门担忧，但这些问题仍未得到实施公司充分解决。此外，还存在许多更微妙的问题，通常没有得到足够的深思，其中之一是个体决策过程中的自主权丧失。一种“强大的”RecSys无疑可以将用户推向某个方向[2]，使他们购买、观看、思考、相信他们如果不受到这种操控本不会做的事情。

因此，我想在我的研究生学习旅程中写一系列文章，随着我开始学习并深入探讨RecSys的优缺点……一切从零开始！我觉得可以从思考电影和……汤普森采样开始！

# **汤普森采样**

**汤普森采样** (TS) 是推荐系统文献和强化学习中的基础算法之一。正如Samuele Mazzanti在这篇精彩的[文章](/when-you-should-prefer-thompson-sampling-over-a-b-tests-5e789b480458)中清楚解释的那样，它可以被认为是在在线学习环境中更好的A/B测试。简单来说，在电影推荐的背景下，TS试图识别出最适合推荐给我的电影，以最大化我点击观看的机会。它可以通过相对较少的数据有效地做到这一点，因为它允许在每次观察到我是否点击电影时更新参数。粗略地说，这种动态特性使得TS能够在考虑我的观看历史和收藏的系列之外，还能实时考虑像浏览或在我当前正在使用的应用程序中的搜索结果等因素，以给我最合适的建议。然而，在这个适合初学者的教程中，我们仅仅看一下下面的简化分析。

## 让我们进一步分析吧！

考虑这3部电影，尽管它们都很棒，但我却有自己个人的排名。假设这3部电影中，有一部是如果出现在我的推荐中我将100%重新观看，有一部是我极不可能重新观看的（5%），还有一部是我每次看到时有70%的机会会点击观看。显然，TS在事先并不了解这些关于我的信息，它的目标是学习我的行为，以便，如常识所说，推荐我它知道我一定会点击观看的电影。

![](../Images/2119d09ba44d1c295b556f676126e67e.png)

作者提供的图片

在TS算法中，主要的工作流程如下：

1.  **行动**：TS建议我观看特定的电影，在数百部电影中选择

1.  **结果**：我决定电影对我来说足够有趣并点击观看，或者我觉得无聊，在阅读了简介后点击退出页面。

1.  **奖励**：可以被看作是TS在我点击观看某部电影时获得的“积分”数量，或者在我不点击时TS失去的积分。在基本的电影或广告推荐设置中，我们可以将奖励视为结果的等价物，因此1次点击电影=1积分！

1.  **更新**知识：TS记录我的选择并更新其对我最喜欢电影的信念。

1.  **重复**第1步（可以在我当前的浏览会话中，或者第二天晚餐时间），但现在有了关于我偏好的额外知识。

## 探索/利用

这是该文献中使用最多的术语，也是区分TS和其他相关算法的关键。上述第5步是这个逻辑开始发挥作用的地方。在TS的世界中，一切都存在某种程度的不确定性。我每周喝三次拿铁和五次抹茶并不一定意味着我比拿铁更喜欢抹茶，如果只是那一周（而我每周平均实际上喝的拿铁比抹茶多）呢？因此，TS中的一切都由某种类型的分布表示，而不仅仅是单个数字。

![](../Images/df7f1bafb93b4aac7b9a48a9e24ec6e0.png)

**图1** 在某一周，我喝了5杯抹茶和3杯拿铁（左），但平均每周我喝的拿铁比抹茶多（右）——作者提供的图片

起初，TS显然对我对电影的偏好有很多不确定性，因此它的优先任务是***探索***这一点，通过给我提供许多不同的电影建议来观察我的反应。在经过几次点击和跳过后，TS可以大致了解我倾向于点击的电影和没有效益的电影，从而对下一次给我推荐的电影有了更多的信心。这时，TS开始***利用***高回报的选项，它会给我推荐我经常点击的电影，但仍然留有一些探索的空间。随着更多观察的积累，信心不断建立，简单情况下，探索的工作将变得非常少，因为TS已经对能够带来大量奖励的推荐有了很大的信心。

探索与利用通常被称为**权衡**或困境，因为过多的探索（即使在获得足够证据后仍然没有排除低价值选项）会导致大量损失，而过多的利用（即过快地排除太多选项）可能会错误地排除真正的最佳行动。

# 分布：Beta-Bernoulli

如上面的抹茶拿铁图所示，TS 使用不同类型的分布来理解我们对不同选项的偏好。在最基本的电影（和广告）情况下，我们通常使用 Beta-Bernoulli 组合。

[伯努利分布](/understanding-bernoulli-and-binomial-distributions-a1eef4e0da8f#:~:text=The%20Bernoulli%20distribution%20is%20the,probability%20(1%2Dp).) 是一种离散分布，其中只有两种可能的结果：1 和 0。伯努利分布只有一个参数，表示某个变量，比如 Y，取值为 1 的概率。因此，如果我们说 Y~ Bern(p)，比如 p = 0.7，这意味着 Y 有 0.7 的机会取值为 1，而 1–p = 1–0.7 = 0.3 的机会取值为 0。因此，伯努利分布适合用于建模奖励（在我们的例子中也是结果），因为我们的奖励只有两种结果：*点击* 或 *未点击*。

另一方面，Beta 分布用于建模 TS 对我电影兴趣的信念。Beta 分布有两个参数，alpha 和 beta，通常被认为是成功和失败的次数，两者都必须 ≥ 1。因此，使用 Beta 分布来建模我点击观看和跳过电影的次数是合适的。我们来看一个例子。这里有 3 个不同的 Beta 分布，代表 3 部电影，在 10 次观察中，所以所有 3 部电影的点击和跳过次数总和相同（10），但点击和跳过率不同。对于电影 1，我点击观看 2 次（alpha = 2）和跳过 8 次（beta = 8）；对于电影 2，我点击观看 5 次和跳过 5 次；对于电影 3，我点击观看 8 次和跳过 2 次。

![](../Images/0123fa0018661d2a4f4501c5bf8ccaa3.png)

**图 2.** 图片由作者提供

根据图表，我们可以看到，我再次观看电影 2 的概率大约在 50% 处达到峰值，而电影 1 的这个概率要低得多。例如。我们可以将这些曲线视为观看电影的概率的概率，因此 Beta 分布非常适合表示 TS 对我电影偏好的信念。

# 算法

在本节中，我将帮助你清楚地理解算法的实现和方法论。首先，这是 Thompson Sampling 算法的一个片段，分别是伪代码和 Python 实现。伪代码摘自一本关于 TS 的精彩书籍，[*A tutorial on Thompson Sampling*](https://arxiv.org/abs/1707.02038) [Russo, 2017]。

![](../Images/bc239424d8d47da61c157fcedac73dbf.png)

**图 3** Thompson Sampling，Python 实现（左）和伪代码（右）— 图片由作者提供

让我们来详细分析一下！

## 样本模型

算法的第一步涉及“猜测”我对每部电影的喜好。如前一节所述，我对每部电影的偏好可以使用Beta曲线表示，如图2所示，而TS对此没有先验知识，并且试图弄清楚这些Beta曲线的样子。在*t = 1*（第一轮）时，TS可以假设我对所有3部电影的喜好相同，即点击和跳过的初始次数相等（我的3条Beta曲线将看起来相同）。

![](../Images/5794df9be46cac194f14024774455cab.png)

**图4** TS对我对3部电影的偏好的首次猜测相同

这里的三种分布就是图3中伪代码中的*p*。从每个分布中，TS将采样一个值，用theta表示，以帮助下一步的动作选择。

![](../Images/a9a97d385abe5254d8229a4680b1d00d.png)

**图5** 示例的alpha-beta值对，表示我们对每部3部电影的初始猜测的分布（也称为动作/手臂）

## 选择并应用动作

在此步骤中，TS根据采样的theta值中最大的值选择要执行的动作（即选择推荐的电影）。以图2为例。假设我们只有2部电影——电影1和电影3。使用最大的theta选择动作的想法是，如果真实分布几乎没有重叠，而我在我们的例子中几乎肯定喜欢一部电影多于另一部，那么电影1的采样theta很可能不会大于电影3的theta。以类似的方式，如果我们只考虑电影2和3，我们可以看到现在这些分布之间有更多重叠。然而，如果我们继续在足够多的轮次中采样更多的theta值，那么我们可以观察到电影3的thetas > 电影2的thetas的比例大于反之，TS将有足够的信息得出电影3是更好的“动作”的结论。一般来说，这也是为什么未知真实分布越明显，TS找出哪个动作或手臂是最优的实验轮次就越少的原因。

在应用选择的动作后，TS将收到我的反馈，即我是否点击观看电影。正如上面提到的，这一结果也被视为我们对相应动作的奖励。TS将记录这一观察结果，并在下一步中用它来更新对我电影偏好的信念。

## 更新分布

在上面的 Beta 分布描述中，我们确定 Beta 分布的特征是成功次数和失败次数。我点击观看某部电影的次数越多，该电影的 Beta 分布的模式就越趋近于 1，而相反地，我跳过推荐的次数越多，模式就越趋近于 0。因此，在电影被推荐并记录响应后，对电影的信念更新是通过将电影的 Beta 分布的 alpha 或 beta 参数加 1 来完成的，具体取决于电影是被点击还是被跳过。

这种简单且易于解释的参数更新方法就是为什么 Beta Bernoulli 是一种非常常见的 TS 模型。

## 结果与讨论

回到文章开始时的情境。我们正在猜测 3 部电影中哪一部最适合推荐给我，假设有一部我会 100% 点击观看，一部我有 70% 的点击概率，另一部只有 5% 的点击概率（再次强调，这些信息 TS 并不知道）。第一行展示了两种不同的模拟起始点，这将使我们观察是否可以通过不同的初始先验信念达到相同的最终结果。

![](../Images/afc3c98d911b6126d07b914ab3ff5e5e.png)

**图 6** TS 模拟的不同轮数 T = 5, 10, 20。 Beta 分布代表了实验结束时 TS 对我电影偏好的信念。 ***左列***：初始分布为 Beta(1, 1) 时的结果。右列：初始分布为 Beta(2, 3) 时的结果

从图 6 中，我们可以看到我最终最喜欢的电影是电影 1 — 《寄生虫》（对不起，漫威粉丝）！！

如我们所见，两种情况的探索过程不同，其中 Beta(1, 1) 的初始猜测导致更快地找到被认为是我最喜欢的电影。只需要 T=10 轮就可以看出 TS 明显在开发电影 1，这意味着 TS 已经推荐了电影 1 并得到了我的点击，因此其 Beta 分布向右拉动，因此从更新的分布中采样的 theta 超过了它的竞争对手，导致了开发。这种开发在 T=5 轮时已经出现，但根据相应的图表，电影 1 和电影 3 的 Beta 之间仍然存在较多重叠，它们的模式并不完全不同，这意味着 TS 仍然不完全确定电影 1 是最优的行动。

另一方面，Beta(2, 3) 的初始信念使得 TS 需要更多的轮次才能到达电影 1（T=20）。即使在 T=10 时，电影 1 和 电影 3 之间仍存在很大的不确定性，并且观察到由于 theta 采样的随机性，电影 3 可能被错误地当作最佳选项。这项实验表明，每个行动的初始先验知识在检测最佳臂的速度上起着作用，关于这个主题我们可以在未来的文章中进一步深入探讨。

需要注意的是，如果电影的实际分布几乎相同（比如电影 1 和电影 3 的点击率分别为 100% 和 98%），TS 很可能无法识别最佳行动，因为来自一个分布的样本 thetas 超过另一个分布的样本 thetas 的比例会被拆分。因此，如果由于偶然性，电影 3 的“较大 thetas”更多，TS 将更多地利用这个选项，导致其被错误地识别为最佳行动。

实验的另一个发现是，TS 仅能告诉我们最佳行动是什么，但不能提供关于其他选项的信息。这是因为在探索过程中，TS 会迅速淘汰那些被认为不是最优的选项，因此 TS 停止接收这些行动的进一步信息，从而不能提供最佳选项以外的行动的正确排序。

# 结论

在这篇文章中，我们探讨了汤普森采样算法，并通过电影推荐模拟进行了演练。汤普森采样在提供预测时涉及大量的分布和先验知识，这是贝叶斯模型的核心概念，我计划在即将到来的文章中与大家进一步讨论。如果你读到了这里，谢谢你的时间，希望这篇教程能给你提供对这个算法的技术和直观理解！如果你有任何进一步的问题，随时通过我的 [LinkedIn](https://www.linkedin.com/in/irene-chang-4356a013a/) 联系我，很高兴与您联系并回答问题！

# 参考文献：

[1] [推荐系统在不同领域和背景中的潜在风险和挑战是什么？](https://www.linkedin.com/advice/3/what-potential-risks-challenges-recommender#:~:text=One%20of%20the%20ethical%20issues,groups%2C%20opinions%2C%20or%20values)

[2] [推荐系统及其伦理挑战](https://link.springer.com/article/10.1007/s00146-020-00950-y)

[3] [何时应该选择“汤普森采样”而不是 A/B 测试](/when-you-should-prefer-thompson-sampling-over-a-b-tests-5e789b480458)
