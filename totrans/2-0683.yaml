- en: 'Decision Trees: Introduction & Intuition'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树：介绍与直观理解
- en: 原文：[https://towardsdatascience.com/decision-trees-introduction-intuition-dac9592f4b7f](https://towardsdatascience.com/decision-trees-introduction-intuition-dac9592f4b7f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/decision-trees-introduction-intuition-dac9592f4b7f](https://towardsdatascience.com/decision-trees-introduction-intuition-dac9592f4b7f)
- en: Making data-informed decisions with Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 做数据驱动决策
- en: '[](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)
    ·10 min read·Feb 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)
    ·阅读时间 10 分钟·2023年2月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/13f81f75cc709437ff2192216ceadf8c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13f81f75cc709437ff2192216ceadf8c.png)'
- en: Photo by [niko photos](https://unsplash.com/@niko_photos?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) peppered
    with thinking emojis.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [niko photos](https://unsplash.com/@niko_photos?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)，并配有思考的表情符号。
- en: This is the first article in a series on Decision Trees. In this post, I introduce
    decision trees and describe how to *grow* them using data. The post concludes
    with example Python code showing how to create and use a decision tree to help
    make medical prognoses.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于决策树系列文章中的第一篇。在这篇文章中，我介绍了决策树，并描述了如何使用数据来*生成*它们。文章最后包含了示例 Python 代码，展示了如何创建和使用决策树来帮助进行医学预测。
- en: 'Key Points:'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重点：
- en: Decision Trees are a widely-used and intuitive machine learning technique used
    to solve prediction problems.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树是一种广泛使用且直观的机器学习技术，用于解决预测问题。
- en: We can *grow* decision trees from data.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以从数据中*生成*决策树。
- en: Hyperparameter tuning can be used to help avoid the *overfitting* problem.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调整可以用来帮助避免*过拟合*问题。
- en: '**What are Decision Trees?**'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**什么是决策树？**'
- en: '**Decision trees** are a **widely-used and intuitive machine learning technique**.
    Typically, they are **used to solve prediction problems**. For example, predicting
    tomorrow’s weather forecast or estimating an individual''s probability of developing
    heart disease.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**是一种**广泛使用且直观的机器学习技术**。通常，它们**用于解决预测问题**。例如，预测明天的天气预报或估算一个人患心脏病的概率。'
- en: They work through a series of yes-no questions, which are used to narrow down
    possible choices and arrive at an outcome. A simple example of a decision tree
    is shown below.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通过一系列是非问题进行工作，这些问题用于缩小可能的选择范围并得出结果。下面展示了一个简单的决策树示例。
- en: '![](../Images/19d95fea62f210ec0bc0b398fdea68a8.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19d95fea62f210ec0bc0b398fdea68a8.png)'
- en: Example decision tree to predict whether I will drink tea or coffee. Image by
    author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 示例决策树预测我是否会喝茶或咖啡。图片由作者提供。
- en: As shown in the above figure, a decision tree consists of nodes connected by
    directed edges. Each node in a decision tree corresponds to a conditional statement
    based on a predictor variable.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，决策树由通过有向边连接的节点组成。决策树中的每个节点对应于一个基于预测变量的条件语句。
- en: At the top of the decision tree shown above is the **root node**, which **sets
    the initial splitting of data** records. Here we evaluate whether it is after
    4 PM or not. Each possible response (yes or no) follows a different path in our
    tree.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 上面显示的决策树的顶部是**根节点**，它**设置了数据记录的初始分裂**。在这里，我们评估时间是否在下午 4 点之后。每个可能的响应（是或否）在树中遵循不同的路径。
- en: If yes, we follow the left branch and end up at a **leaf node** (also called
    the terminal node). **No further splits are required to determine the outcome
    at this type of node**. In this case, we go with tea over coffee so we can get
    to bed at a reasonable hour.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是，我们沿着左侧分支前进，最终到达一个**叶节点**（也称为终端节点）。**在这种类型的节点上不需要进一步分裂来确定结果**。在这种情况下，我们选择茶而不是咖啡，以便能在合理的时间上床睡觉。
- en: Conversely, if it is 4 PM or earlier, we follow the right branch and end up
    at a so-called **splitting node**. These nodes **further split data records**
    based on conditional statements. From here, we evaluate whether the hours of sleep
    from last night were more than 6 hours. If yes, we go with tea again, but if no,
    we go with coffee ☕️.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果时间是下午4点或更早，我们会沿着右侧分支前进，最终到达一个所谓的**分裂节点**。这些节点**进一步分割数据记录**，基于条件语句进行划分。接下来，我们评估昨晚的睡眠时间是否超过6小时。如果是，我们继续选择茶，但如果不是，我们则选择咖啡☕️。
- en: Using Decision Trees
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用决策树
- en: In practice, we often don’t use decision trees like we did just now *(*i.e.
    looking at a decision tree and following along for a particular data record).
    Rather, have a computer evaluate data for us. All we have to do is give the computer
    the data it needs in the form of a table.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，我们通常不会像刚才那样使用决策树（即查看决策树并跟随特定数据记录）。相反，我们让计算机为我们评估数据。我们只需将所需数据以表格形式提供给计算机即可。
- en: 'An example of this is shown below. Here we have tabular data with two variables:
    time of day and hours of sleep from the previous night (blue columns). Then using
    the decision tree above, we can assign an appropriate caffeinated beverage to
    each record (green column).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例。这里我们有两个变量的表格数据：时间和前一晚的睡眠小时数（蓝色列）。然后，使用上述决策树，我们可以为每条记录分配适当的含咖啡因饮料（绿色列）。
- en: '![](../Images/5fcefe3250fcb94b3929efba85702486.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fcefe3250fcb94b3929efba85702486.png)'
- en: Example table of input data and the resulting decision tree prediction. Image
    by author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据的示例表格及其生成的决策树预测。图片由作者提供。
- en: '**Graphical View of a Decision Tree**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**决策树的图形视图**'
- en: Another way to think about decision trees is **graphically**. (*This is personally
    the intuition I carry around for decision trees.)*
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考决策树的方法是**图形化**。*（这是我个人对决策树的直观理解。）*
- en: Imagine we take the two predictor variables from the example decision tree and
    visualize them on a 2D plot. We can then represent the decision tree splits as
    lines that divide our plot into different sections. This then allows us to identify
    the beverage choice by simply looking at which quadrant a data point lies.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们将示例决策树中的两个预测变量绘制在一个二维图上。然后，我们可以将决策树的分裂表示为将图划分为不同区域的线条。这样，我们可以通过简单地查看数据点所在的象限来确定饮料选择。
- en: Intuitively, this is all a decision tree is doing. **Partitioning the predictor
    space into sections and assigning a label (or probability) to each section**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上看，这就是决策树的作用。**将预测空间划分为不同的部分，并为每个部分分配一个标签（或概率）**。
- en: '![](../Images/1dca41b0e04b37add727a37582954df4.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dca41b0e04b37add727a37582954df4.png)'
- en: Graphical view of decision tree predictions for tea or coffee example. Image
    by author.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树对茶或咖啡的预测的图形视图。图片由作者提供。
- en: '**How to Grow a Decision Tree?**'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**如何构建决策树？**'
- en: Decision trees are an intuitive way to partition data. However, it may not be
    easy to draw out an appropriate decision tree by hand using data. In such cases,
    **we can use machine learning** strategies to learn the “best” decision tree for
    a given dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种直观的数据划分方法。然而，使用数据手动绘制一个合适的决策树可能并不容易。在这种情况下，**我们可以使用机器学习**策略来学习适用于给定数据集的“最佳”决策树。
- en: Data can be used to grow decision trees in an optimization process called **training**.
    Training requires a training dataset consisting of predictor variables pre-labeled
    with target values.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以用于在一种称为**训练**的优化过程中构建决策树。训练需要一个训练数据集，其中的预测变量预先标记了目标值。
- en: A standard strategy for training a decision tree uses something called **Greedy
    Search**. This is a popular technique in optimization, where we simplify a more
    complicated optimization problem by finding *locally* optimal solutions instead
    of *globally* optimal ones. (*I give an intuition for greedy search in a previous
    article on* [*causal discovery*](https://medium.com/towards-data-science/causal-discovery-6858f9af6dcb)*.)*
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一种标准的训练决策树的策略使用被称为**贪婪搜索**的方法。这是一种在优化中流行的技术，我们通过找到*局部*最优解来简化更复杂的优化问题，而不是*全局*最优解。(*我在之前的一篇关于*
    [*因果发现*](https://medium.com/towards-data-science/causal-discovery-6858f9af6dcb)*的文章中给出了贪婪搜索的直观解释。*)
- en: In the case of decision trees, the Greedy Search determines the gain from each
    possible splitting option and then chooses the one that provides the greatest
    gain [1,2]. Here “**gain**” is determined by the **split criterion**, which can
    be based on a few different quantities, e.g. **Gini impurity**, **information
    gain**, **mean squared error (MSE)**, among others. This process is repeated recursively
    until the decision tree is fully grown.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树的情况下，贪婪搜索确定每个可能的分裂选项的**增益**，然后选择提供最大增益的那个选项[1,2]。这里的“**增益**”由**分裂准则**决定，这可以基于几种不同的量度，例如**基尼
    impurity**、**信息增益**、**均方误差 (MSE)**等。这个过程会递归地重复，直到决策树完全生成。
- en: For example, if using Gini impurity, data records are recursively split into
    two groups such that the **weighted average impurity of the resulting groups is
    minimized**. This splitting procedure can continue until all data partitions are
    *pure*, meaning all data records in a given partition corresponds to a single
    target value.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果使用基尼 impurity，数据记录会递归地分成两个组，以使**结果组的加权平均 impurity 最小化**。这个分裂过程可以继续，直到所有数据分区都是*纯净的*，即每个分区中的所有数据记录都对应于一个单一的目标值。
- en: Although this implies decision trees can be *perfect* estimators, such an approach
    would result in **overfitting**. The trained **decision tree would not perform
    well on data sufficiently different than the training dataset**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这意味着决策树可以是*完美*的估计器，但这种方法可能会导致**过拟合**。训练好的**决策树在与训练数据集有显著不同的数据上表现不佳**。
- en: '**Hyperparameter Tuning**'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**超参数调优**'
- en: One way to combat the overfitting problem is hyperparameter tuning. **Hyperparameters**
    are **values that constrain the growth of a decision tree**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗过拟合问题的一种方法是超参数调优。**超参数**是**限制决策树增长的值**。
- en: Common decision tree hyperparameters are the maximum number of splits, minimum
    leaf size, and the number of splitting variables. The **key result** of setting
    decision tree hyperparameters is to **limit the tree’s size**, which can help
    avoid overfitting and improve generalizability.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的决策树超参数包括最大分裂次数、最小叶子节点大小和分裂变量的数量。设置决策树超参数的**关键结果**是**限制树的大小**，这有助于避免过拟合并提高泛化能力。
- en: '**Alternative Training Strategies**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**替代训练策略**'
- en: While the training process I have described above is widely-used for decision
    trees, there are alternative approaches we can use.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我上述描述的训练过程在决策树中广泛使用，但我们可以使用其他替代方法。
- en: '**Pruning** — One such approach is called pruning [3]. In a sense, pruning
    is the opposite of growing a decision tree. Instead of starting from a root node
    and recursively adding nodes, we start with a fully grown tree and iteratively
    remove nodes.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**剪枝**——一种这样的方式叫做剪枝[3]。从某种意义上讲，剪枝是决策树生长的反面。我们不是从根节点开始递归地添加节点，而是从完全生成的树开始，逐步去除节点。'
- en: While the pruning process can be done in multiple ways, it commonly will drop
    nodes that do not significantly increase model error. This is an alternative way
    to avoid overfitting in lieu of hyperparameter tuning to limit tree growth [3].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然剪枝过程可以通过多种方式进行，但通常会删除那些不会显著增加模型误差的节点。这是一种替代超参数调优来限制树的生长以避免过拟合的方法[3]。
- en: '**Maximum Likelihood** — We can train a decision tree using the maximum likelihood
    framework [4]. While this approach is less well-known, it sits on a strong theoretical
    framework. It allows us to use information criteria such as AIC and BIC to objectively
    optimize the number of parameters in the tree and its performance, which helps
    side-step the need for extensive hyperparameter tuning.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大似然**——我们可以使用最大似然框架训练决策树[4]。虽然这种方法不太为人知，但它基于一个强大的理论框架。它允许我们使用信息准则如AIC和BIC来客观优化树中的参数数量及其性能，这有助于避免广泛的超参数调优。'
- en: 'Example code: Sepsis Survival Prediction Using a Decision Tree'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例代码：使用决策树进行脓毒症生存预测
- en: Now, with a basic understanding of decision trees and how we can develop one
    from data, let’s dive into a concrete example using Python. Here we will use a
    dataset from the [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/Sepsis+survival+minimal+clinical+records)
    to train a decision tree to predict whether a patient will survive based on their
    age, sex, and number of sepsis episodes they’ve experienced [5,6].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，了解了决策树的基本概念以及如何从数据中构建决策树后，让我们使用Python深入探讨一个具体的例子。在这里，我们将使用来自[UCI机器学习库](https://archive.ics.uci.edu/ml/datasets/Sepsis+survival+minimal+clinical+records)的数据集来训练一个决策树，以预测患者是否会生存，基于他们的年龄、性别和经历的脓毒症发作次数[5,6]。
- en: For the decision tree training, we will use the sklearn Python library [7].
    The code for this example is freely available in the [GitHub repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于决策树的训练，我们将使用sklearn Python库[7]。此示例的代码可以在[GitHub仓库](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree)中自由获取。
- en: We start by importing some helpful libraries.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从导入一些有用的库开始。
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we load our data from a .csv file and do some data preparation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从.csv文件中加载数据并进行一些数据准备。
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/fea3c309833e691f553f7aaf79ce4e0e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fea3c309833e691f553f7aaf79ce4e0e.png)'
- en: Histograms for each variable in dataset. Image by author.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中每个变量的直方图。图片由作者提供。
- en: Notice in the bottom-right histogram we have many more alive records than dead.
    This is called an **imbalanced dataset**. For a simple decision tree classifier,
    learning from imbalanced data can lead to the **decision tree *over-predicting*
    the majority class**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在右下角的直方图中，我们有更多的存活记录而非死亡记录。这被称为**不平衡数据集**。对于一个简单的决策树分类器，从不平衡的数据中学习可能会导致**决策树*过度预测*多数类**。
- en: To handle this situation, we can over-sample the minority class to make our
    data more balanced. One way to do this is using a technique called **Synthetic
    Minority Over-sampling Technique** (**SMOTE)**. While I will leave further details
    of SMOTE for a future article, for now, it will suffice to say this helps us balance
    our data and improve our decision tree model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这种情况，我们可以通过过采样少数类来使数据更平衡。一种方法是使用一种叫做**合成少数类过采样技术**（**SMOTE**）的技术。虽然我会在未来的文章中详细介绍SMOTE，但目前只需知道这有助于我们平衡数据并改进决策树模型即可。
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/045d096e7d85b27e2f9ba1701d3cb90b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/045d096e7d85b27e2f9ba1701d3cb90b.png)'
- en: Outcome histogram after SMOTE. Image by author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE之后的结果直方图。图片由作者提供。
- en: The final step for our data preparation is to split our resampled data into
    training and testing datasets. The **training data** will be used to **grow the
    decision tree,** and **testing data** will be used to **evaluate its performance**.
    Here we use an 80–20 train-test split.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备的最后一步是将我们的重采样数据拆分为训练和测试数据集。**训练数据**将用于**构建决策树**，而**测试数据**将用于**评估其性能**。这里我们使用80–20的训练-测试拆分。
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now with our training data, we can create our decision tree. Sklearn makes this
    super easy, with just two lines of code, we have a decision tree.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有了训练数据，我们可以创建我们的决策树。Sklearn使这一过程非常简单，仅用两行代码，我们就能得到一个决策树。
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let’s take a look at the result.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下结果。
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/cad6d35f2ad5a40779dfc1f54b3ee1e9.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cad6d35f2ad5a40779dfc1f54b3ee1e9.png)'
- en: Fully grown decision tree. Image by author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 完全生长的决策树。图片由作者提供。
- en: Needless to say, this is a very big decision tree, which can make interpreting
    the results difficult. However, let’s put that point aside for now and evaluate
    the model’s performance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 不用说，这是一棵非常大的决策树，这可能会使解释结果变得困难。然而，让我们暂时搁置这一点，评估模型的性能。
- en: For evaluating performance, we use a **confusion matrix**, which **displays
    the number of true positives (TP), true negatives (TN), false positives (FP),
    and false negatives (FN)**.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估性能，我们使用**混淆矩阵**，**它显示了真正例（TP）、真负例（TN）、假正例（FP）和假负例（FN）的数量**。
- en: I won’t get into a discussion of confusion matrices here, but for now what **we
    want** is for the **on-diagonal numbers to be big** and the **off-diagonal terms
    to be small**.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会在这里讨论混淆矩阵，但目前**我们希望**的是**对角线上的数字要大**，而**非对角线上的数字要小**。
- en: '![](../Images/847c78b12ad89d7bf2bf0e5f6bc326b9.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/847c78b12ad89d7bf2bf0e5f6bc326b9.png)'
- en: Confusion matrices for fully-grown decision tree. (Left) Training data set.
    (Right) Testing dataset. Image by author.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 完全生长的决策树的混淆矩阵。（左）训练数据集。（右）测试数据集。图片由作者提供。
- en: 'We can take the numbers from our confusion matrices and compute three different
    performance metrics: precision, recall, and f1-score**.** Briefly, **precision**
    = TP / (TP + FP), **recall** = TP / (TP + FN), and the **f1-score** is the harmonic
    mean of precision and recall.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从混淆矩阵中获取数据，并计算三个不同的性能指标：精确度、召回率和 f1-score**。** 简单来说，**精确度** = TP / (TP +
    FP)，**召回率** = TP / (TP + FN)，**f1-score** 是精确度和召回率的调和均值。
- en: '![](../Images/867103da82f0cb2b241406a704a3ed82.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/867103da82f0cb2b241406a704a3ed82.png)'
- en: Three performance metrics for fully-grown decision tree. Image by author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 完全成长的决策树的三个性能指标。 图片由作者提供。
- en: The code to generate these results is given below.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 生成这些结果的代码如下。
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Hyperparameter Tuning
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整
- en: While the decision tree has decent performance on the data used here, there
    is still the lingering issue of **interpretability**. Looking at the decision
    tree displayed previously, it would be challenging for a clinician to extract
    any meaningful insights from the decision tree’s logic.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然决策树在这里使用的数据上表现不错，但仍然存在**可解释性**的问题。查看之前展示的决策树，临床医生从决策树的逻辑中提取有意义的见解将是具有挑战性的。
- en: This is where hyperparameter tuning can help. To do this with sklearn, we can
    simply add input arguments to our decision tree training step.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是超参数调整可以发挥作用的地方。为了使用 sklearn 完成这一点，我们只需在决策树训练步骤中添加输入参数即可。
- en: Here we will try setting the max_depth = 3.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们将尝试设置 max_depth = 3。
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, let’s take a look at the resulting decision tree.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看结果决策树。
- en: '![](../Images/f9b0eaf963d0a43c1b277088afe9ea3f.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9b0eaf963d0a43c1b277088afe9ea3f.png)'
- en: Tuned decision tree so that max_depth=3\. Image by author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的决策树，max_depth=3。 图片由作者提供。
- en: Since we constrained the max depth of the tree, we can plainly see what splits
    are happening here.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们限制了树的最大深度，我们可以清楚地看到这里发生了哪些分裂。
- en: We again evaluate the model’s performance using confusion matrices and the same
    three performance metrics as before.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用混淆矩阵和之前相同的三个性能指标来评估模型的表现。
- en: '![](../Images/9540e446233230f2be271a1a4f1045aa.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9540e446233230f2be271a1a4f1045aa.png)'
- en: Confusion matrices for hyperparameter tuned decision tree. (Left) Training data
    set. (Right) Testing dataset. Image by author
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整决策树的混淆矩阵。 （左）训练数据集。 （右）测试数据集。 图片由作者提供
- en: '![](../Images/0246f2318fcfd809c78fd1598db48ef0.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0246f2318fcfd809c78fd1598db48ef0.png)'
- en: 3 performance metrics for hyperparameter tuned decision tree. Image by author.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整后的决策树的三个性能指标。 图片由作者提供。
- en: Although it may seem the fully-grown tree is preferable to the hyperparameter-tuned
    one, this goes back to the discussion on overfitting. Yes, the fully-grown tree
    performance is better on the current data, but I would not expect this to be the
    case for other data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然完全成长的树可能看起来比超参数调整的树更可取，但这回到了过拟合的讨论。是的，完全成长的树在当前数据上的表现更好，但我不认为这适用于其他数据。
- en: Put another way, **although the simpler decision tree has worse performance
    here, it will likely generalize better than the fully-grown tree.**
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，**虽然简单的决策树在这里的表现较差，但它可能比完全成长的树更具泛化能力。**
- en: This hypothesis can be tested by applying each model to the other two datasets
    available in the GitHub [repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个假设可以通过将每个模型应用于 GitHub [repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree)
    中的其他两个数据集来进行测试。
- en: '[](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree?source=post_page-----dac9592f4b7f--------------------------------)
    [## YouTube-Blog/decision-tree/decision_tree at main · ShawhinT/YouTube-Blog'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree?source=post_page-----dac9592f4b7f--------------------------------)
    [## YouTube-Blog/decision-tree/decision_tree at main · ShawhinT/YouTube-Blog'
- en: Codes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/decision-tree/decision_tree
    at main ·…
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码补充了 YouTube 视频和 Medium 上的博客文章。 - YouTube-Blog/decision-tree/decision_tree
    at main ·…
- en: github.com](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree?source=post_page-----dac9592f4b7f--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree?source=post_page-----dac9592f4b7f--------------------------------)
- en: Decision Tree Ensembles
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树集成
- en: While hyperparameter tuning can improve the generalizability of a decision tree,
    it still leaves something to be desired in regard to performance. In our example
    above, after hyperparameter tuning, the decision tree still mislabelled the training
    data **35%** of the time, which is a big deal when talking about life and death
    (*like with the example here*).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然超参数调优可以提高决策树的泛化能力，但在性能方面仍有不足。在上面的例子中，经过超参数调优后，决策树仍然将训练数据**35%**的时间标记错误，这在讨论生死问题时（*如此处的例子所示*）是一个大问题。
- en: A popular solution to this problem is to use an **ensemble of trees rather than
    a single decision tree to make predictions.** These are called **decision tree
    ensembles** and will be the topic of the [next article](https://medium.com/towards-data-science/10-decision-trees-are-better-than-1-719406680564)
    in this series.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种流行方法是使用**多个决策树而不是单一的决策树来进行预测**。这些被称为**决策树集成**，将成为本系列的[下一篇文章](https://medium.com/towards-data-science/10-decision-trees-are-better-than-1-719406680564)的主题。
- en: '[](/10-decision-trees-are-better-than-1-719406680564?source=post_page-----dac9592f4b7f--------------------------------)
    [## 10 Decision Trees are Better Than 1'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/10-decision-trees-are-better-than-1-719406680564?source=post_page-----dac9592f4b7f--------------------------------)
    [## 10 决策树比 1 个更好'
- en: Breaking down bagging, boosting, Random Forest, and AdaBoost
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解析 bagging、boosting、随机森林和 AdaBoost
- en: towardsdatascience.com](/10-decision-trees-are-better-than-1-719406680564?source=post_page-----dac9592f4b7f--------------------------------)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/10-decision-trees-are-better-than-1-719406680564?source=post_page-----dac9592f4b7f--------------------------------)
- en: Resources
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '**Connect**: [My website](https://shawhintalebi.com/) | [Book a call](https://calendly.com/shawhintalebi)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**联系**: [我的网站](https://shawhintalebi.com/) | [预约电话](https://calendly.com/shawhintalebi)'
- en: '**Socials**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**社交媒体**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
- en: '**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) ☕️'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持**: [请我喝咖啡](https://www.buymeacoffee.com/shawhint) ☕️'
- en: '[](https://shawhin.medium.com/subscribe?source=post_page-----dac9592f4b7f--------------------------------)
    [## Get FREE access to every new story I write'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/subscribe?source=post_page-----dac9592f4b7f--------------------------------)
    [## 免费获取我写的每个新故事'
- en: Get FREE access to every new story I write P.S. I do not share your email with
    anyone By signing up, you will create a…
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 免费获取我写的每个新故事 P.S. 我不会将你的邮箱与任何人分享。通过注册，你将创建一个…
- en: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----dac9592f4b7f--------------------------------)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----dac9592f4b7f--------------------------------)
- en: '[1] [*Classification and Regression Trees by* Breiman et al.](https://doi.org/10.1201/9781315139470)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [*分类与回归树* 由 Breiman 等人著](https://doi.org/10.1201/9781315139470)'
- en: '[2] [Decision trees: a recent overview by Kotsiantis, S. B.](https://link.springer.com/article/10.1007/s10462-011-9272-4)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [决策树：Kotsiantis, S. B. 最近的概述](https://link.springer.com/article/10.1007/s10462-011-9272-4)'
- en: '[3] [A comparative analysis of methods for pruning decision trees by Esposito
    et al.](https://doi.org/10.1109/34.589207)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [Esposito 等人对剪枝决策树方法的比较分析](https://doi.org/10.1109/34.589207)'
- en: '[4] [Maximum likelihood regression trees by Su et al.](https://doi.org/10.1198/106186004X2165)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [Su 等人提出的最大似然回归树](https://doi.org/10.1198/106186004X2165)'
- en: '[5] Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/census+income)
    [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School
    of Information and Computer Science. (CC BY 4.0)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Dua, D. 和 Graff, C. (2019). [UCI 机器学习库](https://archive.ics.uci.edu/ml/datasets/census+income)
    [http://archive.ics.uci.edu/ml]。加利福尼亚州尔湾: 加州大学信息与计算机科学学院。（CC BY 4.0）'
- en: '[6] [Survival prediction of patients with sepsis from age, sex, and septic
    episode number alone by Chicco & Jurman](https://www.nature.com/articles/s41598-020-73558-3)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [Chicco 和 Jurman 通过年龄、性别和脓毒症发作次数来预测脓毒症患者的生存率](https://www.nature.com/articles/s41598-020-73558-3)'
- en: '[7] [Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html),
    Pedregosa *et al.*, JMLR 12, pp. 2825–2830, 2011.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [Scikit-learn: Python中的机器学习](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html)，Pedregosa
    *等人*，JMLR 12，2825–2830页，2011年。'
