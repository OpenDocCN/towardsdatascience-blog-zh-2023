- en: 'The History of Open-Source LLMs: Better Base Models (Part Two)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源LLMs的历史：更好的基础模型（第二部分）
- en: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe](https://towardsdatascience.com/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe](https://towardsdatascience.com/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe)
- en: How LLaMA, MPT, Falcon, and LLaMA-2 put open-source LLMs on the map…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何让LLaMA、MPT、Falcon 和 LLaMA-2 使开源LLMs声名鹊起…
- en: '[](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----6ca51ae74ebe--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)
    ·16 min read·Nov 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ca51ae74ebe--------------------------------)
    ·阅读时间16分钟·2023年11月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1cad7310b9d455e5c1b8cf3ad38f5a03.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1cad7310b9d455e5c1b8cf3ad38f5a03.png)'
- en: (Photo by [Iñaki del Olmo](https://unsplash.com/@inakihxz?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/assorted-title-of-books-piled-in-the-shelves-NIJuEQw0RKg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: (照片由 [Iñaki del Olmo](https://unsplash.com/@inakihxz?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，刊登在 [Unsplash](https://unsplash.com/photos/assorted-title-of-books-piled-in-the-shelves-NIJuEQw0RKg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
- en: Open-source research on large language models (LLMs) is incredibly valuable,
    as it aims to democratize a powerful and influential technology. Although open-source
    LLMs are now commonly used and widely studied, this area of research saw some
    initial struggles that were difficult to overcome. Namely, open-source LLMs performed
    poorly at first and were heavily criticized. Within this overview, we will study
    a line of research that changed this narrative by making high-performing pre-trained
    LLMs available to everyone. Given that pre-training a language model is so expensive,
    the models we will study here are especially impactful. After these high-performing
    base models were created and released, many people could conduct research using
    these models at marginal added cost.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对大型语言模型（LLMs）的开源研究极其宝贵，因为它旨在使这一强大而有影响力的技术民主化。尽管开源LLMs现在被广泛使用和研究，但这一领域的研究在最初遇到了一些难以克服的困难。具体来说，开源LLMs最初表现不佳，并遭到了大量批评。在本概述中，我们将探讨一系列研究，这些研究通过让高性能的预训练LLMs对所有人开放，改变了这种局面。由于预训练语言模型的成本极高，我们将研究的模型尤其具有影响力。在这些高性能基础模型创建并发布后，许多人能够以极低的额外成本使用这些模型进行研究。
- en: “The capabilities of LLMs are remarkable considering the seemingly straightforward
    nature of the training methodology.” *— from [14]*
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “考虑到训练方法的表面简单性，LLMs的能力确实非常出色。” *— 摘自 [14]*
- en: '**The current series.** This overview is part two of a three part series on
    the history of open-source LLMs. The [first part](https://medium.com/towards-data-science/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8)
    in the series overviewed initial attempts at creating open-source LLMs. Here,
    we will study the most popular open-source base models (i.e., language models
    that have been pre-trained but not fine-tuned or aligned) that are currently available.
    Next time, we will go over how these models can be fine-tuned or aligned to create
    a variety of useful applications.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**当前系列。** 本概述是关于开源LLM历史的三部分系列中的第二部分。系列中的[第一部分](https://medium.com/towards-data-science/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8)概述了创建开源LLM的初步尝试。在这里，我们将研究目前可用的最受欢迎的开源基础模型（即已经预训练但未经过微调或对齐的语言模型）。下一次，我们将探讨如何微调或对齐这些模型以创建各种有用的应用。'
- en: '![](../Images/b08d1b5626668731078252e9ca6d9c37.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b08d1b5626668731078252e9ca6d9c37.png)'
- en: (from [10, 12, 14, 15])
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [10, 12, 14, 15])
- en: Early Days of Open-Source LLMs
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源LLM的早期日子
- en: In part one of this series, we saw that the early days of research on open-source
    LLMs resulted in the proposal of several important base models, such as OPT and
    BLOOM. However, these models were widely considered to perform quite poorly compared
    to closed-source pre-trained models (e.g., GPT-3). *How do we solve this?* First,
    we need to take a deeper look at the LLM training process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列的第一部分中，我们看到早期的开源LLM研究结果提出了几个重要的基础模型，例如OPT和BLOOM。然而，这些模型普遍被认为相比于闭源的预训练模型（例如GPT-3）表现较差。*我们该如何解决这个问题？*
    首先，我们需要深入了解LLM的训练过程。
- en: '**Training pipeline.** LLMs are trained in several steps, as shown in the figure
    below. First, we pre-train the model over a lot of raw text. Then, we perform
    alignment with techniques like SFT and RLHF. Finally, we can perform further fine-tuning
    or in-context learning to specialize the LLM to a particular task.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练流程。** LLM的训练分为几个步骤，如下图所示。首先，我们在大量原始文本上对模型进行预训练。然后，我们使用如SFT和RLHF等技术进行对齐。最后，我们可以进行进一步的微调或上下文学习，将LLM专门化为特定任务。'
- en: '![](../Images/6f6a2c9988d315b1a2c4cccd562fdd69.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f6a2c9988d315b1a2c4cccd562fdd69.png)'
- en: (created by author)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: (由作者创建)
- en: Recently, we have seen strong empirical evidence that most of a language model’s
    knowledge is gained during pre-training. The alignment process simply teaches
    the model to properly format or surface this knowledge gained during pre-training.
    As coined by LIMA [3], this idea is known as the “Superficial Alignment Hypothesis”.
    Although this hypothesis might not seem entirely relevant to the topic of this
    overview, we learn from it something important — *a model that undergoes insufficient
    pre-training is unlikely to be “fixed” by fine-tuning or alignment*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我们已经看到有力的实证证据表明，大多数语言模型的知识是在预训练过程中获得的。对齐过程只是教会模型如何正确地格式化或展示这些在预训练过程中获得的知识。正如LIMA
    [3] 所提出的，这个观点被称为“表面对齐假说”。尽管这个假说似乎与本概述的主题并不完全相关，但我们从中学到了一些重要的东西——*经过不足预训练的模型不太可能通过微调或对齐“修复”*。
- en: “A model’s knowledge and capabilities are learnt almost entirely during pretraining,
    while alignment teaches it which subdistribution of formats should be used when
    interacting with users.” *— from [3]*
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “模型的知识和能力几乎完全在预训练期间获得，而对齐则教会它在与用户互动时应使用哪个子分布格式。” *— 引自 [3]*
- en: '**What’s the solution?** Given the poor performance of initial open-source
    LLMs, it quickly became clear that the community needed to re-create higher-quality
    base models from scratch if any forward progress was to be made. Additionally,
    these models needed to be pre-trained over much more data so that their performance
    could be improved. Given that pre-training is incredibly expensive (especially
    when executed over a lot of data), such an effort is not trivial. The creation
    of better open-source base models had to be an undertaking of organizations with
    sufficient funding (e.g., [Meta](https://ai.meta.com/) or [MosaicML](https://www.databricks.com/company/newsroom/press-releases/databricks-completes-acquisition-mosaicml))
    that could pay the cost of training these models and make them freely available
    to others in the community.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案是什么？** 鉴于最初的开源 LLM 性能较差，很快就明确了社区需要从头开始重新创建更高质量的基础模型，才能取得任何进展。此外，这些模型还需要在更多的数据上进行预训练，以提高其性能。由于预训练成本极高（特别是当数据量很大时），这种努力并不简单。创建更好的开源基础模型必须由拥有足够资金的组织进行（例如，[Meta](https://ai.meta.com/)
    或 [MosaicML](https://www.databricks.com/company/newsroom/press-releases/databricks-completes-acquisition-mosaicml)），这些组织能够支付训练这些模型的费用，并将其免费提供给社区中的其他人。'
- en: Towards Better Base Models
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朝着更好的基础模型迈进
- en: The performance of open-source LLMs was initially too poor to warrant significant
    usage and exploration, but this problem was quickly solved. Here, we will review
    several models that changed this narrative by making powerful pre-trained LLMs
    available to all.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 开源 LLM 的性能最初过于差劲，不足以支撑广泛使用和探索，但这个问题很快得到了解决。在这里，我们将回顾几种通过提供强大的预训练 LLM 来改变这一局面的模型。
- en: 'LLaMA: A Leap in Open-Source Quality'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLaMA：开源质量的飞跃
- en: LLaMA [1] was one of the first pre-trained LLMs to be released that was both
    high-performing and open-source. However, LLaMA is not just a single model, but
    rather a suite of different LLMs with sizes ranging from 7 billion to 65 billion
    parameters. These models each achieve a different tradeoff between performance
    and inference efficiency. Although LLaMA cannot be used commercially (i.e., only
    for research), it is nonetheless an impactful proposal that served to catalyze
    several directions of open-source research with LLMs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA [1] 是首批发布的高性能且开源的预训练 LLM 之一。然而，LLaMA 不仅仅是一个单一的模型，而是一组不同的 LLM，其规模从 70 亿到
    650 亿参数不等。这些模型在性能和推理效率之间实现了不同的权衡。尽管 LLaMA 不能用于商业用途（即仅限于研究），但它仍然是一个具有深远影响的提案，推动了开源
    LLM 研究的多个方向。
- en: '![](../Images/a63c11bbd093661d57e7102568ebf623.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a63c11bbd093661d57e7102568ebf623.png)'
- en: (from [1])
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**The data.** Inspired by lessons from Chinchilla [2], LLaMA models are pre-trained
    over a corpus that contains over 1.4 trillion tokens of text. This pre-training
    dataset was significantly larger than that of any prior open-source LLM. The sources
    and distribution of data are depicted above. Interestingly, LLaMA is pre-trained
    solely using publicly-available data sources, meaning that the entire pre-training
    process can be replicated by anyone with sufficient compute.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据。** 受到 Chinchilla [2] 的启发，LLaMA 模型在一个包含超过 1.4 万亿个文本标记的语料库上进行预训练。这个预训练数据集明显大于任何之前的开源
    LLM。数据的来源和分布如上所示。有趣的是，LLaMA 完全使用公开可用的数据源进行预训练，这意味着任何拥有足够计算资源的人都可以复制整个预训练过程。'
- en: “GPT-4 has learned from a variety of licensed, created, and publicly available
    data sources, which may include publicly available personal information.” *— from
    GPT-4 blog*
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “GPT-4 从各种许可的、创造的和公开可用的数据源中学习，这些数据源可能包括公开可用的个人信息。” *— 引自 GPT-4 博客*
- en: Such a property is especially desirable given that many proprietary LLMs are
    trained using internal data that is not openly available. Put simply, LLaMA was
    a step towards improved transparency and openness in more ways than one.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于许多专有的大型语言模型（LLMs）是使用内部数据进行训练的，而这些数据并不公开，因此这种特性尤为可取。简单来说，LLaMA 是在多个方面朝着更高透明度和开放性迈出的重要一步。
- en: '![](../Images/5f5d62aa5ade1ff73497a211e3c60034.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f5d62aa5ade1ff73497a211e3c60034.png)'
- en: (from [1])
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**Improved performance.** Compared to its predecessors, *LLaMA is a huge leap
    forward in the performance of open-source LLMs*. Still, the quality lagged behind
    that of top proprietary LLMs (e.g., [ChatGPT](https://openai.com/blog/chatgpt)
    or [GPT-4](https://openai.com/research/gpt-4)), but we should recall that LLaMA
    models have not undergone alignment. Notably, LLaMA-13B performs comparably to
    GPT-3 [3], while LLaMA-65B outperforms PaLM [4] in several cases, indicating that
    the LLaMA suite performs comparably to other widely-used base models. Detailed
    metrics are provided in the tables above.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能提升。** 与其前身相比，*LLaMA 在开源 LLM 的性能上有了巨大的飞跃*。尽管如此，其质量仍落后于顶级专有 LLM（例如，[ChatGPT](https://openai.com/blog/chatgpt)
    或 [GPT-4](https://openai.com/research/gpt-4)），但我们应当记住 LLaMA 模型尚未进行对齐。值得注意的是，LLaMA-13B
    的性能与 GPT-3 [3] 相当，而 LLaMA-65B 在多个情况下超越了 PaLM [4]，这表明 LLaMA 套件的性能与其他广泛使用的基础模型相当。详细的指标见上表。'
- en: '![](../Images/7abf77a393f0ee4b2c1d597cfd0abff3.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7abf77a393f0ee4b2c1d597cfd0abff3.png)'
- en: (from [5, 6, 7, 8])
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [5, 6, 7, 8]）
- en: '**The open-source explosion.** One of the most interesting aspects of LLaMA’s
    proposal was the wake of open-source LLM research that followed it; see above.
    After the weights of LLaMA models were made publicly available, the open-source
    research community quickly began to release a variety of different model variants
    and software packages. These developments included anything from fine-tuned versions
    of LLaMA to a C++ library for efficiently running inference with any of the LLaMA
    models from a laptop. Such developments truly demonstrate the beauty of openness
    in research. *We went from interacting with these powerful models solely via an
    API to running them on our laptop in only a few weeks!*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**开源爆发。** LLaMA 提案中最有趣的方面之一是随之而来的开源 LLM 研究浪潮；见上文。在 LLaMA 模型的权重公开后，开源研究社区迅速开始发布各种不同的模型变体和软件包。这些进展包括从
    LLaMA 的微调版本到一个用于在笔记本电脑上高效运行任何 LLaMA 模型的 C++ 库。这些进展真正展示了研究开放性的美好。*我们仅仅用了几周时间，就从通过
    API 互动这些强大的模型，转而在我们的笔记本电脑上运行它们！*'
- en: 'MPT: LLMs that are High-Quality, Commercial, and Open-Source'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MPT：高质量、商业化和开源的 LLM
- en: '![](../Images/75283179a1c900a72ba34c79e7f90e3c.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75283179a1c900a72ba34c79e7f90e3c.png)'
- en: (from [10])
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [10]）
- en: Although LLaMA was impressive, none of the models within this suite could be
    used in commercial applications — *they were valuable solely from a research perspective*.
    Luckily, the proposal of LLaMA was quickly followed by the development and release
    of the commercially-usable (i.e., released under an [Apache 2.0 license](https://www.planetcrust.com/what-does-apache-2-0-license-mean))
    MPT suite by MosaicML. MPT-7B [9] was released first, which garnered a lot of
    interest (i.e., it was basically a commercially-usable alternative for LLaMA-7B!).
    In fact, MPT-7B was downloaded over 3M times on [HuggingFace](https://huggingface.co/mosaicml/mpt-7b)
    before the larger MPT-30B [10] model was made available!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LLaMA 令人印象深刻，但该套件中的模型都无法用于商业应用——*它们仅从研究的角度具有价值*。幸运的是，LLaMA 的提案很快被 MosaicML
    开发并发布的商业可用（即，按照 [Apache 2.0 许可](https://www.planetcrust.com/what-does-apache-2-0-license-mean)
    发布）的 MPT 套件所跟进。首先发布的是 MPT-7B [9]，它引起了大量关注（即，它基本上是 LLaMA-7B 的商业可用替代品！）。实际上，在更大的
    MPT-30B [10] 模型发布之前，MPT-7B 在 [HuggingFace](https://huggingface.co/mosaicml/mpt-7b)
    上的下载量超过了 300 万次！
- en: '![](../Images/6667da3f4b5edca42345e89e4ce1ff88.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6667da3f4b5edca42345e89e4ce1ff88.png)'
- en: (from [9, 10])
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [9, 10]）
- en: 'The main differences between these two models are:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型之间的主要区别是：
- en: They are pre-trained using slightly different mixes of data; see above.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们使用略有不同的数据混合进行预训练；见上文。
- en: MPT-30B is trained using a longer context length of 8K tokens.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MPT-30B 采用了更长的上下文长度，达到 8K 令牌。
- en: However, these models both perform well and can be used in commercial applications,
    which led them to become popular in the AI community.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些模型都表现出色，并可以用于商业应用，这使它们在人工智能社区中变得非常受欢迎。
- en: '![](../Images/f6ddad759e3ef2bc8b2c5ed5be03ed9b.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6ddad759e3ef2bc8b2c5ed5be03ed9b.png)'
- en: (from [9])
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [9]）
- en: '**Does MPT live up to the hype?** Although LLaMA drastically improved state-of-the-art
    performance for open-source LLMs, the MPT suite rivaled this performance. In particular,
    MPT-7B matches the performance of LLaMA-7B across a variety of standard benchmarks;
    see above. Going further, MPT-30B tends to match the performance of GPT-3\. Compared
    to similarly-sized open-source models (e.g., LLaMA-30B and Falcon-40B), MPT-30B
    tends to perform slightly worse; see below. However, it is better than these models
    on coding-related tasks and can be hosted on a single GPU (with quantization).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**MPT 是否符合炒作？** 尽管 LLaMA 显著提升了开源 LLM 的最先进性能，但 MPT 套件与此性能不相上下。特别是，MPT-7B 在各种标准基准测试中与
    LLaMA-7B 的表现相匹配；见上文。更进一步，MPT-30B 的表现趋向于与 GPT-3 相当。与同样大小的开源模型（例如 LLaMA-30B 和 Falcon-40B）相比，MPT-30B
    的表现略逊一筹；见下文。然而，它在与编码相关的任务中优于这些模型，并且可以在单个 GPU 上运行（通过量化）。'
- en: '![](../Images/acc9091d4b79f5f783ce5ece777d65cf.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acc9091d4b79f5f783ce5ece777d65cf.png)'
- en: (from [10])
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [10]）
- en: '**MPT variants.** In addition to the pre-trained MPT-7B and MPT-30B models,
    a variety of fine-tuned MPT models were released, such as [instruct](https://huggingface.co/mosaicml/mpt-30b-instruct)
    and [chat](https://huggingface.co/mosaicml/mpt-30b-chat) versions of both MPT
    models. Additionally, a “StoryWriter” version of MPT-7B was created by fine-tuning
    on data with a 64K token context length. Given that pre-training an LLM is significantly
    more expensive than fine-tuning, a variety of different fine-tuned MPT variants
    could be created at marginal cost; see below.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**MPT 变体。** 除了预训练的 MPT-7B 和 MPT-30B 模型，还发布了各种微调的 MPT 模型，例如 [instruct](https://huggingface.co/mosaicml/mpt-30b-instruct)
    和 [chat](https://huggingface.co/mosaicml/mpt-30b-chat) 版本的 MPT 模型。此外，通过对具有 64K
    token 上下文长度的数据进行微调，还创建了 “StoryWriter” 版本的 MPT-7B。鉴于预训练 LLM 比微调要昂贵得多，因此可以以边际成本创建各种不同的微调
    MPT 变体；见下文。'
- en: '![](../Images/cadb7f5b56f274d1a89097a7a98f06f5.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cadb7f5b56f274d1a89097a7a98f06f5.png)'
- en: (from [9])
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [9]）
- en: '**But wait… there’s more!** MPT models are useful (especially for those working
    on commercial applications), but the models are also accompanied by an entire
    suite of software (i.e., the [LLM foundry](https://github.com/mosaicml/llm-foundry))
    released by MosaicML. This open-source code can be used to pre-train and fine-tune
    MPT models, making the MPT suite an incredibly valuable tool for exploring specialized
    use cases with LLMs.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**但等一下…还有更多！** MPT 模型非常有用（尤其是对于从事商业应用的人），但这些模型还伴随着一整套软件（即，[LLM 工厂](https://github.com/mosaicml/llm-foundry)），由
    MosaicML 发布。这些开源代码可以用于预训练和微调 MPT 模型，使 MPT 套件成为探索 LLM 特殊用例的极其宝贵的工具。'
- en: 'Falcon: Reaching New Heights in Open-Source Performance'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Falcon：在开源性能方面达到新高度
- en: '![](../Images/ad0740575bb930c8ee0391e4195bac92.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad0740575bb930c8ee0391e4195bac92.png)'
- en: (from [1])
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Although many advances had been made in the space of open-source LLMs, available
    models still lagged behind proprietary LLMs in terms of performance for quite
    some time. The proposal of the Falcon suite of LLMs [11] was the first time that
    the quality of proprietary LLMs was truly rivaled by an open-source alternative.
    Two variants of Falcon are available — Falcon-7B and Falcon-40B. In addition to
    being commercially licensed, these Falcon models perform incredibly well due to
    being pre-trained on a massive, custom-curated corpus. Notably, the instruct variant
    of Falcon-40B was the top-performing model on the [OpenLLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    (by a significant margin) for several months.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在开源 LLM 领域取得了许多进展，但可用的模型在性能上仍然落后于专有 LLM 一段时间。Falcon 套件的提出 [11] 是开源替代方案首次真正与专有
    LLM 的质量相匹敌。Falcon 提供了两种变体 — Falcon-7B 和 Falcon-40B。除了商业许可外，这些 Falcon 模型由于在大规模、自定义策划的语料库上进行预训练，表现非常出色。值得注意的是，Falcon-40B
    的 instruct 变体在 [OpenLLM 排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    上连续几个月是表现最佳的模型（差距显著）。
- en: “Challenging existing beliefs on data quality and LLMs, models trained on adequately
    filtered and deduplicated web data alone can match the performance of models trained
    on curated data.” *— from [12]*
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “挑战现有的数据质量和 LLM 信念，仅凭经过适当筛选和去重的网络数据训练的模型可以匹敌使用策划数据训练的模型。” *— 来自 [12]*
- en: '**Curating data from the web.** The Falcon models are trained over a massive
    textual corpus called RefinedWeb [12] that contains over 5 trillion tokens of
    text. Only 1.5 trillion tokens and 1 trillion tokens of RefinedWeb are actually
    used for pre-training Falcon-7B and Falcon-40B, respectively. Although a majority
    of LLMs are pre-trained over public sources of curated data, the authors of Falcon
    choose instead to construct their own pre-training dataset exclusively using data
    from the web (i.e., [CommonCrawl](https://commoncrawl.org/)). To filter this data,
    a novel pipeline is created that emphasizes simple, but effective, components;
    see below.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**从网络中策划数据。** Falcon 模型是在一个名为 RefinedWeb [12] 的大规模文本语料库上训练的，该语料库包含超过 5 万亿个标记的文本。实际上，仅有
    1.5 万亿个标记和 1 万亿个标记的 RefinedWeb 被用于分别预训练 Falcon-7B 和 Falcon-40B。尽管大多数 LLM 是在公开策划的数据源上进行预训练的，但
    Falcon 的作者选择仅使用来自网络的数据（即 [CommonCrawl](https://commoncrawl.org/)）构建自己的预训练数据集。为筛选这些数据，创建了一个新的流程，强调简单但有效的组件；见下文。'
- en: '![](../Images/341b3a3c1bb2512198d79709973a53bf.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/341b3a3c1bb2512198d79709973a53bf.png)'
- en: (from [12, 13])
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [12, 13])
- en: The RefinedWeb corpus shows that a massive amount of high-quality text data
    — *beyond the scale of datasets explored previously* — can be efficiently curated
    from the web. After filtering is applied, models trained on this data can even
    outperform comparable models trained over curated sources of data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: RefinedWeb 语料库表明，可以从网络中高效策划出大量高质量的文本数据——*超出了以前探索的数据集的规模*。在应用过滤后，基于这些数据训练的模型甚至可以超越在策划数据源上训练的类似模型。
- en: '![](../Images/40ccf29d0c7358a4dd95dd7527665ac0.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40ccf29d0c7358a4dd95dd7527665ac0.png)'
- en: (from [12])
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [12])
- en: The exact datasets used to train Falcon-7B and Falcon-40B are shown above. Notably,
    Falcon-7B is trained over English-only data, while Falcon-40B has data from a
    variety of European languages inserted into its pre-training set.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练 Falcon-7B 和 Falcon-40B 的确切数据集如上所示。值得注意的是，Falcon-7B 仅在英语数据上进行训练，而 Falcon-40B
    的预训练集中则包含了各种欧洲语言的数据。
- en: '**A new SOTA.** Currently, no publication for the Falcon LLMs has been released.
    As such, the only formal evaluation of these models was performed via the OpenLLM
    leaderboard, where the Falcon-40B model fared quite well. In particular, [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)
    was the state-of-the-art model for some time, outperforming other models by a
    significant margin; see below.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**新的 SOTA。** 目前尚未发布关于 Falcon LLM 的任何出版物。因此，对这些模型的唯一正式评估是通过 OpenLLM 排行榜进行的，其中
    Falcon-40B 模型表现相当不错。特别是，[Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)
    曾是领先的模型，显著超越其他模型；见下文。'
- en: '![](../Images/61a93b643212a4d71d7eb1866dc9bb15.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61a93b643212a4d71d7eb1866dc9bb15.png)'
- en: (from the Open LLM Leaderboard)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 Open LLM 排行榜)
- en: Qualitatively, some practitioners have claimed that Falcon-40B seems to underperform
    LLaMA-based models. Although an awareness of these remarks is useful, such evidence
    is anecdotal and subjective. In standardized natural language benchmarks, Falcon
    LLMs perform incredibly well, leading them to retain state-of-the-art performance
    among open-source models for a long time.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从质量上看，一些从业者声称 Falcon-40B 似乎不如 LLaMA 基础模型。尽管了解这些评论是有用的，但这些证据是轶事性和主观的。在标准化的自然语言基准测试中，Falcon
    LLM 的表现非常出色，使其在开源模型中长时间保持最先进的性能。
- en: 'LLaMA-2: Current State-of-the-Art'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'LLaMA-2: 当前的最先进技术'
- en: '![](../Images/e4a98130bd933b03c24a04b294d44d6a.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4a98130bd933b03c24a04b294d44d6a.png)'
- en: (from [14])
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [14])
- en: Although Falcon-40B was the state-of-the-art open-source LLM for some time,
    the recent release of the LLaMA-2 model suite dethroned this model. Similarly
    to LLAMA-1, LLaMA-2 [14] is comprised of several different LLMs with sizes ranging
    from 7 billion to 70 billion parameters and uses only publicly available data
    for pre-training. Both pre-trained and fine-tuned[6](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-6-135439692)
    versions of LLAMA-2 models are released, though we will only cover the pre-trained
    models within this overview due to our focus upon open-source base models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Falcon-40B 曾是领先的开源 LLM，但最近发布的 LLaMA-2 模型套件已使该模型失去领先地位。与 LLAMA-1 类似，LLaMA-2
    [14] 包含多个不同的 LLM，其规模从 70 亿到 700 亿参数不等，并仅使用公开数据进行预训练。LLAMA-2 模型既有预训练版本也有微调版本[6](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-6-135439692)，但由于我们专注于开源基础模型，本文仅涵盖预训练模型。
- en: “There have been public releases of pre-trained LLMs (such as BLOOM that match
    the performance of closed pre-trained competitors like GPT-3 and Chinchilla, but
    none of these models are suitable substitutes for closed product LLMs, such as
    ChatGPT, BARD, and Claude.” *— from [14]*
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “已经有公开发布的预训练LLMs（例如BLOOM，其性能与封闭预训练的竞争对手如GPT-3和Chinchilla相当，但这些模型都不适合作为封闭产品LLMs的替代品，例如ChatGPT、BARD和Claude。”
    *— 来自 [14]*
- en: LLaMA-2 continues to narrow the gap in performance between open and closed-source
    language models by releasing a suite of higher-performing base models that are
    pre-trained over a massive dataset. As we will see, these models still fall short
    of matching the quality of proprietary models, but they come much closer than
    any open-source model before them.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-2通过发布一套在大规模数据集上进行预训练的更高性能基础模型，继续缩小开源和闭源语言模型之间的性能差距。正如我们将看到的，这些模型仍未达到专有模型的质量，但比之前的任何开源模型都要接近得多。
- en: '![](../Images/b801cdd176fb57461ca9cb6f109e2535.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b801cdd176fb57461ca9cb6f109e2535.png)'
- en: (from [14])
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [14]）
- en: '**How is it different?** LLaMA-2 adopts an approach that is quite similar to
    its predecessor, aside from a few minor (but impactful) differences. First, LLaMA-2
    models are pre-trained over 40% more data — 2 trillion tokens in total, compared
    to 1.4 trillion tokens for LLaMA-1\. Additionally, LLaMA-2 models are trained
    with a slightly longer context length, and the larger models use grouped query
    attention (GQA) within their underlying architecture. Interestingly, authors in
    [14] note that LLaMA-2’s pre-training set up-samples sources of data that are
    known to be more knowledgeable. Such a change is made in an attempt to emphasize
    factual sources, increase knowledge, and reduce hallucinations.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**它有什么不同？** LLaMA-2采用了一种与其前身非常相似的方法，除了几个小的（但影响深远的）区别。首先，LLaMA-2模型在超过40%更多的数据上进行预训练——总共2万亿个标记，而LLaMA-1为1.4万亿个标记。此外，LLaMA-2模型使用稍长的上下文长度进行训练，较大的模型在其底层架构中使用了分组查询注意力（GQA）。有趣的是，[14]中的作者指出，LLaMA-2的预训练设置采样了更多已知更具知识性的来源。这样的变化旨在强调事实来源、增加知识和减少幻觉。'
- en: '![](../Images/d5d43f1550c7ee0b23a708140d768fdc.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5d43f1550c7ee0b23a708140d768fdc.png)'
- en: (from [15])
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [15]）
- en: '**What is GQA?** As proposed in [15], GQA is a modification to multi-headed
    self-attention that can improve inference efficiency in LLMs. A typical multi-headed
    self-attention mechanism has `N` total query, key, and value heads, creating `N`
    self-attention heads in total. In GQA, we divide these `N` total heads into groups,
    where key and value heads are shared within each group; see above. Such an approach
    is an interpolation between vanilla multi-headed self-attention and multi-query
    attention, which uses a shared key and value projection across all `N` heads.
    GQA is found in [15] to improve inference speed comparably to multi-query attention,
    while maintaining the performance of vanilla multi-headed attention.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是GQA？** 如[15]所提出，GQA是对多头自注意力的修改，可以提高LLMs的推理效率。典型的多头自注意力机制有`N`个查询、键和值头，总共创建`N`个自注意力头。在GQA中，我们将这些`N`个头分成组，其中键和值头在每组内共享；见上文。这种方法是在普通多头自注意力和多查询注意力之间的插值，后者在所有`N`个头中使用共享的键和值投影。[15]发现，GQA在提高推理速度方面与多查询注意力相当，同时保持普通多头注意力的性能。'
- en: '![](../Images/b5c3329f9503ea48e5ab203aa4479509.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5c3329f9503ea48e5ab203aa4479509.png)'
- en: (from [14])
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [14]）
- en: '**LLaMA-2 is really good.** Compared to popular open-source models (e.g., MPT,
    Falcon, and LLaMA-1), the LLaMA-2 base LLMs perform quite well. In fact, LLaMA-2–70B
    sets a new state-of-the-art among open-source LLMs on all tasks considered; see
    above. Notably, however, LLaMA-2 was somewhat criticized for its (relatively)
    poor performance on coding-based tasks (e.g., [HumanEval](https://arxiv.org/abs/2107.03374)).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLaMA-2确实很出色。** 与流行的开源模型（例如MPT、Falcon和LLaMA-1）相比，LLaMA-2基础LLMs表现非常好。事实上，LLaMA-2–70B在所有考虑的任务中设立了开源LLMs的新最佳水平；见上文。然而，值得注意的是，LLaMA-2因在编码任务上的（相对）较差表现而受到一些批评（例如，[HumanEval](https://arxiv.org/abs/2107.03374)）。'
- en: '![](../Images/6db4644805a2095441473a7763fb7247.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6db4644805a2095441473a7763fb7247.png)'
- en: (from [14])
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [14]）
- en: When compared to proprietary models, LLaMA-2 base models perform worse; see
    above. However, we should keep in mind that this comparison is made between a
    base LLM and aligned models like GPT-3.5 and GPT-4\. When compared to other popular
    base LLMs (e.g., PaLM [4]), LLaMA-2 performs favorably!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与专有模型相比，LLaMA-2基础模型的表现较差；见上文。然而，我们应当记住，这一比较是在基础LLM与像GPT-3.5和GPT-4这样的对齐模型之间进行的。与其他流行的基础LLM（例如，PaLM
    [4]）相比，LLaMA-2的表现是有利的！
- en: '**Commercial license.** While LLaMA-1 could only be used for research, LLaMA-2
    is released under a [commercial license](https://github.com/facebookresearch/llama/blob/main/LICENSE),
    meaning that — *like MPT and Falcon* — the models can be used in commercial applications.
    However, the license used for LLaMA-2 is not a standard Apache 2.0 license — it
    has a few caveats that should be considered by practitioners. Most notably, any
    entity/application powered by LLaMA-2 with over 700 million monthly active users
    must obtain a license from Meta to use LLaMA-2\. Read more about LLaMA-2’s license
    [here](https://opensourceconnections.com/blog/2023/07/19/is-llama-2-open-source-no-and-perhaps-we-need-a-new-definition-of-open/).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**商业许可证。** 尽管LLaMA-1仅可用于研究，LLaMA-2则在[商业许可证](https://github.com/facebookresearch/llama/blob/main/LICENSE)下发布，这意味着—
    *与MPT和Falcon类似* — 这些模型可以用于商业应用。然而，LLaMA-2的许可证并非标准的Apache 2.0许可证，它有一些需要从业者考虑的附加条件。最值得注意的是，任何由LLaMA-2驱动、每月活跃用户超过7亿的实体/应用程序必须从Meta处获得使用LLaMA-2的许可证。有关LLaMA-2许可证的更多信息，请参阅[这里](https://opensourceconnections.com/blog/2023/07/19/is-llama-2-open-source-no-and-perhaps-we-need-a-new-definition-of-open/)。'
- en: Trends in Open-Source LLMs
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源LLM的趋势
- en: 'Given that LLaMA, MPT, Falcon, and LLaMA-2 perform so much better than their
    predecessors, we might reasonably ask: *what led the current generation of open-source
    LLMs to perform so well?* Here, we will quickly look at a few key properties of
    these models that were especially valuable in catalyzing their impressive performance
    and quick rise to popularity. In particular, these models *i)* were pre-trained
    over a massive amount of data and *ii)* emphasize inference efficiency.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于LLaMA、MPT、Falcon和LLaMA-2的表现远超其前身，我们可以合理地问：*是什么导致当前一代开源LLM表现如此出色？* 在这里，我们将快速查看这些模型的一些关键特性，这些特性在推动它们的卓越表现和迅速流行中尤为重要。特别是，这些模型
    *i)* 在大量数据上进行了预训练，以及 *ii)* 强调推理效率。
- en: Better Data = Better Performance!
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更好的数据 = 更好的表现！
- en: 'The key difference between current open-source LLMs and those that came before
    them is the dataset used for pre-training. While models like OPT and BLOOM are
    trained on 180 billion and 341 billion tokens, respectively, current open-source
    models are pre-trained over significantly larger datasets:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当前开源LLM与之前的LLM之间的关键区别在于用于预训练的数据集。尽管像OPT和BLOOM这样的模型分别在1800亿和3410亿个标记上进行训练，但当前开源模型是在显著更大的数据集上进行预训练的：
- en: '*LLaMA*: 1.4 trillion tokens'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLaMA*: 1.4万亿个标记'
- en: '*MPT*: 1 trillion token'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MPT*: 1万亿个标记'
- en: '*Falcon*: 1–1.5 trillion token'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Falcon*: 1–1.5万亿个标记'
- en: '*LLaMA-2*: 2 trillion tokens'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLaMA-2*: 2万亿个标记'
- en: Current open-source LLMs increase the amount of data used for pre-training by
    (nearly) an order of magnitude! In fact, these pre-training datasets are similarly-sized
    to those used for proprietary LLMs. For example, MassiveText (i.e., used to train
    Gopher [13] and Chinchilla [2]) contains roughly 2.3 trillion tokens, though only
    a subset is actually used for pre-training; see below.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的开源LLM将用于预训练的数据量提高了（几乎）一个数量级！事实上，这些预训练数据集的规模与用于专有LLM的数据集相似。例如，MassiveText（即用于训练Gopher
    [13]和Chinchilla [2]）包含大约2.3万亿个标记，尽管实际上仅使用了其中的一部分进行预训练；见下文。
- en: '![](../Images/5dad8fba9cf56bcd2c7a8db9376f26af.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5dad8fba9cf56bcd2c7a8db9376f26af.png)'
- en: (from [13])
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: （来源 [13]）
- en: '**Size isn’t everything!** In addition to increasing the amount of pre-training
    data significantly, current open-source LLMs pay close attention to the composition
    and quality of data. For example, the proportion of code is increased within the
    datasets used for training MPT, allowing the resulting models to perform much
    better on coding-based tasks. Additionally, Falcon-40B proposes an entirely new
    pipeline for constructing high-quality corpora of text from the web, while LLaMA-2
    claims to use an updated data pipeline and mix for pre-training. Overall, focusing
    on the quality and composition of the pre-training dataset seems to be a common
    trend within recent research on open-source LLMs.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**规模并非一切！** 除了显著增加预训练数据的数量，当前的开源LLM还非常关注数据的组成和质量。例如，在用于训练MPT的数据集中，代码的比例得到了增加，从而使得生成的模型在基于编码的任务上表现更佳。此外，Falcon-40B提出了一种全新的从网络构建高质量文本语料库的管道，而LLaMA-2则声称使用了更新的数据管道和混合方式进行预训练。总的来说，专注于预训练数据集的质量和组成似乎是近期开源LLM研究中的一种共同趋势。'
- en: “We performed more robust data cleaning, updated our data mixes, trained on
    40% more total tokens, doubled the context length, and used grouped-query attention
    (GQA) to improve inference scalability for our larger models.” *— from [14]*
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们进行了更全面的数据清理，更新了数据混合，训练了更多的总标记，增加了上下文长度，并使用了分组查询注意力（GQA）以提升我们较大模型的推理可扩展性。”
    *— 来源于 [14]*
- en: Optimizing for Faster Inference
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化以实现更快的推理
- en: In making the decision between using an open or closed-source LLM, practitioners
    have to consider more than just performance. Paid language model APIs might achieve
    impressive performance across a wide scope of tasks, but they oftentimes cannot
    be fine-tuned on domain-specific data. On the other hand, however, a major consideration
    when building applications with open-source LLMs is the cost of deploying the
    model. Given the difficulty of hosting LLMs, recent open-source models are oftentimes
    optimized for fast and easy inference. In fact, MPT-30B [10] is specifically sized
    so that it can be hosted on a single GPU!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定使用开源或闭源LLM时，实践者必须考虑的不仅仅是性能。付费语言模型API可能在广泛的任务范围内取得令人印象深刻的表现，但它们往往无法在特定领域的数据上进行微调。另一方面，使用开源LLM构建应用时，一个主要考虑因素是部署模型的成本。鉴于托管LLM的难度，近期的开源模型往往经过优化，以实现快速和简便的推理。实际上，MPT-30B
    [10] 特别调整了尺寸，以便可以在单个GPU上托管！
- en: '![](../Images/bb4a8b793ab705597715a58ce57e7250.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb4a8b793ab705597715a58ce57e7250.png)'
- en: (from [15, 16, 17])
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [15, 16, 17]）
- en: '**Modified architecture.** Beyond being slightly smaller than most proprietary
    models, current open-source LLMs adopt a variety of architectural tricks — shown
    in the figure above — to speed up the inference process, such as:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**修改后的架构。** 除了比大多数专有模型稍小之外，当前的开源LLM还采用了多种架构技巧 —— 如上图所示 —— 来加速推理过程，例如：'
- en: Low Precision Layer Norm
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低精度层归一化
- en: Flash Attention
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速注意力
- en: Multi-Query Attention
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多查询注意力
- en: Parallel Transformer
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行变换器
- en: Group-Query Attention
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组查询注意力
- en: Additionally, several other architecture modifications — e.g., RoPE embeddings,
    ALiBi, SwiGLU activations, and more — are adopted to improve performance. Current
    open-source LLMs apply simple modifications to the decoder-only transformer architecture
    to improve performance and inference speed.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还采用了多种其他架构修改，例如RoPE嵌入、ALiBi、SwiGLU激活函数等，以提升性能。当前的开源LLM对仅有解码器的变换器架构进行了简单的修改，以改善性能和推理速度。
- en: Final Thoughts
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Within this overview, we have studied the evolution of open-source LLMs from
    initial, lower-quality models (e.g., BLOOM and OPT) to the more recent, powerful
    base models (e.g., LLaMA and MPT). To improve upon the performance of their predecessors,
    these recent models primarily focused upon curating larger, higher-quality datasets
    for pre-training, which resulted in a drastic improvement in quality. Given that
    a high-quality base model is a prerequisite for any LLM application, these models
    had a significant impact upon the raise in popularity of open-source LLMs. Instead
    of having to invest significant funds into pre-training a model from scratch,
    any practitioner can now leverage powerful base LLMs whether is be for research
    purposes or commercial applications.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本概述中，我们研究了开源 LLM 的演变，从初期的低质量模型（例如 BLOOM 和 OPT）到最近的强大基础模型（例如 LLaMA 和 MPT）。为了提升前辈模型的性能，这些最近的模型主要集中于策划更大、更高质量的预训练数据集，这导致了质量的显著提高。鉴于高质量的基础模型是任何
    LLM 应用的前提，这些模型对开源 LLM 的流行程度产生了重大影响。如今，任何从业者都可以利用强大的基础 LLM，无论是用于研究目的还是商业应用，而无需投入大量资金从头开始预训练模型。
- en: Connect with me!
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与我联系！
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. If you liked this overview, subscribe
    to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I help readers understand AI research via overviews of relevant topics from
    the ground up. You can also follow me on [X](https://twitter.com/cwolferesearch)
    and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/), or
    check out my [other writings](https://medium.com/@wolfecameron) on medium!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢阅读本文。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的 AI 总监。我研究深度学习的实证和理论基础。如果你喜欢这个概述，请订阅我的 [Deep (Learning) Focus 时事通讯](https://cameronrwolfe.substack.com/)，在这里我帮助读者通过从基础到高级的相关主题概述来理解
    AI 研究。你也可以在 [X](https://twitter.com/cwolferesearch) 和 [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)
    上关注我，或者查看我在 medium 上的 [其他文章](https://medium.com/@wolfecameron)！
- en: Bibliography
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考书目
- en: '[1] Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.”
    *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Touvron, Hugo 等人。“Llama: 开放且高效的基础语言模型。” *arXiv 预印本 arXiv:2302.13971* (2023)。'
- en: '[2] Hoffmann, Jordan, et al. “Training compute-optimal large language models.”
    *arXiv preprint arXiv:2203.15556* (2022).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Hoffmann, Jordan 等人。“训练计算最优的大型语言模型。” *arXiv 预印本 arXiv:2203.15556* (2022)。'
- en: '[3] Zhou, Chunting, et al. “Lima: Less is more for alignment.” *arXiv preprint
    arXiv:2305.11206* (2023).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Zhou, Chunting 等人。“Lima: 对齐的‘少即是多’。” *arXiv 预印本 arXiv:2305.11206* (2023)。'
- en: '[4] Chowdhery, Aakanksha, et al. “Palm: Scaling language modeling with pathways.”
    *arXiv preprint arXiv:2204.02311* (2022).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Chowdhery, Aakanksha 等人。“Palm: 使用路径扩展语言建模。” *arXiv 预印本 arXiv:2204.02311*
    (2022)。'
- en: '[5] Taori, Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.”
    (2023).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Taori, Rohan 等人。“斯坦福 Alpaca: 一种遵循指令的 LLaMA 模型。” (2023)。'
- en: '[6] Chiang, Wei-Lin et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4
    with 90%* ChatGPT Quality.” (2023).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Chiang, Wei-Lin 等人。“Vicuna: 一个以 90%* ChatGPT 质量给 GPT-4 留下深刻印象的开源聊天机器人。”
    (2023)。'
- en: '[7] Geng, Xinyang et al. “Koala: A Dialogue Model for Academic Research.” (2023).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Geng, Xinyang 等人。“Koala: 一种用于学术研究的对话模型。” (2023)。'
- en: '[8] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and
    Andriy Mulyar. GPT4All: Training an assistant-style chatbot with large scale data
    distillation from GPT-3.5-Turbo, 2023.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt 和 Andriy
    Mulyar。“GPT4All: 通过从 GPT-3.5-Turbo 大规模数据蒸馏训练助理风格聊天机器人。” 2023年。'
- en: '[9] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable
    Llms.” *MosaicML*, 5 May 2023, [www.mosaicml.com/blog/mpt-7b.](http://www.mosaicml.com/blog/mpt-7b.)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] “介绍 MPT-7B: 开源、可商业化 LLM 的新标准。” *MosaicML*, 2023年5月5日, [www.mosaicml.com/blog/mpt-7b.](http://www.mosaicml.com/blog/mpt-7b.)'
- en: '[10] “MPT-30B: Raising the Bar for Open-Source Foundation Models.” *MosaicML*,
    22 June 2023, [www.mosaicml.com/blog/mpt-30b.](http://www.mosaicml.com/blog/mpt-30b.)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] “MPT-30B: 提高开源基础模型的标准。” *MosaicML*，2023年6月22日，[www.mosaicml.com/blog/mpt-30b.](http://www.mosaicml.com/blog/mpt-30b.)'
- en: '[11] “Introducing Falcon LLM”, *Technology Innovation Institute*, 7 June 2023,
    [https://falconllm.tii.ae/.](https://falconllm.tii.ae/.)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] “介绍 Falcon LLM”， *技术创新研究所*，2023年6月7日，[https://falconllm.tii.ae/.](https://falconllm.tii.ae/.)'
- en: '[12] Penedo, Guilherme, et al. “The RefinedWeb dataset for Falcon LLM: outperforming
    curated corpora with web data, and web data only.” *arXiv preprint arXiv:2306.01116*
    (2023).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Penedo, Guilherme 等人. “Falcon LLM 的 RefinedWeb 数据集：通过网络数据超越精选语料库，仅使用网络数据。”
    *arXiv 预印本 arXiv:2306.01116* (2023)。'
- en: '[13] Rae, Jack W., et al. “Scaling language models: Methods, analysis & insights
    from training gopher.” *arXiv preprint arXiv:2112.11446* (2021).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Rae, Jack W. 等人. “语言模型扩展：方法、分析与从训练 Gopher 中获得的见解。” *arXiv 预印本 arXiv:2112.11446*
    (2021)。'
- en: '[14] Touvron, Hugo, et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.”
    *arXiv preprint arXiv:2307.09288* (2023).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Touvron, Hugo 等人. “Llama 2: 开放基础与微调聊天模型。” *arXiv 预印本 arXiv:2307.09288*
    (2023)。'
- en: '[15] Ainslie, Joshua, et al. “GQA: Training Generalized Multi-Query Transformer
    Models from Multi-Head Checkpoints.” *arXiv preprint arXiv:2305.13245* (2023).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Ainslie, Joshua 等人. “GQA: 从多头检查点训练通用多查询变换器模型。” *arXiv 预印本 arXiv:2305.13245*
    (2023)。'
- en: '[16] Vaswani, Ashish, et al. “Attention is all you need.” *Advances in neural
    information processing systems* 30 (2017).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Vaswani, Ashish 等人. “注意力即你所需。” *《神经信息处理系统进展》* 30 (2017)。'
- en: '[17] Dao, Tri, et al. “Flashattention: Fast and memory-efficient exact attention
    with io-awareness.” *Advances in Neural Information Processing Systems* 35 (2022):
    16344–16359.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Dao, Tri 等人. “Flashattention: 快速且内存高效的精确注意力机制，具备 IO 感知功能。” *《神经信息处理系统进展》*
    35 (2022): 16344–16359。'
- en: '[18] Dao, Tri. “FlashAttention-2: Faster Attention with Better Parallelism
    and Work Partitioning.” *arXiv preprint arXiv:2307.08691* (2023).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Dao, Tri. “FlashAttention-2: 更快的注意力机制，具有更好的并行性和工作分区。” *arXiv 预印本 arXiv:2307.08691*
    (2023)。'
