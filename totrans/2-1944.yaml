- en: 'Support Vector Machine with Scikit-Learn: A Friendly Introduction'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Scikit-Learn 的支持向量机：友好的介绍
- en: 原文：[https://towardsdatascience.com/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d](https://towardsdatascience.com/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d](https://towardsdatascience.com/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d)
- en: Every data scientist should have SVM in their toolbox. Learn how to master this
    versatile model with a hands-on introduction.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 每个数据科学家都应该在他们的工具箱中拥有 SVM。通过实践介绍学习如何掌握这一多功能模型。
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page-----a2969f2ff00d--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----a2969f2ff00d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a2969f2ff00d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a2969f2ff00d--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----a2969f2ff00d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@riccardo.andreoni?source=post_page-----a2969f2ff00d--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----a2969f2ff00d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a2969f2ff00d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a2969f2ff00d--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----a2969f2ff00d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a2969f2ff00d--------------------------------)
    ·9 min read·Oct 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a2969f2ff00d--------------------------------)
    ·阅读时间 9 分钟·2023年10月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ca089d89ca86dd60ffd60783581574e6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca089d89ca86dd60ffd60783581574e6.png)'
- en: 'Image source: [unsplash.com](https://unsplash.com/photos/3DkouQeZjp4).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[unsplash.com](https://unsplash.com/photos/3DkouQeZjp4)。
- en: 'Among the available Machine Learning models, there exists one whose versatility
    makes it a must-have tool for **every data scientist toolbox**: [**Support Vector
    Machine**](https://en.wikipedia.org/wiki/Support_vector_machine) ([SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有的机器学习模型中，有一种模型的多功能性使它成为**每个数据科学家工具箱中的必备工具**：[**支持向量机**](https://en.wikipedia.org/wiki/Support_vector_machine)
    ([SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html))。
- en: 'SVM is a powerful and versatile algorithm, which, at its core, can delineate
    **optimal hyperplanes** in a high-dimensional space, effectively **segregating
    the different classes** of a dataset. But it doesn’t stop here! Its effectiveness
    is not limited to **classification** tasks: SVM is well-suited even for **regression**
    and **outlier detection** tasks.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）是一种强大而多功能的算法，其核心能够在高维空间中划分**最优超平面**，有效地**隔离数据集中的不同类别**。但它不仅仅止于此！它的有效性不限于**分类**任务：SVM
    也非常适合**回归**和**异常值检测**任务。
- en: One feature makes the SVM approach particularly effective. Instead of processing
    the entire dataset, as [KNN does](https://levelup.gitconnected.com/optical-character-recognition-with-knn-classifier-a-step-by-step-guide-b6d7b90e1122),
    SVM strategically focuses only on the subset of data points located near the decision
    boundaries. These points are called [*support vectors*](https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/#:~:text=Support%20Vectors%3A%20These%20are%20the,is%20considered%20a%20good%20margin.),
    and the mathematics behind this unique idea will be simply explained in the upcoming
    sections.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个特点使得 SVM 方法特别有效。与 [KNN 方法](https://levelup.gitconnected.com/optical-character-recognition-with-knn-classifier-a-step-by-step-guide-b6d7b90e1122)
    处理整个数据集不同，SVM 战略性地只关注位于决策边界附近的数据点。这些点被称为[*支持向量*](https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/#:~:text=Support%20Vectors%3A%20These%20are%20the,is%20considered%20a%20good%20margin.)，这种独特想法背后的数学将在接下来的部分中简单解释。
- en: By doing so, SVM Algorithm is **computationally conservative** and ideal for
    tasks involving medium or even medium-large datasets.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，SVM 算法在**计算上是保守的**，非常适合处理中等或甚至中大型数据集的任务。
- en: As I do in all my articles, I won't just explain the theoretical concepts, but
    I will also provide you with **coding examples** to familiarize yourself with
    the [**Scikit-Learn**](https://scikit-learn.org/) (sklearn) [**Python**](https://www.python.org/)
    library.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在所有文章中所做的，我不仅会解释理论概念，还会提供**编码示例**，以帮助你熟悉[**Scikit-Learn**](https://scikit-learn.org/)（sklearn）[**Python**](https://www.python.org/)
    库。
- en: Let’s analyze Support Vector Machine (SVM) algorithms, and explore Machine Learning
    techniques, Python programming, and Data Science applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析支持向量机（SVM）算法，并探索机器学习技术、Python 编程和数据科学应用。
- en: Linear SVM Classification
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性 SVM 分类
- en: At its core, SVM classification resembles the elegant simplicity of Linear Algebra.
    Imagine a dataset in two-dimensional space, with two distinct classes to be separated.
    Linear SVM tries to separate the two classes with the best possible straight line.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，SVM 分类类似于线性代数的优雅简洁。想象一个二维空间中的数据集，其中有两个需要分开的不同类别。线性 SVM 尝试用最佳的直线分隔这两个类别。
- en: '![](../Images/0c3a48ffc42b897621e5195ca0b7126a.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c3a48ffc42b897621e5195ca0b7126a.png)'
- en: Image by the author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: 'What does it mean “best” in this context? SVM searches for the optimal separation
    line: a line that not only separates the classes, but does it with the maximum
    possible distance from the closest training instances of each classes. That distance
    is called **margin**. The data points that lay on the margin edge are the key
    element of the linear SVM classifier, and are called **Support Vectors**.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，“最佳”是什么意思？SVM 寻找的是最优的分隔线：不仅能分隔类别，而且与每个类别的最近训练实例之间的距离尽可能远。这个距离称为**边际**。位于边际边缘的数据点是线性
    SVM 分类器的关键元素，被称为**支持向量**。
- en: '![](../Images/69aaa61db77873bc703ca9c4bf91ea0e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69aaa61db77873bc703ca9c4bf91ea0e.png)'
- en: Equation of the decision boundary to be optimized.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 需要优化的决策边界的方程。
- en: It is important to note that the separator is defined exclusively by the Support
    Vectors. As a consequence, adding more “off-the-street” training instances has
    no impact on the decision boundary. When more training instances that are not
    Support Vectors are added into the training set, the decision boundary doesn’t
    move and those instances are “ignored”. This feature is a great advantage of SVM,
    as it is not required to memorize the entire training set.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，分隔线仅由支持向量定义。因此，添加更多“随机”训练实例对决策边界没有影响。当更多不是支持向量的训练实例被添加到训练集中时，决策边界不会移动，这些实例会被“忽略”。这一特性是
    SVM 的一个巨大优势，因为它不需要记住整个训练集。
- en: For an m-dimensional dataset, the separation line becomes a **separation hyperplane**,
    but the inspiring idea still holds.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个 m 维数据集，分隔线变成了**分隔超平面**，但激发的思想依然有效。
- en: Since the SVM model is based on distances, it is extremely **sensitive to feature
    scales**. For this reason it is always a good idea to normalize the features’
    values.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 SVM 模型基于距离，因此对**特征缩放极为敏感**。因此，通常建议对特征值进行标准化。
- en: 'In Scikit-Learn we can instantiate a `LinearSVC()` object from the `.svm` module:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scikit-Learn 中，我们可以从 `.svm` 模块中实例化一个 `LinearSVC()` 对象：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The resulting accuracy is a mere 66%: not a remarkable result. In the next
    section, we will understand why the Linear Support Vector Machine classifier didn’t
    perform better and what to do to cope with this.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的准确率仅为 66%：这是一个不令人满意的结果。在下一节中，我们将了解为什么线性支持向量机分类器表现不佳以及如何应对这种情况。
- en: Soft Margin Classification
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软边际分类
- en: As you can imagine, having a dataset whose classes can be perfercty separated
    by linear surfaces is a luxury. In a real-world scenario, it doesn’t happen and
    data only needs one outlier to **make it impossible** for a Linear SVM to find
    a viable decision boundary.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所想，拥有一个类别可以通过线性面完美分隔的数据集是一种奢侈。在现实世界中，这种情况并不会发生，数据只需要一个离群点就会**使线性 SVM 无法找到可行的决策边界**。
- en: '![](../Images/1430d7521724daaf61b6e79826ae6359.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1430d7521724daaf61b6e79826ae6359.png)'
- en: Image by the author.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: As shown in the image above, in this case, there doesn’t exist any line capable
    of separating the two classes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，在这种情况下，没有任何直线能够分开这两个类别。
- en: To cope with real-world scenarios we need to introduce the Soft Margin Support
    Vector Machine Classification.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对实际场景，我们需要引入软边际支持向量机分类。
- en: Unlike traditional Linear SVM (also called Hard Margin SVM), Soft Margin SVM
    doesn’t require a rigid separation between the classes, allowing some **elements
    of flexibility**. By using a Soft Margin SVM, we acknowledge the existence of
    noisy data points and outliers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的线性 SVM（也称为硬间隔 SVM）不同，软间隔 SVM 不要求类别之间有严格的分隔，允许一些**灵活性元素**。通过使用软间隔 SVM，我们承认存在噪声数据点和离群点。
- en: 'More practically, we can specify an hyperparameter `C` which serves as a regularization
    parameter:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 更实际地说，我们可以指定一个超参数 `C`，它作为正则化参数：
- en: setting `C` to a large value, we enforce a stricter margin, reducing misclassifications
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `C` 设置为较大值，我们强制执行更严格的间隔，从而减少误分类。
- en: setting `C` to a small value, we encourage a wider margin, allowing a greater
    number of misclassifications
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `C` 设置为较小的值，我们鼓励更宽的间隔，从而允许更多的误分类。
- en: '![](../Images/3ffe5e0ccf465f762fd7e6a0cbae1f06.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ffe5e0ccf465f762fd7e6a0cbae1f06.png)'
- en: Increasing C value reduces margin violations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 增加 C 值减少间隔违规。
- en: 'There are two ways to implement the Soft Margin Linear SVM classifier in Scikit-Learn:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以在 Scikit-Learn 中实现软间隔线性 SVM 分类器：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Running these lines of code for the same datasets, we obtain a slighter higher
    accuracy than the one we obtained for the Hard Margin LinearSVM. Although linear
    SVM classifiers are efficient and perform well in many scenarios, most datasets’
    classes are not linearly separable. In these cases, a linear decision boundary
    yields poor results.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于相同的数据集运行这些代码行，我们获得的准确度略高于我们为硬间隔线性 SVM 获得的准确度。尽管线性 SVM 分类器在许多场景中效率高且表现良好，但大多数数据集的类别并不是线性可分的。在这些情况下，线性决策边界会产生较差的结果。
- en: Fortunately for us, SVM classifiers are not limited to linear decision boundaries.
    Thanks to the [*kernel trick*](/the-kernel-trick-c98cdbcaeb3f), they can learn
    even the most complex separation shapes. The next section will focus on that.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，SVM 分类器不仅限于线性决策边界。多亏了 [*核技巧*](/the-kernel-trick-c98cdbcaeb3f)，它们可以学习甚至最复杂的分隔形状。下一节将集中于此。
- en: Non-Linear SVM Classification
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性 SVM 分类
- en: As I anticipated, many datasets are **not linearly separable**. Even if we account
    some flexibility, results obtained through a linear separation line are not optimal.
    To handle this issue, we can **add more features**, such as polynomial features.
    Adding new features will transform the original dataset into a higher-dimensional
    one, where it may be separated by a line or hyperplane.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如我所预见的，许多数据集**不是线性可分的**。即使我们考虑一些灵活性，通过线性分隔线获得的结果也不是最佳的。为了解决这个问题，我们可以**添加更多特征**，例如多项式特征。添加新特征会将原始数据集转换为更高维度的数据集，在那里它可能会被一条直线或超平面分开。
- en: 'Consider this simple example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个简单的例子：
- en: '![](../Images/24d97585963430cd93a00ccc2411bb64.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24d97585963430cd93a00ccc2411bb64.png)'
- en: Non-linearly separable data. Image by the author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性可分的数据。图像由作者提供。
- en: Data have only one feature and it is not separable by any straight line. If
    we add an artificial feature, computed as
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据只有一个特征，且无法通过任何直线分开。如果我们添加一个人工特征，计算方式为
- en: '![](../Images/59799aed154bf5939e96dea66eefd910.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59799aed154bf5939e96dea66eefd910.png)'
- en: 'we obtain the following dataset, which can be separated by a linear boundary:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下数据集，它可以被线性边界分开：
- en: '![](../Images/e62ed4bcb50b84bae69211782aedd6cd.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e62ed4bcb50b84bae69211782aedd6cd.png)'
- en: Linearly separable data. Image by the author.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 线性可分的数据。图像由作者提供。
- en: However, adding polynomial features, like we did above, is **not feasible for
    large and complex datasets**. The resulting number of features would be too high.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，添加多项式特征，如我们上面所做的，对**大型和复杂数据集**来说**不可行**。结果特征数量会太高。
- en: Fortunately, there exists a technique called **kernel trick** that makes it
    possible even with high-degree polynomials. The mathematics behind the kernel
    trick is not complex, but as I want to focus on the practical implementation,
    I will leave [this guide](https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/lectures/lec3.pdf)
    explaining it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，存在一种称为**核技巧**的技术，即使在高次多项式下也能实现。核技巧背后的数学并不复杂，但由于我想专注于实际实施，所以我将留给 [这份指南](https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/lectures/lec3.pdf)
    来解释。
- en: 'To implement a SVM classifier with a polynomial kernel in Python, we simply
    use the `SVC()` class and specify the type of kernel we intend to use and its
    degree:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Python 中实现带有多项式核的 SVM 分类器，我们只需使用 `SVC()` 类，并指定我们打算使用的核类型及其度数：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `coef0` hyperparameter acts as a regularization parameter, allowing to control
    how the model is influenced by high-degree polynomials.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`coef0` 超参数作为正则化参数，允许控制模型如何受到高次多项式的影响。'
- en: Finding the correct balance between the `degree`, `C` and`coef0` hyperparameters
    is not a straightforward task. It is typically recommended to use a Grid Search
    approach to find some viable values, and later perform a finer manual search to
    tune them precisely.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 找到 `degree`、`C` 和 `coef0` 超参数之间的正确平衡并不是一项简单的任务。通常建议使用网格搜索方法来找到一些可行的值，然后进行更精细的手动搜索以精确调整它们。
- en: 'Similarity Features: Gaussian Radial Basis Function'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相似性特征：高斯径向基函数
- en: 'Polynomial kernels often work for a variety of machine learning problems, however,
    there exists a notable technique that often works even better: Similarity Features.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式核函数通常适用于各种机器学习问题，然而，还有一种显著的技术常常效果更佳：相似性特征。
- en: Instead of adding artificial polynomial features based on the original features’
    values, we place several *landmarks* in our high-dimensional feature space and
    measure the distance from them to each data point. These distance measures become
    the **new model’s features**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与其基于原始特征值添加人工多项式特征，不如在我们高维特征空间中放置多个 *标志* 并测量它们到每个数据点的距离。这些距离度量成为 **新模型的特征**。
- en: 'Consider a training set with two independent features that look like this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有两个独立特征的训练集，如下所示：
- en: '![](../Images/396662f8494f869be53ae7034d77d92d.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/396662f8494f869be53ae7034d77d92d.png)'
- en: Image by the author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: By adding a certain number of landmarks in specific locations, we can create
    additional features computed as the distance from the data point to each landmark.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在特定位置添加一定数量的标志，我们可以创建额外的特征，计算为数据点到每个标志的距离。
- en: '![](../Images/2fa21b729b109a5ce564920018b6c59c.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fa21b729b109a5ce564920018b6c59c.png)'
- en: In this example, we can place two landmarks in those precise locations. The
    SVM classifier can then learn to predict as class A the instances close to the
    landmarks and as class B the ones far away from the landmarks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以在这些精确的位置放置两个标志。SVM 分类器可以学习将接近标志的实例预测为 A 类，将远离标志的实例预测为 B 类。
- en: 'Any distance measure can be used, however, it is proven to be convenient to
    implement the [**Gaussian Kernel function**](https://pages.stat.wisc.edu/~mchung/teaching/MIA/reading/diffusion.gaussian.kernel.pdf.pdf):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用任何距离度量，然而，已证明实现 [**高斯核函数**](https://pages.stat.wisc.edu/~mchung/teaching/MIA/reading/diffusion.gaussian.kernel.pdf.pdf)
    是很方便的：
- en: '![](../Images/6d5f0622a2fd929365149e4b74c87e1e.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d5f0622a2fd929365149e4b74c87e1e.png)'
- en: 'where:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*x* is the vector containing the data point coordinates'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x* 是包含数据点坐标的向量'
- en: '*l* is the vector containing the landmark coordinates'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*l* 是包含标志坐标的向量'
- en: '*gamma* acts as a regularization hyperparameter'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*gamma* 作为正则化超参数'
- en: 'The question now is: “*Where do I place the landmarks?*”. The most diffused
    approach is to place a landmark at each training example location. Having *m*
    training examples means creating *m* landmarks, and, as a consequence, it will
    result in *m* new features.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是：“*我应该将标志放在哪里？*”。最常见的方法是在每个训练样本的位置放置一个标志。拥有 *m* 个训练样本意味着创建 *m* 个标志，从而产生
    *m* 个新特征。
- en: The downside of this approach is that for large training sets, we will end up
    with an equally large number of distance features.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是对于大规模训练集，我们最终会得到同样大量的距离特征。
- en: 'In Scikit-Learn a SVM classifier with a Gaussian kernel is implemented as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scikit-Learn 中，带有高斯核的 SVM 分类器实现如下：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Conclusion
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, we detailed the theory of this versatile and powerful model, and
    we understood how easy it is to implement it in Python through the Scikit-Learn
    library.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们详细介绍了这一多功能且强大的模型的理论，并了解了通过 Scikit-Learn 库在 Python 中实现它是多么简单。
- en: I conclude this introductory guide by delineating the pros and cons of Support
    Vector Machine model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过阐述支持向量机模型的优缺点来总结这篇入门指南。
- en: 'Without any question, the strength of SVM coincide with its **accuracy in high-dimensional
    spaces**, making it ideal for data with numerous features, like images or genetic
    data. Additionally, I must remark SVM **versatility**: beyond classification,
    it seamlessly adapts to regression and anomaly detection tasks. Finally, on the
    pros side, I must point out its **flexibility**. SVM ability to handle non-linear
    relationships through different kernel functions, grants it a vast application
    spectrum.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，SVM的优势在于其**高维空间中的准确性**，使其非常适合特征众多的数据，如图像或基因数据。此外，我必须强调SVM的**多样性**：除了分类，它还可以无缝地适应回归和异常检测任务。最后，我必须指出其**灵活性**。SVM通过不同的核函数处理非线性关系的能力，赋予了它广泛的应用范围。
- en: 'As we all know the mantra “*there is no free meal in Machine Learning*”, we
    must acknowledge SVM cons. First of all, SVM can easily be **affected by noisy
    data**, as we saw in the **Linear SVM** section. **Scalability** is also an Achilles
    heel for SVM: training large datasets with SVM can be computationally intensive
    and demanding.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，“*机器学习没有免费的午餐*”这一箴言，我们必须承认SVM的缺点。首先，SVM容易受到**噪声数据的影响**，正如我们在**线性SVM**部分所见。**可扩展性**也是SVM的一个致命弱点：用SVM训练大规模数据集可能会计算密集且要求高。
- en: As I reach the end of this guide, I need to point out that, despite having touched
    all the most relevant aspects of SVM, its field extends far beyond the insight
    provided here. For this reason, I recommend digging into the resources and references
    attached to this article.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成本指南时，我需要指出，尽管已经涉及了SVM的所有最相关的方面，但其领域远超本指南所提供的见解。因此，我建议深入研究附在本文后的资源和参考文献。
- en: If you liked this story, consider following me to be notified of my upcoming
    projects and articles!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这个故事，考虑关注我，以便及时了解我即将发布的项目和文章！
- en: 'Here are some of my past projects:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我过去的一些项目：
- en: '[](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----a2969f2ff00d--------------------------------)
    [## Social Network Analysis with NetworkX: A Gentle Introduction'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用NetworkX进行社交网络分析：友好入门](https://towardsdatascience.com/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----a2969f2ff00d--------------------------------)'
- en: Learn how companies like Facebook and LinkedIn extract insights from networks
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解像Facebook和LinkedIn这样的公司如何从网络中提取洞察
- en: 'towardsdatascience.com](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----a2969f2ff00d--------------------------------)
    [](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----a2969f2ff00d--------------------------------)
    [## Ensemble Learning with Scikit-Learn: A Friendly Introduction'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[集成学习与Scikit-Learn：友好入门](https://towardsdatascience.com/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----a2969f2ff00d--------------------------------)'
- en: Ensemble learning algorithms like XGBoost or Random Forests are among the top-performing
    models in Kaggle competitions…
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 像XGBoost或随机森林这样的集成学习算法是Kaggle竞赛中的顶尖模型之一……
- en: 'towardsdatascience.com](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----a2969f2ff00d--------------------------------)
    [](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----a2969f2ff00d--------------------------------)
    [## Use Deep Learning to Generate Fantasy Names: Build a Language Model from Scratch'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[深度学习生成幻想名字：从零构建语言模型](https://towardsdatascience.com/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----a2969f2ff00d--------------------------------)'
- en: Can a language model invent unique fantasy character names? Let’s build it from
    scratch
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个语言模型能否创造独特的幻想角色名字？让我们从零开始构建它
- en: towardsdatascience.com](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----a2969f2ff00d--------------------------------)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[深度学习生成幻想名字：从零构建语言模型](https://towardsdatascience.com/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----a2969f2ff00d--------------------------------)'
- en: References
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Machine Learning with Python](https://www.coursera.org/learn/machine-learning-with-python)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Python进行机器学习](https://www.coursera.org/learn/machine-learning-with-python)'
- en: '[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
    — Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《动手学机器学习：基于 Scikit-Learn、Keras 和 TensorFlow 的实践（第2版）》 — 奥雷利安·热龙](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)'
- en: '[Scikit-Learn SVM documentation](https://scikit-learn.org/stable/modules/svm.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-Learn SVM 文档](https://scikit-learn.org/stable/modules/svm.html)'
- en: '[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习专项课程](https://www.coursera.org/specializations/machine-learning-introduction)'
