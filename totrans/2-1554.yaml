- en: My First Exploratory Data Analysis with ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/my-first-exploratory-data-analysis-with-chatgpt-7f100005efdc](https://towardsdatascience.com/my-first-exploratory-data-analysis-with-chatgpt-7f100005efdc)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Unleashing the power of ChatGPT: A deep dive into an exploratory data analysis
    and future opportunities'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jyesr.medium.com/?source=post_page-----7f100005efdc--------------------------------)[![Jye
    Sawtell-Rickson](../Images/e16c2b5020d24331812a4f35e9bd7890.png)](https://jyesr.medium.com/?source=post_page-----7f100005efdc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7f100005efdc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7f100005efdc--------------------------------)
    [Jye Sawtell-Rickson](https://jyesr.medium.com/?source=post_page-----7f100005efdc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7f100005efdc--------------------------------)
    ¬∑15 min read¬∑May 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d8f14196cf7b2834ea040accdadbbe9.png)'
  prefs: []
  type: TYPE_IMG
- en: ‚ÄúAn AI exploring a vast world of data. Digital art. Vivid colours.‚Äù (Author
    generated via DALL-E 2)
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT is an extraordinary tool for working more efficiently, and that doesn‚Äôt
    stop with data analytics. In this article we‚Äôll run through an example of exploratory
    data analysis (EDA) run by ChatGPT. We‚Äôll cover the various stages of an EDA,
    see some impressive outputs (Wordclouds!) and note where ChatGPT does well (and
    not so well). Finally, we‚Äôll touch on the future of LLMs in analytics and how
    excited we are for it.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used for the analysis is a sample from [Common Crawl](https://commoncrawl.org/),
    which is [free to be accessed and analysed by anyone](https://commoncrawl.org/terms-of-use/).
    The Common Crawl dataset is a vast collection of web crawl data, comprising billions
    of web pages from the internet. The dataset includes various web content types
    and is regularly updated. It serves as a significant resource for training language
    models like LLMs and made up [60% of the training data for ChatGPT](https://medium.com/@dlaytonj2/chatgpt-show-me-the-data-sources-11e9433d57e8).
    You can find the dataset sample curated by the author hosted on Kaggle [here](https://www.kaggle.com/datasets/jyesawtellrickson/commoncrawl).
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the post, content will be truncated, so feel free to follow along
    directly on [the Google Colab used to run this analysis](https://colab.research.google.com/drive/1REJ3xa37Z3milf8-4-1-2fB69AONI0Zz).
  prefs: []
  type: TYPE_NORMAL
- en: 'We‚Äôve broken down the analysis into five sections:'
  prefs: []
  type: TYPE_NORMAL
- en: ü§î Overall Structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üßπ Data Cleaning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üíØ Basic Analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üá¨üáß NLP Approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üîë Key Findings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With all that out of the way, let‚Äôs jump into the analysis!
  prefs: []
  type: TYPE_NORMAL
- en: Structuring Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before starting analytics work, it‚Äôs good to formulate a plan. EDA‚Äôs typically
    follow a standard structure to start with, before going off on tangents as interesting
    insights are discovered in the analysis. One such template is [this](/exploratory-data-analysis-eda-a-practical-guide-and-template-for-structured-data-abfbf3ee3bd9),
    which details the core components: preview data, clean data, plot data. We could
    use this as a starting point, but let‚Äôs assume we know nothing and see what ChatGPT
    comes up with.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Let''s run an EDA on the Common Crawl text dataset together. The dataset is
    a scrape of many web pages on the internet. The columns are ''text'', ''WARC-Type'',
    ''WARC-Target-URI'', ''WARC-Date'', ''WARC-Record-ID'', ''WARC-Refers-To'', ''WARC-Block-Digest'',
    ''WARC-Identified-Content-Language'', ''Content-Type'', and ''Content-Length''.
    The dataset is saved in a csv file called common_crawl_text.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`You should give me an overall plan, then we can go through step-by-step. In
    each step you should write some code and I will run it in the terminal then report
    back to you the results.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Sure, here''s a high-level plan for the EDA on the Common Crawl text dataset:`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Load the dataset into a pandas DataFrame.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Check for missing values in each column and handle them appropriately.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Explore the distribution of the target variable ''WARC-Type''.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Check the distribution of ''WARC-Identified-Content-Language''.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Check the distribution of ''Content-Type''.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Explore the ''text'' column, including the distribution of text lengths and
    the most common words.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Check for any correlations between the different columns.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Summarize the findings and identify any interesting patterns or insights.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Let''s start with step 1 and load the dataset into a pandas DataFrame.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ChatGPT has done a pretty good job here covering the standard steps. Notice
    that it doesn‚Äôt want us to deeply analyse all columns, but has a subset that it‚Äôs
    specifically interested in. This is sensible, but I think it‚Äôs missing some valuable
    data points. For example, ‚ÄúWARC-Target-URI‚Äù is worth exploring, and the ‚ÄúWARC-Date‚Äù
    column will give insights to when the data was collected.
  prefs: []
  type: TYPE_NORMAL
- en: I prompted ChatGPT with this, and it happily included these into the plan. At
    the same time, it added in the creation of a Wordcloud, something I had in the
    back of my mind.
  prefs: []
  type: TYPE_NORMAL
- en: I love this sort of interaction, where it feels like a partnership, rather than
    just reading a book.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data Cleaning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With each of the steps outlined in the structure, we can ask ChatGPT to expand
    on that step and give us the exact code to run. To start off, it suggests:'
  prefs: []
  type: TYPE_NORMAL
- en: '`We can start by loading the Common Crawl text dataset and taking a look at
    its structure. Here''s the code to load the dataset into a Pandas DataFrame:`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`import pandas as pd`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`df = pd.read_csv("common_crawl_text.csv")`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Once you''ve loaded the dataset, you can check the structure of the DataFrame
    using the following code:`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`df.head()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`This will show you the first five rows of the DataFrame. You can also use
    the [df.info](<http://df.info/>)() method to get an overview of the columns and
    their data types, as well as the number of non-null values in each column.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'After giving it the results, it‚Äôs able to share some basic information back:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Based on the provided data, it seems like we have a dataset of web archive
    records with different attributes such as the type of the record, the URI (Uniform
    Resource Identifier) of the record, the date it was recorded, the length of the
    content, and the content itself. The content is in different languages and formats,
    such as plain text, and the length of the content varies. The dataset contains
    37832 records, and some of them have null values in the content length column.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We can go a little further and ask it to give a detailed description of each
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Can you give a detailed list of the columns and the expected meaning?`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Sure! Here''s a detailed list of the columns in the DataFrame and their expected
    meaning:`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`WARC-Type: The type of WARC record. A WARC (Web ARChive) record is a format
    for storing internet resources such as web pages and their associated metadata.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`WARC-Target-URI: The target URI (Uniform Resource Identifier) of the archived
    web page.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`WARC-Date: The date and time when the web page was archived.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`WARC-Record-ID: The unique identifier of the WARC record.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`WARC-Refers-To: The identifier of a related WARC record that this record refers
    to.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`WARC-Block-Digest: A cryptographic hash value computed over the contents of
    the WARC record.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`WARC-Identified-Content-Language: The language(s) identified in the content
    of the web page.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Content-Type: The media type of the content, such as text/plain or text/html.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Content-Length: The length of the content in bytes.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`text: The extracted text content of the web page.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Note that some of these columns are specific to the WARC format and may not
    be present in other types of data.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It‚Äôs done a fairly good job at guessing what these columns mean. Some of these
    may not be correct (e.g. Content-Length), but it‚Äôs a helpful starting point. It
    may actually know these specific names thanks to its training data, but I‚Äôve experimented
    around with arbitrary column names and it can usually guess what the column will
    contain from a combination of the name and data returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it noticed there were missing values, so let‚Äôs see how to deal with that:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Should we do something about the missing values?`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Yes, it would be a good idea to handle the missing values in the Content-Length
    column. One option is to remove the rows that have missing values if they are
    not a significant portion of the data. Another option is to impute the missing
    values using some statistical method such as mean, median, or mode. However, the
    best approach would depend on the nature of the data and the analysis being performed.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These are the ways I would probably deal with it. In this case, I‚Äôm happy to
    just delete the entries since there aren‚Äôt¬†too¬†many.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Let''s just remove them as there are very few. How can I do that?`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '`You can drop the rows containing missing values using the dropna() function
    in pandas. Here''s an example:`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`import pandas as pd`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`# create a dataframe`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`df = pd.DataFrame({''A'': [1, 2, None, 4], ''B'': [5, None, 7, 8]})`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`# drop rows with missing values`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`df.dropna(inplace=True) print(df)`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Output:`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`A B 0 1.0 5.0 3 4.0 8.0`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In this example, the dropna() function removes the second row which contains
    missing values. The inplace=True argument modifies the original DataFrame, so
    the resulting DataFrame only contains rows without any missing values.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Notice how it gives working code, and comments each line of the code. It also
    gives an explanation of what is happening at the end. This is great because we
    can learn what‚Äôs happening in the code, and if there‚Äôs anything we want to modify,
    it‚Äôs a lot easier.
  prefs: []
  type: TYPE_NORMAL
- en: We can now get started with the first parts of the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Steps 3‚Äì5 are roughly the same, plot some distributions of data and see if there‚Äôs
    anything interesting there.
  prefs: []
  type: TYPE_NORMAL
- en: 'When prompted to start step 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: And that outputs a graph for us. In this case, it‚Äôs not very interesting as
    there‚Äôs actually only one ‚ÄúWARC-Type‚Äù, but it was a nice idea. The issue with
    images is that it‚Äôs hard to feed the results back to ChatGPT. I found that it‚Äôs
    better to suggest ChatGPT to return tables of data to solve this, as well as additional
    plots where it makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'After learning the column only has one value, ChatGPT is happy to move on to
    the next stage. When exploring the ‚ÄúWARC-Content-Language‚Äù we start to see something
    a little more complex. The results returned are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`eng 15903`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`rus 1551`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`...`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`hrv,nno,bel 1`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`zul,eng 1`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Of which it learns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`It seems like there are 1187 unique values for the ''WARC-Identified-Content-Language''
    column, and the most common language is English (''eng'') followed by Russian
    (''rus''), German (''deu''), and Chinese (''zho''). Since there are so many unique
    values, it might be difficult to analyze the distribution of the variable. One
    approach to simplifying the analysis could be to group languages into broader
    categories, such as "English", "Romance languages", "Slavic languages", etc. depending
    on the research question.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It‚Äôs a great point about grouping, but it‚Äôs missed something here ‚Äî each row
    can have multiple languages. When prompted about this (a few times), it comes
    up with a good approach.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After combining this with some prompts about the language grouping, we‚Äôre able
    to get out a nice plot:'
  prefs: []
  type: TYPE_NORMAL
- en: One thing I particularly like about this is that it is giving labels to the
    chart. It can be a little annoying to write out labels every time, but it does
    make a difference, so it‚Äôs nice to see that it‚Äôs smart enough to give sensible
    ones here.
  prefs: []
  type: TYPE_NORMAL
- en: Seems English is still the dominant language ‚Äî by how much?
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '`It looks like it''s mostly English language. Can you calculate what percentage
    of the agg data is english?`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Sure, here''s some code to calculate the percentage of the aggregated data
    that is English: english_percentage = (agg_data.loc[''eng'', ''weight''] / agg_data[''weight''].sum())
    * 100`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`print(f"The percentage of the aggregated data that is English is: {english_percentage:.2f}%")`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`This should output the percentage of the aggregated data that is English,
    rounded to 2 decimal places.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/97e45cc4ac8d41f1e2bc71e205b86932.png)'
  prefs: []
  type: TYPE_IMG
- en: (Author created image)
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we can move on to step 6 which involves doing some exploration of the text
    column. This is going to be more analytics heavy and will require Natural Language
    Processing (NLP) techniques. Let‚Äôs see how ChatGPT does.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Great. Let''s move on to step 6:`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`"Explore the ''text'' column, including the distribution of text lengths and
    the most common words."`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[(''nan'', 36847), (''0'', 3467), (''1'', 2809), (''keres≈ëoptimaliz√°l√°s'',
    2498), (''id≈ësgondoz√°s'', 1852), (''2'', 1740), (''3'', 1559), (''szerz≈ëd√©s'',
    1529), (''eltart√°si'', 1527), (''brands'', 1442), (''5'', 1362), (''4'', 1350),
    (''usd'', 1245), (''us'', 1205), (''2023'', 1204), (''new'', 1178), (''sex'',
    1165), (''2022'', 1031), (''home'', 919), (''cookies'', 904)]`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I wasn‚Äôt very happy with this, as we can see that a lot of the ‚Äòtop words‚Äô are
    just numbers, or stopwords from another language (it only used English ones).
    I was able to prod it on this, and it helped clean it up by filtering out words
    which were non-alphabetic tokens and including stopword lists for other languages.
    Now let‚Äôs visualise these in a Wordcloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Can you make it a wordcloud?`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f2578e3bf779c3c2340e134247601bd0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'WARNING: contains explicit language. For those who are wondering, ‚Äúkeres≈ëoptimaliz√°l√°s‚Äù
    is Hungarian for ‚ÄúSearch engine optimization‚Äù (author created image).'
  prefs: []
  type: TYPE_NORMAL
- en: 'I‚Äôm not sure what to take away from this plot. It seems there are some words
    which could have useful content associated with them (e.g ‚Äúinformation‚Äù, ‚Äúblog‚Äù,
    ‚Äúbrands‚Äù), but there is also a lot of undesirable content. I shared the top words
    to ChatGPT and asked what it thought:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '`What can we say about this?`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Based on the top words found in the ''text'' column, it appears that the data
    may contain a lot of spam or irrelevant information, as many of the top words
    are related to online dating, reviews, loans, and escorts. Additionally, the presence
    of words like ''nan'' suggests that there may be missing or incomplete data in
    the ''text'' column. It would be important to further investigate the nature and
    quality of the data in the ''text'' column to determine how useful it is for any
    downstream analysis.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: That‚Äôs a fair conclusion to make, and indeed, companies that work with the Common
    Crawl dataset have stated that they invest considerable resources cleaning up
    the dataset to make it usable for training AI models.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This concludes the plan outlined by ChatGPT, so we‚Äôll wrap up the analysis here
    and see what findings it has.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Great. Let''s move on to step 8\. Can you summarise the findings from all
    our analysis so far?`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Sure, based on the analysis we have conducted so far, here are some key findings:
    Language distribution: The most common language in the dataset is English, followed
    by Russian, German, Chinese, and Spanish. Content-Type distribution: The majority
    of the content in the dataset is of the type "text/plain". Text length: The text
    length in the dataset ranges from 0 to 8,542,524, with an average length of 20,566\.
    Common words: The most frequent words in the dataset are not particularly informative,
    as they include many stop words and other non-informative terms. Correlations:
    There is a strong positive correlation between the length of the text and the
    content length. There is also a weak positive correlation between the number of
    languages detected in a text and the content length.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If a data analyst came to me with this, I‚Äôd be pretty disappointed. As mentioned
    at the start, there are some columns it could have analysed in more detailed,
    and it‚Äôs generally better to make some suggestions for next steps. However, we
    never gave it a goal for the analysis, nor told it to give us suggestions so I‚Äôm
    not going to penalise it too much.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you can see, the prompting used throughout was relatively simple. Given
    that, it‚Äôs impressive at how well ChatGPT was able to understand what I wanted
    done and give useful answers. These prompts could definitely be improved by providing
    more context in each prompt and being stricter on what we want back. For example,
    each prompt could contain references to the specific task it is focusing on, as
    well as additional text to have it do exactly what we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Don‚Äôt respond with superfluous text. Assume pandas, numpy and matplotlib have
    been imported in the standard way.`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These could be maintained in your own set of prompt templates to speed up this
    sort of work, or done with a tool such as [LangChain](https://python.langchain.com/en/latest/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: We could also define our own overall template. I let ChatGPT come up with a
    plan, but it wasn‚Äôt perfect. We could define an overall structure for it to follow,
    and a standard way to e.g. analyse each variable. With templates, ChatGPT is less
    likely to miss insights in such an analysis.
  prefs: []
  type: TYPE_NORMAL
- en: While it was fun going back and forth with ChatGPT to get data outputs to it,
    it quickly became tiring. ChatGPT is much more powerful when it can run the code
    directly itself. ChatGPT can connected to a Python runtime by instead working
    with the [Python API](https://medium.com/geekculture/a-simple-guide-to-chatgpt-api-with-python-c147985ae28).
    In this case, the code could be run automatically, but to cut the human out of
    the loop we‚Äôll need one more tool.
  prefs: []
  type: TYPE_NORMAL
- en: AutoGPT has been very popular in the last month as a power-up to ChatGPT which
    effectively provides a guide to ChatGPT agents which allows them to keep executing
    towards some goal. AutoGPT could replace me in this situation, asking ChatGPT
    agents to design code, then executing it, feeding the results back to ChatGPT,
    proceeding until it has a detailed analysis. It would also interface with a memory
    database which would allow it to execute much larger analyses.
  prefs: []
  type: TYPE_NORMAL
- en: With a tool like AutoGPT we can set a clear goal with requirements such as detail
    of analysis and expected conclusion style. In this case, we can check in less
    regularly with the outcomes and eventually have to do little work to get a decent
    analysis out.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we should call out that ChatGPT is far from ‚Äòperfect‚Äô and even in this
    mock analysis, I had to massage the prompts to get an answer that was close to
    what I wanted. It was a lot less difficult than I expected, but still worth noting.
    It created some code that had errors, though it managed to fix the errors every
    time it was told. At times it created code that I wouldn‚Äôt have wanted to run,
    and I needed to suggest it follow a different path, but again, upon prompting
    it could come up with a decent solution.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we‚Äôve seen how ChatGPT can be used to support the running of
    an Exploratory Data Analysis (EDA). We‚Äôve seen that we‚Äôre able to get surprisingly
    good results working with the system, with little exterior help. We also noted
    that there are already tools which allow us to extend this idea such as AutoGPT
    which could make an even more powerful assistant.
  prefs: []
  type: TYPE_NORMAL
- en: As a data analyst, I‚Äôm already using ChatGPT to help with my analytics in some
    of the ways described above, though I rarely use it for an end-to-end analysis
    as detailed in this article. As more integrations are built out with tools like
    AutoGPT, and the friction to use is reduced, I expect to be using it more and
    more and am very much excited for it (while I‚Äôm not made obsolete ;) ).
  prefs: []
  type: TYPE_NORMAL
