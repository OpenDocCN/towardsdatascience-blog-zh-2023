- en: How to Store Historical Data Much More Efficiently
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-store-historical-data-much-more-efficiently-78b0f2c8c811](https://towardsdatascience.com/how-to-store-historical-data-much-more-efficiently-78b0f2c8c811)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A hands-on tutorial using PySpark to store up to only 0.01% of a DataFrame’s
    rows without losing any information.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@tomergabay?source=post_page-----78b0f2c8c811--------------------------------)[![Tomer
    Gabay](../Images/1fb1d408bc89415918c1aa6733df44e1.png)](https://medium.com/@tomergabay?source=post_page-----78b0f2c8c811--------------------------------)[](https://towardsdatascience.com/?source=post_page-----78b0f2c8c811--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----78b0f2c8c811--------------------------------)
    [Tomer Gabay](https://medium.com/@tomergabay?source=post_page-----78b0f2c8c811--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----78b0f2c8c811--------------------------------)
    ·10 min read·Sep 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e6a3ccf160e7ca3d2ab0eba9ba6199b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Supratik Deshmukh](https://unsplash.com/@supratikdeshmukh?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'In an era where companies and organizations are collecting more data than ever
    before, datasets tend to accumulate millions of unnecessary rows that don’t contain
    any new or valuable information. In this article, we’ll focus on a critical aspect
    of data management: deleting rows in a dataset if they provide no added value,
    using PySpark*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*[PySpark](https://spark.apache.org/docs/latest/api/python/index.html#:~:text=PySpark%20is%20the%20Python%20API,for%20interactively%20analyzing%20your%20data.)
    is used over pandas when dealing with very large datasets because it can process
    data across multiple computers, making it faster and more scalable. Pandas works
    well for smaller datasets that can fit in memory on a single machine but may become
    slow or even impractical for big data.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s imagine the following situation: you work as a data engineer/scientist
    in the maintenance department of a real estate company. For the past ten years,
    your company has done a full load of all maintenance data from an external database
    containing the conditions of your buildings and stored it in the company’s cloud
    storage. The data could e.g. look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de618a13bba4b78438dde61c188f70b8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[image by Author]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three columns are present in this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`id` -> for the ID of the building.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`condition` -> an integer between 1 (terrible) and 10 (excellent) that represents
    the condition of the building.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import_date` -> a *datetime* column representing the day this row was imported
    from the external software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To create this dataset yourself please run the snippet below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at Building2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b17e0710fd198f1566e579996ba467ad.png)'
  prefs: []
  type: TYPE_IMG
- en: '[image by Author]'
  prefs: []
  type: TYPE_NORMAL
- en: On the first and second of January, the condition of the building is 4\. On
    the third of January, the condition changed to 5\. We don’t actually need the
    middle row about the second of January. If we just look at which date a condition
    changes, we could potentially omit many, many rows. The first occurrence, in this
    dataset, of Building2 is on the first of January, and the condition changes on
    the third of January. We can simply omit the second row about the second of January
    by adding a `from` and `until` column which provides information on when the row’s
    values were encountered first and when one of the row’s values changed.
  prefs: []
  type: TYPE_NORMAL
- en: This artificial dataset is very small and contains only three different import
    dates. But let’s consider a more realistic situation. Your real-estate company
    owns thousands of buildings, and the imports span over 10 years. This would result
    in many million rows without actual information. In my own real-life situation
    working for a housing corporation, using this technique, we only had to save about
    ~0.2% of all rows to store all information.
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s take a look at the code to add the** `**from**` **and** `**until**`
    **columns.**'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we have to specify the partition columns and the datetime column
    for the table. The partition column is similar to the column you’d like to group
    by when you’re using pandas.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then we need to specify the columns on which we want to track any possible changes,
    in our case, that’s `condition`. However, to make the code more general and reusable
    we can specify that the columns we want to track the changes of are every column
    except the partition columns and the datetime column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to create a PySpark Window, which acts similarly to pandas’ `.groupby()`.
    In that Window, we need to specify that we want to sort the rows on the`datetime_column`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To check whether the value of one of the `track_columns` has changed, we can
    add the value of the previous row to the current row, and check whether they’re
    identical.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code leads to the following DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f3493f143637650a42cf27c0259d14c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[image by Author]'
  prefs: []
  type: TYPE_NORMAL
- en: Here we can see, that based on whether `previous_condition` is equal to `condition`
    we can determine whether a value has changed, and we store this in `changed_condition`.
    Because of the specified `Window`, the values of `previous_condition` are only
    determined within the reach of each unique `id`.
  prefs: []
  type: TYPE_NORMAL
- en: However, we cannot only keep the rows when there’s a change in values because
    that would result in deleting all the first occurrences of a row for a given `id`.
    We also have to save all the first rows explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we can use `row_number()*`.
  prefs: []
  type: TYPE_NORMAL
- en: '* In this situation, you could also determine the first row by looking at the
    *null* values in `previous_condition` and/or `changed_condition`, however, for
    illustration purposes, we’ll use `row_number()` here.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1afbc5c914fe7f8a4c1a9a0d1053170f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[image by Author]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now there are two conditions for when we would like to keep a row:'
  prefs: []
  type: TYPE_NORMAL
- en: When `changed_condition is True` in any of the `f"changed_{column}"` columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When `row_num == 1`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, what if a row gets deleted? Because there is no next row, `changed_condition
    is False`, and `row_num != 1`.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need to make sure to check whether an `id` is still present in
    the last import.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first determine what the last import date is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Unfamiliar with the syntax `print(f"{variable = }")`? Read the article below
    for this and other useful Python tricks!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/5-python-tricks-that-distinguish-senior-developers-from-juniors-826d57ab3940?source=post_page-----78b0f2c8c811--------------------------------)
    [## 5 Python Tricks That Distinguish Senior Developers From Juniors'
  prefs: []
  type: TYPE_NORMAL
- en: Illustrated through differences in approaches to Advent of Code puzzles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/5-python-tricks-that-distinguish-senior-developers-from-juniors-826d57ab3940?source=post_page-----78b0f2c8c811--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s determine the latest import date of an `id`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3845d7a52a0bd838198708cd16edbc12.png)'
  prefs: []
  type: TYPE_IMG
- en: '[image by Author]'
  prefs: []
  type: TYPE_NORMAL
- en: In the image above you can see by looking at the `last_partition_import_date`
    that Building3 has disappeared from the data set on the third of January. This
    could e.g. mean that the building has been demolished. This would be very important
    information, which is why it is important to keep track of whether an `id` has
    been deleted or not.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now filter out all the rows that are irrelevant to keep. As we mentioned
    before, the rows we want to keep should have a changed `condition` value, or it
    should be the first occurrence of a building in the data set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1f575a8cb929186560fb6e6c584523c4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image by Author]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have a DataFrame in which each row adds information! Let’s remove some
    columns that we don’t need anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, let’s add an `until` column which indicates until when the row’s values
    are valid. We can do this by shifting, the next row’s `import_date` one up using
    `f.lag`, as long as the next row’s `id` is still the same and as long as the rows
    are ordered by `import_date`. For that, we’ll reuse the earlier defined Window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f5a465c83734fc9bde99f6c57f2d8ba3.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image by Author]'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see e.g. for Building1 the `until` value of the first row is the `import_date`
    of the second row. The `until` value of the second row is the `import_date` of
    the third row. The `until` value of the third row is `null` because there is no
    fourth row.
  prefs: []
  type: TYPE_NORMAL
- en: We’re almost done! As mentioned above, we still see some `null` values in the
    `until` column. This is not problematic for Building1, but it is problematic for
    Building3 because we can’t use the `until` column to detect that Building3 is
    no longer present on the third of January. This is where we’ll make use of the
    `last_partition_import_date` and the `latest_import` value.
  prefs: []
  type: TYPE_NORMAL
- en: If `until` is `null` and the `last_partition_import_date < latest_import`, then
    we add the `last_partition_import_date` plus one day to the `until` column of
    that row.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ee77e422fb1ad2ab3c86fabcb9c1a920.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image by Author]'
  prefs: []
  type: TYPE_NORMAL
- en: Using this technique, we can see that Building3’s last seen day was on the second
    of January (we use a non-inclusive until here). If a row still has a `null` value
    in the `until` column, it means that that row is still valid.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last thing we need to do is to drop the `last_partition_import_date` column
    and rename the `import_date` column to `from`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/451c01e6eddc714ad4a2e5872a9cadfb.png)'
  prefs: []
  type: TYPE_IMG
- en: '[image by Author]'
  prefs: []
  type: TYPE_NORMAL
- en: Here above you can see our final DataFrame! In real life, this technique can
    save you millions of rows that don’t carry any information, which saves money
    on storage and makes further processing a lot faster!
  prefs: []
  type: TYPE_NORMAL
- en: To conclude
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we’ve learned how you can save millions of rows by only retaining
    the rows that actually carry information, by looking at changed values within
    each subset. To summarize what we did:'
  prefs: []
  type: TYPE_NORMAL
- en: Grouped the DataFrame into partitions based on their IDs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each partition, we determined the consecutive identical combination of values.
    Consecutive identical combinations of values are dropped.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We added `from` and `until` as new columns to the rows to determine the lifespan
    of a combination of consecutive values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We checked whether the last `until` value of a partition is before the last
    `until` value of the entire DataFrame to add an `until` value for deleted partitions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To make the code above reusable I’ve created a function for it which you can
    copy-paste into your own project and use there!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: With this function, you can easily remove all uninformative rows!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If you found this article informative, please take a look at my account to see
    more articles on Python and data science!
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-extract-more-information-from-categorical-plots-8ffac1133eb0?source=post_page-----78b0f2c8c811--------------------------------)
    [## How to extract more information from categorical plots'
  prefs: []
  type: TYPE_NORMAL
- en: Easily get deeper insights into your categorical data by using these two methods.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-extract-more-information-from-categorical-plots-8ffac1133eb0?source=post_page-----78b0f2c8c811--------------------------------)
  prefs: []
  type: TYPE_NORMAL
