- en: 'A Birds-Eye View of Linear Algebra: Why Is Matrix Multiplication Like That?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性代数的全景视角：为什么矩阵乘法是这样的？
- en: 原文：[https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e](https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e](https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)
- en: Why should the columns of the first matrix match the rows of the second? Why
    not have the rows of both match?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么第一个矩阵的列数必须与第二个矩阵的行数匹配？为什么不让两个矩阵的行数匹配呢？
- en: '[](https://medium.com/@rohitpandey576?source=post_page-----a4d94067651e--------------------------------)[![Rohit
    Pandey](../Images/af817d8f68f2984058f0afb8fd7ecbe9.png)](https://medium.com/@rohitpandey576?source=post_page-----a4d94067651e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a4d94067651e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a4d94067651e--------------------------------)
    [Rohit Pandey](https://medium.com/@rohitpandey576?source=post_page-----a4d94067651e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rohitpandey576?source=post_page-----a4d94067651e--------------------------------)[![Rohit
    Pandey](../Images/af817d8f68f2984058f0afb8fd7ecbe9.png)](https://medium.com/@rohitpandey576?source=post_page-----a4d94067651e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a4d94067651e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a4d94067651e--------------------------------)
    [Rohit Pandey](https://medium.com/@rohitpandey576?source=post_page-----a4d94067651e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a4d94067651e--------------------------------)
    ·18 min read·Nov 23, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a4d94067651e--------------------------------)
    ·18 分钟阅读·2023年11月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2512b5389933ce8260d05fe8308ab411.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2512b5389933ce8260d05fe8308ab411.png)'
- en: Image created with midjourney
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由 midjourney 创建
- en: 'This is the third chapter of the in-progress book on linear algebra, “A birds
    eye view of linear algebra”. The table of contents so far:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是正在进行的线性代数书籍《线性代数的全景视角》的第三章。到目前为止的目录：
- en: '[Chapter-1: The basics](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-the-basics-29ad2122d98f)'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[第1章：基础知识](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-the-basics-29ad2122d98f)'
- en: 'Chapter-2: [The measure of a map — determinants](https://medium.com/p/1e5fd752a3be)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第2章：[映射的度量 — 行列式](https://medium.com/p/1e5fd752a3be)
- en: '**Chapter-3:** (Current) Why is matrix multiplication the way it is?'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**第3章：**（当前）为什么矩阵乘法是这样的？'
- en: 'Chapter-4: [Systems of equations, linear regression and neural networks](https://medium.com/p/fe5b88a57f66)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第4章：[方程组、线性回归和神经网络](https://medium.com/p/fe5b88a57f66)
- en: 'Chapter-5: [Rank nullity and why row rank == col rank](/a-birds-eye-view-of-linear-algebra-rank-nullity-and-why-row-rank-equals-column-rank-bc084e0e1075)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第5章：[秩、零空间及为什么行秩 == 列秩](/a-birds-eye-view-of-linear-algebra-rank-nullity-and-why-row-rank-equals-column-rank-bc084e0e1075)
- en: Here, we will describe operations we can do with two matrices, but keeping in
    mind they are just representations of linear maps.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将描述我们可以用两个矩阵进行的操作，但要记住它们只是线性映射的表示。
- en: I) Why care about matrix multiplication?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: I) 为什么关心矩阵乘法？
- en: Almost any information can be embedded in a vector space. Images, video, language,
    speech, biometric information and whatever else you can imagine. And all the applications
    of machine learning and artificial intelligence (like the recent chat-bots, text
    to image, etc.) work on top of these vector embeddings. Since linear algebra is
    the science of dealing with high dimensional vector spaces, it is an indispensable
    building block.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎任何信息都可以嵌入到向量空间中。图像、视频、语言、语音、生物识别信息以及你能想象的任何其他东西。所有的机器学习和人工智能应用（如最近的聊天机器人、文本转图像等）都基于这些向量嵌入。由于线性代数是处理高维向量空间的科学，它是一个不可或缺的基础。
- en: '![](../Images/359f7de8bc4e3ed8bbd719f387d18415.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/359f7de8bc4e3ed8bbd719f387d18415.png)'
- en: Complex concepts from our real world like images, text, speech, etc. can be
    embedded in high dimensional vector spaces. The higher the dimensionality of the
    vector space, the more complex information it can encode. Image created with midjourney.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中的复杂概念，如图像、文本、语音等，可以嵌入到高维向量空间中。向量空间的维度越高，它能编码的信息就越复杂。图像由 midjourney 创建。
- en: A lot of the techniques involve taking some input vectors from one space and
    mapping them to other vectors from some other space.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 许多技术涉及将某些输入向量从一个空间映射到其他空间的其他向量。
- en: But why the focus on “linear” when most interesting functions are non-linear?
    It’s because the problem of making our models high dimensional and that of making
    them non-linear (general enough to capture all kinds of complex relationships)
    turn out to be orthogonal to each other. Many neural network architectures work
    by using linear layers with simple one dimensional non-linearities in between
    them. And there is a theorem that says this kind of architecture can model any
    function.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么要关注“线性”，而大多数有趣的函数是非线性的？这是因为使模型高维和使其非线性（足够通用以捕捉各种复杂关系）的问题被发现是正交的。许多神经网络架构通过在它们之间使用简单的一维非线性线性层来工作。而且有一个定理表明，这种架构可以建模任何函数。
- en: Since the way we manipulate high dimensional vectors is primarily matrix multiplication,
    it isn’t a stretch to say it is the bedrock of the modern AI revolution.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们操作高维向量的方式主要是矩阵乘法，所以说它是现代人工智能革命的基石并不为过。
- en: '![](../Images/1eb2d34de16b7a676591669ffcf655b0.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1eb2d34de16b7a676591669ffcf655b0.png)'
- en: Deep neural networks have layers with vectors at each layer and connections
    between successive layers encoded as matrices. Translation between layers happens
    with linear algebra and matrix multiplication. Image created with midjourney.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络每一层都有向量，并且相邻层之间的连接以矩阵的形式编码。层间的转换通过线性代数和矩阵乘法完成。使用 midjourney 创建的图像。
- en: II) Algebra on maps
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: II) 地图上的代数
- en: '![](../Images/57446621a614cbd30acff5168fb9b656.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57446621a614cbd30acff5168fb9b656.png)'
- en: Image created with midjourney
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 midjourney 创建的图像
- en: In [chapter 2](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-the-measure-of-a-map-determinant-1e5fd752a3be),
    we learnt how to quantify linear maps with determinants. Now, let’s do some algebra
    with them. We’ll need two linear maps and a basis.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第2章](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-the-measure-of-a-map-determinant-1e5fd752a3be)中，我们学习了如何通过行列式量化线性映射。现在，让我们对它们进行一些代数运算。我们需要两个线性映射和一个基。
- en: II-A) Addition
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II-A) 加法
- en: If we can add matrices, we can add linear maps since matrices are the representations
    of linear maps. And matrix addition is not very interesting if you know scalar
    addition. Just as with vectors, it’s only defined if the two matrices are the
    same size (same rows and columns) and involves lining them up and adding element
    by element.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以加矩阵，我们可以加线性映射，因为矩阵是线性映射的表示。如果你了解标量加法，那么矩阵加法并不很有趣。就像向量一样，它仅在两个矩阵大小相同（行列相同）时定义，并涉及对齐它们并逐元素相加。
- en: '![](../Images/e9e135280d6c0614afaa336d2d0858de.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9e135280d6c0614afaa336d2d0858de.png)'
- en: 'Animation-0: Addition of two matrices. Image by author.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 动画-0：两个矩阵的加法。图像由作者提供。
- en: So, we’re just doing a bunch of scalar additions. Which means that the properties
    of scalar addition logically extend.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们只是在做一堆标量加法。这意味着标量加法的属性在逻辑上扩展。
- en: '**Commutative: if you switch, the result won’t twitch**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**交换律：如果你交换，结果不会改变**'
- en: '*A+B = B+A*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*A+B = B+A*'
- en: '*But commuting to work might not be commutative since going from A to B might
    take longer than B to A.*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*但上班通勤可能不是交换的，因为从 A 到 B 可能比从 B 到 A 花的时间更长。*'
- en: '**Associative: in a chain, don’t refrain, take any 2 and continue**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**结合律：在链中，不要犹豫，任选两个继续**'
- en: '***A+(B+C) = (A+B)+C***'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '***A+(B+C) = (A+B)+C***'
- en: '**Identity: And here I am where I began! That’s no way to treat a man!**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**恒等：我在这里回到起点！这对一个人来说可不公平！**'
- en: The presence of a special element that when added to anything results in the
    same thing. In the case of scalars, it is the number *0*. In the case of matrices,
    it is a matrix full of zeros.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个特殊的元素，当它与任何东西相加时，结果还是原来的东西。对于标量来说，它是数字 *0*。对于矩阵来说，它是全零矩阵。
- en: '***A + 0 = A or 0 + A = A***'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '***A + 0 = A 或 0 + A = A***'
- en: Also, it is possible to start at any element and end up at any other via addition.
    So it must be possible to start at *A* and end up at the additive identity, *0*.
    The thing that must be added to *A* to achieve this is the additive inverse of
    *A* and it’s called *-A.*
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，可以从任何元素开始，通过加法到达任何其他元素。因此，必须能够从 *A* 开始，到达加法单位 *0*。实现这一点所需加上的内容是 *A* 的加法逆元，称为
    *-A*。
- en: '*A + (-A) = 0*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*A + (-A) = 0*'
- en: For matrices, you just go to each scalar element in the matrix and replace with
    the additive inverse of each one (switching the signs if the scalars are numbers)
    to get the additive inverse of the matrix.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于矩阵，你只需访问矩阵中的每一个标量元素，并用每个元素的加法逆元替换（如果标量是数字，则切换符号）以获得矩阵的加法逆元。
- en: II-B) Subtraction
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II-B) 减法
- en: Subtraction is just addition with the additive inverse of the second matrix
    instead.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 减法实际上只是将第二个矩阵的加法逆元用于加法。
- en: '*A-B = A+(-B)*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*A-B = A+(-B)*'
- en: II-C) Multiplication
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II-C) 乘法
- en: We could have defined matrix multiplication just as we defined matrix addition.
    Just take two matrices that are the same size (rows and columns) and then multiply
    the scalars element by element. There is a name for that kinds of operation, the
    [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像定义矩阵加法一样定义矩阵乘法。只需取两个大小相同（行和列）的矩阵，然后逐元素相乘。对这种操作有一个名称，称为[Hadamard积](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))。
- en: But no, we defined matrix multiplication as a far more convoluted operation,
    more “exotic” than addition. And it isn’t complex just for the sake of it. It
    is the most important operation in linear algebra by far.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但实际上，我们将矩阵乘法定义为一种更复杂的操作，比加法更“异域”。它并不是为了复杂而复杂。它是线性代数中最重要的操作。
- en: It enjoys this special status because it’s the means by which linear maps are
    applied to vectors, building on top of dot products.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它享有这种特殊地位，因为它是将线性映射应用于向量的方式，建立在点积的基础上。
- en: The way it actually works requires a dedicated section, so we’ll cover that
    in section III. Here, let’s list some of its properties.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上它是如何工作的需要一个专门的部分来讨论，我们将在第III节中覆盖这一点。在这里，让我们列出一些它的属性。
- en: '**Commutative**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**交换律**'
- en: Unlike addition, matrix multiplication is not always commutative. Which means
    that the order in which you apply linear maps to your input vector matters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与加法不同，矩阵乘法并不总是满足交换律。这意味着你应用线性映射到输入向量的顺序很重要。
- en: '*A.B != B.A*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*A.B != B.A*'
- en: '**Associative**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**结合律**'
- en: It is still associative
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 它仍然是结合律的
- en: '*A.B.C = A.(B.C) = (A.B).C*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*A.B.C = A.(B.C) = (A.B).C*'
- en: And there is a lot of depth to this property, as we will see in section IV.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 而且这个属性有很大的深度，我们将在第IV节中看到。
- en: '**Identity**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**单位**'
- en: Just like addition, matrix multiplication also has an identity element, *I,*
    an element that when any matrix is multiplied to results in the same matrix. The
    big caveat being that this element only exists for square matrices and is itself
    square.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与加法类似，矩阵乘法也有一个单位元素，*I*，当任何矩阵与其相乘时会得到相同的矩阵。大问题在于，这个元素仅存在于方阵中，而且它自身也是方阵。
- en: Now, because of the importance of matrix multiplication, “the identity matrix”
    in general is defined as the identity element of matrix multiplication (not that
    of addition or the Hadamard product for example).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于矩阵乘法的重要性，通常所说的“单位矩阵”是指矩阵乘法的单位元素（而不是加法或Hadamard积的单位元素）。
- en: 'The identity element for addition is a matrix composed of *0*’s and that of
    the Hadamard product is a matrix composed of *1*’s. The identity element of matrix
    multiplication is:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 加法的单位元素是由*0*构成的矩阵，而Hadamard积的单位元素是由*1*构成的矩阵。矩阵乘法的单位元素是：
- en: '![](../Images/66b7e53fa0d6d41acf5f6a8968fa78a4.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66b7e53fa0d6d41acf5f6a8968fa78a4.png)'
- en: The identity matrix of matrix multiplication and also linear algebra. Image
    by author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法的单位矩阵以及线性代数的单位矩阵。图像由作者提供。
- en: So, *1*’s on the main diagonal and *0*’s everywhere else. What kind of definition
    for matrix multiplication would lead to an identity element like this? We’ll need
    to describe how it works to see, but first let’s go to the final operation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，主对角线上的*1*和其他地方的*0*。什么样的矩阵乘法定义会导致这样的单位元素？我们需要描述它是如何工作的，但首先让我们继续到最后的操作。
- en: II-D) Division
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II-D) 除法
- en: Just as with addition, the presence of an identity matrix suggests any matrix,
    *A* can be multiplied with another matrix, *A^-1* and taken to the identity. This
    is called the inverse. Since matrix multiplication isn’t commutative, there are
    two ways to this. Thankfully, both lead to the identity matrix.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 就像加法一样，单位矩阵的存在表明任何矩阵* A* 可以与另一个矩阵*A^-1*相乘并得到单位矩阵。这称为逆矩阵。由于矩阵乘法不是交换的，这里有两种方法来实现这一点。幸运的是，两者都导致单位矩阵。
- en: '*A.(A^-1) = (A^-1).A = I*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*A.(A^-1) = (A^-1).A = I*'
- en: So, “dividing” a matrix by another is simply multiplication with the second
    ones inverse, *A.B^-1*. If matrix multiplication is very important, then this
    operation is as well since it’s the inverse. It is also related to how we historically
    developed (or maybe stumbled upon) linear algebra. But more on that in the next
    chapter (4).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，“除以”一个矩阵其实就是与第二个矩阵的逆相乘，即 *A.B^-1*。如果矩阵乘法非常重要，那么这个操作也同样重要，因为它是逆操作。它也与我们如何历史性地发展（或许偶然发现）线性代数有关。不过这一点将在下一章（第4章）中讨论。
- en: 'Another property we’ll be using that is a combined property of addition and
    multiplication is the distributive property. It applies to all kinds of matrix
    multiplication from the traditional one to the Hadamard product:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用一个结合了加法和乘法的性质，即分配律。它适用于所有类型的矩阵乘法，从传统的乘法到Hadamard积：
- en: '*A.(B+C) = A.B + A.C*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*A.(B+C) = A.B + A.C*'
- en: III) Why is matrix multiplication defined this way?
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: III) 为什么矩阵乘法被定义成这样？
- en: We have arrived at last to the section where we will answer the question in
    the title, the meat of this chapter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于来到了本章的核心部分，回答标题中的问题。
- en: Matrix multiplication is the way linear maps act on vectors. So, we get to motivate
    it that way.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法是线性映射作用于向量的方式。所以，我们可以这样来激励它。
- en: III-A) How are linear maps applied in practice?
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III-A) 线性映射在实践中的应用是什么？
- en: Consider a linear map that takes *m* dimensional vectors (from *R^m*) as input
    and maps them to *n* dimensional vectors (in *R^n*). Let’s call the *m* dimensional
    input vector, *v*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个线性映射，它将 *m* 维向量（来自 *R^m*）作为输入，并映射到 *n* 维向量（在 *R^n* 中）。我们将 *m* 维输入向量称为 *v*。
- en: At this point, it might be helpful to think of yourself actually coding up this
    linear map in some programming language. It should be a function that takes the
    m-dimensional vector, *v* as input and returns the *n* dimensional vector, *u*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，考虑一下你自己实际在某种编程语言中编写这个线性映射的代码可能会有帮助。它应该是一个将 *m* 维向量 *v* 作为输入并返回 *n* 维向量
    *u* 的函数。
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The linear map has to take this vector and turn it into an *n* dimensional vector
    somehow. In the function above, you’ll notice we just generated some vector at
    random. But this completely ignored the input vector, *v*. That’s unreasonable,
    *v* should have some say. Now, *v* is just an ordered list of *m* scalars *v =
    [v1, v2, v3, …, vm]*. What do scalars do? They scale vectors. And the output vector
    we need should be *n* dimensional. How about we take some (fixed) *m* vectors
    (pulled out of thin air, each *n* dimensional), *w1, w2, …, wm*. Then, scale *w1*
    by v*1*, *w2* by v*2* and so on and add them all up. This leads to an equation
    for our linear map (with the output on the left).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 线性映射必须以某种方式将这个向量转化为 *n* 维向量。在上面的函数中，你会注意到我们只是随机生成了一些向量。但这完全忽略了输入向量 *v*。这是不合理的，*v*
    应该有所影响。现在，*v* 只是一个有序的 *m* 个标量的列表 *v = [v1, v2, v3, …, vm]*。标量的作用是什么？它们缩放向量。我们需要的输出向量应该是
    *n* 维的。我们可以用一些（固定的）*m* 个向量（凭空抽取，每个都是 *n* 维的），*w1, w2, …, wm*。然后，将 *w1* 按 *v1*
    缩放，*w2* 按 *v2* 缩放，依此类推，并将它们全部加起来。这就得到我们的线性映射的方程（输出在左边）。
- en: '![](../Images/ff8453f94f83af00cc41ad37db8b27c9.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff8453f94f83af00cc41ad37db8b27c9.png)'
- en: Eq (1) A linear map motivated as a linear combination of m vectors. Image by
    author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Eq (1) 作为 *m* 个向量的线性组合激发的线性映射。图片由作者提供。
- en: Make note of the equation (1) above since we’ll be using it again.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意上述方程（1），因为我们会再次使用它。
- en: Since the *w1*, *w2,…* are all *n* dimensional, so is *u.* And all the elements
    of *v=[v1, v2, …, vm]* have an influence on the output, *u.* The idea in equation
    (1) is implemented below. We take some randomly generated vectors for the *w*’s
    but with fixed seeds (ensuring that the vectors are the same across every call
    of the function).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *w1*, *w2,…* 都是 *n* 维的，所以 *u* 也是。并且 *v=[v1, v2, …, vm]* 的所有元素都对输出 *u* 产生影响。方程（1）的思想在下面得到实现。我们取一些随机生成的向量作为
    *w*，但用固定的种子（确保每次调用函数时向量相同）。
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We have a way now to “map” *m* dimensional vectors (*v)* to *n* dimensional
    vectors (*u)*. But does this “map” satisfy the properties of a linear map? Recall
    from chapter-1, section II the properties of a linear map, *f* (here, *a* and
    *b* are vectors and *c* is a scalar):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一种方法可以将 *m* 维向量 (*v*) 映射到 *n* 维向量 (*u*)。但这个“映射”是否满足线性映射的性质？请回顾第1章第II节的线性映射性质，*f*（这里，*a*
    和 *b* 是向量，*c* 是标量）：
- en: '*f(a+b) = f(a) + f(b)*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*f(a+b) = f(a) + f(b)*'
- en: '*f(c.a) = c.f(a)*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*f(c.a) = c.f(a)*'
- en: It’s clear that the map specified by equation (1) satisfies the above two properties
    of a linear map.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，方程（1）所指定的映射满足上述两个线性映射的性质。
- en: '![](../Images/cf3004024206b300d57137c118695129.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf3004024206b300d57137c118695129.png)'
- en: The function defined by equation (1) satisfies the property that the function
    of sum is the sum of functions. Image by author.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（1）中定义的函数满足一个属性，即函数的和是函数的和。图片来源：作者。
- en: '![](../Images/019f8e8cc74f75ac4b3844d59b696145.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/019f8e8cc74f75ac4b3844d59b696145.png)'
- en: The function defined in equation (1) satisfies the property that a scalar times
    a vector passed to the function is equivalent to the scalar times the vector passed
    to the function. Image by author.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（1）中定义的函数满足一个属性，即标量乘以传递给函数的向量等同于标量乘以传递给函数的向量。图片来源：作者。
- en: The *m* vectors, *w1, w2, …, wm* are arbitrary and no matter what we choose
    for them, the function, *f* defined in equation (1) is a linear map. So, different
    choices for those *w* vectors results in different linear maps. Moreover, for
    any linear map you can imagine, there will be some vectors *w1, w2,…* that can
    be applied in conjunction with equation (1) to represent it.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*m*个向量*w1, w2, …, wm*是任意的，无论我们选择什么，它们，方程（1）中定义的函数*f*都是一个线性映射。因此，对于这些*w*向量的不同选择会导致不同的线性映射。此外，对于你能想象的任何线性映射，都将有一些向量*w1,
    w2,…*可以与方程（1）一起应用来表示它。'
- en: Now, for a given linear map, we can collect the vectors *w1, w2,…* into the
    columns of a matrix. Such a matrix will have *n* rows and *m* columns. This matrix
    represents the linear map, *f* and its multiplication with an input vector, *v*
    represents the application of the linear map, *f* to *v*. And this application
    is where the definition of matrix multiplication comes from.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于给定的线性映射，我们可以将向量*w1, w2,…*收集到一个矩阵的列中。这样的矩阵将具有*n*行和*m*列。这个矩阵表示了线性映射，*f*，它与输入向量*v*的乘积表示了线性映射*f*作用于*v*。这种应用就是矩阵乘法定义的来源。
- en: '![](../Images/f7cfaab8913283d2eef05d37e5b65326.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7cfaab8913283d2eef05d37e5b65326.png)'
- en: 'Animation (1): Matrix vector multiplication. Image by author.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 动画（1）：矩阵向量乘法。 图片来源：作者。
- en: 'We can now see why the identity element for matrix multiplication is the way
    it is:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到为什么矩阵乘法的单位元素是现在的样子：
- en: '![](../Images/f8f795682bef71d6ae09a2baea562089.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8f795682bef71d6ae09a2baea562089.png)'
- en: Animation (2) Why is the identity matrix of multiplication what it is? Image
    by author.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 动画（2）为什么乘法的单位矩阵是它现在的样子？ 图片来源：作者。
- en: We start with a column vector, *v* and end with a column vector, *u* (so just
    one column for each of them). And since the elements of *v* must align with the
    column vectors of the matrix representing the linear map, the number of columns
    of the matrix must equal the number of elements in *v*. More on this in section
    III-C.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个列向量*v*开始，并以一个列向量*u*结束（每个只有一列）。由于*v*的元素必须与表示线性映射的矩阵的列向量对齐，因此矩阵的列数必须等于*v*中的元素数。更多内容请参见第III-C节。
- en: III-B) Matrix multiplication as a composition of linear maps
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III-B) 矩阵乘法作为线性映射的组合
- en: Now that we described how a matrix is multiplied to a vector, we can move on
    to multiplying a matrix with another matrix.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经描述了矩阵如何与向量相乘，我们可以继续讨论矩阵与另一个矩阵的相乘。
- en: The definition of matrix multiplication is much more natural when we consider
    the matrices as representations of linear maps.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将矩阵视为线性映射的表示时，矩阵乘法的定义自然得多。
- en: Linear maps are functions that take a vector as input and produce a vector as
    output. Let’s say the linear maps corresponding to two matrices are *f* and *g*.
    How would you think of adding these maps *(f+g)*?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 线性映射是将一个向量作为输入并产生一个向量作为输出的函数。假设与两个矩阵对应的线性映射是*f*和*g*。你如何理解这些映射的加法*(f+g)*？
- en: '*(f+g)(v) = f(v)+g(v)*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*(f+g)(v) = f(v)+g(v)*'
- en: This is reminiscent of the distributive property of addition where the argument
    goes inside the bracket to both the functions and we add the results. And if we
    fix a basis, this corresponds to applying both linear maps to the input vector
    and adding the result. By the distributive property of matrix and vector multiplication,
    this is the same as adding the matrices corresponding to the linear maps and applying
    the result to the vector.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这让人想起了加法的分配律，其中参数进入括号内的两个函数，然后我们将结果相加。如果我们固定一个基，这对应于将两个线性映射应用于输入向量并将结果相加。根据矩阵和向量乘法的分配律，这等同于将对应于线性映射的矩阵相加，并将结果应用于向量。
- en: Now, let’s think of multiplication *(f.g)*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑乘法*(f.g)*。
- en: '*(f.g)(v) = f(g(v))*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*(f.g)(v) = f(g(v))*'
- en: Since linear maps are functions, the most natural interpretation of multiplication
    is to compose them (apply them one at a time, in sequence to the input vector).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于线性映射是函数，乘法的最自然解释是将它们组合在一起（按顺序一个接一个地应用到输入向量上）。
- en: When two matrices are multiplied, the resulting matrix represents the composition
    of the corresponding linear maps. Consider matrices A and B; the product *AB*
    embodies the transformation achieved by applying the linear map represented by
    *B* to the input vector first and then applying the linear map represented by
    *A*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个矩阵相乘时，结果矩阵表示相应线性映射的组合。考虑矩阵A和B；积*AB*体现了先对输入向量应用由*B*表示的线性映射，然后再应用由*A*表示的线性映射所实现的变换。
- en: So we have a linear map corresponding to the matrix, *A* and a linear map corresponding
    to the matrix, *B*. We’d like to know the matrix, *C* corresponding to the composition
    of the two linear maps. So, applying *B* to any vector first and then applying
    *A* to the result should be equivalent to just applying *C.*
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有一个与矩阵*A*对应的线性映射，以及一个与矩阵*B*对应的线性映射。我们希望知道与这两个线性映射的组合对应的矩阵*C*。因此，首先对任意向量应用*B*，然后对结果应用*A*，应该等同于直接应用*C*。
- en: '*A.(B.v) = C.v = (A.B).v*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*A.(B.v) = C.v = (A.B).v*'
- en: In the last section, we learnt how to multiply a matrix and a vector. Let’s
    do that twice for *A.(B.v).* Say the columns of *B* are the column vectors, *b1,
    b2, …, bm.* From equation (1) in the previous section,
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分，我们学习了如何乘以矩阵和向量。让我们对*A.(B.v)*再做一次。假设*B*的列是列向量*b1, b2, …, bm*。从前一部分的方程（1）来看，
- en: '![](../Images/55f3db899766ddec60fad8d62cea67a3.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55f3db899766ddec60fad8d62cea67a3.png)'
- en: Proving that matrix multiplication is just linear maps applied in sequence.
    Image by author.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 证明矩阵乘法只是顺序应用的线性映射。图片由作者提供。
- en: And what if we applied the linear map corresponding to *C=A.B* directly to the
    vector, *v.* The column vectors of the matrix *C* are *c1, c2, …, ck.*
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们直接将与*C=A.B*对应的线性映射应用到向量*v*上会怎样呢？矩阵*C*的列向量是*c1, c2, …, ck*。
- en: '![](../Images/8f851974344b774f823d6e290325a22e.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f851974344b774f823d6e290325a22e.png)'
- en: The same result as animation (2), multiplying a matrix with a vector. Image
    by author.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与动画（2）相同，矩阵与向量的乘法。图片由作者提供。
- en: Comparing the two equations above we get,
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 比较上述两个方程，我们得到，
- en: '![](../Images/3717d381958068a05e19f6d392d5168d.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3717d381958068a05e19f6d392d5168d.png)'
- en: 'Eq (2): the column vectors of the matrix C = AB where b1, b2,… are the column
    vectors of B. Image by author.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（2）：矩阵C = AB的列向量，其中b1, b2,… 是B的列向量。图片由作者提供。
- en: So, the columns of the product matrix, *C=AB* are obtained by applying the linear
    map corresponding to matrix *A* to each of the columns of the matrix *B.* And
    collecting those resulting vectors into a matrix gives us *C*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，乘积矩阵*C=AB*的列是通过对矩阵*B*的每一列应用与矩阵*A*对应的线性映射来获得的。将这些结果向量收集成一个矩阵就得到了*C*。
- en: We have just extended our matrix-vector multiplication result from the previous
    section to the multiplication of two matrices. We just break the second matrix
    into a collection of vectors, multiply the first matrix to all of them and collect
    the resulting vectors into the columns of the result matrix.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚将矩阵-向量乘法的结果扩展到了两个矩阵的乘法。我们只是将第二个矩阵分解为一个向量集合，将第一个矩阵应用到所有这些向量上，然后将结果向量收集到结果矩阵的列中。
- en: '![](../Images/81ed7b47ad5c845fcecf966b514563be.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81ed7b47ad5c845fcecf966b514563be.png)'
- en: Eq (3) The column vectors of the C matrix (C=A.B). Image by author.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（3）：矩阵C（C=A.B）的列向量。图片由作者提供。
- en: So the first row and first column of the result matrix, *C* is the dot product
    of the first column of *B* and the first row of *A.* And in general the *i*-th
    row and *j*-th column of *C* is the dot product of the *i*-th row of *A* and the
    *j*-th column of *B.* This is the definition of matrix multiplication most of
    us first learn.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 所以结果矩阵*C*的第一行第一列是矩阵*B*的第一列与矩阵*A*的第一行的点积。一般而言，*C*的第*i*行第*j*列是矩阵*A*的第*i*行与矩阵*B*的第*j*列的点积。这是我们大多数人最初学习的矩阵乘法定义。
- en: '![](../Images/52d4de8e1be991023858062a4b73c900.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52d4de8e1be991023858062a4b73c900.png)'
- en: 'Animation (3): Matrix multiplication as dot products. Image by author.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 动画（3）：矩阵乘法作为点积。图片由作者提供。
- en: '**Associative proof**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**结合律证明**'
- en: 'We can also show that matrix multiplication is associative now. Instead of
    the single vector, *v*, let’s apply the product C=*AB* individuallyto a group
    of vectors, *w1, w2, …, wl*. Let’s say the matrix that has these as column vectors
    is *W*. We can use the exact same trick as above to show:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在也可以展示矩阵乘法是结合的。用 *AB* 的乘积代替单一向量 *v*，将其应用于一组向量 *w1, w2, …, wl*。假设含有这些向量作为列向量的矩阵是
    *W*。我们可以使用上面完全相同的技巧来展示：
- en: '*(A.B).W = A.(B.W)*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*(A.B).W = A.(B.W)*'
- en: It’s because *(A.B).w1 = A.(B.w1)* and the same for all the other *w* vectors.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为 *(A.B).w1 = A.(B.w1)*，所有其他 *w* 向量也一样。
- en: '**Sum of outer products**'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**外积的和**'
- en: 'Say we’re multiplying two matrices *A* and *B*:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在乘两个矩阵 *A* 和 *B*：
- en: '![](../Images/829020840b10a380d034fe52c71c8eab.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/829020840b10a380d034fe52c71c8eab.png)'
- en: Multiplication of two matrices. Image by author.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 两个矩阵的乘法。图片由作者提供。
- en: 'Equation (3) can be generalized to show that the *i*,*j* element of the resulting
    matrix, *C* is:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 (3) 可以推广为显示结果矩阵 *C* 的 *i*,*j* 元素是：
- en: '![](../Images/ca37125b6f2da37bcf5ffae9aae53412.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca37125b6f2da37bcf5ffae9aae53412.png)'
- en: 'Eq (4): Generalization of equation (3). Image by author.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 等式 (4)：方程 (3) 的推广。图片由作者提供。
- en: 'We have a sum over *k* terms. What if we took each of those terms and created
    *k* individual matrices out of them. For example, the first matrix will have as
    its *i*,*j-*th entry: *b_{i,1}. a_{1,j}*. The *k* matrices and their relationship
    to *C:*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有对 *k* 项的求和。如果我们将每一项取出并创建 *k* 个单独的矩阵。例如，第一个矩阵将具有 *i*,*j-* 项：*b_{i,1}. a_{1,j}*。这
    *k* 个矩阵及其与 *C* 的关系：
- en: '![](../Images/2e2b65fa5b4b005b938f3164a290daa5.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e2b65fa5b4b005b938f3164a290daa5.png)'
- en: Matrix multiplication is the sum of k sub-matrices. Image by author.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法是 *k* 个子矩阵的和。图片由作者提供。
- en: 'This process of summing over *k* matrices can be visualized as follows (reminiscent
    of the animation in section III-A that visualized a matrix multiplied to a vector):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对 *k* 个矩阵的求和过程可以如下可视化（类似于 III-A 节中将矩阵乘法可视化为向量的动画）：
- en: '![](../Images/38c2f82ac398fcfa5d6ed1b09133f768.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38c2f82ac398fcfa5d6ed1b09133f768.png)'
- en: 'Animation (4): Matrix multiplication by expanding to 3-d. Image by author.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 动画 (4)：通过扩展到三维展示矩阵乘法。图片由作者提供。
- en: We see here the sum over *k* matrices all of the same size (*n*x*m*) which is
    the same size as the result matrix, *C*. Notice in equation (4) how for the first
    matrix, *A*, the column index stays the same while for the second matrix, *B*,
    the row index stays the same. So the *k* matrices we’re getting are the matrix
    products of the *i*-th column of *A* and the *i*-th **row** of *B*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到对 *k* 个相同大小 (*n*x*m*) 矩阵的求和，这些矩阵的大小与结果矩阵 *C* 相同。注意在等式 (4) 中，对于第一个矩阵 *A*，列索引保持不变，而对于第二个矩阵
    *B*，行索引保持不变。所以我们得到的 *k* 个矩阵是 *A* 的第 *i* 列和 *B* 的第 *i* 个**行**的矩阵乘积。
- en: '![](../Images/f3148dbd9e5da635bb3ba467c4ba0997.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3148dbd9e5da635bb3ba467c4ba0997.png)'
- en: Matrix multiplication as a sum of outer products. Image by author.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法作为外积的和。图片由作者提供。
- en: 'Inside the summation, two vectors are multiplied to produce matrices. It’s
    a special case of matrix multiplication when applied to vectors (special cases
    of matrices) and called “outer product”. Here is yet another animation to show
    this sum of outer products process:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在求和内部，两个向量相乘生成矩阵。这是一种应用于向量（矩阵的特殊情况）的矩阵乘法，称为“外积”。这里还有另一个动画展示这个外积和过程：
- en: '![](../Images/64038aec7fcf352594bed9a59b424235.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64038aec7fcf352594bed9a59b424235.png)'
- en: 'Animation (5): Matrix multiplication as a sum of outer products. Image by author.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 动画 (5)：将矩阵乘法视为外积的和。图片由作者提供。
- en: This tells us why the number of row vectors in *B* should be the same as the
    number of column vectors in *A*. Because they have to be mapped together to get
    the individual matrices.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们为什么 *B* 的行向量数量应该与 *A* 的列向量数量相同。因为它们必须一一对应以获得各自的矩阵。
- en: We’ve seen a lot of visualizations and some math, now let’s see the same thing
    via code for the special case where *A* and *B* are square matrices. This is based
    on section 4.2 of the book Introduction to Algorithms, [2].
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到很多可视化和一些数学内容，现在让我们通过代码查看相同的内容，特别是 *A* 和 *B* 是方阵的情况。这基于《算法导论》第4.2节，[2]。
- en: '[PRE2]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'III-C) Matrix multiplication: the structural choices'
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III-C) 矩阵乘法：结构选择
- en: '![](../Images/d317ff7f16031fd1f6b778bc4788e2eb.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d317ff7f16031fd1f6b778bc4788e2eb.png)'
- en: Image created with midjourney
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 midjourney 创建
- en: Matrix multiplication seems to be structured in a weird way. It’s clear that
    we need to take a bunch of dot products. So, one of the dimensions has to match.
    But why make the columns of the first matrix be equal to the number of rows of
    the second one?
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法似乎以一种奇怪的方式结构化。很明显，我们需要进行一堆点积运算。因此，其中一个维度必须匹配。但是为什么要使第一个矩阵的列数等于第二个矩阵的行数呢？
- en: Won’t it make things more straightforward if we redefine it in a way that the
    number of rows of the two matrices should be the same (or the number of columns)?
    This would make it much easier to identify when two matrices can be multiplied.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重新定义矩阵，使得两个矩阵的行数（或列数）相同，这样会不会使事情变得更简单？这将使识别两个矩阵是否可以相乘变得更加容易。
- en: The traditional definition where we require the rows of the first matrix to
    align with the columns of the second one has more than one advantage. Let’s go
    first to matrix-vector multiplication. Animation (1) in section III-A showed us
    how the traditional version works. Let’s visualize what it if we required the
    rows of the matrix to align with the number of elements in the vector instead.
    Now, the *n* rows of the matrix will need to align with the *n* elements of the
    vector.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 传统定义中要求第一个矩阵的行与第二个矩阵的列对齐有多个优点。我们首先来看矩阵-向量乘法。第三节A中的动画（1）向我们展示了传统版本是如何工作的。让我们可视化一下，如果我们要求矩阵的行与向量中的元素数量对齐会是什么样的。现在，矩阵的
    *n* 行将需要与向量的 *n* 元素对齐。
- en: '![](../Images/a5d51746d8fad51a27f5dba51300a5bf.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5d51746d8fad51a27f5dba51300a5bf.png)'
- en: 'Animation (6): what would an alternate setup for matrix multiplication look
    like? Image by author.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 动画（6）：矩阵乘法的替代设置会是什么样的？作者图片。
- en: We see that we’d have to start with a column vector, *v* with *n* rows and one
    column and end up with a row vector, *u* with *1* row and *m* columns. This is
    awkward and makes defining an identity element for matrix multiplication challenging
    since the input and output vectors can never have the same shape. With the traditional
    definition, this isn’t an issue since the input is a column vector and the output
    is also a column vector (see animation (1)).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们必须以一个具有 *n* 行和一列的列向量 *v* 开始，最终得到一个具有 *1* 行和 *m* 列的行向量 *u*。这很尴尬，并且使得为矩阵乘法定义一个单位元素变得具有挑战性，因为输入和输出向量永远不能具有相同的形状。使用传统定义，这不是问题，因为输入是一个列向量，而输出也是一个列向量（见动画（1））。
- en: Another consideration is multiplying a chain of matrices. In the traditional
    method, it is so easy to see first of all that the chain of matrices below can
    be multiplied together based on their dimensionalities.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个考虑因素是矩阵链的乘法。在传统方法中，很容易首先看到下面的矩阵链可以根据它们的维度进行相乘。
- en: '![](../Images/034ee3517b1dbb116eeb38dd902efad5.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/034ee3517b1dbb116eeb38dd902efad5.png)'
- en: What matrix chain multiplication looks like with the traditional accepted method.
    Image by author.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 传统接受的方法下矩阵链乘法的样子。作者图片。
- en: Further, we can tell that the output matrix will have *l* rows and *p* columns.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以确定输出矩阵将有 *l* 行和 *p* 列。
- en: In the framework where the rows of the two matrices should line up, this quickly
    becomes a mess. For the first two matrices, we can tell that the rows should align
    and that the result will have *n* rows and *l* columns. But visualizing how many
    rows and columns the result will have and then reasoning about weather it’ll be
    compatible with *C*, etc. becomes a nightmare.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在要求两个矩阵的行对齐的框架下，这很快就会变得混乱。对于前两个矩阵，我们可以看出行应该对齐，并且结果将会有 *n* 行和 *l* 列。但是，想象一下结果会有多少行和列，然后再推断它是否与
    *C* 兼容等，就成了一场噩梦。
- en: '![](../Images/bb16626a5b91d35097a894ec7282d6c4.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb16626a5b91d35097a894ec7282d6c4.png)'
- en: What matrix chain multiplication might look like if we modified the definition
    of matrix multiplication. Image by author.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们修改矩阵乘法的定义，矩阵链乘法可能会是什么样的。作者图片。
- en: And that is why we require the rows of the first matrix to align with the columns
    of the second matrix. But maybe I missed something. Maybe there is an alternate
    definition that is “cleaner” and manager to side-step these two challenges. Would
    love to hear ideas in the comments :)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们要求第一个矩阵的行与第二个矩阵的列对齐的原因。但也许我遗漏了什么。也许有一个“更干净”的替代定义能够绕过这两个挑战。欢迎在评论中分享想法
    :)
- en: III-D) Matrix multiplication as a change of basis
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III-D) 矩阵乘法作为基变换
- en: So far, we’ve thought of matrix multiplication with vectors as a linear map
    that takes a vector as input and returns some other vector as output. But there
    is another way to think of matrix multiplication — as a way to change perspective.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们把矩阵乘法与向量视为一种线性映射，它接受一个向量作为输入，并返回另一个向量作为输出。但矩阵乘法还有另一种思考方式——作为一种改变视角的方式。
- en: Let’s consider two-dimensional space, *R².* We represent any vector in this
    space with two numbers. What do those numbers represent? The coordinates along
    the x-axis and y-axis. A unit vector that points just along the x-axis is *[1,0]*
    and one that points along the y-axis is *[0,1]*. These are our basis for the space.
    Every vector now has an address. For example, the vector *[2,3]* means we scale
    the first basis vector by *2* and the second one by *3*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑二维空间 *R²*。我们用两个数字来表示这个空间中的任何向量。这些数字代表什么？它们分别是沿x轴和y轴的坐标。一个指向x轴的单位向量是 *[1,0]*，一个指向y轴的单位向量是
    *[0,1]*。这些是我们空间的基底。现在每个向量都有了一个地址。例如，向量 *[2,3]* 意味着我们将第一个基底向量缩放 *2* 倍，第二个基底向量缩放
    *3* 倍。
- en: But this isn’t the only basis for the space. Someone else (say, he who shall
    not be named) might want to use two other vectors as their basis. For example,
    the vectors *e1=[3,2]* and *e2=[1,1]*. Any vector in the space *R²* can also be
    expressed in their basis. The same vector would have different representations
    in our basis and their basis. Like different addresses for the same house (perhaps
    based on different postal systems).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不是空间的唯一基底。其他人（比如，那位不可提及者）可能想使用另外两个向量作为他们的基底。例如，向量 *e1=[3,2]* 和 *e2=[1,1]*。空间
    *R²* 中的任何向量也可以用他们的基底来表示。相同的向量在我们的基底和他们的基底下会有不同的表示。就像同一所房子的不同地址（可能基于不同的邮政系统）。
- en: When we’re in the basis of he who shall not be named, the vector *e1 = [1,0]*
    and the vector *e2 = [0,1] (*which are the basis vectors from his perspective
    by definition of basis vectors). And the functions that translates vectors from
    our basis system to that of he who shall not be named and vise-versa are linear
    maps. And so the translations can be represented as matrix multiplications. Let’s
    call the matrix that takes vectors from us to the vectors to he who shall not
    be named, *M1* and the matrix that does the opposite, *M2\.* How do we find the
    matrices for these matrices?
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在那位不可提及者的基底下时，向量 *e1 = [1,0]* 和向量 *e2 = [0,1]* （这是按基底向量的定义，从他的视角看是基底向量）。将向量从我们的基底系统转换到那位不可提及者的基底系统以及反向转换的函数是线性映射。因此，这些转换可以表示为矩阵乘法。我们将将向量从我们这里转换到那位不可提及者的向量的矩阵称为
    *M1*，而执行相反操作的矩阵称为 *M2*。我们如何找到这些矩阵呢？
- en: '![](../Images/bdd89151aa58d32c4ad7d74f701b8d01.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdd89151aa58d32c4ad7d74f701b8d01.png)'
- en: Shifting your perspective in seeing the world. Image by midjourney. Image by
    author.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 改变你看待世界的视角。图像由midjourney提供。图像由作者提供。
- en: We know that the vectors we call *e1=[3,2]* and *e2=[1,1],* he who shall not
    be named calls *e1=[1,0]* and *e2=[0,1].* Let’s collect our version of the vectors
    into the columns of a matrix.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，我们称之为 *e1=[3,2]* 和 *e2=[1,1]* 的向量，而那位不可提及者称 *e1=[1,0]* 和 *e2=[0,1]*。让我们将我们版本的向量收集到一个矩阵的列中。
- en: '![](../Images/c36906403917474a9f09bae0a2441edf.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c36906403917474a9f09bae0a2441edf.png)'
- en: A basic 2 by 2 matrix composed of two column vectors. Image by author.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由两个列向量组成的基本2乘2矩阵。图像由作者提供。
- en: And also collect the vectors, *e1* and *e2* of he who shall not be named into
    the columns of another matrix. This is just the identity matrix.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 并将那位不可提及者的向量 *e1* 和 *e2* 收集到另一个矩阵的列中。这只是单位矩阵。
- en: '![](../Images/e5ced9ba006de4b7d3bdc4f8a9abd423.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5ced9ba006de4b7d3bdc4f8a9abd423.png)'
- en: We’d like to transform the 2 by 2 matrix into the identity matrix to change
    basis. Image by author.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将2乘2矩阵转换为单位矩阵以改变基底。图像由作者提供。
- en: Since matrix multiplication operates independently on the columns of the second
    matrix,
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 由于矩阵乘法对第二个矩阵的列进行独立操作，
- en: '![](../Images/bbe606253de58d7921b2c29a9cacae9c.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbe606253de58d7921b2c29a9cacae9c.png)'
- en: The equation that moves the matrix to the identity. Image by author.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 将矩阵移到单位矩阵的方程。图像由作者提供。
- en: Pre-multiplying by an appropriate matrix on both sides gives us *M1:*
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在两侧乘以适当的矩阵得到 *M1:*
- en: '![](../Images/58516bf67ec981508e7bb87a71fa9c8e.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58516bf67ec981508e7bb87a71fa9c8e.png)'
- en: Changing basis with a matrix. Image by author.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 用矩阵改变基底。图像由作者提供。
- en: Doing the same thing in reverse gives us *M2:*
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 反向操作得到 *M2:*
- en: '![](../Images/d3b9a7e810226d19f4bfe489a5072f25.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3b9a7e810226d19f4bfe489a5072f25.png)'
- en: Reverse mapping to basis. Image by author.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 反向映射到基底。图像由作者提供。
- en: 'This can all be generalized into the following statement: A matrix with column
    vectors; *w1, w2, …, wn* translates vectors expressed in a basis where *w1, w2,
    …, wn* are the basis vectors to our basis.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以概括为以下陈述：一个列向量为 *w1, w2, …, wn* 的矩阵将以 *w1, w2, …, wn* 为基向量的基中表达的向量转换到我们的基中。
- en: And the inverse of that matrix translates vectors from our basis to the one
    where *w1, w2, …, wn* are the basis.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 而该矩阵的逆矩阵将向量从我们的基变换到* w1, w2, …, wn* 为基的那个基。
- en: All square matrices can hence be thought of as “basis changers”.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，所有方阵可以被视为“基变换器”。
- en: 'Note: In the special case of an orthonormal matrix (where every column is a
    unit vector and orthogonal to every other column), the inverse becomes the same
    as the transpose. So, changing to the basis of the columns of such a matrix becomes
    equivalent to taking the dot product of a vector with each of the rows.'
  id: totrans-197
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：在特殊情况下，正交归一矩阵（每列是单位向量且与其他列正交）的逆矩阵与其转置矩阵相同。因此，将矩阵的列作为基变换就等同于将一个向量与每一行进行点积。
- en: For more on this, see the [3B1B video, [1]](https://www.youtube.com/watch?v=P2LTAUO1TdA&t=2s).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参见 [3B1B 视频，[1]](https://www.youtube.com/watch?v=P2LTAUO1TdA&t=2s)。
- en: Conclusion
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Matrix multiplication is arguably one of the most important operations in modern
    computing and also with almost any data science field. Understanding deeply how
    it works is important for any data scientist. Most linear algebra textbooks describe
    the “what” but not why its structured the way it is. Hopefully this blog filled
    that gap.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法可以说是现代计算和几乎所有数据科学领域中最重要的操作之一。深入理解其工作原理对任何数据科学家来说都非常重要。大多数线性代数教材描述了“是什么”，但没有解释其结构的原因。希望这篇博客填补了这个空白。
- en: References
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] 3B1B video on change of basis: [https://www.youtube.com/watch?v=P2LTAUO1TdA&t=2s](https://www.youtube.com/watch?v=P2LTAUO1TdA&t=2s)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 关于基变换的 3B1B 视频: [https://www.youtube.com/watch?v=P2LTAUO1TdA&t=2s](https://www.youtube.com/watch?v=P2LTAUO1TdA&t=2s)'
- en: '[2] Introduction to Algorithms by Cormen et.al. Third edition'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 《算法导论》，作者 Cormen 等，第三版'
- en: '[3] Matrix multiplication as sum of outer products: [https://math.stackexchange.com/questions/2335457/matrix-at-a-as-sum-of-outer-products](https://math.stackexchange.com/questions/2335457/matrix-at-a-as-sum-of-outer-products)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 矩阵乘法作为外积之和: [https://math.stackexchange.com/questions/2335457/matrix-at-a-as-sum-of-outer-products](https://math.stackexchange.com/questions/2335457/matrix-at-a-as-sum-of-outer-products)'
- en: '[4] Catalan numbers wikipedia article [https://en.wikipedia.org/wiki/Catalan_number](https://en.wikipedia.org/wiki/Catalan_number)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 卡塔兰数维基百科条目 [https://en.wikipedia.org/wiki/Catalan_number](https://en.wikipedia.org/wiki/Catalan_number)'
