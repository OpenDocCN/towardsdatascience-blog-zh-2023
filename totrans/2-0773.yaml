- en: Dynamically Rewired Delayed Message Passing GNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/dynamically-rewired-delayed-message-passing-gnns-2d5ff18687c2](https://towardsdatascience.com/dynamically-rewired-delayed-message-passing-gnns-2d5ff18687c2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Delayed Message Passing
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Message-passing graph neural networks (MPNNs) tend to suffer from the phenomenon
    of *over-squashing,* causing performance deterioration for tasks relying on long-range
    interactions. This can be largely attributed to message passing only occurring
    *locally*, over a nodeâ€™s immediate neighbours. Traditional static graph rewiring
    techniques typically attempt to counter this effect by allowing distant nodes
    to communicate instantly (and in the extreme case of Transformers, by making all
    nodes accessible at every layer). However, this incurs a computational price and
    comes at the expense of breaking the inductive bias provided by the input graph
    structure. In this post, we describe two novel mechanisms to overcome over-squashing
    while offsetting the side effects of static rewiring approaches: *dynamic rewiring*
    and *delayed* message passing. These techniques can be incorporated into any MPNN
    and lead to better performance than graph Transformers on long-range tasks.'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://michael-bronstein.medium.com/?source=post_page-----2d5ff18687c2--------------------------------)[![Michael
    Bronstein](../Images/1aa876fce70bb07bef159fecb74e85bf.png)](https://michael-bronstein.medium.com/?source=post_page-----2d5ff18687c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2d5ff18687c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2d5ff18687c2--------------------------------)
    [Michael Bronstein](https://michael-bronstein.medium.com/?source=post_page-----2d5ff18687c2--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2d5ff18687c2--------------------------------)
    Â·9 min readÂ·Jun 19, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8144f73a5808c0a411b10b828b6ec09.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: 'Image: based on Shutterstock.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '*This post was co-authored with Francesco Di Giovanni and Ben Gutteridge and
    is based on the paper by B. Gutteridge et al., DRew:* [*Dynamically rewired message
    passing with delay*](https://arxiv.org/pdf/2305.08018.pdf) *(2023), ICML.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Classical message-passing graph neural networks (MPNNs) operate by aggregating
    information from 1-hop neighbours of every node. Consequently, learning tasks
    requiring *long-range interactions* (i.e., there exists a node *v* whose representation
    needs to account for the information contained in some node *u* at shortest-walk
    (geodesic) distance *d*(*u*,*v*) = *r* > 1) require deep MPNNs with multiple message-passing
    layers. If the graph structure is such that the receptive field expands exponentially
    fast with the hop distance [1], one may need to â€œsqueezeâ€ too many messages into
    a fixed node feature vector â€” a phenomenon known as *over-squashing* [2].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69c830c6c70d0657852dd68b8b8e2d8d.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: '**Over-squashing**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous works [3â€“4], we formalised over-squashing as the *lack of sensitivity*
    of the MPNN output at a node *u* to the input at an *r*-distant node. This can
    be quantified by a bound on the partial derivative (*Jacobian*) of the form
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '|âˆ‚**x***áµ¤*â½*Ê³*â¾/âˆ‚**x***áµ¥*â½â°â¾| < *c* (**A***Ê³*)*áµ¤áµ¥.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Here *c* is a constant dependent on the MPNN architecture (e.g., Lipschitz regularity
    of the activation function, depth, etc.) and **A** is the normalised adjacency
    matrix of the graph. Over-squashing occurs when the entries of **A***Ê³* decay
    exponentially fast with distance *r*. In fact, it is now known that over-squashing
    is more generally a phenomenon that can be related to local structure of the graph
    (such as negative curvature [3]), or its global structure beyond the shortest-walk
    distance (e.g., commute time or effective resistance [4, 5]).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The powers **A***Ê³* in the above expression reflect the fact that the communication
    between *nodes u and v at distance r* in an MPNN is a sequence of interactions
    between adjacent nodes comprising different paths that connect *u* and *v*. As
    a result, the nodes *u* and *v* exchange information only from *r*th layer onwards,
    and with *latency* equal to their distance *r*. Over-squashing is caused by this
    information being â€œdilutedâ€ through repeated message passing over intermediate
    nodes along these paths.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Graph rewiring**'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The issue of over-squashing can be addressed by partially decoupling the input
    graph structure from the one used as support for computing messages, a procedure
    known as *graph rewiring* [6]. Typically, rewiring is performed as a pre-processing
    step in which the input graph *G* is replaced with some other graph *Gâ€™* that
    is â€œfriendlierâ€ for message-passing, according to some spatial or spectral connectivity
    measure.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to achieve this amounts to connecting all nodes within a certain
    distance, thus allowing them to exchange information *directly*. This is the idea
    behind the multi-hop message passing scheme [7]. Graph Transformers [8] take this
    to the extreme, connecting *all* pairs of nodes through an attention-weighted
    edge.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: This way, the information is no longer â€œmixedâ€ with that of other nodes along
    the way and over-squashing can be avoided. However, such a rewiring makes the
    graph much denser from the first layer, increasing the computational footprint
    and partly compromising the inductive bias afforded by the input graph, since
    both local and global nodes interact identically and *instantaneously* at each
    layer.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œä¿¡æ¯ä¸å†ä¸æ²¿é€”å…¶ä»–èŠ‚ç‚¹çš„ä¿¡æ¯â€œæ··åˆâ€ï¼Œå¯ä»¥é¿å…è¿‡åº¦å‹ç¼©ã€‚ç„¶è€Œï¼Œè¿™ç§é‡è¿ä½¿å›¾åœ¨ç¬¬ä¸€å±‚æ—¶å˜å¾—æ›´åŠ ç¨ å¯†ï¼Œå¢åŠ äº†è®¡ç®—è´Ÿæ‹…ï¼Œå¹¶ä¸”éƒ¨åˆ†å¦¥åäº†è¾“å…¥å›¾æ‰€æä¾›çš„å½’çº³åå·®ï¼Œå› ä¸ºåœ¨æ¯ä¸€å±‚ä¸­ï¼Œæœ¬åœ°å’Œå…¨å±€èŠ‚ç‚¹çš„äº¤äº’éƒ½æ˜¯*ç¬æ—¶çš„*ã€‚
- en: '![](../Images/d2061ebebf4f75b3a8a5695c95ba70d4.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2061ebebf4f75b3a8a5695c95ba70d4.png)'
- en: In a classical MPNN (left), information from node *u* arrives at node *v* (which
    is 3-hops away) after 3 message passing steps along the input graph. Accordingly,
    node v always â€œseesâ€ node u with a constant lag (delay) equal to their distance
    on the graph. In the extreme example of graph rewiring used in graph Transformers
    (right), all the nodes are connected, making the information of node *u* available
    at *v* immediately; however, this comes at the expense of losing the partial ordering
    afforded by the graph distance, which needs to be rediscovered through positional
    and structural augmentation of the features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»å…¸ MPNNï¼ˆå·¦ä¾§ï¼‰ä¸­ï¼Œæ¥è‡ªèŠ‚ç‚¹ *u* çš„ä¿¡æ¯åœ¨ç»è¿‡ 3 æ¬¡æ¶ˆæ¯ä¼ é€’æ­¥éª¤ååˆ°è¾¾è·ç¦» 3 æ­¥çš„èŠ‚ç‚¹ *v*ã€‚å› æ­¤ï¼ŒèŠ‚ç‚¹ *v* æ€»æ˜¯ä»¥å›ºå®šçš„å»¶è¿Ÿï¼ˆæ»åï¼‰â€œçœ‹åˆ°â€èŠ‚ç‚¹
    *u*ï¼Œè¿™ä¸ªå»¶è¿Ÿç­‰äºå®ƒä»¬åœ¨å›¾ä¸­çš„è·ç¦»ã€‚åœ¨å›¾ Transformersï¼ˆå³ä¾§ï¼‰ä¸­ä½¿ç”¨çš„å›¾é‡è¿çš„æç«¯ä¾‹å­ä¸­ï¼Œæ‰€æœ‰èŠ‚ç‚¹éƒ½ç›¸äº’è¿æ¥ï¼Œä½¿å¾—èŠ‚ç‚¹ *u* çš„ä¿¡æ¯èƒ½ç«‹å³åœ¨
    *v* ä¸Šå¯ç”¨ï¼›ç„¶è€Œï¼Œè¿™ä¼šä»¥å¤±å»å›¾è·ç¦»æ‰€æä¾›çš„éƒ¨åˆ†é¡ºåºä¸ºä»£ä»·ï¼Œè¿™éœ€è¦é€šè¿‡ç‰¹å¾çš„ä½ç½®å’Œç»“æ„å¢å¼ºæ¥é‡æ–°å‘ç°ã€‚
- en: Dynamic graph rewiring
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ¨æ€å›¾é‡è¿
- en: Looking at our previous example of two nodes *u* and *v* at distance *r* > 1,
    in a classical MPNN, one has to wait for *r* layers before *u* and *v* can interact,
    and this interaction is never direct. We argue instead that once we reach layer
    *r*, the two nodes have now waited â€œlong enoughâ€ and can hence be allowed to interact
    directly (through an inserted extra edge, without going through intermediate neighbours).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥æˆ‘ä»¬ä¹‹å‰çš„ä¸¤ä¸ªè·ç¦»ä¸º *r* > 1 çš„èŠ‚ç‚¹ *u* å’Œ *v* çš„ä¾‹å­æ¥çœ‹ï¼Œåœ¨ç»å…¸ MPNN ä¸­ï¼Œå¿…é¡»ç­‰å¾… *r* å±‚æ‰èƒ½ä½¿ *u* å’Œ *v* è¿›è¡Œäº¤äº’ï¼Œå¹¶ä¸”è¿™ç§äº¤äº’ä»æœªæ˜¯ç›´æ¥çš„ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œä¸€æ—¦æˆ‘ä»¬è¾¾åˆ°ç¬¬
    *r* å±‚ï¼Œè¿™ä¸¤ä¸ªèŠ‚ç‚¹å·²ç»ç­‰å¾…äº†â€œè¶³å¤Ÿé•¿â€çš„æ—¶é—´ï¼Œå› æ­¤å¯ä»¥ç›´æ¥äº¤äº’ï¼ˆé€šè¿‡æ’å…¥çš„é¢å¤–è¾¹ï¼Œè€Œä¸é€šè¿‡ä¸­é—´çš„é‚»å±…ï¼‰ã€‚
- en: 'Accordingly, at the first layer we propagate messages only over the edges of
    the input graph (as in classical MPNNs), but at each subsequent layer the receptive
    field of node *u* expands by one hop [9]. This allows distant nodes to exchange
    information without intermediate steps while preserving the inductive bias afforded
    by the input graph topology: the graph is now densified gradually in deeper layers
    according to the distance.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåœ¨ç¬¬ä¸€å±‚ï¼Œæˆ‘ä»¬åªåœ¨è¾“å…¥å›¾çš„è¾¹ä¸Šä¼ æ’­æ¶ˆæ¯ï¼ˆå¦‚ç»å…¸ MPNNsï¼‰ï¼Œä½†åœ¨æ¯ä¸€å±‚ï¼ŒèŠ‚ç‚¹ *u* çš„æ„Ÿå—é‡æ‰©å±•ä¸€ä¸ªè·³è·ƒ [9]ã€‚è¿™å…è®¸è¿œè·ç¦»çš„èŠ‚ç‚¹åœ¨ä¸ç»è¿‡ä¸­é—´æ­¥éª¤çš„æƒ…å†µä¸‹äº¤æ¢ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™è¾“å…¥å›¾æ‹“æ‰‘æä¾›çš„å½’çº³åå·®ï¼šå›¾åœ¨æ›´æ·±å±‚æ¬¡ä¸­ä¼šæ ¹æ®è·ç¦»é€æ¸å˜å¯†ã€‚
- en: We call this mechanism *dynamic graph rewiring*, or *DRew* for short [10]. DRew-MPNNs
    can be seen as the â€œmiddle groundâ€ between classical MPNNs acting locally on the
    input graph and graph Transformers that consider all pairwise interactions at
    once.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç§°è¿™ç§æœºåˆ¶ä¸º*åŠ¨æ€å›¾é‡è¿*ï¼Œç®€ç§°ä¸º*DRew* [10]ã€‚DRew-MPNNs å¯ä»¥è¢«è§†ä¸ºåœ¨å±€éƒ¨ä½œç”¨äºè¾“å…¥å›¾çš„ç»å…¸ MPNNs å’ŒåŒæ—¶è€ƒè™‘æ‰€æœ‰æˆå¯¹äº¤äº’çš„å›¾
    Transformer ä¹‹é—´çš„â€œä¸­é—´åœ°å¸¦â€ã€‚
- en: Delayed message passing
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å»¶è¿Ÿæ¶ˆæ¯ä¼ é€’
- en: In classical MPNNs, two nodes *u* and *v* at distance *r* always interact with
    a constant delay of *r* layers, the minimum time it takes information to reach
    one node from the other. Thus, node *v* â€˜seesâ€™ the state of node *u* (mixed with
    other nodesâ€™ features) from *r* layers ago. In DRew-MPNNs instead, when two nodes
    interact, they do so instantaneously, through an inserted edge, using their *current
    state*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»å…¸ MPNNs ä¸­ï¼Œä¸¤ä¸ªè·ç¦»ä¸º *r* çš„èŠ‚ç‚¹ *u* å’Œ *v* æ€»æ˜¯ä»¥ *r* å±‚çš„å›ºå®šå»¶è¿Ÿè¿›è¡Œäº¤äº’ï¼Œè¿™æ˜¯ä¿¡æ¯ä»ä¸€ä¸ªèŠ‚ç‚¹ä¼ é€’åˆ°å¦ä¸€ä¸ªèŠ‚ç‚¹æ‰€éœ€çš„æœ€çŸ­æ—¶é—´ã€‚å› æ­¤ï¼ŒèŠ‚ç‚¹
    *v* ä» *r* å±‚ä¹‹å‰â€œçœ‹åˆ°â€èŠ‚ç‚¹ *u* çš„çŠ¶æ€ï¼ˆæ··åˆäº†å…¶ä»–èŠ‚ç‚¹çš„ç‰¹å¾ï¼‰ã€‚è€Œåœ¨ DRew-MPNNs ä¸­ï¼Œå½“ä¸¤ä¸ªèŠ‚ç‚¹è¿›è¡Œäº¤äº’æ—¶ï¼Œå®ƒä»¬æ˜¯é€šè¿‡æ’å…¥çš„è¾¹ç¬æ—¶è¿›è¡Œçš„ï¼Œä½¿ç”¨å®ƒä»¬çš„*å½“å‰çŠ¶æ€*ã€‚
- en: '*Delayed message passing* is a tradeoff between these two extreme cases: we
    add a global *delay* (a hyperparameter **ğ¼**) for messages sent between the nodes.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*å»¶è¿Ÿæ¶ˆæ¯ä¼ é€’* æ˜¯è¿™ä¸¤ç§æç«¯æƒ…å†µä¹‹é—´çš„æƒè¡¡ï¼šæˆ‘ä»¬ä¸ºèŠ‚ç‚¹ä¹‹é—´å‘é€çš„æ¶ˆæ¯æ·»åŠ äº†ä¸€ä¸ªå…¨å±€*å»¶è¿Ÿ*ï¼ˆä¸€ä¸ªè¶…å‚æ•° **ğ¼**ï¼‰ã€‚'
- en: 'For simplicity, we consider here two simple cases: either no delay (like in
    DRew), or the case of maximal delay, where two nodes *u* and *v* at distance *r*
    interact directly from layer *r* onwards, but with a constant delay of *r* (as
    in classical MPNNs): at layer *r*, node *u* can exchange information with the
    state of node *v* as it was *r* layers before [11].'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œè€ƒè™‘ä¸¤ç§ç®€å•çš„æƒ…å†µï¼šæ²¡æœ‰å»¶è¿Ÿï¼ˆå¦‚åœ¨ DRew ä¸­ï¼‰ï¼Œæˆ–æœ€å¤§å»¶è¿Ÿçš„æƒ…å†µï¼Œå…¶ä¸­ä¸¤ä¸ªè·ç¦» *r* çš„èŠ‚ç‚¹ *u* å’Œ *v* ä»å±‚ *r*
    å¼€å§‹ç›´æ¥äº’åŠ¨ï¼Œä½†å…·æœ‰ *r* çš„å¸¸é‡å»¶è¿Ÿï¼ˆå¦‚åœ¨ç»å…¸çš„ MPNNs ä¸­ï¼‰ï¼šåœ¨å±‚ *r*ï¼ŒèŠ‚ç‚¹ *u* å¯ä»¥ä¸èŠ‚ç‚¹ *v* åœ¨ *r* å±‚ä¹‹å‰çš„çŠ¶æ€äº¤æ¢ä¿¡æ¯ [11]ã€‚
- en: The delay controls *how fast* information flows over the graph. No delay means
    that messages travel faster, with distant nodes interacting instantly once an
    edge is added; conversely, the more delay, the slower the information flow, with
    distant nodes accessing past states when an edge is added.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å»¶è¿Ÿæ§åˆ¶ä¿¡æ¯åœ¨å›¾ä¸Šçš„*æµåŠ¨é€Ÿåº¦*ã€‚æ²¡æœ‰å»¶è¿Ÿæ„å‘³ç€æ¶ˆæ¯ä¼ é€’æ›´å¿«ï¼Œä¸€æ—¦æ·»åŠ è¾¹ç¼˜ï¼Œè¿œç¦»çš„èŠ‚ç‚¹ä¼šç«‹å³äº¤äº’ï¼›ç›¸åï¼Œå»¶è¿Ÿè¶Šå¤šï¼Œä¿¡æ¯æµåŠ¨è¶Šæ…¢ï¼Œè¿œç¦»çš„èŠ‚ç‚¹åœ¨æ·»åŠ è¾¹ç¼˜æ—¶ä¼šè®¿é—®è¿‡å»çš„çŠ¶æ€ã€‚
- en: '![](../Images/59f7173d373f601f48b14e089d1be7ee.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59f7173d373f601f48b14e089d1be7ee.png)'
- en: A comparison of DRew and its delayed variant ğ¼DRew. On the left, nodes at distance
    *r* exchange information through an additional edge from layer *r* onwards, instantaneously.
    On the right, we show the case of maximal delay (in our paper corresponding to
    the case **ğ¼** = 1), where the delay between two nodes coincides with their distance;
    the newly added edge between nodes at distance (layer) *r* looks â€œin the pastâ€
    to access the state of a node as it was *r* layers ago.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: DRewåŠå…¶å»¶è¿Ÿå˜ä½“ğ¼DRewçš„æ¯”è¾ƒã€‚å·¦ä¾§ï¼Œè·ç¦» *r* çš„èŠ‚ç‚¹é€šè¿‡ä»å±‚ *r* å¼€å§‹çš„é¢å¤–è¾¹ç¼˜å³æ—¶äº¤æ¢ä¿¡æ¯ã€‚å³ä¾§æ˜¾ç¤ºçš„æ˜¯æœ€å¤§å»¶è¿Ÿçš„æƒ…å†µï¼ˆåœ¨æˆ‘ä»¬çš„è®ºæ–‡ä¸­å¯¹åº”**ğ¼**
    = 1ï¼‰ï¼Œå…¶ä¸­ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„å»¶è¿Ÿä¸å®ƒä»¬çš„è·ç¦»ä¸€è‡´ï¼›åœ¨è·ç¦»ï¼ˆå±‚ï¼‰*r* çš„èŠ‚ç‚¹ä¹‹é—´æ–°å¢çš„è¾¹ç¼˜â€œçœ‹èµ·æ¥åƒæ˜¯â€è¿‡å»çš„ï¼Œä»¥è®¿é—® *r* å±‚å‰èŠ‚ç‚¹çš„çŠ¶æ€ã€‚
- en: The ğ¼DRew framework
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğ¼DRew æ¡†æ¶
- en: We call an architecture combining dynamic rewiring with delayed message passing
    ğ¼DRew (pronounced â€œAndrewâ€).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç§°ç»“åˆåŠ¨æ€é‡è¿å’Œå»¶è¿Ÿæ¶ˆæ¯ä¼ é€’çš„æ¶æ„ä¸ºğ¼DRewï¼ˆå‘éŸ³ä¸ºâ€œAndrewâ€ï¼‰ã€‚
- en: One way to view ğ¼DRew is as an architecture with *sparse skip-connections*,
    allowing messages to travel not only â€œhorizontallyâ€ (between nodes of the graph
    within the same layer, as in classical MPNN) but also â€œverticallyâ€ (across different
    layers). The idea of relying on vertical edges in GNNs is not new, and in fact
    one can think of residual connections as vertical links connecting each node to
    the same node at the previous layer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è§‚å¯Ÿ ğ¼DRew çš„ä¸€ç§æ–¹å¼æ˜¯å°†å…¶è§†ä¸ºä¸€ç§å…·æœ‰*ç¨€ç–è·³è·ƒè¿æ¥*çš„æ¶æ„ï¼Œå…è®¸æ¶ˆæ¯ä¸ä»…â€œæ¨ªå‘â€ä¼ é€’ï¼ˆåœ¨å›¾çš„åŒä¸€å±‚çº§çš„èŠ‚ç‚¹ä¹‹é—´ï¼Œå¦‚ç»å…¸ MPNN ä¸­ï¼‰ï¼Œè¿˜â€œçºµå‘â€ä¼ é€’ï¼ˆè·¨ä¸åŒå±‚çº§ï¼‰ã€‚åœ¨
    GNN ä¸­ä¾èµ–å‚ç›´è¾¹ç¼˜çš„æƒ³æ³•å¹¶ä¸æ–°é²œï¼Œå®é™…ä¸Šå¯ä»¥å°†æ®‹å·®è¿æ¥è§†ä¸ºå°†æ¯ä¸ªèŠ‚ç‚¹è¿æ¥åˆ°å‰ä¸€å±‚ç›¸åŒèŠ‚ç‚¹çš„å‚ç›´é“¾æ¥ã€‚
- en: The delay mechanism extends this approach by creating vertical edges that connect
    a node *u* and a *different* node *v* at some previous layer depending on the
    graph distance between *u* and *v*. This way, we can leverage benefits intrinsic
    to skip-connections for deep neural networks while conditioning them on the extra
    geometric information we have at our disposal in the form of graph distance.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å»¶è¿Ÿæœºåˆ¶é€šè¿‡åˆ›å»ºå‚ç›´è¾¹ç¼˜æ¥æ‰©å±•è¿™ç§æ–¹æ³•ï¼Œè¿™äº›è¾¹ç¼˜è¿æ¥ä¸€ä¸ªèŠ‚ç‚¹ *u* å’Œä¸€ä¸ªåœ¨æŸä¸ªå…ˆå‰å±‚çš„ *ä¸åŒ* èŠ‚ç‚¹ *v*ï¼Œå…·ä½“å–å†³äº *u* å’Œ *v* ä¹‹é—´çš„å›¾è·ç¦»ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œçš„è·³è·ƒè¿æ¥çš„å›ºæœ‰ä¼˜åŠ¿ï¼ŒåŒæ—¶åŸºäºæˆ‘ä»¬æ‰‹å¤´çš„é¢å¤–å‡ ä½•ä¿¡æ¯ï¼ˆä»¥å›¾è·ç¦»çš„å½¢å¼ï¼‰æ¥è¿›è¡Œæ¡ä»¶åŒ–ã€‚
- en: ğ¼DRew alleviates over-squashing since distant nodes now have access to multiple
    (shorter) pathways to exchange information, bypassing the â€œinformation dilutionâ€
    of repeated local message passing. Differently from static rewiring, ğ¼DRew achieves
    this effect by slowing down the densification of the graph and making it layer-dependent,
    hence reducing the memory footprint.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ğ¼DRew ç¼“è§£äº†è¿‡åº¦å‹ç¼©ï¼Œå› ä¸ºè¿œç¦»çš„èŠ‚ç‚¹ç°åœ¨å¯ä»¥é€šè¿‡å¤šä¸ªï¼ˆæ›´çŸ­çš„ï¼‰è·¯å¾„äº¤æ¢ä¿¡æ¯ï¼Œç»•è¿‡äº†é‡å¤æœ¬åœ°æ¶ˆæ¯ä¼ é€’çš„â€œä¿¡æ¯ç¨€é‡Šâ€é—®é¢˜ã€‚ä¸é™æ€é‡è¿ä¸åŒï¼Œğ¼DRew
    é€šè¿‡å‡ç¼“å›¾çš„ç¨ å¯†åŒ–å¹¶ä½¿å…¶ä¾èµ–äºå±‚çº§æ¥å®ç°è¿™ä¸€æ•ˆæœï¼Œä»è€Œå‡å°‘äº†å†…å­˜å ç”¨ã€‚
- en: ğ¼DRew is suitable to explore the graph at different speeds, deal with long-range
    interactions, and generally enhance the power of very deep GNNs. Since ğ¼DRew determines
    *where* and *when* messages are being exchanged, but not *how*, it can be seen
    as a meta-architecture that can augment existing MPNNs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ğ¼DRew é€‚ç”¨äºä»¥ä¸åŒé€Ÿåº¦æ¢ç´¢å›¾å½¢ï¼Œå¤„ç†é•¿ç¨‹äº¤äº’ï¼Œå¹¶ä¸€èˆ¬æ€§åœ°å¢å¼ºéå¸¸æ·±çš„ GNN çš„èƒ½åŠ›ã€‚ç”±äº ğ¼DRew ç¡®å®šäº†æ¶ˆæ¯äº¤æ¢çš„*ä½•å¤„*å’Œ*ä½•æ—¶*ï¼Œä½†ä¸*å¦‚ä½•*ï¼Œå› æ­¤å¯ä»¥è§†ä¸ºä¸€ç§å…ƒæ¶æ„ï¼Œå¯ä»¥å¢å¼ºç°æœ‰çš„
    MPNNsã€‚
- en: Experimental results
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®éªŒç»“æœ
- en: In our paper [10], we provide an extensive comparison of ğ¼DRew with classical
    MPNNs baselines, static rewiring, and Transformer-type architectures, using a
    fixed parameter budget. On the recent long-range benchmark (LRGB) introduced by
    Vijay Dwivedi and co-authors [11], ğ¼DRew outperforms in most cases all of the
    above.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6dab81c55ebc175e0c273e373a52e3aa.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Comparison of various classical MPNNs (GCN, GINE, etc.), static graph rewiring
    (MixHop-GCN, DIGL), and graph Transformer-type architectures (Transformer, SAN,
    GraphGPS, including positional Laplacian encoding) with ğ¼DRew-MPNN variants on
    four Long-Range Graph Benchmark (LRGB) tasks. Green, orange, and purple represent
    first-, second-, and third-best models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'An ablation study of ğ¼DRew on one of the LRGB tasks reveals another crucial
    contribution of our framework: the ability to tune ğ¼ to suit the task. We observe
    that the more delay used (lower value of ğ¼), the better the performance for large
    number of layers *L*, whereas using less delay (high ğ¼) ensures faster filling
    of the computational graph and greater density of connections after fewer layers.
    Consequently, in shallow architectures (small *L*), removing delay altogether
    (ğ¼=âˆ) performs better. Conversely, in deep architectures (large *L*), more delay
    (small ğ¼) â€œslows downâ€ the densification of the message passing graph, leading
    to better performance.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9dcf49a8950930132e945d2eb173f808.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Performance of ğ¼DRew-MPNNs with different number of layers *L* and different
    delay parameter ğ¼. While dynamic rewiring helps for long-range tasks in all regimes,
    delay significantly improves the performance over deeper models. Our framework
    can also be controlled for compute/memory budget depending on the application,
    e.g. in situations where Transformers are computationally intractable.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Traditional MPNN-type architectures differ in *how* messages are exchanged [12].
    Graph rewiring techniques add an extra level of control of *where* they are sent
    on the graph. Our new approach of dynamic graph rewiring with delayed message
    passing allows to further control *when* messages are exchanged.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: This approach appears to be very powerful, and our work is only a first attempt
    at leveraging the idea of accessing past states in a graph neural network depending
    on â€œgeometric propertiesâ€ of the underlying data. We hope that this new paradigm
    can lead to more theoretically-principled frameworks and challenge the idea that
    MPNNs are unable to solve long-range tasks unless augmented with quadratic attention
    layers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[1] This is typical e.g. in â€œsmall-worldâ€ graphs such as social networks.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[2] U. Alon and E. Yahav, On the bottleneck of graph neural networks and its
    practical implications (2021), *ICLR*.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[3] J. Topping *et al.*, Understanding over-squashing and bottlenecks on graphs
    via curvature (2022), *ICLR.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Commute time is the expected time it takes a random walk to go from node
    *v* to node *u* and back. See F. Di Giovanni *et al.*, [On over-squashing in message
    passing neural networks: The impact of width, depth, and topology](https://arxiv.org/pdf/2302.02941.pdf)
    (2023), *ICML*.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] é€šå‹¤æ—¶é—´æ˜¯éšæœºæ¸¸èµ°ä»èŠ‚ç‚¹ *v* åˆ°èŠ‚ç‚¹ *u* å†è¿”å›æ‰€éœ€çš„é¢„æœŸæ—¶é—´ã€‚è§ F. Di Giovanni *ç­‰*ï¼Œ[å…³äºæ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œä¸­è¿‡åº¦å‹ç¼©çš„å½±å“ï¼šå®½åº¦ã€æ·±åº¦å’Œæ‹“æ‰‘çš„å½±å“](https://arxiv.org/pdf/2302.02941.pdf)ï¼ˆ2023ï¼‰ï¼Œ*ICML*ã€‚'
- en: '[5] See Theorem 4.3 in F. Di Giovanni *et al.* [How does over-squashing affect
    the power of GNNs?](https://arxiv.org/pdf/2306.03589.pdf) (2023) *arXiv*:2306.03589.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] è§ F. Di Giovanni *ç­‰* çš„å®šç† 4.3ï¼Œ[è¿‡åº¦å‹ç¼©å¦‚ä½•å½±å“ GNN çš„èƒ½åŠ›ï¼Ÿ](https://arxiv.org/pdf/2306.03589.pdf)ï¼ˆ2023ï¼‰*arXiv*:2306.03589ã€‚'
- en: '[6] Graph rewiring is somewhat a controversial technique in the GNN community
    as some believe the input graph is sacrosanct and must not be touched. *De facto*,
    most modern GNNs do employ some form of graph rewiring, whether explicitly (as
    a pre-processing step) or implicitly (e.g., by neighbour sampling or using virtual
    nodes).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] å›¾é‡è¿åœ¨ GNN ç¤¾åŒºä¸­æœ‰äº›äº‰è®®ï¼Œå› ä¸ºä¸€äº›äººè®¤ä¸ºè¾“å…¥å›¾æ˜¯ç¥åœ£ä¸å¯ä¾µçŠ¯çš„ï¼Œä¸åº”è¢«è§¦åŠã€‚*å®é™…ä¸Š*ï¼Œå¤§å¤šæ•°ç°ä»£ GNN éƒ½é‡‡ç”¨æŸç§å½¢å¼çš„å›¾é‡è¿ï¼Œæ— è®ºæ˜¯æ˜¾å¼çš„ï¼ˆä½œä¸ºé¢„å¤„ç†æ­¥éª¤ï¼‰è¿˜æ˜¯éšå¼çš„ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡é‚»å±…é‡‡æ ·æˆ–ä½¿ç”¨è™šæ‹ŸèŠ‚ç‚¹ï¼‰ã€‚'
- en: '[7] R. Abboud, R. Dimitrov, and I. Ceylan, [Shortest path networks for graph
    property prediction](https://arxiv.org/pdf/2206.01003.pdf) (2022),'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] R. Abboud, R. Dimitrov å’Œ I. Ceylanï¼Œ[å›¾å±æ€§é¢„æµ‹çš„æœ€çŸ­è·¯å¾„ç½‘ç»œ](https://arxiv.org/pdf/2206.01003.pdf)ï¼ˆ2022ï¼‰ï¼Œ'
- en: '*arXiv*:2206.01003.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*arXiv*:2206.01003ã€‚'
- en: '[8] See e.g. V. P. Dwivedi and X. Bresson, [A generalization of Transformer
    networks to graphs](https://arxiv.org/pdf/2012.09699v2.pdf) (2021), *arXiv*:2012.09699
    and C. Ying et al., [Do Transformers Really Perform Badly for Graph Representation?](https://www.microsoft.com/en-us/research/publication/do-transformers-really-perform-badly-for-graph-representation/)
    (2021), NeurIPS.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] è§ä¾‹å¦‚ V. P. Dwivedi å’Œ X. Bressonï¼Œ [Transformer ç½‘ç»œå¯¹å›¾çš„æ¨å¹¿](https://arxiv.org/pdf/2012.09699v2.pdf)ï¼ˆ2021ï¼‰ï¼Œ*arXiv*:2012.09699
    å’Œ C. Ying ç­‰ï¼Œ[Transformer å¯¹å›¾è¡¨ç¤ºçš„æ€§èƒ½çœŸçš„å¾ˆå·®å—ï¼Ÿ](https://www.microsoft.com/en-us/research/publication/do-transformers-really-perform-badly-for-graph-representation/)ï¼ˆ2021ï¼‰ï¼ŒNeurIPSã€‚'
- en: '[9] Dynamic rewiring results in the following message passing formula:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] åŠ¨æ€é‡è¿ç»“æœä¸ºä»¥ä¸‹æ¶ˆæ¯ä¼ é€’å…¬å¼ï¼š'
- en: '**m***áµ¤*â½*Ë¡ áµ* â¾=AGG({**x***áµ¥*â½*Ë¡* â¾ : *vâˆˆğ’©â‚–*(*u*)}) with 1 â‰¤ *k â‰¤ l*+1'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**m***áµ¤*â½*Ë¡ áµ* â¾=AGG({**x***áµ¥*â½*Ë¡* â¾ : *vâˆˆğ’©â‚–*(*u*)})ï¼Œå…¶ä¸­ 1 â‰¤ *k â‰¤ l*+1'
- en: '**x***áµ¤*â½*Ë¡* âºÂ¹â¾=UP(**x***áµ¤*â½*Ë¡* â¾, **m***áµ¤*â½*Ë¡* Â¹â¾,â€¦, **m***áµ¤*â½*Ë¡ Ë¡* âºÂ¹â¾)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**x***áµ¤*â½*Ë¡* âºÂ¹â¾=UP(**x***áµ¤*â½*Ë¡* â¾, **m***áµ¤*â½*Ë¡* Â¹â¾,â€¦, **m***áµ¤*â½*Ë¡ Ë¡* âºÂ¹â¾)'
- en: where AGG is a permutation-invariant aggregation operator, *ğ’©â‚–*(*u*) is the
    *k*-hop neighbourhood of node *u*, and UP is an update operation receiving messages
    from each *k*-hop separately. See equation 5 in [10].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼ŒAGG æ˜¯ä¸€ä¸ªç½®æ¢ä¸å˜çš„èšåˆæ“ä½œç¬¦ï¼Œ*ğ’©â‚–*(*u*) æ˜¯èŠ‚ç‚¹ *u* çš„ *k*-è·³é‚»åŸŸï¼Œè€Œ UP æ˜¯ä¸€ä¸ªæ¥æ”¶æ¥è‡ªæ¯ä¸ª *k*-è·³çš„æ¶ˆæ¯çš„æ›´æ–°æ“ä½œã€‚è§[10]ä¸­çš„å…¬å¼5ã€‚
- en: '[10] B. Gutteridge et al., DRew: [Dynamically rewired message passing with
    delay](https://arxiv.org/pdf/2305.08018.pdf) (2023), *ICML*.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] B. Gutteridge ç­‰ï¼ŒDRew: [åŠ¨æ€é‡è¿æ¶ˆæ¯ä¼ é€’ä¸å»¶è¿Ÿ](https://arxiv.org/pdf/2305.08018.pdf)ï¼ˆ2023ï¼‰ï¼Œ*ICML*ã€‚'
- en: '[11] Delayed message passing takes the form'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] å»¶è¿Ÿæ¶ˆæ¯ä¼ é€’å‘ˆç°ä»¥ä¸‹å½¢å¼'
- en: '**m***áµ¤*â½*Ë¡ áµ* â¾=AGG({**x***áµ¥*â½*Ë¡* á¨*Ë¢* â½*áµ* â¾â¾ : *vâˆˆğ’©â‚–*(*u*)}) with 1 â‰¤ *k
    â‰¤ l*+1'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**m***áµ¤*â½*Ë¡ áµ* â¾=AGG({**x***áµ¥*â½*Ë¡* á¨*Ë¢* â½*áµ* â¾â¾ : *vâˆˆğ’©â‚–*(*u*)})ï¼Œå…¶ä¸­ 1 â‰¤ *k â‰¤
    l*+1'
- en: '**x***áµ¤*â½*Ë¡* âºÂ¹â¾=UP(**x***áµ¤*â½*Ë¡* â¾, **m***áµ¤*â½*Ë¡* Â¹â¾,â€¦, **m***áµ¤*â½*Ë¡ Ë¡* âºÂ¹â¾)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**x***áµ¤*â½*Ë¡* âºÂ¹â¾=UP(**x***áµ¤*â½*Ë¡* â¾, **m***áµ¤*â½*Ë¡* Â¹â¾,â€¦, **m***áµ¤*â½*Ë¡ Ë¡* âºÂ¹â¾)'
- en: where *s*(*k*)=max{0,*k*ï¹£ğ¼}, see equation 6 in [10]. The choice ğ¼=âˆ corresponds
    to no delay (like in DRew) and ğ¼ = 1 corresponds to classical MPNN (two nodes
    *u* and *v* at distance *r* interact directly from layer *r* onwards, but with
    a constant delay of *r*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *s*(*k*)=max{0,*k*ï¹£ğ¼}ï¼Œè§[10]ä¸­çš„å…¬å¼6ã€‚é€‰æ‹© ğ¼=âˆ è¡¨ç¤ºæ²¡æœ‰å»¶è¿Ÿï¼ˆå¦‚åœ¨ DRew ä¸­ï¼‰ï¼Œè€Œ ğ¼ = 1 è¡¨ç¤ºç»å…¸çš„ MPNNï¼ˆä¸¤ä¸ªèŠ‚ç‚¹
    *u* å’Œ *v* åœ¨è·ç¦» *r* çš„ä½ç½®ä¸Šä»ç¬¬ *r* å±‚å¼€å§‹ç›´æ¥äº¤äº’ï¼Œä½†æœ‰ä¸€ä¸ªæ’å®šçš„ *r* å»¶è¿Ÿï¼‰ã€‚
- en: '[11] V. P. Dwivedi *et al.*, [Long range graph benchmark](https://arxiv.org/abs/2206.08164)
    (2022), *arXiv*:2206.08164.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] V. P. Dwivedi *ç­‰*ï¼Œ[é•¿è·ç¦»å›¾åŸºå‡†](https://arxiv.org/abs/2206.08164)ï¼ˆ2022ï¼‰ï¼Œ*arXiv*:2206.08164ã€‚'
- en: '[12] In our proto-book M. M. Bronstein *et al.,* [Geometric Deep Learning:
    Grids, Groups, Graphs, Geodesics, and Gauges](https://arxiv.org/abs/2104.13478)
    (2021), we distinguish between three â€œflavoursâ€ of MPNNs: convolutional, attentional,
    and generic message passing.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] åœ¨æˆ‘ä»¬çš„åŸå‹ä¹¦ä¸­ M. M. Bronstein *ç­‰*ï¼Œ[å‡ ä½•æ·±åº¦å­¦ä¹ ï¼šç½‘æ ¼ã€ç¾¤ä½“ã€å›¾ã€æµ‹åœ°çº¿å’Œé‡è§„](https://arxiv.org/abs/2104.13478)ï¼ˆ2021ï¼‰ï¼Œæˆ‘ä»¬åŒºåˆ†äº†ä¸‰ç§â€œé£æ ¼â€çš„
    MPNNï¼šå·ç§¯å‹ã€æ³¨æ„åŠ›å‹å’Œé€šç”¨æ¶ˆæ¯ä¼ é€’ã€‚'
- en: '*We are grateful to* [*Federico Barbero*](https://twitter.com/fedzbar)*,* [*Fabrizio
    Frasca*](https://twitter.com/ffabffrasca)*, and* [*Emanuele Rossi*](https://twitter.com/emaros96)
    *for proofreading this post and providing insightful comments. For additional
    articles about deep learning on graphs, see Michaelâ€™s* [*other posts*](https://towardsdatascience.com/graph-deep-learning/home)
    *in Towards Data Science,* [*subscribe*](https://michael-bronstein.medium.com/subscribe)
    *to his posts and* [*YouTube channel*](https://www.youtube.com/c/MichaelBronsteinGDL)*,
    get* [*Medium membership*](https://michael-bronstein.medium.com/membership)*,
    or follow him on* [*Twitter*](https://twitter.com/mmbronstein)*.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬æ„Ÿè°¢* [*Federico Barbero*](https://twitter.com/fedzbar)*,* [*Fabrizio Frasca*](https://twitter.com/ffabffrasca)*
    å’Œ* [*Emanuele Rossi*](https://twitter.com/emaros96) *ä¸ºæœ¬æ–‡è¿›è¡Œæ ¡å¯¹å¹¶æä¾›äº†æœ‰è§åœ°çš„è¯„è®ºã€‚æœ‰å…³å›¾å½¢æ·±åº¦å­¦ä¹ çš„æ›´å¤šæ–‡ç« ï¼Œè¯·å‚è§
    Michael çš„* [*å…¶ä»–æ–‡ç« *](https://towardsdatascience.com/graph-deep-learning/home) *åœ¨
    Towards Data Science ä¸Šï¼Œ* [*è®¢é˜…*](https://michael-bronstein.medium.com/subscribe)
    *ä»–çš„æ–‡ç« å’Œ* [*YouTube é¢‘é“*](https://www.youtube.com/c/MichaelBronsteinGDL)*ï¼Œè·å–* [*Medium
    ä¼šå‘˜èµ„æ ¼*](https://michael-bronstein.medium.com/membership)*ï¼Œæˆ–è€…åœ¨* [*Twitter*](https://twitter.com/mmbronstein)*
    ä¸Šå…³æ³¨ä»–ã€‚*'
