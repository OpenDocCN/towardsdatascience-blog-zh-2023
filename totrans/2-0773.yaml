- en: Dynamically Rewired Delayed Message Passing GNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/dynamically-rewired-delayed-message-passing-gnns-2d5ff18687c2](https://towardsdatascience.com/dynamically-rewired-delayed-message-passing-gnns-2d5ff18687c2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Delayed Message Passing
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Message-passing graph neural networks (MPNNs) tend to suffer from the phenomenon
    of *over-squashing,* causing performance deterioration for tasks relying on long-range
    interactions. This can be largely attributed to message passing only occurring
    *locally*, over a node’s immediate neighbours. Traditional static graph rewiring
    techniques typically attempt to counter this effect by allowing distant nodes
    to communicate instantly (and in the extreme case of Transformers, by making all
    nodes accessible at every layer). However, this incurs a computational price and
    comes at the expense of breaking the inductive bias provided by the input graph
    structure. In this post, we describe two novel mechanisms to overcome over-squashing
    while offsetting the side effects of static rewiring approaches: *dynamic rewiring*
    and *delayed* message passing. These techniques can be incorporated into any MPNN
    and lead to better performance than graph Transformers on long-range tasks.'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://michael-bronstein.medium.com/?source=post_page-----2d5ff18687c2--------------------------------)[![Michael
    Bronstein](../Images/1aa876fce70bb07bef159fecb74e85bf.png)](https://michael-bronstein.medium.com/?source=post_page-----2d5ff18687c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2d5ff18687c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2d5ff18687c2--------------------------------)
    [Michael Bronstein](https://michael-bronstein.medium.com/?source=post_page-----2d5ff18687c2--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2d5ff18687c2--------------------------------)
    ·9 min read·Jun 19, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8144f73a5808c0a411b10b828b6ec09.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: 'Image: based on Shutterstock.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '*This post was co-authored with Francesco Di Giovanni and Ben Gutteridge and
    is based on the paper by B. Gutteridge et al., DRew:* [*Dynamically rewired message
    passing with delay*](https://arxiv.org/pdf/2305.08018.pdf) *(2023), ICML.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Classical message-passing graph neural networks (MPNNs) operate by aggregating
    information from 1-hop neighbours of every node. Consequently, learning tasks
    requiring *long-range interactions* (i.e., there exists a node *v* whose representation
    needs to account for the information contained in some node *u* at shortest-walk
    (geodesic) distance *d*(*u*,*v*) = *r* > 1) require deep MPNNs with multiple message-passing
    layers. If the graph structure is such that the receptive field expands exponentially
    fast with the hop distance [1], one may need to “squeeze” too many messages into
    a fixed node feature vector — a phenomenon known as *over-squashing* [2].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69c830c6c70d0657852dd68b8b8e2d8d.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: '**Over-squashing**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous works [3–4], we formalised over-squashing as the *lack of sensitivity*
    of the MPNN output at a node *u* to the input at an *r*-distant node. This can
    be quantified by a bound on the partial derivative (*Jacobian*) of the form
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '|∂**x***ᵤ*⁽*ʳ*⁾/∂**x***ᵥ*⁽⁰⁾| < *c* (**A***ʳ*)*ᵤᵥ.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Here *c* is a constant dependent on the MPNN architecture (e.g., Lipschitz regularity
    of the activation function, depth, etc.) and **A** is the normalised adjacency
    matrix of the graph. Over-squashing occurs when the entries of **A***ʳ* decay
    exponentially fast with distance *r*. In fact, it is now known that over-squashing
    is more generally a phenomenon that can be related to local structure of the graph
    (such as negative curvature [3]), or its global structure beyond the shortest-walk
    distance (e.g., commute time or effective resistance [4, 5]).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The powers **A***ʳ* in the above expression reflect the fact that the communication
    between *nodes u and v at distance r* in an MPNN is a sequence of interactions
    between adjacent nodes comprising different paths that connect *u* and *v*. As
    a result, the nodes *u* and *v* exchange information only from *r*th layer onwards,
    and with *latency* equal to their distance *r*. Over-squashing is caused by this
    information being “diluted” through repeated message passing over intermediate
    nodes along these paths.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Graph rewiring**'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The issue of over-squashing can be addressed by partially decoupling the input
    graph structure from the one used as support for computing messages, a procedure
    known as *graph rewiring* [6]. Typically, rewiring is performed as a pre-processing
    step in which the input graph *G* is replaced with some other graph *G’* that
    is “friendlier” for message-passing, according to some spatial or spectral connectivity
    measure.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to achieve this amounts to connecting all nodes within a certain
    distance, thus allowing them to exchange information *directly*. This is the idea
    behind the multi-hop message passing scheme [7]. Graph Transformers [8] take this
    to the extreme, connecting *all* pairs of nodes through an attention-weighted
    edge.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: This way, the information is no longer “mixed” with that of other nodes along
    the way and over-squashing can be avoided. However, such a rewiring makes the
    graph much denser from the first layer, increasing the computational footprint
    and partly compromising the inductive bias afforded by the input graph, since
    both local and global nodes interact identically and *instantaneously* at each
    layer.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，信息不再与沿途其他节点的信息“混合”，可以避免过度压缩。然而，这种重连使图在第一层时变得更加稠密，增加了计算负担，并且部分妥协了输入图所提供的归纳偏差，因为在每一层中，本地和全局节点的交互都是*瞬时的*。
- en: '![](../Images/d2061ebebf4f75b3a8a5695c95ba70d4.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2061ebebf4f75b3a8a5695c95ba70d4.png)'
- en: In a classical MPNN (left), information from node *u* arrives at node *v* (which
    is 3-hops away) after 3 message passing steps along the input graph. Accordingly,
    node v always “sees” node u with a constant lag (delay) equal to their distance
    on the graph. In the extreme example of graph rewiring used in graph Transformers
    (right), all the nodes are connected, making the information of node *u* available
    at *v* immediately; however, this comes at the expense of losing the partial ordering
    afforded by the graph distance, which needs to be rediscovered through positional
    and structural augmentation of the features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典 MPNN（左侧）中，来自节点 *u* 的信息在经过 3 次消息传递步骤后到达距离 3 步的节点 *v*。因此，节点 *v* 总是以固定的延迟（滞后）“看到”节点
    *u*，这个延迟等于它们在图中的距离。在图 Transformers（右侧）中使用的图重连的极端例子中，所有节点都相互连接，使得节点 *u* 的信息能立即在
    *v* 上可用；然而，这会以失去图距离所提供的部分顺序为代价，这需要通过特征的位置和结构增强来重新发现。
- en: Dynamic graph rewiring
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态图重连
- en: Looking at our previous example of two nodes *u* and *v* at distance *r* > 1,
    in a classical MPNN, one has to wait for *r* layers before *u* and *v* can interact,
    and this interaction is never direct. We argue instead that once we reach layer
    *r*, the two nodes have now waited “long enough” and can hence be allowed to interact
    directly (through an inserted extra edge, without going through intermediate neighbours).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们之前的两个距离为 *r* > 1 的节点 *u* 和 *v* 的例子来看，在经典 MPNN 中，必须等待 *r* 层才能使 *u* 和 *v* 进行交互，并且这种交互从未是直接的。我们认为，一旦我们达到第
    *r* 层，这两个节点已经等待了“足够长”的时间，因此可以直接交互（通过插入的额外边，而不通过中间的邻居）。
- en: 'Accordingly, at the first layer we propagate messages only over the edges of
    the input graph (as in classical MPNNs), but at each subsequent layer the receptive
    field of node *u* expands by one hop [9]. This allows distant nodes to exchange
    information without intermediate steps while preserving the inductive bias afforded
    by the input graph topology: the graph is now densified gradually in deeper layers
    according to the distance.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在第一层，我们只在输入图的边上传播消息（如经典 MPNNs），但在每一层，节点 *u* 的感受野扩展一个跳跃 [9]。这允许远距离的节点在不经过中间步骤的情况下交换信息，同时保留输入图拓扑提供的归纳偏差：图在更深层次中会根据距离逐渐变密。
- en: We call this mechanism *dynamic graph rewiring*, or *DRew* for short [10]. DRew-MPNNs
    can be seen as the “middle ground” between classical MPNNs acting locally on the
    input graph and graph Transformers that consider all pairwise interactions at
    once.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这种机制为*动态图重连*，简称为*DRew* [10]。DRew-MPNNs 可以被视为在局部作用于输入图的经典 MPNNs 和同时考虑所有成对交互的图
    Transformer 之间的“中间地带”。
- en: Delayed message passing
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 延迟消息传递
- en: In classical MPNNs, two nodes *u* and *v* at distance *r* always interact with
    a constant delay of *r* layers, the minimum time it takes information to reach
    one node from the other. Thus, node *v* ‘sees’ the state of node *u* (mixed with
    other nodes’ features) from *r* layers ago. In DRew-MPNNs instead, when two nodes
    interact, they do so instantaneously, through an inserted edge, using their *current
    state*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典 MPNNs 中，两个距离为 *r* 的节点 *u* 和 *v* 总是以 *r* 层的固定延迟进行交互，这是信息从一个节点传递到另一个节点所需的最短时间。因此，节点
    *v* 从 *r* 层之前“看到”节点 *u* 的状态（混合了其他节点的特征）。而在 DRew-MPNNs 中，当两个节点进行交互时，它们是通过插入的边瞬时进行的，使用它们的*当前状态*。
- en: '*Delayed message passing* is a tradeoff between these two extreme cases: we
    add a global *delay* (a hyperparameter **𝝼**) for messages sent between the nodes.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*延迟消息传递* 是这两种极端情况之间的权衡：我们为节点之间发送的消息添加了一个全局*延迟*（一个超参数 **𝝼**）。'
- en: 'For simplicity, we consider here two simple cases: either no delay (like in
    DRew), or the case of maximal delay, where two nodes *u* and *v* at distance *r*
    interact directly from layer *r* onwards, but with a constant delay of *r* (as
    in classical MPNNs): at layer *r*, node *u* can exchange information with the
    state of node *v* as it was *r* layers before [11].'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们在这里考虑两种简单的情况：没有延迟（如在 DRew 中），或最大延迟的情况，其中两个距离 *r* 的节点 *u* 和 *v* 从层 *r*
    开始直接互动，但具有 *r* 的常量延迟（如在经典的 MPNNs 中）：在层 *r*，节点 *u* 可以与节点 *v* 在 *r* 层之前的状态交换信息 [11]。
- en: The delay controls *how fast* information flows over the graph. No delay means
    that messages travel faster, with distant nodes interacting instantly once an
    edge is added; conversely, the more delay, the slower the information flow, with
    distant nodes accessing past states when an edge is added.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟控制信息在图上的*流动速度*。没有延迟意味着消息传递更快，一旦添加边缘，远离的节点会立即交互；相反，延迟越多，信息流动越慢，远离的节点在添加边缘时会访问过去的状态。
- en: '![](../Images/59f7173d373f601f48b14e089d1be7ee.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59f7173d373f601f48b14e089d1be7ee.png)'
- en: A comparison of DRew and its delayed variant 𝝼DRew. On the left, nodes at distance
    *r* exchange information through an additional edge from layer *r* onwards, instantaneously.
    On the right, we show the case of maximal delay (in our paper corresponding to
    the case **𝝼** = 1), where the delay between two nodes coincides with their distance;
    the newly added edge between nodes at distance (layer) *r* looks “in the past”
    to access the state of a node as it was *r* layers ago.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: DRew及其延迟变体𝝼DRew的比较。左侧，距离 *r* 的节点通过从层 *r* 开始的额外边缘即时交换信息。右侧显示的是最大延迟的情况（在我们的论文中对应**𝝼**
    = 1），其中两个节点之间的延迟与它们的距离一致；在距离（层）*r* 的节点之间新增的边缘“看起来像是”过去的，以访问 *r* 层前节点的状态。
- en: The 𝝼DRew framework
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 𝝼DRew 框架
- en: We call an architecture combining dynamic rewiring with delayed message passing
    𝝼DRew (pronounced “Andrew”).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称结合动态重连和延迟消息传递的架构为𝝼DRew（发音为“Andrew”）。
- en: One way to view 𝝼DRew is as an architecture with *sparse skip-connections*,
    allowing messages to travel not only “horizontally” (between nodes of the graph
    within the same layer, as in classical MPNN) but also “vertically” (across different
    layers). The idea of relying on vertical edges in GNNs is not new, and in fact
    one can think of residual connections as vertical links connecting each node to
    the same node at the previous layer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 观察 𝝼DRew 的一种方式是将其视为一种具有*稀疏跳跃连接*的架构，允许消息不仅“横向”传递（在图的同一层级的节点之间，如经典 MPNN 中），还“纵向”传递（跨不同层级）。在
    GNN 中依赖垂直边缘的想法并不新鲜，实际上可以将残差连接视为将每个节点连接到前一层相同节点的垂直链接。
- en: The delay mechanism extends this approach by creating vertical edges that connect
    a node *u* and a *different* node *v* at some previous layer depending on the
    graph distance between *u* and *v*. This way, we can leverage benefits intrinsic
    to skip-connections for deep neural networks while conditioning them on the extra
    geometric information we have at our disposal in the form of graph distance.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟机制通过创建垂直边缘来扩展这种方法，这些边缘连接一个节点 *u* 和一个在某个先前层的 *不同* 节点 *v*，具体取决于 *u* 和 *v* 之间的图距离。这样，我们可以利用深度神经网络的跳跃连接的固有优势，同时基于我们手头的额外几何信息（以图距离的形式）来进行条件化。
- en: 𝝼DRew alleviates over-squashing since distant nodes now have access to multiple
    (shorter) pathways to exchange information, bypassing the “information dilution”
    of repeated local message passing. Differently from static rewiring, 𝝼DRew achieves
    this effect by slowing down the densification of the graph and making it layer-dependent,
    hence reducing the memory footprint.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 𝝼DRew 缓解了过度压缩，因为远离的节点现在可以通过多个（更短的）路径交换信息，绕过了重复本地消息传递的“信息稀释”问题。与静态重连不同，𝝼DRew
    通过减缓图的稠密化并使其依赖于层级来实现这一效果，从而减少了内存占用。
- en: 𝝼DRew is suitable to explore the graph at different speeds, deal with long-range
    interactions, and generally enhance the power of very deep GNNs. Since 𝝼DRew determines
    *where* and *when* messages are being exchanged, but not *how*, it can be seen
    as a meta-architecture that can augment existing MPNNs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 𝝼DRew 适用于以不同速度探索图形，处理长程交互，并一般性地增强非常深的 GNN 的能力。由于 𝝼DRew 确定了消息交换的*何处*和*何时*，但不*如何*，因此可以视为一种元架构，可以增强现有的
    MPNNs。
- en: Experimental results
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验结果
- en: In our paper [10], we provide an extensive comparison of 𝝼DRew with classical
    MPNNs baselines, static rewiring, and Transformer-type architectures, using a
    fixed parameter budget. On the recent long-range benchmark (LRGB) introduced by
    Vijay Dwivedi and co-authors [11], 𝝼DRew outperforms in most cases all of the
    above.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6dab81c55ebc175e0c273e373a52e3aa.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Comparison of various classical MPNNs (GCN, GINE, etc.), static graph rewiring
    (MixHop-GCN, DIGL), and graph Transformer-type architectures (Transformer, SAN,
    GraphGPS, including positional Laplacian encoding) with 𝝼DRew-MPNN variants on
    four Long-Range Graph Benchmark (LRGB) tasks. Green, orange, and purple represent
    first-, second-, and third-best models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'An ablation study of 𝝼DRew on one of the LRGB tasks reveals another crucial
    contribution of our framework: the ability to tune 𝝼 to suit the task. We observe
    that the more delay used (lower value of 𝝼), the better the performance for large
    number of layers *L*, whereas using less delay (high 𝝼) ensures faster filling
    of the computational graph and greater density of connections after fewer layers.
    Consequently, in shallow architectures (small *L*), removing delay altogether
    (𝝼=∞) performs better. Conversely, in deep architectures (large *L*), more delay
    (small 𝝼) “slows down” the densification of the message passing graph, leading
    to better performance.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9dcf49a8950930132e945d2eb173f808.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Performance of 𝝼DRew-MPNNs with different number of layers *L* and different
    delay parameter 𝝼. While dynamic rewiring helps for long-range tasks in all regimes,
    delay significantly improves the performance over deeper models. Our framework
    can also be controlled for compute/memory budget depending on the application,
    e.g. in situations where Transformers are computationally intractable.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Traditional MPNN-type architectures differ in *how* messages are exchanged [12].
    Graph rewiring techniques add an extra level of control of *where* they are sent
    on the graph. Our new approach of dynamic graph rewiring with delayed message
    passing allows to further control *when* messages are exchanged.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: This approach appears to be very powerful, and our work is only a first attempt
    at leveraging the idea of accessing past states in a graph neural network depending
    on “geometric properties” of the underlying data. We hope that this new paradigm
    can lead to more theoretically-principled frameworks and challenge the idea that
    MPNNs are unable to solve long-range tasks unless augmented with quadratic attention
    layers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[1] This is typical e.g. in “small-world” graphs such as social networks.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[2] U. Alon and E. Yahav, On the bottleneck of graph neural networks and its
    practical implications (2021), *ICLR*.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[3] J. Topping *et al.*, Understanding over-squashing and bottlenecks on graphs
    via curvature (2022), *ICLR.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Commute time is the expected time it takes a random walk to go from node
    *v* to node *u* and back. See F. Di Giovanni *et al.*, [On over-squashing in message
    passing neural networks: The impact of width, depth, and topology](https://arxiv.org/pdf/2302.02941.pdf)
    (2023), *ICML*.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 通勤时间是随机游走从节点 *v* 到节点 *u* 再返回所需的预期时间。见 F. Di Giovanni *等*，[关于消息传递神经网络中过度压缩的影响：宽度、深度和拓扑的影响](https://arxiv.org/pdf/2302.02941.pdf)（2023），*ICML*。'
- en: '[5] See Theorem 4.3 in F. Di Giovanni *et al.* [How does over-squashing affect
    the power of GNNs?](https://arxiv.org/pdf/2306.03589.pdf) (2023) *arXiv*:2306.03589.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 见 F. Di Giovanni *等* 的定理 4.3，[过度压缩如何影响 GNN 的能力？](https://arxiv.org/pdf/2306.03589.pdf)（2023）*arXiv*:2306.03589。'
- en: '[6] Graph rewiring is somewhat a controversial technique in the GNN community
    as some believe the input graph is sacrosanct and must not be touched. *De facto*,
    most modern GNNs do employ some form of graph rewiring, whether explicitly (as
    a pre-processing step) or implicitly (e.g., by neighbour sampling or using virtual
    nodes).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 图重连在 GNN 社区中有些争议，因为一些人认为输入图是神圣不可侵犯的，不应被触及。*实际上*，大多数现代 GNN 都采用某种形式的图重连，无论是显式的（作为预处理步骤）还是隐式的（例如，通过邻居采样或使用虚拟节点）。'
- en: '[7] R. Abboud, R. Dimitrov, and I. Ceylan, [Shortest path networks for graph
    property prediction](https://arxiv.org/pdf/2206.01003.pdf) (2022),'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] R. Abboud, R. Dimitrov 和 I. Ceylan，[图属性预测的最短路径网络](https://arxiv.org/pdf/2206.01003.pdf)（2022），'
- en: '*arXiv*:2206.01003.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*arXiv*:2206.01003。'
- en: '[8] See e.g. V. P. Dwivedi and X. Bresson, [A generalization of Transformer
    networks to graphs](https://arxiv.org/pdf/2012.09699v2.pdf) (2021), *arXiv*:2012.09699
    and C. Ying et al., [Do Transformers Really Perform Badly for Graph Representation?](https://www.microsoft.com/en-us/research/publication/do-transformers-really-perform-badly-for-graph-representation/)
    (2021), NeurIPS.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 见例如 V. P. Dwivedi 和 X. Bresson， [Transformer 网络对图的推广](https://arxiv.org/pdf/2012.09699v2.pdf)（2021），*arXiv*:2012.09699
    和 C. Ying 等，[Transformer 对图表示的性能真的很差吗？](https://www.microsoft.com/en-us/research/publication/do-transformers-really-perform-badly-for-graph-representation/)（2021），NeurIPS。'
- en: '[9] Dynamic rewiring results in the following message passing formula:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 动态重连结果为以下消息传递公式：'
- en: '**m***ᵤ*⁽*ˡ ᵏ* ⁾=AGG({**x***ᵥ*⁽*ˡ* ⁾ : *v∈𝒩ₖ*(*u*)}) with 1 ≤ *k ≤ l*+1'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**m***ᵤ*⁽*ˡ ᵏ* ⁾=AGG({**x***ᵥ*⁽*ˡ* ⁾ : *v∈𝒩ₖ*(*u*)})，其中 1 ≤ *k ≤ l*+1'
- en: '**x***ᵤ*⁽*ˡ* ⁺¹⁾=UP(**x***ᵤ*⁽*ˡ* ⁾, **m***ᵤ*⁽*ˡ* ¹⁾,…, **m***ᵤ*⁽*ˡ ˡ* ⁺¹⁾)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**x***ᵤ*⁽*ˡ* ⁺¹⁾=UP(**x***ᵤ*⁽*ˡ* ⁾, **m***ᵤ*⁽*ˡ* ¹⁾,…, **m***ᵤ*⁽*ˡ ˡ* ⁺¹⁾)'
- en: where AGG is a permutation-invariant aggregation operator, *𝒩ₖ*(*u*) is the
    *k*-hop neighbourhood of node *u*, and UP is an update operation receiving messages
    from each *k*-hop separately. See equation 5 in [10].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，AGG 是一个置换不变的聚合操作符，*𝒩ₖ*(*u*) 是节点 *u* 的 *k*-跳邻域，而 UP 是一个接收来自每个 *k*-跳的消息的更新操作。见[10]中的公式5。
- en: '[10] B. Gutteridge et al., DRew: [Dynamically rewired message passing with
    delay](https://arxiv.org/pdf/2305.08018.pdf) (2023), *ICML*.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] B. Gutteridge 等，DRew: [动态重连消息传递与延迟](https://arxiv.org/pdf/2305.08018.pdf)（2023），*ICML*。'
- en: '[11] Delayed message passing takes the form'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] 延迟消息传递呈现以下形式'
- en: '**m***ᵤ*⁽*ˡ ᵏ* ⁾=AGG({**x***ᵥ*⁽*ˡ* ᐨ*ˢ* ⁽*ᵏ* ⁾⁾ : *v∈𝒩ₖ*(*u*)}) with 1 ≤ *k
    ≤ l*+1'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**m***ᵤ*⁽*ˡ ᵏ* ⁾=AGG({**x***ᵥ*⁽*ˡ* ᐨ*ˢ* ⁽*ᵏ* ⁾⁾ : *v∈𝒩ₖ*(*u*)})，其中 1 ≤ *k ≤
    l*+1'
- en: '**x***ᵤ*⁽*ˡ* ⁺¹⁾=UP(**x***ᵤ*⁽*ˡ* ⁾, **m***ᵤ*⁽*ˡ* ¹⁾,…, **m***ᵤ*⁽*ˡ ˡ* ⁺¹⁾)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**x***ᵤ*⁽*ˡ* ⁺¹⁾=UP(**x***ᵤ*⁽*ˡ* ⁾, **m***ᵤ*⁽*ˡ* ¹⁾,…, **m***ᵤ*⁽*ˡ ˡ* ⁺¹⁾)'
- en: where *s*(*k*)=max{0,*k*﹣𝝼}, see equation 6 in [10]. The choice 𝝼=∞ corresponds
    to no delay (like in DRew) and 𝝼 = 1 corresponds to classical MPNN (two nodes
    *u* and *v* at distance *r* interact directly from layer *r* onwards, but with
    a constant delay of *r*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *s*(*k*)=max{0,*k*﹣𝝼}，见[10]中的公式6。选择 𝝼=∞ 表示没有延迟（如在 DRew 中），而 𝝼 = 1 表示经典的 MPNN（两个节点
    *u* 和 *v* 在距离 *r* 的位置上从第 *r* 层开始直接交互，但有一个恒定的 *r* 延迟）。
- en: '[11] V. P. Dwivedi *et al.*, [Long range graph benchmark](https://arxiv.org/abs/2206.08164)
    (2022), *arXiv*:2206.08164.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] V. P. Dwivedi *等*，[长距离图基准](https://arxiv.org/abs/2206.08164)（2022），*arXiv*:2206.08164。'
- en: '[12] In our proto-book M. M. Bronstein *et al.,* [Geometric Deep Learning:
    Grids, Groups, Graphs, Geodesics, and Gauges](https://arxiv.org/abs/2104.13478)
    (2021), we distinguish between three “flavours” of MPNNs: convolutional, attentional,
    and generic message passing.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] 在我们的原型书中 M. M. Bronstein *等*，[几何深度学习：网格、群体、图、测地线和量规](https://arxiv.org/abs/2104.13478)（2021），我们区分了三种“风格”的
    MPNN：卷积型、注意力型和通用消息传递。'
- en: '*We are grateful to* [*Federico Barbero*](https://twitter.com/fedzbar)*,* [*Fabrizio
    Frasca*](https://twitter.com/ffabffrasca)*, and* [*Emanuele Rossi*](https://twitter.com/emaros96)
    *for proofreading this post and providing insightful comments. For additional
    articles about deep learning on graphs, see Michael’s* [*other posts*](https://towardsdatascience.com/graph-deep-learning/home)
    *in Towards Data Science,* [*subscribe*](https://michael-bronstein.medium.com/subscribe)
    *to his posts and* [*YouTube channel*](https://www.youtube.com/c/MichaelBronsteinGDL)*,
    get* [*Medium membership*](https://michael-bronstein.medium.com/membership)*,
    or follow him on* [*Twitter*](https://twitter.com/mmbronstein)*.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们感谢* [*Federico Barbero*](https://twitter.com/fedzbar)*,* [*Fabrizio Frasca*](https://twitter.com/ffabffrasca)*
    和* [*Emanuele Rossi*](https://twitter.com/emaros96) *为本文进行校对并提供了有见地的评论。有关图形深度学习的更多文章，请参见
    Michael 的* [*其他文章*](https://towardsdatascience.com/graph-deep-learning/home) *在
    Towards Data Science 上，* [*订阅*](https://michael-bronstein.medium.com/subscribe)
    *他的文章和* [*YouTube 频道*](https://www.youtube.com/c/MichaelBronsteinGDL)*，获取* [*Medium
    会员资格*](https://michael-bronstein.medium.com/membership)*，或者在* [*Twitter*](https://twitter.com/mmbronstein)*
    上关注他。*'
