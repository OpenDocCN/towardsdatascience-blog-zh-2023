["```py\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n...\n# Authenticate BigQuery client:\nservice_acount_str = config.get('BigQuery') # Use config\ncredentials = service_account.Credentials.from_service_account_info(service_acount_str)\nclient = bigquery.Client(credentials=credentials, project=credentials.project_id)\n\n...\ndef load_table_from_dataframe(table_schema, table_name, dataset_id):\n    #! source data file format must be outer array JSON:\n    \"\"\"\n    [\n    {\"id\":\"1\"},\n    {\"id\":\"2\"}\n    ]\n    \"\"\"\n    blob = \"\"\"\n            [\n    {\"id\":\"1\",\"first_name\":\"John\",\"last_name\":\"Doe\",\"dob\":\"1968-01-22\",\"addresses\":[{\"status\":\"current\",\"address\":\"123 First Avenue\",\"city\":\"Seattle\",\"state\":\"WA\",\"zip\":\"11111\",\"numberOfYears\":\"1\"},{\"status\":\"previous\",\"address\":\"456 Main Street\",\"city\":\"Portland\",\"state\":\"OR\",\"zip\":\"22222\",\"numberOfYears\":\"5\"}]},\n    {\"id\":\"2\",\"first_name\":\"John\",\"last_name\":\"Doe\",\"dob\":\"1968-01-22\",\"addresses\":[{\"status\":\"current\",\"address\":\"123 First Avenue\",\"city\":\"Seattle\",\"state\":\"WA\",\"zip\":\"11111\",\"numberOfYears\":\"1\"},{\"status\":\"previous\",\"address\":\"456 Main Street\",\"city\":\"Portland\",\"state\":\"OR\",\"zip\":\"22222\",\"numberOfYears\":\"5\"}]}\n    ]\n    \"\"\"\n    body = json.loads(blob)\n    print(pandas.__version__)\n\n    table_id = client.dataset(dataset_id).table(table_name)\n    job_config = bigquery.LoadJobConfig()\n    schema = create_schema_from_yaml(table_schema) \n    job_config.schema = schema\n\n    df = pandas.DataFrame(\n    body,\n    # In the loaded table, the column order reflects the order of the\n    # columns in the DataFrame.\n    columns=[\"id\", \"first_name\",\"last_name\",\"dob\",\"addresses\"],\n\n    )\n    df['addresses'] = df.addresses.astype(str)\n    df = df[['id','first_name','last_name','dob','addresses']]\n\n    print(df)\n\n    load_job = client.load_table_from_dataframe(\n        df,\n        table_id,\n        job_config=job_config,\n    )\n\n    load_job.result()\n    print(\"Job finished.\")\n```", "```py\ncreate temp table last_online as (\n    select 1 as user_id\n    , timestamp('2000-10-01 00:00:01') as last_online\n)\n;\ncreate temp table connection_data  (\n  user_id int64\n  ,timestamp timestamp\n)\nPARTITION BY DATE(_PARTITIONTIME)\n;\ninsert connection_data (user_id, timestamp)\n    select 2 as user_id\n    , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp\nunion all\n    select 1 as user_id\n        , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp\nunion all\n    select 1 as user_id\n        , timestamp_sub(current_timestamp(),interval 20 hour) as timestamp\nunion all\n    select 1 as user_id\n    , timestamp_sub(current_timestamp(),interval 1 hour) as timestamp\n;\n\nmerge last_online t\nusing (\n  select\n      user_id\n    , last_online\n  from\n    (\n        select\n            user_id\n        ,   max(timestamp) as last_online\n\n        from \n            connection_data\n        where\n            date(_partitiontime) >= date_sub(current_date(), interval 1 day)\n        group by\n            user_id\n\n    ) y\n\n) s\non t.user_id = s.user_id\nwhen matched then\n  update set last_online = s.last_online, user_id = s.user_id\nwhen not matched then\n  insert (last_online, user_id) values (last_online, user_id)\n;\nselect * from last_online\n;\n```", "```py\nselect \n     target_id\n    ,product_id\n    ,product_type_id\n    ,production.purchase_summary_udf()(\n        ARRAY_AGG(\n            STRUCT(\n                target_id\n                , user_id\n                , product_type_id\n                , product_id\n                , item_count\n                , days\n                , expire_time_after_purchase\n                , transaction_id \n                , purchase_created_at \n                , updated_at\n            ) \n            order by purchase_created_at\n        )\n    ) AS processed\n\nfrom new_batch\n;\n```", "```py\n\"\"\"DAG definition for recommendation_bespoke model training.\"\"\"\n\nimport airflow\nfrom airflow import DAG\nfrom airflow.contrib.operators.bigquery_operator import BigQueryOperator\nfrom airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.operators.app_engine_admin_plugin import AppEngineVersionOperator\nfrom airflow.operators.ml_engine_plugin import MLEngineTrainingOperator\n\nimport datetime\n\ndef _get_project_id():\n  \"\"\"Get project ID from default GCP connection.\"\"\"\n\n  extras = BaseHook.get_connection('google_cloud_default').extra_dejson\n  key = 'extra__google_cloud_platform__project'\n  if key in extras:\n    project_id = extras[key]\n  else:\n    raise ('Must configure project_id in google_cloud_default '\n           'connection from Airflow Console')\n  return project_id\n\nPROJECT_ID = _get_project_id()\n\n# Data set constants, used in BigQuery tasks.  You can change these\n# to conform to your data.\nDATASET = 'staging' #'analytics'\nTABLE_NAME = 'recommendation_bespoke'\n\n# GCS bucket names and region, can also be changed.\nBUCKET = 'gs://rec_wals_eu'\nREGION = 'us-central1' #'europe-west2' #'us-east1'\nJOB_DIR = BUCKET + '/jobs'\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': airflow.utils.dates.days_ago(2),\n    'email': ['mike.shakhomirov@gmail.com'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 5,\n    'retry_delay': datetime.timedelta(minutes=5)\n}\n\n# Default schedule interval using cronjob syntax - can be customized here\n# or in the Airflow console.\nschedule_interval = '00 21 * * *'\n\ndag = DAG('recommendations_training_v6', default_args=default_args,\n          schedule_interval=schedule_interval)\n\ndag.doc_md = __doc__\n\n#\n#\n# Task Definition\n#\n#\n\n# BigQuery training data export to GCS\n\ntraining_file = BUCKET + '/data/recommendations_small.csv' # just a few records for staging\n\nt1 = BigQueryToCloudStorageOperator(\n    task_id='bq_export_op',\n    source_project_dataset_table='%s.recommendation_bespoke' % DATASET,\n    destination_cloud_storage_uris=[training_file],\n    export_format='CSV',\n    dag=dag\n)\n\n# ML Engine training job\ntraining_file = BUCKET + '/data/recommendations_small.csv'\njob_id = 'recserve_{0}'.format(datetime.datetime.now().strftime('%Y%m%d%H%M'))\njob_dir = BUCKET + '/jobs/' + job_id\noutput_dir = BUCKET\ndelimiter=','\ndata_type='user_groups'\nmaster_image_uri='gcr.io/my-project/recommendation_bespoke_container:tf_rec_latest'\n\ntraining_args = ['--job-dir', job_dir,\n                 '--train-file', training_file,\n                 '--output-dir', output_dir,\n                 '--data-type', data_type]\n\nmaster_config = {\"imageUri\": master_image_uri,}\n\nt3 = MLEngineTrainingOperator(\n    task_id='ml_engine_training_op',\n    project_id=PROJECT_ID,\n    job_id=job_id,\n    training_args=training_args,\n    region=REGION,\n    scale_tier='CUSTOM',\n    master_type='complex_model_m_gpu',\n    master_config=master_config,\n    dag=dag\n)\n\nt3.set_upstream(t1)\n```", "```py\ndef sum_example(*args):\n    result = 0\n    for x in args:\n        result += x\n    return result\n\nprint(sum_example(1, 2, 3))\n\ndef concat(**kwargs):\n    result = \"\"\n    for arg in kwargs.values():\n        result += arg\n    return result\n\nprint(concat(a=\"Data\", b=\"Engineering\", c=\"is\", d=\"Great\", e=\"!\"))\n```", "```py\naws lambda invoke \\\n    --function-name pipeline-manager \\\n    --payload '{ \"key\": \"something\" }' \\\n    response.json \n```", "```py\n# ./deploy.sh\n# Run ./deploy.sh\nLAMBDA_BUCKET=$1 # your-lambda-packages.aws\nSTACK_NAME=SimpleETLService\nAPP_FOLDER=pipeline_manager\n# Get date and time to create unique s3-key for deployment package:\ndate\nTIME=`date +\"%Y%m%d%H%M%S\"`\n# Get the name of the base application folder, i.e. pipeline_manager.\nbase=${PWD##*/}\n# Use this name to name zip:\nzp=$base\".zip\"\necho $zp\n# Remove old package if exists:\nrm -f $zp\n# Package Lambda\nzip -r $zp \"./${APP_FOLDER}\" -x deploy.sh\n\n# Check if Lambda bucket exists:\nLAMBDA_BUCKET_EXISTS=$(aws  s3 ls ${LAMBDA_BUCKET} --output text)\n#  If NOT:\nif [[ $? -eq 254 ]]; then\n    # create a bucket to keep Lambdas packaged files:\n    echo  \"Creating Lambda code bucket ${LAMBDA_BUCKET} \"\n    CREATE_BUCKET=$(aws  s3 mb s3://${LAMBDA_BUCKET} --output text)\n    echo ${CREATE_BUCKET}\nfi\n\n# Upload the package to S3:\naws s3 cp ./${base}.zip s3://${LAMBDA_BUCKET}/${APP_FOLDER}/${base}${TIME}.zip\n\n# Deploy / Update:\naws --profile $PROFILE \\\ncloudformation deploy \\\n--template-file stack.yaml \\\n--stack-name $STACK_NAME \\\n--capabilities CAPABILITY_IAM \\\n--parameter-overrides \\\n\"StackPackageS3Key\"=\"${APP_FOLDER}/${base}${TIME}.zip\" \\\n\"AppFolder\"=$APP_FOLDER \\\n\"LambdaCodeLocation\"=$LAMBDA_BUCKET \\\n\"Environment\"=\"staging\" \\\n\"Testing\"=\"false\"\n```", "```py\nCREATE OR REPLACE MODEL sample_churn_model.churn_model\n\nOPTIONS(\n  MODEL_TYPE=\"LOGISTIC_REG\",\n  INPUT_LABEL_COLS=[\"churned\"]\n) AS\n\nSELECT\n  * except (\n     user_pseudo_id\n    ,first_seen_ts    \n    ,last_seen_ts     \n  )\nFROM\n  sample_churn_model.churn\n```", "```py\nwith checks as (\n    select\n      count( transaction_id )                                                           as t_cnt\n    , count(distinct transaction_id)                                                    as t_cntd\n    , count(distinct (case when payment_date is null then transaction_id end))          as pmnt_date_null\n    from\n        production.user_transaction\n)\n, row_conditions as (\n\n    select if(t_cnt = 0,'Data for yesterday missing; ', NULL)  as alert from checks\nunion all\n    select if(t_cnt != t_cntd,'Duplicate transactions found; ', NULL) from checks\nunion all\n    select if(pmnt_date_null != 0, cast(pmnt_date_null as string )||' NULL payment_date found', NULL) from checks\n)\n\n, alerts as (\nselect\n        array_to_string(\n            array_agg(alert IGNORE NULLS) \n        ,'.; ')                                         as stringify_alert_list\n\n    ,   array_length(array_agg(alert IGNORE NULLS))     as issues_found\nfrom\n    row_conditions\n)\n\nselect\n    alerts.issues_found,\n    if(alerts.issues_found is null, 'all good'\n        , ERROR(FORMAT('ATTENTION: production.user_transaction has potential data quality issues for yesterday: %t. Check dataChecks.check_user_transaction_failed_v for more info.'\n        , stringify_alert_list)))\nfrom\n    alerts\n;\n```", "```py\n# Create a file first: ./very_big_file.csv as:\n# transaction_id,user_id,total_cost,dt\n# 1,John,10.99,2023-04-15\n# 2,Mary, 4.99,2023-04-12\n\n# Example.py\ndef etl(item):\n    # Do some etl here\n    return item.replace(\"John\", '****') \n\n# Create a generator \ndef batch_read_file(file_object, batch_size=19):\n    \"\"\"Lazy function (generator) can read a file in chunks.\n    Default chunk: 1024 bytes.\"\"\"\n    while True:\n        data = file_object.read(batch_size)\n        if not data:\n            break\n        yield data\n# and read in chunks\nwith open('very_big_file.csv') as f:\n    for batch in batch_read_file(f):\n        print(etl(batch))\n\n# In command line run\n# Python example.py\n```"]