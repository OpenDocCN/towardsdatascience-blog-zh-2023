- en: 'Gaussian Mixture Models (GMMs): from Theory to Implementation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gaussian-mixture-models-gmms-from-theory-to-implementation-4406c7fe9847](https://towardsdatascience.com/gaussian-mixture-models-gmms-from-theory-to-implementation-4406c7fe9847)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In-depth explanation of GMMs and the Expectation-Maximization algorithm used
    to train them
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roiyeho?source=post_page-----4406c7fe9847--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----4406c7fe9847--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4406c7fe9847--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4406c7fe9847--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----4406c7fe9847--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4406c7fe9847--------------------------------)
    ·17 min read·Nov 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Mixture Models (GMMs) are statistical models that represent the data
    as a mixture of Gaussian (normal) distributions. These models can be used to identify
    groups within the dataset, and to capture the complex, multi-modal structure of
    data distributions.
  prefs: []
  type: TYPE_NORMAL
- en: GMMs are used in a variety of machine learning applications, including [clustering](https://medium.com/ai-made-simple/introduction-to-clustering-2ffc22673b5a),
    density estimation, and pattern recognition.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we will first explore mixture models, focusing on Gaussian mixture
    models and their underlying principles. Then, we will examine how to estimate
    the parameters of these models using a powerful technique known as Expectation-Maximization
    (EM), and provide a step-by-step guide to implementing it from scratch in Python.
    Finally, we will demonstrate how to perform clustering with GMM using the Scikit-Learn
    library.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3bd2c1d7f6076240b8c9b005b168df4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Markéta Klimešová](https://pixabay.com/users/maky_orel-436253/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5029714)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5029714)
  prefs: []
  type: TYPE_NORMAL
- en: Mixture Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **mixture model** is a probability model for representing data that may arise
    from several different sources or categories, each of which is modeled by a separate
    probability distribution. For example, financial returns typically behave differently
    under normal market conditions and during periods of crisis, and thus can be modeled
    as a mixture of two distinct distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, if *X* is a random variable whose distribution is a mixture of *K*
    component distributions, the probability density function (PDF) or probability
    mass function (PMF) of *X* can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7cd249072205eae2a81a2f4a16b960a.png)'
  prefs: []
  type: TYPE_IMG
- en: A mixture model
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*x*) is the overall density or mass function of the mixture model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*K* is the number of component distributions in the mixture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*fₖ*(*x*; *θₖ*) is the density or mass function of the *k*-th component distribution,
    parametrized by *θₖ*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*wₖ* is the mixing weight of the *k*-th component, with 0 ≤ *wₖ* ≤ 1 and the
    sum of the weights being 1\. *wₖ* is also known as the prior probability of component
    *k*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*θₖ* represents the parameters of the *k*-th component, such as the mean and
    standard deviation in the case of Gaussian distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mixture model assumes that each data point comes from one of the *K* component
    distributions, with the specific distribution being selected according to the
    mixing weights *wₖ*. The model does not require knowing which component each data
    point belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **Gaussian mixture model** (GMM) is a common mixture model, where the probability
    density is given by a mixture of Gaussian distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c7d05610e42ddb1f4d5d9a32980fe4c.png)'
  prefs: []
  type: TYPE_IMG
- en: A Gaussian mixture model
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**x** is a *d*-dimensional vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*μₖ* is the mean vector of the *k*-th Gaussian component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Σ*ₖ* is the covariance matrix of the *k*-th Gaussian component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N*(**x**; *μₖ*, Σ*ₖ*) is the multivariate normal density function for the
    *k*-th component:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b52166ac55f9654920a211e60e2f7de4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the case of univariate Gaussian distributions, the probability density can
    be simplified to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6cf79b16461597a21f8883f63854f56.png)'
  prefs: []
  type: TYPE_IMG
- en: A mixture model of univariate Gaussian distributions
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*μₖ* is the mean of the *k*-th Gaussian component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*σₖ* is the covariance matrix of the *k*-th Gaussian component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N*(*x*; *μₖ*, *σₖ*) is the univariate normal density function for the *k*-th
    component:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/dd2755ff599b04845f69172865e74fa2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, the following Python function plots a mixture distribution of
    two univariate Gaussian distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use this function to plot a mixture of two Gaussian distributions with
    parameters *μ*₁ = -1, *σ*₁ = 1, *μ*₂ = 4, *σ*₂ = 1.5, and mixture weights of *w*₁
    = 0.7 and *w*₂ = 0.3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/24c30ab9796f6acc90b3b8211c2caf4f.png)'
  prefs: []
  type: TYPE_IMG
- en: A mixture model of two univariate Gaussian distributions
  prefs: []
  type: TYPE_NORMAL
- en: The dashed lines represent the individual normal distributions, and the solid
    black line shows the resulting mixture. This plot illustrates how the mixture
    model combines the two distributions, each with its own mean, standard deviation,
    and weight in the overall mixture. ​
  prefs: []
  type: TYPE_NORMAL
- en: Learning the GMM Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our goal is to find the parameters of the GMM (means, covariances, and mixing
    coefficients) that will best explain the observed data. To that end, we first
    define the likelihood of the model given the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a GMM with *K* components and a dataset *X* = {**x**₁, …, **x***ₙ*} of
    *n* data points, the likelihood function *L* is given by the product of the probability
    densities of each data point, as defined by the GMM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d535098186869a806272b8e77b8dc0b0.png)'
  prefs: []
  type: TYPE_IMG
- en: The likelihood of the GMM model
  prefs: []
  type: TYPE_NORMAL
- en: where *θ* represents all the parameters of the model (means, variances, and
    mixture weights).
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, it is easier to work with the log-likelihood, since the product
    of probabilities can to lead to numerical underflow for large datasets. The log-
    likelihood is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d33dd4e600a81b27aa9e63056e6d3585.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The parameters of the GMM can be estimated by maximizing this log-likelihood
    function with respect to *θ*.However, we cannot directly apply [Maximum Likelihood
    Estimation](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43) (MLE)
    to estimate the parameters of a GMM due to the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The log-likelihood function is highly non-linear and complex to maximize analytically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model has latent variables (the mixture weights), which are not directly
    observable in the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To overcome these issues, the Expectation-Maximization (EM) algorithm is commonly
    used instead. This algorithm is described in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Expectation-Maximization (EM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The EM algorithm is a powerful method for finding maximum likelihood estimates
    of parameters in statistical models that depend on unobserved latent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm begins by randomly initializing the model parameters. Then it
    iterates between two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Expectation step (E-step)**: Compute the expected log-likelihood of the model
    with respect to the distribution of the latent variables, given the observed data
    and the current estimates of the model parameters. This step involves an estimation
    of the probabilities of the latent variables.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Maximization step (M-step)**: Update the parameters of the model to maximize
    the log-likelihood of the observed data, given the estimated latent variables
    from the E-step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These two steps are repeated until convergence, typically determined by a threshold
    on the change in the log-likelihood or a maximum number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s formulate the update equations used in the EM steps for estimating the
    parameters of a Gaussian Mixture Model. In GMMs, the latent variables represent
    the unknown component memberships of each data point. Let *Zᵢ* be the random variable
    indicating the component from which data point **x***ᵢ* was generated. *Zᵢ* can
    takeone of the values {1, …, *K*}, corresponding to the *K* components.
  prefs: []
  type: TYPE_NORMAL
- en: '**E-Step**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the E-step, we compute the probability distributions of the latent variables
    *Zᵢ* given the current estimates of the model parameters. In other words, we calculate
    the membership probabilities for each data point in each Gaussian component.
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability that *Zᵢ* = *k*, i.e., that **x***ᵢ* belongs to the *k*-th
    component, can be computed using Bayes’ rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9353f697ea94e2f8820260d98a41ffad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s denote this probability by the variable *γ*(*zᵢₖ*). Thus, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0726d9e0a17379ffa1c58df10606f4f0.png)'
  prefs: []
  type: TYPE_IMG
- en: The variables *γ*(*zᵢₖ*) are often referred to as *responsibilities*,since they
    describe how responsible is each component for each observation. These responsibilities
    serve as proxies for the missing information about the latent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The expected log-likelihood with respect to the distribution of the latent
    variables can now be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1981d13ae914c4fbc57b2459df48b0a.png)'
  prefs: []
  type: TYPE_IMG
- en: The function *Q* is a weighted sum of the log-likelihoods of all the data points
    under each Gaussian component, with the weights being the responsibilities. Note
    that *Q* is different from the log-likelihood function *l*(*θ|X*) shown earlier.
    The log-likelihood *l*(*θ|X*) expresses the likelihood of the observed data under
    the mixture model as a whole, without explicitly accounting for the latent variables,
    whereas *Q* represents an expected log-likelihood over both the observed data
    and the estimated latent variable distributions.
  prefs: []
  type: TYPE_NORMAL
- en: M-Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the M-step, we update the parameters *θ* of the GMM (means, covariances,
    and mixing weights) so as to maximize the expected likelihood *Q*(*θ*) using the
    responsibilities calculated in the E-step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter updates are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the means for each component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/41c74ec7627348b2991314b742bf53eb.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, the new mean of the *k*-th component is a weighted average of all the
    data points, with the weights being the probabilities that these points belong
    to component *k*.
  prefs: []
  type: TYPE_NORMAL
- en: This update formula can be derived from maximizing the expected log-likelihood
    function *Q* with respect to the means *μₖ*. I will show here the proof for the
    univariate Gaussian distributions case.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The expected log-likelihood in the case of univariate Gaussian distributions
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0e8ff7a09be9551b9c5364f201bb426.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Taking the derivative of this function with respect to *μₖ* and setting it
    to 0 gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03339871eea729abae42f648fb1a1c0c.png)'
  prefs: []
  type: TYPE_IMG
- en: '2\. Update the covariances for each component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c56b50366d4f2301351bb3cea30cb8f4.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, the new covariance of the *k*-th component is a weighted average of
    the squared deviations of each data point from the component’s mean, where the
    weights are the probabilities of the points assigned to that component.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of univariate normal distributions, this update is simplified to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd84db349f8a5b6fb88c9810b4a012ec.png)'
  prefs: []
  type: TYPE_IMG
- en: '3\. Update the mixing weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc23e3bcd3535ad48505ed812b90c405.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, the new weight of the *k*-th component is the total probability of
    the points belonging to this component, normalized by the number of points *n*.
  prefs: []
  type: TYPE_NORMAL
- en: Repeating these two steps is guaranteed to convergence to a local maximum of
    the likelihood function. Since the final optimum reached depends on the initial
    random parameter values, it is a common practice to run the EM algorithm several
    times with varied random initializations and keep the model that obtains the highest
    likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now implement the EM algorithm for estimating the parameters of a GMM
    of two univariate Gaussian distributions from a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s write a function to initialize the parameters of the GMM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The means are initialized from random data points in the dataset, the standard
    deviations are set to 1, and the mixing weights are set uniformly to be 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now implement the E-step, in which we compute the responsibilities (probabilities)
    for each data point belonging to each Gaussian component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the M-step, we update the model parameters based on the responsibilities
    calculated in the E-step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we write the main function that runs the EM algorithm, iterating between
    the E-step and M-step for a specified number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To test our implementation, we will create a synthetic dataset by sampling data
    from a known mixture distribution with predefined parameters. Then, we will use
    the EM algorithm to estimate the parameters of the distribution, and compare the
    estimated parameters with the original ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s write a function to sample data from a mixture of two univariate
    normal distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now use this function to sample 1,000 data points from the mixture
    distribution we have defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now run the EM algorithm on this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm has converged to parameters that are close to the original parameters
    of the mixture: *μ*₁ = -1.031, *σ*₁ = 1.033, *μ*₂ = 4.181, *σ*₂ = 1.370, and mixture
    weights of *w*₁ = 0.675 and *w*₂ = 0.325.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the `plot_mixture()` function we have written earlier to plot the
    final distribution. We will update the function to plot an histogram of the sampled
    data as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31dd3082554c4b0797212fbd2eab46ef.png)'
  prefs: []
  type: TYPE_IMG
- en: The mixture distribution estimated from the dataset using the EM algorithm
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen, the estimated distribution closely aligns with the histogram
    of the data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise: Extend the code above to handle multivariate normal distributions
    and any number of distributions K.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Hint: You can use the function `scipy.stats.multivariate_normal`to compute
    the PDF of a multivariate normal distribution.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GMM in Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides an implementation of Gaussian mixture model in the class
    `[sklearn.mixture.GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)`.
    Important parameters of this class include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components`: The number of mixture components (defaults to 1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`covariance_type`: The type of covariance parameters to use. Can be one of
    the following options:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- `''full''`: Each component has its own covariance matrix.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- `''tied''`: All components share the same covariance matrix.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- `''diag''`: Each component has its own covariance matrix, which must be diagonal.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- `''spherical''`: Each component has its own single variance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`tol`: The convergence threshold. The EM algorithm will stop when the average
    improvement of the log-likelihood falls below this threshold (defaults to 0.001).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_iter`: The number of EM iterations to perform (defaults to 100).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_init`: The number of random initializations to perform (defaults to 1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_params`: The method used to initialize the parameters of the model. Can
    take one of the following options:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''kmeans''`: The parameters are initialized using *k*-means (the default).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`''k-means++''`: The parameters are initialized using *k*-means++.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`''random''`: The parameters are randomly initialized.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`''random_from_data''`: The initial means are randomly selected from the given
    data points.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In addition, this class provides the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`weights_`: The mixture weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`means_`: The means of each component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`covariances_`: The covariance of each component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`converged_`: A Boolean indicating whether a convergence has been reached by
    the EM algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_iter_`: The number of steps used by the EM to reach convergence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that unlike other clustering algorithms in Scikit-Learn, this class does
    not provide a `labels_` attribute. Therefore, to get the cluster assignments of
    the data points, you need to call the `predict()`method on the fitted model (or
    call `fit_predict()`).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s use this class to perform clustering on the following dataset,
    which consists of two elliptical blobs and a spherical one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we instantiate the `GMM`class with `n_components=3`, and call its `fit_predict()`
    method to get the cluster assignments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check how many iterations it took for the EM algorithm to converge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It took only two iterations for the EM algorithm to converge in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also examine the estimated GMM parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the estimated weights are very close to the original proportions
    of the three blobs, and the mean and variance of the spherical blob are very close
    to its original parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fae51f47bd7ba85a02aa535d0f8a59ac.png)'
  prefs: []
  type: TYPE_IMG
- en: The results of GMM clustering
  prefs: []
  type: TYPE_NORMAL
- en: GMM has correctly identified all three clusters.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we can use the method `predict_proba()` to get the membership probabilities
    for each data point in each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, the first point in the dataset has a very high probability of
    belonging to the green cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize these probabilities by making the size of each point proportional
    to its probability of belonging to the cluster it was assigned to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/44fa0489ac3958bb4155bac43d4e4c30.png)'
  prefs: []
  type: TYPE_IMG
- en: Probabilities of cluster assignments
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the points that lie in the border between the two elliptical
    clusters have lower probability. Data points that have a significantly low probability
    density (e.g., falling below a predefined threshold) can be identified as anomalies
    or outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For comparison, the following figure shows the results of other clustering
    algorithms applied to the same dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6913fb1711177424cd27c0a72da01db7.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen, other clustering algorithms fail to identify correctly the elliptical
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Model Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The log-likelihood is a primary measure used to evaluate GMMs. It is also monitored
    during training to check for convergence of the EM algorithm. However, sometimes
    we need to compare models with different number of components or different covariance
    structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'To that end, we have two additional measures, which balance the model complexity
    (number of parameters) against its goodness of fit (represented by the log-likelihood):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Akaike Information Criterion (AIC):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/38a2bbaaecc2ab3d69202d6003eff6eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p* is the number of parameters in the model (including all the means, covariances,
    and mixing weights).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*L* is the maximum likelihood of the model (the likelihood of the model with
    the optimal parameter values).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lower values of AIC indicate a better model. AIC rewards models that fit the
    data well, but also penalizes models with more parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Bayesian Information Criterion (BIC):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe9769c0f223252eed7f79a71f4433c7.png)'
  prefs: []
  type: TYPE_IMG
- en: where *p* and *L* are defined as before, and *n* is the number of data points.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to AIC, BIC balances model fit and complexity, but places a greater
    penalty on models with more parameters, as *p* is multiplied by log(*n*) instead
    of 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scikit-Learn, you can compute these measures using the methods `aic()` and
    `bic()` of the `GMM`class. For example, the AIC and BIC values of the GMM clustering
    of the blobs dataset are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: These measures can be used to find the optimal number of components by fitting
    GMMs with different numbers of components to the dataset and then selecting the
    model with the lowest AIC or BIC value.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s summarize the pros and cons of GMMs as compared to other clustering algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**:'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike *k*-means, which assumes spherical clusters, GMMs can adapt to ellipsoidal
    shapes thanks to the covariance component. This allows GMMs to capture a wider
    variety of cluster shapes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can deal with clusters with varying sizes due to their use of covariance matrices
    and mixing coefficients, which account for the spread and proportion of each cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GMMs provide probabilities (soft assignments) of each point belonging to each
    cluster, which can be more informative in understanding the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can deal with overlapping clusters, since it assigns data points to clusters
    based on probabilities rather than hard boundaries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to explain the clustering results, because each cluster is represented
    by a Gaussian distribution with specific parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to clustering, GMMs can also be used for density estimation and
    anomaly detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**:'
  prefs: []
  type: TYPE_NORMAL
- en: Require specifying the number of components (clusters) in advance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assume that the data in each cluster follows a Gaussian distribution, which
    might not always be a valid assumption for real-world data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May not work well when clusters contain only a few data points, as the model
    relies on sufficient data to accurately estimate the parameters of each component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The clustering results can be sensitive to the initial choice of parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The EM algorithm used in GMMs can get stuck in a local optimum, and its convergence
    can be slow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Badly-conditioned covariance matrices (i.e., matrices that are near singular
    or have a very high condition number) can lead to numerical instabilities during
    the EM computations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computationally more intensive than simpler algorithms like *k*-means, especially
    for large datasets or when the number of components is high.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: All the images are by the author unless stated otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/gmm](https://github.com/roiyeho/medium/tree/main/gmm)'
  prefs: []
  type: TYPE_NORMAL
