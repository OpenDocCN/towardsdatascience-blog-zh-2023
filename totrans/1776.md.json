["```py\nimport torch\n\n# Define L1 regularisation\nl1_lambda = 0.01\n\n# Training loop for the model\nfor input, target in data_loader:\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_function(output, target)\n\n    # Calculate L1 penalty\n    l1_penalty = torch.tensor(0.).to(input.device)\n    for param in model.parameters():\n        l1_penalty += torch.sum(torch.abs(param))\n\n    # Add L1 penalty to the loss\n    loss += l1_lambda * l1_penalty\n\n    # Backward pass and optimize\n    loss.backward()\n    optimizer.step()\n```", "```py\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport plotly.graph_objects as go\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.input_layer = nn.Linear(4, 10)\n        self.output_layer = nn.Linear(10, 3)\n\n    def forward(self, x):\n        x = torch.relu(self.input_layer(x))\n        x = self.output_layer(x)\n        return x\n\n# Training function\ndef train_one_epoch(model, data_loader, optimiser, criterion):\n    model.train()\n    for inputs, targets in data_loader:\n        optimiser.zero_grad()\n        preds = model(inputs)\n        loss = criterion(preds, targets)\n        loss.backward()\n        optimiser.step()\n\n# Validation function\ndef validate(model, data_loader, criterion):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for inputs, targets in data_loader:\n            preds = model(inputs)\n            loss = criterion(preds, targets)\n            total_loss += loss.item()\n    return total_loss / len(data_loader)\n\n# Main Training Function with Early Stopping\ndef train_model(model, train_loader, val_loader, optimiser, criterion, epochs, patience):\n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n    train_losses = []\n    val_losses = []\n    early_stop = 0\n\n    for epoch in range(epochs):\n        train_loss = 0\n        for inputs, targets in train_loader:\n            optimiser.zero_grad()\n            preds = model(inputs)\n            loss = criterion(preds, targets)\n            loss.backward()\n            optimiser.step()\n            train_loss += loss.item()\n        train_loss /= len(train_loader)\n        train_losses.append(train_loss)\n\n        # Get the validation dataset loss\n        val_loss = validate(model, val_loader, criterion)\n        val_losses.append(val_loss)\n\n        # Early stopping check\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        if epochs_no_improve == patience:\n            early_stop = epoch + 1\n            break\n\n    # Plot the early stopping\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=list(range(1, epochs)), y=train_losses, mode='lines', name='Training Loss'))\n    fig.add_trace(go.Scatter(x=list(range(1, epochs)), y=val_losses, mode='lines', name='Validation Loss'))\n\n    if early_stop:\n        fig.add_vline(x=early_stop, line_width=3, line_dash=\"dash\", line_color=\"red\")\n        fig.add_annotation(x=early_stop, y=max(max(train_losses), max(val_losses)),\n                           text=\"Early Stopping\", showarrow=True, arrowhead=1, ax=-50, ay=-100)\n\n    fig.update_layout(title='Early Stopping Example', xaxis_title='Epoch', yaxis_title='Loss', template='plotly_white',\n                      width=900, height=600, font=dict(size=18), xaxis=dict(tickfont=dict(size=16)),\n                      yaxis=dict(tickfont=dict(size=16)), title_font_size=24)\n    fig.show()\n\n    return train_losses, val_losses\n\n# Load and split the data\niris = load_iris()\nX = iris.data\ny = iris.target\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Normalise the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\n\n# Convert the data into PyTorch Tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.long)\n\n# Load the data into PyTorch DataLoaders to allow mini-batch training\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\nval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8)\n\n# Model initialisation\nmodel = Model()\noptimiser = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Train and visualise results\ntrain_losses, val_losses = train_model(model, train_loader, val_loader, optimiser, criterion, epochs=800, patience=10)\n```", "```py\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import relu\n\nclass NerualNet(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n        super(NerualNet, self).__init__()\n        self.input_layer = nn.Linear(input_size, hidden_size)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.output_layer = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = relu(self.input_layer(x))\n        x = self.dropout(x)\n        x = self.output_layer(x)\n        return x\n\n# Example: Network with 100 input features, 10 hidden units\n#          2 output classes, and 20% dropout rate\nmodel = NerualNet(100, 10, 2, 0.2)\n```"]