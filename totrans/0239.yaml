- en: A step-by-step guide to robust ML classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-step-by-step-guide-to-robust-ml-classification-5ce83592eb1d](https://towardsdatascience.com/a-step-by-step-guide-to-robust-ml-classification-5ce83592eb1d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to avoid common pitfalls and dig deeper into our models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ryancburke8.medium.com/?source=post_page-----5ce83592eb1d--------------------------------)[![Ryan
    Burke](../Images/4a6c1ac506da2456406afe46bae8e732.png)](https://ryancburke8.medium.com/?source=post_page-----5ce83592eb1d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ce83592eb1d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ce83592eb1d--------------------------------)
    [Ryan Burke](https://ryancburke8.medium.com/?source=post_page-----5ce83592eb1d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ce83592eb1d--------------------------------)
    ·17 min read·Mar 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1856f0dfbd79cecda67d8bfcb71783b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Luca Bravo](https://unsplash.com/@lucabravo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/wallpapers/nature/forest?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: In previous articles, I focused mainly on presenting individual algorithms that
    I found interesting. Here, I walk through a complete ML classification project.
    The goal is to touch on some of the common pitfalls in ML projects and describe
    to the readers how to avoid them. I will also demonstrate how we can go further
    by analysing our model errors to gain important insights that normally go unseen.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to see the whole notebook, please check it out → [here](https://github.com/ryancburke/forest_cover)
    ←
  prefs: []
  type: TYPE_NORMAL
- en: Libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Below, you will find a list of the libraries I used for today’s analyses. They
    consist of the standard data science toolkit along with the necessary sklearn
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today’s dataset includes the forest cover data that is ready-to-employ with
    sklearn. Here’s a description from sklearn’s site.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Set Characteristics:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The samples in this dataset correspond to 30×30m patches of forest in the US,
    collected for the task of predicting each patch’s cover type, i.e. the dominant
    species of tree. There are seven cover types, making this a multi-class classification
    problem. Each sample has 54 features, described on the dataset’s homepage. Some
    of the features are boolean indicators, while others are discrete or continuous
    measurements.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Number of Instances: 581 012'
  prefs: []
  type: TYPE_NORMAL
- en: Feature information (Name / Data Type / Measurement / Description**)**
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Elevation / quantitative /meters / Elevation in meters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aspect / quantitative / azimuth / Aspect in degrees azimuth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slope / quantitative / degrees / Slope in degrees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal_Distance_To_Hydrology / quantitative / meters / Horz Dist to nearest
    surface water features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical_Distance_To_Hydrology / quantitative / meters / Vert Dist to nearest
    surface water features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal_Distance_To_Roadways / quantitative / meters / Horz Dist to nearest
    roadway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hillshade_9am / quantitative / 0 to 255 index / Hillshade index at 9am, summer
    solstice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hillshade_Noon / quantitative / 0 to 255 index / Hillshade index at noon, summer
    soltice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hillshade_3pm / quantitative / 0 to 255 index / Hillshade index at 3pm, summer
    solstice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal_Distance_To_Fire_Points / quantitative / meters / Horz Dist to nearest
    wildfire ignition points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wilderness_Area (4 binary columns) / qualitative / 0 (absence) or 1 (presence)
    / Wilderness area designation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soil_Type (40 binary columns) / qualitative / 0 (absence) or 1 (presence) /
    Soil Type designation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of classes:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cover_Type (7 types) / integer / 1 to 7 / Forest Cover Type designation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here’s a simple function to load this data into your notebook as a dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using df.info() and df.describe() to get to know our data better, we see that
    there are no missing data and it consists of quantitative variables. The dataset
    is also rather large (> 580 000 rows). I originally tried to run this on the entire
    dataset, but it took FOREVER, so I recommend using a fraction of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the target variable, which is the forest cover class, using df.target.value_counts(),
    we see the following distribution (in descending order):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Class 2 = 283,301'
  prefs: []
  type: TYPE_NORMAL
- en: Class 1 = 211,840
  prefs: []
  type: TYPE_NORMAL
- en: Class 3 = 35,754
  prefs: []
  type: TYPE_NORMAL
- en: Class 7 = 20,510
  prefs: []
  type: TYPE_NORMAL
- en: Class 6 = 17,367
  prefs: []
  type: TYPE_NORMAL
- en: Class 5 = 9,493
  prefs: []
  type: TYPE_NORMAL
- en: Class 4 = 2,747**
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that our classes are imbalanced and we will need to
    keep this in mind when selecting a metric to evaluate our models.
  prefs: []
  type: TYPE_NORMAL
- en: Prepare your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common misunderstandings when running ML models is processing
    our data prior to splitting. Why is this a problem?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we plan on scaling our data using the whole dataset. The equations
    below are taken from their respective links.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ex1** [**StandardScaler()**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)'
  prefs: []
  type: TYPE_NORMAL
- en: z = (x — u) / s
  prefs: []
  type: TYPE_NORMAL
- en: '**Ex2** [**MinMaxScaler()**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)'
  prefs: []
  type: TYPE_NORMAL
- en: X_std = (X - X.min()) / (X.max() - X.min())
  prefs: []
  type: TYPE_NORMAL
- en: X_scaled = X_std * (max - min) + min
  prefs: []
  type: TYPE_NORMAL
- en: The most important thing we should notice is they include information such as
    mean, standard deviation, min, max. If we perform these functions prior to splitting,
    the features in our train set will be computed based on information included in
    the test set. This is an example of [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/).
  prefs: []
  type: TYPE_NORMAL
- en: Data leakage is when information from outside the training dataset is used to
    create the model. This additional information can allow the model to learn or
    know something that it otherwise would not know and in turn invalidate the estimated
    performance of the mode being constructed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Therefore, the first step after getting to know our dataset is to split it and
    keep your test set ***unseen*** until the very end. In the code below, we split
    the data into 80% (training set) and 20% (test set). You will also note that I
    have only kept 50,000 total samples to reduce the time it takes to train & evaluate
    our models. Trust me, you will thank me later!
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth noting that we are stratifying on the target variable. This
    is good practice for imbalanced datasets as it maintains the distribution of classes
    in the train and test set. If we don’t do this, there’s a chance that some of
    the underrepresented classes aren’t even present in our train or test sets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our train and test sets ready, we can now work on the fun stuff. The first
    step in this project is to generate some features that could add useful information
    to train our models.
  prefs: []
  type: TYPE_NORMAL
- en: This step can be a little tricky. In the real world, this requires domain-specific
    knowledge on the particular subject you are working. To be completely transparent
    with you, despite being a lover of nature and everything outdoors, I am no expert
    in why certain trees grow in specific areas.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, I have consulted [[1](https://www.kaggle.com/code/skillsmuggler/forest-cover-feature-engineering-and-reduction/notebook#Feature-reduction)]
    [[2](https://www.kaggle.com/code/codename007/forest-cover-type-eda-baseline-model/notebook)]
    [[3](https://www.kaggle.com/code/kwabenantim/forest-cover-feature-engineering#Adding-new-features)]
    who have a better understanding of this domain than myself. I have amalgamated
    the knowledge from these references to create the features you will find below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: On a side note, when you are working with large datasets, pandas can be somewhat
    slow. Using [swifter](https://pypi.org/project/swifter/), as you can see in the
    last two lines above, you can significantly speed up the time it takes to apply
    a function to your dataframe. The article → [here](/speed-up-your-pandas-processing-with-swifter-6aa314600a13)
    compares several methods used to speed this process up.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point we have more than 70 features. If the goal is end up with the
    best performing model, then you could try to use all of these as inputs. With
    that said, often in business there is a trade-off between performance and complexity
    that needs to be considered.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, suppose we have 94% accuracy in our model using all of these
    features. Then, imagine we have 89% accuracy with only 4 features. What is the
    cost we are willing to pay for a more interpretable model. Always weigh performance
    and complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping that in mind, I will perform feature selection to try and reduce the
    complexity right away. [Sklearn](https://scikit-learn.org/stable/modules/feature_selection.html)
    provides many options worth considering. In this example, I will use [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest)
    which will select a pre-specified number of features that provide the best performance.
    Below, I have requested (and listed) the best performing 15 features. These are
    the features that I will use to train the models in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Baseline models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section I will compare three different classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[KNeighboursClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ExtraTreesClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have provided links for those who wish to investigate each model further.
    They will also be helpful in the section on hyperparameter tuning, where you can
    find all modifiable parameters when trying to improve your models. Below you will
    find two functions to define and evaluate the baseline models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There are some key elements in the second function that are worth discussing
    further. The first of which is [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html).
    Recall, we split the original dataset into 80% training and 20% test. The test
    set will be reserved for the final evaluation of our top performing model.
  prefs: []
  type: TYPE_NORMAL
- en: Using cross-validation will provide us with a better evaluation of our models.
    Specifically, I have set up a 10-fold cross-validation. For those not familiar,
    the model is trained on k — 1 folds and is validated on the remaining fold at
    each step. At the end you will have access to an average and variation of the
    k models, providing you with better insight than a simple train-test evaluation.
    Stratified K fold, as I eluded to earlier, is used to ensure that each fold has
    an approximately equal representation of the target classes.
  prefs: []
  type: TYPE_NORMAL
- en: The second point worth discussing is the scoring metric. There are many [metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)
    available to evaluate the performance of your models, and often there are several
    that could suit your project. It’s important to keep in mind what you are trying
    to demonstrate with the results. If you work in a business setting, often the
    metric that is most easily explained to those without a data background is preferred.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, there are metrics that are unsuitable to your analyses. For
    this project, we have imbalanced classes. If you go to the link provided above,
    you will find options for this case. I opted to use the weighted F1 score. Let’s
    briefly discuss why I chose this metric.
  prefs: []
  type: TYPE_NORMAL
- en: A very common classification metric is accuracy, which is the percent of correct
    classifications. While this may seem like an excellent option, suppose we have
    a binary classification where the target classes are uneven (i.e. group 1 = 90,
    group 2 = 10). It is possible to have 90% accuracy, which is great, but if we
    explore further, we have correctly classified all of group 1 and failed to classify
    any of the group 2\. In this case our model is not terribly informative.
  prefs: []
  type: TYPE_NORMAL
- en: If we would have used the weighted F1 score we would have a result of 42.6%.
    If you’re interested in reading more on the F1 score → [here](/the-f1-score-bec2bbc38aa6)
    is an article explaining how it is calculated.
  prefs: []
  type: TYPE_NORMAL
- en: After training the baseline models, I have plotted the results from each below.
    The baseline models all performed relatively well. Remember, at this point I have
    done nothing to the data (i.e. transform, remove outliers). The Extra trees classifier
    had the highest weighted F1 score at 86.9%.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5d1439a88b1bb065aadfcaa8202ffdb.png)'
  prefs: []
  type: TYPE_IMG
- en: Results from the 10-fold CV. KNN had the lowest at 78.8%, the RF was second
    with 85.9%, and the ET had the highest weighted F1 score at 86.9%. *Image provided
    by author*
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step in this project will look at the effects of data transformation
    on model performance. Whereas many decision tree-based algorithms are not sensitive
    to the magnitude of the data, it is reasonable to expect that models measuring
    distance between samples , such as the KNN perform differently when scaled [[4](https://ai.stackexchange.com/questions/22307/why-are-decision-trees-and-random-forests-scale-invariant#:~:text=Decision%20Tree%20and%20Random%20Forest,work%20fine%20without%20feature%20scaling.)]
    [[5](https://stats.stackexchange.com/questions/244507/what-algorithms-need-feature-scaling-beside-from-svm)].
    In this section, we will scale our data using StandardScaler and MinMaxScaler
    as described above. Below you will find a function that describes a pipeline that
    will apply the scaler and then train the model using scaled data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The results using the StandardScaler are presented below. We see that our hypothesis
    regarding scaling the data appears to hold. Both the random forest and extra trees
    classifiers both performed nearly identically, whereas the KNN improved in performance
    by roughly 4%. Despite this increase, the two tree-based classifiers still outperform
    the scaled KNN.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f929c80f19245ca85081e472bacfb1db.png)'
  prefs: []
  type: TYPE_IMG
- en: Results from the 10-fold CV using the StandardScaler to transform our data.
    KNN still had the lowest although the performance increased to 83.8%. The RF was
    second with 85.8%, and the ET once again had the highest weighted F1 score at
    86.8%. *Image provided by author*
  prefs: []
  type: TYPE_NORMAL
- en: Similar results can be seen when the MinMaxScaler is used. The results from
    all models are almost identical to those presented using the StandardScaler.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75dba5daf7061c01f44fa4fd86846478.png)'
  prefs: []
  type: TYPE_IMG
- en: Results from the 10-fold CV using the MinMaxScaler to transform our data. Each
    performed almost identically to those using StandardScaler. KNN still had the
    lowest at 83.9%. The RF was second with 86.0%, and the ET once again had the highest
    weighted F1 score at 87.0%. *Image provided by author*
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting at this point that I also checked the effect of removing
    outliers. For this, I removed values that were beyond +/- 3 SD for each feature.
    I am not presenting the results here because there were no values outside this
    range. If you are interested in seeing how this was performed, please feel free
    to check out the notebook found at the link provided at the beginning of this
    article.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning using GridSearchCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step is to try and improve our models by tuning the hyperparameters.
    We will do so on the scaled data because it had the best average performance when
    considering our three models. Sklearn discusses this in more detail → [here](https://scikit-learn.org/stable/modules/grid_search.html).
  prefs: []
  type: TYPE_NORMAL
- en: I chose to use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
    (CV for cross validated). Below you will find a function that performs a 10-fold
    cross validation on the models we have been using. The one additional detail here
    is that we need to provide the list of hyperparameters we want to be evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, we have not even looked at our test set. Before commencing
    the grid search, we will scale our train and test data using the StandardScaler.
    We are doing this here because we are going to find the best hyperparameters for
    each model and use those as inputs into a VotingClassifier (as we will discuss
    in the next section).
  prefs: []
  type: TYPE_NORMAL
- en: To properly scale our full dataset we have to follow the procedure below. You
    will see that the scaler is only fit on the training data. Both the training and
    test set are transformed based on the scaling parameters found with the training
    set, thus eliminating any chance of data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Next, I have provided the grid search parameters that were tested for each of
    the models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Voting ensemble classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have determined the best combination of parameters to optimise our models.
    These parameters will be used as the inputs into a [VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html),
    which is an ensemble estimator that trains several models and then aggregates
    the findings for a more robust prediction. I found this → [article](/use-voting-classifier-to-improve-the-performance-of-your-ml-model-805345f9de0e)
    which provides a detailed overview of the voting classifier and the different
    ways to use it.
  prefs: []
  type: TYPE_NORMAL
- en: The best parameters for each model are listed below. The output from the voting
    classifer shows that we achieved a weighted F1 score of 87.5% on the training
    set and 88.4% on the test set. Not bad!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Analysing the errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The performance of our model is pretty good. With that said, it can be very
    insightful to investigate where the model failed. Below, you will find the code
    to generate a confusion matrix. Let’s see if we can learn something.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d5369074906634124bff8df76e7c9c89.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix on the test set. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Right away, it becomes quite evident that the underrepresented classes are not
    learned very well. This is so important because despite using a metric that is
    appropriate to evaluate imbalanced classes, you can’t make a model learn something
    that isn’t there.
  prefs: []
  type: TYPE_NORMAL
- en: To analyse our errors, we could create visualisations; however, with 15 features
    and 7 classes this can start to feel like one of those trippy stereogram images
    that you stare at until an image forms. An alternative approach is the following.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning classification of errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section I am going to compare the predicted values to the ground truth
    in our test set and create a new variable, ‘*error’*. Below, I am setting up a
    dataset to be used in a binary classification analysis, where the target is error
    vs. no error using the same features as above.
  prefs: []
  type: TYPE_NORMAL
- en: Since we already know that the underrepresented classes were not well learned,
    the goal here is to see which features were most associated with errors independent
    of class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: With our new dataset, the next step is to build a classification model. This
    time we are going to add a step using [SHAP](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html).
    This will allow us to understand how each feature impacts the model, which in
    our case is error.
  prefs: []
  type: TYPE_NORMAL
- en: Below, we have used a Random Forest to fit the data. Once again we are using
    K-fold cross-validation to give us a better estimate of the contribution of each
    feature. At the bottom, I have generated a dataframe with the average, standard
    deviation, and maximum shap values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: For a better visual experience, below is a shap summary plot. On the left hand
    side we have the feature names. The plot demonstrates the impact of each feature
    on the model for different values of that feature. Whereas the dispersion (how
    far to the right or left) describes the overall impact of a feature on the model,
    the colouring provides us with a little extra information.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca2d8b4f786111e9c1a89c5ec21d74f0.png)'
  prefs: []
  type: TYPE_IMG
- en: SHAP summary plot for the error classification model. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we notice is that the features with the greatest impact on the
    model relate more to distance features (i.e. to water, road, or fire ignition
    points) than to the type of forest (wilderness area) or soil type.
  prefs: []
  type: TYPE_NORMAL
- en: Next, when we look at the colour distribution, we see a more clear differentiation
    of high vs low values for the first feature Hydro_Road_Fire_mean than the rest.
    The same might be said for Road_Fire_mean, albeit to a lesser degree.
  prefs: []
  type: TYPE_NORMAL
- en: 'To interpret what this means, we can formulate a statement like the following
    : *When the average distance to water, fire ignition points and road is low, there
    is a more likely chance of making an error.*'
  prefs: []
  type: TYPE_NORMAL
- en: Once again, I must insist that my forestry ‘*expertise’* is limited to a couple
    of weeks. I did do some research to help me interpret what this could mean and
    came across a couple of articles [[6](https://www.researchgate.net/publication/352180034_Application_of_remote_sensing_and_machine_learning_algorithms_for_forest_fire_mapping_in_a_Mediterranean_area/figures?lo=1)]
    [[7](https://www.researchgate.net/publication/233978116_GIS-grid-based_and_multi-criteria_analysis_for_identifying_and_mapping_peat_swamp_forest_fire_hazard_in_Pahang_Malaysia/figures?lo=1)]
    that suggest the distance to the road is a significant factor in the risk of forest
    fires.
  prefs: []
  type: TYPE_NORMAL
- en: This leads me to hypothesise that forest fire may be a significant factor influencing
    the errors made on our dataset. It seems logical to me that areas impacted by
    fire would have a very different representation of forest diversity to those unaffected
    by fire. I’m sure someone with more experience could let me know if that makes
    sense :)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today, we went through a step-by-step ML multi-classification problem. We touched
    on some important considerations when conducting these analyses, namely the importance
    of splitting the dataset before we start to manipulate it. This is one of the
    most common pitfalls in ML projects that can lead to serious issues limiting our
    ability to generalise our findings.
  prefs: []
  type: TYPE_NORMAL
- en: We also touched on the importance of selecting an appropriate metric to evaluate
    our models. Here, we used the weighted F1 score, which was appropriate for imbalanced
    classes. Despite this, we still saw that the underrepresented classes were not
    well learned.
  prefs: []
  type: TYPE_NORMAL
- en: In my notebook, I also included a section on oversampling to create balanced
    classes using [ADASYN](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.ADASYN.html),
    which is a variation of [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html).
    To save you the suspense, upsampling significantly improved the results on the
    training set (but not the test set).
  prefs: []
  type: TYPE_NORMAL
- en: This leads us to the error analysis, which is an important part of any ML project.
    A binary error classification was performed and may suggest that forest fires
    were implicated in many of the model errors. This could also explain, to a certain
    extent, why upsampling didn’t improve our final model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I want to thank you all for taking the time to read this article! I
    hope some of you found it to be helpful :)
  prefs: []
  type: TYPE_NORMAL
