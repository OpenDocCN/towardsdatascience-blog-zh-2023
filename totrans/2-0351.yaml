- en: 'Back To Basics, Part Dos: Gradient Descent'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºç¡€å›é¡¾ï¼Œç¬¬äºŒéƒ¨åˆ†ï¼šæ¢¯åº¦ä¸‹é™
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd](https://towardsdatascience.com/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd](https://towardsdatascience.com/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd)
- en: '[](https://medium.com/@shreya.rao?source=post_page-----e3d7d05c56fd--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page-----e3d7d05c56fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e3d7d05c56fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e3d7d05c56fd--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----e3d7d05c56fd--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@shreya.rao?source=post_page-----e3d7d05c56fd--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page-----e3d7d05c56fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e3d7d05c56fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e3d7d05c56fd--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----e3d7d05c56fd--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e3d7d05c56fd--------------------------------)
    Â·11 min readÂ·Feb 4, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e3d7d05c56fd--------------------------------)
    Â·é˜…è¯»æ—¶é—´11åˆ†é’ŸÂ·2023å¹´2æœˆ4æ—¥
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Welcome to the second part of our ***Back To Basics*** series. In the [first
    part](https://medium.com/towards-data-science/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46),
    we covered how to use Linear Regression and Cost Function to find the best-fitting
    line for our house prices data. However, we also saw that testing multiple *intercept*
    values can be tedious and inefficient. In this second part, weâ€™ll delve deeper
    into Gradient Descent, a powerful technique that can help us find the perfect
    *intercept* and optimize our model. Weâ€™ll explore the math behind it and see how
    it can be applied to our linear regression problem.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬¢è¿æ¥åˆ°æˆ‘ä»¬çš„***åŸºç¡€å›é¡¾***ç³»åˆ—çš„ç¬¬äºŒéƒ¨åˆ†ã€‚åœ¨ [ç¬¬ä¸€éƒ¨åˆ†](https://medium.com/towards-data-science/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46)ä¸­ï¼Œæˆ‘ä»¬è®²è§£äº†å¦‚ä½•ä½¿ç”¨çº¿æ€§å›å½’å’Œæˆæœ¬å‡½æ•°æ¥ä¸ºæˆ‘ä»¬çš„æˆ¿ä»·æ•°æ®æ‰¾åˆ°æœ€ä½³æ‹Ÿåˆçº¿ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿçœ‹åˆ°æµ‹è¯•å¤šä¸ª*æˆªè·*å€¼å¯èƒ½æ—¢ç¹çåˆä½æ•ˆã€‚åœ¨ç¬¬äºŒéƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨æ¢¯åº¦ä¸‹é™ï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ‰¾åˆ°å®Œç¾çš„*æˆªè·*å¹¶ä¼˜åŒ–æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬å°†æ¢è®¨å…¶èƒŒåçš„æ•°å­¦åŸç†ï¼Œå¹¶çœ‹çœ‹å®ƒå¦‚ä½•åº”ç”¨äºæˆ‘ä»¬çš„çº¿æ€§å›å½’é—®é¢˜ã€‚
- en: Gradient descent is a powerful optimization algorithm that ***aims to quickly
    and efficiently find the minimum point of a curve.*** The best way to visualize
    this process is to imagine you are standing at the top of a hill, with a treasure
    chest filled with gold waiting for you in the valley.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§å¼ºå¤§çš„ä¼˜åŒ–ç®—æ³•ï¼Œå®ƒ***æ—¨åœ¨å¿«é€Ÿé«˜æ•ˆåœ°æ‰¾åˆ°æ›²çº¿çš„æœ€å°ç‚¹***ã€‚æœ€å¥½çš„å¯è§†åŒ–æ–¹å¼æ˜¯æƒ³è±¡ä½ ç«™åœ¨å±±é¡¶ï¼Œå±±è°·é‡Œæœ‰ä¸€ä¸ªè£…æ»¡é‡‘å¸çš„å®ç®±ç­‰ç€ä½ ã€‚
- en: '![](../Images/31950f4c1265a42f3cdc94e121c9c121.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31950f4c1265a42f3cdc94e121c9c121.png)'
- en: However, the exact location of the valley is unknown because itâ€™s super dark
    out and you canâ€™t see anything. Moreover, you want to reach the valley before
    anyone else does (because you want all of the treasure for yourself duh). Gradient
    descent helps you navigate the terrain and reach this *optimal* point ***efficiently
    and quickly***. At each point, itâ€™ll tell you how many steps to take and in what
    direction you need to take them.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå±±è°·çš„ç¡®åˆ‡ä½ç½®æ˜¯æœªçŸ¥çš„ï¼Œå› ä¸ºå¤–é¢éå¸¸é»‘æš—ï¼Œä½ ä»€ä¹ˆä¹Ÿçœ‹ä¸è§ã€‚æ­¤å¤–ï¼Œä½ å¸Œæœ›åœ¨å…¶ä»–äººä¹‹å‰åˆ°è¾¾å±±è°·ï¼ˆå› ä¸ºä½ æƒ³ç‹¬å æ‰€æœ‰çš„å®è—ï¼‰ã€‚æ¢¯åº¦ä¸‹é™å¸®åŠ©ä½ å¯¼èˆªåœ°å½¢ï¼Œå¹¶***é«˜æ•ˆè€Œè¿…é€Ÿåœ°***åˆ°è¾¾è¿™ä¸ª*æœ€ä½³*ç‚¹ã€‚åœ¨æ¯ä¸ªç‚¹ï¼Œå®ƒä¼šå‘Šè¯‰ä½ è¯¥èµ°å¤šå°‘æ­¥ä»¥åŠéœ€è¦æœå“ªä¸ªæ–¹å‘å‰è¿›ã€‚
- en: 'Similarly, gradient descent can be applied to our linear regression problem
    by using the steps laid out by the algorithm. To visualize the process of finding
    the minimum, letâ€™s plot the **MSE** curve. We already know that the equation of
    the curve is:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œé€šè¿‡ä½¿ç”¨ç®—æ³•åˆ¶å®šçš„æ­¥éª¤ï¼Œæ¢¯åº¦ä¸‹é™å¯ä»¥åº”ç”¨åˆ°æˆ‘ä»¬çš„çº¿æ€§å›å½’é—®é¢˜ä¸­ã€‚ä¸ºäº†å¯è§†åŒ–æ‰¾åˆ°æœ€å°å€¼çš„è¿‡ç¨‹ï¼Œè®©æˆ‘ä»¬ç»˜åˆ¶**MSE**æ›²çº¿ã€‚æˆ‘ä»¬å·²ç»çŸ¥é“æ›²çº¿çš„æ–¹ç¨‹æ˜¯ï¼š
- en: '![](../Images/87267eb6c972fc8f4d5a7b12866e05ba.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87267eb6c972fc8f4d5a7b12866e05ba.png)'
- en: the equation of the curve is the equation used to calculate the MSE
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ›²çº¿æ–¹ç¨‹æ˜¯ç”¨äºè®¡ç®—å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰çš„æ–¹ç¨‹
- en: 'And from the [previous article](/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46#e9d3),
    we know that the equation of **MSE** in our problem is:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä»[ä¸Šä¸€ç¯‡æ–‡ç« ](/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46#e9d3)ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“æˆ‘ä»¬é—®é¢˜ä¸­çš„**MSE**æ–¹ç¨‹æ˜¯ï¼š
- en: '![](../Images/d6eceaa5e7b3d066189f7a1ac2e4866d.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6eceaa5e7b3d066189f7a1ac2e4866d.png)'
- en: 'If we zoom out we can see that an **MSE** curve (which resembles our valley)
    can be found by substituting a bunch of *intercept* values in the above equation.
    So letâ€™s plug in 10,000 values of the *intercept*, to get a curve that looks like
    this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ”¾å¤§ä¸€ç‚¹ï¼Œå°±ä¼šçœ‹åˆ°ä¸€ä¸ª**MSE**æ›²çº¿ï¼ˆç±»ä¼¼äºæˆ‘ä»¬çš„è°·åº•ï¼‰å¯ä»¥é€šè¿‡å°†ä¸€å †*æˆªè·*å€¼ä»£å…¥ä¸Šè¿°æ–¹ç¨‹æ¥æ‰¾åˆ°ã€‚æ‰€ä»¥è®©æˆ‘ä»¬ä»£å…¥ 10,000 ä¸ª*æˆªè·*å€¼ï¼Œå¾—åˆ°å¦‚ä¸‹æ›²çº¿ï¼š
- en: '![](../Images/35693cf8ddf5baac7d203b1097c6fec7.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35693cf8ddf5baac7d203b1097c6fec7.png)'
- en: in reality, we wonâ€™t know what the MSE curve looks like
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“ MSE æ›²çº¿çš„æ ·å­
- en: 'The goal is to reach the bottom of this **MSE** curve, which we can do by following
    these steps:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯è¾¾åˆ°è¿™ä¸ª**MSE**æ›²çº¿çš„åº•éƒ¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°ï¼š
- en: 'Step 1: Start with a random initial guess for the intercept value'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 1ï¼šä»ä¸€ä¸ªéšæœºçš„æˆªè·å€¼åˆå§‹çŒœæµ‹å¼€å§‹
- en: In this case, letâ€™s assume our initial guess for the *intercept* value is 0.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‡è®¾æˆ‘ä»¬å¯¹*æˆªè·*å€¼çš„åˆå§‹çŒœæµ‹æ˜¯ 0ã€‚
- en: 'Step 2: Calculate the gradient of the MSE curve at this point'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 2ï¼šè®¡ç®—æ­¤ç‚¹ MSE æ›²çº¿çš„æ¢¯åº¦
- en: The *gradient* of a curve at a point is represented by the tangent line (a fancy
    way of saying that the line touches the curve only at that point) at that point.
    For example, at Point A, the *gradient* of the **MSE**curve can be represented
    by the red tangent line, when the intercept is equal to 0.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ›²çº¿åœ¨æŸä¸€ç‚¹çš„*æ¢¯åº¦*ç”±è¯¥ç‚¹çš„åˆ‡çº¿è¡¨ç¤ºï¼ˆè¿™æ˜¯è¯´è¯¥ç›´çº¿ä»…åœ¨è¯¥ç‚¹æ¥è§¦æ›²çº¿çš„ä¸€ç§æ–¹å¼ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨ç‚¹ Aï¼Œå½“æˆªè·ç­‰äº 0 æ—¶ï¼Œ**MSE**æ›²çº¿çš„*æ¢¯åº¦*å¯ä»¥ç”±çº¢è‰²åˆ‡çº¿è¡¨ç¤ºã€‚
- en: '![](../Images/fb9166b4caf089cf0c697c648b9dd34c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb9166b4caf089cf0c697c648b9dd34c.png)'
- en: the gradient of the MSE curve when intercept = 0
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆªè· = 0 æ—¶ï¼ŒMSE æ›²çº¿çš„æ¢¯åº¦
- en: 'In order to determine the value of the *gradient*, we apply our knowledge of
    calculus. Specifically, the *gradient* is equal to the derivative of the curve
    with respect to the *intercept* at a given point. This is denoted as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®å®š*æ¢¯åº¦*çš„å€¼ï¼Œæˆ‘ä»¬è¿ç”¨å¾®ç§¯åˆ†çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œ*æ¢¯åº¦*ç­‰äºæ›²çº¿å¯¹*æˆªè·*çš„å¯¼æ•°ï¼Œè¿™åœ¨ç»™å®šç‚¹ä¸Šè¡¨ç¤ºä¸ºï¼š
- en: '![](../Images/7c5e380599fa8a3a18934636beb7952d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c5e380599fa8a3a18934636beb7952d.png)'
- en: 'NOTE: If youâ€™re unfamiliar with derivatives, I recommend watching this [Khan
    Academy video](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-6a/v/derivative-properties-and-polynomial-derivatives)
    if interested. Otherwise you can gloss over the next part and still be able to
    follow the rest of the article.'
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå¦‚æœä½ ä¸ç†Ÿæ‚‰å¯¼æ•°ï¼Œæˆ‘å»ºè®®è§‚çœ‹è¿™ä¸ª[Khan Academy è§†é¢‘](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-6a/v/derivative-properties-and-polynomial-derivatives)ã€‚å¦åˆ™ï¼Œä½ å¯ä»¥ç•¥è¿‡ä¸‹ä¸€éƒ¨åˆ†ï¼Œä»ç„¶èƒ½å¤Ÿè·Ÿéšæ–‡ç« çš„å…¶ä½™å†…å®¹ã€‚
- en: 'We calculate the ***derivative of the MSE curve***as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—***MSE æ›²çº¿çš„å¯¼æ•°***å¦‚ä¸‹ï¼š
- en: '![](../Images/4d20f8c99a090c3d0064f8278a109110.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d20f8c99a090c3d0064f8278a109110.png)'
- en: 'Now to find the ***gradient at point A***, we substitute the value of the *intercept*
    at point A in the above equation. Since *intercept* = 0, the derivative at point
    A is:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä¸ºäº†æ‰¾åˆ°***ç‚¹ A å¤„çš„æ¢¯åº¦***ï¼Œæˆ‘ä»¬å°†ç‚¹ A å¤„çš„*æˆªè·*å€¼ä»£å…¥ä¸Šè¿°æ–¹ç¨‹ä¸­ã€‚ç”±äº*æˆªè·* = 0ï¼Œç‚¹ A å¤„çš„å¯¼æ•°ä¸ºï¼š
- en: '![](../Images/4396d75c2117cd2749a7064d047acc98.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4396d75c2117cd2749a7064d047acc98.png)'
- en: So when the *intercept* = 0, the *gradient* = -190
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å½“*æˆªè·* = 0 æ—¶ï¼Œ*æ¢¯åº¦* = -190
- en: '**NOTE:** As we approach the optimal value, the gradient values approach zero.
    At the optimal value, the gradient is equal to zero. Conversely, the farther away
    we are from the optimal value, the larger the gradient becomes.'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** å½“æˆ‘ä»¬æ¥è¿‘æœ€ä¼˜å€¼æ—¶ï¼Œæ¢¯åº¦å€¼æ¥è¿‘é›¶ã€‚åœ¨æœ€ä¼˜å€¼å¤„ï¼Œæ¢¯åº¦ç­‰äºé›¶ã€‚ç›¸åï¼Œå½“æˆ‘ä»¬ç¦»æœ€ä¼˜å€¼è¶Šè¿œï¼Œæ¢¯åº¦å°±è¶Šå¤§ã€‚'
- en: '![](../Images/53a9ed2d96545e7594f89580a1562d4b.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53a9ed2d96545e7594f89580a1562d4b.png)'
- en: From this, we can infer that the step size should be related to the *gradient*
    since it tells us if we should take a baby step or a big step. This means that
    when the *gradient* of the curve is close to 0, then we should take baby steps
    because we are close to the optimal value. And if the *gradient* is bigger, we
    should take bigger steps to get to the optimal value faster.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸­æˆ‘ä»¬å¯ä»¥æ¨æ–­æ­¥é•¿åº”ä¸*æ¢¯åº¦*ç›¸å…³ï¼Œå› ä¸ºå®ƒå‘Šè¯‰æˆ‘ä»¬æ˜¯é‡‡å–å°æ­¥è¿˜æ˜¯å¤§æ­¥ã€‚è¿™æ„å‘³ç€ï¼Œå½“æ›²çº¿çš„*æ¢¯åº¦*æ¥è¿‘ 0 æ—¶ï¼Œæˆ‘ä»¬åº”é‡‡å–å°æ­¥ï¼Œå› ä¸ºæˆ‘ä»¬æ¥è¿‘æœ€ä¼˜å€¼ã€‚å¦‚æœ*æ¢¯åº¦*è¾ƒå¤§ï¼Œåˆ™æˆ‘ä»¬åº”é‡‡å–è¾ƒå¤§çš„æ­¥ä¼ï¼Œä»¥æ›´å¿«åœ°è¾¾åˆ°æœ€ä¼˜å€¼ã€‚
- en: '**NOTE:** However, if we take a super huge step, then we could make a big jump
    and miss the optimal point. So we need to be careful.'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** å¦‚æœæˆ‘ä»¬è¿ˆå‡ºä¸€ä¸ªè¶…å¤§çš„æ­¥ä¼ï¼Œå¯èƒ½ä¼šè·³è¿‡æœ€ä½³ç‚¹ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦å°å¿ƒã€‚'
- en: '![](../Images/530f203471ffe32da5363fa119668a64.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/530f203471ffe32da5363fa119668a64.png)'
- en: 'Step 3: Calculate the Step Size using the gradient and the Learning Rate and
    update the intercept value'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 3ï¼šä½¿ç”¨æ¢¯åº¦å’Œå­¦ä¹ ç‡è®¡ç®—æ­¥é•¿ï¼Œå¹¶æ›´æ–°æˆªè·å€¼
- en: Since we see that the ***Step Size*** and *gradient* are proportional to each
    other, the *Step Size* is determined by multiplying the *gradient* by a pre-determined
    constant value called the ***Learning Rate:***
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬çœ‹åˆ° ***æ­¥é•¿*** å’Œ *æ¢¯åº¦* å½¼æ­¤æˆæ­£æ¯”ï¼Œ*æ­¥é•¿* ç”± *æ¢¯åº¦* ä¹˜ä»¥ä¸€ä¸ªé¢„å®šçš„å¸¸æ•°å€¼æ¥ç¡®å®šï¼Œè¿™ä¸ªå¸¸æ•°å€¼ç§°ä¸º ***å­¦ä¹ ç‡ï¼š***
- en: '![](../Images/4c6f90e480006770f80b6d63955ff71c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c6f90e480006770f80b6d63955ff71c.png)'
- en: The *Learning Rate* controls the magnitude of the *Step Size* and ensures that
    the step taken is not too large or too small.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*å­¦ä¹ ç‡* æ§åˆ¶ *æ­¥é•¿* çš„å¤§å°ï¼Œå¹¶ç¡®ä¿æ­¥ä¼æ—¢ä¸å¤ªå¤§ä¹Ÿä¸å¤ªå°ã€‚'
- en: In practice, the Learning Rate is usually a small positive number that is â‰¤
    0.001\. But for our problem letâ€™s set it to 0.1.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œå­¦ä¹ ç‡é€šå¸¸æ˜¯ä¸€ä¸ªå°çš„æ­£æ•°ï¼Œâ‰¤ 0.001ã€‚ä½†å¯¹äºæˆ‘ä»¬çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†å…¶è®¾ç½®ä¸º 0.1ã€‚
- en: 'So when the intercept is 0:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å½“æˆªè·ä¸º 0 æ—¶ï¼š
- en: '![](../Images/85a1486d4568655f8eac74ac0b88f057.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85a1486d4568655f8eac74ac0b88f057.png)'
- en: 'Based on the *Step Size* we calculated above, we update the *intercept* (aka
    change our current location)using any of these equivalent formulas:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæˆ‘ä»¬ä¸Šé¢è®¡ç®—çš„ *æ­¥é•¿*ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ç­‰æ•ˆå…¬å¼æ›´æ–° *æˆªè·*ï¼ˆå³æ”¹å˜æˆ‘ä»¬å½“å‰ä½ç½®ï¼‰ï¼š
- en: '![](../Images/f65719bb3785606ade584ef0ccb4f3a0.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f65719bb3785606ade584ef0ccb4f3a0.png)'
- en: To find the new *intercept* in this step, we plug in the relevant valuesâ€¦
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‰¾åˆ°è¿™ä¸€æ­¥ä¸­çš„æ–° *æˆªè·*ï¼Œæˆ‘ä»¬ä»£å…¥ç›¸å…³å€¼â€¦â€¦
- en: '![](../Images/8fe76562abcce203af813c59e5ba3399.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fe76562abcce203af813c59e5ba3399.png)'
- en: â€¦and find that the new *intercept* = 19.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: â€¦å¹¶å‘ç°æ–°çš„ *æˆªè·* = 19ã€‚
- en: Now plugging this value in the **MSE** equation, we find that the **MSE** when
    the *intercept* is 19 = 8064.095\. In one big step, we moved closer to our optimal
    value and reduced the **MSE**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å°†è¿™ä¸ªå€¼ä»£å…¥ **MSE** æ–¹ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°å½“ *æˆªè·* ä¸º 19 æ—¶ï¼Œ**MSE** = 8064.095ã€‚åœ¨ä¸€æ¬¡å¤§çš„æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬æ›´æ¥è¿‘äº†æˆ‘ä»¬çš„æœ€ä½³å€¼ï¼Œå¹¶é™ä½äº†
    **MSE**ã€‚
- en: '![](../Images/b1b82fffb290f2788fea5d3962472167.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1b82fffb290f2788fea5d3962472167.png)'
- en: 'Even if we look at our graph, we see how much better our new line with *intercept*
    19 is fitting our data than our old line with *intercept* 0:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æˆ‘ä»¬æŸ¥çœ‹å›¾è¡¨ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½çœ‹åˆ°æ–°æˆªè·ä¸º 19 çš„ç›´çº¿æ¯”æ—§æˆªè·ä¸º 0 çš„ç›´çº¿æ›´å¥½åœ°æ‹Ÿåˆäº†æ•°æ®ï¼š
- en: '![](../Images/045bf220b37575802b777d3c834da4af.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/045bf220b37575802b777d3c834da4af.png)'
- en: 'Step 4: Repeat steps 2â€“3'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 4ï¼šé‡å¤æ­¥éª¤ 2-3
- en: We repeat Steps 2 and 3 using the updated *intercept* value.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨æ›´æ–°åçš„ *æˆªè·* å€¼é‡å¤æ­¥éª¤ 2 å’Œ 3ã€‚
- en: 'For example, since the new *intercept* value in this iteration is 19, following
    [Step 2](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd#6ab4),
    we will calculate the gradient at this new point:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œç”±äºæ­¤è¿­ä»£ä¸­çš„æ–° *æˆªè·* å€¼ä¸º 19ï¼ŒæŒ‰ç…§ [æ­¥éª¤ 2](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd#6ab4)ï¼Œæˆ‘ä»¬å°†è®¡ç®—è¯¥æ–°ç‚¹çš„æ¢¯åº¦ï¼š
- en: '![](../Images/7fda83bf1af29af75d5d14427aa37f96.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fda83bf1af29af75d5d14427aa37f96.png)'
- en: And we find that the *gradient* of the **MSE** curve at the intercept value
    of 19 is -152 (as represented by the red tangent line in the illustration below).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‘ç° **MSE** æ›²çº¿åœ¨æˆªè·å€¼ 19 å¤„çš„ *æ¢¯åº¦* ä¸º -152ï¼ˆå¦‚ä¸‹å›¾ä¸­çº¢è‰²åˆ‡çº¿æ‰€ç¤ºï¼‰ã€‚
- en: '![](../Images/682e6a49f54e380cbd91d1d30fceeb63.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/682e6a49f54e380cbd91d1d30fceeb63.png)'
- en: 'Next, in accordance with [Step 3](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd#67b0),
    letâ€™s calculate the *Step Size*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼ŒæŒ‰ç…§ [æ­¥éª¤ 3](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd#67b0)ï¼Œæˆ‘ä»¬æ¥è®¡ç®—
    *æ­¥é•¿*ï¼š
- en: '![](../Images/7365eb11df5edd90de1dff199c7cd829.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7365eb11df5edd90de1dff199c7cd829.png)'
- en: 'And subsequently, update the *intercept* value:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: éšåï¼Œæ›´æ–° *æˆªè·* å€¼ï¼š
- en: '![](../Images/0d5165ff0a1326f1cc044713e5cbfd23.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d5165ff0a1326f1cc044713e5cbfd23.png)'
- en: Now we can compare the line with the previous *intercept* of 19 to the new line
    with the new intercept 34.2â€¦
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†ä¹‹å‰æˆªè·ä¸º 19 çš„ç›´çº¿ä¸æ–°çš„æˆªè·ä¸º 34.2 çš„ç›´çº¿è¿›è¡Œæ¯”è¾ƒâ€¦â€¦
- en: '![](../Images/9826e5617c32f3d17e27f6504f07f3fe.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9826e5617c32f3d17e27f6504f07f3fe.png)'
- en: â€¦and we can see that the new line fits the data better.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: â€¦å¹¶ä¸”æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ–°çš„ç›´çº¿æ›´å¥½åœ°æ‹Ÿåˆäº†æ•°æ®ã€‚
- en: Overall, the **MSE** is getting smallerâ€¦
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œ**MSE** æ­£åœ¨å˜å°â€¦â€¦
- en: '![](../Images/a0369469f4ecfeb61ed98f3ce9e610a4.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0369469f4ecfeb61ed98f3ce9e610a4.png)'
- en: 'â€¦and our *Step Sizes* are getting smaller:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: â€¦è€Œæˆ‘ä»¬çš„*æ­¥é•¿*æ­£åœ¨å˜å¾—è¶Šæ¥è¶Šå°ï¼š
- en: '![](../Images/c2c5de345da3c8464070fd8580741258.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2c5de345da3c8464070fd8580741258.png)'
- en: 'We repeat this process iteratively until we converge toward the optimal solution:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åå¤è¿›è¡Œè¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°æˆ‘ä»¬æ”¶æ•›åˆ°æœ€ä½³è§£å†³æ–¹æ¡ˆï¼š
- en: '![](../Images/901fbdaec84cf2dd3bbcab2e37c3d2ae.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/901fbdaec84cf2dd3bbcab2e37c3d2ae.png)'
- en: As we progress toward the minimum point of the curve, we observe that the *Step
    Size* becomes increasingly smaller. After 13 steps, the gradient descent algorithm
    estimates the *intercept* value to be 95\. If we had a crystal ball, this would
    be confirmed as the minimum point of the **MSE** curve. And it is clear to see
    how this method is more efficient compared to the brute force approach that we
    saw in the [previous article](https://medium.com/towards-data-science/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬å‘æ›²çº¿çš„æœ€å°ç‚¹æ¨è¿›æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°*æ­¥é•¿*å˜å¾—è¶Šæ¥è¶Šå°ã€‚åœ¨13æ­¥ä¹‹åï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•ä¼°è®¡*æˆªè·*å€¼ä¸º95ã€‚å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªæ°´æ™¶çƒï¼Œè¿™å°†è¢«ç¡®è®¤ä½œä¸º**MSE**æ›²çº¿çš„æœ€å°ç‚¹ã€‚å¾ˆæ˜æ˜¾ï¼Œè¿™ç§æ–¹æ³•æ¯”æˆ‘ä»¬åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46)ä¸­çœ‹åˆ°çš„è›®åŠ›æ–¹æ³•æ›´æœ‰æ•ˆã€‚
- en: 'Now that we have the optimal value of our *intercept*, the linear regression
    model is:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†*æˆªè·*çš„æœ€ä½³å€¼ï¼Œçº¿æ€§å›å½’æ¨¡å‹æ˜¯ï¼š
- en: '![](../Images/2a023c202b0fa0c978c9d5e9931747b4.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a023c202b0fa0c978c9d5e9931747b4.png)'
- en: 'And the linear regression line looks like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§å›å½’çº¿å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/e8f950f35515deab6e817bef1d18b635.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8f950f35515deab6e817bef1d18b635.png)'
- en: best fitting line with intercept = 95 and slope = 0.069
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä½³æ‹Ÿåˆçº¿çš„æˆªè·ä¸º95ï¼Œæ–œç‡ä¸º0.069
- en: Finally, going back to our friend Markâ€™s question â€” What value should he sell
    his 2400 feetÂ² house for?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå›åˆ°æˆ‘ä»¬æœ‹å‹é©¬å…‹çš„é—®é¢˜â€”â€”ä»–åº”è¯¥ä»¥å¤šå°‘ä»·æ ¼å‡ºå”®ä»–é‚£2400å¹³æ–¹è‹±å°ºçš„æˆ¿å­ï¼Ÿ
- en: '![](../Images/df544c7f5484bf0183180a27be7a2ed3.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df544c7f5484bf0183180a27be7a2ed3.png)'
- en: Plug in the house size of 2400 feetÂ² into the above equationâ€¦
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å°†2400å¹³æ–¹è‹±å°ºçš„æˆ¿å±‹å¤§å°ä»£å…¥ä¸Šè¿°æ–¹ç¨‹â€¦â€¦
- en: '![](../Images/7e3e761cd93dbd21bc734bd60c452953.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e3e761cd93dbd21bc734bd60c452953.png)'
- en: â€¦and voila. We can tell our unnecessarily worried friend Mark that based on
    the 3 houses in his neighborhood, he should look to sell his house for around
    $260,600.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: â€¦ç§ã€‚æˆ‘ä»¬å¯ä»¥å‘Šè¯‰æˆ‘ä»¬ä¸å¿…è¦æ‹…å¿ƒçš„æœ‹å‹é©¬å…‹ï¼Œæ ¹æ®ä»–æ‰€åœ¨ç¤¾åŒºçš„3æ ‹æˆ¿å­ï¼Œä»–åº”è¯¥å°†ä»–çš„æˆ¿å­å‡ºå”®ä»·æ ¼å®šåœ¨çº¦$260,600ã€‚
- en: Now that we have a solid understanding of the concepts, letâ€™s do a quick Q&A
    sesh answering any lingering questions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯¹è¿™äº›æ¦‚å¿µæœ‰äº†æ‰å®çš„ç†è§£ï¼Œè®©æˆ‘ä»¬è¿›è¡Œä¸€ä¸ªç®€çŸ­çš„é—®ç­”ç¯èŠ‚ï¼Œè§£ç­”ä»»ä½•æ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚
- en: Why does finding the gradient actually work?
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæ‰¾å‡ºæ¢¯åº¦å®é™…ä¸Šæœ‰æ•ˆï¼Ÿ
- en: 'To illustrate this, consider a scenario where we are attempting to reach the
    minimum point of curve C, denoted as *x**. And we are currently at point A at
    *x*, located to the left of *x**:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜è¿™ä¸€ç‚¹ï¼Œè€ƒè™‘ä¸€ä¸ªåœºæ™¯ï¼Œæˆ‘ä»¬å°è¯•è¾¾åˆ°æ›²çº¿Cçš„æœ€å°ç‚¹ï¼Œè®°ä½œ*x**ã€‚æˆ‘ä»¬å½“å‰åœ¨ç‚¹Aï¼Œä½äº*x*çš„å·¦ä¾§ï¼š
- en: '![](../Images/e0f02362947f0992318f4878ea407da7.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0f02362947f0992318f4878ea407da7.png)'
- en: If we take the derivative of the curve at point A with respect to *x*, represented
    as *dC(x)/dx*, we obtain a negative value (this means the *gradient* is sloping
    downwards). We also observe that we need to move to the right to reach *x**. Thus,
    we need to increase *x* to arrive at the minimum *x*.*
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬åœ¨ç‚¹Aå¯¹æ›²çº¿å…³äº*x*æ±‚å¯¼ï¼Œè®°ä½œ*dC(x)/dx*ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªè´Ÿå€¼ï¼ˆè¿™æ„å‘³ç€*æ¢¯åº¦*å‘ä¸‹å€¾æ–œï¼‰ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œéœ€è¦å‘å³ç§»åŠ¨æ‰èƒ½åˆ°è¾¾*x**ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¢åŠ *x*ä»¥åˆ°è¾¾æœ€å°*x*ã€‚
- en: '![](../Images/4b8b203b05f324d3298c2bb561b8230a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b8b203b05f324d3298c2bb561b8230a.png)'
- en: the red line, or the gradient, is sloping downwards => a negative Gradient
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: çº¢çº¿ï¼Œæˆ–ç§°æ¢¯åº¦ï¼Œå‘ä¸‹å€¾æ–œ => è´Ÿæ¢¯åº¦
- en: Since *dC(x)/dx* is negative, *x-ğ›‚*dC(x)/dx* will be larger than *x*, thus moving
    towards *x**.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº*dC(x)/dx*æ˜¯è´Ÿå€¼ï¼Œ*x-ğ›‚*dC(x)/dx*å°†å¤§äº*x*ï¼Œå› æ­¤æœç€*x**ç§»åŠ¨ã€‚
- en: Similarly, if we are at point A located to the right of the minimum point x*,
    then we get a **positive *gradient*** (*gradient* is sloping upwards), *dC(x)/dx*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œå¦‚æœæˆ‘ä»¬åœ¨ç‚¹Aï¼Œä½äºæœ€å°ç‚¹*x*çš„å³ä¾§ï¼Œåˆ™æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ª**æ­£çš„*æ¢¯åº¦***ï¼ˆ*æ¢¯åº¦*å‘ä¸Šå€¾æ–œï¼‰ï¼Œ*dC(x)/dx*ã€‚
- en: '![](../Images/de1279dfa948d530aff96b49cfced9bb.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de1279dfa948d530aff96b49cfced9bb.png)'
- en: the red line, or the Gradient, is sloping upwards => a positive Gradient
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: çº¢çº¿ï¼Œæˆ–ç§°æ¢¯åº¦ï¼Œå‘ä¸Šå€¾æ–œ => æ­£æ¢¯åº¦
- en: So *x-ğ›‚*dC(x)/dx* will be less than *x*, thus moving towards *x**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤*x-ğ›‚*dC(x)/dx*å°†å°äº*x*ï¼Œä»è€Œæœç€*x**ç§»åŠ¨ã€‚
- en: How does gradient decent know when to stop taking steps?
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™æ³•å¦‚ä½•çŸ¥é“ä½•æ—¶åœæ­¢ï¼Ÿ
- en: Gradient descent stops when the *Step Size* is very close to 0\. As previously
    discussed, at the minimum point the *gradient* is 0 and as we approach the minimum,
    the *gradient* approaches 0\. Therefore, when the *gradient* at a point is close
    to 0 or in the vicinity of the minimum point, the *Step Size* will also be close
    to 0, indicating that the algorithm has reached the optimal solution.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å½“*æ­¥é•¿*éå¸¸æ¥è¿‘ 0 æ—¶ï¼Œæ¢¯åº¦ä¸‹é™ä¼šåœæ­¢ã€‚å¦‚å‰æ‰€è¿°ï¼Œåœ¨æœ€å°ç‚¹å¤„ï¼Œ*æ¢¯åº¦*ä¸º 0ï¼Œå¹¶ä¸”éšç€æˆ‘ä»¬æ¥è¿‘æœ€å°ç‚¹ï¼Œ*æ¢¯åº¦*ä¹Ÿä¼šæ¥è¿‘ 0ã€‚å› æ­¤ï¼Œå½“æŸä¸€ç‚¹çš„*æ¢¯åº¦*æ¥è¿‘
    0 æˆ–åœ¨æœ€å°ç‚¹é™„è¿‘æ—¶ï¼Œ*æ­¥é•¿*ä¹Ÿä¼šæ¥è¿‘ 0ï¼Œè¿™è¡¨æ˜ç®—æ³•å·²ç»è¾¾åˆ°äº†æœ€ä¼˜è§£ã€‚
- en: '![](../Images/14032af67e4aa8ee4c35958ce14d4b11.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14032af67e4aa8ee4c35958ce14d4b11.png)'
- en: when we are close to the minimum point, the gradient is close to 0, and subsequently,
    Step Size is close to 0
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬æ¥è¿‘æœ€å°ç‚¹æ—¶ï¼Œæ¢¯åº¦æ¥è¿‘ 0ï¼Œéšåï¼Œæ­¥é•¿æ¥è¿‘ 0ã€‚
- en: In practice the Minimum Step Size = 0.001 or smaller
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæœ€å°æ­¥é•¿ = 0.001 æˆ–æ›´å°ã€‚
- en: '![](../Images/233f17195cde7e3426dca77493c57e9d.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/233f17195cde7e3426dca77493c57e9d.png)'
- en: That being said, gradient descent also includes a limit on the number of steps
    it will take before giving up called the *Maximum Number of Steps*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è¯è™½å¦‚æ­¤ï¼Œæ¢¯åº¦ä¸‹é™è¿˜åŒ…æ‹¬ä¸€ä¸ªåœ¨æ”¾å¼ƒä¹‹å‰æ‰€è¿›è¡Œçš„æœ€å¤§æ­¥æ•°é™åˆ¶ï¼Œç§°ä¸º*æœ€å¤§æ­¥æ•°*ã€‚
- en: In practice, the Maximum Number of Steps = 1000 or greater
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæœ€å¤§æ­¥æ•° = 1000 æˆ–æ›´å¤§ã€‚
- en: So even if the *Step Size* is larger than the *Minimum Step Size*, if there
    have been more than the *Maximum Number of Steps*, gradient descent will stop.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿*æ­¥é•¿*å¤§äº*æœ€å°æ­¥é•¿*ï¼Œå¦‚æœå·²ç»è¿›è¡Œäº†è¶…è¿‡*æœ€å¤§æ­¥æ•°*çš„è¿­ä»£ï¼Œæ¢¯åº¦ä¸‹é™ä¹Ÿä¼šåœæ­¢ã€‚
- en: What if the minimum point is more challenging to identify?
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚æœæœ€å°ç‚¹æ›´éš¾ä»¥è¯†åˆ«æ€ä¹ˆåŠï¼Ÿ
- en: 'Until now, we have been working with a curve where itâ€™s easy to identify the
    minimum point (these kinds of curves are called ***convex***). But what if we
    have a curve thatâ€™s not as pretty (technically aka ***non-convex***) and looks
    like this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´åˆ°ç°åœ¨ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨å¤„ç†å®¹æ˜“è¯†åˆ«æœ€å°ç‚¹çš„æ›²çº¿ï¼ˆè¿™äº›æ›²çº¿è¢«ç§°ä¸º***å‡¸æ€§***æ›²çº¿ï¼‰ã€‚ä½†å¦‚æœæˆ‘ä»¬é‡åˆ°ä¸€æ¡ä¸é‚£ä¹ˆç¾è§‚çš„æ›²çº¿ï¼ˆæŠ€æœ¯ä¸Šç§°ä¸º***éå‡¸æ€§***æ›²çº¿ï¼‰ï¼Œå¹¶ä¸”å®ƒçœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: '![](../Images/d6f599cf88e193dbc1ba7cb96b050c3f.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6f599cf88e193dbc1ba7cb96b050c3f.png)'
- en: Here, we can see that Point B is the *global minimum* (actual minimum), and
    Points A and C are *local minimums* (points that can be confused for the *global
    minimum* but arenâ€™t). So if a function has multiple *local minimums* and a *global
    minimum*, it is not guaranteed that gradient descent will find the *global minimum*.
    Moreover, which local minimum it finds will depend on the position of the initial
    guess (as seen in [Step 1](/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd#87cf)
    of gradient descent).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç‚¹ B æ˜¯*å…¨å±€æœ€å°å€¼*ï¼ˆå®é™…æœ€å°å€¼ï¼‰ï¼Œè€Œç‚¹ A å’Œ C æ˜¯*å±€éƒ¨æœ€å°å€¼*ï¼ˆå¯èƒ½è¢«è¯¯è®¤ä¸ºæ˜¯*å…¨å±€æœ€å°å€¼*çš„ç‚¹ï¼‰ã€‚å› æ­¤ï¼Œå¦‚æœä¸€ä¸ªå‡½æ•°æœ‰å¤šä¸ª*å±€éƒ¨æœ€å°å€¼*å’Œä¸€ä¸ª*å…¨å±€æœ€å°å€¼*ï¼Œå¹¶ä¸ä¿è¯æ¢¯åº¦ä¸‹é™èƒ½å¤Ÿæ‰¾åˆ°*å…¨å±€æœ€å°å€¼*ã€‚æ­¤å¤–ï¼Œå®ƒæ‰¾åˆ°å“ªä¸ªå±€éƒ¨æœ€å°å€¼å°†å–å†³äºåˆå§‹çŒœæµ‹çš„ä½ç½®ï¼ˆå¦‚[æ­¥éª¤
    1](/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd#87cf)ä¸­æ‰€ç¤ºï¼‰ã€‚
- en: '![](../Images/cfb3784ccaa78a0fc2f825e8e3c540d0.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cfb3784ccaa78a0fc2f825e8e3c540d0.png)'
- en: Taking this non-convex curve above as an example, if the initial guess is at
    Block A or Block C, gradient descent will declare that the minimum point is at
    local minimums A or C, respectively when in reality itâ€™s at B. Only when the initial
    guess is at Block B, the algorithm will find the global minimum B.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸Šè¿°éå‡¸æ€§æ›²çº¿ä¸ºä¾‹ï¼Œå¦‚æœåˆå§‹çŒœæµ‹ä½äº A åŒºå—æˆ– C åŒºå—ï¼Œæ¢¯åº¦ä¸‹é™ä¼šå£°æ˜æœ€å°ç‚¹ä½äºå±€éƒ¨æœ€å°å€¼ A æˆ– Cï¼Œå®é™…ä¸Šå®ƒä½äº Bã€‚åªæœ‰å½“åˆå§‹çŒœæµ‹åœ¨ B åŒºå—æ—¶ï¼Œç®—æ³•æ‰ä¼šæ‰¾åˆ°å…¨å±€æœ€å°å€¼
    Bã€‚
- en: '**Now the question is â€” how do we make a good initial guess?**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç°åœ¨çš„é—®é¢˜æ˜¯â€”â€”æˆ‘ä»¬å¦‚ä½•åšå‡ºä¸€ä¸ªå¥½çš„åˆå§‹çŒœæµ‹ï¼Ÿ**'
- en: '*Simple answer:* Trial and Error. Kind of.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç®€å•çš„ç­”æ¡ˆï¼š* è¯•é”™æ³•ã€‚æœ‰ç‚¹ã€‚'
- en: '*Not-so-simple answer:* From the graph above, if our minimum guess of *x* was
    0 since that lies in Block A, itâ€™ll lead to the local minimum A. Thus, as you
    can see, 0 may not be a good initial guess in most cases. A common practice is
    to apply a random function based on a uniform distribution on the range of all
    possible values of x. Additionally, if feasible, running the algorithm with different
    initial guesses and comparing their results can provide insight into whether the
    guesses differ significantly from each other. This helps in identifying the global
    minimum more efficiently.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸é‚£ä¹ˆç®€å•çš„ç­”æ¡ˆï¼š* ä»ä¸Šå›¾æ¥çœ‹ï¼Œå¦‚æœæˆ‘ä»¬çš„* x* æœ€å°çŒœæµ‹å€¼ä¸º 0ï¼Œå› ä¸ºå®ƒä½äº A åŒºå—ï¼Œè¿™ä¼šå¯¼è‡´å±€éƒ¨æœ€å°å€¼ Aã€‚å› æ­¤ï¼Œå¦‚ä½ æ‰€è§ï¼Œ0 åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å¯èƒ½ä¸æ˜¯ä¸€ä¸ªå¥½çš„åˆå§‹çŒœæµ‹ã€‚ä¸€ä¸ªå¸¸è§çš„åšæ³•æ˜¯åœ¨æ‰€æœ‰å¯èƒ½çš„
    x å€¼èŒƒå›´å†…åº”ç”¨å‡åŒ€åˆ†å¸ƒçš„éšæœºå‡½æ•°ã€‚æ­¤å¤–ï¼Œå¦‚æœå¯è¡Œï¼Œè¿è¡Œç®—æ³•å¹¶æ¯”è¾ƒä¸åŒåˆå§‹çŒœæµ‹çš„ç»“æœå¯ä»¥æä¾›å…³äºçŒœæµ‹æ˜¯å¦å­˜åœ¨æ˜¾è‘—å·®å¼‚çš„è§è§£ã€‚è¿™æœ‰åŠ©äºæ›´æœ‰æ•ˆåœ°è¯†åˆ«å…¨å±€æœ€å°å€¼ã€‚'
- en: Okay, weâ€™re almost there. Last question.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½äº†ï¼Œæˆ‘ä»¬å¿«åˆ°äº†ã€‚æœ€åä¸€ä¸ªé—®é¢˜ã€‚
- en: What if we are trying to find more than one optimal value?
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°è¯•æ‰¾åˆ°å¤šä¸ªæœ€ä½³å€¼æ€ä¹ˆåŠï¼Ÿ
- en: Until now, we were focused on only finding the optimal intercept value because
    we magically knew the *slope* value of the linear regression is 0.069\. But what
    if donâ€™t have a crystal ball and don't know the optimal *slope* value? Then we
    need to optimize both the slope and intercept values, expressed as *xâ‚€* and *xâ‚*
    respectively.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´åˆ°ç°åœ¨ï¼Œæˆ‘ä»¬åªå…³æ³¨æ‰¾åˆ°æœ€ä½³çš„æˆªè·å€¼ï¼Œå› ä¸ºæˆ‘ä»¬ç¥å¥‡åœ°çŸ¥é“çº¿æ€§å›å½’çš„*slope*å€¼æ˜¯0.069ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ²¡æœ‰æ°´æ™¶çƒï¼Œä¸çŸ¥é“æœ€ä½³çš„*slope*å€¼æ€ä¹ˆåŠï¼Ÿé‚£ä¹ˆæˆ‘ä»¬éœ€è¦åŒæ—¶ä¼˜åŒ–æ–œç‡å’Œæˆªè·å€¼ï¼Œåˆ†åˆ«è¡¨ç¤ºä¸º*xâ‚€*å’Œ*xâ‚*ã€‚
- en: In order to do that, we must utilize partial derivatives instead of just derivatives.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨åå¯¼æ•°ï¼Œè€Œä¸ä»…ä»…æ˜¯å¯¼æ•°ã€‚
- en: 'NOTE: Partial derivates are calculated in the same way as reglar old derivates,
    but are denoted differently because we have more than one variable we are trying
    to optimize for. To learn more about them, read this [article](https://www.mathsisfun.com/calculus/derivatives-partial.html)
    or watch this [video](https://www.youtube.com/watch?v=JAf_aSIJryg).'
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šåå¯¼æ•°çš„è®¡ç®—æ–¹å¼ä¸æ™®é€šå¯¼æ•°ç›¸åŒï¼Œä½†ç”±äºæˆ‘ä»¬æœ‰å¤šä¸ªå˜é‡éœ€è¦ä¼˜åŒ–ï¼Œå› æ­¤è¡¨ç¤ºæ–¹å¼æœ‰æ‰€ä¸åŒã€‚è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·é˜…è¯»è¿™ç¯‡[æ–‡ç« ](https://www.mathsisfun.com/calculus/derivatives-partial.html)æˆ–è§‚çœ‹è¿™ä¸ª[è§†é¢‘](https://www.youtube.com/watch?v=JAf_aSIJryg)ã€‚
- en: However, the process remains relatively similar to that of optimizing a single
    value. The cost function (such as **MSE**) must still be defined and the gradient
    descent algorithm must be applied, but with the added step of finding partial
    derivatives for both xâ‚€ and xâ‚.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸ä¼˜åŒ–å•ä¸€å€¼çš„è¿‡ç¨‹ç›¸å¯¹ç±»ä¼¼ã€‚æˆæœ¬å‡½æ•°ï¼ˆå¦‚**å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰**ï¼‰ä»ç„¶éœ€è¦å®šä¹‰ï¼Œå¹¶ä¸”æ¢¯åº¦ä¸‹é™ç®—æ³•å¿…é¡»åº”ç”¨ï¼Œä½†éœ€è¦é¢å¤–çš„æ­¥éª¤æ¥æ±‚è§£xâ‚€å’Œxâ‚çš„åå¯¼æ•°ã€‚
- en: '**Step 1: Make initial guesses for xâ‚€ and xâ‚**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤1ï¼šå¯¹xâ‚€å’Œxâ‚è¿›è¡Œåˆå§‹çŒœæµ‹**'
- en: '**Step 2: Find the partial derivatives with respect to xâ‚€ and xâ‚ at these points**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤2ï¼šåœ¨è¿™äº›ç‚¹ä¸Šæ‰¾åˆ°å…³äºxâ‚€å’Œxâ‚çš„åå¯¼æ•°**'
- en: '![](../Images/23106bd4ebe44c70dc9e745e6a65f5da.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23106bd4ebe44c70dc9e745e6a65f5da.png)'
- en: '**Step 3: Simultaneously update xâ‚€ and xâ‚ based on the partial derivatives
    and the Learning Rate**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤3ï¼šæ ¹æ®åå¯¼æ•°å’Œå­¦ä¹ ç‡åŒæ—¶æ›´æ–°xâ‚€å’Œxâ‚**'
- en: '![](../Images/320d5c87006b30dd542e0d81eb06fa58.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/320d5c87006b30dd542e0d81eb06fa58.png)'
- en: '**Step 4: Repeat Steps 2â€“3 until the Maximum Number of Steps is reached or
    the Step Size is less that the Minimum Step Size**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤4ï¼šé‡å¤æ­¥éª¤2â€“3ï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§æ­¥æ•°æˆ–æ­¥é•¿å°äºæœ€å°æ­¥é•¿**'
- en: '*And we can extrapolate these steps to 3, 4, or even 100 values to optimize
    for.*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬å¯ä»¥å°†è¿™äº›æ­¥éª¤æ¨å¹¿åˆ°3ã€4ï¼Œç”šè‡³100ä¸ªå€¼è¿›è¡Œä¼˜åŒ–ã€‚*'
- en: In conclusion, gradient descent is a powerful optimization algorithm that efficiently
    helps us reach the optimal value. It can be applied to many other optimization
    problems, making it a fundamental tool for data scientists to have in their arsenal.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œæ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§å¼ºå¤§çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¯ä»¥é«˜æ•ˆåœ°å¸®åŠ©æˆ‘ä»¬è¾¾åˆ°æœ€ä¼˜å€¼ã€‚å®ƒå¯ä»¥åº”ç”¨äºè®¸å¤šå…¶ä»–ä¼˜åŒ–é—®é¢˜ï¼Œæ˜¯æ•°æ®ç§‘å­¦å®¶å¿…å¤‡çš„åŸºæœ¬å·¥å…·ã€‚
- en: 'UPDATE: [Part 3 on Logistic Regression](https://medium.com/towards-data-science/back-to-basics-part-tres-logistic-regression-e309de76bd66)
    is also up now!'
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ›´æ–°ï¼š [ç¬¬3éƒ¨åˆ†ï¼šé€»è¾‘å›å½’](https://medium.com/towards-data-science/back-to-basics-part-tres-logistic-regression-e309de76bd66)
    ä¹Ÿå·²ç»ä¸Šçº¿äº†ï¼
- en: As always, feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/shreyarao24/)
    or email me at *shreya.statistics@gmail.com* to send me questions and suggestions
    for any other algorithms that you want illustrated!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å¦‚æ—¢å¾€ï¼Œæ¬¢è¿é€šè¿‡[LinkedIn](https://www.linkedin.com/in/shreyarao24/)ä¸æˆ‘è”ç³»ï¼Œæˆ–é€šè¿‡*shreya.statistics@gmail.com*å‘é€ç”µå­é‚®ä»¶ï¼Œæå‡ºé—®é¢˜æˆ–å»ºè®®ä»»ä½•å…¶ä»–æ‚¨å¸Œæœ›è¯´æ˜çš„ç®—æ³•ï¼
