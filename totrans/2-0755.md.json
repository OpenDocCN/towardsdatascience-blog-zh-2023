["```py\ndef load_docs(directory: str):\n    \"\"\"\n    Load documents from the given directory.\n    \"\"\"\n    loader = DirectoryLoader(directory)\n    documents = loader.load()\n\n    return documents\n\ndef split_docs(documents, chunk_size=1000, chunk_overlap=20):\n    \"\"\"\n    Split the documents into chunks.\n    \"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    docs = text_splitter.split_documents(documents)\n\n    return docs\n```", "```py\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"\n    Load all the necessary models and data once the server starts.\n    \"\"\"\n    app.directory = '/app/content/'\n    app.documents = load_docs(app.directory)\n    app.docs = split_docs(app.documents)\n\n    app.embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n    app.persist_directory = \"chroma_db\"\n\n    app.vectordb = Chroma.from_documents(\n        documents=app.docs,\n        embedding=app.embeddings,\n        persist_directory=app.persist_directory\n    )\n    app.vectordb.persist()\n\n    app.model_name = \"gpt-3.5-turbo\"\n    app.llm = ChatOpenAI(model_name=app.model_name)\n\n    app.db = Chroma.from_documents(app.docs, app.embeddings)\n    app.chain = load_qa_chain(app.llm, chain_type=\"stuff\", verbose=True)\n```", "```py\n@app.get(\"/query/{question}\")\nasync def query_chain(question: str):\n    \"\"\"\n    Queries the model with a given question and returns the answer.\n    \"\"\"\n    matching_docs_score = app.db.similarity_search_with_score(question)\n    if len(matching_docs_score) == 0:\n        raise HTTPException(status_code=404, detail=\"No matching documents found\")\n\n    matching_docs = [doc for doc, score in matching_docs_score]\n    answer = app.chain.run(input_documents=matching_docs, question=question)\n\n    # Prepare the sources\n    sources = [{\n        \"content\": doc.page_content,\n        \"metadata\": doc.metadata,\n        \"score\": score\n    } for doc, score in matching_docs_score]\n\n    return {\"answer\": answer, \"sources\": sources}\n```", "```py\nFROM python:3.9-buster\nWORKDIR /app\nCOPY . /app\nRUN pip install - no-cache-dir -r requirements.txt\nEXPOSE 1010\nCMD [\"uvicorn\", \"main:app\", \" - host\", \"0.0.0.0\", \" - port\", \"1010\"]\n```", "```py\nlangchain==0.0.221\nuvicorn==0.22.0\nfastapi==0.99.1\nunstructured==0.7.12\nsentence-transformers==2.2.2\nchromadb==0.3.26\nopenai==0.27.8\npython-dotenv==1.0.0\n```", "```py\ncurl --location 'http://0.0.0.0:1010/query/What is Falcon-40b and can I use it for commercial use'\n```", "```py\n{\n    \"answer\": \"Falcon-40B is a state-of-the-art language model developed by the Technology Innovation Institute (TII). It is a transformer-based model that performs well on various language understanding tasks. The significance of Falcon-40B is that it is now available for free commercial and research use, as announced by TII. This means that developers and researchers can access and modify the model according to their specific needs without any royalties. However, it is important to note that while Falcon-40B is available for commercial use, it is still trained on web data and may carry potential biases and stereotypes prevalent online. Therefore, appropriate mitigation strategies should be implemented when using Falcon-40B in a production environment.\",\n    \"sources\": [\n        {\n            \"content\": \"This is where the significance of Falcon-40B lies. In the end of last week, the Technology Innovation Institute (TII) announced that Falcon-40B is now free of royalties for commercial and research use. Thus, it breaks down the barriers of proprietary models, giving developers and researchers free access to a state-of-the-art language model that they can use and modify according to their specific needs.\\n\\nTo add to the above, the Falcon-40B model is now the top performing model on the OpenLLM Leaderboard, outperforming models like LLaMA, StableLM, RedPajama, and MPT. This leaderboard aims to track, rank, and evaluate the performance of various LLMs and chatbots, providing a clear, unbiased metric of their capabilities. Figure 1: Falcon-40B is dominating the OpenLLM Leaderboard (image source)\\n\\nAs always, the code is available on my Github. How was Falcon LLM developed?\",\n            \"metadata\": {\n                \"source\": \"/app/content/Harnessing the Falcon 40B Model, the Most Powerful Open-Source LLM.txt\"\n            },\n            \"score\": 1.045290231704712\n        },\n        {\n            \"content\": \"The decoder-block in Falcon-40B features a parallel attention/MLP (Multi-Layer Perceptron) design with two-layer normalization. This structure offers benefits in terms of model scaling and computational speed. Parallelization of the attention and MLP layers improves the model’s ability to process large amounts of data simultaneously, thereby reducing the training time. Additionally, the implementation of two-layer normalization helps in stabilizing the learning process and mitigating issues related to the internal covariate shift, resulting in a more robust and reliable model. Implementing Chat Capabilities with Falcon-40B-Instruct\\n\\nWe are using the Falcon-40B-Instruct, which is the new variant of Falcon-40B. It is basically the same model but fine tuned on a mixture of Baize. Baize is an open-source chat model trained with LoRA, a low-rank adaptation of large language models. Baize uses 100k dialogs of ChatGPT chatting with itself and also Alpaca’s data to improve its performance.\",\n            \"metadata\": {\n                \"source\": \"/app/content/Harnessing the Falcon 40B Model, the Most Powerful Open-Source LLM.txt\"\n            },\n            \"score\": 1.319214940071106\n        },\n        {\n            \"content\": \"One of the core differences on the development of Falcon was the quality of the training data. The size of the pre-training data for Falcon was nearly five trillion tokens gathered from public web crawls, research papers, and social media conversations. Since LLMs are particularly sensitive to the data they are trained on, the team built a custom data pipeline to extract high-quality data from the pre-training data using extensive filtering and deduplication.\\n\\nThe model itself was trained over the course of two months using 384 GPUs on AWS. The result is an LLM that surpasses GPT-3, requiring only 75% of the training compute budget and one-fifth of the compute at inference time.\",\n            \"metadata\": {\n                \"source\": \"/app/content/Harnessing the Falcon 40B Model, the Most Powerful Open-Source LLM.txt\"\n            },\n            \"score\": 1.3254718780517578\n        },\n        {\n            \"content\": \"Falcon-40B is English-centric, but also includes German, Spanish, French, Italian, Portuguese, Polish, Dutch, Romanian, Czech, and Swedish language capabilities. Be mindful that as with any model trained on web data, it carries the potential risk of reflecting the biases and stereotypes prevalent online. Therefore, please assess these risks adequately and implement appropriate mitigation strategies when using Falcon-40B in a production environment. Model Architecture and Objective\\n\\nFalcon-40B, as a member of the transformer-based models family, follows the causal language modeling task, where the goal is to predict the next token in a sequence of tokens. Its architecture fundamentally builds upon the design principles of GPT-3 [1], with a few important tweaks.\",\n            \"metadata\": {\n                \"source\": \"/app/content/Harnessing the Falcon 40B Model, the Most Powerful Open-Source LLM.txt\"\n            },\n            \"score\": 1.3283030986785889\n        }\n    ]\n}\n```"]