- en: Does Bagging Help to Prevent Overfitting in Decision Trees?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/does-bagging-help-to-prevent-overfitting-in-decision-trees-42262943a81e](https://towardsdatascience.com/does-bagging-help-to-prevent-overfitting-in-decision-trees-42262943a81e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understand why decision trees are highly prone to overfitting and its potential
    remedies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)[![Gurjinder
    Kaur](../Images/d5c6746466025dad06077b1a89a789d1.png)](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)
    [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)
    ¬∑12 min read¬∑Dec 13, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49dc05d1ffe1c84260864fe2ab15c16c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jan Huber](https://unsplash.com/@jan_huber?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are a class of machine learning algorithms well known for their
    ability to solve both classification and regression problems, and not to forget
    the ease of interpretation they offer. However, they suffer from overfitting and
    can fail to generalize well if not controlled properly.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will discuss what is overfitting, to what extent a decision
    tree overfits the training data, why it is an issue, and how it can be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will get ourselves acquainted with one of the ensemble techniques i.e.,
    *bagging*, and see if it can be used to make decision trees more robust.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create our regression dataset using NumPy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train a decision tree model using scikit-learn.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand what overfitting means by looking at the performance of the same
    model on the training set and test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discuss why overfitting is more common in non-parametric models such as decision
    trees (and of course learn what is meant by the term non-parametric) and how it
    can be prevented using regularization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand what *bootstrap aggregation* (*bagging* in short) is and how it can
    potentially help with overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we will implement the bagging version of the decision tree and see
    if it helps or not ü§û
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Still wondering if it‚Äôs worth reading?** ü§î If you‚Äôve ever wondered why Random
    Forests are usually preferred over vanilla Decision Trees, this is the best place
    to start since Random Forests use the idea of *bagging plus something else* to
    improve upon decision trees.'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs get started!
  prefs: []
  type: TYPE_NORMAL
- en: We will set up a Python notebook and import the libraries first.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 1: Creating the dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to use a dataset resembling a quadratic function with *y* as the
    target variable and *X* as the independent variable. Since *y* is numeric, we
    will be fitting a regression tree on this dataset. Let‚Äôs build our dataset as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We have created a dataset with 500 samples where both *X* and *y* are continuous
    as shown below. The link to the full notebook along with visualizations can be
    found at the end of this article, so don‚Äôt worry about the missing viz code in
    this article.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0437751797643f16d797f0f7643e3809.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2: Train-test split**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use scikit-learn‚Äôs *train_test_split* to split our dataset into a training
    set and test set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`Cell Output:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd12191c5e8907a2adb39bb4d9e74aba.png)'
  prefs: []
  type: TYPE_IMG
- en: We will only use the training set for training our model and keep the test set
    aside to use it just for testing the model‚Äôs performance. This will ensure we
    are testing our model against samples it has never seen before and will help us
    evaluate how well it generalizes. How smart, right? üòé
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, the following is what our training and test sets look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8bd44f240b3660c4fc379aa69a2b617.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Fitting the regression tree on the training set**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fitting a decision tree regressor using scikit-learn is just two lines of code.
    However, if you‚Äôre not sure what‚Äôs happening under the hood and are a curious
    learner, [this article](https://medium.com/p/fbb908cf548b) would be your go-to
    guide for understanding how exactly a decision tree solves a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`Cell Output:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f2679ca1bf0088971339172164d23c4.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 4: Evaluating the regression tree on training and test sets**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, since our model has been trained, let‚Äôs use it to make predictions on:'
  prefs: []
  type: TYPE_NORMAL
- en: '**the training set** i.e., the data it already is friends with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**the test set** i.e., the data it has never seen before (***the real test
    lies here***).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are going to use mean squared error to evaluate the quality of our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`Cell Output:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2ad8618288b2a377c418f63a9ed893b.png)'
  prefs: []
  type: TYPE_IMG
- en: The decision tree regressor manages to get a ZERO ERROR on the training set.
    We are almost going to crown this model as the greatest of all time until we see
    the results on the test set, and there we stop ü•∂
  prefs: []
  type: TYPE_NORMAL
- en: The same model gets a whopping 173.336 mean squared error on the test set. Looks
    like it failed miserably on the real test üòî
  prefs: []
  type: TYPE_NORMAL
- en: This is because the model overfitted (over-learned, over-relied, over-crammed,
    over-memorized) the training data points so perfectly that it failed to learn
    the underlying patterns within the data; rather it caught up on the noise of the
    training set that was specific just to the training set and had nothing to do
    with the overall behavior of the data. This is called ***Overfitting.***
  prefs: []
  type: TYPE_NORMAL
- en: '**Overfitting** is the property of a model such that the model predicts very
    well the labels of the examples used during training but frequently makes errors
    when applied to examples that weren‚Äôt seen by the learning algorithm during training
    ‚Äî Andriy Burkov (The Hundred Page Machine Learning Book)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can see in the following plots how the predicted values are exactly overlapping
    with the actual values for the training set, but not for the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41b4140e94e371aa57a713bdc3d10df4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: Following is what the predictions will look like for the given training and
    test data. It can be clearly seen that the model is trying to fit the training
    data very closely. `Depth=None` indicates that unless specified, there is no restriction
    on the maximum depth a tree can reach. It is the default value of the *max_depth*
    hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53e9d9b07733811a938afe0e59d22531.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: '***Just Thinking:*** *The term Tightfitting instead of Overfitting makes a
    much better literal sense because that‚Äôs what our model is doing here* ü§∑üèª‚Äç‚ôÄÔ∏è Anyways,
    let‚Äôs not break the rules and stick with overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Why does overfitting come naturally to decision trees?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision trees are *non-parametric* i.e., they don‚Äôt make any assumptions about
    the training data. If left unconstrained, the tree structure will completely adapt
    itself to the training data, fitting it very closely, most likely overfitting
    it.
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs termed as *non-parametric* not because it doesn‚Äôt have any parameters but
    because the number of parameters is not determined before training and hence the
    model structure is free to stick closely to the training data (*as opposed to
    linear regression, where we have a fixed number of coefficients i.e., parameters
    that we want the model to learn, so its degree of freedom is limited*)
  prefs: []
  type: TYPE_NORMAL
- en: '**Why is overfitting an issue?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overfitting is undesirable because it doesn‚Äôt allow the model to generalize
    well on the new data, and if that happens, it will not be able to perform well
    on the classification or prediction tasks it was originally intended for.
  prefs: []
  type: TYPE_NORMAL
- en: '**What we could‚Äôve done differently**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In case our model shows signs of overfitting, we can infer that the model is
    overly complex and needs to be ***regularized*.**
  prefs: []
  type: TYPE_NORMAL
- en: Regularization is the process of restricting a model‚Äôs freedom by enforcing
    some constraints on it so that the chance of overfitting is reduced.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Several hyperparameters such as maximum depth, minimum number of samples in
    a leaf node, etc. can be tuned to regularize the decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: The least we could do to prevent a situation like above is to set the *max_depth*
    to stop the tree from over-growing. The default value of *max_depth* is set to
    *None* which means there is no limit on the growth of the decision tree. Reducing
    *max_depth* will regularize the model and thus reduce the risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Following are the predicted vs actual plots for the training and test sets for
    different values of *max_depth.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/307b5d4c500bb509fba6ae041a4709e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Did you notice a trade-off?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we increase the *max_depth* the performance of the model keeps getting better
    for the training set but worse for the test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the *max_depth* makes the model more complex and hence reduces its
    generalization capability. *This is the same as having high* ***variance.***
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the *max_depth* makes the model more simple and hence it can underfit
    (this happens when the model is too weak to perform well even on the training
    set, forget about the test set). *This is the same as having high* ***bias.***
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following plot, we can see the model predictions for different values
    of *max_depth* and it can help us understand that high bias leads to underfitting
    whereas high variance leads to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efb0f439058fd1b7c4aad2d47c017786.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Understanding bias-variance tradeoff (Source: Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Attempting to reduce the bias increases the variance, and vice versa. We need
    to find a sweet spot where both the bias and variance are not too high but also
    not too low. **This is called the bias-variance tradeoff.**
  prefs: []
  type: TYPE_NORMAL
- en: The good thing is that we don‚Äôt have to do it manually. We can leverage automated
    hyperparameter tuning and cross-validation to come up with the best values of
    regularization hyperparameters that are not just limited to the *max_depth.*
  prefs: []
  type: TYPE_NORMAL
- en: '**Is Overfitting the Only Problem?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Short Answer:* No (but not too helpful, you still have to read the long answer,
    sorry üòÖ)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Long Answer:* You might be wondering if overfitting can be prevented using
    regularization, then what‚Äôs the need of bagging, or other ensemble techniques.
    The thing is that in addition to overfitting, ***decision trees are also prone
    to instability.***'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are highly sensitive to small variations in the dataset. Even
    minor changes in the training data can lead to drastically different decision
    trees. This instability can be limited by training many trees on random subsamples
    of the data and then averaging the predictions of these trees.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Idea of Ensemble Learning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An *ensemble* is a group of models and the technique of aggregating the predictions
    of these models is known as ***ensemble learning.***
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two approaches to ensemble learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Use different training algorithms such as decision trees, SVM, etc. for each
    predictor and train them on the given training set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the same training algorithm for every predictor and train them on different
    subsets of the training set. *Bagging falls in this category.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Introduction to Bagging**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bagging is a short word for ***bootstrap aggregation****.*
  prefs: []
  type: TYPE_NORMAL
- en: Bagging is an ensemble method in which multiple models are trained on different
    random subsamples of the training set, and the sampling is performed with replacement.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sampling with replacement means that some instances can be sampled several times
    for any given predictor, while others may not be sampled at all. This ensures
    that sensitivity to minor variations in the training data gets accounted for and
    no longer harms the stability of the final ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8efccce1215892f87eb106f225e6a04.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of Bagging (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '***Note:*** *We can either subsample the training set with replacement or without
    replacement.* ***When the sampling is done with replacement, it is known as bagging.
    When the sampling is done without replacement, it is known as pasting.***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all the models are trained on random subsamples of the training data,
    their predictions can be aggregated as:'
  prefs: []
  type: TYPE_NORMAL
- en: averaging the predictions for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: majority voting for classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have an idea of ensemble learning and bagging, let‚Äôs implement it
    in scikit-learn. Let‚Äôs continue the following steps in our notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5: Implement bagging in scikit-learn**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can simply pass our decision tree regressor inside a bagging regressor and
    specify the number of models we want to train (*n_estimators),* and thenumber
    of samples to consider for training each model (*max_samples).*
  prefs: []
  type: TYPE_NORMAL
- en: Here, `bootstrap=True` means that the data will be sampled with replacement
    and if we want to use pasting instead of bagging, we can set `bootstrap=False`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`Cell Output:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/750e3796c72cc86d955b7d1b67cac124.png)'
  prefs: []
  type: TYPE_IMG
- en: It means we have trained 200 decision trees separately such that each decision
    tree has used a random subsample of size 100 as the training set. The end prediction
    will be the average of all predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6: Evaluate the bagged version of the decision tree regressor**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use mean squared error again to evaluate how well the model predicts
    the samples in training as well as the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`Cell Output:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d59594848411f96d374f5a0a10075b58.png)'
  prefs: []
  type: TYPE_IMG
- en: After using bagging, the training MSE has gone up from 0 to 69.438 but the test
    MSE has gone down from 173.336 to 101.521 which is indeed an improvement!
  prefs: []
  type: TYPE_NORMAL
- en: We can verify from the below plot that the final predictions after the bagged
    ensemble of decision trees have a lot better generalization capability than the
    previous one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d64363495f57cadc97ac5144dde52b7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows the bagging regressor‚Äôs predictions for the given
    training and test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e4ed0b338726ca7465bc44100249ec6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: The final predictions from the ensemble are smoother than what a single decision
    tree would have produced and the model shows a similar fit for both the training
    and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Link to full notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can find the notebook [here](https://github.com/gurjinderbassi/Machine-Learning/blob/main/Overfitting%20and%20Bagging%20in%20Decision%20Trees.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'Bonus: Random Forests'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of this article, I specified that random forests use the idea
    of bagging plus *something else.*I don‚Äôt want you to keep pondering upon what
    this *something else* is, and since you‚Äôve almost got to the end of this article,
    this bonus section is your reward üò∏
  prefs: []
  type: TYPE_NORMAL
- en: A random forest is an ensemble of decision trees that are trained via the bagging
    method.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shedding light on *something else:* The random forest algorithm introduces extra
    randomness while growing the trees. While splitting a node, instead of searching
    the entire feature space, it searches for the best feature among a random subset
    of features. This further enhances the diversity of the models and reduces variance,
    giving rise to an overall better ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests can also be implemented using scikit-learn for both the regression
    and classification tasks. It has all the hyperparameters of a *DecisionTreeRegressor
    (or DecisionTreeClassifier)* to control how individual trees are grown, plus all
    the hyperparameters of a *BaggingRegressor (or BaggingClassifier),* with some
    exceptions. The other set of hyperparameters is also there to control the sampling
    of features to consider at each node.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we discussed the issues of overfitting and instability in decision
    trees and how we can use ensemble methods such as bagging to overcome them.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are powerful machine learning algorithms that can solve both
    regression and classification problems, however, they suffer from overfitting
    and instability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting occurs when a model fits the training data so perfectly that it
    fails to generalize well and learn the underlying behavior of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization can be used to reduce the chance of overfitting by limiting the
    growth of the decision tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another problem with decision trees is that they are highly sensitive to small
    variations in the data that make them unstable. This can be overcome by using
    ensemble techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble learning consists of training multiple predictors on random subsets
    of the training data and then aggregating their predictions. Bagging is one such
    technique that samples the training data with replacement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forests improve upon the decision trees by incorporating bagging and
    random feature selection at each node to reduce the overall variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you for reading, I hope it was helpful!
  prefs: []
  type: TYPE_NORMAL
- en: Open to any feedback or suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [https://www.ibm.com/topics/overfitting](https://www.ibm.com/topics/overfitting)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Hands-on machine learning with Scikit-Learn, Keras and TensorFlow: concepts,
    tools, and techniques to build intelligent systems (2nd ed.). O‚ÄôReilly. Aur√©lien
    G√©ron, 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] The Hundred-Page Machine Learning Book, Andriy Burkov, 2019.'
  prefs: []
  type: TYPE_NORMAL
