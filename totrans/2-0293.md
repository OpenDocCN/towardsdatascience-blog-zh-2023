# VGG的实现

> 原文：[https://towardsdatascience.com/an-implementation-of-vgg-dea082804e14](https://towardsdatascience.com/an-implementation-of-vgg-dea082804e14)

## 面向初学者的教程

[](https://medium.com/@mina.ghashami?source=post_page-----dea082804e14--------------------------------)[![Mina Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----dea082804e14--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dea082804e14--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dea082804e14--------------------------------) [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----dea082804e14--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dea082804e14--------------------------------) ·9分钟阅读·2023年10月31日

--

在这篇文章中，我们查看VGG的实现及其在STL10 [2, 3]数据集上的训练。

我们在[上一篇文章](https://medium.com/towards-data-science/image-classification-for-beginners-8546aa75f331)中回顾了VGG架构。如果不熟悉，请查看。

[](/image-classification-for-beginners-8546aa75f331?source=post_page-----dea082804e14--------------------------------) [## 面向初学者的图像分类

### VGG和ResNet架构自2014年

[towardsdatascience.com](/image-classification-for-beginners-8546aa75f331?source=post_page-----dea082804e14--------------------------------)

简而言之，

> ***VGG*** *代表* ***视觉几何组*** *，是牛津大学的一个研究小组。2014年，他们设计了一种用于图像分类任务的深度卷积神经网络架构，并以他们自己命名；即VGG [1].*

VGGNet有几种配置，如VGG16（16层）和VGG19（19层）。

VGG16的架构如下：它有13个卷积层和3个全连接层。

![](../Images/1d4da4d93ace6621f6ce11b018e4bb23.png)

作者提供的图像

# 模型实现

让我们在PyTorch中实现VGG16。

[PRE0]

请注意，实现的结构基于两个属性：

+   特征：包含所有的卷积层和最大池化层

+   分类器：包含全连接层和用于分类的softmax层

还需注意，我们将*input_channel*作为输入参数传递。该参数为3时表示图像为彩色，为1时表示图像为灰度。

最后但同样重要的是，第一个全连接层是*nn.Linear(512 * 3 * 3, 4096).* 之所以输入维度是*512 * 3 * 3*，是因为我们设置它以适应输入图像为*96 * 96*。如果我们传递不同尺寸的图像，则需要更改此值。例如，对于224 * 224的图像，该层变为*nn.Linear(512 * 7 * 7, 4096).*

然后我们实现*forward()*方法：

[PRE1]

现在网络已经完成，让我们通过它传递一个随机张量，并查看它在经过各层时形状的变化。

[PRE2]

它打印出以下形状：

[PRE3]

所以最终输出是一个10维的向量，表示图像属于10个类别中任何一个类别的概率。

# 数据转换 — STL10

现在，让我们在STL10数据集[2,3]上进行训练，该数据集已获得商业使用许可。此数据集包含5000张彩色图像，分为10个类别。

每张图像为96x96像素，10个类别如下：

[PRE4]

让我们加载数据并查看一些图像：

[PRE5]

它打印这些图像及其标签：

![](../Images/b470475af27e16e006cd7b1b4ddb4edd.png)

图片来源于作者

接下来，让我们对数据进行标准化。为了标准化数据，我们首先计算*均值*和*标准差*：

[PRE6]

请注意，在*trainloader*中，我们设置了*batch_size = len(trainset)*，以便加载整个数据集来计算均值和标准差。之后，当我们想要训练模型时，我们将数据以128张图像的小批量加载。

从上面可以看出，*np_images*的形状是(5000, 3, 96, 96)，即5000张96x96像素的彩色图像（注意通道数为3，表示图像是彩色的）。因此，均值和标准差如下：

**均值 = [**0.44671103, 0.43980882, 0.40664575]

**标准差 =** [0.2603408, 0.25657743, 0.2712671

我们将使用这个均值和标准差来标准化测试数据和训练数据。让我们定义每个数据集的转换：

[PRE7]

# 训练模型

我们首先定义超参数，例如：

+   学习率

+   学习率调度器

+   损失函数：用于分类的交叉熵

+   优化器

[PRE8]

接下来，我们定义两个函数：

**方法1：** train_batch：对于数据中的所有批次，它训练模型，计算损失并更新参数。此方法应用反向传播并计算训练损失。

[PRE9]

**方法2**：是validate_batch函数，其中我们在测试加载器的一个批次上验证模型。通常，在每个训练周期之后，我们调用此函数来获取模型在每个训练周期末的性能。此函数计算测试集（即未见数据）的损失，并且**不**进行任何反向传播。

[PRE10]

## 让实际训练开始 …

对于每一个训练周期，我们训练模型并检查模型在测试数据集上的表现。我们调用*vgg_scheduler.step()*，然后通知调度器递增其内部计数器并更新学习率。

[PRE11]

我们看到以下性能：

[PRE12]

我们看到模型在第11个周期的准确率达到80.8%。

接下来，让我们查看10张图像以及模型对它们标签的预测：

[PRE13]

例如，我们看到以下图像是一只鸟，模型正确地预测为鸟。

![](../Images/336b04bab40050a1c566163b49fec182.png)

图片来源于作者

然后我们看到一个错误预测的例子，其中图像是一架飞机，但VGG将其预测为一只鸟：

![](../Images/f1507f014439c9d579fec6483005cf3f.png)

图片来源于作者

这就结束了我们对 VGG 模型的实现部分。我们看到 VGG 具有非常深的架构和许多参数，但其实现非常直接，这归功于其架构的统一性。

到目前为止，我们已经回顾了[VGG 和 ResNet 的概念](/image-classification-for-beginners-8546aa75f331)以及 VGG 的代码。在下一篇文章中，我们可以查看 ResNet 的实现。

如有任何评论或问题，请告知我。

如果你有任何问题或建议，请随时联系我：

邮箱: mina.ghashami@gmail.com

LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)

# 参考资料

1.  [非常深层的卷积网络用于大规模图像识别](https://arxiv.org/pdf/1409.1556.pdf)

1.  [https://pytorch.org/vision/main/generated/torchvision.datasets.STL10.html](https://pytorch.org/vision/main/generated/torchvision.datasets.STL10.html)

1.  [https://cs.stanford.edu/~acoates/stl10/](https://cs.stanford.edu/~acoates/stl10/)
