- en: The ABCs of Differential Privacy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·®åˆ†éšç§çš„ABC
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/abcs-of-differential-privacy-8dc709a3a6b3](https://towardsdatascience.com/abcs-of-differential-privacy-8dc709a3a6b3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/abcs-of-differential-privacy-8dc709a3a6b3](https://towardsdatascience.com/abcs-of-differential-privacy-8dc709a3a6b3)
- en: MASTERING BASICS
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŒæ¡åŸºç¡€çŸ¥è¯†
- en: A Guide to Understanding Fundamental Definitions and Key Principles
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†è§£åŸºæœ¬å®šä¹‰å’Œå…³é”®åŸåˆ™çš„æŒ‡å—
- en: '[](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)[![Essi
    Alizadeh](../Images/be2244231732f93bcadf09682ef8ca37.png)](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)
    [Essi Alizadeh](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)[![Essi
    Alizadeh](../Images/be2244231732f93bcadf09682ef8ca37.png)](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)
    [Essi Alizadeh](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)
    Â·8 min readÂ·Apr 27, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)
    Â·é˜…è¯»æ—¶é—´8åˆ†é’ŸÂ·2023å¹´4æœˆ27æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a9c9c860a5cbeead7b567f365584a0b4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9c9c860a5cbeead7b567f365584a0b4.png)'
- en: Image by Author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: Differential privacy (DP) is a rigorous mathematical framework that permits
    the analysis and manipulation of sensitive data while providing robust privacy
    guarantees.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å·®åˆ†éšç§ï¼ˆDPï¼‰æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ•°å­¦æ¡†æ¶ï¼Œå…è®¸å¯¹æ•æ„Ÿæ•°æ®è¿›è¡Œåˆ†æå’Œå¤„ç†ï¼ŒåŒæ—¶æä¾›å¼ºæœ‰åŠ›çš„éšç§ä¿éšœã€‚
- en: DP is based on the premise that the inclusion or exclusion of a single individual
    should not significantly change the results of any analysis or query carried out
    on the dataset as a whole. In other words, the algorithm should come up with comparable
    findings when comparing these two sets of data, making it difficult to figure
    out anything distinctive about that individual. This safety keeps private information
    from getting out but still lets useful insights be drawn from the data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: DPçš„åŸºç¡€æ˜¯å‡è®¾ä¸€ä¸ªä¸ªä½“çš„åŠ å…¥æˆ–æ’é™¤ä¸åº”è¯¥æ˜¾è‘—æ”¹å˜å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œçš„ä»»ä½•åˆ†ææˆ–æŸ¥è¯¢çš„ç»“æœã€‚æ¢å¥è¯è¯´ï¼Œç®—æ³•åœ¨æ¯”è¾ƒè¿™ä¸¤ç»„æ•°æ®æ—¶åº”è¯¥å¾—å‡ºç›¸ä¼¼çš„å‘ç°ï¼Œä»è€Œä½¿å¾—éš¾ä»¥è¯†åˆ«è¯¥ä¸ªä½“çš„ä»»ä½•ç‹¬ç‰¹ä¿¡æ¯ã€‚è¿™ç§å®‰å…¨æ€§å¯ä»¥é˜²æ­¢ç§äººä¿¡æ¯æ³„éœ²ï¼Œä½†ä»ç„¶å¯ä»¥ä»æ•°æ®ä¸­å¾—å‡ºæœ‰ç”¨çš„è§è§£ã€‚
- en: Differential privacy initially appeared in the study â€œDifferential Privacyâ€
    by Cynthia Dwork [1] while she was working at Microsoft Research.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å·®åˆ†éšç§æœ€åˆå‡ºç°åœ¨Cynthia Dworkåœ¨å¾®è½¯ç ”ç©¶é™¢å·¥ä½œæœŸé—´çš„ç ”ç©¶ã€ŠDifferential Privacyã€‹ä¸­[1]ã€‚
- en: Letâ€™s take a look at an example to better understand how differential privacy
    helps to protect data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªä¾‹å­ï¼Œæ›´å¥½åœ°ç†è§£å·®åˆ†éšç§å¦‚ä½•å¸®åŠ©ä¿æŠ¤æ•°æ®ã€‚
- en: Examples of How Differential Privacy Safeguards Data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·®åˆ†éšç§å¦‚ä½•ä¿æŠ¤æ•°æ®çš„ç¤ºä¾‹
- en: Example 1
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹1
- en: In a study that looks at the link between social class and health results, researchers
    ask subjects for private information like where they live, how much money they
    have, and their medical background [2].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€é¡¹ç ”ç©¶ä¸­ï¼Œç ”ç©¶äººå‘˜è°ƒæŸ¥äº†ç¤¾ä¼šé˜¶å±‚ä¸å¥åº·ç»“æœä¹‹é—´çš„å…³ç³»ï¼Œè¦æ±‚å—è¯•è€…æä¾›ç§äººä¿¡æ¯ï¼Œå¦‚å±…ä½åœ°ã€è´¢å¯ŒçŠ¶å†µå’ŒåŒ»ç–—èƒŒæ™¯[2]ã€‚
- en: John, one of the participants, is worried that his personal information could
    get out and hurt his applications for life insurance or a mortgage. To make sure
    that Johnâ€™s worries are taken care of, the researchers can use differential privacy.
    This makes sure that any data that is shared wonâ€™t reveal specific information
    about him. Different levels of privacy can be shown by Johnâ€™s â€œopt-outâ€ situation,
    in which his data is left out of the study. This protects his anonymity because
    the analysis's results are not tied to any of his personal details.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å‚ä¸è€…ä¹‹ä¸€Johnæ‹…å¿ƒä»–çš„ä¸ªäººä¿¡æ¯å¯èƒ½ä¼šæ³„éœ²ï¼Œå½±å“ä»–ç”³è¯·äººå¯¿ä¿é™©æˆ–æŠµæŠ¼è´·æ¬¾ã€‚ä¸ºäº†ç¡®ä¿Johnçš„æ‹…å¿§å¾—åˆ°è§£å†³ï¼Œç ”ç©¶äººå‘˜å¯ä»¥ä½¿ç”¨å·®åˆ†éšç§ã€‚è¿™ç¡®ä¿äº†ä»»ä½•å…±äº«çš„æ•°æ®ä¸ä¼šé€éœ²å…³äºä»–çš„å…·ä½“ä¿¡æ¯ã€‚ä¸åŒçº§åˆ«çš„éšç§å¯ä»¥é€šè¿‡Johnçš„â€œé€€å‡ºâ€æƒ…å†µæ¥å±•ç¤ºï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»–çš„æ•°æ®è¢«æ’é™¤åœ¨ç ”ç©¶ä¹‹å¤–ã€‚è¿™ä¿æŠ¤äº†ä»–çš„åŒ¿åæ€§ï¼Œå› ä¸ºåˆ†æçš„ç»“æœä¸ä»–çš„ä¸ªäººç»†èŠ‚æ— å…³ã€‚
- en: Differential privacy seeks to protect privacy in the real world as if the data
    were being looked at in an opt-out situation. Since Johnâ€™s data is not part of
    the computation, the results regarding him can only be as accurate as the data
    available to everyone else.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å·®åˆ†éšç§æ—¨åœ¨ä¿æŠ¤ç°å®ä¸–ç•Œä¸­çš„éšç§ï¼Œå°±å¥½åƒæ•°æ®æ˜¯åœ¨é€‰æ‹©é€€å‡ºçš„æƒ…å†µä¸‹æŸ¥çœ‹çš„ä¸€æ ·ã€‚ç”±äºJohnçš„æ•°æ®ä¸å‚ä¸è®¡ç®—ï¼Œå› æ­¤å…³äºä»–çš„ç»“æœåªèƒ½ä¸å…¶ä»–äººå¯ç”¨çš„æ•°æ®ä¸€æ ·å‡†ç¡®ã€‚
- en: A precise description of differential privacy requires formal mathematical language
    and technical concepts, but the basic concept is to protect the privacy of individuals
    by limiting the information that can be obtained about them from the released
    data, thereby ensuring that their sensitive information remains private.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å·®åˆ†éšç§çš„å‡†ç¡®æè¿°éœ€è¦æ­£å¼çš„æ•°å­¦è¯­è¨€å’ŒæŠ€æœ¯æ¦‚å¿µï¼Œä½†åŸºæœ¬æ¦‚å¿µæ˜¯é€šè¿‡é™åˆ¶ä»å‘å¸ƒçš„æ•°æ®ä¸­å¯ä»¥è·å–çš„ä¸ªäººä¿¡æ¯é‡æ¥ä¿æŠ¤ä¸ªäººéšç§ï¼Œä»è€Œç¡®ä¿å…¶æ•æ„Ÿä¿¡æ¯ä¿æŒç§å¯†ã€‚
- en: Example 2
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹2
- en: The U.S. Census Bureau used a differential privacy framework as a part of its
    disclosure avoidance strategy to strike a compromise between the data collection
    and reporting needs and the privacy concerns of the respondents. You can find
    more information about the confidentiality protection provided by the U.S. Census
    Bureau [here](https://www.census.gov/library/working-papers/2022/adrm/CED-WP-2022-003.html).
    Moreover, Garfinkel provides an explanation of how DP was utilized in the 2020
    US Census data [here](https://mit-serc.pubpub.org/pub/differential-privacy-2020-us-census).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç¾å›½æ™®æŸ¥å±€ä½¿ç”¨äº†å·®åˆ†éšç§æ¡†æ¶ä½œä¸ºå…¶æŠ«éœ²é¿å…ç­–ç•¥çš„ä¸€éƒ¨åˆ†ï¼Œä»¥åœ¨æ•°æ®æ”¶é›†å’ŒæŠ¥å‘Šéœ€æ±‚ä¸å—è®¿è€…éšç§å…³åˆ‡ä¹‹é—´æ‰¾åˆ°å¦¥åã€‚æœ‰å…³ç¾å›½æ™®æŸ¥å±€æä¾›çš„ä¿å¯†ä¿æŠ¤çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—®[è¿™é‡Œ](https://www.census.gov/library/working-papers/2022/adrm/CED-WP-2022-003.html)ã€‚æ­¤å¤–ï¼ŒGarfinkelæä¾›äº†æœ‰å…³å·®åˆ†éšç§åœ¨2020å¹´ç¾å›½äººå£æ™®æŸ¥æ•°æ®ä¸­åº”ç”¨çš„è§£é‡Šï¼Œè¯·è§[è¿™é‡Œ](https://mit-serc.pubpub.org/pub/differential-privacy-2020-us-census)ã€‚
- en: Definition and key concepts
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰å’Œå…³é”®æ¦‚å¿µ
- en: The meaning of â€œdifferentialâ€ within the realm of DP
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: â€œå·®åˆ†éšç§â€åœ¨DPé¢†åŸŸä¸­çš„å«ä¹‰
- en: The term â€œdifferentialâ€ privacy refers to its emphasis on the dissimilarity
    between the results produced by a privacy-preserving algorithm on two datasets
    that differ by just one individualâ€™s data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: â€œå·®åˆ†â€éšç§è¿™ä¸ªæœ¯è¯­æŒ‡çš„æ˜¯å…¶å¯¹éšç§ä¿æŠ¤ç®—æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„ç»“æœå·®å¼‚çš„å¼ºè°ƒï¼Œè¿™ä¸¤ä¸ªæ•°æ®é›†ä¹‹é—´ä»…æœ‰ä¸€ä¸ªä¸ªä½“çš„æ•°æ®ä¸åŒã€‚
- en: Mechanism M
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœºåˆ¶ M
- en: A *mechanism* *M* is a mathematical method or process that is used on the data
    to make sure privacy is maintained while still giving useful information.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª*æœºåˆ¶* *M* æ˜¯ä¸€ç§æ•°å­¦æ–¹æ³•æˆ–è¿‡ç¨‹ï¼Œç”¨äºå¤„ç†æ•°æ®ä»¥ç¡®ä¿åœ¨æä¾›æœ‰ç”¨ä¿¡æ¯çš„åŒæ—¶ç»´æŠ¤éšç§ã€‚
- en: Epsilon (Îµ)
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Epsilon (Îµ)
- en: Îµ is a privacy parameter that controls the level of privacy given by a differentially
    private mechanism. In other words, Îµ regulates how much the output of the mechanism
    can vary between two neighboring databases and measures how much privacy is lost
    when the mechanism is run on the database [3].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Îµæ˜¯ä¸€ä¸ªéšç§å‚æ•°ï¼Œå®ƒæ§åˆ¶ç”±å·®åˆ†éšç§æœºåˆ¶æä¾›çš„éšç§çº§åˆ«ã€‚æ¢å¥è¯è¯´ï¼ŒÎµè°ƒèŠ‚æœºåˆ¶çš„è¾“å‡ºåœ¨ä¸¤ä¸ªç›¸é‚»æ•°æ®åº“ä¹‹é—´çš„å˜åŒ–ç¨‹åº¦ï¼Œå¹¶è¡¡é‡å½“æœºåˆ¶åœ¨æ•°æ®åº“ä¸Šè¿è¡Œæ—¶éšç§ä¸§å¤±çš„ç¨‹åº¦[3]ã€‚
- en: Stronger privacy guarantees are provided by a smaller Îµ, but the output may
    be less useful as a result [4]. Îµcontrols the amount of noise added to the data
    and shows how much the output probability distribution can change when the data
    of a single person is altered.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¾ƒå°çš„Îµæä¾›äº†æ›´å¼ºçš„éšç§ä¿éšœï¼Œä½†ç»“æœå¯èƒ½å› æ­¤å˜å¾—ä¸é‚£ä¹ˆæœ‰ç”¨[4]ã€‚Îµæ§åˆ¶æ·»åŠ åˆ°æ•°æ®ä¸­çš„å™ªå£°é‡ï¼Œå¹¶æ˜¾ç¤ºå½“ä¸€ä¸ªäººçš„æ•°æ®è¢«æ›´æ”¹æ—¶ï¼Œè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒå¯ä»¥å‘ç”Ÿçš„å˜åŒ–ã€‚
- en: Delta (ğ›¿)
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta (ğ›¿)
- en: ğ›¿ is an extra privacy option that lets you set how likely it is that your privacy
    will be compromised. Hence, ğ›¿ controls the probability of an extreme privacy breach,
    where the added noise (controlled by Îµ) does not provide sufficient protection.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ğ›¿æ˜¯ä¸€ä¸ªé¢å¤–çš„éšç§é€‰é¡¹ï¼Œå®ƒå…è®¸ä½ è®¾å®šéšç§è¢«æ³„éœ²çš„å¯èƒ½æ€§ã€‚å› æ­¤ï¼Œğ›¿æ§åˆ¶äº†æç«¯éšç§æ³„éœ²çš„æ¦‚ç‡ï¼Œå³æ·»åŠ çš„å™ªå£°ï¼ˆç”±Îµæ§åˆ¶ï¼‰æœªèƒ½æä¾›è¶³å¤Ÿçš„ä¿æŠ¤ã€‚
- en: ğ›¿ is a non-negative number that measures the chance of a data breach. It is
    usually very small and close to zero. This change makes it easier to do more complicated
    studies and machine learning models while still protecting privacy (see [4]).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ğ›¿æ˜¯ä¸€ä¸ªéè´Ÿæ•°ï¼Œç”¨äºè¡¡é‡æ•°æ®æ³„éœ²çš„å¯èƒ½æ€§ã€‚å®ƒé€šå¸¸éå¸¸å°ä¸”æ¥è¿‘äºé›¶ã€‚è¿™ä¸ªå˜åŒ–ä½¿å¾—åœ¨ä»ç„¶ä¿æŠ¤éšç§çš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œæ›´å¤æ‚çš„ç ”ç©¶å’Œæœºå™¨å­¦ä¹ æ¨¡å‹å˜å¾—æ›´åŠ å®¹æ˜“ï¼ˆå‚è§[4]ï¼‰ã€‚
- en: If ğ›¿ is low, there is less of a chance that someoneâ€™s privacy is going to get
    compromised. But this comes at a cost. If ğ›¿ is too small, more noise might be
    introduced into the data, diminishing the quality of the end-result. ğ›¿ is one
    parameter to consider, but it must be balanced with epsilon and the dataâ€™s practicality.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœğ›¿å¾ˆä½ï¼Œé‚£ä¹ˆæŸäººçš„éšç§è¢«æ³„éœ²çš„æœºä¼šè¾ƒå°ã€‚ä½†è¿™æœ‰ä»£ä»·ã€‚å¦‚æœğ›¿å¤ªå°ï¼Œå¯èƒ½ä¼šå¼•å…¥æ›´å¤šå™ªå£°ï¼Œé™ä½æœ€ç»ˆç»“æœçš„è´¨é‡ã€‚ğ›¿ æ˜¯ä¸€ä¸ªéœ€è¦è€ƒè™‘çš„å‚æ•°ï¼Œä½†å¿…é¡»ä¸ epsilon
    å’Œæ•°æ®çš„å®ç”¨æ€§è¿›è¡Œå¹³è¡¡ã€‚
- en: Unveiling the Mathematics Behind Differential Privacy
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­ç¤ºå·®åˆ†éšç§èƒŒåçš„æ•°å­¦
- en: Consider two databases, D and D', that differ by only one record.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸¤ä¸ªæ•°æ®åº“ D å’Œ D'ï¼Œå®ƒä»¬åªå› ä¸€æ¡è®°å½•ä¸åŒã€‚
- en: 'Formally, a mechanism M is Îµ-differentially private if, for any two adjacent
    datasets D and Dâ€™, and for any possible output O, the following holds:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å½¢å¼ä¸Šè®²ï¼Œå¦‚æœå¯¹äºä»»ä½•ä¸¤ä¸ªç›¸é‚»çš„æ•°æ®é›† D å’Œ Dâ€™ä»¥åŠä»»ä½•å¯èƒ½çš„è¾“å‡º Oï¼Œä»¥ä¸‹æ¡ä»¶æˆç«‹ï¼Œåˆ™æœºåˆ¶ M æ˜¯ Îµ-å·®åˆ†éšç§çš„ï¼š
- en: '![](../Images/2180335699dc30ba27c1a4213a95dfce.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2180335699dc30ba27c1a4213a95dfce.png)'
- en: 'However, we can reframe the above equation in terms of divergences, resulting
    in the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸Šè¿°æ–¹ç¨‹é‡æ–°è¡¨è¿°ä¸ºå‘æ•£é‡ï¼Œä»è€Œå¾—åˆ°ä»¥ä¸‹ç»“æœï¼š
- en: '![](../Images/5494a4e9e80abb08a64a578400c1e365.png)![](../Images/7c92b5ab70004d237615fc4579c6ba6c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5494a4e9e80abb08a64a578400c1e365.png)![](../Images/7c92b5ab70004d237615fc4579c6ba6c.png)'
- en: 'Figure 1: Differential privacy in the context of divergences (Image by Author).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šå·®åˆ†éšç§åœ¨å‘æ•£é‡èƒŒæ™¯ä¸‹çš„è¡¨ç°ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰ã€‚
- en: Here **div[â‹…âˆ£âˆ£â‹…]** denotes the RÃ©nyi divergence. See the paper [Renyi Differential
    Privacy](https://arxiv.org/abs/1702.07476) by Ilya Mironov for more information.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œ**div[â‹…âˆ£âˆ£â‹…]**è¡¨ç¤º RÃ©nyi å‘æ•£é‡ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§ Ilya Mironov çš„è®ºæ–‡ [RÃ©nyi Differential
    Privacy](https://arxiv.org/abs/1702.07476)ã€‚
- en: (Îµ, ğ›¿)-DP Definition
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (Îµ, ğ›¿)-DP å®šä¹‰
- en: A randomized *M* is considered (Îµ, ğ›¿)-differentially private if the probability
    of a significant privacy breach (i.e., a breach that would not occur under Îµ-differential
    privacy) is no more than ğ›¿. More formally, a mechanism M is (Îµ, ğ›¿)-differentially
    private if
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºåŒ–çš„*M* è¢«è®¤ä¸ºæ˜¯ (Îµ, ğ›¿)-å·®åˆ†éšç§çš„ï¼Œå¦‚æœæ˜¾è‘—éšç§æ³„éœ²ï¼ˆå³ï¼Œåœ¨ Îµ-å·®åˆ†éšç§ä¸‹ä¸ä¼šå‘ç”Ÿçš„æ³„éœ²ï¼‰çš„æ¦‚ç‡ä¸è¶…è¿‡ğ›¿ã€‚æ›´æ­£å¼åœ°è¯´ï¼Œæœºåˆ¶ M æ˜¯ (Îµ,
    ğ›¿)-å·®åˆ†éšç§çš„ï¼Œå¦‚æœ
- en: '![](../Images/ecfb021e6c59b32d894209bac8c92e90.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ecfb021e6c59b32d894209bac8c92e90.png)'
- en: If ğ›¿ = 0, then (Îµ, ğ›¿)-DP is reduced to a Îµ-DP.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœğ›¿ = 0ï¼Œåˆ™ (Îµ, ğ›¿)-DP å°†ç®€åŒ–ä¸º Îµ-DPã€‚
- en: (Îµ, ğ›¿)-DP mechanism may be thought of informally as Îµ-DP with a probability
    of 1 â€” ğ›¿.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (Îµ, ğ›¿)-DP æœºåˆ¶å¯ä»¥éæ­£å¼åœ°çœ‹ä½œæ˜¯å¸¦æœ‰ 1 â€” ğ›¿ æ¦‚ç‡çš„ Îµ-DPã€‚
- en: Properties of Differential Privacy
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·®åˆ†éšç§çš„å±æ€§
- en: 1\. Post-processing immunity
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. åå¤„ç†å…ç–«
- en: The differentially private output can be subjected to any function or analysis,
    and the outcome will continue to uphold the original privacy assurances. For instance,
    if you apply a differentially private mechanism to a dataset and then take the
    average age of the individuals in the dataset, the resulting average age will
    still be differentially private and will provide the same level of privacy assurances
    as the output it was originally designed to provide.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒially ç§å¯†çš„è¾“å‡ºå¯ä»¥å—åˆ°ä»»ä½•å‡½æ•°æˆ–åˆ†æçš„å½±å“ï¼Œç»“æœä»å°†ä¿æŒåŸå§‹çš„éšç§ä¿éšœã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ å¯¹æ•°æ®é›†åº”ç”¨ä¸€ä¸ªä¸åŒially ç§å¯†çš„æœºåˆ¶ï¼Œç„¶åè®¡ç®—æ•°æ®é›†ä¸­ä¸ªä½“çš„å¹³å‡å¹´é¾„ï¼Œé‚£ä¹ˆå¾—åˆ°çš„å¹³å‡å¹´é¾„ä»ç„¶æ˜¯ä¸åŒially
    ç§å¯†çš„ï¼Œå¹¶å°†æä¾›ä¸å…¶æœ€åˆè®¾è®¡æ—¶ç›¸åŒæ°´å¹³çš„éšç§ä¿éšœã€‚
- en: Thanks to the post-processing feature, we can use differentially private mechanisms
    in the same way as generic ones. Hence, it is possible to combine several differentially
    private mechanisms without sacrificing the integrity of differential privacy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šäºäº†åå¤„ç†åŠŸèƒ½ï¼Œæˆ‘ä»¬å¯ä»¥åƒä½¿ç”¨é€šç”¨æœºåˆ¶ä¸€æ ·ä½¿ç”¨ä¸åŒially ç§å¯†çš„æœºåˆ¶ã€‚å› æ­¤ï¼Œå¯ä»¥åœ¨ä¸ç‰ºç‰²å·®åˆ†éšç§å®Œæ•´æ€§çš„æƒ…å†µä¸‹ï¼Œç»“åˆå¤šç§ä¸åŒially ç§å¯†çš„æœºåˆ¶ã€‚
- en: 2\. Composition
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. ç»„åˆ
- en: When multiple differentially private techniques are used on the same data or
    when queries are combined, composition is the property that ensures the privacy
    guarantees of differential privacy still apply. Composition can be either sequential
    or parallel. If you apply two mechanisms, *M1* with Îµ1-DP and *M2* with Îµ2-DP
    on a dataset, then the composition of *M1* and *M2* is at least (Îµ1 + Îµ2)-DP.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å¯¹ç›¸åŒçš„æ•°æ®ä½¿ç”¨å¤šç§ä¸åŒially ç§å¯†æŠ€æœ¯æˆ–å°†æŸ¥è¯¢ç»„åˆæ—¶ï¼Œç»„åˆæ˜¯ç¡®ä¿å·®åˆ†éšç§çš„éšç§ä¿éšœä»ç„¶é€‚ç”¨çš„å±æ€§ã€‚ç»„åˆå¯ä»¥æ˜¯é¡ºåºçš„æˆ–å¹¶è¡Œçš„ã€‚å¦‚æœä½ åœ¨æ•°æ®é›†ä¸Šåº”ç”¨ä¸¤ä¸ªæœºåˆ¶ï¼Œ*M1*
    å…·æœ‰ Îµ1-DP å’Œ *M2* å…·æœ‰ Îµ2-DPï¼Œåˆ™ *M1* å’Œ *M2* çš„ç»„åˆè‡³å°‘æ˜¯ (Îµ1 + Îµ2)-DPã€‚
- en: 'WARNING: Despite compositionâ€™s ability to protect privacy, the composition
    theorem makes clear that there is a ceiling; as the value of Îµ rises, so does
    the amount of privacy lost whenever a new mechanism is employed. If Îµ becomes
    too large, then differential privacy guarantees are mostly meaningless [3].'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è­¦å‘Šï¼šå°½ç®¡ç»„åˆå…·æœ‰ä¿æŠ¤éšç§çš„èƒ½åŠ›ï¼Œä½†ç»„åˆå®šç†æ˜ç¡®è¡¨æ˜å­˜åœ¨ä¸Šé™ï¼›éšç€Îµå€¼çš„å¢åŠ ï¼Œæ¯å½“ä½¿ç”¨æ–°æœºåˆ¶æ—¶ï¼Œéšç§æŸå¤±çš„é‡ä¹Ÿä¼šå¢åŠ ã€‚å¦‚æœÎµå˜å¾—è¿‡å¤§ï¼Œé‚£ä¹ˆå·®åˆ†éšç§ä¿è¯å¤§å¤šæ˜¯æ— æ„ä¹‰çš„[3]ã€‚
- en: '3\. Robustness to auxiliary information:'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. å¯¹è¾…åŠ©ä¿¡æ¯çš„é²æ£’æ€§ï¼š
- en: Differential privacy is resistant to auxiliary information attackers, which
    means that even if an attacker has access to other relevant data, they will not
    be able to learn anything about a person from a DP output. For instance, if a
    hospital were to share differentially private information regarding individualsâ€™
    medical situations, an attacker with access to other medical records would not
    be able to greatly increase their knowledge of a given patient from the published
    numbers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å·®åˆ†éšç§å¯¹è¾…åŠ©ä¿¡æ¯æ”»å‡»è€…å…·æœ‰æŠµæŠ—åŠ›ï¼Œè¿™æ„å‘³ç€å³ä½¿æ”»å‡»è€…å¯ä»¥è®¿é—®å…¶ä»–ç›¸å…³æ•°æ®ï¼Œä»–ä»¬ä¹Ÿæ— æ³•ä»å·®åˆ†éšç§è¾“å‡ºä¸­å¾—çŸ¥æœ‰å…³æŸä¸ªäººçš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœåŒ»é™¢åˆ†äº«æœ‰å…³ä¸ªäººåŒ»ç–—æƒ…å†µçš„å·®åˆ†éšç§ä¿¡æ¯ï¼Œæ”»å‡»è€…å³ä½¿æ‹¥æœ‰å…¶ä»–åŒ»ç–—è®°å½•ï¼Œä¹Ÿæ— æ³•é€šè¿‡å‘å¸ƒçš„æ•°å­—æ˜¾è‘—å¢åŠ å¯¹æŸä¸ªæ‚£è€…çš„äº†è§£ã€‚
- en: Common Misunderstandings
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¸¸è§è¯¯è§£
- en: 'The notion of differential privacy has been misunderstood in several publications,
    especially during its early days. Dwork *et al.* wrote a short paper [5] to correct
    some widespread misunderstandings. Here are a few examples of common misunderstandings:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å·®åˆ†éšç§çš„æ¦‚å¿µåœ¨æ—©æœŸçš„å‡ ç¯‡å‡ºç‰ˆç‰©ä¸­è¢«è¯¯è§£äº†ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶æ—©æœŸé˜¶æ®µã€‚Dwork *et al.* æ’°å†™äº†ä¸€ç¯‡ç®€çŸ­çš„è®ºæ–‡[5]æ¥çº æ­£ä¸€äº›å¹¿æ³›å­˜åœ¨çš„è¯¯è§£ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§è¯¯è§£çš„ä¾‹å­ï¼š
- en: DP is not an algorithm but rather a definition. DP is a mathematical guarantee
    that an algorithm must meet in order to disclose statistics about a dataset. Several
    distinct algorithms meet the criteria.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å·®åˆ†éšç§ä¸æ˜¯ä¸€ç§ç®—æ³•ï¼Œè€Œæ˜¯ä¸€ç§å®šä¹‰ã€‚å·®åˆ†éšç§æ˜¯ä¸€ä¸ªæ•°å­¦ä¿è¯ï¼Œç®—æ³•å¿…é¡»æ»¡è¶³è¯¥ä¿è¯æ‰èƒ½å…¬å¼€æ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯ã€‚æœ‰å¤šä¸ªä¸åŒçš„ç®—æ³•ç¬¦åˆè¿™ä¸€æ ‡å‡†ã€‚
- en: Various algorithms can be differentialy private while still meeting various
    requirements. If someone claims that differential privacy, a specific requirement
    on ratios of probability distributions, is incompatible with any accuracy target,
    they must provide evidence for that claim. This means proving that there is no
    way a DP algorithm can perform to some specified standard. Itâ€™s challenging to
    come up with that proof, and our first guesses about what is and isnâ€™t feasible
    are often off.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å„ç§ç®—æ³•å¯ä»¥æ˜¯å·®åˆ†éšç§çš„ï¼ŒåŒæ—¶ä»ç„¶æ»¡è¶³ä¸åŒçš„è¦æ±‚ã€‚å¦‚æœæœ‰äººå£°ç§°å·®åˆ†éšç§ï¼Œå³å¯¹æ¦‚ç‡åˆ†å¸ƒçš„ç‰¹å®šè¦æ±‚ï¼Œä¸ä»»ä½•å‡†ç¡®åº¦ç›®æ ‡ä¸å…¼å®¹ï¼Œä»–ä»¬å¿…é¡»æä¾›è¯æ®æ¥æ”¯æŒè¿™ä¸€ä¸»å¼ ã€‚è¿™æ„å‘³ç€è¦è¯æ˜æ²¡æœ‰ä»»ä½•å·®åˆ†éšç§ç®—æ³•èƒ½å¤Ÿè¾¾åˆ°æŸä¸ªæŒ‡å®šçš„æ ‡å‡†ã€‚è¿™ç§è¯æ˜å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæˆ‘ä»¬å¯¹ä»€ä¹ˆæ˜¯å¯è¡Œçš„é¦–ä¸ªçŒœæµ‹å¾€å¾€ä¸å‡†ç¡®ã€‚
- en: There are no â€œgoodâ€ or â€œbadâ€ results for any given database. Generating the
    outputs in a way that preserves privacy (perfect or differential) is the key.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹ä»»ä½•ç»™å®šçš„æ•°æ®åº“ï¼Œæ²¡æœ‰â€œå¥½â€æˆ–â€œåâ€çš„ç»“æœã€‚ä»¥ä¿æŠ¤éšç§çš„æ–¹å¼ç”Ÿæˆè¾“å‡ºï¼ˆæ— è®ºæ˜¯å®Œç¾éšç§è¿˜æ˜¯å·®åˆ†éšç§ï¼‰æ˜¯å…³é”®ã€‚
- en: Conclusion
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: DP has shown itself as a viable paradigm for the protection of data privacy,
    which is particularly important in this day and age, when machine learning and
    big data are becoming more widespread. Several key concepts were covered in this
    essay, including the various DP control settings like Îµ and *Î´*. In addition,
    we provided several mathematical definitions of the DP. We also explained key
    features of the DP and addressed some of the most common misconceptions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å·®åˆ†éšç§ï¼ˆDPï¼‰å·²è¢«è¯æ˜æ˜¯ä¸€ç§å¯è¡Œçš„æ•°æ®éšç§ä¿æŠ¤èŒƒå¼ï¼Œè¿™åœ¨å½“ä»Šæœºå™¨å­¦ä¹ å’Œå¤§æ•°æ®æ—¥ç›Šæ™®åŠçš„æ—¶ä»£å°¤ä¸ºé‡è¦ã€‚æœ¬æ–‡æ¶µç›–äº†å‡ ä¸ªå…³é”®æ¦‚å¿µï¼ŒåŒ…æ‹¬å„ç§å·®åˆ†éšç§æ§åˆ¶è®¾ç½®å¦‚Îµå’Œ*Î´*ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å·®åˆ†éšç§çš„å‡ ä¸ªæ•°å­¦å®šä¹‰ï¼Œè§£é‡Šäº†å·®åˆ†éšç§çš„å…³é”®ç‰¹æ€§ï¼Œå¹¶è§£å†³äº†ä¸€äº›æœ€å¸¸è§çš„è¯¯è§£ã€‚
- en: References
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Dwork, Cynthia (2006). â€œDifferential Privacy.â€ In *Proceedings of the 33rd
    International Colloquium on Automata, Languages and Programming*, 1â€“12\. Berlin,
    Heidelberg: Springer Berlin Heidelberg. [https://doi.org/10.1007/11787006_1](https://doi.org/10.1007/11787006_1).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Dwork, Cynthia (2006). â€œDifferential Privacy.â€ In *Proceedings of the 33rd
    International Colloquium on Automata, Languages and Programming*, 1â€“12\. Berlin,
    Heidelberg: Springer Berlin Heidelberg. [https://doi.org/10.1007/11787006_1](https://doi.org/10.1007/11787006_1).'
- en: '[2] Wood, Alexandra, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi,
    James Honaker, Kobbi Nissim, David Oâ€™Brien, Thomas Steinke, and Salil Vadhan (2018).
    â€œDifferential Privacy: A Primer for a Non-Technical Audience.â€ *Vand. J. Ent.
    & Tech. L.* 21 (1): 209â€“76\. [https://doi.org/10.2139/ssrn.3338027](https://doi.org/10.2139/ssrn.3338027).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Wood, Alexandra, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi,
    James Honaker, Kobbi Nissim, David Oâ€™Brien, Thomas Steinke å’Œ Salil Vadhan (2018).
    â€œå·®åˆ†éšç§: ä¸ºéæŠ€æœ¯å—ä¼—å‡†å¤‡çš„å…¥é—¨æŒ‡å—ã€‚â€ *Vand. J. Ent. & Tech. L.* 21 (1): 209â€“76ã€‚ [https://doi.org/10.2139/ssrn.3338027](https://doi.org/10.2139/ssrn.3338027)ã€‚'
- en: '[3] Brubaker, M., and S. Prince (2021). â€œTutorial #12: Differential Privacy
    I: Introduction.â€ *Borealis AI*. [https://www.borealisai.com/research-blogs/tutorial-12-differential-privacy-i-introduction/](https://www.borealisai.com/research-blogs/tutorial-12-differential-privacy-i-introduction/).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Brubaker, M., å’Œ S. Prince (2021). â€œæ•™ç¨‹ #12: å·®åˆ†éšç§ I: ä»‹ç»ã€‚â€ *Borealis AI*.
    [https://www.borealisai.com/research-blogs/tutorial-12-differential-privacy-i-introduction/](https://www.borealisai.com/research-blogs/tutorial-12-differential-privacy-i-introduction/)ã€‚'
- en: '[4] Dwork, Cynthia, Aaron Roth, et al. (2014). â€œThe Algorithmic Foundations
    of Differential Privacy.â€ *Foundations and Trends in Theoretical Computer Science*
    9 (3â€“4): 211â€“407.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Dwork, Cynthia, Aaron Roth ç­‰. (2014). â€œå·®åˆ†éšç§çš„ç®—æ³•åŸºç¡€ã€‚â€ *ç†è®ºè®¡ç®—æœºç§‘å­¦åŸºç¡€ä¸è¶‹åŠ¿* 9 (3â€“4):
    211â€“407ã€‚'
- en: '[5] Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2011\. â€œDifferential
    Privacy â€” A Primer for the Perplexed.â€ *Joint UNECE/Eurostat Work Session on Statistical
    Data Confidentiality* 11.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Dwork, Cynthia, Frank McSherry, Kobbi Nissim å’Œ Adam Smith. 2011. â€œå·®åˆ†éšç§
    â€” ä¸ºå›°æƒ‘è€…å‡†å¤‡çš„å…¥é—¨æŒ‡å—ã€‚â€ *è”åˆå›½æ¬§æ´²ç»æµå§”å‘˜ä¼š/æ¬§æ´²ç»Ÿè®¡å±€ç»Ÿè®¡æ•°æ®ä¿å¯†å·¥ä½œä¼šè®®* 11ã€‚'
- en: '*Originally published at* [*https://ealizadeh.com*](https://ealizadeh.com/blog/abc-of-differential-privacy/)
    *on April 27, 2023.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ€åˆå‘å¸ƒäº* [*https://ealizadeh.com*](https://ealizadeh.com/blog/abc-of-differential-privacy/)
    *äº2023å¹´4æœˆ27æ—¥ã€‚*'
