# 场景表示网络

> 原文：[https://towardsdatascience.com/scene-representation-networks-bae6186d00d9](https://towardsdatascience.com/scene-representation-networks-bae6186d00d9)

## 在无限分辨率下建模复杂的3D场景

[](https://wolfecameron.medium.com/?source=post_page-----bae6186d00d9--------------------------------)[![Cameron R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----bae6186d00d9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bae6186d00d9--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bae6186d00d9--------------------------------) [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----bae6186d00d9--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bae6186d00d9--------------------------------) ·阅读时间12分钟·2023年2月22日

--

![](../Images/b9e69450b3fa7f9dc92eb4c50cf7c7bc.png)

（照片由 [Alexandra Gorn](https://unsplash.com/@alexagorn?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 提供，来源于 [Unsplash](https://unsplash.com/s/photos/living-room?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）

我们最近已经看到（例如，通过[DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)和[ONets](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets) [3, 5]）如何用神经网络表示3D几何体。但是，这些方法有一些限制，比如需要访问真实数据、3D几何体进行训练和推断。而且，*如果我们想表示整个场景而不仅仅是一个物体或几何体*呢？这需要同时建模几何体和外观；下面有一个示例。幸运的是，神经网络在正确的方法下完全能够以这种方式建模3D场景。

![](../Images/0c7b3bc733de45910ed93862eab0bc9f.png)

（来源于 [1] 和 [3]）

一种建模3D场景的方法是通过场景表示网络（SRNs）[1]。SRNs将场景建模为一个连续函数，该函数将每个3D坐标映射到描述该位置物体形状和外观的表示。这个函数通过前馈神经网络进行学习。然后，SRNs使用可学习的渲染算法生成底层3D场景的新视角（即，仅2D图像）。前馈网络和渲染算法都可以通过仅使用场景的2D图像进行端到端的训练。

与以前的工作相比，SRNs非常有用，因为它们：

+   直接强制3D结构，这鼓励场景的不同视角保持一致。

+   仅需场景的2D图像进行训练。

+   在有限的训练数据下产生更好的结果。

+   建模场景的连续（而非离散）表示，这种表示可以以任意分辨率渲染。

我们可以使用SRNs生成准确的3D场景表示，这对机器人操作和虚拟现实中复杂场景的渲染等应用具有重大影响。

![](../Images/f4835333ec6649baaf8389c6e182bf55.png)

(来自 [1])

# 背景

到目前为止，我们已经回顾了[DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)和[ONet](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)模型来表示3D几何。这些方法从未考虑像SRNs那样建模几何和外观。然而，从这些模型中有一些有用的背景概念在这里会很有用。

+   前馈神经网络 [[link](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]

+   3D形状表示 [[link](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)]

+   签名距离函数 [[link](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)]

但这并不是我们需要了解的全部内容。让我们深入一些背景概念，这些概念对于建立对SRNs工作原理的理解将非常有用。

**超网络。** SRNs是包含多个不同神经网络模块的复杂模型。其中一个模块是特定类型的神经网络，称为超网络。这可能听起来很复杂，但*超网络只是生成另一个神经网络权重的神经网络*。

![](../Images/94b31c7eaf6058003da570c016754b9d.png)

超网络是用于输出另一个神经网络权重的神经网络（由作者创建）。

在SRNs的情况下，我们的超网络是一个前馈神经网络，它接受一个潜在向量（即对应特定场景的唯一向量）作为输入，并产生一个向量作为输出。我们不直接使用这个向量，而是将其值作为另一个前馈神经网络的权重。我们的超网络的输出被用作SRN中另一个神经网络的权重；见上文。

![](../Images/0e52de3ada0d2dde088ab5a513413604.png)

RNN处理有序序列，通过前馈变换的同时保持对序列的记忆（来自 [4]）

**长短期记忆网络。** SRN的另一个组件基于一种称为长短期记忆（LSTM）网络的递归神经网络（RNN）。RNN类似于前馈网络。它们接收一个向量作为输入，产生一个向量作为输出，并在中间进行几个线性变换和非线性变换。主要区别在于，RNN处理的是向量序列，而不仅仅是单个向量；见上文。

给定一个有序的向量序列作为输入，RNN 会使用第一个向量作为输入，产生一个输出，然后将其隐藏状态作为附加输入传递给第二个向量进行处理。这个过程会对每个向量重复，直到我们到达序列的末尾。因此，RNN 类似于前馈网络，但我们在处理一个时间序列的输入，并且维护一个每次看到新输入时都会*(i)* 更新的隐藏/记忆状态，并且*(ii)* 这个状态作为附加输入用于生成每个时间步或序列位置的输出。

![](../Images/416e64923e50f4fa3aaa8d0c0c0225c5.png)

LSTM 使用门控机制来更好地管理用于序列的记忆（来自 [4]）

RNN 的一个主要问题是它们难以处理非常长的序列。我们试图将有关该序列的所有信息存储在一个固定大小的隐藏/记忆状态中，并传递到每个序列位置！为了解决这个问题，LSTM 引入了门控机制到 RNN 的前向传递中，使 RNN 能够处理更长的序列并更好地管理存储在其记忆中的信息；见上文。要了解更多关于 LSTM 的信息，请查看深入了解 [这里](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)。

**相机视角。** 要理解 SRNs，我们需要简要探讨一些通用的计算机视觉概念。特别是，我们需要了解相机视角和参数。给定一个基础场景，场景表示模型的目的是准确地从不同的相机视角生成地面真实场景的 2D 视图或图像。

直观地，我们可以将其视为类似于一个人在现实生活中拍摄一个 3D 场景。我们每天使用相机将现实世界投射到 2D 图像中；见下文。

![](../Images/a9a4ea4971d53142f7a5ca04d2db070a.png)

（来自 [2]）

我们生成的场景视图依赖于多个因素，包括相机的位置、方向和属性。在计算机视觉中，我们将计算外部世界准确的 2D 图像所需的所有参数分为两组：外部参数和内部参数。

外部参数包括相机位置和方向等属性，而内部参数则捕捉相机的内部属性（例如，焦距、分辨率等）。有关更多详细信息，请参见文章 [这里](/what-are-intrinsic-and-extrinsic-camera-parameters-in-computer-vision-7071b72fb8ec)。

对于 SRNs，我们实际上需要的不仅仅是图像来训练我们的模型——我们还需要相关的相机参数。这些额外的信息使我们能够了解用于构建图像的位置、方向和相机属性，这对于理解基础场景至关重要。*如果不知道观察场景的视角，我们怎么知道如何生成场景的图像呢？*

# SRNs 如何工作？

SRN 具有两个基本组件：场景表示模型和渲染函数。我们将首先建立对训练 SRN 所需数据的理解，然后概述这些组件如何工作及其如何结合，形成一个强大的场景表示和渲染系统。

**数据。** 我们首先考虑在与单一场景的不同视角相对应的图像上训练 SRN。获得这些图像后，我们可以训练 SRN 生成与场景任意视角对应的高分辨率图像。然而，单凭图像是不够的。我们还需要每个视角对应的 [外部和内部相机参数](/what-are-intrinsic-and-extrinsic-camera-parameters-in-computer-vision-7071b72fb8ec)。因此，我们的训练数据集将如下所示：

![](../Images/aaee0af6a0d6531d909545d434e8c1be.png)

（来自 [1]）

其中每个 I 是一幅图像，E 是外部参数矩阵，K 是内部参数矩阵。内部参数是相机特定的，而外部参数是相机对场景视角的特定参数。

![](../Images/d661b9b0c046c75e3fe4af65ed87b218.png)

（来自 [1]）

**场景表示。** SRNs 将场景表示为将空间位置（即 `[x, y, z]` 坐标）映射到特征向量的函数。这些特征包含关于指定位置的场景几何和外观的信息（例如，表面颜色，[符号距离](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions) 等）。为了建模这个场景函数，我们使用一个 [前馈神经网络](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)（即简单架构！），它以 `[x, y, z]` 坐标作为输入，并产生这个特征向量作为输出；见上文。

**神经渲染。** 一旦我们为场景中的几个不同空间位置生成了相关的场景特征，*我们如何利用这些特征来渲染场景的一个新视角？* 我们的目标是以由上述神经网络生成的场景表示——在场景中的多个不同空间位置评估——和相机参数矩阵作为输入，然后利用这些信息生成场景的新视角。 SRNs 使用两个独立的模块来完成这个任务。

首先，我们必须推断场景的几何。为此，我们使用 [Ray Marching 算法](https://michaelwalczyk.com/blog-ray-marching.html) 的修改版。我不会详细描述这个算法，但基本思想是：

1.  考虑不同的相机视角

1.  从相机的位置向场景发射“相机光线”

1.  通过寻找光线与物体的交点来推断几何

使用这个过程，我们可以推断出场景中存在的底层物体和几何。

![](../Images/68d9bcdc4ae0f87bed5c8cd60833edd2.png)

（来自 [1]）

在 [1] 中，作者通过使用 LSTM 执行每一个步骤来使光线行进过程可学习，如上图所示。这样，光线行进过程可以使用我们的训练数据进行更新，从而变得更快更准确！有关光线行进的更多细节，请查看 [这里](https://michaelwalczyk.com/blog-ray-marching.html)。

接下来，我们必须通过将每个空间坐标的特征向量映射到相关颜色来揭示场景的外观。在 [1] 中，这种转换再次通过前馈神经网络建模，该网络 *(i)* 以每个坐标的特征向量为输入，并 *(ii)* 生成每个坐标的 RGB 像素值。

**处理多个场景。** 到目前为止，我们只考虑过在与同一场景对应的一组图像上训练 SRN。但，*如果我们想建模多个不同的场景呢？*

![](../Images/29a2e10e6a8a7b8051f2d7011338203d.png)

（来自 [3]）

在 [1] 中，我们将每个场景与一个唯一的潜在向量关联，该向量通过自动编码器架构学习（即，类似于 DeepSDFs [3] 使用的架构！）；见上文。使用这个潜在向量作为输入，我们可以使用一个超网络（即，输出另一个神经网络权重的神经网络）来生成前馈特征表示网络的权重。这种方法使我们能够为每个场景创建独特的特征向量，同时学习能够跨场景泛化的模式。

![](../Images/cc1b5448546b4ea1b3f2b8be58373770.png)

（来自 [1]）

**全局视角。** 上面展示了完整的 SRN 模型。该模型包含多个组件，包括：

1.  一个前馈网络，用于生成给定场景中每个空间位置的特征表示。

1.  一个单独的（前馈）超网络，用于为特定场景生成上述网络的权重。

1.  基于 LSTM 的可学习光线行进模块，用于渲染场景中的 3D 几何形状。

1.  一个前馈网络，用于生成建模场景外观的 RGB 像素值。

所有这些组件结合在一起形成一个端到端可训练的 SRN，能够从带有相机视角信息的图像数据集中学习输出任意的场景视角。

# 它们表现得好吗？

在 [1] 中，SRN 在大规模的 3D 数据集上进行评估，如 [ShapeNet](https://shapenet.org/) 和 [DeepVoxels](https://www.vincentsitzmann.com/deepvoxels/)，以及较小的合成 3D 对象数据集。性能通过两种方式进行测量：

1.  SRN 生成的训练对象视角的准确性

1.  少样本生成的保留测试对象的准确性

为了生成测试对象的视角（即，在训练过程中从未观察过的视角），我们需要首先观察对象的几个样本视角，以便我们可以求解场景的最佳潜在向量。然后，我们使用 SRN 生成带有该潜在向量的视角（即，“少量样本”生成！）。

![](../Images/5db6700e5f439de42028e801f8e4211f.png)

（来源于 [1]）

在较小的合成数据集上，SRNs 明显优于基线技术，这可以通过对模型输出的定性检查来看出；见上文。这些较小数据集上的 SRN 结果几乎达到像素级完美，表明该方法在训练数据有限的情况下表现良好。实际上，我们可以在这些实验中看到，SRNs 通常能够恢复在数据中观察到的底层场景的任何组件。

![](../Images/151133c51c205df9642ecdfc9b425937.png)

（来源于 [1]）

在更大的数据集上，SRNs 在生成已知对象的新视角和进行少样本生成方面，相对于基线方法表现良好；见上文。特别是，SRNs 在少样本情况下表现出色，在这种情况下，模型可以利用训练期间观察到的场景中的学习模式（即，称为先验信息）来呈现新场景的合理视角，数据有限。总体而言，我们可以看到 SRN 输出 *(i)* 相对于先前的模型更清晰、更准确，并且 *(ii)* 在提供更多样本/信息时会有所改善；见下文。

![](../Images/1434e3b95bd46cc1ebb11d81e699cb03.png)

（来源于 [1]）

类似于先前的工作，我们可以插值 SRN 的潜在空间中的嵌入来生成新场景。这些结果，如下所示，表明 SRN 的嵌入空间包含有关底层场景的有用、结构化的信息。

![](../Images/2f119039030c669634b2f6b6c67ab725.png)

（来源于 [1]）

# 主要收获

相对于我们在之前综述中看到的工作，SRNs 很有用，因为它们 *(i)* 允许我们建模的不仅仅是几何信息（即，既包括几何信息也包括外观），而且 *(ii)* 可以仅使用图像进行训练。使用 SRNs，我们可以以端到端的、可学习的方式生成任意分辨率的 3D 场景的新视角。此方法的主要收获列在下方。

**直接编码 3D 信息。** SRNs 直接将 3D 信息注入到视角生成过程中，因为特征表示网络（和像素生成网络）以 `[x, y, z]` 坐标作为输入。此方法并非总是被先前的方法所采用。然而，对于 SRNs，这种直接使用 3D 空间信息的方法使模型能够在同一场景的不同视角之间生成更一致的结果。

**没有地面真实几何信息。** SRNs 仅使用图像和一些相关的相机参数进行训练，这些参数揭示了每张图像的相关视角信息。与先前的方法（例如，[DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf) [3]），这些方法只能在直接访问底层 3D 几何部分时进行推断不同，SRNs 非常灵活。至少，从图像中建模场景是朝着正确方向迈出的一步。

**简单网络并不总是便宜的。** 3D 深度学习方法的一个优点（至少是我们迄今见过的那些）是它们大多使用简单的前馈网络。与需要大规模深度学习模型的研究主题相比（例如，[语言建模](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)），这些简单的网络相当不错！然而，即使底层网络很简单，训练过程仍然可能很昂贵。在 [1] 中，*作者声称大多数 SRN 需要大约一周的时间在单个 GPU 上进行训练*。

![](../Images/7ecf6f646a19e969386a8707f23e342c.png)

(来源于 [1])

**限制。** SRN 在许多方面表现不佳。特别是，它们往往无法捕捉细微的细节，例如物体中的孔洞（例如，见上图中的椅子孔洞）。它们还难以处理与训练数据分布不相似的物体，无法捕捉由于光照条件或半透明度变化带来的效果。未来的工作如 NeRF [2] 通过缓解这些问题，推动向更简单的模型发展，这些模型能够表示复杂的场景和效果；例如，包含许多物体和可变光照及反射条件的整个房间。

## 结束语

非常感谢你阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，一名在 [Alegion](https://www.alegion.com/) 的研究科学家以及在莱斯大学攻读深度学习的实证和理论基础的博士生。你也可以查看我在 medium 上的 [其他文章](https://medium.com/@wolfecameron)！如果你喜欢这篇文章，请在 [twitter](https://twitter.com/cwolferesearch) 上关注我，或者订阅我的 [Deep (Learning) Focus 通讯](https://cameronrwolfe.substack.com/)，我在其中撰写关于重要深度学习主题的易懂概述。

## 参考文献

[1] Sitzmann, Vincent, Michael Zollhöfer, 和 Gordon Wetzstein. “场景表示网络：连续的 3D 结构感知神经场景表示。” *神经信息处理系统进展* 32 (2019)。

[2] Mildenhall, Ben, 等. “Nerf：将场景表示为神经辐射场以进行视图合成。” *ACM 通讯* 65.1 (2021): 99–106。

[3] Park, Jeong Joon, 等. “Deepsdf：学习连续签名距离函数以进行形状表示。” *IEEE/CVF 计算机视觉与模式识别会议论文集*。2019。

[4] Zaremba, Wojciech, Ilya Sutskever, 和 Oriol Vinyals. “递归神经网络正则化。” *arXiv 预印本 arXiv:1409.2329* (2014).

[5] Mescheder, Lars, 等. “占据网络：在函数空间中学习 3D 重建。” *IEEE/CVF 计算机视觉与模式识别会议论文集*。2019。
