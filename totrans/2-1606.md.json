["```py\n// global state\nstruct ggml_state g_state;\n\nstruct ggml_state {\n    struct ggml_context_container contexts[GGML_MAX_CONTEXTS];\n};\n```", "```py\nstruct ggml_context_container {\n    bool used;\n\n    struct ggml_context context;\n};\n```", "```py\nstruct ggml_context {\n    size_t mem_size;\n    void * mem_buffer;\n    bool   mem_buffer_owned;\n\n    int n_objects;\n\n    struct ggml_object * objects_begin;\n    struct ggml_object * objects_end;\n};\n```", "```py\n*ctx = (struct ggml_context) {\n        .mem_size         = params.mem_size,\n        .mem_buffer       = params.mem_buffer ? params.mem_buffer : malloc(params.mem_size),\n        .mem_buffer_owned = params.mem_buffer ? false : true,\n        .n_objects        = 0,\n        .objects_begin    = NULL,\n        .objects_end      = NULL,\n    };\n```", "```py\nGGML_PRINT(\"%s: context %p with %zu bytes of memory\\n\", __func__, (void *) ctx, ctx->mem_size);\nGGML_PRINT(\"%s: context %p with %d objects\\n\", __func__, (void *) ctx, ctx->n_objects);\nGGML_PRINT(\"%s: context %p with %p object starting position\\n\", __func__, (void *) ctx, (void *) ctx->objects_begin);\nGGML_PRINT(\"%s: context %p with %p object ending position\\n\", __func__, (void *) ctx, (void *) ctx->objects_end);\n```", "```py\nstruct ggml_tensor * ggml_new_tensor_impl(\n        struct ggml_context * ctx,\n        enum   ggml_type type,\n        int    n_dims,\n        const int* ne,\n        void*  data) {\n    // always insert objects at the end of the context's memory pool\n    struct ggml_object * obj_cur = ctx->objects_end;\n\n    const size_t cur_offset = obj_cur == NULL ? 0 : obj_cur->offset;\n    const size_t cur_size   = obj_cur == NULL ? 0 : obj_cur->size;\n    const size_t cur_end    = cur_offset + cur_size;\n\n    size_t size_needed = 0;\n\n    if (data == NULL) {\n        size_needed += GGML_TYPE_SIZE[type];\n        GGML_PRINT(\"Size needed %zu \", size_needed);\n        for (int i = 0; i < n_dims; i++) {\n            size_needed *= ne[i];\n        }\n        // align to GGML_MEM_ALIGN\n        size_needed = ((size_needed + GGML_MEM_ALIGN - 1)/GGML_MEM_ALIGN)*GGML_MEM_ALIGN;\n\n    }\n    size_needed += sizeof(struct ggml_tensor);\n\n    if (cur_end + size_needed + GGML_OBJECT_SIZE > ctx->mem_size) {\n        GGML_PRINT(\"\\n%s: not enough space in the context's memory pool\\n\", __func__);\n        assert(false);\n        return NULL;\n    }\n\n    char * const mem_buffer = ctx->mem_buffer;\n\n    struct ggml_object * const obj_new = (struct ggml_object *)(mem_buffer + cur_end);\n\n    *obj_new = (struct ggml_object) {\n        .offset = cur_end + GGML_OBJECT_SIZE,\n        .size   = size_needed,\n        .next   = NULL,\n    };\n\n    if (obj_cur != NULL) {\n        obj_cur->next = obj_new;\n    } else {\n        // this is the first object in this context\n        ctx->objects_begin = obj_new;\n    }\n\n    ctx->objects_end = obj_new;\n\n    struct ggml_tensor * const result = (struct ggml_tensor *)(mem_buffer + obj_new->offset);\n\n    ggml_assert_aligned(result);\n\n    *result = (struct ggml_tensor) {\n        /*.type         =*/ type,\n        /*.n_dims       =*/ n_dims,\n        /*.ne           =*/ { 1, 1, 1, 1 },\n        /*.nb           =*/ { 0, 0, 0, 0 },\n        /*.op           =*/ GGML_OP_NONE,\n        /*.is_param     =*/ false,\n        /*.grad         =*/ NULL,\n        /*.src0         =*/ NULL,\n        /*.src1         =*/ NULL,\n        /*.n_tasks      =*/ 0,\n        /*.perf_runs    =*/ 0,\n        /*.perf_cycles  =*/ 0,\n        /*.perf_time_us =*/ 0,\n        /*.data         =*/ data == NULL ? (void *)(result + 1) : data,\n        /*.pad          =*/ { 0 },\n    };\n\n    ggml_assert_aligned(result->data);\n\n    for (int i = 0; i < n_dims; i++) {\n        result->ne[i] = ne[i];\n    }\n\n    result->nb[0] = GGML_TYPE_SIZE[type];\n    for (int i = 1; i < GGML_MAX_DIMS; i++) {\n        result->nb[i] = result->nb[i - 1]*result->ne[i - 1];\n    }\n\n    ctx->n_objects++;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_new_tensor(\n        struct ggml_context * ctx,\n        enum   ggml_type type,\n        int    n_dims,\n        const int* ne) {\n    return ggml_new_tensor_impl(ctx, type, n_dims, ne, NULL);\n}\n\nstruct ggml_tensor * ggml_new_tensor_1d(\n        struct ggml_context * ctx,\n        enum   ggml_type type,\n        int    ne0) {\n    return ggml_new_tensor(ctx, type, 1, &ne0);\n}\n```", "```py\nstruct ggml_object {\n    size_t offset;\n    size_t size;\n\n    struct ggml_object * next;\n\n    char padding[8];\n};\n```", "```py\n*obj_new = (struct ggml_object) {\n      .offset = cur_end + GGML_OBJECT_SIZE,\n      .size   = size_needed,\n      .next   = NULL,\n  };\n```", "```py\n*result = (struct ggml_tensor) {\n        /*.type         =*/ type,\n        /*.n_dims       =*/ n_dims,\n        /*.ne           =*/ { 1, 1, 1, 1 },\n        /*.nb           =*/ { 0, 0, 0, 0 },\n        /*.op           =*/ GGML_OP_NONE,\n        /*.is_param     =*/ false,\n        /*.grad         =*/ NULL,\n        /*.src0         =*/ NULL,\n        /*.src1         =*/ NULL,\n        /*.n_tasks      =*/ 0,\n        /*.perf_runs    =*/ 0,\n        /*.perf_cycles  =*/ 0,\n        /*.perf_time_us =*/ 0,\n        /*.data         =*/ data == NULL ? (void *)(result + 1) : data,\n        /*.pad          =*/ { 0 },\n    };\n```", "```py\n#include \"ggml/ggml.h\"\n#include \"utils.h\"\n\nint main(int argc, char ** argv) {\n    // define the memory parameters e.g. 16GB memory \n    struct ggml_init_params params = {.mem_size=16*1024*1024,\n                                      .mem_buffer=NULL,\n                                      };\n    // create a computational context \n    struct ggml_context * ctx = ggml_init(params);\n    // define the input tensors\n    struct ggml_tensor *x = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);\n    // x is a variable parameters in our context\n    ggml_set_param(ctx, x);\n    // define a constant a\n    struct ggml_tensor *a = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);\n    // return x^2\n    struct ggml_tensor *x2 = ggml_mul(ctx, x, x);\n    // compute f = ax^2\n    struct ggml_tensor *f = ggml_mul(ctx, a, x2);\n\n    return 0;\n}\n```", "```py\nstruct ggml_tensor * ggml_mul_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b,\n        bool inplace) {\n    assert(ggml_are_same_shape(a, b));\n\n    bool is_node = false;\n\n    if (!inplace && (a->grad || b->grad)) {\n        is_node = true;\n    }\n\n    if (inplace) {\n        assert(is_node == false);\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n    // Here we are transforming the operation \n    result->op   = GGML_OP_MUL;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src0 = a;\n    result->src1 = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_mul(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b) {\n    return ggml_mul_impl(ctx, a, b, false);\n}\n```", "```py\nstruct ggml_cgraph gf = ggml_build_forward(f);\n// set initial params\nggml_set_f32(x, 2.0f);\nggml_set_f32(a, 3.0f);\n// compute\nggml_graph_compute(ctx, &gf);\nprintf(\"k=%f\\n\", ggml_get_f32_1d(f,0));\n```", "```py\nstruct ggml_cgraph result = {\n        /*.n_nodes      =*/ 0,\n        /*.n_leafs      =*/ 0,\n        /*.n_threads    =*/ 0,\n        /*.work_size    =*/ 0,\n        /*.work         =*/ NULL,\n        /*.nodes        =*/ { NULL },\n        /*.grads        =*/ { NULL },\n        /*.leafs        =*/ { NULL },\n        /*.perf_runs    =*/ 0,\n        /*.perf_cycles  =*/ 0,\n        /*.perf_time_us =*/ 0,\n    };\n```", "```py\n// without defining `gf` above run this:\nstruct ggml_cgraph gf = ggml_build_forward(f);\nggml_graph_dump_dot(&gf, &gf, \"name_of_your_graph\");\n```", "```py\ndot -Tpng name_of_your_graph -o name_of_your_graph.png && open name_of_your_graph.png\n```", "```py\nswitch (tensor->op) {\n        case GGML_OP_DUP:\n            {\n                ggml_compute_forward_dup(params, tensor->src0, tensor);\n            } break;\n        case GGML_OP_ADD:\n            {\n                ggml_compute_forward_add(params, tensor->src0, tensor->src1, tensor);\n            } break;\n        case GGML_OP_SUB:\n            {\n                ggml_compute_forward_sub(params, tensor->src0, tensor->src1, tensor);\n            } break;\n        case GGML_OP_MUL:\n            {\n                ggml_compute_forward_mul(params, tensor->src0, tensor->src1, tensor);\n            } break;\n            ...\n...\n```", "```py\nvoid ggml_compute_forward_mul(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_mul_f32(params, src0, src1, dst);\n            } break;\n        case GGML_TYPE_I8:\n        case GGML_TYPE_I16:\n        case GGML_TYPE_I32:\n        case GGML_TYPE_F16:\n        case GGML_TYPE_COUNT:\n            {\n                assert(false);\n            } break;\n    }\n}\n```", "```py\n void ggml_compute_forward_mul_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, src1) && ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    assert( dst->nb[0] == sizeof(float));\n    assert(src0->nb[0] == sizeof(float));\n    assert(src1->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        float * x = (float *) ((char *) dst->data  + i*( dst->nb[1]));\n        float * y = (float *) ((char *) src0->data  + i*( dst->nb[1]));\n        float * z = (float *) ((char *) src1->data  + i*( dst->nb[1]));\n\n        ggml_vec_mul_f32(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])),\n                (float *) ((char *) src1->data + i*(src1->nb[1])));\n    }\n}\n```", "```py\ninline static void ggml_vec_mul_f32 (const int n, float * z, const float * x, const float * y) { for (int i = 0; i < n; ++i) z[i]  = x[i]*y[i];   }\n```", "```py\n#include \"ggml/ggml.h\"\n#include \"utils.h\"\n\nint main(int argc, char ** argv) {\n    struct ggml_init_params params = {.mem_size=16*1024*1024,\n                                      .mem_buffer=NULL,\n                                      };\n    // params set up\n    struct ggml_context * ctx = ggml_init(params);\n    // tensors\n    struct ggml_tensor *x = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);\n    // x as a parameter\n    ggml_set_param(ctx, x);\n    struct ggml_tensor *a = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);\n    struct ggml_tensor *x2 = ggml_mul(ctx, x, x);\n    struct ggml_tensor *f = ggml_mul(ctx, a, x2);\n\n    // build the graph for the operations\n    struct ggml_cgraph gf = ggml_build_forward(f);\n    // set initial params\n    ggml_set_f32(x, 2.0f);\n    ggml_set_f32(a, 3.0f);\n    // compute\n    ggml_graph_compute(ctx, &gf);\n    printf(\"k=%f\\n\", ggml_get_f32_1d(f,0));\n    // print the graph\n    ggml_graph_print(&gf);\n    // save the graph\n    ggml_graph_dump_dot(&gf, &gf, \"final_graph\");\n\n    return 0;\n}\n```", "```py\n#\n# simple_example\n\nset(TEST_TARGET simple_example)\nadd_executable(${TEST_TARGET} main.cpp)\ntarget_link_libraries(${TEST_TARGET} PRIVATE ggml ggml_utils)\n```", "```py\nmkdir build\ncd build\ncmake ../\nmake simple_example\n```"]