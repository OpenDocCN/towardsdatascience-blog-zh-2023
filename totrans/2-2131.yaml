- en: 'Towards Green AI: How to Make Deep Learning Models More Efficient in Production'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朝向绿色 AI：如何在生产中提高深度学习模型的效率
- en: 原文：[https://towardsdatascience.com/towards-green-ai-how-to-make-deep-learning-models-more-efficient-in-production-3b1e7430a14](https://towardsdatascience.com/towards-green-ai-how-to-make-deep-learning-models-more-efficient-in-production-3b1e7430a14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/towards-green-ai-how-to-make-deep-learning-models-more-efficient-in-production-3b1e7430a14](https://towardsdatascience.com/towards-green-ai-how-to-make-deep-learning-models-more-efficient-in-production-3b1e7430a14)
- en: '[The Kaggle Blueprints](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[The Kaggle Blueprints](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)'
- en: 'From Academia to Industry: Finding the best trade-off between predictive performance
    and inference runtime for sustainability in Machine Learning practices'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从学术界到工业界：在机器学习实践中寻找预测性能与推理运行时间之间的最佳权衡以实现可持续性
- en: '[](https://medium.com/@iamleonie?source=post_page-----3b1e7430a14--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----3b1e7430a14--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b1e7430a14--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b1e7430a14--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----3b1e7430a14--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie?source=post_page-----3b1e7430a14--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----3b1e7430a14--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b1e7430a14--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b1e7430a14--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----3b1e7430a14--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b1e7430a14--------------------------------)
    ·14 min read·Aug 8, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b1e7430a14--------------------------------)
    ·14 分钟阅读·2023 年 8 月 8 日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2c688f3626d020d10afe04975cc8d466.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c688f3626d020d10afe04975cc8d466.png)'
- en: Making s’mEARTHs at the GPU bonfire (Image hand-drawn by the author)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 篝火中制作 s’mEARTHs（图片由作者手绘）
- en: '*This article was* [*originally published on Kaggle*](https://www.kaggle.com/code/iamleonie/towards-green-ai)
    *as an entry to the* [*“2023 Kaggle AI Report” competition*](https://www.kaggle.com/competitions/2023-kaggle-ai-report)
    *on July 5th, 2023, in which* [*it won 1st place in the category “Kaggle competitions”*](https://www.kaggle.com/competitions/2023-kaggle-ai-report/discussion/429989)*.
    As it reviews Kaggle competition writeups, it is a special edition of “*[*The
    Kaggle Blueprints*](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)*”
    series.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文最初发表于* [*Kaggle*](https://www.kaggle.com/code/iamleonie/towards-green-ai)
    *，作为* [*“2023 Kaggle AI Report” 竞赛*](https://www.kaggle.com/competitions/2023-kaggle-ai-report)
    *的参赛作品，* [*在“ Kaggle 竞赛”类别中获得第 1 名*](https://www.kaggle.com/competitions/2023-kaggle-ai-report/discussion/429989)*。由于它回顾了
    Kaggle 竞赛的文章，因此它是“*[*The Kaggle Blueprints*](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)*”系列的特别版。*'
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: “I think we’re at the end of the era where it’s going to be these, like, giant,
    giant models. […] We’ll make them better in other ways.”, [said Sam Altman](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/),
    CEO of OpenAI, shortly after their release of GPT-4\. This statement surprised
    many, as GPT-4 is estimated to be ten times larger ([1.76 trillion parameters](https://wandb.ai/byyoung3/ml-news/reports/AI-Expert-Speculates-on-GPT-4-Architecture---Vmlldzo0NzA0Nzg4))
    than its predecessor, GPT-3 ([175 billion parameters](https://arxiv.org/abs/2005.14165)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: “我认为我们已经进入了一个不再依赖这些巨大的模型的时代。[…] 我们将通过其他方式让它们变得更好。”， [OpenAI 首席执行官 Sam Altman](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/)
    在 GPT-4 发布后不久说道。这一声明让许多人感到惊讶，因为 GPT-4 的规模预计比前身 GPT-3 大十倍（[1.76 万亿参数](https://wandb.ai/byyoung3/ml-news/reports/AI-Expert-Speculates-on-GPT-4-Architecture---Vmlldzo0NzA0Nzg4)），而
    GPT-3 具有 [1750 亿参数](https://arxiv.org/abs/2005.14165)。
- en: “I think we’re at the end of the era where it’s going to be these, like, giant,
    giant models. […] We’ll make them better in other ways.” — [Sam Altman](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/)
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我认为我们已经到了这样一个时代，即这些巨大的模型将会结束。[……] 我们会通过其他方式使它们变得更好。” — [Sam Altman](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/)
- en: In 2019, Strubell et al. [1] estimated that training a natural language processing
    (NLP) pipeline, including tuning and experimentation, produces around 35 tonnes
    of carbon dioxide equivalent, more than twice the average U.S. citizen’s annual
    consumption.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，Strubell等人[1]估计，训练一个自然语言处理（NLP）管道，包括调优和实验，产生约35吨的二氧化碳当量，超过了美国公民年均消费的两倍。
- en: 'Let’s put it more into perspective: It was reported that [information technologies
    produced 3.7% of global CO2 emissions in 2019](https://theshiftproject.org/en/article/lean-ict-our-new-report/).
    That’s more than [global aviation (1.9%) and shipping (1.7%)](https://ourworldindata.org/emissions-by-sector)
    combined!'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更具体地来看一下：据报道，[2019年信息技术产生了全球3.7%的CO2排放](https://theshiftproject.org/en/article/lean-ict-our-new-report/)。这比[全球航空（1.9%）和航运（1.7%）](https://ourworldindata.org/emissions-by-sector)的排放总和还要多！
- en: Deep Learning models have been pushing state-of-the-art performances across
    different fields. These performance gains are often the results of larger models.
    But building bigger models requires more computations in both the training and
    the inference stage. And more computations require bigger hardware and more electricity
    and thus emit more CO2 and lead to a bigger carbon footprint, which is bad for
    the environment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型在不同领域推动了最先进的表现。这些性能提升通常是更大模型的结果。但构建更大的模型需要在训练和推理阶段进行更多计算。而更多的计算需要更大的硬件和更多的电力，从而排放更多的CO2，导致更大的碳足迹，这对环境不利。
- en: The eye-opening paper by Strubell et al. [1] has led to the birth of a new research
    field called “Green AI” in 2020\. The term was coined by Schwartz et al. [2] to
    describe research “that yields novel results without increasing computational
    cost and ideally reducing it”. Since then, many papers in the field have emerged
    to reduce the carbon footprint of AI, especially Deep Learning models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Strubell等人[1]的震撼论文促成了2020年“绿色人工智能（Green AI）”这一新研究领域的诞生。这个术语由Schwartz等人[2]提出，用来描述“在不增加计算成本的情况下产生新结果的研究，理想情况下还要减少计算成本”。自那时起，许多领域的论文涌现出来，旨在减少人工智能，尤其是深度学习模型的碳足迹。
- en: '![](../Images/326a38296360c389199f719b73c9c4e3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/326a38296360c389199f719b73c9c4e3.png)'
- en: 'In an effort towards reducing the carbon footprint of Deep Learning models,
    [Kaggle](https://www.kaggle.com/), a platform for Data Science competitions, has
    introduced an “Efficiency Prize” to some of their competitions:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少深度学习模型的碳足迹，[Kaggle](https://www.kaggle.com/)这一个数据科学竞赛平台，在他们的一些比赛中引入了“效率奖”：
- en: We are hosting a second track that focuses on model efficiency, because highly
    accurate models are often computationally heavy. Such models have a stronger carbon
    footprint and frequently prove difficult to utilize in real-world […] contexts.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们正在主办一个第二个专题，专注于模型效率，因为高精度模型通常计算负担较重。这些模型具有更强的碳足迹，并且在实际世界的[…]环境中通常难以使用。
- en: Although carbon footprint is produced along the entire lifecycle of a Deep Learning
    model, Kaggle can’t influence the competitors’ actions at every stage. But by
    evaluating submissions on runtime in addition to their predictive performance,
    Kaggle can encourage competitors to build more efficient solutions to reduce the
    carbon footprint, at least in the inference stage.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管碳足迹在深度学习模型的整个生命周期中产生，但Kaggle无法影响每个阶段参赛者的行动。但是，通过在运行时间和预测性能上评估提交，Kaggle可以鼓励参赛者构建更高效的解决方案，以减少碳足迹，至少在推理阶段。
- en: '![](../Images/fecf54d67a319440ab6ba293713a2319.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fecf54d67a319440ab6ba293713a2319.png)'
- en: At the beginning of this year, a survey was released claiming that the Green
    AI research field is reaching a level of maturity and that it is now time to “port
    the numerous promising academic results to industrial practice” to evaluate the
    techniques outside of laboratory settings [7].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 今年年初，发布了一项调查，声称绿色人工智能（Green AI）研究领域已经达到成熟水平，现在是将众多有前景的学术成果转化为工业实践以在实验室环境之外评估技术的时候了[7]。
- en: 'Although Kaggle can’t be used as a direct proxy for industry practices, Kaggle
    is a perfect place to test new techniques outside of laboratory settings. Thus,
    this articles prompt is:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Kaggle 不能直接作为行业实践的代理，但 Kaggle 是在实验室环境之外测试新技术的理想场所。因此，本文的主题是：
- en: '**What has the community learned over the past two years of balancing model
    performance and inference time in Kaggle competitions with an Efficiency Prize
    to reduce the carbon footprint of Deep Learning models in production?**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**在过去两年中，社区在平衡 Kaggle 竞赛中的模型性能和推理时间方面学到了什么，以减少生产中深度学习模型的碳足迹？**'
- en: We will first review the promising academic results in the “Green AI” literature.
    Then, we will review which of them have already been adopted and proven successful
    by the Kaggle community by examining writeups from winners of the Efficiency Prize.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先回顾“绿色 AI”文献中的有前景的学术成果。然后，我们将审查这些成果中的哪些已经被 Kaggle 社区采纳并取得成功，通过查看效率奖获胜者的文章来进行分析。
- en: Background
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: Carbon footprint is generated along the entire Machine Learning Operations (MLOps)
    lifecycle. Since 2021, a handful of surveys [3, 4, 5, 6, 7] have categorized the
    techniques to reduce carbon footprint by the MLOps stage it is produced in. While
    the stages differ slightly, the main three steps are model design, development,
    and inference [3, 4, 5, 6]. Other categories cover data storage and usage [3,
    5, 6] or hardware [4].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 碳足迹在整个机器学习操作（MLOps）生命周期中产生。自 2021 年以来，一些调查 [3, 4, 5, 6, 7] 已按 MLOps 阶段对减少碳足迹的技术进行了分类。尽管各阶段略有不同，但主要的三个步骤是模型设计、开发和推理
    [3, 4, 5, 6]。其他分类包括数据存储和使用 [3, 5, 6] 或硬件 [4]。
- en: '![](../Images/abf1c0ef88cdbbc11177f49aaa7b3e29.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abf1c0ef88cdbbc11177f49aaa7b3e29.png)'
- en: In 2019, both AWS [21] and NVIDIA [22] estimated that roughly 90% of the machine
    learning workload is inference. With the available context of the Efficiency Prize,
    this essay’s prompt focuses on improving efficiency in the inference stage.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2019 年，AWS [21] 和 NVIDIA [22] 都估计大约 90% 的机器学习工作负载是推理。在“效率奖”的背景下，本文的主题集中在提升推理阶段的效率。
- en: Note that the model design significantly impacts the efficiency in the inference
    stage as well. However, the model design can be specific to the use case and thus
    requires more data to effectively analyze which model design techniques can help
    reduce the carbon footprint in the inference stage. Thus, we will first focus
    on the model-agnostic techniques to reduce carbon footprint in the inference stage
    to keep this analysis concise. Future work is encouraged to analyze the model
    design when the Efficiency prize has been introduced to a wider range of competitions
    (see [Discussion](#35f0)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模型设计也会显著影响推理阶段的效率。然而，模型设计可能会针对特定用例，因此需要更多数据来有效分析哪些模型设计技术可以帮助减少推理阶段的碳足迹。因此，我们将首先关注推理阶段的模型无关技术，以保持分析简洁。鼓励未来的工作在效率奖被引入更广泛的竞赛范围时分析模型设计（见
    [讨论](#35f0)）。
- en: 'The taxonomy proposed by Xu et al. [3] in 2021 has also been adopted and used
    in a recent survey [6]. Thus, we will structure our analysis based on this taxonomy
    as well. The following techniques aim to reduce the model size to gain latency
    improvements because model size directly impacts the latency:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Xu 等人 [3] 在 2021 年提出的分类法也已被采用，并在最近的调查 [6] 中使用。因此，我们将根据这一分类法来结构化我们的分析。以下技术旨在减少模型大小以提高延迟，因为模型大小直接影响延迟：
- en: '**Pruning** reduces the size of the neural network by removing redundant elements,
    such as weights, filters, channels, or even layers, without losing performance.
    It was first proposed by LeCun et al. [8] in 1989.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剪枝**通过去除冗余元素（如权重、滤波器、通道甚至层）来减少神经网络的大小，而不损失性能。它首次由 LeCun 等人 [8] 于 1989 年提出。'
- en: '**Low-rank factorization** reduces the complexity of convolutional or fully
    connected layers in neural networks by factorizing a weight matrix into two matrices
    with low dimensions (matrix/tensor decomposition).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低秩分解**通过将权重矩阵分解为两个低维矩阵（矩阵/张量分解）来减少卷积层或全连接层的复杂性。'
- en: '**Quantization** reduces the model size by reducing the weights’ and activations’
    precision (usually from 32-bit floating point values to 8-bit unsigned integers).
    This reduction in precision can lead to small performance loss. You can quantize
    a model either during training or after training. There’s also static and dynamic
    quantization. For this article, you only need to know that dynamic quantization
    is more accurate than static quantization. However, dynamic quantization also
    requires more computations than static quantization.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**量化**通过降低权重和激活值的精度（通常从 32 位浮点值降到 8 位无符号整数）来减少模型的大小。这种精度的降低可能会导致轻微的性能损失。你可以在训练期间或训练后对模型进行量化。量化有静态和动态之分。对于本文，你只需要知道动态量化比静态量化更为准确。然而，动态量化也比静态量化需要更多的计算。'
- en: '**Knowledge Distillation** is a technique to distill the knowledge of a large-scale
    high-performing model/ensemble (teacher network) into a compact neural network
    (student network) by using additional data pseudo-labeled from the larger model
    to train the smaller model. The pseudo labels are also called soft labels and
    contain so-called dark knowledge, which helps the student network learn to mimic
    the teacher network. This idea was proposed by Hinton et al. [9] in 2015.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识蒸馏**是一种技术，通过使用从大型高性能模型/集合（教师网络）中伪标记的额外数据来训练较小的神经网络（学生网络），从而将知识蒸馏到紧凑的神经网络中。这些伪标签也称为软标签，包含所谓的“黑暗知识”，这有助于学生网络学习模仿教师网络。这个想法由
    Hinton 等人在 2015 年提出[9]。'
- en: Methodology & Data Collection
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法论与数据收集
- en: To evaluate which promising academic results have already been tried and proven
    outside of laboratory settings, we will review the solution writeups of Kaggle
    competitions with an Efficiency Prize.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估哪些有前途的学术成果已经在实验室环境外进行过尝试和验证，我们将回顾带有效率奖的 Kaggle 比赛的解决方案报告。
- en: Unfortunately, the original competition dataset of Kaggle writeups doesn’t cover
    all efficiency writeups because some teams write separate writeups for the Efficiency
    Prize. Thus, we decided to create a custom dataset for this competition.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Kaggle 原始比赛数据集的写作内容并未涵盖所有的效率相关报告，因为一些团队为效率奖写了单独的报告。因此，我们决定为此次比赛创建一个自定义数据集。
- en: Identify all Kaggle competitions with an Efficiency Prize.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定所有带有效率奖的 Kaggle 比赛。
- en: Identify the top 10 teams in the Efficiency Prize according to the [Efficiency
    Leaderboard notebooks](https://www.kaggle.com/code/ryanholbrook/curriculum-recommendation-efficiency-leaderboard).
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据[效率排行榜笔记本](https://www.kaggle.com/code/ryanholbrook/curriculum-recommendation-efficiency-leaderboard)确定效率奖的前
    10 支团队。
- en: Manually collect the writeup of every top efficiency solution by searching the
    discussion forums and leaderboards. If a team wrote two writeups, select the one
    for the Efficiency Prize (e.g. [Standard Leaderboard writeup](https://www.kaggle.com/competitions/feedback-prize-effectiveness/discussion/347536)
    vs. [Efficiency Leaderboard writeup](https://www.kaggle.com/competitions/feedback-prize-effectiveness/discussion/347537)
    from the same team).
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过搜索讨论论坛和排行榜，手动收集每个顶级效率解决方案的报告。如果一个团队写了两份报告，请选择效率奖的那一份（例如，来自同一团队的[标准排行榜报告](https://www.kaggle.com/competitions/feedback-prize-effectiveness/discussion/347536)与[效率排行榜报告](https://www.kaggle.com/competitions/feedback-prize-effectiveness/discussion/347537)）。
- en: Out of the 50 top 10 efficiency solutions, we were able to collect **25 available
    writeups**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 50 个前 10 名的效率解决方案中，我们能够收集到**25 份可用报告**。
- en: '![](../Images/e8b253caf20d534bc1b66ef6515cf311.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8b253caf20d534bc1b66ef6515cf311.png)'
- en: The writeups are distributed among the five competitions, as shown below.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些报告分布在以下五场比赛中。
- en: '![](../Images/67d0efb11c307e030f7f62b38fba1e0a.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67d0efb11c307e030f7f62b38fba1e0a.png)'
- en: Results
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: This section reviews the collected writeups from top solutions in the Kaggle
    Efficiency Prize to see if any techniques for reducing inference costs, including
    pruning, low-rank factorization, quantization, and knowledge distillation, were
    used and whether they were successful.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了 Kaggle 效率奖的顶级解决方案中收集的报告，查看是否使用了包括剪枝、低秩分解、量化和知识蒸馏在内的减少推理成本的技术，以及这些技术是否成功。
- en: 'For each of the techniques, we manually reviewed every single writeup of the
    25 to answer the following questions:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一种技术，我们手动审查了这 25 份报告，以回答以下问题：
- en: Did competitors experiment with the technique? How was it applied?
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 竞争者们对这种技术进行了实验吗？它是如何应用的？
- en: Was the technique effective?
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该技术是否有效？
- en: Is there any indication as to why the technique was effective/ineffective?
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否有任何迹象表明该技术为何有效/无效？
- en: Pruning
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪枝
- en: Only one [20] of the 25 top efficiency writeups mentioned pruning.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在25篇顶级效率报告中，只有一篇[20]提到了剪枝。
- en: '![](../Images/44a32a5f8df42b06f600596e0971ddcf.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44a32a5f8df42b06f600596e0971ddcf.png)'
- en: 'However, they couldn’t report any success [20]:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，他们无法报告任何成功的案例[20]：
- en: Beginning to explore prunning as well as hard quantization showed that the performance
    loss would be significant (which is OK in production but not in a competition)
    so we sticked with a simple TF Lite conversion.
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 开始探索剪枝以及硬量化显示出性能损失会很显著（在生产中可以接受，但在竞赛中则不行），因此我们选择了简单的TF Lite转换。
- en: What’s interesting about this writeup is its mention that the found performance
    loss would have been acceptable in production but not in this competition setting,
    which is an indicator that the criteria for whether a trade-off is good or not
    may vary between a Kaggle competition and the industry (see [Discussion](#35f0)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇报告的有趣之处在于它提到，发现的性能损失在生产中是可以接受的，但在此竞赛设置中则不然，这表明，权衡是否良好的标准在Kaggle竞赛和工业界之间可能有所不同（见[讨论](#35f0)）。
- en: Low-Rank Factorization
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低秩分解
- en: None of the 25 top efficiency writeups mentioned low-rank factorization.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 25篇顶级效率报告中没有提到低秩分解。
- en: '![](../Images/8c6e91e5a278817d6166063f89881670.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c6e91e5a278817d6166063f89881670.png)'
- en: Because there is no mention in the writeups, we don’t know whether none of the
    competitors tried it or whether any competitors experimented with it, but it wasn’t
    practical.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于报告中没有提到，我们不知道是否没有竞争者尝试过，或者是否有竞争者尝试过但效果不佳。
- en: Because low-rank factorization was mentioned in a [discussion post](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/365851)
    in the Feedback Prize — English Language Learning competition, we can assume that
    some competitors were aware of this technique. However, a recent survey [3] already
    concluded that low-rank factorization was computationally complicated and less
    effective in reducing computational cost and inference time than other compression
    methods.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于低秩分解在Feedback Prize——英语语言学习竞赛的[讨论帖](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/365851)中被提到，我们可以假设一些竞争者知道这一技术。然而，最近的调查[3]已经得出结论，低秩分解在计算上复杂，减少计算成本和推理时间的效果不如其他压缩方法。
- en: Quantization
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化
- en: Quantization was not mentioned before the Feedback Prize — English Language
    Learning competition. But in the latter, it was mentioned in over half of the
    writeups but reported not to have been effective. Although quantization was mentioned
    only in one writeup in the Learning Equality — Curriculum Recommendations competition,
    it was reported as successful there.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在Feedback Prize——英语语言学习竞赛之前没有提到量化。但在后者中，超过一半的报告提到过量化，但报告显示效果不佳。虽然在学习平等——课程推荐竞赛中仅有一篇报告提到量化，但报告称在该竞赛中效果良好。
- en: '![](../Images/7e9e60152ea8a35238d9576b6f62f63c.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e9e60152ea8a35238d9576b6f62f63c.png)'
- en: 'Quantization was first mentioned in the Feedback Prize — English Language Learning
    competition. However, the writeups mentioning quantization reported that it didn’t
    work. Two writeups [13, 17] specifically reported that they tried dynamic quantization,
    resulting in performance loss and no runtime improvements:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 量化首次出现在Feedback Prize——英语语言学习竞赛中。然而，提到量化的报告表示它没有效果。两篇报告[13, 17]特别提到他们尝试了动态量化，导致性能损失且没有运行时改进：
- en: Because quantizing `nn.Linear` layers directly affects the output of the model,
    quantizing layers is not suitable for regression tasks.
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 由于量化`nn.Linear`层直接影响模型的输出，因此量化层不适合回归任务。
- en: 'In the Learning Equality — Curriculum Recommendations competition, a competitor
    reported that post-training dynamic quantization was successful with a slight
    performance drop but increased the inference speed [19]:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习平等——课程推荐比赛中，一位竞争者报告说，训练后动态量化成功，虽然性能略有下降，但推理速度提高了[19]：
- en: If using `qint8` also on the Feed Forward part of the transformer on the intermediate
    up sample and output layer, the score drop is even higher so I ended up in only
    using `qint8` on the attention layer.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果在变换器的Feed Forward部分的中间上采样和输出层也使用`qint8`，得分下降会更大，因此我最终只在注意力层使用了`qint8`。
- en: Based on the competitor’s solution [19], we can see that quantizing individual
    layers is almost as easy as quantizing an entire neural network.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 根据参赛者的解决方案[19]，我们可以看到量化单独的层几乎和量化整个神经网络一样容易。
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Thus, the success of quantization depends on which layers in the neural network
    are quantized.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，量化的成功取决于神经网络中哪些层被量化。
- en: Knowledge Distillation
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识蒸馏
- en: Out of the 25 writeups, knowledge distillation was mentioned in nine of them
    across three out of five competitions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在25篇总结中，知识蒸馏在五场竞赛中的三场中提到了九次。
- en: '*Note: Knowledge distillation is often mentioned in combination with pseudo-labeling.
    As many competitors use pseudo labels to retrain the existing model, we only focus
    on writeups where it was explicitly mentioned that pseudo labels were used to
    train a new smaller model.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：知识蒸馏通常与伪标签一起提及。由于许多参赛者使用伪标签重新训练现有模型，我们仅关注那些明确提到使用伪标签训练新小模型的总结。*'
- en: '![](../Images/fa3645ca6494f205c0b77bfd97133ace.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa3645ca6494f205c0b77bfd97133ace.png)'
- en: Interestingly, all writeups mentioning knowledge distillation reported it as
    effective. Many competitors even reported knowledge distillation to have a high
    impact [13, 14, 16, 17] on the Efficiency Prize with only minimal performance
    losses [10, 16, 19].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，所有提到知识蒸馏的总结都报告了其有效性。许多参赛者甚至报告知识蒸馏对效率奖[13, 14, 16, 17]具有很高的影响，仅有最小的性能损失[10,
    16, 19]。
- en: 'A competitor described their knowledge distillation process, which they successfully
    applied in both the Feedback Prize — Predicting Effective Arguments [10] and the
    Feedback Prize — English Language Learning competition [15], as follows [10]:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一位参赛者描述了他们的知识蒸馏过程，并成功应用于反馈奖——有效论点预测[10]和反馈奖——英语语言学习竞赛[15]，如下所述[10]：
- en: 1\. We generate pseudo labels data for the previous Feedback competition by
    using our large ensemble […].
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1\. 我们通过使用我们的大型集成模型为之前的反馈竞赛生成伪标签数据[…]。
- en: ''
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. We also generate out-of-fold pseudo labels for our given train data
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2\. 我们还为给定的训练数据生成了折外伪标签
- en: ''
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3\. Now we combine these 2 datasets with soft pseudo labels together and train
    a single new model without any original labels
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 3\. 现在我们将这两个数据集与软伪标签结合在一起，并训练一个没有任何原始标签的新模型
- en: Despite its simple implementation, we need to note that knowledge distillation
    depends on the availability of additional data. Many competitors [10, 13, 15]
    mentioned curating a dataset from previous competitions in the Feedback Prize
    competition series for pseudo-labeling.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其实现简单，但我们需要注意知识蒸馏依赖于额外数据的可用性。许多参赛者[10, 13, 15]提到在反馈奖竞赛系列中从之前的竞赛中整理数据集以进行伪标签化。
- en: Interestingly, knowledge distillation was applied successfully in NLP competitions,
    but the others, which were a computer vision and a tabular data competition, were
    unsuccessful. However, they didn’t have previous similar competitions for additional
    data as well.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，知识蒸馏在自然语言处理竞赛中成功应用，但在计算机视觉和表格数据竞赛中却失败了。然而，它们也没有类似的竞赛数据作为额外数据。
- en: Discussion
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论
- en: This section discusses what the community has learned about balancing model
    performance and inference time in Kaggle competitions with an Efficiency Prize
    to reduce the carbon footprint of Deep Learning models in production. Specifically,
    we discuss if the promising academic results were practical outside of laboratory
    settings.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了社区对在Kaggle竞赛中平衡模型性能和推理时间的理解，以减少深度学习模型在生产中的碳足迹，特别是我们讨论了这些有前景的学术成果在实验室环境之外是否具有实用性。
- en: What was the impact of Kaggle competitions with an Efficiency Prize on the broader
    ML community, and did the ML community benefit from them?
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kaggle竞赛中的效率奖对更广泛的机器学习社区有什么影响？机器学习社区是否从中受益？
- en: Verdecchia et al. [7] claimed that the Green AI research field is reaching a
    level of maturity and that it is now time to “port the numerous promising academic
    results to industrial practice” to evaluate their practicality outside of laboratory
    settings.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Verdecchia 等人[7]声称绿色人工智能研究领域已达到成熟水平，现在是“将大量有前景的学术成果移植到工业实践中”的时候，以评估其在实验室环境之外的实用性。
- en: For this purpose, we reviewed solution writeups of Kaggle competitions with
    an Efficiency Prize. We saw that the Efficiency Prize encouraged competitors to
    experiment with different techniques to reduce carbon emissions in the inference
    stage.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们回顾了获得效率奖的Kaggle比赛的解决方案总结。我们看到效率奖鼓励参赛者尝试不同技术，以减少推理阶段的碳排放。
- en: The analysis showed that many of the academic techniques were tried by the Kaggle
    community. The Kaggle community could not confirm that pruning and low-rank factorization
    were practical techniques to achieve a good trade-off between efficiency and performance.
    However, it was shown that careful application of quantization and knowledge distillation
    were practical because of their simple implementation and effectiveness.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 分析表明，许多学术技术在Kaggle社区中得到了尝试。Kaggle社区无法确认剪枝和低秩分解是实现效率与性能良好平衡的实用技术。然而，量化和知识蒸馏的细致应用被证明是实际可行的，因为它们简单易用且效果显著。
- en: Did these competitions drive forward any important advancements in the field
    of ML?
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这些比赛是否推动了机器学习领域的任何重要进展？
- en: Although the focus of this analysis was on evaluating the four techniques pruning,
    low-rank factorization, quantization, and knowledge distillation, we came across
    different techniques competitors found effective in reducing runtime without sacrificing
    predictive performance, such as converting a model to ONNX format [15, 16, 18].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这项分析的重点是评估剪枝、低秩分解、量化和知识蒸馏这四种技术，但我们遇到了竞争者发现有效的不同技术，例如将模型转换为ONNX格式[15, 16, 18]，在不牺牲预测性能的情况下减少运行时间。
- en: Thus, the following steps would be to analyze the competition writeups from
    the opposite perspective to see which techniques were established in the Kaggle
    community to improve the inference speed of Deep Learning models.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，接下来的步骤是从相反的角度分析比赛总结，看看哪些技术在Kaggle社区中被确立为提高深度学习模型推理速度的有效技术。
- en: What are the limitations of the impact of these competitions?
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这些比赛的影响有哪些限制？
- en: Carbon footprint is produced along the entire MLOps lifecycle. In this analysis,
    we specifically focused on the techniques categorized by literature under the
    inference stage. However, starting how we design models already impacts the carbon
    emissions of a solution at inference and should be analyzed in future work as
    well.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 碳足迹在整个MLOps生命周期中产生。在这项分析中，我们特别关注了文献中归类于推理阶段的技术。然而，从我们设计模型的起点开始已经影响了推理时的碳排放，这也应在未来的工作中加以分析。
- en: Additionally, although proven effective, knowledge distillation requires training
    a large-scale teacher network before the knowledge is distilled into a smaller
    network. Thus, while knowledge distillation reduces the carbon emissions in the
    inference stage, it must be noted that this technique produces additional carbon
    emissions in the training stage.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管效果已被证明有效，知识蒸馏仍需训练一个大规模的教师网络，然后将知识蒸馏到较小的网络中。因此，虽然知识蒸馏减少了推理阶段的碳排放，但必须指出，这项技术在训练阶段会产生额外的碳排放。
- en: Thus, while the Efficiency Prize helps evaluate techniques to reduce the carbon
    footprint of a solution at inference, we need a more holistic approach to encourage
    competitors to reduce carbon emissions during a competition to move towards Green
    AI.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然效率奖有助于评估减少推理时碳足迹的技术，但我们需要一种更全面的方法来鼓励参赛者在比赛过程中减少碳排放，以迈向绿色AI。
- en: Conclusion
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This analysis aimed to learn if the academically promising techniques to reduce
    the carbon footprint in the inference stage of the MLOps lifecycle were effectively
    applied in Kaggle competitions with an Efficiency Prize.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 本次分析旨在了解在MLOps生命周期的推理阶段减少碳足迹的学术上有前景的技术是否在获得效率奖的Kaggle比赛中得到了有效应用。
- en: 'For this purpose, we first reviewed the available literature and found that
    recent surveys categorized the available techniques to reduce carbon emissions
    at inference into four main techniques: pruning, low-rank factorization, quantization,
    and knowledge distillation. Then we analyzed a custom dataset of Efficiency Prize
    writeups.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们首先回顾了现有文献，并发现最近的调查将减少推理阶段碳排放的技术分为四类：剪枝、低秩分解、量化和知识蒸馏。然后，我们分析了效率奖总结的自定义数据集。
- en: 'We found that the Kaggle community has tried many of the academic proposals:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现Kaggle社区尝试了许多学术提案：
- en: Pruning was reported to be ineffective at achieving a satisfying trade-off.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剪枝被报告为在实现令人满意的折中方面无效。
- en: Low-rank factorization was not mentioned in any of the top 10 writeups. We assume
    that this technique could be more computationally complicated to be practical.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低秩分解在前10篇写作中没有提到。我们假设这一技术可能在实际应用中更具计算复杂性。
- en: Quantization was reported successful in only one case where the competitor did
    not quantize the entire model but curated which layers to quantize to which degree.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化仅在一个案例中成功报告，其中竞争者没有量化整个模型，而是策划了哪些层进行量化以及量化的程度。
- en: Knowledge distillation was shown effective for NLP competitions with additional
    data available from similar competitions.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识蒸馏在有类似竞赛的额外数据可用时，已被证明对自然语言处理竞赛有效。
- en: We conclude that the Kaggle community has helped evaluate promising academic
    results from the Green AI literature outside of laboratory settings for their
    practicality quickly.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得出结论，Kaggle社区帮助评估了绿色人工智能文献中有前景的学术成果，以其实际可行性快速脱离实验室设置。
- en: Thus, **Kaggle should continue the Efficiency Prize** to move towards Green
    AI. As a next step, Kaggle could **add it to all competitions** and, ideally,
    even have it no longer as a separate track but **as the primary metric**.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**Kaggle应该继续推进效率奖**，以迈向绿色人工智能。作为下一步，Kaggle可以**将其添加到所有竞赛中**，理想情况下，甚至可以不再作为单独的赛道，而**作为主要指标**。
- en: Enjoyed This Story?
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 喜欢这个故事吗？
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[*免费订阅*](https://medium.com/subscribe/@iamleonie) *以便在我发布新故事时收到通知。*'
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----3b1e7430a14--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----3b1e7430a14--------------------------------)
    [## 每当Leonie Monigatti发布时获取电子邮件。'
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当Leonie Monigatti发布时获取电子邮件。通过注册，如果您还没有Medium账户，将会创建一个…
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----3b1e7430a14--------------------------------)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----3b1e7430a14--------------------------------)
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*在* [*LinkedIn*](https://www.linkedin.com/in/804250ab/)，[*Twitter*](https://twitter.com/helloiamleonie)*和*
    [*Kaggle*](https://www.kaggle.com/iamleonie)*上找到我！*'
- en: References
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Dataset
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: '[A] arXiv.org submitters. (2023). [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv).
    Kaggle. [https://doi.org/10.34740/KAGGLE/DSV/6141267](https://doi.org/10.34740/KAGGLE/DSV/6141267)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[A] arXiv.org提交者。（2023）。[arXiv数据集](https://www.kaggle.com/datasets/Cornell-University/arxiv)。Kaggle。
    [https://doi.org/10.34740/KAGGLE/DSV/6141267](https://doi.org/10.34740/KAGGLE/DSV/6141267)'
- en: 'License: [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '许可：[CC0: 公共领域](https://creativecommons.org/publicdomain/zero/1.0/)'
- en: '[B] iamleonie (2023). [Kaggle Efficiency Writeups](https://www.kaggle.com/datasets/iamleonie/kaggle-efficiency-writeups)
    in Kaggle Datasets.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[B] iamleonie（2023）。[Kaggle效率写作](https://www.kaggle.com/datasets/iamleonie/kaggle-efficiency-writeups)
    在Kaggle数据集中。'
- en: 'License: [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 许可： [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)
- en: Image References
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像参考
- en: If not otherwise stated, all images are created by the author. See the [Kaggle
    Notebook](https://www.kaggle.com/code/iamleonie/towards-green-ai) for the code.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图像均由作者创建。有关代码，请参见[Kaggle笔记本](https://www.kaggle.com/code/iamleonie/towards-green-ai)。
- en: References
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Literature
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文献
- en: '[1] Strubell, E., Ganesh, A., & McCallum, A. (2019). Energy and policy considerations
    for deep learning in NLP. *arXiv preprint arXiv:1906.02243*.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 斯特拉贝尔，E.，加奈，A.，& 麦考勒姆，A.（2019）。深度学习在自然语言处理中的能源和政策考虑。*arXiv预印本arXiv:1906.02243*。'
- en: '[2] Schwartz, R., Dodge, J., Smith, N. A., & Etzioni, O. (2020). Green ai.
    *Communications of the ACM*, *63*(12), 54–63.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 施瓦茨，R.，道奇，J.，史密斯，N. A.，& 埃提奥尼，O.（2020）。绿色人工智能。*ACM通讯*，*63*(12)，54–63。'
- en: '[3] Xu, J., Zhou, W., Fu, Z., Zhou, H., & Li, L. (2021). A survey on green
    deep learning. *arXiv preprint arXiv:2111.05193*.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 许，J.，周，W.，傅，Z.，周，H.，& 李，L.（2021）。绿色深度学习的调查。*arXiv预印本arXiv:2111.05193*。'
- en: '[4] Menghani, G. (2021). Efficient deep learning: A survey on making deep learning
    models smaller, faster, and better. *ACM Computing Surveys*, *55*(12), 1–37.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 孟哈尼，G.（2021）。高效深度学习：关于让深度学习模型更小、更快、更好的调查。*ACM计算调查*，*55*(12)，1–37。'
- en: '[5] Wu, C. J., et al. (2022). Sustainable ai: Environmental implications, challenges
    and opportunities. *Proceedings of Machine Learning and Systems*, *4*, 795–813.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Wu, C. J., 等（2022）。 可持续人工智能：环境影响、挑战与机遇。 *机器学习与系统会议论文集*，*4*，795–813。'
- en: '[6] Mehlin, V., Schacht, S., & Lanquillon, C. (2023). Towards energy-efficient
    Deep Learning: An overview of energy-efficient approaches along the Deep Learning
    Lifecycle. *arXiv preprint arXiv:2303.01980*.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Mehlin, V., Schacht, S., & Lanquillon, C.（2023）。 朝着节能深度学习的方向：深度学习生命周期中节能方法的概述。
    *arXiv 预印本 arXiv:2303.01980*。'
- en: '[7] Verdecchia, R., Sallou, J., & Cruz, L. (2023). A systematic review of Green
    AI. *Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery*, e1507.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Verdecchia, R., Sallou, J., & Cruz, L.（2023）。 绿色人工智能的系统综述。 *Wiley 跨学科评论：数据挖掘与知识发现*，e1507。'
- en: '[8] LeCun, Y., Denker, J., & Solla, S. (1989). Optimal brain damage. Advances
    in neural information processing systems, 2.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] LeCun, Y., Denker, J., & Solla, S.（1989）。 最优脑损伤。 神经信息处理系统进展，2。'
- en: '[9] Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in
    a neural network. *arXiv preprint arXiv:1503.02531*.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Hinton, G., Vinyals, O., & Dean, J.（2015）。 提炼神经网络中的知识。 *arXiv 预印本 arXiv:1503.02531*。'
- en: Kaggle Writeups
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kaggle 写作
- en: '[10] Team Hydrogen (2022). [1st place writeup in Feedback Prize — Predicting
    Effective Arguments](https://www.kaggle.com/competitions/feedback-prize-effectiveness/discussion/347537)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 氢团队（2022）。 [Feedback Prize 预测有效论点中的第1名分析](https://www.kaggle.com/competitions/feedback-prize-effectiveness/discussion/347537)'
- en: '[11] Now You See Me (2022). [2nd place efficiency writeup in Feedback Prize
    — Predicting Effective Arguments](https://wandb.ai/darek/fbck/reports/How-To-Build-an-Efficient-NLP-Model--VmlldzoyNTE5MDEx?accessToken=pmm41mpdkxif0lsbm927tfxj947to4gbd0nrgjjq9rdoq1c4jr3kruf993ys5kpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] 你现在看到了我（2022）。 [Feedback Prize 预测有效论点中的第2名效率分析](https://wandb.ai/darek/fbck/reports/How-To-Build-an-Efficient-NLP-Model--VmlldzoyNTE5MDEx?accessToken=pmm41mpdkxif0lsbm927tfxj947to4gbd0nrgjjq9rdoq1c4jr3kruf993ys5kpg)'
- en: '[12] Darjeeling Tea (2022). [5th place efficiency writeup in Feedback Prize
    — Predicting Effective Arguments](https://www.kaggle.com/competitions/feedback-prize-effectiveness/discussion/347433)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Darjeeling Tea（2022）。 [Feedback Prize 预测有效论点中的第5名效率分析](https://www.kaggle.com/competitions/feedback-prize-effectiveness/discussion/347433)'
- en: '[13] Team Turing (2022). [1st place efficiency writeup in Feedback Prize —
    English Language Learning](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/369646)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 图灵团队（2022）。 [Feedback Prize 英语语言学习中的第1名效率分析](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/369646)'
- en: '[14] RUN OUT OF IDEAS💨 (2022). [2nd place efficiency writeup in Feedback Prize
    — English Language Learning](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/369623)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] 用尽创意💨（2022）。 [Feedback Prize 英语语言学习中的第2名效率分析](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/369623)'
- en: '[15] Psi (2022). [5th place efficiency writeup in Feedback Prize — English
    Language Learning](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/370020)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Psi（2022）。 [Feedback Prize 英语语言学习中的第5名效率分析](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/370020)'
- en: '[16] william.wu (2022). [6th place efficiency writeup in Feedback Prize — English
    Language Learning](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/369587)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] william.wu（2022）。 [Feedback Prize 英语语言学习中的第6名效率分析](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/369587)'
- en: '[17] ktm (2022). [7th place efficiency writeup in Feedback Prize — English
    Language Learning](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/369540)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] ktm（2022）。 [Feedback Prize 英语语言学习中的第7名效率分析](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/369540)'
- en: '[18] Shobhit Upadhyaya (2022). [9th place efficiency writeup in Feedback Prize
    — English Language Learning](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/370046)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Shobhit Upadhyaya（2022）。 [Feedback Prize 英语语言学习中的第9名效率分析](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/370046)'
- en: '[19] Konni (2023). [2nd place efficiency writeup in Learning Equality — Curriculum
    Recommendations](https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations/discussion/395110)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Konni（2023）。 [Learning Equality 课程推荐中的第2名效率分析](https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations/discussion/395110)'
- en: '[20] French Touch (2023). [5th place efficiency writeup in Predict Student
    Performance from Game Play](https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations/discussion/395110)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] French Touch (2023). [在游戏玩法中预测学生表现的第五名效率总结](https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations/discussion/395110)'
- en: Web
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Web
- en: '[21] Barr, J. (2019). [Amazon EC2 Update — Inf1 Instances with AWS Inferentia
    Chips for High Performance Cost-Effective Inferencing](https://aws.amazon.com/de/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/)
    in AWS News Blog (accessed 16\. July, 2023)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Barr, J. (2019). [Amazon EC2 更新 — Inf1 实例配备 AWS Inferentia 芯片用于高性能且具有成本效益的推理](https://aws.amazon.com/de/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/)
    见于 AWS News Blog（访问于 2023 年 7 月 16 日）'
- en: '[22] Leopold, G. (2019). [AWS to Offer Nvidia’s T4 GPUs for AI Inferencing](https://www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/)
    in HPC Wire (accessed 16\. July, 2023)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Leopold, G. (2019). [AWS 提供 Nvidia 的 T4 GPU 用于 AI 推理](https://www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/)
    见于 HPC Wire（访问于 2023 年 7 月 16 日）'
