- en: Deep Dive into Softmax Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deep-dive-into-softmax-regression-62deea103cb8](https://towardsdatascience.com/deep-dive-into-softmax-regression-62deea103cb8)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understand the math behind softmax regression and how to use it to solve an
    image classification task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roiyeho?source=post_page-----62deea103cb8--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----62deea103cb8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----62deea103cb8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----62deea103cb8--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----62deea103cb8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----62deea103cb8--------------------------------)
    ·13 min read·May 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Softmax regression (or **multinomial logistic regression**) is a generalization
    of logistic regression to multi-class problems.
  prefs: []
  type: TYPE_NORMAL
- en: It can be used to predict the probabilities of different possible outcomes of
    some event, such as a patient having a specific disease out of a group of possible
    diseases based on their characteristics (gender, age, blood pressure, outcomes
    of various tests, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: In this article we will derive the softmax regression model from first principles
    and show how to use it to solve an image classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before reading this article, I strongly recommend that you read my previous
    article on logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/mastering-logistic-regression-3e502686f0ae?source=post_page-----62deea103cb8--------------------------------)
    [## Mastering Logistic Regression'
  prefs: []
  type: TYPE_NORMAL
- en: From theory to implementation in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/mastering-logistic-regression-3e502686f0ae?source=post_page-----62deea103cb8--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Background: Multi-Class Classification Problems'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that in [supervised machine learning](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)
    problems, we are given a training set of *n* labeled samples: *D* = {(**x**₁,
    *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ* is a *m*-dimensional
    vector that contains the **features** of sample *i*, and *yᵢ* represents the **label**
    of that sample. Our goal is to build a model whose predictions are as close as
    possible to the true labels.'
  prefs: []
  type: TYPE_NORMAL
- en: In **multi-class classification problems**, the target label can take any one
    of *k* classes, i.e., *y* ∈ {1, …, *k*}. For example, in a handwritten digit recognition
    task, *k* = 10, since there are 10 possible digits (0–9).
  prefs: []
  type: TYPE_NORMAL
- en: 'As in binary classification problems, we distinguish between two types of classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deterministic classifiers** output a **hard label** for each sample, without
    providing probability estimates for the classes. Examples for such classifiers
    include [K-nearest neighbors](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad),
    [decision trees](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369),
    and SVMs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Probabilistic classifiers** output probability estimates for the *k* classes,
    and then assign a label based on these probabilities (typically the label of the
    class with the highest probability). Examples for such classifiers include softmax
    regression, [Naive Bayes classifiers](https://medium.com/towards-data-science/naive-bayes-classification-41d1fe802a1e)
    and [neural networks](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)
    that use softmax in the output layer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Softmax Regression Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a sample (**x**, *y*), the softmax regression model outputs a vector of
    probabilities **p** = (*p*₁, …, *pₖ*)*ᵗ*,where *pᵢ* represents the probability
    that the sample belongs to class *i:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4257accf938586b77fe4c9805d4f379.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The probabilities in the vector must sum to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f4764292b19f6939d455aa9eb9bc93a.png)'
  prefs: []
  type: TYPE_IMG
- en: In (binary) logistic regression, our assumption was that the **log odds** (the
    logarithm of the ratio between *p* and 1 − *p*) was a linear combination of the
    input features (the vector **x**).
  prefs: []
  type: TYPE_NORMAL
- en: In softmax regression, we choose one of the probabilities as a reference (let’s
    say *pₖ*), and assume that the log-odds ratio between each probability *pᵢ* and
    *pₖ* is a linear combination of the input features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, we can write the log-odds between *pᵢ* and *pₖ* as a dot product
    of some weight vector **w***ᵢ* and the feature vector **x**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/826734b9ff67c5b2cde6e72aad4cfcdc.png)'
  prefs: []
  type: TYPE_IMG
- en: The log odds as a linear combination of the features
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that in softmax regression we have a separate vector of parameters **w***ᵢ*
    for each class *i.* The set of all the parameters of the model is typically stored
    in a matrix *W* of size (*m* + 1) × *k* , obtained by concatenating **w**₁, …,
    **w***ₖ* into columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62c047da2e555aec151fb16ba8772de7.png)'
  prefs: []
  type: TYPE_IMG
- en: The matrix of parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'By taking the exponent of both sides of the log-odds equation we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/44a395b0b0b07bc7f557d7b9cbb8aa0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since all the *k* probabilities sum to one, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac15be5bb428809661d0ac8f717ec62f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now use the expression for *pₖ* to find all the other probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55c80cfc96b15c861fade672b94ff3df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since all the *k* probabilities must sum to 1, the probability *pₖ* is completely
    determined once all the rest are known. Therefore, we have only *k* − 1 separately
    identifiable vectors of coefficients. This means that we can arbitrarily choose
    **w***ₖ* = 0 to make sure that exp(**w***ₖᵗ*) = 1\. This in turn allows us to
    write the equations above more compactly as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31279ee37055f7f0b82edf65dbac7f77.png)'
  prefs: []
  type: TYPE_IMG
- en: The function that converts the linear functions **w***ᵗ***x** into probabilities
    is called **softmax** and is described next.
  prefs: []
  type: TYPE_NORMAL
- en: The Softmax Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The softmax function, *σ*(**z**):ℝ*ᵏ* → ℝ*ᵏ,* converts a vector of *k* real
    numbers **z** = (*z*₁, …, *zₖ*)*ᵗ* into a probability vector (*σ*(*z*₁), …, *σ*(*zₖ*))*ᵗ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e5be549c5901f62a096245da9f8a129c.png)'
  prefs: []
  type: TYPE_IMG
- en: The softmax function
  prefs: []
  type: TYPE_NORMAL
- en: It can be easily verified that all the components of *σ*(**z**) are in the range
    (0,1), and that their sum is 1.
  prefs: []
  type: TYPE_NORMAL
- en: The name “softmax” derives from the fact that the function is a smooth approximation
    of the argmax function. For example, the softmax of the vector (1, 2, 6) is approximately
    (0.007, 0.018, 0.976), which puts almost all of the unit weight on the maximum
    element of the vector.
  prefs: []
  type: TYPE_NORMAL
- en: The softmax function is an extension of the **sigmoid** (logistic) function
    to the multi-class case. In other words, it can be shown that when there are only
    two classes softmax becomes the sigmoid function (left as an exercise to the reader).
  prefs: []
  type: TYPE_NORMAL
- en: The softmax function is also commonly used in [neural networks](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)
    to convert the outputs of the final layer of the network into probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram summarizes the computational process of the softmax regression
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da31f2d6fc02e4775000e375b0cfc989.png)'
  prefs: []
  type: TYPE_IMG
- en: The softmax regression model
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, this process can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6afd6b0148a02f33bdcbc416e0d061a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross-Entropy Loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our goal is to find the set of parameters *W*that will make the model’s predictions
    **p** = *σ*(**w**₁*ᵗ***x**,…**, w***ₖᵗ***x**) as close as possible to the true
    labels *y*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the output of our model is a vector of probabilities, while the true
    label is a scalar. In order to make them comparable, we encode the labels using
    **one-hot encoding,** i.e., we convert each label *y* into a binary vector **y**
    = (*y*₁, …, *yₖ*)*ᵗ*, where *yᵢ* = 1 for the true class *i* and 0 elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'A loss function is used to measure how far our model’s prediction is from the
    true label. The loss function used in softmax regression is called **cross-entropy
    loss**, which is an extension of log loss to the multi-class case. It is defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e427f6ddcc9387b6b90580074048758.png)'
  prefs: []
  type: TYPE_IMG
- en: The cross-entropy loss
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, assume that in a three class problem (*k* = 3), we are given a
    sample whose true class is class no. 2 (i.e., **y** = (0, 1, 0)*ᵗ*), and the prediction
    of our model for this sample is **p** = (0.3, 0.6, 0.1)*ᵗ*. Then the cross-entropy
    loss induced by this sample is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1891f3a6b5b5e64882b4c8a882e70e41.png)'
  prefs: []
  type: TYPE_IMG
- en: Similar to log loss, we can show that cross-entropy loss is the negative of
    the log-likelihood of the model, under the assumption that the labels are sampled
    from a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution)
    (a generalization of Bernoulli distribution to *k* possible outcomes).
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a model of the data (the labels) as a categorical distribution with probabilities
    **p** = (*p*₁, …, *pₖ*)*ᵗ*, the probability that a given sample belongs to class
    *i* is *pᵢ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f942e0a2fc1a0df23c29397166fc2d0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the probability that the true label of the sample is **y** is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d89058ed69762eaf26ec90e40a76d4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Explanation: if the correct class of the given sample is i, then yᵢ = *1*,
    and for all j ≠ i, yⱼ = *0*. Hence, P*(****y***|***p****)* = pᵢ, which is the
    probability that the sample belongs to class i.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Therefore, the log likelihood of our model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94d51e236487cec80bc12c7e7b14f80e.png)'
  prefs: []
  type: TYPE_IMG
- en: The cross-entropy loss is exactly the negative of this function. Therefore,
    minimizing the cross-entropy loss is equivalent to maximizing the log likelihood
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As was the case in logistic regression, there is no closed-form solution for
    the optimal *W* that minimizes the cross-entropy loss. Therefore, we need to use
    an iterative optimization method such as gradient descent in order to find the
    minimum loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, the cross-entropy loss has a simple gradient (although its derivation
    is not so simple…). The gradient of the cross-entropy loss with respect to each
    of the parameter vectors **w***ⱼ* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d87904ae7dbb844ea14bebc3e4e59a1e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Proof**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first write the cross-entropy loss as an explicit function of the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f49e60b99f525a19cb540ddb159bde3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the chain rule, we have that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7f3cddae0fad3f62cb9d6a70e7afce6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We start by computing the first partial derivative. Using the rules for sum
    of derivatives, the derivative of log and the chain rule, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f678a76cde18850b09fa76584fa41f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we compute the partial derivative ∂*pᵢ*/∂*zⱼ* (i.e., the derivative of
    the softmax function) using the quotient rule. We distinguish between two cases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If *i* = *j*, then:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/1aeaa61f656c5b3897460b2532a4d8aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If *i* ≠ *j*, then:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/83996a810715f7a0b43580831531e748.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Together, these two equations give us the derivative of the softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1534b3fc5dc1bfc05510eb9cc0d9d84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using this result, we can finish the computation of the derivative of *L*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61bc7b5402270b53db3b3ae11745e1ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since exactly one of the *yᵢ* is 1 and the others are 0, we can further simplify
    this derivative to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9ecb7484d69285ef852fe2027fae02d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The partial derivative of *zⱼ* with respect to **w***ⱼ* is simply the input
    vector **x**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf628cc5d62fe920283e81c24bc196eb.png)'
  prefs: []
  type: TYPE_IMG
- en: (see [this article](https://medium.com/@roiyeho/a-gentle-introduction-to-matrix-calculus-14584f2c4f60)
    for basic rules of matrix calculus).
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the derivative of the cross-entropy loss with respect to each of
    the weight vectors is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4791b270793a7ee48dd736a999c5135d.png)'
  prefs: []
  type: TYPE_IMG
- en: With these gradients, we can use (stochastic) gradient descent to minimize the
    loss function on the given training set.
  prefs: []
  type: TYPE_NORMAL
- en: Practice Question
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are given a set of images and you need to classify them into dogs/cats and
    outdoor/indoor. Should you implement two logistic regression classifiers or one
    softmax regression classifier?
  prefs: []
  type: TYPE_NORMAL
- en: The solution can be found at the end of the article.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax Regression in Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The class [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
    can handle both binary and multi-class classification problems. It has a parameter
    called *multi_class* which by default is set to ‘auto’. The meaning of this option
    is that Scikit-Learn will automatically apply a softmax regression whenever it
    detects that the problem is multi-class and the chosen solver supports optimization
    of the multinomial loss (all solvers support it except for ‘liblinear’).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Classifying Handwritten Digits'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For example, let’s train a softmax regression model on the [MNIST data set](https://en.wikipedia.org/wiki/MNIST_database),
    which is a widely used data set for image classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The data set contains 60,000 training images and 10,000 testing images of handwritten
    digits. Each image is 28 × 28 pixels in size, and is typically represented by
    a vector of 784 numbers in the range [0, 255]. The task is to classify these images
    into one of the ten digits (0–9).
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Data Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first fetch the MNIST data set using the [fetch_openml()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html)
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s examine the shape of *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*X* consists of 70,000 vectors, each has 784 pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s display the first 50 digits in the data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8cf5f43a5f3f214df8bf8277c60c3530.png)'
  prefs: []
  type: TYPE_IMG
- en: The first 50 digits from the MNIST data set
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we scale the inputs to be within the range [0, 1] instead of [0, 255]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Feature scaling is important whenever you use an iterative optimization method
    such as gradient descent to train your model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now split the data into training and test sets. Note that the first 60,000
    images in MNIST are already designated for training, so we can just use simple
    slicing for the split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Building the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now create a LogisticRegression classifier with its default settings and
    fit it to the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We get a warning message that the maximum number of iterations has been reached:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s increase *max_iter* to 1000 (instead of the default 100):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This time the learning has converged before reaching the maximum number of
    iterations. We can actually check how many iterations were needed for convergence
    by inspecting the *n_iter_* attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It took 795 iterations for the learning to converge.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The accuracy of the model on the training and the test sets is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: These results are good, but recent deep neural networks can achieve much better
    results on this data set (up to 99.91% accuracy on the test!). The softmax regression
    model is roughly equivalent to a neural network with a single layer of perceptrons
    that use the softmax activation function. Therefore, it is not surprising that
    a deep network can achieve better results than our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand better the errors of our model, let’s display its confusion matrix
    on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ea9fb5fd71f859570db77c2d75fc789c.png)'
  prefs: []
  type: TYPE_IMG
- en: The confusion matrix on the test set
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the main confusions of the model are between the digits 5⇔8
    and 4⇔9\. This makes sense since these digits often resemble each other when written
    by hand. To help our model distinguish between these digits, we can add more examples
    from these digits (e.g., by using data augmentation) or extract additional features
    from the images (e.g., the number of closed loops in the digit).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also print the classification report to get the precision, recall and
    F1 score for each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the digits that the model gets the lowest scores on are 5 and 8.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the advantages of softmax regression is that it is highly interpretable
    (unlike “black box” models such as neural networks). The weight associated with
    each feature represents the importance of that feature.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can plot the weights associated with each pixel in each one
    of the digit classes (**w***ⱼ* for each *j* ∈ {1, …, 10}). This will show us the
    important segments in the images that are used to detect each digit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights matrix of the model is stored in an attribute called *coef_*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Row *i* of this matrix contains the learned weights of the model for class
    *i*. We can display each row as a 28 × 28 pixels image in order to examine the
    weights associated with each pixel in each one of the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6780af0aa9cb89dc56cc5e72dcff0467.png)'
  prefs: []
  type: TYPE_IMG
- en: Pixels with bright shades have a positive impact on the prediction while pixels
    with dark shades have a negative impact. Pixels with a gray level around 0 have
    no influence on the prediction (such as the pixels close to the border of the
    image).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The pros and cons of softmax regression as compared to other multi-class classification
    models are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**:'
  prefs: []
  type: TYPE_NORMAL
- en: Provides class probability estimates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly scalable, requiring a number of parameters linear in the number of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly interpretable (the weight associated with each feature represents its
    importance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle redundant features (by assigning them weights close to 0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**:'
  prefs: []
  type: TYPE_NORMAL
- en: Can find only linear decision boundaries between the classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually outperformed by more complex models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cannot deal with missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solution to the Practice Question
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the classes are not mutually exclusive (all the four combinations of dog/cat
    with outdoor/indoor are possible), you should train two logistic regression models.
    This is a multi-label problem, and one softmax classifier will not be able to
    handle it.
  prefs: []
  type: TYPE_NORMAL
- en: Final Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/softmax_regression](https://github.com/roiyeho/medium/tree/main/softmax_regression)'
  prefs: []
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'MNIST Dataset Info:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Citation:** Deng, L., 2012\. The mnist database of handwritten digit images
    for machine learning research. *IEEE Signal Processing Magazine*, 29(6), pp. 141–142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**License:** Yann LeCun and Corinna Cortes hold the copyright of the MNIST
    dataset which is available under the *Creative Commons Attribution-ShareAlike
    4.0 International License* ([CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
