- en: Deep Dive into Softmax Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入研究 Softmax 回归
- en: 原文：[https://towardsdatascience.com/deep-dive-into-softmax-regression-62deea103cb8](https://towardsdatascience.com/deep-dive-into-softmax-regression-62deea103cb8)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deep-dive-into-softmax-regression-62deea103cb8](https://towardsdatascience.com/deep-dive-into-softmax-regression-62deea103cb8)
- en: Understand the math behind softmax regression and how to use it to solve an
    image classification task
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 softmax 回归背后的数学原理以及如何用它解决图像分类任务
- en: '[](https://medium.com/@roiyeho?source=post_page-----62deea103cb8--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----62deea103cb8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----62deea103cb8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----62deea103cb8--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----62deea103cb8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----62deea103cb8--------------------------------)[![Roi
    Yehoshua 博士](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----62deea103cb8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----62deea103cb8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----62deea103cb8--------------------------------)
    [Roi Yehoshua 博士](https://medium.com/@roiyeho?source=post_page-----62deea103cb8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----62deea103cb8--------------------------------)
    ·13 min read·May 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----62deea103cb8--------------------------------)
    ·阅读时间13分钟·2023年5月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Softmax regression (or **multinomial logistic regression**) is a generalization
    of logistic regression to multi-class problems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 回归（或**多项式逻辑回归**）是逻辑回归在多类问题上的一种推广。
- en: It can be used to predict the probabilities of different possible outcomes of
    some event, such as a patient having a specific disease out of a group of possible
    diseases based on their characteristics (gender, age, blood pressure, outcomes
    of various tests, etc.).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用来预测某个事件不同可能结果的概率，例如，根据患者的特征（性别、年龄、血压、各种测试结果等），预测患者是否会患上某种特定疾病。
- en: In this article we will derive the softmax regression model from first principles
    and show how to use it to solve an image classification task.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将从基本原理推导 softmax 回归模型，并展示如何用它解决图像分类任务。
- en: 'Before reading this article, I strongly recommend that you read my previous
    article on logistic regression:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读这篇文章之前，我强烈推荐你阅读我之前关于逻辑回归的文章：
- en: '[](/mastering-logistic-regression-3e502686f0ae?source=post_page-----62deea103cb8--------------------------------)
    [## Mastering Logistic Regression'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/mastering-logistic-regression-3e502686f0ae?source=post_page-----62deea103cb8--------------------------------)
    [## 精通逻辑回归'
- en: From theory to implementation in Python
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从理论到 Python 实现
- en: towardsdatascience.com](/mastering-logistic-regression-3e502686f0ae?source=post_page-----62deea103cb8--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/mastering-logistic-regression-3e502686f0ae?source=post_page-----62deea103cb8--------------------------------)
- en: 'Background: Multi-Class Classification Problems'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景：多类分类问题
- en: 'Recall that in [supervised machine learning](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)
    problems, we are given a training set of *n* labeled samples: *D* = {(**x**₁,
    *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ* is a *m*-dimensional
    vector that contains the **features** of sample *i*, and *yᵢ* represents the **label**
    of that sample. Our goal is to build a model whose predictions are as close as
    possible to the true labels.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，在[监督式机器学习](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)问题中，我们给定一个包含*n*个标记样本的训练集：*D*
    = {(**x**₁, *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}，其中**x***ᵢ* 是一个*m*维向量，包含了样本*i*
    的**特征**，而 *yᵢ* 表示该样本的**标签**。我们的目标是构建一个模型，使其预测尽可能接近真实标签。
- en: In **multi-class classification problems**, the target label can take any one
    of *k* classes, i.e., *y* ∈ {1, …, *k*}. For example, in a handwritten digit recognition
    task, *k* = 10, since there are 10 possible digits (0–9).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在**多类分类问题**中，目标标签可以是 *k* 个类别中的任何一个，即 *y* ∈ {1, …, *k*}。例如，在手写数字识别任务中，*k* = 10，因为有10个可能的数字（0–9）。
- en: 'As in binary classification problems, we distinguish between two types of classifiers:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在二分类问题中，我们区分两种分类器：
- en: '**Deterministic classifiers** output a **hard label** for each sample, without
    providing probability estimates for the classes. Examples for such classifiers
    include [K-nearest neighbors](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad),
    [decision trees](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369),
    and SVMs.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**确定性分类器**为每个样本输出一个**硬标签**，而不提供类别的概率估计。此类分类器的示例包括 [K-近邻](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad)、[决策树](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369)
    和 SVM。'
- en: '**Probabilistic classifiers** output probability estimates for the *k* classes,
    and then assign a label based on these probabilities (typically the label of the
    class with the highest probability). Examples for such classifiers include softmax
    regression, [Naive Bayes classifiers](https://medium.com/towards-data-science/naive-bayes-classification-41d1fe802a1e)
    and [neural networks](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)
    that use softmax in the output layer.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**概率分类器**输出 *k* 个类别的概率估计，然后基于这些概率分配标签（通常是概率最高的类别的标签）。此类分类器的示例包括 softmax 回归，[朴素贝叶斯分类器](https://medium.com/towards-data-science/naive-bayes-classification-41d1fe802a1e)
    和在输出层使用 softmax 的 [神经网络](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)。'
- en: The Softmax Regression Model
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax 回归模型
- en: Given a sample (**x**, *y*), the softmax regression model outputs a vector of
    probabilities **p** = (*p*₁, …, *pₖ*)*ᵗ*,where *pᵢ* represents the probability
    that the sample belongs to class *i:*
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个样本（**x**，*y*），softmax 回归模型输出一个概率向量 **p** = (*p*₁, …, *pₖ*)*ᵗ*，其中 *pᵢ* 表示样本属于类别
    *i:* 的概率。
- en: '![](../Images/a4257accf938586b77fe4c9805d4f379.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4257accf938586b77fe4c9805d4f379.png)'
- en: 'The probabilities in the vector must sum to 1:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 向量中的概率之和必须为 1：
- en: '![](../Images/5f4764292b19f6939d455aa9eb9bc93a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f4764292b19f6939d455aa9eb9bc93a.png)'
- en: In (binary) logistic regression, our assumption was that the **log odds** (the
    logarithm of the ratio between *p* and 1 − *p*) was a linear combination of the
    input features (the vector **x**).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在（二分类）逻辑回归中，我们假设 **对数几率**（*p* 和 1 − *p* 之间的比率的对数）是输入特征（向量 **x**）的线性组合。
- en: In softmax regression, we choose one of the probabilities as a reference (let’s
    say *pₖ*), and assume that the log-odds ratio between each probability *pᵢ* and
    *pₖ* is a linear combination of the input features.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 softmax 回归中，我们选择一个概率作为参考（假设为 *pₖ*），并假设每个概率 *pᵢ* 和 *pₖ* 之间的对数几率比是输入特征的线性组合。
- en: 'In other words, we can write the log-odds between *pᵢ* and *pₖ* as a dot product
    of some weight vector **w***ᵢ* and the feature vector **x**:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以将 *pᵢ* 和 *pₖ* 之间的对数几率表示为某个权重向量**w***ᵢ* 和特征向量**x**的点积：
- en: '![](../Images/826734b9ff67c5b2cde6e72aad4cfcdc.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/826734b9ff67c5b2cde6e72aad4cfcdc.png)'
- en: The log odds as a linear combination of the features
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对数几率作为特征的线性组合
- en: 'Note that in softmax regression we have a separate vector of parameters **w***ᵢ*
    for each class *i.* The set of all the parameters of the model is typically stored
    in a matrix *W* of size (*m* + 1) × *k* , obtained by concatenating **w**₁, …,
    **w***ₖ* into columns:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 softmax 回归中，我们为每个类别 *i* 拥有一个单独的参数向量 **w***ᵢ*。模型的所有参数集合通常存储在一个大小为 (*m*
    + 1) × *k* 的矩阵 *W* 中，通过将 **w**₁, …, **w***ₖ* 连接成列获得：
- en: '![](../Images/62c047da2e555aec151fb16ba8772de7.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62c047da2e555aec151fb16ba8772de7.png)'
- en: The matrix of parameters
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 参数矩阵
- en: 'By taking the exponent of both sides of the log-odds equation we get:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对对数几率方程的两边取指数，我们得到：
- en: '![](../Images/44a395b0b0b07bc7f557d7b9cbb8aa0e.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44a395b0b0b07bc7f557d7b9cbb8aa0e.png)'
- en: 'Since all the *k* probabilities sum to one, we can write:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有 *k* 个概率之和为 1，我们可以写成：
- en: '![](../Images/ac15be5bb428809661d0ac8f717ec62f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac15be5bb428809661d0ac8f717ec62f.png)'
- en: 'We can now use the expression for *pₖ* to find all the other probabilities:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 *pₖ* 的表达式来找到所有其他概率：
- en: '![](../Images/55c80cfc96b15c861fade672b94ff3df.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55c80cfc96b15c861fade672b94ff3df.png)'
- en: 'Since all the *k* probabilities must sum to 1, the probability *pₖ* is completely
    determined once all the rest are known. Therefore, we have only *k* − 1 separately
    identifiable vectors of coefficients. This means that we can arbitrarily choose
    **w***ₖ* = 0 to make sure that exp(**w***ₖᵗ*) = 1\. This in turn allows us to
    write the equations above more compactly as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有 *k* 个概率之和必须为 1，一旦知道了其他所有概率，概率 *pₖ* 就完全确定。因此，我们只有 *k* − 1 个可以单独识别的系数向量。这意味着我们可以任意选择
    **w***ₖ* = 0 来确保 exp(**w***ₖᵗ*) = 1。这样，我们可以更紧凑地写出上述方程：
- en: '![](../Images/31279ee37055f7f0b82edf65dbac7f77.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31279ee37055f7f0b82edf65dbac7f77.png)'
- en: The function that converts the linear functions **w***ᵗ***x** into probabilities
    is called **softmax** and is described next.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 将线性函数**w***ᵗ***x** 转换为概率的函数称为**softmax**，接下来将对此进行描述。
- en: The Softmax Function
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软最大函数
- en: 'The softmax function, *σ*(**z**):ℝ*ᵏ* → ℝ*ᵏ,* converts a vector of *k* real
    numbers **z** = (*z*₁, …, *zₖ*)*ᵗ* into a probability vector (*σ*(*z*₁), …, *σ*(*zₖ*))*ᵗ*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 软最大函数，*σ*(**z**):ℝ*ᵏ* → ℝ*ᵏ,* 将一个包含*k*个实数的向量**z** = (*z*₁, …, *zₖ*)*ᵗ* 转换为一个概率向量
    (*σ*(*z*₁), …, *σ*(*zₖ*))*ᵗ*：
- en: '![](../Images/e5be549c5901f62a096245da9f8a129c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5be549c5901f62a096245da9f8a129c.png)'
- en: The softmax function
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 软最大函数
- en: It can be easily verified that all the components of *σ*(**z**) are in the range
    (0,1), and that their sum is 1.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 可以很容易地验证所有*σ*(**z**)的分量都在(0,1)范围内，并且它们的和为1。
- en: The name “softmax” derives from the fact that the function is a smooth approximation
    of the argmax function. For example, the softmax of the vector (1, 2, 6) is approximately
    (0.007, 0.018, 0.976), which puts almost all of the unit weight on the maximum
    element of the vector.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: “softmax”这个名字源于该函数是argmax函数的平滑近似。例如，向量(1, 2, 6)的softmax大约为(0.007, 0.018, 0.976)，这几乎将所有的权重集中在向量的最大元素上。
- en: The softmax function is an extension of the **sigmoid** (logistic) function
    to the multi-class case. In other words, it can be shown that when there are only
    two classes softmax becomes the sigmoid function (left as an exercise to the reader).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 软最大函数是**sigmoid**（逻辑）函数在多类情况中的扩展。换句话说，可以证明当只有两个类别时，softmax变成sigmoid函数（留给读者作为练习）。
- en: The softmax function is also commonly used in [neural networks](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)
    to convert the outputs of the final layer of the network into probabilities.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 软最大函数也常用于[神经网络](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)中，将网络最后一层的输出转换为概率。
- en: 'The following diagram summarizes the computational process of the softmax regression
    model:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表总结了软最大回归模型的计算过程：
- en: '![](../Images/da31f2d6fc02e4775000e375b0cfc989.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da31f2d6fc02e4775000e375b0cfc989.png)'
- en: The softmax regression model
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 软最大回归模型
- en: 'Mathematically, this process can be written as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，这个过程可以写作如下：
- en: '![](../Images/6afd6b0148a02f33bdcbc416e0d061a3.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6afd6b0148a02f33bdcbc416e0d061a3.png)'
- en: Cross-Entropy Loss
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: Our goal is to find the set of parameters *W*that will make the model’s predictions
    **p** = *σ*(**w**₁*ᵗ***x**,…**, w***ₖᵗ***x**) as close as possible to the true
    labels *y*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到一组参数*W*，使得模型的预测**p** = *σ*(**w**₁*ᵗ***x**,…**, w***ₖᵗ***x**) 尽可能接近真实标签
    *y*。
- en: Note that the output of our model is a vector of probabilities, while the true
    label is a scalar. In order to make them comparable, we encode the labels using
    **one-hot encoding,** i.e., we convert each label *y* into a binary vector **y**
    = (*y*₁, …, *yₖ*)*ᵗ*, where *yᵢ* = 1 for the true class *i* and 0 elsewhere.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们模型的输出是一个概率向量，而真实标签是一个标量。为了使它们可比，我们使用**独热编码**对标签进行编码，即，我们将每个标签*y* 转换为一个二进制向量**y**
    = (*y*₁, …, *yₖ*)*ᵗ*，其中 *yᵢ* = 1 表示真实类别 *i*，其他位置为0。
- en: 'A loss function is used to measure how far our model’s prediction is from the
    true label. The loss function used in softmax regression is called **cross-entropy
    loss**, which is an extension of log loss to the multi-class case. It is defined
    as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数用于衡量我们模型的预测与真实标签之间的差距。软最大回归中使用的损失函数称为**交叉熵损失**，这是对多类情况的对数损失的扩展。它的定义如下：
- en: '![](../Images/2e427f6ddcc9387b6b90580074048758.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e427f6ddcc9387b6b90580074048758.png)'
- en: The cross-entropy loss
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: 'For example, assume that in a three class problem (*k* = 3), we are given a
    sample whose true class is class no. 2 (i.e., **y** = (0, 1, 0)*ᵗ*), and the prediction
    of our model for this sample is **p** = (0.3, 0.6, 0.1)*ᵗ*. Then the cross-entropy
    loss induced by this sample is:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设在一个三类问题中（*k* = 3），我们有一个真实类别为第2类的样本（即**y** = (0, 1, 0)*ᵗ*），并且我们模型对该样本的预测是**p**
    = (0.3, 0.6, 0.1)*ᵗ*。那么由该样本引起的交叉熵损失是：
- en: '![](../Images/1891f3a6b5b5e64882b4c8a882e70e41.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1891f3a6b5b5e64882b4c8a882e70e41.png)'
- en: Similar to log loss, we can show that cross-entropy loss is the negative of
    the log-likelihood of the model, under the assumption that the labels are sampled
    from a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution)
    (a generalization of Bernoulli distribution to *k* possible outcomes).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于对数损失，我们可以证明交叉熵损失是模型对数似然的负值，前提是标签是从一个[分类分布](https://en.wikipedia.org/wiki/Categorical_distribution)中抽取的（这是伯努利分布对*k*种可能结果的一种推广）。
- en: '**Proof**:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明**：'
- en: 'Given a model of the data (the labels) as a categorical distribution with probabilities
    **p** = (*p*₁, …, *pₖ*)*ᵗ*, the probability that a given sample belongs to class
    *i* is *pᵢ*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个数据（标签）的模型作为具有概率**p** = (*p*₁, …, *pₖ*)*ᵗ*的分类分布，则一个给定样本属于类别*i*的概率是*pᵢ*：
- en: '![](../Images/f942e0a2fc1a0df23c29397166fc2d0e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f942e0a2fc1a0df23c29397166fc2d0e.png)'
- en: 'Therefore, the probability that the true label of the sample is **y** is:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，样本的真实标签为**y**的概率是：
- en: '![](../Images/0d89058ed69762eaf26ec90e40a76d4d.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d89058ed69762eaf26ec90e40a76d4d.png)'
- en: 'Explanation: if the correct class of the given sample is i, then yᵢ = *1*,
    and for all j ≠ i, yⱼ = *0*. Hence, P*(****y***|***p****)* = pᵢ, which is the
    probability that the sample belongs to class i.'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 解释：如果给定样本的正确类别是i，则yᵢ = *1*，对于所有j ≠ i，yⱼ = *0*。因此，P*(****y***|***p****)* = pᵢ，这就是样本属于类别i的概率。
- en: 'Therefore, the log likelihood of our model is:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们模型的对数似然是：
- en: '![](../Images/94d51e236487cec80bc12c7e7b14f80e.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94d51e236487cec80bc12c7e7b14f80e.png)'
- en: The cross-entropy loss is exactly the negative of this function. Therefore,
    minimizing the cross-entropy loss is equivalent to maximizing the log likelihood
    of the model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失正好是这个函数的负值。因此，最小化交叉熵损失等价于最大化模型的对数似然。
- en: Gradient Descent
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: As was the case in logistic regression, there is no closed-form solution for
    the optimal *W* that minimizes the cross-entropy loss. Therefore, we need to use
    an iterative optimization method such as gradient descent in order to find the
    minimum loss.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与逻辑回归一样，没有解析解可以找到最小化交叉熵损失的最佳*W*。因此，我们需要使用迭代优化方法，如梯度下降，来寻找最小损失。
- en: 'Fortunately, the cross-entropy loss has a simple gradient (although its derivation
    is not so simple…). The gradient of the cross-entropy loss with respect to each
    of the parameter vectors **w***ⱼ* is:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，交叉熵损失的梯度非常简单（尽管其推导并不简单……）。交叉熵损失对每个参数向量**w***ⱼ*的梯度是：
- en: '![](../Images/d87904ae7dbb844ea14bebc3e4e59a1e.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d87904ae7dbb844ea14bebc3e4e59a1e.png)'
- en: '**Proof**:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明**：'
- en: 'We first write the cross-entropy loss as an explicit function of the weights:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将交叉熵损失写成权重的显式函数：
- en: '![](../Images/4f49e60b99f525a19cb540ddb159bde3.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f49e60b99f525a19cb540ddb159bde3.png)'
- en: 'Using the chain rule, we have that:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用链式法则，我们有：
- en: '![](../Images/c7f3cddae0fad3f62cb9d6a70e7afce6.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7f3cddae0fad3f62cb9d6a70e7afce6.png)'
- en: 'We start by computing the first partial derivative. Using the rules for sum
    of derivatives, the derivative of log and the chain rule, we have:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从计算第一个偏导数开始。利用导数的加法规则、对数的导数规则和链式法则，我们得到：
- en: '![](../Images/9f678a76cde18850b09fa76584fa41f8.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f678a76cde18850b09fa76584fa41f8.png)'
- en: 'Next, we compute the partial derivative ∂*pᵢ*/∂*zⱼ* (i.e., the derivative of
    the softmax function) using the quotient rule. We distinguish between two cases:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用商法则计算偏导数 ∂*pᵢ*/∂*zⱼ*（即softmax函数的导数）。我们区分两种情况：
- en: 'If *i* = *j*, then:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*i* = *j*，则：
- en: '![](../Images/1aeaa61f656c5b3897460b2532a4d8aa.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1aeaa61f656c5b3897460b2532a4d8aa.png)'
- en: 'If *i* ≠ *j*, then:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*i* ≠ *j*，则：
- en: '![](../Images/83996a810715f7a0b43580831531e748.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83996a810715f7a0b43580831531e748.png)'
- en: 'Together, these two equations give us the derivative of the softmax function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这两个方程，我们得到softmax函数的导数：
- en: '![](../Images/b1534b3fc5dc1bfc05510eb9cc0d9d84.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1534b3fc5dc1bfc05510eb9cc0d9d84.png)'
- en: 'Using this result, we can finish the computation of the derivative of *L*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这一结果，我们可以完成对*L*的导数计算：
- en: '![](../Images/61bc7b5402270b53db3b3ae11745e1ec.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61bc7b5402270b53db3b3ae11745e1ec.png)'
- en: 'Since exactly one of the *yᵢ* is 1 and the others are 0, we can further simplify
    this derivative to:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于恰好一个*yᵢ*是1，其他都是0，我们可以进一步简化这个导数为：
- en: '![](../Images/c9ecb7484d69285ef852fe2027fae02d.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9ecb7484d69285ef852fe2027fae02d.png)'
- en: 'The partial derivative of *zⱼ* with respect to **w***ⱼ* is simply the input
    vector **x**:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*zⱼ*关于**w***ⱼ*的偏导数就是输入向量**x**：'
- en: '![](../Images/cf628cc5d62fe920283e81c24bc196eb.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf628cc5d62fe920283e81c24bc196eb.png)'
- en: (see [this article](https://medium.com/@roiyeho/a-gentle-introduction-to-matrix-calculus-14584f2c4f60)
    for basic rules of matrix calculus).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: （参见 [这篇文章](https://medium.com/@roiyeho/a-gentle-introduction-to-matrix-calculus-14584f2c4f60)
    了解矩阵微积分的基本规则）。
- en: 'Therefore, the derivative of the cross-entropy loss with respect to each of
    the weight vectors is:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，交叉熵损失相对于每个权重向量的导数是：
- en: '![](../Images/4791b270793a7ee48dd736a999c5135d.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4791b270793a7ee48dd736a999c5135d.png)'
- en: With these gradients, we can use (stochastic) gradient descent to minimize the
    loss function on the given training set.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些梯度，我们可以使用（随机）梯度下降来最小化给定训练集上的损失函数。
- en: Practice Question
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习题
- en: You are given a set of images and you need to classify them into dogs/cats and
    outdoor/indoor. Should you implement two logistic regression classifiers or one
    softmax regression classifier?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组图像，你需要将它们分类为狗/猫和户外/室内。你应该实现两个逻辑回归分类器还是一个softmax回归分类器？
- en: The solution can be found at the end of the article.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案可以在文章末尾找到。
- en: Softmax Regression in Scikit-Learn
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn中的softmax回归
- en: The class [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
    can handle both binary and multi-class classification problems. It has a parameter
    called *multi_class* which by default is set to ‘auto’. The meaning of this option
    is that Scikit-Learn will automatically apply a softmax regression whenever it
    detects that the problem is multi-class and the chosen solver supports optimization
    of the multinomial loss (all solvers support it except for ‘liblinear’).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
    可以处理二分类和多分类问题。它有一个名为 *multi_class* 的参数，默认设置为‘auto’。这个选项的意思是，当Scikit-Learn检测到问题是多分类且选择的求解器支持多项式损失的优化时（所有求解器都支持，除了‘liblinear’），它将自动应用softmax回归。
- en: 'Example: Classifying Handwritten Digits'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：分类手写数字
- en: For example, let’s train a softmax regression model on the [MNIST data set](https://en.wikipedia.org/wiki/MNIST_database),
    which is a widely used data set for image classification tasks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们在 [MNIST 数据集](https://en.wikipedia.org/wiki/MNIST_database) 上训练一个softmax回归模型，这是一个广泛使用的图像分类任务数据集。
- en: The data set contains 60,000 training images and 10,000 testing images of handwritten
    digits. Each image is 28 × 28 pixels in size, and is typically represented by
    a vector of 784 numbers in the range [0, 255]. The task is to classify these images
    into one of the ten digits (0–9).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含60,000张训练图像和10,000张手写数字测试图像。每张图像的大小为28 × 28像素，通常由一个范围为 [0, 255] 的784个数字组成。任务是将这些图像分类为十个数字（0–9）之一。
- en: Loading the Data Set
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'We first fetch the MNIST data set using the [fetch_openml()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html)
    function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用 [fetch_openml()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html)
    函数获取MNIST数据集：
- en: '[PRE0]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s examine the shape of *X*:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下 *X* 的形状：
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*X* consists of 70,000 vectors, each has 784 pixels.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*X* 由70,000个向量组成，每个向量有784个像素。'
- en: 'Let’s display the first 50 digits in the data set:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示数据集中前50个数字：
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/8cf5f43a5f3f214df8bf8277c60c3530.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cf5f43a5f3f214df8bf8277c60c3530.png)'
- en: The first 50 digits from the MNIST data set
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集中的前50个数字
- en: 'Next, we scale the inputs to be within the range [0, 1] instead of [0, 255]:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将输入数据缩放到 [0, 1] 的范围，而不是 [0, 255]：
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Feature scaling is important whenever you use an iterative optimization method
    such as gradient descent to train your model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放在你使用迭代优化方法（如梯度下降）来训练模型时非常重要。
- en: 'We now split the data into training and test sets. Note that the first 60,000
    images in MNIST are already designated for training, so we can just use simple
    slicing for the split:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将数据分为训练集和测试集。请注意，MNIST中的前60,000张图像已被指定用于训练，因此我们可以仅使用简单的切片来进行分割：
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Building the Model
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'We now create a LogisticRegression classifier with its default settings and
    fit it to the training set:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建一个具有默认设置的 LogisticRegression 分类器，并将其拟合到训练集上：
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We get a warning message that the maximum number of iterations has been reached:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收到一条警告信息，表明已经达到最大迭代次数：
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s increase *max_iter* to 1000 (instead of the default 100):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 *max_iter* 增加到1000（而不是默认的100）：
- en: '[PRE8]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This time the learning has converged before reaching the maximum number of
    iterations. We can actually check how many iterations were needed for convergence
    by inspecting the *n_iter_* attribute:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这次学习在达到最大迭代次数之前已经收敛。我们实际上可以通过检查 *n_iter_* 属性来查看收敛所需的迭代次数：
- en: '[PRE9]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It took 795 iterations for the learning to converge.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 学习收敛花费了795次迭代。
- en: Evaluating the Model
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'The accuracy of the model on the training and the test sets is:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在训练集和测试集上的准确率为：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: These results are good, but recent deep neural networks can achieve much better
    results on this data set (up to 99.91% accuracy on the test!). The softmax regression
    model is roughly equivalent to a neural network with a single layer of perceptrons
    that use the softmax activation function. Therefore, it is not surprising that
    a deep network can achieve better results than our model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果很好，但最近的深度神经网络在这个数据集上可以取得更好的结果（测试准确率高达99.91%）。Softmax 回归模型大致相当于一个使用 softmax
    激活函数的单层感知机神经网络。因此，深度网络能够取得比我们的模型更好的结果并不令人惊讶。
- en: 'To understand better the errors of our model, let’s display its confusion matrix
    on the test set:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解我们模型的错误，让我们显示其在测试集上的混淆矩阵：
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/ea9fb5fd71f859570db77c2d75fc789c.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea9fb5fd71f859570db77c2d75fc789c.png)'
- en: The confusion matrix on the test set
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集上的混淆矩阵
- en: We can see that the main confusions of the model are between the digits 5⇔8
    and 4⇔9\. This makes sense since these digits often resemble each other when written
    by hand. To help our model distinguish between these digits, we can add more examples
    from these digits (e.g., by using data augmentation) or extract additional features
    from the images (e.g., the number of closed loops in the digit).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模型的主要混淆是在数字 5⇔8 和 4⇔9 之间。这是合理的，因为这些数字在手写时常常互相类似。为了帮助我们的模型区分这些数字，我们可以增加这些数字的更多示例（例如，通过数据增强）或从图像中提取额外特征（例如，数字中的闭环数量）。
- en: 'We can also print the classification report to get the precision, recall and
    F1 score for each class:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以打印分类报告，以获取每个类别的精确度、召回率和 F1 分数：
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As expected, the digits that the model gets the lowest scores on are 5 and 8.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，模型得分最低的数字是5和8。
- en: Visualizing the Weights
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重可视化
- en: One of the advantages of softmax regression is that it is highly interpretable
    (unlike “black box” models such as neural networks). The weight associated with
    each feature represents the importance of that feature.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 回归的一个优势是高度可解释（与神经网络等“黑箱”模型不同）。每个特征相关的权重表示该特征的重要性。
- en: For example, we can plot the weights associated with each pixel in each one
    of the digit classes (**w***ⱼ* for each *j* ∈ {1, …, 10}). This will show us the
    important segments in the images that are used to detect each digit.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以绘制每个数字类别中每个像素相关的权重（**w***ⱼ* 对于每个 *j* ∈ {1, …, 10}）。这将展示用于检测每个数字的图像中的重要区域。
- en: 'The weights matrix of the model is stored in an attribute called *coef_*:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的权重矩阵存储在名为 *coef_* 的属性中：
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Row *i* of this matrix contains the learned weights of the model for class
    *i*. We can display each row as a 28 × 28 pixels image in order to examine the
    weights associated with each pixel in each one of the classes:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 该矩阵的 *i* 行包含了模型对类别 *i* 的学习权重。我们可以将每一行显示为 28 × 28 像素的图像，以检查每个类别中与每个像素相关的权重：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/6780af0aa9cb89dc56cc5e72dcff0467.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6780af0aa9cb89dc56cc5e72dcff0467.png)'
- en: Pixels with bright shades have a positive impact on the prediction while pixels
    with dark shades have a negative impact. Pixels with a gray level around 0 have
    no influence on the prediction (such as the pixels close to the border of the
    image).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 亮色像素对预测有正面影响，而暗色像素有负面影响。灰色级别接近0的像素对预测没有影响（如图像边缘附近的像素）。
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'The pros and cons of softmax regression as compared to other multi-class classification
    models are:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 回归与其他多类分类模型相比的优缺点是：
- en: '**Pros**:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: Provides class probability estimates
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供类别概率估计
- en: Highly scalable, requiring a number of parameters linear in the number of features
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度可扩展，需要的参数数量与特征数量线性相关
- en: Highly interpretable (the weight associated with each feature represents its
    importance)
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度可解释（每个特征相关的权重表示其重要性）
- en: Can handle redundant features (by assigning them weights close to 0)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能处理冗余特征（通过将其权重分配接近0）
- en: '**Cons**:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: Can find only linear decision boundaries between the classes
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只能找到类别之间的线性决策边界
- en: Usually outperformed by more complex models
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常被更复杂的模型超越
- en: Cannot deal with missing values
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法处理缺失值
- en: Solution to the Practice Question
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习题的解答
- en: Since the classes are not mutually exclusive (all the four combinations of dog/cat
    with outdoor/indoor are possible), you should train two logistic regression models.
    This is a multi-label problem, and one softmax classifier will not be able to
    handle it.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于类别之间并非完全互斥（所有四种狗/猫与户外/室内的组合都是可能的），你应该训练两个逻辑回归模型。这是一个多标签问题，一个 softmax 分类器无法处理。
- en: Final Notes
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最终说明
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/softmax_regression](https://github.com/roiyeho/medium/tree/main/softmax_regression)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的 GitHub 上找到本文的代码示例：[https://github.com/roiyeho/medium/tree/main/softmax_regression](https://github.com/roiyeho/medium/tree/main/softmax_regression)
- en: All images unless otherwise noted are by the author.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图像均为作者提供。
- en: 'MNIST Dataset Info:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集信息：
- en: '**Citation:** Deng, L., 2012\. The mnist database of handwritten digit images
    for machine learning research. *IEEE Signal Processing Magazine*, 29(6), pp. 141–142.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引用:** Deng, L., 2012\. 用于机器学习研究的手写数字图像的 mnist 数据库。*IEEE 信号处理杂志*, 29(6), 第
    141–142 页。'
- en: '**License:** Yann LeCun and Corinna Cortes hold the copyright of the MNIST
    dataset which is available under the *Creative Commons Attribution-ShareAlike
    4.0 International License* ([CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0/)).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**许可:** Yann LeCun 和 Corinna Cortes 拥有 MNIST 数据集的版权，该数据集在*知识共享署名-相同方式共享 4.0
    国际许可协议*下提供 ([CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0/))。'
- en: Thanks for reading!
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
