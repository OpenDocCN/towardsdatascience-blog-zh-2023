- en: Why Trust and Safety in Enterprise AI Is (Relatively) Easy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-trust-and-safety-in-enterprise-ai-is-relatively-easy-32de240788d0](https://towardsdatascience.com/why-trust-and-safety-in-enterprise-ai-is-relatively-easy-32de240788d0)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data-Driven Leadership and Careers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why traditional AI has the reliability advantage over generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://kozyrkov.medium.com/?source=post_page-----32de240788d0--------------------------------)[![Cassie
    Kozyrkov](../Images/ad18dd12979a4a3ec130bdf8b889af23.png)](https://kozyrkov.medium.com/?source=post_page-----32de240788d0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----32de240788d0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----32de240788d0--------------------------------)
    [Cassie Kozyrkov](https://kozyrkov.medium.com/?source=post_page-----32de240788d0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----32de240788d0--------------------------------)
    ·5 min read·May 31, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Part 1 of this series](https://bit.ly/quaesita_rawmat1), I said something
    that I’d thought I’d never say: when we’re dealing with typical enterprise-scale
    AI systems, trust and safety is easy.'
  prefs: []
  type: TYPE_NORMAL
- en: What?! Blasphemy!
  prefs: []
  type: TYPE_NORMAL
- en: Hear me out.
  prefs: []
  type: TYPE_NORMAL
- en: Yes, okay, it’s [actually pretty hard](http://bit.ly/mfml_part3). But the difficulty
    pales in comparison to the trust and safety headache that is the new wave of [generative
    AI](http://bit.ly/quaesita_uxrevolution). Here’s why.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b5bd26b1d7aac816b3a847fca189110.png)'
  prefs: []
  type: TYPE_IMG
- en: All image rights belong to the author.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re the CEO of an airline without an AI-based [ticket pricing system](https://d3.harvard.edu/platform-rctom/submission/machine-learning-and-ai-at-delta-air-lines/).
    Your Chief of Staff runs into your office panting that some team of data hotshots
    in your organization is hours away from a full-scale AI pricing system launch,
    but they were overheard saying, *“I have no idea how good this AI system is. Dunno
    how much revenue it makes or loses… but it seems useful, so let’s launch it.”*
  prefs: []
  type: TYPE_NORMAL
- en: Heads will roll. Such a system’s reach and potential business impact is too
    massive for this level of sloppiness. You’ll likely fire everyone who had anything
    to do with this completely unhinged scenario and you’ll be right to do it. After
    all, as the CEO, ultimate responsibility for the airline’s success falls to you
    and getting rid of this gaggle of clowns will be a no-brainer given the inappropriate
    level of risk they almost subjected your enterprise to. The whole situation is
    criminally stupid. Your company is better off without them.
  prefs: []
  type: TYPE_NORMAL
- en: Say what you will about large organizations, but the one thing they tend to
    be good at is avoiding anything that frivolously rocks the boat.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Typically, a problem like that is smothered long before it reaches the CEO’s
    desk. Say what you will about large organizations, but the one thing they tend
    to be good at is avoiding anything that frivolously rocks the boat. There’s a
    built-in preference for caution over gambling, which is why an enterprise-scale
    AI system typically only gets out of the gate if (1) it *provably* solves a ***specific***
    problem provably well or (2) it has a *provably* low potential for harm (because
    the stakes are low, because errors wouldn’t be very embarrassing/painful, or because
    the application is of low strategic importance).
  prefs: []
  type: TYPE_NORMAL
- en: The straightforwardness of the AI system’s raison d’être is an extremely powerful
    simplification tool.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Examples from an airline’s point of view:*'
  prefs: []
  type: TYPE_NORMAL
- en: (1) An AI pricing system that is carefully launched in a gradual ramp-up and
    statistically tested to have a positive revenue impact of at least *x%*.
  prefs: []
  type: TYPE_NORMAL
- en: (2) An AI pronunciation system that allows a gate agent listen to a data-driven
    best guess about how to announce a passenger’s name to help the agent out if they’re
    unsure about the pronunciation. A system like this is hardly mission-critical
    and it comes with the upside of being able to tell the world you do AI without
    taking on much risk. Also, harmlessness is easier to achieve when trained humans
    get to approve all the output, so you’d want your gate agents to use their judgment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d692a2d1caced9e9901d3d3de980a5cf.png)'
  prefs: []
  type: TYPE_IMG
- en: “You want me to pronounce what?” (I often get this look when it’s time for people
    to pronounce my last name, Kozyrkov. I tell them to just say “coffee pot”, no
    one will notice.) All image rights belong to the author.
  prefs: []
  type: TYPE_NORMAL
- en: The point is that unless the trust and safety issues are already minimized by
    the very nature of the application, an enterprise-scale AI system isn’t going
    to see the light of day unless there’s *proof* that its upside is worth the risk…
    and getting this kind of proof is impossible by definition when there’s no clarity
    about the value that the system provides.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why does this make things easy? Because it means that every mission-critical
    traditional enterprise-scale AI system (category (1)) tends to have:'
  prefs: []
  type: TYPE_NORMAL
- en: a relatively straightforward use case statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a vision of what the intended “[good behavior](http://bit.ly/mfml_039)” for
    the system looks like
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a clear, monolithic objective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: measurable performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: well-defined [testing criteria](http://bit.ly/mfml_047)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: relative clarity about what could go wrong and thus which [safety nets](https://bit.ly/quaesita_policy)
    are needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are plenty of challenges here too, like how to guarantee that a system
    like this plays nice with all the existing enterprise systems ([see my YouTube
    course for that and more](http://bit.ly/mfml_part3)), but the straightforwardness
    of the system’s raison d’être is an extremely powerful simplification tool.
  prefs: []
  type: TYPE_NORMAL
- en: The key insight here is that the economics for enterprise-grade solutions tend
    to favor scale. Systems intended for deployment at scale usually have a clear
    purpose, else a smart leader sends them straight to the garbage compactor. That’s
    why most enterprise-grade AI systems of the past decade were designed to do one
    very specific thing really well at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Most enterprise-grade AI systems of the past decade were designed to do one
    very specific thing really well at scale.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is a huge advantage for trust and safety. Huge! Sure, there’s plenty of
    general reliability work to do to ensure that you keep your users safe when your
    system meets “[the long tail](http://bit.ly/mfml_086)” (the unusual users), but
    it’s still a lot easier to protect a varied group of users from a single-purpose,
    single-function system than to protect the same group from a *multi*-purpose,
    multi-function system. And from most enterprises’ perspective, Generative AI systems
    are fundamentally multi-purpose and multi-functional.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s the key insight, so let’s repeat it:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s a lot easier to protect a varied group of users from a single-purpose system
    than to protect the same group from a multi-purpose system.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you’d like a better understanding of this insight, continue on to [Part 3](https://bit.ly/quaesita_rawmat3)
    of this series.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if this last insight is obvious to you, then feel free to
    skip Part 3 and head straight to [Part 4](https://bit.ly/quaesita_rawmat4) where
    I explain why generative AI doesn’t come with these same simplifying characteristics
    and what that means for AI regulation.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading! How about a course?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you had fun here and you’re looking for an unboring leadership-oriented
    course designed to delight AI beginners and experts alike, [here’s a little something](https://bit.ly/funaicourse)
    I made for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/300b5280620ea948fc3dbffb708084d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Course link: [https://bit.ly/funaicourse](https://bit.ly/funaicourse)'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kozyrkov.medium.com/membership?source=post_page-----32de240788d0--------------------------------)
    [## Join Medium'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Cassie Kozyrkov (and thousands of other writers on Medium).
    Your membership fee directly supports…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kozyrkov.medium.com](https://kozyrkov.medium.com/membership?source=post_page-----32de240788d0--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*P.S. Have you ever tried hitting the clap button here on Medium more than
    once to see what happens?* ❤️'
  prefs: []
  type: TYPE_NORMAL
- en: Liked the author? Connect with Cassie Kozyrkov
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s be friends! You can find me on [Twitter](https://twitter.com/quaesita),
    [YouTube](https://www.youtube.com/channel/UCbOX--VOebPe-MMRkatFRxw), [Substack](http://decision.substack.com),
    and [LinkedIn](https://www.linkedin.com/in/kozyrkov/). Interested in having me
    speak at your event? Use [this form](http://bit.ly/makecassietalk) to get in touch.
  prefs: []
  type: TYPE_NORMAL
