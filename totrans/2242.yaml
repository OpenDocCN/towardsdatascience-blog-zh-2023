- en: 'Unveiling the Dropout Layer: An Essential Tool for Enhancing Neural Networks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/unveiling-the-dropout-layer-an-essential-tool-for-enhancing-neural-networks-e090b726561e](https://towardsdatascience.com/unveiling-the-dropout-layer-an-essential-tool-for-enhancing-neural-networks-e090b726561e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Understanding the Dropout Layer: Improving Neural Network Training and Reducing
    Overfitting with Dropout Regularization'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@niklas_lang?source=post_page-----e090b726561e--------------------------------)[![Niklas
    Lang](../Images/5fa71386db00d248438c588c5ae79c67.png)](https://medium.com/@niklas_lang?source=post_page-----e090b726561e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e090b726561e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e090b726561e--------------------------------)
    [Niklas Lang](https://medium.com/@niklas_lang?source=post_page-----e090b726561e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e090b726561e--------------------------------)
    ·7 min read·May 19, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cbac42a6864089508adf36e1897f5f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Martin Sanchez](https://unsplash.com/@martinsanchez?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The dropout layer is a layer used in the construction of [neural networks](https://databasecamp.de/en/ml/artificial-neural-networks)
    to prevent [overfitting](https://databasecamp.de/en/ml/overfitting-en). In this
    process, individual nodes are excluded in various training runs using a probability,
    as if they were not part of the network architecture at all.
  prefs: []
  type: TYPE_NORMAL
- en: However, before we can get to the details of this layer, we should first understand
    how a neural network works and why [overfitting](https://databasecamp.de/en/ml/overfitting-en)
    can occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quick Recap: How does a Perceptron work?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The perceptron is a mathematical model inspired by the structure of the human
    brain. It consists of a single neuron that receives numerical inputs with different
    weights. The inputs are multiplied by their weights and summed up, and the result
    is passed through an activation function. In its simplest form, the perceptron
    produces binary outputs, such as “Yes” or “No,” based on the activation function.
    The sigmoid function is commonly used as an activation function, mapping the weighted
    sum to values between 0 and 1\. If the weighted sum exceeds a certain threshold,
    the output transitions from 0 to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/deabfd4e4cf1d6fe680cc9e326079306.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Basic Structure of a Perceptron | Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more detailed look into the concept of perceptrons, feel free to refer
    to this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/what-is-a-perceptron-5ac56720d8cf?source=post_page-----e090b726561e--------------------------------)
    [## Uncovering the Power of Perceptrons: The Building Blocks of Deep Learning'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the foundations of artificial neural networks and their real-world
    applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/what-is-a-perceptron-5ac56720d8cf?source=post_page-----e090b726561e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Quick Recap: What is Overfitting?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Overfitting](https://databasecamp.de/en/ml/overfitting-en) occurs when a predictive
    model becomes too specific to the training data, learning both the patterns and
    noise present in the data. This results in poor generalization and inaccurate
    predictions on new, unseen data. Deep neural networks are particularly susceptible
    to overfitting as they can learn the statistical noise of the training data. However,
    abandoning complex architectures is not desirable, as they enable learning complex
    relationships. The introduction of dropout layers helps address overfitting by
    providing a solution to balance model complexity and generalization.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b85c76203e4d13868674a5d72877acb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Difference between Generalisation and Overfitting | Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more detailed article on overfitting please refer to our article on the
    topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://databasecamp.de/en/ml/overfitting-en?source=post_page-----e090b726561e--------------------------------)
    [## What is Overfitting? | Data Basecamp'
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is a term from the field of data science and describes the property
    of a model to adapt too strongly to the…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: databasecamp.de](https://databasecamp.de/en/ml/overfitting-en?source=post_page-----e090b726561e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: How does the Dropout Layer works?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With dropout, certain nodes are set to the value zero in a training run, i.e.
    removed from the network. Thus, they have no influence on the prediction and also
    in the [backpropagation](https://databasecamp.de/en/ml/backpropagation-basics).
    Thus, a new, slightly modified network architecture is built in each run and the
    network learns to produce good predictions without certain inputs.
  prefs: []
  type: TYPE_NORMAL
- en: When installing the dropout layer, a so-called dropout probability must also
    be specified. This determines how many of the nodes in the layer will be set equal
    to 0\. If we have an input layer with ten input values, a dropout probability
    of 10% means that one random input will be set equal to zero in each training
    pass. If instead, it is a hidden layer, the same logic is applied to the hidden
    nodes. So a dropout probability of 10% means that 10% of the nodes will not be
    used in each run.
  prefs: []
  type: TYPE_NORMAL
- en: The optimal probability also depends strongly on the layer type. As various
    papers have found, for the input layer, a dropout probability close to one is
    optimal. For hidden layers, on the other hand, a probability close to 50% leads
    to better results.
  prefs: []
  type: TYPE_NORMAL
- en: Why does the dropout layer prevent overfitting?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In deep neural networks, overfitting usually occurs because certain neurons
    from different layers influence each other. Simply put, this leads, for example,
    to certain neurons correcting the errors of previous nodes and thus depending
    on each other or simply passing on the good results of the previous layer without
    major changes. This results in comparatively poor generalization.
  prefs: []
  type: TYPE_NORMAL
- en: By using the dropout layer, on the other hand, neurons can no longer rely on
    the nodes from previous or subsequent layers, since they cannot assume that they
    even exist in that particular training run. This leads to neurons, provably, recognizing
    more fundamental structures in data that do not depend on the existence of individual
    neurons. These dependencies actually occur relatively frequently in regular neural
    networks, as this is an easy way to quickly reduce the loss function and thereby
    quickly get closer to the goal of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Also, as mentioned earlier, the dropout slightly changes the architecture of
    the network. Thus, the trained-out model is then a combination of many, slightly
    different models. We are already familiar with this approach from ensemble learning,
    such as in [Random Forests](https://databasecamp.de/en/ml/random-forests). It
    turns out that the ensemble of many, relatively similar models usually gives better
    results than a single model. This phenomenon is known as the “Wisdom of the Crowds”.
  prefs: []
  type: TYPE_NORMAL
- en: How do you build Dropout into an existing network?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, the dropout layer is often used after a fully-connected layer,
    since this has comparatively many parameters and the probability of so-called
    “co-adaptation”, i.e. the dependence of neurons on each other, is very high. However,
    theoretically, a dropout layer can also be inserted after any layer, but this
    can then also lead to worse results.
  prefs: []
  type: TYPE_NORMAL
- en: Practically, the dropout layer is simply inserted after the desired layer and
    then uses the neurons of the previous layer as inputs. Depending on the value
    of the probability, some of these neurons are then set to zero and passed on to
    the subsequent layer.
  prefs: []
  type: TYPE_NORMAL
- en: It is particularly useful to use the dropout layers in larger neural networks.
    This is because an architecture with many layers tends to overfit much more strongly
    than smaller networks. It is also important to increase the number of nodes accordingly
    when a dropout layer is added. As a rule of thumb, the number of nodes before
    the introduction of the dropout is divided by the dropout rate.
  prefs: []
  type: TYPE_NORMAL
- en: What happens to the dropout during inference?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have now established, the use of a dropout layer during training is an
    important factor in avoiding overfitting. However, the question remains whether
    this system is also used when the model has been trained and is then used for
    predictions for new data.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the dropout layers are no longer used for predictions after training.
    This means that all neurons remain for the final prediction. However, the model
    now has more neurons available than it did during training. However, as a result,
    the weights in the output layer are significantly higher than what was learned
    during training. Therefore, the weights are scaled with the amount of the dropout
    rate so that the model still makes good predictions.
  prefs: []
  type: TYPE_NORMAL
- en: How to use the dropout layers in Python?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For [Python](https://databasecamp.de/en/python-coding), there are already many
    predefined implementations with which you can use dropout layers. The best-known
    is probably that of Keras or [TensorFlow](https://databasecamp.de/en/python-coding/tensorflow-en).
    You can import these, like other layer types, via “tf.keras.layers”:'
  prefs: []
  type: TYPE_NORMAL
- en: Then you pass the parameters, i.e. on the one hand the size of the input vector
    and the dropout probability, which you should choose depending on the layer type
    and the network structure. The layer can then be used by passing actual values
    in the variable “data”. There is also the parameter “training”, which specifies
    whether the dropout layer is only used in training and not in the prediction of
    new values, the so-called inference.
  prefs: []
  type: TYPE_NORMAL
- en: If the parameter is not explicitly set, the dropout layer will only be active
    for “model.fit()”, i.e. training, and not for “model.predict()”, i.e. predicting
    new values.
  prefs: []
  type: TYPE_NORMAL
- en: This is what you should take with you
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A dropout is a layer in a neural network that sets neurons to zero with a defined
    probability, i.e. ignores them in a training run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this way, the danger of overfitting can be reduced in deep neural networks,
    since the neurons do not form a so-called adaptation among themselves, but recognize
    deeper structures in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dropout layer can be used in the input layer as well as in the hidden layers.
    However, it has been shown that different dropout probabilities should be used
    depending on the layer type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, once the training has been trained out, the dropout layer is no longer
    used for predictions. However, in order for the model to continue to produce good
    results, the weights are scaled using the dropout rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*If you like my work, please subscribe* [*here*](https://medium.com/subscribe/@niklas_lang)
    *or check out my website* [*Data Basecamp*](http://www.databasecamp.de/en/homepage)*!
    Also, medium permits you to read* ***3 articles*** *per month for free. If you
    wish to have* ***unlimited*** *access to my articles and thousands of great articles,
    don’t hesitate to get a membership for $****5*** *per month by clicking my referral
    link:* [https://medium.com/@niklas_lang/membership](https://medium.com/@niklas_lang/membership)'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/understanding-the-backpropagation-algorithm-c7a99d43088b?source=post_page-----e090b726561e--------------------------------)
    [## Unraveling the Mystery of Back Propagation: A Comprehensive Guide'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the inner workings of the backpropagation algorithm for training
    neural networks.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/understanding-the-backpropagation-algorithm-c7a99d43088b?source=post_page-----e090b726561e--------------------------------)
    [](/what-are-tensors-in-machine-learning-5671814646ff?source=post_page-----e090b726561e--------------------------------)
    [## From Vectors to Tensors: Exploring the Mathematics of Tensor Algebra'
  prefs: []
  type: TYPE_NORMAL
- en: How Tensors are Used in Machine Learning and Beyond
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/what-are-tensors-in-machine-learning-5671814646ff?source=post_page-----e090b726561e--------------------------------)
    [](/the-importance-of-cross-validation-in-machine-learning-35b728bbce33?source=post_page-----e090b726561e--------------------------------)
    [## The Importance of Cross Validation in Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: Explaining why Machine Learning needs Cross Validation and how it is done in
    Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-importance-of-cross-validation-in-machine-learning-35b728bbce33?source=post_page-----e090b726561e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
