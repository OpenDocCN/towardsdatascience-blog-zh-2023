# 梯度下降与梯度提升：逐一对比

> 原文：[https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712](https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712)

## 从初始化到收敛的简单英语

[](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[![Angela and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------) [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------) ·阅读时长5分钟·2023年2月28日

--

# 引言

梯度下降和梯度提升是两种流行的机器学习算法。尽管它们的处理方法和应用不同，但这两种算法都基于梯度计算，并且共享若干相似步骤。本文的主要目的是详细比较这两种算法，帮助读者更好地理解它们的相似之处和不同之处。

![](../Images/8c776904768c4cbd3516306acf0a29c3.png)

图片由[Gregoire Jeanneau](https://unsplash.com/es/@gregjeanneau?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

## 梯度下降

梯度下降是机器学习中常用的优化算法，用于最小化成本函数。目标是找到一组最佳参数，以最小化预测值与实际值之间的误差。该过程开始时随机初始化模型的权重或系数。然后，它通过计算成本函数相对于每个参数的梯度，迭代地更新权重，沿着成本函数的最大下降方向进行。

## 梯度提升

梯度提升是一种集成方法，通过结合多个弱模型来创建一个更强的预测模型。它通过迭代地将新模型拟合到前一个模型的残差错误来进行工作。最终的预测是所有模型预测值的总和。在梯度提升中，重点是前一个模型所犯的错误。

## 不同却相似

为了提供梯度下降和梯度提升的全面比较，我们将首先分别解释这两种算法，然后逐步比较每种算法的优化方法。这种方法将帮助读者更好地理解这两种算法的相似性和差异。

# 梯度下降算法的简单英语解释

以下是一些用简单英语解释梯度下降的步骤：

1.  选择起点：梯度下降从随机或预定义的模型权重或系数开始。

1.  计算梯度：梯度是函数的最陡上升或下降方向。在梯度下降中，我们计算成本函数相对于每个参数的梯度。成本函数衡量模型对训练数据的拟合程度。

1.  更新权重：一旦我们获得梯度，就更新模型的权重，方向与梯度相反。更新的大小由学习率决定，学习率控制每次迭代中权重的调整幅度。

1.  直到收敛：我们重复步骤2和3，直到达到成本函数的最小值，这对应于模型的最佳权重集。收敛标准可能有所不同，例如达到一定的迭代次数或当成本函数的变化变得足够小时。

通过迭代地调整权重，朝着成本函数的最陡下降方向，梯度下降旨在找到最佳的参数集，以最小化预测值和实际值之间的误差。

# 梯度提升算法的简单英语解释

以下是一些用简单英语解释梯度提升的步骤：

1.  训练一个弱模型：我们从训练一个弱模型开始，比如决策树或回归模型，使用训练数据。弱模型可能单独表现不佳，但可以进行一些预测。

1.  计算误差：我们计算弱模型的预测值和实际值之间的误差。这个误差成为下一个模型的目标。

1.  训练新模型：我们训练一个新模型来预测前一个模型的错误。这个新模型在前一个模型的残差或错误上进行拟合。

1.  合并模型：我们将所有模型的预测结果合并以进行最终预测。最终预测是所有模型预测结果的总和。

1.  直到收敛：我们重复步骤2到4，向集成中添加新模型，直到达到预定义的模型数量或在验证集上的性能停止提升。

通过迭代地将新模型拟合到前一个模型的残差中，梯度提升旨在提高模型的准确性。最终预测是所有模型预测的组合，每个模型都纠正前一个模型的错误。梯度提升可以有效地处理非线性关系、缺失值和异常值。

# 并排比较

这里是梯度下降和梯度提升每一步的并排比较：

1\. 初始化：

+   梯度下降：随机或预定义初始化模型的权重或系数。

+   梯度提升：在训练数据上训练一个弱模型，例如决策树或回归模型。

2\. 错误计算：

+   梯度下降：计算模型在整个训练集上的预测值与实际值之间的误差或损失。

+   梯度提升：计算弱模型在训练集上的预测值与实际值之间的误差或残差。

3\. 更新或拟合：

+   梯度下降：根据学习率和成本函数的梯度，沿着梯度的反方向更新模型的权重。

+   梯度提升：拟合一个新模型以预测前一个模型的残差错误，基于弱模型的错误和训练数据。

4\. 组合：

+   梯度下降：不需要组合，因为目标是优化单个模型的参数。

+   梯度提升：结合所有模型的预测结果以做出最终预测。最终预测是所有模型预测结果的总和。

5\. 收敛：

+   梯度下降：重复步骤2到4，直到达到收敛，这可能取决于如迭代次数或成本函数的变化等标准。

+   梯度提升：重复步骤2到4，向集成中添加新模型，直到达到预定数量的模型或在验证集上的性能停止改善。

# 结论

梯度下降和梯度提升都依赖梯度计算来优化模型，但它们在方法和目的上有所不同。梯度下降专注于最小化单一模型的成本函数，而梯度提升则旨在提高模型集成的准确性。

尽管梯度下降和梯度提升具有不同的优化目标，但它们共享基于梯度下降的共同算法基础。在梯度下降中，算法优化单个模型的参数以最小化成本函数。相比之下，梯度提升旨在通过迭代添加新模型来优化模型集成，以最小化集成的成本函数。然而，这两种算法都使用梯度下降作为基本优化技术。
