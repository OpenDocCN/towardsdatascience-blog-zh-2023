- en: How to Tackle Class Imbalance Without Resampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-tackle-class-imbalance-without-resampling-47bbeb2180aa](https://towardsdatascience.com/how-to-tackle-class-imbalance-without-resampling-47bbeb2180aa)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Imbalanced classification beyond resampling, threshold tuning, or cost-sensitive
    models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://vcerq.medium.com/?source=post_page-----47bbeb2180aa--------------------------------)[![Vitor
    Cerqueira](../Images/9e52f462c6bc20453d3ea273eb52114b.png)](https://vcerq.medium.com/?source=post_page-----47bbeb2180aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----47bbeb2180aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----47bbeb2180aa--------------------------------)
    [Vitor Cerqueira](https://vcerq.medium.com/?source=post_page-----47bbeb2180aa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----47bbeb2180aa--------------------------------)
    ·7 min read·Mar 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9ebb8f645bb4e67587a6f3ae607c680.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Denise Johnson](https://unsplash.com/@auntneecey?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Imbalanced classification is a relevant machine learning task. This problem
    is usually handled with one of three approaches: resampling, cost-sensitive models,
    or threshold tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, you’ll learn a different approach. We’ll explore how to use
    clustering analysis to tackle imbalanced classification.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many real-world problems involve imbalanced data sets. In these, one of the
    classes is rare and, usually, more important to users.
  prefs: []
  type: TYPE_NORMAL
- en: Take fraud detection for example. Fraud cases are rare instances among vast
    amounts of regular activity. The accurate detection of rare but fraudulent activity
    is fundamental across many domains. Other common examples involving imbalanced
    data sets include customer churn or credit default prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Imbalanced distributions are a challenge for machine learning algorithms. There’s
    relatively little information about the minority class. This hinders the ability
    of algorithms to train good models because they tend to bias toward the majority
    class.
  prefs: []
  type: TYPE_NORMAL
- en: How to Deal with Class Imbalance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three standard approaches for dealing with class imbalance:'
  prefs: []
  type: TYPE_NORMAL
- en: Resampling methods;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cost-sensitive models;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Threshold tuning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/a058a1d7e62d2e8adbb47f9d68630b5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Viktor Talashuk](https://unsplash.com/@viktortalashuk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Resampling is arguably the most popular strategy for handling imbalanced classification
    tasks. This type of method transforms the training set to improve the relevance
    of the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: Resampling can be used to create new cases for the minority class (over-sampling),
    discard cases from the majority class (under-sampling), or a combination of both.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how resampling methods work using the *imblearn* library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Resampling methods are versatile and easy to couple with any learning algorithm.
    But, they have some limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Under-sampling the majority class may lead to important information loss. Over-sampling
    may increase the chance of overfitting. This occurs if resampling propagates noise
    from cases of the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: There are some alternatives to resampling the training data. These include tuning
    the decision threshold or using cost-sensitive models. Different thresholds lead
    to distinct precision and recall scores. So, adjusting the decision threshold
    can improve the performance of models.
  prefs: []
  type: TYPE_NORMAL
- en: Cost-sensitive models work by assigning different costs to misclassification
    errors. Errors in the minority class are typically more costly. This approach
    requires domain expertise to define the costs of each type of error.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling Class Imbalance with Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most resampling methods work by finding instances close to the decision boundary
    — the frontier that splits the instances from the majority class from those of
    the minority class. Borderline cases are, in principle, the most difficult to
    classify. So, they are used to drive the resampling process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37a847f56433fb33dee5290d8804725e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Decision boundary of an SVM model. Original: Alisneaky Vector: Zirguezi, CC
    BY-SA 4.0\. Image [source](https://commons.wikimedia.org/w/index.php?curid=47868867).'
  prefs: []
  type: TYPE_NORMAL
- en: For example, ADASYN is a popular over-sampling technique. It creates artificial
    instances using cases from the minority class whose nearest neighbors are from
    the majority class.
  prefs: []
  type: TYPE_NORMAL
- en: Finding borderline cases with clustering analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also capture which observations are close to the decision boundary using
    clustering analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose there’s a cluster whose observations all belong to the majority class.
    This might mean that this cluster is somewhat far from the decision boundary,
    on the side of the majority class. Generally, those observations are easy to model.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, an instance can be considered borderline if it belongs to
    a cluster that contains both classes.
  prefs: []
  type: TYPE_NORMAL
- en: We can use this information to build a hierarchical model for imbalanced classification.
  prefs: []
  type: TYPE_NORMAL
- en: How to build a hierarchical model for imbalanced classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We build a hierarchical model based on two levels.
  prefs: []
  type: TYPE_NORMAL
- en: In the first level, a model is built to split easy instances from borderline
    ones. So, the goal is to predict if an input instance belongs to a cluster with
    at least one observation from the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: In the second level, we discard the easy cases. Then, we build a model to solve
    the original classification task with the remaining data. The first level affects
    the second one by removing easy instances from the training set.
  prefs: []
  type: TYPE_NORMAL
- en: In both levels, the imbalanced problem is reduced, which makes the modeling
    task simpler.
  prefs: []
  type: TYPE_NORMAL
- en: Python implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The method described above is called ICLL (for Imbalanced Classification via
    Layered Learning). Here’s its implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The clustering part is done automatically without user input. So, the only thing
    you need to define is the learning algorithm on each level of the hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: And below is an example of how you can use the method. In this example, the
    model in each level is a Random Forest.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A more serious example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How does the hierarchical method compare with resampling?
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is a comparison based on a data set related to diabetes. You can check
    reference [1] for details. Here’s how we can apply both methods to this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Below is the ROC curve for each approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/359d27737414908fd10f5516e15b22b1.png)'
  prefs: []
  type: TYPE_IMG
- en: ROC curve for each method. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: ICLL’s curve is closer to the top-left side, which indicates it is the better
    model.
  prefs: []
  type: TYPE_NORMAL
- en: A lot more experiments were carried out in the paper in reference [2] where
    ICLL was presented. The results suggest that ICLL provides competitive performance
    in imbalanced classification problems. You can check the code for the experiments
    on [Github](https://github.com/vcerqueira/icll).
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imbalanced classification is an important task in data science;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resampling the training set is a common approach to handling these problems.
    But, these may lead to information loss or the propagation of noise. Common alternatives
    are threshold tuning or cost-sensitive models;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also use hierarchical methods to handle the imbalance problem;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ICLL is a hierarchical method for imbalanced classification. It doesn’t need
    any user parameters besides the learning algorithm. ICLL provides a competitive
    performance with resampling methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hope you find this method useful. Thank you for reading, and see you in the
    next story!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [Pima Indians Diabetes data set](https://sci2s.ugr.es/keel/dataset.php?cod=21#sub1)
    (GPL-3 License)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Cerqueira, V., Torgo, L., Branco, P., & Bellinger, C. (2022). Automated
    imbalanced classification via layered learning. *Machine Learning*, 1–22.](https://link.springer.com/article/10.1007/s10994-022-06282-w)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Branco, Paula, Luís Torgo, and Rita P. Ribeiro. “A survey of predictive
    modeling on imbalanced domains.” *ACM Computing Surveys (CSUR)* 49.2 (2016): 1–50.'
  prefs: []
  type: TYPE_NORMAL
