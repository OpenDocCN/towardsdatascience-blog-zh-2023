["```py\n[[ 0\\.  1.], [17\\. 18.], [10\\. 11.], [13\\. 14.], [14\\. 15.], [ 2\\.  3.] ... ]\n```", "```py\n[[0\\. 1.], [1\\. 2.], [ 2\\.  3.], [ 3\\.  4.], [ 4\\.  5.], [ 5\\.  6.] ...]\n```", "```py\n\"\"\"\nImplimenting the alignment function.\n\nThe whole idea is that, given an embedding for the encoder and decoder, the\nalignment func outputs a scalar raiting the alignment. So, the shapes of\nv, W, and U should be such that the output is a scalar.\n\"\"\"\n\nimport torch\nimport torch.nn.functional as F\n\n#defining the size of the input and output vectors of the attention mechanism\nEMBED_DIM = 100\n\n#these need to be sized in such a way that matrix multiplication yields a scalar\n#otherwise, they're just general learnable parameters. Different alignment\n#functions might have different parameters. For instance, \"attention is all you\n#need\" uses a projection head that generates a query, key, and value, which are\n#used in a different self-alignment function. this can allign vectors of different\n#lengths\nencoder_embedding_dim = EMBED_DIM*2\ndecoder_embedding_dim = EMBED_DIM\n\nU_attention = torch.rand(EMBED_DIM, encoder_embedding_dim)\nW_attention = torch.rand(decoder_embedding_dim, EMBED_DIM)\nv_attention = torch.rand(1,EMBED_DIM)\n\ndef alignment_func(s, h, W=W_attention, U=U_attention, v=v_attention):\n    \"\"\"\n    s: si-1, from the paper, the previous decoder state\n    h: hj, from the paper, an input embedding\n    W,U,v: trainable parameters\n\n    calculates v*tanh(W*s + U*h), should return the scalar alpha\n    \"\"\"\n\n    v1 = torch.matmul(W,s)\n    v2 = torch.matmul(U,h)\n    v3 = F.tanh(v1+v2)\n\n    return torch.matmul(v, v3)\n\n#testing the alignment function between one embedded word and another\n#dividing by value to get them in a good range for tanh\ns = torch.rand(decoder_embedding_dim)/50\nh = torch.rand(encoder_embedding_dim)/50\nalignment_func(s, h)\n```", "```py\n\"\"\"\ndefining attention, wich is a list of softmaxed alignment scores for all input\nembeddings (hj) given the previous decoder embedding (si-1). This is equivilent\nto a row of the attention matrix, hence the name of the function.\n\"\"\"\n\ndef compute_attention_row(s, hs, W=W_attention, U=U_attention, v=v_attention):\n    \"\"\"\n    computes alignments for all h values given s\n\n    s is a vector of length embedding size\n    hs is a tensor of shape (sequence length, embedding size)\n    the output is a vector of sequence length\n    \"\"\"\n    return F.softmax(torch.cat([alignment_func(s, h, W, U, v) for h in hs]),0)\n\n#testing the computation of an allignment row between the previous decoder\n#embedding and all encoder embeddings\ncompute_attention_row(torch.rand(decoder_embedding_dim)/50, torch.rand(10,encoder_embedding_dim)/50)\n```", "```py\n\"\"\"\nDefining the attention module\n\"\"\"\n\nfrom torch import nn\n\n#defining the input dimension from the encoder (h) and decoder (s)\nencoder_embedding_dim = 10\ndecoder_embedding_dim = 20\n\n#defining an example sequence length\nsequence_length = 15\n\nclass Attention(nn.Module):\n    \"\"\"\n    -computes an alignment for all encoder embeddings\n    -constructs a context vector using those embeddings\n    -outputs that context vector\n    \"\"\"\n\n    def __init__(self, embed_dim=EMBED_DIM, encoder_embedding_dim=encoder_embedding_dim, decoder_embedding_dim=decoder_embedding_dim):\n        super(Attention, self).__init__()\n\n        #learnable attention parameters\n        self.U = nn.Parameter(torch.rand(embed_dim, encoder_embedding_dim), requires_grad=True )\n        self.W = nn.Parameter(torch.rand(embed_dim, decoder_embedding_dim), requires_grad=True )\n        self.v = nn.Parameter(torch.rand(1,embed_dim), requires_grad=True )\n        self.encoder_embedding_dim = encoder_embedding_dim\n\n        if torch.cuda.is_available():\n            self.cuda()\n\n    def forward(self, s, hn):\n        \"\"\"\n        computes a batch of context vectors given a current the all encoder\n        embeddings and the current decoder embedding\n        \"\"\"\n        #defining a tensor consisting of a context vector for each batch\n        weights = compute_attention_row(s, hn, W=self.W, U=self.U, v=self.v)\n\n        return torch.sum(hn * weights[:, None], axis=0)\n\nprint('==== Testing Attention ====')\n#testing if the attention mechanism can support different sequence lengths\n#and embedding dimensions\ntest_attention = Attention()\n\n#defining previous decoder state\ns = torch.rand(decoder_embedding_dim)/50\n#defining input embeddings\nhn = torch.rand(sequence_length, encoder_embedding_dim)/50\n\ntest_attention(s, hn).shape\n```", "```py\n\"\"\" Training Attention\n\nEssentially, this generates random X/Y pairs, and trains the model to predict\neach output given the previous correct output and all of the inputs.\n\nThis is a proof of concept. In reality using minibatches, better initializations, and\nstochastically providing the true previous output occasionally would probably improve\nconvergence and generalizability.\n\"\"\"\n\nimport random\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmin_len = 5\nmax_len = 20\n\ntest_attention = Attention(20,2,2)\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.SGD(test_attention.parameters(), lr=1e-3, momentum=0.9)\nlr_phase = 0\n\n#training on some number of random sequences\nbatch_losses = []\nfor i in tqdm(range(800)):\n\n    #generating x and y\n    y = []\n    x = []\n    for j in range(random.randint(min_len,max_len)):\n        y.append([j+1, j+2])\n        x.append([j+1,j+2])\n    random.shuffle(x)\n    x = np.array([[0,1]] + x).astype(np.float32)\n    y = np.array([[0,1]] + y).astype(np.float32)\n    x = torch.from_numpy(x)\n    y = torch.from_numpy(y)\n\n    #iterating over all training examples (given s predict s+1)\n    s_in = x[0]\n    sample_losses = []\n    for j in range(2,len(x)):\n\n        y_this = y[j]\n\n        optimizer.zero_grad()\n        s_out = test_attention(s_in, x)\n\n        loss = loss_fn(s_out, y_this)\n        sample_losses.append(loss.detach())\n\n        loss.backward(retain_graph=True)\n        optimizer.step()\n\n        s_in = torch.clone(y_this).detach()\n\n    batch_loss = np.mean(sample_losses)\n    batch_losses.append(batch_loss)\n\n    #hacking together a simple learning rate scheduler\n    if batch_loss<0.05 and lr_phase == 0:\n        optimizer = torch.optim.SGD(test_attention.parameters(), lr=1e-4, momentum=0.9)\n        lr_phase+=1\n\n    #stopping training when loss is good enough\n    if batch_loss<0.03:\n        break\n\nplt.plot(batch_losses)\n```", "```py\n\"\"\"\nVisualizing alignment\n\"\"\"\n\n#generating x\nx = []\nfor j in range(1, random.randint(min_len,max_len)):\n    x.append([j,j+1])\nrandom.shuffle(x)\n\nx = np.array([[0,1]] + x).astype(np.float32)\nx = torch.from_numpy(x)\n\n#Extracting learned parameters for generating alignment visual\nW = test_attention.W\nU = test_attention.U\nv = test_attention.v\n\ns = x[0]\ny_hat = []\nrows = []\n\n#predicting the next element in the sequence.\n#skipping over the trivia first, and not predicting one after the last.\nfor _ in range(0,len(x)-1):\n\n    #computing attention weights for this output, for visualization purposes\n    row = list(compute_attention_row(s, x, W=W, U=U, v=v).detach().numpy())\n    rows.append(row)\n\n    #predicting what should be in this location.\n    with torch.no_grad():\n        s = torch.round(test_attention(s, x))\n\n    y_hat.append(list(s))\n\n#converting to numpy arrays\ny_hat = np.array(y_hat)\nx_p = np.array(x)\n\n#printing intputs and predicted outputs\nprint('input: ')\nprint(x_p)\nprint('output: ')\nprint(y_hat)\n\n#generating attention matrix plot\nfrom matplotlib.ticker import MaxNLocator\nalignments = np.array(rows)\nplt.pcolormesh(alignments, edgecolors='k', linewidth=2)\nax = plt.gca()\nax.set_aspect('equal')\nax.yaxis.set_major_locator(MaxNLocator(integer=True))\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.title('Algnment scores used in attention')\nplt.ylabel('output index (each row is attention for an output)')\nplt.xlabel('input index')\nplt.show()\n```"]