- en: Why Simple Models Are Often Better
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-simple-models-are-often-better-e2428964811a](https://towardsdatascience.com/why-simple-models-are-often-better-e2428964811a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The significance of Occam’s Razor in data science and machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thomasdorfer.medium.com/?source=post_page-----e2428964811a--------------------------------)[![Thomas
    A Dorfer](../Images/9258a1735cee805f1d9b02e2adf01096.png)](https://thomasdorfer.medium.com/?source=post_page-----e2428964811a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e2428964811a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e2428964811a--------------------------------)
    [Thomas A Dorfer](https://thomasdorfer.medium.com/?source=post_page-----e2428964811a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e2428964811a--------------------------------)
    ·8 min read·Jan 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1dc823456e970d8e19dbd386739e884.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Pablo Arroyo](https://unsplash.com/@pablogamedev) on [Unsplash](https://unsplash.com/photos/_SEbdtH4ZLM)
  prefs: []
  type: TYPE_NORMAL
- en: In data science and machine learning, simplicity is an important concept that
    can have significant impact on model characteristics such as performance and interpretability.
    Over-engineered solutions tend to adversely affect these characteristics by increasing
    the likelihood of overfitting, decreasing computational efficiency, and lowering
    the transparency of the model’s output.
  prefs: []
  type: TYPE_NORMAL
- en: The latter is particularly important for areas that require a certain degree
    of interpretability, such as medicine and healthcare, finance, or law. The inability
    to interpret and trust a model’s decision — and to ensure that this decision is
    fair and unbiased — can have serious consequences for individuals whose fate depends
    on it.
  prefs: []
  type: TYPE_NORMAL
- en: This article aims to highlight the importance of giving precedence to simplicity
    when it comes to implementing a data science or machine learning solution. We
    will first introduce the principle of **Occam’s razor**, before delving into the
    advantages of simplicity and ultimately determining when it is necessary to add
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Occam’s Razor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Occam’s razor, also known as the *law of parsimony*, is a philosophical problem-solving
    principle attributed to William of Ockham — a 14th century English philosopher
    and theologian. The original principle is often cited as *Entia non sunt multiplicanda
    praeter necessitatem*, which roughly translates to“Entities must not be multiplied
    beyond necessity”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7be075709ab568cd25a11c5c40b463cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Hair Spies](https://unsplash.com/@hairspies) on [Unsplash](https://unsplash.com/photos/mXw0CfTPUrM)
  prefs: []
  type: TYPE_NORMAL
- en: Within the realm of data science and machine learning, this is typically interpreted
    as something like “Simpler models are generally preferred over complex ones” or
    “All things being equal, the simplest solution tends to be the best”. The *razor*
    in Occam’s razor symbolizes the “shaving away” of unnecessary complexity and assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of Simple Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reduced Susceptibility to Overfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the main advantages of simpler models is that they are less susceptible
    to overfitting. Overfitting occurs when a model becomes too complex and starts
    fitting the noise in the training data, rather than the underlying pattern. This
    often leads to poor performance on unseen data, resulting in a lack of generalizability
    and, consequently, limited applicability of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several techniques that can be applied to obtain a model with a reduced
    susceptibility to overfit:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross-validation:** The model is trained on a training set and its performance
    is assessed on an independent validation set. The most common type is k-fold cross-validation,
    whereby the data is divided into *k* subsets, the model is trained on *k-1* subsets
    and eventually evaluated on the remaining subset (the validation set).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data augmentation:** Larger datasets tend to reduce overfitting. However,
    if large data is not available, the current data can be augmented through the
    production of artificial or synthetic data. The exact process here depends on
    the nature of the data. For instance, when dealing with image data, data size
    can be increased by applying transformations to the images such as rotations,
    flipping, rescaling, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization:** This technique constrains the model’s parameters by adding
    a penalty term to the loss function. The most common techniques are L1, or Lasso,
    regularization, and L2, or Ridge, regularization. While L1 regularization can
    result in some of the model weights being set to zero, effectively removing these
    features from the model, L2 regularization reduces the weights only asymptotically
    toward zero, resulting in all features being used to determine the model output.
    Moreover, dropout is a frequently used regularization method for neural networks,
    whereby a certain percentage of neurons are randomly set to zero during each training
    iteration. This ensures that the remaining neurons learn more robust features
    since they are no longer able to rely on the dropped-out neurons to carry the
    burden of prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection:** Removing features that are deemed redundant or irrelevant
    from the training data will inevitably simplify the model and improve its computational
    performance. Features can either be removed manually, based on domain knowledge,
    or through methods such as univariate filtering, tree-based feature importance,
    recursive feature elimination, etc..'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality reduction:** Similar to feature selection, dimensionality
    reduction techniques such as principal component analysis (PCA), linear discriminant
    analysis (LDA), or t-distributed stochastic neighbor embedding (tSNE) reduce the
    input dimensions to the model. However, the features they return are either a
    linear or nonlinear combination of the original features, which would in turn
    reduce the interpretability of the model as it would be hard to determine which
    original features contributed to the model’s decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Early stopping:** This is a method applied to neural networks that stops
    the model training once its performance on a validation set has started to degrade.
    Essentially, it prevents overfitting by stopping the training before the model
    becomes too complex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decrease model complexity:** Choosing a model with fewer parameters and a
    simpler architecture to begin with can also significantly contribute to the prevention
    of overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increased Computational Efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simpler models generally boost computational performance. This boost is mainly
    achieved by the model requiring **fewer parameters**, **fewer computations**,
    and **lower memory usage**.
  prefs: []
  type: TYPE_NORMAL
- en: This can also result in significant benefits when it comes to model deployment.
    As simpler models tend to have lower inference time and lower memory usage, they
    can be more easily deployed to resource-constrained devices such as smartphones
    and smart watches, potentially establishing a new customer base.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in natural language processing, simple n-gram models have been
    demonstrated to perform as well as their neural counterparts, while being considerably
    faster. [Doval & Gómez‐Rodríguez (2019)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6360409/)
    compared recurrent neural networks with n-gram models on a word segmentation task
    and found that the precision of the n-gram models was almost on par with neural
    network approaches. Moreover, n-gram models significantly outperformed recurrent
    neural networks in execution time.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in optical character recognition, a simple k-nearest neighbor (kNN)
    algorithm has been shown to yield similar accuracies — while having significantly
    shorter execution times — as convolutional neural networks (CNNs) for certain
    tasks. For example, [Sharma et al. (2022)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9307347/)
    assessed the performance of various classifiers on a handwritten digit recognition
    task and found that, while a CNN yielded an accuracy of 98.83%, a lightweight
    kNN performed almost just as well at 97.83%, while achieving much better computational
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Increased Interpretability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Being able to interpret a model’s decision and to ensure that this decision
    is **not unfair or biased** is critical, particularly in domains wherein the outcome
    can have serious consequences for individual persons.
  prefs: []
  type: TYPE_NORMAL
- en: In **medical imaging**, for instance, it matters that doctors are able to interpret,
    understand, and trust the results of their model. If a radiologist is not able
    to confirm or refute a model’s negative prediction on a cancer diagnosis, they
    may order further tests — some requiring invasive procedures — to establish a
    diagnosis. If these ultimately confirm that the patient does not have cancer,
    it can be argued that, as a result of the model’s lack of interpretability, the
    patient was exposed to unnecessary, invasive procedures, which could have been
    avoided had the radiologist been able to trust the model’s decision in the first
    place.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2062c63a6db4fc588ddad736fbb795c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [National Cancer Institute](https://unsplash.com/@nci) on [Unsplash](https://unsplash.com/photos/BDKid0yJcAk)
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the output of a prediction model can determine whether a cancer patient
    will receive treatment or not. Clearly, these results can have substantial consequences
    for the individuals involved and thus it is critical that the model is simple
    enough to be transparent and trustworthy.
  prefs: []
  type: TYPE_NORMAL
- en: In **credit scoring**, an opaque model can yield biased results that are completely
    divorced from reality. Imagine a model, whose aim it is to predict creditworthiness,
    was trained on historical data containing various biases, causing it to favor
    certain groups of people over others. If a person from an underrepresented group
    applies for a loan, the model may decide that they are a high-risk borrower and
    thus deny the application, even though in reality they have a solid credit history.
    This lack of model interpretability can seriously affect individuals who planned
    to purchase a home, start a business, or explore other opportunities that required
    some initial capital.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/752eb830a5688c375f414b57fd4029ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Etienne Martin](https://unsplash.com/@etiennemartin) on [Unsplash](https://unsplash.com/photos/2_K82gx9Uk8)
  prefs: []
  type: TYPE_NORMAL
- en: The same principle applies to the field of **criminal justice**. Imagine a model
    used to predict the probability of a person reoffending based on their personal
    information, criminal history, and various other factors. If that model was trained
    on biased data, it may end up favoring groups of higher socioeconomic status and,
    as a result, predict a higher likelihood of recidivism for all other groups. A
    person from a lower socioeconomic status may thus be denied bail and kept in detention
    even though in reality they may have a very low risk of reoffending. Again, working
    with a model that is both simple and transparent would reduce the probability
    of these adverse outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: In all these categories listed above, the easiest way to achieve higher interpretability
    is by choosing a simpler model. For example, choosing a decision tree-based algorithm
    over a complex neural network could significantly enhance the transparency of
    the output as the internal tree structure can be easily visualized. This allows
    individual decisions to be traced, which can provide critical insights into how
    a particular prediction came about. This makes decision trees widely applicable
    in areas where prediction errors can have dire consequences, such as those outlined
    above.
  prefs: []
  type: TYPE_NORMAL
- en: When Complexity Is Necessary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While simple is *often* better, this is not always the case. An overly simplistic
    model can miss the relevant relations between features and target variables, ultimately
    leading to underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with **large and high-dimensional datasets** whose features have
    **nonlinear relationships** with one another, more complex models such as neural
    networks may be necessary in order to capture the underlying patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Some applications may also have **high accuracy requirements** that a simple
    model is not able to meet. When implementing a more complex model to boost performance,
    however, the result is often reduced transparency. Therefore, it is also important
    to find a healthy **trade-off between model performance and interpretability**.
    For instance, in medical imaging, high accuracy is a critical characteristic when
    it comes to detecting abnormalities and making decisions about a patient’s life.
    However, as discussed above, it is equally important that doctors are able to
    understand, interpret, and trust those decisions in order to establish a reliable
    diagnostic procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While simple models are certainly not a panacea for all problems, they are the
    preferred choice when a high degree of interpretability and computational efficiency
    is desired. Moreover, they tend to prevent overfitting, leading to higher generalizability
    and applicability of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In certain situations, however, there is a necessity to increase the level of
    complexity of a solution. This is often the case when dealing with high-dimensional,
    nonlinear data or when a solution requires a high degree of accuracy that would
    otherwise be hard to achieve using a simpler model.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Doval & Gómez‐Rodríguez (2019). “Comparing neural‐ and N‐gram‐based language
    models for word segmentation*”.* J Assoc Inf Sci Technol, 70(2): 187–197.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Sharma (2022). “A Machine Learning and Deep Learning Approach for Recognizing
    Handwritten Digits*”.* Comput Intell Neurosci, doi: 10.1155/2022/9869948.'
  prefs: []
  type: TYPE_NORMAL
