- en: ü§óHugging Face Transformers Agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/hugging-face-transformers-agent-3a01cf3669ac](https://towardsdatascience.com/hugging-face-transformers-agent-3a01cf3669ac)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Comparisons with ü¶úüîóLangChain Agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://sophiamyang.medium.com/?source=post_page-----3a01cf3669ac--------------------------------)[![Sophia
    Yang, Ph.D.](../Images/c133f918245ea4857dc46df3a07fc2b1.png)](https://sophiamyang.medium.com/?source=post_page-----3a01cf3669ac--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3a01cf3669ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3a01cf3669ac--------------------------------)
    [Sophia Yang, Ph.D.](https://sophiamyang.medium.com/?source=post_page-----3a01cf3669ac--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3a01cf3669ac--------------------------------)
    ¬∑5 min read¬∑May 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Just two days ago, ü§óHugging Face released Transformers Agent ‚Äî an agent that
    leverages natural language to choose a tool from a curated collection of tools
    and accomplish various tasks. Does it sound familiar? Yes, it does because it‚Äôs
    a lot like ü¶úüîóLangChain Tools and Agents. In this blog post, I will cover what
    Transformers Agent is and its comparisons with ü¶úüîóLangChain Agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try out the Code:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can try out the code in [this colab](https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj)
    (provided by Hugging Face).
  prefs: []
  type: TYPE_NORMAL
- en: '**What are Transformers Agents?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In short, it provides a natural language API on top of transformers: we define
    a set of curated tools and design an agent to interpret natural language and to
    use these tools.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I can imagine engineers at HuggingFace be like: We have so many amazing models
    hosted on HuggingFace. Can we integrate those with LLMs? Can we use LLMs to decide
    which model to use, write code, run code, and generate results? Essentially, nobody
    needs to learn all the complicated task-specific models anymore. Just give it
    a task, LLMs (agents) will do everything for us.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a222be0579d543b90b678228080848d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://huggingface.co/docs/transformers/transformers_agents](https://huggingface.co/docs/transformers/transformers_agents)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instruction: the prompt users provide'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt: a prompt template with the specific instruction added, where it lists
    multiple tools to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tools: a curated list of transformers models, e.g., Flan-T5 for question answering,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Agent: an LLM that interprets the question, decides which tools to use, and
    generates code to perform the task with the tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Restricted Python interpreter: execute Python code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Step 1: Instantiate an agent.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Step 1 is to instantiate an agent. An agent is just an LLM, which can be an
    OpenAI model, a StarCoder model, or an OpenAssistant model.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAI model needs the OpenAI API key and the usage is not free. We load
    the StarCoder model and the OpenAssistant model from the HuggingFace Hub, which
    requires HuggingFace Hub API key and it is free to use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Run the agent.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`agent.run` is a single execution method and selects the tool for the task
    automatically, e.g., select the image generator tool to create an image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/067c6d283ea9db4702a5337699127b6e.png)'
  prefs: []
  type: TYPE_IMG
- en: '`agent.chat` keeps the chat history. For example, here it knows we generated
    a picture earlier and it can transform an image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4eaf141fd1387e09e7e726179d35505b.png)'
  prefs: []
  type: TYPE_IMG
- en: How is it different from ü¶úüîóLangChain Agent?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Transformers Agent is still experimental. It‚Äôs a lot smaller scope and less
    flexible. The main focus of Transformers Agent right now is for using Transformer
    models and executing Python code, whereas LangChain Agent does ‚Äúalmost‚Äù everything.
    Let be break it down to compare different components between Transformers and
    LangChain Agents:'
  prefs: []
  type: TYPE_NORMAL
- en: Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ü§óHugging Face Transfomers Agent has an amazing list of tools, each powered
    by transformer models. These tools offer three significant advantages: 1) Even
    though Transformers Agent can only interact with few tools currently, it has the
    potential to communicate with over 100,000 Hugging Face model. It possesses full
    multimodal capabilities, encompassing text, images, video, audio, and documents.;
    2) Since these models are purpose-built for specific tasks, utilizing them can
    be more straightforward and yield more accurate results compared to relying solely
    on LLMs. For example, instead of designing the prompts for the LLM to perform
    text classification, we can simply deploy BART that‚Äôs designed for text classification;
    3) These tools unlocked capabilities that LLMs alone can‚Äôt accomplish. Take BLIP,
    for example, which enables us to generate captivating image captions ‚Äî a task
    beyond the scope of LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ü¶úüîóLangChain tools are all external APIs, such as Google Search, Python REPL.
    In fact, LangChain supports HuggingFace Tools via the `load_huggingface_tool`
    function. LangChain can potentially do a lot of things Transformers Agent can
    do already. On the other hand, Transformers Agents can potentially incorporate
    all the LangChain tools as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both cases, each tool is just a Python file. You can find the files of ü§óHugging
    Face Transformers Agent tools [here](https://github.com/huggingface/transformers/tree/main/src/transformers/tools)
    and ü¶úüîóLangChain tools [here](https://github.com/hwchase17/langchain/tree/master/langchain/utilities).
    As you can see, each Python file contains one class indicating one tool.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ü§óHugging Face Transformers Agent uses [this prompt template](https://github.com/huggingface/transformers/blob/main/src/transformers/tools/prompts.py#L19-L93)
    to determine which tool to use based on the tool‚Äôs description. It asks the LLM
    to provide an explanations and it provides some few-shots learning examples in
    the prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ü¶úüîóLangChain by default uses the ReAct framework to determine which tool to use
    based on the tool‚Äôs description. The ReAct framework is described in this [paper](https://arxiv.org/pdf/2210.03629.pdf).
    It does not only act on a decision but also provides *thoughts* and *reasoning,*
    which is similar to the *explanations* Transformers Agent uses. In addition, ü¶úüîóLangChain
    has four [agent types](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom Agent**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating a custom agent is not too difficult in both cases:'
  prefs: []
  type: TYPE_NORMAL
- en: See the HuggingFace Transformer Agent example towards the end of [this colab](https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See the LangChain [example](https://python.langchain.com/en/latest/modules/agents/agents/custom_agent.html)
    here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚ÄúCode-execution‚Äù
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ü§óHugging Face Transformers Agent includes ‚Äúcode-execution‚Äù as one of the steps
    after the LLM selects the tools and generates the code. This restricts the Transformers
    Agent‚Äôs goal to execute Python code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ü¶úüîóLangChain includes ‚Äúcode-execution‚Äù as one of its tools, which means that
    executing code is not the last step of the whole process. This provides a lot
    more flexibility on what the task goal is: it could be executing Python code,
    or it could also be something else like doing a Google Search and returning search
    results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this blog post, we explored the functionality of ü§óHugging Face Transformers
    Agents and compared it to ü¶úüîóLangChain Agents. I look forward to witnessing further
    developments and advancements in Transformers Agent.
  prefs: []
  type: TYPE_NORMAL
- en: . . .
  prefs: []
  type: TYPE_NORMAL
- en: By [Sophia Yang](https://www.linkedin.com/in/sophiamyang/) on May 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: Sophia Yang is a Senior Data Scientist. Connect with me on [LinkedIn](https://www.linkedin.com/in/sophiamyang/),
    [Twitter](https://twitter.com/sophiamyang), and [YouTube](https://www.youtube.com/SophiaYangDS)
    and join the DS/ML [Book Club](https://dsbookclub.github.io/) ‚ù§Ô∏è
  prefs: []
  type: TYPE_NORMAL
