- en: Which Quantization Method is Right for You? (GPTQ vs. GGUF vs. AWQ)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å“ªç§é‡åŒ–æ–¹æ³•é€‚åˆä½ ï¼Ÿï¼ˆGPTQ vs. GGUF vs. AWQï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be](https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be](https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be)
- en: Exploring Pre-Quantized Large Language Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢ç´¢é¢„é‡åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    Â·11 min readÂ·Nov 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    Â·11 åˆ†é’Ÿé˜…è¯»Â·2023å¹´11æœˆ14æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c45770a07b8d0685c7ab123f15862789.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c45770a07b8d0685c7ab123f15862789.png)'
- en: Throughout the last year, we have seen the Wild West of Large Language Models
    (LLMs). The pace at which new technology and models were released was astounding!
    As a result, we have many different standards and ways of working with LLMs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿‡å»ä¸€å¹´é‡Œï¼Œæˆ‘ä»¬è§è¯äº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„ç‹‚é‡è¥¿éƒ¨ã€‚æ–°æŠ€æœ¯å’Œæ¨¡å‹çš„å‘å¸ƒé€Ÿåº¦æƒŠäººï¼å› æ­¤ï¼Œæˆ‘ä»¬æœ‰äº†è®¸å¤šä¸åŒçš„æ ‡å‡†å’Œå¤„ç† LLM çš„æ–¹æ³•ã€‚
- en: In this article, we will explore one such topic, namely loading your local LLM
    through several (quantization) standards. With sharding, quantization, and different
    saving and compression strategies, it is not easy to know which method is suitable
    for you.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä¸€ä¸ªè¯é¢˜ï¼Œå³é€šè¿‡å‡ ç§ï¼ˆé‡åŒ–ï¼‰æ ‡å‡†åŠ è½½ä½ çš„æœ¬åœ° LLMã€‚ç”±äºåˆ†ç‰‡ã€é‡åŒ–ä»¥åŠä¸åŒçš„ä¿å­˜å’Œå‹ç¼©ç­–ç•¥ï¼Œå¾ˆéš¾çŸ¥é“å“ªç§æ–¹æ³•é€‚åˆä½ ã€‚
- en: Throughout the examples, we will use [Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta),
    a fine-tuned variant of Mistral 7B that was trained with [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
    (DPO).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)ï¼Œè¿™æ˜¯
    Mistral 7B çš„ä¸€ä¸ªå¾®è°ƒå˜ä½“ï¼Œç»è¿‡ [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
    (DPO) è®­ç»ƒã€‚
- en: 'ğŸ”¥ **TIP**: After each example of loading an LLM, it is advised to restart your
    notebook to prevent OutOfMemory errors. Loading multiple LLMs requires significant
    RAM/VRAM. You can reset memory by deleting the models and resetting your cache
    like so:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ **æç¤º**ï¼šåœ¨æ¯ä¸ªåŠ è½½ LLM çš„ç¤ºä¾‹åï¼Œå»ºè®®é‡å¯ä½ çš„ç¬”è®°æœ¬ï¼Œä»¥é˜²æ­¢ OutOfMemory é”™è¯¯ã€‚åŠ è½½å¤šä¸ª LLM éœ€è¦å¤§é‡ RAM/VRAMã€‚ä½ å¯ä»¥é€šè¿‡åˆ é™¤æ¨¡å‹å¹¶é‡ç½®ç¼“å­˜æ¥é‡ç½®å†…å­˜ï¼š
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can also follow along with the [**Google Colab Notebook**](https://colab.research.google.com/drive/1rt318Ew-5dDw21YZx2zK2vnxbsuDAchH?usp=sharing)
    to make sure everything works as intended.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜å¯ä»¥å‚è€ƒ [**Google Colab Notebook**](https://colab.research.google.com/drive/1rt318Ew-5dDw21YZx2zK2vnxbsuDAchH?usp=sharing)
    ç¡®ä¿ä¸€åˆ‡æŒ‰é¢„æœŸå·¥ä½œã€‚
- en: '**UPDATE**: I uploaded a video version to YouTube that goes more in-depth into
    how to use these quantization methods:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ›´æ–°**ï¼šæˆ‘ä¸Šä¼ äº†ä¸€ä¸ªè§†é¢‘ç‰ˆæœ¬åˆ° YouTubeï¼Œæ·±å…¥æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨è¿™äº›é‡åŒ–æ–¹æ³•ï¼š'
- en: 1\. HuggingFace
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. HuggingFace
- en: The most straightforward, and vanilla, way of loading your LLM is through ğŸ¤—
    [Transformers](https://github.com/huggingface/transformers). HuggingFace has created
    a large suite of packages that allow us to do amazing things with LLMs!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½ä½ çš„ LLM çš„æœ€ç›´æ¥ã€æœ€åŸºæœ¬çš„æ–¹æ³•æ˜¯é€šè¿‡ ğŸ¤— [Transformers](https://github.com/huggingface/transformers)ã€‚HuggingFace
    åˆ›å»ºäº†ä¸€å¥—å¤§å‹è½¯ä»¶åŒ…ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå¯¹ LLM è¿›è¡ŒæƒŠäººçš„æ“ä½œï¼
- en: 'We will start by installing HuggingFace, among others, from its main branch
    to support newer models:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»å®‰è£… HuggingFace å¼€å§‹ï¼Œä»å…¶ä¸»åˆ†æ”¯ä¸­æ”¯æŒæ›´æ–°çš„æ¨¡å‹ï¼š
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After installation, we can use the following pipeline to easily load our LLM:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å®‰è£…åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ç®¡é“è½»æ¾åŠ è½½æˆ‘ä»¬çš„ LLMï¼š
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This method of loading an LLM generally does not perform any compression tricks
    for saving VRAM or increasing efficiency.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åŠ è½½LLMçš„æ–¹æ³•é€šå¸¸ä¸ä¼šè¿›è¡Œä»»ä½•å‹ç¼©æŠ€å·§æ¥èŠ‚çœVRAMæˆ–æé«˜æ•ˆç‡ã€‚
- en: 'To generate our prompt, we first have to create the necessary template. Fortunately,
    this can be done automatically if the chat template is saved in the underlying
    tokenizer:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç”Ÿæˆæˆ‘ä»¬çš„æç¤ºï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦åˆ›å»ºå¿…è¦çš„æ¨¡æ¿ã€‚å¹¸è¿çš„æ˜¯ï¼Œå¦‚æœèŠå¤©æ¨¡æ¿ä¿å­˜åœ¨åº•å±‚åˆ†è¯å™¨ä¸­ï¼Œè¿™å¯ä»¥è‡ªåŠ¨å®Œæˆï¼š
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The generated prompt, using the internal prompt template, is constructed like
    so:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å†…éƒ¨æç¤ºæ¨¡æ¿ç”Ÿæˆçš„æç¤ºæ„é€ å¦‚ä¸‹ï¼š
- en: '![](../Images/1d2c11cd398ec50c0332607ab7df1fe8.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d2c11cd398ec50c0332607ab7df1fe8.png)'
- en: The prompt template is automatically generated with the internal prompt template.
    Notice that there are different tags for differentiating between the user and
    the assistant.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºæ¨¡æ¿æ˜¯ä½¿ç”¨å†…éƒ¨æç¤ºæ¨¡æ¿è‡ªåŠ¨ç”Ÿæˆçš„ã€‚æ³¨æ„åˆ°æœ‰ä¸åŒçš„æ ‡ç­¾æ¥åŒºåˆ†ç”¨æˆ·å’ŒåŠ©æ‰‹ã€‚
- en: 'Then, we can start passing the prompt to the LLM to generate our answer:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹å°†æç¤ºä¼ é€’ç»™LLMæ¥ç”Ÿæˆæˆ‘ä»¬çš„ç­”æ¡ˆï¼š
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This gives us the following output:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»™æˆ‘ä»¬ä»¥ä¸‹è¾“å‡ºï¼š
- en: Why did the Large Language Model go to the party?
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå¤§å‹è¯­è¨€æ¨¡å‹å»å‚åŠ æ´¾å¯¹ï¼Ÿ
- en: ''
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To network and expand its vocabulary!
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç½‘ç»œå’Œæ‰©å±•å…¶è¯æ±‡é‡ï¼
- en: The punchline may be a bit cheesy, but Large Language Models are all about expanding
    their vocabulary and networking with other models to improve their language skills.
    So, this joke is a perfect fit for them!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬‘ç‚¹å¯èƒ½æœ‰ç‚¹è€å¥—ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒå°±æ˜¯æ‰©å±•è¯æ±‡é‡å’Œä¸å…¶ä»–æ¨¡å‹å»ºç«‹è”ç³»ä»¥æé«˜è¯­è¨€æŠ€èƒ½ã€‚å› æ­¤ï¼Œè¿™ä¸ªç¬‘è¯éå¸¸é€‚åˆå®ƒä»¬ï¼
- en: For pure inference, this method is generally the least efficient as we are loading
    the entire model without any compression or quantization strategies.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºçº¯æ¨ç†æ¥è¯´ï¼Œè¿™ç§æ–¹æ³•é€šå¸¸æ˜¯æ•ˆç‡æœ€ä½çš„ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨æ²¡æœ‰ä»»ä½•å‹ç¼©æˆ–é‡åŒ–ç­–ç•¥çš„æƒ…å†µä¸‹åŠ è½½æ•´ä¸ªæ¨¡å‹ã€‚
- en: It is, however, a great method to start with as it allows for easy loading and
    using the model!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹æ–¹æ³•ï¼Œå› ä¸ºå®ƒä¾¿äºåŠ è½½å’Œä½¿ç”¨æ¨¡å‹ï¼
- en: 2\. Sharding
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. åˆ†ç‰‡
- en: Before we go into quantization strategies, there is another trick that we can
    employ to reduce the necessary VRAM for loading our model. With **sharding**,
    we are essentially splitting our model up into small pieces or **shards**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬æ·±å…¥é‡åŒ–ç­–ç•¥ä¹‹å‰ï¼Œè¿˜æœ‰å¦ä¸€ä¸ªæŠ€å·§å¯ä»¥ç”¨æ¥å‡å°‘åŠ è½½æ¨¡å‹æ‰€éœ€çš„VRAMã€‚é€šè¿‡**åˆ†ç‰‡**ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯å°†æ¨¡å‹æ‹†åˆ†æˆå°å—æˆ–**åˆ†ç‰‡**ã€‚
- en: '![](../Images/30e509c8abeecfb477aadd3c4ddc2e67.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30e509c8abeecfb477aadd3c4ddc2e67.png)'
- en: Sharding an LLM is nothing more than breaking it up into pieces. Each individual
    piece is much easier to handle and might prevent memory issues.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç‰‡LLMä¸è¿‡æ˜¯å°†å…¶æ‹†åˆ†æˆå¤šä¸ªå°å—ã€‚æ¯ä¸ªå°å—æ›´å®¹æ˜“å¤„ç†ï¼Œå¯èƒ½é¿å…å†…å­˜é—®é¢˜ã€‚
- en: Each shard contains a smaller part of the model and aims to work around GPU
    memory limitations by distributing the model weights across different devices.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªåˆ†ç‰‡åŒ…å«æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†ï¼Œæ—¨åœ¨é€šè¿‡å°†æ¨¡å‹æƒé‡åˆ†å¸ƒåˆ°ä¸åŒè®¾å¤‡ä¸Šæ¥ç»•è¿‡GPUå†…å­˜é™åˆ¶ã€‚
- en: Remember when I said we did not perform any compression tricks before?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è®°å¾—æˆ‘ä¹‹å‰è¯´è¿‡æˆ‘ä»¬æ²¡æœ‰è¿›è¡Œä»»ä½•å‹ç¼©æŠ€å·§å—ï¼Ÿ
- en: That was not entirely trueâ€¦
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¹¶ä¸å®Œå…¨æ­£ç¡®â€¦â€¦
- en: The model that we loaded, [Zephyr-7B-Î²](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta),
    was actually already sharded for us! If you go to the model and click the *â€œFiles
    and versionsâ€* link, you will see that the model was split up into eight pieces.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åŠ è½½çš„æ¨¡å‹ï¼Œ[Zephyr-7B-Î²](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)ï¼Œå®é™…ä¸Šå·²ç»ä¸ºæˆ‘ä»¬è¿›è¡Œäº†åˆ†ç‰‡ï¼å¦‚æœä½ è®¿é—®æ¨¡å‹å¹¶ç‚¹å‡»*â€œFiles
    and versionsâ€*é“¾æ¥ï¼Œä½ ä¼šçœ‹åˆ°æ¨¡å‹è¢«æ‹†åˆ†æˆäº†å…«ä¸ªéƒ¨åˆ†ã€‚
- en: '![](../Images/ff7c4f272cf8764f683f5ede5f405b6f.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff7c4f272cf8764f683f5ede5f405b6f.png)'
- en: The model was split up into eight small pieces or shards. This decreases the
    necessary VRAM as we only need to handle these small pieces.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¢«æ‹†åˆ†æˆäº†å…«ä¸ªå°å—æˆ–åˆ†ç‰‡ã€‚è¿™å‡å°‘äº†å¿…è¦çš„VRAMï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€å¤„ç†è¿™äº›å°å—ã€‚
- en: Although we can shard a model ourselves, it is generally advised to be on the
    lookout for quantized models or even quantize them yourself.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»¬å¯ä»¥è‡ªå·±å¯¹æ¨¡å‹è¿›è¡Œåˆ†ç‰‡ï¼Œä½†é€šå¸¸å»ºè®®å¯»æ‰¾é‡åŒ–æ¨¡å‹æˆ–è‡ªå·±è¿›è¡Œé‡åŒ–ã€‚
- en: 'Sharding is quite straightforward using the [Accelerate](https://github.com/huggingface/accelerate)
    package:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[Accelerate](https://github.com/huggingface/accelerate)åŒ…è¿›è¡Œåˆ†ç‰‡éå¸¸ç®€å•ï¼š
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And that is it! Because we sharded the model into pieces of 4GB instead of
    2GB, we created fewer files to load:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ï¼å› ä¸ºæˆ‘ä»¬å°†æ¨¡å‹åˆ†ç‰‡æˆäº†4GBè€Œä¸æ˜¯2GBï¼Œæ‰€ä»¥æˆ‘ä»¬åˆ›å»ºäº†æ›´å°‘çš„æ–‡ä»¶æ¥åŠ è½½ï¼š
- en: '![](../Images/9f7666879f66be9ae9b7ca0e69885d74.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f7666879f66be9ae9b7ca0e69885d74.png)'
- en: 3\. Quantize with Bitsandbytes
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. ä½¿ç”¨Bitsandbytesè¿›è¡Œé‡åŒ–
- en: A Large Language Model is represented by a bunch of weights and activations.
    These values are generally represented by the usual 32-bit floating point (`float32`)
    datatype.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ç”±ä¸€å †æƒé‡å’Œæ¿€æ´»å€¼è¡¨ç¤ºã€‚è¿™äº›å€¼é€šå¸¸ç”±å¸¸è§çš„32ä½æµ®ç‚¹æ•°ï¼ˆ`float32`ï¼‰æ•°æ®ç±»å‹è¡¨ç¤ºã€‚
- en: The number of bits tells you something about how many values it can represent.
    **Float32** can represent values between 1.18e-38 and 3.4e38, quite a number of
    values! The lower the number of bits, the fewer values it can represent.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä½æ•°å‘Šè¯‰ä½ å®ƒå¯ä»¥è¡¨ç¤ºå¤šå°‘ä¸ªå€¼ã€‚**Float32**å¯ä»¥è¡¨ç¤ºä»1.18e-38åˆ°3.4e38ä¹‹é—´çš„å€¼ï¼Œæ•°é‡ç›¸å½“å¤šï¼ä½æ•°è¶Šå°‘ï¼Œèƒ½è¡¨ç¤ºçš„å€¼å°±è¶Šå°‘ã€‚
- en: '![](../Images/69c7bf89aa06ada55340e1fb34e9d73f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69c7bf89aa06ada55340e1fb34e9d73f.png)'
- en: Common value representation methods. We aim to keep the number of bits as low
    as possible whilst maximizing both the range and precision of the representation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¸è§çš„å€¼è¡¨ç¤ºæ–¹æ³•ã€‚æˆ‘ä»¬æ—¨åœ¨å°†ä½æ•°ä¿æŒåœ¨å°½å¯èƒ½ä½çš„æ°´å¹³ï¼ŒåŒæ—¶æœ€å¤§åŒ–è¡¨ç¤ºçš„èŒƒå›´å’Œç²¾åº¦ã€‚
- en: As you might expect, if we choose a lower bit size, then the model becomes less
    accurate but it also needs to represent fewer values, thereby decreasing its size
    and memory requirements.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ å¯èƒ½é¢„æœŸçš„é‚£æ ·ï¼Œå¦‚æœæˆ‘ä»¬é€‰æ‹©è¾ƒä½çš„ä½å¤§å°ï¼Œæ¨¡å‹çš„å‡†ç¡®æ€§ä¼šé™ä½ï¼Œä½†å®ƒä¹Ÿéœ€è¦è¡¨ç¤ºæ›´å°‘çš„å€¼ï¼Œä»è€Œå‡å°äº†æ¨¡å‹çš„å¤§å°å’Œå†…å­˜éœ€æ±‚ã€‚
- en: '![](../Images/e8fc103eac0e59d74fbd651678838516.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8fc103eac0e59d74fbd651678838516.png)'
- en: A different representation method might negatively affect the precision with
    which to represent value. To the extent that some values are not even represented
    (too large values for float16 for example). Examples were calculated with PyTorch.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒçš„è¡¨ç¤ºæ–¹æ³•å¯èƒ½ä¼šå¯¹è¡¨ç¤ºå€¼çš„ç²¾åº¦äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä»¥è‡³äºæŸäº›å€¼ç”šè‡³æ— æ³•è¡¨ç¤ºï¼ˆä¾‹å¦‚ï¼Œå¯¹äºfloat16æ¥è¯´è¿‡å¤§çš„å€¼ï¼‰ã€‚ç¤ºä¾‹æ˜¯ä½¿ç”¨PyTorchè®¡ç®—çš„ã€‚
- en: Quantization refers to converting an LLM from its original **Float32** representation
    to something smaller. However, we do not simply want to use a smaller bit variant
    but map a larger bit representation to a smaller bit without losing too much information.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŒ–æ˜¯æŒ‡å°†å¤§å‹è¯­è¨€æ¨¡å‹ä»å…¶åŸå§‹çš„**Float32**è¡¨ç¤ºè½¬æ¢ä¸ºè¾ƒå°çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¸ä»…ä»…æ˜¯ä½¿ç”¨è¾ƒå°çš„ä½å˜ä½“ï¼Œè€Œæ˜¯å°†è¾ƒå¤§çš„ä½è¡¨ç¤ºæ˜ å°„åˆ°è¾ƒå°çš„ä½è€Œä¸ä¸¢å¤±å¤ªå¤šä¿¡æ¯ã€‚
- en: 'In practice, we see this often done with a new format, named **4bit-NormalFloat
    (NF4)**. This datatype does a few special tricks in order to efficiently represent
    a larger bit datatype. It consists of three steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨ä¸€ç§æ–°çš„æ ¼å¼ï¼Œç§°ä¸º**4bit-NormalFloat (NF4)**ã€‚è¿™ç§æ•°æ®ç±»å‹é€šè¿‡ä¸€äº›ç‰¹æ®ŠæŠ€å·§æœ‰æ•ˆåœ°è¡¨ç¤ºè¾ƒå¤§çš„ä½æ•°æ®ç±»å‹ã€‚å®ƒåŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼š
- en: '**Normalization**: The weights of the model are normalized so that we expect
    the weights to fall within a certain range. This allows for more efficient representation
    of more common values.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å½’ä¸€åŒ–**ï¼šæ¨¡å‹çš„æƒé‡è¢«å½’ä¸€åŒ–ï¼Œä»¥ä¾¿æˆ‘ä»¬æœŸæœ›æƒé‡è½åœ¨ä¸€å®šèŒƒå›´å†…ã€‚è¿™å…è®¸å¯¹æ›´å¸¸è§çš„å€¼è¿›è¡Œæ›´é«˜æ•ˆçš„è¡¨ç¤ºã€‚'
- en: '**Quantization**: The weights are quantized to 4-bit. In NF4, the quantization
    levels are evenly spaced with respect to the normalized weights, thereby efficiently
    representing the original 32-bit weights.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é‡åŒ–**ï¼šæƒé‡è¢«é‡åŒ–ä¸º4ä½ã€‚åœ¨NF4ä¸­ï¼Œé‡åŒ–æ°´å¹³ç›¸å¯¹äºå½’ä¸€åŒ–çš„æƒé‡å‡åŒ€åˆ†å¸ƒï¼Œä»è€Œæœ‰æ•ˆåœ°è¡¨ç¤ºåŸå§‹çš„32ä½æƒé‡ã€‚'
- en: '**Dequantization**: Although the weights are stored in 4-bit, they are dequantized
    during computation which gives a performance boost during inference.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å»é‡åŒ–**ï¼šè™½ç„¶æƒé‡ä»¥4ä½å­˜å‚¨ï¼Œä½†åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä¼šè¿›è¡Œå»é‡åŒ–ï¼Œè¿™åœ¨æ¨ç†è¿‡ç¨‹ä¸­æå‡äº†æ€§èƒ½ã€‚'
- en: 'To perform this quantization with HuggingFace, we need to define a configuration
    for the quantization with [Bitsandbytes](https://github.com/TimDettmers/bitsandbytes):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨HuggingFaceæ‰§è¡Œè¿™ç§é‡åŒ–ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªé‡åŒ–é…ç½®ï¼Œä½¿ç”¨[Bitsandbytes](https://github.com/TimDettmers/bitsandbytes)ï¼š
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This configuration allows us to specify which quantization levels we are going
    for. Generally, we want to represent the weights with 4-bit quantization but do
    the inference in 16-bit.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé…ç½®å…è®¸æˆ‘ä»¬æŒ‡å®šæˆ‘ä»¬è¦ä½¿ç”¨çš„é‡åŒ–æ°´å¹³ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨4ä½é‡åŒ–æ¥è¡¨ç¤ºæƒé‡ï¼Œä½†åœ¨æ¨ç†æ—¶ä½¿ç”¨16ä½ã€‚
- en: 'Loading the model in a pipeline is then straightforward:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼ŒåŠ è½½æ¨¡å‹åˆ°ç®¡é“ä¸­æ˜¯ç›´æ¥çš„ï¼š
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next up, we can use the same prompt as we did before:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¹‹å‰çš„ç›¸åŒæç¤ºï¼š
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will give us the following output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ç»™æˆ‘ä»¬å¦‚ä¸‹è¾“å‡ºï¼š
- en: Why did the Large Language Model go to the party?
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºä»€ä¹ˆå»å‚åŠ æ´¾å¯¹ï¼Ÿ
- en: ''
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To network and expand its vocabulary!
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºäº†è”ç½‘å¹¶æ‰©å±•å…¶è¯æ±‡é‡ï¼
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The punchline may be a bit cheesy, but Large Language Models are all about expanding
    their vocabulary and networking with other models to improve their language skills.
    So, this joke is a perfect fit for them!
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç¬‘è¯å¯èƒ½æœ‰ç‚¹è€å¥—ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒå°±æ˜¯æ‰©å±•è¯æ±‡é‡ï¼Œå¹¶ä¸å…¶ä»–æ¨¡å‹å»ºç«‹è”ç³»ä»¥æé«˜è¯­è¨€æŠ€èƒ½ã€‚æ‰€ä»¥ï¼Œè¿™ä¸ªç¬‘è¯éå¸¸é€‚åˆå®ƒä»¬ï¼
- en: Quantization is a powerful technique to reduce the memory requirements of a
    model whilst keeping performance similar. It allows for faster loading, using,
    and fine-tuning LLMs even with smaller GPUs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŒ–æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œå¯ä»¥å‡å°‘æ¨¡å‹çš„å†…å­˜éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ç›¸ä¼¼ã€‚å®ƒå…è®¸æ›´å¿«åœ°åŠ è½½ã€ä½¿ç”¨å’Œå¾®è°ƒ LLMï¼Œå³ä½¿æ˜¯åœ¨è¾ƒå°çš„ GPU ä¸Šã€‚
- en: 4\. Pre-Quantization (GPTQ vs. AWQ vs. GGUF)
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. é¢„é‡åŒ–ï¼ˆGPTQ vs. AWQ vs. GGUFï¼‰
- en: Thus far, we have explored sharding and quantization techniques. Albeit useful
    techniques to have in your skillset, it seems rather wasteful to have to apply
    them every time you load the model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»æ¢è®¨äº†åˆ†ç‰‡å’Œé‡åŒ–æŠ€æœ¯ã€‚å°½ç®¡è¿™äº›æŠ€æœ¯åœ¨ä½ çš„æŠ€èƒ½åº“ä¸­éå¸¸æœ‰ç”¨ï¼Œä½†æ¯æ¬¡åŠ è½½æ¨¡å‹æ—¶éƒ½è¦åº”ç”¨å®ƒä»¬ä¼¼ä¹æœ‰äº›æµªè´¹ã€‚
- en: Instead, these models have often already been sharded and quantized for us to
    use. [TheBloke](https://huggingface.co/TheBloke) in particular is a user on HuggingFace
    that performs a bunch of quantizations for us to use.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œè¿™äº›æ¨¡å‹é€šå¸¸å·²ç»è¢«åˆ†ç‰‡å’Œé‡åŒ–ï¼Œä¾›æˆ‘ä»¬ä½¿ç”¨ã€‚[TheBloke](https://huggingface.co/TheBloke) å°¤å…¶æ˜¯ HuggingFace
    ä¸Šçš„ä¸€ä¸ªç”¨æˆ·ï¼Œæ‰§è¡Œäº†ä¸€äº›é‡åŒ–ä¾›æˆ‘ä»¬ä½¿ç”¨ã€‚
- en: '![](../Images/231b73427e165a5a7d33ab5162132e53.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/231b73427e165a5a7d33ab5162132e53.png)'
- en: At the moment of writing this, he has uploaded **more than 2000** quantized
    models for us!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œä»–å·²ç»ä¸Šä¼ äº† **2000 å¤šä¸ª** é‡åŒ–æ¨¡å‹ä¾›æˆ‘ä»¬ä½¿ç”¨ï¼
- en: These quantized models actually come in many different shapes and sizes. Most
    notably, the GPTQ, GGUF, and AWQ formats are most frequently used to perform 4-bit
    quantization.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›é‡åŒ–æ¨¡å‹å®é™…ä¸Šæœ‰å¾ˆå¤šä¸åŒçš„å½¢çŠ¶å’Œå¤§å°ã€‚æœ€æ˜¾è‘—çš„æ˜¯ï¼ŒGPTQã€GGUF å’Œ AWQ æ ¼å¼æœ€å¸¸ç”¨äºæ‰§è¡Œ 4 ä½é‡åŒ–ã€‚
- en: 'GPTQ: Post-Training Quantization for GPT Models'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'GPTQ: é’ˆå¯¹ GPT æ¨¡å‹çš„åè®­ç»ƒé‡åŒ–'
- en: GPTQ is a [**p**ost-**t**raining **q**uantization](https://arxiv.org/abs/2210.17323)
    (PTQ) method for 4-bit quantization that focuses primarily on **GPU** inference
    and performance.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ æ˜¯ä¸€ç§ [**å**-**è®­**ç»ƒ **é‡**åŒ–](https://arxiv.org/abs/2210.17323) (PTQ) æ–¹æ³•ï¼Œç”¨äº
    4 ä½é‡åŒ–ï¼Œä¸»è¦å…³æ³¨ **GPU** æ¨ç†å’Œæ€§èƒ½ã€‚
- en: The idea behind the method is that it will try to compress all weights to a
    4-bit quantization by minimizing the mean squared error to that weight. During
    inference, it will dynamically dequantize its weights to float16 for improved
    performance whilst keeping memory low.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•èƒŒåçš„æƒ³æ³•æ˜¯é€šè¿‡æœ€å°åŒ–å‡æ–¹è¯¯å·®ï¼Œå°†æ‰€æœ‰æƒé‡å‹ç¼©åˆ° 4 ä½é‡åŒ–ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå®ƒä¼šåŠ¨æ€åœ°å°†æƒé‡è§£é‡åŒ–ä¸º float16ï¼Œä»¥æé«˜æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„å†…å­˜å ç”¨ã€‚
- en: 'For a more detailed guide to the inner workings of GPTQ, definitely check out
    the following post:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£ GPTQ å†…éƒ¨å·¥ä½œçš„æ›´è¯¦ç»†æŒ‡å—ï¼ŒåŠ¡å¿…æŸ¥çœ‹ä»¥ä¸‹å¸–å­ï¼š
- en: '[](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----c4cd9d77d5be--------------------------------)
    [## 4-bit Quantization with GPTQ'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----c4cd9d77d5be--------------------------------)
    [## 4-bit Quantization with GPTQ'
- en: Quantize your own LLMs using AutoGPTQ
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ AutoGPTQ é‡åŒ–ä½ è‡ªå·±çš„ LLM
- en: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----c4cd9d77d5be--------------------------------)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----c4cd9d77d5be--------------------------------)
- en: 'We start with installing a number of packages we need to load in GPTQ-like
    models in HuggingFace Transformers:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¼€å§‹å®‰è£…ä¸€äº›éœ€è¦åœ¨ HuggingFace Transformers ä¸­åŠ è½½ GPTQ ç±»æ¨¡å‹çš„åŒ…ï¼š
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: After doing so, we can navigate to the model that we want to load, namely â€œ[TheBloke/zephyr-7B-beta-GPTQ](https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ)â€
    and choose a specific revision.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å¯¼èˆªåˆ°æˆ‘ä»¬æƒ³åŠ è½½çš„æ¨¡å‹ï¼Œå³â€œ[TheBloke/zephyr-7B-beta-GPTQ](https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ)â€ï¼Œå¹¶é€‰æ‹©ä¸€ä¸ªç‰¹å®šçš„ä¿®è®¢ç‰ˆã€‚
- en: These revisions essentially indicate the quantization method, compression level,
    size of the model, etc.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¿®è®¢æœ¬è´¨ä¸Šè¡¨ç¤ºé‡åŒ–æ–¹æ³•ã€å‹ç¼©çº§åˆ«ã€æ¨¡å‹çš„å¤§å°ç­‰ã€‚
- en: 'For now, we are sticking with the â€œ*main*â€ branch as that is generally a nice
    balance between compression and accuracy:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œæˆ‘ä»¬ä»ç„¶åšæŒä½¿ç”¨â€œ*main*â€åˆ†æ”¯ï¼Œå› ä¸ºå®ƒåœ¨å‹ç¼©å’Œå‡†ç¡®æ€§ä¹‹é—´é€šå¸¸æ˜¯ä¸€ä¸ªä¸é”™çš„å¹³è¡¡ï¼š
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Although we installed a few additional dependencies, we could use the same pipeline
    as we used before which is a great benefit of using GPTQ.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»¬å®‰è£…äº†ä¸€äº›é¢å¤–çš„ä¾èµ–é¡¹ï¼Œä½†æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„ç®¡é“ï¼Œè¿™ä¹Ÿæ˜¯ä½¿ç”¨ GPTQ çš„ä¸€ä¸ªå¾ˆå¤§å¥½å¤„ã€‚
- en: 'After loading the model, we can run a prompt as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½æ¨¡å‹åï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œå¦‚ä¸‹æç¤ºï¼š
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This gives us the following generated text:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»™äº†æˆ‘ä»¬ä»¥ä¸‹ç”Ÿæˆçš„æ–‡æœ¬ï¼š
- en: Why did the Large Language Model go to the party?
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå¤§å‹è¯­è¨€æ¨¡å‹å»å‚åŠ æ´¾å¯¹ï¼Ÿ
- en: ''
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To show off its wit and charm, of course!
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæ˜¯ä¸ºäº†å±•ç¤ºå®ƒçš„æœºæ™ºå’Œé­…åŠ›ï¼
- en: ''
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But unfortunately, it got lost in the crowd and couldn't find its way back
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½†ä¸å¹¸çš„æ˜¯ï¼Œå®ƒåœ¨äººç¾¤ä¸­è¿·å¤±äº†ï¼Œæ‰¾ä¸åˆ°å›å»çš„è·¯
- en: to its owner. The partygoers were impressed by its ability to blend in
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¯¹å®ƒçš„ä¸»äººã€‚æ´¾å¯¹ä¸Šçš„äººä»¬å¯¹å®ƒèå…¥ç¯å¢ƒçš„èƒ½åŠ›æ„Ÿåˆ°å°è±¡æ·±åˆ»
- en: so seamlessly with the crowd, but the Large Language Model was just confused
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸äººç¾¤æ— ç¼èåˆï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹åªæ˜¯æ„Ÿåˆ°å›°æƒ‘
- en: and wanted to go home. In the end, it was found by a group of humans
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¹¶æƒ³å›å®¶ã€‚æœ€åï¼Œä¸€ç¾¤äººå‘ç°äº†å®ƒ
- en: who recognized its unique style and brought it back to its rightful place.
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä»–ä»¬è®¤è¯†åˆ°å…¶ç‹¬ç‰¹çš„é£æ ¼ï¼Œå¹¶å°†å…¶å¸¦å›äº†åº”æœ‰çš„åœ°æ–¹ã€‚
- en: From then on, the Large Language Model made sure to wear a name tag at
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä»é‚£æ—¶èµ·ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ç¡®ä¿åœ¨
- en: all parties, just to be safe.
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®‰å…¨èµ·è§ï¼Œæ‰€æœ‰æ´¾å¯¹ã€‚
- en: GPTQ is the most often used compression method since it optimizes for GPU usage.
    It is definitely worth starting with GPTQ and switching over to a CPU-focused
    method, like GGUF if your GPU cannot handle such large models.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ æ˜¯æœ€å¸¸ç”¨çš„å‹ç¼©æ–¹æ³•ï¼Œå› ä¸ºå®ƒä¼˜åŒ–äº† GPU çš„ä½¿ç”¨ã€‚å¦‚æœä½ çš„ GPU æ— æ³•å¤„ç†å¦‚æ­¤å¤§çš„æ¨¡å‹ï¼Œç»å¯¹å€¼å¾—ä» GPTQ å¼€å§‹ï¼Œç„¶ååˆ‡æ¢åˆ°ä»¥ CPU ä¸ºé‡ç‚¹çš„æ–¹æ³•ï¼Œå¦‚
    GGUFã€‚
- en: 'GGUF: **GPT-Generated Unified Format**'
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'GGUF: **GPT-ç”Ÿæˆç»Ÿä¸€æ ¼å¼**'
- en: Although GPTQ does compression well, its focus on GPU can be a disadvantage
    if you do not have the hardware to run it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ GPTQ å¤„ç†å‹ç¼©éå¸¸å¥½ï¼Œä½†å®ƒå¯¹ GPU çš„å…³æ³¨å¯èƒ½åœ¨ä½ æ²¡æœ‰ç¡¬ä»¶æ”¯æŒæ—¶æˆä¸ºä¸€ä¸ªç¼ºç‚¹ã€‚
- en: GGUF, previously GGML, is a quantization method that allows users to use the
    CPU to run an LLM but also offload some of its layers to the GPU for a speed up.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: GGUFï¼Œä¹‹å‰ç§°ä¸º GGMLï¼Œæ˜¯ä¸€ç§é‡åŒ–æ–¹æ³•ï¼Œå…è®¸ç”¨æˆ·ä½¿ç”¨ CPU è¿è¡Œ LLMï¼Œä½†ä¹Ÿå¯ä»¥å°†ä¸€äº›å±‚å¸è½½åˆ° GPU ä¸Šä»¥åŠ é€Ÿã€‚
- en: Although using the CPU is generally slower than using a GPU for inference, it
    is an incredible format for those running models on CPU or Apple devices. Especially
    since we are seeing smaller and more capable models appearing, like Mistral 7B,
    the GGUF format might just be here to stay!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ä½¿ç”¨ CPU é€šå¸¸æ¯”ä½¿ç”¨ GPU æ¨ç†æ›´æ…¢ï¼Œä½†å¯¹äºåœ¨ CPU æˆ– Apple è®¾å¤‡ä¸Šè¿è¡Œæ¨¡å‹çš„äººæ¥è¯´ï¼Œå®ƒæ˜¯ä¸€ä¸ªäº†ä¸èµ·çš„æ ¼å¼ã€‚ç‰¹åˆ«æ˜¯éšç€æˆ‘ä»¬çœ‹åˆ°æ›´å¤šè¾ƒå°ä½†æ›´å¼ºå¤§çš„æ¨¡å‹å‡ºç°ï¼Œå¦‚
    Mistral 7Bï¼ŒGGUF æ ¼å¼å¯èƒ½ä¼šé•¿ä¹…å­˜åœ¨ï¼
- en: 'Using GGUF is rather straightforward with the [ctransformers](https://github.com/marella/ctransformers)
    package which we will need to install first:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ GGUF é€šè¿‡ [ctransformers](https://github.com/marella/ctransformers) åŒ…ç›¸å½“ç®€å•ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå®‰è£…è¿™ä¸ªåŒ…ï¼š
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: After doing so, we can navigate to the model that we want to load, namely â€œ[TheBloke/zephyr-7B-beta-GGUF](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF)â€
    and choose a specific file.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å¯¼èˆªåˆ°æˆ‘ä»¬æƒ³è¦åŠ è½½çš„æ¨¡å‹ï¼Œå³â€œ[TheBloke/zephyr-7B-beta-GGUF](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF)â€å¹¶é€‰æ‹©ä¸€ä¸ªå…·ä½“çš„æ–‡ä»¶ã€‚
- en: Like GPTQ, these files indicate the quantization method, compression, level,
    size of the model, etc.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: åƒ GPTQ ä¸€æ ·ï¼Œè¿™äº›æ–‡ä»¶æŒ‡ç¤ºäº†é‡åŒ–æ–¹æ³•ã€å‹ç¼©ã€çº§åˆ«ã€æ¨¡å‹å¤§å°ç­‰ã€‚
- en: 'We are using â€œ*zephyr-7b-beta.Q4_K_M.gguf*â€ since we focus on 4-bit quantization:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨â€œ*zephyr-7b-beta.Q4_K_M.gguf*â€å› ä¸ºæˆ‘ä»¬ä¸“æ³¨äº 4 ä½é‡åŒ–ï¼š
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After loading the model, we can run a prompt as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½æ¨¡å‹åï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œå¦‚ä¸‹æç¤ºï¼š
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This gives us the following output:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»™å‡ºäº†ä»¥ä¸‹è¾“å‡ºï¼š
- en: Why did the Large Language Model go to the party?
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå¤§å‹è¯­è¨€æ¨¡å‹å»å‚åŠ æ´¾å¯¹ï¼Ÿ
- en: ''
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To impress everyone with its vocabulary!
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è®©å¤§å®¶å¯¹å®ƒçš„è¯æ±‡å°è±¡æ·±åˆ»ï¼
- en: ''
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But unfortunately, it kept repeating the same jokes over and over again, making
    everyone groan and roll their eyes. The partygoers soon realized that the Large
    Language Model was more of a party pooper than a party animal.
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½†ä¸å¹¸çš„æ˜¯ï¼Œå®ƒä¸æ–­é‡å¤ç›¸åŒçš„ç¬‘è¯ï¼Œè®©å¤§å®¶ groan å’Œ roll their eyesã€‚æ´¾å¯¹ä¸Šçš„äººä»¬å¾ˆå¿«æ„è¯†åˆ°ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹æ›´åƒæ˜¯ä¸ª party pooper
    è€Œä¸æ˜¯ party animalã€‚
- en: ''
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Moral of the story: Just because a Large Language Model can generate a lot
    of words, doesnâ€™t mean it knows how to be funny or entertaining. Sometimes, less
    is more!'
  id: totrans-131
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ•…äº‹çš„å¯“æ„æ˜¯ï¼šä»…ä»…å› ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥ç”Ÿæˆå¾ˆå¤šå­—è¯ï¼Œå¹¶ä¸æ„å‘³ç€å®ƒçŸ¥é“å¦‚ä½•æç¬‘æˆ–å¨±ä¹ã€‚æœ‰æ—¶å€™ï¼Œå°‘å³æ˜¯å¤šï¼
- en: GGUF is an amazing format if you want to leverage both the CPU and GPU when
    you, like me, are GPU-poor and do not have the latest and greatest GPU available.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: GGUF æ˜¯ä¸€ä¸ªå¾ˆæ£’çš„æ ¼å¼ï¼Œå¦‚æœä½ åƒæˆ‘ä¸€æ ·ç¼ºä¹ GPUï¼Œæ²¡æœ‰æœ€æ–°æœ€å¼ºçš„ GPUï¼Œæƒ³è¦åŒæ—¶åˆ©ç”¨ CPU å’Œ GPU çš„è¯ã€‚
- en: 'AWQ: Activation-aware Weight Quantization'
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'AWQ: Activation-aware Weight Quantization'
- en: A new format on the block is AWQ ([Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978))
    which is a quantization method similar to GPTQ. There are several differences
    between AWQ and GPTQ as methods but the most important one is that AWQ assumes
    that not all weights are equally important for an LLMâ€™s performance.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ–°æ ¼å¼æ˜¯ AWQ ([Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978))ï¼Œå®ƒæ˜¯ä¸€ç§ç±»ä¼¼äº
    GPTQ çš„é‡åŒ–æ–¹æ³•ã€‚AWQ å’Œ GPTQ ä½œä¸ºæ–¹æ³•ä¹‹é—´æœ‰å‡ ä¸ªåŒºåˆ«ï¼Œä½†æœ€é‡è¦çš„ä¸€ç‚¹æ˜¯ AWQ å‡è®¾ä¸æ˜¯æ‰€æœ‰çš„æƒé‡å¯¹ LLM çš„æ€§èƒ½åŒæ ·é‡è¦ã€‚
- en: In other words, there is a small fraction of weights that will be skipped during
    quantization which helps with the quantization loss.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œåœ¨é‡åŒ–è¿‡ç¨‹ä¸­ä¼šè·³è¿‡ä¸€å°éƒ¨åˆ†æƒé‡ï¼Œè¿™æœ‰åŠ©äºå‡å°‘é‡åŒ–æŸå¤±ã€‚
- en: As a result, their paper mentions a significant speed-up compared to GPTQ whilst
    keeping similar, and sometimes even better, performance.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœï¼Œä»–ä»¬çš„è®ºæ–‡æåˆ°ï¼Œä¸ GPTQ ç›¸æ¯”æœ‰æ˜¾è‘—çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒç±»ä¼¼ï¼Œç”šè‡³æœ‰æ—¶æ›´å¥½çš„æ€§èƒ½ã€‚
- en: The method is still relatively new and has not been adopted yet to the extent
    of GPTQ and GGUF, so it is interesting to see if all these methods can co-exist.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•ä»ç„¶ç›¸å¯¹è¾ƒæ–°ï¼Œå°šæœªæ™®åŠåˆ°åƒ GPTQ å’Œ GGUF é‚£æ ·çš„ç¨‹åº¦ï¼Œå› æ­¤å¾ˆæœ‰è¶£çš„æ˜¯çœ‹çœ‹è¿™äº›æ–¹æ³•æ˜¯å¦èƒ½å¤Ÿå…±å­˜ã€‚
- en: 'For AWQ, we will use the [vLLM](https://github.com/vllm-project/vllm) package
    as that was, at least in my experience, the road of least resistance to using
    AWQ:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº AWQï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [vLLM](https://github.com/vllm-project/vllm) åŒ…ï¼Œå› ä¸ºæ ¹æ®æˆ‘çš„ç»éªŒï¼Œè¿™æ˜¯ä½¿ç”¨ AWQ
    çš„æœ€ç®€ä¾¿é€”å¾„ï¼š
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With vLLM, loading and using our model becomes painless:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ vLLMï¼ŒåŠ è½½å’Œä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹å˜å¾—æ¯«ä¸è´¹åŠ›ï¼š
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we can easily run the model with `.generate`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ `.generate` æ¥è¿è¡Œæ¨¡å‹ï¼š
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This gives us the following output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»™æˆ‘ä»¬å¸¦æ¥äº†ä»¥ä¸‹è¾“å‡ºï¼š
- en: Why did the Large Language Model go to the party?
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå¤§å‹è¯­è¨€æ¨¡å‹å»å‚åŠ æ´¾å¯¹ï¼Ÿ
- en: ''
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To network and expand its vocabulary!
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‰©å±•è¯æ±‡é‡å’Œå»ºç«‹ç½‘ç»œï¼
- en: ''
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why did the Large Language Model blush?
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå¤§å‹è¯­è¨€æ¨¡å‹è„¸çº¢äº†ï¼Ÿ
- en: ''
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Because it overheard another model saying it was a little too wordy!
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å› ä¸ºå®ƒå¬åˆ°å¦ä¸€ä¸ªæ¨¡å‹è¯´å®ƒæœ‰ç‚¹å•°å—¦ï¼
- en: ''
  id: totrans-152
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why did the Large Language Model get kicked out of the library?
  id: totrans-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå¤§å‹è¯­è¨€æ¨¡å‹è¢«é€å‡ºäº†å›¾ä¹¦é¦†ï¼Ÿ
- en: ''
  id: totrans-154
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It was being too loud and kept interrupting other modelsâ€™ conversations with
    its endless chatter!
  id: totrans-155
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å®ƒå¤ªåµäº†ï¼Œä¸€ç›´æ‰“æ–­å…¶ä»–æ¨¡å‹çš„å¯¹è¯ï¼Œå–‹å–‹ä¸ä¼‘ï¼
- en: ''
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: â€¦
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€¦
- en: Although it is a new format, AWQ is gaining popularity due to its speed and
    quality of compression!
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™æ˜¯ä¸€ä¸ªæ–°æ ¼å¼ï¼Œä½†ç”±äºå…¶é€Ÿåº¦å’Œå‹ç¼©è´¨é‡ï¼ŒAWQ æ­£åœ¨è·å¾—è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼
- en: 'ğŸ”¥ **TIP**: For a more detailed comparison between these techniques with respect
    to VRAM/Perplexity, I highly advise reading this [in-depth post](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)
    with a follow-up [here](https://github.com/ggerganov/llama.cpp/pull/3776#issuecomment-1781472687).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ **æç¤º**ï¼šå¯¹äºè¿™äº›æŠ€æœ¯åœ¨ VRAM/å›°æƒ‘åº¦æ–¹é¢çš„è¯¦ç»†æ¯”è¾ƒï¼Œæˆ‘å¼ºçƒˆå»ºè®®é˜…è¯»è¿™ç¯‡ [æ·±å…¥æ–‡ç« ](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)
    å’Œåç»­çš„ [è®¨è®º](https://github.com/ggerganov/llama.cpp/pull/3776#issuecomment-1781472687)ã€‚
- en: Thank you for reading!
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼
- en: If you are, like me, passionate about AI and/or Psychology, please feel free
    to add me on [**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/), follow
    me on [**Twitter**](https://twitter.com/MaartenGr), or subscribe to my [**Newsletter**](http://maartengrootendorst.substack.com/).
    You can also find some of my content on my [**Personal Website**](https://maartengrootendorst.com/).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å’Œæˆ‘ä¸€æ ·ï¼Œå¯¹ AI å’Œ/æˆ–å¿ƒç†å­¦å……æ»¡çƒ­æƒ…ï¼Œè¯·éšæ—¶åœ¨ [**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/)
    ä¸Šæ·»åŠ æˆ‘ï¼Œåœ¨ [**Twitter**](https://twitter.com/MaartenGr) ä¸Šå…³æ³¨æˆ‘ï¼Œæˆ–è€…è®¢é˜…æˆ‘çš„ [**Newsletter**](http://maartengrootendorst.substack.com/)ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨æˆ‘çš„
    [**Personal Website**](https://maartengrootendorst.com/) ä¸Šæ‰¾åˆ°æˆ‘çš„ä¸€äº›å†…å®¹ã€‚
- en: '*All images without a source credit were created by the author â€” Which means
    all of them, I like creating my own images ;)*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ‰€æœ‰æ²¡æœ‰æ¥æºä¿¡ç”¨çš„å›¾ç‰‡å‡ç”±ä½œè€…åˆ›ä½œ â€”â€” ä¹Ÿå°±æ˜¯è¯´ï¼Œå…¨éƒ¨éƒ½æ˜¯æˆ‘ï¼Œæˆ‘å–œæ¬¢è‡ªå·±åˆ¶ä½œå›¾ç‰‡ ;)*'
