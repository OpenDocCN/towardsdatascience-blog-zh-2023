- en: Which Quantization Method is Right for You? (GPTQ vs. GGUF vs. AWQ)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be](https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring Pre-Quantized Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    ¬∑11 min read¬∑Nov 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c45770a07b8d0685c7ab123f15862789.png)'
  prefs: []
  type: TYPE_IMG
- en: Throughout the last year, we have seen the Wild West of Large Language Models
    (LLMs). The pace at which new technology and models were released was astounding!
    As a result, we have many different standards and ways of working with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore one such topic, namely loading your local LLM
    through several (quantization) standards. With sharding, quantization, and different
    saving and compression strategies, it is not easy to know which method is suitable
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the examples, we will use [Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta),
    a fine-tuned variant of Mistral 7B that was trained with [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
    (DPO).
  prefs: []
  type: TYPE_NORMAL
- en: 'üî• **TIP**: After each example of loading an LLM, it is advised to restart your
    notebook to prevent OutOfMemory errors. Loading multiple LLMs requires significant
    RAM/VRAM. You can reset memory by deleting the models and resetting your cache
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can also follow along with the [**Google Colab Notebook**](https://colab.research.google.com/drive/1rt318Ew-5dDw21YZx2zK2vnxbsuDAchH?usp=sharing)
    to make sure everything works as intended.
  prefs: []
  type: TYPE_NORMAL
- en: '**UPDATE**: I uploaded a video version to YouTube that goes more in-depth into
    how to use these quantization methods:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. HuggingFace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most straightforward, and vanilla, way of loading your LLM is through ü§ó
    [Transformers](https://github.com/huggingface/transformers). HuggingFace has created
    a large suite of packages that allow us to do amazing things with LLMs!
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by installing HuggingFace, among others, from its main branch
    to support newer models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After installation, we can use the following pipeline to easily load our LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This method of loading an LLM generally does not perform any compression tricks
    for saving VRAM or increasing efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate our prompt, we first have to create the necessary template. Fortunately,
    this can be done automatically if the chat template is saved in the underlying
    tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The generated prompt, using the internal prompt template, is constructed like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d2c11cd398ec50c0332607ab7df1fe8.png)'
  prefs: []
  type: TYPE_IMG
- en: The prompt template is automatically generated with the internal prompt template.
    Notice that there are different tags for differentiating between the user and
    the assistant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can start passing the prompt to the LLM to generate our answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: Why did the Large Language Model go to the party?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To network and expand its vocabulary!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The punchline may be a bit cheesy, but Large Language Models are all about expanding
    their vocabulary and networking with other models to improve their language skills.
    So, this joke is a perfect fit for them!
  prefs: []
  type: TYPE_NORMAL
- en: For pure inference, this method is generally the least efficient as we are loading
    the entire model without any compression or quantization strategies.
  prefs: []
  type: TYPE_NORMAL
- en: It is, however, a great method to start with as it allows for easy loading and
    using the model!
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Sharding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we go into quantization strategies, there is another trick that we can
    employ to reduce the necessary VRAM for loading our model. With **sharding**,
    we are essentially splitting our model up into small pieces or **shards**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30e509c8abeecfb477aadd3c4ddc2e67.png)'
  prefs: []
  type: TYPE_IMG
- en: Sharding an LLM is nothing more than breaking it up into pieces. Each individual
    piece is much easier to handle and might prevent memory issues.
  prefs: []
  type: TYPE_NORMAL
- en: Each shard contains a smaller part of the model and aims to work around GPU
    memory limitations by distributing the model weights across different devices.
  prefs: []
  type: TYPE_NORMAL
- en: Remember when I said we did not perform any compression tricks before?
  prefs: []
  type: TYPE_NORMAL
- en: That was not entirely true‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: The model that we loaded, [Zephyr-7B-Œ≤](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta),
    was actually already sharded for us! If you go to the model and click the *‚ÄúFiles
    and versions‚Äù* link, you will see that the model was split up into eight pieces.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff7c4f272cf8764f683f5ede5f405b6f.png)'
  prefs: []
  type: TYPE_IMG
- en: The model was split up into eight small pieces or shards. This decreases the
    necessary VRAM as we only need to handle these small pieces.
  prefs: []
  type: TYPE_NORMAL
- en: Although we can shard a model ourselves, it is generally advised to be on the
    lookout for quantized models or even quantize them yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sharding is quite straightforward using the [Accelerate](https://github.com/huggingface/accelerate)
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And that is it! Because we sharded the model into pieces of 4GB instead of
    2GB, we created fewer files to load:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f7666879f66be9ae9b7ca0e69885d74.png)'
  prefs: []
  type: TYPE_IMG
- en: 3\. Quantize with Bitsandbytes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Large Language Model is represented by a bunch of weights and activations.
    These values are generally represented by the usual 32-bit floating point (`float32`)
    datatype.
  prefs: []
  type: TYPE_NORMAL
- en: The number of bits tells you something about how many values it can represent.
    **Float32** can represent values between 1.18e-38 and 3.4e38, quite a number of
    values! The lower the number of bits, the fewer values it can represent.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69c7bf89aa06ada55340e1fb34e9d73f.png)'
  prefs: []
  type: TYPE_IMG
- en: Common value representation methods. We aim to keep the number of bits as low
    as possible whilst maximizing both the range and precision of the representation.
  prefs: []
  type: TYPE_NORMAL
- en: As you might expect, if we choose a lower bit size, then the model becomes less
    accurate but it also needs to represent fewer values, thereby decreasing its size
    and memory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e8fc103eac0e59d74fbd651678838516.png)'
  prefs: []
  type: TYPE_IMG
- en: A different representation method might negatively affect the precision with
    which to represent value. To the extent that some values are not even represented
    (too large values for float16 for example). Examples were calculated with PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization refers to converting an LLM from its original **Float32** representation
    to something smaller. However, we do not simply want to use a smaller bit variant
    but map a larger bit representation to a smaller bit without losing too much information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, we see this often done with a new format, named **4bit-NormalFloat
    (NF4)**. This datatype does a few special tricks in order to efficiently represent
    a larger bit datatype. It consists of three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalization**: The weights of the model are normalized so that we expect
    the weights to fall within a certain range. This allows for more efficient representation
    of more common values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Quantization**: The weights are quantized to 4-bit. In NF4, the quantization
    levels are evenly spaced with respect to the normalized weights, thereby efficiently
    representing the original 32-bit weights.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dequantization**: Although the weights are stored in 4-bit, they are dequantized
    during computation which gives a performance boost during inference.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To perform this quantization with HuggingFace, we need to define a configuration
    for the quantization with [Bitsandbytes](https://github.com/TimDettmers/bitsandbytes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This configuration allows us to specify which quantization levels we are going
    for. Generally, we want to represent the weights with 4-bit quantization but do
    the inference in 16-bit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Loading the model in a pipeline is then straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, we can use the same prompt as we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: Why did the Large Language Model go to the party?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To network and expand its vocabulary!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The punchline may be a bit cheesy, but Large Language Models are all about expanding
    their vocabulary and networking with other models to improve their language skills.
    So, this joke is a perfect fit for them!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Quantization is a powerful technique to reduce the memory requirements of a
    model whilst keeping performance similar. It allows for faster loading, using,
    and fine-tuning LLMs even with smaller GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Pre-Quantization (GPTQ vs. AWQ vs. GGUF)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus far, we have explored sharding and quantization techniques. Albeit useful
    techniques to have in your skillset, it seems rather wasteful to have to apply
    them every time you load the model.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, these models have often already been sharded and quantized for us to
    use. [TheBloke](https://huggingface.co/TheBloke) in particular is a user on HuggingFace
    that performs a bunch of quantizations for us to use.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/231b73427e165a5a7d33ab5162132e53.png)'
  prefs: []
  type: TYPE_IMG
- en: At the moment of writing this, he has uploaded **more than 2000** quantized
    models for us!
  prefs: []
  type: TYPE_NORMAL
- en: These quantized models actually come in many different shapes and sizes. Most
    notably, the GPTQ, GGUF, and AWQ formats are most frequently used to perform 4-bit
    quantization.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPTQ: Post-Training Quantization for GPT Models'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPTQ is a [**p**ost-**t**raining **q**uantization](https://arxiv.org/abs/2210.17323)
    (PTQ) method for 4-bit quantization that focuses primarily on **GPU** inference
    and performance.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind the method is that it will try to compress all weights to a
    4-bit quantization by minimizing the mean squared error to that weight. During
    inference, it will dynamically dequantize its weights to float16 for improved
    performance whilst keeping memory low.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more detailed guide to the inner workings of GPTQ, definitely check out
    the following post:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----c4cd9d77d5be--------------------------------)
    [## 4-bit Quantization with GPTQ'
  prefs: []
  type: TYPE_NORMAL
- en: Quantize your own LLMs using AutoGPTQ
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----c4cd9d77d5be--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with installing a number of packages we need to load in GPTQ-like
    models in HuggingFace Transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: After doing so, we can navigate to the model that we want to load, namely ‚Äú[TheBloke/zephyr-7B-beta-GPTQ](https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ)‚Äù
    and choose a specific revision.
  prefs: []
  type: TYPE_NORMAL
- en: These revisions essentially indicate the quantization method, compression level,
    size of the model, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we are sticking with the ‚Äú*main*‚Äù branch as that is generally a nice
    balance between compression and accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Although we installed a few additional dependencies, we could use the same pipeline
    as we used before which is a great benefit of using GPTQ.
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading the model, we can run a prompt as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following generated text:'
  prefs: []
  type: TYPE_NORMAL
- en: Why did the Large Language Model go to the party?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To show off its wit and charm, of course!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But unfortunately, it got lost in the crowd and couldn't find its way back
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: to its owner. The partygoers were impressed by its ability to blend in
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: so seamlessly with the crowd, but the Large Language Model was just confused
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: and wanted to go home. In the end, it was found by a group of humans
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: who recognized its unique style and brought it back to its rightful place.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From then on, the Large Language Model made sure to wear a name tag at
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: all parties, just to be safe.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GPTQ is the most often used compression method since it optimizes for GPU usage.
    It is definitely worth starting with GPTQ and switching over to a CPU-focused
    method, like GGUF if your GPU cannot handle such large models.
  prefs: []
  type: TYPE_NORMAL
- en: 'GGUF: **GPT-Generated Unified Format**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although GPTQ does compression well, its focus on GPU can be a disadvantage
    if you do not have the hardware to run it.
  prefs: []
  type: TYPE_NORMAL
- en: GGUF, previously GGML, is a quantization method that allows users to use the
    CPU to run an LLM but also offload some of its layers to the GPU for a speed up.
  prefs: []
  type: TYPE_NORMAL
- en: Although using the CPU is generally slower than using a GPU for inference, it
    is an incredible format for those running models on CPU or Apple devices. Especially
    since we are seeing smaller and more capable models appearing, like Mistral 7B,
    the GGUF format might just be here to stay!
  prefs: []
  type: TYPE_NORMAL
- en: 'Using GGUF is rather straightforward with the [ctransformers](https://github.com/marella/ctransformers)
    package which we will need to install first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: After doing so, we can navigate to the model that we want to load, namely ‚Äú[TheBloke/zephyr-7B-beta-GGUF](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF)‚Äù
    and choose a specific file.
  prefs: []
  type: TYPE_NORMAL
- en: Like GPTQ, these files indicate the quantization method, compression, level,
    size of the model, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are using ‚Äú*zephyr-7b-beta.Q4_K_M.gguf*‚Äù since we focus on 4-bit quantization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the model, we can run a prompt as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: Why did the Large Language Model go to the party?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To impress everyone with its vocabulary!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But unfortunately, it kept repeating the same jokes over and over again, making
    everyone groan and roll their eyes. The partygoers soon realized that the Large
    Language Model was more of a party pooper than a party animal.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Moral of the story: Just because a Large Language Model can generate a lot
    of words, doesn‚Äôt mean it knows how to be funny or entertaining. Sometimes, less
    is more!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GGUF is an amazing format if you want to leverage both the CPU and GPU when
    you, like me, are GPU-poor and do not have the latest and greatest GPU available.
  prefs: []
  type: TYPE_NORMAL
- en: 'AWQ: Activation-aware Weight Quantization'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A new format on the block is AWQ ([Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978))
    which is a quantization method similar to GPTQ. There are several differences
    between AWQ and GPTQ as methods but the most important one is that AWQ assumes
    that not all weights are equally important for an LLM‚Äôs performance.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, there is a small fraction of weights that will be skipped during
    quantization which helps with the quantization loss.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, their paper mentions a significant speed-up compared to GPTQ whilst
    keeping similar, and sometimes even better, performance.
  prefs: []
  type: TYPE_NORMAL
- en: The method is still relatively new and has not been adopted yet to the extent
    of GPTQ and GGUF, so it is interesting to see if all these methods can co-exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'For AWQ, we will use the [vLLM](https://github.com/vllm-project/vllm) package
    as that was, at least in my experience, the road of least resistance to using
    AWQ:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'With vLLM, loading and using our model becomes painless:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can easily run the model with `.generate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: Why did the Large Language Model go to the party?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To network and expand its vocabulary!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why did the Large Language Model blush?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Because it overheard another model saying it was a little too wordy!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why did the Large Language Model get kicked out of the library?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It was being too loud and kept interrupting other models‚Äô conversations with
    its endless chatter!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ‚Ä¶
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Although it is a new format, AWQ is gaining popularity due to its speed and
    quality of compression!
  prefs: []
  type: TYPE_NORMAL
- en: 'üî• **TIP**: For a more detailed comparison between these techniques with respect
    to VRAM/Perplexity, I highly advise reading this [in-depth post](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)
    with a follow-up [here](https://github.com/ggerganov/llama.cpp/pull/3776#issuecomment-1781472687).'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are, like me, passionate about AI and/or Psychology, please feel free
    to add me on [**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/), follow
    me on [**Twitter**](https://twitter.com/MaartenGr), or subscribe to my [**Newsletter**](http://maartengrootendorst.substack.com/).
    You can also find some of my content on my [**Personal Website**](https://maartengrootendorst.com/).
  prefs: []
  type: TYPE_NORMAL
- en: '*All images without a source credit were created by the author ‚Äî Which means
    all of them, I like creating my own images ;)*'
  prefs: []
  type: TYPE_NORMAL
