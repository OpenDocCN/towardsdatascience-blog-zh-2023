- en: Getting started with JAX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/getting-started-with-jax-a6f8d8d0e20](https://towardsdatascience.com/getting-started-with-jax-a6f8d8d0e20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Powering the future of high-performance numerical computing and ML research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pierpaoloippolito28.medium.com/?source=post_page-----a6f8d8d0e20--------------------------------)[![Pier
    Paolo Ippolito](../Images/981abb84149adab275473b76bdbde66f.png)](https://pierpaoloippolito28.medium.com/?source=post_page-----a6f8d8d0e20--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a6f8d8d0e20--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a6f8d8d0e20--------------------------------)
    [Pier Paolo Ippolito](https://pierpaoloippolito28.medium.com/?source=post_page-----a6f8d8d0e20--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a6f8d8d0e20--------------------------------)
    ·5 min read·Jul 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17e806ec59ab7110eb1eb8acbdca9117.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Lance Asper](https://unsplash.com/@lance_asper?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JAX is a Python library developed by Google to perform high-performance numerical
    computing on any type of device (CPU, GPU, TPU, etc…). One of the main applications
    of JAX is Machine Learning and Deep Learning research development, although the
    library is mainly designed to provide all the necessary capabilities to perform
    general-purpose scientific computing tasks (highly dimensional matrices operations,
    etc…).
  prefs: []
  type: TYPE_NORMAL
- en: Considering the focus specifically on high-performance computing, JAX has been
    designed to be extremely fast being built on top of XLA (Accelerated Linear Algebra).
    XLA is in fact a compiler designed to speed up linear algebra operations and can
    be used to work behind also other frameworks such as TensorFlow and Pytorch. Additionally,
    JAX arrays have been designed to follow the same principles as Numpy, making it
    really easy to migrate old Numpy code to JAX and take advantage of performance
    speed-ups through GPUs and TPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the main characteristics of JAX are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Just in Time (JIT) compilation**: JIT and accelerated hardware are what can
    enable JAX to be much faster than plain Numpy. Using the *jit()* function can
    be possible to compile and cache custom functions with the XLA kernel. Using caching
    we will increase the overall execution time we first run the function, to then
    drastically reduce the time for the successive runs. When using caching it is
    important to make sure to clear the caches when needed in order to avoid stale
    results (e.g. global variables changing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic Parallelization:** Asynchronous Dispatch enables JAX vectors to
    be lazily evaluated, materializing content just when accessed (control is returned
    to the program before computation completion). Additionally, in order to make
    possible graph optimization, JAX arrays are immutable (similar concepts with lazy
    evaluation and graph optimization applies to [Apache Spark](/getting-started-with-apache-spark-cb703e1b3ee9)).
    The *pmap()* function can be used to parallelize computations on multiple GPUs/TPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic Vectorization**: Automatic vectorization to parallelize operations
    can be performed using the *vmap()* function. During vectorization, an algorithm
    is transformed from operating with a single value to a set of values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic Differentiation**: The *grad()* function can be used to automatically
    calculate the gradient (derivative) of functions. In particular, JAX Automatic
    Differentiation enables the development of general-purpose differential programs
    outside the spectrum of Deep Learning. Making it possible to differentiate through
    recursion, branches, loops, perform higher-order differentiation (e.g. Jacobians
    and Hessians), and using both forward and reverse mode differentiation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore JAX is able to provide us with all the necessary foundations to build
    advanced Deep Learning models but not out-of-the-box high-level utils for some
    of the most common Deep Learning operations (e.g. loss/activation functions, layers,
    etc…). For example, model parameters learned during ML training can be stored
    in a Pytree structure in JAX. Considering all the advantages provided by JAX different
    DL-oriented frameworks have been built on top of it such as Haiku (used by DeepMind)
    and Flax (used by Google Brain).
  prefs: []
  type: TYPE_NORMAL
- en: Demonstration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As part of this article, we are now going to see how to solve a simple classification
    problem using JAX and the [Kaggle Mobile Price Classification dataset](https://www.kaggle.com/datasets/iabhishekofficial/mobile-price-classification)
    [1] to predict in which price range a phone will be. All the code used throughout
    this article (and more!) is available on [my GitHub](https://github.com/pierpaolo28)
    and [Kaggle accounts](https://www.kaggle.com/pierpaolo28).
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we need to make sure to have JAX installed in our environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we are ready to import the necessary libraries and datasets (Figure
    1). In order to simplify our analysis, instead of using all the classes in our
    label we filter the data to use just 2 classes and subset the number of features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/650db3e4167067f50af65e416b5a9563.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Mobile Price Classification Dataset (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: Once cleaned the dataset, we can now divide it into training and test subsets
    and standardize the input features so that to make sure they all lie within the
    same ranges. At this point, the input data is also converted in JAX arrays.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In order to predict the price range of the phones, we are going to create a
    Logistic Regression model from scratch. To do so, we first need to create a couple
    of helper functions (one to create the Sigmoid activation function, and another
    for the binary loss function).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to create our training loop and plot the results (Figure 2).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6204d9b6d257691a91df3ceb667ba819.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Logistic Regression Training History (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: Once happy with the results, we can then test the model against our test set
    (Figure 3).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/effce9fd630a04de06c437405e3f83b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Classification Report on Test Data (Image by Author).'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As demonstrated in this brief example, JAX has a very intuitive API that closely
    follows Numpy conventions while making it possible to use the same code for CPU/GPU/TPU
    usage. Making use of these building blocks it can then be possible to create highly
    customizable Deep Learning models optimized by design for performance.
  prefs: []
  type: TYPE_NORMAL
- en: Contacts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to keep updated with my latest articles and projects [follow me
    on Medium](https://pierpaoloippolito28.medium.com/subscribe) and subscribe to
    my [mailing list](http://eepurl.com/gwO-Dr). These are some of my contacts details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Linkedin](https://uk.linkedin.com/in/pier-paolo-ippolito-202917146)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Personal Website](https://pierpaolo28.github.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Medium Profile](https://towardsdatascience.com/@pierpaoloippolito28)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GitHub](https://github.com/pierpaolo28)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kaggle](https://www.kaggle.com/pierpaolo28)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bibliography
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] “Mobile Price Classification” (ABHISHEK SHARMA). Accessed at: [https://thecleverprogrammer.com/2021/03/05/mobile-price-classification-with-machine-learning/](https://thecleverprogrammer.com/2021/03/05/mobile-price-classification-with-machine-learning/)
    (MIT License: [https://github.com/alifrmf/Mobile-Price-Prediction-Classification-Analysis/tree/main](https://github.com/alifrmf/Mobile-Price-Prediction-Classification-Analysis/tree/main))'
  prefs: []
  type: TYPE_NORMAL
