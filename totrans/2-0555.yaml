- en: Contextual Text Correction Using NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NLP进行上下文文本校正
- en: 原文：[https://towardsdatascience.com/contextual-text-correction-using-nlp-81a1363c5fc3](https://towardsdatascience.com/contextual-text-correction-using-nlp-81a1363c5fc3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/contextual-text-correction-using-nlp-81a1363c5fc3](https://towardsdatascience.com/contextual-text-correction-using-nlp-81a1363c5fc3)
- en: '**Detecting and correcting errors that involve modeling context**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**检测和校正涉及建模上下文的错误**'
- en: '[](https://jagota-arun.medium.com/?source=post_page-----81a1363c5fc3--------------------------------)[![Arun
    Jagota](../Images/3c3eb142f671b5fb933c2826d8ed78d9.png)](https://jagota-arun.medium.com/?source=post_page-----81a1363c5fc3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----81a1363c5fc3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----81a1363c5fc3--------------------------------)
    [Arun Jagota](https://jagota-arun.medium.com/?source=post_page-----81a1363c5fc3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jagota-arun.medium.com/?source=post_page-----81a1363c5fc3--------------------------------)[![Arun
    Jagota](../Images/3c3eb142f671b5fb933c2826d8ed78d9.png)](https://jagota-arun.medium.com/?source=post_page-----81a1363c5fc3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----81a1363c5fc3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----81a1363c5fc3--------------------------------)
    [Arun Jagota](https://jagota-arun.medium.com/?source=post_page-----81a1363c5fc3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----81a1363c5fc3--------------------------------)
    ·23 min read·Jan 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----81a1363c5fc3--------------------------------)
    ·阅读时间23分钟·2023年1月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bcadaafc6c2554d8c8a66ada008560c7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcadaafc6c2554d8c8a66ada008560c7.png)'
- en: Image by [Lorenzo Cafaro](https://pixabay.com/users/3844328-3844328/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1870721)
    from [Pixabay](https://pixabay.com/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Lorenzo Cafaro](https://pixabay.com/users/3844328-3844328/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1870721)提供，来自[Pixabay](https://pixabay.com/)
- en: 'In the previous article, we discussed the problem of detecting and correcting
    common errors in text using methods from statistical NLP:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我们讨论了使用统计NLP方法检测和校正文本中常见错误的问题：
- en: '[](/text-correction-using-nlp-b68c7233b86?source=post_page-----81a1363c5fc3--------------------------------)
    [## Text Correction Using NLP'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/text-correction-using-nlp-b68c7233b86?source=post_page-----81a1363c5fc3--------------------------------)
    [## 使用NLP进行文本校正'
- en: 'Detecting and correcting common errors: problems and methods'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检测和校正常见错误：问题和方法
- en: towardsdatascience.com](/text-correction-using-nlp-b68c7233b86?source=post_page-----81a1363c5fc3--------------------------------)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/text-correction-using-nlp-b68c7233b86?source=post_page-----81a1363c5fc3--------------------------------)
- en: There we took an inventory of several issues, with accompanying real examples
    and discussion. Below are the ones that we had not fully resolved in that post.
    (The last two were not even touched.) These are the ones needing handling of context.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在那里我们盘点了几个问题，并附有实际示例和讨论。以下是我们在那篇文章中未完全解决的问题（最后两个甚至没有涉及）。这些是需要处理上下文的。
- en: Missing commas.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 漏掉逗号。
- en: Missing or incorrect articles.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 漏掉或错误使用冠词。
- en: Using singular instead of plural, or vice-versa.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单数而非复数，或反之亦然。
- en: Using the wrong preposition or other connective.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用错误的介词或其他连接词。
- en: In this post, we start with issues involving articles. We look at elaborate
    examples of this scenario and delve into what we mean by “issues” on each.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们从涉及冠词的问题开始。我们查看这些场景的详细示例，并深入探讨每个“问题”的含义。
- en: We then describe a method that addresses them. It uses a key idea of self-supervision.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们描述了一个解决这些问题的方法。它使用了自我监督的关键思想。
- en: We then move on to the various other scenarios and discuss how this same method
    addresses them as well. Albeit with some slightly different specifications of
    the outcomes for the self-supervision, and slightly different preprocessing.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们转到各种其他场景，并讨论相同的方法如何解决这些问题。尽管结果的自我监督规格略有不同，预处理也略有不同。
- en: '**Issues Involving Articles**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及冠词的问题**'
- en: Consider these examples.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这些示例。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the first sentence, there should be an *a* between *within* and *matter*.
    In the second sentence, there should be a *the* right after *capitalize* and another
    one right after *in*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一句话中，*within*和*matter*之间应该有一个*a*。在第二句话中，*capitalize*后面应该有一个*the*，在*in*后面也应该有一个。
- en: Consider this rule.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个规则。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Would you agree this rule makes sense? Never mind its narrow scope of applicability.
    As will become clear later, this will hardly matter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你同意这个规则是有意义的吗？不要介意它的适用范围狭窄。稍后会变得清楚，这几乎无关紧要。
- en: If you agree, then *within* and *matter of* are the left and the right contexts
    respectively, for where the *a* should be inserted.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你同意，那么*within*和*matter of*分别是左侧和右侧上下文，确定*a*应该插入的位置。
- en: We can succinctly represent such a rule as L**a**R which should be read as follows.
    If the left context is L and the right context is R, then there should be an ***a***
    between the two. In our setting, L and R are both sequences of tokens, perhaps
    bounded in length.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简洁地将这样的规则表示为L**a**R，应该这样解读。如果左侧上下文是L，右侧上下文是R，那么它们之间应该有一个***a***。在我们的设置中，L和R都是令牌序列，可能有长度限制。
- en: As will become clear in the paragraph that follows, in fact, it is better to
    express this rule in a somewhat generalized form as L**M**R.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如接下来的段落将清楚地说明，实际上，将这个规则以稍微泛化的形式表示为L**M**R会更好。
- en: Here **M** denotes a fixed set of possibilities defining exactly the problem
    we are trying to solve. In our cas*e,* we might choose **M** to be the set{ **a***,*
    **an***,* **the***, _***none***_* }*.*
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里**M**表示一个固定的可能性集合，准确地定义了我们要解决的问题。在我们的例子中，我们可以选择**M**为集合{ **a***,* **an***,*
    **the***, _***none***_* }*。
- en: We would read this rule as “if the left context is L and the right context is
    R, then there are four possibilities we want to model. _**None***_*, meaning there
    is no article in between L and R, and the other three for the three specific articles.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会将这个规则理解为“如果左侧上下文是L，右侧上下文是R，那么我们要建模四种可能性。_**None***_，表示L和R之间没有文章，其余三种是三种特定的文章。
- en: What we are really doing here is formalizing the problem we want to be solved
    as a supervised learning problem with specific outcomes, in our case **M**. This
    will not require any human labeling of the data. Just defining **M**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正做的是将我们希望解决的问题形式化为一个具有特定结果的监督学习问题，在我们的例子中是**M**。这不需要任何人工标记数据。只需定义**M**。
- en: What we are really doing is *self-supervision*. We can define as many problems
    as we want, for different choices of **M**. (In fact, in this post we will address
    a few more.) Then we can apply the power of supervised learning without having
    to incur the cost of labeling data. Very powerful.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正做的是*自我监督*。我们可以为不同的**M**选择定义尽可能多的问题。（实际上，在这篇文章中我们会再讨论几个。）然后我们可以利用监督学习的力量，而不需要承担标记数据的成本。这非常强大。
- en: Let’s see an example. Consider **M** = {_**none**_, _**a_**, _**the_**, _**an_**
    }. Say our training set has exactly one sentence in it. The underscores are there
    just for readability — to distinguish between the outcomes in **M** and the other
    words in the text.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 看一个例子。考虑**M** = {_**none**_, _**a_**, _**the_**, _**an_** }。假设我们的训练集中恰好有一个句子。下划线只是为了可读性——区分**M**中的结果和文本中的其他词。
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We’ll further assume that our rule does not cross sentence boundaries. This
    is a reasonable assumption. Nothing in the modeling depends on this assumption
    so it can always be relaxed as needed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还假设我们的规则不会跨越句子边界。这是一个合理的假设。建模中的任何内容都不依赖于这个假设，因此它可以根据需要放宽。
- en: 'From this one-sentence corpus we will derive the following labeled data:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个单句语料库中，我们将得出以下标记数据：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: On each line, the word flanked by the underscores is an outcome in **M**, the
    words to the left its left context, and the words to the right its right context.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行中，被下划线标记的词是**M**中的一个结果，左侧的词是左侧上下文，右侧的词是右侧上下文。
- en: For instance,
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: says that if the left context is [*John*, *is*] and the right context is [*man*],
    then there is an *a* between the left and the right contexts. So this labeled
    instance captures where the article should be and what its identity should be.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 说如果左侧上下文是[*John*, *is*]，右侧上下文是[*man*]，那么在左侧和右侧上下文之间有一个*a*。因此，这个标记实例捕捉了文章应该出现的位置和它的身份。
- en: The remaining instances capture the negatives, i.e. where the article should
    not be.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的实例捕捉负例，即文章不应该出现的地方。
- en: Once we have such labeled data sets we can in principle use any suitable supervised
    learning method to learn to predict the label from the input (L, R).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有这样的标记数据集，我们原则上可以使用任何合适的监督学习方法来学习从输入（L，R）预测标签。
- en: In this post, we will focus on a particular supervised learning method that
    we think is a strong fit for our particular supervised learning problem. It is
    a for-purpose method that models L and R as sequences of tokens.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将专注于一种我们认为非常适合我们特定监督学习问题的监督学习方法。这是一种为特定目的建模L和R为标记序列的方法。
- en: The reader might ask, why not use the latest and greatest NLP techniques for
    this problem as they handle very elaborate scenarios? Such as recurrent neural
    networks, transformers, and most recently large language models such as ChatGPT.
    Perhaps even Hidden Markov Models or Conditional Random Fields. (For more on elaborate
    language models, see [6] and [7].) Some if not all of them should work very well.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可能会问，为什么不使用最新最好的NLP技术来处理这个问题呢？它们可以处理非常复杂的场景？例如递归神经网络、变换器以及最近的大型语言模型如ChatGPT。甚至可能是隐马尔可夫模型或条件随机场。（有关复杂语言模型的更多信息，请参见[6]和[7]。）这些方法中的一些如果不是全部都应该效果很好。
- en: There are tradeoffs. If one is trying to solve these problems for the long term,
    perhaps to build a product around it, e.g., Grammarly [3], the latest and greatest
    methods should of course be considered.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有权衡。如果有人试图从长远解决这些问题，或许是为了围绕它构建一个产品，例如Grammarly [3]，那么最新最好的方法当然应该被考虑。
- en: If on the other hand, one wishes to build or at least understand simpler yet
    effective methods from scratch, then the method of this post should be considered.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果有人希望从头开始构建或至少理解更简单但有效的方法，则本文中的方法应该被考虑。
- en: The aforementioned method is also easy to implement incrementally. For readers
    who want to give this a try, check out the section **Mini Project**. The project
    described there could be executed in a few hours, tops a day. By a programmer
    well-versed in Python or some other scripting language.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方法也容易逐步实现。对于那些想尝试的人，请查看**迷你项目**部分。那里描述的项目可以在几小时内完成，最多一天，由一个精通Python或其他脚本语言的程序员完成。
- en: '**The Method**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法**'
- en: First, let’s describe this method for the particular problem of missing or incorrect
    articles. Following that we will apply it to several of the other issues mentioned
    earlier in this post.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们描述这种方法如何解决缺失或不正确的冠词问题。接着，我们将把它应用于本文前面提到的其他几个问题。
- en: Consider L**M**R. We will work with a probability distribution P(**M**|L, R)
    attached to this rule. P(**M**|L, R) will tell us which of the outcomes in **M**
    is more likely than others in the context of (L, R).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以L**M**R为例。我们将使用附加到这个规则上的概率分布P(**M**|L, R)。P(**M**|L, R)会告诉我们在**M**中的哪一个结果在（L,
    R）的上下文中比其他结果更可能。
- en: For instance, we would expect *P*(**a**|L=*John is*, R=*man*) to be close to
    1 if not 1.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果*P*(**a**|L=*John is*, R=*man*) 接近1（如果不是1的话），我们就会期待这种情况。
- en: '*P*(**M**|L, R) can be learned from our training data in an obvious way.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(**M**|L, R) 可以从我们的训练数据中以明显的方式学习。'
- en: '*P*(**m**|L, R) = #(L,**m**,R)/sum_**m’** #(L,**m’**,R)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(**m**|L, R) = #(L,**m**,R)/sum_**m’** #(L,**m’**,R)'
- en: 'Here #(L, **m’**, R) is the number of instances in our training set in which
    the label on input (L, R) is **m’.** Note that if **m’** is _**none**_ then R
    begins right after L ends.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '这里 #(L, **m’**, R) 是我们训练集中输入（L, R）上标签为**m’**的实例数量。请注意，如果**m’** 是_**none**_，则R在L结束后紧接着开始。'
- en: Let’s say our training corpus now has exactly two sentences.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的训练语料库现在恰好有两个句子。
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*P*(**a**|L=*John is*, R=*man*) would be ½ since there are two instances of
    this (L, R) of which one is labeled **a**, the other **the**.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(**a**|L=*John is*, R=*man*) 将是½，因为这个（L, R）有两个实例，其中一个标记为**a**，另一个标记为**the**。'
- en: '**Generalizing, in the ML Sense**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**在机器学习意义上的泛化**'
- en: Consider the labeled instances
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑标记实例
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If our corpus had enough of these, we’d want our ML to be able to learn the
    rule
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的语料库中有足够多的这些数据，我们希望我们的机器学习能学会这个规则。
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: i.e., that P(**a**|L=*is*, R=*man*) is also close to 1\. Such a rule would generalize
    better as it is applicable to any scenario in which the left context is *is* and
    the right context is *man*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 即，*P*(**a**|L=*is*, R=*man*) 也接近1。这种规则会更好地泛化，因为它适用于左侧上下文是*is*而右侧上下文是*man*的任何场景。
- en: In our approach, we will address this as follows.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的方法中，我们将按如下方式解决这个问题。
- en: Say L**m**R is an instance in the training set. Below we’ll assume L and R are
    sequences of tokens. In our setting, the tokenization may be based on white space,
    for instance. That said, our method will work with any tokenization.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 设L**m**R是训练集中的一个实例。下面我们将假设L和R是标记序列。在我们的设置中，标记化可能基于空格。例如，我们的方法将适用于任何标记化方式。
- en: From L**m**R we will derive new training instances L’**m**R’ where L’ is a suffix
    of L and R’ a prefix of R. L’ or R’ or both may have zero tokens.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从L**m**R我们将派生新的训练实例L’**m**R’，其中L’是L的一个后缀，R’是R的一个前缀。L’或R’或两者可能都没有标记。
- en: The derived instances will cover all combinations of L’ and R’.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从中派生的实例将覆盖所有L’和R’的组合。
- en: Sure the size of the training set could explode if applied to a large corpus
    and the lengths of L and R are not bounded. Okay, bound them.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果将其应用于大型语料库且L和R的长度没有限制，训练集的大小可能会爆炸。好吧，限制它们。
- en: '**Recap**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**回顾**'
- en: Okay, let’s see where we are. Consider the examples earlier in this post.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们看看现在的情况。考虑本文前面的例子。
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Assuming our training corpus is rich enough, for example, all of Wikipedia pre-segmented
    into sentences, we should have no difficulty whatsoever in detecting where the
    articles are missing in these two sentences, and recommending specific fixes.
    The sentences that would result from applying these fixes are
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的训练语料库足够丰富，例如，维基百科的所有内容都已预先分段成句子，我们应该完全没有困难来检测这些句子中缺失的文章位置，并推荐具体的修复方案。应用这些修复方案后得到的句子是
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now consider
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using our trained model we can detect that **the** here should probably be **a.**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们训练好的模型，我们可以检测到**the**这里可能应该是**a**。
- en: '**Prediction Algorithm**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测算法**'
- en: To this point, we’ve only discussed informally how we might use the learned
    rules to identify issues, not in fine detail. We now close this gap.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只非正式地讨论了如何使用学习到的规则来识别问题，而没有详细讨论。我们现在填补这个空白。
- en: Consider a window L**m**R on which we want to compare m with the predictions
    from the rules that apply in this situation. For example, were L**m**R to be
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个窗口L**m**R，我们要在此窗口中将m与应用于这种情况的规则的预测进行比较。例如，L**m**R如果是
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: we’d want to predict off the rules L’ _the_ R’, where L’ is [*within*] or [],
    and R’ is [*matter*, of, *minutes*], [*matter*, of], [*matter*], or [] and from
    these predictions somehow come up with a final prediction.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想基于规则L’ _the_ R’进行预测，其中L’是[*within*]或[]，R’是[*matter*, of, *minutes*]，[*matter*,
    of]，[*matter*]或[]，并从这些预测中以某种方式得出最终预测。
- en: The approach we will take is the following. We assume that we are given some
    cutoff, call it *c*, on the minimum value that *P*(**m’**|L, R) needs to be for
    us to surface that our method predicts **m’** in the context (L, R).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采取以下方法。我们假设给定一个截止值，称之为*c*，这是*P*(**m’**|L, R)在我们的方法预测**m’**的上下文（L, R）中需要达到的最小值。
- en: We will examine our rules in order of nonincreasing|L’|+|R’|. Here |T| denotes
    the number of tokens in a list T. We will stop as soon as we find some m’ for
    some L’, R’ such that *P*(**m’**|L’, R’) is at least *c*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照非递增的|L’|+|R’|的顺序检查我们的规则。这里|T|表示列表T中的标记数。我们将停止，直到我们找到某个L’、R’使得*P*(**m’**|L’,
    R’)至少为*c*。
- en: In plain English, we are doing this. Among all the rules that apply to a particular
    situation, we are finding one that is sufficiently predictive of some outcome
    in **M** and is also the most general among those that do.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 用简单的英语来说，我们在做这个。在适用于特定情况的所有规则中，我们找到一个对**M**的某些结果具有足够预测性且在这些规则中最通用的规则。
- en: '**Try These**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**尝试这些**'
- en: Consider these examples, also from [https://en.wikipedia.org/wiki/Shannon_Lucid](https://en.wikipedia.org/wiki/Shannon_Lucid)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这些例子，来自[https://en.wikipedia.org/wiki/Shannon_Lucid](https://en.wikipedia.org/wiki/Shannon_Lucid)
- en: 'I have removed the articles. I would like the reader to guess where an article
    should go and what it should be: *the*, *a*, or *an*.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经移除了文章。我希望读者猜测文章应该放在哪里以及它应该是什么：*the*、*a*或*an*。
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Look below only after you have made all your predictions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在你完成所有预测后才查看下面的内容。
- en: The instances from which these were derived were
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实例派生的来源是
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: How good were your predictions? If you did well, the method described so far
    in this post would also have worked well.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你的预测效果如何？如果你做得很好，那么本文中描述的方法也会有效。
- en: '**Mini Project**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**小项目**'
- en: If you are interested in a mini project that could be implemented in hours,
    consider this. Write a script, probably just a few lines, to input a text document
    and output a labeled training set. Then inspect the labeled training set to get
    a sense of whether it contains useful instances for the prediction of the locations
    and the identities of the articles.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对一个可以在几个小时内实现的小项目感兴趣，考虑这个。编写一个脚本，可能只有几行代码，输入一个文本文件并输出一个标记的训练集。然后检查标记的训练集，以了解它是否包含对预测文章位置和身份有用的实例。
- en: If your assessment shows potential and you have the time, you can then consider
    taking it further. Perhaps use an existing ML implementation, such as from scikit-learn,
    on the training set. Or implement the method from scratch.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的评估显示出潜力并且你有时间，你可以考虑进一步推进。也许使用现有的机器学习实现，例如来自 scikit-learn 的实现，来处理训练集。或者从头实现该方法。
- en: Now some more detail will help with your script. Consider limiting the context
    to L and R to be exactly one word each. Scan the words in the document in sequence,
    and construct the negative and positive instances on the fly. Ignore sentence
    boundaries unless you have access to an NLP tool such as NLTK and can use its
    tokenizer to segment the text into sentences.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一些更多的细节将有助于你的脚本。考虑将上下文限制为 L 和 R 各自恰好一个单词。在文档中按顺序扫描单词，并即时构建负实例和正实例。忽略句子边界，除非你有
    NLP 工具（如 NLTK），并可以使用其分词器将文本分割成句子。
- en: Deposit the constructed instances incrementally into a pandas data frame of
    three columns L, R, **M**. **M** is the set we chose in this section. Output this
    data frame to a CSV file.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 将构造出的实例逐步存入一个包含三列 L、R、**M** 的 pandas 数据框。**M** 是我们在本节中选择的集合。将此数据框输出到 CSV 文件中。
- en: How to get a reasonable training set for your script? Download a Wikipedia page
    or two by copying and pasting.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如何为你的脚本获取一个合理的训练集？通过复制和粘贴下载一两篇维基百科页面。
- en: '**Issues Involving Commas**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及逗号的问题**'
- en: Next, let’s turn our attention to issues involving commas. In [1] we covered
    some simple scenarios. The ones below are more nuanced.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们把注意力转向涉及逗号的问题。在 [1] 中我们覆盖了一些简单的场景。下面的那些更为微妙。
- en: Consider this from [https://en.wikipedia.org/wiki/Zork](https://en.wikipedia.org/wiki/Zork)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 参考 [https://en.wikipedia.org/wiki/Zork](https://en.wikipedia.org/wiki/Zork)
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: First off, let’s observe that to apply our method, we should retain the comma
    as a separate token. Then the problem looks like the one we addressed earlier,
    the one on articles. It would make sense to choose **M** = {_**comma**_, _**none**_}.
    That is, the supervised learning problem is to predict whether there is a comma
    or not in the context (L, R).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们观察到，为了应用我们的方法，我们应该将逗号保留为一个单独的标记。然后，这个问题看起来就像我们之前讨论的那个关于冠词的问题。选择**M** =
    {_**comma**_, _**none**_} 是有意义的。也就是说，监督学习的问题是预测在上下文 (L, R) 中是否存在逗号。
- en: From what we have seen so far, while the rules we learn might be effective,
    they may not generalize adequately. This is because the last token of the left
    context would be *Zork*. We are not really learning the general pattern
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们目前看到的情况来看，尽管我们学习的规则可能有效，但它们可能无法充分推广。这是因为左侧上下文的最后一个标记将是 *Zork*。我们并没有真正学习到一般模式。
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Is there a straightforward way to generalize our method so it can learn more
    general rules?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有没有简单的方法来推广我们的方法，使其能够学习更一般的规则？
- en: The answer is yes. Here is how.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是肯定的。这里是方法。
- en: We’ll introduce the concept of an abstracted token. We’ll start with a single
    abstraction that is relevant to our example. Later in this post, we’ll introduce
    other abstractions as needed.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍抽象标记的概念。我们将从一个与我们示例相关的单一抽象开始。在这篇文章后面，我们将根据需要介绍其他抽象。
- en: We’ll assume the word on which this abstraction is applied contains only characters
    from *a* through *z*. That is, no digits; no special characters.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设应用此抽象的单词仅包含从 *a* 到 *z* 的字符。也就是说，没有数字；没有特殊字符。
- en: 'This abstraction will produce one of three strings: /**capitalized/** denoting
    that the word begins with a letter in upper case followed by zero or more letters
    in lower case, /**all_lower/** denoting that all the letters in the word are in
    lower case, and /**all_caps/** denoting that all the letters in the word are in
    upper case.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这种抽象将生成三种字符串之一：/**capitalized/** 表示单词以大写字母开头，后跟零个或多个小写字母，/**all_lower/** 表示单词中的所有字母都是小写字母，以及
    /**all_caps/** 表示单词中的所有字母都是大写字母。
- en: Next, we will derive new sequences of tokens from the existing ones by selectively
    applying this abstraction operator.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过选择性地应用这个抽象操作符，从现有标记中推导出新的标记序列。
- en: Let’s elaborate on “selectively”. If for every token in the sequence, we considered
    two possibilities, the original token or the abstracted one, we would get a combinatorial
    explosion of generated sequences.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细说明“选择性”。如果对序列中的每个标记，我们考虑两种可能性，即原始标记或抽象标记，我们将会得到生成序列的组合爆炸。
- en: To mitigate this issue, we will only abstract out the tokens that occur sufficiently
    infrequently if at all in our training set. Or abstract out only those that yield
    /**capitalized**/ or /**all-caps**/.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这个问题，我们将仅抽象出在我们的训练集中出现频率足够低的标记，或者仅抽象出那些产生/**首字母大写**/或/**全大写**/的标记。
- en: Below is the sequence we may derive from *In Zork*, *the* …
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们可能从*In Zork*中派生出的序列，*the* …
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We only abstracted *Zork* as it is both capitalized and an uncommon word.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只抽象出*Zork*，因为它既是首字母大写又是一个不常见的词。
- en: Now imagine that we add, to the training set, new labeled instances derived
    from the abstracted sequences. The label is the one associated with the original
    sequence.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们将从抽象序列派生的新的标记实例添加到训练集中。标签是与原始序列相关的标签。
- en: In our example, the derived labeled instance would be
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，得到的标记实例是
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now we train our algorithm exactly as before. It will learn the generalized
    rules as well.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们按照之前的方式训练我们的算法。它也会学习到一般化的规则。
- en: Note that when we say “add new labeled instances to the training set” we are
    not implying that this needs to be done offline. We can simply add these labeled
    instances on the fly. This is analogous to what is often done in ML practice.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当我们说“将新的标记实例添加到训练集中”时，我们并不是暗示这需要离线完成。我们可以简单地动态添加这些标记实例。这类似于在机器学习实践中常做的事情。
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Also, note that we described our method as “adding new labeled instances” only
    because we felt it was useful to explain it this way. We can view this alternately
    as if we did not add new labeled instances but merely extracted additional features.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，我们将我们的方法描述为“添加新的标记实例”，仅仅是因为我们觉得这样解释更有用。我们也可以将其视作没有添加新的标记实例，而只是提取了额外的特征。
- en: This is because all the newly-added instances have the same label — the original
    one. So we can collapse them all into the original instance, just with additional
    features extracted.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为所有新添加的实例都具有相同的标签——原始标签。因此，我们可以将它们全部合并到原始实例中，只是提取了额外的特征。
- en: '**More Nuanced Examples**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**更细致的例子**'
- en: Now consider these examples from [https://en.wikipedia.org/wiki/Shannon_Lucid](https://en.wikipedia.org/wiki/Shannon_Lucid)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑这些来自 [https://en.wikipedia.org/wiki/Shannon_Lucid](https://en.wikipedia.org/wiki/Shannon_Lucid)
    的示例
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: These ones are more intricate.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是更复杂的。
- en: Nonetheless, we will continue with our method for the reasons we mentioned earlier
    in the post. One is that a basic yet meaningful version can be implemented in
    days if not hours from scratch. (No ML libraries needed.)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们将继续使用我们的方法，理由是我们在帖子中提到的那些。一个理由是，一个基本但有意义的版本可以在几天甚至几小时内从零开始实现。（不需要机器学习库。）
- en: To these, we’ll add one more. This method’s predictions are explainable. Specifically,
    if it detects an issue and makes a recommendation, then the specific rule that
    was involved can be attached as an explanation. As we’ve seen, rules are generally
    transparent.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些，我们还要增加一点。这种方法的预测是可以解释的。具体来说，如果它检测到一个问题并提出建议，那么涉及的具体规则可以作为解释附加上。正如我们所见，规则通常是透明的。
- en: Okay, back to the examples.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，回到例子。
- en: Let’s examine the scenarios involving commas in the above examples one by one.
    We won’t examine all.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一检查上述示例中涉及逗号的场景。我们不会检查所有的。
- en: With those that we do examine, we will also weigh in on whether we think our
    current method has a good chance of working as is. These inspections will also
    generate ideas for further enhancement.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们检查的那些规则，我们还会评估我们当前的方法是否有很好的机会按现状运行。这些检查也会产生进一步改进的想法。
- en: Consider
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The sequence we derive from this is
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从中派生出的序列是
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The labeled instances derived from these two sequences also include all combinations
    of suffices of the left context paired with prefixes of the right context. In
    the terminology of machine learning, this means that we are enumerating lots of
    hypotheses in the space of hypotheses (in our setting, the hypotheses are the
    rules).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 从这两个序列中派生的标记实例还包括左侧上下文的所有后缀组合与右侧上下文的前缀配对。用机器学习的术语来说，这意味着我们在假设空间中枚举了大量假设（在我们的设置中，假设就是规则）。
- en: The point we are trying to make in the previous paragraph is that by generating
    lots of hypotheses, we increase the likelihood of finding some rules that are
    sufficiently predictive.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一段中试图说明的要点是，通过生成大量假设，我们增加了找到一些足够具有预测性的规则的可能性。
- en: Of course, there is no free lunch. This impacts the training time and model
    complexity as well.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，没有免费的午餐。这也影响训练时间和模型复杂度。
- en: This also assumes that we are somehow able to discard the rules we found during
    this process that turned out to be noisy or ineffective. Specifically, those that
    were either *insufficiently* predictive or could be covered by more general rules
    that are equally predictive.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这也假设我们能够丢弃在这个过程中发现的那些结果是嘈杂或无效的规则。具体来说，就是那些*预测能力不足*或可以被同样具有预测能力的更通用规则覆盖的规则。
- en: In a section downstream in this post, we will address all these issues. That
    said, only an empirical evaluation over a wide range of scenarios would ultimately
    reveal how effective our approach is.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的后续部分，我们将解决所有这些问题。也就是说，只有通过对广泛场景的实证评估，才能最终揭示我们的方法的有效性。
- en: Back to this specific example. First, let’s see it again.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 回到这个具体的例子。首先，再次查看它。
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: There is a fair chance our method will work adequately as-is. If not on this
    particular one, then at least on similar examples. Furthermore, nothing specific
    comes to mind in terms of enhancements. So let’s move on to other examples.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法有很大机会在现状下有效。如果在这个具体例子中无效，那至少在类似例子中有效。此外，关于改进的具体内容没有什么特别的想法。所以让我们继续看看其他例子。
- en: Next, consider
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，考虑
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We think the current method, as is, is likely to work for this. Why? Consider
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，当前的方法在这种情况下可能有效。为什么？考虑
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Would you not consider inserting a comma between *old* and *the* based on this
    information alone? (I do mean “consider”.)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些信息，你不会考虑在*old*和*the*之间插入一个逗号吗？（我确实是说“考虑”。）
- en: If you would, the algorithm could also work well. It sees the same information.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，这个算法也可能效果很好。它看到相同的信息。
- en: Next, consider
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，考虑
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The abstraction we presented earlier, which abstracts certain words out into
    /**capitalized**/, /**all_lower**/, or /**all_caps**/ should help here.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提出的抽象，将某些词抽象为/**capitalized**/、/**all_lower**/或/**all_caps**/应该在这里有所帮助。
- en: If it doesn’t help adequately, we can tack on a second, finer abstraction. Specifically,
    involving detecting the named entities *city* and *state*. These would let us
    derive two new sequences.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有足够的帮助，我们可以加上第二个、更精细的抽象。具体来说，涉及检测命名实体*city*和*state*。这些可以让我们得出两个新的序列。
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Even More Nuanced Cases**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**更加细致的案例**'
- en: Below are even more nuanced examples of issues involving commas. These are also
    from [https://en.wikipedia.org/wiki/Shannon_Lucid](https://en.wikipedia.org/wiki/Shannon_Lucid)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是涉及逗号问题的更细致的例子。这些也来自[https://en.wikipedia.org/wiki/Shannon_Lucid](https://en.wikipedia.org/wiki/Shannon_Lucid)
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: These suggest that we probably need to allow for quite long left and right contexts,
    possibly up to 20 words each. And maybe add more abstractions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表明我们可能需要允许相当长的左侧和右侧上下文，可能每侧最多20个单词。也许还要增加更多抽象。
- en: Keeping abstractions aside, how will this impact our model training? First of
    all, since we are learning a complex model, we’ll need our training set to be
    sufficiently large, rich, and diverse. Fortunately, such a data set can be assembled
    without much effort. Download and use all of Wikipedia. See [9].
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 抛开抽象，如何影响我们的模型训练？首先，由于我们在学习一个复杂的模型，我们需要我们的训练集足够大、丰富且多样。幸运的是，这样的数据集可以轻松组建。下载并使用整个维基百科。见[9]。
- en: Okay, now onto training time. This can be large as we have a huge training set
    combined with a complex model we are trying to learn, one involving lots and lots
    of rules. Of course, the learned model itself is potentially huge, with perhaps
    the vast majority of the learned rules turning out to be noisy.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在谈谈训练时间。由于我们有一个庞大的训练集和一个复杂的模型，训练时间可能很长，这个模型涉及许多规则。当然，学习到的模型本身也可能很庞大，学习到的大多数规则可能会变得嘈杂。
- en: Later on in this post, we will discuss these challenges in detail and how to
    mitigate them. In particular, we will posit specific ways to weed out rules that
    are *insufficiently* predictive or those that can be covered with more general
    rules that remain *sufficiently* predictive.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文后续部分，我们将详细讨论这些挑战及其缓解方法。特别是，我们将提出具体的方式来剔除那些*预测能力不足*的规则或那些可以通过更通用且*足够*预测的规则来覆盖的规则。
- en: For now, let’s move on to the next use case, which is
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续下一个用例，即
- en: '**Issues Involving Prepositions Or Other Connectives**'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及介词或其他连接词的问题**'
- en: Now consider these examples, also from [https://en.wikipedia.org/wiki/Shannon_Lucid](https://en.wikipedia.org/wiki/Shannon_Lucid)
    which I have mutated slightly. Specifically, I replaced certain connectives with
    others that are somewhat plausible though not as good.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑这些示例，这些示例也来自[https://en.wikipedia.org/wiki/Shannon_Lucid](https://en.wikipedia.org/wiki/Shannon_Lucid)，我对其做了一些小的修改。具体而言，我将某些连接词替换成了其他稍微合理但不如原本的词。
- en: '[PRE28]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Can you spot the errors and fix them?
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你能发现错误并修复它们吗？
- en: Below are the original, i.e. correct, versions.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是原始的，即正确的版本。
- en: '[PRE29]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: If you did well, so will the method.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你做得很好，那么方法也会很好。
- en: Now to the modeling. We will let **M** denote the set of connectives we wish
    to model. **M** could be defined, for example, by the words tagged as prepositions
    by a certain part-of-speech tagger. Or some other way.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进行建模。我们将让**M**表示我们希望建模的连接词集合。**M**可以由某个词性标注器标记为介词的单词定义。例如，或者其他方式。
- en: Regardless, we will need to ensure that we can determine with certainty and
    reasonably efficiently whether or not a particular token is in **M**.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们需要确保可以准确且合理高效地确定特定标记是否在**M**中。
- en: This is because during training, while scanning a particular text, we will need
    to know, for every word, whether it is an instance of **M** or not.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为在训练过程中，在扫描特定文本时，我们需要知道每个单词是否属于**M**。
- en: To keep things simple, we will leave _**none**_ out of **M**. This means that
    we will only be able to model replacement errors, i.e., using the wrong connective.
    It is easy to add _**none**_ in but it clutters up the description a bit.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们将_**none**_排除在**M**之外。这意味着我们只能模拟替换错误，即使用错误的连接词。虽然很容易将_**none**_添加进来，但这会稍微增加描述的复杂性。
- en: '**Singular Versus Plural**'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**单数与复数**'
- en: Consider these examples, with the words we want to examine for the so-called
    grammatical number highlighted in bold.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这些示例，其中我们想要检查所谓的语法数的单词已用粗体标出。
- en: '[PRE30]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: First off, let’s ask how we would even detect the words in /…/ in an automated
    fashion. Here is a start. We could run a part-of-speech tagger and pick up only
    nouns.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要问一下如何以自动化的方式检测/…/中的单词。这是一个起点。我们可以运行一个词性标注器，只提取名词。
- en: Let’s try this out on our examples. Using the part-of-speech tagger at [https://parts-of-speech.info/](https://parts-of-speech.info/)
    we get
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的示例上试一下。使用[https://parts-of-speech.info/](https://parts-of-speech.info/)上的词性标注器，我们得到
- en: '![](../Images/230734bad82e51007f797cb80760ff5d.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/230734bad82e51007f797cb80760ff5d.png)'
- en: The color codes of the various parts of speech are below.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 各种词性标记的颜色代码如下。
- en: '![](../Images/63e1067603e53f8c2eb8f1ecbb1b3fe4.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63e1067603e53f8c2eb8f1ecbb1b3fe4.png)'
- en: This, while not great, seems good enough to start with. It got *problems*, *contexts*,
    and *words* correctly. It had a false positive, *and*, and a false negative, *set*.
    It also picked up *training* which perhaps we don’t care about.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这种方法虽然不是最佳，但足以作为起点。它正确地标出了*problems*、*contexts*和*words*。它有一个假阳性，*and*，和一个假阴性，*set*。它还标出了*training*，这可能我们并不在意。
- en: As we will discuss in more detail later, while the false positives may yield
    additional irrelevant rules, these will tend not to be harmful, only useless.
    Furthermore, we’ll catch them during the pruning phase.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们稍后将详细讨论的那样，尽管假阳性可能会产生额外的无关规则，但这些规则通常不会有害，只是无用。此外，我们将在修剪阶段捕捉到它们。
- en: That said, if we are concerned about accuracy upfront, we might consider a more
    advanced part-of-speech tagger. Or some other way to refine our detection approach.
    We won’t pursue either in this post.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，如果我们对准确性有较高要求，我们可能会考虑使用更先进的词性标注器，或其他方式来改进我们的检测方法。我们在这篇文章中不会追究这两个方向。
- en: Next, we’ll do a type of preprocessing we haven’t yet had to do in any of our
    use cases discussed thus far. Say the procedure we described in the previous paragraph
    detects a particular word that is the object of our study. By “object of our study”
    we mean whether it should be in singular or in the plural.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行一种在之前讨论的使用案例中尚未遇到的预处理。假设我们在前一段描述的过程中检测到了一个特定的单词，这是我们研究的对象。这里的“研究对象”是指它应为单数还是复数。
- en: Right after we have detected such a word, we will run a grammatical number classifier,
    possibly one using a very simple heuristic such as if the word ends with *s* or
    *ies* deem it plural else deem it singular. Next, we will add _singular_ or _plural_
    to a copy of our text, depending on this classifier’s prediction. Importantly,
    we will also singularize the word which precedes the label.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们检测到这样的词，我们将运行一个语法数分类器，可能使用一个非常简单的启发式方法，例如，如果单词以*s*或*ies*结尾，则视为复数，否则视为单数。接下来，我们将根据该分类器的预测，将
    _singular_ 或 _plural_ 添加到文本的副本中。重要的是，我们还将对紧接着标签之前的单词进行单数化处理。
- en: In our examples, after all of this has been done, and using the part-of-speech
    tagger we used earlier, we will get
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，完成所有这些操作后，使用我们之前使用的词性标注器，我们会得到
- en: '[PRE31]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: So our **M** will be the set { _**singular**_, _**plural**_ }.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们的**M**将是集合{ _**singular**_, _**plural**_ }。
- en: Note that the left context includes the word whose grammatical number we are
    trying to predict. This is by design. This is why we added the labels explicitly
    to the text.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，左侧上下文包括我们试图预测语法数的词。这是有意为之的。这就是为什么我们在文本中明确添加了标签的原因。
- en: Also, note that the words flanked by asterisks are the ones we singularized.
    We did so because these words are in the left context of the label to be predicted.
    We want to strip off any information in the word itself that can be used to predict
    its label. Other than any information inherently in the singularized version of
    the word.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，被星号包围的单词是我们进行单数化处理的单词。我们这样做是因为这些词在待预测标签的左侧上下文中。我们希望去除单词本身中任何可以用来预测其标签的信息。除了单数化版本本身固有的信息。
- en: If we didn’t singularize these words we would have label leakage. This would
    have bad consequences. We might learn rules that seem to be good but don’t work
    well at prediction time.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有对这些词进行单数化处理，就会发生标签泄漏。这将产生不良后果。我们可能会学习到看似有效但在预测时表现不佳的规则。
- en: Next, let’s do a quick review of the text as a sanity check. To assess whether
    or not the contexts seem to have enough signal to at least predict better than
    random. How accurately we can predict the labels will have to await an empirical
    evaluation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们快速回顾一下文本，以作为合理性检查。评估上下文是否似乎有足够的信号来至少比随机预测得更好。我们能够多准确地预测标签，将有待于经验评估。
- en: It does seem that *for some of the problems* predicts _**plural**_. *left and
    right context* would also seem to predict _**plural**_ better than random. How
    much better is hard to say without seeing more examples. Similarly, *Up to 20
    word* would seem to predict _**plural**_. The prediction might possibly improve,
    and certainly generalize better, were we to use the abstraction that 20 is _**integer_greater_than_1**_.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎*对于某些问题*，预测 _**plural**_。*左侧和右侧上下文* 也似乎比随机预测 _**plural**_ 更好。没有看到更多示例，很难说效果有多好。类似地，*最多20个单词*
    似乎也能预测 _**plural**_。如果我们使用将20视作 _**integer_greater_than_1**_ 的抽象，预测可能会有所改善，并且确实会更好地泛化。
- en: '**Model Complexity, Training Time, And Lookup Time**'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型复杂性、训练时间和查找时间**'
- en: As we’ve seen, for some of the problems we are trying to solve, we may need
    long left and right contexts. Up to 20 words each. Perhaps longer.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，对于我们试图解决的某些问题，我们可能需要较长的左侧和右侧上下文。每侧最多20个单词。也许更长。
- en: We’ve also discussed that we’d preferably want a very rich data set for training.
    Such as all of Wikipedia. Our mechanism also relies on abstractions, which amplify
    the size of the training set possibly by another order of magnitude.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论过，我们最好能拥有一个非常丰富的训练数据集。例如，整个维基百科。我们的机制也依赖于抽象，这可能会将训练集的规模放大到另一个数量级。
- en: So is this a show-stopper for our method? Well, no. We can substantially prune
    the size of the model and substantially speed up training. We’ll discuss these
    below individually. We’ll also discuss how to be fast at what we are calling lookup
    time, as it will impact both training time and prediction time.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这会是我们方法的致命问题吗？其实不会。我们可以显著减少模型的大小，并大幅加快训练速度。我们将在下文中逐一讨论这些问题。我们还会讨论如何提高我们称之为查找时间的效率，因为这会影响训练时间和预测时间。
- en: '**Reducing The Model Size**'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**减少模型大小**'
- en: Let’s start with the model size. First off, keep in mind that in modern times
    large-scale real models do use billions of parameters. So we may be okay even
    without any pruning. That said, we’ll cover it anyhow.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从模型大小开始。首先，请记住，在现代，大规模的实际模型确实使用数十亿个参数。因此，即使没有任何剪枝，我们也可能没问题。话虽如此，我们还是会对其进行讨论。
- en: When considering whether a particular rule should be deleted or not, we will
    distinguish between two cases.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑是否删除特定规则时，我们将区分两种情况。
- en: Is the rule insufficiently predictive?
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规则是否预测不足？
- en: Is a more general rule sufficiently predictive compared to this one?
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与这个规则相比，更通用的规则是否足够预测？
- en: Our main reason for distinguishing between these two cases is that we will not
    explicitly prune for the first case. Instead, we will rely on either the second
    case addressing the first one as well or on the prediction algorithm doing on-the-fly
    pruning sufficiently well. With regards to the latter, also note that the prediction
    algorithm takes the cutoff *c* as a parameter, which allows us to get more conservative
    or more sensitive at prediction time.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们区分这两种情况的主要原因是我们不会对第一种情况进行显式剪枝。相反，我们将依赖于第二种情况也能处理第一种情况，或者预测算法能够足够好地进行动态剪枝。关于后者，还要注意预测算法将截止点*c*作为参数，这允许我们在预测时变得更保守或更敏感。
- en: Okay, with that out of the way, let’s address the second case.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，既然这些解决了，我们来处理第二种情况。
- en: To explain this method, let’s start with a learned rule L**M**R that is general
    enough. Here is an example.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释这个方法，让我们从一个足够通用的学习规则L**M**R开始。这是一个例子。
- en: '[PRE32]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We deem it general because the left and the right contexts are a single word
    each.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为它通用，因为左右上下文都是一个单词。
- en: Imagine that in the training corpus, the expression *from a learned model* appears
    at least once somewhere. So we would also have learned the rule
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，在训练语料库中，表达式*from a learned model* 至少出现一次。所以我们也会学习到这个规则。
- en: '[PRE33]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This rule is more specific. So we will deem it a child of the rule
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规则更具体。因此我们将其视为规则的子规则。
- en: '[PRE34]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now that we have defined child-parent relationships we can arrange the rules
    into a tree.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了子父关系，我们可以将规则排列成树状结构。
- en: Now we are ready to describe the pruning criterion. For a particular node *v*
    in the tree, if all its descendants predict the same outcome as *v* does, we will
    prune away all the nodes under *v*’s subtree.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备描述剪枝标准。对于树中的特定节点*v*，如果它的所有后代预测的结果与*v*相同，我们将修剪掉*v*子树下的所有节点。
- en: Let’s apply this to our example. In the setting **M** = {_**a_**, _**an_**,
    _**the_**, _**none**_}, the rule
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其应用于我们的例子。在设置**M** = {_**a**_, _**an**_, _**the**_, _**none**_}中，规则
- en: '[PRE35]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: predicts the same outcome, _**a_**, as does
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 预测相同结果的*_**a**_*，如
- en: '[PRE36]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Furthermore imagine that the latter rule only has one rule, the former one,
    in its subtree. So we prune away the former.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，假设后一个规则在其子树中仅有一个规则，即前一个规则。那么我们将剪除前一个规则。
- en: Okay, we’ve defined the pruning criterion. Next, we discuss how to do the actual
    pruning, i.e. apply the criterion efficiently. *The short answer is bottom-up.*
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们已经定义了剪枝标准。接下来，我们讨论如何实际进行剪枝，即如何高效地应用标准。*简短的答案是自下而上*。
- en: We start with the leaves of the tree and find their parents. We then consider
    each of these parents one by one. We prune away a parent’s children if they all
    predict the same outcome as the parent.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从树的叶子开始，找到它们的父节点。然后，我们逐一考虑这些父节点。如果父节点的所有子节点预测的结果与父节点相同，我们就剪除这些子节点。
- en: We now have a new tree. We repeat this same process on it.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了一棵新树。我们在它上面重复相同的过程。
- en: We stop when we can’t prune further or when we have pruned enough.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们不能再剪枝或剪枝足够时，我们停止。
- en: '**Speeding Up Training**'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**加速训练**'
- en: On the one hand, we only need one pass over the sentences in the training set.
    Moreover, we only need to stop at the tokens that are instances of **M**. To pause
    and update various counters as described earlier. That’s good.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，我们只需对训练集中的句子进行一次遍历。此外，我们只需停留在那些**M**的实例上，以暂停并更新之前描述的各种计数器。这很好。
- en: On the other hand, at a particular stopping point **m**, we may need to enumerate
    all admissible windows L**m**R so we can increment their counters involving **m**.
    For each of these, we also need to derive additional windows based on the abstractions
    we are modeling.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在特定的停止点**m**，我们可能需要枚举所有可接受的窗口L**m**R，以便我们可以增加涉及**m**的计数器。对于这些窗口中的每一个，我们还需要根据我们建模的抽象来推导额外的窗口。
- en: We’ve already discussed how to constrain the abstractions, so we won’t repeat
    that discussion here.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了如何约束抽象，所以在这里我们不会重复讨论。
- en: The key point we’d like to bring out is that pruning the model in the manner
    we described earlier not only reduces the model’s size, it also speeds up subsequent
    training. This is because, at any particular stopping point **m**, there will
    generally be far fewer rules that trigger in the pruned model compared to the
    unpruned one.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望指出的关键点是，以我们之前描述的方式修剪模型不仅减少了模型的大小，还加快了后续的训练。这是因为，在任何特定的停止点**m**，在修剪模型中触发的规则通常会比未修剪的模型少得多。
- en: '**Lookup Time**'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**查找时间**'
- en: By lookup, we mean that we want to efficiently look up rules that apply in a
    particular situation. Let’s start with an example. Say we have learned the rule
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的查找是指我们希望在特定情况下高效地查找适用的规则。我们从一个例子开始。假设我们已经学会了规则
- en: '[PRE37]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: for the issue involving articles. Recall that we chose **M** to be { **a**,
    **an**, **the**, _**none**_ }.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对于涉及冠词的问题。回忆一下我们选择了**M** 为 { **a**、**an**、**the**、_**none**_ }。
- en: Now consider the text *Jeremy is a man.* We want to scan it for issues. We will
    be on *a* since *a* is in **M**. We want to check the following, in order. For
    this **M**, is there a rule with L = [*is*] and R = []? Is there a rule with L
    = [] and R = [*man*]? Is there a rule with L = [*is*] and R = [*man*]? And so
    on. Let’s call “Is there a rule” a look-up. The look-up inputs **M**, L, and R.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑文本*Jeremy is a man.* 我们要扫描它以查找问题。我们将关注*a*，因为*a* 在**M**中。我们要按顺序检查以下内容。对于这个**M**，是否有规则L
    = [*is*] 和 R = []？是否有规则L = [] 和 R = [*man*]？是否有规则L = [*is*] 和 R = [*man*]？等等。我们称“是否有规则”为查找。查找输入**M**、L和R。
- en: We obviously want the lookups to be fast. We can make this happen by indexing
    the set of rules in a hashmap, let’s call it H. H is keyed on the triple (**M**,
    L, R). Think of H as a 3d hashmap, expressed as H[**M**][L][R].
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们显然希望查找能够快速进行。我们可以通过在哈希表中索引规则集来实现这一点，称之为H。H的键是三元组（**M**、L、R）。可以将H视为一个3维哈希表，表示为H[**M**][L][R]。
- en: '**Summary**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this post, we covered elaborate versions of scenarios involving detecting
    and correcting errors in text. By “elaborate” we mean those in which context seems
    important. We covered issues involving missing or incorrect articles, missing
    commas, using singular when it should be plural or the other way around, and using
    the wrong connective such as a wrong preposition.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们涵盖了涉及检测和纠正文本错误的详细场景。我们所说的“详细”是指那些上下文显得重要的情况。我们讨论了缺少或不正确的冠词、缺少逗号、在应使用复数时使用单数或反之亦然、以及使用错误的连词如错误的介词等问题。
- en: We modeled each as a self-supervised learning problem. We described an approach
    that works on all these problems. It is based on a probability distribution on
    the space of outcomes conditioned jointly over a left context and a right context.
    The definition of the outcomes and some preprocessing do depend on the particular
    problem.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每个问题建模为自监督学习问题。我们描述了一种适用于所有这些问题的方法。它基于对结果空间的概率分布，该分布在左上下文和右上下文上共同条件化。结果的定义和一些预处理确实依赖于特定问题。
- en: We discussed enumerating left context, and right context pairs of increasing
    length, and also abstraction mechanisms to learn more general rules.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了枚举左右上下文对及其逐渐增加的长度，以及用于学习更一般规则的抽象机制。
- en: The method we described is straightforward to implement in its basic form.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述的方法在其基本形式上容易实现。
- en: We also described how to prune the set of learned rules, how to speed up training,
    and how to efficiently look up which of the rules apply to a particular situation.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还描述了如何修剪学习到的规则集，如何加速训练，以及如何高效地查找适用于特定情况的规则。
- en: '**References**'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[Text Correction Using NLP. Detecting and correcting common errors… | by Arun
    Jagota | Jan, 2023 | Towards Data Science](/text-correction-using-nlp-b68c7233b86)'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 NLP 的文本修正。检测和纠正常见错误…… | 作者：阿伦·贾戈塔 | 2023年1月 | 数据科学之路](/text-correction-using-nlp-b68c7233b86)'
- en: '[Association rule learning — Wikipedia](https://en.wikipedia.org/wiki/Association_rule_learning)'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[关联规则学习 — 维基百科](https://en.wikipedia.org/wiki/Association_rule_learning)'
- en: '[Grammarly](https://app.grammarly.com/) *I used it extensively. Very useful.*'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Grammarly](https://app.grammarly.com/) *我广泛使用它，非常有用。*'
- en: '[ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/)'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ChatGPT: 优化对话的语言模型](https://openai.com/blog/chatgpt/)'
- en: '[Wikipedia:Database download](https://en.wikipedia.org/wiki/Wikipedia:Database_download)'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[维基百科：数据库下载](https://en.wikipedia.org/wiki/Wikipedia:Database_download)'
- en: '[Statistical Language Models | by Arun Jagota | Towards Data Science | Medium](https://medium.com/towards-data-science/statistical-language-models-4e539d57bcaa)'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[统计语言模型 | 阿伦·贾戈塔 | 数据科学前沿 | Medium](https://medium.com/towards-data-science/statistical-language-models-4e539d57bcaa)'
- en: '[Neural Language Models, Arun Jagota, Towards Data Science, Medium](https://medium.com/towards-data-science/neural-language-models-32bec14d01dc)'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[神经语言模型，阿伦·贾戈塔，数据科学前沿，Medium](https://medium.com/towards-data-science/neural-language-models-32bec14d01dc)'
