- en: 6 Embarrassing Sklearn Mistakes You May Be Making And How to Avoid Them
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šçŠ¯çš„ 6 ä¸ªå°´å°¬çš„ Sklearn é”™è¯¯åŠå¦‚ä½•é¿å…å®ƒä»¬
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/6-embarrassing-sklearn-mistakes-you-may-be-making-and-how-to-avoid-them-6be5bbdbb987](https://towardsdatascience.com/6-embarrassing-sklearn-mistakes-you-may-be-making-and-how-to-avoid-them-6be5bbdbb987)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/6-embarrassing-sklearn-mistakes-you-may-be-making-and-how-to-avoid-them-6be5bbdbb987](https://towardsdatascience.com/6-embarrassing-sklearn-mistakes-you-may-be-making-and-how-to-avoid-them-6be5bbdbb987)
- en: There are no error messages â€” thatâ€™s what makes them subtle
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ²¡æœ‰é”™è¯¯ä¿¡æ¯â€”â€”è¿™å°±æ˜¯å®ƒä»¬å¾®å¦™çš„åŸå› 
- en: '[](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)[![Bex
    T.](../Images/516496f32596e8ad56bf07f178a643c6.png)](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)
    [Bex T.](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)[![Bex
    T.](../Images/516496f32596e8ad56bf07f178a643c6.png)](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)
    [Bex T.](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)
    Â·10 min readÂ·Jun 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒåœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)
    Â·10 åˆ†é’Ÿé˜…è¯»Â·2023å¹´6æœˆ5æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Learn to avoid the six most serious mistakes related to machine learning theory
    that beginners often make through Sklearn.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ Sklearn å­¦ä¹ é¿å…åˆå­¦è€…å¸¸çŠ¯çš„å…­ä¸ªä¸æœºå™¨å­¦ä¹ ç†è®ºç›¸å…³çš„ä¸¥é‡é”™è¯¯ã€‚
- en: '![](../Images/e79383dd47c27048f5947ee350129670.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e79383dd47c27048f5947ee350129670.png)'
- en: Image by me with Leonardo AI
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±æˆ‘ä¸ Leonardo AI åˆ¶ä½œ
- en: Often, Sklearn throws big red error messages and warnings when you make a mistake.
    These messages suggest something is seriously wrong with your code, preventing
    the Sklearn magic from doing its job.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼ŒSklearn åœ¨ä½ çŠ¯é”™æ—¶ä¼šæŠ›å‡ºå¤§çº¢è‰²çš„é”™è¯¯ä¿¡æ¯å’Œè­¦å‘Šã€‚è¿™äº›ä¿¡æ¯è¡¨æ˜ä½ çš„ä»£ç æœ‰ä¸¥é‡é—®é¢˜ï¼Œé˜»æ­¢äº† Sklearn é­”æ³•çš„æ­£å¸¸è¿è¡Œã€‚
- en: But what happens if you donâ€™t get any errors or warnings? Does that mean you
    are crushing it so far? *Not necessarily*. Many knobs and dials make Sklearn the
    greatest ML library, its world-class *code design* being an example.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœä½ æ²¡æœ‰æ”¶åˆ°ä»»ä½•é”™è¯¯æˆ–è­¦å‘Šä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿè¿™æ˜¯å¦æ„å‘³ç€ä½ åšå¾—å¾ˆå¥½ï¼Ÿ*ä¸ä¸€å®š*ã€‚è®¸å¤šæ—‹é’®å’Œæ‹¨ç›˜ä½¿ Sklearn æˆä¸ºæœ€ä¼˜ç§€çš„ ML åº“ï¼Œå…¶ä¸–ç•Œçº§çš„*ä»£ç è®¾è®¡*å°±æ˜¯ä¸€ä¸ªä¾‹å­ã€‚
- en: The mistakes while writing Sklearn code can easily be fixed. What *can* go unnoticed
    is the mistakes related to the *internal logic* and ML theory that powers Sklearn
    algorithms and transformers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–å†™ Sklearn ä»£ç æ—¶çš„é”™è¯¯å¾ˆå®¹æ˜“ä¿®å¤ã€‚*å¯ä»¥*è¢«å¿½è§†çš„æ˜¯ä¸*å†…éƒ¨é€»è¾‘*å’Œæ”¯æŒ Sklearn ç®—æ³•åŠå˜æ¢å™¨çš„ ML ç†è®ºç›¸å…³çš„é”™è¯¯ã€‚
- en: These mistakes are especially more common and subtle when you are a beginner.
    So this post will be about the six such mistakes I made and learned to avoid when
    I was a beginner myself.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›é”™è¯¯åœ¨ä½ æ˜¯åˆå­¦è€…æ—¶å°¤å…¶å¸¸è§ä¸”å¾®å¦™ã€‚å› æ­¤ï¼Œæœ¬æ–‡å°†è®²è¿°æˆ‘åœ¨åˆå­¦è€…é˜¶æ®µçŠ¯çš„å…­ä¸ªé”™è¯¯ä»¥åŠå¦‚ä½•é¿å…å®ƒä»¬ã€‚
- en: 1ï¸âƒ£. Using `fit` or `fit_transform` everywhere
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£. åˆ°å¤„ä½¿ç”¨ `fit` æˆ– `fit_transform`
- en: Letâ€™s start with the most serious mistake â€” a mistake that is related to *data
    leakage*. Data leakage is subtle and can be destructive to model performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»æœ€ä¸¥é‡çš„é”™è¯¯å¼€å§‹â€”â€”è¿™æ˜¯ä¸*æ•°æ®æ³„æ¼*ç›¸å…³çš„é”™è¯¯ã€‚æ•°æ®æ³„æ¼å¾ˆå¾®å¦™ï¼Œå¯èƒ½å¯¹æ¨¡å‹æ€§èƒ½é€ æˆç ´åã€‚
- en: It occurs when information that would not be available at prediction time is
    used during the model training. Data leakage causes models to give very optimistic
    results, even in cross-validation, but perform terribly when testing on *actual*
    novel data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å½“åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨é¢„æµ‹æ—¶æ— æ³•è·å¾—çš„ä¿¡æ¯æ—¶ï¼Œå°±ä¼šå‘ç”Ÿæ•°æ®æ³„æ¼ã€‚æ•°æ®æ³„æ¼ä¼šå¯¼è‡´æ¨¡å‹ç»™å‡ºéå¸¸ä¹è§‚çš„ç»“æœï¼Œå³ä½¿åœ¨äº¤å‰éªŒè¯ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œä½†åœ¨æµ‹è¯•*å®é™…*çš„æ–°æ•°æ®æ—¶è¡¨ç°ä¼šéå¸¸ç³Ÿç³•ã€‚
- en: Data leakage is common during data preprocessing, particularly if the training
    and test sets are not separated. Many Sklearn preprocessing transformers such
    as [imputers](/advanced-missing-data-imputation-methods-with-sklearn-d9875cbcc6eb),
    [normalizers, standardization functions, and log transformers](/how-to-differentiate-between-scaling-normalization-and-log-transformations-69873d365a94)
    tap into the underlying data distribution during the fit time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®æ³„æ¼åœ¨æ•°æ®é¢„å¤„ç†ä¸­å¾ˆå¸¸è§ï¼Œç‰¹åˆ«æ˜¯å½“è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ²¡æœ‰åˆ†å¼€æ—¶ã€‚è®¸å¤š Sklearn é¢„å¤„ç†å˜æ¢å™¨ï¼Œå¦‚ [æ’è¡¥å™¨](/advanced-missing-data-imputation-methods-with-sklearn-d9875cbcc6eb)ï¼Œ[å½’ä¸€åŒ–å™¨ã€æ ‡å‡†åŒ–å‡½æ•°å’Œå¯¹æ•°å˜æ¢å™¨](/how-to-differentiate-between-scaling-normalization-and-log-transformations-69873d365a94)ï¼Œåœ¨æ‹ŸåˆæœŸé—´ä¼šæ¥è§¦åˆ°åº•å±‚çš„æ•°æ®åˆ†å¸ƒã€‚
- en: For example, `StandardScaler` normalizes the data by subtracting the mean from
    each sample and dividing it by the standard deviation. Calling the `fit()` function
    on the full data (X) allows the transformer to learn the mean and standard deviation
    of the whole distribution of each feature.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œ`StandardScaler`é€šè¿‡ä»æ¯ä¸ªæ ·æœ¬ä¸­å‡å»å‡å€¼å¹¶é™¤ä»¥æ ‡å‡†å·®æ¥å½’ä¸€åŒ–æ•°æ®ã€‚å¯¹å®Œæ•´æ•°æ®ï¼ˆXï¼‰è°ƒç”¨`fit()`å‡½æ•°å…è®¸å˜æ¢å™¨å­¦ä¹ æ¯ä¸ªç‰¹å¾çš„æ•´ä¸ªåˆ†å¸ƒçš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚
- en: If this data is split into train and test sets **after** transformation, the
    train set would be *contaminated* because `StandardScaler` leaked important information
    from the actual distribution.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¿™äº›æ•°æ®åœ¨å˜æ¢**ä¹‹å**è¢«æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œé‚£ä¹ˆè®­ç»ƒé›†å°†ä¼šè¢«*æ±¡æŸ“*ï¼Œå› ä¸º`StandardScaler`æ³„éœ²äº†å®é™…åˆ†å¸ƒä¸­çš„é‡è¦ä¿¡æ¯ã€‚
- en: Even though this might not be apparent to us, Sklearn algorithms are powerful
    enough to notice this and take advantage during testing. In other words, the train
    data would be too perfect for the model because it has useful information about
    the test set, and the test would not be novel enough to test the modelâ€™s performance
    on actual unseen data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™å¯¹æˆ‘ä»¬å¯èƒ½ä¸æ˜æ˜¾ï¼Œä½† Sklearn ç®—æ³•è¶³å¤Ÿå¼ºå¤§ï¼Œèƒ½å¤Ÿå¯Ÿè§‰åˆ°è¿™ä¸€ç‚¹å¹¶åœ¨æµ‹è¯•ä¸­åˆ©ç”¨å®ƒã€‚æ¢å¥è¯è¯´ï¼Œè®­ç»ƒæ•°æ®å¯¹æ¨¡å‹æ¥è¯´å¯èƒ½è¿‡äºå®Œç¾ï¼Œå› ä¸ºå®ƒåŒ…å«äº†å…³äºæµ‹è¯•é›†çš„æœ‰ç”¨ä¿¡æ¯ï¼Œè€Œæµ‹è¯•é›†å¯¹äºæ¨¡å‹åœ¨å®é™…æœªè§æ•°æ®ä¸Šçš„è¡¨ç°æµ‹è¯•å¯èƒ½ä¸å¤Ÿæ–°é¢–ã€‚
- en: The easiest solution is to never call `fit` on the full data. Before doing any
    preprocessing, always split the data into train and test sets. Even after the
    split, you should never call `fit` or `fit_transform` on the test set because
    you will end up with the same problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯ç»ä¸è¦åœ¨å…¨éƒ¨æ•°æ®ä¸Šè°ƒç”¨`fit`ã€‚åœ¨è¿›è¡Œä»»ä½•é¢„å¤„ç†ä¹‹å‰ï¼Œå§‹ç»ˆå°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚å³ä½¿åœ¨æ‹†åˆ†ä¹‹åï¼Œä½ ä¹Ÿç»ä¸è¦åœ¨æµ‹è¯•é›†ä¸Šè°ƒç”¨`fit`æˆ–`fit_transform`ï¼Œå¦åˆ™ä½ å°†é‡åˆ°ç›¸åŒçš„é—®é¢˜ã€‚
- en: Since both train and test sets should receive the same preprocessing steps,
    a golden rule is to use `fit_transform` on the train data - this ensures that
    the transformer learns from the train set only and transforms it simultaneously.
    Then, call the `transform` method on the test set to transform it based on the
    information learned from the training data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½åº”æ¥å—ç›¸åŒçš„é¢„å¤„ç†æ­¥éª¤ï¼Œä¸€ä¸ªé»„é‡‘æ³•åˆ™æ˜¯å¯¹è®­ç»ƒæ•°æ®ä½¿ç”¨`fit_transform`â€”â€”è¿™ç¡®ä¿äº†å˜æ¢å™¨ä»…ä»è®­ç»ƒé›†ä¸­å­¦ä¹ å¹¶åŒæ—¶è¿›è¡Œè½¬æ¢ã€‚ç„¶åï¼Œåœ¨æµ‹è¯•é›†ä¸Šè°ƒç”¨`transform`æ–¹æ³•ï¼Œæ ¹æ®ä»è®­ç»ƒæ•°æ®ä¸­å­¦åˆ°çš„ä¿¡æ¯è¿›è¡Œè½¬æ¢ã€‚
- en: 'A more robust solution would be using Sklearnâ€™s built-in pipelines. Pipeline
    classes are specifically built to guard algorithms against data leakage. Using
    pipelines ensures that the training data is used during `fit` and the test data
    is used only for calculations. You can learn about them in detail in my separate
    article:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨ Sklearn çš„å†…ç½®ç®¡é“ã€‚ç®¡é“ç±»ä¸“é—¨ç”¨äºä¿æŠ¤ç®—æ³•å…å—æ•°æ®æ³„æ¼çš„å½±å“ã€‚ä½¿ç”¨ç®¡é“å¯ä»¥ç¡®ä¿è®­ç»ƒæ•°æ®ä»…åœ¨`fit`æœŸé—´ä½¿ç”¨ï¼Œè€Œæµ‹è¯•æ•°æ®ä»…ç”¨äºè®¡ç®—ã€‚ä½ å¯ä»¥åœ¨æˆ‘çš„å¦ä¸€ç¯‡æ–‡ç« ä¸­è¯¦ç»†äº†è§£å®ƒä»¬ï¼š
- en: '[](/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=post_page-----6be5bbdbb987--------------------------------)
    [## How to Use Sklearn Pipelines For Ridiculously Neat Code'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=post_page-----6be5bbdbb987--------------------------------)
    [## å¦‚ä½•ä½¿ç”¨ Sklearn ç®¡é“ç¼–å†™æç®€ä»£ç '
- en: Edit description
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼–è¾‘æè¿°
- en: towardsdatascience.com](/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=post_page-----6be5bbdbb987--------------------------------)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=post_page-----6be5bbdbb987--------------------------------)'
- en: 2ï¸âƒ£. Judging Model Performance Only By Test Scores
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£. ä»…é€šè¿‡æµ‹è¯•åˆ†æ•°è¯„ä¼°æ¨¡å‹æ€§èƒ½
- en: You got a test score over 0.85 â€” should you be celebrating? Big, fat NO!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è·å¾—äº†è¶…è¿‡0.85çš„æµ‹è¯•åˆ†æ•°â€”â€”è¿™å€¼å¾—åº†ç¥å—ï¼Ÿå¤§å¤§çš„â€œä¸â€ï¼
- en: Even though high test scores generally mean robust performance, there are important
    caveats to interpreting test results. First and most importantly, regardless of
    the value, test scores should only be judged based on the score you get from training.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡é«˜æµ‹è¯•å¾—åˆ†é€šå¸¸æ„å‘³ç€é²æ£’çš„è¡¨ç°ï¼Œä½†è§£è¯»æµ‹è¯•ç»“æœæ—¶æœ‰ä¸€äº›é‡è¦çš„è­¦å‘Šã€‚é¦–å…ˆä¹Ÿæ˜¯æœ€é‡è¦çš„ï¼Œæ— è®ºå¾—åˆ†å€¼å¦‚ä½•ï¼Œæµ‹è¯•å¾—åˆ†åº”ä»…æ ¹æ®ä½ ä»è®­ç»ƒä¸­å¾—åˆ°çš„åˆ†æ•°æ¥åˆ¤æ–­ã€‚
- en: The only time you should be happy with your model is when the training score
    is higher than the test score, and both are high enough to satisfy the expectations
    of your unique case. However, this does not imply that the higher the difference
    between train and test scores, the better.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å”¯ä¸€åº”è¯¥å¯¹ä½ çš„æ¨¡å‹æ„Ÿåˆ°æ»¡æ„çš„æ—¶å€™ï¼Œæ˜¯å½“è®­ç»ƒå¾—åˆ†é«˜äºæµ‹è¯•å¾—åˆ†ï¼Œå¹¶ä¸”ä¸¤è€…éƒ½è¶³å¤Ÿé«˜ä»¥æ»¡è¶³ä½ ç‹¬ç‰¹æƒ…å†µçš„æœŸæœ›ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ„å‘³ç€è®­ç»ƒå¾—åˆ†å’Œæµ‹è¯•å¾—åˆ†ä¹‹é—´çš„å·®å¼‚è¶Šå¤§è¶Šå¥½ã€‚
- en: For example, 0.85 training score and 0.8 test score suggest a good model that
    is neither overfit nor underfit. But, if the training score is over 0.9 and the
    test score is 0.8, your model is overfitting. Instead of generalizing during training,
    the model memorized some of the training data resulting in a much lower test score.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œ0.85çš„è®­ç»ƒå¾—åˆ†å’Œ0.8çš„æµ‹è¯•å¾—åˆ†è¡¨æ˜æ¨¡å‹æ—¢ä¸è¿‡æ‹Ÿåˆä¹Ÿä¸æ¬ æ‹Ÿåˆã€‚ä½†å¦‚æœè®­ç»ƒå¾—åˆ†è¶…è¿‡0.9è€Œæµ‹è¯•å¾—åˆ†ä¸º0.8ï¼Œä½ çš„æ¨¡å‹å°±æ˜¯è¿‡æ‹Ÿåˆäº†ã€‚æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰è¿›è¡Œæ³›åŒ–ï¼Œè€Œæ˜¯è®°ä½äº†éƒ¨åˆ†è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æµ‹è¯•å¾—åˆ†å¤§å¹…ä¸‹é™ã€‚
- en: You will often see such cases with tree-based and [ensemble models](/beginners-guide-to-xgboost-for-classification-problems-50f75aac5390).
    For example, algorithms such as Random Forest tend to achieve very high training
    scores if their tree depth is not controlled, leading to overfitting. You can
    read [this discussion](https://stats.stackexchange.com/questions/156694/how-can-training-and-testing-error-comparisons-be-indicative-of-overfitting?noredirect=1&lq=1)
    on StackExchange to learn more about the difference between train and test scores.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¸¸å¸¸ä¼šåœ¨åŸºäºæ ‘çš„å’Œ[é›†æˆæ¨¡å‹](/beginners-guide-to-xgboost-for-classification-problems-50f75aac5390)ä¸­çœ‹åˆ°è¿™ç§æƒ…å†µã€‚ä¾‹å¦‚ï¼Œåƒéšæœºæ£®æ—è¿™æ ·çš„ç®—æ³•å¦‚æœä¸æ§åˆ¶æ ‘çš„æ·±åº¦ï¼Œå¾€å¾€ä¼šè·å¾—éå¸¸é«˜çš„è®­ç»ƒå¾—åˆ†ï¼Œå¯¼è‡´è¿‡æ‹Ÿåˆã€‚ä½ å¯ä»¥é˜…è¯»[è¿™ä¸ªè®¨è®º](https://stats.stackexchange.com/questions/156694/how-can-training-and-testing-error-comparisons-be-indicative-of-overfitting?noredirect=1&lq=1)æ¥äº†è§£è®­ç»ƒå’Œæµ‹è¯•å¾—åˆ†ä¹‹é—´çš„å·®å¼‚ã€‚
- en: There is also the case where the test score is higher than the train. If the
    test score is higher than training even in the slightest, feel alarmed because
    you made a mistake! The major cause of such scenarios is data leakage, and we
    discussed an example of that in the last section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸€ç§æƒ…å†µæ˜¯æµ‹è¯•å¾—åˆ†é«˜äºè®­ç»ƒå¾—åˆ†ã€‚å¦‚æœæµ‹è¯•å¾—åˆ†å³ä½¿ç•¥å¾®é«˜äºè®­ç»ƒå¾—åˆ†ï¼Œä¹Ÿè¦æ„Ÿåˆ°è­¦æƒ•ï¼Œå› ä¸ºä½ çŠ¯äº†é”™è¯¯ï¼è¿™ç§æƒ…å†µçš„ä¸»è¦åŸå› æ˜¯æ•°æ®æ³„éœ²ï¼Œæˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ä¸­è®¨è®ºäº†ä¸€ä¸ªä¾‹å­ã€‚
- en: Sometimes, it is also possible to get a good training score and an extremely
    low testing score. When the difference between train and test scores is huge,
    the problem will often be associated with *the test set* rather than overfitting.
    This might happen by using different preprocessing steps for the train and test
    sets or simply forgetting to apply preprocessing to the test set.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶å€™ï¼Œå³ä½¿åœ¨è®­ç»ƒé›†ä¸Šå–å¾—äº†è‰¯å¥½çš„è¯„åˆ†ï¼Œæµ‹è¯•é›†ä¸Šçš„è¯„åˆ†ä¹Ÿå¯èƒ½æä½ã€‚å½“è®­ç»ƒå’Œæµ‹è¯•å¾—åˆ†ä¹‹é—´çš„å·®å¼‚å¾ˆå¤§æ—¶ï¼Œè¿™ä¸ªé—®é¢˜å¾€å¾€ä¸*æµ‹è¯•é›†*æœ‰å…³ï¼Œè€Œä¸æ˜¯è¿‡æ‹Ÿåˆã€‚è¿™å¯èƒ½æ˜¯ç”±äºå¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä½¿ç”¨äº†ä¸åŒçš„é¢„å¤„ç†æ­¥éª¤ï¼Œæˆ–è€…ä»…ä»…æ˜¯å¿˜è®°å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„å¤„ç†æ‰€å¯¼è‡´çš„ã€‚
- en: In summary, always examine the gap between train and test scores closely. Doing
    so will tell you whether you should apply [regularization](/intro-to-regularization-with-ridge-and-lasso-regression-with-sklearn-edcf4c117b7a)
    to overcome overfitting, look for possible mistakes you made during [preprocessing](https://towardsdev.com/data-type-constraints-data-range-constraints-duplicate-data-with-pandas-44897a350b1e)
    or the best-case scenario, prepare the model for final evaluation and deployment.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œå§‹ç»ˆä»”ç»†æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•å¾—åˆ†ä¹‹é—´çš„å·®è·ã€‚è¿™å°†å‘Šè¯‰ä½ æ˜¯å¦åº”è¯¥åº”ç”¨[æ­£åˆ™åŒ–](/intro-to-regularization-with-ridge-and-lasso-regression-with-sklearn-edcf4c117b7a)ä»¥å…‹æœè¿‡æ‹Ÿåˆï¼Œå¯»æ‰¾åœ¨[é¢„å¤„ç†](https://towardsdev.com/data-type-constraints-data-range-constraints-duplicate-data-with-pandas-44897a350b1e)ä¸­å¯èƒ½çŠ¯çš„é”™è¯¯ï¼Œæˆ–è€…åœ¨æœ€ä½³æƒ…å†µä¸‹ï¼Œä¸ºæœ€ç»ˆè¯„ä¼°å’Œéƒ¨ç½²å‡†å¤‡æ¨¡å‹ã€‚
- en: 3ï¸âƒ£. Generating Incorrect Train/Test Sets in Classification
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£. åœ¨åˆ†ç±»ä¸­ç”Ÿæˆä¸æ­£ç¡®çš„è®­ç»ƒ/æµ‹è¯•é›†
- en: A common mistake among beginners is forgetting to generate *stratified* train
    and test sets for classification.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå­¦è€…å¸¸è§çš„ä¸€ä¸ªé”™è¯¯æ˜¯å¿˜è®°ä¸ºåˆ†ç±»ä»»åŠ¡ç”Ÿæˆ*åˆ†å±‚*çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
- en: A model is more likely to generate correct predictions when the new data distribution
    matches trainingâ€™s as much as possible. In classification, we only care about
    the class weights or proportions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ–°æ•°æ®åˆ†å¸ƒå°½å¯èƒ½åŒ¹é…è®­ç»ƒæ•°æ®æ—¶ï¼Œæ¨¡å‹æ›´å¯èƒ½äº§ç”Ÿæ­£ç¡®çš„é¢„æµ‹ã€‚åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬åªå…³å¿ƒç±»åˆ«æƒé‡æˆ–æ¯”ä¾‹ã€‚
- en: For example, in a 3-class classification problem, the class weights are 0.4,
    0.3, 0.3\. When we divide this data into train and test sets, the distributions
    of both sets should reflect the distribution of the full data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'We commonly use `train_test_split` function of Sklearn to divide the data and
    Sklearn provides a handy argument - `stratify` to generate [stratified splits](/how-to-master-the-subtle-art-of-train-test-set-generation-7a8408bcd578).
    Here is an example of train/test sets with and without stratified splits:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Look at class weights before splitting
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Check the class weights again in both train and test sets without stratifying.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, both train and test sets have different class weights for the
    first and second classes. Letâ€™s fix that:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Check the train/test set class weights after stratified splitting.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Setting `stratify` to the target (`y`) yielded identical class weights in both
    the train and test sets.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Altered class weights are a serious problem that might make a model more biased
    towards a particular class. Forgetting to generate stratified splits might result
    in a more favorable train or test sets or cause problems such as these:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92e183a04e8bb7963ae715a54d926c5b.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The above is the performance of a KNN classifier I built when I started learning
    Sklearn. As you can see, almost all test scores are higher than training because
    I had forgotten to generate stratified splits. As a result, the test set yielded
    too favorable a distribution for my model to take advantage of.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'After fixing the problem:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59000bfb1f595a56c6420f648433704f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: It is all back to normal.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: When using cross-validation or [pipelines](/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d),
    you donâ€™t have to worry about this problem because CV splitters perform stratification
    under the hood using `[StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)`
    for classification problems.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 4ï¸âƒ£. Using `LabelEcoder` to Encode the X array
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ever got annoyed when you found out that `[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)`
    encodes categorical columns only one at a time? Compared to other text transformers,
    such as `[OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)`
    which can transform multiple features simultaneously, this seems kind of a letdownğŸ˜”
    by Sklearn.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'But I am here to tell you that it isnâ€™t! It is simply the result of our unwillingness
    to read the documentation. Here is an excerpt of `LabelEncoder`''s 2-sentence
    documentation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: This transformer should be used to encode target values, i.e. `*y*`, and not
    the input `*X*`.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Then, what do we use to encode ordinal text features? If we kindly move on
    to the Sklearn user guide on *encoding categorical features*, we will see that
    it clearly states:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: To convert categorical features to integer codes, we can use the `*OrdinalEncoder*`.
    This estimator transforms each categorical feature to one new feature of integers
    (0 to n_categories - 1)
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Using `[OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)`
    allows us to transform multiple columns at once as expected, and it has the benefit
    of being able to integrate into Pipeline instances, which `LabelEncoder` cannot.
    The encoder follows the familiar transformer API of Sklearn:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: You can learn a lot about Sklearn by just reading the documentation and the
    user guide!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 5ï¸âƒ£. Judging Model Performance Without Cross-validation
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I think you are already comfortable with the topic of overfitting. It is such
    a pressing issue in machine learning that countless techniques have been devised
    to overcome it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The most basic one is holding out a part of the data as a test set to simulate
    and measure a modelâ€™s performance on unseen data. However, hyperparameters of
    the models can be tweaked until the model reaches the maximum score on that particular
    test set, which again means overfitting.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: We might take another part of the full data as a â€˜validation setâ€™ to go around
    this once again. A model would be trained on the training data, fine-tune its
    performance on the validation set and run it through the test set for final evaluation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: But dividing our precious data into 3 sets would mean a smaller amount of data
    a model can learn from. The whole performance of the model would depend on that
    particular pair of train and validation set.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'So, ML practitioners commonly use a procedure called K-fold cross-validation
    (CV for short). Depending on its value, the full data is divided into *K* sets
    called folds, and for each fold, the model would use the *K-1* number of folds
    as training data and the rest as a testing set. After the CV is done, the model
    will have been trained and tested on all data. Here is the diagram of this process
    by Sklearn for 5-fold CV:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b972b2905eafa1295d79116097a98acd.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: Image by Sklearn user guide
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of cross-validation is that it completely takes the randomness
    out of the question. In other words, you wonâ€™t have to worry that `train_test_split`
    accidentally generates too favorable train and test sets that bias the objective
    function of your model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[](/one-stop-tutorial-on-all-cross-validation-techniques-you-can-should-use-7e1645fb703c?source=post_page-----6be5bbdbb987--------------------------------)
    [## One-Stop Tutorial On ALL Cross-Validation Techniques You Can (Should) Use'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: All CV procedures you need to know as a data scientist, explained
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/one-stop-tutorial-on-all-cross-validation-techniques-you-can-should-use-7e1645fb703c?source=post_page-----6be5bbdbb987--------------------------------)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about implementing CV in code from the [official user guide](https://scikit-learn.org/stable/modules/cross_validation.html).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 6ï¸âƒ£. Using Accuracy as a Metric to Evaluate the Performance of Classifiers
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6ï¸âƒ£. ä½¿ç”¨å‡†ç¡®ç‡ä½œä¸ºè¯„ä¼°åˆ†ç±»å™¨æ€§èƒ½çš„æŒ‡æ ‡
- en: By default, all Sklearn classifiers use *accuracy* as a scoring method when
    we call `.score` function. Because of this easy access to the metric, it is common
    to see beginners using it extensively to judge the performance of their model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰Sklearnåˆ†ç±»å™¨åœ¨è°ƒç”¨`.score`å‡½æ•°æ—¶ä½¿ç”¨*å‡†ç¡®ç‡*ä½œä¸ºè¯„åˆ†æ–¹æ³•ã€‚ç”±äºè¿™ç§æŒ‡æ ‡çš„æ˜“å¾—æ€§ï¼Œåˆå­¦è€…é€šå¸¸ä¼šå¹¿æ³›ä½¿ç”¨å®ƒæ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: Unfortunately, the vanilla *accuracy* *score* is useful in only one scenario
    â€” a [binary classification problem](/how-to-tune-models-like-a-puppet-master-based-on-confusion-matrix-fd488f9b5e65)
    with equal, balanced class weights.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼ŒåŸå§‹çš„*å‡†ç¡®ç‡* *è¯„åˆ†*ä»…åœ¨ä¸€ç§åœºæ™¯ä¸‹æœ‰ç”¨â€”â€”ä¸€ä¸ª[äºŒåˆ†ç±»é—®é¢˜](/how-to-tune-models-like-a-puppet-master-based-on-confusion-matrix-fd488f9b5e65)ï¼Œå³å…·æœ‰ç›¸ç­‰ã€å¹³è¡¡çš„ç±»åˆ«æƒé‡ã€‚
- en: Other times, it is such a misleading metric that even the worst-performing models
    can hide behind high accuracy scores. For example, if the model detects spam emails,
    it can reach over 90% accuracy without even finding a single spam email.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶ï¼Œå®ƒæ˜¯ä¸€ä¸ªæå…·è¯¯å¯¼æ€§çš„æŒ‡æ ‡ï¼Œç”šè‡³æœ€å·®çš„æ¨¡å‹ä¹Ÿå¯ä»¥é€šè¿‡é«˜å‡†ç¡®ç‡æ©ç›–å…¶çœŸæ­£çš„è¡¨ç°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ¨¡å‹æ£€æµ‹åƒåœ¾é‚®ä»¶ï¼Œå®ƒå¯ä»¥åœ¨æ²¡æœ‰å‘ç°ä»»ä½•åƒåœ¾é‚®ä»¶çš„æƒ…å†µä¸‹è¾¾åˆ°è¶…è¿‡90%çš„å‡†ç¡®ç‡ã€‚
- en: Why? As spam emails are not as common, the classifier can detect all non-spam
    emails, which can boost its accuracy even though the classifier completely fails
    at its purpose â€” detecting spam!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆï¼Ÿç”±äºåƒåœ¾é‚®ä»¶å¹¶ä¸å¸¸è§ï¼Œåˆ†ç±»å™¨å¯ä»¥æ£€æµ‹æ‰€æœ‰éåƒåœ¾é‚®ä»¶ï¼Œè¿™å¯ä»¥æå‡å…¶å‡†ç¡®ç‡ï¼Œå³ä½¿åˆ†ç±»å™¨å®Œå…¨æœªèƒ½å®ç°å…¶ç›®çš„â€”â€”æ£€æµ‹åƒåœ¾é‚®ä»¶ï¼
- en: This problem is even worse for [multiclass classification](/comprehensive-guide-to-multiclass-classification-with-sklearn-127cc500f362).
    If you achieve 80% accuracy, does it mean the model is more accurate at detecting
    class 1, class 2, class 3, or even all classes?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº[å¤šåˆ†ç±»é—®é¢˜](/comprehensive-guide-to-multiclass-classification-with-sklearn-127cc500f362)æ¥è¯´ï¼Œè¿™ä¸ªé—®é¢˜æ›´ä¸ºä¸¥é‡ã€‚å¦‚æœä½ è·å¾—äº†80%çš„å‡†ç¡®ç‡ï¼Œè¿™æ˜¯å¦æ„å‘³ç€æ¨¡å‹åœ¨æ£€æµ‹ç±»åˆ«1ã€ç±»åˆ«2ã€ç±»åˆ«3ï¼Œè¿˜æ˜¯æ‰€æœ‰ç±»åˆ«ä¸Šéƒ½æ›´å‡†ç¡®ï¼Ÿ
- en: 'Accuracy can never answer such questions, but thankfully, multiple other classification
    metrics give a much more informative performance summary. You can read about them
    in my separate post, where I discuss metrics that apply to both binary and multiclass
    problems:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†ç¡®ç‡æ— æ³•å›ç­”æ­¤ç±»é—®é¢˜ï¼Œä½†å¹¸è¿çš„æ˜¯ï¼Œè¿˜æœ‰å¤šç§å…¶ä»–åˆ†ç±»æŒ‡æ ‡å¯ä»¥æä¾›æ›´æœ‰ä¿¡æ¯é‡çš„æ€§èƒ½æ€»ç»“ã€‚ä½ å¯ä»¥åœ¨æˆ‘å¦ä¸€ç¯‡æ–‡ç« ä¸­é˜…è¯»è¿™äº›æŒ‡æ ‡çš„ä»‹ç»ï¼Œæ–‡ç« è®¨è®ºäº†é€‚ç”¨äºäºŒåˆ†ç±»å’Œå¤šåˆ†ç±»é—®é¢˜çš„æŒ‡æ ‡ï¼š
- en: '[](/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd?source=post_page-----6be5bbdbb987--------------------------------)
    [## Comprehensive Guide on Multiclass Classification Metrics'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd?source=post_page-----6be5bbdbb987--------------------------------)
    [## å¤šåˆ†ç±»æŒ‡æ ‡ç»¼åˆæŒ‡å—'
- en: Edit description
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼–è¾‘æè¿°
- en: towardsdatascience.com](/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd?source=post_page-----6be5bbdbb987--------------------------------)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd?source=post_page-----6be5bbdbb987--------------------------------)'
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Making embarrassing mistakes certainly sucks but it is all part of the journey.
    Even the most experienced folks will admit to making blunders that had made them
    go red to the roots of their hair in their own time.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¯é”™ç¡®å®å¾ˆå°´å°¬ï¼Œä½†è¿™æ˜¯æ—…ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚å³ä½¿æ˜¯æœ€æœ‰ç»éªŒçš„äººä¹Ÿä¼šæ‰¿è®¤ä»–ä»¬åœ¨è‡ªå·±çš„æ—¶é—´é‡Œæ›¾çŠ¯è¿‡è®©ä»–ä»¬è„¸çº¢çš„é”™è¯¯ã€‚
- en: But all of them had someone who finally pointed out their errors and showed
    the proper way of handling things. If you were making any of the mistakes above,
    I hope I became that someone for you.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ‰€æœ‰è¿™äº›æŒ‡æ ‡éƒ½æœ‰æŸäº›äººæœ€ç»ˆæŒ‡å‡ºäº†å®ƒä»¬çš„é”™è¯¯å¹¶å±•ç¤ºäº†æ­£ç¡®çš„å¤„ç†æ–¹å¼ã€‚å¦‚æœä½ æ›¾ç»çŠ¯è¿‡ä¸Šè¿°é”™è¯¯ï¼Œæˆ‘å¸Œæœ›æˆ‘æˆä¸ºäº†ä½ å¿ƒç›®ä¸­çš„é‚£ä¸ªäººã€‚
- en: Thank you for reading!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼
- en: Loved this article and, letâ€™s face it, its bizarre writing style? Imagine having
    access to dozens more just like it, all written by a brilliant, charming, witty
    author (thatâ€™s me, by the way :).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å–œæ¬¢è¿™ç¯‡æ–‡ç« å—ï¼Ÿè®©æˆ‘ä»¬é¢å¯¹ç°å®ï¼Œå®ƒé‚£å¥‡ç‰¹çš„å†™ä½œé£æ ¼ï¼Ÿæƒ³è±¡ä¸€ä¸‹ï¼Œèƒ½è®¿é—®æ›´å¤šç±»ä¼¼çš„æ–‡ç« ï¼Œéƒ½æ˜¯ç”±ä¸€ä½æ‰åæ¨ªæº¢ã€è¿·äººã€é£è¶£çš„ä½œè€…ï¼ˆé¡ºä¾¿è¯´ä¸€å¥ï¼Œå°±æ˜¯æˆ‘ :)ï¼‰æ’°å†™çš„ã€‚
- en: For only 4.99$ membership, you will get access to not just my stories, but a
    treasure trove of knowledge from the best and brightest minds on Medium. And if
    you use [my referral link](https://ibexorigin.medium.com/membership), you will
    earn my supernova of gratitude and a virtual high-five for supporting my work.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…éœ€4.99ç¾å…ƒçš„ä¼šå‘˜èµ„æ ¼ï¼Œä½ å°†ä¸ä»…èƒ½è®¿é—®æˆ‘çš„æ•…äº‹ï¼Œè¿˜èƒ½è·å¾—æ¥è‡ªMediumä¸Šæœ€èªæ˜æ‰æ™ºçš„å®è´µçŸ¥è¯†ã€‚å¦‚æœä½ ä½¿ç”¨[æˆ‘çš„æ¨èé“¾æ¥](https://ibexorigin.medium.com/membership)ï¼Œä½ å°†è·å¾—æˆ‘çš„è¶…çº§æ„Ÿæ¿€å’Œä¸€ä¸ªè™šæ‹Ÿçš„å‡»æŒï¼Œæ„Ÿè°¢ä½ å¯¹æˆ‘å·¥ä½œçš„æ”¯æŒã€‚
- en: '[](https://ibexorigin.medium.com/membership?source=post_page-----6be5bbdbb987--------------------------------)
    [## Join Medium with my referral link â€” Bex T.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ibexorigin.medium.com/membership?source=post_page-----6be5bbdbb987--------------------------------)
    [## ä½¿ç”¨æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥Medium â€” Bex T.'
- en: Get exclusive access to all my âš¡premiumâš¡ content and all over Medium without
    limits. Support my work by buying me aâ€¦
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è·å–å¯¹æˆ‘æ‰€æœ‰âš¡ä¼˜è´¨âš¡å†…å®¹çš„ç‹¬å®¶è®¿é—®æƒé™ï¼Œå¹¶åœ¨Mediumä¸Šæ— é™åˆ¶é˜…è¯»ã€‚é€šè¿‡è´­ä¹°æˆ‘çš„â€¦
- en: ibexorigin.medium.com](https://ibexorigin.medium.com/membership?source=post_page-----6be5bbdbb987--------------------------------)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[ibexorigin.medium.com](https://ibexorigin.medium.com/membership?source=post_page-----6be5bbdbb987--------------------------------)
    æ”¯æŒæˆ‘çš„å·¥ä½œ'
