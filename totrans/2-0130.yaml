- en: 6 Embarrassing Sklearn Mistakes You May Be Making And How to Avoid Them
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你可能会犯的 6 个尴尬的 Sklearn 错误及如何避免它们
- en: 原文：[https://towardsdatascience.com/6-embarrassing-sklearn-mistakes-you-may-be-making-and-how-to-avoid-them-6be5bbdbb987](https://towardsdatascience.com/6-embarrassing-sklearn-mistakes-you-may-be-making-and-how-to-avoid-them-6be5bbdbb987)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/6-embarrassing-sklearn-mistakes-you-may-be-making-and-how-to-avoid-them-6be5bbdbb987](https://towardsdatascience.com/6-embarrassing-sklearn-mistakes-you-may-be-making-and-how-to-avoid-them-6be5bbdbb987)
- en: There are no error messages — that’s what makes them subtle
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 没有错误信息——这就是它们微妙的原因
- en: '[](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)[![Bex
    T.](../Images/516496f32596e8ad56bf07f178a643c6.png)](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)
    [Bex T.](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)[![Bex
    T.](../Images/516496f32596e8ad56bf07f178a643c6.png)](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)
    [Bex T.](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)
    ·10 min read·Jun 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)
    ·10 分钟阅读·2023年6月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Learn to avoid the six most serious mistakes related to machine learning theory
    that beginners often make through Sklearn.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Sklearn 学习避免初学者常犯的六个与机器学习理论相关的严重错误。
- en: '![](../Images/e79383dd47c27048f5947ee350129670.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e79383dd47c27048f5947ee350129670.png)'
- en: Image by me with Leonardo AI
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由我与 Leonardo AI 制作
- en: Often, Sklearn throws big red error messages and warnings when you make a mistake.
    These messages suggest something is seriously wrong with your code, preventing
    the Sklearn magic from doing its job.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，Sklearn 在你犯错时会抛出大红色的错误信息和警告。这些信息表明你的代码有严重问题，阻止了 Sklearn 魔法的正常运行。
- en: But what happens if you don’t get any errors or warnings? Does that mean you
    are crushing it so far? *Not necessarily*. Many knobs and dials make Sklearn the
    greatest ML library, its world-class *code design* being an example.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你没有收到任何错误或警告会发生什么？这是否意味着你做得很好？*不一定*。许多旋钮和拨盘使 Sklearn 成为最优秀的 ML 库，其世界级的*代码设计*就是一个例子。
- en: The mistakes while writing Sklearn code can easily be fixed. What *can* go unnoticed
    is the mistakes related to the *internal logic* and ML theory that powers Sklearn
    algorithms and transformers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 编写 Sklearn 代码时的错误很容易修复。*可以*被忽视的是与*内部逻辑*和支持 Sklearn 算法及变换器的 ML 理论相关的错误。
- en: These mistakes are especially more common and subtle when you are a beginner.
    So this post will be about the six such mistakes I made and learned to avoid when
    I was a beginner myself.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这些错误在你是初学者时尤其常见且微妙。因此，本文将讲述我在初学者阶段犯的六个错误以及如何避免它们。
- en: 1️⃣. Using `fit` or `fit_transform` everywhere
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1️⃣. 到处使用 `fit` 或 `fit_transform`
- en: Let’s start with the most serious mistake — a mistake that is related to *data
    leakage*. Data leakage is subtle and can be destructive to model performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最严重的错误开始——这是与*数据泄漏*相关的错误。数据泄漏很微妙，可能对模型性能造成破坏。
- en: It occurs when information that would not be available at prediction time is
    used during the model training. Data leakage causes models to give very optimistic
    results, even in cross-validation, but perform terribly when testing on *actual*
    novel data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当在模型训练过程中使用预测时无法获得的信息时，就会发生数据泄漏。数据泄漏会导致模型给出非常乐观的结果，即使在交叉验证中也是如此，但在测试*实际*的新数据时表现会非常糟糕。
- en: Data leakage is common during data preprocessing, particularly if the training
    and test sets are not separated. Many Sklearn preprocessing transformers such
    as [imputers](/advanced-missing-data-imputation-methods-with-sklearn-d9875cbcc6eb),
    [normalizers, standardization functions, and log transformers](/how-to-differentiate-between-scaling-normalization-and-log-transformations-69873d365a94)
    tap into the underlying data distribution during the fit time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据泄漏在数据预处理中很常见，特别是当训练集和测试集没有分开时。许多 Sklearn 预处理变换器，如 [插补器](/advanced-missing-data-imputation-methods-with-sklearn-d9875cbcc6eb)，[归一化器、标准化函数和对数变换器](/how-to-differentiate-between-scaling-normalization-and-log-transformations-69873d365a94)，在拟合期间会接触到底层的数据分布。
- en: For example, `StandardScaler` normalizes the data by subtracting the mean from
    each sample and dividing it by the standard deviation. Calling the `fit()` function
    on the full data (X) allows the transformer to learn the mean and standard deviation
    of the whole distribution of each feature.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`StandardScaler`通过从每个样本中减去均值并除以标准差来归一化数据。对完整数据（X）调用`fit()`函数允许变换器学习每个特征的整个分布的均值和标准差。
- en: If this data is split into train and test sets **after** transformation, the
    train set would be *contaminated* because `StandardScaler` leaked important information
    from the actual distribution.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些数据在变换**之后**被拆分为训练集和测试集，那么训练集将会被*污染*，因为`StandardScaler`泄露了实际分布中的重要信息。
- en: Even though this might not be apparent to us, Sklearn algorithms are powerful
    enough to notice this and take advantage during testing. In other words, the train
    data would be too perfect for the model because it has useful information about
    the test set, and the test would not be novel enough to test the model’s performance
    on actual unseen data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这对我们可能不明显，但 Sklearn 算法足够强大，能够察觉到这一点并在测试中利用它。换句话说，训练数据对模型来说可能过于完美，因为它包含了关于测试集的有用信息，而测试集对于模型在实际未见数据上的表现测试可能不够新颖。
- en: The easiest solution is to never call `fit` on the full data. Before doing any
    preprocessing, always split the data into train and test sets. Even after the
    split, you should never call `fit` or `fit_transform` on the test set because
    you will end up with the same problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的解决方案是绝不要在全部数据上调用`fit`。在进行任何预处理之前，始终将数据拆分为训练集和测试集。即使在拆分之后，你也绝不要在测试集上调用`fit`或`fit_transform`，否则你将遇到相同的问题。
- en: Since both train and test sets should receive the same preprocessing steps,
    a golden rule is to use `fit_transform` on the train data - this ensures that
    the transformer learns from the train set only and transforms it simultaneously.
    Then, call the `transform` method on the test set to transform it based on the
    information learned from the training data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练集和测试集都应接受相同的预处理步骤，一个黄金法则是对训练数据使用`fit_transform`——这确保了变换器仅从训练集中学习并同时进行转换。然后，在测试集上调用`transform`方法，根据从训练数据中学到的信息进行转换。
- en: 'A more robust solution would be using Sklearn’s built-in pipelines. Pipeline
    classes are specifically built to guard algorithms against data leakage. Using
    pipelines ensures that the training data is used during `fit` and the test data
    is used only for calculations. You can learn about them in detail in my separate
    article:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 更稳健的解决方案是使用 Sklearn 的内置管道。管道类专门用于保护算法免受数据泄漏的影响。使用管道可以确保训练数据仅在`fit`期间使用，而测试数据仅用于计算。你可以在我的另一篇文章中详细了解它们：
- en: '[](/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=post_page-----6be5bbdbb987--------------------------------)
    [## How to Use Sklearn Pipelines For Ridiculously Neat Code'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=post_page-----6be5bbdbb987--------------------------------)
    [## 如何使用 Sklearn 管道编写极简代码'
- en: Edit description
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编辑描述
- en: towardsdatascience.com](/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=post_page-----6be5bbdbb987--------------------------------)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=post_page-----6be5bbdbb987--------------------------------)'
- en: 2️⃣. Judging Model Performance Only By Test Scores
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2️⃣. 仅通过测试分数评估模型性能
- en: You got a test score over 0.85 — should you be celebrating? Big, fat NO!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你获得了超过0.85的测试分数——这值得庆祝吗？大大的“不”！
- en: Even though high test scores generally mean robust performance, there are important
    caveats to interpreting test results. First and most importantly, regardless of
    the value, test scores should only be judged based on the score you get from training.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管高测试得分通常意味着鲁棒的表现，但解读测试结果时有一些重要的警告。首先也是最重要的，无论得分值如何，测试得分应仅根据你从训练中得到的分数来判断。
- en: The only time you should be happy with your model is when the training score
    is higher than the test score, and both are high enough to satisfy the expectations
    of your unique case. However, this does not imply that the higher the difference
    between train and test scores, the better.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你唯一应该对你的模型感到满意的时候，是当训练得分高于测试得分，并且两者都足够高以满足你独特情况的期望。然而，这并不意味着训练得分和测试得分之间的差异越大越好。
- en: For example, 0.85 training score and 0.8 test score suggest a good model that
    is neither overfit nor underfit. But, if the training score is over 0.9 and the
    test score is 0.8, your model is overfitting. Instead of generalizing during training,
    the model memorized some of the training data resulting in a much lower test score.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，0.85的训练得分和0.8的测试得分表明模型既不过拟合也不欠拟合。但如果训练得分超过0.9而测试得分为0.8，你的模型就是过拟合了。模型在训练过程中没有进行泛化，而是记住了部分训练数据，导致测试得分大幅下降。
- en: You will often see such cases with tree-based and [ensemble models](/beginners-guide-to-xgboost-for-classification-problems-50f75aac5390).
    For example, algorithms such as Random Forest tend to achieve very high training
    scores if their tree depth is not controlled, leading to overfitting. You can
    read [this discussion](https://stats.stackexchange.com/questions/156694/how-can-training-and-testing-error-comparisons-be-indicative-of-overfitting?noredirect=1&lq=1)
    on StackExchange to learn more about the difference between train and test scores.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你常常会在基于树的和[集成模型](/beginners-guide-to-xgboost-for-classification-problems-50f75aac5390)中看到这种情况。例如，像随机森林这样的算法如果不控制树的深度，往往会获得非常高的训练得分，导致过拟合。你可以阅读[这个讨论](https://stats.stackexchange.com/questions/156694/how-can-training-and-testing-error-comparisons-be-indicative-of-overfitting?noredirect=1&lq=1)来了解训练和测试得分之间的差异。
- en: There is also the case where the test score is higher than the train. If the
    test score is higher than training even in the slightest, feel alarmed because
    you made a mistake! The major cause of such scenarios is data leakage, and we
    discussed an example of that in the last section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种情况是测试得分高于训练得分。如果测试得分即使略微高于训练得分，也要感到警惕，因为你犯了错误！这种情况的主要原因是数据泄露，我们在上一节中讨论了一个例子。
- en: Sometimes, it is also possible to get a good training score and an extremely
    low testing score. When the difference between train and test scores is huge,
    the problem will often be associated with *the test set* rather than overfitting.
    This might happen by using different preprocessing steps for the train and test
    sets or simply forgetting to apply preprocessing to the test set.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，即使在训练集上取得了良好的评分，测试集上的评分也可能极低。当训练和测试得分之间的差异很大时，这个问题往往与*测试集*有关，而不是过拟合。这可能是由于对训练集和测试集使用了不同的预处理步骤，或者仅仅是忘记对测试集进行预处理所导致的。
- en: In summary, always examine the gap between train and test scores closely. Doing
    so will tell you whether you should apply [regularization](/intro-to-regularization-with-ridge-and-lasso-regression-with-sklearn-edcf4c117b7a)
    to overcome overfitting, look for possible mistakes you made during [preprocessing](https://towardsdev.com/data-type-constraints-data-range-constraints-duplicate-data-with-pandas-44897a350b1e)
    or the best-case scenario, prepare the model for final evaluation and deployment.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，始终仔细检查训练和测试得分之间的差距。这将告诉你是否应该应用[正则化](/intro-to-regularization-with-ridge-and-lasso-regression-with-sklearn-edcf4c117b7a)以克服过拟合，寻找在[预处理](https://towardsdev.com/data-type-constraints-data-range-constraints-duplicate-data-with-pandas-44897a350b1e)中可能犯的错误，或者在最佳情况下，为最终评估和部署准备模型。
- en: 3️⃣. Generating Incorrect Train/Test Sets in Classification
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3️⃣. 在分类中生成不正确的训练/测试集
- en: A common mistake among beginners is forgetting to generate *stratified* train
    and test sets for classification.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 初学者常见的一个错误是忘记为分类任务生成*分层*的训练集和测试集。
- en: A model is more likely to generate correct predictions when the new data distribution
    matches training’s as much as possible. In classification, we only care about
    the class weights or proportions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当新数据分布尽可能匹配训练数据时，模型更可能产生正确的预测。在分类问题中，我们只关心类别权重或比例。
- en: For example, in a 3-class classification problem, the class weights are 0.4,
    0.3, 0.3\. When we divide this data into train and test sets, the distributions
    of both sets should reflect the distribution of the full data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'We commonly use `train_test_split` function of Sklearn to divide the data and
    Sklearn provides a handy argument - `stratify` to generate [stratified splits](/how-to-master-the-subtle-art-of-train-test-set-generation-7a8408bcd578).
    Here is an example of train/test sets with and without stratified splits:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Look at class weights before splitting
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Check the class weights again in both train and test sets without stratifying.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, both train and test sets have different class weights for the
    first and second classes. Let’s fix that:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Check the train/test set class weights after stratified splitting.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Setting `stratify` to the target (`y`) yielded identical class weights in both
    the train and test sets.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Altered class weights are a serious problem that might make a model more biased
    towards a particular class. Forgetting to generate stratified splits might result
    in a more favorable train or test sets or cause problems such as these:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92e183a04e8bb7963ae715a54d926c5b.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The above is the performance of a KNN classifier I built when I started learning
    Sklearn. As you can see, almost all test scores are higher than training because
    I had forgotten to generate stratified splits. As a result, the test set yielded
    too favorable a distribution for my model to take advantage of.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'After fixing the problem:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59000bfb1f595a56c6420f648433704f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: It is all back to normal.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: When using cross-validation or [pipelines](/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d),
    you don’t have to worry about this problem because CV splitters perform stratification
    under the hood using `[StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)`
    for classification problems.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 4️⃣. Using `LabelEcoder` to Encode the X array
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ever got annoyed when you found out that `[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)`
    encodes categorical columns only one at a time? Compared to other text transformers,
    such as `[OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)`
    which can transform multiple features simultaneously, this seems kind of a letdown😔
    by Sklearn.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'But I am here to tell you that it isn’t! It is simply the result of our unwillingness
    to read the documentation. Here is an excerpt of `LabelEncoder`''s 2-sentence
    documentation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: This transformer should be used to encode target values, i.e. `*y*`, and not
    the input `*X*`.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Then, what do we use to encode ordinal text features? If we kindly move on
    to the Sklearn user guide on *encoding categorical features*, we will see that
    it clearly states:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: To convert categorical features to integer codes, we can use the `*OrdinalEncoder*`.
    This estimator transforms each categorical feature to one new feature of integers
    (0 to n_categories - 1)
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Using `[OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)`
    allows us to transform multiple columns at once as expected, and it has the benefit
    of being able to integrate into Pipeline instances, which `LabelEncoder` cannot.
    The encoder follows the familiar transformer API of Sklearn:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: You can learn a lot about Sklearn by just reading the documentation and the
    user guide!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 5️⃣. Judging Model Performance Without Cross-validation
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I think you are already comfortable with the topic of overfitting. It is such
    a pressing issue in machine learning that countless techniques have been devised
    to overcome it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The most basic one is holding out a part of the data as a test set to simulate
    and measure a model’s performance on unseen data. However, hyperparameters of
    the models can be tweaked until the model reaches the maximum score on that particular
    test set, which again means overfitting.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: We might take another part of the full data as a ‘validation set’ to go around
    this once again. A model would be trained on the training data, fine-tune its
    performance on the validation set and run it through the test set for final evaluation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: But dividing our precious data into 3 sets would mean a smaller amount of data
    a model can learn from. The whole performance of the model would depend on that
    particular pair of train and validation set.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'So, ML practitioners commonly use a procedure called K-fold cross-validation
    (CV for short). Depending on its value, the full data is divided into *K* sets
    called folds, and for each fold, the model would use the *K-1* number of folds
    as training data and the rest as a testing set. After the CV is done, the model
    will have been trained and tested on all data. Here is the diagram of this process
    by Sklearn for 5-fold CV:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b972b2905eafa1295d79116097a98acd.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: Image by Sklearn user guide
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of cross-validation is that it completely takes the randomness
    out of the question. In other words, you won’t have to worry that `train_test_split`
    accidentally generates too favorable train and test sets that bias the objective
    function of your model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[](/one-stop-tutorial-on-all-cross-validation-techniques-you-can-should-use-7e1645fb703c?source=post_page-----6be5bbdbb987--------------------------------)
    [## One-Stop Tutorial On ALL Cross-Validation Techniques You Can (Should) Use'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: All CV procedures you need to know as a data scientist, explained
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/one-stop-tutorial-on-all-cross-validation-techniques-you-can-should-use-7e1645fb703c?source=post_page-----6be5bbdbb987--------------------------------)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about implementing CV in code from the [official user guide](https://scikit-learn.org/stable/modules/cross_validation.html).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 6️⃣. Using Accuracy as a Metric to Evaluate the Performance of Classifiers
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6️⃣. 使用准确率作为评估分类器性能的指标
- en: By default, all Sklearn classifiers use *accuracy* as a scoring method when
    we call `.score` function. Because of this easy access to the metric, it is common
    to see beginners using it extensively to judge the performance of their model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有Sklearn分类器在调用`.score`函数时使用*准确率*作为评分方法。由于这种指标的易得性，初学者通常会广泛使用它来评估模型的性能。
- en: Unfortunately, the vanilla *accuracy* *score* is useful in only one scenario
    — a [binary classification problem](/how-to-tune-models-like-a-puppet-master-based-on-confusion-matrix-fd488f9b5e65)
    with equal, balanced class weights.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，原始的*准确率* *评分*仅在一种场景下有用——一个[二分类问题](/how-to-tune-models-like-a-puppet-master-based-on-confusion-matrix-fd488f9b5e65)，即具有相等、平衡的类别权重。
- en: Other times, it is such a misleading metric that even the worst-performing models
    can hide behind high accuracy scores. For example, if the model detects spam emails,
    it can reach over 90% accuracy without even finding a single spam email.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，它是一个极具误导性的指标，甚至最差的模型也可以通过高准确率掩盖其真正的表现。例如，如果模型检测垃圾邮件，它可以在没有发现任何垃圾邮件的情况下达到超过90%的准确率。
- en: Why? As spam emails are not as common, the classifier can detect all non-spam
    emails, which can boost its accuracy even though the classifier completely fails
    at its purpose — detecting spam!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？由于垃圾邮件并不常见，分类器可以检测所有非垃圾邮件，这可以提升其准确率，即使分类器完全未能实现其目的——检测垃圾邮件！
- en: This problem is even worse for [multiclass classification](/comprehensive-guide-to-multiclass-classification-with-sklearn-127cc500f362).
    If you achieve 80% accuracy, does it mean the model is more accurate at detecting
    class 1, class 2, class 3, or even all classes?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[多分类问题](/comprehensive-guide-to-multiclass-classification-with-sklearn-127cc500f362)来说，这个问题更为严重。如果你获得了80%的准确率，这是否意味着模型在检测类别1、类别2、类别3，还是所有类别上都更准确？
- en: 'Accuracy can never answer such questions, but thankfully, multiple other classification
    metrics give a much more informative performance summary. You can read about them
    in my separate post, where I discuss metrics that apply to both binary and multiclass
    problems:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率无法回答此类问题，但幸运的是，还有多种其他分类指标可以提供更有信息量的性能总结。你可以在我另一篇文章中阅读这些指标的介绍，文章讨论了适用于二分类和多分类问题的指标：
- en: '[](/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd?source=post_page-----6be5bbdbb987--------------------------------)
    [## Comprehensive Guide on Multiclass Classification Metrics'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd?source=post_page-----6be5bbdbb987--------------------------------)
    [## 多分类指标综合指南'
- en: Edit description
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编辑描述
- en: towardsdatascience.com](/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd?source=post_page-----6be5bbdbb987--------------------------------)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd?source=post_page-----6be5bbdbb987--------------------------------)'
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Making embarrassing mistakes certainly sucks but it is all part of the journey.
    Even the most experienced folks will admit to making blunders that had made them
    go red to the roots of their hair in their own time.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 犯错确实很尴尬，但这是旅程的一部分。即使是最有经验的人也会承认他们在自己的时间里曾犯过让他们脸红的错误。
- en: But all of them had someone who finally pointed out their errors and showed
    the proper way of handling things. If you were making any of the mistakes above,
    I hope I became that someone for you.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但所有这些指标都有某些人最终指出了它们的错误并展示了正确的处理方式。如果你曾经犯过上述错误，我希望我成为了你心目中的那个人。
- en: Thank you for reading!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: Loved this article and, let’s face it, its bizarre writing style? Imagine having
    access to dozens more just like it, all written by a brilliant, charming, witty
    author (that’s me, by the way :).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢这篇文章吗？让我们面对现实，它那奇特的写作风格？想象一下，能访问更多类似的文章，都是由一位才华横溢、迷人、风趣的作者（顺便说一句，就是我 :)）撰写的。
- en: For only 4.99$ membership, you will get access to not just my stories, but a
    treasure trove of knowledge from the best and brightest minds on Medium. And if
    you use [my referral link](https://ibexorigin.medium.com/membership), you will
    earn my supernova of gratitude and a virtual high-five for supporting my work.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 仅需4.99美元的会员资格，你将不仅能访问我的故事，还能获得来自Medium上最聪明才智的宝贵知识。如果你使用[我的推荐链接](https://ibexorigin.medium.com/membership)，你将获得我的超级感激和一个虚拟的击掌，感谢你对我工作的支持。
- en: '[](https://ibexorigin.medium.com/membership?source=post_page-----6be5bbdbb987--------------------------------)
    [## Join Medium with my referral link — Bex T.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ibexorigin.medium.com/membership?source=post_page-----6be5bbdbb987--------------------------------)
    [## 使用我的推荐链接加入Medium — Bex T.'
- en: Get exclusive access to all my ⚡premium⚡ content and all over Medium without
    limits. Support my work by buying me a…
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取对我所有⚡优质⚡内容的独家访问权限，并在Medium上无限制阅读。通过购买我的…
- en: ibexorigin.medium.com](https://ibexorigin.medium.com/membership?source=post_page-----6be5bbdbb987--------------------------------)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[ibexorigin.medium.com](https://ibexorigin.medium.com/membership?source=post_page-----6be5bbdbb987--------------------------------)
    支持我的工作'
