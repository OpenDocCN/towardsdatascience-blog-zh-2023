- en: 6 Embarrassing Sklearn Mistakes You May Be Making And How to Avoid Them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/6-embarrassing-sklearn-mistakes-you-may-be-making-and-how-to-avoid-them-6be5bbdbb987](https://towardsdatascience.com/6-embarrassing-sklearn-mistakes-you-may-be-making-and-how-to-avoid-them-6be5bbdbb987)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are no error messages ‚Äî that‚Äôs what makes them subtle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)[![Bex
    T.](../Images/516496f32596e8ad56bf07f178a643c6.png)](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)
    [Bex T.](https://ibexorigin.medium.com/?source=post_page-----6be5bbdbb987--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6be5bbdbb987--------------------------------)
    ¬∑10 min read¬∑Jun 5, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Learn to avoid the six most serious mistakes related to machine learning theory
    that beginners often make through Sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e79383dd47c27048f5947ee350129670.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by me with Leonardo AI
  prefs: []
  type: TYPE_NORMAL
- en: Often, Sklearn throws big red error messages and warnings when you make a mistake.
    These messages suggest something is seriously wrong with your code, preventing
    the Sklearn magic from doing its job.
  prefs: []
  type: TYPE_NORMAL
- en: But what happens if you don‚Äôt get any errors or warnings? Does that mean you
    are crushing it so far? *Not necessarily*. Many knobs and dials make Sklearn the
    greatest ML library, its world-class *code design* being an example.
  prefs: []
  type: TYPE_NORMAL
- en: The mistakes while writing Sklearn code can easily be fixed. What *can* go unnoticed
    is the mistakes related to the *internal logic* and ML theory that powers Sklearn
    algorithms and transformers.
  prefs: []
  type: TYPE_NORMAL
- en: These mistakes are especially more common and subtle when you are a beginner.
    So this post will be about the six such mistakes I made and learned to avoid when
    I was a beginner myself.
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£. Using `fit` or `fit_transform` everywhere
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs start with the most serious mistake ‚Äî a mistake that is related to *data
    leakage*. Data leakage is subtle and can be destructive to model performance.
  prefs: []
  type: TYPE_NORMAL
- en: It occurs when information that would not be available at prediction time is
    used during the model training. Data leakage causes models to give very optimistic
    results, even in cross-validation, but perform terribly when testing on *actual*
    novel data.
  prefs: []
  type: TYPE_NORMAL
- en: Data leakage is common during data preprocessing, particularly if the training
    and test sets are not separated. Many Sklearn preprocessing transformers such
    as [imputers](/advanced-missing-data-imputation-methods-with-sklearn-d9875cbcc6eb),
    [normalizers, standardization functions, and log transformers](/how-to-differentiate-between-scaling-normalization-and-log-transformations-69873d365a94)
    tap into the underlying data distribution during the fit time.
  prefs: []
  type: TYPE_NORMAL
- en: For example, `StandardScaler` normalizes the data by subtracting the mean from
    each sample and dividing it by the standard deviation. Calling the `fit()` function
    on the full data (X) allows the transformer to learn the mean and standard deviation
    of the whole distribution of each feature.
  prefs: []
  type: TYPE_NORMAL
- en: If this data is split into train and test sets **after** transformation, the
    train set would be *contaminated* because `StandardScaler` leaked important information
    from the actual distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Even though this might not be apparent to us, Sklearn algorithms are powerful
    enough to notice this and take advantage during testing. In other words, the train
    data would be too perfect for the model because it has useful information about
    the test set, and the test would not be novel enough to test the model‚Äôs performance
    on actual unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest solution is to never call `fit` on the full data. Before doing any
    preprocessing, always split the data into train and test sets. Even after the
    split, you should never call `fit` or `fit_transform` on the test set because
    you will end up with the same problem.
  prefs: []
  type: TYPE_NORMAL
- en: Since both train and test sets should receive the same preprocessing steps,
    a golden rule is to use `fit_transform` on the train data - this ensures that
    the transformer learns from the train set only and transforms it simultaneously.
    Then, call the `transform` method on the test set to transform it based on the
    information learned from the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more robust solution would be using Sklearn‚Äôs built-in pipelines. Pipeline
    classes are specifically built to guard algorithms against data leakage. Using
    pipelines ensures that the training data is used during `fit` and the test data
    is used only for calculations. You can learn about them in detail in my separate
    article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=post_page-----6be5bbdbb987--------------------------------)
    [## How to Use Sklearn Pipelines For Ridiculously Neat Code'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=post_page-----6be5bbdbb987--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£. Judging Model Performance Only By Test Scores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You got a test score over 0.85 ‚Äî should you be celebrating? Big, fat NO!
  prefs: []
  type: TYPE_NORMAL
- en: Even though high test scores generally mean robust performance, there are important
    caveats to interpreting test results. First and most importantly, regardless of
    the value, test scores should only be judged based on the score you get from training.
  prefs: []
  type: TYPE_NORMAL
- en: The only time you should be happy with your model is when the training score
    is higher than the test score, and both are high enough to satisfy the expectations
    of your unique case. However, this does not imply that the higher the difference
    between train and test scores, the better.
  prefs: []
  type: TYPE_NORMAL
- en: For example, 0.85 training score and 0.8 test score suggest a good model that
    is neither overfit nor underfit. But, if the training score is over 0.9 and the
    test score is 0.8, your model is overfitting. Instead of generalizing during training,
    the model memorized some of the training data resulting in a much lower test score.
  prefs: []
  type: TYPE_NORMAL
- en: You will often see such cases with tree-based and [ensemble models](/beginners-guide-to-xgboost-for-classification-problems-50f75aac5390).
    For example, algorithms such as Random Forest tend to achieve very high training
    scores if their tree depth is not controlled, leading to overfitting. You can
    read [this discussion](https://stats.stackexchange.com/questions/156694/how-can-training-and-testing-error-comparisons-be-indicative-of-overfitting?noredirect=1&lq=1)
    on StackExchange to learn more about the difference between train and test scores.
  prefs: []
  type: TYPE_NORMAL
- en: There is also the case where the test score is higher than the train. If the
    test score is higher than training even in the slightest, feel alarmed because
    you made a mistake! The major cause of such scenarios is data leakage, and we
    discussed an example of that in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, it is also possible to get a good training score and an extremely
    low testing score. When the difference between train and test scores is huge,
    the problem will often be associated with *the test set* rather than overfitting.
    This might happen by using different preprocessing steps for the train and test
    sets or simply forgetting to apply preprocessing to the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, always examine the gap between train and test scores closely. Doing
    so will tell you whether you should apply [regularization](/intro-to-regularization-with-ridge-and-lasso-regression-with-sklearn-edcf4c117b7a)
    to overcome overfitting, look for possible mistakes you made during [preprocessing](https://towardsdev.com/data-type-constraints-data-range-constraints-duplicate-data-with-pandas-44897a350b1e)
    or the best-case scenario, prepare the model for final evaluation and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£. Generating Incorrect Train/Test Sets in Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common mistake among beginners is forgetting to generate *stratified* train
    and test sets for classification.
  prefs: []
  type: TYPE_NORMAL
- en: A model is more likely to generate correct predictions when the new data distribution
    matches training‚Äôs as much as possible. In classification, we only care about
    the class weights or proportions.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a 3-class classification problem, the class weights are 0.4,
    0.3, 0.3\. When we divide this data into train and test sets, the distributions
    of both sets should reflect the distribution of the full data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We commonly use `train_test_split` function of Sklearn to divide the data and
    Sklearn provides a handy argument - `stratify` to generate [stratified splits](/how-to-master-the-subtle-art-of-train-test-set-generation-7a8408bcd578).
    Here is an example of train/test sets with and without stratified splits:'
  prefs: []
  type: TYPE_NORMAL
- en: Look at class weights before splitting
  prefs: []
  type: TYPE_NORMAL
- en: Check the class weights again in both train and test sets without stratifying.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, both train and test sets have different class weights for the
    first and second classes. Let‚Äôs fix that:'
  prefs: []
  type: TYPE_NORMAL
- en: Check the train/test set class weights after stratified splitting.
  prefs: []
  type: TYPE_NORMAL
- en: Setting `stratify` to the target (`y`) yielded identical class weights in both
    the train and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Altered class weights are a serious problem that might make a model more biased
    towards a particular class. Forgetting to generate stratified splits might result
    in a more favorable train or test sets or cause problems such as these:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92e183a04e8bb7963ae715a54d926c5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The above is the performance of a KNN classifier I built when I started learning
    Sklearn. As you can see, almost all test scores are higher than training because
    I had forgotten to generate stratified splits. As a result, the test set yielded
    too favorable a distribution for my model to take advantage of.
  prefs: []
  type: TYPE_NORMAL
- en: 'After fixing the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59000bfb1f595a56c6420f648433704f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: It is all back to normal.
  prefs: []
  type: TYPE_NORMAL
- en: When using cross-validation or [pipelines](/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d),
    you don‚Äôt have to worry about this problem because CV splitters perform stratification
    under the hood using `[StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)`
    for classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 4Ô∏è‚É£. Using `LabelEcoder` to Encode the X array
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ever got annoyed when you found out that `[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)`
    encodes categorical columns only one at a time? Compared to other text transformers,
    such as `[OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)`
    which can transform multiple features simultaneously, this seems kind of a letdownüòî
    by Sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: 'But I am here to tell you that it isn‚Äôt! It is simply the result of our unwillingness
    to read the documentation. Here is an excerpt of `LabelEncoder`''s 2-sentence
    documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: This transformer should be used to encode target values, i.e. `*y*`, and not
    the input `*X*`.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Then, what do we use to encode ordinal text features? If we kindly move on
    to the Sklearn user guide on *encoding categorical features*, we will see that
    it clearly states:'
  prefs: []
  type: TYPE_NORMAL
- en: To convert categorical features to integer codes, we can use the `*OrdinalEncoder*`.
    This estimator transforms each categorical feature to one new feature of integers
    (0 to n_categories - 1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Using `[OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)`
    allows us to transform multiple columns at once as expected, and it has the benefit
    of being able to integrate into Pipeline instances, which `LabelEncoder` cannot.
    The encoder follows the familiar transformer API of Sklearn:'
  prefs: []
  type: TYPE_NORMAL
- en: You can learn a lot about Sklearn by just reading the documentation and the
    user guide!
  prefs: []
  type: TYPE_NORMAL
- en: 5Ô∏è‚É£. Judging Model Performance Without Cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I think you are already comfortable with the topic of overfitting. It is such
    a pressing issue in machine learning that countless techniques have been devised
    to overcome it.
  prefs: []
  type: TYPE_NORMAL
- en: The most basic one is holding out a part of the data as a test set to simulate
    and measure a model‚Äôs performance on unseen data. However, hyperparameters of
    the models can be tweaked until the model reaches the maximum score on that particular
    test set, which again means overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: We might take another part of the full data as a ‚Äòvalidation set‚Äô to go around
    this once again. A model would be trained on the training data, fine-tune its
    performance on the validation set and run it through the test set for final evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: But dividing our precious data into 3 sets would mean a smaller amount of data
    a model can learn from. The whole performance of the model would depend on that
    particular pair of train and validation set.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, ML practitioners commonly use a procedure called K-fold cross-validation
    (CV for short). Depending on its value, the full data is divided into *K* sets
    called folds, and for each fold, the model would use the *K-1* number of folds
    as training data and the rest as a testing set. After the CV is done, the model
    will have been trained and tested on all data. Here is the diagram of this process
    by Sklearn for 5-fold CV:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b972b2905eafa1295d79116097a98acd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Sklearn user guide
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of cross-validation is that it completely takes the randomness
    out of the question. In other words, you won‚Äôt have to worry that `train_test_split`
    accidentally generates too favorable train and test sets that bias the objective
    function of your model.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/one-stop-tutorial-on-all-cross-validation-techniques-you-can-should-use-7e1645fb703c?source=post_page-----6be5bbdbb987--------------------------------)
    [## One-Stop Tutorial On ALL Cross-Validation Techniques You Can (Should) Use'
  prefs: []
  type: TYPE_NORMAL
- en: All CV procedures you need to know as a data scientist, explained
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/one-stop-tutorial-on-all-cross-validation-techniques-you-can-should-use-7e1645fb703c?source=post_page-----6be5bbdbb987--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about implementing CV in code from the [official user guide](https://scikit-learn.org/stable/modules/cross_validation.html).
  prefs: []
  type: TYPE_NORMAL
- en: 6Ô∏è‚É£. Using Accuracy as a Metric to Evaluate the Performance of Classifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, all Sklearn classifiers use *accuracy* as a scoring method when
    we call `.score` function. Because of this easy access to the metric, it is common
    to see beginners using it extensively to judge the performance of their model.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the vanilla *accuracy* *score* is useful in only one scenario
    ‚Äî a [binary classification problem](/how-to-tune-models-like-a-puppet-master-based-on-confusion-matrix-fd488f9b5e65)
    with equal, balanced class weights.
  prefs: []
  type: TYPE_NORMAL
- en: Other times, it is such a misleading metric that even the worst-performing models
    can hide behind high accuracy scores. For example, if the model detects spam emails,
    it can reach over 90% accuracy without even finding a single spam email.
  prefs: []
  type: TYPE_NORMAL
- en: Why? As spam emails are not as common, the classifier can detect all non-spam
    emails, which can boost its accuracy even though the classifier completely fails
    at its purpose ‚Äî detecting spam!
  prefs: []
  type: TYPE_NORMAL
- en: This problem is even worse for [multiclass classification](/comprehensive-guide-to-multiclass-classification-with-sklearn-127cc500f362).
    If you achieve 80% accuracy, does it mean the model is more accurate at detecting
    class 1, class 2, class 3, or even all classes?
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy can never answer such questions, but thankfully, multiple other classification
    metrics give a much more informative performance summary. You can read about them
    in my separate post, where I discuss metrics that apply to both binary and multiclass
    problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd?source=post_page-----6be5bbdbb987--------------------------------)
    [## Comprehensive Guide on Multiclass Classification Metrics'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd?source=post_page-----6be5bbdbb987--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Making embarrassing mistakes certainly sucks but it is all part of the journey.
    Even the most experienced folks will admit to making blunders that had made them
    go red to the roots of their hair in their own time.
  prefs: []
  type: TYPE_NORMAL
- en: But all of them had someone who finally pointed out their errors and showed
    the proper way of handling things. If you were making any of the mistakes above,
    I hope I became that someone for you.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs: []
  type: TYPE_NORMAL
- en: Loved this article and, let‚Äôs face it, its bizarre writing style? Imagine having
    access to dozens more just like it, all written by a brilliant, charming, witty
    author (that‚Äôs me, by the way :).
  prefs: []
  type: TYPE_NORMAL
- en: For only 4.99$ membership, you will get access to not just my stories, but a
    treasure trove of knowledge from the best and brightest minds on Medium. And if
    you use [my referral link](https://ibexorigin.medium.com/membership), you will
    earn my supernova of gratitude and a virtual high-five for supporting my work.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://ibexorigin.medium.com/membership?source=post_page-----6be5bbdbb987--------------------------------)
    [## Join Medium with my referral link ‚Äî Bex T.'
  prefs: []
  type: TYPE_NORMAL
- en: Get exclusive access to all my ‚ö°premium‚ö° content and all over Medium without
    limits. Support my work by buying me a‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ibexorigin.medium.com](https://ibexorigin.medium.com/membership?source=post_page-----6be5bbdbb987--------------------------------)
  prefs: []
  type: TYPE_NORMAL
