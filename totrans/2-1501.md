# 数据流中的矩阵近似

> 原文：[https://towardsdatascience.com/matrix-approximation-in-data-streams-7585720e8671](https://towardsdatascience.com/matrix-approximation-in-data-streams-7585720e8671)

## 在没有所有行的情况下近似矩阵

[](https://medium.com/@mina.ghashami?source=post_page-----7585720e8671--------------------------------)[![Mina Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----7585720e8671--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7585720e8671--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7585720e8671--------------------------------) [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----7585720e8671--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7585720e8671--------------------------------) ·13 分钟阅读·2023年9月17日

--

![](../Images/dc927665ca6a15dbc7eedcdf011561fe.png)

图片来源：unsplash.com

矩阵近似是数据挖掘和机器学习中一个广泛研究的子领域。许多数据分析任务依赖于获得矩阵的*低秩近似*。例如，降维、异常检测、数据去噪、聚类和推荐系统。本文将深入探讨矩阵近似的问题，以及在数据不完全时如何计算它！

本文内容部分取自我在 [斯坦福大学-CS246课程](https://web.stanford.edu/class/cs246/) 的[讲座](https://web.stanford.edu/class/cs246/slides/17-matrix_sketching.pdf)。希望对你有用。完整内容请见 [此处](https://web.stanford.edu/class/cs246/slides/17-matrix_sketching.pdf)。

# 数据作为矩阵

大多数在网上生成的数据可以表示为矩阵，其中矩阵的每一行是一个数据点。例如，在路由器中，每个通过网络发送的包都是一个数据点，可以表示为所有数据点矩阵中的一行。在零售中，每次购买都是所有交易矩阵中的一行。

![](../Images/c2174d5fadd7df8c1421ee119c9792c1.png)

图1：数据作为矩阵 — 作者提供的图像

与此同时，几乎所有在网上生成的数据都是*流式性质的*；这意味着数据由外部源以我们无法控制的快速速率生成。想象一下用户每秒在Google搜索引擎上进行的所有搜索。我们称这种数据为*流式数据*；因为它像溪流一样源源不断地涌入。

一些典型流式网页规模数据的示例如下：

![](../Images/e077ae79ff78e06cf0eeb9950c667107.png)

图2：典型的流式网页规模数据的大小 — 作者提供的图像

将流数据视为包含*n* 行、*d* 维空间中的矩阵*A*，其中通常 *n >> d*。通常 *n* 是以十亿为单位并不断增加的。

# 数据流模型

在流模型中，数据以高速到达，一次一行，算法必须快速处理这些项目，否则它们将永远丢失。

![](../Images/10a27202839bd9b4ebaadd879632b4aa.png)

图3：数据流模型 — 图片由作者提供

在数据流模型中，算法只能对数据进行一次遍历，并且需要使用较小的内存进行处理。

# 秩-k 近似

矩阵*A*的*秩-k 近似*是一个秩为*k*的较小矩阵*B*，使得*B*对*A*进行准确的近似。图2展示了这一概念。

![](../Images/53dc66610adbfedd4d744da8ee67bd13.png)

图4：从*A*获取更小的草图*B* — 图片由作者提供

*B*通常被称为*A*的草图。注意，在数据流模型中，*B*会比*A*小得多，以便适合内存。此外，rank(B) << rank(A)。例如，如果*A*是一个包含 100 亿文档和 100 万词的术语-文档矩阵，那么*B*可能是一个 1000×100 万的矩阵；即，少 1000 万行！

秩-k 近似必须“*准确*”地近似*A*。虽然准确是一个模糊的概念，但我们可以通过各种误差定义来量化它：

1️⃣ **协方差误差**：

协方差误差是矩阵 A 的协方差与矩阵 B 的协方差之间差异的 Frobenius 范数或 L2 范数。这个误差在数学上定义如下：

![](../Images/658c13473308fa953a1b50cc5155ef34.png)

协方差误差定义 — 图片由作者提供

2️⃣ **投影误差**：

投影误差是当*A*中的数据点被投影到*B*的子空间时的残差的范数。这个残差范数被测量为 L2 范数或 Frobenius 范数：

![](../Images/310d422bb40a8df456d5b33e90b82a46.png)

投影误差定义 — 图片由作者提供

这些误差评估了近似的质量；它们越小，近似效果越好。但它们可以小到什么程度呢？

当我们计算这些误差时，我们必须有一个基准来进行比较。在矩阵草图领域，每个人使用的基准是由*奇异值分解（SVD）*创建的秩-k 近似！SVD 计算最佳的秩-k 近似！这意味着它在“*协方差误差*”和“*投影误差*”上造成的误差最小。

对*A*的最佳秩-k 近似记作Aₖ。因此，SVD引起的最小误差是：

![](../Images/037bdb52e3a227e51d760b95a27eef7a.png)

最小秩k近似误差 — 图片由作者提供

SVD 将矩阵*A*分解为三个矩阵：

+   左奇异矩阵 *U*

+   奇异值矩阵 *S*

+   右奇异矩阵 V

U 和 V 是正交的，意味着它们的列是单位范数且它们彼此正交；即 U 中每两列（V 中也是）之间的点积为零。矩阵 S 是对角矩阵；只有对角线上的条目是非零的，并且按降序排列。

![](../Images/4e10bfbec83481719adb175f24178a92.png)

图 5：奇异值分解 — 图片来自作者

SVD 通过取 U 和 V 的前 k 列以及 S 的前 k 项来计算最佳的秩-k 近似：

![](../Images/674b0fccc2d859376d7e9d4bb285847c.png)

图 6：SVD 的秩 k 近似 — 图片来自作者

如前所述，以这种方式计算的 Aₖ 在任何秩为 k 或更低的矩阵 B 中具有最低的近似误差。然而，SVD 是一种非常耗时的方法，如果 A 是 n×d，则需要运行时间 O(nd²)，并且不适用于数据流中的矩阵。此外，SVD 对稀疏矩阵效率不高；它在计算近似时没有利用矩阵的稀疏性。

❓现在的问题是我们如何以流式方式计算矩阵近似？

**流式矩阵近似方法主要有三大类：**

1️⃣ 基于行抽样

2️⃣ 随机投影方法

3️⃣ 迭代草图法

# **基于行抽样的方法**

这些方法从相对于良好定义的概率分布的“重要”行中进行抽样。这些方法的不同之处在于它们如何定义“重要性”的概念。通用框架是它们按以下方式构建草图 B：

1.  它们首先给流式矩阵*A*中的每一行分配一个概率

1.  然后他们从*A*中抽取*l*行（通常是有放回的）来构建*B*

1.  最后，它们将 *B* 适当缩放，使其成为 *A* 的无偏估计

![](../Images/abb1b4fd0514a3f347059642385d0014.png)

图 7：有放回的行抽样以构建草图 B — 图片来自作者

注意，步骤 1 中分配给行的概率实际上是行的“重要性”。将“重要性”视为与项相关的权重，例如，对于文件记录，权重可以是文件的大小。或者对于 IP 地址，权重可以是 IP 地址发出请求的次数。

在矩阵中，每个项都是一个行向量，其权重是其范数的平方；也称为 *L2 范数*。有一种行抽样算法根据行的 L2 范数在数据的一次遍历中进行抽样。这个算法被称为“*L2 范数行抽样*”，其伪代码如下：

![](../Images/c4351950a8a133bb15b461a8547faac1.png)

图 8：L2 范数行抽样算法 — 图片来自作者

该算法以有放回的方式抽样 *l = O(k/ε²)* 行，并实现以下误差界限：

![](../Images/c4f00ca30599e08831b10004abfa0dee.png)

图 9：L2 范数行抽样的误差保证 — 图片来自作者

注意，这是一个较弱的误差界限，因为它受限于矩阵A的Frobenius范数，总体来说可能是一个很大的数值！有一个改进的算法表现更好；我们来看看它。

**扩展**：有一种变体算法同时采样行和列！它被称为“*CUR*”算法，并且比“*L2-范数行采样*”方法表现更好。*CUR*方法通过从A中采样行和列来创建三个矩阵C、U和R。它的工作原理如下：

步骤1：*CUR*首先从*A*中采样几列，每列的采样概率与该列的范数成正比。这形成了矩阵*C*。

![](../Images/f850303fc206dc110d8bea962fbe599b.png)

图10：CUR算法步骤1—— 图片由作者提供

步骤2：然后*CUR*从*A*中随机抽取几行，每行的抽取概率与该行的范数成正比。这形成了矩阵*R*。

步骤3：CUR然后计算C和R的交集的伪逆。这被称为矩阵U。

![](../Images/2d65f393f1a9bdf84c008271097e31b6.png)

图11：CUR算法步骤2,3—— 图片由作者提供

最终，这三个矩阵的乘积，即*C.U.R*，近似于*A*，并提供了一个低秩近似。该算法在采样*l = O(k log k/ε²)*行和列时达到了以下误差界限。

![](../Images/1e4dcb2aafc66e61401b8c4ec1791d95.png)

图12：CUR误差保证—— 图片由作者提供

注意，与*L2-范数行采样*相比，这个界限要紧得多。

**总结：** 行采样方法家族（包括CUR）通过采样行（和列）来形成低秩近似，因此它们非常直观并形成可解释的近似。

在下一部分，我们将看到另一类数据无关的方法。

# **基于随机投影的方法**

这些方法组的关键思想是，如果将向量空间中的点投影到一个随机选择的适当高维子空间中，则点之间的距离大致保持不变*。*

**Johnson-Lindenstrauss变换（JLT）**很好地描述了这一点：***d***个数据点在任何维度（例如，对于n≫d的n维空间）中可以被嵌入到大约***log d***维的空间中，使得它们的成对距离在某种程度上得以保持。

JLT的更精确和数学化的定义如下：

![](../Images/fa42af817b3ba58390a124621bb9dd9e.png)

图13：JLT定义—— 图片由作者提供

有许多方法可以构造一个矩阵S，以保持成对距离。所有这些矩阵都称为具有*JLT属性*。下图展示了一些创建这样的矩阵S的常见方法：

![](../Images/40fec39e26e49d953b0503b0d5d96494.png)

图片由作者提供

如上图所示，*S*的一个简单构造是从*N(0,1)*中抽取独立随机变量作为*S*的条目，然后将S按√(1/r)进行缩放：

![](../Images/0eb53e2113540584d8817b5bcf163dbc.png)

图14：JLT矩阵—— 图片由作者提供

这个矩阵具有 *JLT 属性* [6]*，我们用它来设计随机投影方法如下：

![](../Images/9ee91d359bb577156591f1798af06386.png)

图15：随机投影方法 — 作者提供的图片

注意第二步，它将数据点从高维空间投影到低维空间。很容易证明 [6] 该方法生成了无偏的草图：

![](../Images/bb25673959486234e80ad3ab294d0d4a.png)

图16：随机投影提供了无偏的近似 — 作者提供的图片

随机投影方法在设置 *r = O(k/ε + k log k)* 时能达到以下误差保证。请注意，它们的界限优于行采样方法。

![](../Images/0b86a215b239461b2af2ae74e967a33c.png)

图17：随机投影误差界限 — 作者提供的图片

有一类与随机投影类似的工作可以实现更好的时间界限。它被称为 *哈希技术* [5]。这种方法采用一个每列只有一个非零条目的矩阵 S，而该条目是1或-1。它们计算近似值为 *B = SA*。

![](../Images/b5950efa90c38aa8d34de9166328aaf4.png)

哈希技术 — 作者提供的图片

**总结**：随机投影方法计算效率高，并且数据无关，因为其计算仅涉及一个随机矩阵 S。相比之下，行采样方法需要访问数据以形成草图。

# **迭代草图**

这些方法在流 A=<a1,a2,…> 上工作，其中每个项目被读取一次，迅速处理且不再读取。读取每个项目时，它们更新草图 B。

![](../Images/4ec8205e7236541afe7681edcecf06b3.png)

图18：迭代草图方法 — 作者提供的图片

该组的最先进方法称为“*频繁方向*”，基于 *Misra-Gries 算法* 查找数据流中的频繁项。接下来，我们首先了解 Misra-Gries 算法如何查找频繁项，然后将其扩展到矩阵。

## Misra-Gries 算法用于查找频繁项

假设有一个项目流，我们想找到每个项目的频率 *f(i)*。

![](../Images/54d29ccd557f9d13039452fd568f5703.png)

图19：流中的频繁项计数 — 作者提供的图片

如果我们保持 ***d*** 个计数器，我们可以计算每个项的频率。但这不够好，因为在某些领域，如 IP 地址、查询等，唯一项的数量太多了。

![](../Images/206173be287481bc8955b9da0bedef23.png)

图20：用于项频率估计的d个计数器 — 作者提供的图片

所以让我们保持 *l* 个计数器，其中 *l≪d*。如果流中到达的新项目在计数器中，我们将其计数加1：

![](../Images/3a394fab3373e54819d58c3b2995ae35.png)

图21：增加项的计数器 — 作者提供的图片

如果新项目不在计数器中且我们有空间，我们为其创建一个计数器并将其设置为1。

![](../Images/0708aad8dd0be90b5b604c637ae20f16.png)

图 22：为新项目设置计数器 — 作者提供的图像

但如果我们没有空间容纳新项目（这里的新项目是棕色盒子），我们获得中位数计数器，即位置为*l/2*的计数器：

![](../Images/db530dd4cd999766870474f0a0d4254e.png)

图 23：从每一个计数器中减去中间计数器。— 作者提供的图像

并从所有计数器中减去它。对于所有变成负值的计数器，我们将其重置为零。所以它变成如下：

![](../Images/6cf66db80be1c254e6e4e3bf4f034900.png)

图 24：一半计数器为零 — 作者提供的图像

如我们所见，现在我们有空间容纳新项目，所以我们继续处理流🙂。

在流的任何时刻，项目的近似计数是我们迄今为止保留的计数，例如：

![](../Images/27582695b5d02ddba678f230d04bac92.png)

图 25：估计项目计数 — 作者提供的图像

这种方法会低估计数，因此对于任何项目 i，其近似频率小于或等于其真实频率：

![](../Images/9acefc419d289c334bc6c37b25734dd8.png)

与此同时，它的近似频率是下界的，因为每次我们减少时，最多减少*l/2*位置计数器的计数。

![](../Images/546c48ec031b97d17e06bd9c22111c61.png)

在流中看到***n***个元素的任何点，我们有：

![](../Images/ac227771f0be7275d8fb40c84e90ca74.png)

因此，它提供的错误保证如下：

![](../Images/ede440a1b30bc4c853a4dd087778ee65.png)

Misra-Gries 错误界限 — 作者提供的图像

因此，Misra-Gries 对所有真实频率大于*2n/l*的项目生成一个非零近似频率。例如，要找到出现超过 20% 的项目，我们必须采取*l = 10*计数器并运行 Misra-Gries 算法。

## 频繁方向：Misra-Gries 的扩展

现在，让我们将 Misra-Gries 扩展到向量和矩阵。在矩阵的情况下，流中的项目是***d***维的**行向量**。在流中的任何时刻***n***，所有行一起形成一个*有*n*行的高矩阵*A*。目标是找到*A*的**最重要方向**。这些方向对应于*A*的前几个奇异向量。一个方向越重要，它在数据点中出现的频率就越高，这就是我们称下一个算法为**频繁方向** [2,3]的原因。

*频繁方向*算法的伪代码如下：

![](../Images/561c183c6cd15343f8b9d982a2a4bddf.png)

图 26：FrequentDirections — 作者提供的图像

如我们所见，该算法以矩阵*A*和草图大小*l*作为输入。然后，在第一步（即第一行突出显示的蓝色部分），算法将草图*B*初始化为空矩阵，具有*l*行。然后对于流中的每一行*A*，算法将其插入*B*中，直到*B*满为止：

![](../Images/a2fbac30a27748109eb50e886b82d496.png)

图 27：当草图 B 满时 — 作者提供的图像

然后我们计算*B*的*SVD*；这将产生左奇异矩阵*U*、奇异值矩阵*S*和右奇异矩阵*V*。注意，*U*和*V*提供了子空间的旋转，因为它们是正交矩阵。

![](../Images/1175b22b2c060f147b19d1a8c06428b7.png)

图 28: B 的 SVD — 作者提供的图像

然后我们通过从所有奇异值的平方中减去中间奇异值的平方来降低*B*的秩！注意，这一步类似于Misra-Gries中的部分操作，我们从所有计数器中减去中间计数器。从所有奇异值的平方中减去中间奇异值的平方使得一半的奇异值变为零。

![](../Images/105b9e0fd0dcc1737c05d7b47ea2d5f4.png)

图 29: 降低 B 的秩 — 作者提供的图像

然后我们将*S*（奇异值矩阵）乘以*V* 转置（右奇异矩阵）并将其分配给*B*。换句话说，我们通过去掉左奇异矩阵*U*来重构*B*。这种操作的效果是得到一个新的矩阵*B*，其一半的行为空。这是好消息，因为它为流矩阵*A*中的下一行提供了空间。

**误差保证**：类似于频繁项的情况，该方法具有以下误差保证，其中*l*是草图大小，*k*是秩：

![](../Images/26af11f02db213823a89fd08a730206a.png)

FrequentDirections 协方差误差界限 — 作者提供的图像

![](../Images/3e84e4752ba2fe64ba053b39926b8c0e.png)

FrequentDirections 投影误差界限 — 作者提供的图像

比较第二个误差界限与随机投影和行采样的误差界限。注意这是一个更紧凑和更好的误差界限。

**实验**：实验[2,4]表明，*Frequent Directions* 算法优于上述讨论的所有其他流式算法。以下是与协方差误差界限相关的实验：

![](../Images/053c83c270c62d2990bfd0ecab32cd13.png)

图 30: 协方差误差中的实验 — 来自 [2] 的图像

这是关于投影误差界限的实验：

![](../Images/275ca9112d6e71ab891f32ca8da25aa4.png)

图 31: 投影误差中的实验 — 来自 [2] 的图像

本文到此结束。正如我们所见，*Frequent Directions* 不仅在近似误差中优于所有其他方法，而且使用的空间最少。换句话说，它在实现的误差界限方面是空间最优的。

# 总结

数据流中的低秩矩阵近似是计算低秩矩阵近似的问题，其中矩阵的行以流式方式到达。这意味着无法一次性访问矩阵的所有行，也不知道矩阵的大小。有三种主要的近似方法类别：行采样——随机投影——迭代草图。虽然第一组方法是最直观的，因为它们采样实际数据点，第二组方法在运行时效率最高。最先进的方法（SOTA）属于第三组，称为频繁方向。该方法基于频繁项估计的旧方法，并且在误差界限方面具有空间最优性。

如果你有任何问题或建议，请随时联系我：

电子邮件：mina.ghashami@gmail.com

LinkedIn：[https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)

# 参考文献

1.  [快速蒙特卡罗矩阵算法 I：矩阵乘法的近似，P. Drineas 等，2006](https://www.stat.berkeley.edu/~mmahoney/pubs/matrix1_SICOMP.pdf)

1.  [频繁方向：简单且确定性的矩阵草图](https://arxiv.org/abs/1501.01711)

1.  [https://github.com/edoliberty/frequent-directions](https://github.com/edoliberty/frequent-directions)

1.  [具有保证的改进实用矩阵草图](https://arxiv.org/abs/1501.06561)

1.  [输入稀疏时间下的低秩近似和回归](https://arxiv.org/abs/1207.6365)

1.  [通过随机投影改进的大矩阵近似算法](https://ieeexplore.ieee.org/document/4031351)
