# 机器学习不仅仅预测未来，它还积极地创造未来

> 原文：[https://towardsdatascience.com/machine-learning-does-not-only-predict-the-future-it-actively-creates-it-1615895c80a9](https://towardsdatascience.com/machine-learning-does-not-only-predict-the-future-it-actively-creates-it-1615895c80a9)

## 关于位置偏差的入门（以及它为何重要）

[](https://medium.com/@samuel.flender?source=post_page-----1615895c80a9--------------------------------)[![Samuel Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----1615895c80a9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1615895c80a9--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1615895c80a9--------------------------------) [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----1615895c80a9--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----1615895c80a9--------------------------------) ·4 min read·2023年1月11日

--

![](../Images/d5f8f135bbb8c7670a8290bcd5c6cea9.png)

图片由Stable Diffusion生成

标准的机器学习课程教导我们，机器学习模型从过去存在的模式中学习，以预测未来。

这是一个很好的简化，但一旦这些模型的预测被用于生产环境中，情况会发生戏剧性的变化，因为它们会产生反馈循环：现在，模型的预测本身正在影响模型试图从中学习的世界。我们的模型不再仅仅是预测未来，它们实际上是在创造未来。

其中一个反馈循环是**位置偏差**，这一现象已经在[排名模型](/learning-to-rank-a-primer-40d2ff9960af)中被观察到，这些模型支持搜索引擎、推荐系统、社交媒体信息流和广告排名器等。

## 什么是位置偏差？

位置偏差意味着排名最高的项目（Netflix上的视频、Google上的页面、Amazon上的产品、Facebook上的帖子或Twitter上的推文）之所以创造了最多的互动，不是因为它们实际上是用户最需要的内容，而仅仅是因为它们的排名最高。

这种偏差的表现形式是因为排名模型非常好，以至于用户开始盲目相信排名最高的项目，而不再进一步查看（“盲目信任偏差”），或者用户根本没有考虑其他可能更好的项目，因为它们的排名太低，用户甚至没有注意到（“展示偏差”）。

## 为什么这是一个问题？

让我们回到基础。排名模型的目标是展示最相关的内容，按参与概率的顺序排序。这些模型是基于隐式用户数据进行训练的：每次用户点击搜索结果页面上的一个项目或参与界面时，我们将该点击作为下一个模型训练迭代中的正标签。

如果用户只是因为内容的排名而非其相关性而开始与内容互动，我们的训练数据就会被污染：模型不仅仅是从用户真正想要的东西中学习，而是从自身过去的预测中学习。随着时间的推移，预测会变得静态，缺乏多样性。结果，用户可能会感到厌倦或烦恼，并最终转向其他地方。

另一个位置偏差的问题是离线测试变得不可靠。根据定义，位置偏倚的用户参与数据总是会偏向现有的生产模型，因为这是生成用户看到的排名的模型。一个实际上更好的新模型在离线测试中可能看起来更差，可能会被过早地丢弃。只有[在线测试](/is-my-model-really-better-560e729f81d2)才能揭示真相。

## 我们如何减轻位置偏差？

模型从数据中学习，因此为了去偏模型，我们需要去偏训练数据。正如[Joachims et al](https://arxiv.org/abs/1608.04468)（2016）所示，这可以通过根据位置偏差的倒数加权每个训练样本来实现，为低偏差的样本赋予更多权重，为高偏差的样本赋予较少的权重。直观地，这很有意义：点击排名第一的项目（具有高位置偏差）可能比点击第十个项目（具有低位置偏差）信息量少。

因此，减轻位置偏差的问题归结为测量它。我们如何做到这一点？

一种方法是**结果随机化**：对于服务人群中的一个小子集，简单地随机重新排序前 N 项，然后测量在该人群中排名变化所引起的参与度变化。这种方法有效，但成本较高：随机搜索结果或推荐，尤其是对于较大的 N，会导致用户体验较差，从而影响用户留存率和商业收入。

因此，更好的替代方法可能是**干预采集**，由[Argawal et al](https://arxiv.org/pdf/1812.05161.pdf)（2018）在全文档搜索的背景下提出，同时由[Aslanyan et al](https://arxiv.org/pdf/1812.09338.pdf)（2019）在电子商务搜索的背景下提出。关键思想是，成熟排名系统中记录的用户参与数据已经包含了来自多个不同排名模型的排名，例如来自历史 A/B 测试或仅仅来自时间上推出的不同版本的生产模型。这种历史多样性在排名中创造了固有的随机性，我们可以“采集”这些数据来估计位置偏差，而无需任何昂贵的干预。

最后，还有一个更简单的想法，即谷歌的“[规则 36](https://developers.google.com/machine-learning/guides/rules-of-ml#rule_36_avoid_feedback_loops_with_positional_features)”。他们建议在训练模型时将排名本身作为另一个特征添加，然后在推断时将该特征设置为默认值（例如 -1）。直觉是，通过提前将所有信息提供给模型，它会在后台隐式地学习参与模型和位置偏见模型。无需额外步骤。

## 最终思考

让我们回顾一下。位置偏见是一个在整个行业中都被观察到的真实问题。它之所以成问题，是因为它可能会限制排名模型的多样性。但我们可以通过使用偏见估计对训练数据进行去偏差来减轻它，这些偏见估计可以通过结果随机化或干预采集获得。另一种减轻策略是直接将排名作为模型特征，并让模型隐式地学习偏见，无需额外步骤。

从整体上考虑，位置偏见的存在确实有些讽刺。如果我们不断改进我们的排名模型，这些改进可能会导致越来越多的用户盲目相信排名最高的结果，从而增强位置偏见，并**最终**降低我们的模型效果。除非我们采取有意识的步骤来监测和减轻位置偏见，否则任何模型改进最终可能会变得适得其反。

[](https://samflender.gumroad.com/l/mlontheground?source=post_page-----1615895c80a9--------------------------------) [## 现实中的机器学习：真实世界 ML 应用的设计与操作

### 这本书是什么？机器学习最奇特的事情之一是学术 ML 研究的二分法…

samflender.gumroad.com](https://samflender.gumroad.com/l/mlontheground?source=post_page-----1615895c80a9--------------------------------)
