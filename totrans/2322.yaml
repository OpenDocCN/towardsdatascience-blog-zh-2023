- en: What Are the Data-Centric AI Concepts behind GPT Models?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727](https://towardsdatascience.com/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unpacking the data-centric AI techniques used in ChatGPT and GPT-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@a0987284901?source=post_page-----a590071bb727--------------------------------)[![Henry
    Lai](../Images/eaa1b4eb6f6cebc131f4cf0cfdd4cda7.png)](https://medium.com/@a0987284901?source=post_page-----a590071bb727--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a590071bb727--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a590071bb727--------------------------------)
    [Henry Lai](https://medium.com/@a0987284901?source=post_page-----a590071bb727--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a590071bb727--------------------------------)
    ·8 min read·Mar 29, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa65e6643cb2712bc70bc825d8ba03a3.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://arxiv.org/abs/2303.10158](https://arxiv.org/abs/2303.10158). Image
    by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Intelligence (AI) has made incredible strides in transforming the
    way we live, work, and interact with technology. Recently, that one area that
    has seen significant progress is the development of Large Language Models (LLMs),
    such as [GPT-3](https://arxiv.org/abs/2005.14165), [ChatGPT](https://openai.com/blog/chatgpt),
    and [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf). These models are capable
    of performing tasks such as language translation, text summarization, and question-answering
    with impressive accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: While it’s difficult to ignore the increasing model size of LLMs, it’s also
    important to recognize that their success is due largely to the large amount and
    high-quality data used to train them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will present an overview of the recent advancements in
    LLMs from a data-centric AI perspective, drawing upon insights from our recent
    survey papers [1,2] with corresponding technical resources on [GitHub](https://github.com/daochenzha/data-centric-AI).
    Particularly, we will take a closer look at GPT models through the lens of [data-centric
    AI](https://github.com/daochenzha/data-centric-AI), a growing concept in the data
    science community. We’ll unpack the data-centric AI concepts behind GPT models
    by discussing three data-centric AI goals: [training data development, inference
    data development, and data maintenance](https://arxiv.org/abs/2303.10158).'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) and GPT Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are a type of Natual Language Processing model that are trained to infer
    words within a context. For example, the most basic function of an LLM is to predict
    missing tokens given the context. To do this, LLMs are trained to predict the
    probability of each token candidate from massive data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/960f620655f6281a5eab620cc245b37c.png)'
  prefs: []
  type: TYPE_IMG
- en: An illustrative example of predicting the probabilities of missing tokens with
    an LLM within a context. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: GPT models refer to a series of LLMs created by OpenAI, such as [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf),
    [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf),
    [GPT-3](https://arxiv.org/abs/2005.14165), [InstructGPT](https://arxiv.org/abs/2203.02155),
    and [ChatGPT/GPT-4.](https://cdn.openai.com/papers/gpt-4.pdf) Just like other
    LLMs, GPT models’ architectures are largely based on [Transformers](https://arxiv.org/abs/1706.03762),
    which use text and positional embeddings as input, and attention layers to model
    tokens’ relationships.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/256a7a43daaf08de5607d27bf8348635.png)'
  prefs: []
  type: TYPE_IMG
- en: GPT-1 model architecture. Image from the paper [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: The later GPT models use similar architectures as GPT-1, except for using more
    model parameters with more layers, larger context length, hidden layer size, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48108b80ae1227f06f83795294cd1a56.png)'
  prefs: []
  type: TYPE_IMG
- en: Models size comparison of GPT models. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: What is data-centric AI?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Data-centric AI](https://github.com/daochenzha/data-centric-AI) is an emerging
    new way of thinking about how to build AI systems. It has been advocated by Andrew
    Ng, an AI pioneer.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data-centric AI is the discipline of systematically engineering the data used
    to build an AI system. — Andrew Ng*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the past, we mainly focused on creating better models with data largely unchanged
    (model-centric AI). However, this approach can lead to problems in the real world
    because it doesn’t consider the different problems that may arise in the data,
    such as inaccurate labels, duplicates, and biases. As a result, “overfitting”
    a dataset may not necessarily lead to better model behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, data-centric AI focuses on improving the quality and quantity of
    data used to build AI systems. This means that the attention is on the data itself,
    and the models are relatively more fixed. Developing AI systems with a data-centric
    approach holds more potential in real-world scenarios, as the data used for training
    ultimately determines the maximum capability of a model.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that “data-centric” differs fundamentally from “data-driven”,
    as the latter only emphasizes the use of data to guide AI development, which typically
    still centers on developing models rather than engineering data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/682d043538c4955991ee62d93f9ad98a.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison between data-centric AI and model-centric AI. [https://arxiv.org/abs/2301.04819](https://arxiv.org/abs/2301.04819)
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [data-centric AI framework](https://github.com/daochenzha/data-centric-AI)
    consists of three goals:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training data development** is to collect and produce rich and high-quality
    data to support the training of machine learning models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference data development** is to create novel evaluation sets that can
    provide more granular insights into the model or trigger a specific capability
    of the model with engineered data inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data maintenance** is to ensure the quality and reliability of data in a
    dynamic environment. Data maintenance is critical as data in the real world is
    not created once but rather necessitates continuous maintenance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8b70bbf9b3ae3d73ee0a67793217d787.png)'
  prefs: []
  type: TYPE_IMG
- en: Data-centric AI framework. [https://arxiv.org/abs/2303.10158](https://arxiv.org/abs/2303.10158).
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Why Data-centric AI Made GPT Models Successful
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Months earlier, Yann LeCun tweeted that ChatGPT was nothing new. Indeed, all
    techniques (transformer, reinforcement learning from human feedback, etc.) used
    in ChatGPT and GPT-4 are not new at all. However, they did achieve incredible
    results that previous models couldn’t. So, what is the driving force of their
    success?
  prefs: []
  type: TYPE_NORMAL
- en: '**Training data development.** The quantity and quality of the data used for
    training GPT models have seen a significant increase through better data collection,
    data labeling, and data preparation strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-1:** [BooksCorpus dataset](https://huggingface.co/datasets/bookcorpus)
    is used in training. This dataset contains 4629.00 MB of raw text, covering books
    from a range of genres such as Adventure, Fantasy, and Romance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**- *Data-centric AI strategies*:** None.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**- *Result:*** Pertaining GPT-1 on this dataset can increase performances
    on downstream tasks with fine-tuning.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**GPT-2:** [WebText](https://paperswithcode.com/dataset/webtext) is used in
    training. This is an internal dataset in OpenAI created by scraping outbound links
    from Reddit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**- *Data-centric AI strategies:*** (1) Curate/filter data by only using the
    outbound links from Reddit, which received at least 3 karma. (2) Use tools [Dragnet](https://dl.acm.org/doi/abs/10.1145/2487788.2487828)
    and [Newspaper](https://github.com/codelucas/newspaper) to extract clean contents.
    (3) Adopt de-duplication and some other heuristic-based cleaning (details not
    mentioned in the paper)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**- *Result:*** 40 GB of text is obtained after filtering. GPT-2 achieves strong
    zero-shot results without fine-tuning.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**GPT-3:** The training of GPT-3 is mainly based on [Common Crawl](https://commoncrawl.org/the-data/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**- *Data-centric AI strategies:***(1) Train a classifier to filter out low-quality
    documents based on the similarity of each document to WebText, a proxy for high-quality
    documents. (2) Use Spark’s MinHashLSH to fuzzily deduplicate documents. (3) Augment
    the data with WebText, books corpora, and Wikipedia.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '***- Result:*** 570GB of text is obtained after filtering from 45TB of plaintext
    (only 1.27% of data is selected in this quality filtering). GPT-3 significantly
    outperforms GPT-2 in the zero-shot setting.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**InstructGPT:** Let humans evaluate the answer to tune GPT-3 so that it can
    better align with human expectations. They have designed tests for annotators,
    and only those who can pass the tests are eligible to annotate. They have even
    designed a survey to ensure that the annotators enjoy the annotating process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**- *Data-centric AI strategies:***(1) Use human-provided answers to prompts
    to tune the model with supervised training. (2) Collect comparison data to train
    a reward model and then use this reward model to tune GPT-3 with reinforcement
    learning from human feedback (RLHF).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '***- Result:*** InstructGPT shows better truthfulness and less bias, i.e.,
    better alignment.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ChatGPT/GPT-4:** The details are not disclosed by OpenAI. But it is known
    that ChatGPT/GPT-4 largely follow the design of previous GPT models, and they
    still use RLHF to tune models (with potentially more and higher quality data/labels).
    It is commonly believed that GPT-4 used an even larger dataset, as the model weights
    have been increased.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference data development.** As recent GPT models are already sufficiently
    powerful, we can achieve various goals by tuning prompts (or tuning inference
    data) with the model fixed. For example, we can conduct text summarization by
    offering the text to be summarized alongside an instruction like “summarize it”
    or “TL;DR” to steer the inference process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/002c7658b5abc0122c062efbae7b3852.png)'
  prefs: []
  type: TYPE_IMG
- en: Prompt tuning. [https://arxiv.org/abs/2303.10158](https://arxiv.org/abs/2303.10158).
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the proper prompts for inference is a challenging task. It heavily
    relies on heuristics. A nice [survey](https://arxiv.org/abs/2107.13586) has summarized
    different promoting methods. Sometimes, even semantically similar prompts can
    have very diverse outputs. In this case, [Soft Prompt-Based Calibration](https://arxiv.org/abs/2303.13035v1)
    may be required to reduce variance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ec843a99cc26c78f25d50e5ac8ebd5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Soft prompt-based calibration. Image from the paper [https://arxiv.org/abs/2303.13035v1](https://arxiv.org/abs/2303.13035v1)
    with original authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: The research of inference data development for LLMs is still in its early stage.
    More [inference data development techniques that have been used in other tasks](https://arxiv.org/abs/2303.10158)
    could be applied in LLMs in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data maintenance.** ChatGPT/GPT-4, as a commercial product, is not only trained
    once but rather is updated continuously and maintained. Clearly, we can’t know
    how data maintenance is executed outside of OpenAI. So, we discuss some general
    data-centric AI strategies that are or will be very likely used for GPT models:'
  prefs: []
  type: TYPE_NORMAL
- en: '***- Continuous data collection:*** When we use ChatGPT/GPT-4, our prompts/feedback
    could be, in turn, used by OpenAI to further advance their models. Quality metrics
    and assurance strategies may have been designed and implemented to collect high-quality
    data in this process.'
  prefs: []
  type: TYPE_NORMAL
- en: '***- Data understanding tools:*** Various tools could have been developed to
    visualize and comprehend user data, facilitating a better understanding of users’
    requirements and guiding the direction of future improvements.'
  prefs: []
  type: TYPE_NORMAL
- en: '***- Efficient data processing:*** As the number of users of ChatGPT/GPT-4
    grows rapidly, an efficient data administration system is required to enable fast
    data acquisition.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce19edca589650b43eca2390c3615710.png)'
  prefs: []
  type: TYPE_IMG
- en: ChatGPT/GPT-4 collects user feedback with “thumb up” and “thumb down” to further
    evolve their system. Screenshot from [https://chat.openai.com/chat](https://chat.openai.com/chat).
  prefs: []
  type: TYPE_NORMAL
- en: What Can the Data Science Community Learn from this Wave of LLMs?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The success of LLMs has revolutionized AI. Looking forward, LLMs could further
    revolutionize the data science lifecycle. We make two predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data-centric AI becomes even more important.** After years of research, the
    model design is already very mature, especially after Transformer. Engineering
    data becomes a crucial (or possibly the only) way to improve AI systems in the
    future. Also, when the model becomes sufficiently powerful, we don’t need to train
    models in our daily work. Instead, we only need to design the proper inference
    data (prompt engineering) to probe knowledge from the model. Thus, the research
    and development of data-centric AI will drive future advancements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLMs will enable better data-centric AI solutions.** Many of the tedious
    data science works could be performed much more efficiently with the help of LLMs.
    For example, ChaGPT/GPT-4 can already write workable codes to process and clean
    data. Additionally, LLMs can even be used to create data for training. For example,
    [recent work](https://arxiv.org/abs/2303.04360) has shown that generating synthetic
    data with LLMs can boost model performance in clinical text mining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/0ba4a3065f5e08b2d7c60e0b9da4c6d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Generating synthetic data with LLMs to train the model. Image from the paper
    [https://arxiv.org/abs/2303.04360](https://arxiv.org/abs/2303.04360) with the
    original authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I hope this article can inspire you in your own work. You can learn more about
    the data-centric AI framework and how it benefits LLMs in the following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] [Data-centric Artificial Intelligence: A Survey](https://arxiv.org/abs/2303.10158)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] [Data-centric AI: Perspectives and Challenges](https://arxiv.org/abs/2301.04819)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have maintained a [GitHub repo](https://github.com/daochenzha/data-centric-AI),
    which will regularly update the relevant data-centric AI resources. Stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: In the later articles, I will delve into the three goals of data-centric AI
    (training data development, inference data development, and data maintenance)
    and introduce the representative methods.
  prefs: []
  type: TYPE_NORMAL
