- en: 'Unsupervised data pruning: less data to learn better'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督数据剪枝：更少的数据学习更好
- en: 原文：[https://towardsdatascience.com/unsupervised-data-pruning-less-data-to-learn-better-30cd2bfbd855](https://towardsdatascience.com/unsupervised-data-pruning-less-data-to-learn-better-30cd2bfbd855)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unsupervised-data-pruning-less-data-to-learn-better-30cd2bfbd855](https://towardsdatascience.com/unsupervised-data-pruning-less-data-to-learn-better-30cd2bfbd855)
- en: Foundation models | Scaling law | Large models | Data pruning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础模型 | 扩展法则 | 大型模型 | 数据剪枝
- en: Not always more data is meaning a more accurate model, but how to choose your
    data?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据量的增加并不总是意味着模型更准确，那么如何选择数据呢？
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----30cd2bfbd855--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----30cd2bfbd855--------------------------------)[](https://towardsdatascience.com/?source=post_page-----30cd2bfbd855--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----30cd2bfbd855--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----30cd2bfbd855--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----30cd2bfbd855--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----30cd2bfbd855--------------------------------)[](https://towardsdatascience.com/?source=post_page-----30cd2bfbd855--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----30cd2bfbd855--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----30cd2bfbd855--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----30cd2bfbd855--------------------------------)
    ·11 min read·Feb 27, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----30cd2bfbd855--------------------------------)
    ·11分钟阅读·2023年2月27日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1c41d0d7b79c96998daaf3dda4a30237.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c41d0d7b79c96998daaf3dda4a30237.png)'
- en: image by the author using [DALL-E](https://openai.com/dall-e-2/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用[DALL-E](https://openai.com/dall-e-2/)生成
- en: Scaling law has been observed in different contexts (pictures, text, language,
    speech, and so on). Is increasing the number of parameters really the only recipe
    for a better model? And if not what you can actually do?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展法则在不同的背景下（图像、文本、语言、语音等）都有观察到。增加参数数量是否真的是提高模型性能的唯一秘诀？如果不是，你实际可以做些什么？
- en: What is scaling law and why is it problematic
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是扩展法则，它为何成为问题？
- en: In recent years we have seen the number of parameters in models increase by
    leaps and bounds. All the biggest companies have been pushing to create more and
    more capable models. This has led to a reduced error in benchmark datasets and
    the emergence of unanticipated behavior. **But what is scaling law?**
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，我们看到模型中的参数数量飞跃式增长。所有大型公司都在不断推进创建更强大的模型。这导致了基准数据集误差的减少和意外行为的出现。**但什么是扩展法则？**
- en: '**In short, the scaling law states that the “test error often falls off as
    a power law with either the amount of training data, model size, or compute.”**
    Put another way, to improve the performance of a model one must increase one of
    these three factors: the number of examples during training, the number of parameters,
    or the duration of training.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**简而言之，扩展法则指出“测试误差通常会随着训练数据量、模型规模或计算量的增加而呈幂律下降。”** 换句话说，要提高模型的性能，必须增加这三者中的任意一个：训练时的示例数量、参数数量或训练时长。'
- en: Previous technical studies suggested that test loss could decrease as a power
    law of the training dataset. This was formally defined in 2017 when [Hestness
    explored this idea](https://arxiv.org/pdf/1712.00409.pdf) in different machine-learning
    domains (machine translation, language modeling, image processing, and speech
    recognition).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的技术研究表明，测试损失可能会随着训练数据集的增大而呈幂律下降。2017年，[Hestness探讨了这一观点](https://arxiv.org/pdf/1712.00409.pdf)在不同的机器学习领域（机器翻译、语言建模、图像处理和语音识别）。
- en: The scaling law was then defined in an OpenAI article where they showed that
    by increasing the size of the model or dataset or the amount of computing used
    for training, performance improved.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展法则随后在OpenAI的一篇文章中定义，他们展示了通过增加模型大小、数据集规模或训练计算量，性能得到了提升。
- en: '![](../Images/248dec3b7b7d462be4e99ed3e97c4469.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/248dec3b7b7d462be4e99ed3e97c4469.png)'
- en: 'scaling law as shown by OpenAI (source: [here](https://arxiv.org/pdf/2001.08361.pdf))'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展法则由OpenAI展示（来源：[这里](https://arxiv.org/pdf/2001.08361.pdf)）
- en: 'Formally they wrote:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 他们正式写道：
- en: 'Model performance depends most strongly on scale, which consists of three factors:
    the number of model parameters N (excluding embeddings), the size of the dataset
    D, and the amount of compute C used for training. Within reasonable limits, performance
    depends very weakly on other architectural hyperparameters such as depth vs. width.
    — [source](https://arxiv.org/abs/2001.08361)'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 模型性能最强烈地依赖于规模，这包括三个因素：模型参数数量 N（不包括嵌入）、数据集大小 D 和用于训练的计算量 C。在合理的范围内，性能对其他架构超参数（如深度与宽度）的依赖非常弱。—
    [source](https://arxiv.org/abs/2001.08361)
- en: In summary, they assert that performance has a power-law relationship with each
    of these three factors. In addition, if you increase the number of parameters
    N you must also increase the number of examples D in tandem, or a penalty (overfitting)
    emerges. There is a relationship between N and D whereby you have to increase
    the amount of data by 5 times if you increase the number of parameters by 8 times.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，他们断言性能与这三种因素中的每一种都有幂律关系。此外，如果增加参数数量 N，你也必须相应增加示例数量 D，否则会出现惩罚（过拟合）。N 和 D
    之间存在一种关系，即如果将参数数量增加 8 倍，你必须将数据量增加 5 倍。
- en: This concept was taken to extremes with [GPT-3](https://arxiv.org/abs/2005.14165)
    and later models ([Google’s LaMDA](https://blog.google/technology/ai/lamda/) reached
    more than 500 B parameters). These models have shown incredible capabilities,
    and some have speculated that massively increasing the number of parameters will
    give us general intelligence. **And will it?**
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这一概念在 [GPT-3](https://arxiv.org/abs/2005.14165) 和后续模型（[Google的LaMDA](https://blog.google/technology/ai/lamda/)
    达到了5000亿参数以上）中被推向极端。这些模型展示了惊人的能力，有些人猜测大幅增加参数数量将带来通用智能。**那么会吗？**
- en: No. In a nutshell, [neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network)
    are pattern-matching machines (or universal approximators). What neural networks
    do is find patterns that they saw in the training set. A neural network that is
    larger and has been trained with more data may contain more patterns in its parameters
    and may recognize more of them. **So infinite data? Well, the data is not infinite.**
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不。简而言之，[神经网络](https://en.wikipedia.org/wiki/Artificial_neural_network) 是模式匹配机器（或通用逼近器）。神经网络的作用是找到它们在训练集中见过的模式。一个更大并且经过更多数据训练的神经网络可能包含更多的模式并且可能识别更多的模式。**那么无限数据？实际上，数据并不是无限的。**
- en: Such power law scaling has motivated significant societal investments in data
    collection, compute, and associated energy consumption. However, power law scaling
    is extremely weak and unsustainable. — [source](https://arxiv.org/pdf/2206.14486.pdf)
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这种幂律扩展促使了对数据收集、计算和相关能源消耗的大量社会投资。然而，幂律扩展是极其微弱且不可持续的。— [source](https://arxiv.org/pdf/2206.14486.pdf)
- en: It can be seen in the OpenAI article that it takes an order of magnitude of
    data or parameters or compute to decrease the error by a mere 3–2 %. Also, to
    scale vision transformers it took two billion data points to get a few more accuracy
    points in [ImageNet](https://ieeexplore.ieee.org/abstract/document/5206848).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从 OpenAI 的文章中可以看出，需要一个数量级的数据、参数或计算才能将误差降低仅仅 3–2%。此外，为了扩展视觉变换器，需用到二十亿数据点才能在 [ImageNet](https://ieeexplore.ieee.org/abstract/document/5206848)
    上获得更多的准确性。
- en: '![](../Images/29b064230b826eb3b037211ce4e3da47.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29b064230b826eb3b037211ce4e3da47.png)'
- en: source ([here](https://arxiv.org/pdf/2106.04560.pdf))
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: source ([here](https://arxiv.org/pdf/2106.04560.pdf))
- en: '**To recap, so far we have witnessed a paradigm that states more is better.
    But is it the only strategy?**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结一下，到目前为止我们见证了一种认为更多即更好的范式。但这真的是唯一的策略吗？**'
- en: “can we achieve exponential scaling instead, with a good strategy for selecting
    training examples?”
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们能否通过一种好的选择训练样本的策略实现指数扩展？”
- en: Meanwhile, it can be said that many of the data are redundant. The model often
    sees many similar examples. Datasets are often acquired by randomly downloading
    thousands and thousands of examples from the Internet. Previous studies have shown
    that one can, for example, sort the examples in the training set in order of increasing
    difficulty (easy and redundant examples) down to the most difficult ones. One
    can reduce the number of examples in the dataset while maintaining performance
    (in other words, easy and redundant examples only consume training cycles without
    adding anything to learning).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，可以说许多数据是冗余的。模型通常看到许多相似的示例。数据集通常通过随机从互联网下载成千上万的示例来获取。以往的研究表明，例如，可以按难度递增的顺序对训练集中的示例进行排序（从简单和冗余的示例到最困难的示例）。可以在保持性能的同时减少数据集中的示例数量（换句话说，简单和冗余的示例只消耗训练周期，而没有任何学习增益）。
- en: 'More about these concepts, how efficiently scale vision transformers and the
    difference between model-centric and data-centric AI:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些概念的更多信息，如何高效地缩放视觉变换器，以及以模型为中心和以数据为中心的人工智能之间的区别：
- en: '[](/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6?source=post_page-----30cd2bfbd855--------------------------------)
    [## Why Do We Have Huge Language Models and Small Vision Transformers?'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 为什么我们有巨大语言模型和小型视觉变换器？](/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6?source=post_page-----30cd2bfbd855--------------------------------)'
- en: Google ViT-22 paves the way for new large transformers and to revolutionize
    computer vision
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google ViT-22 为新的大型变换器铺平了道路，并将彻底改变计算机视觉
- en: towardsdatascience.com](/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6?source=post_page-----30cd2bfbd855--------------------------------)
    [](/a-critical-analysis-of-your-dataset-2b388e7ca01e?source=post_page-----30cd2bfbd855--------------------------------)
    [## A critical analysis of your dataset
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 对你的数据集的关键分析](/a-critical-analysis-of-your-dataset-2b388e7ca01e?source=post_page-----30cd2bfbd855--------------------------------)'
- en: 'Stop finetuning your model: your model is already good, but not your data'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 停止微调你的模型：你的模型已经很好，但数据不够好
- en: towardsdatascience.com](/a-critical-analysis-of-your-dataset-2b388e7ca01e?source=post_page-----30cd2bfbd855--------------------------------)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/a-critical-analysis-of-your-dataset-2b388e7ca01e?source=post_page-----30cd2bfbd855--------------------------------)'
- en: '**Unsupervised data pruning: can you remove useless training data points without
    knowing the labels?**'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**无监督数据剪枝：你能在不知道标签的情况下去除无用的训练数据点吗？**'
- en: Previous studies have left different questions open. Meanwhile, whether a power
    law of error with respect to data can also be defined (exponentially reducing
    the number of examples without sacrificing performance). Second, the strategies
    described above require that the collected examples be labeled anyway (time-consuming
    and expensive). Therefore, an optimal strategy should be unsupervised.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究留下了不同的问题悬而未决。同时，是否也可以定义一个关于数据的误差幂律（在不牺牲性能的情况下指数减少示例数量）。其次，上述策略要求所收集的示例必须被标记（费时且昂贵）。因此，最终的策略应该是无监督的。
- en: 'A paper was recently published that tries to answer these questions. The study
    was conducted by a collaboration between META, Stanford, and the University of
    Tubingen:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最近发表了一篇论文试图回答这些问题。该研究由META、斯坦福大学和图宾根大学的合作进行：
- en: '[](https://arxiv.org/abs/2206.14486?source=post_page-----30cd2bfbd855--------------------------------)
    [## Beyond neural scaling laws: beating power law scaling via data pruning'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 超越神经网络规模定律：通过数据剪枝战胜幂律缩放](https://arxiv.org/abs/2206.14486?source=post_page-----30cd2bfbd855--------------------------------)'
- en: Widely observed neural scaling laws, in which error falls off as a power of
    the training set size, model size, or both…
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 广泛观察到的神经网络规模定律，其中误差作为训练集大小、模型大小或两者的幂次递减…
- en: arxiv.org](https://arxiv.org/abs/2206.14486?source=post_page-----30cd2bfbd855--------------------------------)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2206.14486?source=post_page-----30cd2bfbd855--------------------------------)'
- en: '**The authors started from the fact that the scaling law is inefficient**:
    The exponents of the power law are close to zero (suggesting inefficient use of
    resources). Moreover, an increase in the parameters or amount of data leads to
    a minimal reduction in error. **What we would like is that one can prune a dataset
    without altering the performance of the model, even if the dataset is unlabeled**
    (after all, labeling a dataset is one of the most expensive and time-consuming
    operations). **How?**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者从扩展法则低效这一事实出发**：幂律的指数接近零（这表明资源使用不充分）。此外，参数或数据量的增加对错误的减少影响很小。**我们希望能够在不改变模型性能的情况下修剪数据集，即使数据集是未标注的**（毕竟，标注数据集是最昂贵和耗时的操作之一）。**怎么做？**'
- en: The authors explored this possibility in the case of the teacher-student setting.
    This is a training method in which you have a model that has been trained with
    a large number of examples (not restricted to a CNN even though it is one of the
    most widely used cases). You provide some data to the pre-trained, larger model
    (teacher) and train the smaller model using instead of original labels the teacher’s
    output (the teacher provides class probabilities as output, also called soft labels).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在教师-学生设置的情况下探索了这种可能性。这是一种训练方法，其中你有一个已经用大量示例训练过的模型（尽管它是最广泛使用的情况之一，但不限于CNN）。你向预训练的大模型（教师）提供一些数据，并使用教师的输出（教师提供类别概率作为输出，也称为软标签）训练较小的模型。
- en: in a nutshell, the authors used [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)
    as a dataset, obtained probabilities from a teacher model, and trained a student
    model for a few epochs using the teacher’s output probabilities as labels. They
    then calculated the margin between student and teacher outputs (which provide
    a measure of the learning difficulty of an example for a model).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，作者使用了[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)作为数据集，从教师模型中获得概率，并使用教师的输出概率作为标签训练了学生模型几个时期。然后，他们计算了学生和教师输出之间的边际（这提供了模型学习示例难度的度量）。
- en: '![](../Images/ec01c84d699123032f1a2f3e10b1ac55.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec01c84d699123032f1a2f3e10b1ac55.png)'
- en: 'image source: [original article](https://arxiv.org/pdf/2206.14486.pdf)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[原始文章](https://arxiv.org/pdf/2206.14486.pdf)
- en: '**The first interesting result is when the number of examples in the training
    set is large it is better to keep the difficult examples while pruning** (and
    thus reduce the easy examples). When you have few examples in the training set
    it is better to prune the difficult examples. This may seem counterintuitive.
    The authors note that the easy examples provide coarse-grained information about
    the target function (basically the general patterns of the dataset). In contrast,
    the more difficult examples provide fine-tuned information that might be missed
    in a large dataset.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一个有趣的结果是，当训练集中的示例数量较多时，保留困难的示例而修剪（从而减少容易的示例）更好**。当你在训练集中有少量示例时，修剪困难的示例更好。这可能看起来有悖直觉。作者指出，容易的示例提供了关于目标函数的粗略信息（基本上是数据集的一般模式）。相反，更困难的示例提供了可能在大数据集中被遗漏的细化信息。'
- en: 'Intuitively, in the limited data regime, it is challenging to model outliers
    since the basics are not adequately captured; hence, it is more important to keep
    easy examples so that the model can get to moderate error. However, with a larger
    dataset, the easy examples can be learned without difficulty, making modeling
    outliers the fundamental challenge. source: [here](https://arxiv.org/abs/2206.14486)'
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 直观地说，在有限的数据情况下，由于基础未得到充分捕捉，建模异常值是具有挑战性的；因此，保留容易的示例更为重要，以便模型能够达到适中的错误。然而，随着数据集的增大，容易的示例可以轻松学习，使得建模异常值成为根本挑战。来源：[这里](https://arxiv.org/abs/2206.14486)
- en: In other words, when there is little data it is better for the model to learn
    general information about the patterns in the data, while when there is a lot
    of data the hard examples help the model to better understand the decision boundary
    between classes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，当数据量较少时，模型更适合学习关于数据模式的一般信息，而当数据量较大时，困难的示例有助于模型更好地理解类别之间的决策边界。
- en: Also from an [information-theoretic perspective](https://en.wikipedia.org/wiki/Information_theory),
    the authors suggest that data pruning increases the information obtained from
    each individual example as uninformative examples are filtered.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从[信息论的角度](https://en.wikipedia.org/wiki/Information_theory)来看，作者建议数据修剪可以增加从每个单独示例中获得的信息，因为无信息的示例会被过滤掉。
- en: Data pruning improves transfer learning
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据修剪提高了迁移学习
- en: '![](../Images/5253d8baf8ea1c41067763087fd078d8.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5253d8baf8ea1c41067763087fd078d8.png)'
- en: 'image from Pixabay (source: [here](https://pixabay.com/it/photos/potatura-di-rose-potatura-7470829/))'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自Pixabay（来源：[这里](https://pixabay.com/it/photos/potatura-di-rose-potatura-7470829/)）
- en: One of the reasons for our interest in scaling law is that we are interested
    in having foundation models. A [**foundation model**](https://en.wikipedia.org/wiki/Foundation_models)
    is a wide model (transformer, vision transformer, and so on) that has been trained
    with a large amount of unlabeled data and then can be used for different downstream
    tasks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对扩展法则感兴趣的原因之一是我们对拥有基础模型感兴趣。一个 [**基础模型**](https://en.wikipedia.org/wiki/Foundation_models)
    是一个宽模型（例如变换器、视觉变换器等），它已使用大量未标记数据进行训练，然后可以用于不同的下游任务。
- en: Simply put, we train a very wide model on a large amount of data, and then through
    transfer learning, we adopt it for another task. Examples are [BERT](https://arxiv.org/abs/1810.04805),
    GPT3 for text tasks, and [ResNet](https://en.wikipedia.org/wiki/Residual_neural_network)
    for computer vision tasks (actually DALL-E and [stable diffusion](https://arxiv.org/abs/2112.10752)
    also have a pre-trained language model as a component).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们在大量数据上训练一个非常宽的模型，然后通过迁移学习，将其应用于另一任务。例子有 [BERT](https://arxiv.org/abs/1810.04805)、GPT3用于文本任务，以及
    [ResNet](https://en.wikipedia.org/wiki/Residual_neural_network) 用于计算机视觉任务（实际上，DALL-E
    和 [稳定扩散](https://arxiv.org/abs/2112.10752) 也将预训练语言模型作为一个组件）。
- en: '**Training a foundation model is extremely expensive, and so far we have tried
    to increase its parameters and the amount of data used for training.** However,
    [DeepMind’s Chinchilla](https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training)
    and subsequent studies show that improving the data used for training would perhaps
    be more useful. So the authors of this study wondered: **could data pruning help
    with transfer learning?**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练基础模型极其昂贵，到目前为止，我们尝试增加其参数和用于训练的数据量。** 然而，[DeepMind的Chinchilla](https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training)
    及其后续研究表明，改善用于训练的数据可能会更有用。因此，这项研究的作者们想知道：**数据修剪是否可以帮助迁移学习？**'
- en: The authors used a pre-trained [vision transformer (ViT)](https://en.wikipedia.org/wiki/Vision_transformer)
    and then fine-tuned it on a pruned subset of 10 % of CIFAR-10\. This approach
    worked better than fine-tuning the ViT on the entire CIFAR-10\. In addition, the
    authors pre-trained ResNet50 on different pruned subsets of ImageNet (a reduced
    version) and then conducted fine-tuning on CIFAR-10\. The result shows that training
    a model on a pruned dataset has better performance than using the whole ImageNet.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了一个预训练的 [视觉变换器 (ViT)](https://en.wikipedia.org/wiki/Vision_transformer)，然后在10%的CIFAR-10的修剪子集上进行了微调。这种方法比在整个CIFAR-10上微调ViT效果更好。此外，作者在不同的修剪子集（一个缩减版本）上预训练了ResNet50，然后在CIFAR-10上进行了微调。结果显示，在修剪数据集上训练模型比使用整个ImageNet的效果更好。
- en: '![](../Images/ffa1b299f1bc2fdd89dcb861b6596866.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffa1b299f1bc2fdd89dcb861b6596866.png)'
- en: 'image source: [original article](https://arxiv.org/pdf/2206.14486.pdf)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[原始文章](https://arxiv.org/pdf/2206.14486.pdf)
- en: Thus intriguingly pruning pre-training data on an upstream task can still maintain
    high performance on a different downstream task. Overall these results demonstrate
    the promise of data pruning in transfer learning for both the pre-training and
    fine-tuning phases.
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，令人感兴趣的是，在上游任务中修剪预训练数据仍然可以在不同的下游任务中保持高性能。总体而言，这些结果展示了数据修剪在迁移学习中，无论是在预训练还是微调阶段的潜力。
- en: '[](https://levelup.gitconnected.com/sparsegpt-fewer-parameters-is-better-7b47ad60ac00?source=post_page-----30cd2bfbd855--------------------------------)
    [## SparseGPT: fewer parameters is better?'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/sparsegpt-fewer-parameters-is-better-7b47ad60ac00?source=post_page-----30cd2bfbd855--------------------------------)
    [## SparseGPT: fewer parameters is better?'
- en: How to get rid of 100 billion parameters and happily infer on one GPU
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何去除1000亿个参数并在一个GPU上愉快地进行推断
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/sparsegpt-fewer-parameters-is-better-7b47ad60ac00?source=post_page-----30cd2bfbd855--------------------------------)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/sparsegpt-fewer-parameters-is-better-7b47ad60ac00?source=post_page-----30cd2bfbd855--------------------------------)'
- en: Scaling the approach on a large dataset
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在大型数据集上扩展方法
- en: Previous pruning studies have been done on small datasets but it is important
    to know how these generalize to large datasets. For these reasons, the authors
    benchmarked the various previous approaches on ImageNet and how they impact model
    performance (they chose 8 different approaches).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的剪枝研究主要集中在小数据集上，但了解这些方法在大数据集上的表现也很重要。因此，作者在 ImageNet 上对各种先前的方法进行了基准测试，并评估了它们对模型性能的影响（他们选择了
    8 种不同的方法）。
- en: 'The result shows that these metrics retain only a fraction of the hard examples
    and perform better than random pruning. **But while they perform well in a small
    dataset, only a few of them match the performance obtained by training on the
    full dataset**. Also, note the authors:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，这些指标仅保留了少量的困难样本，并且表现优于随机剪枝。**但尽管它们在小数据集上表现良好，只有少数指标与在完整数据集上训练所获得的性能相匹配**。此外，作者还指出：
- en: 'We found that all pruning metrics amplify class imbalance, which results in
    degraded performance. Fig. 5 shows many data pruning metrics do not scale well
    to ImageNet, while the few that do require substantial amounts of compute. Furthermore,
    all these metrics require labels, thereby limiting their ability to prune data
    for large-scale foundation models trained on massive unlabeled datasets. Thus
    there is a clear need for simple, scalable, self-supervised pruning metrics. source:
    [here](https://arxiv.org/abs/2206.14486)'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们发现所有的剪枝指标都会放大类别不平衡问题，从而导致性能下降。图 5 显示，许多数据剪枝指标在 ImageNet 上的扩展效果不好，而少数能够扩展的指标则需要大量的计算资源。此外，这些指标都需要标签，从而限制了它们对大规模基础模型进行剪枝的能力，这些基础模型通常是在大量未标记的数据集上训练的。因此，迫切需要简单、可扩展的自监督剪枝指标。来源：[这里](https://arxiv.org/abs/2206.14486)
- en: '![](../Images/eb55d35f2fb7ebbb2925282f944b7da7.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb55d35f2fb7ebbb2925282f944b7da7.png)'
- en: 'image source: [original article](https://arxiv.org/pdf/2206.14486.pdf)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[原文文章](https://arxiv.org/pdf/2206.14486.pdf)
- en: 'The authors as a solution propose:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提出了以下解决方案：
- en: The first step is to use a pre-trained model called [SWaV](https://arxiv.org/abs/2006.09882)
    to extract a low-dimensional representation of each example in the dataset.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一步是使用一个名为 [SWaV](https://arxiv.org/abs/2006.09882) 的预训练模型来提取数据集中每个样本的低维表示。
- en: Using k-means clustering they group the representation of the examples.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 k-means 聚类对样本的表示进行分组。
- en: After that, calculate the distance to the center of the cluster using cosine
    distance. If an example is closer to the center of the cluster (thus closer as
    a representation to the others as well) it is considered an easy example to rank,
    but if it is farther from the center it is a difficult example.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，使用余弦距离计算样本到簇中心的距离。如果一个样本离簇中心较近（因此作为表示也更接近其他样本），则它被认为是容易排序的样本；但如果它离中心较远，则被认为是难排序的样本。
- en: Lastly, one can decide to prune a percentage of easy or difficult examples according
    to the case.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，可以根据情况决定剪枝一部分容易或困难的样本。
- en: 'we find our self-supervised prototype metric matches or exceeds the performance
    of the best supervised metric, memorization, until only 70–80% of the data is
    kept, despite the fact that our metric does not use labels and is much simpler
    and cheaper to compute than many previously proposed supervised metrics. source:
    [here](https://arxiv.org/abs/2206.14486)'
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们发现，我们的自监督原型指标与最佳监督指标记忆法的性能相匹配或超过，直到仅保留 70%–80% 的数据，尽管我们的指标不使用标签，并且比许多之前提出的监督指标计算起来更简单、更便宜。来源：[这里](https://arxiv.org/abs/2206.14486)
- en: The results matched the state-of-the-art technique memorization which requires
    labels and it is much slower to compute.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与当前最先进的技术记忆法相符，后者需要标签且计算速度较慢。
- en: '![](../Images/a9ce6547fa3ac216f55471ea9afe86f1.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9ce6547fa3ac216f55471ea9afe86f1.png)'
- en: 'image source: [original article](https://arxiv.org/pdf/2206.14486.pdf)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[原文文章](https://arxiv.org/pdf/2206.14486.pdf)
- en: Conclusions
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The authors show how data pruning can affect errors on par with the scaling
    law. In addition, they show how through unsupervised learning one can obtain a
    [coreset](https://arxiv.org/abs/1910.08707) (a subset of a dataset that allows
    one to train a model as performing equal to the full dataset). **The approach
    is inexpensive, scalable, and does not need labels.**
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作者展示了数据剪枝如何影响与缩放法则一致的错误。此外，他们还展示了通过无监督学习可以获得一个 [coreset](https://arxiv.org/abs/1910.08707)（一个数据集的子集，允许一个模型在该子集上表现与在完整数据集上的表现相同）。**该方法成本低、可扩展，并且不需要标签。**
- en: 'Looking forward, the authors suggest that this approach can still be improved
    and allow even more aggressive pruning. **Which would be extremely useful for
    the training of large foundation models.** They also suggest:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，作者建议这种方法仍然可以改进，并允许更激进的剪枝。**这对大型基础模型的训练将极为有用。**他们还建议：
- en: 'If highly pruned versions of these datasets can be used to train a large number
    of different models, one can conceive of such carefully chosen data subsets as
    foundation datasets in which the initial computational cost of data pruning can
    be amortized across efficiency gains in training many downstream models, just
    at the initial computational cost of training foundation models is amortized across
    the efficiency gains of fine-tuning across many downstream tasks. source: [here](https://arxiv.org/abs/2206.14486)'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果这些高度剪枝的数据集版本可以用来训练大量不同的模型，可以考虑这些精心挑选的数据子集作为基础数据集，其中数据剪枝的初始计算成本可以在许多下游模型的训练效率提升中摊销，就像训练基础模型的初始计算成本在许多下游任务的微调效率提升中摊销一样。来源：[这里](https://arxiv.org/abs/2206.14486)
- en: In conclusion, reducing the dataset before training saves time and cost (less
    labeling work). Not to mention that prospectively, if you reduce the overrepresented
    populations this would help combat and/or identify bias during training.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，减少数据集的规模在训练前可以节省时间和成本（减少标注工作）。更不用说，前瞻性地，如果你减少过度代表的群体，这将有助于在训练过程中对抗和/或识别偏差。
- en: '**What do you guys think? Have you tried pruning your dataset?**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**你们怎么看？你们尝试过剪枝你们的数据集吗？**'
- en: 'If you have found this interesting:'
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得这很有趣：
- en: You can look for my other articles, you can also [**subscribe**](https://salvatore-raieli.medium.com/subscribe)
    to get notified when I publish articles, and you can also connect or reach me
    on[**LinkedIn**](https://www.linkedin.com/in/salvatore-raieli/)**.**
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看我的其他文章，你也可以[**订阅**](https://salvatore-raieli.medium.com/subscribe)以便在我发布文章时收到通知，你也可以在[**LinkedIn**](https://www.linkedin.com/in/salvatore-raieli/)**上联系我**。
- en: Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的 GitHub 仓库的链接，我计划在这里收集与机器学习、人工智能以及更多相关的代码和资源。
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----30cd2bfbd855--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----30cd2bfbd855--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: 机器学习、人工智能、数据科学的教程…]'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习、人工智能、数据科学的教程，包括数学解释和可重复使用的代码（使用 Python…）
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----30cd2bfbd855--------------------------------)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----30cd2bfbd855--------------------------------)'
- en: 'or you may be interested in one of my recent articles:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 或许你会对我最近的一篇文章感兴趣：
- en: '[](https://levelup.gitconnected.com/microsoft-biogpt-towards-the-chatgpt-of-life-science-56e251536af6?source=post_page-----30cd2bfbd855--------------------------------)
    [## Microsoft BioGPT: Towards the ChatGPT of life science?'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/microsoft-biogpt-towards-the-chatgpt-of-life-science-56e251536af6?source=post_page-----30cd2bfbd855--------------------------------)
    [## Microsoft BioGPT: 迈向生命科学领域的 ChatGPT？]'
- en: BioGPT achieves the SOTA in different biomedical NLP tasks
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BioGPT 在不同的生物医学自然语言处理任务中达到了**最先进的水平**。
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/microsoft-biogpt-towards-the-chatgpt-of-life-science-56e251536af6?source=post_page-----30cd2bfbd855--------------------------------)
    [](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2?source=post_page-----30cd2bfbd855--------------------------------)
    [## Everything but everything you need to know about ChatGPT
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/microsoft-biogpt-towards-the-chatgpt-of-life-science-56e251536af6?source=post_page-----30cd2bfbd855--------------------------------)
    [](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2?source=post_page-----30cd2bfbd855--------------------------------)
    [## 关于 ChatGPT 的所有信息]'
- en: what is known, the latest news, what it is impacting, and what is changing.
    all in one article
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解已知的最新消息、其影响以及正在发生的变化。一篇文章包含所有内容。
- en: medium.com](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2?source=post_page-----30cd2bfbd855--------------------------------)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2?source=post_page-----30cd2bfbd855--------------------------------)'
