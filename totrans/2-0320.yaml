- en: 'Applied Reinforcement Learning IV: Implementation of DQN'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/applied-reinforcement-learning-iv-implementation-of-dqn-7a9cb2c12f97](https://towardsdatascience.com/applied-reinforcement-learning-iv-implementation-of-dqn-7a9cb2c12f97)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Implementation of the DQN algorithm, and application to OpenAI Gym’s CartPole-v1
    environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@JavierMtz5?source=post_page-----7a9cb2c12f97--------------------------------)[![Javier
    Martínez Ojeda](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----7a9cb2c12f97--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a9cb2c12f97--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a9cb2c12f97--------------------------------)
    [Javier Martínez Ojeda](https://medium.com/@JavierMtz5?source=post_page-----7a9cb2c12f97--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a9cb2c12f97--------------------------------)
    ·8 min read·Jan 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9381b94512b701bbdf635b646b1733b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [DeepMind](https://unsplash.com/@deepmind?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: If you want to read this article without a Premium Medium account, you can do
    it from this friend link :)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://www.learnml.wiki/applied-reinforcement-learning-iv-implementation-of-dqn/](https://www.learnml.wiki/applied-reinforcement-learning-iv-implementation-of-dqn/)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In the previous article of this series, [*Applied Reinforcement Learning III:
    Deep Q-Networks (DQN)*](https://medium.com/@JavierMtz5/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9),
    the Deep Q-Networks algorithm was introduced and explained, as well as the advantages
    and disadvantages of its application with respect to its predecessor: [Q-Learning](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437).
    In this article everything previously explained will be put into practice by applying
    DQN to a real use case. If you are not familiar with the basic concepts of the
    DQN algorithm or its rationale, I recommend you take a look at the previous article
    before continuing with this one.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@JavierMtz5/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9?source=post_page-----7a9cb2c12f97--------------------------------)
    [## Applied Reinforcement Learning III: Deep Q-Networks (DQN)'
  prefs: []
  type: TYPE_NORMAL
- en: Learn the behavior of the DQN algorithm step by step, as well as its improvements
    compared to previous Reinforcement…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@JavierMtz5/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9?source=post_page-----7a9cb2c12f97--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Environment — CartPole-v1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simulation environment to be used will be the [**gym CartPole environment**](https://www.gymlibrary.dev/environments/classic_control/cart_pole/),
    which consists of a cart that moves along a horizontal guide with a vertical pole
    mounted on it, as shown in *Figure 1*. The objective is that, by means of the
    inertia generated by the cart in its horizontal displacements, the pole remains
    vertical as long as possible. An episode is considered successful when the pole
    remains vertical for more than 500 timesteps, and an episode is considered unsuccessful
    if the cart runs off the horizontal guide on the right or left side, or if the
    pole tilts more than 12 degrees (~0.2095 radians) with respect to the vertical
    axis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fdeab548cbd53c60384ae10bdf7d5e05.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1**. Image of the CartPole environment. Image extracted from the rendering
    of the [gym environment](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)'
  prefs: []
  type: TYPE_NORMAL
- en: The environment has a **discrete action space**, since the DQN algorithm is
    not applicable to environments with a continuous action space, and the **state
    space is continuous**, because the main advantage of DQN over Q-Learning is the
    possibility of working with continuous or high-dimensional states, which is to
    be proved.
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/563e1e434fd290e3b681f283021b5d99.png)'
  prefs: []
  type: TYPE_IMG
- en: Actions for the CartPole environment. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: States
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The states are represented as an array of 4 elements, where the meaning of
    each element, as well as their allowed value ranges are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d0952fee0aa07fbbe3e693b151cafb36.png)'
  prefs: []
  type: TYPE_IMG
- en: States for the CartPole environment. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be noted that, although the allowed value ranges are those shown
    in the table, an episode will be terminated if:'
  prefs: []
  type: TYPE_NORMAL
- en: The Cart position on the x axis leaves the (-2.4, 2.4) range
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Pole angle leaves the (-0.2095, 0.2095) range
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rewards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The agent receives a reward of +1 for each time step, with the intention of
    keeping the pole standing for as long as possible.
  prefs: []
  type: TYPE_NORMAL
- en: DQN Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pseudocode extracted from the paper presented by Mnih et al. **[1]** will
    be used as a reference to support the implementation of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17a2556d0c81cad2169e3ea55a8b7795.png)'
  prefs: []
  type: TYPE_IMG
- en: DQN Pseudocode. Extracted from **[1]**
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Initialize Replay Buffer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Replay Buffer is implemented as a double-ended queue (deque), and two methods
    are created to interact with it: ***save_experience()*** and ***sample_experience_batch()***.
    ***save_experience()*** allows adding an experience to the replay buffer, while
    ***sample_experience_batch()*** randomly picks up a batch of experiences, which
    will be used to train the agent. The batch of experiences is returned as a tuple
    of ***(states, actions, rewards, next_states, terminals)***, where each element
    in the tuple corresponds to an array of {batch_size} items.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deques**, according to [Python’s documentation](https://docs.python.org/3/library/collections.html#collections.deque),
    support thread-safe, memory efficient appends and pops from either side of the
    deque with approximately the same O(1) performance in either direction. Python
    **lists**, on the other hand, have a complexity of O(n) when inserting or popping
    elements on the left-side of the list, which makes deques a much more efficient
    option when you need to work on the left-side elements.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Initialize Main and Target Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both the Main and Target Neural Networks are initialized with the ***create_nn()***
    method, which creates a keras model with 3 layers of 64, 64 and 2 (action size
    for CartPole environment) neurons each, and whose input is the state size of the
    environment. The loss function used is the Mean Squared Error (MSE), as stated
    in the algorithm’s peudocode (and shown in *Figure 2*), and the optimizer is Adam.
  prefs: []
  type: TYPE_NORMAL
- en: If the DQN algorithm is used to teach an agent how to play an Atari game as
    mentioned in the [previous article](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437),
    the neural network should be convolutional, since for this type of training the
    states are the frames of the video game.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b0c9a869249e0327a6cab00632d52fc.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 2**. Extracted from the DQN Pseudocode in **[1]**'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Execute loop for each timestep, within each episode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following steps are repeated for each timestep, and form the main loop of
    the DQN agent training.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Pick action following Epsilon-Greedy Policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The action with the best Q-Value, which is the one with highest value on the
    output of the main neural network, is chosen with probability 1-ε, and a random
    action is chosen otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: The full algorithm implementation, which is available in [my GitHub repository](https://github.com/JavierMtz5/ArtificialIntelligence),
    updates the epsilon value in each episode, making it lower and lower. This makes
    the exploration phase happen mainly at the beginning of the training, and the
    exploitation phase in the remaining episodes. The evolution of the epsilon values
    during a training are in *Figure 3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2be1d80158a5c96427cab34620e87f8c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3**. Epsilon decay during training. Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Perform action on environment and store experience in Replay Buffer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The action extracted from the previous step is applied on the gym environment
    via the ***env.step()*** method, which receives the action to be applied as parameter.
    This method returns a tuple **(next_state, reward, terminated, truncated, info)**,
    from which the next state, reward and terminated fields are used together with
    the current state and the action chosen for saving the experience in the Replay
    Buffer with the ***save_experience()*** method defined before.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Train agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, the agent is trained with the experiences stored along the episodes.
    As explained in [the previous article](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9),
    the output of the main neural network is taken as predicted value, and the target
    value is calculated from the reward and the output of the target network for the
    action with highest Q-Value on the next state. The loss is then calculated as
    the squared difference between the predicted value and the target value, and gradient
    descent is performed on the main neural network from this loss.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Main Loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After having defined the behavior of the algorithm in each of the previous steps,
    as well as its implementation in code, all the pieces can be put together and
    the DQN algorithm can be built, as shown in the code below.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are certain behaviors of the code worth mentioning:'
  prefs: []
  type: TYPE_NORMAL
- en: The ***update_target_network()*** method, which has not been mentioned in this
    article, copies the weights from the main neural network to the target neural
    network. This process is repeated every {update_rate} timesteps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training of the main neural network is only performed when there are enough
    experiences in the Replay Buffer to fill a batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The epsilon value is reduced in each episode as long as a minimum value has
    not been reached, by multiplying the previous value by a reducing factor less
    than 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training stops when a cumulative reward of more than 499 has been reached in
    each of the last 10 episodes, as the agent is considered to have learned to perform
    the task successfully.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test the DQN Agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The performance of the trained DQN agent is evaluated by plotting the rewards
    obtained during training, and by running a test episode.
  prefs: []
  type: TYPE_NORMAL
- en: The reward plot is used to see if the agent is able to make the rewards converge
    towards the maximum, or if on the contrary it is not able to make the rewards
    increase with each training episode, which would mean that the training has failed.
    As for the execution of the test episode, it is a realistic way to evaluate the
    agent, since it shows in a practical way whether the agent has really learned
    to perform the task for which he has been trained.
  prefs: []
  type: TYPE_NORMAL
- en: Reward Plot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The graph clearly shows how the rewards converge towards the maximum, despite
    the fact that in some episodes the agent fails in a few timesteps. This may occur
    because in those episodes the agent is initialized in a position that it does
    not know well, since it has not been initialized that way many times throughout
    the training. Likewise, the tendency of the rewards to maximum values is an indicator
    that the agent has trained correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a0b6ca05c2d705221fdebdfa5787ab3.png)'
  prefs: []
  type: TYPE_IMG
- en: Reward Plot. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Execution of Test Episode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The agent succeeds in executing the test episode with the maximum score, 500,
    which is unequivocal proof that he has learned to perform the task perfectly,
    as the reward plot suggested.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23faa34cb2bde64cf1068398928a1343.png)'
  prefs: []
  type: TYPE_IMG
- en: Code and execution of test episode. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The DQN algorithm has managed to learn the task perfectly, so its application
    to environments with a continuous state space can be considered valid. Likewise,
    the algorithm manages to solve in a reasonable time the problem of working with
    a continuous and complex state space through the use of two neural networks. As
    only one of the two networks is trained, the computation time is significantly
    reduced, although it should be noted that the execution times of the algorithm
    are still long even for simple tasks, when compared to the Q-Learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the execution times of the algorithm, this article shows only the
    implementation of DQN for a simple and easy-to-learn environment using dense neural
    networks, but it must be taken into account that the application of DQN in environments
    such as Atari games, which feed convolutional neural networks with images of the
    game, need much more time both to reach the convergence to the optimal Q-Values
    and to perform the forward and backpropagation processes, due to the greater complexity
    of the environment and the neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the limitations of using a Replay Buffer, in terms of memory cost,
    must be taken into account. Although the maximum length of the queue (deque) can
    be specified, thus avoiding excessive memory consumption, it is still a huge RAM
    consumption. This problem, however, can be solved by loading most of the Replay
    Buffer on disk, and keeping a small fraction in memory, or by using a more efficient
    buffer for the task, instead of the double-ended queue.
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The complete implementation of the DQN algorithm, both as Jupyter Notebook and
    Python script can be found in my [ArtificialIntelligence GitHub repository](https://github.com/JavierMtz5/ArtificialIntelligence).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/JavierMtz5/ArtificialIntelligence?source=post_page-----7a9cb2c12f97--------------------------------)
    [## GitHub - JavierMtz5/ArtificialIntelligence'
  prefs: []
  type: TYPE_NORMAL
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/JavierMtz5/ArtificialIntelligence?source=post_page-----7a9cb2c12f97--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: REFERENCES
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**[1]** MNIH, Volodymyr, et al. Playing atari with deep reinforcement learning.
    *arXiv preprint arXiv:1312.5602*, 2013.'
  prefs: []
  type: TYPE_NORMAL
