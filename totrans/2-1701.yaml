- en: Prompt Ensembles Make LLMs More Reliable
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示集使LLMs更可靠
- en: 原文：[https://towardsdatascience.com/prompt-ensembles-make-llms-more-reliable-ae57ec35b5f7](https://towardsdatascience.com/prompt-ensembles-make-llms-more-reliable-ae57ec35b5f7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/prompt-ensembles-make-llms-more-reliable-ae57ec35b5f7](https://towardsdatascience.com/prompt-ensembles-make-llms-more-reliable-ae57ec35b5f7)
- en: Simple strategies for getting the most out of any language model…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高任何语言模型使用效果的简单策略……
- en: '[](https://wolfecameron.medium.com/?source=post_page-----ae57ec35b5f7--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----ae57ec35b5f7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ae57ec35b5f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ae57ec35b5f7--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----ae57ec35b5f7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----ae57ec35b5f7--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----ae57ec35b5f7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ae57ec35b5f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ae57ec35b5f7--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----ae57ec35b5f7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ae57ec35b5f7--------------------------------)
    ·18 min read·Aug 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----ae57ec35b5f7--------------------------------)
    ·18分钟阅读·2023年8月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/917f4253782589036edfaf468a0ea247.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/917f4253782589036edfaf468a0ea247.png)'
- en: (Photo by [Manuel Nägeli](https://unsplash.com/@gwundrig?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/7CcPLtywRso?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由[Manuel Nägeli](https://unsplash.com/@gwundrig?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，来源于[Unsplash](https://unsplash.com/photos/7CcPLtywRso?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: Anyone who has worked with large language models (LLMs) will know that prompt
    engineering is an informal and difficult process. Small changes to a prompt can
    cause massive changes to the model’s output, it is difficult (or even impossible
    in some cases) to know the impact that changing a prompt will have, and prompting
    behavior is highly dependent on the type of model being used. The fragile nature
    of prompt engineering is a harsh reality when we think about creating applications
    with LLMs. If we cannot predict how our model will behave, *how can we build a
    dependable system around this model?* Although LLMs are incredibly capable, this
    problem complicates their use in many practical scenarios.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 任何使用过大型语言模型（LLMs）的人都知道，提示工程是一个非正式且困难的过程。对提示进行微小的更改可能会导致模型输出发生巨大变化，因此很难（有时甚至是不可能）预测更改提示会产生的影响，而且提示行为高度依赖于所使用的模型类型。提示工程的脆弱性是我们在考虑使用LLMs创建应用程序时的严酷现实。如果我们无法预测模型的行为，*我们如何围绕这个模型构建一个可靠的系统？*
    尽管LLMs具有极强的能力，但这个问题使得它们在许多实际场景中的使用变得复杂。
- en: “Prompting is a brittle process wherein small modifications to the prompt can
    cause large variations in the model predictions, and therefore significant effort
    is dedicated towards designing a painstakingly perfect prompt for a task.” *—
    from [2]*
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “提示是一个脆弱的过程，微小的修改可能导致模型预测的大幅变化，因此在为任务设计一个完美的提示时需要付出大量努力。” *— 引自[2]*
- en: Given the fragile nature of LLMs, finding techniques that make these models
    more accurate and reliable has recently become a popular research topic. In this
    overview, we will focus on one technique in particular — *prompt ensembles.* Put
    simply, prompt ensembles are just sets of diverse prompts that are meant to solve
    the same problem. To improve LLM reliability, we can generate an answer to a question
    by querying the LLM with multiple different input prompts and considering each
    of the model’s responses when inferring a final answer. As we will see, some research
    on this topic is quite technical. However, the basic idea behind these techniques
    is simple and can drastically improve LLM performance, making prompt ensembles
    a go-to approach for improving LLM reliability.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于LLM的脆弱性，寻找使这些模型更准确和可靠的技术最近已成为一个热门研究话题。在本概述中，我们将特别关注一种技术——*提示集合。* 简单来说，提示集合就是一组多样的提示，旨在解决同一个问题。为了提高LLM的可靠性，我们可以通过向LLM提出多个不同的输入提示并考虑每个模型的响应来生成问题的答案。正如我们将看到的，关于这一主题的一些研究相当技术性。然而，这些技术背后的基本理念很简单，并且可以显著提高LLM的性能，使提示集合成为提升LLM可靠性的首选方法。
- en: '![](../Images/8a57c2f2369878545cb2fad6fe324833.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a57c2f2369878545cb2fad6fe324833.png)'
- en: (from [1, 2])
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1, 2])
- en: Background
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: Prior to learning about recent research on prompt ensembles and LLM reliability,
    let’s take a look at a few core concepts and background information related to
    LLMs that will help to make this overview more complete and understandable.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解关于提示集合和LLM可靠性的最新研究之前，让我们先看看与LLM相关的一些核心概念和背景信息，这将帮助使本概述更加完整和易于理解。
- en: Some prerequisites…
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些前提条件…
- en: '![](../Images/99a7da5dbdb3456b72bd7f8eb3820275.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99a7da5dbdb3456b72bd7f8eb3820275.png)'
- en: (from [11, 12, 13, 14])
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [11, 12, 13, 14])
- en: 'The majority of this post will be heavily focused upon prompt engineering and
    prompt ensembles. As such, we will not cover much background material on modern
    LLMs and how they are created. Luckily, I have written a lot of overviews of this
    topic that can be used to quickly gain a baseline understanding of how these systems
    work. To get started, I’d recommend the following resources:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的大部分内容将重点关注提示工程和提示集合。因此，我们不会深入讲解现代大语言模型（LLMs）的背景材料及其创建过程。幸运的是，我写了很多关于这个主题的概述，可以快速获得对这些系统如何运作的基本了解。为了开始，我建议以下资源：
- en: The History of LLMs [[link](https://twitter.com/cwolferesearch/status/1639378997627826176?s=20)]
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs 的历史 [[link](https://twitter.com/cwolferesearch/status/1639378997627826176?s=20)]
- en: The Building Blocks of LLMs [[link](https://twitter.com/cwolferesearch/status/1635693551584522256?s=20)]
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs 的构建模块 [[link](https://twitter.com/cwolferesearch/status/1635693551584522256?s=20)]
- en: Language Model Pre-Training [[link](https://cameronrwolfe.substack.com/i/85568430/language-modeling)]
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型预训练 [[link](https://cameronrwolfe.substack.com/i/85568430/language-modeling)]
- en: Decoder-Only Transformers [[link](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)]
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅解码器变压器 [[link](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)]
- en: 'Beyond LLMs, we should also understand the idea of prompt engineering. When
    interacting with a language model, we provide textual input (i.e., a prompt),
    and the language model provides a textual completion. This text-to-text format
    is incredibly flexible, allowing a variety of different problems to be solved
    by just extracting the correct answer from an LLM’s output. However, this flexibility
    can also become a burden, as there are many options for how the input to an LLM
    can be worded or phrased when attempting to solve a problem. Prompt engineering
    is an empirical science that studies this problem and attempts to find input prompts
    that maximize LLM performance. To learn more, check out the overviews below:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 除了大语言模型之外，我们还应了解提示工程的概念。在与语言模型互动时，我们提供文本输入（即提示），语言模型提供文本完成。这个文本到文本的格式非常灵活，允许通过从LLM的输出中提取正确答案来解决各种不同的问题。然而，这种灵活性也可能成为负担，因为在尝试解决问题时，LLM的输入可以用许多不同的方式进行措辞或表达。提示工程是一门经验科学，研究这个问题并试图找到能最大化LLM性能的输入提示。要了解更多内容，请查看以下概述：
- en: Practical Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part)]
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实用提示工程 [[link](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part)]
- en: Advanced Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering)]
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级提示工程 [[link](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering)]
- en: What is Reliability?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠性是什么？
- en: As we study reliability, it is useful to provide a precise definition of the
    concept. As mentioned perviously, LLMs can be quite brittle. If we change the
    input a bit, we might get a drastically different output. Oftentimes, the output
    of an LLM is unpredictable and inaccurate. These issues were discussed extensively
    by [Chip Huyen](https://huyenchip.com/) in a recent [blog post](https://huyenchip.com/2023/04/11/llm-engineering.html).
    The quote below outlines the difficulty of building applications with LLMs compared
    to traditional programming tasks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究可靠性时，提供该概念的精确定义是有用的。如前所述，LLM可能会相当脆弱。如果我们稍微改变输入，可能会得到截然不同的输出。LLM的输出往往不可预测且不准确。这些问题在最近的[Chip
    Huyen](https://huyenchip.com/)的[博客文章](https://huyenchip.com/2023/04/11/llm-engineering.html)中进行了广泛讨论。以下引用概述了使用LLM构建应用程序相比于传统编程任务的困难。
- en: “Programming languages are mostly exact… In prompt engineering, instructions
    are written in natural languages, which are a lot more flexible than programming
    languages… The ambiguity in LLMs’ generated responses can be a dealbreaker.”
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “编程语言大多是精确的……在提示工程中，指令是用自然语言编写的，比编程语言灵活得多……LLM生成的回应中的歧义可能会成为决定性因素。”
- en: '**Reliability is the solution.** At a high level, reliability refers to a system’s
    ability to mitigate noise and abstract or avoid such inconsistent behavior with
    LLMs. This could mean anything from making the LLM more accurate to improving
    the consistency or predictability of the model’s behavior. If we want to maximize
    the utility of LLMs, we must find ways to make their behavior more reliable so
    that applications can be built around them without any unexpected “surprises”
    that break the system. Practically, this means that we must:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**可靠性是解决方案。** 从高层次看，可靠性指的是系统在处理噪音和抽象或避免LLM的不一致行为方面的能力。这可能意味着从提高LLM的准确性到改善模型行为的一致性或可预测性。如果我们想最大化LLM的效用，就必须找到使其行为更可靠的方法，以便可以围绕LLM构建应用程序，而不会出现打破系统的意外“惊喜”。实际上，这意味着我们必须：'
- en: Adopt a more rigorous/systematic approach to prompt engineering.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采取更严格/系统的方法来进行提示工程。
- en: Find techniques that make LLMs more predictable and accurate.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找使LLM更具预测性和准确性的技术。
- en: Enact safeguards/boundaries when the LLM fails to match our desired format.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在LLM未能匹配我们期望的格式时，实施保护措施/边界。
- en: Each of the above points is a step towards improving the reliability of LLMs.
    Put simply, we just want to find ways to make our LLM behave more consistently
    in an application so that the end user has less confusion and a more desirable
    experience. If we commit to a more rigorous approach to working with LLMs, *it
    is totally possible to minimize the fragility of prompt engineering and lessen
    the overall ambiguity of LLMs*. Within this overview, we will mostly focus on
    the second point outlined above — techniques that make LLMs more predictable and
    accurate.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述每一点都是提高LLM可靠性的步骤。简单来说，我们只希望找到方法，使我们的LLM在应用程序中表现得更加一致，从而减少终端用户的困惑，提升体验。如果我们承诺采取更严格的方法来处理LLM，*完全有可能最小化提示工程的脆弱性，并减少LLM的整体歧义*。在这次概述中，我们将主要关注上述第二点——使LLM更具预测性和准确性的技术。
- en: Solving Tough Problems with LLMs
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LLM解决棘手问题
- en: '![](../Images/3ddd1bf2e3aa30d0dd96b07c3073b2e2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ddd1bf2e3aa30d0dd96b07c3073b2e2.png)'
- en: (from [3, 4, 16, 17])
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于[3, 4, 16, 17]）
- en: Although LLMs can solve many tasks with techniques like [few-shot learning](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning),
    they tend to struggle with solving multi-step problems or those that require reasoning
    [15]. As a solution, recent research has explored techniques like [chain of thought
    (CoT) prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms)
    [3], including several notable [extensions](https://twitter.com/cwolferesearch/status/1657122778984660993?s=20)
    like [self-consistency](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting)
    [4], to improve the reasoning capabilities of LLMs. From this work, we learn that
    language models already have the ability to solve difficult (reasoning-based)
    problems — we just need to use the correct prompting approach!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM可以通过如 [少样本学习](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning)
    等技术解决许多任务，但它们在解决多步骤问题或需要推理的任务时往往会遇到困难[15]。为此，最近的研究探索了如 [思维链（CoT）提示](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms)
    [3] 等技术，包括一些显著的 [扩展](https://twitter.com/cwolferesearch/status/1657122778984660993?s=20)
    如 [自洽性](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting)
    [4]，以提高LLM的推理能力。从这项工作中，我们了解到语言模型已经具备解决困难（基于推理的）问题的能力——我们只需使用正确的提示方法！
- en: “Large pretrained language models have built in reasoning capabilities, but
    they need specific prompts to unleash their power.” *— from [1]*
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “大型预训练语言模型内建了推理能力，但它们需要特定的提示来释放它们的潜力。” *— 来源于 [1]*
- en: '**Self-consistency.** Given a CoT prompting approach, self-consistency [4]
    can improve the accuracy of an LLM by just *i)* generating multiple, different
    outputs from the same model and *ii)* using a majority vote of each output’s answer
    as our final answer; see below. This technique improves LLM accuracy by aggregating
    results over a diverse set of outputs. Self-consistency is both simple and effective,
    demonstrating that practical techniques for improving the reliability of LLMs
    may not be far beyond our reach. As such, we may wonder: *how can we take this
    approach further? Are there other, simple techniques that work even better?*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**自洽性**。在使用CoT提示方法的情况下，自洽性[4]可以通过*i)* 从同一个模型生成多个不同的输出，以及 *ii)* 使用每个输出答案的多数票作为最终答案来提高LLM的准确性；见下文。这种技术通过对一组多样的输出结果进行汇总来提高LLM的准确性。自洽性既简单又有效，表明提高LLM可靠性的实用技术可能离我们并不遥远。因此，我们可能会想知道：*我们如何进一步利用这种方法？是否还有其他更简单的技术效果更佳？*'
- en: '![](../Images/4606200ea5a23cfd7cd56c732672f8a7.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4606200ea5a23cfd7cd56c732672f8a7.png)'
- en: (from [1])
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [1])
- en: '**Prompt ensembles.** The effectiveness of self-consistency stems from the
    diversity of generated outputs that are considered when forming a final answer.
    However, there is one key detail of this technique that we should notice — *all
    outputs are generated with the same prompt*. To increase the diversity of generated
    outputs, we could consider a diverse set of multiple prompts for solving the same
    problem.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示集**。自洽性的有效性源于在形成最终答案时考虑的生成输出的多样性。然而，这种技术有一个关键细节需要注意——*所有输出都是用相同的提示生成的*。为了增加生成输出的多样性，我们可以考虑一组多样的提示来解决同一个问题。'
- en: “People think differently, [but] different thoughts often lead to the same,
    correct answer.” *— from [1]*
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “人们的思维方式不同，[但] 不同的思维往往会得到相同的正确答案。” *— 来源于 [1]*
- en: Such an approach, called a prompt ensemble, can be used to generate an even
    more diverse set of model outputs compared to self-consistency, thus further improving
    the reliability LLM applications. Plus, prompt ensembles are simple to understand
    and can be constructed automatically without significant implementation effort.
    Within this post, we will explore recent research on prompt ensembles, focusing
    on practical tools making LLMs more effective.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法，被称为提示集，可以用来生成比自洽性方法更为多样化的模型输出，从而进一步提高LLM应用的可靠性。此外，提示集易于理解且可以自动构建，无需大量实施工作。在本文中，我们将探讨关于提示集的最新研究，重点关注使LLM更有效的实用工具。
- en: Other Important Concepts
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他重要概念
- en: Beyond the ideas covered so far, there are a couple of small concepts and terms
    referenced later in the overview that might be useful to understand.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 除了迄今为止涉及的观点外，概述中还提到了一些可能对理解有用的小概念和术语。
- en: '**Bootstrapping.** This is a generic term that is commonly used in the broader
    computer science community. It refers to the idea of leveraging an existing resource
    to do something new or useful. In the case of this overview, we use bootstrapping
    to describe using an existing, pre-trained LLMs as a component within a system
    that generates new prompts for use in an ensemble.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**自举。** 这是一个在更广泛的计算机科学社区中常用的通用术语。它指的是利用现有资源来做一些新的或有用的事情。在本概述中，我们使用自举来描述在一个系统中利用现有的、预训练的LLMs作为组件，生成新的提示以用于集合中。'
- en: '**Weak supervision.** There are [many different ways](https://www.geeksforgeeks.org/supervised-unsupervised-learning/)
    to train a machine learning model. Weak supervision is a technique that falls
    between [supervised and unsupervised](https://www.geeksforgeeks.org/supervised-unsupervised-learning/)
    learning. It is not completely reliant upon labeled data like supervised learning,
    but it does use some form of “labels” as a training signal. For example, we might
    generate “pseudo labels” using some heuristic, or even use a combination of labeled
    and unlabeled data during training. For more details, check out the awesome overview
    from [Snorkel AI](https://snorkel.ai/) linked [here](https://snorkel.ai/weak-supervision/).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**弱监督。** 有[许多不同的方法](https://www.geeksforgeeks.org/supervised-unsupervised-learning/)来训练机器学习模型。弱监督是一种介于[监督学习和无监督学习](https://www.geeksforgeeks.org/supervised-unsupervised-learning/)之间的技术。它不像监督学习那样完全依赖于标记数据，但它确实使用某种形式的“标签”作为训练信号。例如，我们可能使用某些启发式方法生成“伪标签”，或在训练过程中使用标记和未标记的数据的组合。有关更多细节，请查看[Snorkel
    AI](https://snorkel.ai/)的精彩概述，链接[在此](https://snorkel.ai/weak-supervision/)。'
- en: '**Jaccard index.** The Jaccard Index, usually referred to as intersection over
    union (IoU) within the ML community, is used to compute the similarity between
    two finite sets. To compute the value of the Jaccard Index, we find the number
    of intersecting elements between the two sets, then divide this number by the
    size of the union between the two sets. For example, if we have two sets given
    by `{a, b, c}` and `{b, c, d}`, the Jaccard Index would be 0.5 (i.e., two elements
    intersect and there are four unique elements between both sets).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jaccard指数。** Jaccard指数，通常在机器学习社区中称为交集与并集（IoU），用于计算两个有限集合之间的相似性。要计算Jaccard指数的值，我们首先找到两个集合之间的交集元素数量，然后将这个数字除以两个集合的并集大小。例如，如果我们有两个集合
    `{a, b, c}` 和 `{b, c, d}`，Jaccard指数将是0.5（即，两个元素相交，两个集合之间共有四个唯一元素）。'
- en: Research on Prompt Ensembles
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示集合的研究
- en: Prior work on CoT prompting and self-consistency has shown us that smart prompting
    strategies can drastically improve the ability of LLMs to reliably solve difficult
    problems. We will now go beyond these straightforward baselines and take a look
    at recent research that studies the use of prompt ensembles with LLMs. Such work
    offers a wealth of practical knowledge regarding best practices that we can adopt
    to make LLMs more reliable.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 先前关于CoT提示和自洽的研究已经向我们展示了聪明的提示策略可以极大地提高LLM可靠解决困难问题的能力。现在我们将超越这些简单的基线，深入研究最近的研究，研究使用LLM的提示集合。这类工作提供了大量关于最佳实践的实用知识，我们可以采用这些知识来提高LLM的可靠性。
- en: Diverse Verifier on Reasoning Step (DiVeRSE) [1]
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑推理步骤中的多样性验证器（DiVeRSE） [1]
- en: “People think differently, [but] different thoughts often lead to the same,
    correct answer.” *— from [1]*
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “人们的思维方式各异，[但]不同的思维常常能得出相同的、正确的答案。” *— 引自 [1]*
- en: Authors in [1] explore an extension to CoT and self-consistency prompting techniques
    [3, 4] that improves performance on complex, multi-step reasoning tasks. This
    technique, called DiVeRSE, uses prompt ensembles (i.e., collections of different
    prompts that aim to solve the same problem) to enhance the diversity of generated
    [reasoning paths](https://cameronrwolfe.substack.com/i/116166267/chain-of-thought-prompting),
    then trains a verification module to infer the correctness of each output; see
    below.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 的作者探讨了对CoT和自洽提示技术 [3, 4] 的扩展，这些技术提高了在复杂的多步骤推理任务中的表现。这个被称为DiVeRSE的技术，利用提示集合（即，旨在解决同一问题的不同提示的集合）来增强生成的[推理路径](https://cameronrwolfe.substack.com/i/116166267/chain-of-thought-prompting)的多样性，然后训练一个验证模块以推断每个输出的正确性；见下文。'
- en: '![](../Images/023fee04eb1211cec091a8389b8d786f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/023fee04eb1211cec091a8389b8d786f.png)'
- en: (from [1])
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: (引自 [1])
- en: 'Both self-consistency and DiVeRSE generate multiple reasoning paths that are
    combined to form a final answer. However, self-consistency samples multiple reasoning
    paths from an LLM using the same prompt, while DiVeRSE constructs an ensemble
    of diverse prompts for solving the same problem and samples multiple reasoning
    paths from each of these prompts. Additionally, self-consistency just takes a
    majority vote over reasoning paths to form its final answer. DiVeRSE adopts a
    more sophisticated approach that:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 自我一致性和 DiVeRSE 都生成多个推理路径，这些路径被结合形成最终答案。然而，自我一致性从使用相同提示的 LLM 中抽取多个推理路径，而 DiVeRSE
    为解决同一问题构造多样的提示集合，并从每个提示中抽取多个推理路径。此外，自我一致性仅对推理路径进行多数投票以形成最终答案。DiVeRSE 采用更复杂的方法：
- en: Trains a verifier/classifier to predict the correctness of each reasoning path.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个验证器/分类器以预测每个推理路径的正确性。
- en: Takes a weighted average over reasoning paths based on correctness.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据正确性对推理路径进行加权平均。
- en: Put simply, DiVeRSE improves LLM reasoning capabilities by *i)* enhancing the
    diversity of generated reasoning paths and *ii)* assigning more weight to reasoning
    paths that are likely to be correct when constructing the final answer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，DiVeRSE 通过 *i)* 增强生成推理路径的多样性和 *ii)* 在构造最终答案时对可能正确的推理路径分配更多权重，从而提高 LLM 推理能力。
- en: '![](../Images/3b808ea0dccbe3f996425ead22b8c1f5.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b808ea0dccbe3f996425ead22b8c1f5.png)'
- en: Constructing a prompt ensemble (from [3])
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 构造提示集合（摘自 [3]）
- en: '**Constructing prompt ensembles.** One of the main benefits of DiVeRSE is its
    use of prompt ensembles to maximize the diversity of generated outputs. But, *is
    generating these prompt ensembles expensive? Can we construct prompt ensembles
    automatically?* Considering CoT prompting in particular, we can generate a prompt
    ensemble in two main ways (see figure above):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**构造提示集合。** DiVeRSE 的主要好处之一是使用提示集合来最大化生成输出的多样性。但是，*生成这些提示集合是否昂贵？我们可以自动构造提示集合吗？*
    特别是考虑到 CoT 提示，我们可以通过两种主要方式生成提示集合（见上图）：'
- en: '*Resampling:* Given a CoT prompt that contains a question, answer, and `K`
    rationales for few-shot learning, we can generate unique prompts by randomly sampling
    `R < K` exemplars from the full set of rationales.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*重采样：* 给定一个包含问题、答案和 `K` 个推理依据的 CoT 提示，我们可以通过从完整的推理依据集中随机抽取 `R < K` 个示例来生成独特的提示。'
- en: '*Bootstrapping:* If our CoT prompt does not contain enough few-shot exemplars
    to perform resampling, we can simply prompt a separate LLM to generate pseudo
    reasoning paths to be included when performing resampling.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*引导自举：* 如果我们的 CoT 提示中没有足够的少样本示例来进行重采样，我们可以简单地提示一个独立的 LLM 生成伪推理路径，以便在执行重采样时包含这些路径。'
- en: Using these techniques, we can generate prompt ensembles for DiVeRSE automatically
    without the requirement of significant, manual human effort.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些技术，我们可以自动生成 DiVeRSE 的提示集合，而无需大量的手动人工努力。
- en: “Causal language models have no mechanism to correct previous errors in earlier
    steps, which quickly leads to disoriented results.” *— from [1]*
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “因果语言模型没有机制来纠正早期步骤中的前期错误，这很快会导致结果偏离。” *— 摘自 [1]*
- en: '**Verification module.** To form its final answer, DiVeRSE uses a verification
    module to predict the correctness of each reasoning path that it generates, then
    takes a weighted average based on these predictions. The verifier is a [binary
    classifier](https://en.wikipedia.org/wiki/Binary_classification) (e.g., based
    upon [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [5] or DeBERTa [6]) that is trained over a dataset of correct and incorrect reasoning
    paths generated by the underlying LLM. Notably, labeled data is needed to generate
    this dataset for the verifier, as labels are used to determine if the final answer
    of any reasoning path is actually correct.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**验证模块。** 为了形成最终答案，DiVeRSE 使用验证模块预测每个生成的推理路径的正确性，然后根据这些预测进行加权平均。验证器是一个 [二分类器](https://en.wikipedia.org/wiki/Binary_classification)（例如，基于
    [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [5] 或 DeBERTa [6]） ，它在由底层 LLM 生成的正确和不正确的推理路径数据集上进行训练。值得注意的是，需要标注数据来生成这个验证器的数据集，因为标签用于确定任何推理路径的最终答案是否实际正确。'
- en: At test time, this verifier is used to generate a correctness score for each
    reasoning path generated by DiVeRSE, and paths with low correctness scores are
    given less of a vote towards the final answer. Interestingly, we see in [1] that
    performing step-wise verification (i.e., training the verifier to predict the
    correctness of each individual reasoning step instead of the path as a whole)
    in particular can lead to large improvements in reasoning performance; see below.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试时，使用这个验证器为DiVeRSE生成的每条推理路径生成一个正确性分数，低正确性分数的路径在最终答案中所占的权重较小。有趣的是，我们在[1]中看到，进行步骤级验证（即训练验证器预测每个单独推理步骤的正确性，而不是整体路径）特别可以大幅提升推理性能；见下文。
- en: '![](../Images/a3433241755eb1531860c7eaf83a1035.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3433241755eb1531860c7eaf83a1035.png)'
- en: Step-wise verification vs. response-level verification (from [3])
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤级验证与响应级验证（来自 [3]）
- en: '**How does it perform?** DiVeRSE is compared to baseline techniques like [greedy
    decoding](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#common-decoding-methods)
    and self-consistency using several different LLMs, such as davinci (GPT-3), text-davinci-002
    (GPT-3.5), and code-davinci-002 from the [OpenAI API](https://platform.openai.com/docs/models).
    On eight different reasoning tasks that perform arithmetic, commonsense, and inductive
    reasoning, DiVeRSE achieves consistent improvements over self-consistency; see
    below. Most notably, DiVeRSE with code-davinci-002 achieves state-of-the-art performance
    on six total benchmarks, even outperforming the powerful, 540 billion parameter
    [PaLM model](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)
    [7].'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**它的表现如何？** DiVeRSE与基线技术如[贪婪解码](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#common-decoding-methods)和自一致性进行了比较，使用了多种不同的LLMs，如davinci
    (GPT-3)、text-davinci-002 (GPT-3.5)和来自[OpenAI API](https://platform.openai.com/docs/models)的code-davinci-002。在执行算术、常识和归纳推理的八种不同推理任务中，DiVeRSE在自一致性基础上取得了一致性改进；见下文。特别值得注意的是，DiVeRSE与code-davinci-002在六个基准测试中达到了最先进的性能，甚至超越了强大的、5400亿参数的[PaLM模型](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)
    [7]。'
- en: '![](../Images/bdc0b9d8b131a6c8889e2625b83f43b6.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdc0b9d8b131a6c8889e2625b83f43b6.png)'
- en: (from [1])
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Going a bit further, authors in [1] perform analysis to show that *i)* prompt
    ensembles are beneficial to reasoning performance, *ii)* reasoning performance
    plateaus after a certain number of prompts are included in the ensemble, and iii)
    using verifiers (and step-wise verifiers in particular) performs better than majority
    voting (though majority voting is a lot simpler!); see below.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步，作者在[1]中进行分析，展示了* i)* 提示集对推理性能的好处，*ii)* 推理性能在包含一定数量的提示后会达到饱和，并且 iii) 使用验证器（尤其是步骤级验证器）比多数投票表现更好（尽管多数投票简单得多！）；见下文。
- en: '![](../Images/16423a6bdd5fe31cc5dea6d06b5b56be.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16423a6bdd5fe31cc5dea6d06b5b56be.png)'
- en: (from [1])
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Ask Me Anything (AMA) [2]
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问我任何事 (AMA) [2]
- en: '![](../Images/ed7733c4a16b037f3524627895a5d71d.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed7733c4a16b037f3524627895a5d71d.png)'
- en: (from [2])
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: Authors in [2] explore practical techniques for effectively constructing and
    using prompt ensembles. At a high level, the proposed technique, called Ask Me
    Anything (AMA), is motivated by eliminating the need to construct a “perfect”
    prompt. Instead, we can design an effective and reliable prompting strategy by
    generating an ensemble of imperfect (but still effective) prompts and aggregating
    their results. But, we need to be smart about how we aggregate the results of
    these prompts (i.e., majority voting does not work well enough!). Plus, we can’t
    just use any set of prompts! In particular, we see in [2] that the most effective
    prompt ensembles utilize prompts that encourage open-ended generation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在[2]中探讨了有效构建和使用提示集的实用技术。从高层次来看，提出的技术称为“问我任何事”（AMA），其动机在于消除构建“完美”提示的需求。相反，我们可以通过生成一组不完美（但仍然有效）的提示并汇总其结果，设计一种有效且可靠的提示策略。但，我们需要聪明地汇总这些提示的结果（即多数投票效果不佳！）。此外，我们不能随便使用任何一组提示！特别是，我们在[2]中看到，最有效的提示集利用了鼓励开放式生成的提示。
- en: 'Although this sounds great, we might have a few questions. *Does this work
    for all tasks? Is collecting the prompts expensive? How should we aggregate results?*
    The approach proposed in [2] is designed to be both scalable and generic, meaning
    that it can be used to improve performance on any model or task. Such efficiency
    and effectiveness comes from three major ideas:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这听起来很不错，我们可能还有一些问题。*这对所有任务都有效吗？收集提示是否昂贵？我们应如何汇总结果？* [2]中提出的方法旨在既具有可扩展性又通用，意味着它可以用于提高任何模型或任务的性能。这种效率和效果来源于三个主要思想：
- en: '*Prompt structure:* AMA emphasizes the use of open-ended prompts, as opposed
    to those that restrict tokens in the output.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*提示结构：* AMA强调使用开放性提示，而不是那些限制输出令牌的提示。'
- en: '*Scalable prompt generation:* Instead of using humans to write ensembles of
    open-ended prompts manually, AMA uses LLMs to both generate and answer prompts,
    thus reducing human-required effort.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*可扩展的提示生成：* AMA不是依靠人工手动编写开放性提示集合，而是使用LLMs生成和回答提示，从而减少了对人力的需求。'
- en: '*Weak supervision:* Because majority voting does not perform well, AMA uses
    weak supervision to learn dependencies between prompts and aggregate LLM outputs
    into an accurate, final answer.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*弱监督：* 由于多数投票效果不佳，AMA使用弱监督来学习提示之间的依赖关系，并将LLM输出汇总为一个准确的最终答案。'
- en: '**Why do majority votes not work well?** In addition to improving the quality
    and structure of prompts present in an ensemble, work in [2] is motivated by the
    fact that generating a final answer by taking a majority vote of LLM outputs over
    a prompt ensemble (e.g., as in self-consistency [4]) works poorly. But, *why is
    this the case?* Interestingly, authors in [2] provide a pretty clear and intuitive
    answer.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么多数投票效果不好？** 除了提高集合中提示的质量和结构，[2]中的工作受到以下事实的启发：通过对提示集合的LLM输出进行多数投票（例如，如自一致性[4]所示）生成最终答案效果不好。但是，*为什么会这样？*
    有趣的是，[2]中的作者提供了一个相当清晰且直观的答案。'
- en: “We see 9.5% average variation in accuracy and that the Jaccard index over errors
    is 69% higher than if prompt errors were i.i.d. Majority vote (MV) is the primary
    unsupervised aggregation strategy in prior work but it does not account for either
    property, making it unreliable.” *— from [2]*
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们观察到准确性平均变化为9.5%，Jaccard指数相比于提示错误为i.i.d.的情况高出69%。多数投票（MV）是先前工作的主要无监督汇总策略，但它没有考虑到这些属性，使其不可靠。”*—
    来源于 [2]*
- en: To rephrase the above statement, *the errors that LLMs make are not randomly
    distributed*! Rather, LLM outputs over multiple different prompts may cluster
    around a single, wrong answer. This is a huge problem for majority voting, because
    a wrong answer may actually be our majority vote! To account for this, we need
    a more sophisticated strategy that can detect and handle such cases by modeling
    the accuracy and dependencies between prompt outputs in an ensemble.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，*LLMs所犯的错误并非随机分布*! 实际上，LLM在多个不同提示下的输出可能会集中在一个错误答案上。这对多数投票是个巨大问题，因为错误的答案可能实际上是我们的多数投票！为了解决这个问题，我们需要一个更复杂的策略来通过建模集合中提示输出的准确性和依赖关系来检测和处理这种情况。
- en: '**Constructing an awesome prompt ensemble.** As a first step, authors in [2]
    study the types of prompts that comprise the most effective ensembles. Three different
    categories of prompting techniques are considered, as shown in the figure below.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建一个出色的提示集合。** 作为第一步，[2]中的作者研究了构成最有效集合的提示类型。考虑了三种不同的提示技术类别，如下图所示。'
- en: '![](../Images/ad3f4b4710c2ba99c550a91a0ae82783.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad3f4b4710c2ba99c550a91a0ae82783.png)'
- en: Different prompting strategies for LLMs (created by author)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的LLM提示策略（由作者创建）
- en: When these prompting strategies are compared, we see that open-ended prompt
    formats (i.e., [Cloze](https://cameronrwolfe.substack.com/i/76273144/training-bert)
    and free form) outperform restrictive prompt formats that ask the LLM to output
    a particular set of tokens. Going a bit further, making the questions within a
    free-form prompt more [precise or specific](https://cameronrwolfe.substack.com/i/117151147/what-is-prompt-engineering)
    also provides a decent boost in accuracy. *Why does free form prompting work better?*
    The answer is not completely clear, but free-form generation more closely mirrors
    the [next-token prediction task](https://cameronrwolfe.substack.com/i/85568430/language-modeling)
    used for pre-training most LLMs, which intuitively means that such models may
    be better at solving tasks in this format.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些提示策略进行比较时，我们发现开放式提示格式（即，[Cloze](https://cameronrwolfe.substack.com/i/76273144/training-bert)
    和自由格式）优于要求LLM输出特定令牌集的限制性提示格式。进一步说，使自由格式提示中的问题更[精准或具体](https://cameronrwolfe.substack.com/i/117151147/what-is-prompt-engineering)也能显著提高准确性。*为什么自由格式提示效果更好？*
    答案尚不完全清楚，但自由格式生成更接近于大多数LLMs预训练时使用的[下一个令牌预测任务](https://cameronrwolfe.substack.com/i/85568430/language-modeling)，这直观上意味着这些模型可能更擅长处理这种格式的任务。
- en: '![](../Images/231347708bc920bd987b9340a4f7179c.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/231347708bc920bd987b9340a4f7179c.png)'
- en: Generating sets of questions that will provide more context about a problem
    (created by author)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 生成可以提供更多上下文的问答集（由作者创建）
- en: Motivated by the effectiveness of open-ended prompts, AMA forms prompt ensembles
    by generating sets of questions about a given input; see above. These questions
    follow a free-form format and emphasize different aspects of the input that may
    provide useful and complementary information. However, *generating these questions
    manually can be expensive*! To avoid this, we can just use LLMs! In [2], we see
    that LLMs can be used to generate useful questions about a desired topic by using
    few-shot learning; see below.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 受到开放式提示有效性的启发，AMA通过生成有关给定输入的一组问题来形成提示集；见上文。这些问题遵循自由格式，并强调输入的不同方面，可能提供有用的互补信息。然而，*手动生成这些问题可能非常昂贵*!
    为了避免这一点，我们可以使用LLMs！在 [2] 中，我们看到LLMs可以通过少量示例学习生成有关所需主题的有用问题；见下文。
- en: '![](../Images/f13b8d7047f4b09bef7f736fab19f1be.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f13b8d7047f4b09bef7f736fab19f1be.png)'
- en: (from [2])
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: （出自 [2]）
- en: By varying the in-context examples that are used and adopting a set of several
    prompt templates that are empirically determined to perform well, authors in [2]
    completely automate the construction of prompt ensembles within AMA!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改变使用的上下文示例和采用一组经过实证确定表现良好的提示模板，[2]的作者完全自动化了AMA中提示集的构建！
- en: “To aggregate prompt predictions reliably, we apply tools from weak supervision,
    a powerful approach for learning high-quality models from weaker sources of signal
    without labeled data.” *— from [2]*
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “为了可靠地汇总提示预测，我们使用弱监督的方法，这是一种从较弱信号源而非标注数据中学习高质量模型的强大方法。” *— 出自 [2]*
- en: '**Aggregating the results.** We can construct a prompt ensemble, but there’s
    one question remaining: *how do we aggregate the LLM’s output from each prompt?*
    Especially for free form prompts, extracting a correct answer from LLM outputs
    can be quite difficult. The aggregation approach in [2] draws upon prior work
    in weak supervision and graphical models [8, 9, 10]. The high level idea is to
    use weak supervision to learn and predict the dependencies between different prompts
    and the accuracy of each prompt. We can use this information to aggregate prompts
    and infer the most likely final answer. Unlike DiVeRSE, such an approach requires
    no labeled data and solves common failure cases of majority voting (e.g., LLMs
    producing the same error on different prompts).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**汇总结果。** 我们可以构建提示集，但还有一个问题待解：*我们如何汇总每个提示的LLM输出？* 尤其是对于自由格式提示，从LLM输出中提取正确答案可能非常困难。
    [2]中的汇总方法借鉴了弱监督和图模型的先前工作 [8, 9, 10]。高层次的想法是利用弱监督学习并预测不同提示之间的依赖关系和每个提示的准确性。我们可以使用这些信息来汇总提示并推断最可能的最终答案。与DiVeRSE不同，这种方法不需要标注数据，并解决了多数投票的常见失败案例（例如，LLMs在不同提示上产生相同的错误）。'
- en: '**Does AMA perform well?** The AMA approach is tested on 20 different benchmarks
    using variety of LLMs (i.e., four different model families, including [BLOOM](https://bigscience.huggingface.co/blog/bloom),
    [OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15),
    [EleutherAI](https://www.eleuther.ai/language-modeling), and [T0](https://github.com/bigscience-workshop/t-zero))
    with sizes ranging from 125 million to 175 billion parameters. The goal of analysis
    in [2] is to determine whether AMA is a generic approach that can work across
    many different settings. The results of this analysis are quite positive. Using
    AMA, we see that small, open-source models (i.e., [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6b)
    in particular) can outperform large models like [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners);
    see below.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**AMA表现如何？** AMA方法在20个不同的基准测试上测试，使用了各种LLM（即四种不同的模型系列，包括 [BLOOM](https://bigscience.huggingface.co/blog/bloom)、
    [OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)、
    [EleutherAI](https://www.eleuther.ai/language-modeling) 和 [T0](https://github.com/bigscience-workshop/t-zero)），其规模从1.25亿到1750亿参数不等。[2]中分析的目标是确定AMA是否是一种可以在多种不同设置下有效的通用方法。这项分析的结果相当积极。使用AMA，我们看到小型开源模型（特别是
    [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6b)）能够超越像 [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)
    这样的大型模型；见下文。'
- en: '![](../Images/f11ee66dd5bb72aa32db8f71d1790a51.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f11ee66dd5bb72aa32db8f71d1790a51.png)'
- en: (from [2])
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [2]）
- en: When all different models are tested, we see that models of intermediate scale
    (i.e., 6–20 billion parameters ) see the most benefit from AMA; see below. Relative
    to baseline, few-shot prompting techniques, AMA provides a ~10% absolute improvement
    in performance across all models and tasks. Thus, it is a generic approach that
    can reliably improve the performance of nearly any LLM.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有不同的模型经过测试后，我们发现中等规模的模型（即6–20亿参数）从AMA中获得的收益最大；见下文。与基线相比，少量提示技术，AMA在所有模型和任务中的表现提高了约10%。因此，它是一种通用方法，可以可靠地提升几乎任何LLM的性能。
- en: '![](../Images/4ee847d177ab4567c8cbcd8efd7e3d25.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ee847d177ab4567c8cbcd8efd7e3d25.png)'
- en: (from [2])
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [2]）
- en: AMA provides an insightful approach for constructing prompt ensembles. The publication
    is full of practical tips for bootstrapping pretrained LLMs to write effective
    prompts. We see in [2] that the methodology used to aggregate LLM responses over
    the prompt ensemble is incredibly important — majority voting is not sufficient!
    The aggregation approach proposed in [2] is technically complex and would likely
    require implementation effort, but it performs well and requires no supervised
    labels. By adopting approaches such as AMA, we can improve LLM reliability by
    making any LLM more accurate and consistent.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: AMA提供了一种构建提示集的深刻方法。该出版物充满了关于如何引导预训练LLM编写有效提示的实用建议。在[2]中我们看到，汇总LLM对提示集的响应的 методология是极其重要的——多数投票是不够的！[2]中提出的汇总方法在技术上复杂，可能需要实现的努力，但表现良好且不需要监督标签。通过采用如AMA这样的方式，我们可以通过使任何LLM更准确和一致，从而提高LLM的可靠性。
- en: “We hope AMA and future work help address pain points of using LLMs by improving
    the ability to proceed with less-than-perfect prompts and enabling the use of
    small, private, and open-source LLMs.” *— from [2]*
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们希望AMA和未来的工作能通过提高处理不完美提示的能力和使用小型、私人及开源LLM的能力，来解决使用LLM的痛点。” *— 来源于 [2]*
- en: Takeaways
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收获
- en: Hopefully, we should now understand that prompt ensembles are easy to use and
    hold a massive amount of potential. To leverage this technique, we just need to
    *i)* construct a set of diverse prompts that are meant to solve the same problem,
    *ii)* generate multiple LLM outputs with each of these prompts, and *iii)* aggregate
    these outputs to form a final answer. As we have seen, the aggregation process
    might be somewhat complex (i.e., majority voting usually isn’t enough). However,
    the construction and use of prompt ensembles is simple, making them a powerful
    tool for LLM reliability. Some of the major takeaways are outlined below.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在应该能够理解，提示集易于使用，并且具有巨大的潜力。要利用这种技术，我们只需 *i)* 构建一组旨在解决相同问题的多样化提示， *ii)* 使用这些提示生成多个LLM输出，以及
    *iii)* 汇总这些输出以形成最终答案。如我们所见，汇总过程可能有些复杂（即，简单的多数投票通常是不够的）。然而，构建和使用提示集很简单，这使得它们成为提高LLM可靠性的强大工具。下面概述了一些主要收获。
- en: '**Reliability is important.** To use LLMs in the real world, we need to build
    software systems around them. But, to build software systems around LLMs, we need
    to mitigate the unpredictable/ambiguous nature of these models. Prompt ensembles
    provide a pretty straightforward approach for making LLMs more accurate and reliable.
    By encouraging an LLM to produce a diverse set of outputs for solving a particular
    problem, we can study the relationship between these responses and develop automatic
    techniques to produce a higher-quality, final result.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**可靠性很重要。** 要在现实世界中使用LLMs，我们需要围绕它们构建软件系统。但是，为了围绕LLMs构建软件系统，我们需要缓解这些模型的不确定性/模糊性。提示集合提供了一种相当直接的方法来使LLMs更加准确和可靠。通过鼓励LLM为解决特定问题产生多样化的输出，我们可以研究这些响应之间的关系，并开发自动化技术以产生更高质量的最终结果。'
- en: '**Generalization across LLMs.** Typically, prompt engineering strategies are
    brittle. If we tweak our prompt, we may get a drastically different result. The
    same holds true if we keep the prompt fixed and change our model. If we build
    an LLM-powered application and later decide to switch the underlying model being
    used, we will probably have to change most of the prompts being used as well.
    With techniques like AMA [2], however, we see that prompt ensembles can mitigate
    this problem, as they provide a consistent improvement in performance across a
    variety of different models. Thus, prompt ensembles improve reliability through
    their lack of sensitivity to the underlying model being used.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLMs的泛化。** 通常，提示工程策略是脆弱的。如果我们调整提示，可能会得到截然不同的结果。如果我们保持提示不变但更换模型，同样的情况也会发生。如果我们构建一个基于LLM的应用程序，后来决定更换所使用的底层模型，我们可能还需要更改大部分提示。然而，通过像AMA
    [2]这样的技术，我们看到提示集合可以缓解这个问题，因为它们在各种不同模型中提供了一致的性能提升。因此，提示集合通过对底层模型的不敏感性提高了可靠性。'
- en: '**Aggregation is difficult.** After reading about self-consistency, I was optimistic
    that LLMs could be made significantly more reliable with simple prompting techniques.
    As we see in this overview, this is not always true. We can easily generate multiple,
    diverse outputs for any given problem with an LLM, but the manner in which we
    aggregate these responses is pivotal. Unfortunately, the approaches proposed by
    DiVeRSE and AMA are quite complex and would likely require a significant amount
    of implementation effort. But, we clearly see that just taking a majority vote
    falls short of the performance of more complex techniques. Hopefully, simpler
    aggregation techniques will be proposed soon.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚合很困难。** 在阅读了关于自一致性的内容后，我曾对LLMs能够通过简单的提示技术显著提高可靠性感到乐观。然而，正如我们在这次概述中看到的，这并不总是正确的。我们可以轻松生成多样化的输出，但聚合这些响应的方式至关重要。不幸的是，DiVeRSE和AMA提出的方法相当复杂，可能需要大量的实施工作。不过，我们明显看到，单纯的多数投票无法达到更复杂技术的性能。希望能尽快提出更简单的聚合技术。'
- en: '**Limitations.** Although prompt ensembles are awesome, they are not perfect.
    Techniques like DiVeRSE and AMA rely upon producing numerous outputs with an LLM
    for every question that is answered. We use multiple prompts and might even generate
    multiple responses for every prompt — *that’s a lot of inference with an LLM*!
    Because of this, prompt ensembles can be expensive, both monetarily and from a
    latency perspective. If we want to leverage prompt ensembles in a real-world application,
    we must be very careful about how it is applied, as it could drastically alter
    the application’s cost and efficiency.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**局限性。** 尽管提示集合非常出色，但它们并不完美。像DiVeRSE和AMA这样的技术依赖于为每个回答的问题生成大量的LLM输出。我们使用多个提示，甚至可能为每个提示生成多个响应——*这需要大量的LLM推理*！因此，提示集合在经济和延迟方面都可能非常昂贵。如果我们希望在实际应用中利用提示集合，我们必须非常小心其应用方式，因为这可能会极大地改变应用的成本和效率。'
- en: Closing Remarks
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. You can also check out my [other
    writings](https://medium.com/@wolfecameron) on medium! If you liked it, please
    follow me on [twitter](https://twitter.com/cwolferesearch) or subscribe to my
    [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/), where
    I help readers build a deeper understanding of topics in AI research via understandable
    overviews of popular papers.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的人工智能总监。我研究深度学习的实证和理论基础。你还可以查看我在 medium 上的 [其他文章](https://medium.com/@wolfecameron)！如果你喜欢，请在
    [twitter](https://twitter.com/cwolferesearch) 上关注我，或者订阅我的 [Deep (Learning) Focus
    新闻通讯](https://cameronrwolfe.substack.com/)，在这里我通过对热门论文的易懂概述，帮助读者深入理解人工智能研究中的话题。
- en: Bibliography
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Li, Yifei, et al. “On the advance of making language models better reasoners.”
    *arXiv preprint arXiv:2206.02336* (2022).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Li, Yifei, 等. “使语言模型成为更好的推理器的进展。” *arXiv 预印本 arXiv:2206.02336* (2022)。'
- en: '[2] Arora, Simran, et al. “Ask Me Anything: A simple strategy for prompting
    language models.” *arXiv preprint arXiv:2210.02441* (2022).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Arora, Simran, 等. “问我任何事: 语言模型的简单提示策略。” *arXiv 预印本 arXiv:2210.02441* (2022)。'
- en: '[3] Wei, Jason, et al. “Chain of thought prompting elicits reasoning in large
    language models.” *arXiv preprint arXiv:2201.11903* (2022).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Wei, Jason, 等. “思维链提示在大型语言模型中引发推理。” *arXiv 预印本 arXiv:2201.11903* (2022)。'
- en: '[4] Wang, Xuezhi, et al. “Self-consistency improves chain of thought reasoning
    in language models.” *arXiv preprint arXiv:2203.11171* (2022).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Wang, Xuezhi, 等. “自洽性改善语言模型中的思维链推理。” *arXiv 预印本 arXiv:2203.11171* (2022)。'
- en: '[5] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Devlin, Jacob, 等. “Bert: 语言理解的深度双向变换器预训练。” *arXiv 预印本 arXiv:1810.04805*
    (2018)。'
- en: '[6] He, Pengcheng, et al. “Deberta: Decoding-enhanced bert with disentangled
    attention.” *arXiv preprint arXiv:2006.03654* (2020).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] He, Pengcheng, 等. “Deberta: 解码增强的 bert 结合解耦注意力。” *arXiv 预印本 arXiv:2006.03654*
    (2020)。'
- en: '[7] Chowdhery, Aakanksha, et al. “Palm: Scaling language modeling with pathways.”
    *arXiv preprint arXiv:2204.02311* (2022).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Chowdhery, Aakanksha, 等. “Palm: 通过路径扩展语言建模。” *arXiv 预印本 arXiv:2204.02311*
    (2022)。'
- en: '[8] Ratner, Alexander, et al. “Snorkel: Rapid training data creation with weak
    supervision.” *Proceedings of the VLDB Endowment. International Conference on
    Very Large Data Bases*. Vol. 11\. №3\. NIH Public Access, 2017.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Ratner, Alexander, 等. “Snorkel: 使用弱监督快速创建训练数据。” *VLDB 会议论文集. 国际大型数据会议*.
    第 11 卷，第 3 期，NIH 公共访问，2017。'
- en: '[9] Varma, Paroma, et al. “Learning dependency structures for weak supervision
    models.” *International Conference on Machine Learning*. PMLR, 2019.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Varma, Paroma, 等. “为弱监督模型学习依赖结构。” *国际机器学习会议*. PMLR，2019。'
- en: '[10] Ratner, Alexander, et al. “Training complex models with multi-task weak
    supervision.” *Proceedings of the AAAI Conference on Artificial Intelligence*.
    Vol. 33\. №01\. 2019.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Ratner, Alexander, 等. “用多任务弱监督训练复杂模型。” *AAAI 人工智能会议论文集*. 第 33 卷，第 01 期，2019。'
- en: '[11] Taylor, Ross, et al. “Galactica: A large language model for science.”
    *arXiv preprint arXiv:2211.09085* (2022).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Taylor, Ross, 等. “Galactica: 用于科学的大型语言模型。” *arXiv 预印本 arXiv:2211.09085*
    (2022)。'
- en: '[12] Thoppilan, Romal, et al. “Lamda: Language models for dialog applications.”
    *arXiv preprint arXiv:2201.08239* (2022).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Thoppilan, Romal, 等. “Lamda: 对话应用的语言模型。” *arXiv 预印本 arXiv:2201.08239*
    (2022)。'
- en: '[13] Glaese, Amelia, et al. “Improving alignment of dialogue agents via targeted
    human judgements.” *arXiv preprint arXiv:2209.14375* (2022).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Glaese, Amelia, 等. “通过有针对性的人类判断改善对话代理的对齐。” *arXiv 预印本 arXiv:2209.14375*
    (2022)。'
- en: '[14] Chowdhery, Aakanksha, et al. “Palm: Scaling language modeling with pathways.”
    *arXiv preprint arXiv:2204.02311* (2022).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Chowdhery, Aakanksha, 等. “Palm: 通过路径扩展语言建模。” *arXiv 预印本 arXiv:2204.02311*
    (2022)。'
- en: '[15] Cobbe, Karl, et al. “Training verifiers to solve math word problems.”
    *arXiv preprint arXiv:2110.14168* (2021).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Cobbe, Karl, 等. “训练验证器解决数学问题。” *arXiv 预印本 arXiv:2110.14168* (2021)。'
- en: '[16] Kojima, Takeshi, et al. “Large language models are zero-shot reasoners.”
    *arXiv preprint arXiv:2205.11916* (2022).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Kojima, Takeshi, 等. “大型语言模型是零-shot 推理器。” *arXiv 预印本 arXiv:2205.11916*
    (2022)。'
- en: '[17] Zhou, Denny, et al. “Least-to-most prompting enables complex reasoning
    in large language models.” *arXiv preprint arXiv:2205.10625* (2022).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Zhou, Denny, 等. “最少到最多提示使大型语言模型能够进行复杂推理。” *arXiv 预印本 arXiv:2205.10625*
    (2022)。'
