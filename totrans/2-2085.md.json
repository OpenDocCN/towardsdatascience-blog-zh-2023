["```py\nclass Agent:\n    def __init__(\n        self, \n        nrow_maze: int,\n        ncol_maze: int,\n        actions: list = [0, 1, 2, 3],\n        rewards: dict = {\n            'step': 0.0, \n            'wall': 0.0,\n            'goal': 10,\n        }, \n        gamma: float = 0.9,\n        alpha: float = 0.1,\n        epsilon: float = 0.1,\n        seed: int = 42,\n        ) -> None:\n        \"\"\"\n        Creates an agent for the maze environment.\n\n        Parameters\n        ----------\n        nrow_maze : int\n            The number of rows in the maze.\n        ncol_maze : int\n            The number of columns in the maze.\n        actions : list, optional\n            A list of actions that the agent can take. The default is [0, 1, 2, 3]. \n            0: Up\n            1: Down\n            2: Left\n            3: Right\n        rewards : dict, optional\n            A dictionary of rewards for the agent. The default is {'step': -1, 'wall': -10, 'goal': 10}.\n        gamma : float, optional\n            The discount factor. The default is 0.9.\n        alpha : float, optional\n            The learning rate. The default is 0.1.\n        epsilon : float, optional\n            The exploration rate. The default is 0.1.\n        seed : int, optional\n            The seed for the random generator. The default is 42.\n        \"\"\"\n        self.nrow_maze = nrow_maze\n        self.ncol_maze = ncol_maze\n        self.rewards = rewards\n        self.gamma = gamma\n        self.alpha = alpha\n        self.epsilon = epsilon\n        self.seed = seed\n        self.actions = actions\n\n        # By default, the starting index is 0 0 \n        self.start_state = 0\n\n        # By default, the goal index is the last index\n        self.goal_state = nrow_maze * ncol_maze - 1\n\n        # Creating the random generator with a fixed seed\n        self.random_generator = np.random.default_rng(seed)\n\n        # Creating the maze; We will denote it internaly as S \n        self.init_S_table()\n\n        # Initiating the Q-table \n        self.init_Q_table()\n\n        # Saving the initial past_action and past_state\n        self.past_action = None\n        self.past_state = None\n\n        # Creating the action name dictionary \n        self.action_name_dict = {\n            0: 'up',\n            1: 'down',\n            2: 'left',\n            3: 'right',\n        }\n\n        # Counter for the number of times our agent has seen the terminal state\n        self.num_goal_reached = 0\n\n        # Counter for each state and how many times the agent visited each \n        self.state_visit_counter = {}\n\n        # Empty dictionary of states visition paths\n        self.state_visit_paths = {}\n\n        # Placeholder for the current episode of learning \n        self.current_episode = 0\n\n  #####\n  # OTHER METHODS BELLOW\n  #####\n```", "```py\ndef argmax(self, q_values: np.array):\n        \"\"\"argmax with random tie-breaking\n        Args:\n            q_values (Numpy array): the array of action values\n        Returns:\n            action (int): an action with the highest value\n        \"\"\"\n        top = float(\"-inf\")\n        ties = []\n\n        for i in range(len(q_values)):\n            if q_values[i] > top:\n                top = q_values[i]\n                ties = []\n\n            if q_values[i] == top:\n                ties.append(i)\n\n        return self.random_generator.choice(ties)\n\n    def get_greedy_action(self, state: int) -> int:\n        \"\"\"\n        Returns the greedy action given the current state\n        \"\"\"\n        # Getting the q values for the current state\n        q_values = self.Q[state]\n\n        # Getting the greedy action\n        greedy_action = self.argmax(q_values)\n\n        # Returning the greedy action\n        return greedy_action\n\n    def get_epsilon_greedy_action(self, state: int) -> int: \n        \"\"\"\n        Returns an epsilon greedy action\n        \"\"\"\n        if self.random_generator.random() < self.epsilon:\n            return self.get_action()\n        else:\n            return self.get_greedy_action(state)\n```", "```py\ndef update_Q_table(self, new_state: int): \n        \"\"\"\n        Function that applies the RL update function\n        \"\"\" \n        # Getting the next_state's reward\n        reward = self.reward_dict[new_state]\n\n        # Saving the current Q value\n        current_Q = self.Q[self.past_state][self.past_action]\n\n        # If the new state is the terminal state or the wall state, then the max_Q is 0\n        max_Q = 0\n\n        # Else we get the max Q value for the new state\n        if new_state != self.goal_state:\n            new_state_Q_values = self.Q[new_state]\n\n            # Getting the max Q value \n            max_Q = np.max(new_state_Q_values)\n\n        # Updating inplace the Q value \n        self.Q[self.past_state][self.past_action] = current_Q + self.alpha * (reward + self.gamma * max_Q - current_Q)\n```", "```py\n def terminal_step(self, new_state: int):\n        \"\"\"\n        Updates the agent one last time and resets the agent to the starting position\n        \"\"\" \n        # Updating the Q table\n        self.update_Q_table(new_state)\n\n        # Resetting the agent\n        self.past_state = self.start_state\n        self.past_action = self.get_epsilon_greedy_action(self.past_state)\n\n        # Incrementing the number of episodes\n        self.current_episode += 1\n\n    def get_next_state(self, s: int, action: int) -> int: \n          \"\"\"\n          Given the current state and the current action, returns the next state index\n          \"\"\"\n          # Getting the state coordinates\n          s_row, s_col = self.get_state_coords(s)\n\n          # Setting the boolean indicating that we have reached the terminal state \n          reached_terminal = False\n\n          # Getting the next state\n          next_state = -1\n          if action == 0:\n              next_state = self.get_state_index(s_row - 1, s_col)\n          elif action == 1:\n              next_state = self.get_state_index(s_row + 1, s_col)\n          elif action == 2:\n              next_state = self.get_state_index(s_row, s_col - 1)\n          elif action == 3:\n              next_state = self.get_state_index(s_row, s_col + 1)\n\n          # If next_state is a wall or the agent is out of bounds, we will stay in the same state\n          if (next_state == -1) or (next_state in self.wall_states):\n              return s, reached_terminal\n\n          # If next_state is the goal state, we will return to the starting state\n          if next_state == self.goal_state:\n              # Incrementing the number of times our agent has reached the goal state\n              self.num_goal_reached += 1\n              reached_terminal = True\n\n          # Returning the next state\n          return next_state, reached_terminal\n\n    def move_agent(self): \n        \"\"\" \n        The function that moves the agent to the next state\n        \"\"\"\n        # Getting the next state\n        next_state, reached_terminal = self.get_next_state(self.past_state, self.past_action)\n\n        # Updating the Q table\n        if not reached_terminal:\n            # Checking if the past_state is the same as the next_state; If that is true, it means our agent hit a wall \n            # or went out of bounds\n            if self.past_state != next_state:\n                self.update_Q_table(next_state)\n\n            # Setting the past_state as the next_state\n            self.past_state = next_state\n\n            # Getting the next action\n            self.past_action = self.get_epsilon_greedy_action(self.past_state)\n        else: \n            self.terminal_step(next_state)\n```", "```py\n def train_episodes(self, num_episodes: int):\n      \"\"\"\n      Function that trains the agent for one episode\n      \"\"\"\n      # Calculating the episode number to end the training \n      end_episode = self.current_episode + num_episodes - 1\n\n      # Moving the agent until we reach the goal state\n      while self.current_episode != end_episode:\n          self.move_agent()\n```", "```py\n# Creating an agent object\nagent = Agent(\n    nrow_maze=6,\n    ncol_maze=8,\n    seed=6,\n    rewards={'step': 0, 'goal': 10}\n)\n\n# Initiating the maze \nagent.init_maze(maze_density=11)\n\n# Training the agent for one episode\nagent.train_episodes(num_episodes=1)\n```", "```py\nagent.Q[30]\n\n# Returns \n# array([0., 1., 0., 0.])\n```", "```py\n# Training the agent for one episode\nagent.train_episodes(num_episodes=1)\n\n# Printing out the agent's 37 state\nagent.Q[37]\n\n# Returns \n# array([0., 0., 0., 1.])\n```", "```py\n# Creating an agent object\nagent = Agent(\n    nrow_maze=6,\n    ncol_maze=8,\n    seed=6,\n    rewards={'step': 0, 'goal': 10}\n)\n\n# Initiating the maze \nagent.init_maze(maze_density=11)\n\n# Letting the agent wonder for 1000 episodes\nagent.train_episodes(100)\nstate_visits = agent.state_visit_paths\nsteps = [len(state_visits[episode]) for episode in state_visits]\n\n# Ploting the number of steps per episode\nplt.plot(steps)\nplt.title(\"Number of steps per episode\")\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Number of steps\")\nplt.show()\n```"]