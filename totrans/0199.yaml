- en: A Guide to Building Effective Training Pipelines for Maximum Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee](https://towardsdatascience.com/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[THE FULL STACK 7-STEPS MLOPS FRAMEWORK](https://towardsdatascience.com/tagged/full-stack-mlops)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lesson 2: Training Pipelines. ML Platforms. Hyperparameter Tuning.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pauliusztin.medium.com/?source=post_page-----6fdaef594cee--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----6fdaef594cee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6fdaef594cee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6fdaef594cee--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----6fdaef594cee--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6fdaef594cee--------------------------------)
    ¬∑19 min read¬∑May 9, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27d8728645d28e45ee8757d4bedf42c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial represents **lesson 2 out of a 7-lesson course** that will walk
    you step-by-step through how to **design, implement, and deploy an ML system**
    using **MLOps good practices**. During the course, you will build a production-ready
    model to forecast energy consumption levels for the next 24 hours across multiple
    consumer types from Denmark.
  prefs: []
  type: TYPE_NORMAL
- en: '*By the end of this course, you will understand all the fundamentals of designing,
    coding and deploying an ML system using a batch-serving architecture.*'
  prefs: []
  type: TYPE_NORMAL
- en: This course *targets mid/advanced machine learning engineers* who want to level
    up their skills by building their own end-to-end projects.
  prefs: []
  type: TYPE_NORMAL
- en: '*Nowadays, certificates are everywhere. Building advanced end-to-end projects
    that you can later show off is the best way to get recognition as a professional
    engineer.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Table of Contents:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Course Introduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Course Lessons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lesson 2: Training Pipelines. ML Platforms. Hyperparameter Tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lesson 2: Code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Course Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***At the end of this 7 lessons course, you will know how to:***'
  prefs: []
  type: TYPE_NORMAL
- en: design a batch-serving architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Hopsworks as a feature store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: design a feature engineering pipeline that reads data from an API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build a training pipeline with hyper-parameter tunning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use W&B as an ML Platform to track your experiments, models, and metadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: implement a batch prediction pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Poetry to build your own Python packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: deploy your own private PyPi server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: orchestrate everything with Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use the predictions to code a web app using FastAPI and Streamlit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Docker to containerize your code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Great Expectations to ensure data validation and integrity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: monitor the performance of the predictions over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: deploy everything to GCP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build a CI/CD pipeline using GitHub Actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If that sounds like a lot, don't worry, after you will cover this course you
    will understand everything I said before. Most importantly, you will know WHY
    I used all these tools and how they work together as a system.
  prefs: []
  type: TYPE_NORMAL
- en: '**If you want to get the most out of this course,** [**I suggest you access
    the GitHub repository**](https://github.com/iusztinpaul/energy-forecasting) **containing
    all the lessons'' code. This course is designed to read and replicate the code
    along the articles quickly.**'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the course, you will know how to implement the diagram below.
    Don't worry if something doesn't make sense to you. I will explain everything
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of the architecture you will build during the course [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: By the **end of Lesson 2**, you will know how to implement and integrate the
    **training pipeline** and **ML platform**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** This is the most extended lesson, as I couldn''t logically split
    the training pipeline from the ML platform. Enjoy!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Course Lessons:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Batch Serving. Feature Stores. Feature Engineering Pipelines.](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training Pipelines. ML Platforms. Hyperparameter Tuning.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Batch Prediction Pipeline. Package Python Modules with Poetry.](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Private PyPi Server. Orchestrate Everything with Airflow.](https://medium.com/towards-data-science/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Data Validation for Quality and Integrity using GE. Model Performance Continuous
    Monitoring.](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Consume and Visualize your Model‚Äôs Predictions using FastAPI and Streamlit.
    Dockerize Everything.](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Deploy All the ML Components to GCP. Build a CI/CD Pipeline Using Github Actions.](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[[Bonus] Behind the Scenes of an ‚ÄòImperfect‚Äô ML Project ‚Äî Lessons and Insights](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you want to grasp this lesson fully, we recommend you check out the previous
    lesson, which talks about designing a batch-serving architecture, building a FE
    pipeline, and loading features into the feature store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f?source=post_page-----6fdaef594cee--------------------------------)
    [## A Framework for Building a Production-Ready Feature Engineering Pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f?source=post_page-----6fdaef594cee--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Data Source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used a free & open API that provides hourly energy consumption values for
    all the energy consumer types within Denmark [1].
  prefs: []
  type: TYPE_NORMAL
- en: They provide an intuitive interface where you can easily query and visualize
    the data. [You can access the data here](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1].
  prefs: []
  type: TYPE_NORMAL
- en: 'The data has 4 main attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hour UTC:** the UTC datetime when the data point was observed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Price Area:** Denmark is divided into two price areas: DK1 and DK2 ‚Äî divided
    by the Great Belt. DK1 is west of the Great Belt, and DK2 is east of the Great
    Belt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumer Type:** The consumer type is the Industry Code DE35, owned and maintained
    by Danish Energy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total Consumption:** Total electricity consumption in kWh'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note:** The observations have a lag of 15 days! But for our demo use case,
    that is not a problem, as we can simulate the same steps as it would be in real-time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot from our web app showing how we forecasted the energy consumption
    for area = 1 and consumer_type = 212 [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: 'The data points have an hourly resolution. For example: ‚Äú2023‚Äì04‚Äì15 21:00Z‚Äù,
    ‚Äú2023‚Äì04‚Äì15 20:00Z‚Äù, ‚Äú2023‚Äì04‚Äì15 19:00Z‚Äù, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: We will model the data as multiple time series. Each unique **price area** and
    **consumer type tuple represents its** unique time series.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we will build a model that independently forecasts the energy consumption
    for the next 24 hours for every time series.
  prefs: []
  type: TYPE_NORMAL
- en: '*Check out the video below to better understand what the data looks like* üëá'
  prefs: []
  type: TYPE_NORMAL
- en: Course & data source overview [Video by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 2: **Training Pipelines. ML Platforms. Hyperparameter Tuning.**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Goal of Lesson 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This lesson will teach you how to build the training pipeline and use an ML
    platform, as shown in the diagram below üëá
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0596cb714571c8febed92fc310c1a6f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of the final architecture with the Lesson 2 components highlighted in
    blue [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: More concretely, we will show you how to use the data from the Hopsworks feature
    store to train your model.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we will show you how to build a forecasting model using LightGBM and Sktime
    that will predict the energy consumption levels for the next 24 hours between
    multiple consumer types across Denmark.
  prefs: []
  type: TYPE_NORMAL
- en: Another critical step we will cover is how to use W&B as an ML platform that
    will track your experiments, register your models & configurations as artifacts,
    and perform hyperparameter tuning to find the best configuration for your model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, based on the best config found in the hyperparameter tuning step, we
    will train the final model on the whole dataset and load it into the Hopsworks
    model registry to be used further by the batch prediction pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '***NOTE:***This course is not about time series forecasting or hyperparameter
    tuning. This is an ML engineering course where I want to show you how multiple
    pieces come together into a single system. Thus, I will keep things straight to
    the point for the DS part of the code without going into too much detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical Concepts & Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Sktime:** Sktimeis a Python package that provides tons of functionality for
    time series. It follows the same interface as Sklearn, hence its name. Using Sktime,
    we can quickly wrap LightGBM and perform forecasting for 24 hours in the future,
    cross-validation, and more. [Sktime official documentation](https://www.sktime.net/en/latest/index.html)
    [3]'
  prefs: []
  type: TYPE_NORMAL
- en: '**LightGBM:** LightGBM is a boosting tree-based model. It is built on top of
    Gradient Boosting and XGBoost, offering performance and speed improvements. Starting
    with XGBoost or LightGBM is a common practice. [LightGBM official documentation](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html)
    [4]'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about LightGBM, check out my article, where I [explain
    in 15 minutes everything you need to know, from decision trees to LightGBM](https://medium.com/mlearning-ai/decision-trees-from-0-to-xgboost-lightgbm-a5f6827dfa23).
  prefs: []
  type: TYPE_NORMAL
- en: '**ML Platform:** An ML platform is a tool that allows you to easily track your
    experiments, log metadata about your training, upload and version artifacts, data
    lineage and more. An ML platform is a must in any training pipeline. You can intuitively
    see an ML platform as your central research & experimentation hub.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weights & Biases:** W&B is a popular serverless ML platform. We choose them
    as our ML platform because of 3 main reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: their tool is fantastic & very intuitive to use
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: they provide a generous freemium version for personal research and projects
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: it is serverless ‚Äî no pain in deploying & maintaining your tools
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training Pipeline:** The training pipeline is a logical construct (a single
    script, an application, or more) that takes curated and validated data as input
    (a result from the data and feature engineering pipelines) and outputs a working
    model as an artifact. Usually, the model is uploaded into a model registry that
    can later be accessed by various inference pipelines (the batch prediction pipeline
    from our series is an example of a concrete implementation of an inference pipeline).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 2: Code'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[You can access the GitHub repository here.](https://github.com/iusztinpaul/energy-forecasting)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** All the installation instructions are in the READMEs of the repository.
    Here we will jump straight to the code.'
  prefs: []
  type: TYPE_NORMAL
- en: '*All the code within Lesson 2 is located under the* [***training-pipeline***](https://github.com/iusztinpaul/energy-forecasting/tree/main/training-pipeline)*folder.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The files under the [**training-pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/training-pipeline)folderare
    structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1df1548f211cb1fc256f23c4f77a28c4.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot that shows the structure of the training-pipeline folder [Image
    by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: All the code is located under the [**training_pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/training-pipeline/training_pipeline)directory
    (note the "_" instead of "-")**.**
  prefs: []
  type: TYPE_NORMAL
- en: Directly storing credentials in your git repository is a huge security risk.
    That is why you will inject sensitive information using a **.env** file.
  prefs: []
  type: TYPE_NORMAL
- en: The **.env.default** is an example of all the variables you must configure.
    It is also helpful to store default values for attributes that are not sensitive
    (e.g., project name).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b4f40ca19a12ac8ff070610a8530d46.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot of the .env.default file [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: '***Prepare Credentials***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, we have to create a **.env** filewhere we will add all our credentials.
    I already showed you in [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how to set up your **.env** file. Also, I explained in [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how the variables from the **.env** file are loaded from your **ML_PIPELINE_ROOT_DIR**
    directory into a **SETTINGS** Python dictionary to be used throughout your code.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if you want to replicate what I have done, I strongly recommend checking
    out [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f).
  prefs: []
  type: TYPE_NORMAL
- en: '*If you only want a light read, you can completely skip the ‚Äú****Prepare Credentials****‚Äù
    step.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Lesson 2, we will use two services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Hopsworks](https://www.hopsworks.ai/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Weights & Biases](https://wandb.ai/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[***Hopsworks***](https://www.hopsworks.ai/) ***(free)***'
  prefs: []
  type: TYPE_NORMAL
- en: We already showed you in [Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how to set up the credentials for **Hopsworks**. Please visit the ["Prepare Credentials"
    section from Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f),
    where we showed you in detail how to set up the API KEY for Hopsworks.
  prefs: []
  type: TYPE_NORMAL
- en: '[***Weights & Biases***](https://wandb.ai/) ***(free)***'
  prefs: []
  type: TYPE_NORMAL
- en: To keep the lessons compact, we assume that you already read and applied the
    steps for preparing the credentials for Hopsworks from [Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f).
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that 90% of the steps are similar to the ones for configuring
    **Hopsworks**, except for how you can get your API key from W&B.
  prefs: []
  type: TYPE_NORMAL
- en: First, create an account on W&B. After, create a team (aka entity) and a project
    (or use your default ones, if you have any).
  prefs: []
  type: TYPE_NORMAL
- en: '***Then, check the image below to see how to get your own W&B API KEY üëá***'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/42df4ffff19dc073c2ff162ff980c136.png)'
  prefs: []
  type: TYPE_IMG
- en: Go to your W&B account. After, in the top-right corner, click your profile account,
    then "User settings." Once in your user settings, scroll down until you reach
    the "Danger Zone" card. Then, under the "API keys," hit the "New key" button.
    Copy your API key, and that is it. You have your API key [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have all your W&B credentials, go to your **.env** file and replace
    them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**WANDB_ENTITY:** your entity/team name (ours: *‚Äúteaching-mlops‚Äù*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WANDB_PROJECT:** your project name (ours: *‚Äúenergy_consumption‚Äù*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WANDB_API_KEY**: your API key'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the Data From the Feature Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As always, the first step is to access the data used to train and test the model.
    We already have all the data in the Hopsworks feature store. Thus, downloading
    it becomes a piece of cake.
  prefs: []
  type: TYPE_NORMAL
- en: The code snippet below has the **load_dataset_from_feature_store**() IO function
    under the [**training_pipeline/data.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/training-pipeline/training_pipeline/data.py)
    file. You will use this function to download the data for a given **feature_view_version**
    and **training_dataset_version.**
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE:** By giving a specific data version, you will always know with what
    data you trained and evaluated the model. Thus, you can consistently reproduce
    your results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the function below, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We access the Hopsworks feature store.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We get a reference to the given version of the feature view.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We get a reference to the given version of the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We log to W&B all the metadata that relates to the used dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we downloaded the dataset, we run it through the **prepare_data()**
    function. We will detail it a bit later. For now, notice that we split the data
    between train and test.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We log to W&B all the metadata related to how we split the dataset, plus some
    basic statistics for every split, such as split size and features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Important observation:** Using W&B, you log all the metadata that describes
    how you extracted and prepared the data. By doing so, you can easily understand
    for every experiment the origin of its data.'
  prefs: []
  type: TYPE_NORMAL
- en: By using **run.use_artifact("<artifact_name>"),** you can link different artifacts
    between them. In our example, by calling **run.use_artifact(‚Äúenergy_consumption_denmark_feature_view:latest‚Äù)**
    we linked this W&B run with an artifact created in a different W&B run.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the video below to see how the W&B runs & artifacts look like in the
    W&B interface üëá
  prefs: []
  type: TYPE_NORMAL
- en: W&B artifacts overview [Video by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's dig into the **prepare_data()** function.
  prefs: []
  type: TYPE_NORMAL
- en: '*I want to highlight that in the* ***prepare_data()*** *function, we won''t
    perform any feature engineering steps.*'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see below, in this function, you will restructure the data to be
    compatible with the **sktime** interface, pick the target, and split the data.
  prefs: []
  type: TYPE_NORMAL
- en: The data is modeled for hierarchical time series, translating to multiple independent
    observations of the same variable in different contexts. In our example, we observe
    the energy consumption for various areas and energy consumption types.
  prefs: []
  type: TYPE_NORMAL
- en: Sktime, for hierarchical time series, expects the data to be modeled using multi-indexes,
    where the datetime index is the last. To learn more about hierarchical forecasting,
    check out [Sktime's official tutorial](https://github.com/sktime/sktime/blob/main/examples/01c_forecasting_hierarchical_global.ipynb)
    [7].
  prefs: []
  type: TYPE_NORMAL
- en: Also, we can safely split the data using **sktime's temporal_train_test_split()**
    function. The test split has the length of the given **fh (=forecast horizon)**.
  prefs: []
  type: TYPE_NORMAL
- en: One key observation is that the test split isn't sampled randomly but based
    on the latest observation. For example, if you have data from the 1st of May 2023
    until the 7th of May 2023 with a frequency of 1 hour, then the test split with
    a length of 24 hours will contain all the values from the last day of the data,
    which is 7th of May 2023.
  prefs: []
  type: TYPE_NORMAL
- en: '**Building the Forecasting Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Baseline model**'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, you will create a naive baseline model to use as a reference. This
    model predicts the last value based on a given seasonal periodicity.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if **seasonal_periodicity = 24 hours**, it will return the value
    from "**present - 24 hours"**.
  prefs: []
  type: TYPE_NORMAL
- en: Using a baseline is a healthy practice that helps you compare your fancy ML
    model to something simpler. The ML model is useless if you can't beat the baseline
    model with your fancy model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fancy ML model**'
  prefs: []
  type: TYPE_NORMAL
- en: We will build the model using Sktime and LightGBM.
  prefs: []
  type: TYPE_NORMAL
- en: Check out [Sktime documentation](https://www.sktime.net/en/latest/index.html)
    [3] and [LightGBM documentation](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html)
    [4] here.
  prefs: []
  type: TYPE_NORMAL
- en: If you are into time series, check out this [Forecasting with Sktime tutorial](https://www.sktime.net/en/latest/examples/01_forecasting.html#1.-Basic-forecasting-workflows)
    [6]. If you only want to understand the system's big picture, you can continue.
  prefs: []
  type: TYPE_NORMAL
- en: LightGBM will be your regressor that learns patterns within the data and forecasts
    future values.
  prefs: []
  type: TYPE_NORMAL
- en: Using the **WindowSummarizer** class from **Sktime,** you can quickly compute
    lags and mean & standard deviation for various windows.
  prefs: []
  type: TYPE_NORMAL
- en: For example, for the lag, we provide a default value of **list(range(1, 72 +
    1)),** which translates to "compute the lag for the last 72 hours".
  prefs: []
  type: TYPE_NORMAL
- en: Also, as an example of the mean lag, we have the default value of **[[1, 24],
    [1, 48], [1, 72]].** For example, **[1, 24]** translates to a lag of 1 and a window
    size of 24, meaning it will compute the mean in the last 24 days. Thus, in the
    end, for **[[1, 24], [1, 48], [1, 72]],** you will have the mean for the last
    24, 46, and 72 days.
  prefs: []
  type: TYPE_NORMAL
- en: The same principle applies to the standard deviation values. [Check out this
    doc to learn more](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.transformations.series.summarize.WindowSummarizer.html?highlight=windowsummarizer)
    [2].
  prefs: []
  type: TYPE_NORMAL
- en: You wrap the LightGBM model using the **make_reduction()** function from **Sktime.**
    By doing so, you can easily attach the **WindowSummarizer** you initialized earlier.
    Also, by specifying **strategy = "recursive",** you can easily forecast multiple
    values into the future using a recursive paradigm. For example, if you want to
    predict 3 hours into the future, the model will first forecast the value for T
    + 1\. Afterward, it will use as input the value it forecasted at T + 1 to forecast
    the value at T + 2, and so on‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will build the **ForecastingPipeline** where we will attach two
    transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**transformers.AttachAreaConsumerType():** a custom transformer that takes
    the area and consumer type from the index and adds it as an exogenous variable.
    We will show you how we defined it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**DateTimeFeatures():** a transformer from **Sktime** that computes different
    datetime-related exogenous features. In our case, we used only the day of the
    week and the hour of the day as additional features.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that these transformers are similar to the ones from **Sklearn,** as **Sktime**
    kept the same interface and design. Using transformers is a critical step in designing
    modular models. To learn more about Sklearn transformers and pipelines, check
    out my article about [How to Quickly Design Advanced Sklearn Pipelines](/how-to-quickly-design-advanced-sklearn-pipelines-3cc97b59ce16).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we initialized the hyperparameters of the pipeline and model with the
    given configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The **AttachAreaConsumerType** transformer is quite easy to comprehend. We implemented
    it as an example to show what is possible.
  prefs: []
  type: TYPE_NORMAL
- en: Long story short, it just copies the values from the index into its own column.
  prefs: []
  type: TYPE_NORMAL
- en: '**IMPORTANT OBSERVATION ‚Äî DESIGN DECISION**'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, all the feature engineering steps are built-in into the forecasting
    pipeline object.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might ask: "But why? By doing so, don''t we keep the feature engineering
    logic in the training pipeline?"'
  prefs: []
  type: TYPE_NORMAL
- en: Well, yes‚Ä¶ and no‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: We indeed defined the forecasting pipeline in the training script, but the key
    idea is that we will save the whole forecasting pipeline to the model registry.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, when we load the model, we will also load all the preprocessing and postprocessing
    steps included in the forecasting pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: This means all the feature engineering is encapsulated in the forecasting pipeline,
    and we can safely treat it as a black box.
  prefs: []
  type: TYPE_NORMAL
- en: This is one way to store the transformation + the raw data in the feature store,
    as discussed in [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f).
  prefs: []
  type: TYPE_NORMAL
- en: We could have also stored the transformation functions independently in the
    feature store, but composing a single pipeline object is cleaner.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**How to use W&B sweeps**'
  prefs: []
  type: TYPE_NORMAL
- en: You will use W&B to perform hyperparameter tuning. They provide all the methods
    you need. Starting from a regular Grid Search until a Bayesian Search.
  prefs: []
  type: TYPE_NORMAL
- en: W&B uses ***sweeps*** to do hyperparameter tuning. A sweep is a fancy word for
    a single experiment within multiple experiments based on your hyperparameter search
    space.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the MAPE (mean absolute percentage error) metric to compare experiments
    to find the best hyperparameter configuration. We chose MAPE over MAE or RMSE
    because the values are normalized between [0, 1], thus making it easier to analyze.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the video below to see how the sweeps board looks in W&B üëá
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand our goal let's look at the code under the [**training_pipeline/hyperparamter_tuning.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/training-pipeline/training_pipeline/hyperparameter_tuning.py)
    file.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the function below, we load the dataset from the feature store
    for a specific **feature_view_version** and a **training_dataset_version.**
  prefs: []
  type: TYPE_NORMAL
- en: Using solely the training data, we start the hyperparameter optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** It is essential that you don''t use your test data for your hyperparameter
    optimization search. Otherwise, you risk overfitting your test split, and your
    model will not generalize. Your test split should be used only for the final decision.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we save the metadata of the run, which contains the **sweep_id** of
    the search.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at the **run_hyperparameter_optimization()** function, which
    takes the training data, creates a new sweep and starts a W&B agent.
  prefs: []
  type: TYPE_NORMAL
- en: Within a single sweep run, we build the model and train the model using cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the config is provided by W&B based on the given hyperparameter
    search space (we will explain this in a bit). Also, we log the config as an artifact
    to access it later.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we used a simple grid search to perform hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see below, we created a Python dictionary called **sweep_config**
    with themethod, the metric to minimize, and the parameters to search for.
  prefs: []
  type: TYPE_NORMAL
- en: '[Check out W&B official docs to learn more about sweeps](https://docs.wandb.ai/guides/sweeps)
    [5].'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** With a few tweaks, you can quickly run multiple W&B agents in parallel
    within a single sweep. Thus, speeding up the hyperparameter tuning drastically.
    [Check out their docs if you want to learn more](https://docs.wandb.ai/guides/sweeps/parallelize-agents)
    [5].'
  prefs: []
  type: TYPE_NORMAL
- en: '**How to do cross-validation with time series data**'
  prefs: []
  type: TYPE_NORMAL
- en: So, I highlighted that it is critical to do hyperparameter-tuning only using
    the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: But then, on what split should you compute your metrics?
  prefs: []
  type: TYPE_NORMAL
- en: Well, you will be using cross-validation adapted to time series.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the image below, we used a 3-fold cross-validation technique. The
    key idea is that because you are using time series data, you can't pick the whole
    dataset for every fold. It makes sense, as you can't learn from the future to
    predict the past.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, using the same principles as when we split the data between train and
    test, we sample 1/3 from the beginning of the dataset, where the **forecasting
    horizon (the orange segment)** is used to compute the validation metric. The next
    fold takes 2/3, and the last one 3/3 of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f6149c500f3a27660f20c03f1fc9d6c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Once again, **Sktime** makes our lives easier. Using the **ExpandingWindowSplitter**
    class and **cv_evaluate()** function, you can quickly train and evaluate the model
    using the specified cross-validation strategy ‚Äî [official docs here](https://github.com/sktime/sktime/blob/main/examples/window_splitters.ipynb)
    [8]
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we restructured the **results** DataFrame, which the **cv_evaluate()**
    function returned to fit our interface.
  prefs: []
  type: TYPE_NORMAL
- en: Excellent, now you finished running your hyperparameter tuning step using W&B
    sweeps.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this step, we have a **sweep_id** that has attached multiple experiments,
    where each experiment has a **config artifact.**
  prefs: []
  type: TYPE_NORMAL
- en: Now we have to parse this information and create a **best_config artifact.**
  prefs: []
  type: TYPE_NORMAL
- en: Upload the Best Configuration from the Hyperparameter Tuning Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the [**training_pipeline/best_config.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/training-pipeline/training_pipeline/best_config.py)script,
    we will parse all the experiments for the given **sweep_id** and find the best
    experiment with the lowest MAPE validation score.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, this is done automatically by W&B when we call the **best_run()**
    function. After, you resume the **best_run** and rename the run to **best_experiment.**
  prefs: []
  type: TYPE_NORMAL
- en: Also, you upload the config attached to the best configuration into its artifact
    called **best_config.**
  prefs: []
  type: TYPE_NORMAL
- en: Later, we will use this artifact to train models from scratch as often as we
    want.
  prefs: []
  type: TYPE_NORMAL
- en: Now you have the **best_config** artifact that tells you precisely what hyperparameters
    you should use to train your final model on.
  prefs: []
  type: TYPE_NORMAL
- en: Train the Final Model Using the Best Configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, training and loading the final model to the model registry is the last
    piece of the puzzle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the **from_best_config()** function from the [**training_pipeline/train.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/training-pipeline/training_pipeline/train.py)file,
    we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the data from Hopsworks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a W&B run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the best_config artifact.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the baseline model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train and evaluate the baseline model on the test split.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the fancy model using the latest best configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train and evaluate the fancy model on the test split.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Render the results to see how they perform visually.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrain the model on the whole dataset. This is critical for time series models
    as you must retrain them until the present moment to forecast the future.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forecast future values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Render the forecasted values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the best model as an Artifact in W&B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the best model in the Hopsworks' model registry
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Note:** You can either use W&B Artifacts as a model registry or directly
    use the Hopsworks model registry feature. We will show you how to do it both ways.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Notice how we used* ***wandb.log()*** *to upload to W&B all the variables
    of interest.*'
  prefs: []
  type: TYPE_NORMAL
- en: Check out this video to visually see how we use W&B as an experiment tracker
    üëá
  prefs: []
  type: TYPE_NORMAL
- en: '**Train & evaluate the model.**'
  prefs: []
  type: TYPE_NORMAL
- en: To train any **Sktime** model, we implemented this general function that takes
    in any model, the data, and the forecast horizon.
  prefs: []
  type: TYPE_NORMAL
- en: Using the method below, we evaluated the model on the test split using both
    aggregated metrics and slices over all the unique combinations of areas and consumer
    types.
  prefs: []
  type: TYPE_NORMAL
- en: By evaluating the model on slices, you can quickly investigate for fairness
    and bias.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, most of the heavy lifting, such as the implementation of MAPE
    and RMSPE, is directly accessible from **Sktime**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Render the results**'
  prefs: []
  type: TYPE_NORMAL
- en: Using **Sktime,** you can quickly render various time series into a single plot.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the video above, we rendered the results for every (area, consumer_type)
    combination in the W&B experiment tracker.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c490f268faaed419d02eeec4355a86f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Visually comparing the prediction and real observations for area = 2 and consumer
    type = 119 [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d681a7c647abe694bb25917c6cb84b61.png)'
  prefs: []
  type: TYPE_IMG
- en: Visually observing forecasted values into the future for area = 2 and consumer
    type = 119 [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: '**Upload the model to the model registry**'
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to upload the model to a model registry. After the model is
    uploaded, it will be downloaded and used by our batch prediction pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: During the experiment, we already uploaded the model as a W&B Artifact. If you
    plan to have dependencies with W&B in your applications, using it directly from
    there is perfectly fine.
  prefs: []
  type: TYPE_NORMAL
- en: But we wanted to keep the batch prediction pipeline dependent only on Hopsworks.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we used Hopswork's model registry feature.
  prefs: []
  type: TYPE_NORMAL
- en: In the following code, based on the given **best_model_artifact,** we added
    a tag to the Hopsworks feature view to link the two. This is helpful for debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we downloaded the best model weights and loaded them to the Hopsworks
    model registry using the **mr.python.create_model()** method.
  prefs: []
  type: TYPE_NORMAL
- en: Now with a few lines of code, you can download and run inference on your model
    without carrying any more about all the complicated steps we showed you in this
    lesson.
  prefs: []
  type: TYPE_NORMAL
- en: '[Check out Lesson 3](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)
    to see how we will build a batch prediction pipeline using the model from the
    Hopsworks model registry.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You finished the **second lesson** from the **Full Stack 7-Steps
    MLOps Framework** course.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have reached this far, you know how to:'
  prefs: []
  type: TYPE_NORMAL
- en: use an ML platform for experiment & metadata tracking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use an ML platform for hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: read data from the feature store based on a given version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build an encapsulated ML model and pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: upload your model to a model registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you understand the power of using an ML platform, you can finally take
    control over your experiments and quickly export your model as an artifact to
    be easily used in your inference pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '[Check out Lesson 3](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)
    to learn about implementing a batch prediction pipeline and packaging your Python
    modules using Poetry.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Also,** [**you can access the GitHub repository here**](https://github.com/iusztinpaul/energy-forecasting)**.**'
  prefs: []
  type: TYPE_NORMAL
- en: üí° My goal is to help machine learning engineers level up in designing and productionizing
    ML systems. Follow me on [LinkedIn](https://www.linkedin.com/in/pauliusztin/)
    or subscribe to my [weekly newsletter](https://pauliusztin.substack.com/) for
    more insights!
  prefs: []
  type: TYPE_NORMAL
- en: üî• If you enjoy reading articles like this and wish to support my writing, consider
    [becoming a Medium member](https://pauliusztin.medium.com/membership). By using
    [my referral link](https://pauliusztin.medium.com/membership), you can support
    me without any extra cost while enjoying limitless access to Medium‚Äôs rich collection
    of stories.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pauliusztin.medium.com/membership?source=post_page-----6fdaef594cee--------------------------------)
    [## Join Medium with my referral link - Paul Iusztin'
  prefs: []
  type: TYPE_NORMAL
- en: ü§ñ Join to get exclusive content about designing and building production-ready
    ML systems üöÄ Unlock full access to‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----6fdaef594cee--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [Energy Consumption per DE35 Industry Code from Denmark API](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour),
    [Denmark Energy Data Service](https://www.energidataservice.dk/about/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [WindowSummarizer Documentation](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.transformations.series.summarize.WindowSummarizer.html?highlight=windowsummarizer),
    Sktime Documentation'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [Sktime Documentation](https://www.sktime.net/en/latest/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [LightGBM Documentation](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [W&B Sweeps Documentation](https://docs.wandb.ai/guides/sweeps), W&B Documentation'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] [Sktime Forecasting Tutorial](https://www.sktime.net/en/latest/examples/01_forecasting.html#1.-Basic-forecasting-workflows),
    Sktime Documentation'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] [Sktime Hierarchical, Global, and Panel Forecasting Tutorial](https://github.com/sktime/sktime/blob/main/examples/01c_forecasting_hierarchical_global.ipynb),
    Sktime Documentation'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] [Sktime Window Splitters Tutorial](https://github.com/sktime/sktime/blob/main/examples/window_splitters.ipynb),
    Sktime Documentation'
  prefs: []
  type: TYPE_NORMAL
