- en: Activation Functions For Neural Networks & Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/activation-functions-non-linearity-neural-networks-101-ab0036a2e701](https://towardsdatascience.com/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explaining why neural networks can learn (nearly) anything and everything
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)
    ·8 min read·Oct 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f20f48d260492d8ecb7fb2bf39ec6862.png)'
  prefs: []
  type: TYPE_IMG
- en: Machine learning icons created by Becris — Flatico. [https://www.flaticon.com/free-icons/machine-learning](https://www.flaticon.com/free-icons/machine-learning)
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In my previous article, we introduced the [***multi-layer perceptron***](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    ***(MLP)****,* which is just a set of stacked interconnected [***perceptrons***](https://en.wikipedia.org/wiki/Perceptron).
    I highly recommend you check my previous post if you are unfamiliar with the perceptron
    and MLP as will discuss it quite a bit in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ab0036a2e701--------------------------------)
    [## Intro, Perceptron & Architecture: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Neural Networks and their building blocks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ab0036a2e701--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'An example MLP with two hidden layers is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94cdac9897a5e852d9cd6edd425fcba0.png)'
  prefs: []
  type: TYPE_IMG
- en: A basic two-hidden multi-layer perceptron. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the problem with the MLP is that it can only fit a [***linear classifier***](https://en.wikipedia.org/wiki/Linear_separability).
    This is because the individual perceptrons have a [***step function***](https://en.wikipedia.org/wiki/Step_function)
    as their [***activation function***](https://en.wikipedia.org/wiki/Activation_function),
    which is linear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa3d36442d4a95bbdd62ecbac912ad3a.png)'
  prefs: []
  type: TYPE_IMG
- en: The Perceptron, which is the simplest neural network. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: So despite stacking our perceptrons may look like a modern-day neural network,
    it is still a linear classifier and not that much different from regular linear
    regression!
  prefs: []
  type: TYPE_NORMAL
- en: Another problem is that it is not fully differentiable over the whole domain
    range.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, what do we do about it?
  prefs: []
  type: TYPE_NORMAL
- en: Non-Linear Activation Functions!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why Do We Need Non-Linearity?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is Linearity?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s quickly state what linearity means to build some context. Mathematically,
    a function is considered linear if it satisfies the following condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ff20f8ab0a88e38e57c203934156124.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also another condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7924532d6bff8fdc08fb33c5d3f79615.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: But, we will work with the previously equation for this demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take this very simple case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27e61559c5cf914c7b7fe6a28aeac415.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: So the function ***f(x) = 10x*** is linear!
  prefs: []
  type: TYPE_NORMAL
- en: If we added a bias term to the above equation it’s no longer be a linear function
    but rather an [**affine function**](https://en.wikipedia.org/wiki/Affine_transformation).
    See [this statexchange thread](https://math.stackexchange.com/questions/275310/what-is-the-difference-between-linear-and-affine-function)
    for the difference.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Now, consider this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/856701d94e726496754d19133fe10b32.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'So the function ***f(x) = 10x²*** is *not* linear. As most of us will know,
    this function is quadratic and has a parabola shape:'
  prefs: []
  type: TYPE_NORMAL
- en: Gist by author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9100d65fd7dc2194a0e23a1f4a431761.png)'
  prefs: []
  type: TYPE_IMG
- en: Example parabola. Plot generated by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Definitely not linear!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Linearity in Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already answered why we need non-linearity in neural networks, but let’s
    really dive in to this concept and what it means to our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of each perceptron can be mathematically written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60e9f3aa07b7135e6c0eadb2b8a184ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Where ***f*** is the step function (activation function), ***y*** is the output
    of the perceptron, ***b*** is the bias term, ***n*** is the number of features,
    and ***w_i*** and ***x_i*** are the [***weights***](https://en.wikipedia.org/wiki/Weighting)
    and their corresponding input values. This equation is just a mathematical version
    of the perceptron diagram above.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s imagine our activation function is the identity function, in other
    words, we have no function. Using this concept, now consider a feed-forward two-hidden
    layer MLP with two neurons in the middle layer (ignoring the bias terms):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e743bf6079fe4879b43c69e4dc07d27.png)'
  prefs: []
  type: TYPE_IMG
- en: Two layer feed-forward neural network.Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering what this means.
  prefs: []
  type: TYPE_NORMAL
- en: Well, what we have done here is condense the 2-layer MLP into a single-layer
    MLP! The final equation, in the above derivation, is simply a linear regression
    model with features ***x_1*** and ***x_2*** and their corresponding coefficients***.***
  prefs: []
  type: TYPE_NORMAL
- en: So despite the MLP being ‘two layers’ it would to a single layer and become
    the good old linear regression model! This is not good as the neural network won’t
    be able to model or fit complex functions to the data
  prefs: []
  type: TYPE_NORMAL
- en: By allowing the MLP to have non-linear activation functions we satisfy something
    called the [***Universal Approximation Theorem***](https://en.wikipedia.org/wiki/Universal_approximation_theorem).
    This theorem basically says that an MLP, or more correctly a neural network, can
    approximate and fit any function. Check out this post [here](http://mcneela.github.io/machine_learning/2017/03/21/Universal-Approximation-Theorem.html)
    if you want to learn more!
  prefs: []
  type: TYPE_NORMAL
- en: Being linear is not necessarily a bad thing, it’s more that many real-world
    problems are non-linear, so we need non-linear models if we want to predict these
    phenomena.
  prefs: []
  type: TYPE_NORMAL
- en: Another requirement for the activation function is that needs to be differentiable
    to enable [**gradient descent**](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now quickly run through some of the most common non-linear activation
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [***sigmoid function***](https://en.wikipedia.org/wiki/Sigmoid_function)is
    well known in the industry and derives from the [***binomial distribution***](https://medium.com/towards-artificial-intelligence/decoding-the-binomial-distribution-a-fundamental-concept-for-data-scientists-81c64c7e4580).
    It ‘squashes’ the inputs to an output between 0 and 1, so it can be viewed as
    a probability and has an ‘S-shape.’ However, most importantly it’s non-linear
    as it contains a division and an exponential function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sigmoid function mathematically and visually looks like as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29fff662bec33adab7258d011d2e9db6.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Gist by author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b71b01559c120a8967c3bf80bc04c16f.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid function. Plot generated by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the sigmoid is not used in cutting-edge neural networks for the following
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Vanishing Gradient Problem**](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)
    **—** From the above plot, we can see the gradients at the extreme positive or
    negative values are near zero. This is a problem when training neural networks
    as it slows down learning and convergence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not Centred On Zero** — The sigmoid is between 0 and 1, so it is always positive.
    This is a problem when using gradient descent-based algorithms, as it will push
    or descend in one direction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computationally Expensive** — Computing exponentials are not efficient, and
    so this function is expensive especially in large neural nets when it is computed
    thousands of times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanh
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[***Tanh***](https://en.wikipedia.org/wiki/Hyperbolic_functions) is a common
    trigonometric hyperbolic function that maps inputs to be between -1 and 1\. Therefore,
    it is zero-centred, so is an improvement upon the sigmoid function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The tanh function mathematically and visually looks like as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc548876673a33f9076e8494e4315631.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Gist by author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b43c2e3f628349da147ec521cb2ded5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Tanh function. Plot generated by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite being zero-centred, the tanh function suffers from similar issues as
    the sigmoid function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vanishing Gradient Problem —** Tanh’s gradients are also near zero at the
    extremes of positives and negatives. This is a problem when training neural networks
    as it slows down learning and convergence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computationally Expensive** — Tanh has more exponentials to calculate, which
    makes it even less computationally efficient than the sigmoid.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [***rectified linear unit (ReLU)***](https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29)
    is the most popular activation function as it is computationally efficient and
    removes the issues with the vanishing gradient problem mentioned above.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically and visually it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad1cf21ebe2a1666034bee8102b78d29.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Gist by author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf6409fc078754bcab71802d9bbe477e.png)'
  prefs: []
  type: TYPE_IMG
- en: ReLU. Plot generated by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though the ReLU activation function has many advantages over the sigmoid
    and tanh functions, it suffers from some new issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Dead Neurons**](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks)
    **—** There is a “dying ReLU problem” where the neuron always outputs zero for
    any input due to this zero line for any value less than zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unbounded Value —** The gradient can go to positive infinity. This can cause
    compute problems in extreme cases with such large numbers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not Centred On Zero** — The ReLU is also not symmetrical around zero and
    its mean is positive. This can lead to issues during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaky ReLU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can adjust the ReLU function to produce the ‘Leaky’ ReLU, where the negative
    input is not zero, but rather has some shallow slope with gradient ***α***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5dbcbc68a24c8ab953e5b19704fabfa.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Gist by author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d44ec1c36473a7f01361080256878af.png)'
  prefs: []
  type: TYPE_IMG
- en: Leaky ReLU. Plot generated by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Leaky ReLU avoids the dead neuron problem by having a little gradient,
    so the neurons can keep learning. Not to mention avoiding the vanishing gradients
    problem and being computationally efficient. However, like the other functions,
    it also has some flaws:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Slope Choice**: The gradient of the leaky slope is not prescribed and needs
    to be chosen manually. It can always be hyperparameter tuned though.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not Centred On Zero** — Like normal ReLU it’s also not centred about zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unbounded Value —** Like ReLU, its gradient can go to positive infinity and
    is effectively unbounded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Others
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here I have listed the four most common activation functions, however there
    are several others such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Swish**](https://medium.com/@neuralnets/swish-activation-function-by-google-53e1ea86f820)**:**
    This is a differentiable approximation of the ReLU function developed by Google,
    ***swish(x) = x * sigmoid(x)***.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gated Linear Unit (GLU)**: This is used primarily in [**recurrent neural
    networks**](https://en.wikipedia.org/wiki/Recurrent_neural_network).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Softmax**](/softmax-activation-function-how-it-actually-works-d292d335bd78#:~:text=Softmax%20is%20an%20activation%20function,all%20possible%20outcomes%20or%20classes.):
    Primarily used for multi-nomial classification problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One activation is not better than the other, its always best to try out various
    ones and see which one best works for your model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary & Further Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make a neural network satisfy the universal approximation theorem, it needs
    to contain a non-linear activation function, otherwise you can squash into a single
    layer, basically a linear regression model. There are many suitable activation
    function, the one that is most frequently used is the ReLU. However, this is not
    a hard and fast rule, and you should experiment with various functions to find
    the best one for your model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code used in this article is available at my GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----ab0036a2e701--------------------------------)
    [## Medium-Articles/Data Science Basics/activation_functions.py at main · egorhowell/Medium-Articles'
  prefs: []
  type: TYPE_NORMAL
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----ab0036a2e701--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References & Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Great article on activation functions*](/activation-functions-neural-networks-1cbd9f8d91d6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Non-linear neural networks*](https://medium.com/ml-cheat-sheet/understanding-non-linear-activation-functions-in-neural-networks-152f5e101eeb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Intro to neural networks*](https://web.pdx.edu/~nauna/week7b-neuralnetwork.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another Thing!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----ab0036a2e701--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----ab0036a2e701--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Connect With Me!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**GitHub**](https://github.com/egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
