- en: Ludwig — A “Friendlier” Deep Learning Framework
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ludwig — 一个“更友好”的深度学习框架
- en: 原文：[https://towardsdatascience.com/ludwig-a-friendlier-deep-learning-framework-946ee3d3b24](https://towardsdatascience.com/ludwig-a-friendlier-deep-learning-framework-946ee3d3b24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ludwig-a-friendlier-deep-learning-framework-946ee3d3b24](https://towardsdatascience.com/ludwig-a-friendlier-deep-learning-framework-946ee3d3b24)
- en: Deep Learning Made Easy with this Low-Code, Declarative Framework
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用这个低代码、声明式框架让深度学习变得简单
- en: '[](https://johnadeojo.medium.com/?source=post_page-----946ee3d3b24--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----946ee3d3b24--------------------------------)[](https://towardsdatascience.com/?source=post_page-----946ee3d3b24--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----946ee3d3b24--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----946ee3d3b24--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://johnadeojo.medium.com/?source=post_page-----946ee3d3b24--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----946ee3d3b24--------------------------------)[](https://towardsdatascience.com/?source=post_page-----946ee3d3b24--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----946ee3d3b24--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----946ee3d3b24--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----946ee3d3b24--------------------------------)
    ·11 min read·Jun 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----946ee3d3b24--------------------------------)
    ·阅读时间11分钟·2023年6月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ee8f8ddbdb0f202acd51b5a02d1e371e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee8f8ddbdb0f202acd51b5a02d1e371e.png)'
- en: 'Image by Author: Generated with Midjourney'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者：使用 Midjourney 生成
- en: Background — Deep Learning, too Complex?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景 — 深度学习，是否过于复杂？
- en: I have tended to shy away from deep learning for industry use cases. Not due
    to a lack of interest, rather I find the popular deep learning frameworks cumbersome.
    I appreciate [Pytorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/)
    are fantastic tools for research purposes, but the APIs are not the most user
    friendly. In situations where I need to deliver a quick proof of concept for a
    client, the last thing I want to be doing is tinkering with Pytorch tensors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直倾向于避免将深度学习应用于行业用例。这并不是因为缺乏兴趣，而是因为我觉得流行的深度学习框架很繁琐。我欣赏 [Pytorch](https://pytorch.org/)
    和 [TensorFlow](https://www.tensorflow.org/) 是用于研究目的的绝佳工具，但它们的 API 并不是最用户友好的。在需要为客户快速交付概念验证的情况下，我最不希望做的就是捣鼓
    Pytorch 张量。
- en: While attending the AI summit in London, I stumbled across a team claiming to
    have a solution to my deep learning problem. They have been leveraging a different
    approach, which they described as “the mid-point between TensorFlow and AutoML”,
    a framework called Ludwig.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在伦敦参加 AI 峰会时，我偶然发现一个团队声称他们有解决我的深度学习问题的方案。他们采用了一种不同的方法，他们将其描述为“介于 TensorFlow
    和 AutoML 之间的中点”，这是一个名为 Ludwig 的框架。
- en: What is Ludwig?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 Ludwig？
- en: Developed by Uber, [Ludwig](https://ludwig.ai/latest/) is an open-source framework
    for building deep learning models. It is declarative, which means that instead
    of constructing complex models layer-by-layer like you would in TensorFlow, you
    simply declare the structure of the model with a configuration file. This sounded
    too good to be true, so I had to see for myself. In the remainder of this article,
    I detail my experience with Ludwig through an example project I have taken from
    [Kaggle](https://www.kaggle.com/datasets/shelvigarg/sales-forecasting-womart-store).
    Along the way I will discuss some of its strengths, pain-points, and give you
    my conclusion on whether it’s worth using.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Ludwig 是由 Uber 开发的，[Ludwig](https://ludwig.ai/latest/) 是一个用于构建深度学习模型的开源框架。它是声明式的，这意味着你无需像在
    TensorFlow 中那样一层层构建复杂的模型，而是只需通过配置文件声明模型的结构。这听起来好得令人难以置信，所以我决定亲自体验一下。在这篇文章的其余部分，我将通过一个我从
    [Kaggle](https://www.kaggle.com/datasets/shelvigarg/sales-forecasting-womart-store)
    上获得的示例项目详细描述我对 Ludwig 的体验。在此过程中，我将讨论它的一些优点、痛点，并给出是否值得使用的结论。
- en: '*Note — Although initially developed by Uber, Ludwig is an* [*open source*](https://github.com/ludwig-ai/ludwig/blob/master/LICENSE)
    *library under the Apache 2.0 License. The project is hosted by the* [*Linux Foundation
    AI & Data*](https://lfaidata.foundation/)*. I have no commercial affiliation with
    Uber or the developers of Ludwig.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意 — 尽管最初由Uber开发，Ludwig是一个* [*开源*](https://github.com/ludwig-ai/ludwig/blob/master/LICENSE)
    *库，采用Apache 2.0许可证。该项目由* [*Linux Foundation AI & Data*](https://lfaidata.foundation/)*主办。我与Uber或Ludwig的开发者没有商业关系。*'
- en: The project — Demand Forecasting
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目 — 需求预测
- en: '**The project brief**: Forecast the final 30 days of orders across each store
    for the retailer WOMart.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**项目简介**：预测零售商WOMart各个门店的最后30天订单。'
- en: Your Client WOMart is a leading nutrition and supplement retail chain that offers
    a comprehensive range of products for all your wellness and fitness needs.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你的客户WOMart是领先的营养和补充品零售连锁店，提供全面的产品以满足你的健康和健身需求。
- en: ''
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: WOMart follows a multi-channel distribution strategy with 350+ retail stores
    spread across 100+ cities.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: WOMart遵循多渠道分销战略，在100多个城市拥有350多个零售店。
- en: The Data
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: The dataset had a total of 22,265 observations, each pertaining to one day of
    sales at a given store. For brevity I won’t go into the full details of the data
    set, but you can see some descriptive statistics [here](https://www.kaggle.com/datasets/shelvigarg/sales-forecasting-womart-store).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集共有22,265个观察值，每个观察值对应于一个特定门店的一天销售数据。为了简洁起见，我不会详细介绍数据集的所有细节，但你可以在[这里](https://www.kaggle.com/datasets/shelvigarg/sales-forecasting-womart-store)查看一些描述性统计数据。
- en: '**Note**: The data is free to use for any purposes under the Open Data Commons
    license.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：数据在Open Data Commons许可证下可以用于任何目的。'
- en: Link to the [data](https://www.kaggle.com/datasets/shelvigarg/sales-forecasting-womart-store)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的[链接](https://www.kaggle.com/datasets/shelvigarg/sales-forecasting-womart-store)
- en: '[License](https://opendatacommons.org/licenses/dbcl/1-0/) for the data'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据许可证](https://opendatacommons.org/licenses/dbcl/1-0/)'
- en: '**The data dictionary:**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据字典：**'
- en: A Brief Overview of the Methodology
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法论概述
- en: I won’t go into too much detail on the methodology here as this isn’t the main
    purpose of this article. I’ll cover how I framed the problem at a high level to
    give you some context.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会在这里详细介绍方法论，因为这不是本文的主要目的。我将高层次地介绍我如何框定问题，以便为你提供一些背景。
- en: I framed the forecasting issue as a ‘pseudo’ sequence-to-sequence deep learning
    problem. The method entailed utilising 360 days of time series data points to
    predict the subsequent 30 days of customer orders. I incorporated some categorical
    variables and had to generate separate predictions for each day of orders, which
    led to a somewhat unconventional setup — thus the term ‘pseudo’ sequence-to-sequence
    for describing the problem. I will expand on the specifics of the feature engineering
    later in this article.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我将预测问题框定为一个“伪”序列到序列深度学习问题。这种方法涉及利用360天的时间序列数据点来预测接下来的30天的客户订单。我引入了一些分类变量，并且需要为每一天的订单生成单独的预测，这导致了一个略显非传统的设置——因此使用了“伪”序列到序列来描述这个问题。我将在本文后面详细讨论特征工程的具体细节。
- en: Besides this, the methodology I followed was standard for model development.
    I split my data into a training data set and a holdout dataset and did some re-scaling
    of features and labels. Model training was done on the training data, and testing
    on the holdout.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我遵循的方法论对于模型开发来说是标准的。我将数据分为训练数据集和保留数据集，并对特征和标签进行了重新缩放。模型训练在训练数据上进行，测试在保留数据上进行。
- en: '*Note: Ludwig does provide the capability to natively split data within the
    API. However, to maintain rigour, I established a separate holdout dataset. The
    training dataset was subsequently divided into additional training, validation,
    and testing subsets. The holdout dataset was excluded entirely, used only for
    analysing the predictions generated by the model.*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：Ludwig确实提供了在API中本地拆分数据的功能。然而，为了保持严谨性，我建立了一个单独的保留数据集。训练数据集随后被进一步划分为训练、验证和测试子集。保留数据集完全被排除，仅用于分析模型生成的预测。*'
- en: Feature Engineering
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: At the time of writing this article, I would say that sequence-to-sequence modelling
    as it pertains to timeseries forecasting is tricky to do in Ludwig. The reason
    for this is the feature engineering. The Ludwig API has an excellent ability to
    handle sequences as inputs, but they haven’t (yet) developed a coherent approach
    for time series sequences as outputs. You can develop a “pseudo” sequence-to-sequence
    model by declaring multiple outputs, but the overall feature engineering experience
    feels quite “hacky”.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，我认为在Ludwig中进行时间序列预测的序列到序列建模是很棘手的。这是因为特征工程。Ludwig API 在处理序列作为输入方面表现出色，但它们尚未（还未）开发出对时间序列作为输出的连贯方法。你可以通过声明多个输出来开发一个“伪”序列到序列模型，但整体特征工程体验感觉相当“黑客”。
- en: '**Sequence features**: Except for those features that don’t change with time,
    I engineered all the predictive features to be “Ludwig-formatted” sequences. Each
    input sequence is a horizontal stacking of each “time series” feature across a
    pre-defined timeframe. Each feature sequence is determined at the store level
    and encapsulated in one cell of the dataframe (it looks as messy as it sounds).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**序列特征**：除了那些随时间变化的特征外，我将所有预测特征工程为“Ludwig格式”的序列。每个输入序列是每个“时间序列”特征在预定义时间范围内的水平堆叠。每个特征序列在商店级别确定，并封装在数据框的一个单元格中（看起来就像听起来那么乱）。'
- en: '**Sequence labels**: For sequence labels, you must declare each point in the
    sequence as an individual label for the model. This ended up with me declaring
    30 labels for each store, one label per day of orders to be predicted.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**序列标签**：对于序列标签，你必须将序列中的每一点声明为模型的单独标签。结果是我为每个商店声明了30个标签，每天一个标签，用于预测订单。'
- en: '*Example of the feature engineering process below:*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*下面是特征工程过程的示例：*'
- en: 'Example of Data: Bold values will be used to construct sequence labels, regular
    values will be used to construct sequence features.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数据示例：粗体值将用于构造序列标签，常规值将用于构造序列特征。
- en: 'Examples of engineering features: Order_sequence is a “Ludwig-formatted” sequence.
    Labels are returned individually to be later declared as model outputs (labels).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程示例：Order_sequence 是一个“Ludwig格式”的序列。标签会被单独返回，以便后续声明为模型输出（标签）。
- en: Architecting Your Model
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计你的模型
- en: The Ludwig API allows you to architect fairly complex and customisable models
    declaratively. Ludwig does this through a .yaml file. Now, I appreciate many data
    scientists reading this might not have used *.yaml* files, but generally in software
    development these are used for configuration. The files might appear scary at
    first glance, but they are quite friendly. Let’s step through the main parts of
    the file I create to build the model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Ludwig API 允许你通过声明方式构建相当复杂和可定制的模型。Ludwig 通过 .yaml 文件来实现这一点。现在，我知道许多数据科学家可能没有使用过*.yaml*
    文件，但在软件开发中，这些文件通常用于配置。文件乍一看可能显得吓人，但实际上非常友好。让我们逐步了解一下我创建模型时使用的文件的主要部分。
- en: '![](../Images/4fb639a21a0f7b60a4ca5ce8cd586ed4.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fb639a21a0f7b60a4ca5ce8cd586ed4.png)'
- en: 'Image by Author: Model architecture'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：模型架构
- en: 'Before we delve into the configurations, it’s worth briefly introducing the
    [architecture](https://ludwig.ai/latest/user_guide/how_ludwig_works/) at the heart
    of Ludwig’s deep learning framework: the Encoder, Combiner, and Decoder. Most
    of the models you configure in Ludwig will predominantly adhere to this architecture.
    Understanding this can simplify the process of stacking components to quickly
    build your deep learning models.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入配置之前，值得简要介绍一下Ludwig深度学习框架的核心架构：[架构](https://ludwig.ai/latest/user_guide/how_ludwig_works/)：编码器、组合器和解码器。你在Ludwig中配置的大多数模型将主要遵循这一架构。理解这一点可以简化堆叠组件的过程，从而快速构建你的深度学习模型。
- en: Declaring your Model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 声明你的模型
- en: Right at the top of the file you declare the model type used. Ludwig provides
    two options, tree-based models, and deep neural networks for which I chose the
    latter.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件的最上方，你声明所使用的模型类型。Ludwig 提供了两种选择：基于树的模型和深度神经网络，我选择了后者。
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Declaring your data splits
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 声明数据拆分
- en: You can split data sets natively by declaring your split percentages, type of
    split, and column or variable you’re splitting on. For my purposes I wanted to
    ensure that a store could only appear in one of the data sets, hash splitting
    was perfect for that.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过声明拆分百分比、拆分类型以及你要拆分的列或变量来本地拆分数据集。出于我的目的，我希望确保一个商店只能出现在一个数据集中，哈希拆分正好适合这一点。
- en: For best practice, I would probably advise constructing a holdout set outside
    of the Ludwig API especially where you are doing some initial feature engineering
    like one-hot-encoding or normalisation. This should help prevent data leakage.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是，我建议在 Ludwig API 之外构建一个保留集，尤其是在进行初步特征工程（如独热编码或归一化）时。这有助于防止数据泄漏。
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Declaring the Model Inputs
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 声明模型输入
- en: You declare inputs by name, type, and encoder. Depending on the type of input
    to the model you have a variety of options for encoders. Essentially encoders
    are a way of transforming your inputs such that it can be interpreted by the model.
    The choice of encoder really depends on the data and the modelling task.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过名称、类型和编码器来声明输入。根据模型输入的类型，你有多种编码器选项。编码器本质上是一种将输入转换为模型可以解读的方式。编码器的选择实际上取决于数据和建模任务。
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Declaring the Combiner
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 声明组合器
- en: '[Combiners](https://ludwig.ai/latest/configuration/combiner/), as the name
    suggests, amalgamate the outputs of your encoders. The Ludwig API offers an array
    of different combiners, each with its own specific use case. The choice of combiner
    can depend on the structure of your model and the relationships between your features.
    For instance, you might use a ‘concat’ combiner if you want to simply concatenate
    the outputs of your encoders, or a ‘sequence’ combiner if your features have a
    sequential relationship.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[组合器](https://ludwig.ai/latest/configuration/combiner/)，顾名思义，用于合并你的编码器的输出。Ludwig
    API 提供了多种不同的组合器，每种都有其特定的使用场景。组合器的选择可能取决于模型的结构和特征之间的关系。例如，如果你想简单地将编码器的输出进行连接，可以使用“concat”组合器；如果你的特征有顺序关系，可以使用“sequence”组合器。'
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As with many aspects of deep learning, the optimal choice of combiner often
    depends on the specifics of your dataset and problem, and may require some experimentation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与深度学习的许多方面一样，最佳的组合器选择通常取决于你的数据集和问题的具体情况，并可能需要一些实验。
- en: Declaring the Model Outputs
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 声明模型输出
- en: Finalising your network is as simple as declaring your outputs, which are just
    your labels. My pet peeve with Ludwig for timeseries is that you can’t (yet) declare
    timeseries outputs. As I mentioned previously, you have to “hack” it by declaring
    each point in your time series separately. This left me with thirty separate declarations,
    which looks very messy in all honesty. For each output you can specify the loss
    function too adding additional configurability. Ludwig has a myriad of options
    pre-built for different output types, however I don’t know if you are able to
    implement custom loss functions as you can with Pytorch.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 完成你的网络就像声明输出一样简单，输出就是你的标签。我对 Ludwig 的时间序列处理的一个小抱怨是，当前你无法（还）声明时间序列输出。正如我之前提到的，你必须通过单独声明时间序列中的每个点来“破解”它。这让我有了三十个单独的声明，说实话看起来非常杂乱。对于每个输出，你还可以指定损失函数，增加额外的可配置性。Ludwig
    为不同的输出类型预设了大量选项，但我不确定你是否能够像在 Pytorch 中那样实现自定义损失函数。
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Declaring the Trainer
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 声明训练器
- en: The trainer configuration in Ludwig, while optional due to Ludwig’s provision
    of sensible defaults, allows for a high degree of customisation. It gives you
    control over the specifics of how your model is trained. This includes the ability
    to specify the type of optimiser used, the number of training epochs, the learning
    rate, and criteria for early stopping, among other parameters.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Ludwig 的训练器配置虽然是可选的（因为 Ludwig 提供了合理的默认设置），但允许高度的自定义。这让你能够控制模型训练的具体细节。这包括指定所用优化器的类型、训练轮数、学习率以及早停标准等参数。
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For your particular use case, you might find it beneficial to define these parameters
    yourself. For instance, you might want to adjust the learning rate, or the number
    of epochs based on the complexity of your model and the size of your dataset.
    Similarly, early stopping can be a useful tool to prevent overfitting by halting
    the training process if the model’s performance on a validation set stops improving.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你的特定用例，你可能会发现自己定义这些参数会更有益。例如，你可能希望根据模型的复杂性和数据集的大小调整学习率或训练轮数。同样，早停可以成为防止过拟合的有用工具，通过在模型在验证集上的表现不再改善时停止训练过程。
- en: Train your Model
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练你的模型
- en: 'Training your model can be easily done with Ludwig’s python expermiment API.
    See the script example below:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 训练你的模型可以通过 Ludwig 的 Python 实验 API 轻松完成。请参见下面的脚本示例：
- en: Other Configurations
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他配置
- en: Outside of those I mentioned, Ludwig has a myriad of possible configurations.
    They are all very well documented and well structured. I would advise having a
    read of their [documentation](https://ludwig.ai/latest/configuration/) to familiarise
    yourself.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我提到的，Ludwig 还有大量可能的配置。它们都记录得很好且结构清晰。我建议阅读他们的 [文档](https://ludwig.ai/latest/configuration/)
    来熟悉它们。
- en: Model Performance Analysis — A brief view
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能分析——简要视图
- en: The aim of this article is to explore some of the functionalities of the Ludwig
    framework through a practical example project. Although demonstrating model performance
    is part of this, there’s no need to delve into the specifics of the metrics. I
    will limit the discussion to presenting some of the charts generated from the
    model’s analysis. Note that the comprehensive end-to-end script is available on
    my GitHub, linked at the conclusion of the article.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在通过一个实际的示例项目来探讨 Ludwig 框架的一些功能。虽然展示模型性能是其中的一部分，但无需深入探讨指标的细节。我将讨论限制在展示一些从模型分析中生成的图表。请注意，全面的端到端脚本在我的
    GitHub 上可以找到，链接在文章结尾。
- en: '![](../Images/8d558538c1cc0f0f325945847d204843.png)![](../Images/0b4b25d91efa44007d98674b289715ef.png)![](../Images/8d558538c1cc0f0f325945847d204843.png)![](../Images/307430a711653cafa78cdadb6cda1fcd.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d558538c1cc0f0f325945847d204843.png)![](../Images/0b4b25d91efa44007d98674b289715ef.png)![](../Images/8d558538c1cc0f0f325945847d204843.png)![](../Images/307430a711653cafa78cdadb6cda1fcd.png)'
- en: 'Image by Author: Model predictions (red) against actual orders (blue) for the
    last 30 days of data. These examples are drawn from the holdout set.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：过去30天的数据中，模型预测（红色）与实际订单（蓝色）的对比。这些例子来自保留集。
- en: '![](../Images/dcdb1587d4122569b075e2b12a921617.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcdb1587d4122569b075e2b12a921617.png)'
- en: 'Image by Author: Distribution of error, where error is Actual Orders minus
    Predicted Orders.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：误差分布，其中误差为实际订单减去预测订单。
- en: '![](../Images/1b73422bb47ff310479e5311faffa274.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b73422bb47ff310479e5311faffa274.png)'
- en: 'Image by Author: Loss curve for the model training. Loss metric is Mean Absolute
    Error'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：模型训练的损失曲线。损失指标为均方绝对误差
- en: My Verdict
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我的判决
- en: I’ll begin by confessing that I was initially sceptical about Ludwig. However,
    after experimenting with it myself, I’m convinced of its capabilities and believe
    it delivers as promised. I think there are several truly impressive features worth
    highlighting.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我将首先承认，最初我对 Ludwig 持怀疑态度。然而，在自己实验之后，我相信它的能力，并认为它如承诺般有效。我认为有几个真正令人印象深刻的功能值得突出。
- en: '**Coding experience**: The coding experience feels more like constructing an
    elaborate Lego set. You really can have a lot of fun messing around with components
    and different architectures to find your perfect model.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码体验**：编码体验更像是在构建一个精致的乐高模型。你可以通过玩弄组件和不同的架构来找到你的完美模型，确实非常有趣。'
- en: '**Documentation**: The documentation is clear and well structured. It was very
    easy to figure out how to implement different architectures and change your model
    configurations. Most of the documentation appears to be up to date too, which
    is a bonus.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**文档**：文档清晰且结构良好。很容易搞清楚如何实现不同的架构和更改模型配置。大部分文档似乎也很及时更新，这是一大优势。'
- en: '**Backend**: The backend experience is excellent. The library developers have
    done a great job abstracting away a lot of the usual configurations you have to
    do to train deep neural networks on GPUs. I trained my model on a Google Collab,
    and Ludwig automatically moved the workloads onto the GPU.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**后端**：后端体验非常出色。库的开发者在抽象掉训练深度神经网络所需的大部分常规配置方面做得很好。我在 Google Collab 上训练了我的模型，Ludwig
    自动将工作负载转移到 GPU 上。'
- en: Another great thing about Ludwig is that the backend is highly configurable.
    For example, if you’re running massive workloads and need a cluster of GPUs, you
    can configure this too!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Ludwig 还有一个很棒的特点，就是后端高度可配置。例如，如果你在运行大规模工作负载并需要一个 GPU 集群，你也可以进行配置！
- en: '**Experiment Tracking**: Ludwig provides you with an Experiment API that can
    be used to track your model artefacts between experiment runs. I believe there
    are integrations with MLflow too which is fantastic for commercial scale MLOps.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验追踪**：Ludwig 提供了一个实验 API，可用于在实验运行之间跟踪模型工件。我相信它也与 MLflow 集成，这对于商业规模的 MLOps
    来说非常棒。'
- en: Pet Peeves
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 个人偏好
- en: There are a few areas in which the framework could be enhanced, let’s get into
    those too.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些领域可以进一步增强这个框架，让我们一起来探讨一下。
- en: '**Visualisation**: Ludwig does offer a visualisation API to track training
    losses across your data sets. However, at the time of writing, it does not function
    particularly well and its usage is unintuitive. I tried running it in a Google
    Collab, but to no avail. Eventually, I created my own loss curve visualisations
    by scripting a Python function to parse the training_statistics.json files that
    Ludwig saves for you after each experiment run.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**可视化**：Ludwig 确实提供了一个可视化 API 来跟踪数据集中的训练损失。然而，在撰写本文时，它的功能并不特别好，其使用也不够直观。我尝试在
    Google Collab 中运行，但没有成功。最终，我通过编写一个 Python 函数来解析 Ludwig 在每次实验运行后保存的 training_statistics.json
    文件，创建了自己的损失曲线可视化。'
- en: '**Support**: Whilst there is some support available, Ludwig’s community does
    not seem to be as extensive as those of TensorFlow or Pytorch yet. A few issues
    have been raised on GitHub, and there are some threads that might offer assistance,
    but for the most part, it feels like you are left to your own devices. As for
    ChatGPT, it provides a certain level of support up until 2021.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持**：虽然有一定的支持可用，但 Ludwig 的社区似乎还没有 TensorFlow 或 Pytorch 那么广泛。在 GitHub 上提出了一些问题，有些线程可能提供帮助，但大部分情况下，感觉你只能依靠自己。至于
    ChatGPT，它提供了截至 2021 年的一定程度的支持。'
- en: '**Transparency**: Ludwig excels in eliminating the more challenging aspects
    of building deep learning models. However, this comes at the expense of transparency,
    which occasionally renders the logs somewhat cryptic and challenging to debug.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**透明度**：Ludwig 在消除构建深度学习模型中更具挑战性的方面表现出色。然而，这也以牺牲透明度为代价，偶尔使日志显得有些难以理解和调试。'
- en: Conclusion
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In my opinion, Ludwig is an excellent tool for those looking to start utilising
    deep learning whether in a commercial setting or just for learning. While it may
    be too high-level for state-of-the-art research purposes, it is well-suited to
    solving clearly defined problems swiftly. A certain degree of understanding of
    deep learning is still necessary, but once you’ve grasped the concept, the barrier
    to entry for Ludwig is considerably lower than it is for TensorFlow or Pytorch.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，Ludwig 是一个出色的工具，适合那些希望开始使用深度学习的人，无论是在商业环境中还是仅仅为了学习。虽然它可能对前沿研究目的来说过于高层次，但它非常适合快速解决明确定义的问题。尽管仍然需要对深度学习有一定的理解，但一旦掌握了概念，Ludwig
    的入门门槛比 TensorFlow 或 Pytorch 低得多。
- en: The end-to-end notebook is available in my [GitHub](https://github.com/john-adeojo/womartdata/blob/main/ludwig_deep_learning_notebook%20(2).ipynb)
    repo, please feel free to experiment.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端的笔记本可以在我的 [GitHub](https://github.com/john-adeojo/womartdata/blob/main/ludwig_deep_learning_notebook%20(2).ipynb)
    仓库中找到，请随意进行实验。
- en: '*Follow me on* [*LinkedIn*](https://www.linkedin.com/in/john-adeojo/)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*关注我在* [*LinkedIn*](https://www.linkedin.com/in/john-adeojo/)'
- en: '*Subscribe to medium to get more insights from me:*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*订阅 Medium 以获取更多我的见解：*'
- en: '[](https://johnadeojo.medium.com/membership?source=post_page-----946ee3d3b24--------------------------------)
    [## Join Medium with my referral link — John Adeojo'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://johnadeojo.medium.com/membership?source=post_page-----946ee3d3b24--------------------------------)
    [## 通过我的推荐链接加入 Medium — John Adeojo'
- en: I share data science projects, experiences, and expertise to assist you on your
    journey You can sign up to medium via…
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我分享数据科学项目、经验和专业知识，以助你一臂之力。你可以通过 Medium 注册…
- en: johnadeojo.medium.com](https://johnadeojo.medium.com/membership?source=post_page-----946ee3d3b24--------------------------------)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: johnadeojo.medium.com](https://johnadeojo.medium.com/membership?source=post_page-----946ee3d3b24--------------------------------)
- en: '*Should you be interested in integrating AI or data science into your business
    operations, we invite you to schedule a complimentary initial consultation with
    us:*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你有兴趣将 AI 或数据科学集成到业务运营中，我们邀请你预约一次免费的初步咨询：*'
- en: '[](https://www.data-centric-solutions.com/book-online?source=post_page-----946ee3d3b24--------------------------------)
    [## Book Online | Data-Centric Solutions'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.data-centric-solutions.com/book-online?source=post_page-----946ee3d3b24--------------------------------)
    [## 在线预约 | 数据驱动解决方案'
- en: Discover our expertise in helping businesses achieve ambitious goals with a
    free consultation. Our data scientists and…
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过免费的咨询发现我们在帮助企业实现雄心勃勃目标方面的专业知识。我们的数据科学家和…
- en: www.data-centric-solutions.com](https://www.data-centric-solutions.com/book-online?source=post_page-----946ee3d3b24--------------------------------)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: www.data-centric-solutions.com](https://www.data-centric-solutions.com/book-online?source=post_page-----946ee3d3b24--------------------------------)
