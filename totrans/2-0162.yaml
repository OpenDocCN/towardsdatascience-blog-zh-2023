- en: 'A Birdâ€™s Eye View of Linear Algebra: Systems of Equations, Linear Regression
    and Neural Networks'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-systems-of-equations-linear-regression-and-neural-networks-fe5b88a57f66](https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-systems-of-equations-linear-regression-and-neural-networks-fe5b88a57f66)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The humble matrix multiplication along with its inverse is almost exclusively
    whatâ€™s going on in many simple ML models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rohitpandey576?source=post_page-----fe5b88a57f66--------------------------------)[![Rohit
    Pandey](../Images/af817d8f68f2984058f0afb8fd7ecbe9.png)](https://medium.com/@rohitpandey576?source=post_page-----fe5b88a57f66--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe5b88a57f66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe5b88a57f66--------------------------------)
    [Rohit Pandey](https://medium.com/@rohitpandey576?source=post_page-----fe5b88a57f66--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe5b88a57f66--------------------------------)
    Â·18 min readÂ·Dec 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c6e139e72e16ccb0e834afec721d153.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Image by midjourney
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the fourth chapter of the in-progress book on linear algebra, â€œA birds
    eye view of linear algebraâ€. The table of contents so far:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter-1: The basics](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-the-basics-29ad2122d98f)'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter-2: [The measure of a map â€” determinants](https://medium.com/p/1e5fd752a3be)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter-3:[Why is matrix multiplication the way it is?](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter-4 (current): Systems of equations, linear regression and neural networks'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter-5: [Rank nullity and why row rank == col rank](/a-birds-eye-view-of-linear-algebra-rank-nullity-and-why-row-rank-equals-column-rank-bc084e0e1075)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All images in this blog, unless otherwise stated, are by the author.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: I) Introduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern AI models leverage high dimensional vector spaces to encode information.
    And ***the*** tool we have for reasoning about high dimensional spaces and mappings
    between them is linear algebra.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: And within that field, matrix multiplication (along with its inverse) is literally
    all you need to build many simple machine learning models end to end. Which is
    why spending the time to understand it really well is a great investment. And
    this is what we did in [chapter 3](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: These simple models, useful in their own right, form the building blocks of
    more complex ML and AI models with state of the art performance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Weâ€™ll cover a few of these applications (from linear regression to elementary
    neural networks) in this chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸€äº›è¿™äº›åº”ç”¨ï¼ˆä»çº¿æ€§å›å½’åˆ°åˆçº§ç¥ç»ç½‘ç»œï¼‰ã€‚
- en: But first, we need to go to the simplest case in the simplest model â€” when the
    number of data points equals the number of model parameters. The case of solving
    a system of linear equations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è½¬åˆ°æœ€ç®€å•çš„æƒ…å†µå’Œæœ€ç®€å•çš„æ¨¡å‹â€”â€”å½“æ•°æ®ç‚¹çš„æ•°é‡ç­‰äºæ¨¡å‹å‚æ•°çš„æ•°é‡æ—¶ï¼Œå³æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„çš„æƒ…å†µã€‚
- en: II) Systems of linear equations
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: II) çº¿æ€§æ–¹ç¨‹ç»„
- en: We have finally arrived (in the context of this book) at the heart of linear
    algebra. Solving systems of linear equations is how we discovered linear algebra
    in the first place and the motivations for most concepts in this field have deep
    roots in this application.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç»ˆäºæ¥åˆ°äº†çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒï¼ˆåœ¨æœ¬ä¹¦çš„èƒŒæ™¯ä¸‹ï¼‰ã€‚æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„æ˜¯æˆ‘ä»¬æœ€åˆå‘ç°çº¿æ€§ä»£æ•°çš„æ–¹å¼ï¼Œè¿™ä¸ªé¢†åŸŸçš„å¤§å¤šæ•°æ¦‚å¿µçš„åŠ¨æœºåœ¨è¿™ä¸ªåº”ç”¨ä¸­æœ‰ç€æ·±è¿œçš„æ ¹åŸºã€‚
- en: Letâ€™s start simple and one dimensional. The concept of division is rooted in
    one dimensional linear equations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»ç®€å•çš„ä¸€ç»´æƒ…å†µå¼€å§‹ã€‚é™¤æ³•çš„æ¦‚å¿µæ ¹æ¤äºä¸€ç»´çº¿æ€§æ–¹ç¨‹ä¸­ã€‚
- en: '*ax = b*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*ax = b*'
- en: 'The equation reads, â€œwhat number when multiplied by *a* leads to *b*â€. And
    the solution is what defines scalar division:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹ç¨‹çš„æ„æ€æ˜¯ï¼Œâ€œå“ªä¸ªæ•°å­—åœ¨ä¹˜ä»¥*a*åå¾—åˆ°*b*â€ã€‚è§£å†³æ–¹æ¡ˆå®šä¹‰äº†æ ‡é‡é™¤æ³•ï¼š
- en: '*x = b/a* _(1)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*x = b/a* _(1)'
- en: This is with one dimension, *x*. Most interesting things happen when things
    go multi-dimensional. So instead of just one variable, *x*, we have *n* variables
    (*x_1, x_2, x_3, â€¦, x_n*).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åœ¨ä¸€ç»´çš„æƒ…å†µä¸‹ï¼Œå³*x*ã€‚å½“è¿›å…¥å¤šç»´æ—¶ï¼Œæœ€æœ‰è¶£çš„äº‹æƒ…å‘ç”Ÿäº†ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸ä»…æœ‰ä¸€ä¸ªå˜é‡*x*ï¼Œè¿˜æœ‰*n*ä¸ªå˜é‡ï¼ˆ*x_1, x_2, x_3, â€¦, x_n*ï¼‰ã€‚
- en: And as soon as you go multi-dimensional, linear algebra jumps into the scene.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è¿›å…¥å¤šç»´ç©ºé—´ï¼Œçº¿æ€§ä»£æ•°å°±ä¼šå‡ºç°ã€‚
- en: '![](../Images/2f774d782fa426023a1b05562e31b18e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f774d782fa426023a1b05562e31b18e.png)'
- en: When things get multi-dimensional, linear algebra makes an entry. Image by midjourney.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å½“é—®é¢˜å˜å¾—å¤šç»´æ—¶ï¼Œçº¿æ€§ä»£æ•°å°±ä¼šå‡ºç°ã€‚å›¾åƒæ¥è‡ªmidjourneyã€‚
- en: Now, instead of *a.x = b,* our equation should include the *n* variables, *x_1,
    x_2, x_3, â€¦, x_n.* And just like *x* had the coefficient, *a,* before; each of
    the *x_i*â€™s gets its own coefficient, *a_i* this time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬çš„æ–¹ç¨‹åº”åŒ…æ‹¬*n*ä¸ªå˜é‡ï¼Œå³*x_1, x_2, x_3, â€¦, x_n*ã€‚å°±åƒ*x*ä¹‹å‰æœ‰ç³»æ•°*a*ä¸€æ ·ï¼Œè¿™æ¬¡æ¯ä¸ª*x_i*éƒ½æœ‰è‡ªå·±çš„ç³»æ•°*a_i*ã€‚
- en: '![](../Images/85b15880feff61042a6d27878e9c2b6a.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85b15880feff61042a6d27878e9c2b6a.png)'
- en: 'But unlike in the one dimensional case, this one equation is not enough to
    solve for the *x*â€™s since it doesnâ€™t uniquely specify them. For example, we can
    pick any values we want for *x_2, x_3,â€¦, x_n* (ex: all 0â€™s) and only then will
    we have a unique value for *x_1\.* If we already knew the values of *x_1, x_2,
    â€¦, x_n* and wanted to communicate them in the form of a system of equations, they
    would look something like:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä¸ä¸€ç»´æƒ…å†µä¸åŒï¼Œè¿™ä¸ªæ–¹ç¨‹ä¸è¶³ä»¥æ±‚è§£*x*ï¼Œå› ä¸ºå®ƒä¸èƒ½å”¯ä¸€åœ°ç¡®å®š*x*ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥éšæ„é€‰æ‹©*x_2, x_3,â€¦, x_n*ï¼ˆä¾‹å¦‚ï¼šå…¨éƒ¨ä¸º0ï¼‰ï¼Œåªæœ‰è¿™æ ·æˆ‘ä»¬æ‰èƒ½å¾—åˆ°å”¯ä¸€çš„*x_1*å€¼ã€‚å¦‚æœæˆ‘ä»¬å·²ç»çŸ¥é“äº†*x_1,
    x_2, â€¦, x_n*çš„å€¼ï¼Œå¹¶ä¸”æƒ³ä»¥æ–¹ç¨‹ç»„çš„å½¢å¼ä¼ è¾¾å®ƒä»¬ï¼Œå®ƒä»¬ä¼šçœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: '![](../Images/bac0590dc865ba0985cf0515d38be4d3.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bac0590dc865ba0985cf0515d38be4d3.png)'
- en: So, we needed *n* equations (equal to the number of variables). Any general
    system can now be created by â€œmixingâ€ the equations above by taking linear combinations
    of them. This allows adding any two equations and multiplying any of the equations
    by any scalar. These operations will obviously not change the solution of the
    system (or the question of its existence).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦*n*ä¸ªæ–¹ç¨‹ï¼ˆç­‰äºå˜é‡çš„æ•°é‡ï¼‰ã€‚ç°åœ¨å¯ä»¥é€šè¿‡â€œæ··åˆâ€ä¸Šè¿°æ–¹ç¨‹æ¥åˆ›å»ºä»»ä½•ä¸€èˆ¬ç³»ç»Ÿï¼Œå³å–å®ƒä»¬çš„çº¿æ€§ç»„åˆã€‚è¿™å…è®¸æ·»åŠ ä»»æ„ä¸¤ä¸ªæ–¹ç¨‹ï¼Œå¹¶å°†ä»»æ„æ–¹ç¨‹ä¹˜ä»¥ä»»æ„æ ‡é‡ã€‚è¿™äº›æ“ä½œæ˜¾ç„¶ä¸ä¼šæ”¹å˜ç³»ç»Ÿçš„è§£ï¼ˆæˆ–å…¶å­˜åœ¨æ€§é—®é¢˜ï¼‰ã€‚
- en: 'For example, we can add three times the second equation to the first equation
    and get:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç¬¬äºŒä¸ªæ–¹ç¨‹ä¹˜ä»¥ä¸‰åŠ åˆ°ç¬¬ä¸€ä¸ªæ–¹ç¨‹ä¸­ï¼Œå¾—åˆ°ï¼š
- en: '![](../Images/d8f833bb4004d7c74524b1038ba05380.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8f833bb4004d7c74524b1038ba05380.png)'
- en: Then we can replace the first equation (*x_1=4.6*) by this one and the resulting
    system will still have the same solution (because we can just undo the change
    we made by subtracting three times the second equation from the new first equation).
    If we â€œmixâ€ in this way very throughly, replacing one of the equations at random
    with the â€œmixedâ€ one and repeat many times, weâ€™ll end up with *n* equations each
    of which involves multiples of all the *n* variables.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥ç”¨è¿™ä¸ªæ–¹ç¨‹æ›¿æ¢ç¬¬ä¸€ä¸ªæ–¹ç¨‹ï¼ˆ*x_1=4.6*ï¼‰ï¼Œå¾—åˆ°çš„æ–¹ç¨‹ç»„ä»ç„¶æœ‰ç›¸åŒçš„è§£ï¼ˆå› ä¸ºæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»æ–°çš„ç¬¬ä¸€ä¸ªæ–¹ç¨‹ä¸­å‡å»ä¸‰å€çš„ç¬¬äºŒä¸ªæ–¹ç¨‹æ¥æ’¤é”€æˆ‘ä»¬æ‰€åšçš„æ›´æ”¹ï¼‰ã€‚å¦‚æœæˆ‘ä»¬ä»¥è¿™ç§æ–¹å¼éå¸¸å½»åº•åœ°â€œæ··åˆâ€ï¼Œéšæœºæ›¿æ¢å…¶ä¸­ä¸€ä¸ªæ–¹ç¨‹ï¼Œå¹¶é‡å¤å¤šæ¬¡ï¼Œæˆ‘ä»¬å°†å¾—åˆ°*n*ä¸ªæ–¹ç¨‹ï¼Œæ¯ä¸ªæ–¹ç¨‹æ¶‰åŠæ‰€æœ‰*n*ä¸ªå˜é‡çš„å€æ•°ã€‚
- en: '![](../Images/4ee12009646ef9d2cba54bbb94bbc80c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ee12009646ef9d2cba54bbb94bbc80c.png)'
- en: 'The coefficients *a_{ij}* look an awfully lot like the elements of a square
    matrix. And indeed, from what we learnt about matrix vector multiplication in
    chapter 3 (see animation-1 in section III-A), we can express the system of equations
    as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç³»æ•° *a_{ij}* çœ‹èµ·æ¥éå¸¸åƒä¸€ä¸ªæ–¹é˜µçš„å…ƒç´ ã€‚ç¡®å®ï¼Œæ ¹æ®æˆ‘ä»¬åœ¨ç¬¬3ç« ä¸­å­¦åˆ°çš„çŸ©é˜µå‘é‡ä¹˜æ³•ï¼ˆè§III-AèŠ‚ä¸­çš„åŠ¨ç”»-1ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ–¹ç¨‹ç»„è¡¨ç¤ºä¸ºï¼š
- en: '![](../Images/6f6ee90f99fcf1a6e9349e9a75b502e8.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f6ee90f99fcf1a6e9349e9a75b502e8.png)'
- en: Eq (1) Matrix form of the system of linear equations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ï¼ˆ1ï¼‰çº¿æ€§æ–¹ç¨‹ç»„çš„çŸ©é˜µå½¢å¼ã€‚
- en: And just like in equation (1) we took the multiplicative inverse to get the
    scalar variable, *x (x=b/a)*, here we take the inverse of matrix multiplication
    to get the vector, *x*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒåœ¨æ–¹ç¨‹ï¼ˆ1ï¼‰ä¸­æˆ‘ä»¬å–äº†ä¹˜æ³•é€†æ¥å¾—åˆ°æ ‡é‡å˜é‡ *x (x=b/a)* ä¸€æ ·ï¼Œè¿™é‡Œæˆ‘ä»¬å–çŸ©é˜µä¹˜æ³•çš„é€†æ¥å¾—åˆ°å‘é‡ *x*ã€‚
- en: '![](../Images/df7466a98205a67f83917570fcf70642.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df7466a98205a67f83917570fcf70642.png)'
- en: And we have reached the iconic equation where it all started. Where linear algebra
    began. The system of linear equations. There is a whole science behind calculating
    the inverse and doing it well which we will cover in subsequent chapters.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»è¾¾åˆ°äº†ä¸€åˆ‡å¼€å§‹çš„æ ‡å¿—æ€§æ–¹ç¨‹ã€‚çº¿æ€§ä»£æ•°çš„èµ·ç‚¹ã€‚çº¿æ€§æ–¹ç¨‹ç»„ã€‚è®¡ç®—é€†çŸ©é˜µåŠå…¶ç›¸å…³æ“ä½œæœ‰ä¸€ä¸ªå®Œæ•´çš„ç§‘å­¦èƒŒæ™¯ï¼Œæˆ‘ä»¬å°†åœ¨åç»­ç« èŠ‚ä¸­è®¨è®ºã€‚
- en: Note that the number of rows in the *A* matrix was the number of equations in
    the system and the number of columns was the number of variables.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œ*A* çŸ©é˜µçš„è¡Œæ•°æ˜¯ç³»ç»Ÿä¸­çš„æ–¹ç¨‹æ•°ï¼Œåˆ—æ•°æ˜¯å˜é‡æ•°ã€‚
- en: 'Geometrically, each equation is a hyperplane in the space with one lower dimensionality.
    Take the system of equations below:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å‡ ä½•ä¸Šè®²ï¼Œæ¯ä¸ªæ–¹ç¨‹åœ¨ç©ºé—´ä¸­éƒ½æ˜¯ä¸€ä¸ªç»´åº¦é™ä½çš„è¶…å¹³é¢ã€‚è€ƒè™‘ä»¥ä¸‹æ–¹ç¨‹ç»„ï¼š
- en: '![](../Images/7fea6e24adffbec2cf05a7a6a1e86c97.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fea6e24adffbec2cf05a7a6a1e86c97.png)'
- en: The solution to this system of equations is *(x=0, y=0, z=1)*. Since there are
    three variables in this system (*x, y* and *z*), the vector space is three dimensional
    as shown in figure-1 below. Refer to that figure for the proceeding discussion.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹ç¨‹ç»„çš„è§£æ˜¯*(x=0, y=0, z=1)*ã€‚ç”±äºè¿™ä¸ªæ–¹ç¨‹ç»„ä¸­æœ‰ä¸‰ä¸ªå˜é‡ï¼ˆ*x, y* å’Œ *z*ï¼‰ï¼Œå› æ­¤çŸ¢é‡ç©ºé—´æ˜¯ä¸‰ç»´çš„ï¼Œå¦‚ä¸‹å›¾-1æ‰€ç¤ºã€‚è¯·å‚é˜…è¯¥å›¾ä»¥è·å–åç»­è®¨è®ºã€‚
- en: The *x=0* equation corresponds to the yellow hyper-plane in this 3-d space.
    Since the equation represents the addition of one constraint, the dimensionality
    of the space where this equation is satisfied goes down by one (*3â€“1=2)*. Similarly,
    the *y=0* equation corresponds to the blue hyperplane, also two dimensional. Now
    that we have two equations, so two constraints. The dimensionality of the sub-space
    where both of them are satisfied will go down by two, making it *3â€“2=1* dimensional.
    A one dimensional sub-space is just a line, and indeed we get the green line figure-1\.
    Finally, when we add the third equation, *x+y+z=1*, represented by the pink plane.
    We now have three equations/ constraints. And they restrict the dimensionality
    of the 3-d space by 3\. So weâ€™re left with *3â€“3 = 0* dimensions (which is a point)
    and indeed, we get the red point, the only element of the vector space that satisfies
    all three of the equations in the system simultaneously.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*x=0* æ–¹ç¨‹å¯¹åº”äºè¿™ä¸ªä¸‰ç»´ç©ºé—´ä¸­çš„é»„è‰²è¶…å¹³é¢ã€‚ç”±äºè¯¥æ–¹ç¨‹è¡¨ç¤ºå¢åŠ ä¸€ä¸ªçº¦æŸï¼Œæ»¡è¶³è¯¥æ–¹ç¨‹çš„ç©ºé—´çš„ç»´åº¦é™ä½ä¸€ä¸ªï¼ˆ*3â€“1=2*ï¼‰ã€‚ç±»ä¼¼åœ°ï¼Œ*y=0* æ–¹ç¨‹å¯¹åº”äºè“è‰²è¶…å¹³é¢ï¼Œä¹ŸäºŒç»´ã€‚ç°åœ¨æˆ‘ä»¬æœ‰ä¸¤ä¸ªæ–¹ç¨‹ï¼Œå› æ­¤ä¸¤ä¸ªçº¦æŸã€‚ä¸¤ä¸ªæ–¹ç¨‹éƒ½æ»¡è¶³çš„å­ç©ºé—´çš„ç»´åº¦å°†é™ä½ä¸¤ä¸ªï¼Œå˜æˆ*3â€“2=1*ç»´ã€‚ä¸€ç»´å­ç©ºé—´å°±æ˜¯ä¸€æ¡ç›´çº¿ï¼Œå®é™…ä¸Šæˆ‘ä»¬å¾—åˆ°çš„æ˜¯ç»¿è‰²ç›´çº¿å›¾-1ã€‚æœ€åï¼Œå½“æˆ‘ä»¬åŠ å…¥ç¬¬ä¸‰ä¸ªæ–¹ç¨‹
    *x+y+z=1*ï¼Œå®ƒç”±ç²‰è‰²å¹³é¢è¡¨ç¤ºã€‚ç°åœ¨æˆ‘ä»¬æœ‰ä¸‰ä¸ªæ–¹ç¨‹/çº¦æŸã€‚å®ƒä»¬å°†ä¸‰ç»´ç©ºé—´çš„ç»´åº¦é™åˆ¶ä¸º3ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å‰©ä¸‹çš„ç»´åº¦æ˜¯*3â€“3=0*ï¼ˆå³ä¸€ä¸ªç‚¹ï¼‰ï¼Œç¡®å®ï¼Œæˆ‘ä»¬å¾—åˆ°çº¢è‰²ç‚¹ï¼Œå®ƒæ˜¯åŒæ—¶æ»¡è¶³ç³»ç»Ÿä¸­æ‰€æœ‰ä¸‰ä¸ªæ–¹ç¨‹çš„å”¯ä¸€çŸ¢é‡ç©ºé—´å…ƒç´ ã€‚'
- en: '![](../Images/9eaac84a66add4954150e439f1952df8.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9eaac84a66add4954150e439f1952df8.png)'
- en: 'Figure-1: A system of equations in 3 variables. The space is three dimensional.
    Each equation carves out a hyper-plane one dimension lower. Combinations of two
    equations remove two dimensions and so on. Image by author.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾-1ï¼šä¸‰å˜é‡çš„æ–¹ç¨‹ç»„ã€‚ç©ºé—´æ˜¯ä¸‰ç»´çš„ã€‚æ¯ä¸ªæ–¹ç¨‹åˆ‡å‰²å‡ºä¸€ä¸ªä½ä¸€ç»´çš„è¶…å¹³é¢ã€‚ä¸¤ä¸ªæ–¹ç¨‹çš„ç»„åˆå»é™¤ä¸¤ä¸ªç»´åº¦ï¼Œä¾æ­¤ç±»æ¨ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: 'In the example above, we had *n* equations and *n* variables. In general, they
    neednâ€™t be the same. Letâ€™s say there are *n* equations and *m* variables. It''s
    clear from the figure above:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æœ‰ *n* ä¸ªæ–¹ç¨‹å’Œ *n* ä¸ªå˜é‡ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå®ƒä»¬ä¸ä¸€å®šç›¸åŒã€‚å‡è®¾æœ‰ *n* ä¸ªæ–¹ç¨‹å’Œ *m* ä¸ªå˜é‡ã€‚ä»ä¸Šé¢çš„å›¾å¯ä»¥æ¸…æ¥šåœ°çœ‹å‡ºï¼š
- en: When *n<m*, we have more variables than equations and the system has infinite
    solutions. Think of equations as requirements. Not enough requirements specified,
    tons of possible solutions.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When *n>m*, weâ€™ll have more equations than variables. There are now too many
    constraints to satisfy and we wonâ€™t be able to find any points in the vector space
    that satisfy all of them.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So when *n=m*, we should always have a single, unique solution? Not quite, there
    are ways things can go wrong (for all three cases).
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: II-A) How things go wrong
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Consistency**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: One thing that will make any system have no solution is inconsistency. Say the
    first equation is *2x+y=3* and the second one is 2*x+y = 5*. Now, regardless of
    how many more equations we add, there will be no way to satisfy both of them simultaneously
    (they simply contradict each other). Any system that has those two equations is
    inconsistent.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '**Dependency**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Next, the system could deceive us, looking like it has more equations than it
    actually does. The most obvious way this could happen is if we simply copy one
    of the equations. This will add one equation to the system, making it so that
    technically, we have *m+1* equations now. Itâ€™s clear, however, that we didnâ€™t
    really add any new information. The new equation we added is redundant. This is
    called a dependent system because some of our equations donâ€™t bring any new information
    of their own but â€œdependâ€ on others. If we downright plagiarize one of the equations,
    its going to be obvious to spot. But the same thing can be done in a more clandestine
    way. Instead of downright copying one of the equations, we can take a linear combination
    of a few of them to create a new one. The linear combination can be done in a
    way that it is almost impossible to spot just by looking at the equations that
    there is an â€œimposter equationâ€ in the mix. Of course, once such a dependent equation
    is added into the mix, it is impossible to tell which one the â€œimposterâ€ is, like
    the three spider men below.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/caf534a1f63b31cf151d423005032d8d.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Two Bobâ€™s. But there can be only one. One of them is an imposter. Hard to tell
    which one. Much like dependent equations introduced in a linear system by taking
    linear combinations of the others. Image by midjourney.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The two problems of consistency and dependency manifest in the same way as far
    as the *A* matrix (from equation (1)) goes. The vector *b* then determines whether
    the system is inconsistent (no solution) or dependent (infinitely many solutions).
    What happens is that some of the rows of *A* turn out to be linear combinations
    of the others.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: II-B) The case of data analysis
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The case that is most interesting to us is building models on top of data. If
    you have data points collected independently, with an underlying random process
    (like data collection should be), it's almost certain that the rows of your data
    matrix wonâ€™t be linearly dependent (in probability terms, â€œ[almost surely](https://en.wikipedia.org/wiki/Almost_surely)â€).
    So, we donâ€™t have to worry about getting either an inconsistent or dependent system.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æˆ‘ä»¬æœ€æ„Ÿå…´è¶£çš„æƒ…å†µæ˜¯å»ºç«‹åœ¨æ•°æ®ä¹‹ä¸Šçš„æ¨¡å‹ã€‚å¦‚æœä½ æœ‰ç‹¬ç«‹æ”¶é›†çš„æ•°æ®ç‚¹ï¼Œå¹¶ä¸”æ•°æ®æ”¶é›†è¿‡ç¨‹æ˜¯éšæœºçš„ï¼ˆå°±åƒæ•°æ®æ”¶é›†åº”è¯¥æ˜¯çš„é‚£æ ·ï¼‰ï¼Œå‡ ä¹å¯ä»¥è‚¯å®šä½ çš„æ•°æ®çŸ©é˜µçš„è¡Œä¸ä¼šçº¿æ€§ç›¸å…³ï¼ˆåœ¨æ¦‚ç‡æœ¯è¯­ä¸­ï¼Œ"[å‡ ä¹è‚¯å®š](https://en.wikipedia.org/wiki/Almost_surely)"ï¼‰ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬ä¸å¿…æ‹…å¿ƒå¾—åˆ°ä¸€ä¸ªä¸ä¸€è‡´æˆ–ç›¸å…³çš„ç³»ç»Ÿã€‚
- en: Also, we typically have many more rows (data points) than columns (variables/
    features). So, the matrix will be â€œskinnyâ€, with *n>m*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬é€šå¸¸æœ‰æ¯”åˆ—æ•°ï¼ˆå˜é‡/ç‰¹å¾ï¼‰å¤šå¾—å¤šçš„è¡Œï¼ˆæ•°æ®ç‚¹ï¼‰ã€‚æ‰€ä»¥ï¼ŒçŸ©é˜µå°†æ˜¯â€œç˜¦é•¿çš„â€ï¼Œå…¶ä¸­ *n>m*ã€‚
- en: '![](../Images/8f8a0257f0933335933906c026e2a415.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f8a0257f0933335933906c026e2a415.png)'
- en: A tall, skinny rectangular matrix of data with more rows than columns. Image
    by midjourney.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé«˜è€Œç˜¦çš„çŸ©å½¢æ•°æ®çŸ©é˜µï¼Œè¡Œæ•°å¤šäºåˆ—æ•°ã€‚å›¾ç‰‡ç”± midjourney æä¾›ã€‚
- en: Weâ€™re firmly in the domain of no solution to the system of equations. If we
    choose any *m* of the *n* equations and delete/ ignore the rest, then we will
    be back to the *m=m* case with an equal number of equations and variables and
    now there will be a unique solution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ­£å¤„äºæ— è§£æ–¹ç¨‹ç»„çš„é¢†åŸŸã€‚å¦‚æœæˆ‘ä»¬é€‰æ‹©ä»»ä½• *m* ä¸ª *n* ä¸ªæ–¹ç¨‹å¹¶åˆ é™¤/å¿½ç•¥å…¶ä½™çš„æ–¹ç¨‹ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†å›åˆ° *m=m* çš„æƒ…å†µï¼Œè¿™æ—¶æ–¹ç¨‹æ•°å’Œå˜é‡æ•°ç›¸ç­‰ï¼Œç°åœ¨å°†ä¼šæœ‰ä¸€ä¸ªå”¯ä¸€çš„è§£ã€‚
- en: So, there are a total of (*n* choose *m)* points the hyperplanes corresponding
    to the equations in the system form. None of these points will lie on all the
    hyperplanes simultaneously.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¶…å¹³é¢å¯¹åº”äºç³»ç»Ÿä¸­æ–¹ç¨‹çš„æ€»æ•°æ˜¯ (*n* é€‰æ‹© *m*) ä¸ªç‚¹ã€‚è¿™äº›ç‚¹ä¸­æ²¡æœ‰ä¸€ä¸ªç‚¹ä¼šåŒæ—¶ä½äºæ‰€æœ‰è¶…å¹³é¢ä¸Šã€‚
- en: Even though there is no point that simultaneously satisfies all the equations,
    we can still ask â€œwhat is the point in the entire vector space that comes closest
    to satisfying all the equationsâ€. And this is where linear regression comes in.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æ²¡æœ‰ä¸€ä¸ªç‚¹åŒæ—¶æ»¡è¶³æ‰€æœ‰æ–¹ç¨‹ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥é—®ï¼šâ€œåœ¨æ•´ä¸ªå‘é‡ç©ºé—´ä¸­å“ªä¸ªç‚¹æœ€æ¥è¿‘æ»¡è¶³æ‰€æœ‰æ–¹ç¨‹â€ã€‚è¿™å°±æ˜¯çº¿æ€§å›å½’çš„ä½œç”¨æ‰€åœ¨ã€‚
- en: '**III) Linear regression**'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**III) çº¿æ€§å›å½’**'
- en: '![](../Images/9b61ce9e94a1b0d7305e2e036150475e.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b61ce9e94a1b0d7305e2e036150475e.png)'
- en: Linear regression draws a linear hyperplane that is closest to the points in
    your data. Much like the ninja attempts to swing his sword in a way that its closest
    to as many leaves as possible. Image by midjourney.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§å›å½’ç»˜åˆ¶ä¸€ä¸ªæœ€æ¥è¿‘ä½ æ•°æ®ç‚¹çš„çº¿æ€§è¶…å¹³é¢ã€‚å°±åƒå¿è€…å°è¯•ä»¥ä¸€ç§å°½å¯èƒ½æ¥è¿‘æœ€å¤šå¶å­çš„æ–¹å¼æŒ¥åŠ¨ä»–çš„å‰‘ä¸€æ ·ã€‚å›¾ç‰‡ç”± midjourney æä¾›ã€‚
- en: 'First, in the context of linear regression, the matrices and vectors are named
    slightly differently than they are with systems of equations. Now, the matrix
    of coefficients, *A* becomes the matrix that contains the data and we call it
    *X*. Each row of this matrix, the row vector *x_i*, is one data entry. The vector
    *x* contained the unknown variables in equation (1). Now, the unknown variables
    are the linear regression coefficients and we denote them, *ğ›½.* And finally, the
    right hand vector of the linear system, *b* becomes the vector that contains the
    dependent variable and we call it *y*. So, equation (1) in the context of linear
    regression becomes:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œåœ¨çº¿æ€§å›å½’çš„èƒŒæ™¯ä¸‹ï¼ŒçŸ©é˜µå’Œå‘é‡çš„å‘½åä¸æ–¹ç¨‹ç»„æœ‰æ‰€ä¸åŒã€‚ç°åœ¨ï¼Œç³»æ•°çŸ©é˜µ *A* å˜æˆäº†åŒ…å«æ•°æ®çš„çŸ©é˜µï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º *X*ã€‚è¿™ä¸ªçŸ©é˜µçš„æ¯ä¸€è¡Œï¼Œå³è¡Œå‘é‡
    *x_i*ï¼Œæ˜¯ä¸€ä¸ªæ•°æ®æ¡ç›®ã€‚å‘é‡ *x* åŒ…å«äº†æ–¹ç¨‹ (1) ä¸­çš„æœªçŸ¥å˜é‡ã€‚ç°åœ¨ï¼ŒæœªçŸ¥å˜é‡æ˜¯çº¿æ€§å›å½’ç³»æ•°ï¼Œæˆ‘ä»¬ç”¨ *ğ›½* è¡¨ç¤ºã€‚æœ€åï¼Œçº¿æ€§ç³»ç»Ÿçš„å³è¾¹å‘é‡ *b*
    å˜æˆäº†åŒ…å«å› å˜é‡çš„å‘é‡ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º *y*ã€‚å› æ­¤ï¼Œåœ¨çº¿æ€§å›å½’çš„èƒŒæ™¯ä¸‹ï¼Œæ–¹ç¨‹ (1) å˜æˆäº†ï¼š
- en: '![](../Images/d8509af2748e53c64e7a7a3027387cb1.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8509af2748e53c64e7a7a3027387cb1.png)'
- en: 'Eq (2): The basic equation of linear regression. Image by author.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Eq (2)ï¼šçº¿æ€§å›å½’çš„åŸºæœ¬æ–¹ç¨‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'Expanded out, this equation looks like the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å±•å¼€åï¼Œè¿™ä¸ªæ–¹ç¨‹çœ‹èµ·æ¥å¦‚ä¸‹ï¼š
- en: '![](../Images/dd16e8cb07bddee8c058fe6df57d0683.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd16e8cb07bddee8c058fe6df57d0683.png)'
- en: 'Notice the first column of *1*â€™s. This corresponds to the constant term. For
    example, in the one variable case, without that column our model would be: *y
    = m.x.* This would exclude lines like *y=x+1* from consideration. In order to
    consider lines like: *y = mx+c* (which we do want in most cases), we need that
    column of *1*â€™s.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ç¬¬ä¸€åˆ—çš„*1*ã€‚è¿™å¯¹åº”äºå¸¸æ•°é¡¹ã€‚ä¾‹å¦‚ï¼Œåœ¨å•å˜é‡çš„æƒ…å†µä¸‹ï¼Œå¦‚æœæ²¡æœ‰é‚£ä¸€åˆ—ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°†ä¼šæ˜¯ï¼š*y = m.x.* è¿™å°†æ’é™¤åƒ *y=x+1* è¿™æ ·çš„ç›´çº¿ã€‚å¦‚æœè¦è€ƒè™‘åƒ
    *y = mx+c* ï¼ˆæˆ‘ä»¬åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ç¡®å®éœ€è¦ï¼‰çš„ç›´çº¿ï¼Œæˆ‘ä»¬éœ€è¦é‚£ä¸€åˆ—*1*ã€‚
- en: 'Just as with the linear system of equations, weâ€™d like to take the multiplicative
    inverse of *X* on both sides and find *ğ›½.* Like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒçº¿æ€§æ–¹ç¨‹ç»„ä¸€æ ·ï¼Œæˆ‘ä»¬å¸Œæœ›å¯¹ä¸¤è¾¹è¿›è¡Œ*X*çš„ä¹˜æ³•é€†è¿ç®—ï¼Œæ‰¾åˆ°*ğ›½*ã€‚å¦‚å›¾æ‰€ç¤ºï¼š
- en: '![](../Images/e4b7b4373714516a8aff0531a10a9373.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4b7b4373714516a8aff0531a10a9373.png)'
- en: This equation doesnâ€™t make any sense. The matrix, X is rectangular, so it canâ€™t
    be inverted.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹ç¨‹æ²¡æœ‰ä»»ä½•æ„ä¹‰ã€‚çŸ©é˜µ *X* æ˜¯çŸ©å½¢çš„ï¼Œæ‰€ä»¥å®ƒä¸èƒ½è¢«é€†è½¬ã€‚
- en: Unfortunately, this doesnâ€™t make sense. Only square matrices can be inverted.
    Rectangular ones simply canâ€™t. And our data matrix, *X* is rectangular with the
    number of rows (*n*) > number of columns (*m*).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œè¿™æ²¡æœ‰æ„ä¹‰ã€‚åªæœ‰æ–¹é˜µæ‰å¯ä»¥é€†è½¬ã€‚çŸ©å½¢çŸ©é˜µåˆ™ä¸è¡Œã€‚æˆ‘ä»¬çš„æ•°æ®çŸ©é˜µ *X* æ˜¯ä¸€ä¸ªçŸ©å½¢çŸ©é˜µï¼Œè¡Œæ•°ï¼ˆ*n*ï¼‰å¤§äºåˆ—æ•°ï¼ˆ*m*ï¼‰ã€‚
- en: One way to see this is that it represents a linear system with more equations
    than variables and the arguments of section II apply. In terms of the linear map
    behind the matrix, for a â€œskinnyâ€ rectangular matrix with more rows than columns
    (like *X* is), it wonâ€™t be a one to one mapping (multiple points from the same
    space will map to the same point in the second one, which isnâ€™t allowed).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§ç†è§£è¿™ä¸€ç‚¹çš„æ–¹æ³•æ˜¯ï¼Œå®ƒè¡¨ç¤ºä¸€ä¸ªæ–¹ç¨‹æ•°é‡å¤šäºå˜é‡æ•°é‡çš„çº¿æ€§ç³»ç»Ÿï¼Œå› æ­¤ç¬¬äºŒéƒ¨åˆ†çš„è®ºç‚¹é€‚ç”¨ã€‚å°±çŸ©é˜µèƒŒåçš„çº¿æ€§æ˜ å°„è€Œè¨€ï¼Œå¯¹äºä¸€ä¸ªâ€œç˜¦é•¿â€çš„çŸ©é˜µï¼ˆå¦‚ *X*
    æ‰€ç¤ºï¼‰ï¼Œè¡Œæ•°å¤šäºåˆ—æ•°ï¼Œå®ƒä¸ä¼šæ˜¯ä¸€ä¸ªä¸€å¯¹ä¸€çš„æ˜ å°„ï¼ˆåŒä¸€ç©ºé—´ä¸­çš„å¤šä¸ªç‚¹ä¼šæ˜ å°„åˆ°ç¬¬äºŒä¸ªç©ºé—´ä¸­çš„åŒä¸€ç‚¹ï¼Œè¿™ç§æƒ…å†µæ˜¯ä¸å…è®¸çš„ï¼‰ã€‚
- en: Is there a â€œhackâ€ we can do to just make the matrix multiplied by ğ›½ a square
    one? The matrix, *X* is currently *nâ¨‰m*. What if we multiplied by some other *mâ¨‰n*
    matrix, *U.* Then, the resulting matrix, *V* will be square *mâ¨‰m*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯å¦æœ‰ä¸€ç§â€œæŠ€å·§â€å¯ä»¥è®©ä¹˜ä»¥ ğ›½ çš„çŸ©é˜µå˜æˆæ–¹é˜µï¼ŸçŸ©é˜µ *X* ç›®å‰æ˜¯ *nâ¨‰m*ã€‚å¦‚æœæˆ‘ä»¬ä¹˜ä»¥å¦ä¸€ä¸ª *mâ¨‰n* çš„çŸ©é˜µ *U*ï¼Œé‚£ä¹ˆå¾—åˆ°çš„çŸ©é˜µ *V*
    å°†æ˜¯æ–¹é˜µ *mâ¨‰m*ã€‚
- en: '![](../Images/c724f4c232c633cddaa7f1681c1d03fa.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c724f4c232c633cddaa7f1681c1d03fa.png)'
- en: Pre-multiplying equation (2) by *U*, we are then able to invert the resulting
    matrix and get ğ›½.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å¯¹æ–¹ç¨‹ï¼ˆ2ï¼‰è¿›è¡Œ *U* çš„å·¦ä¹˜ï¼Œæˆ‘ä»¬å¯ä»¥é€†è½¬ç»“æœçŸ©é˜µå¹¶å¾—åˆ° ğ›½ã€‚
- en: '![](../Images/538314b27c122ecfc4158219482ec859.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/538314b27c122ecfc4158219482ec859.png)'
- en: Now, the question is, where do we get this *U* matrix (*mâ¨‰n)* from? The *X*
    matrix, *nâ¨‰m* is what we have (the data itself). One thing we can do is flip the
    *X* matrix so its rows become its columns and columns become rows. This kind of
    operation on a matrix is called transpose, denoted by X^T visualized below.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œé—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬ä»å“ªé‡Œå¾—åˆ°è¿™ä¸ª *U* çŸ©é˜µï¼ˆ*mâ¨‰n*ï¼‰ï¼Ÿæˆ‘ä»¬æ‹¥æœ‰çš„æ˜¯ *X* çŸ©é˜µï¼Œ*nâ¨‰m*ï¼ˆå³æ•°æ®æœ¬èº«ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥åšçš„ä¸€ä»¶äº‹æ˜¯å°† *X* çŸ©é˜µç¿»è½¬ï¼Œä½¿å…¶è¡Œå˜æˆåˆ—ï¼Œåˆ—å˜æˆè¡Œã€‚è¿™ç§å¯¹çŸ©é˜µçš„æ“ä½œç§°ä¸ºè½¬ç½®ï¼Œè®°ä½œ
    X^Tï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
- en: '![](../Images/ce5afab467995b5e57c21423cef1568a.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce5afab467995b5e57c21423cef1568a.png)'
- en: 'Transpose of a matrix. Image credit: [Wikipedia](https://en.wikipedia.org/wiki/Transpose)
    article on transpose.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ©é˜µçš„è½¬ç½®ã€‚å›¾ç‰‡æ¥æºï¼š[ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Transpose) å…³äºè½¬ç½®çš„æ–‡ç« ã€‚
- en: 'So, we can replace *U* by the transpose of *X.* This leads to:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ *X* çš„è½¬ç½®æ¥æ›¿ä»£ *U*ã€‚è¿™å°†å¾—åˆ°ï¼š
- en: '![](../Images/4df291644e59c673764233817454e49c.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4df291644e59c673764233817454e49c.png)'
- en: 'Eq (3): The linear regression coefficients.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ï¼ˆ3ï¼‰ï¼šçº¿æ€§å›å½’ç³»æ•°ã€‚
- en: And we have now found the coefficients of the regression model. If we get a
    new data point, *x_new* (in the form of a row vector) we can take its dot product
    with *ğ›½* and obtain the corresponding *y*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å·²ç»æ‰¾åˆ°äº†å›å½’æ¨¡å‹çš„ç³»æ•°ã€‚å¦‚æœæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªæ–°çš„æ•°æ®ç‚¹ï¼Œ*x_new*ï¼ˆä»¥è¡Œå‘é‡çš„å½¢å¼ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ä¸*ğ›½*è¿›è¡Œç‚¹ç§¯ï¼Œä»è€Œè·å¾—ç›¸åº”çš„*y*ã€‚
- en: '![](../Images/deeca6076c56e7dd972bbdf81b5d664c.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/deeca6076c56e7dd972bbdf81b5d664c.png)'
- en: Where ğ›½ is given by equation (3) above. Now, we provided the motivation for
    replacing *U* by *X^T* since it was an obvious choice with the dimensions we were
    looking for. But, the same formula has a much stronger mathematical motivation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ ğ›½ ç”±ä¸Šè¿°æ–¹ç¨‹ï¼ˆ3ï¼‰ç»™å‡ºã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æä¾›äº†ç”¨*X^T*æ›¿ä»£*U*çš„åŠ¨æœºï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªæ˜¾è€Œæ˜“è§çš„é€‰æ‹©ï¼Œç¬¦åˆæˆ‘ä»¬æ‰€éœ€çš„ç»´åº¦ã€‚ç„¶è€Œï¼Œç›¸åŒçš„å…¬å¼è¿˜æœ‰æ›´å¼ºçš„æ•°å­¦åŠ¨æœºã€‚
- en: III-A) Mathematical motivation
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III-A) æ•°å­¦åŠ¨æœº
- en: We need to motivate a specific value of ğ›½ (the one in equation (3)). So, letâ€™s
    think about what happens for various values of ğ›½. As explained in animation-1,
    section III-A of chapter 3 ([matrix multiplication](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)),
    we can interpret *X*ğ›½as taking apart the column vectors of *X* and combining them
    into a single column vector via a linear combination. More specifically, we multiply
    the first column vector of *X* by the first element of the vector ğ›½, the second
    column vector by the second element and so on and then adding up all the results.
    So, as we change the vector ğ›½, weâ€™re exploring the â€œcolumn spaceâ€ of the matrix
    *X,* which is the set of all vectors we can get via a linear combination of the
    column vectors of *X*. And then we also have the vector *y,* the right hand side
    of our equation (2). Just like the column vectors of *X*, the dimensionality of
    this vector is also *n* (the number of data points/ rows in the matrix).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: The dimensionality of this vector space (that contains *y* and the column vectors
    of *X*) is *n.* On the other hand, the number of column vectors from *X* is *m.*
    And remember, weâ€™re in the case where *n>>m*. So, the column space of *X* (space
    spanned by these *m* vectors) will have dimensionality *m.* This is much lower
    than *n,* the larger space where those column vectors live.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: The vector *y* lives in the same larger space of dimensionality, *n*. Since
    the number of column vectors, *m* is so much smaller than *n*, the vector *y*
    will not be in the column space spanned by the column vectors of *X,* almost surely.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: If it were, weâ€™d have a ğ›½ that satisfies all the equations exactly. This is
    impossible to do (as discussed before). But, we still want to find a ğ›½ that brings
    it as close as possible to the vector, *y*. To do this, we minimize the distance
    between *y* and *X*ğ›½.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s take a concrete example. Say we have a data set with *n=3* data points.
    In our regression model, we choose *m=2* features. The *Xğ›½=y* equation looks like
    this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d0925cc07a7ea9d60383892997d6628.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: So, the matrix *X* has two column vectors, *[1,1,1]* and *[2,5,7]*. They live
    in a 3 dimensional space (*n=3*). These two column vectors are plotted in blue
    in figure-2 below. The space spanned by those vectors (column space) is two dimensional
    (*m=2*) and a section of it is shaded in pink.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9b472f068d83b4bde301f5057de70bd.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: 'Figure-2: Demonstrating linear regression as an exploration of the column space
    of the data matrix, X. Image by author.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, look at the triangle formed by the black vector, grey vector and red vector.
    The black vector is *Xğ›½,* the red vector is *y* and the grey vector is *d*. The
    three form a triangle and hence satisfy:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f21e4a02d2d69b8291133b7a59ae053.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: Itâ€™s this *d* vector whose length we want to find the point in the column space
    of *X* (controlled by ğ›½) that is closest to *y.*
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°è¿™ä¸ª *d* å‘é‡åœ¨ *X* çš„åˆ—ç©ºé—´ï¼ˆç”± ğ›½ æ§åˆ¶ï¼‰ä¸­æœ€æ¥è¿‘ *y* çš„ç‚¹ã€‚
- en: We get the squared length of a column vector by matrix-multiplying its transpose
    with itself.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡å°†åˆ—å‘é‡çš„è½¬ç½®ä¸å…¶è‡ªèº«è¿›è¡ŒçŸ©é˜µä¹˜æ³•æ¥è·å¾—åˆ—å‘é‡çš„å¹³æ–¹é•¿åº¦ã€‚
- en: '![](../Images/c61f481b7cb31beb88bf77f4ba3ec792.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c61f481b7cb31beb88bf77f4ba3ec792.png)'
- en: Finally, letâ€™s take the derivative with respect to *ğ›½* and set it to *0*. This
    will give us the ğ›½ that minimized the squared length of vector *d.*
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè®©æˆ‘ä»¬å¯¹ *ğ›½* å–å¯¼æ•°å¹¶è®¾ç½®ä¸º *0*ã€‚è¿™å°†ç»™å‡ºæœ€å°åŒ–å‘é‡ *d* çš„å¹³æ–¹é•¿åº¦çš„ ğ›½ã€‚
- en: '![](../Images/78bbdad103f792818909ee130e02f5a9.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78bbdad103f792818909ee130e02f5a9.png)'
- en: 'Here, we use equation (78) from the [matrix cook-book](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf),
    [2]. This leads to:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨æ¥è‡ª[çŸ©é˜µå®å…¸](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)çš„æ–¹ç¨‹ï¼ˆ78ï¼‰ï¼Œ[2]ã€‚è¿™å¯¼è‡´ï¼š
- en: '![](../Images/44d119216d0ea765d0504d35161f49b0.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44d119216d0ea765d0504d35161f49b0.png)'
- en: And this is the same as equation (3).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æ–¹ç¨‹ï¼ˆ3ï¼‰æ˜¯ç›¸åŒçš„ã€‚
- en: We motivated here with the column space of *X* (as explained in more detail
    in [1]). The same equations can also be motivated as minimizing the square error
    terms in the predicted and actual values of vector, *y.* This approach is covered
    in [2].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨è¿™é‡Œç”¨ *X* çš„åˆ—ç©ºé—´æ¥æ¿€åŠ±ï¼ˆå¦‚[1]ä¸­æ›´è¯¦ç»†åœ°è§£é‡Šï¼‰ã€‚ç›¸åŒçš„æ–¹ç¨‹ä¹Ÿå¯ä»¥è¢«æ¿€åŠ±ä¸ºæœ€å°åŒ–é¢„æµ‹å’Œå®é™…å€¼å‘é‡ *y* ä¸­çš„å¹³æ–¹è¯¯å·®é¡¹ã€‚è¿™ç§æ–¹æ³•åœ¨[2]ä¸­æ¶µç›–ã€‚
- en: III-B) Online linear regression
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III-B) åœ¨çº¿çº¿æ€§å›å½’
- en: '![](../Images/fc9238e9e85c96c937a424b6170d16bb.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc9238e9e85c96c937a424b6170d16bb.png)'
- en: Online linear regression. Data comes in as a constant stream, day over day.
    We need to keep our linear regression model up to date as that happens. Image
    by midjourney.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨çº¿çº¿æ€§å›å½’ã€‚æ•°æ®ä»¥æ’å®šçš„æµå…¥æ–¹å¼æ¯å¤©è¿›å…¥ã€‚æˆ‘ä»¬éœ€è¦åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ä¿æŒæˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹çš„æœ€æ–°çŠ¶æ€ã€‚å›¾åƒæ¥æºäº midjourneyã€‚
- en: So far, we thought of linear regression as a static model where the data matrix,
    *X* and the corresponding vector of responses, *y* are given to us. Very often,
    we have data coming in as a constant stream over time. Some data points might
    drop today, some more tomorrow and so on. And weâ€™d like our model to use a rolling
    window of data (say 30 days) and the parameters should be updated every day for
    the data from the last 30 days. The obvious way to do it is to use equation (3)
    on the past 30 days of data every day and refresh the parameters on a daily basis.
    But what if the number of data points, *n* (rows in the matrix *X*) is very large?
    Thatâ€™ll make the computation in equation (3) very expensive. And if you think
    about the model yesterday versus today. Most of the data considered by the two
    models is the same because of the rolling window. On day 31, we used the data
    from day 1 to day 30 and on day 32, we used the data from day 2 to day 31\. The
    data from day 2 to day 30 is common. What makes the model for day 32 different
    from the one on day 31 is that it does not consider all the data points that dropped
    on day 1 and does consider the ones that dropped on day 31\. Apart from that,
    the vast majority of the data (days 2 through 29) is common. It seems wasteful
    then to ignore this and train the entire model from scratch everyday. If we could
    reformulate equation (3) in a way that its a sum over some function of the rows
    of *X*, *x_i*, we could keep adding in the contributions of new data as it came
    in while subtracting the contribution from the old data that falls out of the
    rolling window.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æŠŠçº¿æ€§å›å½’çœ‹ä½œä¸€ä¸ªé™æ€æ¨¡å‹ï¼Œå…¶ä¸­æ•°æ®çŸ©é˜µ *X* å’Œå¯¹åº”çš„å“åº”å‘é‡ *y* å·²ç»ç»™å®šã€‚å¾ˆå¸¸è§çš„æ˜¯ï¼Œæ•°æ®ä¼šéšç€æ—¶é—´ä½œä¸ºä¸€ä¸ªæ’å®šçš„æµå…¥ã€‚ä¸€äº›æ•°æ®ç‚¹ä»Šå¤©å¯èƒ½ä¼šä¸¢å¤±ï¼Œæ˜å¤©å¯èƒ½ä¼šæ›´å¤šï¼Œä»¥æ­¤ç±»æ¨ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨ä¸€ä¸ªæ»šåŠ¨çª—å£çš„æ•°æ®ï¼ˆæ¯”å¦‚30å¤©ï¼‰ï¼Œå¹¶ä¸”å‚æ•°æ¯å¤©æ›´æ–°ä¸€æ¬¡ï¼Œé’ˆå¯¹è¿‡å»30å¤©çš„æ•°æ®ã€‚æ˜¾è€Œæ˜“è§çš„æ–¹æ³•æ˜¯æ¯å¤©ä½¿ç”¨è¿‡å»30å¤©çš„æ•°æ®åº”ç”¨æ–¹ç¨‹ï¼ˆ3ï¼‰ï¼Œå¹¶æ¯å¤©åˆ·æ–°å‚æ•°ã€‚ä½†å¦‚æœæ•°æ®ç‚¹çš„æ•°é‡
    *n*ï¼ˆçŸ©é˜µ *X* çš„è¡Œæ•°ï¼‰éå¸¸å¤§å‘¢ï¼Ÿè¿™ä¼šä½¿æ–¹ç¨‹ï¼ˆ3ï¼‰çš„è®¡ç®—å˜å¾—éå¸¸æ˜‚è´µã€‚å¦‚æœä½ è€ƒè™‘æ˜¨å¤©çš„æ¨¡å‹ä¸ä»Šå¤©çš„æ¨¡å‹ã€‚ç”±äºæ»šåŠ¨çª—å£ï¼Œå¤§éƒ¨åˆ†æ•°æ®éƒ½æ˜¯ç›¸åŒçš„ã€‚ç¬¬31å¤©ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç¬¬1å¤©åˆ°ç¬¬30å¤©çš„æ•°æ®ï¼Œè€Œç¬¬32å¤©ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç¬¬2å¤©åˆ°ç¬¬31å¤©çš„æ•°æ®ã€‚ç¬¬2å¤©åˆ°ç¬¬30å¤©çš„æ•°æ®æ˜¯å…±åŒçš„ã€‚ç¬¬32å¤©çš„æ¨¡å‹ä¸ç¬¬31å¤©çš„æ¨¡å‹ä¸åŒä¹‹å¤„åœ¨äºå®ƒä¸è€ƒè™‘ç¬¬1å¤©ä¸¢å¤±çš„æ‰€æœ‰æ•°æ®ç‚¹ï¼Œä½†è€ƒè™‘äº†ç¬¬31å¤©ä¸¢å¤±çš„æ•°æ®ç‚¹ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œç»å¤§å¤šæ•°æ•°æ®ï¼ˆç¬¬2å¤©åˆ°ç¬¬29å¤©ï¼‰æ˜¯å…±åŒçš„ã€‚å› æ­¤ï¼Œå¿½ç•¥è¿™ä¸€ç‚¹å¹¶æ¯å¤©ä»å¤´å¼€å§‹è®­ç»ƒæ•´ä¸ªæ¨¡å‹ä¼¼ä¹æ˜¯æµªè´¹çš„ã€‚å¦‚æœæˆ‘ä»¬å¯ä»¥å°†æ–¹ç¨‹ï¼ˆ3ï¼‰é‡æ–°è¡¨è¿°ä¸ºå¯¹
    *X* çš„è¡Œ *x_i* çš„æŸäº›å‡½æ•°çš„æ±‚å’Œï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ–°æ•°æ®åˆ°æ¥æ—¶ä¸æ–­å¢åŠ è´¡çŒ®ï¼ŒåŒæ—¶å‡å»æ‰å‡ºæ»šåŠ¨çª—å£çš„æ—§æ•°æ®çš„è´¡çŒ®ã€‚
- en: 'And one of the many interpretations of matrix multiplication we covered in
    chapter 3 helps us do just that. In section III-B of [chapter 3](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)
    (around animation 5) the interpretation of matrix multiplication as a sum of outer
    products of the rows of the two matrices is covered. Using that, we can square
    the matrix on the left of equation (3) as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ 3 ç« ä¸­æˆ‘ä»¬è®¨è®ºçš„çŸ©é˜µä¹˜æ³•çš„ä¼—å¤šè§£é‡Šä¹‹ä¸€å¯ä»¥å¸®åŠ©æˆ‘ä»¬åšåˆ°è¿™ä¸€ç‚¹ã€‚åœ¨[ç¬¬ 3 ç« ](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)
    çš„ III-B èŠ‚ï¼ˆå¤§çº¦åŠ¨ç”» 5ï¼‰ä¸­ï¼Œä»‹ç»äº†å°†çŸ©é˜µä¹˜æ³•è§£é‡Šä¸ºä¸¤ä¸ªçŸ©é˜µè¡Œçš„å¤–ç§¯ä¹‹å’Œã€‚åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ–¹ç¨‹ (3) å·¦ä¾§çš„çŸ©é˜µå¹³æ–¹ä¸ºï¼š
- en: '![](../Images/63d949b23d420e4f65cf1fe5193d7e5c.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63d949b23d420e4f65cf1fe5193d7e5c.png)'
- en: Eq (4) Image by author
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ (4) å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Here, the vector *x_i* is the *i*-th row of the matrix *X.* It has one row and
    *m* columns (*1*â¨‰*m).*
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œå‘é‡ *x_i* æ˜¯çŸ©é˜µ *X* çš„ç¬¬ *i* è¡Œã€‚å®ƒæœ‰ä¸€è¡Œå’Œ *m* åˆ— (*1*â¨‰*m*)ã€‚
- en: 'Similarly, the vector on the left side of that equation can be written as:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œé‚£ä¸ªæ–¹ç¨‹å·¦ä¾§çš„å‘é‡å¯ä»¥å†™æˆï¼š
- en: '![](../Images/4a4efd17ea90a82712d09b9b413d8494.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a4efd17ea90a82712d09b9b413d8494.png)'
- en: Eq (5) Image by author
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ (5) å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Since both the terms have been expressed as sums over the *n* data points, we
    can add and remove the contributions from individual data points at will. We can
    keep running versions of the matrix in equation (4) and vector in equation (5)
    based on the data points that fall in the last 7 days. If a data point falls outside
    the 30 day window, we can subtract the terms it corresponds to from both the equations
    and if a new data point comes in, we can add its terms to both the equations.
    And since *m* is small, we can efficiently calculate ğ›½, keeping it up to date
    every time we update those terms. This method can also be used to add weights
    to the data points. This can be useful if (for example), youâ€™re getting data from
    multiple sources and want to weigh some of them more than others. Youâ€™d just multiply
    the weight, *w_i* for the *i*-th data point into each of the terms of equations
    (4) and (5).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™ä¸¤ä¸ªé¡¹å·²ç»ä»¥ *n* ä¸ªæ•°æ®ç‚¹çš„æ€»å’Œè¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥éšæ„åœ°æ·»åŠ æˆ–åˆ é™¤æ¥è‡ªå•ä¸ªæ•°æ®ç‚¹çš„è´¡çŒ®ã€‚æˆ‘ä»¬å¯ä»¥åŸºäºè¿‡å» 7 å¤©å†…çš„æ•°æ®ç‚¹ï¼Œä¿æŒæ–¹ç¨‹ (4) ä¸­çš„çŸ©é˜µå’Œæ–¹ç¨‹
    (5) ä¸­çš„å‘é‡çš„ä¸æ–­æ›´æ–°ã€‚å¦‚æœæŸä¸ªæ•°æ®ç‚¹è¶…å‡ºäº† 30 å¤©çš„çª—å£ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸¤ä¸ªæ–¹ç¨‹ä¸­å‡å»å®ƒå¯¹åº”çš„é¡¹ï¼Œå¦‚æœæœ‰æ–°çš„æ•°æ®ç‚¹è¿›å…¥ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶é¡¹æ·»åŠ åˆ°ä¸¤ä¸ªæ–¹ç¨‹ä¸­ã€‚è€Œä¸”ç”±äº
    *m* å¾ˆå°ï¼Œæˆ‘ä»¬å¯ä»¥é«˜æ•ˆåœ°è®¡ç®— ğ›½ï¼Œæ¯æ¬¡æ›´æ–°è¿™äº›é¡¹æ—¶ä¿æŒå…¶æœ€æ–°ã€‚è¿™ç§æ–¹æ³•ä¹Ÿå¯ä»¥ç”¨æ¥ä¸ºæ•°æ®ç‚¹æ·»åŠ æƒé‡ã€‚å¦‚æœï¼ˆä¾‹å¦‚ï¼‰ä½ ä»å¤šä¸ªæ¥æºè·å–æ•°æ®å¹¶å¸Œæœ›å¯¹å…¶ä¸­ä¸€äº›è¿›è¡ŒåŠ æƒï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚ä½ åªéœ€å°†ç¬¬
    *i* ä¸ªæ•°æ®ç‚¹çš„æƒé‡ *w_i* ä¹˜åˆ°æ–¹ç¨‹ (4) å’Œ (5) çš„æ¯ä¸€é¡¹ä¸­ã€‚
- en: IV) Neural networks
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IV) ç¥ç»ç½‘ç»œ
- en: '![](../Images/90a8651ed207be30f96827b8cacb9991.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90a8651ed207be30f96827b8cacb9991.png)'
- en: Neural network architectures consist of layers of vectors. The first layer is
    the input layer, the last layer is the output layer and everything in between
    is a hidden layer. Image by midjourney.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œæ¶æ„ç”±å‘é‡å±‚ç»„æˆã€‚ç¬¬ä¸€å±‚æ˜¯è¾“å…¥å±‚ï¼Œæœ€åä¸€å±‚æ˜¯è¾“å‡ºå±‚ï¼Œä¸­é—´çš„æ‰€æœ‰å±‚éƒ½æ˜¯éšè—å±‚ã€‚å›¾ç‰‡ç”± Midjourney æä¾›ã€‚
- en: Neural networks are machine learning models that are inspired by the neural
    connections in biological brains. They are the current weapon of choice in our
    quest towards artificial general intelligence. All recent advances, from text
    to image to conversation bots use models that are neural networks at their core.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œæ˜¯å—ç”Ÿç‰©å¤§è„‘ç¥ç»è¿æ¥å¯å‘çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å®ƒä»¬æ˜¯æˆ‘ä»¬è¿½æ±‚äººå·¥é€šç”¨æ™ºèƒ½çš„å½“å‰é€‰æ‹©æ­¦å™¨ã€‚æ‰€æœ‰è¿‘æœŸçš„è¿›å±•ï¼Œä»æ–‡æœ¬åˆ°å›¾åƒå†åˆ°å¯¹è¯æœºå™¨äººï¼Œéƒ½ä½¿ç”¨äº†ä»¥ç¥ç»ç½‘ç»œä¸ºæ ¸å¿ƒçš„æ¨¡å‹ã€‚
- en: Linear regression can be thought of as the simplest possible neural network.
    Letâ€™s concern ourselves with inference for now, the process of obtaining the output
    vector given an instance of the input to the model. For linear regression, weâ€™ll
    get a row vector, *x* as input and a single scalar value, *y* as output. The parameter
    vector, ğ›½ takes us from the input to the output.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§å›å½’å¯ä»¥è¢«çœ‹ä½œæ˜¯æœ€ç®€å•çš„ç¥ç»ç½‘ç»œã€‚ç°åœ¨æˆ‘ä»¬å…³æ³¨æ¨æ–­ï¼Œå³åœ¨ç»™å®šæ¨¡å‹è¾“å…¥å®ä¾‹çš„æƒ…å†µä¸‹è·å¾—è¾“å‡ºå‘é‡çš„è¿‡ç¨‹ã€‚å¯¹äºçº¿æ€§å›å½’ï¼Œæˆ‘ä»¬å°†è·å¾—ä¸€ä¸ªè¡Œå‘é‡ *x* ä½œä¸ºè¾“å…¥å’Œä¸€ä¸ªå•ä¸€çš„æ ‡é‡å€¼
    *y* ä½œä¸ºè¾“å‡ºã€‚å‚æ•°å‘é‡ ğ›½ å°†è¾“å…¥è½¬æ¢ä¸ºè¾“å‡ºã€‚
- en: '![](../Images/9e1f1c9dbebdc8c1dda272454bb725ab.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e1f1c9dbebdc8c1dda272454bb725ab.png)'
- en: In linear regression, we get a vector corresponding to a new data point, x_j.
    This is multiplied with the parameter vector, beta to produce the response, y_j
    which is a single scalar. Image by author.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªå¯¹åº”äºæ–°æ•°æ®ç‚¹ x_j çš„å‘é‡ã€‚è¿™ä¸å‚æ•°å‘é‡ beta ç›¸ä¹˜ä»¥äº§ç”Ÿå“åº” y_jï¼Œå®ƒæ˜¯ä¸€ä¸ªå•ä¸€çš„æ ‡é‡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: To extend this to neural networks, we generalize in two ways.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å°†å…¶æ‰©å±•åˆ°ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬ä»¥ä¸¤ç§æ–¹å¼è¿›è¡Œæ¦‚æ‹¬ã€‚
- en: First, in order for the model to be able to output all kinds of interesting
    things like images, sentences, videos and spaceships, we need it to output not
    just a scalar (like with linear regression), but a vector. If this vector is high
    dimensional enough, any complex response we expect can be embedded effectively
    in its corresponding vector space.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä¸ºäº†ä½¿æ¨¡å‹èƒ½å¤Ÿè¾“å‡ºå„ç§æœ‰è¶£çš„å†…å®¹ï¼Œå¦‚å›¾åƒã€å¥å­ã€è§†é¢‘å’Œå®‡å®™é£èˆ¹ï¼Œæˆ‘ä»¬éœ€è¦å®ƒè¾“å‡ºçš„ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ ‡é‡ï¼ˆå¦‚çº¿æ€§å›å½’ï¼‰ï¼Œè€Œæ˜¯ä¸€ä¸ªå‘é‡ã€‚å¦‚æœè¿™ä¸ªå‘é‡çš„ç»´åº¦è¶³å¤Ÿé«˜ï¼Œæˆ‘ä»¬æœŸæœ›çš„ä»»ä½•å¤æ‚å“åº”éƒ½å¯ä»¥æœ‰æ•ˆåœ°åµŒå…¥å…¶ç›¸åº”çš„å‘é‡ç©ºé—´ä¸­ã€‚
- en: 'So we need to go from vector->scalar for the case of linear regression to vector->vector.
    The scalar output is then just a special case, since its a one dimensional vector.
    This changes the picture above to this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°†çº¿æ€§å›å½’çš„å‘é‡->æ ‡é‡æƒ…å†µæ”¹ä¸ºå‘é‡->å‘é‡ã€‚æ ‡é‡è¾“å‡ºåªæ˜¯ä¸€ä¸ªç‰¹ä¾‹ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªä¸€ç»´å‘é‡ã€‚è¿™å°†ä¸Šè¿°å›¾åƒæ›´æ”¹ä¸ºï¼š
- en: '![](../Images/0f8b0d9706704844d15d87a110a11969.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f8b0d9706704844d15d87a110a11969.png)'
- en: In general, weâ€™d like the response, y to be a vector instead of a scalar. That
    way, we can embed complex information about the real world in that vector space.
    Image by author.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›å“åº” `y` æ˜¯ä¸€ä¸ªå‘é‡è€Œä¸æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é‚£ä¸ªå‘é‡ç©ºé—´ä¸­åµŒå…¥å…³äºç°å®ä¸–ç•Œçš„å¤æ‚ä¿¡æ¯ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Now, the parameter vector, ğ›½ from before will need to become a parameter matrix.
    This is a linear map that maps the input vector to the output vector.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¹‹å‰çš„å‚æ•°å‘é‡ ğ›½ å°†éœ€è¦å˜æˆå‚æ•°çŸ©é˜µã€‚è¿™æ˜¯ä¸€ä¸ªçº¿æ€§æ˜ å°„ï¼Œå°†è¾“å…¥å‘é‡æ˜ å°„åˆ°è¾“å‡ºå‘é‡ã€‚
- en: But, most interesting relationships in the real world are non-linear. To accommodate
    this, we first insert a bunch of intermediate â€œhidden layersâ€, shown in blue below.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œç°å®ä¸–ç•Œä¸­å¤§å¤šæ•°æœ‰è¶£çš„å…³ç³»éƒ½æ˜¯éçº¿æ€§çš„ã€‚ä¸ºäº†é€‚åº”è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆæ’å…¥äº†ä¸€å †ä¸­é—´çš„â€œéšè—å±‚â€ï¼Œå¦‚ä¸‹å›¾ä¸­çš„è“è‰²éƒ¨åˆ†æ‰€ç¤ºã€‚
- en: '![](../Images/f8be912112e18df8d6db0f6ff35087ae.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8be912112e18df8d6db0f6ff35087ae.png)'
- en: Architecture of a deep neural network. The hidden layers in blue are intermediate
    layers of the model. Image by author.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦ç¥ç»ç½‘ç»œçš„ç»“æ„ã€‚å›¾ä¸­çš„è“è‰²éšè—å±‚æ˜¯æ¨¡å‹çš„ä¸­é—´å±‚ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Adding the layers in this way doesnâ€™t change anything on its own. Its still
    equivalent to the original model without any hidden layers. To see this, note
    that the parameter matrices, *B_1, B_2, B_3, â€¦* in the figure below just multiply
    together and become yet another parameter matrix.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥è¿™ç§æ–¹å¼æ·»åŠ å±‚æœ¬èº«ä¸ä¼šæ”¹å˜ä»»ä½•ä¸œè¥¿ã€‚å®ƒä»ç„¶ç­‰åŒäºæ²¡æœ‰ä»»ä½•éšè—å±‚çš„åŸå§‹æ¨¡å‹ã€‚è¦çœ‹åˆ°è¿™ä¸€ç‚¹ï¼Œè¯·æ³¨æ„ï¼Œä¸‹é¢å›¾ä¸­çš„å‚æ•°çŸ©é˜µ *B_1, B_2, B_3, â€¦*
    åªæ˜¯ç›¸ä¹˜å¹¶å˜æˆå¦ä¸€ä¸ªå‚æ•°çŸ©é˜µã€‚
- en: '*B = (B_1\. B_2\. B_3.B_4)*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*B = (B_1\. B_2\. B_3.B_4)*'
- en: But this one simple trick takes this method from being able to approximate only
    linear maps arbitrarily well to being able to approximate *any* maps arbitrarily
    well. To make the hidden layers worth our trouble, we add a simple, element-wise
    non-linearity at every layer, *f*. This is a simple, one dimensional function,
    *f* that takes a scalar as input and returns a scalar as output. We simply apply
    *f* to every element of the vector at each layer *before* multiplying by the next
    parameter matrix, *B_j*. A popular choice of this *f* is the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™ä¸ªç®€å•çš„æŠ€å·§ä½¿å¾—è¿™ç§æ–¹æ³•ä»åªèƒ½ä»»æ„é€¼è¿‘çº¿æ€§æ˜ å°„å˜æˆå¯ä»¥ä»»æ„é€¼è¿‘*ä»»ä½•*æ˜ å°„ã€‚ä¸ºäº†ä½¿éšè—å±‚å€¼å¾—æˆ‘ä»¬è´¹å¿ƒï¼Œæˆ‘ä»¬åœ¨æ¯ä¸€å±‚æ·»åŠ ä¸€ä¸ªç®€å•çš„é€å…ƒç´ éçº¿æ€§å‡½æ•° *f*ã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ä¸€ç»´å‡½æ•°
    *f*ï¼Œå®ƒæ¥å—ä¸€ä¸ªæ ‡é‡ä½œä¸ºè¾“å…¥å¹¶è¿”å›ä¸€ä¸ªæ ‡é‡ä½œä¸ºè¾“å‡ºã€‚æˆ‘ä»¬åªéœ€åœ¨ä¹˜ä»¥ä¸‹ä¸€ä¸ªå‚æ•°çŸ©é˜µ *B_j* ä¹‹å‰ï¼Œå°† *f* åº”ç”¨äºå‘é‡çš„æ¯ä¸ªå…ƒç´ ã€‚è¿™ä¸ª *f* çš„ä¸€ä¸ªæµè¡Œé€‰æ‹©æ˜¯
    [Sigmoid å‡½æ•°](https://en.wikipedia.org/wiki/Sigmoid_function)ã€‚
- en: The [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
    [4], says that this kind of architecture can approximate any mapping between two
    vector spaces (linear or non-linear) arbitrarily well.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[é€šç”¨é€¼è¿‘å®šç†](https://en.wikipedia.org/wiki/Universal_approximation_theorem) [4]
    è¯´ï¼Œè¿™ç§æ¶æ„å¯ä»¥ä»»æ„é€¼è¿‘ä¸¤ä¸ªå‘é‡ç©ºé—´ä¹‹é—´çš„ä»»ä½•æ˜ å°„ï¼ˆçº¿æ€§æˆ–éçº¿æ€§ï¼‰ã€‚'
- en: V) Conclusion
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: V) ç»“è®º
- en: The humble matrix multiplication is an unimaginably powerful tool. Youâ€™ll find
    it in the simplest of models as well as the most complex, cutting edge ones. In
    this chapter, we went over aspects of some simple models that have matrix multiplication
    as their core engine. In the further chapters of this book, we will explore more
    linear algebra concepts, highlighting their role in modern AI models.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: è°¦é€Šçš„çŸ©é˜µä¹˜æ³•æ˜¯ä¸€ç§æå…¶å¼ºå¤§çš„å·¥å…·ã€‚ä½ ä¼šåœ¨æœ€ç®€å•çš„æ¨¡å‹ä»¥åŠæœ€å¤æ‚ã€æœ€å‰æ²¿çš„æ¨¡å‹ä¸­æ‰¾åˆ°å®ƒã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†ä¸€äº›ä»¥çŸ©é˜µä¹˜æ³•ä¸ºæ ¸å¿ƒå¼•æ“çš„ç®€å•æ¨¡å‹ã€‚åœ¨æœ¬ä¹¦çš„åç»­ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢æ›´å¤šçº¿æ€§ä»£æ•°æ¦‚å¿µï¼Œçªå‡ºå®ƒä»¬åœ¨ç°ä»£
    AI æ¨¡å‹ä¸­çš„ä½œç”¨ã€‚
- en: If you enjoyed the article, buy me a coffee :) [https://www.buymeacoffee.com/w045tn0iqw](https://www.buymeacoffee.com/w045tn0iqw)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œç»™æˆ‘ä¹°æ¯å’–å•¡å§ :) [https://www.buymeacoffee.com/w045tn0iqw](https://www.buymeacoffee.com/w045tn0iqw)
- en: References
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] A beautiful way of looking at linear algebra: [https://medium.com/towards-data-science/a-beautiful-way-of-looking-at-linear-regressions-a4df174cdce](https://medium.com/towards-data-science/a-beautiful-way-of-looking-at-linear-regressions-a4df174cdce)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Deriving linear regressions from least squares: [https://stats.stackexchange.com/questions/46151/how-to-derive-the-least-square-estimator-for-multiple-linear-regression](https://stats.stackexchange.com/questions/46151/how-to-derive-the-least-square-estimator-for-multiple-linear-regression)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Matrix cook-book: [https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Universal approximation theorem: [https://en.wikipedia.org/wiki/Universal_approximation_theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
