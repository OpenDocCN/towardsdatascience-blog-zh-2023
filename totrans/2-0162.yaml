- en: 'A Bird’s Eye View of Linear Algebra: Systems of Equations, Linear Regression
    and Neural Networks'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-systems-of-equations-linear-regression-and-neural-networks-fe5b88a57f66](https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-systems-of-equations-linear-regression-and-neural-networks-fe5b88a57f66)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The humble matrix multiplication along with its inverse is almost exclusively
    what’s going on in many simple ML models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rohitpandey576?source=post_page-----fe5b88a57f66--------------------------------)[![Rohit
    Pandey](../Images/af817d8f68f2984058f0afb8fd7ecbe9.png)](https://medium.com/@rohitpandey576?source=post_page-----fe5b88a57f66--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe5b88a57f66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe5b88a57f66--------------------------------)
    [Rohit Pandey](https://medium.com/@rohitpandey576?source=post_page-----fe5b88a57f66--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe5b88a57f66--------------------------------)
    ·18 min read·Dec 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c6e139e72e16ccb0e834afec721d153.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Image by midjourney
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the fourth chapter of the in-progress book on linear algebra, “A birds
    eye view of linear algebra”. The table of contents so far:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter-1: The basics](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-the-basics-29ad2122d98f)'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter-2: [The measure of a map — determinants](https://medium.com/p/1e5fd752a3be)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter-3:[Why is matrix multiplication the way it is?](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter-4 (current): Systems of equations, linear regression and neural networks'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter-5: [Rank nullity and why row rank == col rank](/a-birds-eye-view-of-linear-algebra-rank-nullity-and-why-row-rank-equals-column-rank-bc084e0e1075)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All images in this blog, unless otherwise stated, are by the author.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: I) Introduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern AI models leverage high dimensional vector spaces to encode information.
    And ***the*** tool we have for reasoning about high dimensional spaces and mappings
    between them is linear algebra.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: And within that field, matrix multiplication (along with its inverse) is literally
    all you need to build many simple machine learning models end to end. Which is
    why spending the time to understand it really well is a great investment. And
    this is what we did in [chapter 3](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: These simple models, useful in their own right, form the building blocks of
    more complex ML and AI models with state of the art performance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: We’ll cover a few of these applications (from linear regression to elementary
    neural networks) in this chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将介绍一些这些应用（从线性回归到初级神经网络）。
- en: But first, we need to go to the simplest case in the simplest model — when the
    number of data points equals the number of model parameters. The case of solving
    a system of linear equations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，我们需要转到最简单的情况和最简单的模型——当数据点的数量等于模型参数的数量时，即求解线性方程组的情况。
- en: II) Systems of linear equations
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: II) 线性方程组
- en: We have finally arrived (in the context of this book) at the heart of linear
    algebra. Solving systems of linear equations is how we discovered linear algebra
    in the first place and the motivations for most concepts in this field have deep
    roots in this application.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于来到了线性代数的核心（在本书的背景下）。求解线性方程组是我们最初发现线性代数的方式，这个领域的大多数概念的动机在这个应用中有着深远的根基。
- en: Let’s start simple and one dimensional. The concept of division is rooted in
    one dimensional linear equations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单的一维情况开始。除法的概念根植于一维线性方程中。
- en: '*ax = b*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*ax = b*'
- en: 'The equation reads, “what number when multiplied by *a* leads to *b*”. And
    the solution is what defines scalar division:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程的意思是，“哪个数字在乘以*a*后得到*b*”。解决方案定义了标量除法：
- en: '*x = b/a* _(1)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*x = b/a* _(1)'
- en: This is with one dimension, *x*. Most interesting things happen when things
    go multi-dimensional. So instead of just one variable, *x*, we have *n* variables
    (*x_1, x_2, x_3, …, x_n*).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在一维的情况下，即*x*。当进入多维时，最有趣的事情发生了。所以我们不仅有一个变量*x*，还有*n*个变量（*x_1, x_2, x_3, …, x_n*）。
- en: And as soon as you go multi-dimensional, linear algebra jumps into the scene.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦进入多维空间，线性代数就会出现。
- en: '![](../Images/2f774d782fa426023a1b05562e31b18e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f774d782fa426023a1b05562e31b18e.png)'
- en: When things get multi-dimensional, linear algebra makes an entry. Image by midjourney.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当问题变得多维时，线性代数就会出现。图像来自midjourney。
- en: Now, instead of *a.x = b,* our equation should include the *n* variables, *x_1,
    x_2, x_3, …, x_n.* And just like *x* had the coefficient, *a,* before; each of
    the *x_i*’s gets its own coefficient, *a_i* this time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的方程应包括*n*个变量，即*x_1, x_2, x_3, …, x_n*。就像*x*之前有系数*a*一样，这次每个*x_i*都有自己的系数*a_i*。
- en: '![](../Images/85b15880feff61042a6d27878e9c2b6a.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85b15880feff61042a6d27878e9c2b6a.png)'
- en: 'But unlike in the one dimensional case, this one equation is not enough to
    solve for the *x*’s since it doesn’t uniquely specify them. For example, we can
    pick any values we want for *x_2, x_3,…, x_n* (ex: all 0’s) and only then will
    we have a unique value for *x_1\.* If we already knew the values of *x_1, x_2,
    …, x_n* and wanted to communicate them in the form of a system of equations, they
    would look something like:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但与一维情况不同，这个方程不足以求解*x*，因为它不能唯一地确定*x*。例如，我们可以随意选择*x_2, x_3,…, x_n*（例如：全部为0），只有这样我们才能得到唯一的*x_1*值。如果我们已经知道了*x_1,
    x_2, …, x_n*的值，并且想以方程组的形式传达它们，它们会看起来像这样：
- en: '![](../Images/bac0590dc865ba0985cf0515d38be4d3.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bac0590dc865ba0985cf0515d38be4d3.png)'
- en: So, we needed *n* equations (equal to the number of variables). Any general
    system can now be created by “mixing” the equations above by taking linear combinations
    of them. This allows adding any two equations and multiplying any of the equations
    by any scalar. These operations will obviously not change the solution of the
    system (or the question of its existence).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要*n*个方程（等于变量的数量）。现在可以通过“混合”上述方程来创建任何一般系统，即取它们的线性组合。这允许添加任意两个方程，并将任意方程乘以任意标量。这些操作显然不会改变系统的解（或其存在性问题）。
- en: 'For example, we can add three times the second equation to the first equation
    and get:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将第二个方程乘以三加到第一个方程中，得到：
- en: '![](../Images/d8f833bb4004d7c74524b1038ba05380.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8f833bb4004d7c74524b1038ba05380.png)'
- en: Then we can replace the first equation (*x_1=4.6*) by this one and the resulting
    system will still have the same solution (because we can just undo the change
    we made by subtracting three times the second equation from the new first equation).
    If we “mix” in this way very throughly, replacing one of the equations at random
    with the “mixed” one and repeat many times, we’ll end up with *n* equations each
    of which involves multiples of all the *n* variables.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以用这个方程替换第一个方程（*x_1=4.6*），得到的方程组仍然有相同的解（因为我们可以通过从新的第一个方程中减去三倍的第二个方程来撤销我们所做的更改）。如果我们以这种方式非常彻底地“混合”，随机替换其中一个方程，并重复多次，我们将得到*n*个方程，每个方程涉及所有*n*个变量的倍数。
- en: '![](../Images/4ee12009646ef9d2cba54bbb94bbc80c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ee12009646ef9d2cba54bbb94bbc80c.png)'
- en: 'The coefficients *a_{ij}* look an awfully lot like the elements of a square
    matrix. And indeed, from what we learnt about matrix vector multiplication in
    chapter 3 (see animation-1 in section III-A), we can express the system of equations
    as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 系数 *a_{ij}* 看起来非常像一个方阵的元素。确实，根据我们在第3章中学到的矩阵向量乘法（见III-A节中的动画-1），我们可以将方程组表示为：
- en: '![](../Images/6f6ee90f99fcf1a6e9349e9a75b502e8.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f6ee90f99fcf1a6e9349e9a75b502e8.png)'
- en: Eq (1) Matrix form of the system of linear equations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（1）线性方程组的矩阵形式。
- en: And just like in equation (1) we took the multiplicative inverse to get the
    scalar variable, *x (x=b/a)*, here we take the inverse of matrix multiplication
    to get the vector, *x*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在方程（1）中我们取了乘法逆来得到标量变量 *x (x=b/a)* 一样，这里我们取矩阵乘法的逆来得到向量 *x*。
- en: '![](../Images/df7466a98205a67f83917570fcf70642.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df7466a98205a67f83917570fcf70642.png)'
- en: And we have reached the iconic equation where it all started. Where linear algebra
    began. The system of linear equations. There is a whole science behind calculating
    the inverse and doing it well which we will cover in subsequent chapters.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经达到了一切开始的标志性方程。线性代数的起点。线性方程组。计算逆矩阵及其相关操作有一个完整的科学背景，我们将在后续章节中讨论。
- en: Note that the number of rows in the *A* matrix was the number of equations in
    the system and the number of columns was the number of variables.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*A* 矩阵的行数是系统中的方程数，列数是变量数。
- en: 'Geometrically, each equation is a hyperplane in the space with one lower dimensionality.
    Take the system of equations below:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何上讲，每个方程在空间中都是一个维度降低的超平面。考虑以下方程组：
- en: '![](../Images/7fea6e24adffbec2cf05a7a6a1e86c97.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fea6e24adffbec2cf05a7a6a1e86c97.png)'
- en: The solution to this system of equations is *(x=0, y=0, z=1)*. Since there are
    three variables in this system (*x, y* and *z*), the vector space is three dimensional
    as shown in figure-1 below. Refer to that figure for the proceeding discussion.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程组的解是*(x=0, y=0, z=1)*。由于这个方程组中有三个变量（*x, y* 和 *z*），因此矢量空间是三维的，如下图-1所示。请参阅该图以获取后续讨论。
- en: The *x=0* equation corresponds to the yellow hyper-plane in this 3-d space.
    Since the equation represents the addition of one constraint, the dimensionality
    of the space where this equation is satisfied goes down by one (*3–1=2)*. Similarly,
    the *y=0* equation corresponds to the blue hyperplane, also two dimensional. Now
    that we have two equations, so two constraints. The dimensionality of the sub-space
    where both of them are satisfied will go down by two, making it *3–2=1* dimensional.
    A one dimensional sub-space is just a line, and indeed we get the green line figure-1\.
    Finally, when we add the third equation, *x+y+z=1*, represented by the pink plane.
    We now have three equations/ constraints. And they restrict the dimensionality
    of the 3-d space by 3\. So we’re left with *3–3 = 0* dimensions (which is a point)
    and indeed, we get the red point, the only element of the vector space that satisfies
    all three of the equations in the system simultaneously.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*x=0* 方程对应于这个三维空间中的黄色超平面。由于该方程表示增加一个约束，满足该方程的空间的维度降低一个（*3–1=2*）。类似地，*y=0* 方程对应于蓝色超平面，也二维。现在我们有两个方程，因此两个约束。两个方程都满足的子空间的维度将降低两个，变成*3–2=1*维。一维子空间就是一条直线，实际上我们得到的是绿色直线图-1。最后，当我们加入第三个方程
    *x+y+z=1*，它由粉色平面表示。现在我们有三个方程/约束。它们将三维空间的维度限制为3。因此，我们剩下的维度是*3–3=0*（即一个点），确实，我们得到红色点，它是同时满足系统中所有三个方程的唯一矢量空间元素。'
- en: '![](../Images/9eaac84a66add4954150e439f1952df8.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9eaac84a66add4954150e439f1952df8.png)'
- en: 'Figure-1: A system of equations in 3 variables. The space is three dimensional.
    Each equation carves out a hyper-plane one dimension lower. Combinations of two
    equations remove two dimensions and so on. Image by author.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图-1：三变量的方程组。空间是三维的。每个方程切割出一个低一维的超平面。两个方程的组合去除两个维度，依此类推。图像由作者提供。
- en: 'In the example above, we had *n* equations and *n* variables. In general, they
    needn’t be the same. Let’s say there are *n* equations and *m* variables. It''s
    clear from the figure above:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们有 *n* 个方程和 *n* 个变量。一般来说，它们不一定相同。假设有 *n* 个方程和 *m* 个变量。从上面的图可以清楚地看出：
- en: When *n<m*, we have more variables than equations and the system has infinite
    solutions. Think of equations as requirements. Not enough requirements specified,
    tons of possible solutions.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When *n>m*, we’ll have more equations than variables. There are now too many
    constraints to satisfy and we won’t be able to find any points in the vector space
    that satisfy all of them.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So when *n=m*, we should always have a single, unique solution? Not quite, there
    are ways things can go wrong (for all three cases).
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: II-A) How things go wrong
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Consistency**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: One thing that will make any system have no solution is inconsistency. Say the
    first equation is *2x+y=3* and the second one is 2*x+y = 5*. Now, regardless of
    how many more equations we add, there will be no way to satisfy both of them simultaneously
    (they simply contradict each other). Any system that has those two equations is
    inconsistent.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '**Dependency**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Next, the system could deceive us, looking like it has more equations than it
    actually does. The most obvious way this could happen is if we simply copy one
    of the equations. This will add one equation to the system, making it so that
    technically, we have *m+1* equations now. It’s clear, however, that we didn’t
    really add any new information. The new equation we added is redundant. This is
    called a dependent system because some of our equations don’t bring any new information
    of their own but “depend” on others. If we downright plagiarize one of the equations,
    its going to be obvious to spot. But the same thing can be done in a more clandestine
    way. Instead of downright copying one of the equations, we can take a linear combination
    of a few of them to create a new one. The linear combination can be done in a
    way that it is almost impossible to spot just by looking at the equations that
    there is an “imposter equation” in the mix. Of course, once such a dependent equation
    is added into the mix, it is impossible to tell which one the “imposter” is, like
    the three spider men below.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/caf534a1f63b31cf151d423005032d8d.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Two Bob’s. But there can be only one. One of them is an imposter. Hard to tell
    which one. Much like dependent equations introduced in a linear system by taking
    linear combinations of the others. Image by midjourney.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The two problems of consistency and dependency manifest in the same way as far
    as the *A* matrix (from equation (1)) goes. The vector *b* then determines whether
    the system is inconsistent (no solution) or dependent (infinitely many solutions).
    What happens is that some of the rows of *A* turn out to be linear combinations
    of the others.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: II-B) The case of data analysis
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The case that is most interesting to us is building models on top of data. If
    you have data points collected independently, with an underlying random process
    (like data collection should be), it's almost certain that the rows of your data
    matrix won’t be linearly dependent (in probability terms, “[almost surely](https://en.wikipedia.org/wiki/Almost_surely)”).
    So, we don’t have to worry about getting either an inconsistent or dependent system.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们最感兴趣的情况是建立在数据之上的模型。如果你有独立收集的数据点，并且数据收集过程是随机的（就像数据收集应该是的那样），几乎可以肯定你的数据矩阵的行不会线性相关（在概率术语中，"[几乎肯定](https://en.wikipedia.org/wiki/Almost_surely)"）。所以，我们不必担心得到一个不一致或相关的系统。
- en: Also, we typically have many more rows (data points) than columns (variables/
    features). So, the matrix will be “skinny”, with *n>m*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们通常有比列数（变量/特征）多得多的行（数据点）。所以，矩阵将是“瘦长的”，其中 *n>m*。
- en: '![](../Images/8f8a0257f0933335933906c026e2a415.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f8a0257f0933335933906c026e2a415.png)'
- en: A tall, skinny rectangular matrix of data with more rows than columns. Image
    by midjourney.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个高而瘦的矩形数据矩阵，行数多于列数。图片由 midjourney 提供。
- en: We’re firmly in the domain of no solution to the system of equations. If we
    choose any *m* of the *n* equations and delete/ ignore the rest, then we will
    be back to the *m=m* case with an equal number of equations and variables and
    now there will be a unique solution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正处于无解方程组的领域。如果我们选择任何 *m* 个 *n* 个方程并删除/忽略其余的方程，那么我们将回到 *m=m* 的情况，这时方程数和变量数相等，现在将会有一个唯一的解。
- en: So, there are a total of (*n* choose *m)* points the hyperplanes corresponding
    to the equations in the system form. None of these points will lie on all the
    hyperplanes simultaneously.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，超平面对应于系统中方程的总数是 (*n* 选择 *m*) 个点。这些点中没有一个点会同时位于所有超平面上。
- en: Even though there is no point that simultaneously satisfies all the equations,
    we can still ask “what is the point in the entire vector space that comes closest
    to satisfying all the equations”. And this is where linear regression comes in.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管没有一个点同时满足所有方程，我们仍然可以问：“在整个向量空间中哪个点最接近满足所有方程”。这就是线性回归的作用所在。
- en: '**III) Linear regression**'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**III) 线性回归**'
- en: '![](../Images/9b61ce9e94a1b0d7305e2e036150475e.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b61ce9e94a1b0d7305e2e036150475e.png)'
- en: Linear regression draws a linear hyperplane that is closest to the points in
    your data. Much like the ninja attempts to swing his sword in a way that its closest
    to as many leaves as possible. Image by midjourney.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归绘制一个最接近你数据点的线性超平面。就像忍者尝试以一种尽可能接近最多叶子的方式挥动他的剑一样。图片由 midjourney 提供。
- en: 'First, in the context of linear regression, the matrices and vectors are named
    slightly differently than they are with systems of equations. Now, the matrix
    of coefficients, *A* becomes the matrix that contains the data and we call it
    *X*. Each row of this matrix, the row vector *x_i*, is one data entry. The vector
    *x* contained the unknown variables in equation (1). Now, the unknown variables
    are the linear regression coefficients and we denote them, *𝛽.* And finally, the
    right hand vector of the linear system, *b* becomes the vector that contains the
    dependent variable and we call it *y*. So, equation (1) in the context of linear
    regression becomes:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在线性回归的背景下，矩阵和向量的命名与方程组有所不同。现在，系数矩阵 *A* 变成了包含数据的矩阵，我们称之为 *X*。这个矩阵的每一行，即行向量
    *x_i*，是一个数据条目。向量 *x* 包含了方程 (1) 中的未知变量。现在，未知变量是线性回归系数，我们用 *𝛽* 表示。最后，线性系统的右边向量 *b*
    变成了包含因变量的向量，我们称之为 *y*。因此，在线性回归的背景下，方程 (1) 变成了：
- en: '![](../Images/d8509af2748e53c64e7a7a3027387cb1.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8509af2748e53c64e7a7a3027387cb1.png)'
- en: 'Eq (2): The basic equation of linear regression. Image by author.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Eq (2)：线性回归的基本方程。图片由作者提供。
- en: 'Expanded out, this equation looks like the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 展开后，这个方程看起来如下：
- en: '![](../Images/dd16e8cb07bddee8c058fe6df57d0683.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd16e8cb07bddee8c058fe6df57d0683.png)'
- en: 'Notice the first column of *1*’s. This corresponds to the constant term. For
    example, in the one variable case, without that column our model would be: *y
    = m.x.* This would exclude lines like *y=x+1* from consideration. In order to
    consider lines like: *y = mx+c* (which we do want in most cases), we need that
    column of *1*’s.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意第一列的*1*。这对应于常数项。例如，在单变量的情况下，如果没有那一列，我们的模型将会是：*y = m.x.* 这将排除像 *y=x+1* 这样的直线。如果要考虑像
    *y = mx+c* （我们在大多数情况下确实需要）的直线，我们需要那一列*1*。
- en: 'Just as with the linear system of equations, we’d like to take the multiplicative
    inverse of *X* on both sides and find *𝛽.* Like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 就像线性方程组一样，我们希望对两边进行*X*的乘法逆运算，找到*𝛽*。如图所示：
- en: '![](../Images/e4b7b4373714516a8aff0531a10a9373.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4b7b4373714516a8aff0531a10a9373.png)'
- en: This equation doesn’t make any sense. The matrix, X is rectangular, so it can’t
    be inverted.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程没有任何意义。矩阵 *X* 是矩形的，所以它不能被逆转。
- en: Unfortunately, this doesn’t make sense. Only square matrices can be inverted.
    Rectangular ones simply can’t. And our data matrix, *X* is rectangular with the
    number of rows (*n*) > number of columns (*m*).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这没有意义。只有方阵才可以逆转。矩形矩阵则不行。我们的数据矩阵 *X* 是一个矩形矩阵，行数（*n*）大于列数（*m*）。
- en: One way to see this is that it represents a linear system with more equations
    than variables and the arguments of section II apply. In terms of the linear map
    behind the matrix, for a “skinny” rectangular matrix with more rows than columns
    (like *X* is), it won’t be a one to one mapping (multiple points from the same
    space will map to the same point in the second one, which isn’t allowed).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一种理解这一点的方法是，它表示一个方程数量多于变量数量的线性系统，因此第二部分的论点适用。就矩阵背后的线性映射而言，对于一个“瘦长”的矩阵（如 *X*
    所示），行数多于列数，它不会是一个一对一的映射（同一空间中的多个点会映射到第二个空间中的同一点，这种情况是不允许的）。
- en: Is there a “hack” we can do to just make the matrix multiplied by 𝛽 a square
    one? The matrix, *X* is currently *n⨉m*. What if we multiplied by some other *m⨉n*
    matrix, *U.* Then, the resulting matrix, *V* will be square *m⨉m*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有一种“技巧”可以让乘以 𝛽 的矩阵变成方阵？矩阵 *X* 目前是 *n⨉m*。如果我们乘以另一个 *m⨉n* 的矩阵 *U*，那么得到的矩阵 *V*
    将是方阵 *m⨉m*。
- en: '![](../Images/c724f4c232c633cddaa7f1681c1d03fa.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c724f4c232c633cddaa7f1681c1d03fa.png)'
- en: Pre-multiplying equation (2) by *U*, we are then able to invert the resulting
    matrix and get 𝛽.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对方程（2）进行 *U* 的左乘，我们可以逆转结果矩阵并得到 𝛽。
- en: '![](../Images/538314b27c122ecfc4158219482ec859.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/538314b27c122ecfc4158219482ec859.png)'
- en: Now, the question is, where do we get this *U* matrix (*m⨉n)* from? The *X*
    matrix, *n⨉m* is what we have (the data itself). One thing we can do is flip the
    *X* matrix so its rows become its columns and columns become rows. This kind of
    operation on a matrix is called transpose, denoted by X^T visualized below.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是，我们从哪里得到这个 *U* 矩阵（*m⨉n*）？我们拥有的是 *X* 矩阵，*n⨉m*（即数据本身）。我们可以做的一件事是将 *X* 矩阵翻转，使其行变成列，列变成行。这种对矩阵的操作称为转置，记作
    X^T，如下图所示。
- en: '![](../Images/ce5afab467995b5e57c21423cef1568a.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce5afab467995b5e57c21423cef1568a.png)'
- en: 'Transpose of a matrix. Image credit: [Wikipedia](https://en.wikipedia.org/wiki/Transpose)
    article on transpose.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的转置。图片来源：[维基百科](https://en.wikipedia.org/wiki/Transpose) 关于转置的文章。
- en: 'So, we can replace *U* by the transpose of *X.* This leads to:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以用 *X* 的转置来替代 *U*。这将得到：
- en: '![](../Images/4df291644e59c673764233817454e49c.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4df291644e59c673764233817454e49c.png)'
- en: 'Eq (3): The linear regression coefficients.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（3）：线性回归系数。
- en: And we have now found the coefficients of the regression model. If we get a
    new data point, *x_new* (in the form of a row vector) we can take its dot product
    with *𝛽* and obtain the corresponding *y*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经找到了回归模型的系数。如果我们得到一个新的数据点，*x_new*（以行向量的形式），我们可以将其与*𝛽*进行点积，从而获得相应的*y*。
- en: '![](../Images/deeca6076c56e7dd972bbdf81b5d664c.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/deeca6076c56e7dd972bbdf81b5d664c.png)'
- en: Where 𝛽 is given by equation (3) above. Now, we provided the motivation for
    replacing *U* by *X^T* since it was an obvious choice with the dimensions we were
    looking for. But, the same formula has a much stronger mathematical motivation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 𝛽 由上述方程（3）给出。现在，我们提供了用*X^T*替代*U*的动机，因为这是一个显而易见的选择，符合我们所需的维度。然而，相同的公式还有更强的数学动机。
- en: III-A) Mathematical motivation
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III-A) 数学动机
- en: We need to motivate a specific value of 𝛽 (the one in equation (3)). So, let’s
    think about what happens for various values of 𝛽. As explained in animation-1,
    section III-A of chapter 3 ([matrix multiplication](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)),
    we can interpret *X*𝛽as taking apart the column vectors of *X* and combining them
    into a single column vector via a linear combination. More specifically, we multiply
    the first column vector of *X* by the first element of the vector 𝛽, the second
    column vector by the second element and so on and then adding up all the results.
    So, as we change the vector 𝛽, we’re exploring the “column space” of the matrix
    *X,* which is the set of all vectors we can get via a linear combination of the
    column vectors of *X*. And then we also have the vector *y,* the right hand side
    of our equation (2). Just like the column vectors of *X*, the dimensionality of
    this vector is also *n* (the number of data points/ rows in the matrix).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: The dimensionality of this vector space (that contains *y* and the column vectors
    of *X*) is *n.* On the other hand, the number of column vectors from *X* is *m.*
    And remember, we’re in the case where *n>>m*. So, the column space of *X* (space
    spanned by these *m* vectors) will have dimensionality *m.* This is much lower
    than *n,* the larger space where those column vectors live.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: The vector *y* lives in the same larger space of dimensionality, *n*. Since
    the number of column vectors, *m* is so much smaller than *n*, the vector *y*
    will not be in the column space spanned by the column vectors of *X,* almost surely.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: If it were, we’d have a 𝛽 that satisfies all the equations exactly. This is
    impossible to do (as discussed before). But, we still want to find a 𝛽 that brings
    it as close as possible to the vector, *y*. To do this, we minimize the distance
    between *y* and *X*𝛽.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a concrete example. Say we have a data set with *n=3* data points.
    In our regression model, we choose *m=2* features. The *X𝛽=y* equation looks like
    this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d0925cc07a7ea9d60383892997d6628.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: So, the matrix *X* has two column vectors, *[1,1,1]* and *[2,5,7]*. They live
    in a 3 dimensional space (*n=3*). These two column vectors are plotted in blue
    in figure-2 below. The space spanned by those vectors (column space) is two dimensional
    (*m=2*) and a section of it is shaded in pink.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9b472f068d83b4bde301f5057de70bd.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: 'Figure-2: Demonstrating linear regression as an exploration of the column space
    of the data matrix, X. Image by author.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, look at the triangle formed by the black vector, grey vector and red vector.
    The black vector is *X𝛽,* the red vector is *y* and the grey vector is *d*. The
    three form a triangle and hence satisfy:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f21e4a02d2d69b8291133b7a59ae053.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: It’s this *d* vector whose length we want to find the point in the column space
    of *X* (controlled by 𝛽) that is closest to *y.*
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望找到这个 *d* 向量在 *X* 的列空间（由 𝛽 控制）中最接近 *y* 的点。
- en: We get the squared length of a column vector by matrix-multiplying its transpose
    with itself.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将列向量的转置与其自身进行矩阵乘法来获得列向量的平方长度。
- en: '![](../Images/c61f481b7cb31beb88bf77f4ba3ec792.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c61f481b7cb31beb88bf77f4ba3ec792.png)'
- en: Finally, let’s take the derivative with respect to *𝛽* and set it to *0*. This
    will give us the 𝛽 that minimized the squared length of vector *d.*
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们对 *𝛽* 取导数并设置为 *0*。这将给出最小化向量 *d* 的平方长度的 𝛽。
- en: '![](../Images/78bbdad103f792818909ee130e02f5a9.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78bbdad103f792818909ee130e02f5a9.png)'
- en: 'Here, we use equation (78) from the [matrix cook-book](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf),
    [2]. This leads to:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用来自[矩阵宝典](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)的方程（78），[2]。这导致：
- en: '![](../Images/44d119216d0ea765d0504d35161f49b0.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44d119216d0ea765d0504d35161f49b0.png)'
- en: And this is the same as equation (3).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这与方程（3）是相同的。
- en: We motivated here with the column space of *X* (as explained in more detail
    in [1]). The same equations can also be motivated as minimizing the square error
    terms in the predicted and actual values of vector, *y.* This approach is covered
    in [2].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里用 *X* 的列空间来激励（如[1]中更详细地解释）。相同的方程也可以被激励为最小化预测和实际值向量 *y* 中的平方误差项。这种方法在[2]中涵盖。
- en: III-B) Online linear regression
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III-B) 在线线性回归
- en: '![](../Images/fc9238e9e85c96c937a424b6170d16bb.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc9238e9e85c96c937a424b6170d16bb.png)'
- en: Online linear regression. Data comes in as a constant stream, day over day.
    We need to keep our linear regression model up to date as that happens. Image
    by midjourney.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在线线性回归。数据以恒定的流入方式每天进入。我们需要在这个过程中保持我们的线性回归模型的最新状态。图像来源于 midjourney。
- en: So far, we thought of linear regression as a static model where the data matrix,
    *X* and the corresponding vector of responses, *y* are given to us. Very often,
    we have data coming in as a constant stream over time. Some data points might
    drop today, some more tomorrow and so on. And we’d like our model to use a rolling
    window of data (say 30 days) and the parameters should be updated every day for
    the data from the last 30 days. The obvious way to do it is to use equation (3)
    on the past 30 days of data every day and refresh the parameters on a daily basis.
    But what if the number of data points, *n* (rows in the matrix *X*) is very large?
    That’ll make the computation in equation (3) very expensive. And if you think
    about the model yesterday versus today. Most of the data considered by the two
    models is the same because of the rolling window. On day 31, we used the data
    from day 1 to day 30 and on day 32, we used the data from day 2 to day 31\. The
    data from day 2 to day 30 is common. What makes the model for day 32 different
    from the one on day 31 is that it does not consider all the data points that dropped
    on day 1 and does consider the ones that dropped on day 31\. Apart from that,
    the vast majority of the data (days 2 through 29) is common. It seems wasteful
    then to ignore this and train the entire model from scratch everyday. If we could
    reformulate equation (3) in a way that its a sum over some function of the rows
    of *X*, *x_i*, we could keep adding in the contributions of new data as it came
    in while subtracting the contribution from the old data that falls out of the
    rolling window.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们把线性回归看作一个静态模型，其中数据矩阵 *X* 和对应的响应向量 *y* 已经给定。很常见的是，数据会随着时间作为一个恒定的流入。一些数据点今天可能会丢失，明天可能会更多，以此类推。我们希望我们的模型使用一个滚动窗口的数据（比如30天），并且参数每天更新一次，针对过去30天的数据。显而易见的方法是每天使用过去30天的数据应用方程（3），并每天刷新参数。但如果数据点的数量
    *n*（矩阵 *X* 的行数）非常大呢？这会使方程（3）的计算变得非常昂贵。如果你考虑昨天的模型与今天的模型。由于滚动窗口，大部分数据都是相同的。第31天，我们使用了第1天到第30天的数据，而第32天，我们使用了第2天到第31天的数据。第2天到第30天的数据是共同的。第32天的模型与第31天的模型不同之处在于它不考虑第1天丢失的所有数据点，但考虑了第31天丢失的数据点。除此之外，绝大多数数据（第2天到第29天）是共同的。因此，忽略这一点并每天从头开始训练整个模型似乎是浪费的。如果我们可以将方程（3）重新表述为对
    *X* 的行 *x_i* 的某些函数的求和，我们可以在新数据到来时不断增加贡献，同时减去掉出滚动窗口的旧数据的贡献。
- en: 'And one of the many interpretations of matrix multiplication we covered in
    chapter 3 helps us do just that. In section III-B of [chapter 3](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)
    (around animation 5) the interpretation of matrix multiplication as a sum of outer
    products of the rows of the two matrices is covered. Using that, we can square
    the matrix on the left of equation (3) as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章中我们讨论的矩阵乘法的众多解释之一可以帮助我们做到这一点。在[第 3 章](https://medium.com/towards-data-science/a-birds-eye-view-of-linear-algebra-why-is-matrix-multiplication-like-that-a4d94067651e)
    的 III-B 节（大约动画 5）中，介绍了将矩阵乘法解释为两个矩阵行的外积之和。利用这一点，我们可以将方程 (3) 左侧的矩阵平方为：
- en: '![](../Images/63d949b23d420e4f65cf1fe5193d7e5c.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63d949b23d420e4f65cf1fe5193d7e5c.png)'
- en: Eq (4) Image by author
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 (4) 图片由作者提供
- en: Here, the vector *x_i* is the *i*-th row of the matrix *X.* It has one row and
    *m* columns (*1*⨉*m).*
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，向量 *x_i* 是矩阵 *X* 的第 *i* 行。它有一行和 *m* 列 (*1*⨉*m*)。
- en: 'Similarly, the vector on the left side of that equation can be written as:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，那个方程左侧的向量可以写成：
- en: '![](../Images/4a4efd17ea90a82712d09b9b413d8494.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a4efd17ea90a82712d09b9b413d8494.png)'
- en: Eq (5) Image by author
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 (5) 图片由作者提供
- en: Since both the terms have been expressed as sums over the *n* data points, we
    can add and remove the contributions from individual data points at will. We can
    keep running versions of the matrix in equation (4) and vector in equation (5)
    based on the data points that fall in the last 7 days. If a data point falls outside
    the 30 day window, we can subtract the terms it corresponds to from both the equations
    and if a new data point comes in, we can add its terms to both the equations.
    And since *m* is small, we can efficiently calculate 𝛽, keeping it up to date
    every time we update those terms. This method can also be used to add weights
    to the data points. This can be useful if (for example), you’re getting data from
    multiple sources and want to weigh some of them more than others. You’d just multiply
    the weight, *w_i* for the *i*-th data point into each of the terms of equations
    (4) and (5).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个项已经以 *n* 个数据点的总和表示，我们可以随意地添加或删除来自单个数据点的贡献。我们可以基于过去 7 天内的数据点，保持方程 (4) 中的矩阵和方程
    (5) 中的向量的不断更新。如果某个数据点超出了 30 天的窗口，我们可以从两个方程中减去它对应的项，如果有新的数据点进入，我们可以将其项添加到两个方程中。而且由于
    *m* 很小，我们可以高效地计算 𝛽，每次更新这些项时保持其最新。这种方法也可以用来为数据点添加权重。如果（例如）你从多个来源获取数据并希望对其中一些进行加权，这将非常有用。你只需将第
    *i* 个数据点的权重 *w_i* 乘到方程 (4) 和 (5) 的每一项中。
- en: IV) Neural networks
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IV) 神经网络
- en: '![](../Images/90a8651ed207be30f96827b8cacb9991.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90a8651ed207be30f96827b8cacb9991.png)'
- en: Neural network architectures consist of layers of vectors. The first layer is
    the input layer, the last layer is the output layer and everything in between
    is a hidden layer. Image by midjourney.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构由向量层组成。第一层是输入层，最后一层是输出层，中间的所有层都是隐藏层。图片由 Midjourney 提供。
- en: Neural networks are machine learning models that are inspired by the neural
    connections in biological brains. They are the current weapon of choice in our
    quest towards artificial general intelligence. All recent advances, from text
    to image to conversation bots use models that are neural networks at their core.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是受生物大脑神经连接启发的机器学习模型。它们是我们追求人工通用智能的当前选择武器。所有近期的进展，从文本到图像再到对话机器人，都使用了以神经网络为核心的模型。
- en: Linear regression can be thought of as the simplest possible neural network.
    Let’s concern ourselves with inference for now, the process of obtaining the output
    vector given an instance of the input to the model. For linear regression, we’ll
    get a row vector, *x* as input and a single scalar value, *y* as output. The parameter
    vector, 𝛽 takes us from the input to the output.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归可以被看作是最简单的神经网络。现在我们关注推断，即在给定模型输入实例的情况下获得输出向量的过程。对于线性回归，我们将获得一个行向量 *x* 作为输入和一个单一的标量值
    *y* 作为输出。参数向量 𝛽 将输入转换为输出。
- en: '![](../Images/9e1f1c9dbebdc8c1dda272454bb725ab.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e1f1c9dbebdc8c1dda272454bb725ab.png)'
- en: In linear regression, we get a vector corresponding to a new data point, x_j.
    This is multiplied with the parameter vector, beta to produce the response, y_j
    which is a single scalar. Image by author.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，我们得到一个对应于新数据点 x_j 的向量。这与参数向量 beta 相乘以产生响应 y_j，它是一个单一的标量。图片由作者提供。
- en: To extend this to neural networks, we generalize in two ways.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要将其扩展到神经网络，我们以两种方式进行概括。
- en: First, in order for the model to be able to output all kinds of interesting
    things like images, sentences, videos and spaceships, we need it to output not
    just a scalar (like with linear regression), but a vector. If this vector is high
    dimensional enough, any complex response we expect can be embedded effectively
    in its corresponding vector space.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了使模型能够输出各种有趣的内容，如图像、句子、视频和宇宙飞船，我们需要它输出的不仅仅是一个标量（如线性回归），而是一个向量。如果这个向量的维度足够高，我们期望的任何复杂响应都可以有效地嵌入其相应的向量空间中。
- en: 'So we need to go from vector->scalar for the case of linear regression to vector->vector.
    The scalar output is then just a special case, since its a one dimensional vector.
    This changes the picture above to this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要将线性回归的向量->标量情况改为向量->向量。标量输出只是一个特例，因为它是一个一维向量。这将上述图像更改为：
- en: '![](../Images/0f8b0d9706704844d15d87a110a11969.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f8b0d9706704844d15d87a110a11969.png)'
- en: In general, we’d like the response, y to be a vector instead of a scalar. That
    way, we can embed complex information about the real world in that vector space.
    Image by author.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们希望响应 `y` 是一个向量而不是一个标量。这样，我们可以在那个向量空间中嵌入关于现实世界的复杂信息。图片由作者提供。
- en: Now, the parameter vector, 𝛽 from before will need to become a parameter matrix.
    This is a linear map that maps the input vector to the output vector.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，之前的参数向量 𝛽 将需要变成参数矩阵。这是一个线性映射，将输入向量映射到输出向量。
- en: But, most interesting relationships in the real world are non-linear. To accommodate
    this, we first insert a bunch of intermediate “hidden layers”, shown in blue below.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，现实世界中大多数有趣的关系都是非线性的。为了适应这一点，我们首先插入了一堆中间的“隐藏层”，如下图中的蓝色部分所示。
- en: '![](../Images/f8be912112e18df8d6db0f6ff35087ae.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8be912112e18df8d6db0f6ff35087ae.png)'
- en: Architecture of a deep neural network. The hidden layers in blue are intermediate
    layers of the model. Image by author.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络的结构。图中的蓝色隐藏层是模型的中间层。图片由作者提供。
- en: Adding the layers in this way doesn’t change anything on its own. Its still
    equivalent to the original model without any hidden layers. To see this, note
    that the parameter matrices, *B_1, B_2, B_3, …* in the figure below just multiply
    together and become yet another parameter matrix.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式添加层本身不会改变任何东西。它仍然等同于没有任何隐藏层的原始模型。要看到这一点，请注意，下面图中的参数矩阵 *B_1, B_2, B_3, …*
    只是相乘并变成另一个参数矩阵。
- en: '*B = (B_1\. B_2\. B_3.B_4)*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*B = (B_1\. B_2\. B_3.B_4)*'
- en: But this one simple trick takes this method from being able to approximate only
    linear maps arbitrarily well to being able to approximate *any* maps arbitrarily
    well. To make the hidden layers worth our trouble, we add a simple, element-wise
    non-linearity at every layer, *f*. This is a simple, one dimensional function,
    *f* that takes a scalar as input and returns a scalar as output. We simply apply
    *f* to every element of the vector at each layer *before* multiplying by the next
    parameter matrix, *B_j*. A popular choice of this *f* is the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个简单的技巧使得这种方法从只能任意逼近线性映射变成可以任意逼近*任何*映射。为了使隐藏层值得我们费心，我们在每一层添加一个简单的逐元素非线性函数 *f*。这是一个简单的一维函数
    *f*，它接受一个标量作为输入并返回一个标量作为输出。我们只需在乘以下一个参数矩阵 *B_j* 之前，将 *f* 应用于向量的每个元素。这个 *f* 的一个流行选择是
    [Sigmoid 函数](https://en.wikipedia.org/wiki/Sigmoid_function)。
- en: The [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
    [4], says that this kind of architecture can approximate any mapping between two
    vector spaces (linear or non-linear) arbitrarily well.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[通用逼近定理](https://en.wikipedia.org/wiki/Universal_approximation_theorem) [4]
    说，这种架构可以任意逼近两个向量空间之间的任何映射（线性或非线性）。'
- en: V) Conclusion
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: V) 结论
- en: The humble matrix multiplication is an unimaginably powerful tool. You’ll find
    it in the simplest of models as well as the most complex, cutting edge ones. In
    this chapter, we went over aspects of some simple models that have matrix multiplication
    as their core engine. In the further chapters of this book, we will explore more
    linear algebra concepts, highlighting their role in modern AI models.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 谦逊的矩阵乘法是一种极其强大的工具。你会在最简单的模型以及最复杂、最前沿的模型中找到它。在本章中，我们回顾了一些以矩阵乘法为核心引擎的简单模型。在本书的后续章节中，我们将探索更多线性代数概念，突出它们在现代
    AI 模型中的作用。
- en: If you enjoyed the article, buy me a coffee :) [https://www.buymeacoffee.com/w045tn0iqw](https://www.buymeacoffee.com/w045tn0iqw)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，给我买杯咖啡吧 :) [https://www.buymeacoffee.com/w045tn0iqw](https://www.buymeacoffee.com/w045tn0iqw)
- en: References
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A beautiful way of looking at linear algebra: [https://medium.com/towards-data-science/a-beautiful-way-of-looking-at-linear-regressions-a4df174cdce](https://medium.com/towards-data-science/a-beautiful-way-of-looking-at-linear-regressions-a4df174cdce)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Deriving linear regressions from least squares: [https://stats.stackexchange.com/questions/46151/how-to-derive-the-least-square-estimator-for-multiple-linear-regression](https://stats.stackexchange.com/questions/46151/how-to-derive-the-least-square-estimator-for-multiple-linear-regression)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Matrix cook-book: [https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Universal approximation theorem: [https://en.wikipedia.org/wiki/Universal_approximation_theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
