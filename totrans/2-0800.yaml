- en: 'Emergent Abilities in AI: Are We Chasing a Myth?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9](https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Opinion
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Changing Perspective on Large Language Models emerging properties
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----fead754a1bf9--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----fead754a1bf9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fead754a1bf9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fead754a1bf9--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----fead754a1bf9--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fead754a1bf9--------------------------------)
    ·11 min read·May 16, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e488a59261a210747293aba21891933.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: image by the author using DALL-E
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: The emergent properties of a model
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Emergent properties](https://en.wikipedia.org/wiki/Emergence) are not only
    a concept that belongs to artificial intelligence but to all disciplines (from
    physics to biology). This concept has always fascinated scientists, both in describing
    and trying to understand the origin. Nobel Prize-winning physicist [P.W. Anderson](https://en.wikipedia.org/wiki/Philip_W._Anderson)
    synthesized the idea with “More Is Different.” In a certain, sense it can be defined
    as an emergent property, a property that appears as the complexity of the system
    increases and cannot be predicted.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can encode information with a small molecule, but DNA (a large
    molecule) is encoding a genome. Or a [small amount of Uranium](https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/)
    is not leading to a nuclear reaction.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/42e267a8f3a5e72ec1c124b73371d304.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: '“The formation of complex symmetrical and [fractal](https://en.wikipedia.org/wiki/Fractal)
    [patterns](https://en.wikipedia.org/wiki/Patterns_in_nature) in [snowflakes](https://en.wikipedia.org/wiki/Snowflake)
    exemplifies emergence in a physical system”. image source: [here](https://en.wikipedia.org/wiki/Emergence)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently the same behavior has been observed with artificial intelligence models,
    one of the most commonly [used definitions being](https://arxiv.org/abs/2206.07682):
    “An ability is emergent if it is not present in smaller models but is present
    in larger models.”'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: What does this mean and how is it observed?
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'OpenAI stated in an article that the performance of a model follows a [scaling
    law](https://arxiv.org/abs/2001.08361): the more data and parameters, the better
    the performance. In the case of emergent properties, what is expected is a particular
    pattern: as the number of parameters increases, performance is almost random until
    at a certain threshold a certain property is observed (performance begins to improve
    noticeably). Basically, we see a sharp turn of the curve (called phase transition).
    This also is called emergent, because it is impossible to predict by examining
    a small-scale model.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI在一篇文章中声明，模型的性能遵循[规模法则](https://arxiv.org/abs/2001.08361)：数据和参数越多，性能越好。对于涌现属性，预期是一个特定的模式：随着参数数量的增加，性能几乎是随机的，直到达到某个阈值时，观察到某种属性（性能开始显著改善）。基本上，我们看到曲线的急剧转折（称为相变）。这也被称为涌现，因为通过检查小规模模型无法预测。
- en: '![](../Images/57f2aacc51ebcdbc8a2f20c0192b861b.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57f2aacc51ebcdbc8a2f20c0192b861b.png)'
- en: Emergent abilities of large language models. image source ([here](https://arxiv.org/abs/2304.15004))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的涌现能力。图片来源 ([这里](https://arxiv.org/abs/2304.15004))
- en: 'So in short, we can say that a property is considered emergent if it satisfies
    these two conditions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们可以说，如果一个属性满足这两个条件，它被认为是涌现的：
- en: '**Sharpness**, the transition is discontinuous between being present or not
    present.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**敏锐性**，其过渡在存在或不存在之间是不连续的。'
- en: '**Unpredictability**, its appearance cannot be predicted as parameters increase'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可预测性**，随着参数的增加，其出现无法预测。'
- en: 'In addition, scaling a transformer mainly takes into consideration three factors:
    the amount of computation, the number of model parameters, and the training dataset
    size.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，扩展一个变换器模型主要考虑三个因素：计算量、模型参数数量和训练数据集大小。
- en: All three factors make a model expensive. On the other hand, these properties
    are particularly sought after and have also been used as a justification for increasing
    the number of parameters (despite the fact that models are not trained optimally).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个因素都使得模型昂贵。另一方面，这些属性特别受到追求，也被用作增加参数数量的理由（尽管模型并未得到最佳训练）。
- en: '![](../Images/a6df68ed07d1dffe957909f7772c0abf.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6df68ed07d1dffe957909f7772c0abf.png)'
- en: 'image source: [here](https://arxiv.org/abs/2206.07682)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2206.07682)
- en: 'Several studies have also focused on why these properties emerge, why they
    do so in this way, and why at a particular threshold. According to some the emergence
    of some properties can be predicted:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究还集中在为什么这些属性会出现，为什么以这种方式出现，以及为什么在特定阈值下出现。据一些观点，某些属性的出现是可以预测的：
- en: For instance, if a multi-step reasoning task requires l steps of sequential
    computation, this might require a model with a depth of at least O (l) layers.
    ([source](https://arxiv.org/abs/2206.07682))
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，如果一个多步推理任务需要l步顺序计算，这可能需要一个至少有O (l) 层深度的模型。 ([source](https://arxiv.org/abs/2206.07682))
- en: Alternative explanations have been proposed such as that the larger number of
    parameters aids memorization. The model gains knowledge as the data increases
    and at some point reaches critical mass to be able to support that property
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 也提出了替代解释，如更多的参数有助于记忆。随着数据的增加，模型获得知识，并在某一点达到临界质量，能够支持该属性。
- en: In addition, some authors have proposed that different architectures and better
    data quality could lead to the appearance of these properties in even smaller
    models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些作者提出，不同的架构和更好的数据质量可能会导致这些属性在更小的模型中也出现。
- en: This was noted with LLaMA, where a significantly smaller model of GPT-3 showed
    comparable properties and performance.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLaMA的研究中，观察到一个比GPT-3小得多的模型展示了相似的属性和性能。
- en: '[](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----fead754a1bf9--------------------------------)
    [## META’s LLaMA: A small language model beating giants'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----fead754a1bf9--------------------------------)
    [## META的LLaMA：一个小型语言模型战胜巨头'
- en: META open-source model will help us to understand how LMs biases arise
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: META开源模型将帮助我们理解语言模型的偏见是如何产生的。
- en: medium.com](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----fead754a1bf9--------------------------------)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----fead754a1bf9--------------------------------)
- en: In any case, the question remains, why do these properties appear?
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 无论如何，问题依然存在，为什么这些属性会出现？
- en: 'Anthropic in one study states that:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic 在一项研究中指出：
- en: large generative models have a paradoxical combination of high predictability
    — model loss improves in relation to resources expended on training, and tends
    to correlate loosely with improved performance on many tasks — and high unpredictability
    — specific model capabilities, inputs, and outputs can’t be predicted ahead of
    time ([source](https://arxiv.org/abs/2202.07785))
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大型生成模型具有矛盾的特性——高可预测性——模型损失与训练所花费的资源有关，且与许多任务的性能改善有松散的相关性——以及高不可预测性——特定的模型能力、输入和输出无法提前预测（[来源](https://arxiv.org/abs/2202.07785)）
- en: In simpler words, for an [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    there are things we can predict and things we cannot predict. For example, the
    scaling law allows us to predict that increasing the number of parameters will
    improve performance in scale, but at the same time, we cannot predict the emergence
    of certain properties that instead appear abruptly as the parameters increase.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，对于一个[大型语言模型](https://en.wikipedia.org/wiki/Large_language_model)，我们可以预测的事物和无法预测的事物是存在的。例如，扩展规律允许我们预测增加参数数量将提升规模上的性能，但同时，我们无法预测某些属性的涌现，这些属性会随着参数的增加而突然出现。
- en: So according to this principle, we should not even try to predict them.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据这一原则，我们甚至不应尝试预测这些属性。
- en: '![](../Images/a9a342a4d06704e5c32536726f596a50.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9a342a4d06704e5c32536726f596a50.png)'
- en: 'Scaling laws reliably predict that model performance. image source: [here](https://arxiv.org/abs/2202.07785)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展规律可靠地预测模型性能。图像来源：[这里](https://arxiv.org/abs/2202.07785)
- en: '![](../Images/b32237eddc17eb0befa89620880d7e7a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b32237eddc17eb0befa89620880d7e7a.png)'
- en: 'Three examples of abrupt specific capability scaling properties. image source:
    [here](https://arxiv.org/abs/2202.07785)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 三个突发特定能力扩展属性的例子。图像来源：[这里](https://arxiv.org/abs/2202.07785)
- en: Why are we so interested in predicting these properties?
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么我们如此关注预测这些属性？
- en: 'The first reason is pure economics: **if a property emerges only at a certain
    number of parameters, we cannot use a smaller model**. This significantly increases
    the cost of both training and hardware. On the other hand, if a property cannot
    be predicted, we cannot even estimate the cost of obtaining it.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个原因是纯粹的经济学：**如果某个属性只有在特定数量的参数下才会出现，我们就不能使用较小的模型**。这显著增加了训练和硬件的成本。另一方面，如果属性无法预测，我们甚至无法估算获得该属性的成本。
- en: Second, it justifies the inordinate increase in parameters in the search for
    new properties that appear at trillions of parameters. After all, this may be
    the only way to be able to obtain certain properties.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，这解释了在寻找新属性时参数异常增加的原因，这些新属性会在参数达到万亿级时出现。毕竟，这可能是获得某些属性的唯一途径。
- en: Moreover, this presents a safety problem, as we cannot predict what property
    a model will have at a certain scale. A model may develop problematic properties
    and may not be safe for deployment. Also, models this large are more difficult
    to test for bias and harm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这还带来了安全问题，因为我们无法预测模型在特定规模下会具备什么属性。一个模型可能会发展出问题属性，可能不适合部署。而且，这么大的模型也更难检测偏见和危害。
- en: Moreover, scaling law and emergent properties have been one of the reasons for
    the rush to large models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，扩展规律和涌现属性也是推动大型模型快速发展的原因之一。
- en: '![](../Images/b7db443fa6264c45636e608fe0a5efd8.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7db443fa6264c45636e608fe0a5efd8.png)'
- en: This opens a scary scenario, on the one hand, we have an explosion of open-source
    models, a reduction in the cost of their training, and an increase in the use
    of [chatbots](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2).
    But on the other hand, we have no way to predict the properties of these models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这开启了一个令人担忧的情景，一方面，我们有开源模型的爆炸性增长、训练成本的降低和[聊天机器人](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2)的使用增加。但另一方面，我们无法预测这些模型的属性。
- en: '[](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----fead754a1bf9--------------------------------)
    [## The Infinite Babel Library of LLMs'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----fead754a1bf9--------------------------------)
    [## 无限的语言模型图书馆'
- en: 'Open-source, data, and attention: How the future of LLMs will change'
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开源、数据和注意力：LLM的未来将如何变化
- en: towardsdatascience.com](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----fead754a1bf9--------------------------------)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----fead754a1bf9--------------------------------)
- en: What if emerging properties were a mirage?
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果涌现属性只是海市蜃楼呢？
- en: '![](../Images/9e03a1cca803f04578ce99b97125d168.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: image by [Nick Fewings](https://unsplash.com/fr/@jannerboy62) on Unsplash
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: In 2020, Google researchers realized the potential of LLMs and predicted that
    they would be transformative. So they asked the community to provide examples
    of tasks that were both different and difficult and that could then be used to
    test the capabilities of an LLM. Thus was born the [Beyond the Imitation Game
    Benchmark](https://arxiv.org/abs/2206.04615) (BIG-bench) project.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: This project was actually also focused on studying emergent and surprising properties
    and trying to be able to understand their origin.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd13fb32b2b62412ceb15230b36a25bc.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [here](https://arxiv.org/abs/2206.04615)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the dataset and article discussed the emergence of probabilities and
    tried to provide explanations. For example, models over ten billion parameters
    could solve three-digit addition or two-digit multiplication problems.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Building on this article, researchers at Stanford questioned in a recent paper
    the very concept of emergent property for a language model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2304.15004?source=post_page-----fead754a1bf9--------------------------------)
    [## Are Emergent Abilities of Large Language Models a Mirage?'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Recent work claims that large language models display emergent abilities, abilities
    not present in smaller-scale models…
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2304.15004?source=post_page-----fead754a1bf9--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the authors noticed that emergent properties seemed to appear only
    with metrics that were [nonlinear](https://en.wikipedia.org/wiki/Nonlinear_system)
    or otherwise discontinuous.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: The authors provided an alternative hypothesis to the emergence of properties.
    according to them is the choice of performance measurement. In other, words the
    error per-tokens grows smoothly, continuously, and predictably with increasing
    model scale. But then the study authors measure performance for tasks using discontinuous
    metrics, and so it appears that the model performs the task abruptly.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: In other words, a small model performs decently on a task but we can’t detect
    it because the chosen metric is discontinuous, and only under a certain error
    (achieved over a certain model size) can we observe performance in the tasks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/899e3e4e70ca8556f75c7274ca73085e.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2304.15004)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: According to the authors, it is also the small number of examples for the test
    that leads to small models not being properly evaluated.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate this, the authors started with the scaling law, according to
    which performance (or error) increases as a function of the number of metrics
    and which is indeed shown to be consistent at different magnitudes. As the authors
    note, many metrics require all tokens in the sequence to be correct, especially
    when dealing with long sequences leads to seeing sharp increases.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: They were able to do these experiments using InstructGPT/GPT-3 because models
    such as LaMDA, Gopher, and Chinchilla are unfortunately not accessible. This prevented
    them to do an extensive evaluation of the different models. Since LLMs are trained
    only on text (and GPT is trained on predicting the next word), one of the surprising
    abilities of LLMs is integer arithmetic tasks. As showed from the GPT-3 introduction
    article, this property is defined as emergent in function of the scale/
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d53f51d1996ca43a89b97641a7fcd29.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: 'Results on all 10 arithmetic tasks in the few-shot settings for models of different
    sizes. image source: [here](https://arxiv.org/abs/2005.14165)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the image (top) when performance is measured with a non-linear metric
    we see an emergent property. when a linear metric is used (bottom) on the other
    hand we see a continuous and predictable increase in performance as a function
    of scale.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de539001f6d394738e6b710f7828d14c.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2304.15004)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the authors noted that by increasing the data for small model evaluation
    even with nonlinear metrics the effect was not as pronounced. In other words,
    if the test dataset is larger even with nonlinear metrics we do not observe such
    a dramatic effect.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: In fact, with low resolution (few test data) is more probable to assist in zero
    accuracies for small models, which support the claim that a property emerges just
    after a certain threshold.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20ad137c1586811363bef1487f979212.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2304.15004)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The authors then decided to extend to a meta-analysis on emerging properties,
    using BigBench (since it is public and is also well documented). In addition,
    this dataset offers more than one evaluation metric. When the authors look at
    nonlinear metrics (Exact String Match, Multiple Choice Grade, ROUGE-L-Sum) emergent
    properties could be observed. On the other hand, using linear metrics no emergent
    properties are observed.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: The most surprising finding is that 92 % of claimed emergent abilities come
    from using two discontinuous metrics Multiple Choice Grade and Exact String Match.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0907f498f4c607342d153202cee01b6b.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: So if indeed the cause of emergent properties is the use of discontinuous metrics,
    just changing metrics would be enough to make them disappear. Keeping the model
    and task fixed, just change the rating metrics and the emergent properties disappear.
    In this case, the authors simply reused the outputs of the LaMDA family of models
    and changed the metrics from discontinuous (Multiple Choice Grade) to continuous
    (Brier Score).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98787f64096d8bdbb74e69cbdd157e16.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: 'one final question remains: **but if emergent properties appear by choosing
    discontinuous metrics can we create emergent properties using discontinuous metrics?**'
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The authors take as an example the classification ability of the handwritten
    digits dataset (MNIST or the data scientist’s favorite dataset). Anyone who has
    tried to train a convolutional network on this dataset has noticed that even with
    a few layers a decent result is obtained. Increasing the number of layers can
    improve the accuracy. If it were an emergent property, we would expect that at
    first, the accuracy would be near zero, and by increasing the parameters above
    a certain threshold the accuracy would start to increase significantly.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 作者以手写数字数据集（MNIST或数据科学家的最爱数据集）的分类能力为例。任何尝试在此数据集上训练卷积网络的人都注意到，即使只有少数几层，也能获得相当好的结果。增加层数可以提高准确率。如果这是一个突现属性，我们会期望一开始准确率接近零，通过将参数增加到某个阈值以上，准确率会显著提高。
- en: 'The authors used the [LeNet](https://en.wikipedia.org/wiki/LeNet) family (several
    models with increasing numbers of parameters). They simply chose a new metric
    called subset accuracy: “1 if the network classifies K out of K (independent)
    test data correctly, 0 otherwise.”'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了[LeNet](https://en.wikipedia.org/wiki/LeNet)家族（多个具有递增参数数量的模型）。他们简单地选择了一种新的度量，称为子集准确率：“如果网络正确分类了K个K（独立）测试数据，则为1，否则为0。”
- en: While using test accuracy we notice the classic increase in accuracy with a
    sigmoidal trend, with the new discontinuous metric it seems that the ability to
    classify handwritten digits is an emergent property.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用测试准确率时，我们注意到经典的准确率随S型趋势增加，使用新的不连续度量时，似乎分类手写数字的能力是一种突现属性。
- en: '![](../Images/0e75b3aa8ff5818f310c90f4494cf523.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e75b3aa8ff5818f310c90f4494cf523.png)'
- en: 'The authors provide another example: image reconstruction with autoencoders.
    Just by creating a new discontinuous metric, the ability to reconstruct autoencoders
    becomes an emergent property.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供了另一个例子：使用自编码器进行图像重建。通过创建一个新的不连续度量，自编码器的重建能力成为了一种突现属性。
- en: '![](../Images/469249a740648835f47191ba03197e76.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/469249a740648835f47191ba03197e76.png)'
- en: 'The authors conclude:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 作者总结道：
- en: Emergent abilities may be creations of the researcher’s choices, not a fundamental
    property of the model family on the specific task ([source](https://arxiv.org/abs/2304.15004))
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 突现能力可能是研究者选择的产物，而不是模型家族在特定任务上的基本属性（[source](https://arxiv.org/abs/2304.15004)）
- en: In other words, if someone wants an emergent property all they have to do is
    choose a discontinuous metric and magically they will see a property appear over
    a certain threshold of parameters.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果有人想要突现属性，他们所要做的就是选择一个不连续度量，神奇地，他们将看到在某个参数阈值之上出现一个属性。
- en: The authors conservatively state, “*This paper should be interpreted as claiming
    that large language models cannot display emergent abilities.*” They merely claim
    that the properties seen so far are instead produced by choice from the metric.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 作者保守地声明：“*本文应被解读为声称大型语言模型无法展示突现能力。*” 他们只是声称，迄今为止看到的属性是由度量的选择产生的。
- en: Now it is true that until you see a black swan, all swans are white. But the
    next time an emergent property appears, though, one must check under what conditions
    it appears. Also, this is another call to rethink benchmarks that may now be unsuitable
    for measuring the quality of a model. Second, LLMs should be open-source, because
    any claim could simply be due to a choice of evaluation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在确实是这样，直到你看到一只黑天鹅之前，所有天鹅都是白色的。然而，下次出现突现属性时，必须检查其出现的条件。此外，这也是再次思考现在可能不适合评估模型质量的基准的呼吁。其次，大型语言模型应该是开源的，因为任何声明都可能仅仅是由于评估的选择。
- en: Parting thoughts
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: For a long time, emergent properties have been considered among the most surprising
    behaviors of Large Language Models (LLMs). The fact that beyond a certain number
    of parameters, an ability would emerge was a fascinating but at the same time
    terrifying concept. Indeed, on the one hand, it was further justification to look
    for larger and larger models. On the other, the emergence of potentially dangerous
    abilities without warning was problematic.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 长时间以来，突现属性被认为是大型语言模型（LLMs）中最令人惊讶的行为之一。超出一定数量的参数后，能力会出现，这既是一个迷人的概念，又是一个令人恐惧的概念。确实，一方面，它进一步证明了寻找越来越大模型的理由。另一方面，潜在危险能力的突现没有警告是个问题。
- en: This article surprisingly shows how the choice of evaluation metrics leads to
    the emergence of properties. This prompts a rethinking of benchmarks with a new
    focus on the choice of evaluation metrics. Second, emergent properties may not
    exist.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章意外地展示了评价指标的选择如何导致属性的出现。这促使我们重新思考基准测试，并将重点转向评价指标的选择。其次，可能并不存在涌现属性。
- en: More broadly, all along, many authors choose the evaluation metric that makes
    their data shine. Thus, we can only be sure of a claim when a model and its outputs
    are open to the public for independent scientific investigations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛地说，许多作者一直选择让他们的数据脱颖而出的评价指标。因此，只有当模型及其输出对公众开放，供独立的科学研究时，我们才能确信某一主张的真实性。
- en: 'If you have found this interesting:'
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得这很有趣：
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, you can* [***become a Medium member***](https://medium.com/@salvatore-raieli/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以查看我的其他文章，你也可以* [***订阅***](https://salvatore-raieli.medium.com/subscribe)
    *以便在我发布文章时获得通知，你还可以* [***成为 Medium 会员***](https://medium.com/@salvatore-raieli/membership)
    *以访问所有故事（这是平台的附属链接，我会从中获得少量收入，不会给你带来任何费用），你也可以在*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***上与我联系或找到我。***'
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是我 GitHub 仓库的链接，我计划在这里收集与机器学习、人工智能等相关的代码和许多资源。*'
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----fead754a1bf9--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----fead754a1bf9--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: 机器学习、人工智能、数据科学的教程…'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习、人工智能、数据科学的教程，包含数学解释和可重用的代码（使用 python…
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----fead754a1bf9--------------------------------)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----fead754a1bf9--------------------------------)
- en: '*or you may be interested in one of my recent articles:*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*或者你可能对我最近的文章之一感兴趣：*'
- en: '[](https://levelup.gitconnected.com/pmc-llama-because-googling-symptoms-is-not-enough-e1b875ee4c4a?source=post_page-----fead754a1bf9--------------------------------)
    [## PMC-LLaMA: Because Googling Symptoms is Not Enough'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/pmc-llama-because-googling-symptoms-is-not-enough-e1b875ee4c4a?source=post_page-----fead754a1bf9--------------------------------)
    [## PMC-LLaMA：因为谷歌搜索症状还不够'
- en: A small model that can be your best friend in medical school (or on trivia night)
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个在医学院（或在知识竞赛之夜）能成为你最佳朋友的小模型
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/pmc-llama-because-googling-symptoms-is-not-enough-e1b875ee4c4a?source=post_page-----fead754a1bf9--------------------------------)
    [](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----fead754a1bf9--------------------------------)
    [## Welcome Back 80s: Transformers Could Be Blown Away by Convolution'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/pmc-llama-because-googling-symptoms-is-not-enough-e1b875ee4c4a?source=post_page-----fead754a1bf9--------------------------------)
    [](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----fead754a1bf9--------------------------------)
    [## 欢迎回到80年代：卷积可能会打破变形金刚
- en: The Hyena model shows how convolution could be faster than self-attention
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hyena 模型展示了卷积如何可能比自注意力更快
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----fead754a1bf9--------------------------------)
    [](https://levelup.gitconnected.com/looking-into-your-eyes-how-google-ai-model-can-predict-your-age-from-the-eye-857979339da9?source=post_page-----fead754a1bf9--------------------------------)
    [## Looking into Your Eyes: How Google AI Model Can Predict Your Age from the
    Eye'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----fead754a1bf9--------------------------------)
    [](https://levelup.gitconnected.com/looking-into-your-eyes-how-google-ai-model-can-predict-your-age-from-the-eye-857979339da9?source=post_page-----fead754a1bf9--------------------------------)
    [## 注视你的眼睛：谷歌 AI 模型如何通过眼睛预测你的年龄
- en: The new model can unlock secrets of aging by analyzing eye photos
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/looking-into-your-eyes-how-google-ai-model-can-predict-your-age-from-the-eye-857979339da9?source=post_page-----fead754a1bf9--------------------------------)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
