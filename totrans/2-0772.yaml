- en: 'Dynamic Pricing with Reinforcement Learning from Scratch: Q-Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/dynamic-pricing-with-reinforcement-learning-from-scratch-q-learning-fb3fb764da49](https://towardsdatascience.com/dynamic-pricing-with-reinforcement-learning-from-scratch-q-learning-fb3fb764da49)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An introduction to Q-Learning with a practical Python example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://nicolo-albanese.medium.com/?source=post_page-----fb3fb764da49--------------------------------)[![Nicolo
    Cosimo Albanese](../Images/9a2c26207146741b58c3742927d09450.png)](https://nicolo-albanese.medium.com/?source=post_page-----fb3fb764da49--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fb3fb764da49--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fb3fb764da49--------------------------------)
    [Nicolo Cosimo Albanese](https://nicolo-albanese.medium.com/?source=post_page-----fb3fb764da49--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fb3fb764da49--------------------------------)
    ·12 min read·Aug 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c762b0a93c8cd128f116b6cdd17b8eda.png)'
  prefs: []
  type: TYPE_IMG
- en: Exploring prices to find the optimal action-state values to maximize profit.
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Table of contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Introduction](#502a)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A primer on Reinforcement Learning](#9e6b)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2.1 [Key concepts](#6e64)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.2 [Q-function](#b05f)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.3 [Q-value](#fed6)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.4 [Q-Learning](#3d03)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.5 [The Bellman equation](#1112)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.6 [Exploration vs. exploitation](#3cf3)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.7 [Q-Table](#7e4c)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[The Dynamic Pricing problem](#f694)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3.1 [Problem statement](#589b)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2 [Implementation](#7607)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Conclusions](#68d5)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[References](#6cb9)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we introduce the core concepts of Reinforcement Learning and dive
    into Q-Learning, an approach that empowers intelligent agents to learn optimal
    policies by making informed decisions based on rewards and experiences.
  prefs: []
  type: TYPE_NORMAL
- en: We also share a practical Python example built from the ground up. In particular,
    we train an agent to master the art of pricing, a crucial aspect of business,
    so that it can learn how to maximize profit.
  prefs: []
  type: TYPE_NORMAL
- en: Without further ado, let us begin our journey.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. A primer on Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 2.1 Key concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL) is an area of Machine Learning where an agent learns
    to accomplish a task by trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: In brief, the agent tries actions which are associated to a positive or negative
    feedback through a reward mechanism. The agent adjusts its behavior to maximize
    a reward, thus learning the best course of action to achieve the final goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us introduce the key concepts of RL through a practical example. Imagine
    a simplified arcade game, where a cat should navigate a maze to collect treasures
    — a glass of milk and a ball of yarn — while avoiding construction sites:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4a16cd2293d152c9e46d88fae8c601b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The **agent** is the one choosing the course of actions. In the example, the
    agent is the player who controls the joystick deciding the next move of the cat.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **environment** is the context in which the agent is operating. In our case,
    a two-dimensional maze.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'An **action** `a` is a the minimum amount of steps to move from one state to
    another. In this game, the player has a finite set of possible actions to choose
    from: *up*, *left*, *down* and *right*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **state** `s` represents the current situation of the player and the environment.
    It includes information such as the cat’s current and allowed positions, as well
    as the location of treasures and traps, and any other relevant feature to the
    game state (points, remaining lives, …).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **reward** `r` represents the feedback assigned to the result of taking
    an action. For example, the game may assign:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: • +5 points when reaching the ball of yarn,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: • +10 points for the glass of milk,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: • -1 points for an empty cell,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: • -10 points for a construction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The described RL framework is depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e2fbf6da700c48e8ded78c1a89d17e8.png)'
  prefs: []
  type: TYPE_IMG
- en: RL framework. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to learn a **policy** `π`, i.e., the set of rules that enables the
    agent to follow the course of action while maximizing the reward, thus achieving
    its target.
  prefs: []
  type: TYPE_NORMAL
- en: We can learn the optimal policy `π*` directly, or indirectly by learning the
    values (rewards) of action-state pairs, and using them to decide the best course
    of action. These two strategies are named **policy-based** and **value-based**,
    respectively. Let us now introduce Q-Learning, a popular value-based approach.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Q-function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We introduce the **Q-function**, denoted as `Q(s,a)`, representing the expected
    cumulative reward an agent can achieve by taking action `a` in state `s` , while
    following the policy `π`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b63d9c960ba373707938146b95d3765.png)'
  prefs: []
  type: TYPE_IMG
- en: Q-function. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`π`is the policy being followed by the agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s` is the current state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`a` is the action taken in that state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r` is the reward associated to the given action and state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t` represents the current iterate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`γ` is the **discount factor**. It represents the agent’s preference for immediate
    rewards (exploitation) over delayed rewards (exploration).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.3 Q-value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Q-value** refers to the numeric value assigned by the Q-function to a
    specific state-action pair. In our example, the Q-value provides the expected
    cumulative reward the player could obtain by moving the cat in a new position
    inside the maze through a specific action, starting from a certain state. **In
    brief, it tells how “good” the player’s choice is**.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Q-Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given the concept of Q-value, the **Q-Learning** algorithm works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialize Q-values arbitrarily**, e.g. `Q(s, a) = 0 ∀ s ∈ S, a ∈ A`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Initialize state`s`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2\. For each step of episode:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1\. **Choose action** `a`, observe the reward `r`, obtain new state `s'`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Update the Q-values using the **Bellman equation** 3\. `s ← s'`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Until `s` is terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2.5 The Bellman equation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Bellman equation allows an agent to express the value of a state-action
    pair in terms of cumulative expected reward. It is used to update the Q-values
    in the Q-Learning algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63690ff9b3a38b0beb844f2875024e8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Bellman equation. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous expression:'
  prefs: []
  type: TYPE_NORMAL
- en: The **learning rate** `α` (between 0 and 1) determines how much an agent updates
    the Q-values based on new experiences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **discount rate** `γ` (between 0 and 1) influences the agent’s preference
    for immediate rewards over future rewards. A high `γ` can promote exploitation,
    as the agent will tend to prefer known actions with immediate gains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.6 Exploration vs. exploitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How does the agent choose the next action?
  prefs: []
  type: TYPE_NORMAL
- en: The agent may “explore” new actions, or “exploit” actions known to be associated
    to a higher reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn an effective policy, we should strike for a balance between exploration
    and exploitation during training. In our example, we can adopt a straightforward
    method by defining an exploration probability, i.e. a float between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: If a random number from the uniform distribution on (0, 1) is **higher** than
    the exploration probability, the agent will perform **exploitation**, preferring
    already known actions with a high reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the number is **smaller** than the exploration probability, the agent will
    perform **exploration**, encouraging the experimentation of new actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach is known as **epsilon-greedy** algorithm (see [Cheng et Al. 2023,
    Appendix C](https://arxiv.org/pdf/2302.06953.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Q-Table
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the problem consists of a finite set of potential actions — such as *up*,
    *left*, *down* and *up*, it is possible to simply **tabulate** all the combinations
    of states and actions. This table, named **Q-Table**, is populated with Q-values
    during training, as the agent explores state and action pairs, and collects their
    associated reward. In our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b34ea9c5c76ffee6e87510abc65c9b25.png)'
  prefs: []
  type: TYPE_IMG
- en: Updating the Q-Table. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. The Dynamic Pricing problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given a product associated to a price and a demand, our goal is to train an
    intelligent agent that, using Reinforcement Learning, will adjust prices over
    time to maximize profit:'
  prefs: []
  type: TYPE_NORMAL
- en: “*Dynamic pricing is related to price-fixing for perishable resources taking
    into account demand so that to maximize revenue or profit*” ([Fleischmann, Hall,
    Pyke, 2004](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=845826)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We model a simplified environment with a discrete **action** space `A`, where
    the agent can increase, decrease or keep the price constant: `A = {+1, -1, 0}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The action (price manipulation) results in a new demand, and we create discrete
    demand levels as **state** `S = {Low demand, Medium demand, High demand}`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To estimate the new demand (state `s`) from a price change (action `a`), we
    leverage the concept of price elasticity `k`. Price elasticity estimates the sensitivity
    between a change in price `Δp` and its resulting change in demand `Δv`, and we
    assume it to be known in our example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b0ece6a54fd8e49941dcac7a1823e833.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reward `r` corresponds to the **profit** stemmed from the application of
    a price `p` and its consequent demand `v`, considering the unitary costs `c` associated
    to the product:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8e15cc3be8f3eeac35772cd813946ab3.png)'
  prefs: []
  type: TYPE_IMG
- en: Reward r is a function of the action (price p) and state (demand v). Image by
    author.
  prefs: []
  type: TYPE_NORMAL
- en: We assign a negative reward when the new price increases or decreases too much
    compared to the initial price using an arbitrary threshold. In this way, we penalize
    strong price variations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.2 Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `DynamicPricingQL` class implements the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`calculate_demand_level` assigns a continuous volume to a discrete state value
    (low, medium or high demand).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`calculate_demand` uses an input price to estimate the volume through price
    elasticity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fit` trains the agent. We decide to interrupt an episode when the maximum
    number of steps has been reached, or the profit (reward) has reached a certain
    threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_q_table` returns the Q-Table learned by the agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plot_rewards` shows a chart of the rewards achieved during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predict` uses the Q-values to predict the optimal price given a starting price
    and demand as input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us instantiate and fit the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to get the Q-Table after training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also plot the rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c762b0a93c8cd128f116b6cdd17b8eda.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of the code snippet. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: We observe how the rewards increase during the training procedure, as the agent
    learns, through Q-values, the pricing policy leading to a profit increase.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the Q-values to predict the next price through the trained agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we explored the key concepts of Reinforcement Learning and introduced
    the Q-Leaning method for training a smart agent. We also provided a hands-on Python
    example built from scratch. In particular, we implemented a dynamic pricing agent
    that learns the optimal pricing policy for a product in order to maximize profit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our example is simplified. We aim at sharing a functional, comprehensive illustration
    from the ground up. For a real-world application, we should consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning requires a discrete action space, which means continuous actions
    must be discretized into a finite set of values. Therefore, we converted price
    manipulation into a discrete set of actions `A = {+1, -1, 0}`. In reality, pricing
    decisions may be more complex and continuous.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'States should capture relevant information about the environment that helps
    the agent make decisions. Although discrete demand levels provide a simple and
    intuitive state representation, our choice may prove limiting in a real-world
    application. Instead, the state should embed any relevant feature to the environment
    (business scenario). For example, in a study on dynamic pricing for e-commerce
    platforms, [Liu et al. (2021)](https://arxiv.org/pdf/1912.02572.pdf) proposed
    a state representation made of four features categories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '- price features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- sales features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- customer traffic features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- competitiveness features.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5\. References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Watkins, 1989](https://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Watkins and Dayan, 1992](https://link.springer.com/article/10.1007/BF00992698)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sutton and Barto, 2018](http://incompleteideas.net/book/RLbook2020.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fleischmann, Hall, Pyke, 2004](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=845826)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cheng et al., 2023](https://arxiv.org/pdf/2302.06953.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Liu et al. (2021)](https://arxiv.org/pdf/1912.02572.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
