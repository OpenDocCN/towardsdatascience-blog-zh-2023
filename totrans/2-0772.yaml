- en: 'Dynamic Pricing with Reinforcement Learning from Scratch: Q-Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始的动态定价与强化学习：Q-Learning
- en: 原文：[https://towardsdatascience.com/dynamic-pricing-with-reinforcement-learning-from-scratch-q-learning-fb3fb764da49](https://towardsdatascience.com/dynamic-pricing-with-reinforcement-learning-from-scratch-q-learning-fb3fb764da49)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/dynamic-pricing-with-reinforcement-learning-from-scratch-q-learning-fb3fb764da49](https://towardsdatascience.com/dynamic-pricing-with-reinforcement-learning-from-scratch-q-learning-fb3fb764da49)
- en: An introduction to Q-Learning with a practical Python example
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 Q-Learning 并附带实际的 Python 示例
- en: '[](https://nicolo-albanese.medium.com/?source=post_page-----fb3fb764da49--------------------------------)[![Nicolo
    Cosimo Albanese](../Images/9a2c26207146741b58c3742927d09450.png)](https://nicolo-albanese.medium.com/?source=post_page-----fb3fb764da49--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fb3fb764da49--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fb3fb764da49--------------------------------)
    [Nicolo Cosimo Albanese](https://nicolo-albanese.medium.com/?source=post_page-----fb3fb764da49--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://nicolo-albanese.medium.com/?source=post_page-----fb3fb764da49--------------------------------)[![Nicolo
    Cosimo Albanese](../Images/9a2c26207146741b58c3742927d09450.png)](https://nicolo-albanese.medium.com/?source=post_page-----fb3fb764da49--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fb3fb764da49--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fb3fb764da49--------------------------------)
    [Nicolo Cosimo Albanese](https://nicolo-albanese.medium.com/?source=post_page-----fb3fb764da49--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fb3fb764da49--------------------------------)
    ·12 min read·Aug 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fb3fb764da49--------------------------------)
    ·12 分钟阅读·2023年8月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c762b0a93c8cd128f116b6cdd17b8eda.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c762b0a93c8cd128f116b6cdd17b8eda.png)'
- en: Exploring prices to find the optimal action-state values to maximize profit.
    Image by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 探索价格以寻找最佳的行动-状态值来最大化利润。图片由作者提供。
- en: Table of contents
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[Introduction](#502a)'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[介绍](#502a)'
- en: '[A primer on Reinforcement Learning](#9e6b)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[强化学习概述](#9e6b)'
- en: 2.1 [Key concepts](#6e64)
  id: totrans-11
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.1 [关键概念](#6e64)
- en: 2.2 [Q-function](#b05f)
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.2 [Q-函数](#b05f)
- en: 2.3 [Q-value](#fed6)
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.3 [Q-值](#fed6)
- en: 2.4 [Q-Learning](#3d03)
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.4 [Q-Learning](#3d03)
- en: 2.5 [The Bellman equation](#1112)
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.5 [贝尔曼方程](#1112)
- en: 2.6 [Exploration vs. exploitation](#3cf3)
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.6 [探索与利用](#3cf3)
- en: 2.7 [Q-Table](#7e4c)
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.7 [Q-表](#7e4c)
- en: '[The Dynamic Pricing problem](#f694)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[动态定价问题](#f694)'
- en: 3.1 [Problem statement](#589b)
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3.1 [问题陈述](#589b)
- en: 3.2 [Implementation](#7607)
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3.2 [实现](#7607)
- en: '[Conclusions](#68d5)'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[结论](#68d5)'
- en: '[References](#6cb9)'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[参考文献](#6cb9)'
- en: 1\. Introduction
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: In this post, we introduce the core concepts of Reinforcement Learning and dive
    into Q-Learning, an approach that empowers intelligent agents to learn optimal
    policies by making informed decisions based on rewards and experiences.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了强化学习的核心概念，并深入探讨 Q-Learning，一种使智能代理通过基于奖励和经验做出明智决策来学习最佳策略的方法。
- en: We also share a practical Python example built from the ground up. In particular,
    we train an agent to master the art of pricing, a crucial aspect of business,
    so that it can learn how to maximize profit.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还分享了一个从零开始构建的实际 Python 示例。特别是，我们训练一个代理掌握定价艺术，这是商业中的一个关键方面，以便它可以学习如何最大化利润。
- en: Without further ado, let us begin our journey.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 话不多说，让我们开始我们的旅程吧。
- en: 2\. A primer on Reinforcement Learning
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 强化学习概述
- en: 2.1 Key concepts
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 关键概念
- en: Reinforcement Learning (RL) is an area of Machine Learning where an agent learns
    to accomplish a task by trial and error.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是机器学习的一个领域，其中代理通过试错来学习完成任务。
- en: In brief, the agent tries actions which are associated to a positive or negative
    feedback through a reward mechanism. The agent adjusts its behavior to maximize
    a reward, thus learning the best course of action to achieve the final goal.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，代理尝试与正面或负面反馈相关的动作，通过奖励机制来调整其行为，以最大化奖励，从而学习实现最终目标的最佳行动路径。
- en: 'Let us introduce the key concepts of RL through a practical example. Imagine
    a simplified arcade game, where a cat should navigate a maze to collect treasures
    — a glass of milk and a ball of yarn — while avoiding construction sites:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个实际的例子介绍强化学习（RL）的关键概念。想象一个简化的街机游戏，在这个游戏中，一只猫需要穿越迷宫来收集宝物——一杯牛奶和一团毛线——同时避免施工现场：
- en: '![](../Images/4a16cd2293d152c9e46d88fae8c601b1.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a16cd2293d152c9e46d88fae8c601b1.png)'
- en: Image by author.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: The **agent** is the one choosing the course of actions. In the example, the
    agent is the player who controls the joystick deciding the next move of the cat.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**代理** 是选择行动路径的个体。在这个例子中，代理是控制操纵杆决定猫的下一步动作的玩家。'
- en: The **environment** is the context in which the agent is operating. In our case,
    a two-dimensional maze.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**环境** 是代理操作的背景。在我们的例子中，是一个二维迷宫。'
- en: 'An **action** `a` is a the minimum amount of steps to move from one state to
    another. In this game, the player has a finite set of possible actions to choose
    from: *up*, *left*, *down* and *right*.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**行动** `a` 是从一个状态移动到另一个状态所需的最小步数。在这个游戏中，玩家有有限的可能行动可供选择：*上*、*左*、*下* 和 *右*。'
- en: The **state** `s` represents the current situation of the player and the environment.
    It includes information such as the cat’s current and allowed positions, as well
    as the location of treasures and traps, and any other relevant feature to the
    game state (points, remaining lives, …).
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**状态** `s` 表示玩家和环境的当前情况。它包括猫的当前和允许的位置、宝藏和陷阱的位置，以及游戏状态的其他相关特征（分数、剩余生命等）。'
- en: 'The **reward** `r` represents the feedback assigned to the result of taking
    an action. For example, the game may assign:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**奖励** `r` 代表分配给采取某个行动结果的反馈。例如，游戏可能分配：'
- en: • +5 points when reaching the ball of yarn,
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: • 奖励 +5 分，当到达毛线球时，
- en: • +10 points for the glass of milk,
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: • 奖励 +10 分，针对牛奶杯，
- en: • -1 points for an empty cell,
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: • 惩罚 -1 分，针对空白单元格，
- en: • -10 points for a construction.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: • 惩罚 -10 分，针对建造。
- en: 'The described RL framework is depicted in the following figure:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 描述的 RL 框架在下图中显示：
- en: '![](../Images/0e2fbf6da700c48e8ded78c1a89d17e8.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e2fbf6da700c48e8ded78c1a89d17e8.png)'
- en: RL framework. Image by author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: RL 框架。图像由作者提供。
- en: Our goal is to learn a **policy** `π`, i.e., the set of rules that enables the
    agent to follow the course of action while maximizing the reward, thus achieving
    its target.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是学习一个 **策略** `π`，即一套规则，使代理能够在最大化奖励的同时遵循行动路径，从而实现目标。
- en: We can learn the optimal policy `π*` directly, or indirectly by learning the
    values (rewards) of action-state pairs, and using them to decide the best course
    of action. These two strategies are named **policy-based** and **value-based**,
    respectively. Let us now introduce Q-Learning, a popular value-based approach.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接学习最优策略 `π*`，或通过学习行动-状态对的值（奖励）间接学习，并利用它们决定最佳行动路径。这两种策略分别被称为 **基于策略** 和
    **基于价值**。现在让我们介绍 Q学习，一种流行的基于价值的方法。
- en: 2.2 Q-function
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 Q函数
- en: 'We introduce the **Q-function**, denoted as `Q(s,a)`, representing the expected
    cumulative reward an agent can achieve by taking action `a` in state `s` , while
    following the policy `π`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍 **Q函数**，表示为 `Q(s,a)`，代表代理在状态 `s` 中采取行动 `a` 时，遵循策略 `π` 所能获得的期望累积奖励：
- en: '![](../Images/2b63d9c960ba373707938146b95d3765.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b63d9c960ba373707938146b95d3765.png)'
- en: Q-function. Image by author.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Q函数。图像由作者提供。
- en: 'In the equation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程中：
- en: '`π`is the policy being followed by the agent.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`π` 是代理遵循的策略。'
- en: '`s` is the current state.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s` 是当前状态。'
- en: '`a` is the action taken in that state.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`a` 是在该状态下采取的行动。'
- en: '`r` is the reward associated to the given action and state.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r` 是与给定行动和状态相关的奖励。'
- en: '`t` represents the current iterate.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t` 代表当前迭代。'
- en: '`γ` is the **discount factor**. It represents the agent’s preference for immediate
    rewards (exploitation) over delayed rewards (exploration).'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`γ` 是 **折扣因子**。它代表代理对即时奖励（利用）相对于延迟奖励（探索）的偏好。'
- en: 2.3 Q-value
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 Q值
- en: The **Q-value** refers to the numeric value assigned by the Q-function to a
    specific state-action pair. In our example, the Q-value provides the expected
    cumulative reward the player could obtain by moving the cat in a new position
    inside the maze through a specific action, starting from a certain state. **In
    brief, it tells how “good” the player’s choice is**.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q值** 指 Q函数分配给特定状态-行动对的数值。在我们的例子中，Q值提供了玩家通过在迷宫中通过特定行动移动猫到新位置，起始于某个状态时，可能获得的期望累积奖励。**简言之，它告诉我们玩家的选择有多“好”**。'
- en: 2.4 Q-Learning
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 Q学习
- en: 'Given the concept of Q-value, the **Q-Learning** algorithm works as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 Q 值的概念，**Q学习** 算法的工作原理如下：
- en: '**Initialize Q-values arbitrarily**, e.g. `Q(s, a) = 0 ∀ s ∈ S, a ∈ A`.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化 Q 值** 任意，例如 `Q(s, a) = 0 ∀ s ∈ S, a ∈ A`。'
- en: 'For each episode:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个回合：
- en: 1\. Initialize state`s`
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1. 初始化状态 `s`
- en: '2\. For each step of episode:'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个步骤：
- en: 1\. **Choose action** `a`, observe the reward `r`, obtain new state `s'`
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1. **选择行动** `a`，观察奖励 `r`，获得新状态 `s'`
- en: 2\. Update the Q-values using the **Bellman equation** 3\. `s ← s'`
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2. 更新Q值使用**Bellman方程** 3. `s ← s'`
- en: Until `s` is terminal.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到`s`是终止状态。
- en: 2.5 The Bellman equation
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 Bellman方程
- en: 'The Bellman equation allows an agent to express the value of a state-action
    pair in terms of cumulative expected reward. It is used to update the Q-values
    in the Q-Learning algorithm as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Bellman方程允许代理用累积期望奖励的值来表示状态-动作对的价值。它用于在Q学习算法中更新Q值，如下所示：
- en: '![](../Images/63690ff9b3a38b0beb844f2875024e8f.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63690ff9b3a38b0beb844f2875024e8f.png)'
- en: Bellman equation. Image by author.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Bellman方程。图片由作者提供。
- en: 'In the previous expression:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的表达式中：
- en: The **learning rate** `α` (between 0 and 1) determines how much an agent updates
    the Q-values based on new experiences.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**`α`（介于0和1之间）决定了代理基于新经验更新Q值的程度。'
- en: The **discount rate** `γ` (between 0 and 1) influences the agent’s preference
    for immediate rewards over future rewards. A high `γ` can promote exploitation,
    as the agent will tend to prefer known actions with immediate gains.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**折扣率**`γ`（介于0和1之间）影响代理对即时奖励与未来奖励的偏好。较高的`γ`可以促进利用，因为代理会倾向于偏好已知的、带来即时收益的动作。'
- en: 2.6 Exploration vs. exploitation
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 探索与利用
- en: How does the agent choose the next action?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 代理如何选择下一个动作？
- en: The agent may “explore” new actions, or “exploit” actions known to be associated
    to a higher reward.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以“探索”新的动作，或“利用”已知与更高奖励相关的动作。
- en: 'To learn an effective policy, we should strike for a balance between exploration
    and exploitation during training. In our example, we can adopt a straightforward
    method by defining an exploration probability, i.e. a float between 0 and 1:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习有效的策略，我们应该在训练过程中在探索和利用之间取得平衡。在我们的例子中，我们可以通过定义一个探索概率，即介于0和1之间的浮点数，来采用一种简单的方法：
- en: If a random number from the uniform distribution on (0, 1) is **higher** than
    the exploration probability, the agent will perform **exploitation**, preferring
    already known actions with a high reward.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果从(0, 1)的均匀分布中生成的随机数**高于**探索概率，代理将执行**利用**，偏好已知的、高奖励的动作。
- en: If the number is **smaller** than the exploration probability, the agent will
    perform **exploration**, encouraging the experimentation of new actions.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数字**小于**探索概率，代理将执行**探索**，鼓励尝试新的动作。
- en: This approach is known as **epsilon-greedy** algorithm (see [Cheng et Al. 2023,
    Appendix C](https://arxiv.org/pdf/2302.06953.pdf)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法称为**epsilon-greedy**算法（参见[Cheng et Al. 2023, 附录 C](https://arxiv.org/pdf/2302.06953.pdf)）。
- en: 2.7 Q-Table
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 Q-表
- en: 'When the problem consists of a finite set of potential actions — such as *up*,
    *left*, *down* and *up*, it is possible to simply **tabulate** all the combinations
    of states and actions. This table, named **Q-Table**, is populated with Q-values
    during training, as the agent explores state and action pairs, and collects their
    associated reward. In our example:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当问题涉及有限数量的潜在动作时——例如*向上*、*向左*、*向下*和*向上*，可以简单地**列举**所有状态和动作的组合。这个表格，称为**Q-表**，在训练过程中会填充Q值，因为代理探索状态和动作对，并收集它们的相关奖励。在我们的例子中：
- en: '![](../Images/b34ea9c5c76ffee6e87510abc65c9b25.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b34ea9c5c76ffee6e87510abc65c9b25.png)'
- en: Updating the Q-Table. Image by author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 更新Q-表。图片由作者提供。
- en: 3\. The Dynamic Pricing problem
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 动态定价问题
- en: 'Given a product associated to a price and a demand, our goal is to train an
    intelligent agent that, using Reinforcement Learning, will adjust prices over
    time to maximize profit:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个与价格和需求相关的产品，我们的目标是训练一个智能代理，利用强化学习，随着时间的推移调整价格以最大化利润：
- en: “*Dynamic pricing is related to price-fixing for perishable resources taking
    into account demand so that to maximize revenue or profit*” ([Fleischmann, Hall,
    Pyke, 2004](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=845826)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: “*动态定价与对易腐资源的价格固定相关，考虑需求以最大化收入或利润*”（[Fleischmann, Hall, Pyke, 2004](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=845826)）。
- en: 3.1 Problem statement
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 问题陈述
- en: 'We model a simplified environment with a discrete **action** space `A`, where
    the agent can increase, decrease or keep the price constant: `A = {+1, -1, 0}`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对一个简化的环境进行建模，该环境具有离散的**动作**空间`A`，其中代理可以增加、减少或保持价格不变：`A = {+1, -1, 0}`。
- en: The action (price manipulation) results in a new demand, and we create discrete
    demand levels as **state** `S = {Low demand, Medium demand, High demand}`.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作（价格操控）会导致新的需求，我们将离散的需求水平创建为**状态**`S = {低需求, 中需求, 高需求}`。
- en: 'To estimate the new demand (state `s`) from a price change (action `a`), we
    leverage the concept of price elasticity `k`. Price elasticity estimates the sensitivity
    between a change in price `Δp` and its resulting change in demand `Δv`, and we
    assume it to be known in our example:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了从价格变化（动作 `a`）中估算新的需求（状态 `s`），我们利用了价格弹性 `k` 的概念。价格弹性估计价格 `Δp` 变化与其导致的需求 `Δv`
    变化之间的敏感度，我们假设在我们的例子中这是已知的：
- en: '![](../Images/b0ece6a54fd8e49941dcac7a1823e833.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0ece6a54fd8e49941dcac7a1823e833.png)'
- en: Image by author.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源于作者。
- en: 'The reward `r` corresponds to the **profit** stemmed from the application of
    a price `p` and its consequent demand `v`, considering the unitary costs `c` associated
    to the product:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励 `r` 对应于**利润**，它源于价格 `p` 的应用及其相应的需求 `v`，并考虑到与产品相关的单位成本 `c`：
- en: '![](../Images/8e15cc3be8f3eeac35772cd813946ab3.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e15cc3be8f3eeac35772cd813946ab3.png)'
- en: Reward r is a function of the action (price p) and state (demand v). Image by
    author.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励 `r` 是动作（价格 `p`）和状态（需求 `v`）的函数。图像来源于作者。
- en: We assign a negative reward when the new price increases or decreases too much
    compared to the initial price using an arbitrary threshold. In this way, we penalize
    strong price variations.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当新价格与初始价格相比增加或减少过多时，我们会根据一个任意阈值分配负奖励。这样，我们惩罚价格的强烈波动。
- en: 3.2 Implementation
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 实现
- en: 'The `DynamicPricingQL` class implements the following methods:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`DynamicPricingQL` 类实现了以下方法：'
- en: '`calculate_demand_level` assigns a continuous volume to a discrete state value
    (low, medium or high demand).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`calculate_demand_level` 将离散状态值（低、 中或高需求）分配给连续的需求量。'
- en: '`calculate_demand` uses an input price to estimate the volume through price
    elasticity.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`calculate_demand` 使用输入价格通过价格弹性来估算需求量。'
- en: '`fit` trains the agent. We decide to interrupt an episode when the maximum
    number of steps has been reached, or the profit (reward) has reached a certain
    threshold.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit` 训练代理。我们决定在达到最大步数时或利润（奖励）达到某个阈值时中断一个回合。'
- en: '`get_q_table` returns the Q-Table learned by the agent.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_q_table` 返回代理学习到的 Q-Table。'
- en: '`plot_rewards` shows a chart of the rewards achieved during training.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plot_rewards` 显示了训练过程中获得的奖励图表。'
- en: '`predict` uses the Q-values to predict the optimal price given a starting price
    and demand as input.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict` 使用 Q 值来预测给定起始价格和需求作为输入的最佳价格。'
- en: '[PRE0]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let us instantiate and fit the agent:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实例化并拟合代理：
- en: '[PRE1]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It is possible to get the Q-Table after training:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后可以获得 Q-Table：
- en: '[PRE3]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can also plot the rewards:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制奖励图：
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/c762b0a93c8cd128f116b6cdd17b8eda.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c762b0a93c8cd128f116b6cdd17b8eda.png)'
- en: Output of the code snippet. Image by author.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段的输出。图像来源于作者。
- en: We observe how the rewards increase during the training procedure, as the agent
    learns, through Q-values, the pricing policy leading to a profit increase.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到奖励在训练过程中增加，因为代理通过 Q 值学习了导致利润增加的定价策略。
- en: 'We can use the Q-values to predict the next price through the trained agent:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过训练好的代理使用 Q 值来预测下一个价格：
- en: '[PRE6]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 4\. Conclusions
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 结论
- en: In this post, we explored the key concepts of Reinforcement Learning and introduced
    the Q-Leaning method for training a smart agent. We also provided a hands-on Python
    example built from scratch. In particular, we implemented a dynamic pricing agent
    that learns the optimal pricing policy for a product in order to maximize profit.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探讨了强化学习的关键概念，并介绍了用于训练智能代理的 Q 学习方法。我们还提供了一个从头开始构建的实际 Python 示例。特别地，我们实现了一个动态定价代理，该代理学习了产品的最佳定价策略，以最大化利润。
- en: 'Our example is simplified. We aim at sharing a functional, comprehensive illustration
    from the ground up. For a real-world application, we should consider the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的例子是简化版的。我们旨在从头到尾分享一个功能全面的说明。对于实际应用，我们应考虑以下几点：
- en: Q-learning requires a discrete action space, which means continuous actions
    must be discretized into a finite set of values. Therefore, we converted price
    manipulation into a discrete set of actions `A = {+1, -1, 0}`. In reality, pricing
    decisions may be more complex and continuous.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q 学习需要离散的动作空间，这意味着连续动作必须被离散化为有限的值集合。因此，我们将价格操作转换为离散的动作集合 `A = {+1, -1, 0}`。实际上，定价决策可能更加复杂和连续。
- en: 'States should capture relevant information about the environment that helps
    the agent make decisions. Although discrete demand levels provide a simple and
    intuitive state representation, our choice may prove limiting in a real-world
    application. Instead, the state should embed any relevant feature to the environment
    (business scenario). For example, in a study on dynamic pricing for e-commerce
    platforms, [Liu et al. (2021)](https://arxiv.org/pdf/1912.02572.pdf) proposed
    a state representation made of four features categories:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 状态应捕捉有关环境的相关信息，以帮助智能体做出决策。虽然离散需求水平提供了简单直观的状态表示，但在实际应用中，这种选择可能会显得有限。相反，状态应包含对环境（业务场景）相关的任何特征。例如，在一个关于电子商务平台动态定价的研究中，[Liu
    等人（2021）](https://arxiv.org/pdf/1912.02572.pdf) 提出了由四类特征构成的状态表示：
- en: '- price features'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 价格特征'
- en: '- sales features'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 销售特征'
- en: '- customer traffic features'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 顾客流量特征'
- en: '- competitiveness features.'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 竞争力特征。'
- en: 5\. References
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 参考文献
- en: '[Watkins, 1989](https://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Watkins, 1989](https://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf)'
- en: '[Watkins and Dayan, 1992](https://link.springer.com/article/10.1007/BF00992698)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Watkins 和 Dayan, 1992](https://link.springer.com/article/10.1007/BF00992698)'
- en: '[Sutton and Barto, 2018](http://incompleteideas.net/book/RLbook2020.pdf)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sutton 和 Barto, 2018](http://incompleteideas.net/book/RLbook2020.pdf)'
- en: '[Fleischmann, Hall, Pyke, 2004](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=845826)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fleischmann, Hall, Pyke, 2004](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=845826)'
- en: '[Cheng et al., 2023](https://arxiv.org/pdf/2302.06953.pdf)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Cheng 等人, 2023](https://arxiv.org/pdf/2302.06953.pdf)'
- en: '[Liu et al. (2021)](https://arxiv.org/pdf/1912.02572.pdf)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Liu 等人（2021）](https://arxiv.org/pdf/1912.02572.pdf)'
