- en: A Beginnerâ€™s Guide to LLM Fine-Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMå¾®è°ƒçš„åˆå­¦è€…æŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672](https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672](https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672)
- en: How to fine-tune Llama and other LLMs with one tool
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä½¿ç”¨ä¸€ä¸ªå·¥å…·å¾®è°ƒLlamaåŠå…¶ä»–LLM
- en: '[](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    Â·8 min readÂ·Aug 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    Â·8åˆ†é’Ÿé˜…è¯»Â·2023å¹´8æœˆ30æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3cd56f68c14e07ab9ae3eb624bd064ed.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cd56f68c14e07ab9ae3eb624bd064ed.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: The growing interest in Large Language Models (LLMs) has led to a surge in **tools
    and wrappers designed to streamline their training process**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ—¥ç›Šå…³æ³¨å¯¼è‡´äº†**æ—¨åœ¨ç®€åŒ–å…¶è®­ç»ƒè¿‡ç¨‹çš„å·¥å…·å’Œå°è£…çš„æ¿€å¢**ã€‚
- en: Popular options include [FastChat](https://github.com/lm-sys/FastChat) from
    LMSYS (used to train [Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.5)) and
    Hugging Faceâ€™s [transformers](https://github.com/huggingface/transformers)/[trl](https://github.com/huggingface/trl)
    libraries (used in [my previous article](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32)).
    In addition, each big LLM project, like [WizardLM](https://github.com/nlpxucan/WizardLM/tree/main),
    tends to have its own training script, inspired by the original [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
    implementation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æµè¡Œçš„é€‰é¡¹åŒ…æ‹¬LMSYSçš„[FastChat](https://github.com/lm-sys/FastChat)ï¼ˆç”¨äºè®­ç»ƒ[Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.5)ï¼‰å’ŒHugging
    Faceçš„[transformers](https://github.com/huggingface/transformers)/[trl](https://github.com/huggingface/trl)åº“ï¼ˆç”¨äº[æˆ‘ä¹‹å‰çš„æ–‡ç« ](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32)ï¼‰ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªå¤§å‹LLMé¡¹ç›®ï¼Œå¦‚[WizardLM](https://github.com/nlpxucan/WizardLM/tree/main)ï¼Œé€šå¸¸éƒ½æœ‰è‡ªå·±ç‹¬ç‰¹çš„è®­ç»ƒè„šæœ¬ï¼Œçµæ„Ÿæ¥è‡ªäºæœ€åˆçš„[Alpaca](https://github.com/tatsu-lab/stanford_alpaca)å®ç°ã€‚
- en: In this article, we will use [**Axolotl**](https://github.com/OpenAccess-AI-Collective/axolotl),
    a tool created by the OpenAccess AI Collective. We will use it to fine-tune a
    [**Code Llama 7b**](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml)
    model on an evol-instruct dataset comprised of 1,000 samples of Python code.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç”±OpenAccess AI Collectiveåˆ›å»ºçš„[**Axolotl**](https://github.com/OpenAccess-AI-Collective/axolotl)å·¥å…·ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å®ƒåœ¨ä¸€ä¸ªåŒ…å«1,000ä¸ªPythonä»£ç æ ·æœ¬çš„evol-instructæ•°æ®é›†ä¸Šå¾®è°ƒä¸€ä¸ª[**Code
    Llama 7b**](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml)æ¨¡å‹ã€‚
- en: ğŸ¤” Why Axolotl?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¤” ä¸ºä»€ä¹ˆé€‰æ‹©Axolotlï¼Ÿ
- en: 'The main appeal of Axolotl is that it provides a one-stop solution, which includes
    numerous features, model architectures, and an active community. Hereâ€™s a quick
    list of my favorite things about it:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Axolotlçš„ä¸»è¦å¸å¼•åŠ›åœ¨äºå®ƒæä¾›äº†ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆï¼Œå…¶ä¸­åŒ…æ‹¬ä¼—å¤šåŠŸèƒ½ã€æ¨¡å‹æ¶æ„å’Œä¸€ä¸ªæ´»è·ƒçš„ç¤¾åŒºã€‚ä»¥ä¸‹æ˜¯æˆ‘æœ€å–œæ¬¢çš„å‡ ç‚¹ï¼š
- en: '**Configuration**: All parameters used to train an LLM are neatly stored in
    a yaml config file. This makes it convenient for sharing and reproducing models.
    You can see an example for Llama 2 [here](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples/llama-2).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é…ç½®**ï¼šç”¨äºè®­ç»ƒLLMçš„æ‰€æœ‰å‚æ•°éƒ½æ•´é½åœ°å­˜å‚¨åœ¨yamlé…ç½®æ–‡ä»¶ä¸­ã€‚è¿™ä½¿å¾—æ¨¡å‹çš„å…±äº«å’Œå†ç°å˜å¾—æ–¹ä¾¿ã€‚ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples/llama-2)æŸ¥çœ‹Llama
    2çš„ç¤ºä¾‹ã€‚'
- en: '**Dataset Flexibility**: Axolotl allows the specification of multiple datasets
    with varied prompt formats such as alpaca (`{"instruction": "...", "input": "...",
    "output": "..."}`), sharegpt:chat (`{"conversations": [{"from": "...", "value":
    "..."}]}`), and raw completion (`{"text": "..."}`). Combining datasets is seamless,
    and the hassle of unifying the prompt format is eliminated.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•°æ®é›†çµæ´»æ€§**ï¼šAxolotlå…è®¸æŒ‡å®šå¤šä¸ªæ•°æ®é›†ï¼Œæ”¯æŒå„ç§æç¤ºæ ¼å¼ï¼Œå¦‚alpacaï¼ˆ`{"instruction": "...", "input":
    "...", "output": "..."}`ï¼‰ï¼Œsharegpt:chatï¼ˆ`{"conversations": [{"from": "...", "value":
    "..."}]}`ï¼‰å’ŒåŸå§‹å®Œæˆï¼ˆ`{"text": "..."}`ï¼‰ã€‚ç»„åˆæ•°æ®é›†éå¸¸é¡ºç•…ï¼Œå¹¶ä¸”é¿å…äº†ç»Ÿä¸€æç¤ºæ ¼å¼çš„éº»çƒ¦ã€‚'
- en: '**Features**: Axolotl is packed with SOTA techniques such as FSDP, deepspeed,
    LoRA, QLoRA, ReLoRA, sample packing, GPTQ, FlashAttention, xformers, and rope
    scaling.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŠŸèƒ½**ï¼šAxolotlé…å¤‡äº†SOTAæŠ€æœ¯ï¼Œå¦‚FSDPã€deepspeedã€LoRAã€QLoRAã€ReLoRAã€æ ·æœ¬æ‰“åŒ…ã€GPTQã€FlashAttentionã€xformerså’Œrope
    scalingã€‚'
- en: '**Utilities**: There are numerous user-friendly utilities integrated, including
    the addition or alteration of special tokens, or a custom wandb configuration.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å·¥å…·**ï¼šé›†æˆäº†ä¼—å¤šç”¨æˆ·å‹å¥½çš„å·¥å…·ï¼ŒåŒ…æ‹¬æ·»åŠ æˆ–æ›´æ”¹ç‰¹æ®Štokenï¼Œæˆ–è‡ªå®šä¹‰wandbé…ç½®ã€‚'
- en: Some well-known models trained using this tool are [Manticore-13b](https://huggingface.co/openaccess-ai-collective/manticore-13b)
    from the OpenAccess AI Collective and [Samantha-1.11â€“70b](https://huggingface.co/ehartford/Samantha-1.11-70b)
    from Eric Hartford. Like other wrappers, it is built on top of the transformers
    library and uses many of its features.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›ä½¿ç”¨æ­¤å·¥å…·è®­ç»ƒçš„çŸ¥åæ¨¡å‹åŒ…æ‹¬OpenAccess AI Collectiveçš„[Manticore-13b](https://huggingface.co/openaccess-ai-collective/manticore-13b)å’ŒEric
    Hartfordçš„[Samantha-1.11â€“70b](https://huggingface.co/ehartford/Samantha-1.11-70b)ã€‚åƒå…¶ä»–åŒ…è£…å™¨ä¸€æ ·ï¼Œå®ƒå»ºç«‹åœ¨transformersåº“ä¹‹ä¸Šï¼Œå¹¶ä½¿ç”¨äº†è®¸å¤šå…¶åŠŸèƒ½ã€‚
- en: âš™ï¸ Create your own config file
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: âš™ï¸ åˆ›å»ºä½ è‡ªå·±çš„é…ç½®æ–‡ä»¶
- en: Before anything, we need a configuration file. You can reuse an existing configuration
    from the `[examples](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples)`
    folder. In our case, we will tweak the [QLoRA config](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml)
    for Llama 2 to create our own **Code Llama** model. The model will be trained
    on a subset of 1,000 Python samples from the `[nickrosh/Evol-Instruct-Code-80k-v1](https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1)`
    dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé…ç½®æ–‡ä»¶ã€‚ä½ å¯ä»¥é‡ç”¨`[examples](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples)`æ–‡ä»¶å¤¹ä¸­çš„ç°æœ‰é…ç½®ã€‚æˆ‘ä»¬å°†è°ƒæ•´[QLoRAé…ç½®](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml)ä»¥åˆ›å»ºæˆ‘ä»¬è‡ªå·±çš„**Code
    Llama**æ¨¡å‹ã€‚è¯¥æ¨¡å‹å°†åœ¨`[nickrosh/Evol-Instruct-Code-80k-v1](https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1)`æ•°æ®é›†ä¸­çš„1,000ä¸ªPythonæ ·æœ¬å­é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: First, we must change the `base_model` and `base_model_config` fields to "codellama/CodeLlama-7b-hf".
    To push our trained adapter to the Hugging Face Hub, let's add a new field `hub_model_id`,
    which corresponds to the name of our model, "EvolCodeLlama-7b". Now, we have to
    update the dataset to `[mlabonne/Evol-Instruct-Python-1k](https://huggingface.co/datasets/mlabonne/Evol-Instruct-Python-1k)`
    and set `type` to "alpaca".
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»å°†`base_model`å’Œ`base_model_config`å­—æ®µæ›´æ”¹ä¸º"codellama/CodeLlama-7b-hf"ã€‚ä¸ºäº†å°†æˆ‘ä»¬è®­ç»ƒçš„é€‚é…å™¨æ¨é€åˆ°Hugging
    Face Hubï¼Œæˆ‘ä»¬éœ€è¦æ·»åŠ ä¸€ä¸ªæ–°çš„å­—æ®µ`hub_model_id`ï¼Œå¯¹åº”æˆ‘ä»¬çš„æ¨¡å‹åç§°"EvolCodeLlama-7b"ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¿…é¡»å°†æ•°æ®é›†æ›´æ–°ä¸º`[mlabonne/Evol-Instruct-Python-1k](https://huggingface.co/datasets/mlabonne/Evol-Instruct-Python-1k)`å¹¶å°†`type`è®¾ç½®ä¸º"alpaca"ã€‚
- en: There's no sample bigger than 2048 tokens in this dataset, so we can reduce
    the `sequence_len` to "2048" and save some VRAM. Talking about VRAM, weâ€™re going
    to use a `micro_batch_size` of 10 and a `gradient_accumulation_steps` of 1 to
    maximize its use. In practice, you try different values until you use >95% of
    the available VRAM.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†ä¸­æ²¡æœ‰å¤§äº2048ä¸ªtokençš„æ ·æœ¬ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†`sequence_len`å‡å°‘åˆ°"2048"ä»¥èŠ‚çœä¸€äº›VRAMã€‚è¯´åˆ°VRAMï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`micro_batch_size`ä¸º10å’Œ`gradient_accumulation_steps`ä¸º1ï¼Œä»¥æœ€å¤§åŒ–å…¶ä½¿ç”¨ã€‚åœ¨å®é™…æ“ä½œä¸­ï¼Œä½ å¯ä»¥å°è¯•ä¸åŒçš„å€¼ï¼Œç›´åˆ°ä½¿ç”¨è¶…è¿‡95%çš„å¯ç”¨VRAMã€‚
- en: For convenience, I'm going to add the name "axolotl" to the `wandb_project`
    field so it's easier to track on my account. I'm also setting the `warmup_steps`
    to "100" (personal preference) and the `eval_steps` to 0.01 so we'll end up with
    100 evaluations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘å°†"axolotl"æ·»åŠ åˆ°`wandb_project`å­—æ®µä¸­ï¼Œä»¥ä¾¿åœ¨æˆ‘çš„è´¦æˆ·ä¸Šæ›´å®¹æ˜“è¿½è¸ªã€‚æˆ‘è¿˜å°†`warmup_steps`è®¾ç½®ä¸º"100"ï¼ˆä¸ªäººåå¥½ï¼‰ï¼Œå°†`eval_steps`è®¾ç½®ä¸º0.01ï¼Œä»¥ä¾¿è¿›è¡Œ100æ¬¡è¯„ä¼°ã€‚
- en: 'Hereâ€™s how the final config file should look:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆçš„é…ç½®æ–‡ä»¶åº”è¯¥æ˜¯è¿™æ ·çš„ï¼š
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can also find this config file [here](https://gist.github.com/mlabonne/8055f6335e2b85f082c8c75561321a66)
    as a GitHub gist.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥åœ¨[è¿™é‡Œ](https://gist.github.com/mlabonne/8055f6335e2b85f082c8c75561321a66)æ‰¾åˆ°è¿™ä¸ªé…ç½®æ–‡ä»¶ï¼Œä½œä¸ºä¸€ä¸ªGitHub
    gistã€‚
- en: 'Before we start training our model, I want to introduce a few parameters that
    are important to understand:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘æƒ³ä»‹ç»å‡ ä¸ªé‡è¦çš„å‚æ•°ï¼š
- en: '**QLoRA**: Weâ€™re using QLoRA for fine-tuning, which is why weâ€™re loading the
    base model in 4-bit precision (NF4 format). You can check [this article](https://medium.com/towards-data-science/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b)
    from [Benjamin Marie](https://medium.com/u/ad2a414578b3?source=post_page-----4bae7d4da672--------------------------------)
    to know more about QLoRA.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**QLoRA**ï¼šæˆ‘ä»¬ä½¿ç”¨QLoRAè¿›è¡Œå¾®è°ƒï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä»¥4ä½ç²¾åº¦ï¼ˆNF4æ ¼å¼ï¼‰åŠ è½½åŸºç¡€æ¨¡å‹ã€‚ä½ å¯ä»¥æŸ¥çœ‹[è¿™ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b)æ¥äº†è§£æ›´å¤šå…³äºQLoRAçš„å†…å®¹ï¼Œä½œè€…æ˜¯[æœ¬æ°æ˜Â·ç›ä¸½](https://medium.com/u/ad2a414578b3?source=post_page-----4bae7d4da672--------------------------------)ã€‚'
- en: '**Gradient checkpointing**: It lowers the VRAM requirements by removing some
    activations that are re-computed on demand during the backward pass. It also slows
    down training by about 20%, according to Hugging Faceâ€™s [documentation](https://huggingface.co/docs/transformers/v4.18.0/en/performance).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼šé€šè¿‡åˆ é™¤åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æŒ‰éœ€é‡æ–°è®¡ç®—çš„ä¸€äº›æ¿€æ´»æ¥é™ä½VRAMéœ€æ±‚ã€‚æ ¹æ®Hugging Faceçš„[æ–‡æ¡£](https://huggingface.co/docs/transformers/v4.18.0/en/performance)ï¼Œå®ƒè¿˜ä¼šä½¿è®­ç»ƒé€Ÿåº¦é™ä½çº¦20%ã€‚'
- en: '**FlashAttention**: This implements the [FlashAttention](https://github.com/Dao-AILab/flash-attention)
    mechanism, which improves the speed and memory efficiency of our model thanks
    to a clever fusion of GPU operations (learn more about it in [this article](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)
    from [Aleksa GordiÄ‡](https://medium.com/u/37f02ae83e8c?source=post_page-----4bae7d4da672--------------------------------)).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FlashAttention**ï¼šè¿™å®ç°äº†[FlashAttention](https://github.com/Dao-AILab/flash-attention)æœºåˆ¶ï¼Œé€šè¿‡å·§å¦™èåˆGPUæ“ä½œæ¥æé«˜æ¨¡å‹çš„é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡ï¼ˆäº†è§£æ›´å¤šå†…å®¹ï¼Œè¯·å‚é˜…[è¿™ç¯‡æ–‡ç« ](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)ï¼Œä½œè€…æ˜¯[Aleksa
    GordiÄ‡](https://medium.com/u/37f02ae83e8c?source=post_page-----4bae7d4da672--------------------------------)ï¼‰ã€‚'
- en: '**Sample packing**: Smart way of creating batches with as little padding as
    possible, by reorganizing the order of the samples ([bin packing problem](https://en.wikipedia.org/wiki/Bin_packing_problem)).
    As a result, we need fewer batches to train the model on the same dataset. It
    was inspired by the [Multipack Sampler](https://github.com/imoneoi/multipack_sampler/tree/master)
    (see [my note](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/multipack_sampler.html))
    and [Krell et al.](https://arxiv.org/pdf/2107.02027.pdf)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ ·æœ¬æ‰“åŒ…**ï¼šä¸€ç§é€šè¿‡é‡æ–°ç»„ç»‡æ ·æœ¬é¡ºåºæ¥åˆ›å»ºå°½å¯èƒ½å°‘å¡«å……çš„æ‰¹æ¬¡çš„æ™ºèƒ½æ–¹æ³•ï¼ˆ[è£…ç®±é—®é¢˜](https://en.wikipedia.org/wiki/Bin_packing_problem)ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ›´å°‘çš„æ‰¹æ¬¡æ¥è®­ç»ƒç›¸åŒçš„æ•°æ®é›†ã€‚å®ƒçš„çµæ„Ÿæ¥è‡ªäº[Multipack
    Sampler](https://github.com/imoneoi/multipack_sampler/tree/master)ï¼ˆè§[æˆ‘çš„ç¬”è®°](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/multipack_sampler.html)ï¼‰å’Œ[Krellç­‰äºº](https://arxiv.org/pdf/2107.02027.pdf)ã€‚'
- en: You can find FlashAttention in some other tools, but sample packing is relatively
    new. As far as I know, [OpenChat](https://github.com/imoneoi/openchat) was the
    first project to use sample packing during fine-tuning. Thanks to Axolotl, weâ€™ll
    use these techniques for free.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨å…¶ä»–ä¸€äº›å·¥å…·ä¸­æ‰¾åˆ°FlashAttentionï¼Œä½†æ ·æœ¬æ‰“åŒ…ç›¸å¯¹è¾ƒæ–°ã€‚æ®æˆ‘æ‰€çŸ¥ï¼Œ[OpenChat](https://github.com/imoneoi/openchat)æ˜¯ç¬¬ä¸€ä¸ªåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä½¿ç”¨æ ·æœ¬æ‰“åŒ…çš„é¡¹ç›®ã€‚æ„Ÿè°¢Axolotlï¼Œæˆ‘ä»¬å¯ä»¥å…è´¹ä½¿ç”¨è¿™äº›æŠ€æœ¯ã€‚
- en: ğŸ¦™ Fine-tune Code Llama
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦™ å¾®è°ƒCode Llama
- en: Having the config file ready, itâ€™s time to get our hands dirty with the actual
    fine-tuning. You might consider running the training on a Colab notebook. However,
    for those without access to a high-performance GPU, a more cost-effective solution
    consists of renting **cloud-based GPU services**, like AWS, [Lambda Labs](https://lambdalabs.com/),
    [Vast.ai](https://vast.ai/), [Banana](https://www.banana.dev/), or [RunPod](https://www.runpod.io/).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®æ–‡ä»¶å‡†å¤‡å¥½ä¹‹åï¼Œå°±è¯¥å¼€å§‹å®é™…çš„å¾®è°ƒå·¥ä½œäº†ã€‚ä½ å¯ä»¥è€ƒè™‘åœ¨Colabç¬”è®°æœ¬ä¸Šè¿è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œå¯¹äºé‚£äº›æ²¡æœ‰é«˜æ€§èƒ½GPUçš„äººæ¥è¯´ï¼Œä¸€ä¸ªæ›´å…·æˆæœ¬æ•ˆç›Šçš„è§£å†³æ–¹æ¡ˆæ˜¯ç§Ÿç”¨**åŸºäºäº‘çš„GPUæœåŠ¡**ï¼Œå¦‚AWSã€[Lambda
    Labs](https://lambdalabs.com/)ã€[Vast.ai](https://vast.ai/)ã€[Banana](https://www.banana.dev/)æˆ–[RunPod](https://www.runpod.io/)ã€‚
- en: Personally, I use RunPod, which is a popular option in the fine-tuning community.
    Itâ€™s not the cheapest service but it hits a good tradeoff with a clean UI. You
    can easily replicate the following steps using your favorite service.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æˆ‘ä¸ªäººè€Œè¨€ï¼Œæˆ‘ä½¿ç”¨RunPodï¼Œè¿™æ˜¯å¾®è°ƒç¤¾åŒºä¸­ä¸€ä¸ªå—æ¬¢è¿çš„é€‰é¡¹ã€‚å®ƒä¸æ˜¯æœ€ä¾¿å®œçš„æœåŠ¡ï¼Œä½†åœ¨ç•Œé¢æ¸…æ™°åº¦å’Œæˆæœ¬ä¹‹é—´è¾¾åˆ°äº†ä¸€ä¸ªè‰¯å¥½çš„æŠ˜ä¸­ã€‚ä½ å¯ä»¥ä½¿ç”¨ä½ å–œæ¬¢çš„æœåŠ¡è½»æ¾å¤åˆ¶ä»¥ä¸‹æ­¥éª¤ã€‚
- en: 'When your RunPod account is set up, go to Manage > Templates and click on â€œNew
    Templateâ€. Here is a simple template:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ çš„RunPodè´¦æˆ·è®¾ç½®å¥½åï¼Œè½¬åˆ°â€œç®¡ç†â€>â€œæ¨¡æ¿â€å¹¶ç‚¹å‡»â€œæ–°å»ºæ¨¡æ¿â€ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„æ¨¡æ¿ï¼š
- en: '![](../Images/827c395c5294c1d157914fe4661ef301.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/827c395c5294c1d157914fe4661ef301.png)'
- en: Image by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: 'Letâ€™s review the different fields and their corresponding values:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹ä¸åŒçš„å­—æ®µåŠå…¶å¯¹åº”çš„å€¼ï¼š
- en: '**Template Name**: Axolotl (you can choose whatever you want)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¨¡æ¿åç§°**ï¼šAxolotlï¼ˆä½ å¯ä»¥é€‰æ‹©ä»»ä½•ä½ æƒ³è¦çš„åç§°ï¼‰'
- en: '**Container Image**: winglian/axolotl-runpod:main-py3.10-cu118â€“2.0.1'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å®¹å™¨é•œåƒ**: winglian/axolotl-runpod:main-py3.10-cu118â€“2.0.1'
- en: '**Container Disk**: 100 GB'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å®¹å™¨ç£ç›˜**: 100 GB'
- en: '**Volume Disk**: 0 GB'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­˜å‚¨å·**: 0 GB'
- en: '**Volume Mount Path**: /workspace'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å·æŒ‚è½½è·¯å¾„**: /workspace'
- en: 'In addition, there are two handy environment variables can include:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè¿˜æœ‰ä¸¤ä¸ªæœ‰ç”¨çš„ç¯å¢ƒå˜é‡å¯ä»¥åŒ…å«ï¼š
- en: '**HUGGING_FACE_HUB_TOKEN**: you can find your token on [this page](https://huggingface.co/settings/tokens)
    (requires an account)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HUGGING_FACE_HUB_TOKEN**: ä½ å¯ä»¥åœ¨ [æ­¤é¡µé¢](https://huggingface.co/settings/tokens)
    ä¸Šæ‰¾åˆ°ä½ çš„ä»¤ç‰Œï¼ˆéœ€è¦è´¦æˆ·ï¼‰'
- en: '**WANDB_API_KEY**: you can find your key on [this page](https://wandb.ai/authorize)
    (requires an account)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WANDB_API_KEY**: ä½ å¯ä»¥åœ¨ [æ­¤é¡µé¢](https://wandb.ai/authorize) ä¸Šæ‰¾åˆ°ä½ çš„å¯†é’¥ï¼ˆéœ€è¦è´¦æˆ·ï¼‰'
- en: 'Alternatively, you can simply log in the terminal later (using huggingface-cli
    login and wandb login). Once youâ€™re set-up, go to Community Cloud and deploy an
    RTX 3090\. Here you can search for the name of your template and select it as
    follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œä½ å¯ä»¥ç¨ååœ¨ç»ˆç«¯ç™»å½•ï¼ˆä½¿ç”¨ huggingface-cli login å’Œ wandb loginï¼‰ã€‚è®¾ç½®å®Œæˆåï¼Œå‰å¾€ Community Cloud
    å¹¶éƒ¨ç½² RTX 3090ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œæœç´¢ä½ çš„æ¨¡æ¿åç§°å¹¶é€‰æ‹©å®ƒï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/93732516ab48d2df1b5b62b9641f1117.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93732516ab48d2df1b5b62b9641f1117.png)'
- en: Image by author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: You can click on â€œContinueâ€ and RunPod will deploy your template. You can see
    the installation in your podâ€™s logs (Manage > Pods). When the option becomes available,
    click on â€œConnectâ€. Here, click on â€œStart Web Terminalâ€ and then â€œConnect to Web
    Terminalâ€. You are now connected to your pod!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ç‚¹å‡»â€œç»§ç»­â€ï¼ŒRunPod å°†éƒ¨ç½²ä½ çš„æ¨¡æ¿ã€‚ä½ å¯ä»¥åœ¨ä½ çš„ pod çš„æ—¥å¿—ä¸­æŸ¥çœ‹å®‰è£…æƒ…å†µï¼ˆç®¡ç† > Podsï¼‰ã€‚å½“é€‰é¡¹å¯ç”¨æ—¶ï¼Œç‚¹å‡»â€œè¿æ¥â€ã€‚åœ¨è¿™é‡Œï¼Œç‚¹å‡»â€œå¯åŠ¨
    Web ç»ˆç«¯â€ï¼Œç„¶åâ€œè¿æ¥åˆ° Web ç»ˆç«¯â€ã€‚ä½ ç°åœ¨å·²è¿æ¥åˆ°ä½ çš„ podï¼
- en: 'The following steps are **the same no matter what service you choose**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ­¥éª¤æ˜¯**æ— è®ºä½ é€‰æ‹©å“ªä¸ªæœåŠ¡éƒ½ç›¸åŒ**çš„ï¼š
- en: 'We install Axolotl and the PEFT library as follows:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æŒ‰å¦‚ä¸‹æ–¹å¼å®‰è£… Axolotl å’Œ PEFT åº“ï¼š
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '2\. Download the config file we created:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. ä¸‹è½½æˆ‘ä»¬åˆ›å»ºçš„é…ç½®æ–‡ä»¶ï¼š
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '3\. You can now **start fine-tuning the model** with the following command:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. ä½ ç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ **å¼€å§‹å¾®è°ƒæ¨¡å‹**ï¼š
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If everything is configured correctly, you should be able to train the model
    in a little more than **one hour** (it took me 1h 11m 44s). If you check the GPU
    memory used, youâ€™ll see almost 100% with this config, which means weâ€™re optimizing
    it pretty nicely. If youâ€™re using a GPU with more VRAM (like an A100), you can
    increase the micro-batch size to make sure youâ€™re fully using it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸€åˆ‡é…ç½®æ­£ç¡®ï¼Œä½ åº”è¯¥èƒ½å¤Ÿåœ¨ **ä¸€ä¸ªå¤šå°æ—¶** å†…è®­ç»ƒæ¨¡å‹ï¼ˆæˆ‘èŠ±äº† 1å°æ—¶ 11åˆ†é’Ÿ 44ç§’ï¼‰ã€‚å¦‚æœä½ æ£€æŸ¥ä½¿ç”¨çš„ GPU å†…å­˜ï¼Œä½ ä¼šå‘ç°å‡ ä¹è¾¾åˆ° 100%ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æ­£åœ¨å¾ˆå¥½åœ°ä¼˜åŒ–å®ƒã€‚å¦‚æœä½ ä½¿ç”¨å…·æœ‰æ›´å¤š
    VRAM çš„ GPUï¼ˆå¦‚ A100ï¼‰ï¼Œä½ å¯ä»¥å¢åŠ å¾®æ‰¹é‡å¤§å°ä»¥ç¡®ä¿å……åˆ†åˆ©ç”¨å®ƒã€‚
- en: 'In the meantime, feel free to close the web terminal and check your loss on
    Weights & Biases. Weâ€™re using tmux so the training wonâ€™t stop if you close the
    terminal. Here are my loss curves:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ­¤åŒæ—¶ï¼Œå¯ä»¥å…³é—­ Web ç»ˆç«¯ï¼Œå¹¶åœ¨ Weights & Biases ä¸Šæ£€æŸ¥ä½ çš„æŸå¤±ã€‚æˆ‘ä»¬ä½¿ç”¨ tmuxï¼Œæ‰€ä»¥å³ä½¿ä½ å…³é—­ç»ˆç«¯ï¼Œè®­ç»ƒä¹Ÿä¸ä¼šåœæ­¢ã€‚ä»¥ä¸‹æ˜¯æˆ‘çš„æŸå¤±æ›²çº¿ï¼š
- en: '![](../Images/fab0126f54062cf424f89f11e300d20c.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fab0126f54062cf424f89f11e300d20c.png)'
- en: Image by author
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'We see a steady improvement in the eval loss, which is a good sign. However,
    you can also spot drops in the eval loss that are not correlated with a decrease
    in the quality of the outputsâ€¦ The best way to evaluate your model is simply by
    using it: you can run it in the terminal with the command `accelerate launch scripts/finetune.py
    EvolCodeLlama-7b.yaml --inference --lora_model_dir="./qlora-out"`.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°è¯„ä¼°æŸå¤±ç¨³æ­¥æ”¹å–„ï¼Œè¿™æ˜¯ä¸€ä¸ªå¥½å…†å¤´ã€‚ç„¶è€Œï¼Œä½ ä¹Ÿå¯ä»¥å‘ç°è¯„ä¼°æŸå¤±çš„ä¸‹é™å¹¶æœªä¸è¾“å‡ºè´¨é‡çš„é™ä½ç›¸å…³è”â€¦ è¯„ä¼°ä½ çš„æ¨¡å‹çš„æœ€ä½³æ–¹å¼æ˜¯ç›´æ¥ä½¿ç”¨å®ƒï¼šä½ å¯ä»¥åœ¨ç»ˆç«¯ä¸­è¿è¡Œå‘½ä»¤
    `accelerate launch scripts/finetune.py EvolCodeLlama-7b.yaml --inference --lora_model_dir="./qlora-out"`ã€‚
- en: 'The QLoRA adapter should already be uploaded to the Hugging Face Hub. However,
    you can also **merge the base Code Llama model with this adapter and push the
    merged model** there by following these steps:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA é€‚é…å™¨åº”è¯¥å·²ç»ä¸Šä¼ åˆ° Hugging Face Hubã€‚ç„¶è€Œï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤**å°†åŸºç¡€ Code Llama æ¨¡å‹ä¸æ­¤é€‚é…å™¨åˆå¹¶å¹¶å°†åˆå¹¶åçš„æ¨¡å‹æ¨é€**åˆ°é‚£é‡Œï¼š
- en: 'Download [this script](https://gist.github.com/mlabonne/a3542b0519708b8871d0703c938bba9f):'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸‹è½½ [è¿™ä¸ªè„šæœ¬](https://gist.github.com/mlabonne/a3542b0519708b8871d0703c938bba9f)ï¼š
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '2\. Execute it with this command:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ‰§è¡Œï¼š
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Congratulations, you should have **your own EvolCodeLlama-7b** on the Hugging
    Face Hub at this point! For reference, you can access my own model trained with
    this process here: `[mlabonne/EvolCodeLlama-7b](https://huggingface.co/mlabonne/EvolCodeLlama-7b)`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ­å–œï¼Œä½ ç°åœ¨åº”è¯¥åœ¨ Hugging Face Hub ä¸Šæ‹¥æœ‰**ä½ è‡ªå·±çš„ EvolCodeLlama-7b**ï¼ä½œä¸ºå‚è€ƒï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œè®¿é—®æˆ‘ç”¨è¿™ä¸ªè¿‡ç¨‹è®­ç»ƒçš„æ¨¡å‹:
    `[mlabonne/EvolCodeLlama-7b](https://huggingface.co/mlabonne/EvolCodeLlama-7b)`'
- en: 'Considering that our EvolCodeLlama-7b is a code LLM, it would be interesting
    to compare its performance with other models on **standard benchmarks**, such
    as [HumanEval](https://github.com/openai/human-eval) and [MBPP](https://github.com/google-research/google-research/tree/master/mbpp).
    For reference, you can find a leaderboard at the following address: [Multilingual
    Code Evals](https://huggingface.co/spaces/bigcode/multilingual-code-evals).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ°æˆ‘ä»¬çš„EvolCodeLlama-7bæ˜¯ä¸€ä¸ªä»£ç LLMï¼Œæ¯”è¾ƒå®ƒåœ¨**æ ‡å‡†åŸºå‡†**ä¸Šçš„è¡¨ç°ä¸å…¶ä»–æ¨¡å‹ä¼šå¾ˆæœ‰è¶£ï¼Œä¾‹å¦‚[HumanEval](https://github.com/openai/human-eval)å’Œ[MBPP](https://github.com/google-research/google-research/tree/master/mbpp)ã€‚ä½œä¸ºå‚è€ƒï¼Œä½ å¯ä»¥åœ¨ä»¥ä¸‹åœ°å€æ‰¾åˆ°æ’è¡Œæ¦œï¼š[å¤šè¯­è¨€ä»£ç è¯„ä¼°](https://huggingface.co/spaces/bigcode/multilingual-code-evals)ã€‚
- en: If youâ€™re happy with this model, you can **quantize** it with GGML for local
    inference with [this free Google Colab notebook](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing).
    You can also fine-tune **bigger models** (e.g., 70b parameters) thanks to [deepspeed](https://github.com/microsoft/DeepSpeed),
    which only requires an additional config file.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹è¿™ä¸ªæ¨¡å‹æ»¡æ„ï¼Œä½ å¯ä»¥ä½¿ç”¨GGMLè¿›è¡Œ**é‡åŒ–**ä»¥ä¾¿æœ¬åœ°æ¨ç†ï¼Œä½¿ç”¨[è¿™ä¸ªå…è´¹çš„Google Colabç¬”è®°æœ¬](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing)ã€‚ä½ è¿˜å¯ä»¥åˆ©ç”¨[deepspeed](https://github.com/microsoft/DeepSpeed)å¯¹**æ›´å¤§çš„æ¨¡å‹**ï¼ˆä¾‹å¦‚ï¼Œ70bå‚æ•°ï¼‰è¿›è¡Œå¾®è°ƒï¼Œåªéœ€é¢å¤–çš„é…ç½®æ–‡ä»¶å³å¯ã€‚
- en: Conclusion
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, weâ€™ve covered the essentials of **how to efficiently fine-tune
    LLMs**. We customized parameters to train on our Code Llama model on a small Python
    dataset. Finally, we merged the weights and uploaded the result on Hugging Face.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¦†ç›–äº†**å¦‚ä½•é«˜æ•ˆå¾®è°ƒLLMs**çš„åŸºæœ¬è¦ç‚¹ã€‚æˆ‘ä»¬è‡ªå®šä¹‰äº†å‚æ•°ï¼Œåœ¨ä¸€ä¸ªå°çš„Pythonæ•°æ®é›†ä¸Šè®­ç»ƒäº†æˆ‘ä»¬çš„Code Llamaæ¨¡å‹ã€‚æœ€åï¼Œæˆ‘ä»¬åˆå¹¶äº†æƒé‡ï¼Œå¹¶å°†ç»“æœä¸Šä¼ åˆ°Hugging
    Faceã€‚
- en: I hope you found this guide useful. I recommend using Axolotl with a cloud-based
    GPU service to get some experience and upload a few models on Hugging Face. Build
    your own datasets, play with the parameters, and break stuff along the way. Like
    with every wrapper, donâ€™t hesitate to check the source code to get a good intuition
    of what itâ€™s actually doing. It will massively help in the long run.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›ä½ è§‰å¾—è¿™ä»½æŒ‡å—æœ‰ç”¨ã€‚æˆ‘æ¨èä½¿ç”¨Axolotlä¸åŸºäºäº‘çš„GPUæœåŠ¡æ¥è·å–ä¸€äº›ç»éªŒï¼Œå¹¶åœ¨Hugging Faceä¸Šä¸Šä¼ å‡ ä¸ªæ¨¡å‹ã€‚æ„å»ºè‡ªå·±çš„æ•°æ®é›†ï¼Œè°ƒæ•´å‚æ•°ï¼Œè¿‡ç¨‹ä¸­å¯èƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ã€‚å°±åƒä½¿ç”¨ä»»ä½•åŒ…è£…å™¨ä¸€æ ·ï¼Œä¸è¦çŠ¹è±«å»æŸ¥çœ‹æºä»£ç ï¼Œä»¥ä¾¿å¯¹å…¶å®é™…æ“ä½œæœ‰ä¸€ä¸ªå¾ˆå¥½çš„ç›´è§‚äº†è§£ã€‚è¿™æ ·åœ¨é•¿æœŸå†…ä¼šå¤§å¤§å—ç›Šã€‚
- en: Thanks to the OpenAccess AI Collective and all the contributors!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢OpenAccess AI Collectiveå’Œæ‰€æœ‰è´¡çŒ®è€…ï¼
- en: If youâ€™re interested in more technical content around LLMs, [follow me on Medium](https://medium.com/@mlabonne).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹LLMsçš„æ›´å¤šæŠ€æœ¯å†…å®¹æ„Ÿå…´è¶£ï¼Œ[åœ¨Mediumä¸Šå…³æ³¨æˆ‘](https://medium.com/@mlabonne)ã€‚
- en: Related articles
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›¸å…³æ–‡ç« 
- en: '[](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----4bae7d4da672--------------------------------)
    [## Fine-Tune Your Own Llama 2 Model in a Colab Notebook'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[åœ¨Colabç¬”è®°æœ¬ä¸­å¾®è°ƒä½ è‡ªå·±çš„Llama 2æ¨¡å‹](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----4bae7d4da672--------------------------------)'
- en: A practical introduction to LLM fine-tuning
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLMå¾®è°ƒçš„å®ç”¨ä»‹ç»
- en: towardsdatascience.com](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----4bae7d4da672--------------------------------)
    [](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----4bae7d4da672--------------------------------)
    [## 4-bit Quantization with GPTQ
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[4ä½é‡åŒ–ä¸GPTQ](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----4bae7d4da672--------------------------------) '
- en: Quantize your own LLMs using AutoGPTQ
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨AutoGPTQå¯¹è‡ªå·±çš„LLMsè¿›è¡Œé‡åŒ–
- en: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----4bae7d4da672--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[4ä½é‡åŒ–ä¸GPTQ](https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----4bae7d4da672--------------------------------)'
- en: '*Learn more about machine learning and support my work with one click â€” become
    a Medium member here:*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*äº†è§£æ›´å¤šå…³äºæœºå™¨å­¦ä¹ çš„å†…å®¹ï¼Œå¹¶é€šè¿‡ä¸€é”®æ”¯æŒæˆ‘çš„å·¥ä½œâ€”â€”åœ¨è¿™é‡Œæˆä¸ºMediumä¼šå‘˜ï¼š*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----4bae7d4da672--------------------------------)
    [## Join Medium with my referral link - Maxime Labonne'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[åŠ å…¥Mediumï¼Œä½¿ç”¨æˆ‘çš„æ¨èé“¾æ¥ - Maxime Labonne](https://medium.com/@mlabonne/membership?source=post_page-----4bae7d4da672--------------------------------)'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every storyâ€¦
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½œä¸ºMediumä¼šå‘˜ï¼Œä½ çš„ä¼šå‘˜è´¹ç”¨çš„ä¸€éƒ¨åˆ†ä¼šç»™ä½ é˜…è¯»çš„ä½œè€…ï¼ŒåŒæ—¶ä½ å¯ä»¥å…¨é¢è®¿é—®æ¯ä¸ªæ•…äº‹â€¦
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----4bae7d4da672--------------------------------)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@mlabonne/membership?source=post_page-----4bae7d4da672--------------------------------)'
