- en: Recurrent Neural Networks, Explained and Visualized from the Ground Up
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/recurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe](https://towardsdatascience.com/recurrent-neural-networks-explained-and-visualized-from-the-ground-up-51c023f2b6fe)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0b3a777d2e37126f432db24797c1b6ba.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: '[Unsplash](https://unsplash.com/photos/rSrK-P0Wips)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: With an application to machine translation
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://andre-ye.medium.com/?source=post_page-----51c023f2b6fe--------------------------------)[![Andre
    Ye](../Images/c69b022a665d3ee13f9fec2f1f73bf32.png)](https://andre-ye.medium.com/?source=post_page-----51c023f2b6fe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51c023f2b6fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51c023f2b6fe--------------------------------)
    [Andre Ye](https://andre-ye.medium.com/?source=post_page-----51c023f2b6fe--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----51c023f2b6fe--------------------------------)
    ·23 min read·Jun 22, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNNs) are neural networks that can operate sequentially.
    Although they’re not as popular as they were even just several years ago, they
    represent an important development in the progression of deep learning and are
    a natural extension of feedforward networks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, we’ll cover the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The step from feedforward to recurrent networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayer recurrent networks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long short-term memory networks (LSTMs)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequential output (‘text output’)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bidirectionality
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoregressive generation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An application to machine translation (a high-level understanding of Google
    Translate’s 2016 model architecture)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The aim of the post is not only to explain how RNNs work (there are plenty of
    posts which do that), but to explore their design choices and high-level intuitive
    logic with the aid of illustrations. I hope this article will provide some unique
    value not only to your grasp of this particular technical topic but also more
    generally the flexibility of deep learning design.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'The design of the [Recurrent Neural Network](https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf)
    (1985) is premised upon two observations about how an ideal model, such as a human
    reading text, would process sequential information:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '*It should track the information ‘learned’ so far so it can relate new information
    to previously seen information*. To understand the sentence “the quick brown fox
    jumped over the lazy dog”, I need to keep track of the words ‘quick’ and ‘brown’
    to understand later that these apply to the word ‘fox’. If I do not retain any
    of this information in my ‘short-term memory’, so to speak, I will not understand
    the sequential significance of information. When I finish the sentence on ‘lazy
    dog’, I read this noun in relationship to the ‘quick brown fox’ which I previous
    encountered.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Even though later information will always be read in the context of earlier
    information, we want to process each word (token) in a similar way regardless
    of its position*. We should not for some reason systematically transform the word
    at the third position different from the word in the first position, even though
    we might read the former in light of the latter. Note that the previous proposed
    approach — in which embeddings for all tokens are stacked side-to-side and presented
    simultaneously to the model — does not possess this property, since there is no
    guarantee that the embedding corresponding to the first word is read with the
    same rules as the embedding corresponding to the third one. This general property
    is also known as positional invariance.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Recurrent Neural Network is comprised, at the core, of recurrent layers. A
    recurrent layer, like a feed-forward layer, is a set of learnable mathematical
    transformations. It turns out that we can approximately understand recurrent layers
    in terms of Multi-Layer Perceptrons.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'The ‘short-term memory’ of a recurrent layer is referred to as its *hidden
    state*. This is a vector — just a list of numbers — which communicates crucial
    information about what the network has learned so far. Then, for every token in
    the standardized text, we *incorporate the new information into the hidden state*.
    We do this using two MLPs: one MLP transforms the current embedding, and the other
    transforms the current hidden state. The outputs of these two MLPs are added together
    to form the updated hidden state, or the ‘updated short-term memory’.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d496c088659d1b9f85ebf37ad7cdd03.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: 'We then repeat this for the next token — the embedding is passed into an MLP
    and the updated hidden state is passed into another; the outputs of both are added
    together. This is repeated for each token in the sequence: one MLP transforms
    the input into a form ready for incorporation into short term memory (hidden state),
    while another prepares the short term memory (hidden state) to be updated. This
    satisfies our first requirement — that we want to read new information in context
    of old information. Moreover, both of these MLPs *are the same across each timestep*.
    That is, we use the same rules for how to merge the current hidden state with
    new information. This satisfies our second requirement — that we must use the
    same rules for each timestep.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Both of these MLPs are generally implemented as just one layer deep: that is,
    it is just one large stack of logistic regressions. For instance, the following
    figure demonstrates how the architecture for MLP A might look like, assuming that
    each embedding is eight numbers long and that the hidden state also consists of
    eight numbers. This is a simple but effective transformation to map the embedding
    vector to a vector suitable for merging with the hidden state.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种MLP通常仅实现为单层深度：也就是说，它们只是一个大的逻辑回归堆栈。例如，下面的图示展示了MLP A的架构是如何工作的，假设每个嵌入长度为八个数字，隐藏状态也由八个数字组成。这是一种简单但有效的转换，将嵌入向量映射到适合与隐藏状态合并的向量。
- en: '![](../Images/c12377e1cbe382becda6e97204b6edaa.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c12377e1cbe382becda6e97204b6edaa.png)'
- en: When we finish incorporating the last token into the hidden state, the recurrent
    layer’s job is finished. It has produced a vector — a list of numbers — which
    represents information accumulated by reading over a sequence of tokens in a sequential
    way. We can then pass this vector through a third MLP, which learns the relationship
    between the ‘current state of memory’ and the prediction task (in this case, whether
    the stock price went down or up).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们完成将最后一个标记融入隐藏状态后，循环层的任务也就完成了。它生成了一个向量——一个数字列表——代表了通过按顺序读取标记所累积的信息。然后，我们可以将这个向量传递通过第三个MLP，MLP学习“当前记忆状态”与预测任务之间的关系（在这个案例中，是股票价格是上涨还是下跌）。
- en: The mechanics for updating the weights are too complex to discuss in detail
    in this book, but it is similar to the logic of the backpropagation algorithm.
    The additional complication is to trace the compounded effect of each parameter
    acting repeatedly on its own output (hence the ‘recurrent’ nature of the model),
    which can mathematically be addressed with a modified algorithm termed ‘backpropagation
    through time’.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 更新权重的机制过于复杂，无法在本书中详细讨论，但其逻辑类似于反向传播算法。额外的复杂性在于追踪每个参数对其自身输出重复作用的累积效应（因此模型具有“递归”特性），这可以通过一种称为“时间反向传播”的修改算法在数学上解决。
- en: 'The Recurrent Neural Network is a fairly intuitive way to approach the modeling
    of sequential data. It is yet another case of complex arrangements of linear regression
    models, but it is quite powerful: it allows us to systematically approach difficult
    sequential learning problems such as language.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络是一种相当直观的方法来处理序列数据建模。它是线性回归模型复杂排列的另一种情况，但它非常强大：它允许我们系统地处理如语言这样的困难序列学习问题。
- en: For convenience of diagramming and simplicity, you will often see the recurrent
    layer represented simply as a block, rather than as an expanded cell acting sequentially
    on a series of inputs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便图示和简洁性，你通常会看到递归层被简单地表示为一个块，而不是作为一个扩展的单元，顺序处理一系列输入。
- en: '![](../Images/640cbaa9e4a7ed59d00fda968150d2dd.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/640cbaa9e4a7ed59d00fda968150d2dd.png)'
- en: 'This is the simplest flavor of a Recurrent Neural Network for text: standardized
    input tokens are mapped to embeddings, which are fed into a recurrent layer; the
    output of the recurrent layer (the ‘most recent state of memory’) is processed
    by an MLP and mapped to a predicted target.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于文本的最简单的递归神经网络变种：标准化的输入标记被映射到嵌入中，然后输入到循环层；循环层的输出（“最新的记忆状态”）由MLP处理，并映射到预测目标。
- en: Complex Flavors of Recurrent Networks
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环网络的复杂变种
- en: Recurrent layers allow for networks to approach sequential problems. However,
    there are a few problems with our current model of a Recurrent Neural Network.
    To understand how recurrent neural networks are used in real applications to model
    difficult problems, we need to add a few more bells and whistles.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 循环层使得网络能够处理序列问题。然而，我们当前的递归神经网络模型存在一些问题。为了理解递归神经网络如何在实际应用中用于建模复杂问题，我们需要添加一些附加功能。
- en: 'One of these problems is a *lack of depth*: a recurrent layer simply passes
    once over the text, and thus obtains only a surface-level, cursory reading of
    the content. Consider the sentence “Happiness is not an ideal of reason but of
    imagination”, from the philosopher Immanuel Kant. To understand this sentence
    in its true depth, we cannot simply pass over the words once. Instead, we read
    over the words, and then — this is the critical step — *we read over our thoughts*.
    We evaluate if our immediate interpretation of the sentence makes sense, and perhaps
    modify it to make deeper sense. We might even read over our thoughts about our
    thoughts. This all happens very quickly and often without our conscious knowledge,
    but it is a process which enables us to extract multiple layers of depth from
    the content of text.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Correspondingly, we can add *multiple recurrent layers* to increase the depth
    of understanding. While the first recurrent layer picks up on surface-level information
    form the text, the second recurrent layer reads over the ‘thoughts’ of the first
    recurrent layer. The double-informed ‘most recent memory state’ of the *second*
    layer is then used as the input to the MLP which makes the final decision. Alternatively,
    we could add more than two recurrent layers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cdfe60807421ede75a0442622f33b49.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: 'To be specific about how this stacking mechanism works, consult the following
    figure: rather than simply passing each hidden state on to be updated, we also
    give this input state to the *next* recurrent layer. While the first input to
    the first recurrent layer is an embedding, the first input to the *second* recurrent
    layer is “what the first recurrent layer thought about the first input”.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ca82cb796c8ef732be6536e48d7ae2c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: Almost all Recurrent Neural Networks employed for real-world language modeling
    problems use stacks of recurrent layers rather than a single recurrent layer due
    to the increased depth of understanding and language reasoning. For large stacks
    of recurrent layers, we often use *recurrent residual connections*. Recall the
    concept of a residual connection, in which an earlier version of information is
    added to a later version of information. Similarly, we can place residual connections
    between the hidden states of each layer such that layers can refer to various
    ‘depths of thinking’.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8351e7ce0df3d2b3c6a8789cb1a7d439.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'While recurrent models may perform well on short and simple sentences such
    as “feds announce recession”, financial documents and news articles are often
    much longer than a few words. For longer sequences, standard recurrent models
    run into a persistent *long-term memory loss* problem: often the signal or importance
    of words earlier on in the sequence are diluted and overshadowed by later words.
    Since each timestep adds its own influence to the hidden state, it partially destroys
    a bit of the earlier information. Thus, at the end of the sequence, most of the
    information at the beginning becomes unrecoverable. The recurrent model has a
    narrow window of attentive focus/memory. If we want to make a model which can
    look over and analyze documents with comparable understanding and depth as a human,
    we need to address this memory problem.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然递归模型在短小简单的句子如“美联储宣布经济衰退”中表现良好，但金融文档和新闻文章通常远比几个词要长。对于较长的序列，标准递归模型会遇到持续的*长期记忆丧失*问题：序列中较早的单词的信号或重要性常常被后面的单词稀释和掩盖。由于每个时间步都会对隐藏状态产生影响，它会部分破坏早期信息。因此，在序列结束时，大部分起始处的信息变得不可恢复。递归模型的关注/记忆窗口很窄。如果我们希望创建一个能够以类似人类的理解和深度来查看和分析文档的模型，我们需要解决这个记忆问题。
- en: 'The [Long Short-Term Memory (LSTM)](https://www.bioinf.jku.at/publications/older/2604.pdf)
    (1997) layer is a more complex recurrent layer. Its specific mechanics are too
    complex to be discussed accurately or completely in this book, but we can roughly
    understand it as an attempt to separate ‘long-term memory’ from ‘short-term memory’.
    Both components are relevant when ‘reading’ over a sequence: we need long-term
    memory to track information across large distances in time, but also short-term
    memory to focus on specific, localized information. Therefore, instead of just
    storing a single hidden state, the LSTM layer also uses a ‘cell state’ (representing
    the ‘long term memory’).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[长短期记忆（LSTM）](https://www.bioinf.jku.at/publications/older/2604.pdf)（1997）层是一个更复杂的递归层。其具体机制过于复杂，无法在本书中准确或完全讨论，但我们可以大致理解为试图将“长期记忆”与“短期记忆”分开。两个组件在“读取”序列时都很重要：我们需要长期记忆来追踪跨越时间的大量信息，同时也需要短期记忆来专注于特定的、本地化的信息。因此，LSTM层不仅存储单一的隐藏状态，还使用“细胞状态”（代表“长期记忆”）。'
- en: 'Each step, the input is incorporated with the hidden state in the same fashion
    as in the standard recurrent layer. Afterwards, however, comes three steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步中，输入与隐藏状态以与标准递归层相同的方式结合。然而，之后会有三个步骤：
- en: '*Long-term memory clearing*. Long-term memory is precious; it holds information
    that we will keep throughout time. The current short-term memory state is used
    to determine what part of the long-term memory is no longer needed and ‘cuts it
    out’ to make room for new memory.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*长期记忆清除*。长期记忆是宝贵的；它保存了我们会持久保留的信息。当前的短期记忆状态用于确定长期记忆中哪些部分不再需要，并“剪切”这些部分以腾出空间给新的记忆。'
- en: '*Long-term memory update*. Now that space has been cleared in the long-term
    memory, the short-term memory is used to update (add to) the long-term memory,
    thereby committing new information to long-term memory.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*长期记忆更新*。现在长期记忆中的空间已被清除，短期记忆被用来更新（添加到）长期记忆，从而将新信息记录到长期记忆中。'
- en: '*Short-term memory informing*. At this point, the long-term memory state is
    fully updated with respect to the current timestep. Because we want the long-term
    memory to inform how short-term memory function, the long-term memory helps cut
    out and modify the short-term memory. Ideally, the long-term memory is greater
    oversight on what is important and what is not important to keep in short-term
    memory.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*短期记忆通知*。此时，长期记忆状态已经根据当前时间步完全更新。因为我们希望长期记忆能够指导短期记忆的功能，长期记忆帮助剪切和修改短期记忆。理想情况下，长期记忆对什么是重要的、什么是不重要的有更大的监督。'
- en: Therefore, the short-term memory and long-term memory — which, remember, are
    both lists of numbers — interact with each other and the input at each timestep
    to read the input sequence in a way which allows for close reading without catastrophic
    forgetting. This three-step process is depicted graphically in the following figure.
    A `+`indicates information addition, whereas `x` indicates information removing
    or cleansing. (Addition and multiplication are the mathematical operations used
    to implement these ideas in practice. Say the current value of the hidden state
    is 10\. If I multiply it by 0.1, it becomes 1 — therefore, I have ‘cut down’ the
    information in the hidden state.)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，短期记忆和长期记忆——请记住，这两者都是数字列表——在每个时间步与彼此及输入进行交互，以一种允许仔细阅读而不会发生灾难性遗忘的方式读取输入序列。这个三步过程在下图中以图形方式描述。`+`
    表示信息添加，而 `x` 表示信息移除或清理。（加法和乘法是实现这些思想的数学运算。例如，假设当前隐藏状态的值为 10。如果我将其乘以 0.1，它变成 1——因此，我已经‘减少’了隐藏状态中的信息。）
- en: '![](../Images/28669a8fb613065d4bb8434c24c02fc8.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28669a8fb613065d4bb8434c24c02fc8.png)'
- en: Using stacks of LSTMs with residual connections, we can build powerful language
    interpretation models which are capable of reading (‘understanding’, if you like)
    paragraphs and even entire articles of text. Besides being used in financial analysis
    to pore through large volumes of financial and news reports, such models can also
    be used to predict potentially suicidal or terroristic individuals from their
    social media post texts and messages, to recommend customers novel products they
    are likely to purchase given their previous product reviews, and to detect toxic
    or harassing comments and posts on online platforms.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用带有残差连接的 LSTM 堆叠，我们可以构建强大的语言理解模型，这些模型能够阅读（如果你愿意，也可以说是‘理解’）段落甚至整篇文章。除了用于金融分析以浏览大量金融和新闻报告外，这些模型还可以用于预测社交媒体帖子和消息中的潜在自杀或恐怖分子，推荐客户可能会购买的新产品（根据他们以前的产品评论），以及检测在线平台上的有毒或骚扰性评论和帖子。
- en: 'Such applications force us to think critically about their material philosophical
    implications. The government has a strong interest in detecting potential terrorists,
    and the shooters behind recent massacres have often been shown to have had a troubling
    public social media record — but the tragedy was that they were not found in a
    sea of Internet information. Language models like recurrent models, as you have
    seen for yourself, function purely mathematically: they attempt to find the weights
    and biases which best model the relationship between the input text and the output
    text. But to the extent that these weights and biases mean something, they can
    ‘read’ information in an effective and exceedingly quick manner — much more quickly
    and maybe even more effectively than human readers. These models may allow the
    government to detect, track, and stop potential terrorists before they act. Of
    course, this can come at the cost of privacy. Moreover, we have seen how language
    models — while capable of mechanically tracking down patterns and relationships
    within the data — are really just mathematical algorithms which are capable of
    making mistakes. How should a model’s mistaken labeling of an individual as a
    potential terrorist be reconciled?'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的应用迫使我们批判性地思考其材料的哲学意义。政府对检测潜在恐怖分子有强烈的兴趣，而最近大屠杀的枪手往往有令人担忧的公共社交媒体记录——但悲剧在于，他们在海量的互联网信息中未被发现。语言模型，如你所见，纯粹是数学性的：它们试图找到最能建模输入文本和输出文本之间关系的权重和偏置。但在这些权重和偏置有意义的程度上，它们可以以有效且极其迅速的方式‘阅读’信息——比人类读者更快，甚至可能更有效。这些模型可能使政府能够在潜在恐怖分子行动之前检测、追踪和阻止他们。当然，这可能会以隐私为代价。此外，我们已经看到，尽管语言模型能够机械地追踪数据中的模式和关系，但它们实际上只是能够犯错的数学算法。如何调和模型对个体的错误标记为潜在恐怖分子的情况呢？
- en: 'Social media platforms, both under pressure from users and the government,
    want to reduce harassment and toxicity on online forums. This may seem to be a
    deceptively simple task, conceptually speaking: label a corpus of social media
    comments as toxic or not toxic, then train a language model to predict a particular
    text sample’s toxicity. The immediate problem is that digital discourse is incredibly
    challenging due to the reliance upon quickly changing references (memes), in-jokes,
    well-veiled sarcasm, and prerequisite contextual knowledge. The more interesting
    philosophical problem, however, is if one can and should really train a mathematical
    model (an ‘objective’ model) to predict a seemingly ‘subjective’ target like toxicity.
    After all, what is toxic to one individual may not be toxic to another.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: As we venture into models which work with increasingly personal forms of data
    — language being the medium through which we communicate and absorb almost all
    of our knowledge — we find an increased significance to think about and work towards
    answering these questions. If you are interested in this line of research, you
    may want to look into alignment, jury learning, constitutional AI, RLHF, and value
    pluralism.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Neural Machine Translation
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Concepts*: multi-output recurrent models, bidirectionality, attention'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine translation is an incredible technology: it allows individuals who
    previously could not communicate at all without significant difficulty to engage
    in free dialogue. A Hindi speaker can read a website written in Spanish with a
    click of a ‘Translate this page’ button, and vice versa. An English speaker watching
    a Russian movie can enable live-translated transcriptions. A Chinese tourist in
    France can order food by obtaining a photo-based translation of the menu. Machine
    translation, in a very literal way, melds languages and cultures together.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Prior to the rise of deep learning, the dominant approach to machine translation
    was based on lookup tables. For instance, in Chinese, ‘I’ translates to ‘我’, ‘drive’
    translates to ‘开’, and ‘car’ translates to ‘车’. Thus ‘I drive car’ would be translated
    word-to-word as ‘我开车’. Any bilingual speaker, however, knows the weaknesses of
    this system. Many words which are spelled the same have different meanings. One
    language may have multiple words which are translated in another language as just
    one word. Moreover, different languages have different grammatical structures,
    so the translated words themselves would need to be rearranged. Articles in English
    have multiple different context-dependent translations in gendered languages like
    Spanish and French. Many attempts to reconcile these problems with clever linguistic
    solutions have been devised, but are limited in efficacy to short and simple sentences.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning, on the other hand, provides us the chance to build models which
    more deeply understand language — perhaps even closer to how humans understand
    language — and therefore more effectively perform the important task of translation.
    In this section, we will introduce multiple additional ideas from the deep modeling
    of language and culminate in a technical exploration of how Google Translate works.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习则为我们提供了构建更深层次理解语言的模型的机会——甚至可能接近人类理解语言的方式——从而更有效地执行翻译这一重要任务。在这一部分，我们将介绍语言深度建模的多个额外想法，并最终深入探索
    Google 翻译的工作原理。
- en: Text-Output Recurrent Models
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本输出递归模型
- en: Currently, the most glaring obstacle to building a viable recurrent model is
    the inability to *output text*. The previously discussed recurrent models could
    ‘read’ but not ‘write’ — the output, instead, was a single number (or a collection
    of numbers, a vector). To address this, we need to endow language models with
    the ability to output entire series of text.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，构建可行的递归模型的最大障碍是无法*输出文本*。之前讨论的递归模型可以‘读取’但不能‘写入’——输出的结果是一个单一的数字（或一组数字，向量）。为了解决这个问题，我们需要赋予语言模型输出整个文本序列的能力。
- en: 'Luckily, we do not have to do much work. Recall the previously introduced concept
    of recurrent layer stacking: rather than only collecting the ‘memory state’ after
    the recurrent layer has run through the entire sequence, we collect the ‘memory
    state’ *at each timestep*. Thus, to output a sequence, we can collect the output
    of a memory state at each timestep. Then, we pass each memory state into a designated
    MLP which predicts which word of the output vocabulary to predict given the memory
    state (marked as ‘MLP C’). The word with the highest predicted probability is
    selected as the output.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不需要做太多工作。回顾之前介绍的递归层堆叠概念：我们不是仅在递归层处理完整个序列后收集‘记忆状态’，而是在*每个时间步*收集‘记忆状态’。因此，为了输出一个序列，我们可以在每个时间步收集一个记忆状态的输出。然后，我们将每个记忆状态传递给指定的MLP，它预测给定记忆状态下输出词汇中的哪个词（标记为‘MLP
    C’）。概率最高的词被选为输出。
- en: '![](../Images/f2dad127d02a4facb045fea2058df644.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2dad127d02a4facb045fea2058df644.png)'
- en: To be absolutely clear about how each memory-state is transformed into an output
    prediction, consider the following progression of figures.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绝对清楚地了解每个记忆状态是如何转化为输出预测的，请考虑以下图示的进展。
- en: 'In the first figure, the first outputted hidden state (this the hidden state
    derived after the layer has read the first word, ‘the’) is passed into MLP C.
    MLP C outputs a *probability distribution over the output vocabulary*; that is,
    it gives each word in the output vocabulary a probability indicating how likely
    it is for that word to be chosen as the translation at that time. This is a feedforward
    network: we are essentially performing a logistic regression on the hidden state
    to determine the likelihood of a given word. Ideally, the word with the largest
    probability should be ‘les’, since this is the French translation of ‘the’.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个图中，第一个输出的隐藏状态（这是在层读取了第一个词‘the’后得到的隐藏状态）被传递到MLP C。MLP C输出一个*输出词汇的概率分布*；即，它为输出词汇中的每个词提供一个概率，指示该词在该时间点作为翻译被选择的可能性。这是一个前馈网络：我们本质上是在对隐藏状态进行逻辑回归，以确定给定词的可能性。理想情况下，概率最大的词应该是‘les’，因为这是‘the’的法语翻译。
- en: '![](../Images/ded365557efb1cccd1ebb61582ea21a1.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ded365557efb1cccd1ebb61582ea21a1.png)'
- en: The next hidden state, derived after the recurrent layer has read through both
    ‘the’ and ‘machines’, is passed into MLP C again. This time, the word with the
    highest probability should ideally be ‘machine’ (this is the plural translation
    of ‘machines’ in French).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个隐藏状态是在递归层读取了‘the’和‘machines’之后得到的，再次传递到MLP C。这次，概率最高的词应该是‘machine’（这是‘machines’在法语中的复数形式）。
- en: '![](../Images/2087111f8fcaebabce4e80930e56985a.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2087111f8fcaebabce4e80930e56985a.png)'
- en: 'The most likely word selected in the last timestep should be ‘gagnent’, which
    is the translation for ‘win’ in its particular tense. The model should select
    ‘gagnent’ and not ‘gagner’, or some different tense of the word, based on the
    previous information it has read. This is where the advantages of using a deep
    learning model for translation shines: the ability to grasp grammatical rules
    which manifest across the entire sentence.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个时间步中最可能选择的词应该是‘gagnent’，它是‘win’在特定时态中的翻译。模型应该选择‘gagnent’，而不是‘gagner’或其他不同的时态，基于它之前阅读的信息。这就是使用深度学习模型进行翻译的优势所在：能够掌握贯穿整句话的语法规则。
- en: '![](../Images/08ff1fb65794487c0421087dd13ee9fd.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08ff1fb65794487c0421087dd13ee9fd.png)'
- en: Practically speaking, we often want to stack multiple recurrent layers together
    rather than just a single recurrent layer. This allows us to develop multiple
    layers of understanding, first ‘understanding’ what the input text means, then
    re-expressing the ‘meaning’ of the input text in terms of the output language.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们通常希望将多个递归层堆叠在一起，而不仅仅是一个递归层。这允许我们发展多层次的理解，首先是‘理解’输入文本的含义，然后以输出语言重新表达输入文本的‘意义’。
- en: '![](../Images/c9fb96be5f2f1ff841854239b49c2ea8.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9fb96be5f2f1ff841854239b49c2ea8.png)'
- en: Bidirectionality
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双向性
- en: 'Note that the recurrent layer proceeds sequentially. When it reads the text
    “the machines win”, it first reads “the”, then “machines”, then “win”. While the
    last word, “win”, is read in context of the previous words “the” and “machines”,
    this converse is not true: the first word, “the”, is *not* read in context of
    the *later* words “machines” and “win”. This is a problem, because language is
    often spoken in anticipation of what we will say later. In a gendered language
    like French, an article like “the” can take on many different forms — “la” for
    a feminine object, “le” for a masculine object, and “les” for plural objects.
    We do not know which version of “the” to translate. Of course, once we read the
    rest of the sentence — “the machines” — we know that the object is plural and
    that we should use “les”. This is a case in which earlier parts of a text are
    informed by later parts. More generally speaking, when we re-read a sentence —
    which we often do instinctively without realizing it — we are reading the beginning
    in context of the beginning. Even though language is read in sequence, it must
    often be interpreted ‘out of sequence’ (that is, not strictly unidirectionally
    from-beginning-to-end).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，递归层是顺序进行的。当它读取文本“the machines win”时，它首先读取“the”，然后是“machines”，再是“win”。虽然最后一个词“win”是根据前面的词“the”和“machines”来读取的，但这种反向关系并不成立：第一个词“the”*不是*在*后面*的词“machines”和“win”上下文中读取的。这是一个问题，因为语言通常是为了预期我们将要说的内容而被说出的。在像法语这样的性别语言中，像“the”这样的冠词可以有许多不同的形式——“la”用于阴性物体，“le”用于阳性物体，而“les”用于复数物体。我们不知道要翻译哪个版本的“the”。当然，一旦我们阅读了句子的其余部分——“the
    machines”——我们就知道对象是复数的，应该使用“les”。这是文本的早期部分受到后期部分影响的一个例子。更一般地说，当我们重新阅读一个句子时——我们常常在不自觉中这样做——我们是在以句子的开始部分为背景来阅读开始部分。尽管语言是按顺序读取的，但它往往需要以‘非顺序’的方式解释（即，不严格从开始到结束的单向顺序）。
- en: To address this problem, we can use *bidirectionality* — a simple modification
    to recurrent models which enables layers to ‘read’ both forwards and backwards.
    A *bidirectional recurrent layer* is really two different recurrent layers. One
    layer reads forward in time, whereas the other reads backwards. After both are
    finished reading, their outputs at each timestep are added together.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以使用*双向性*——这是一种对递归模型的简单修改，使得层能够‘前向’和‘后向’读取。*双向递归层*实际上是两个不同的递归层。一个层向前读取时间，而另一个层向后读取。两个层都读取完成后，它们在每个时间步的输出会被加在一起。
- en: '![](../Images/1d09726387b5e789a76ad9bf772c9da4.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d09726387b5e789a76ad9bf772c9da4.png)'
- en: Bidirectionality enables the model to read text in a way such that the past
    is read in the context of the future, in addition to reading the future in context
    of the past (the default functionality of a recurrent layer). Note that the output
    of the bidirectional recurrent layer at each timestep is informed by the entire
    sequence rather than just all the timesteps before it. For instance, in a 10-timestep
    sequence, the timestep at *t* = 3 is informed by a ‘memory state’ which has already
    read through the sequence [*t* = 0] → [*t =* 1] → [*t* = 2] → [*t* = 3] *as well*
    *as* another ‘memory state’ which has already read through the sequence [*t* =
    9] → [*t* = 8] → [*t* = 7] → [*t* = 6] → [*t* = 5] → [*t* = 4] → [*t* = 3].
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: This simple modification enables significantly richer depth of language understanding.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Autoregressive Generation
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our current working model of a translation model is a large stack of (bidirectional)
    recurrent layers. However, there is a problem: when we translate some text *A*
    into some other text *B*, we don’t just write *B* with reference to *A*, we also
    write *B in reference to itself*.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'We can’t directly translate complex sentences from the Russian “Грузовик внезапно
    остановился потому что дорогу переходила курица” into the English “The truck suddenly
    stopped because a chicken was crossing the road” by directly reading out the Russian:
    if we translated the Russian word-for-word in order, we would get “Truck suddenly
    stopped because road was crossed by chicken”. In Russian, the object is placed
    after the noun, but keeping this form in English is certainly readable but not
    smooth nor ‘optimal’, so to speak. The key idea is this: to obtain a comprehensible
    and usable translation, we not only need to make sure the translation is faithful
    to the original text but also ‘faithful to itself’ (self-consistent).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: In order to do this, we need a different text generation called *autoregressive
    generation*. This allows the model to translate each word not only in relationship
    to the original text, but to what the model has already translated. Autoregressive
    generaiton is the dominant paradigm not only for neural translation models but
    for all sorts of modern text generation models, including advanced chatbots and
    content generators.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: We begin with an ‘encoder’ model. The encoder model, in this case, can be represented
    as a stack of recurrent layers. The encoder reads in the input sequence and derives
    a single output, the encoded representation. This single list of numbers represents
    the ‘essence’ of the input text sequence in quantitative form — its ‘universal/real
    meaning’, if you will. The objective of the encoder is to distill the input sequence
    into this fundamental packet of meaning.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/11ef4e5c4899c524da9d813e7b45ddb3.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: 'Once this encoded representation has been obtained, we begin the task of *decoding*.
    The decoder is similarly structured to the encoder — we can think of it as another
    stack of recurrent layers which accepts a sequence and produces an output. In
    this case, the decoder accepts the encoded representation (i.e. the output of
    the encoder) *and* a special ‘start token’ (denoted </s>). The start token represents
    the beginning of a sentence. The decoder’s task is to predict the next word in
    the given sentence; in this case, it is given a ‘zero-word sentence’ and therefore
    must predict the first word. In this case, there is no previous translated content,
    so the decoder is relying wholly on the encoded representation: it predicts the
    first word, ‘The’.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f494d3a6e3215df562f7d5a6b0a967cb.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: 'Next is the key autoregressive step: we take the decoder’s previous outputs
    and plug them back into the decoder. We now have a ‘one-word sentence’ (the start
    token followed by the word ‘The’). Both tokens are passed into the decoder, along
    the encoded representation — the same one as before, outputted by the encoder
    — and now the decoder predicts the next word, “truck”.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a9874d558fc2823e7b4a092ca7545b6.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'This token is then treated as another input. Here, we can more clearly realize
    why autoregressive generation is a helpful algorithmic scaffold for text generation:
    being given the knowledge that the current working sentence is “The truck” constrains
    how we can complete it. In this case, the next word will likely be a verb or an
    adverb, which we ‘know’ as a grammatical structure. On the other hand, if the
    decoder only had access to the original Russian text, it would not be able to
    effectively constrain the set of possibilities. In this case, the decoder is able
    to reference both what has previously been translated and the meaning of the original
    Russian sentence to correctly predict the next word as “suddenly”.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4048674b8a1e510831b706bf5644b08d.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: 'This autoregressive generation process continues:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba152de9d0d54c0c925cc0ac96cf0a5f.png)![](../Images/4abbf1a12c74e288f7cb519d4746d6f6.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: Lastly, to end a sentence, the decoder model predicts a designated ‘end token’
    (denoted as </e>). In this case, the decoder will have ‘matched’ the current translated
    sentence against the encoded representation to determine whether the translation
    is satisfactory and stop the sentence generation process.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d537de8d81e29164eff4ecc8f58d930.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: 2016 Google Translate
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By now, we’ve covered a lot of ground. Now, we have most of the pieces needed
    to develop a somewhat thorough understanding of how the model for Google Translate
    was designed. I need to say very little towards the significance of a model like
    that provided by Google Translate: even if rough, an accurate and accessible neural
    machine translation system breaks down many language barriers. For us, this particular
    model helps unify many of the concepts we’ve talked about in one cohesive application.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，我们已经覆盖了很多内容。现在，我们掌握了开发对 Google 翻译模型的较全面理解所需的大部分要素。我不需要多说这种模型的重要性：即使粗略，一个准确且易于使用的神经机器翻译系统也能打破许多语言障碍。对我们来说，这个特定的模型有助于将我们讨论的许多概念统一到一个连贯的应用中。
- en: This information is taken from the 2016 [Google Neural Machine Translation paper](https://arxiv.org/abs/1609.08144),
    which introduced Google’s deep learning system for machine translation. While
    it is almost certain that the model in use has changed in the many years since
    then, this system still provides an interesting case study into neural machine
    translation systems. For clarity, we will refer to this system as ‘Google Translate’,
    acknowledging that it is likely not current.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息摘自 2016 年的[Google 神经机器翻译论文](https://arxiv.org/abs/1609.08144)，该论文介绍了 Google
    的深度学习机器翻译系统。尽管在这些年里使用的模型几乎肯定已经发生了变化，但该系统仍然为神经机器翻译系统提供了有趣的案例研究。为了清楚起见，我们将该系统称为“Google
    翻译”，并承认它可能不是最新的。
- en: 'Google Translate uses an encoder-decoder autoregressive model. That is, the
    model consists of *encoder component* and a *decoder component*; the decoder is
    autoregressive (recall from earlier: it accepts previously generated outputs as
    an input in addition to other information, in this case the output of the encoder).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Google 翻译使用了编码器-解码器自回归模型。也就是说，该模型由*编码器组件*和*解码器组件*组成；解码器是自回归的（回顾前述：它在接受其他信息的同时，还接受之前生成的输出作为输入，这里是编码器的输出）。
- en: The encoder is a stack of *seven* long short-term memory (LSTM) layers. The
    first layer is bidirectional (there are therefore technically 8 layers, since
    a bidirectional layer ‘counts as two’), which allows it to capture important patterns
    in the input text going in both directions (bottom figure, left). Moreover, the
    architecture employs *residual connections* between every layer (bottom figure,
    right). Recall from previous discussion that residual connections in recurrent
    neural networks can be implemented by adding the input to a recurrent layer to
    the output at every timestep, such that the recurrent layer ends up learning the
    optimal difference to apply to the input.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器是由*七层*长短期记忆（LSTM）层组成的堆叠。第一层是双向的（因此技术上有 8 层，因为双向层“算作两个”），这使得它能够捕捉输入文本中双向的重要模式（底部图像，左侧）。此外，该架构在每一层之间采用了*残差连接*（底部图像，右侧）。回顾之前的讨论，残差连接在递归神经网络中可以通过将输入添加到递归层的输出上来实现，从而使递归层学习到对输入施加的最佳差异。
- en: '![](../Images/22774fe6edf6d852af058c78b744cdc4.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22774fe6edf6d852af058c78b744cdc4.png)'
- en: The decoder is also a stack of eight LSTM layers. It accepts the previously
    generated sequence in autoregressive fashion, beginning with the start token `</s>`.
    The Google Neural Machine Translation architecture, however, uses both autoregressive
    generation *and* attention.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器也是由八层 LSTM 组成的堆叠。它以自回归的方式接收之前生成的序列，从开始标记`</s>`开始。然而，Google 神经机器翻译架构使用了自回归生成*和*注意力机制。
- en: 'Attention scores are computed for each of the original text words (represented
    by hidden states in the encoder, which iteratively transform text but still positionally
    represents it). We can think of attention as a dialogue between the decoder and
    the encoder. The decoder says: “I have generated [sentence] so far, I want to
    predict the next translated word. Which words in the original sentence are most
    relevant to this next translated word?” The encoder replies, “Let me look at what
    you are thinking about, and I will match it to what I have learned about each
    word in the original input… ah, you should pay attention to [word A] but not so
    much to [word B] and [word C], they are less relevant to predicting the next particular
    word.” The decoder thanks the encoder: “I will think about this information to
    determine how I go about generating, such that I indeed focus on [word A].” Information
    about attention is sent to every LSTM layer, such that this attention information
    is known at all levels of generation.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力分数是为每个原始文本单词计算的（由编码器中的隐藏状态表示，编码器迭代地转换文本但仍保持位置表示）。我们可以把注意力看作是解码器和编码器之间的对话。解码器说：“我已经生成了[sentence]，我想预测下一个翻译词。原始句子中的哪些词与下一个翻译词最相关？”编码器回答：“让我看看你在想什么，我会将其与我对原始输入中每个词的了解进行匹配……啊，你应该关注[word
    A]，但不要过多关注[word B]和[word C]，它们与预测下一个特定词的关系较小。”解码器感谢编码器：“我会考虑这些信息来确定如何生成，以便确实关注[word
    A]。”关于注意力的信息被发送到每个LSTM层，使得这种注意力信息在生成的所有层级中都可知。
- en: 'This represents the main mass of the Google Neural Machine Translation system.
    The model is trained on a large dataset of translation tasks: given the input
    in English, say, predict the output in Spanish. The model learns the optimal ways
    of reading (i.e. the parameters in the encoder), the optimal ways of attending
    to the input (i.e. the attention calculation), and the optimal ways of relating
    the attended input to an output in Spanish (i.e. the parameters in the decoder).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表了谷歌神经机器翻译系统的主要部分。该模型在一个大型翻译任务数据集上进行训练：给定英文输入，预测西班牙文输出。模型学习读取的最佳方式（即编码器中的参数）、关注输入的最佳方式（即注意力计算）以及将关注的输入与西班牙文输出关联的最佳方式（即解码器中的参数）。
- en: '![](../Images/254fcf5ffc5a0a13c1220b2c37848389.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/254fcf5ffc5a0a13c1220b2c37848389.png)'
- en: Subsequent work has expanded neural machine translation systems to multilingual
    capability, in which a single model can be used to translate between multiple
    pairs of languages. This is not only necessary from a practical standpoint — it
    is infeasible to train and store a model for every pair of languages — but also
    has shown to improve the translation between any two pair of languages. Moreover,
    the GNMT paper provides details on training — this is a very deep architecture
    which is constrained by hardware — and actual deployment — large models are slow
    not only to train but also to get predictions on, but Google Translate users don’t
    want to have to wait more than a few seconds to translate text.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 后续工作扩展了神经机器翻译系统的多语言能力，在这种能力下，一个模型可以用于在多个语言对之间进行翻译。这不仅从实际角度来看是必要的——为每对语言训练和存储一个模型是不可行的——而且也已显示出能改善任何两对语言之间的翻译。此外，GNMT论文提供了训练的细节——这是一个受硬件限制的非常深的架构——以及实际部署——大型模型不仅训练慢，预测也慢，但谷歌翻译用户不希望等待超过几秒钟来翻译文本。
- en: While the GNMT system certainly is a landmark in computational language understanding,
    just a few years later a new, in some ways radically simplified, approach would
    completely change up language modeling — and do away altogether with the once-common
    recurrent layers which we so painstakingly worked to understand. Keep posted for
    a second post on Transformers!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然GNMT系统确实是计算语言理解的一个里程碑，但仅仅几年后，一种在某些方面极简化的新方法将彻底改变语言建模——并完全摆脱曾经常见的递归层，我们曾如此费力地去理解。请关注关于Transformers的第二篇文章！
- en: Thanks for reading!
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: 'In this post, we did a thorough survey of recurrent neural networks: their
    design logic, more complex features, and applications.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们对递归神经网络进行了深入的调查：它们的设计逻辑、更多复杂特性和应用。
- en: 'Some key points:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关键点：
- en: RNNs are a natural extension of feedforward networks and can be used for machine
    translation. RNNs are designed to track information learned so far and relate
    new information to previously seen information, similar to how humans process
    sequential information.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN（循环神经网络）是前馈网络的自然扩展，可以用于机器翻译。RNN旨在跟踪迄今为止学习到的信息，并将新信息与先前见过的信息相关联，类似于人类处理顺序信息的方式。
- en: RNNs use recurrent layers that have a hidden state representing short-term memory.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN使用递归层，其中隐藏状态代表短期记忆。
- en: Recurrent layers can be stacked to increase the depth of understanding and reasoning
    in the network.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以堆叠递归层，以增加网络对理解和推理的深度。
- en: Long Short-Term Memory (LSTM) networks are a more complex type of recurrent
    layer that separate long-term memory from short-term memory. LSTMs have mechanisms
    for clearing, updating, and informing both long-term and short-term memory.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）网络是更复杂的递归层类型，它将长期记忆与短期记忆分开。LSTM具有清除、更新和通知长期记忆和短期记忆的机制。
- en: RNNs have applications in various fields such as financial analysis, social
    media analysis, recommendation systems, and language translation.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN在金融分析、社交媒体分析、推荐系统和语言翻译等多个领域都有应用。
- en: The use of RNNs in real-world applications raises philosophical and ethical
    questions regarding privacy, model mistakes, and subjectivity in labeling toxic
    or harmful content.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN在现实世界应用中的使用引发了关于隐私、模型错误以及在标记有毒或有害内容时的主观性等哲学和伦理问题。
- en: Neural Machine Translation is a powerful application of RNNs that enables language
    translation between different languages, facilitating communication and cultural
    exchange.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经机器翻译是RNN的一个强大应用，它使不同语言之间的翻译成为可能，促进了沟通和文化交流。
- en: '*All photos by author.*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有照片由作者提供。*'
