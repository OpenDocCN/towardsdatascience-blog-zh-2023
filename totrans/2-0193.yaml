- en: A Framework for Building a Production-Ready Feature Engineering Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f](https://towardsdatascience.com/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[The Full Stack 7-Steps MLOps Framework](https://towardsdatascience.com/tagged/full-stack-mlops)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pauliusztin.medium.com/?source=post_page-----f0b29609b20f--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----f0b29609b20f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f0b29609b20f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f0b29609b20f--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----f0b29609b20f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f0b29609b20f--------------------------------)
    ·13 min read·Apr 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ae381d9f40ec629b5dacf7b06536a4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial represents **lesson 1 out of a 7-lesson course** that will walk
    you step-by-step through how to **design, implement, and deploy an ML system**
    using **MLOps good practices**. During the course, you will build a production-ready
    model to forecast energy consumption levels for the next 24 hours across multiple
    consumer types from Denmark.
  prefs: []
  type: TYPE_NORMAL
- en: '*By the end of this course, you will understand all the fundamentals of designing,
    coding and deploying an ML system using a batch-serving architecture.*'
  prefs: []
  type: TYPE_NORMAL
- en: This course *targets mid/advanced machine learning engineers* who want to level
    up their skills by building their own end-to-end projects.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, certificates are everywhere. Building advanced end-to-end projects
    that you can later show off is the best way to get recognition as a professional
    engineer.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Table of Contents:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Course Introduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Course Lessons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lesson 1: Code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***At the end of this 7 lessons course, you will know how to:***'
  prefs: []
  type: TYPE_NORMAL
- en: design a batch-serving architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Hopsworks as a feature store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: design a feature engineering pipeline that reads data from an API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build a training pipeline with hyper-parameter tunning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use W&B as an ML Platform to track your experiments, models, and metadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: implement a batch prediction pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Poetry to build your own Python packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: deploy your own private PyPi server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: orchestrate everything with Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use the predictions to code a web app using FastAPI and Streamlit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Docker to containerize your code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Great Expectations to ensure data validation and integrity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: monitor the performance of the predictions over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: deploy everything to GCP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build a CI/CD pipeline using GitHub Actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If that sounds like a lot, don't worry, after you will cover this course you
    will understand everything I said before. Most importantly, you will know WHY
    I used all these tools and how they work together as a system.
  prefs: []
  type: TYPE_NORMAL
- en: '**If you want to get the most out of this course,** [**I suggest you access
    the GitHub repository**](https://github.com/iusztinpaul/energy-forecasting) **containing
    all the lessons'' code. I designed the articles so you can read and run the code
    while reading the course.**'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the course, you will know how to implement the diagram below.
    Don't worry if something doesn't make sense to you. I will explain everything
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of the architecture you will build during the course [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: '***Why batch serving?***'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 4 main types of deploying a model:'
  prefs: []
  type: TYPE_NORMAL
- en: batch serving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: request-response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: embedded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch serving is the perfect starting point for getting hands-on experience
    with building a real-world ML system because most AI applications start using
    batch architecture and move towards request-response or streaming.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lessons:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Batch Serving. Feature Stores. Feature Engineering Pipelines.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Training Pipelines. ML Platforms. Hyperparameter Tuning.](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Batch Prediction Pipeline. Package Python Modules with Poetry.](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Private PyPi Server. Orchestrate Everything with Airflow.](https://medium.com/towards-data-science/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Data Validation for Quality and Integrity using GE. Model Performance Continuous
    Monitoring.](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Consume and Visualize your Model’s Predictions using FastAPI and Streamlit.
    Dockerize Everything.](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Deploy All the ML Components to GCP. Build a CI/CD Pipeline Using Github Actions.](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[[Bonus] Behind the Scenes of an ‘Imperfect’ ML Project — Lessons and Insights](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data Source:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used an open API that provides hourly energy consumption values for all the
    energy consumer types within Denmark.
  prefs: []
  type: TYPE_NORMAL
- en: They provide an intuitive interface where you can easily query and visualize
    the data. [You can access the data here](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1].
  prefs: []
  type: TYPE_NORMAL
- en: 'The data has 4 main attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hour UTC:** the UTC datetime when the data point was observed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Price Area:** Denmark is divided into two price areas: DK1 and DK2 — divided
    by the Great Belt. DK1 is west of the Great Belt, and DK2 is east of the Great
    Belt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumer Type:** The consumer type is the Industry Code DE35, owned and maintained
    by Danish Energy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total Consumption:** Total electricity consumption in kWh'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note:** The observations have a lag of 15 days! But for our demo use case,
    that is not a problem, as we can simulate the same steps as it would be in real-time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot from the app shows how we forecasted the energy consumption for
    area = 1 and consumer type = 212 [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: 'The data points have an hourly resolution. For example: "2023–04–15 21:00Z",
    "2023–04–15 20:00Z", "2023–04–15 19:00Z", etc.'
  prefs: []
  type: TYPE_NORMAL
- en: We will model the data as multiple time series. Each unique **price area** and
    **consumer type tuple represents its** unique time series.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we will build a model that independently forecasts the energy consumption
    for the next 24 hours for every time series.
  prefs: []
  type: TYPE_NORMAL
- en: '*Check out the video below to better understand what the data looks like* 👇'
  prefs: []
  type: TYPE_NORMAL
- en: Course & data source overview [Video by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Goal of Lesson 1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In lesson 1, we will focus on the components highlighted in blue: "API," "Feature
    Engineering," and the "Feature Store," as we can see in the diagram below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8a16b0c5164f20ee8c313126196f321.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of the final architecture with the Lesson 1 components highlighted in
    blue [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, we will build an ETL pipeline that extracts data from the energy
    consumption API, pass them through the feature engineering pipeline, which cleans
    and transforms the features, and loads the features in the feature store for further
    usage across the system.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the feature store stands at the heart of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical Concepts & Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Batch Serving:** in the batch serving paradigm, you can prepare your data,
    train your model, and make predictions in an offline fashion. Afterward, you store
    the predictions in a database from where a client/application will use the predictions
    down the line. The word **batch** comes from the idea that you can process multiple
    samples simultaneously, which in this paradigm is usually valid. We computed all
    the predictions in our use case and stored them in a blob storage/bucket.'
  prefs: []
  type: TYPE_NORMAL
- en: If we would oversimplify our architecture to reflect only the main steps of
    a batch architecture, this is how it would look like 👇
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a22ababb6bd63765cd79be166f81414.png)'
  prefs: []
  type: TYPE_IMG
- en: Batch architecture [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: The biggest downside of the batch-serving paradigm is that your predictions
    will almost always lag. For example, in our case, we predict the energy consumption
    for the next 24 hours, and because of this lag, our predictions might be 1 hour
    late.
  prefs: []
  type: TYPE_NORMAL
- en: '[Check out this article](https://medium.com/mlearning-ai/this-is-what-you-need-to-know-to-build-an-mlops-end-to-end-architecture-c0be1deaa3ce)
    to learn more about a *standardized architecture* *suggested by* *Google Cloud*
    that can be leveraged in almost any ML system.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Store:** the feature store stays at the heart of any ML system. Using
    a feature store, you can easily store and share features across the system. You
    can intuitively see a feature store as a fancy database that adds the following
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: data versioning and lineage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: data validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the ability to create datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the ability to hold train/validation/test splits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'two types of storage: offline (cheap, but high latency) and online (more expensive,
    but low latency).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'time-travel: easily access data given a time window'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: hold feature transformation in addition to the feature themselves
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: data monitoring, etc...…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to read about feature stores, [check out this article](https://www.kdnuggets.com/2020/12/feature-store-vs-data-warehouse.html)
    [3].
  prefs: []
  type: TYPE_NORMAL
- en: We chose [Hopsworks](https://www.hopsworks.ai/) as our feature store because
    it is serverless and offers a generous free plan that is more than enough to create
    this course.
  prefs: []
  type: TYPE_NORMAL
- en: Also, Hopsworks is very well designed and provides all the features mentioned
    above. If you are looking for a serverless feature store, I recommend them.
  prefs: []
  type: TYPE_NORMAL
- en: If you want also to run the code while reading this lesson, you have to go to
    [Hopswork](https://www.hopsworks.ai/), create an account, and a project. All the
    other steps will be explained in the rest of the class.
  prefs: []
  type: TYPE_NORMAL
- en: I ensured that all the steps from this course would remain in their free plan.
    Thus it won't cost you any $$$.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Engineering Pipeline:** the piece of code that reads data from one
    or more data sources, cleans, transforms, validates the data and loads it to a
    feature store (basically an ETL pipeline).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pandas vs. Spark:** we chose to use Pandas in this course as our data processing
    library because the data is small. Thus, it easily fits in the computer''s memory
    and using a distributed computing framework such as Spark would have made everything
    too complicated. But in many real-world case scenarios, when the data is too big
    to fit on a single computer (aka big data), you will use Spark (or another distributed
    computing tool) to do the exact same steps as in this lesson. [Check out this
    article](https://pub.towardsai.net/this-is-how-you-can-build-a-churn-prediction-model-using-spark-e187b7eca339)
    to see how Spark can predict churn with big data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 1: Code'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[You can access the GitHub repository here.](https://github.com/iusztinpaul/energy-forecasting)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** All the installation instructions are in the READMEs of the repository.
    Here we will jump straight to the code.'
  prefs: []
  type: TYPE_NORMAL
- en: '*All the code within Lesson 1 is located under the* [***feature-pipeline***](https://github.com/iusztinpaul/energy-forecasting/tree/main/feature-pipeline)*folder.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The files under the [**feature-pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/feature-pipeline)folderare
    structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a00200e79a2ce7bf64f173db2f6975fc.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot that shows the structure of the feature-pipeline folder [Image
    by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: All the code is located under the [**feature_pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/feature-pipeline/feature_pipeline)directory
    (note the "_" instead of "-")**.**
  prefs: []
  type: TYPE_NORMAL
- en: '***Prepare Credentials***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this lesson, you will use a single service, which you will use as your feature
    store: [Hopsworks](https://www.hopsworks.ai/) (for our use case, it will be *free
    of charge).*'
  prefs: []
  type: TYPE_NORMAL
- en: Create an account on [Hopsworks](https://www.hopsworks.ai/) and a new project
    (or use the default project). Be careful to name your project differently than
    “**energy_consumption,”** as Hopsworks requires unique names across its serverless
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you need an **API_KEY** from [Hopsworks](https://www.hopsworks.ai/) to
    log in and access the cloud resources using their Python module for this step.
  prefs: []
  type: TYPE_NORMAL
- en: Directly storing credentials in your git repository is a huge security risk.
    That is why you will inject sensitive information using a **.env** file. The **.env.default**
    is an example of all the variables you must configure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a925923b5eecad13761347d1f873293c.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of the **.env.default** file [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: 'From your **feature-pipeline** directory, run in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: …and fill your newly generated Hopsworks API KEY under the **FS_API_KEY** variable
    and your Hopsworks project name under the **FS_PROJECT_NAME** variable (in our
    case, it was *“energy_consumption”*).
  prefs: []
  type: TYPE_NORMAL
- en: '***See the image below to see how to get your own Hopsworks API KEY 👇***'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3eeba288d913985db231e149fe5148ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Go to your [Hopsworks](https://www.hopsworks.ai/) project. After, in the top-right
    corner, click on your username and after on "Account Settings." Finally, click
    "New API KEY," set a name, select all scopes, hit "Create API KEY," copy the API
    KEY, and you are done. You have your Hopswork API KEY [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: Afterward, in the [**feature_pipeline/settings.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/feature-pipeline/feature_pipeline/settings.py)file,
    we will load all the variables from the **.env** file using the good old **dotenv**
    Python package.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to load the **.env** file from a different place than the current
    directory, you canexport the **ML_PIPELINE_ROOT_DIR** environment variable whenrunning
    the script. This is a "HOME" environment variable that points to the rest of the
    configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: We will also use the **ML_PIPELINE_ROOT_DIR** env var to point to a single directory
    from where to load the **.env** file and read/write data across all the processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of how to use the **ML_PIPELINE_ROOT_DIR** variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using the following code, we will have access to all the credentials/sensitive
    information across our code using the SETTINGS dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '***ETL Code***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the [**feature_pipeline/pipeline.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/feature-pipeline/feature_pipeline/pipeline.py)file,
    we have the main entry point of the pipeline under the **run()** method.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see below, the run method follows on a high level the exact steps
    of an ETL pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '**extract.from_api()** — Extract the data from the energy consumption API.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**transform()** — Transform the extracted data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**validation.build_expectation_suite()** — Build the data validation and integrity
    suite. Ignore this step, as we will insist on it in Lesson 6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**load.to_feature_store()** — Load the data in the feature store.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please note how I used the logger to reflect the system's current state. When
    your program is deployed and running 24/7, having verbose logging is crucial to
    debugging the system. Also, always use the Python logger instead of the print
    method, as you can choose different logging levels and output streams.
  prefs: []
  type: TYPE_NORMAL
- en: On a higher level, it seems easy to understand. Let's dive into each component
    separately.
  prefs: []
  type: TYPE_NORMAL
- en: '**#1\. Extract**'
  prefs: []
  type: TYPE_NORMAL
- en: In the extracting step, we request data for a given window length. The window
    will have a length equal to **days_export**. The first data point of the window
    is **export_end_reference_datetime - days_delay - days_export,** and the last
    data point of the window is equal to **export_end_reference_datetime - days_delay.**
  prefs: []
  type: TYPE_NORMAL
- en: We used the parameter **days_delay** to move the window based on the delay of
    the data. In our use case, the API has a delay of 15 days.
  prefs: []
  type: TYPE_NORMAL
- en: As explained above, the function makes an HTTP GET request to the API requesting
    data. Afterward, the response is decoded and loaded into a Pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: The function returns the DataFrame plus additional metadata containing information
    about the data's extraction.
  prefs: []
  type: TYPE_NORMAL
- en: '**#2\. Transform**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The transform steps take the raw DataFrame and apply the following transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: rename the columns to a Python-standardized format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cast the columns to their suited type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: encode the strings columns to ints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that we haven't included our EDA step (e.g., looking for null values),
    as our primary focus is on designing the system, not on the standard data science
    process.
  prefs: []
  type: TYPE_NORMAL
- en: '**#3\. Data Validation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where we ensure that the data is as expected. In our case, based on
    our EDA and transformations, we are looking that:'
  prefs: []
  type: TYPE_NORMAL
- en: the data doesn't have any nulls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the types of columns are as expected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the range of values is as expected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*More on this subject in Lesson 6.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**#4\. Load**'
  prefs: []
  type: TYPE_NORMAL
- en: This is where we load our processed DataFrame into the feature store.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hopsworks has a set of great tutorials which you can check [here](https://docs.hopsworks.ai/3.1/tutorials/).
    But let me explain what is going on:'
  prefs: []
  type: TYPE_NORMAL
- en: We login into our Hopsworks project using our API_KEY.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We get a reference to the feature store.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We get or create a feature group which is basically a database table with all
    the goodies of a feature store on top of it (read more [here](https://docs.hopsworks.ai/3.1/concepts/fs/feature_group/fg_overview/)
    [5]).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We insert our new processed data samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We add a set of feature descriptions for every feature of our data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We command Hopsworks to compute statistics for every feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check out the video below to see what I explained above looks into Hopsworks
    👇
  prefs: []
  type: TYPE_NORMAL
- en: Hopsworks overview [Video by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: Awesome! Now we have a Python ETL script that extracts the data from the energy
    consumption API for a given time window and loads it into the feature store.
  prefs: []
  type: TYPE_NORMAL
- en: Create a Feature View & Training Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One final step is to create a feature view and training dataset that will later
    be ingested into the training pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** The feature pipeline is the only process that does WRITES to the
    feature store. Other components will only query the feature store for various
    datasets. By doing so, we can safely use the feature store as our only source
    of truth and share the feature across the system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the[**feature_pipeline/feature_view.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/feature-pipeline/feature_pipeline/feature_view.py)file,
    we have the **create()** method that runs the following logic:'
  prefs: []
  type: TYPE_NORMAL
- en: We load the metadata from the feature pipeline. Remember that the FE metadata
    contains the start and end of the extraction window, the version of the feature
    group, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We login into the Hopswork project & create a reference to the feature store.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We delete all the old feature views (usually, you don't have to do this step.
    Quite the opposite, you want to keep your old datasets. But Hopwork's free version
    limits you to 100 feature views. Thus, we wanted to keep our free version).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We get the feature group based on the given version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a feature view with all the data from the loaded feature group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a training dataset using only the given time window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a snapshot of the metadata and save it to disk.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Note:** A feature view is a smart way of combining multiple feature groups
    into a single "dataset." It is similar to a VIEW in a SQL database. You can read
    more about feature views [here](https://docs.hopsworks.ai/3.1/concepts/fs/feature_view/fv_overview/)
    [4].'
  prefs: []
  type: TYPE_NORMAL
- en: That was it. You built a feature pipeline that extracts, transforms, and loads
    the data to a feature store. Based on the data from the feature store, you created
    a feature view and training dataset that will later be used within the system
    as the single source of truth.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** You need good software engineering principles and patterns knowledge
    to build robust feature engineering pipelines. [You can read some hands-on examples
    here](https://pub.towardsai.net/10-underrated-software-patterns-every-ml-engineer-should-know-92e702b96407).'
  prefs: []
  type: TYPE_NORMAL
- en: Important Design Decision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you saw, we haven't actually computed any features in this lesson. We just
    cleaned, validated and ensured that the data was ready to be used later in the
    system.
  prefs: []
  type: TYPE_NORMAL
- en: '*But this is called "the feature pipeline," why we haven''t computed any features?*'
  prefs: []
  type: TYPE_NORMAL
- en: Let me explain.
  prefs: []
  type: TYPE_NORMAL
- en: '**a feature = raw data + a transformation function**'
  prefs: []
  type: TYPE_NORMAL
- en: What if, instead of computing and storing the features, we store the raw data
    and the transformation functions within the feature store?
  prefs: []
  type: TYPE_NORMAL
- en: 'Doingso we have the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Faster experimentation, as the data scientist doesn't require to ask the data
    engineer to compute a new feature. He needs to add a new transformation to the
    feature store.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You save a lot of storage. For example, instead of saving 5 features computed
    from the same raw data column, you save only the raw data column + the 5 transformations,
    which will use only 1/5 of the space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Downsides of using this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Your features will be computed on the cloud or the inference pipeline at runtime.
    Thus, you will add extra latency at runtime.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But, when using the batch-serving paradigm, latency is not a significant constraint.
    Thus, we did just that!
  prefs: []
  type: TYPE_NORMAL
- en: '[Check out Lesson 2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)
    to see how we modeled our time series to forecast the energy consumption for the
    next 24 hours. In [Lesson 2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee),
    we will show you how we stored the transformations directly in the feature store.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You finished the **first lesson** from the **Full Stack 7-Steps
    MLOps Framework** course.
  prefs: []
  type: TYPE_NORMAL
- en: 'You learned about how to design a batch-serving architecture and about developing
    your own ETL pipeline that:'
  prefs: []
  type: TYPE_NORMAL
- en: extracts data from an HTTP API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cleans it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: transforms it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: loads it into a feature store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: creates a new training dataset version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you understand the power of using a feature store and its importance
    to any ML system, you can deploy your model in weeks instead of months.
  prefs: []
  type: TYPE_NORMAL
- en: '[Check out Lesson 2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)
    to learn about training pipelines, ML platforms, and hyperparameter tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Also**, [**you can access the GitHub repository here.**](https://github.com/iusztinpaul/energy-forecasting)'
  prefs: []
  type: TYPE_NORMAL
- en: 💡 My goal is to help machine learning engineers level up in designing and productionizing
    ML systems. Follow me on [LinkedIn](https://www.linkedin.com/in/pauliusztin/)
    or subscribe to my [weekly newsletter](https://pauliusztin.substack.com/) for
    more insights!
  prefs: []
  type: TYPE_NORMAL
- en: 🔥 If you enjoy reading articles like this and wish to support my writing, consider
    [becoming a Medium member](https://pauliusztin.medium.com/membership). By using
    [my referral link](https://pauliusztin.medium.com/membership), you can support
    me without any extra cost while enjoying limitless access to Medium’s rich collection
    of stories.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pauliusztin.medium.com/membership?source=post_page-----f0b29609b20f--------------------------------)
    [## Join Medium with my referral link - Paul Iusztin'
  prefs: []
  type: TYPE_NORMAL
- en: 🤖 Join to get exclusive content about designing and building production-ready
    ML systems 🚀 Unlock full access to…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----f0b29609b20f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [Energy Consumption per DE35 Industry Code from Denmark API](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour),
    [Denmark Energy Data Service](https://www.energidataservice.dk/about/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Hopsworks Tutorials](https://docs.hopsworks.ai/3.1/tutorials/), Hopsworks
    Documentation'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Jim Dowling, [Feature Store vs Data Warehouse](https://www.kdnuggets.com/2020/12/feature-store-vs-data-warehouse.html)
    (2020), KDnuggets'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [Hopsworks Feature Views](https://docs.hopsworks.ai/3.1/concepts/fs/feature_view/fv_overview/),
    Hopsworks Documentation'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [Hopsworks Feature Groups](https://docs.hopsworks.ai/3.1/concepts/fs/feature_group/fg_overview/),
    Hopsworks Documentation'
  prefs: []
  type: TYPE_NORMAL
