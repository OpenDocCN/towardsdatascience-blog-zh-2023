- en: When Stochastic Policies Are Better Than Deterministic Ones
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/when-stochastic-policies-are-better-than-deterministic-ones-b950cd0d60f4](https://towardsdatascience.com/when-stochastic-policies-are-better-than-deterministic-ones-b950cd0d60f4)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why we let randomness dictate our action selection in Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wvheeswijk.medium.com/?source=post_page-----b950cd0d60f4--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----b950cd0d60f4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b950cd0d60f4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b950cd0d60f4--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----b950cd0d60f4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b950cd0d60f4--------------------------------)
    ·6 min read·Feb 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8175bb620e2361f15241cbfd404190f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Rock-paper-scissors would be a dull affair with deterministic policies [Photo
    by [Marcus Wallis](https://unsplash.com/@marcus_wallis?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  prefs: []
  type: TYPE_NORMAL
- en: If you are used to deterministic decision-making policies (e.g., as in [Deep
    Q-learning](https://medium.com/towards-data-science/deep-q-learning-for-the-cliff-walking-problem-b54835409046)),
    the need for and use of stochastic policies might elude you. After all, **deterministic
    policies** offer a convenient state-action mapping *π:s ↦ a*, ideally even the
    optimal mapping (that is, if all the Bellman equations are learned to perfection).
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, **stochastic policies —** represented by a conditional probability
    distribution over the actions in a given state, *π:P(a|s)* — seem rather inconvenient
    and imprecise. Why would we allow randomness to direct our actions, why leave
    the selection of the best known decisions to chance?
  prefs: []
  type: TYPE_NORMAL
- en: In reality, a huge number of Reinforcement Learning (RL) algorithms indeed deploys
    stochastic policies, judging by the sheer number of actor-critic algorithms out
    there. Evidently, there must be some benefit to this approach. This article discusses
    four cases in which stochastic policies are superior to their deterministic counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a?source=post_page-----b950cd0d60f4--------------------------------)
    [## The Four Policy Classes of Reinforcement Learning'
  prefs: []
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a?source=post_page-----b950cd0d60f4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**I. Multi-agent environments**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predictable is not always good.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a game of **rock-paper-scissors**, it is perfectly clear that a deterministic
    policy would fail miserably. The opponent would quickly figure out that you *always*
    play rock, and choose the counter-action accordingly. Evidently, the Nash equilibrium
    here is a uniformly distributed policy that selects each action with probability
    1/3\. How to learn it? You guessed it: stochastic policies.'
  prefs: []
  type: TYPE_NORMAL
- en: Especially in adversarial environments — in which opponents have diverging objectives
    and try to anticipate your decisions — it is often beneficial to have a degree
    of randomness in a policy. After all, game theory dictates there is often no pure
    strategy that always formulates a single optimal response to an opponent, instead
    propagating **mixed strategies** as the best action selection mechanism for many
    games.
  prefs: []
  type: TYPE_NORMAL
- en: Unpredictability is a powerful competitive tool, and if this trait is desired,
    stochastic policies are the clear way to go.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/abe1c928c3bb5f601addc0b7417fd458.png)'
  prefs: []
  type: TYPE_IMG
- en: When facing a worthy adversary, always playing the same game will quickly backfire.
    [Photo by [Christian Tenguan](https://unsplash.com/@christiantenguan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  prefs: []
  type: TYPE_NORMAL
- en: II. Partial observations (POMDP)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many situations, we do not have a perfect picture of the true problem state,
    but instead try to infer them from **imperfect observations**. The field of Partially
    Observed Markov Decision Processes (POMDPs) is built around this discrepancy between
    state and observation. The same imperfection applies when we represent states
    by means of **features**, as is often needed to handle large state spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the famous *aliased* *GridWorld* by David Silver. Here, states are
    represented by the *observation* of surrounding walls. In the illustration below,
    in both shaded states the agent observes a wall on the upside and one on the downside.
    Although the **true states are distinct** and require different actions, they
    are **identical in the observation**.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the imperfect observation alone, the agent must make a decision. A
    **value function approximation** (e.g., [Q-learning](https://medium.com/towards-data-science/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff))
    can easily get stuck here, always picking the same action (e.g., always left)
    and thus never reaching the reward. An ϵ-greedy reward might mitigate the situation,
    but still gets stuck most of the time.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, a policy gradient algorithm will **learn to go left or right with
    probability 0.5** for these identical observations, thus finding the treasure
    much quicker. By acknowledging that the agent has an imperfect perception of its
    environment, it deliberately takes probabilistic actions to counteract the inherent
    uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b759c9c47c9075f1d04d6a3e71596e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Deterministic policy in Aliased GridWorld. The agent can only observe walls
    and is thus unable to distinguish the shaded states. A deterministic policy will
    make identical decisions in both states, which will often trap the agent in the
    upper-left corner. [Image by author, based on example by David Silver, clipart
    by [GDJ](https://openclipart.org/artist/GDJ) [[1](https://openclipart.org/detail/221913/pirate-paraphernalia),
    [2](https://openclipart.org/detail/314002/pirate-finds-treasure-by-richardsdrawings)]
    via [OpenClipArt.org](https://openclipart.org/)]
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0319c39fdc70c401803e026423da6f36.png)'
  prefs: []
  type: TYPE_IMG
- en: Stochastic policy in Aliased GridWorld. The agent can only observe walls and
    is thus unable to distinguish the shaded states. An optimal stochastic policy
    will choose left and right with equal probability in the aliased states, making
    it much more likely to find the treasure. [Image by author, based on example by
    David Silver, clipart by [GDJ](https://openclipart.org/artist/GDJ) [[1](https://openclipart.org/detail/221913/pirate-paraphernalia),
    [2](https://openclipart.org/detail/314002/pirate-finds-treasure-by-richardsdrawings)]
    via [OpenClipArt.org](https://openclipart.org/)]
  prefs: []
  type: TYPE_NORMAL
- en: III. Stochastic environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most environments — especially those in real life — exhibit **substantial uncertainty**.
    Even if we were to make the exact same decision in the exact same state, the corresponding
    reward trajectories may vary wildly. Before we have a reasonable estimate of expected
    downstream values, we may have to perform many, many training iterations.
  prefs: []
  type: TYPE_NORMAL
- en: If we encounter such considerable **uncertainty in the environment** itself
    — as reflected in its [transition function](https://medium.com/towards-data-science/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7)
    — stochastic policies often aid in its discovery. Policy gradient methods offer
    a powerful and inherent exploration mechanism that is not present in vanilla implementations
    of value-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, we do not necessarily seek an inherently probabilistic policy
    as an end goal, but it surely helps while exploring the environment. The combination
    of probabilistic action selection and policy gradient updates directs our improvement
    steps in uncertain environments, even if that search ultimately guides us to a
    **near-deterministic policy**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1aceafe692cfa1f208f9c14ccc38799c.png)'
  prefs: []
  type: TYPE_IMG
- en: When deploying stochastic policies, the interplay between control and environment
    generate a powerful exploration dynamic [image by author]
  prefs: []
  type: TYPE_NORMAL
- en: 'Truth be told, if we look past the standard ϵ-greedy algorithm in value function
    approximation, there are number of powerful exploration strategies that work perfectly
    well while learning deterministic policies:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b?source=post_page-----b950cd0d60f4--------------------------------)
    [## Seven Exploration Strategies In Reinforcement Learning You Should Know'
  prefs: []
  type: TYPE_NORMAL
- en: Pure exploration and -exploitation, ϵ-greedy, Boltzmann exploration, optimistic
    initialization, confidence intervals…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b?source=post_page-----b950cd0d60f4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: IV. Continuous action spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although there are some workarounds, to apply value-based methods in continuous
    spaces we are generally required to **discretize the action space**. The more
    fine-grained the discretization, the closer the original problem is approximated.
    However, this comes at the cost of increased computational complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a self-driving car. How hard to hit the gas, how hard to hit the breaks,
    how much to throttle the gas — they are all **intrinsically continuous actions**.
    In a continuous action space, they can be represented by three variables that
    can each adopt values within a certain range.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we define 100 intensity levels for both gas and break and 360 degrees
    for the steering wheel. With 100*100*360=3.6M combinations, we have a pretty big
    action space, while still lacking the fine touch of continuous control. Clearly,
    the **combination of high dimensionality and continuous variables** is particularly
    hard to handle through discretization.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, policy gradient methods are perfectly capable of **drawing continuous
    actions from representative probability distributions**, making them the default
    choice for continuous control problems. For instance, we might represent the policy
    by three parameterized Gaussian distributions, learning both mean and standard
    deviation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a099554fafbb8ebc152abef4efb2091b.png)'
  prefs: []
  type: TYPE_IMG
- en: Driving a car is an example of inherently continuous control. The finer the
    action space is discretized, the more cumbersome value-based learning becomes
    [Photo by [Nathan Van Egmond](https://unsplash.com/@thevanegmond?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  prefs: []
  type: TYPE_NORMAL
- en: Convergence to near-deterministic policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before concluding the article, it is important to emphasize that a stochastic
    policy does not imply that we keep making semi-random decisions until the end
    of time.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases (e.g., the aforementioned rock-paper-scissors or Aliased GridWorld)
    the **optimal policy requires mixed action selection** (with probabilities 30%/30%/30%
    and 50%/50%, respectively).
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, (e.g., [identifying the best slot machine](https://medium.com/towards-data-science/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7))
    the optimal response may in fact be a deterministic one. In such cases, the stochastic
    policy will converge to a **near-deterministic** one, e.g., selecting a particular
    action with 99.999% probability. For continuous action spaces, the policy will
    converge to very small standard deviations.
  prefs: []
  type: TYPE_NORMAL
- en: That said, the policy will never be *completely* deterministic. For mathematicians
    that write **convergence proofs** this is actually a nice property, ensuring infinite
    exploration in the limit. Real-world practitioners may have to be a bit pragmatic
    to avoid the occasional idiotic action.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a52248119e5cdba726c85f18d7c7d10f.png)'
  prefs: []
  type: TYPE_IMG
- en: Depending on the problem, stochastic policies may converge to near-deterministic
    ones, virtually always picking the action yielding the highest expected reward
    [image by author]
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'And there you have it, four cases in which stochastic policies are preferable
    over deterministic ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-agent environments**: Our predictability gets punished by other agents.
    Adding randomness to our actions makes it hard for the opponent to anticipate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic environments**: Uncertain environments beg a high degree of exploration,
    which is not inherently provided by algorithms based on deterministic policies.
    Stochastic policies automatically explore the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partially observable environments**: As observations (e.g., feature representations
    of states) are imperfect representations of the true system states, we struggle
    to distinguish between states that require different actions. Mixing up our decisions
    may resolve the problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous action spaces**: We must otherwise finely discretize the action
    space to learn value functions. In contrast, policy-based methods gracefully explore
    continuous action spaces by drawing from corresponding probability density functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf](https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/](https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/#:~:text=On%20the%20other%20hand%2C%20a,when%20the%20environment%20is%20uncertain)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Strategy_(game_theory)](https://en.wikipedia.org/wiki/Strategy_(game_theory))'
  prefs: []
  type: TYPE_NORMAL
