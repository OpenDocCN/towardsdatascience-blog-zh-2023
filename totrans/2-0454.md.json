["```py\nfrom ipywidgets import Dropdown\nfrom sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n\n# Retrieves all Text-to-Image generation models.\nfilter_value = \"task == txt2img\"\ntxt2img_models = list_jumpstart_models(filter=filter_value)\n\n# display the model-ids in a dropdown to select a model for inference.\nmodel_dropdown = Dropdown(\n    options=txt2img_models,\n    value=\"model-txt2img-stabilityai-stable-diffusion-v2-1-base\",\n    description=\"Select a model\",\n    style={\"description_width\": \"initial\"},\n    layout={\"width\": \"max-content\"},\n)\ndisplay(model_dropdown)\n\n# Or just hard code the model id and version=*. \n# Eg. if we want the latest 2.1 base model\nself._model_id, self._model_version = (\n    \"model-txt2img-stabilityai-stable-diffusion-v2-1-base\",\n    \"*\",\n)\n```", "```py\nopenai.api_key = self._api_key\nprompt = \"Write me a 1000-word story about Bob the penguin who wants to travel to Europe to see famous landmarks\"\nresponse = openai.Completion.create(\n    model=\"text-davinci-003\",\n    prompt=prompt,\n    temperature=0.7,\n    max_tokens=2089,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0\n)\n```", "```py\n# [Optional] Override default hyperparameters with custom values\nhyperparams[\"max_steps\"] = 400\nhyperparams[\"with_prior_preservation\"] = False\nhyperparams[\"train_text_encoder\"] = False\n\ntraining_job_name = name_from_base(f\"stable-diffusion-{self._model_id}-transfer-learning\")\n\n# Create SageMaker Estimator instance\nsd_estimator = Estimator(\n    role=self._aws_role,\n    image_uri=image_uri,\n    source_dir=source_uri,\n    model_uri=model_uri,\n    entry_point=\"transfer_learning.py\",  # Entry-point file in source_dir and present in train_source_uri.\n    instance_count=self._training_instance_count,\n    instance_type=self._training_instance_type,\n    max_run=360000,\n    hyperparameters=hyperparams,\n    output_path=s3_output_location,\n    base_job_name=training_job_name,\n    sagemaker_session=session,\n)\n\n# Launch a SageMaker Training job by passing s3 path of the training data\nsd_estimator.fit({\"training\": training_dataset_s3_path}, logs=True)\n```", "```py\n{\"instance_prompt\": \"a photo of bob penguin\",\n   \"class_prompt\": \"a photo of a penguin\"\n}\n```", "```py\ndef _get_model_uris(self, model_id, model_version, scope):\n    # Retrieve the inference docker container uri\n    image_uri = image_uris.retrieve(\n        region=None,\n        framework=None,  # automatically inferred from model_id\n        image_scope=scope,\n        model_id=model_id,\n        model_version=model_version,\n        instance_type=self._inference_instance_type,\n    )\n    # Retrieve the inference script uri. This includes scripts for model loading, inference handling etc.\n    source_uri = script_uris.retrieve(\n        model_id=model_id, model_version=model_version, script_scope=scope\n    )\n    if scope == \"training\":\n        # Retrieve the pre-trained model tarball to further fine-tune\n        model_uri = model_uris.retrieve(\n            model_id=model_id, model_version=model_version, model_scope=scope\n        )\n    else:\n        model_uri = None\n\n    return image_uri, source_uri, model_uri\n\nimage_uri, source_uri, model_uri = self._get_model_uris(self._model_id, self._model_version, \"inference\")\n\n# Get model artifact location by estimator.model_data, or give an S3 key directly\nmodel_artifact_s3_location = f\"s3://{self._bucket}/output-model/{job_id}/{training_job_name}/output/model.tar.gz\"\n\nenv = {\n    \"MMS_MAX_RESPONSE_SIZE\": \"20000000\",\n}\n\n# Create model from saved model artifact\nsm_model = model.Model(\n    model_data=model_artifact_s3_location,\n    role=self._aws_role,\n    entry_point=\"inference.py\",  # entry point file in source_dir and present in deploy_source_uri\n    image_uri=image_uri,\n    source_dir=source_uri,\n    env=env\n)\n\ntransformer = sm_model.transformer(instance_count=self._inference_instance_count, instance_type=self._inference_instance_type,\n                                output_path=f\"s3://{self._bucket}/processing/{job_id}/output-images\",\n                                accept='application/json')\ntransformer.transform(data=f\"s3://{self._bucket}/processing/{job_id}/batch_transform_input/\",\n                      content_type='application/json')\n```", "```py\nself._pollyClient = boto3.Session(\n                region_name=aws_region).client('polly')\nftext = f\"<speak><prosody rate=\\\"90%\\\">{text}</prosody></speak>\"\nresponse = self._pollyClient.synthesize_speech(VoiceId=self._speaker,\n                OutputFormat='mp3',\n                Engine='neural',\n                Text=ftext,\n                TextType='ssml')\n\nwith open(mp3_path, 'wb') as file:\n    file.write(response['AudioStream'].read())\n    file.close()\n```", "```py\n# Basic settings\nJOB_ID = \"penguin-images\" # key to S3 folder containing the training images\nSTORY_TOPIC = \"bob the penguin who wants to travel to Europe\"\nSTORY_CHARACTER = \"bob the penguin\"\n\n# Advanced settings\nTRAIN_TEXT_ENCODER = False\nPRIOR_RESERVATION = False\nMAX_STEPS = 400\nNUM_IMAGE_VARIATIONS = 5\n```", "```py\n\"Write me a {max_words} words story about a given character and a topic.\\nPlease break the story down into \" \\\n\"seven to ten short sections with 30 maximum words per section. For each section please describe the scene in \" \\\n\"details and always include the location in one sentence within [] with the following format \" \\\n\"[a photo of character in the location], [a photo of character in front of an object], \" \\\n\"[a photo of character next to an object], [a photo of a location]. Please provide three different variations \" \\\n\"of the scene details separated by |\\\\nAt the start of the story please suggest song style from the following \" \\\n\"list only which matches the story and put it within <>. Song style list are action, calm, dramatic, epic, \" \\\n\"happy and touching.\"\n```"]