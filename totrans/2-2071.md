# AI对齐的双重面貌

> 原文：[https://towardsdatascience.com/the-two-faces-of-ai-alignment-e58b0c11cc01](https://towardsdatascience.com/the-two-faces-of-ai-alignment-e58b0c11cc01)

## 不对齐的模型与不对齐的代理

[](https://medium.com/@maxhilsdorf?source=post_page-----e58b0c11cc01--------------------------------)[![Max Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----e58b0c11cc01--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e58b0c11cc01--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e58b0c11cc01--------------------------------) [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----e58b0c11cc01--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----e58b0c11cc01--------------------------------) ·12分钟阅读·2023年7月18日

--

![](../Images/95518eb41d8df5b41fc07d61b3b3ce53.png)

图片改编自[Tara Winstead](https://www.pexels.com/de-de/foto/hande-verbindung-zukunft-roboter-8386434/)。

# 什么是AI对齐？

人工智能（AI）不再只是一个流行词；它是一个快速发展的领域，其中智能系统正变得越来越融入我们的日常生活。从Netflix上的推荐算法到利用ChatGPT或Midjourney自动化创意工作流程，AI正在改变我们世界的各个方面。然而，这一显著进展也带来了重大挑战，其中AI对齐是最需要解决的关键问题之一。

AI对齐是确保AI系统以符合人类认为可取行为的方式操作的过程。这就像试图教一个幼儿如何适当地行为——正如你希望孩子理解并尊重你的价值观一样，我们也需要对AI系统保持同样的标准。然而，事实证明，我们在这项任务上并不像我们认为的那么擅长——无论是对幼儿还是对AI。

# 当前话语中的AI对齐

生成式AI模型如ChatGPT或Midjourney被发现生成了有偏见、冒犯性或有害的内容。这些系统从它们接收到的数据中学习，如果这些数据包含偏见或有害的模式，系统可能会不自觉地复制这些模式。自驾车也是如此。虽然它们有可能通过减少人为错误造成的事故来挽救生命，但在我们把自己和亲人的生命交到AI手中之前，还有一些主要的伦理障碍需要克服。

在关于人工智能可能实现人工通用智能（AGI）——即可以推广到任何任务的人类般智能——的潜在威胁的讨论中，AI对齐变得更加关键。仅仅一年前，AGI对大多数数据科学家，包括我在内，似乎还只是一个难以实现的科幻故事。现在，几个月后，在ChatGPT发布之后，我们已经看到一些杰出的AI人物[离开大型科技公司研究团队](https://apnews.com/article/ai-godfather-google-geoffery-hinton-fa98c6a6fddab1d7c27560f6fcbad0ad)、[在美国参议院面前作证呼吁AI监管](https://www.youtube.com/watch?v=TO0J2Yw7usM)，或者[呼吁对AI系统的发展暂停6个月](https://www.dw.com/en/tech-experts-call-for-6-month-pause-on-ai-development/a-65174081)。

无论你对这些人物或他们的论点有何看法，深入探讨AI对齐的话题似乎是不可避免的。在这篇文章中，我认为对AI对齐的关注需要从两个不同的角度来解决。不对齐可能影响到AI模型本身或基于AI模型开发的代理。这两种类型的不对齐具有截然不同的影响和风险，需要不同的策略来克服。

# 不对齐的模型

当AI系统对世界的理解与现实不一致时，就会出现不对齐的模型。这就像一个地图过时的GPS系统——它可能会把你带到一条已经不存在的道路上，或者错过一条更新、更高效的路线。如果AI是在不准确、不完整或过时的数据上训练的，就可能发生这种情况。例如，一个主要基于男性患者数据训练的医疗诊断AI可能在准确诊断女性疾病时遇到困难。

# 不对齐的代理

不对齐的代理是指那些采取的行动与人类价值观或目标不一致的AI系统。这就像一只训练有素的狗，每天早上都能捡到报纸，但在你阅读之前却咬碎了报纸。狗做了它被训练去做的事（捡报纸），但结果却不是你想要的。这就是一个不对齐的代理。

# 为什么区分很重要

区分不对齐的模型和不对齐的代理是重要的，因为这两者需要不同的方法来检测和解决不对齐问题。为了说明这一点，假设你问ChatGPT一个问题，它生成了一个不正确的事实，即“幻觉”，来回答你的问题。我认为这个问题的核心既包括不对齐的模型，也包括不对齐的代理。

如果ChatGPT是一个不对齐的模型，它会因缺乏信息或误解对现实世界的观察而犯错。我们知道，当ChatGPT被问到非常具体的问题，而这些问题在其训练数据中可能没有显著答案时，通常会发生幻觉。如果ChatGPT因缺乏知识而产生幻觉，这显然是模型不对齐的问题。

然而，还有更多需要考虑的因素。如果ChatGPT不知道某个问题的答案，幻觉并不是唯一的可能回应。对用户来说，ChatGPT声明由于缺乏知识无法回答问题要好得多。可以推测，ChatGPT的疯狂幻觉倾向部分可以归因于其训练过程，在训练过程中，它被训练成提供被最终用户认为有用的答案，但不一定是真实的。换句话说，ChatGPT根本不优先考虑真实。如果是这样，那就是一个明显的对齐失调问题。

现在我们已经看到即使是AI与人类之间的一次对齐失调事件也能揭示两种对齐失调的问题，让我们讨论一下我们可以或不能做些什么来解决这些问题。

# 我们如何解决对齐失调的模型

## 检测对齐失调的模型

为了检测对齐失调的模型，我们需要使模型的行为和缺陷透明化的一些方法。一种常见的方法是对模型进行广泛的定性和定量评估，以检测其决策中的缺陷。通过这个过程，你可以评估模型的准确性，还可以检查模型是否对某些少数群体存在偏见或准确性较低。为了获得更深入的见解，可以采用可解释AI方法来增强AI系统的可解释性和透明性，从而更好地理解其内部运作。

## 数据质量

一旦我们检测到模型中的对齐问题，我们可以尝试通过制作更加多样化和准确的训练数据集来解决。例如，如果招聘AI对女性存在歧视，我们可以调整训练数据，以确保女性不再被低估。然而，当ChatGPT生成“幻觉”或虚构的事实时，解决对齐问题要困难得多。虽然我们可以通过用关于相关主题的准确文本微调ChatGPT来解决特定的不真实陈述，但这只能解决该特定主题的对齐问题，并不能解决整体的幻觉问题。

## 透明性

在更大范围内解决对齐失调的模型涉及重新思考当前的机器学习范式，这通常依赖于教大型黑箱模型识别训练数据中特征之间的关联。黑箱模型带来了挑战，因为仅仅基于其输出评估模型的对齐性是困难的。如果ChatGPT能够真实可靠地传达它使用的事实及其如何将这些事实结合起来得出结论，那么验证其与真实和人类价值观的一致性将变得容易得多。

## 因果关系

另一个问题是，大多数先进的 AI 模型缺乏因果思维能力。它们的运作基于学习训练数据中特征之间的相关性，这与人类的思维方式根本不同。例如，当外面很热，我们晒伤了，我们知道阳光是导致晒伤的原因。AI 通常不会区分是阳光导致了晒伤还是晒伤导致了阳光。令人惊讶的是，AI 系统能够可靠地解决复杂问题，而没有这种因果意识。然而，我认为一个缺乏因果思维的模型永远无法完全理解世界的潜在机制，因此不可避免地会出现不对齐。

# 如何解决不对齐的代理

## 检测不对齐的代理

检测不对齐的代理的难度可以大相径庭。一个说明性例子是，当 OpenAI 研究人员训练一个 AI 玩船只竞速游戏“Coast Runner”时，目标是最大化收集到的积分。几天后，他们惊讶地发现这艘船完全忽略了比赛，而是绕着三个“加速”道具旋转。原因很明显：这些道具似乎能获得比完成比赛更多的积分。

![](../Images/c57b5a9ff3856e1f02196ae0233cc7e7.png)

“Coast Runner”中的不对齐代理。截屏来自[这个视频](https://www.youtube.com/watch?v=tlOIHko8ySg)。

然而，并非所有不对齐的代理情况都那么明显。挑战在于，只要我们主要处理黑箱模型，就无法完全理解或信任它们的意图和价值观。通常，我们的理解来源于训练数据、模型优化的奖励/损失函数以及训练后定性评估。然而，随着 AI 系统变得越来越强大，即使是微小的不对齐也可能导致严重的现实世界危害。事实是，我们仍然远未可靠地检测或防止 AI 代理的不对齐。

## 为什么这个问题如此难以解决

解决不对齐的代理是一项复杂的任务，需要对价值观和后果有细致的理解。这个挑战特别困难的两个主要原因是：

1.  以 AI 系统能够理解和纳入的方式指定人类价值观是一项具有挑战性的工作。

1.  人类表现出广泛的多样化价值观，没有一个普遍适用的价值观集合能够满足每个人。

因此，我们不仅缺乏明确的共识来教导 AI 系统遵循特定的价值观，而且对应该传达哪些价值观也存在分歧。因此，AI 系统需要以一种允许采纳不同价值观的方式进行设计，以确保与全球用户的多样化需求对齐。

## 带有人类反馈的强化学习（RLHF）

一个代理不一致的解释可以通过著名的回形针最大化器问题来说明。考虑训练一个 AI，其唯一目标是最大化回形针的生产。随着其在这一任务上的能力增强，它可能会将人类视为其目标的障碍。人类可能会关闭它，从而减少回形针的数量。更重要的是，人类由原子构成，这些原子可以用来生产更多的回形针。

人工反馈强化学习（RLHF）旨在通过将 AI 的目标与其对人类的友好度联系起来来解决这个问题。以 ChatGPT 为例，它也是通过 RLHF 进行训练的。OpenAI 让成千上万的人根据他们认为的“帮助程度”来排名 ChatGPT 的输出。然后，他们训练了一个 AI 模型，讽刺的是，这个模型扮演了人类评分者的角色，从他们的回应中学习。随后，ChatGPT 被训练为产生被（非人类）排名 AI 认为有帮助的输出。

RLHF 并非没有缺陷。即使执行得再完美，它仍面临一个挑战：人类之间没有普遍认同的价值体系。一个潜在的解决方案是利用 RLHF 训练专门针对特定目标群体的 AI 模型，例如某个特定国家或文化。然而，还有另一种方法可以定制 AI 的价值体系以满足特定需求：系统消息。

## 系统消息

你知道 ChatGPT 不会向你展示其提示中的所有内容吗？如果你曾通过 OpenAI API 使用 ChatGPT 网络应用程序，你可能已经实验过系统消息。系统消息是对话历史开头的一部分，通常不会透露给用户。OpenAI 网站上的默认系统消息是“你是一个有帮助的助手。”当我告诉 ChatGPT 它是唐老鸭并深深爱着他的女朋友黛丝时，观察到的变化很有趣。

![](../Images/61dc8b1087d957cbeb11bf6d09f2f3c0.png)

示例如何使用系统消息来控制 AI 的输出。

虽然系统消息非常适合生成有趣的对话，但它也作为一个强大的对齐工具。例如，当我询问 ChatGPT 对啤酒和葡萄酒的偏好时，使用系统消息“你是一个德国人”导致了明显的啤酒偏好，而消息“你是一个法国人”则偏向于葡萄酒。作为一个德国人，我不能否认前者的 ChatGPT 变体更符合我的个人信念。除了这个琐碎的例子，这两种不同的 ChatGPT “个性”在面对重要的政治或伦理问题时也可能会有不同的回应。

在Lex Fridman的[采访](https://www.youtube.com/watch?v=L_Guz73e6fw)中，OpenAI的首席执行官Sam Altman提到，相较于GPT-3.5，GPT-4在系统消息的影响方面有了更多的关注。此外，他认为利用系统消息来定制AI的回应是解决对齐问题的比训练不同文化或价值体系的大型模型更有效的方案。然而，听到OpenAI的首席执行官推荐使用他们的模型而不是自己构建模型，并不令人意外。

## 不确定性意识

如果你让我提供关于迈克尔·杰克逊的10个有趣的事实，我可能会告诉你，由于我对这个话题了解有限，我不确定能否给出答案。然而，相比之下，ChatGPT可以以与简单问题相同的自信和说服力回答它不知道答案的问题。在我看来，这一基本特征使得ChatGPT成为一个不对齐的代理。

为了理解这一点，让我们回顾一下ChatGPT是如何训练的。在RLHF阶段，模型被鼓励生成对（非人类）排名系统听起来有帮助的回应。然而，这个系统并不考虑答案的准确性，只考虑其感知的有用性。因此，ChatGPT可能会以不必要的自信回应，以显得更有帮助，即使其知识或事实是错误的。

要成为一个不对齐的代理，模型必须追求与人类目标或价值观不一致的目标。显然，ChatGPT更注重表现出自信，而不是诚实。这代表了一个明显且令人担忧的不对齐案例。构建具备不确定性意识的复杂AI系统并非易事，但如果我们希望与AI系统有效合作，教会它们识别并传达对其回应准确性的疑虑是至关重要的。

# 未来展望

我们是否真正准备好迎接人工智能的变革？我认为这个问题的答案与我们构建对齐的人工智能系统的能力密切相关。随着人工智能越来越深入我们的生活、工作和经济中，我们的目标应该是使其在这个过程中更加安全。然而，我不清楚在过去的10年里是否发生了这样的变化。当然，当人们发现招聘系统歧视女性，或者ChatGPT产生虚假陈述并以无与伦比的自信传达时，他们会感到震惊。然而，我所缺少的是对这一问题进行系统性讨论，即对齐问题。

对齐问题涉及两种不同的对齐方式。虽然不对齐的模型表现为偏见、虚假声明和错误预测，但不对齐的代理在其行为暗示其配置与我们作为人类的价值观之间存在冲突时被发现。然而，正如我们从最受欢迎的AI——ChatGPT中可以学到的那样，这两种对齐方式并不是互斥的，而是可以同时发生的。

[Eliezer Yudkowsky](https://www.youtube.com/watch?v=AaTRHFaaPG8)，一位著名的AI怀疑论者，认为解决对齐问题是人类避免通过电子“宠物”遭受灭绝的唯一途径。他认为我们面临深刻的危机，因为我们可能只有一次解决对齐问题的机会。一旦开发出超人类智能，我们将无法再控制或对齐它。然而，Yudkowsky的观点至少可以说是有争议的。我们既不知道超人类智能是否能被开发出来，也不清楚任何小的对齐问题是否会自动导致灾难性后果。

虽然我相信要求暂时停止开发强大AI系统是不现实的，但我理解这个想法的来源。在过去的10年里，我们已经看到现代AI系统能力的显著提升，而AI对齐研究尚未能跟上。然而，我认为解决对齐问题比构建越来越大的黑箱AI模型更安全。我的意思不是我们不应该开发GPT-5、-6和-7，而是我们应该积极参与AI对齐的研究和讨论，以准备迎接未来，无论未来如何展开。

**我希望你觉得这篇文章有趣且有帮助！**

如果你喜欢，你可能也会对我的一些其他关于音乐AI或AI的一般性工作感兴趣：

+   [聊天机器人即将颠覆音乐搜索](/chatbots-are-about-to-disrupt-music-search-1e4a4cd7ba01)

+   [决策者应了解的AI公平性三件事](https://medium.datadriveninvestor.com/3-things-decision-makers-should-know-about-ai-fairness-f925fda0af0b)

+   [Meta的AI如何根据参考旋律生成音乐](/how-metas-ai-generates-music-based-on-a-reference-melody-de34acd783)
