- en: Turn Linear Regression into Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/turn-linear-regression-into-logistic-regression-e088e2408ec9](https://towardsdatascience.com/turn-linear-regression-into-logistic-regression-e088e2408ec9)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Comprehensive Guideline on How to Implement Logistic Regression from Scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://zubairhossain.medium.com/?source=post_page-----e088e2408ec9--------------------------------)[![Md.
    Zubair](../Images/1b983a23226ce7561796fa5b28c00d65.png)](https://zubairhossain.medium.com/?source=post_page-----e088e2408ec9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e088e2408ec9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e088e2408ec9--------------------------------)
    [Md. Zubair](https://zubairhossain.medium.com/?source=post_page-----e088e2408ec9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e088e2408ec9--------------------------------)
    ·10 min read·Mar 27, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/764eaf14aa790304658c9ca236dfedb0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Rutger Leistra](https://unsplash.com/ko/@rutgerleistra?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you read my previous articles on [*simple linear regression*](https://medium.com/towards-data-science/deep-understanding-of-simple-linear-regression-3776afe34473)
    and [*multiple linear regression*](https://medium.com/towards-data-science/multiple-linear-regression-a-deep-dive-f104c8ede236),
    you will get to know that linear regression predicts continuous value. But not
    all of our real-life prediction problems are associated with continuous values.
    Sometimes we need to classify an object or data based on its features. Linear
    regression algorithms can’t solve these problems. In this scenario, logistic regression’s
    necessity comes in. The title of the algorithm, ‘**Logistic Regression’** holds
    the word **‘Regression’.** It is the modified version of linear regression so
    that it can predict the discrete class value rather than continuous values.
  prefs: []
  type: TYPE_NORMAL
- en: '`So, this article will explain how logistic regression generates the prediction
    value of a class derived from linear regression.`'
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`[**What does Make Linear Regression a Logistic Regression?**](#b874)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`[**Which Function Plays a Key Role?**](#a923)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`[**How Does Linear Regression Fall into Logistic Regression?**](#8b86)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`[**Generate a Loss Function**](#7623)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`[**Why can’t We use MSE as a cost function?**](#7222)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`[**Gradient Descent for Parameter Optimization**](#daa3)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`[**Putting All the Concept Together for Python Implementation from Scratch**](#1a72)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What Does Make Linear Regression a Logistic Regression?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I will bring two equations mentioned in my previous articles on [*simple*](https://medium.com/towards-data-science/deep-understanding-of-simple-linear-regression-3776afe34473)
    and [*multiple linear regression*](https://medium.com/towards-data-science/multiple-linear-regression-a-deep-dive-f104c8ede236).
  prefs: []
  type: TYPE_NORMAL
- en: The first one is the equation for simple linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/920f82c0f7922826fbf6113a6fcd5063.png)'
  prefs: []
  type: TYPE_IMG
- en: We will get the predicted regression value `(y)` only by plugging the value
    of the independent variable `(x)`. But we need to fit the value of the coefficients
    slope`(m)` and the y-intercept value `c`.
  prefs: []
  type: TYPE_NORMAL
- en: The second equation is similar to the first one, but there is more than one
    independent variable `*(x1……..xn)*`*, coefficients m* `*(m1…….m0)*`, and y-intercept
    `m0`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/282ecd71530478bad24425e4ceefaa2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Both for the ***1st and 2nd*** equations, if we have the best-fit values of
    the coefficients, we can easily get the regression value like 34, 687.93 etc.
  prefs: []
  type: TYPE_NORMAL
- en: But it doesn’t give us any sense of transforming the continuous value into a
    distinct classification value. So, we need a function or way by which we can convert
    all the regression values into a range of values between `**[0,1]**`. In logistic
    regression, we exactly do the same. I am going to discuss the function in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Which Function Plays a Key Role?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of directly mentioning the function, I will explain it gradually. *Let’s
    try to have some visual impacts of linear regression and logistic regression.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/83a42f810ece4934a0e2ed0b27530a3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Regression Model Graph (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Look at the above regression model graph. The diagonal blue line is the regression
    line. We can predict any value of `***y***`only by plugging the`***x***`value.
    Try to formulate a logistic regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8828848f104c2f56fe8d8a28eb85890c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: The above dataset has one feature, **‘Age’,** based on the feature the target
    class is defined. The value **1** indicates the person is a student, and **0**
    represents the person is not a student. And it is impossible to predict such categorical
    values with linear regression. ***How does it look like if we plot it? Let’s see.***
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f2f120d89c87e3d29036fb8d601003d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: The stars represent the level of the class (Student or not). Simply, a regression
    line won’t be an appropriate way to predict the classification values.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the **‘S-shaped** function named ‘‘**sigmoid** ’’ comes to play the role.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b777a2e970131850f52585d18304177b.png)'
  prefs: []
  type: TYPE_IMG
- en: This function can convert any number between `*[0,1]*`. I will show you a coding
    example of the sigmoid function.
  prefs: []
  type: TYPE_NORMAL
- en: '***Creating a function for sigmoid function***'
  prefs: []
  type: TYPE_NORMAL
- en: '***Plotting the sigmoid graph***'
  prefs: []
  type: TYPE_NORMAL
- en: This **S-shaped** sigmoid graph would be the best fit for the classification
    problem rather than a straight line. With the increasing value of **x,** the **y**
    value goes from ***0 to 1*** and for ***x=0, y=0.5***. It’s a good function to
    be used. We can easily set a threshold value like 0.5\. All values greater than
    the threshold (0.5) will be 1; otherwise, 0.
  prefs: []
  type: TYPE_NORMAL
- en: '*Yes! finally, we have found the appropriate function.*'
  prefs: []
  type: TYPE_NORMAL
- en: How Does Linear Regression Fall into Logistic Regression?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we have all the staff to translate a linear regression into a logistic
    regression. Let’s put it all together.
  prefs: []
  type: TYPE_NORMAL
- en: In the [**first section**](#b874), I have shown the linear regression equations
    for simple and multiple linear regression. The value of the linear regression
    is continuous. It can be any continuous numerical value.
  prefs: []
  type: TYPE_NORMAL
- en: But the sigmoid function helps us produce a categorical value like ***0* and
    *1***, as shown in the [**last section**](#a923).
  prefs: []
  type: TYPE_NORMAL
- en: So, the equation for the logistics regression will be as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5df00f2eb8c759ad124fac20919765cc.png)'
  prefs: []
  type: TYPE_IMG
- en: The symbol **σ** represents the sigmoid function. If we pass the output of the
    equation into the sigmoid function, we will get results ranging from ***0 to 1.***
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can calculate the linear equation’s value by manually multiplying and
    adding the values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b0152745c586c8f2e5aef39d2285af9.png)'
  prefs: []
  type: TYPE_IMG
- en: But the manual process is time-consuming. Vectorized implementation is much
    faster and easy. Let’s formulate the linear equation to make it compatible with
    vectorized implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98a2acce6d48d63851f8a1cc4ae2041e.png)'
  prefs: []
  type: TYPE_IMG
- en: We have added an extra constant variable `***xi0=1***`***.***
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ef0d02a2078307aedd85548cff7852a.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix Implementation for Calculating the Linear Equation (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: The **X** holds all the values of the independent variables, and the transpose
    of **M** represents the transpose matrix of all the coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: '**Vectorized Logistic Regression Equation**'
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized implementation of the logistic regression will be something like
    this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98867d6ce9809b40eebe991c7324044f.png)'
  prefs: []
  type: TYPE_IMG
- en: It will transform the linear equation values between 0 to 1\. A function with
    python is given below.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the function with a demo value.
  prefs: []
  type: TYPE_NORMAL
- en: Yeah! We have successfully created the function.
  prefs: []
  type: TYPE_NORMAL
- en: Generate a Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we look back to our previous [**multiple linear regression**](https://medium.com/towards-data-science/multiple-linear-regression-a-deep-dive-f104c8ede236)
    article, we will find the **Mean Square Error (MSE)** as a cost function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac1432e6410acf46306052db39d43187.png)'
  prefs: []
  type: TYPE_IMG
- en: But we know logistics regression is not a regression algorithm. Rather, it is
    a binary classification (two classes) algorithm. In logistic regression, there
    are two classes, **1** and **0**. So, **MSE** is not an appropriate cost function
    to be used in case of logistic regression. `*(But why? I will explain the exact
    reason a bit later)*`
  prefs: []
  type: TYPE_NORMAL
- en: Now, I will introduce a new cost function for this classification algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88464bbf8811c6148e81e80b913ba542.png)'
  prefs: []
  type: TYPE_IMG
- en: The above cost function is suitable for logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to have some intuition about the cost function. For `**yi = 1**`,
    the cost function is —
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb64152ec90b186342590592b4c2a425.png)'
  prefs: []
  type: TYPE_IMG
- en: How does the function look like? Let’s plot the function.
  prefs: []
  type: TYPE_NORMAL
- en: The above plot is the graphical representation of the loss function when `yi=1`.
    The graph says the more the prediction value closer to **1**, the less the error.
    And when the prediction value is 0.0, the error is infinity.
  prefs: []
  type: TYPE_NORMAL
- en: '*Let’s plot the cost function for* `***y0=1***`*.*'
  prefs: []
  type: TYPE_NORMAL
- en: For `yi=0`, when the prediction value is close to `**1**`**,** the error is
    infinity and reduces the errors by decreasing the value. Now, we will plot the
    two graphs combined.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the graphical representation is more intuitive. If we combine the loss
    functions for `**yi=0 and yi=1**`, we will get an appropriate function to apply
    gradient descent which has global minima.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b9bf8f7e5cb2ebf8fec25254e1f469b.png)'
  prefs: []
  type: TYPE_IMG
- en: If we plugin the target value`***yi=1*** *or* ***yi=0***`in the above equation,
    one part will be cancelled and turn out to be the same equation I mentioned. This
    is what we need.
  prefs: []
  type: TYPE_NORMAL
- en: '***Convert the cost function into code.***'
  prefs: []
  type: TYPE_NORMAL
- en: '**Why can’t We Use MSE as a cost function?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In logistic regression, the target or output value is discrete or categorical.
    It is not a continuous value like regression problems. If we plug in the value
    in the **MSE** cost function *(the cost function which we have used for linear
    and multiple linear regression),* we will get the following graph rather than
    a convex curve.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b0d02be4a76ad791e5986ce4fd52591.png)'
  prefs: []
  type: TYPE_IMG
- en: Cost Function with Many Local Minima (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: As this type of curve contains many local minima, we will be in trouble to apply
    gradient descent in the cost function. That’s why we won’t use MSE as a cost function
    in logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent for Parameter Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradient descent is a way of minimizing the loss/cost function by optimizing
    the coefficients of a machine learning algorithm how the cost function looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28ca4da8d625104cf9c1ab593f94ec5e.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient Descent (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: The cost function is a **convex** curve, as shown in the **Loss Function** section.
    Now, we have to calculate the derivative of the cost function. Derivative indicates
    how the cost is changed in which direction.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we will randomly initialize the weights of the coefficients and update
    the weights gradually. The main target is finding the minimum cost shown in the
    above image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b9bf8f7e5cb2ebf8fec25254e1f469b.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The derivative of the cost function will be —*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34e0a5589e0fbd3ae80df4f2517a1ee0.png)'
  prefs: []
  type: TYPE_IMG
- en: '*[N.B. If I show the details calculation of the derivative, the article will
    be unnecessarily long. Read out the* [***article***](https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d)
    *for a details explanation.]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Vectorized implementation will be as follows.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9aef7d6eaa9ef8b78d26ff839b06d03a.png)'
  prefs: []
  type: TYPE_IMG
- en: '`***X***` is the matrix form of all the features value, `**M**` represents
    the vectorized form of the coefficients, and `**Y**` stands for the vectorized
    representation of the target value.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Code for vectorized gradient descent implementation.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*We are one step ahead of our final implementation. We have all the functions
    ready to implement the logistic regression. In the next step, we will combine
    all tools and implement the complete algorithms.*'
  prefs: []
  type: TYPE_NORMAL
- en: Putting All the Concepts Together for Python Implementation from Scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s load the dataset of titanic (first thing first). The [**dataset is publicly**
    **available**](https://www.kaggle.com/datasets/brendan45774/test-file) and licenced
    under the public domain.
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing the necessary libraries**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*[Our main target is to show the basic mechanism of the algorithm. So, we
    have kept the pre-processing simple and easy. Rather than concentrate on the data
    analysis, we will keep our eyes on the core implementation.]*`'
  prefs: []
  type: TYPE_NORMAL
- en: '*For our convenience, we have selected some features —*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s have some insights into the selected features.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The features ‘`Age’` and `‘Fare’` have some missing values. We will fill the
    values with the average value. And map the `‘Sex’` **male** with **1** *and* **female**
    with **0**.
  prefs: []
  type: TYPE_NORMAL
- en: Now, all the features are numerical, and no missing value exists.
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s extract the independent variables (x) and dependent variable (y)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normalizing the data so that it will increase the performance of gradient
    descent**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Splitting the train and test set**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kept 25% data for testing and rest of the data for training. Now, we will feed
    the data to our scratch model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Putting all the functions together for performing logistic regression**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fit the model with training data**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**See how the coefficients of the model are optimized**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create a prediction function**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, I have used a threshold value of *0.5* to classify the data. All the results
    below *0.5* are considered as class *0*, and above or equal to *0.5* is *1*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s compare our model with the benchmark scikit-learn library**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Creating a logistic regression model with scikit-learn*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prediction on the test data*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Results of scikit-learn model vs our scratch model**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**👉Result of our scratch model**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Confusion matrix*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision, recall and f1-score*'
  prefs: []
  type: TYPE_NORMAL
- en: '**👉 Result of scikit-learn model**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Confusion matrix*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision, recall and f1-score*'
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that both our scratch model and scikit-learn model possess the
    same results. So, we claim that our scratch model is identical to the scikit-learn
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nowadays, machine learning models are very easy to implement for some built-in
    libraries. So, learning the core mechanisms may be unnecessary for you. As a researcher
    and academician, I always consider it from a different point of view. If you know
    the core concepts of the algorithms, it will be very helpful for you to work at
    the core level, like research, development and optimization of the algorithm,
    etc. You can implement the concept in those programming languages where machine
    learning libraries don’t exist.
  prefs: []
  type: TYPE_NORMAL
- en: '`[***Full Notebook and dataset is available in the repository***](https://github.com/Zubair063/ML_articles/tree/main/Logistic%20Regression%20from%20Scratch)***.***`'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Logistic Regression from scratch — Philipp Muens](https://philippmuens.com/logistic-regression-from-scratch)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Machine Learning Course By Andrew Ng
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`My previous **Algorithm from Scratch** series articles.`'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/multiple-linear-regression-a-deep-dive-f104c8ede236?source=post_page-----e088e2408ec9--------------------------------)
    [## Multiple Linear Regression: A Deep Dive'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple Linear Regression from Scratch: Deep Understanding'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/multiple-linear-regression-a-deep-dive-f104c8ede236?source=post_page-----e088e2408ec9--------------------------------)
    [](/deep-understanding-of-simple-linear-regression-3776afe34473?source=post_page-----e088e2408ec9--------------------------------)
    [## Deep Understanding of Simple Linear Regression
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear Regression from Scratch: Detailed Explanation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/deep-understanding-of-simple-linear-regression-3776afe34473?source=post_page-----e088e2408ec9--------------------------------)
    [](/knn-algorithm-from-scratch-37febe0c15b3?source=post_page-----e088e2408ec9--------------------------------)
    [## KNN Algorithm from Scratch
  prefs: []
  type: TYPE_NORMAL
- en: Implementation and Details Explanation of the KNN Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/knn-algorithm-from-scratch-37febe0c15b3?source=post_page-----e088e2408ec9--------------------------------)
    [](/unsupervised-learning-and-k-means-clustering-from-scratch-f4e5e9947c39?source=post_page-----e088e2408ec9--------------------------------)
    [## K-means Clustering from Scratch
  prefs: []
  type: TYPE_NORMAL
- en: 'K-means: The Best ML Algorithm to Cluster Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/unsupervised-learning-and-k-means-clustering-from-scratch-f4e5e9947c39?source=post_page-----e088e2408ec9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '`**Statistics and data visualization** for data science series.`'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/ultimate-guide-to-statistics-for-data-science-a3d8f1fd69a7?source=post_page-----e088e2408ec9--------------------------------)
    [## Ultimate Guide to Statistics for Data Science'
  prefs: []
  type: TYPE_NORMAL
- en: 'Statistics at a glance for data science: standard guidelines'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/ultimate-guide-to-statistics-for-data-science-a3d8f1fd69a7?source=post_page-----e088e2408ec9--------------------------------)
    [](https://medium.datadriveninvestor.com/ultimate-guide-to-data-visualization-for-data-science-90b0b13e72ab?source=post_page-----e088e2408ec9--------------------------------)
    [## Ultimate Guide to Data Visualization for Data Science
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Visualization at a glance for data science: standard guidelines'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.datadriveninvestor.com](https://medium.datadriveninvestor.com/ultimate-guide-to-data-visualization-for-data-science-90b0b13e72ab?source=post_page-----e088e2408ec9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
