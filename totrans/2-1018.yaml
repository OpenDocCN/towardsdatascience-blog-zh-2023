- en: Guide to ChatGPT’s Advanced Settings — Top P, Frequency Penalties, Temperature,
    and More
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT 高级设置指南——Top P、频率惩罚、温度等
- en: 原文：[https://towardsdatascience.com/guide-to-chatgpts-advanced-settings-top-p-frequency-penalties-temperature-and-more-b70bae848069](https://towardsdatascience.com/guide-to-chatgpts-advanced-settings-top-p-frequency-penalties-temperature-and-more-b70bae848069)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/guide-to-chatgpts-advanced-settings-top-p-frequency-penalties-temperature-and-more-b70bae848069](https://towardsdatascience.com/guide-to-chatgpts-advanced-settings-top-p-frequency-penalties-temperature-and-more-b70bae848069)
- en: Unlock the hidden potential of ChatGPT by optimizing extended configurations
    like Top P, frequency and presence penalties, stop sequences, and maximum length
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过优化扩展配置，如 Top P、频率和存在惩罚、停止序列和最大长度，来解锁 ChatGPT 的隐藏潜力。
- en: '[](https://kennethleungty.medium.com/?source=post_page-----b70bae848069--------------------------------)[![Kenneth
    Leung](../Images/2514dffb34529d6d757c0c4ec5f98334.png)](https://kennethleungty.medium.com/?source=post_page-----b70bae848069--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b70bae848069--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b70bae848069--------------------------------)
    [Kenneth Leung](https://kennethleungty.medium.com/?source=post_page-----b70bae848069--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kennethleungty.medium.com/?source=post_page-----b70bae848069--------------------------------)[![Kenneth
    Leung](../Images/2514dffb34529d6d757c0c4ec5f98334.png)](https://kennethleungty.medium.com/?source=post_page-----b70bae848069--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b70bae848069--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b70bae848069--------------------------------)
    [Kenneth Leung](https://kennethleungty.medium.com/?source=post_page-----b70bae848069--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b70bae848069--------------------------------)
    ·8 min read·Nov 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b70bae848069--------------------------------)
    ·阅读时间 8 分钟·2023 年 11 月 7 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d194737499056062f9858dd926170f53.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d194737499056062f9858dd926170f53.png)'
- en: Photo by [Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/three-crumpled-yellow-papers-on-green-surface-surrounded-by-yellow-lined-papers-V5vqWC9gyEU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    在 [Unsplash](https://unsplash.com/photos/three-crumpled-yellow-papers-on-green-surface-surrounded-by-yellow-lined-papers-V5vqWC9gyEU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: While ChatGPT already yields impressive results with the default settings, there
    is huge untapped potential that comes from its advanced parameters.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 ChatGPT 在默认设置下已经能产生令人印象深刻的结果，但其高级参数中蕴藏着巨大的未开发潜力。
- en: By adjusting settings like **Top P, frequency penalty, presence penalty, stop
    sequences, maximum length, and temperature**, we can steer text generation to
    meet nuanced demands through new levels of creativity and specificity.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整设置，如**Top P、频率惩罚、存在惩罚、停止序列、最大长度和温度**，我们可以引导文本生成以满足复杂的需求，带来新的创造力和具体性水平。
- en: In this article, we explore these advanced settings and learn how to tune them
    effectively.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨这些高级设置，并学习如何有效调整它们。
- en: Table of Contents
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '***(1)***[*Temperature*](#289d)***(2)***[*Maximum Length*](#c377)***(3)***[*Stop
    Sequences*](#c733)***(4)***[*Top P*](#0829)***(5)***[*Frequency Penalty*](#da2d)***(6)***[*Presence
    Penalty*](#ad10)'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***(1)***[*温度*](#289d)***(2)***[*最大长度*](#c377)***(3)***[*停止序列*](#c733)***(4)***[*Top
    P*](#0829)***(5)***[*频率惩罚*](#da2d)***(6)***[*存在惩罚*](#ad10)'
- en: Introduction
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Using ChatGPT is simple — type a prompt and receive a response. Yet, there are
    numerous advanced parameters that we can configure to enrich the output generated.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ChatGPT 很简单——只需输入提示并接收响应。然而，我们可以配置许多高级参数以丰富生成的输出。
- en: 'The [OpenAI Playground](https://platform.openai.com/playground) lets us interact
    with language models while offering various configuration options, as shown below:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAI Playground](https://platform.openai.com/playground) 让我们与语言模型互动，同时提供各种配置选项，如下所示：'
- en: '![](../Images/24ce1a3b429ed9729831343882283393.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24ce1a3b429ed9729831343882283393.png)'
- en: OpenAI Playground landing page with advanced settings in the right-hand panel
    | Image by author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Playground 登陆页面，右侧面板中的高级设置 | 图片来源：作者
- en: 'These advanced settings can also be configured in the API codes:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些高级设置也可以在 API 代码中配置：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Aside from the self-explanatory parameters of the mode and model, let’s do a
    deep dive into the other six parameters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模式和模型的显而易见的参数外，我们还深入探讨其他六个参数。
- en: (1) Temperature
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (1) 温度
- en: Temperature **controls the degree of randomness** in the responses and its value
    ranges between 0 and 2.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 温度**控制响应中的随机性程度**，其值范围在 0 到 2 之间。
- en: At zero, the outputs are more predictable and will stick closely to the most
    likely words. If we want consistent answers, a zero temperature is ideal, especially
    when using these models for grounded responses.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在零温度下，输出更加可预测，并将紧密贴近最可能的词汇。如果我们想要一致的答案，零温度是理想选择，特别是在使用这些模型进行有依据的响应时。
- en: 'If we want responses that are more creative and unpredictable, we can increase
    the temperature. Consider the following sentence:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要更具创意和不可预测的响应，可以提高温度。考虑以下句子：
- en: “The cat sat on the mat and started to ___”
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “猫坐在垫子上并开始___”
- en: At a **low temperature** (e.g., 0), the model will choose a highly probable
    word like “purr” or “sleep.”
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**低温度**（例如 0）下，模型将选择像“purr”或“sleep”这样的高概率词汇。
- en: At a **medium temperature** (e.g., 1), the model might introduce slightly less
    expected yet reasonable words, such as “groom” or “stretch.”
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**中等温度**（例如 1）下，模型可能会引入稍微不那么预期但合理的词汇，如“groom”或“stretch”。
- en: At a **high temperature** (e.g., 2), the model may generate more diverse and
    less predictable outcomes, such as “contemplate” or “brainstorm.”
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**高温度**（例如 2）下，模型可能生成更多样化和不那么可预测的结果，如“contemplate”或“brainstorm”。
- en: 'At higher temperatures, the model is more inclined to take risks, leading to
    a wider variety of possible completions. However, high temperatures may lead to
    nonsensical output, as shown below:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在较高的温度下，模型更倾向于冒险，从而产生更广泛的可能完成。然而，高温度可能导致无意义的输出，如下所示：
- en: '![](../Images/a35649f98faea7a15e73958be1200651.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a35649f98faea7a15e73958be1200651.png)'
- en: Example of gibberish output from the maximum temperature value | Image by author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最大温度值的胡言乱语输出示例 | 作者提供的图片
- en: From a technical perspective, a higher temperature flattens the probability
    distribution such that the typically less-common tokens now become as likely to
    be generated as the more common ones. On the other hand, a lower temperature skews
    the distribution such that the the more common tokens will have an even higher
    probability of being generated.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，更高的温度会使概率分布变平，使得通常不常见的令牌现在变得和更常见的令牌一样可能被生成。另一方面，较低的温度会使分布偏向，使得更常见的令牌生成的概率更高。
- en: (2) Maximum Length
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (2) 最大长度
- en: The maximum length relates to the maximum number of tokens that will be generated.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最大长度涉及将生成的令牌数量的最大值。
- en: '*For English text,* ***1 token is roughly 0.75 words*** *(or 4 characters).
    Check out* [*OpenAI’s token counter*](https://platform.openai.com/tokenizer) *to
    calculate the number of tokens in your text.*'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*对于英文文本，* ***1 个令牌大约是 0.75 个词汇*** *(或 4 个字符)。查看* [*OpenAI 的令牌计数器*](https://platform.openai.com/tokenizer)
    *以计算文本中的令牌数量。*'
- en: One caveat is that the **maximum length** **includes the input prompt**. If
    we set the maximum length at 1,000 and our input has 300 tokens, the output will
    be capped at 1,000 – 300 = 700 tokens.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一点需要注意的是，**最大长度** **包括输入提示**。如果我们将最大长度设置为 1,000，而我们的输入包含 300 个令牌，则输出将限制为 1,000
    – 300 = 700 个令牌。
- en: Furthermore, the upper limit to the maximum length is model-specific. For example,
    the GPT-4 model can reach 8,191 tokens.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最大长度的上限是特定于模型的。例如，GPT-4 模型可以达到 8,191 个令牌。
- en: With this, we can generate responses that fit within custom token limits for
    different use cases. An example is marketing campaigns where we create SMS messages
    within the 160-character (~40 tokens) limit.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们可以生成符合不同使用场景的自定义令牌限制的响应。例如，在营销活动中，我们创建符合 160 个字符（约 40 个令牌）限制的短信。
- en: 'Suppose we have the following 20-token prompt:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下 20 个令牌的提示：
- en: “Generate an SMS marketing message for a local bakery in London called Delights
    that is offering a discount.”
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “为一家位于伦敦的本地面包店 Delights 生成一条短信营销信息，该面包店正在提供折扣。”
- en: 'To generate a message that fits the SMS limit, we set the maximum length as
    **40 + 20 = 60** tokens. After entering the prompt, we get this concise SMS message:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成符合短信限制的消息，我们将最大长度设置为**40 + 20 = 60** 个令牌。输入提示后，我们得到这个简洁的短信消息：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: (3) Stop Sequences
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (3) 停止序列
- en: The “stop sequences” parameter instructs the model to halt generation upon reaching
    a certain string. This is useful when we want the output to end at specific points,
    ensuring that the response is concise and omits unwanted information.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: “停用序列”参数指示模型在达到特定字符串时停止生成。这在我们希望输出在特定点结束时非常有用，确保响应简洁且省略不需要的信息。
- en: 'Suppose we have the following output after asking ChatGPT to generate a resignation
    letter template:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在要求 ChatGPT 生成辞职信模板后得到以下输出：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we want to exclude the sign-off at the bottom, we can set the string “Best”
    as one of the stop sequences. By doing so, the regenerated output will cut off
    at the stop sequence, as seen below:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想排除底部的签名，可以将字符串“Best”设置为其中一个停用序列。这样，重新生成的输出将在停用序列处截断，如下所示：
- en: '![](../Images/ce98baca88b66e155b03170e50612a97.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce98baca88b66e155b03170e50612a97.png)'
- en: Stop sequence output in OpenAI Playground | Image by author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Playground 中的停用序列输出 | 作者提供的图片
- en: The returned output excludes the stop sequence itself, and up to **four** string
    sequences can be defined for each execution.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的输出不包括停用序列本身，每次执行可以定义多达**四**个字符串序列。
- en: (4) Top P
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (4) Top P
- en: Top P is associated with the top-p sampling technique (aka **nucleus sampling**).
    As a recap, GPT models generate the next word by assigning probabilities to all
    possible next words in its vocabulary.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Top P 与 top-p 采样技术（也称为**核心采样**）相关。回顾一下，GPT 模型通过为词汇表中的所有可能下一个词分配概率来生成下一个词。
- en: With top-p sampling, instead of considering the entire vocabulary, the next
    word will be sampled from a smaller set of words that collectively have a cumulative
    probability above the Top P value.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 top-p 采样时，不是考虑整个词汇表，而是从一个较小的词汇集合中进行采样，这些词汇的累积概率总和高于 Top P 值。
- en: Top P ranges from 0 to 1 (default), and a **lower Top P means the model samples
    from a narrower selection of words. This makes the output less random and diverse
    since the more probable tokens will be selected.**
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Top P 范围从 0 到 1（默认），**较低的 Top P 意味着模型从更窄的词汇选择中采样。这使得输出较少随机和多样，因为更可能的标记将被选中。**
- en: For instance, if Top P is set at 0.1, only tokens comprising the **top 10% probability
    mass** are considered.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 Top P 设置为 0.1，则只考虑**前 10% 概率质量**中的标记。
- en: '*Given that Top P impacts output randomness,* [*OpenAI*](https://platform.openai.com/docs/api-reference/chat/create#chat-create-top_p)***recommends
    adjusting either Top P or temperature, but not both****. Nonetheless, there is
    no harm in experimenting with tuning both.*'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*鉴于 Top P 影响输出的随机性，* [*OpenAI*](https://platform.openai.com/docs/api-reference/chat/create#chat-create-top_p)***建议调整
    Top P 或温度，但不建议同时调整这两者****。不过，尝试同时调整两者也没有害处。*'
- en: 'The following shows the outputs for different Top P values based on this prompt:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了不同 Top P 值下的输出结果：
- en: “Write a wildly creative short synopsis about a whale”
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “写一个关于鲸鱼的极具创意的短篇简介”
- en: '![](../Images/85851f7f6b610fa51260d1c6e44f4a1e.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85851f7f6b610fa51260d1c6e44f4a1e.png)'
- en: Comparing outputs of different Top P values | Image by author
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 不同 Top P 值的输出比较 | 作者提供的图片
- en: The example above shows that the output from a lower Top P of **0.01** appears
    **less creative and fancy** in its description.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例显示，较低的 Top P 值**0.01**生成的输出在描述上显得**不那么有创意和华丽**。
- en: Technical Details
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术细节
- en: If Top P is set to 0.1, it does not strictly mean that tokens in the top 10%
    probability mass are considered. Rather, the model finds the **smallest set of
    most probable tokens** **whose cumulative probability** exceeds 10%.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Top P 设置为 0.1，这并不严格意味着考虑的是前 10% 概率质量中的标记。相反，模型找到的是**累积概率**超过 10% 的**最小可能标记集**。
- en: It starts from the most probable token and adds others in descending probabilities
    until the Top P is met. In some cases, this could involve many tokens if no single
    token has a very high probability and the distribution is relatively flat.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从最可能的标记开始，并按递减概率添加其他标记，直到满足 Top P。有时，如果没有单一标记具有非常高的概率且分布相对平坦，这可能涉及许多标记。
- en: (5) Frequency Penalty
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (5) 频率惩罚
- en: 'The frequency penalty addresses a common problem in text generation: **repetition**.
    By applying penalties to frequently appearing words, the model is encouraged to
    diversify language use.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 频率惩罚解决了文本生成中的一个常见问题：**重复**。通过对频繁出现的词汇施加惩罚，模型被鼓励多样化语言使用。
- en: Positive frequency penalty values penalize new tokens based on their existing
    frequency in the text so far, decreasing the model’s likelihood of repeating the
    same line verbatim.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正的频率惩罚值根据 token 在文本中当前的频率来惩罚新 token，减少模型逐字重复相同句子的可能性。
- en: '*Based on API documentation, the frequency penalty ranges from* ***-2 to 2***
    *(default 0). However, the range available on the Playground is* ***0 to 2****.
    We shall follow the API documentation’s range.*'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*根据 API 文档，频率惩罚的范围是* ***-2 到 2*** *(默认为 0)。然而，Playground 上的范围是* ***0 到 2****。我们将遵循
    API 文档中的范围。*'
- en: 'The following shows the outputs for different frequency penalties based on
    this prompt:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了基于该提示的不同频率惩罚的输出：
- en: “Write a poem where every word starts with Z”
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “写一首每个词都以 Z 开头的诗”
- en: '![](../Images/d49cb93e2db359443be667e561857372.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d49cb93e2db359443be667e561857372.png)'
- en: Comparing outputs of different frequency penalties | Image by author
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 不同频率惩罚的输出比较 | 图片作者提供
- en: The example above shows that a larger frequency penalty leads to fewer repeated
    words and greater diversity, such that we even get words that do not begin with
    ‘Z.’
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例表明，更大的频率惩罚会导致更少的重复词和更大的多样性，以至于我们甚至得到不以 ‘Z’ 开头的词。
- en: Reasonable values for the frequency penalty are around **0.1 to 1\.** We can
    increase it further to suppress repetition strongly, but it can degrade output
    quality. Negative values can also be set to increase repetition instead of reducing
    it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 频率惩罚的合理值大约是 **0.1 到 1**。我们可以进一步增加它以强力抑制重复，但这可能会降低输出质量。也可以设置负值来增加重复，而不是减少它。
- en: (6) Presence Penalty
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (6) 存在惩罚
- en: Like the frequency penalty, the presence penalty aims to reduce token repetition.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与频率惩罚类似，存在惩罚旨在减少 token 的重复。
- en: Positive presence penalty values penalize new tokens based on whether they have
    appeared in the text so far, increasing the model’s likelihood of talking about
    new topics.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正的存在惩罚值会根据新 token 是否已在文本中出现过来惩罚新 token，从而增加模型讨论新主题的可能性。
- en: '*Based on API documentation, the presence penalty ranges from* ***-2 to 2 (****default
    0), whereas the range on the Playground is* ***0 to 2****.*'
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*根据 API 文档，存在惩罚的范围是* ***-2 到 2（****默认为 0），而 Playground 上的范围是* ***0 到 2****。*'
- en: What is the difference between a frequency penalty and a presence penalty?
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 频率惩罚和存在惩罚之间的区别是什么？
- en: The subtle difference lies mainly in the degree of penalty on the repeated tokens.
    The **frequency penalty** is **proportional** (i.e., relative marker)to how often
    a particular token has been generated.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 细微的区别主要在于对重复 token 的惩罚程度。**频率惩罚** 是 **成比例的**（即，相对标记），与特定 token 的生成频率相关。
- en: On the other hand, the **presence penalty** **is a once-off (additive)** penalty
    applied to a token that has appeared at least once, like a **Boolean** (1/0) marker.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**存在惩罚** **是一次性的（加性）** 惩罚，应用于至少出现过一次的 token，类似于 **布尔**（1/0）标记。
- en: 'The impact of these penalties is seen in the following equation for the logit
    (unnormalized log probability) **μ** of a token `j`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些惩罚的影响在以下方程式中体现，用于计算 token `j` 的 logit（未归一化的对数概率）**μ**：
- en: '![](../Images/9aa96a24492ae4c83044973d024d023e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9aa96a24492ae4c83044973d024d023e.png)'
- en: Equation showing logit of the j-th token subtracted by two penalty terms | Image
    by author
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 显示第 j 个 token 的 logit 减去两个惩罚项的方程式 | 图片作者提供
- en: '`c[j]` refers to how often a token has been generated previously, and the **α**
    values are the penalty coefficients (i.e., between -2 and 2).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`c[j]` 指代一个 token 之前生成的次数，**α** 值是惩罚系数（即介于 -2 和 2 之间）。'
- en: Reasonable values for the presence penalty are the same as described for the
    frequency penalty.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 存在惩罚的合理值与频率惩罚相同。
- en: Wrapping It Up
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: After understanding what each parameter does, we can tweak these advanced settings
    more confidently to meet our needs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 理解每个参数的作用后，我们可以更自信地调整这些高级设置，以满足我们的需求。
- en: Tuning these parameters is a delicate blend of art and science, so it is recommended
    to play around with different configurations to see what works best for various
    use cases.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 调整这些参数是艺术与科学的微妙结合，因此建议尝试不同的配置，以了解哪些最适合各种用例。
- en: Before you go
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在你离开之前
- en: I welcome you to **join me on a journey of data science discovery!** Follow
    this [Medium](https://kennethleungty.medium.com/) page and visit my [GitHub](https://github.com/kennethleungty)
    to stay updated with more engaging and practical content. Meanwhile, have fun
    experimenting with ChatGPT’s advanced settings!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我欢迎你**加入我的数据科学探索之旅！** 关注这个 [Medium](https://kennethleungty.medium.com/) 页面，并访问我的
    [GitHub](https://github.com/kennethleungty) 以获取更多有趣且实用的内容。同时，尽情享受 ChatGPT 的高级设置带来的实验乐趣！
- en: '[](/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8?source=post_page-----b70bae848069--------------------------------)
    [## Running Llama 2 on CPU Inference Locally for Document Q&A'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8?source=post_page-----b70bae848069--------------------------------)
    [## 在 CPU 上本地运行 Llama 2 进行文档问答'
- en: Clearly explained guide for running quantized LLM applications on CPUs using
    LLama 2, C Transformers, and GGML](/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8?source=post_page-----b70bae848069--------------------------------)
    [](https://betterprogramming.pub/text-to-audio-generation-with-bark-clearly-explained-4ee300a3713a?source=post_page-----b70bae848069--------------------------------)
    [## Text-to-Audio Generation with Bark, Clearly Explained
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 清晰解释了如何使用 LLama 2、C Transformers 和 GGML 在 CPU 上运行量化 LLM 应用的指南](/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8?source=post_page-----b70bae848069--------------------------------)
    [](https://betterprogramming.pub/text-to-audio-generation-with-bark-clearly-explained-4ee300a3713a?source=post_page-----b70bae848069--------------------------------)
    [## Bark 的文本到音频生成，清晰解释
- en: Discover the capabilities of Bark, the open-source GenAI model for text-to-audio](https://betterprogramming.pub/text-to-audio-generation-with-bark-clearly-explained-4ee300a3713a?source=post_page-----b70bae848069--------------------------------)
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索 Bark 的能力，这是一个开源的 GenAI 模型，用于文本到音频的转换](https://betterprogramming.pub/text-to-audio-generation-with-bark-clearly-explained-4ee300a3713a?source=post_page-----b70bae848069--------------------------------)
