- en: Guide to ChatGPT’s Advanced Settings — Top P, Frequency Penalties, Temperature,
    and More
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/guide-to-chatgpts-advanced-settings-top-p-frequency-penalties-temperature-and-more-b70bae848069](https://towardsdatascience.com/guide-to-chatgpts-advanced-settings-top-p-frequency-penalties-temperature-and-more-b70bae848069)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unlock the hidden potential of ChatGPT by optimizing extended configurations
    like Top P, frequency and presence penalties, stop sequences, and maximum length
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://kennethleungty.medium.com/?source=post_page-----b70bae848069--------------------------------)[![Kenneth
    Leung](../Images/2514dffb34529d6d757c0c4ec5f98334.png)](https://kennethleungty.medium.com/?source=post_page-----b70bae848069--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b70bae848069--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b70bae848069--------------------------------)
    [Kenneth Leung](https://kennethleungty.medium.com/?source=post_page-----b70bae848069--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b70bae848069--------------------------------)
    ·8 min read·Nov 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d194737499056062f9858dd926170f53.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/three-crumpled-yellow-papers-on-green-surface-surrounded-by-yellow-lined-papers-V5vqWC9gyEU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: While ChatGPT already yields impressive results with the default settings, there
    is huge untapped potential that comes from its advanced parameters.
  prefs: []
  type: TYPE_NORMAL
- en: By adjusting settings like **Top P, frequency penalty, presence penalty, stop
    sequences, maximum length, and temperature**, we can steer text generation to
    meet nuanced demands through new levels of creativity and specificity.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we explore these advanced settings and learn how to tune them
    effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***(1)***[*Temperature*](#289d)***(2)***[*Maximum Length*](#c377)***(3)***[*Stop
    Sequences*](#c733)***(4)***[*Top P*](#0829)***(5)***[*Frequency Penalty*](#da2d)***(6)***[*Presence
    Penalty*](#ad10)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using ChatGPT is simple — type a prompt and receive a response. Yet, there are
    numerous advanced parameters that we can configure to enrich the output generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [OpenAI Playground](https://platform.openai.com/playground) lets us interact
    with language models while offering various configuration options, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24ce1a3b429ed9729831343882283393.png)'
  prefs: []
  type: TYPE_IMG
- en: OpenAI Playground landing page with advanced settings in the right-hand panel
    | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'These advanced settings can also be configured in the API codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Aside from the self-explanatory parameters of the mode and model, let’s do a
    deep dive into the other six parameters.
  prefs: []
  type: TYPE_NORMAL
- en: (1) Temperature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Temperature **controls the degree of randomness** in the responses and its value
    ranges between 0 and 2.
  prefs: []
  type: TYPE_NORMAL
- en: At zero, the outputs are more predictable and will stick closely to the most
    likely words. If we want consistent answers, a zero temperature is ideal, especially
    when using these models for grounded responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want responses that are more creative and unpredictable, we can increase
    the temperature. Consider the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: “The cat sat on the mat and started to ___”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At a **low temperature** (e.g., 0), the model will choose a highly probable
    word like “purr” or “sleep.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At a **medium temperature** (e.g., 1), the model might introduce slightly less
    expected yet reasonable words, such as “groom” or “stretch.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At a **high temperature** (e.g., 2), the model may generate more diverse and
    less predictable outcomes, such as “contemplate” or “brainstorm.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At higher temperatures, the model is more inclined to take risks, leading to
    a wider variety of possible completions. However, high temperatures may lead to
    nonsensical output, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a35649f98faea7a15e73958be1200651.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of gibberish output from the maximum temperature value | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: From a technical perspective, a higher temperature flattens the probability
    distribution such that the typically less-common tokens now become as likely to
    be generated as the more common ones. On the other hand, a lower temperature skews
    the distribution such that the the more common tokens will have an even higher
    probability of being generated.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Maximum Length
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The maximum length relates to the maximum number of tokens that will be generated.
  prefs: []
  type: TYPE_NORMAL
- en: '*For English text,* ***1 token is roughly 0.75 words*** *(or 4 characters).
    Check out* [*OpenAI’s token counter*](https://platform.openai.com/tokenizer) *to
    calculate the number of tokens in your text.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One caveat is that the **maximum length** **includes the input prompt**. If
    we set the maximum length at 1,000 and our input has 300 tokens, the output will
    be capped at 1,000 – 300 = 700 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the upper limit to the maximum length is model-specific. For example,
    the GPT-4 model can reach 8,191 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we can generate responses that fit within custom token limits for
    different use cases. An example is marketing campaigns where we create SMS messages
    within the 160-character (~40 tokens) limit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have the following 20-token prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: “Generate an SMS marketing message for a local bakery in London called Delights
    that is offering a discount.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To generate a message that fits the SMS limit, we set the maximum length as
    **40 + 20 = 60** tokens. After entering the prompt, we get this concise SMS message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: (3) Stop Sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The “stop sequences” parameter instructs the model to halt generation upon reaching
    a certain string. This is useful when we want the output to end at specific points,
    ensuring that the response is concise and omits unwanted information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have the following output after asking ChatGPT to generate a resignation
    letter template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to exclude the sign-off at the bottom, we can set the string “Best”
    as one of the stop sequences. By doing so, the regenerated output will cut off
    at the stop sequence, as seen below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce98baca88b66e155b03170e50612a97.png)'
  prefs: []
  type: TYPE_IMG
- en: Stop sequence output in OpenAI Playground | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The returned output excludes the stop sequence itself, and up to **four** string
    sequences can be defined for each execution.
  prefs: []
  type: TYPE_NORMAL
- en: (4) Top P
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Top P is associated with the top-p sampling technique (aka **nucleus sampling**).
    As a recap, GPT models generate the next word by assigning probabilities to all
    possible next words in its vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: With top-p sampling, instead of considering the entire vocabulary, the next
    word will be sampled from a smaller set of words that collectively have a cumulative
    probability above the Top P value.
  prefs: []
  type: TYPE_NORMAL
- en: Top P ranges from 0 to 1 (default), and a **lower Top P means the model samples
    from a narrower selection of words. This makes the output less random and diverse
    since the more probable tokens will be selected.**
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if Top P is set at 0.1, only tokens comprising the **top 10% probability
    mass** are considered.
  prefs: []
  type: TYPE_NORMAL
- en: '*Given that Top P impacts output randomness,* [*OpenAI*](https://platform.openai.com/docs/api-reference/chat/create#chat-create-top_p)***recommends
    adjusting either Top P or temperature, but not both****. Nonetheless, there is
    no harm in experimenting with tuning both.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The following shows the outputs for different Top P values based on this prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: “Write a wildly creative short synopsis about a whale”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/85851f7f6b610fa51260d1c6e44f4a1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing outputs of different Top P values | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The example above shows that the output from a lower Top P of **0.01** appears
    **less creative and fancy** in its description.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If Top P is set to 0.1, it does not strictly mean that tokens in the top 10%
    probability mass are considered. Rather, the model finds the **smallest set of
    most probable tokens** **whose cumulative probability** exceeds 10%.
  prefs: []
  type: TYPE_NORMAL
- en: It starts from the most probable token and adds others in descending probabilities
    until the Top P is met. In some cases, this could involve many tokens if no single
    token has a very high probability and the distribution is relatively flat.
  prefs: []
  type: TYPE_NORMAL
- en: (5) Frequency Penalty
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The frequency penalty addresses a common problem in text generation: **repetition**.
    By applying penalties to frequently appearing words, the model is encouraged to
    diversify language use.'
  prefs: []
  type: TYPE_NORMAL
- en: Positive frequency penalty values penalize new tokens based on their existing
    frequency in the text so far, decreasing the model’s likelihood of repeating the
    same line verbatim.
  prefs: []
  type: TYPE_NORMAL
- en: '*Based on API documentation, the frequency penalty ranges from* ***-2 to 2***
    *(default 0). However, the range available on the Playground is* ***0 to 2****.
    We shall follow the API documentation’s range.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The following shows the outputs for different frequency penalties based on
    this prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: “Write a poem where every word starts with Z”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/d49cb93e2db359443be667e561857372.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing outputs of different frequency penalties | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The example above shows that a larger frequency penalty leads to fewer repeated
    words and greater diversity, such that we even get words that do not begin with
    ‘Z.’
  prefs: []
  type: TYPE_NORMAL
- en: Reasonable values for the frequency penalty are around **0.1 to 1\.** We can
    increase it further to suppress repetition strongly, but it can degrade output
    quality. Negative values can also be set to increase repetition instead of reducing
    it.
  prefs: []
  type: TYPE_NORMAL
- en: (6) Presence Penalty
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like the frequency penalty, the presence penalty aims to reduce token repetition.
  prefs: []
  type: TYPE_NORMAL
- en: Positive presence penalty values penalize new tokens based on whether they have
    appeared in the text so far, increasing the model’s likelihood of talking about
    new topics.
  prefs: []
  type: TYPE_NORMAL
- en: '*Based on API documentation, the presence penalty ranges from* ***-2 to 2 (****default
    0), whereas the range on the Playground is* ***0 to 2****.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is the difference between a frequency penalty and a presence penalty?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The subtle difference lies mainly in the degree of penalty on the repeated tokens.
    The **frequency penalty** is **proportional** (i.e., relative marker)to how often
    a particular token has been generated.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the **presence penalty** **is a once-off (additive)** penalty
    applied to a token that has appeared at least once, like a **Boolean** (1/0) marker.
  prefs: []
  type: TYPE_NORMAL
- en: 'The impact of these penalties is seen in the following equation for the logit
    (unnormalized log probability) **μ** of a token `j`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9aa96a24492ae4c83044973d024d023e.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation showing logit of the j-th token subtracted by two penalty terms | Image
    by author
  prefs: []
  type: TYPE_NORMAL
- en: '`c[j]` refers to how often a token has been generated previously, and the **α**
    values are the penalty coefficients (i.e., between -2 and 2).'
  prefs: []
  type: TYPE_NORMAL
- en: Reasonable values for the presence penalty are the same as described for the
    frequency penalty.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping It Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After understanding what each parameter does, we can tweak these advanced settings
    more confidently to meet our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning these parameters is a delicate blend of art and science, so it is recommended
    to play around with different configurations to see what works best for various
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Before you go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I welcome you to **join me on a journey of data science discovery!** Follow
    this [Medium](https://kennethleungty.medium.com/) page and visit my [GitHub](https://github.com/kennethleungty)
    to stay updated with more engaging and practical content. Meanwhile, have fun
    experimenting with ChatGPT’s advanced settings!
  prefs: []
  type: TYPE_NORMAL
- en: '[](/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8?source=post_page-----b70bae848069--------------------------------)
    [## Running Llama 2 on CPU Inference Locally for Document Q&A'
  prefs: []
  type: TYPE_NORMAL
- en: Clearly explained guide for running quantized LLM applications on CPUs using
    LLama 2, C Transformers, and GGML](/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8?source=post_page-----b70bae848069--------------------------------)
    [](https://betterprogramming.pub/text-to-audio-generation-with-bark-clearly-explained-4ee300a3713a?source=post_page-----b70bae848069--------------------------------)
    [## Text-to-Audio Generation with Bark, Clearly Explained
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Discover the capabilities of Bark, the open-source GenAI model for text-to-audio](https://betterprogramming.pub/text-to-audio-generation-with-bark-clearly-explained-4ee300a3713a?source=post_page-----b70bae848069--------------------------------)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
