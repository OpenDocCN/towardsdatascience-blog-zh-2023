- en: 'META’s Hiera: reduce complexity to increase accuracy'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b](https://towardsdatascience.com/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| ARTIFICIAL INTELLIGENCE | COMPUTER VISION | VITs |'
  prefs:
  - PREF_H2
  type: TYPE_TB
- en: Simplicity allows AI to reach incredible performance and surprising speed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----30f7a147ad0b--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----30f7a147ad0b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----30f7a147ad0b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----30f7a147ad0b--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----30f7a147ad0b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----30f7a147ad0b--------------------------------)
    ·12 min read·Jun 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2ab2597bedfa9804d3e3e8da0d02bb6.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Alexander Redl](https://unsplash.com/@alexanderredl?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[Convolutional networks](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    have dominated the field of computer vision for more than twenty years. With the
    arrival of [transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)),
    it was believed that they would be abandoned. **Yet many practitioners use convolution-based
    models for projects. Why?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This article tries to answer these questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What are Vision Transformers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are their limitations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we try to overcome them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why and how does META Hiera seem to succeed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vision transformer: an image is worth how many words?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/dfc18ba75f38c986f153869f0ac5dd4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://en.wikipedia.org/wiki/Vision_transformer#/media/File:Vision_Transformer.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Vision transformers](https://en.wikipedia.org/wiki/Vision_transformer) have
    dominated vision benchmarks in recent years, **but exactly what are they?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Until a few years ago, [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    were the standard in vision tasks. In 2017, however, the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    was released and turned the [NLP](https://en.wikipedia.org/wiki/Natural_language_processing)
    world upside down. In the article [Attention is all you need](https://arxiv.org/abs/1706.03762),
    the authors show that a model built using only self-attention is capable of far
    superior performance to [RNNs](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    and [LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory). So one soon
    wonders: i**s it possible to apply a transformer to images?**'
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid models where self-attention integration was included had been attempted
    before 2020\. In any case, these models could not scale well. The idea was to
    find a way in which the transformer could be used natively with images.
  prefs: []
  type: TYPE_NORMAL
- en: In 2020, the [Google authors decided that the best way](https://arxiv.org/pdf/2010.11929.pdf)
    was to split the images into different patches and then have [embedding](https://en.wikipedia.org/wiki/Embedding)
    of the sequence. In this way, the images are basically treated as if they were
    tokens (words) from the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2290f47891093801107e0192fe143e60.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2010.11929.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In a short time, the dominance of computer vision by CNNs is gradually being
    undermined. [Vision transformers](https://en.wikipedia.org/wiki/Vision_transformer)
    prove to be superior on benchmarks (such as [ImageNet](https://en.wikipedia.org/wiki/ImageNet))
    where CNNs had hitherto dominated.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a1a1150becbe7d4eb7c116a1aa1f2a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2101.11986.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, providing enough data the [Vision Transformers (ViTs)](https://en.wikipedia.org/wiki/Vision_transformer)
    show that they are superior to the CNNs. It is also shown that although there
    are several differences there are also several similarities:'
  prefs: []
  type: TYPE_NORMAL
- en: both ViTs and CNNs construct a complex and progressive representation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, ViTs are more capable of exploiting information present in the background
    and appear to be more robust.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----30f7a147ad0b--------------------------------)
    [## A Visual Journey in What Vision-Transformers See'
  prefs: []
  type: TYPE_NORMAL
- en: How some of the largest models see the world
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----30f7a147ad0b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Also, an additional advantage is the transformer’s ability to scale. This has
    been a competitive advantage of ViTs that has made them a popular choice.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6278945da0662487535a58db51d7bfc7.png)'
  prefs: []
  type: TYPE_IMG
- en: usage of vision transformers. [source](https://paperswithcode.com/method/vision-transformer)
  prefs: []
  type: TYPE_NORMAL
- en: '**In fact, through the years we have seen CNNs of millions of parameters and
    ViTs reaching billions of parameters.** Last year, Google showed how can even
    scale up ViTs up to 20 B parameters and probably in the future we will see even
    bigger models.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6?source=post_page-----30f7a147ad0b--------------------------------)
    [## Why Do We Have Huge Language Models and Small Vision Transformers?'
  prefs: []
  type: TYPE_NORMAL
- en: Google ViT-22 paves the way for new large transformers and to revolutionize
    computer vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6?source=post_page-----30f7a147ad0b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The limit of the Vision Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/ebd16380a18ca436710c00065f67d7af.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Joshua Hoehne](https://unsplash.com/pt-br/@mrthetrain?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Having natively adapted the transformer still comes at a cost: [ViTs](https://en.wikipedia.org/wiki/Vision_transformer)
    use their parameters inefficiently. **This comes from the fact that they use the
    same spatial resolution and the same number of channels in the network.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs had precisely two aspects that determined their initial fortunes (both
    inspired by the human cortex):'
  prefs: []
  type: TYPE_NORMAL
- en: Reduction in spatial resolution while going up in the hierarchy of layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase in the number of different “channels,” and each of these channels becomes
    more and more specialized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/12971175a97579dcc92ca9ff331499f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/1412.6631.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The transformer, on the other hand, has a different structure a sequence of
    [self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)) blocks
    where two main operations occur that allow it to generalize well:'
  prefs: []
  type: TYPE_NORMAL
- en: the attention operation that is used to model inter-element relationships.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [fully-connected layer](/convolutional-layers-vs-fully-connected-layers-364f05ab460b)
    that instead models the inter-element relationship.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**This was in fact noted earlier and stems from the fact that the transformer
    was designed for words and not images**. After all, text and images are two modes
    that are different. One of the differences is that words do not vary in scale
    while images do. **This is conflicting when you have to give attention to elements
    that change at scale in object detection.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Also, the resolution of pixels in an image is higher than the resolution
    of words in a text passage.** Since attention has a quadratic cost, using high-resolution
    images has a high computational cost with a transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: Previous studies have tried to solve this problem with the use of hierarchical
    feature maps. For example, the [Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf)
    constructs a hierarchical representation by starting with small patches and then
    gradually merging the various neighbor patches.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa9aefe51bfe7f45591d1ccd96962772.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2103.14030.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Other studies have tried to implement multi-channel in ViTs. For example, [MVITs](https://arxiv.org/pdf/2104.11227.pdf)
    have tried to create initial channels that focus on simple low-level visual information
    while deeper channels focus on complex high-level features as in CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4dafb92e01ae94c3714832ab8aeb8e33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2104.11227.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: These, however, did not completely solve the problem. Over time, increasingly
    complex models, and specialized modules, have been proposed, which have improved
    performance to some extent but made [ViTs](https://en.wikipedia.org/wiki/Vision_transformer)
    rather slow in training.
  prefs: []
  type: TYPE_NORMAL
- en: Can we solve these transformer limitations without the need for complex solutions?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to learn the spatial relationship
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/9668f1e26fbd3d2d3a891632260e20e5.png)'
  prefs: []
  type: TYPE_IMG
- en: image by [Ali Kazal](https://unsplash.com/it/@lureofadventure) on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: ViTs have emerged as a model for computer vision, however, increasingly complex
    modifications have been necessary to adapt them.
  prefs: []
  type: TYPE_NORMAL
- en: Can we solve these transformer limitations without the need for complex solutions?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In recent years, efforts have been made to streamline the models and speed them
    up. One way often used is the introduction of sparsity. In the case of computer
    vision, one model that has been very successful has been [masked autoencoders](https://arxiv.org/abs/2111.06377)
    (MAE).
  prefs: []
  type: TYPE_NORMAL
- en: In that case, after dividing by patches, a number of patches are masked. Then,
    the decoder has to reconstruct from the masked patches. The ViT encoder then works
    on only 25 percent of the patches. In this, you can train wide encoders with a
    fraction of computation and memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b71b5a523a47b3e83edf61685dc7728.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2111.06377.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: This method has been shown to be capable of teaching spatial reasoning, achieving
    results comparable if not superior to Swin and Mvit (which, however, are computationally
    much more complex).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if it is true that the sparsity regime obtained training
    efficiency, one of the great advantages of CNNs is the hierarchy approach. But
    it is conflictual with sparsity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, it has been tested before but without much success:'
  prefs: []
  type: TYPE_NORMAL
- en: The obtained model was too slow ([MaskFeat](https://arxiv.org/abs/2112.09133)
    or [SimMIM](https://arxiv.org/abs/2111.09886)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifications made the model unnecessarily complex and without gains in accuracy
    ([UM-MAE](https://arxiv.org/abs/2205.10063) or [MCMAE](https://openreview.net/forum?id=qm5LpHyyOUO)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is possible to design a sparse and hierarchic yet efficient model?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: New work by META has departed from MAE training and other tricks to build a
    ViT that is efficient and accurate without the need for all those complex structures
    that have been used in the past.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2306.00989?source=post_page-----30f7a147ad0b--------------------------------)
    [## Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles'
  prefs: []
  type: TYPE_NORMAL
- en: Modern hierarchical vision transformers have added several vision-specific components
    in the pursuit of supervised…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2306.00989?source=post_page-----30f7a147ad0b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Hiera: Hierarchical, Sparse, and Efficient ViT'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e94128fe37300b44f444c4d77f29d7f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jordan Opel](https://unsplash.com/@opeleye?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The basic idea is that to train a hierarchical ViT with high accuracy in visual
    tasks it is not necessary to use a whole series of elements that make it slow
    and complex. According to the authors, spatial bias can be learned from the model
    using masked autoencoder training.
  prefs: []
  type: TYPE_NORMAL
- en: In MAE, patches are deleted, so in a hierarchical model, it has problems reconstructing
    the 2D grid (and spatial relationships). The authors solve so that the kernel
    cannot overlap between mask units (during [pooling](https://www.kaggle.com/questions-and-answers/59502),
    there is no overlap with masked units).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5b2a999605ce83214c7591015680af1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors then started from an existing hierarchical model of ViT, MViTv2,
    and decided to repurpose it using MAE training. The model is built of several
    ViT blocks but as seen in the structure at some point there is a reduction in
    size which is achieved by using pooling attention.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf10662ed79873cca0f7e0f2aa8a548e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  prefs: []
  type: TYPE_NORMAL
- en: During pooling attention features are aggregated locally using a 3x3 convolution
    and then self-attention is computed (this is to reduce the size of K and V and
    thus reduce computational computation). This mechanism can become expensive when
    using video. So the authors replaced it with Mask Unit Attention.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, in Hiera during pooling the kernel shifts so that masked parts
    do not end up in the pooling. So a kind of local attention for each group of tokens
    (of mask size)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfd862004618641d663af19169e85fe0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  prefs: []
  type: TYPE_NORMAL
- en: 'MViTv2 had then introduced a whole series of accouterments that increased complexity,
    though, and the authors deemed them nonessential and eliminated them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Relative Position Embeddings.** The positional embedding is added to the
    attention in each block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max polling layers**, which would have required padding for use in Hiera.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attention residual**, where there is a residual connection between Q (query)
    and output to better learn pooling attention. The authors have reduced the number
    of layers so that it is no longer necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors show the impact of these changes leads singularly to improved performance
    in both accuracy (acc.) and speed (images per second).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e45e95dab71d633b7ea68450cabd8199.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  prefs: []
  type: TYPE_NORMAL
- en: In general, simplifying the model makes Hiera not only much faster (for both
    images and video) but also more accurate ( than both its counterpart MViTv2 but
    also other models).
  prefs: []
  type: TYPE_NORMAL
- en: Hiera is 2.4× faster on images and 5.1× faster on video than the MViTv2 we started
    with and is actually more accurate because of MAE ([source](https://arxiv.org/abs/2306.00989))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e52c9cfe80d77bd284049760866af0ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors point out that the model is not only faster in inference but also
    the training is much faster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d8f52ae6b374ae22c8b66103629c1f09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors show how indeed the basic model with a limited number of parameters
    achieves good results on Imagenet 1K (one of the most important image classification
    datasets).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bd56e0149eeb0665a2be4fcfebd22b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  prefs: []
  type: TYPE_NORMAL
- en: The second point is that normally at low parameter regimes, the convolution-based
    models dominated. Here the smaller model shows very good results. For the authors,
    this confirms their intuitions that a spatial bias can be learned during training
    and thus make ViTs competitive with the convolutional network even for small models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/42cc512ca5d7121c369905f9b9e25dea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The fortune of the large CNN models was to use them for [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning).
    Both [ResNet](https://arxiv.org/abs/1512.03385) and VGG-based models have been
    trained on Imagenet and then adapted by the community for many tasks. Therefore,
    the authors test Hiera for its transfer learning capability using two datasets:
    iNaturalists and Places.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37b0be1d3a12d598c689daafc26588ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors fine-tune the model on the two datasets and show that their model
    is superior to previous ViTs. This shows that their model could be used for other
    datasets as well.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the authors use another popular dataset COCO. While iNaturalists
    and Places was a dataset for image classification, COCO is one of the most widely
    used datasets for image segmentation and object detection (two other popular tasks
    in computer science). Again, the model shows strong scaling behavior (an increase
    in performance as parameters increase). In addition, the model is faster both
    during training and in inference.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f5712d44b2b5f71760394e0f330fd22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the model has been tested on video. Specifically on two video classification
    datasets. Hiera shows that it performs better with fewer parameters. The model
    is also faster in inference. The authors show that the model achieves state-of-the-art
    for this type of task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1dbe18103414490b90cd738ef6a4a4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors show that the model can also be used on other video tasks, such
    as action detection.
  prefs: []
  type: TYPE_NORMAL
- en: Parting thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this work, we create a simple hierarchical vision transformer by taking an
    existing one and removing all its bells-and-whistles while supplying the model
    with spatial bias through MAE pretraining. ([source](https://arxiv.org/pdf/2306.00989.pdf))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The authors showed how many of the elements that have been added to improve
    transformer performance are actually not only unnecessary but increase the complexity
    of the model, making it slower.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, the authors showed that using MAE and a hierarchical structure can
    result in a ViT that is faster and more accurate for both images and video.
  prefs: []
  type: TYPE_NORMAL
- en: This work is important because for many tasks the community still uses convolution-based
    models. ViTs are very large models and have a high computational cost. So often,
    people prefer to use models based on ResNet and VGG. ViTs that are more accurate
    but especially faster in inference could be game-changers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, it highlights a trend seen elsewhere: leverage the sparsity for the
    training. Which has the advantage of reducing parameters, and speeding up training
    and inference. In general, the idea of sparsity is also being seen in other fields
    of artificial intelligence and is an active field of research.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have found this interesting:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, you can* [***become a Medium member***](https://medium.com/@salvatore-raieli/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----30f7a147ad0b--------------------------------)
    [## GitHub — SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----30f7a147ad0b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*or you may be interested in one of my recent articles:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5?source=post_page-----30f7a147ad0b--------------------------------)
    [## The imitation game: Taming the gap between open source and proprietary models'
  prefs: []
  type: TYPE_NORMAL
- en: Can imitation models reach the performance of proprietary models like ChatGPT?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5?source=post_page-----30f7a147ad0b--------------------------------)
    [](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----30f7a147ad0b--------------------------------)
    [## Scaling Isn’t Everything: How Bigger Models Fail Harder'
  prefs: []
  type: TYPE_NORMAL
- en: Are Large Language Models really understanding programming languages?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'salvatore-raieli.medium.com](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----30f7a147ad0b--------------------------------)
    [](https://levelup.gitconnected.com/metas-lima-maria-kondo-s-way-for-llms-training-8411e3907fed?source=post_page-----30f7a147ad0b--------------------------------)
    [## META’S LIMA: Maria Kondo’s way for LLMs training'
  prefs: []
  type: TYPE_NORMAL
- en: Less and tidy data to create a model capable to rival ChatGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/metas-lima-maria-kondo-s-way-for-llms-training-8411e3907fed?source=post_page-----30f7a147ad0b--------------------------------)
    [](https://levelup.gitconnected.com/is-ai-funny-maybe-a-bit-fd5183f68779?source=post_page-----30f7a147ad0b--------------------------------)
    [## Is AI funny? Maybe, a Bit
  prefs: []
  type: TYPE_NORMAL
- en: Why AI is still struggling with humor and why this an important step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/is-ai-funny-maybe-a-bit-fd5183f68779?source=post_page-----30f7a147ad0b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here is the list of the principal references I consulted to write this article,
    only the first name for an article is cited.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chaitanya Ryali et al, 2023, Hiera: A Hierarchical Vision Transformer without
    the Bells-and-Whistles, [link](https://arxiv.org/abs/2306.00989)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Peng Gao et al, 2022, MCMAE: Masked Convolution Meets Masked Autoencoders,
    [link](https://openreview.net/forum?id=qm5LpHyyOUO)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Xiang Li et al, 2022, Uniform Masking: Enabling MAE Pre-training for Pyramid-based
    Vision Transformers with Locality, link'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Zhenda Xie et al, 2022, SimMIM: A Simple Framework for Masked Image Modeling,
    [link](https://arxiv.org/abs/2111.09886)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ze Liu et al, 2021, Swin Transformer: Hierarchical Vision Transformer using
    Shifted Windows, [link](https://arxiv.org/abs/2205.10063)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Haoqi Fan et al, 2021, Multiscale Vision Transformers, [link](https://arxiv.org/abs/2104.11227)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kaiming He et al, 2021, Masked Autoencoders Are Scalable Vision Learners, [link](https://arxiv.org/abs/2111.06377)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chen Wei et al, 2021, Masked Feature Prediction for Self-Supervised Visual Pre-Training,
    [link](https://arxiv.org/abs/2112.09133)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alexey Dosovitskiy et al, 2020, An Image is Worth 16x16 Words: Transformers
    for Image Recognition at Scale, [link](https://arxiv.org/abs/2010.11929)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ashish Vaswani et al, 2017, Attention Is All You Need. [link](https://arxiv.org/abs/1706.03762)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kaiming He et al, 2015, Deep Residual Learning for Image Recognition, [link](https://arxiv.org/abs/1512.03385)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wei Yu et al, 2014, Visualizing and Comparing Convolutional Neural Networks.
    [link](https://arxiv.org/abs/1412.6631)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Karen Simonyan et al, 2014, Very Deep Convolutional Networks for Large-Scale
    Image Recognition, [link](https://arxiv.org/abs/1409.1556)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why Do We Have Huge Language Models and Small Vision Transformers?, TDS, [link](/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Visual Journey in What Vision-Transformers See, TowardsAI, [link](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vision Transformer, paperswithcode, [link](https://paperswithcode.com/method/vision-transformer)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
