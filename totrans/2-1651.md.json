["```py\naws s3api create-bucket \n  --bucket automated-training-pipeline \n  --region us-east-1\n```", "```py\n{\n    \"Location\": \"/automated-training-pipeline\"\n}\n```", "```py\nimport boto3\nimport requests\n\ndef lambda_handler(event, context):\n    s3 = boto3.client('s3')\n    data_url = 'https://example.com/dataset.csv'\n    response = requests.get(data_url)\n\n    # Assuming the data is small enough to fit into memory\n    s3.put_object(\n        Bucket='your-s3-bucket',\n        Key='data/dataset.csv',\n        Body=response.content\n    )\n```", "```py\naws ecr create-repository --repository-name fetch-iris-data --region us-east-1\n```", "```py\n# Use a base image with Python 3.11\nFROM public.ecr.aws/lambda/python:3.11\n\n# Install dependencies (if any)\nRUN pip install requests boto3\n\n# Copy your function code from the src/data directory to the Lambda task root directory\nCOPY src/data/fetch_iris_data.py ${LAMBDA_TASK_ROOT}\n\n# Set the CMD to your handler (could also be done as a parameter override outside of the Dockerfile)\nCMD [\"fetch_iris_data.lambda_handler\"]\n```", "```py\ndocker build -t fetch-iris-data .\n```", "```py\naws ecr get-login-password --region region | docker login --username AWS --password-stdin <your-account-id>.dkr.ecr.<region>.amazonaws.com\n```", "```py\ndocker tag fetch-iris-data:latest <your-account-id>.dkr.ecr.<region>.amazonaws.com/fetch-iris-data:latest\n```", "```py\ndocker push <your-account-id>.dkr.ecr.<region>.amazonaws.com/fetch-iris-data:latest\n```", "```py\naws lambda create-function \\\n    --function-name <function-name> \\\n    --package-type Image \\\n    --code ImageUri=<your-account-id>.dkr.ecr.<region>.amazonaws.com/<repository-name>:<tag> \\\n    --role arn:aws:iam::<your-account-id>:role/<role-name> \\\n    --region <region>\n```", "```py\naws lambda invoke --function-name fetch-iris-data-function --payload '{}' \n\naws s3 ls s3://automated-training-pipeline/data/\n\n#2023-10-24 13:52:41       4551 iris.data\n```", "```py\nimport argparse\nimport os\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\ndef preprocess(input_data_path, output_data_path):\n    column_names = [\n        \"sepal_length\",\n        \"sepal_width\",\n        \"petal_length\",\n        \"petal_width\",\n        \"species\",\n    ]\n    df = pd.read_csv(input_data_path, header=None, names=column_names)\n\n    df = df.dropna(subset=[\"species\"])\n\n    encoder = LabelEncoder()\n    df[\"species\"] = encoder.fit_transform(df[\"species\"])\n\n    train, test = train_test_split(df, test_size=0.2, random_state=42)\n\n    mean_train = train.mean()\n\n    train.fillna(mean_train, inplace=True)\n    test.fillna(mean_train, inplace=True)\n\n    scaler = StandardScaler()\n    scaler.fit(train[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]])\n\n    train[\n        [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n    ] = scaler.transform(\n        train[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]\n    )\n    test[\n        [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n    ] = scaler.transform(\n        test[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]\n    )\n\n    train.to_csv(\n        os.path.join(output_data_path, \"train_data.csv\"), header=False, index=False\n    )\n    test.to_csv(\n        os.path.join(output_data_path, \"test_data.csv\"), header=False, index=False\n    )\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input-data\", type=str)\n    parser.add_argument(\"--output-data\", type=str)\n    args = parser.parse_args()\n\n    preprocess(args.input_data, args.output_data)\n```", "```py\naws s3 cp preprocess.py s3://automated-training-pipeline/modeling/preprocess.py\n```", "```py\naws ecr create-repository --repository-name preprocess-iris-data --region <region>\n```", "```py\n# Use a standard Python base image\nFROM python:3.11-slim-buster\n\n# Install necessary libraries: requests, boto3, scikit-learn, pandas, numpy, etc.\nRUN pip install requests boto3 scikit-learn pandas numpy\n\n# No need to copy any Python script here since it will be provided from S3 at runtime\n\n# Set working directory\nWORKDIR /app\n\n# Set a default CMD or ENTRYPOINT in case you want to run the container for testing, but this isn't strictly necessary\n# since the Processing job will override this with ContainerEntrypoint from your Lambda function\nCMD [\"python\", \"-c\", \"print('Container started successfully')\"]\n```", "```py\ndocker build -t preprocess-iris-data .\n```", "```py\naws ecr get-login-password --region region | docker login --username AWS --password-stdin <your-account-id>.dkr.ecr.<region>.amazonaws.com\n```", "```py\ndocker tag preprocess-iris-data:latest <your-account-id>.dkr.ecr.<region>.amazonaws.com/preprocess-iris-data:latest\n```", "```py\ndocker push <your-account-id>.dkr.ecr.<region>.amazonaws.com/preprocess-iris-data:latest\n```", "```py\nimport boto3\nimport datetime\n\ndef lambda_handler(event, context):\n    sagemaker_client = boto3.client(\"sagemaker\")\n\n    # Parameters for the processing job\n    job_name = \"iris-preprocessing-job-\" + datetime.datetime.now().strftime(\n        \"%Y%m%d%H%M%S\"\n    )\n    role_arn = \"arn:aws:iam::<your-account-id>:role/service-role/<role>\"\n    image_uri = (\n        \"<your-account-id>.dkr.<region>.amazonaws.com/preprocess-iris-data:latest\"\n    )\n    input_s3_uri = \"s3://automated-training-pipeline/data/iris.data\"\n    preprocess_s3_uri = \"s3://automated-training-pipeline/modeling/preprocess.py\"\n    output_s3_uri = \"s3://automated-training-pipeline/data/\"\n\n    response = sagemaker_client.create_processing_job(\n        ProcessingJobName=job_name,\n        RoleArn=role_arn,\n        ProcessingInputs=[\n            {\n                \"InputName\": \"input-1\",\n                \"S3Input\": {\n                    \"S3Uri\": input_s3_uri,\n                    \"LocalPath\": \"/opt/ml/processing/input\",\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3InputMode\": \"File\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"S3CompressionType\": \"None\",\n                },\n            },\n            {\n                \"InputName\": \"code\",\n                \"S3Input\": {\n                    \"S3Uri\": preprocess_s3_uri,\n                    \"LocalPath\": \"/opt/ml/processing/code\",\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3InputMode\": \"File\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"S3CompressionType\": \"None\",\n                },\n            },\n        ],\n        ProcessingOutputConfig={\n            \"Outputs\": [\n                {\n                    \"OutputName\": \"output-1\",\n                    \"S3Output\": {\n                        \"S3Uri\": output_s3_uri,\n                        \"LocalPath\": \"/opt/ml/processing/output\",\n                        \"S3UploadMode\": \"EndOfJob\",\n                    },\n                }\n            ]\n        },\n        ProcessingResources={\n            \"ClusterConfig\": {\n                \"InstanceCount\": 1,\n                \"InstanceType\": \"ml.t3.medium\",\n                \"VolumeSizeInGB\": 5,\n            }\n        },\n        AppSpecification={\n            \"ImageUri\": image_uri,\n            \"ContainerArguments\": [\n                \"--input-data\",\n                \"/opt/ml/processing/input/iris.data\",\n                \"--output-data\",\n                \"/opt/ml/processing/output\",\n            ],\n            \"ContainerEntrypoint\": [\"python3\", \"/opt/ml/processing/code/preprocess.py\"],\n        },\n    )\n\n    return {\"statusCode\": 200, \"body\": response}\n```", "```py\nzip lambda_function_preprocess.zip lambda_handler_preprocess.py\n\naws s3 cp lambda_function_preprocess.zip s3://automated-training-pipeline/modeling/lambda_function_preprocess.zip\n```", "```py\naws lambda create-function \\\n    --function-name iris-preprocessing-trigger \\\n    --runtime python3.11 \\\n    --role arn:aws:iam::<your-account-id>:role/<role>\\\n    --handler lambda_handler_preprocess.lambda_handler \\\n    --code S3Bucket=automated-training-pipeline,S3Key=modeling/lambda_function_preprocess.zip \\\n    --memory-size 256 \\\n    --timeout 900 \n```", "```py\naws lambda invoke --function-name iris-preprocessing-trigger --payload '{}' \n\naws s3 ls s3://automated-training-pipeline/data/\n\n#2023-10-24 14:50:46       4551 iris.data\n#2023-10-24 17:08:42       2404 test_data.csv\n#2023-10-24 17:08:42       9635 train_data.csv\n```", "```py\nimport boto3\nimport datetime\n\ndef lambda_handler(event, context):\n    sagemaker_client = boto3.client(\"sagemaker\")\n\n    job_name = \"iris-training-job-\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    role_arn = \"arn:aws:iam::<your-account-id>:role/service-role/<role>\"  # Update the ARN as required\n\n    image_uri = \"382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:1\" # Public learner\n\n    training_data_s3_uri = \"s3://automated-training-pipeline/data/train_data.csv\"\n    output_s3_uri = \"s3://automated-training-pipeline/models/\"\n\n    response = sagemaker_client.create_training_job(\n        TrainingJobName=job_name,\n        RoleArn=role_arn,\n        AlgorithmSpecification={\n            \"TrainingImage\": image_uri,\n            \"TrainingInputMode\": \"File\",\n        },\n        InputDataConfig=[\n            {\n                \"ChannelName\": \"train\",\n                \"DataSource\": {\n                    \"S3DataSource\": {\n                        \"S3DataType\": \"S3Prefix\",\n                        \"S3Uri\": training_data_s3_uri,\n                        \"S3DataDistributionType\": \"FullyReplicated\",\n                    },\n                },\n                \"ContentType\": \"text/csv\",  # Specify that we're using CSV format\n            },\n        ],\n        OutputDataConfig={\n            \"S3OutputPath\": output_s3_uri,\n        },\n        ResourceConfig={\n            \"InstanceCount\": 1,\n            \"InstanceType\": \"ml.m5.large\",\n            \"VolumeSizeInGB\": 5,\n        },\n        HyperParameters={\n            \"predictor_type\": \"multiclass_classifier\",\n            \"num_classes\": \"3\",\n            \"mini_batch_size\": \"30\",\n        },\n        StoppingCondition={\n            \"MaxRuntimeInSeconds\": 3600,\n        },\n    )\n\n    model_artifact_uri = f\"{output_s3_uri}/{job_name}/output/model.tar.gz\"\n\n    return {\n        \"statusCode\": 200,\n        \"body\": {\"trainingJobName\": job_name, \"modelArtifactUri\": model_artifact_uri},\n    }\n```", "```py\nzip lambda_function_train.zip lambda_handler_train.py\n\naws s3 cp lambda_function_train.zip s3://automated-training-pipeline/modeling/lambda_function_train.zip\n```", "```py\naws lambda create-function \\                                                                           \n    --function-name iris-training-trigger \\\n    --runtime python3.11 \\\n    --role arn:aws:iam::<your-account-id>:role/<role> \\\n    --handler lambda_handler_train.lambda_handler \\\n    --code S3Bucket=automated-training-pipeline,S3Key=modeling/lambda_function_train.zip \\\n```", "```py\naws lambda invoke --function-name iris-training-trigger --payload '{}' output.txt\n\naws s3 ls s3://automated-training-pipeline/models/\n\n#PRE iris-training-job-20231025015943/\n```", "```py\n{\n  \"Comment\": \"A simple AWS Step Functions state machine that orchestrates 3 Lambda functions.\",\n  \"StartAt\": \"DataIngestion\",\n  \"States\": {\n    \"DataIngestion\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:data-ingestion-lambda\",\n      \"Next\": \"Preprocessing\"\n    },\n    \"Preprocessing\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:preprocessing-lambda\",\n      \"Next\": \"Training\"\n    },\n    \"Training\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:training-lambda\",\n      \"End\": true\n    }\n  }\n}\n```", "```py\naws stepfunctions create-state-machine \\\n    --name “automated-training-pipeline“ \\\n    --definition file://pipeline.json \\\n    --role-arn \"arn:aws:iam::<your-account-id>:role/<execution role>”\n```", "```py\naws stepfunctions start-execution \\\n    --state-machine-arn \"arn:aws:states:us-east-1:<your-account-id>:stateMachine:<YourStateMachineName>\"\n```"]