- en: Deploying Large Language Models With HuggingFace TGI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 HuggingFace TGI 部署大型语言模型
- en: 原文：[https://towardsdatascience.com/deploying-large-language-models-with-huggingface-tgi-981747c669e3](https://towardsdatascience.com/deploying-large-language-models-with-huggingface-tgi-981747c669e3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deploying-large-language-models-with-huggingface-tgi-981747c669e3](https://towardsdatascience.com/deploying-large-language-models-with-huggingface-tgi-981747c669e3)
- en: Another way to efficiently host and scale your LLMs with Amazon SageMaker
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker 高效托管和扩展您的 LLMs 的另一种方法
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----981747c669e3--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----981747c669e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----981747c669e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----981747c669e3--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----981747c669e3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page-----981747c669e3--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----981747c669e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----981747c669e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----981747c669e3--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----981747c669e3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----981747c669e3--------------------------------)
    ·5 min read·Jul 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----981747c669e3--------------------------------)
    ·阅读时间 5 分钟·2023 年 7 月 14 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/87080a29c8a17cddf9ed8b4ece860f12.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87080a29c8a17cddf9ed8b4ece860f12.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/4NYtYSiZVlA)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Unsplash](https://unsplash.com/photos/4NYtYSiZVlA)
- en: Large Language Models (LLMs) continue to soar in popularity as a new one is
    released nearly every week. With the number of these models increasing, so are
    the options for how we can host them. In my previous article we explored how we
    could utilize [DJL Serving](https://github.com/deepjavalibrary/djl-serving) within
    Amazon SageMaker to efficiently host LLMs. In this article we explore another
    optimized model server and solution in [HuggingFace Text Generation Inference
    (TGI)](https://github.com/huggingface/text-generation-inference).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的受欢迎程度持续上升，几乎每周都有新的模型发布。随着这些模型数量的增加，我们托管它们的选择也越来越多。在我之前的文章中，我们探讨了如何在
    Amazon SageMaker 中利用 [DJL Serving](https://github.com/deepjavalibrary/djl-serving)
    来高效托管 LLMs。在这篇文章中，我们将探索另一种优化的模型服务器和解决方案——[HuggingFace 文本生成推理（TGI）](https://github.com/huggingface/text-generation-inference)。
- en: '**NOTE**: For those of you new to AWS, make sure you make an account at the
    following [link](https://aws.amazon.com/console/) if you want to follow along.
    The article also assumes an intermediate understanding of SageMaker Deployment,
    I would suggest following this [article](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)
    for understanding Deployment/Inference more in depth.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：如果你是 AWS 新手，请确保你在以下 [链接](https://aws.amazon.com/console/) 创建一个账户，以便跟进本文。文章还假设你对
    SageMaker 部署有一定的了解，我建议你阅读这篇 [文章](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)
    以更深入地了解部署/推理。'
- en: '**DISCLAIMER**: I am a Machine Learning Architect at AWS and my opinions are
    my own.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明**：我是在 AWS 的机器学习架构师，我的观点仅代表个人意见。'
- en: Why HuggingFace Text Generation Inference? How Does It Work With Amazon SageMaker?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么选择 HuggingFace 文本生成推理？它如何与 Amazon SageMaker 配合使用？
- en: TGI is a Rust, Python, gRPC model server created by HuggingFace that can be
    used to host specific large language models. HuggingFace has long been the central
    hub for NLP and it contains a large set of optimizations when it comes to LLMs
    specifically, look below for a few and the [documentation](https://github.com/huggingface/text-generation-inference#features)
    for an extensive list.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: TGI 是由 HuggingFace 创建的 Rust、Python、gRPC 模型服务器，旨在托管特定的大型语言模型。HuggingFace 一直是
    NLP 的核心枢纽，它在 LLMs 上包含了大量优化，下面列出了一些，详细的优化列表请参见 [文档](https://github.com/huggingface/text-generation-inference#features)。
- en: Tensor Parallelism for efficient hosting across multiple GPUs
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个 GPU 上高效托管的张量并行
- en: Token Streaming with SSE
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SSE 进行令牌流媒体
- en: Quantization with [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Logits warper](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.LogitsProcessor)
    (different params such as temperature, top-k, top-n, etc)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large positive of this solution that I noted is the simplicity of use. TGI
    at this moment supports the following optimized model architectures that you can
    directly deploy utilizing the TGI containers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[BLOOM](https://huggingface.co/bigscience/bloom)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FLAN-T5](https://huggingface.co/google/flan-t5-xxl)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Galactica](https://huggingface.co/facebook/galactica-120b)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GPT-Neox](https://huggingface.co/EleutherAI/gpt-neox-20b)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Llama](https://github.com/facebookresearch/llama)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OPT](https://huggingface.co/facebook/opt-66b)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SantaCoder](https://huggingface.co/bigcode/santacoder)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Starcoder](https://huggingface.co/bigcode/starcoder)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Falcon 7B](https://huggingface.co/tiiuae/falcon-7b)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Falcon 40B](https://huggingface.co/tiiuae/falcon-40b)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even more nicely it directly integrates with Amazon SageMaker as we will explore
    in this article. SageMaker now provides managed [TGI containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)
    that we will [retrieve](https://aws.plainenglish.io/how-to-retrieve-amazon-sagemaker-deep-learning-images-ff4a5866299e)
    and use to deploy a [Llama model](https://github.com/facebookresearch/llama) in
    this example.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: TGI vs DJL Serving vs JumpStart
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far in this LLM Hosting series we’ve explored two alternative options with
    Amazon SageMaker:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[DJL Serving](/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c)'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[SageMaker JumpStart](/deploying-cohere-language-models-on-amazon-sagemaker-23a3f79639b1)'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**When do we use what and where does TGI fall into the picture?** SageMaker
    JumpStart is great if the model is already provided in its offerings. You don’t
    really need to mess with any containers or model server level work, this is all
    abstracted out for you. DJL Serving is great if you have a custom use-case with
    a model that is not supported by JumpStart. You can pull directly from the HuggingFace
    Hub or also load your own artifacts in S3 and point towards it. Another positive
    is also if you’ve mastered a specific model partitioning framework such as Accelerate
    or FasterTransformers that it integrates with, this allows you to add some advanced
    performance optimization techniques to your LLM hosting.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Where does TGI come in? TGI as we’ve noted natively supports specific model
    architectures. TGI is a great option if you are trying to deploy one of these
    supported models as it’s optimized specifically for that architecture. In addition,
    if you understand and know the model server deeper you can tune some of the environment
    variables that are exposed. Think of TGI almost as an intersection between ease
    of use and performance optimization. It’s essential to choose the right option
    out of these three for your deployment depending on your use-case and weighing
    the different pros and cons each solution has.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: TGI 的作用是什么？正如我们所提到的，TGI 原生支持特定的模型架构。如果你尝试部署这些支持的模型，TGI 是一个很好的选择，因为它专门为这些架构进行了优化。此外，如果你更深入地了解模型服务器，你可以调整一些暴露的环境变量。可以将
    TGI 视为易用性和性能优化之间的交集。根据你的用例及权衡每种解决方案的优缺点，选择合适的选项对于你的部署至关重要。
- en: Llama Deployment with TGI on Amazon SageMaker
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Amazon SageMaker 上使用 TGI 部署 Llama
- en: To work with Amazon SageMaker we will utilize the [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/)
    to grab our TGI container and streamline deployment. In this example we will follow
    Phillip Schmid’s [TGI blog](https://huggingface.co/blog/sagemaker-huggingface-llm)
    and adjust it to Llama. Make sure to first install the latest version of the SageMaker
    Python SDK and setup the necessary clients to work with the service.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与 Amazon SageMaker 配合工作，我们将利用[SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/)来获取我们的
    TGI 容器并简化部署。在这个示例中，我们将遵循 Phillip Schmid 的[TGI 博客](https://huggingface.co/blog/sagemaker-huggingface-llm)并将其调整为
    Llama。确保首先安装最新版本的 SageMaker Python SDK 并设置必要的客户端以便与服务配合使用。
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next we [retrieve](https://aws.plainenglish.io/how-to-retrieve-amazon-sagemaker-deep-learning-images-ff4a5866299e)
    the necessary SageMaker container for TGI deployment. AWS manages a set of [Deep
    Learning containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)
    (you can also [bring your own](/bring-your-own-container-with-amazon-sagemaker-37211d8412f4))
    that integrate with different model servers such as TGI, DJL Serving, TorchServe,
    and more. In this case we point the SDK to the version of TGI we need.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们[检索](https://aws.plainenglish.io/how-to-retrieve-amazon-sagemaker-deep-learning-images-ff4a5866299e)部署
    TGI 所需的 SageMaker 容器。AWS 管理一组[深度学习容器](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)（你也可以[自带容器](/bring-your-own-container-with-amazon-sagemaker-37211d8412f4)），这些容器与不同的模型服务器如
    TGI、DJL Serving、TorchServe 等集成。在这种情况下，我们将 SDK 指向我们需要的 TGI 版本。
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next comes the specific magic related to the TGI container. The simplicity of
    TGI is that we can just provide the model hub ID for the specific LLM we are working
    with. Note that it must be from the supported model architectures of TGI. In this
    case we utilize a form of the [Llama-7B model](https://huggingface.co/decapoda-research/llama-7b-hf).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是与 TGI 容器相关的具体魔法。TGI 的简单之处在于，我们只需提供特定 LLM 的模型中心 ID。请注意，它必须来自 TGI 支持的模型架构。在这种情况下，我们使用的是[Llama-7B
    模型](https://huggingface.co/decapoda-research/llama-7b-hf)的一种形式。
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the config object above you can also specify any additional optimizations
    that the TGI container supports, this includes for example quantization format
    (ex: [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)). We then pass
    in this config to a HuggingFace Model object that SageMaker understands.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的配置对象中，你还可以指定 TGI 容器支持的任何额外优化，例如量化格式（例如：[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)）。然后我们将这个配置传递给
    SageMaker 理解的 HuggingFace 模型对象。
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can then directly deploy this model object to a SageMaker Real-Time Endpoint.
    Here you can specify your hardware, for these larger models a GPU family such
    as a g5 instance is recommended. We also enable a larger container health check
    timeout due to the size of these models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以直接将这个模型对象部署到 SageMaker 实时端点。在这里你可以指定你的硬件，对于这些较大的模型，建议使用如 g5 实例的 GPU 系列。由于这些模型的大小，我们还启用了更长的容器健康检查超时时间。
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Creation of the endpoint should take a few minutes, but then you should be able
    to perform sample inference with the following code.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 创建端点应该需要几分钟时间，但随后你应该能够使用以下代码进行样本推断。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Additional Resources & Conclusion
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附加资源与结论
- en: '[](https://github.com/RamVegiraju/SageMaker-Deployment/blob/master/LLM-Hosting/TGI/tgi-llama.ipynb?source=post_page-----981747c669e3--------------------------------)
    [## SageMaker-Deployment/LLM-Hosting/TGI/tgi-llama.ipynb at master · RamVegiraju/SageMaker-Deployment'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RamVegiraju/SageMaker-Deployment/blob/master/LLM-Hosting/TGI/tgi-llama.ipynb?source=post_page-----981747c669e3--------------------------------)
    [## SageMaker-Deployment/LLM-Hosting/TGI/tgi-llama.ipynb 在 master 分支 · RamVegiraju/SageMaker-Deployment'
- en: Compilation of examples of SageMaker inference options and other features. …
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对 SageMaker 推理选项和其他功能的示例进行了汇编。…
- en: github.com](https://github.com/RamVegiraju/SageMaker-Deployment/blob/master/LLM-Hosting/TGI/tgi-llama.ipynb?source=post_page-----981747c669e3--------------------------------)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/RamVegiraju/SageMaker-Deployment/blob/master/LLM-Hosting/TGI/tgi-llama.ipynb?source=post_page-----981747c669e3--------------------------------)
- en: You can find the code for the entire example at the link above, you can also
    alternatively generate this code directly from the HuggingFace Hub at the SageMaker
    deployment tab for your supported model. Text Generation Inference provides a
    highly optimized model server that also greatly simplifies the deployment process.
    Stay tuned for more content around the LLM space, and as always thank you for
    reading and feel free to leave any feedback.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在上述链接找到整个示例的代码，也可以从 HuggingFace Hub 的 SageMaker 部署选项卡直接生成此代码以支持你的模型。文本生成推理提供了一个高度优化的模型服务器，这也大大简化了部署过程。请继续关注更多关于
    LLM 领域的内容，感谢阅读，欢迎随时留下反馈。
- en: '*If you enjoyed this article feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *and subscribe to my Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*.
    If you’re new to Medium, sign up using my* [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，欢迎通过* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *与我联系，并订阅我的 Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*。如果你是
    Medium 新手，可以通过我的* [*会员推荐*](https://ram-vegiraju.medium.com/membership)*注册。*'
