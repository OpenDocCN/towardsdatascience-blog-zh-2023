- en: Getting Started with Databricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/getting-started-with-databricks-11af3db4f595](https://towardsdatascience.com/getting-started-with-databricks-11af3db4f595)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Beginner’s Guide to Databricks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://spierre91.medium.com/?source=post_page-----11af3db4f595--------------------------------)[![Sadrach
    Pierre, Ph.D.](../Images/0e4aab43c2b981546d552beccf2259ab.png)](https://spierre91.medium.com/?source=post_page-----11af3db4f595--------------------------------)[](https://towardsdatascience.com/?source=post_page-----11af3db4f595--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----11af3db4f595--------------------------------)
    [Sadrach Pierre, Ph.D.](https://spierre91.medium.com/?source=post_page-----11af3db4f595--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----11af3db4f595--------------------------------)
    ·9 min read·May 8, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2c2e8095e951f4e978e8ed16e16747c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Alexander Grey](https://www.pexels.com/@mccutcheon/) on [Pexels](https://www.pexels.com/photo/assorted-color-bricks-1148496/)
  prefs: []
  type: TYPE_NORMAL
- en: '[Databricks](https://www.databricks.com/) allows data scientists to easily
    create and manage notebooks for research, experimentation, and deployment. The
    appeal of platforms like Databricks includes seamless integration with cloud services,
    tooling for model maintenance, and scalability.'
  prefs: []
  type: TYPE_NORMAL
- en: Databricks is very useful for model experimentation and maintenance. Databricks
    has a machine learning library, called [MLflow](https://mlflow.org/docs/latest/index.html),
    that provides useful tooling for model development and deployment. With MLflow,
    you can log models as well as metadata associated with the models such as performance
    metrics and hyperparameters. This makes it very straightforward to run experiments
    and analyze results.
  prefs: []
  type: TYPE_NORMAL
- en: Many Databricks features are useful for scaling steps within the machine learning
    workflow such as data loading, model training, and model logging. [Koalas](https://koalas.readthedocs.io/en/latest/user_guide/index.html)
    is a library in Databricks that is a more efficient alternative to pandas. [Pandas
    User-defined functions](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html)
    (UDF) allow you to apply custom functions, which are usually computationally costly,
    in a distributed manner which can significantly reduce runtime. Databricks also
    allows you to configure jobs on larger machines which can be useful for dealing
    with large data and heavy computation. Further, the model registry allows you
    to run and store experiment results for hundreds or even thousands of models.
    This is useful in terms of scaling the number of models that are researcher develops
    and eventually deploys.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will cover some of the basics of Databricks. First, we will
    walk through a simple data science workflow where we will build a churn classification
    model. We will then see how we can use tools like Koalas and Pandas UDF to speed
    up specific operations. Finally, we will see how we can use Mlflow to help us
    run experiments and inspect results.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will be working with the [Telco churn data set](https://www.kaggle.com/datasets/blastchar/telco-customer-churn).
    This data contains customer billing information for a fictional Telco company.
    It specifies whether a customer stopped or continued using the service, known
    as churning. The data is publicly available and is free to use, share and modify
    under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0).
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start, navigate to the [Databricks](https://www.databricks.com/try-databricks?itm_data=SiteWide-Footer-Trial#account)
    website and click on “Get Started for Free”:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1833bb5257a37267be55369da10dd456.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7132230adf1dd365961072333422b604.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: Enter your information and click continue. Next you will be prompted to select
    a cloud platform. We won’t be working with any external cloud platforms in this
    article. At the bottom of the right-hand panel click on “Get Started with Community
    Edition”
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47b4cf8fadf2dc1b18ecdda54087b542.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: Next follow the steps to create a Community Edition Account.
  prefs: []
  type: TYPE_NORMAL
- en: Importing Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by navigating to the ‘data’ tab in left-hand panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/710105e82b0f331d186f3977383befea.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Next click on ‘data’ and then click on create table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82186947892e23899896d43944c6f617.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: Next drag and drop the churn CSV file in the space where it says “Drop files
    to upload, or click to browse”
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aef4c8446f872bd219dc0a9697379116.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon uploading the CSV you should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f6159b520e0f8d5a8a600ca26ed1884.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Next click on “Create Table in Notebook”. A Databricks filestore (DBFS) example
    notebook with logic for writing this file to the Databricks filestore will pop
    up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d5c8a6f415b7ec67c04372e63aa9ecf.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: DBFS allows Databricks users to upload and manage data. The system is distributed
    so it is very useful for storing and managing large amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first cell specifies logic for reading the Churn data we uploaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this cell we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e89cf88ee2e09b9320df833875babbe.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'We see that the table includes column names that aren’t very useful (_c0, _c1,
    … etc). To fix this we need to specify first_row_is_header= “true”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this cell, we now get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c184ccb2e954b0405be52c3e2e73e2d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click on the table you can scroll to the right and see the additional
    columns in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94ff60c5df24c025d4a7a23810c85e82.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: Building a Classification Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s proceed by building a churn classification model using our uploaded data
    in Databricks. On the left hand panel click on ‘create’:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00b82dbbe6d74f453ccd9374bf21b383.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Next click on notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4171ef867f29c334133c338ae695725.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s name our notebook “churn_model”:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07459b4fe80e8ee77549ac32d1cd8e88.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can copy the logic from the DBFS example notebook allowing us to access
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c9c84a0c875711beccfd49796f6504d.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Next let’s convert the spark dataframe into a Pandas dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s build a [Catboost](https://catboost.ai/en/docs/) classification model.
    Catboost is a tree-based ensemble machine learning algorithm that uses gradient
    boosting to improve the performance of the successive trees used in the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s pip install the Catboost package. We do this in a cell at the top of
    the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be5bf3c5bf77210667036049c47f64cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'And let’s build a Catboost churn classification model. Let’s use tenure, monthly
    charges, and contract to predict churn outcome. Let’s convert the churn column
    to binary values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Catboost allows us to handle categorical variables directly without the need
    to convert them to machine readable codes. To do this we just define a list that
    contains the names of the categorical columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When defining the Catboost model object we set the cat_features parameter equal
    to this list. Let’s split our data for training and testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can train out Catboost model. We’ll just use default parameter values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can evaluate performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b06f8c585dc03a0c56b49e5fcd11e531.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: Koalas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here we converted a spark dataframe to a pandas dataframe. This is fine for
    our small data set, but as data size grows Pandas becomes slow and inefficient.
    An alternative to Pandas is the [Koalas](https://koalas.readthedocs.io/en/latest/user_guide/index.html)
    library. Koalas is a package developed by Databricks that is a distributed version
    of Pandas. To use Koalas we can pip install Databricks at the top of our notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'And we import Koalas from databricks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'And to convert our spark dataframe to a Koalas dataframe we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Pandas UDF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pandas UDF is another useful tool in databricks. It allows you to apply a function
    to a dataframe in a distributed manner. This is useful for increasing the efficiency
    of calculations done on large dataframes. For example, we can define a function
    that takes a data frame and builds a catboost model. We can then use Pandas UDF
    to apply this function at a grouped or categorical level. Let’s build a model
    for each value of internet service.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start we need to define our function and schema for Pandas UDF. The schema
    simply specifies the column names and their data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we will define our function. We will simply include the logic we defined
    earlier in a function called ‘build_model’. To use pandas UDF we add the decorator
    ‘@pandas_udf’:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can include the model building logic in our function. We’ll also store
    the predictions and the true churn values in our dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally we can apply this function to our dataframe. Let’s convert our Koalas
    dataframe back to a spark dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can convert the resulting spark data frame to a Pandas dataframe (also
    can convert back to Koalas) and display the first five rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7d7a6d6e0d90aa171bb314febfa49294.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: Even though we stored predictions, you can use Pandas UDF to store any information
    that you get as a result of a calculation done on a dataframe. An interesting
    excercise is to include accuracy score and precision score in the output spark
    dataframe for each internet service value.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another useful tool in Databricks is MlFlow. MlFlow allows you to easily run,
    log and analyze experiments. For this demonstration we will work with the first
    model object we defined earlier in our notebook. Let’s pip install Mlflow at the
    top of our notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'and import Mlflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s proceed by setting an experiment name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ece718ef0810f75c49e364d8783c5125.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing we can log is the Catboost feature importance which will allow us
    to analyze which features are important for predicting churn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/61d697cf0cd3ade2fa45a47407969ff9.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then log our Catboost model using the log_model method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We get a notification stating “Logged 1 run to an experiment in Mlflow”:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5dcbf7507226f71a575afc05f72460f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'We can click on the run and see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9cdd08df9849da12ea2ec121b4e60a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: This is where we can see metrics like model performance and model artifacts
    such as feature importance. Both of these we will show how to log in Mlflow shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also click on the experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6746fb34aa53ee8160544871102f1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: This is where we see each run associated with the experiment. This is useful
    for keeping track of experiments such as modifying Catboost parameters, training
    data, engineered features etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s log feature importance as an artifact, accuracy score and precision
    score as metrics, and the list of categorical inputs as a parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If we click on the run we see we logged feature importance, accuracy score
    and precision score, and categorical inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/643d1f2fbfce6e299a1fc5b6c4fa401a.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot Taken by Author
  prefs: []
  type: TYPE_NORMAL
- en: The code in the Databricks notebook has been ported to a ipython file and is
    available in [GitHub](https://github.com/spierre91/medium_code/blob/master/data_bricks_tutorials/churn_model.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this post, we discussed how to get started with Databricks. First, we saw
    how to add upload data to the DBFS. We then created a notebook and showed how
    to access the uploaded file in the notebook. We then proceed to discuss tools
    available in Databricks that help data scientists and researchers scale data science
    solutions. First, we saw how to convert spark dataframes to Koalas dataframe,
    which are a faster alternative to Pandas. We then saw how to apply customer functions
    to spark data frames using Pandas UDF. This is very useful for heavy computational
    tasks that need to be performed on large dataframes. Finally, we saw how to log
    metrics, parameters, and artifacts associated with modeling experiments. Having
    familiarity with these tools is important for anyone working in the data science,
    machine learning, and machine learning engineering spaces.
  prefs: []
  type: TYPE_NORMAL
