- en: Regularization In Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/regularisation-techniques-neural-networks-101-1f746ad45b72](https://towardsdatascience.com/regularisation-techniques-neural-networks-101-1f746ad45b72)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to avoid overfitting whilst training your neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page-----1f746ad45b72--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----1f746ad45b72--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1f746ad45b72--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1f746ad45b72--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----1f746ad45b72--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1f746ad45b72--------------------------------)
    ·8 min read·Dec 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e56562ba2f8d215063ee836b97f5018d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
    title=”neural network icons.” Neural network icons created by Vectors Tank — Flaticon.'
  prefs: []
  type: TYPE_NORMAL
- en: Table of Content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Background**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**What is Overfitting?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Lasso (L1) and Ridge (L2) Regularisation**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Early Stopping**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dropout**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Other Methods**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far in this neural networks 101 series we have discussed two ways to improve
    the performance of neural networks: [**hyperparameter tuning**](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5)
    and faster gradient descent optimisers. You can check those posts below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----1f746ad45b72--------------------------------)
    [## Hyperparameter Tuning: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: How you can improve the “learning” and “training” of neural networks through
    tuning hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----1f746ad45b72--------------------------------)
    [](/optimisation-algorithms-neural-networks-101-256e16a88412?source=post_page-----1f746ad45b72--------------------------------)
    [## Optimisation Algorithms: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: How to improve training beyond the “vanilla” gradient descent algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/optimisation-algorithms-neural-networks-101-256e16a88412?source=post_page-----1f746ad45b72--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: There is one other set of techniques that aid in performance and that is regularisation.
    This helps prevent the model from overfitting to the training dataset to have
    more accurate and consistent predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will cover a wide range of methods to regularise your neural
    network and how you can do it in PyTorch!
  prefs: []
  type: TYPE_NORMAL
- en: What is Overfitting?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s quickly recap of what we mean by overfitting in machine learning and statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '[Wikipedia](https://en.wikipedia.org/wiki/Overfitting) describes overfitting
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: “The production of an analysis that corresponds too closely or exactly to a
    particular set of data, and may therefore fail to fit to additional data or predict
    future observations reliably”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In layman’s terms, this is saying that the model is learning the data it is
    training on, but is failing to generalise. Therefore, it will have poor predictions
    on data it has not seen before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is a visual example depicting a [***null***](https://en.wikipedia.org/wiki/Null_model)
    model (under fit), a [***proposed***](/saturated-models-deviance-and-the-derivation-of-sum-of-squares-ee6fa040f52)
    model (good generalisation) and a [***saturated***](https://en.wikipedia.org/wiki/Saturated_model#:~:text=In%20mathematical%20logic%2C%20and%20particularly,reasonably%20expected%22%20given%20its%20size.)
    model (overfit):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6965182c664f2ccb7043a2e417225987.png)'
  prefs: []
  type: TYPE_IMG
- en: Plots showing, from left to right, Null Model, Proposed Model and the Saturated
    Model. Images produced by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the overfit (saturated) model goes through every data point (its
    ‘connecting-the-dots’), so it fits directly into the data. Whereas the proposed
    model has clearly generalised a lot better even though its line is not going through
    every data point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code used to generate the above plot is available on my GitHub here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Statistics/General/saturated_models.py?source=post_page-----1f746ad45b72--------------------------------)
    [## Medium-Articles/Statistics/General/saturated_models.py at main · egorhowell/Medium-Articles'
  prefs: []
  type: TYPE_NORMAL
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Statistics/General/saturated_models.py?source=post_page-----1f746ad45b72--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Lasso & Ridge Regularisation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[***Lasso***](https://en.wikipedia.org/wiki/Lasso_%28statistics%29) and [***Ridge***](https://en.wikipedia.org/wiki/Ridge_regression)
    regularisation can be similarly used for neural networks to how they are applied
    to [***linear regression***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c).
    They apply an additional penalty term to the loss function to help keep the model
    weights small or sparse to encourage simpler models to reduce the chance of overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lasso**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For Lasso regularisation (***L1***), the penalty term is the sum of the absolute
    weights used in the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/badde2d7728763cf3f5f8991400c82e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Lasso regression for a neural network. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '***λ****: The regularisation parameter*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Original Loss:*** *The initial loss without taking into account the regularisation
    terms.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***w_i****:*​ *The model’s weights*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lasso can cause some weights to become zero, creating a sparser neural network.
    This curtails the complexity of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lasso is not available directly in PyTorch, but we can add it by editing the
    loss function inside the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Ridge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ridge regularisation adds the square of the weights as the penalty term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88ac7fc23725babae3c7ec53f0a3f003.png)'
  prefs: []
  type: TYPE_IMG
- en: Ridge regression for a neural network. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: The terms in this equation are the same as above for Lasso regularisation.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The difference to Lasso regularisation is the squaring of the weights. This
    leads to the weights not being zero but does minimise their value, thus helping
    in overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ridge regularisation is much easier to implement than Lasso inside PyTorch.
    It is done by specifying the `weight_decay` argument which is the regularisation
    strength:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If `weight_decay` is too small, then there will be minimum regularisation. Therefore,
    it must be set initialised correctly. This can be achieved through trial and error
    or using hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Early Stopping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Early stopping is probably the best regularisation method for neural networks
    and machine learning in general.
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping measures the performance on an external validation set while
    the model is “learning.” If the performance on the validation set improves each
    epoch, then the neural network continues learning on the training data.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the performance on the validation set doesn’t improve for a certain
    number of epochs, typically referred to as *patience*, then training is terminated
    early.
  prefs: []
  type: TYPE_NORMAL
- en: The validation set allows us to evaluate the model on a hold-out dataset that
    is not used to train the model. This is how early stopping helps with any potential
    overfitting problem.
  prefs: []
  type: TYPE_NORMAL
- en: Some research shows that a neural network can generalise even if the performance
    on the validation set starts to degrade. This is known as [**double descent**](https://en.wikipedia.org/wiki/Double_descent)or
    [**grokking**](https://arxiv.org/abs/2201.02177), and highly recommend checking
    this out as it is such a fascinating result.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Below is an example of how you can implement early stopping on the famous [***iris
    datase***](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)t
    ([MIT License](https://github.com/olafplacha/Iris-Dataset/blob/master/LICENSE))
    using [***PyTorch***](https://pytorch.org/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4b1c352311e37dfb03fb1de0556f65fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of early stopping. Plot generated by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, training terminated at ***~260*** epochs, despite setting training
    to ***800*** epochs, as the performance on the validation set didn’t improve for
    ***10*** epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the above plot is available on my GitHub here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[## Medium-Articles/Neural Networks/regularisation.py at main · egorhowell/Medium-Articles'
  prefs: []
  type: TYPE_NORMAL
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/regularisation.py?source=post_page-----1f746ad45b72--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[***Dropout***](https://en.wikipedia.org/wiki/Dilution_%28neural_networks%29)
    is one of the most famous regularisation techniques introduced by the ‘godfather’
    of deep learning, [***Geoffrey Hinton***](https://en.wikipedia.org/wiki/Geoffrey_Hinton).
    It has been shown to improve the performance of state-of-the-art neural networks
    by a couple of percentage points.'
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind is drop is quite simple. At every epoch, each neuron has some
    probability ***p*** of being “dropped” from the learning process and is ignored.
    However, for the next epoch, it may be “active” and continue learning its optimal
    weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: the output neurons are not considered for dropout.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The probability of dropout happening, ***p,*** is a hyperparameter that can
    and should be hyperparameter-tuned for the network you are considering. In general,
    it ranges from 10–50% depending on the type of neural network you are building.
    Types include [***recurrent***](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    and [***convolutional***](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram below illustrates the dropout technique for a three-layer network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/066c830d15e40b55b6adef5eca59c189.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram for dropout where two neurons have been “dropped” from traning. Created
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: The reason dropout is so effective is that it teaches neurons to be useful on
    their own and not co-adapt with neighboring neurons. This makes them generalise
    better as they consider their inputs more sensitively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another nice way of thinking about it is that dropout leads to us training
    several different neural networks. If our network has ***n*** neurons, then we
    have ***2^n*** permutations of networks as each neuron has two states: “active”
    or “dropped.” Therefore, after 1,000 epochs, we have trained 1,000 neural networks.
    The final model is just an average of all these smaller networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dropout is easily added in PyTorch when declaring the architecture of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Other Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can reduce the number of hidden layers and the number of neurons in these
    layers to reduce complexity, hence curtailing the chance of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: More Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As always, the more data you have, the better. Having more training rows for
    your model to learn from, leads to the neural network finding the best weights
    and biases much more likely.
  prefs: []
  type: TYPE_NORMAL
- en: Augment Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Particularly for computer vision tasks, you can augment the data using random
    transformations (flip, rotate, sheer, etc.) to increase the pool of training data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary & Further Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regularisation is an important concept to get right for your neural network
    model to prevent it from overfitting on the training data. The main methods I
    recommend to add to your neural network to regularise it are early stopping and
    dropout. The coupling of these two is very effective in reducing the chance of
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Another Thing!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----1f746ad45b72--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----1f746ad45b72--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Connect With Me!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**GitHub**](https://github.com/egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References & Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Andrej Karpathy Neural Network Course*](https://www.youtube.com/watch?v=i94OvYb6noo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*PyTorch site*](https://pytorch.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition.
    Aurélien Géron. September 2019\. Publisher(s): O’Reilly Media, Inc. ISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dropout paper:* [*https://jmlr.org/papers/v15/srivastava14a.html*](https://jmlr.org/papers/v15/srivastava14a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
