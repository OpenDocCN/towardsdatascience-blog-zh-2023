- en: 'Applied Reinforcement Learning III: Deep Q-Networks (DQN)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9](https://towardsdatascience.com/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn the behavior of the DQN algorithm step by step, as well as its improvements
    compared to previous Reinforcement Learning algorithms
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@JavierMtz5?source=post_page-----8f0e38196ba9--------------------------------)[![Javier
    Martínez Ojeda](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----8f0e38196ba9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8f0e38196ba9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8f0e38196ba9--------------------------------)
    [Javier Martínez Ojeda](https://medium.com/@JavierMtz5?source=post_page-----8f0e38196ba9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8f0e38196ba9--------------------------------)
    ·7 min read·Jan 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc0aa599232d7c5acfafd8073654bc95.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Андрей Сизов](https://unsplash.com/@alpridephoto?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: If you want to read this article without a Premium Medium account, you can do
    it from this friend link :)
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://www.learnml.wiki/applied-reinforcement-learning-iii-deep-q-networks-dqn/](https://www.learnml.wiki/applied-reinforcement-learning-iii-deep-q-networks-dqn/)'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deep Q-Networks, first reported by Mnih et al. in 2013 in the paper **[1]**,
    is one of the best-known Reinforcement Learning algorithms to date, due to the
    ability it has shown since its publication to achieve higher-than-human performance
    in countless Atari games, as shown in *Figure 1*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/870e39adce5d7679af12de6a4e2dbcd3.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1**. Plot of median human-normalized score over 57 Atari games for
    each agent. **Image extracted from** [**deepmind/dqn_zoo GitHub repository**](https://github.com/deepmind/dqn_zoo)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, aside from how fascinating it is to see a DQN agent playing any of these
    Atari games as if it was a professional gamer, DQN solves some of the problems
    of an algorithm that has been known for decades: Q-Learning, which was already
    introduced and explained in the first article of this series:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[](/applied-reinforcement-learning-i-q-learning-d6086c1f437?source=post_page-----8f0e38196ba9--------------------------------)
    [## Applied Reinforcement Learning I: Q-Learning'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Understand the Q-Learning algorithm step by step, as well as the main components
    of any RL-based system
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/applied-reinforcement-learning-i-q-learning-d6086c1f437?source=post_page-----8f0e38196ba9--------------------------------)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Q-Learning intends to find a function (Q-Function) in a form of a state-action
    table that calculates the expected total sum of rewards for a given state-action
    pair, such that the agent is capable of making optimal decisions by executing
    the action that implies the highest output of the Q-Function. Although Watkins
    and Dayan demonstrated mathematically in 1992 that this algorithm converges to
    optimal Action-Values as long as the action space is discrete and each and every
    possible state and action are explored repeatedly **[2]**, this convergence is
    difficult to achieve in realistic environments. After all, in an environment with
    a continuous state space it is impossible to go through all the possible states
    and actions repeatedly, since there are an infinite number of them and the Q-Table
    would be too big.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Q-Learning 旨在找到一个形式为状态-动作表的函数（Q-Function），该函数计算给定状态-动作对的期望总奖励，以便智能体能够通过执行 Q-Function
    输出最高的动作来做出最佳决策。尽管 Watkins 和 Dayan 在 1992 年数学上证明了只要动作空间是离散的，并且每个可能的状态和动作都被反复探索，这个算法就会收敛到最佳的行动值
    **[2]**，但在实际环境中很难实现这种收敛。毕竟，在连续状态空间的环境中，不可能反复遍历所有可能的状态和动作，因为状态和动作是无限的，Q-Table 将会太大。
- en: DQN solves this problem by approximating the Q-Function through a Neural Network
    and learning from previous training experiences, so that the agent can learn more
    times from experiences already lived without the need to live them again, as well
    as avoiding the excessive computational cost of calculating and updating the Q-Table
    for continuous state spaces.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 通过通过神经网络近似 Q-Function 并从之前的训练经验中学习来解决这个问题，以便智能体能够从已经经历过的经验中多次学习，而不必再次经历这些经验，同时避免计算和更新连续状态空间的
    Q-Table 的过度计算成本。
- en: DQN Components
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN 组件
- en: Leaving aside the environment with which the agent interacts, the three main
    components of the DQN algorithm are the **Main Neural Network**, the **Target
    Neural Network**, and the **Replay Buffer**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 撇开智能体交互的环境不谈，DQN 算法的三个主要组件是**主神经网络**、**目标神经网络**和**回放缓冲区**。
- en: Main Neural Network
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主神经网络
- en: The Main NN tries to predict the expected return of taking each action for the
    given state. Therefore, the network’s output will have as many values as possible
    actions to take, and the network’s input will be a state.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 主神经网络尝试预测给定状态下采取每个动作的期望回报。因此，网络的输出将有尽可能多的值以匹配要采取的动作，而网络的输入将是状态。
- en: The neural network is trained by performing Gradient Descent to minimize the
    Loss Function, but for this a predicted value and a target value are needed with
    which to calculate the loss. The predicted value is the output of the main neural
    network for the current state and action, and the target value is calculated as
    the sum of the reward obtained plus the highest value of the target neural network’s
    output for the next state multiplied by a discount rate **γ**. The calculation
    of the loss can be mathematically understood by means of *Figure 2*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通过执行梯度下降来最小化损失函数，但需要一个预测值和一个目标值来计算损失。预测值是主神经网络对当前状态和动作的输出，目标值是通过将获得的奖励加上目标神经网络对下一个状态的输出的最高值乘以折扣率**γ**来计算的。损失的计算可以通过*图
    2*在数学上理解。
- en: '![](../Images/3df054a9f6cf96ffa3df0e25e51896de.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3df054a9f6cf96ffa3df0e25e51896de.png)'
- en: '**Figure 2**. Loss Function. Image by author'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2**。损失函数。图片由作者提供'
- en: As for why these two values are used for the loss calculation, the reason is
    that the target network, by getting the prediction of future rewards for a future
    state and action, has more information about how useful the current action is
    in the current state.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 至于为什么使用这两个值来计算损失，原因在于目标网络通过获取未来状态和动作的未来奖励预测，对当前动作在当前状态下的有用性有更多的信息。
- en: Target Neural Network
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标神经网络
- en: The Target Neural Network is used, as mentioned above, to get the target value
    for calculating the loss and optimizing it. This neural network, unlike the main
    one, will be updated every N timesteps with the weights of the main network.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 目标神经网络如上所述，用于获取计算损失和优化的目标值。这个神经网络不同于主神经网络，将每 N 个时间步更新一次，使用主网络的权重。
- en: Replay Buffer
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回放缓冲区
- en: The Replay Buffer is a list that is filled with the experiences lived by the
    agent. Making a parallelism with a Supervised Learning training, this buffer will
    be the equivalent of the dataset that is used to train, with the difference that
    the buffer must be filled little by little, as the agent interacts with the environment
    and collects information.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Replay Buffer是一个列表，包含代理所经历的经验。与监督学习训练进行类比，这个缓冲区相当于用于训练的数据集，不同之处在于缓冲区必须逐步填充，随着代理与环境互动并收集信息。
- en: In the case of the DQN algorithm, each of the experiences (rows in the dataset)
    that make up this buffer are represented by the **current state**, the **action
    taken in the current state**, the **reward** obtained after taking that action,
    whether it is a **terminal state** or not, and the **next state reached** after
    taking the action.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DQN算法，该缓冲区中的每个经验（数据集中的行）由**当前状态**、**在当前状态下采取的动作**、**采取该动作后的奖励**、是否为**终止状态**以及**采取动作后的下一个状态**表示。
- en: This method to learn from experiences, unlike the one used in the Q-Learning
    algorithm, allows learning from all the agent’s interactions with the environment
    independently of the interactions that the agent has just had with the environment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与Q-Learning算法中使用的方法不同，这种从经验中学习的方法允许从代理与环境的所有互动中学习，而不依赖于代理刚刚与环境的互动。
- en: DQN Algorithm Flow
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN算法流程
- en: The flow for the DQN algorithm is presented following the pseudocode from **[1]**,
    which is shown below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: DQN算法的流程遵循**[1]**中的伪代码，如下所示。
- en: '![](../Images/92ad62cf270f94b3af254269e6f5c4f3.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92ad62cf270f94b3af254269e6f5c4f3.png)'
- en: DQN Pseudocode. Extracted from **[1]**
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: DQN伪代码。摘自**[1]**
- en: 'For each episode, the agent performs the following steps:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一集，代理执行以下步骤：
- en: 1\. From given state, select an action
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 从给定状态中选择一个动作
- en: The action is chosen following the epsilon-greedy policy, which was previously
    explained in **[3]**. This policy will take the action with the best Q-Value,
    which is the output of the main neural network, with probability 1-ε, and will
    pick a random action with probability ε (see *Figure 3*). Epsilon (ε) is one of
    the hyperparameters to be set for the algorithm.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 动作的选择遵循ε-贪心策略，这在**[3]**中已解释过。该策略将以概率1-ε选择具有最佳Q值的动作，这个Q值是主神经网络的输出，而以概率ε选择一个随机动作（见*图3*）。ε（epsilon）是算法的一个超参数。
- en: '![](../Images/b86b14ac8e0fb54231805899983d4c93.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b86b14ac8e0fb54231805899983d4c93.png)'
- en: '**Figure 3**. Epsilon-Greedy Policy. Image by author'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3**. ε-贪心策略。图片来源于作者'
- en: 2\. Perform action on environment
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 在环境中执行动作
- en: The agent performs the action on the environment, and gets the new state reached,
    the reward obtained, and whether a terminal state has been reached. These values
    are usually returned by most gym environments **[4]** when performing an action
    via the ***step()*** method.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在环境中执行动作，获取到新的状态、获得的奖励以及是否达到终止状态。这些值通常由大多数健身环境**[4]**在通过***step()***方法执行动作时返回。
- en: 3\. Store experience in Replay Buffer
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 将经验存储在Replay Buffer中
- en: As previously mentioned, experiences are saved in the Replay Buffer as ***{s,
    a, r, terminal, s’}***, being ***s*** and ***a*** the current state and action,
    ***r*** and ***s’*** the reward and new state reached after performing the action,
    and ***terminal*** a boolean indicating whether a goal state has been reached.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，经验在Replay Buffer中保存为***{s, a, r, terminal, s’}***，其中***s***和***a***分别是当前状态和动作，***r***和***s’***是执行动作后的奖励和新状态，***terminal***是一个布尔值，表示是否达到目标状态。
- en: 4\. Sample a random batch of experiences from Replay Buffer
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 从Replay Buffer中抽取一批随机经验
- en: If the Replay Buffer has enough experiences to fill a batch (if the batch size
    is 32 and the replay buffer only has 5 experiences, the batch cannot be filled
    and this step is skipped), a batch of random experiences is taken as training
    data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Replay Buffer中有足够的经验来填充一个批次（如果批次大小为32，而Replay Buffer只有5个经验，则批次不能填充，此步骤将被跳过），将随机抽取一批经验作为训练数据。
- en: 5\. Set the target value
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 设置目标值
- en: The target value is defined in two different ways, depending on whether a terminal
    state has been reached. If a terminal state has been reached, the target value
    will be the reward received, while if the new state is not terminal, the target
    value will be, as explained before, the sum of the reward and the output of the
    target neural network with the highest Q-Value for the next state multiplied by
    a discount factor **γ**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c90c6642e0468d174acc0bd6cb25a10.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Target Value Definition. Extracted from the DQN Pseudocode in **[1]**
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: The **discount factor γ** is another hyperparameter to be set for the algorithm.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Perform Gradient Descent
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradient Descent is applied to the loss calculated from the output of the main
    neural network and the previously calculated target value, following the equation
    shown in *Figure 2*. As can be seen, the loss function used is the MSE, so the
    loss will be the difference between the output of the main network and the target
    squared.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Execute the following timestep
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the previous steps have been completed, this same process is repeated over
    and over again until the maximum number of timesteps per episode is reached or
    until the agent reaches a terminal state. When this happens, it goes to the next
    episode.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The DQN algorithm represents a considerable improvement over Q-Learning when
    it comes to training an agent in environments with continuous states, thus allowing
    greater versatility of use. In addition, the fact of using a neural network (specifically
    a convolutional one) instead of Q-Learning’s Q-Table allows images to be fed to
    the agent (for example frames of an Atari game), which adds even more versatility
    and usability to DQN.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the weaknesses of the algorithm, it should be noted that the use
    of neural networks can lead to higher execution times in each frame compared to
    Q-Learning, especially when large batch sizes are used, since carrying out the
    process of forward propagation and backpropagation with lots of data is considerably
    slower than updating the Q-Values of the Q-Table using the modified Bellman Optimality
    Equation introduced in [*Applied Reinforcement Learning I: Q-Learning*](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437).
    In addition, the use of the Replay Buffer can be a problem, as the algorithm works
    with a huge amount of data that is stored in memory, which can easily exceed the
    RAM of some computers.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: REFERENCES
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**[1]** MNIH, Volodymyr, et al. Playing atari with deep reinforcement learning.
    *arXiv preprint arXiv:1312.5602*, 2013.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '**[2]** WATKINS, Christopher JCH; DAYAN, Peter. Q-learning. *Machine learning*,
    1992, vol. 8, no 3, p. 279–292.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '**[3]** *Applied Reinforcement Learning I: Q-Learning*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '**[4]** *Gym Documentation* [https://www.gymlibrary.dev/](https://www.gymlibrary.dev/)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**[4]** *Gym 文档* [https://www.gymlibrary.dev/](https://www.gymlibrary.dev/)'
