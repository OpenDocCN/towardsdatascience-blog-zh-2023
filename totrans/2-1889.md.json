["```py\ndef create_model(mean, var, a_init=None, b_init=None, trainable=None, verbose=False):\n    \"\"\"Definition of a DeepONet with fully connected branch and trunk layers.\n\n    Args:\n    ----\n    mean: dictionary, mean values of the inputs\n    var: dictionary, variance values of the inputs\n    a_init: float, initial value for parameter a\n    b_init: float, initial value for parameter b\n    trainable: boolean, indicate whether the parameters a and b will be updated during training\n    verbose: boolean, indicate whether to show the model summary\n\n    Outputs:\n    --------\n    model: the DeepONet model\n    \"\"\"\n\n    # Branch net\n    branch_input = tf.keras.Input(shape=(len(mean['forcing'])), name=\"forcing\")\n    branch = tf.keras.layers.Normalization(mean=mean['forcing'], variance=var['forcing'])(branch_input)\n    for i in range(3):\n        branch = tf.keras.layers.Dense(50, activation=\"tanh\")(branch)\n\n    # Trunk net\n    trunk_input = tf.keras.Input(shape=(len(mean['time'])), name=\"time\")\n    trunk = tf.keras.layers.Normalization(mean=mean['time'], variance=var['time'])(trunk_input)   \n    for i in range(3):\n        trunk = tf.keras.layers.Dense(50, activation=\"tanh\")(trunk)\n\n    # Merge results \n    dot_product = tf.reduce_sum(tf.multiply(branch, trunk), axis=1, keepdims=True)\n\n    # Add the bias\n    dot_product_with_bias = BiasLayer()(dot_product)\n\n    # Add a & b trainable parameters\n    output = ParameterLayer(a_init, b_init, trainable)(dot_product_with_bias)\n\n    # Create the model\n    model = tf.keras.models.Model(inputs=[branch_input, trunk_input], outputs=output)\n\n    if verbose:\n        model.summary()\n\n    return model \n```", "```py\nclass ParameterLayer(tf.keras.layers.Layer):\n\n    def __init__(self, a, b, trainable=True):\n        super(ParameterLayer, self).__init__()\n        self._a = tf.convert_to_tensor(a, dtype=tf.float32)\n        self._b = tf.convert_to_tensor(b, dtype=tf.float32)\n        self.trainable = trainable\n\n    def build(self, input_shape):\n        self.a = self.add_weight(\"a\", shape=(1,), \n                                 initializer=tf.keras.initializers.Constant(value=self._a),\n                                 trainable=self.trainable)\n        self.b = self.add_weight(\"b\", shape=(1,), \n                                 initializer=tf.keras.initializers.Constant(value=self._b),\n                                 trainable=self.trainable)\n\n    def get_config(self):\n        return super().get_config()\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n```", "```py\n@tf.function\ndef ODE_residual_calculator(t, u, u_t, model):\n    \"\"\"ODE residual calculation.\n\n    Args:\n    ----\n    t: temporal coordinate\n    u: input function evaluated at discrete temporal coordinates\n    u_t: input function evaluated at t\n    model: DeepONet model\n\n    Outputs:\n    --------\n    ODE_residual: residual of the governing ODE\n    \"\"\"\n\n    with tf.GradientTape() as tape:\n        tape.watch(t)\n        s = model({\"forcing\": u, \"time\": t})\n\n    # Calculate gradients\n    ds_dt = tape.gradient(s, t)\n\n    # ODE residual\n    ODE_residual = ds_dt - model.layers[-1].a*u_t - model.layers[-1].b\n\n    return ODE_residual\n```", "```py\n@tf.function\ndef train_step(X, y, X_init, IC_weight, ODE_weight, data_weight, model):\n    \"\"\"Calculate gradients of the total loss with respect to network model parameters.\n\n    Args:\n    ----\n    X: training dataset for evaluating ODE residuals\n    y: target value of the training dataset\n    X_init: training dataset for evaluating initial conditions\n    IC_weight: weight for initial condition loss\n    ODE_weight: weight for ODE loss\n    data_weight: weight for data loss\n    model: DeepONet model\n\n    Outputs:\n    --------\n    ODE_loss: calculated ODE loss\n    IC_loss: calculated initial condition loss\n    data_loss: calculated data loss\n    total_loss: weighted sum of ODE loss, initial condition loss, and data loss\n    gradients: gradients of the total loss with respect to network model parameters.\n    \"\"\"\n\n    with tf.GradientTape() as tape:\n        tape.watch(model.trainable_weights)\n\n        # Initial condition prediction\n        y_pred_IC = model({\"forcing\": X_init[:, 1:-1], \"time\": X_init[:, :1]})\n\n        # Equation residual\n        ODE_residual = ODE_residual_calculator(t=X[:, :1], u=X[:, 1:-1], u_t=X[:, -1:], model=model)\n\n        # Data loss\n        y_pred_data = model({\"forcing\": X[:, 1:-1], \"time\": X[:, :1]})\n\n        # Calculate loss\n        IC_loss = tf.reduce_mean(keras.losses.mean_squared_error(0, y_pred_IC))\n        ODE_loss = tf.reduce_mean(tf.square(ODE_residual))\n        data_loss = tf.reduce_mean(keras.losses.mean_squared_error(y, y_pred_data))\n\n        # Weight loss\n        total_loss = IC_loss*IC_weight + ODE_loss*ODE_weight + data_loss*data_weight\n\n    gradients = tape.gradient(total_loss, model.trainable_variables)\n\n    return ODE_loss, IC_loss, data_loss, total_loss, gradients\n```", "```py\n# Set up training configurations\nn_epochs = 100\nIC_weight= tf.constant(1.0, dtype=tf.float32)   \nODE_weight= tf.constant(1.0, dtype=tf.float32)\ndata_weight= tf.constant(1.0, dtype=tf.float32)\n\n# Initial value for unknown parameters\na, b = 1, 1\n\n# Optimizer\noptimizer = keras.optimizers.Adam(learning_rate=1e-3)\n\n# Instantiate the PINN model\nPI_DeepONet = create_model(mean, var, a_init=a, b_init=b, trainable=True)\nPI_DeepONet.compile(optimizer=optimizer)\n\n# Start training process\nfor epoch in range(1, n_epochs + 1):  \n    print(f\"Epoch {epoch}:\")\n\n    for (X_init, _), (X, y) in zip(ini_ds, train_ds):\n\n        # Calculate gradients\n        ODE_loss, IC_loss, data_loss, total_loss, gradients = train_step(X, y, X_init, \n                                                                        IC_weight, ODE_weight,\n                                                                        data_weight, PI_DeepONet)\n        # Gradient descent\n        PI_DeepONet.optimizer.apply_gradients(zip(gradients, PI_DeepONet.trainable_variables))\n\n    # Re-shuffle dataset\n    ini_ds = tf.data.Dataset.from_tensor_slices((X_train_ini, y_train_ini))\n    ini_ds = ini_ds.shuffle(5000).batch(ini_batch_size)\n\n    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n    train_ds = train_ds.shuffle(100000).batch(col_batch_size) \n```", "```py\n@tf.function\ndef gradient_u(model, u, s_target):\n    \"\"\" Calculate the gradients of the objective function with respect to u(Â·).\n\n    Args:\n    ----\n    model: the trained PI-DeepONet model\n    u: the current u()\n    s_target: the observed s()\n\n    Outputs:\n    --------\n    obj: objective function\n    gradients: calculated gradients\n    \"\"\"\n\n    with tf.GradientTape() as tape:\n        tape.watch(u)\n\n        # Initial condition loss\n        y_pred_IC = model({\"forcing\": u, \"time\": 0})\n        IC_loss = tf.reduce_mean(keras.losses.mean_squared_error(0, y_pred_IC))\n\n        # ODE residual\n        ODE_residual = ODE_residual_calculator(t=tf.reshape(tf.linspace(0.0, 1.0, 100), (-1, 1)), \n                                               u=tf.tile(tf.reshape(u, (1, -1)), [100, 1]), \n                                               u_t=tf.reshape(u, (-1, 1)), model=model)\n        ODE_loss = tf.reduce_mean(tf.square(ODE_residual))\n\n        # Predict s() with the current u()\n        y_pred = model({\"forcing\": tf.tile(tf.reshape(u, (1, -1)), [100, 1]), \n                        \"time\": tf.reshape(tf.linspace(0.0, 1.0, 100), (-1, 1))})\n\n        # Data loss\n        data_loss = tf.reduce_mean(keras.losses.mean_squared_error(s_target, y_pred))\n\n        # Objective function\n        obj = IC_loss + ODE_loss + data_loss\n\n    # Gradient descent\n    gradients = tape.gradient(obj, u)\n\n    return obj, gradients\n```", "```py\ndef optimize_u(model, initial_u, s_target, learning_rate=5e-3, opt_steps=10000):\n    \"\"\" Determine u() that generates the observed s().\n\n    Args:\n    ----\n    model: the trained PI-DeepONet model\n    initial_u: initial guess for u()\n    s_target: the observed s()\n    learning_rate: learning rate for the optimizer\n    opt_steps: number of optimization iterations\n\n    Outputs:\n    --------\n    u: the optimized u()\n    u_hist: intermedian u results\n    objective_func_hist: value history of the objective function\n    \"\"\"\n\n    # Optimizer\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\n    # Preparation\n    u_var = tf.Variable(initial_u, dtype=tf.float32)\n    objective_func_hist = []\n    u_hist = []\n\n    for step in range(opt_steps):\n\n        # Calculate gradients\n        obj, gradients = gradient_u(model, u_var, s_target)\n\n        # Gradient descent\n        optimizer.apply_gradients([(gradients, u_var)])\n\n        # Record results\n        objective_func_hist.append(obj.numpy())\n        u_hist.append(u_var.numpy())\n\n        # Show progress\n        if step % 500 == 0:\n            print(f'Step {step} ==> Objective function = {obj.numpy()}')\n\n    # Optimized u()\n    u = u_var.numpy()\n\n    return u, objective_func_hist, u_hist\n```", "```py\ndef obj_fn(u_var, model, s_target, convert_to_numpy=False):\n    \"\"\" Calculate objective function.\n\n    Args:\n    ----\n    u_var: u()\n    model: the trained PI-DeepONet model\n    s_target: the observed s()\n    convert_to_numpy: boolean, indicate whether to convert the \n                      calculated objective value to numpy\n\n    Outputs:\n    --------\n    obj: objective function\n    \"\"\"\n\n    # Initial condition prediction\n    y_pred_IC = model({\"forcing\": u_var, \"time\": 0})\n\n    # ODE residual\n    ODE_residual = ODE_residual_calculator(t=tf.reshape(tf.linspace(0.0, 1.0, 100), (-1, 1)), \n                                           u=tf.tile(tf.reshape(u_var, (1, -1)), [100, 1]), \n                                           u_t=tf.cast(tf.reshape(u_var, (-1, 1)), tf.float32), model=model)\n\n    # Predict the output with the current inputs\n    y_pred = model({\"forcing\": tf.tile(tf.reshape(u_var, (1, -1)), [100, 1]), \n                    \"time\": tf.reshape(tf.linspace(0.0, 1.0, 100), (-1, 1))})\n\n    # Compute the data loss\n    data_loss = tf.reduce_mean(keras.losses.mean_squared_error(y_target, y_pred))\n\n    # Calculate other losses\n    IC_loss = tf.reduce_mean(keras.losses.mean_squared_error(0, y_pred_IC))\n    ODE_loss = tf.reduce_mean(tf.square(ODE_residual))\n\n    # Obj function\n    obj = IC_loss + ODE_loss + data_loss\n    obj = obj if not convert_to_numpy else obj.numpy().astype(np.float64)\n\n    return obj\n```", "```py\ndef obj_fn_numpy(u_var, model, s_target):\n    return obj_fn(u_var, model, s_target, convert_to_numpy=True)\n\ndef obj_fn_tensor(u_var, model, s_target):\n    return obj_fn(u_var, model, s_target, convert_to_numpy=False)\n```", "```py\ndef grad_fn(u, model, s_target):\n    u_var = tf.Variable(u, dtype=tf.float32)\n\n    with tf.GradientTape() as tape:\n        tape.watch(u_var)\n        obj = obj_fn_tensor(u_var, model, s_target)\n\n    # Calculate gradients\n    grads = tape.gradient(obj, u_var)\n\n    return grads.numpy().astype(np.float64)\n```", "```py\nfrom scipy.optimize import minimize\n\ndef optimize_u(model, initial_u, s_target, opt_steps=1000):\n    \"\"\"Optimize the inputs to a model to achieve a target output.\n\n    Args:\n    ----\n    model: the trained PI-DeepONet model\n    initial_u: initial guess for u()\n    s_target: the observed s()\n    opt_steps: number of optimization iterations\n\n    Outputs:\n    --------\n    res.x: the optimized u\n    \"\"\"\n\n    # L-BFGS optimizer \n    res = minimize(fun=obj_fn_numpy, x0=initial_u, args=(model, s_target),\n                   method='L-BFGS-B', jac=grad_fn, options={'maxiter': opt_steps, 'disp': True})\n\n    return res.x\n```"]