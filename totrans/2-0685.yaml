- en: 'Decoding LLMs: Creating Transformer Encoders and Multi-Head Attention Layers
    in Python from Scratch'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/decoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8](https://towardsdatascience.com/decoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring the intricacies of encoder, multi-head attention, and positional encoding
    in large language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)
    ·13 min read·Dec 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*This post was co-authored with Rafael Nardi.*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Today, Computational Natural Language Processing (NLP) is a rapidly evolving
    endeavour in which the power of computation meets linguistics. The linguistic
    side of it is mainly attributed to the theory of *Distributive Semantics* by John
    Rupert Firth. He once said the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“You shall know a word by the company it keeps”*'
  prefs: []
  type: TYPE_NORMAL
- en: So, the semantic representation of a word is determined by the context in which
    it is being used. It is precisely in attendance to this assumption that the paper
    “Attention is all you need” by Ashish Vaswani et. al. [[1]](https://github.com/zaai-ai/large-language-models-math/blob/main/attention_is_all_you_need.md#attention)
    assumes its groundbreaking relevance. It set the transformer architecture as the
    core of many of the rapidly growing tools like BERT, GPT4, Llama, etc.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we examine the key mathematical operations at the heart of
    the encoder segment in the transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ac257ecfec0d1b7c2570c38e383cfbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Self-Attention is complex (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: As always, the code is available on our [GitHub](https://github.com/zaai-ai/large-language-models).
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization, Embeddings, and Vector Spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first task one has to face while dealing with NLP problems is how to encode
    the information contained in a sentence so that the machine can handle it. Machines
    can only work with numbers which means that the words, their meanings, punctuation,
    etc, must be translated into a numeric representation. This is essentially the
    problem of embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into what embeddings are, we need to take an intermediate step
    and discuss tokenization. Here, the blocks of words or pieces of words are defined
    as the basic building blocks (so-called tokens) which will lately be represented
    as numbers. One important note is that we cannot characterize a word or piece
    of word with a single number and, thus, we use lists of numbers (vectors). It
    gives us a much bigger representation power.
  prefs: []
  type: TYPE_NORMAL
- en: 'How are they constructed? In which space do they live? The original paper works
    with vector representations of the tokens of dimension 512\. Here we are going
    to use the simplest way to represent a set of words as vectors. If we have a sentence
    composed of 3 words ‘Today is sunday’, each word in the sentence is going to be
    represented by a vector. The simplest form, and considering just these 3 words,
    is a 3-dimensional vector space. For instance, the vectors could be assigned to
    each word following a one-hot encoding rule:'
  prefs: []
  type: TYPE_NORMAL
- en: ‘Today’ — (1,0,0)
  prefs: []
  type: TYPE_NORMAL
- en: ‘is’ — (0,1,0)
  prefs: []
  type: TYPE_NORMAL
- en: ‘sunday’ — (0,0,1)
  prefs: []
  type: TYPE_NORMAL
- en: This structure (of 3 3-dimensional vectors), although possible to use, has its
    shortcomings. First, it embeds the words in a way that every one of them is orthogonal
    to any other. This means that one cannot assign the concept of semantic relation
    between words. The inner product between the associated vectors would always be
    zero.
  prefs: []
  type: TYPE_NORMAL
- en: Second, this particular structure could further be used to represent any other
    sentence of 3 different words. The problem arises when trying to represent different
    sentences made up of three words each. For a 3-dimensional space, you can only
    have 3 linearly independent vectors. Linear independence means that no vector
    in the set can be formed by a linear combination of the others. In the context
    of one-hot encoding, each vector is already linearly independent. Thus, the total
    amount of words the proposed embedding can handle is the same as the total dimension
    of the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: The average amount of words a fluent English speaker knows is around 30k, which
    would mean that we need vectors of this size to work with any typical text. Such
    a high-dimensional space poses challenges, particularly in terms of memory. Recall
    that each vector would have only one non-zero component, which would lead to a
    very inefficient use of memory and computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, let’s stick to it to complete our example. In order to describe
    at least one simple variation of this sentence, we need to extend the size of
    our vectors. In this case, let’s allow the usage of ‘sunday’ or ‘saturday’. Now,
    every word is described by a 4-dimensional vector space:'
  prefs: []
  type: TYPE_NORMAL
- en: ‘Today’ — (1,0,0,0)
  prefs: []
  type: TYPE_NORMAL
- en: ‘is’ — (0,1,0,0)
  prefs: []
  type: TYPE_NORMAL
- en: ‘sunday’ — (0,0,1,0)
  prefs: []
  type: TYPE_NORMAL
- en: ‘saturday’ — (0,0,0,1)
  prefs: []
  type: TYPE_NORMAL
- en: 'The 3 words in our sentence can be stacked together to form a matrix *X* with
    3 lines and 4 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/295b391916edc9478423fbb0c9c41758.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The Single-Head Attention Layer: Query, Key, and Value'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From *X* the transformer architecture begins by constructing 3 other sets of
    vectors (i.e. (3×4)-matrices) *Q*, *K,* and *V* (Queries, Keys, and Values). If
    you look them up online you will find the following: the Query is the information
    you are looking for, the Key is the information you have to offer and the Value
    is the information you actually get. It surely explains something about these
    objects by the analogy with database systems. Even so, we believe that the core
    understanding of them comes from the mathematical role they play in the model
    architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Matrices *Q*, *K,* and *V* are built by just multiplying *X* by 3 other matrices
    *W^Q*, *W^K,* and *W^V* of (4×4) — shape. These W-matrices contain the parameters
    that will be adjusted along the training of the model — learnable parameters.
    Thus, W’s are initially randomly chosen and they get updated by every sentence
    (or, in practice by every batch of sentences).
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let’s consider the following 3 W-matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ca264b9c08970804ac9ac62abc77625.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can create them by sampling a uniform distribution from -1 to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let’s create an abstraction to store our weight matrices so that we can use
    them later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After the multiplication with input *X,* we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c37c9b35ad7a1936fa2e9c13e5b23c1f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to (dot) multiply the query and key matrices to produce the
    attention scores. As discussed above, the resulting matrix is a result of the
    dot products (similarity) between every pair of vectors in the *Q* and *K* sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73bfc735726ba637715f4103655babdd.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We stress once again that, essentially, the attention scores represent the proximity
    of vectors in space. That is to say, for two normalized vectors, the more their
    dot product gets to 1, the closer they are to each other. It also means that the
    words are closer to each other. So, this way, the model takes into account the
    measure of proximity of words from the context of the sentence they appear in.
  prefs: []
  type: TYPE_NORMAL
- en: Then the matrix A is divided by the square root of 4\. This operation intends
    to avoid the problem of vanishing/exploding gradients. It emerges here due to
    the fact that two vectors of dimension *d_k*, whose components are distributed
    randomly with 0 mean and standard deviation 1, produce a scalar product that has
    also 0 mean but standard deviation *d_k*. Since the next step involves the exponentiation
    of these scalar product values, this implies that for some values there would
    be huge factors like *exp(d_k)* (consider the fact that the actual dimension used
    in the paper is 512). For others, there would be very small ones like *exp(−d_k)*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b449cfef13ee58bda7bf7b0467c3f30f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to apply the softmax map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/089ca17afc6cd186f7becde826e073f5.png)'
  prefs: []
  type: TYPE_IMG
- en: where *x_i* is the i-th component of a generic vector. Thus, it results in a
    distribution of probabilities. It is worth mentioning that this function is defined
    only for vectors, not for 2-dimensional matrices. When it is said that softmax
    is applied to *A_norm,* in reality, softmax is applied separately to each row
    (vector) of *A_norm.* It assumes a format of a stack of 1-dimensional vectors
    representing the weights for each vector in *V*. It means that operations that
    are typically applied to matrices, like rotations, don’t make sense in this context.
    This is because we are not dealing with a cohesive 2-dimensional entity, but rather
    a collection of separate vectors that just happen to be arranged in a 2-dimensional
    format.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc905dccc2c5644be8f6f6b666b7cd15.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Our result is then multiplied by *V* and we arrive at the one-head attention
    matrix which is an updated version of the initial *V*’s (and also the initial
    X’s):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47b2e575b0e423aa7a910466008e0689.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now build a class that initializes our weight matrices and implements
    a method to compute a single-head attention layer. Notice that we are only concerned
    about the forward pass, so methods such as backpropagation will be discussed in
    an upcoming article.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The Multi-Head Attention Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The paper defines multi-head attention as the application of this mechanism
    ℎ times in parallel, each one with its own *W^Q*, *W^K* and *W^V* matrices. At
    the end of the procedure, one has ℎ self-attention matrices, called heads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/543088b960bde7e099d67bd5bbed13a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *Q_i*, *K_i*, *V_i* are defined by multiplication of their respective
    weight matrices *W_i^Q*, *W_i^K* and *W_i^V*. In our example, we already have
    one single-head attention calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3a947fd62b7387fe8d748125b51a6c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s consider a second head matrix, which, after all the same calculations
    we carried out here, produces the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b6c75f45a8ea18297c748af21b250a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we have the single-head attention matrices, we can define the multi-head
    attention as the concatenation of all the head_i’s multiplied by a new learnable
    matrix *W_0*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20186ac7b3c4ec5ead999dff4f9f7631.png)'
  prefs: []
  type: TYPE_IMG
- en: where *W_0* (in our case, an 8×4 matrix) is randomly initiated as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa51831441a55fc2ef9fb1aba5ac3f92.png)'
  prefs: []
  type: TYPE_IMG
- en: 'to give our multi-head attention:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5cd8725df53f86e554eb1a7711233ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the result is added to the initial vectors *X* (an operation called
    residual connection):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3325d79660aebb3d14fdd08444426b61.png)'
  prefs: []
  type: TYPE_IMG
- en: The residual connection prevents the model from running into a vanishing/exploding
    gradient problem (again). The main idea is that when we add the original *X* vector
    to the result of the matrix multiplication, the norm of each final vector is resized
    to be of the same order of magnitude as the original one.
  prefs: []
  type: TYPE_NORMAL
- en: By this procedure, one maps 3 4-dimensional vectors (X’s) into another pack
    of 3 4-dimensional vectors (updated V’s). What is the gain, then? The answer is
    that now we have 3 vectors that somehow encode (and this encoding gets better
    as long training takes place) the attention/semantic relations between the words
    occurring in the sentence contrasting with the 3 initial vectors that were written
    down by the simple one-hot encoding algorithm. That is to say, now we have an
    embedding of such vectors that takes into account the context in which they appear
    in a much more refined way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement the multi-head attention layer as a new class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Positional Encoding and Fully Connected Feed-Forward Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered what we consider to be the core of the paper “Attention is all
    you need” regarding the encoder part. There are 2 important pieces that we left
    out and will discuss in this section. The first is presented at the very beginning
    of the encoder stack, namely, the positional encoding procedure.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of simplicity, the way we input the vectors into the attention
    mechanism does not consider the order of the words that appear in the sentence.
    This is indeed a major drawback. It is evident that the order of words is a crucial
    element of their semantic value, and so, it must be present in the embeddings.
    In the paper, the authors propose a solution to this problem by making use of
    sine and cosine functions. They are used to encode the order in every component
    of the embedding vectors. For a word that occurs at the i-th position in the sentence,
    each of its j-th component is associated with the positional encoding as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0282959056109d68271354154a97cc6e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and, since *PE_i* is a vector of the same size as the embedding vectors, it
    is added to them to include the positional information the word assumes in the
    sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56f2bdbda8e7ee0f0bcbcc08488b0e22.png)'
  prefs: []
  type: TYPE_IMG
- en: One great advantage of using this construction resides in the fact we have included
    new information without requiring any further space. Another one is that the information
    is spread over the whole vector and consequently, it communicates with all the
    other components of the other vectors through the many matrix multiplications
    occurring in the layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to implement our positional encoding layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Finally, at the end of the encoder, there is a simple fully connected feed-forward
    network consisting of 2 layers. Albeit it is not an innovative part of the paper,
    it plays an important role in adding nonlinearity through the ReLu activation
    functions and, as a result, capturing other semantic associations [[2]](https://github.com/zaai-ai/large-language-models-math/blob/main/attention_is_all_you_need.md#FFN).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started by exploring the concept of embeddings in NLP, explaining how words
    and their semantic meanings are translated into numerical forms that AI models
    can process. We then delved into the Transformer architecture, starting with the
    Single-Head Attention layer and explaining the roles of Queries, Keys, and Values
    in this framework.
  prefs: []
  type: TYPE_NORMAL
- en: We then covered the Attention Scores, what they represent, and how to normalize
    them in order to address the challenges of vanishing and exploding gradients.
    After guiding the understanding of how a Single-Head Attention layer works, we
    went through the process of creating a Multi-Head Attention mechanism. It allows
    the model to process and integrate multiple perspectives of input data simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: As a final component, we covered Positional Encoding and the simple fully connected
    feed-forward network. The first allows us to preserve the order of words, which
    is key to understanding the contextual meaning of sentences. The second plays
    the important role of adding nonlinearity through the activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: About me
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serial entrepreneur and leader in the AI space. I develop AI products for businesses
    and invest in AI-focused startups.
  prefs: []
  type: TYPE_NORMAL
- en: '[Founder @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Ashish Vaswani et al. (2017), Attention is all you need [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Mor Geva, et al. (2022). Transformer Feed-Forward Layers Build Predictions
    by Promoting Concepts in the Vocabulary Space, Proceedings of the 2022 Conference
    on Empirical Methods in Natural Language Processing, 30–45 [https://arxiv.org/pdf/2203.14680.pdf](https://arxiv.org/pdf/2203.14680.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: —
  prefs: []
  type: TYPE_NORMAL
- en: All images are by the author, unless noted otherwise.
  prefs: []
  type: TYPE_NORMAL
