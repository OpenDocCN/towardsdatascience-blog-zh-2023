- en: 'Decoding LLMs: Creating Transformer Encoders and Multi-Head Attention Layers
    in Python from Scratch'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码LLMs：从零开始在Python中创建Transformer编码器和多头注意力层
- en: 原文：[https://towardsdatascience.com/decoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8](https://towardsdatascience.com/decoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/decoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8](https://towardsdatascience.com/decoding-llms-creating-transformer-encoders-and-multi-head-attention-layers-in-python-from-scratch-631429553ce8)
- en: Exploring the intricacies of encoder, multi-head attention, and positional encoding
    in large language models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索大型语言模型中编码器、多头注意力和位置编码的复杂性
- en: '[](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----631429553ce8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)
    ·13 min read·Dec 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----631429553ce8--------------------------------)
    ·13分钟阅读·2023年12月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*This post was co-authored with Rafael Nardi.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇文章由Rafael Nardi共同撰写。*'
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Today, Computational Natural Language Processing (NLP) is a rapidly evolving
    endeavour in which the power of computation meets linguistics. The linguistic
    side of it is mainly attributed to the theory of *Distributive Semantics* by John
    Rupert Firth. He once said the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，计算自然语言处理（NLP）是一个迅速发展的领域，其中计算的力量与语言学相结合。语言学方面主要归因于John Rupert Firth的*分布式语义*理论。他曾说过以下话：
- en: '*“You shall know a word by the company it keeps”*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*“你可以通过一个词的陪伴来了解它”*'
- en: So, the semantic representation of a word is determined by the context in which
    it is being used. It is precisely in attendance to this assumption that the paper
    “Attention is all you need” by Ashish Vaswani et. al. [[1]](https://github.com/zaai-ai/large-language-models-math/blob/main/attention_is_all_you_need.md#attention)
    assumes its groundbreaking relevance. It set the transformer architecture as the
    core of many of the rapidly growing tools like BERT, GPT4, Llama, etc.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单词的语义表示由其使用的上下文决定。正是基于这一假设，Ashish Vaswani等人的论文“Attention is all you need”
    [[1]](https://github.com/zaai-ai/large-language-models-math/blob/main/attention_is_all_you_need.md#attention)
    具有开创性的相关性。它将transformer架构确立为许多快速发展的工具的核心，如BERT、GPT4、Llama等。
- en: In this article, we examine the key mathematical operations at the heart of
    the encoder segment in the transformer architecture.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨transformer架构中编码器部分核心数学操作的关键内容。
- en: '![](../Images/4ac257ecfec0d1b7c2570c38e383cfbc.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ac257ecfec0d1b7c2570c38e383cfbc.png)'
- en: 'Figure 1: Self-Attention is complex (image by author)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：自注意力机制很复杂（图像来源：作者）
- en: As always, the code is available on our [GitHub](https://github.com/zaai-ai/large-language-models).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，代码可以在我们的 [GitHub](https://github.com/zaai-ai/large-language-models) 上找到。
- en: Tokenization, Embeddings, and Vector Spaces
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记化、嵌入和向量空间
- en: The first task one has to face while dealing with NLP problems is how to encode
    the information contained in a sentence so that the machine can handle it. Machines
    can only work with numbers which means that the words, their meanings, punctuation,
    etc, must be translated into a numeric representation. This is essentially the
    problem of embedding.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 处理自然语言处理（NLP）问题时，首先要面对的任务是如何将句子中包含的信息编码成机器能够处理的形式。机器只能处理数字，这意味着单词、其含义、标点等必须转换成数值表示。这本质上是嵌入的问题。
- en: Before diving into what embeddings are, we need to take an intermediate step
    and discuss tokenization. Here, the blocks of words or pieces of words are defined
    as the basic building blocks (so-called tokens) which will lately be represented
    as numbers. One important note is that we cannot characterize a word or piece
    of word with a single number and, thus, we use lists of numbers (vectors). It
    gives us a much bigger representation power.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解嵌入（embeddings）之前，我们需要采取一个中间步骤并讨论分词（tokenization）。这里，单词块或单词片段被定义为基本构建块（所谓的token），这些构建块将来会被表示为数字。一个重要的注意点是，我们不能用一个单一的数字来表征一个单词或单词片段，因此我们使用数字列表（向量）。这给了我们更大的表示能力。
- en: 'How are they constructed? In which space do they live? The original paper works
    with vector representations of the tokens of dimension 512\. Here we are going
    to use the simplest way to represent a set of words as vectors. If we have a sentence
    composed of 3 words ‘Today is sunday’, each word in the sentence is going to be
    represented by a vector. The simplest form, and considering just these 3 words,
    is a 3-dimensional vector space. For instance, the vectors could be assigned to
    each word following a one-hot encoding rule:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是如何构建的？它们生活在哪个空间中？原始论文中使用的是512维的向量表示token。在这里，我们将使用最简单的方式将一组单词表示为向量。如果我们有一个由3个单词组成的句子‘今天
    是 星期天’，句子中的每个单词将由一个向量表示。最简单的形式，考虑这3个单词，是一个3维向量空间。例如，向量可以按照独热编码规则分配给每个单词：
- en: ‘Today’ — (1,0,0)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ‘今天’ — (1,0,0)
- en: ‘is’ — (0,1,0)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ‘是’ — (0,1,0)
- en: ‘sunday’ — (0,0,1)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ‘星期天’ — (0,0,1)
- en: This structure (of 3 3-dimensional vectors), although possible to use, has its
    shortcomings. First, it embeds the words in a way that every one of them is orthogonal
    to any other. This means that one cannot assign the concept of semantic relation
    between words. The inner product between the associated vectors would always be
    zero.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结构（由3个3维向量组成），虽然可以使用，但有其缺点。首先，它以一种每个单词都与其他单词正交的方式嵌入单词。这意味着无法为单词之间分配语义关系的概念。相关向量之间的内积总是零。
- en: Second, this particular structure could further be used to represent any other
    sentence of 3 different words. The problem arises when trying to represent different
    sentences made up of three words each. For a 3-dimensional space, you can only
    have 3 linearly independent vectors. Linear independence means that no vector
    in the set can be formed by a linear combination of the others. In the context
    of one-hot encoding, each vector is already linearly independent. Thus, the total
    amount of words the proposed embedding can handle is the same as the total dimension
    of the vector space.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，这种特定的结构可以进一步用于表示任何其他由3个不同单词组成的句子。问题出现在尝试表示由三个单词组成的不同句子时。对于一个3维空间，你只能拥有3个线性独立的向量。线性独立性意味着集合中的任何向量都不能通过其他向量的线性组合来形成。在独热编码（one-hot
    encoding）的上下文中，每个向量已经是线性独立的。因此，所提出的嵌入所能处理的总单词数与向量空间的总维度相同。
- en: The average amount of words a fluent English speaker knows is around 30k, which
    would mean that we need vectors of this size to work with any typical text. Such
    a high-dimensional space poses challenges, particularly in terms of memory. Recall
    that each vector would have only one non-zero component, which would lead to a
    very inefficient use of memory and computational resources.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流利的英语使用者通常知道的单词数量约为30,000，这意味着我们需要如此大小的向量来处理任何典型的文本。这种高维空间带来了挑战，特别是在内存方面。回想一下，每个向量只有一个非零分量，这将导致内存和计算资源的非常低效的使用。
- en: 'Nevertheless, let’s stick to it to complete our example. In order to describe
    at least one simple variation of this sentence, we need to extend the size of
    our vectors. In this case, let’s allow the usage of ‘sunday’ or ‘saturday’. Now,
    every word is described by a 4-dimensional vector space:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，让我们坚持完成这个示例。为了描述这个句子的至少一种简单变体，我们需要扩展向量的大小。在这种情况下，我们允许使用‘星期天’或‘星期六’。现在，每个单词都由一个4维向量空间来描述：
- en: ‘Today’ — (1,0,0,0)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ‘今天’ — (1,0,0,0)
- en: ‘is’ — (0,1,0,0)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ‘是’ — (0,1,0,0)
- en: ‘sunday’ — (0,0,1,0)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ‘星期天’ — (0,0,1,0)
- en: ‘saturday’ — (0,0,0,1)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ‘星期六’ — (0,0,0,1)
- en: 'The 3 words in our sentence can be stacked together to form a matrix *X* with
    3 lines and 4 columns:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们句子中的3个单词可以组合成一个矩阵 *X*，该矩阵有3行4列：
- en: '![](../Images/295b391916edc9478423fbb0c9c41758.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/295b391916edc9478423fbb0c9c41758.png)'
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The Single-Head Attention Layer: Query, Key, and Value'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单头注意力层：查询、键和值
- en: 'From *X* the transformer architecture begins by constructing 3 other sets of
    vectors (i.e. (3×4)-matrices) *Q*, *K,* and *V* (Queries, Keys, and Values). If
    you look them up online you will find the following: the Query is the information
    you are looking for, the Key is the information you have to offer and the Value
    is the information you actually get. It surely explains something about these
    objects by the analogy with database systems. Even so, we believe that the core
    understanding of them comes from the mathematical role they play in the model
    architecture.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *X* 开始，变换器架构通过构建另外三组向量（即（3×4）矩阵）*Q*、*K* 和 *V*（查询、键和值）。如果你在线查找，你会发现以下内容：查询是你正在寻找的信息，键是你提供的信息，而值是你实际获得的信息。这确实通过与数据库系统的类比解释了一些关于这些对象的内容。尽管如此，我们相信它们的核心理解来自于它们在模型架构中所扮演的数学角色。
- en: Matrices *Q*, *K,* and *V* are built by just multiplying *X* by 3 other matrices
    *W^Q*, *W^K,* and *W^V* of (4×4) — shape. These W-matrices contain the parameters
    that will be adjusted along the training of the model — learnable parameters.
    Thus, W’s are initially randomly chosen and they get updated by every sentence
    (or, in practice by every batch of sentences).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *Q*、*K* 和 *V* 通过将 *X* 乘以另外三个矩阵 *W^Q*、*W^K* 和 *W^V*（形状为4×4）构建。这些 W 矩阵包含在模型训练过程中会调整的参数——可学习参数。因此，W
    最初是随机选择的，并在每个句子（或实际中每个批次的句子）中更新。
- en: 'For instance, let’s consider the following 3 W-matrices:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下3个 W 矩阵：
- en: '![](../Images/8ca264b9c08970804ac9ac62abc77625.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ca264b9c08970804ac9ac62abc77625.png)'
- en: 'We can create them by sampling a uniform distribution from -1 to 1:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过从-1到1的均匀分布中采样来创建它们：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s create an abstraction to store our weight matrices so that we can use
    them later.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来创建一个抽象层来存储我们的权重矩阵，以便稍后使用。
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After the multiplication with input *X,* we get:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在与输入 *X* 相乘之后，我们得到：
- en: '![](../Images/c37c9b35ad7a1936fa2e9c13e5b23c1f.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c37c9b35ad7a1936fa2e9c13e5b23c1f.png)'
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The next step is to (dot) multiply the query and key matrices to produce the
    attention scores. As discussed above, the resulting matrix is a result of the
    dot products (similarity) between every pair of vectors in the *Q* and *K* sets:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是（点）乘查询和键矩阵以生成注意力分数。如上所述，结果矩阵是 *Q* 和 *K* 集合中每对向量之间点积（相似性）的结果：
- en: '![](../Images/73bfc735726ba637715f4103655babdd.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73bfc735726ba637715f4103655babdd.png)'
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We stress once again that, essentially, the attention scores represent the proximity
    of vectors in space. That is to say, for two normalized vectors, the more their
    dot product gets to 1, the closer they are to each other. It also means that the
    words are closer to each other. So, this way, the model takes into account the
    measure of proximity of words from the context of the sentence they appear in.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次强调，本质上，注意力分数代表了向量在空间中的接近程度。也就是说，对于两个标准化的向量，它们的点积越接近1，它们之间的距离就越近。这也意味着这些词更接近彼此。因此，模型考虑了词语在它们出现的句子上下文中的接近度。
- en: Then the matrix A is divided by the square root of 4\. This operation intends
    to avoid the problem of vanishing/exploding gradients. It emerges here due to
    the fact that two vectors of dimension *d_k*, whose components are distributed
    randomly with 0 mean and standard deviation 1, produce a scalar product that has
    also 0 mean but standard deviation *d_k*. Since the next step involves the exponentiation
    of these scalar product values, this implies that for some values there would
    be huge factors like *exp(d_k)* (consider the fact that the actual dimension used
    in the paper is 512). For others, there would be very small ones like *exp(−d_k)*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将矩阵 A 除以 4 的平方根。这个操作旨在避免梯度消失/爆炸的问题。这里出现这个问题是因为两个维度为 *d_k* 的向量，其分量随机分布，均值为0，标准差为1，会产生一个标量积，其均值为0，但标准差为
    *d_k*。由于下一步涉及这些标量积值的指数化，这意味着对于某些值会有很大的因子，如 *exp(d_k)*（考虑到论文中实际使用的维度为512）。对于其他值，则会有非常小的因子，如
    *exp(−d_k)*。
- en: '![](../Images/b449cfef13ee58bda7bf7b0467c3f30f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b449cfef13ee58bda7bf7b0467c3f30f.png)'
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We are now ready to apply the softmax map:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备应用 softmax 映射：
- en: '![](../Images/089ca17afc6cd186f7becde826e073f5.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/089ca17afc6cd186f7becde826e073f5.png)'
- en: where *x_i* is the i-th component of a generic vector. Thus, it results in a
    distribution of probabilities. It is worth mentioning that this function is defined
    only for vectors, not for 2-dimensional matrices. When it is said that softmax
    is applied to *A_norm,* in reality, softmax is applied separately to each row
    (vector) of *A_norm.* It assumes a format of a stack of 1-dimensional vectors
    representing the weights for each vector in *V*. It means that operations that
    are typically applied to matrices, like rotations, don’t make sense in this context.
    This is because we are not dealing with a cohesive 2-dimensional entity, but rather
    a collection of separate vectors that just happen to be arranged in a 2-dimensional
    format.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x_i* 是一个通用向量的第 i 个分量。因此，它导致了概率的分布。值得提到的是，这个函数仅定义在向量上，而不是 2 维矩阵上。当说对 *A_norm*
    应用 softmax 时，实际上是对 *A_norm* 的每一行（向量）分别应用 softmax。它假定格式为 1 维向量的堆栈，表示 *V* 中每个向量的权重。这意味着通常应用于矩阵的操作，比如旋转，在这种情况下没有意义。这是因为我们处理的不是一个整体的
    2 维实体，而是一个以 2 维格式排列的单独向量的集合。
- en: '![](../Images/fc905dccc2c5644be8f6f6b666b7cd15.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc905dccc2c5644be8f6f6b666b7cd15.png)'
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Our result is then multiplied by *V* and we arrive at the one-head attention
    matrix which is an updated version of the initial *V*’s (and also the initial
    X’s):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果然后乘以 *V*，最终得到一个单头注意力矩阵，这是初始 *V*（以及初始 X）的更新版本：
- en: '![](../Images/47b2e575b0e423aa7a910466008e0689.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47b2e575b0e423aa7a910466008e0689.png)'
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let’s now build a class that initializes our weight matrices and implements
    a method to compute a single-head attention layer. Notice that we are only concerned
    about the forward pass, so methods such as backpropagation will be discussed in
    an upcoming article.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来构建一个类，初始化我们的权重矩阵并实现计算单头注意力层的方法。注意，我们只关注前向传播，因此诸如反向传播的方法将在即将到来的文章中讨论。
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The Multi-Head Attention Layer
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多头注意力层
- en: 'The paper defines multi-head attention as the application of this mechanism
    ℎ times in parallel, each one with its own *W^Q*, *W^K* and *W^V* matrices. At
    the end of the procedure, one has ℎ self-attention matrices, called heads:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 论文定义多头注意力为在并行中应用此机制 ℎ 次，每次使用自己独特的 *W^Q*、*W^K* 和 *W^V* 矩阵。在程序结束时，我们会得到 ℎ 个自注意力矩阵，称为头：
- en: '![](../Images/543088b960bde7e099d67bd5bbed13a2.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/543088b960bde7e099d67bd5bbed13a2.png)'
- en: 'where *Q_i*, *K_i*, *V_i* are defined by multiplication of their respective
    weight matrices *W_i^Q*, *W_i^K* and *W_i^V*. In our example, we already have
    one single-head attention calculated:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *Q_i*、*K_i*、*V_i* 是通过其各自的权重矩阵 *W_i^Q*、*W_i^K* 和 *W_i^V* 的乘法来定义的。在我们的例子中，我们已经计算出了一个单头注意力：
- en: '![](../Images/f3a947fd62b7387fe8d748125b51a6c2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3a947fd62b7387fe8d748125b51a6c2.png)'
- en: 'Let’s consider a second head matrix, which, after all the same calculations
    we carried out here, produces the following matrix:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个第二个头矩阵，经过我们在这里进行的相同计算后，生成以下矩阵：
- en: '![](../Images/0b6c75f45a8ea18297c748af21b250a3.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b6c75f45a8ea18297c748af21b250a3.png)'
- en: 'Once we have the single-head attention matrices, we can define the multi-head
    attention as the concatenation of all the head_i’s multiplied by a new learnable
    matrix *W_0*:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到单头注意力矩阵，我们可以将多头注意力定义为所有 head_i 的串联，乘以一个新的可学习矩阵 *W_0*：
- en: '![](../Images/20186ac7b3c4ec5ead999dff4f9f7631.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20186ac7b3c4ec5ead999dff4f9f7631.png)'
- en: where *W_0* (in our case, an 8×4 matrix) is randomly initiated as
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *W_0*（在我们的例子中，是一个 8×4 的矩阵）被随机初始化为
- en: '![](../Images/aa51831441a55fc2ef9fb1aba5ac3f92.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa51831441a55fc2ef9fb1aba5ac3f92.png)'
- en: 'to give our multi-head attention:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成我们的多头注意力：
- en: '![](../Images/c5cd8725df53f86e554eb1a7711233ae.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5cd8725df53f86e554eb1a7711233ae.png)'
- en: 'Finally, the result is added to the initial vectors *X* (an operation called
    residual connection):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，结果被添加到初始向量 *X* 中（一个称为残差连接的操作）：
- en: '![](../Images/3325d79660aebb3d14fdd08444426b61.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3325d79660aebb3d14fdd08444426b61.png)'
- en: The residual connection prevents the model from running into a vanishing/exploding
    gradient problem (again). The main idea is that when we add the original *X* vector
    to the result of the matrix multiplication, the norm of each final vector is resized
    to be of the same order of magnitude as the original one.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接防止模型遇到消失/爆炸梯度问题（再次）。主要思想是，当我们将原始 *X* 向量添加到矩阵乘法结果中时，每个最终向量的范数被调整为与原始向量相同的数量级。
- en: By this procedure, one maps 3 4-dimensional vectors (X’s) into another pack
    of 3 4-dimensional vectors (updated V’s). What is the gain, then? The answer is
    that now we have 3 vectors that somehow encode (and this encoding gets better
    as long training takes place) the attention/semantic relations between the words
    occurring in the sentence contrasting with the 3 initial vectors that were written
    down by the simple one-hot encoding algorithm. That is to say, now we have an
    embedding of such vectors that takes into account the context in which they appear
    in a much more refined way.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个过程，将3个4维向量（X）映射到另一组3个4维向量（更新后的V）。那么，这样做的好处是什么？答案是，现在我们拥有3个向量，这些向量以某种方式编码（并且这种编码随着训练的进行而变得更好）句子中出现的单词之间的注意力/语义关系，相对于最初通过简单的独热编码算法得到的3个向量。也就是说，现在我们有了这样一种向量嵌入，它以更精细的方式考虑了它们出现的上下文。
- en: 'Let’s implement the multi-head attention layer as a new class:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将多头注意力层实现为一个新类：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Positional Encoding and Fully Connected Feed-Forward Network
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 位置编码和全连接前馈网络
- en: We have covered what we consider to be the core of the paper “Attention is all
    you need” regarding the encoder part. There are 2 important pieces that we left
    out and will discuss in this section. The first is presented at the very beginning
    of the encoder stack, namely, the positional encoding procedure.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了我们认为是论文《Attention is all you need》中编码器部分的核心内容。我们遗漏了两个重要部分，将在本节中讨论。第一个是出现在编码器堆栈最开头的，即位置编码过程。
- en: 'For the sake of simplicity, the way we input the vectors into the attention
    mechanism does not consider the order of the words that appear in the sentence.
    This is indeed a major drawback. It is evident that the order of words is a crucial
    element of their semantic value, and so, it must be present in the embeddings.
    In the paper, the authors propose a solution to this problem by making use of
    sine and cosine functions. They are used to encode the order in every component
    of the embedding vectors. For a word that occurs at the i-th position in the sentence,
    each of its j-th component is associated with the positional encoding as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们输入到注意力机制中的向量方式并未考虑句子中出现的单词顺序。这确实是一个重大缺陷。显然，单词的顺序是其语义价值的关键元素，因此，它必须存在于嵌入中。在论文中，作者提出了一个解决方案，利用正弦和余弦函数。它们用于对嵌入向量的每个分量进行顺序编码。对于句子中第
    i 个位置的单词，它的每个 j 个分量与位置编码关联如下：
- en: '![](../Images/0282959056109d68271354154a97cc6e.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0282959056109d68271354154a97cc6e.png)'
- en: 'and, since *PE_i* is a vector of the same size as the embedding vectors, it
    is added to them to include the positional information the word assumes in the
    sentence:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，由于*PE_i*是与嵌入向量大小相同的向量，它会被加到嵌入向量中，以包括单词在句子中的位置相关信息：
- en: '![](../Images/56f2bdbda8e7ee0f0bcbcc08488b0e22.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56f2bdbda8e7ee0f0bcbcc08488b0e22.png)'
- en: One great advantage of using this construction resides in the fact we have included
    new information without requiring any further space. Another one is that the information
    is spread over the whole vector and consequently, it communicates with all the
    other components of the other vectors through the many matrix multiplications
    occurring in the layers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种构造的一个很大优势在于我们包含了新的信息而无需额外空间。另一个优势是信息被分布在整个向量中，因此，它通过在层中发生的多次矩阵乘法与其他向量的所有组件进行通信。
- en: 'We are now ready to implement our positional encoding layer:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备实现我们的位置信息编码层：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Finally, at the end of the encoder, there is a simple fully connected feed-forward
    network consisting of 2 layers. Albeit it is not an innovative part of the paper,
    it plays an important role in adding nonlinearity through the ReLu activation
    functions and, as a result, capturing other semantic associations [[2]](https://github.com/zaai-ai/large-language-models-math/blob/main/attention_is_all_you_need.md#FFN).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在编码器的末尾，有一个简单的全连接前馈网络，由2层组成。尽管它不是论文中创新的部分，但它通过ReLu激活函数添加了非线性，从而捕获其他语义关联[[2]](https://github.com/zaai-ai/large-language-models-math/blob/main/attention_is_all_you_need.md#FFN)。
- en: 'Let’s implement them:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来实现这些：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Conclusions
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We started by exploring the concept of embeddings in NLP, explaining how words
    and their semantic meanings are translated into numerical forms that AI models
    can process. We then delved into the Transformer architecture, starting with the
    Single-Head Attention layer and explaining the roles of Queries, Keys, and Values
    in this framework.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先探讨了 NLP 中的嵌入概念，解释了词语及其语义如何被转换为 AI 模型可以处理的数值形式。接着，我们深入研究了 Transformer 架构，从单头注意力层开始，并解释了查询、键和值在这一框架中的作用。
- en: We then covered the Attention Scores, what they represent, and how to normalize
    them in order to address the challenges of vanishing and exploding gradients.
    After guiding the understanding of how a Single-Head Attention layer works, we
    went through the process of creating a Multi-Head Attention mechanism. It allows
    the model to process and integrate multiple perspectives of input data simultaneously.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们讲解了注意力得分，它们代表了什么，以及如何对其进行归一化以解决梯度消失和爆炸的问题。在指导了如何理解单头注意力层的工作原理后，我们介绍了创建多头注意力机制的过程。这使得模型能够同时处理和整合多个视角的输入数据。
- en: As a final component, we covered Positional Encoding and the simple fully connected
    feed-forward network. The first allows us to preserve the order of words, which
    is key to understanding the contextual meaning of sentences. The second plays
    the important role of adding nonlinearity through the activation functions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一个组成部分，我们涵盖了位置编码和简单的全连接前馈网络。前者使我们能够保持词语的顺序，这对理解句子的上下文意义至关重要。后者则通过激活函数发挥了增加非线性的关键作用。
- en: About me
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于我
- en: Serial entrepreneur and leader in the AI space. I develop AI products for businesses
    and invest in AI-focused startups.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 连续创业者和 AI 领域的领袖。我为企业开发 AI 产品，并投资于以 AI 为重点的初创公司。
- en: '[Founder @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[创始人 @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
- en: References
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Ashish Vaswani et al. (2017), Attention is all you need [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Ashish Vaswani 等人（2017），《注意力机制就是你所需要的一切》 [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)'
- en: '[2] Mor Geva, et al. (2022). Transformer Feed-Forward Layers Build Predictions
    by Promoting Concepts in the Vocabulary Space, Proceedings of the 2022 Conference
    on Empirical Methods in Natural Language Processing, 30–45 [https://arxiv.org/pdf/2203.14680.pdf](https://arxiv.org/pdf/2203.14680.pdf)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Mor Geva 等人（2022）。《变压器前馈层通过在词汇空间中促进概念来构建预测》，2022年自然语言处理实证方法会议论文集，第30–45页
    [https://arxiv.org/pdf/2203.14680.pdf](https://arxiv.org/pdf/2203.14680.pdf)'
- en: —
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: —
- en: All images are by the author, unless noted otherwise.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图像均由作者提供。
