- en: 'What People Write about Climate: Twitter Data Clustering in Python'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于气候的言论：Twitter 数据的 Python 聚类
- en: 原文：[https://towardsdatascience.com/what-people-write-about-climate-twitter-data-clustering-in-python-2fbbd2b95906](https://towardsdatascience.com/what-people-write-about-climate-twitter-data-clustering-in-python-2fbbd2b95906)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/what-people-write-about-climate-twitter-data-clustering-in-python-2fbbd2b95906](https://towardsdatascience.com/what-people-write-about-climate-twitter-data-clustering-in-python-2fbbd2b95906)
- en: Clustering of Twitter data with K-Means, TF-IDF, Word2Vec, and Sentence-BERT
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 K-Means、TF-IDF、Word2Vec 和 Sentence-BERT 对 Twitter 数据进行聚类
- en: '[](https://dmitryelj.medium.com/?source=post_page-----2fbbd2b95906--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page-----2fbbd2b95906--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2fbbd2b95906--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2fbbd2b95906--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page-----2fbbd2b95906--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dmitryelj.medium.com/?source=post_page-----2fbbd2b95906--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page-----2fbbd2b95906--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2fbbd2b95906--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2fbbd2b95906--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page-----2fbbd2b95906--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2fbbd2b95906--------------------------------)
    ·21 min read·May 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2fbbd2b95906--------------------------------)
    ·阅读时间 21 分钟·2023年5月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b562105ac69264137465079315ba7c15.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b562105ac69264137465079315ba7c15.png)'
- en: Tweet clusters visualization, Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 推文集群可视化，作者提供的图片
- en: 'What do people think and write about the climate, pandemics, war, or any other
    burning issue? Questions like this are interesting from a sociological perspective;
    knowing the current trends in people’s opinions can also be interesting for scientists,
    journalists, or politicians. But how can we get answers? Collecting responses
    from millions of people could have been an expensive process in the past, but
    today we can get these answers from social network posts. Many social platforms
    are available nowadays; I selected Twitter for the analysis based on several reasons:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 人们对气候、疫情、战争或其他紧迫问题的看法和言论是什么？从社会学角度看，这些问题非常有趣；了解人们意见的当前趋势对科学家、记者或政治家也很有意义。但我们如何获得答案呢？过去，收集数百万人的回应可能是一个昂贵的过程，但今天我们可以从社交网络帖子中获得这些答案。现在有许多社交平台；我选择
    Twitter 进行分析基于几个原因：
- en: Twitter was originally designed for making short posts, which could be easier
    to analyze. At least I hope that while having text size limitations, people try
    to share their thoughts in a more laconic way.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter 最初设计用于发布简短的帖子，这可能更容易分析。至少我希望在文字长度有限的情况下，人们尝试以更简洁的方式分享他们的想法。
- en: Twitter is a large social network; it was founded almost 20 years ago. It has
    about 450 million active users at the time of writing this article, so it is easy
    to get plenty of data to work with.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter 是一个大型社交网络；它成立已近 20 年。到本文撰写时，活跃用户约为 4.5 亿，因此很容易获取大量数据进行分析。
- en: Twitter has an official API, and its license allows us to use the data for research
    purposes.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter 有一个官方 API，其许可证允许我们将数据用于研究目的。
- en: The whole analysis may be pretty complex, and to make the process more clear,
    let's first describe the steps we need to implement.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 整个分析可能相当复杂，为了使过程更清晰，我们先描述一下需要实施的步骤。
- en: Methodology
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法论
- en: Our data processing pipeline will consist of several steps.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据处理管道将包括几个步骤。
- en: Collecting tweets and saving them in a CSV file.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集推文并将其保存到 CSV 文件中。
- en: Cleaning the data.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理数据。
- en: Converting the text data into numerical form. I will use 3 methods (TF-IDF,
    Word2Vec, and Sentence-BERT) to get text embeddings, and we will see which one
    is better.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本数据转换为数字形式。我将使用 3 种方法（TF-IDF、Word2Vec 和 Sentence-BERT）来获取文本嵌入，我们将看到哪一种更好。
- en: Clustering the numerical data using the K-Means algorithm and analyzing the
    results. For data visualization, I will use t-SNE (t-distributed Stochastic Neighbor
    Embedding) methods, and we will also build a word cloud for the most interesting
    clusters.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K-Means算法对数值数据进行聚类，并分析结果。为了数据可视化，我将使用t-SNE（t-分布随机邻域嵌入）方法，我们还将为最有趣的聚类构建一个词云。
- en: Without further ado, let’s get right into it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 不再赘述，让我们直接开始吧。
- en: 1\. Loading the data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 加载数据
- en: Collecting the data from Twitter is straightforward. Twitter has an official
    API and a [developer portal](https://developer.twitter.com/en/portal/dashboard);
    a free account is limited to one project, which is enough for this task. A free
    account allows us to get recent tweets for only the last 7 days. I collected data
    within a month, and it was not a problem to run the code once per week. It can
    be done manually or it can be automated using Apache Airflow, Cron, GitHub Actions,
    or any other tool. If *historical* data is really needed, Twitter also has a special
    [academic research access](https://developer.twitter.com/en/products/twitter-api/academic-research)
    program.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从Twitter收集数据很简单。Twitter有一个官方API和一个[开发者门户](https://developer.twitter.com/en/portal/dashboard)；一个免费账户限制为一个项目，这对于这项任务已经足够。免费账户允许我们获取最近7天的推文。我在一个月内收集了数据，每周运行一次代码没有问题。可以手动完成，也可以使用Apache
    Airflow、Cron、GitHub Actions或其他工具进行自动化。如果*历史*数据确实需要，Twitter还提供了一个特别的[学术研究访问](https://developer.twitter.com/en/products/twitter-api/academic-research)程序。
- en: 'After free registration in the [portal](https://developer.twitter.com/), we
    can get an API “key” and “secret” for our project. For accessing the data, I was
    using a “[tweepy”](https://docs.tweepy.org/en/stable/api.html) Python library.
    This code allows us to get all tweets with a “#climate” hashtag and save them
    in a CSV file:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在[门户](https://developer.twitter.com/)中免费注册后，我们可以为我们的项目获取API“key”和“secret”。为了访问数据，我使用了一个“[tweepy](https://docs.tweepy.org/en/stable/api.html)”Python库。该代码允许我们获取所有带有“#climate”标签的推文并将其保存到CSV文件中：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As we see, we can get a text body, hashtags, and a user id for each tweet, but
    if the tweet was retweeted, we need to get the data from the original one. Other
    fields, like the number of likes, retweets, geo coordinates, etc., are optional
    but can also be interesting for future analysis. A “wait_on_rate_limit” parameter
    is important; it allows the library to automatically make a pause if the free
    limit of API calls is reached.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们可以获取每条推文的文本正文、标签和用户ID，但如果推文被转发，我们需要从原始推文中获取数据。其他字段，如点赞数、转发数、地理坐标等，都是可选的，但对未来的分析也可能有趣。一个“wait_on_rate_limit”参数很重要；它允许库在达到API调用的免费限制时自动暂停。
- en: After running this code, I’ve got about 50,000 tweets with the hashtag “#climate”,
    posted within the last 7 days.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，我收到了大约50,000条带有“#climate”标签的推文，都是在最近7天内发布的。
- en: 2\. Text Cleaning and Transformation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 文本清理和转换
- en: Cleaning the data is one of the challenges in natural language processing, especially
    when parsing social network posts. Interestingly, there is no “only right” approach
    to that. For example, hashtags can contain important information, but sometimes
    users just copy-paste the same hashtags into all their messages, so the relevance
    of the hashtags to the message body can vary. Unicode emoji symbols can also be
    cleaned, but it may be better to convert them into text, and so on. After some
    experiments, I developed a conversion pipeline, that may not be perfect, but it
    works well enough for this task.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 清理数据是自然语言处理中的一个挑战，尤其是在解析社交网络帖子时。有趣的是，没有“唯一正确”的方法。例如，标签可能包含重要信息，但有时用户只是将相同的标签复制粘贴到他们所有的消息中，因此标签与消息正文的相关性可能会有所不同。Unicode表情符号也可以被清理，但将其转换为文本可能更好，等等。经过一些实验，我开发了一个转换管道，虽然可能不完美，但对这项任务足够有效。
- en: URLs and Mentioned user names
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: URLs和提及的用户名
- en: 'Many users just post tweets with URLs, often without any comments. It is nice
    to keep the fact that the URL was posted, so I converted all URLs to the virtual
    “#url” tag:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 许多用户只是发布带有URLs的推文，通常没有任何评论。保持记录URL的事实是很好的，因此我将所有URLs转换为虚拟的“#url”标签：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Twitter users often mention other people in the text using the “@” tag. User
    names are not relevant to the text context, and even more, names like “@AngryBeaver2021”
    are only adding noise to the data, so I removed them all:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter用户经常在文本中使用“@”标签提及其他人。用户名与文本上下文无关，而且像“@AngryBeaver2021”这样的名字只会增加数据噪声，因此我将它们全部删除：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Hashtags
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标签
- en: 'Converting hashtags is more challenging. First, I converted the sentence to
    tokens using NLTK [TweetTokenizer](https://www.nltk.org/api/nltk.tokenize.casual.html):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 转换标签更具挑战性。首先，我使用 NLTK [TweetTokenizer](https://www.nltk.org/api/nltk.tokenize.casual.html)
    将句子转换为标记：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It works, but it is not enough. People often use hashtags in the middle of
    the sentence, something like “*important #news about the climate*”. In that case,
    the word “*news*” is important to keep. At the same time, users often add a bunch
    of hashtags at the end of each message, and in most cases, those hashtags are
    just copied and pasted and not directly relevant to the text itself. So, I decided
    to remove hashtags only at the end of the sentence:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '它有效，但还不够。人们常常在句子中间使用标签，例如“*important #news about the climate*”。在这种情况下，词语“*news*”很重要，需要保留。同时，用户常常在每条消息的末尾添加一堆标签，在大多数情况下，这些标签只是复制和粘贴的，与文本本身并不直接相关。因此，我决定只在句子的末尾移除标签：'
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This is better, but it is still not good enough. People often combine several
    words in one hashtag, like “#ActOnClimate” from the last example. We can split
    this one into three words:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这更好，但仍然不够好。人们经常在一个标签中组合几个词，例如上一个例子中的“#ActOnClimate”。我们可以将这个标签拆分为三个词：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As a final result of this step, the phrase “*This system combines #solar with
    #wind turbines. #ActOnClimate now. #Capitalism #climate #economics*” will be converted
    into “*This system combines #solar with #wind turbines. Act On Climate now.*”.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '作为这一阶段的最终结果，短语“*This system combines #solar with #wind turbines. #ActOnClimate
    now. #Capitalism #climate #economics*”将被转换为“*This system combines #solar with
    #wind turbines. Act On Climate now.*”。'
- en: Removing short tweets
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移除短推文
- en: Many users often post pictures or videos without providing any text at all.
    In that case, the message body is almost empty. These posts are mostly useless
    for analysis, so I keep in the dataframe only sentences longer than 32 characters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 许多用户经常发布没有任何文本的图片或视频。在这种情况下，消息体几乎是空的。这些帖子在分析中大多无用，因此我只保留长度超过 32 个字符的句子在数据框中。
- en: Lemmatization
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词形还原
- en: Lemmatization is the process of converting words into their original, canonical
    form.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原是将单词转换为其原始规范形式的过程。
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Lemmatizing the text can reduce the number of words in the text, and the clustering
    algorithm may work better. A [spaCy lemmatizer](https://spacy.io/api/lemmatizer)
    is analyzing the whole sentence; for example, the phrases “I saw a mouse” and
    “cut wood with a saw” will provide different results for the word “saw”. Thus,
    the lemmatizer should be called *before* cleaning the stopwords.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对文本进行词形还原可以减少文本中的单词数量，聚类算法可能会更有效。一个 [spaCy 词形还原器](https://spacy.io/api/lemmatizer)
    正在分析整个句子；例如，“I saw a mouse”和“cut wood with a saw”这两个短语将为“saw”提供不同的结果。因此，词形还原器应该在清理停用词之前调用。
- en: 'These steps are enough to clean up tweets. Of course, nothing is perfect, but
    for our task, it looks good enough. For readers who would like to do experiments
    on their own, the full code is presented below:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤足以清理推文。当然，没有什么是完美的，但对我们的任务来说，这看起来已经足够好了。对于希望自行实验的读者，完整的代码如下：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As a bonus, with clean text, we can easily draw a word cloud:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外收获，使用干净的文本，我们可以轻松绘制词云：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The result looks like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![](../Images/a9de7799b276aae146ca5d60e8ca7e26.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9de7799b276aae146ca5d60e8ca7e26.png)'
- en: Word cloud made from the cleaned text, Image by author
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从清理后的文本中生成的词云，图片由作者提供
- en: It's not a real analysis yet but this image can already give some insights into
    what people write about the climate. For example, we can see that often people
    post links (“URL” is the biggest word in the cloud), and words like “energy”,
    “waste”, “fossil”, or “crisis” are also relevant and important.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这还不是实际的分析，但这张图片已经能提供一些关于人们讨论气候的见解。例如，我们可以看到人们经常发布链接（“URL”是云图中最大的词），以及“energy”、“waste”、“fossil”或“crisis”等词也是相关且重要的。
- en: 3\. Vectorization
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 向量化
- en: Text vectorization is the process of converting text data into a numerical representation.
    Most of the algorithms, including K-Means clustering as well, require vectors
    and not plain text. And the conversion itself is not straightforward. The challenge
    is not just to *somehow* assign some random vectors to all words; ideally, the
    words-to-vector conversion should keep the relationship between those words in
    the original language.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 文本向量化是将文本数据转换为数值表示的过程。大多数算法，包括 K-Means 聚类，都需要向量而不是纯文本。而且转换本身并不简单。挑战不仅仅是*以某种方式*为所有单词分配一些随机向量；理想情况下，单词到向量的转换应该保持这些单词在原始语言中的关系。
- en: I will test three different approaches, and we can see the advantages and disadvantages
    of each one.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我将测试三种不同的方法，我们可以看到每种方法的优缺点。
- en: TF-IDF
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF
- en: '[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Term Frequency-Inverse
    Document Frequency) is a pretty old algorithm; a term-weighting function known
    as IDF was already proposed in the 1970s. The TF-IDF result is based on a numerical
    statistic, where the TF (term frequency) is the number of times the word appeared
    in the document (in our case, in the tweet), and the IDF (inverse document frequency)
    shows how often the same word appears in the text corpus (full set of documents).
    The higher the score of the particular word, the more important this word is in
    the specific tweet.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)（词频-逆文档频率）是一个相当古老的算法；早在
    1970 年代就提出了一个称为 IDF 的术语加权函数。TF-IDF 的结果基于数字统计，其中 TF（词频）是单词在文档中出现的次数（在我们的例子中是推文中），而
    IDF（逆文档频率）显示了相同单词在文本语料库（文档全集）中出现的频率。特定单词的得分越高，该词在特定推文中的重要性就越大。'
- en: 'Before processing the real dataset, let’s consider a toy example. Two tweets,
    cleaned from stop words, the first one is about climate, and the second is about
    cats:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理真实数据集之前，让我们考虑一个简单的例子。两个去除停用词的推文，第一个关于气候，第二个关于猫：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The result looks like this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '![](../Images/beacff456ff0c0907e2055190bad1ccc.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/beacff456ff0c0907e2055190bad1ccc.png)'
- en: As we can see, we got two vectors from two tweets. Each digit in the vector
    is proportional to the “importance” of the word in the particular tweet. For example,
    the word “climate” was repeated twice in the first tweet. It has a high value
    there and a zero value in the second tweet (and obviously, the output for the
    word “cat” is the opposite).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们从两个推文中得到了两个向量。向量中的每个数字与特定推文中单词的“重要性”成正比。例如，“climate”一词在第一条推文中出现了两次，因此在第一条推文中它的值很高，而在第二条推文中它的值为零（显然，“cat”一词的输出则相反）。
- en: 'Let’s try the same approach on a real dataset we collected before:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在之前收集的真实数据集上尝试相同的方法：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: TfidfVectorizer did the job; it converted each tweet to a vector. The dimension
    of vectors is equal to the *total number of words* in the corpus, which is pretty
    large. In my case, 19,197 tweets have 22,735 unique tokens, and as an output,
    I got a matrix of the 19,197x22,735 shape! Using such a matrix can be challenging,
    even for modern computers.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: TfidfVectorizer 完成了任务；它将每条推文转换为一个向量。向量的维度等于语料库中*单词的总数*，这相当庞大。在我的案例中，19,197 条推文有
    22,735 个唯一的标记，最终得到了一个 19,197x22,735 的矩阵！使用这样一个矩阵可能会很具挑战性，即使对于现代计算机也是如此。
- en: We will cluster this data in the next step, but before that, let's test other
    vectorization methods.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一步对这些数据进行聚类，但在此之前，让我们测试其他向量化方法。
- en: Word2Vec
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2Vec
- en: 'Word2Vec is another approach for word vectorization; the first paper about
    this method was introduced by [Tomas Mikolov in 2013 at Google](https://arxiv.org/pdf/1310.4546.pdf).
    There are different algorithms ([Skip-gram and CBOW models](https://radimrehurek.com/gensim/models/word2vec.html))
    available in the implementation; the general idea is to train the model on a large
    text corpus and get accurate word-to-vector representations. This model is able
    to learn the relations between different words, as shown in the original paper:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 是另一种词向量化方法；有关此方法的第一篇论文是 [Tomas Mikolov 于 2013 年在 Google 介绍的](https://arxiv.org/pdf/1310.4546.pdf)。实现中提供了不同的算法（[Skip-gram
    和 CBOW 模型](https://radimrehurek.com/gensim/models/word2vec.html)）；一般思想是对大型文本语料库进行模型训练，从而获得准确的词到向量的表示。该模型能够学习不同单词之间的关系，如原始论文中所示：
- en: '![](../Images/b448f996de8225322ce93647bf174cf2.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b448f996de8225322ce93647bf174cf2.png)'
- en: Country and Capital Vectors Projected by PCA, Source © [https://arxiv.org/pdf/1310.4546.pdf](https://arxiv.org/pdf/1310.4546.pdf)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 投影的国家和首都向量，来源 © [https://arxiv.org/pdf/1310.4546.pdf](https://arxiv.org/pdf/1310.4546.pdf)
- en: Probably the most famous example of using this model is the relationship between
    the words “king”, “man” and “queen”. Those who are interested in details can read
    [this nice article](/word-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最著名的使用此模型的例子是“king”、“man”和“queen”之间的关系。那些对细节感兴趣的人可以阅读 [这篇很好的文章](/word-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d)。
- en: 'For our task, I will be using a [pre-trained vector](https://code.google.com/archive/p/word2vec/)
    file. This model was trained using the Google News dataset; the file contains
    vectors for 3 million words and phrases. Before using a real dataset, let’s again
    consider a toy example:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的任务，我将使用一个[预训练向量](https://code.google.com/archive/p/word2vec/)文件。这个模型是用Google新闻数据集训练的；文件包含300万词和短语的向量。在使用真实数据集之前，我们先考虑一个玩具示例：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The result looks like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '![](../Images/1fd975b98107512becb6215834d08b9a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fd975b98107512becb6215834d08b9a.png)'
- en: As we can see, the word “climate” was converted into a 300-digit length array.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，单词“climate”被转换成了一个300位长度的数组。
- en: 'Using Word2Vec, we can get embeddings for each word, but we need an embedding
    for the whole tweet. As the easiest approach, we can use the word embedding arithmetic
    and get the mean of all vectors:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Word2Vec，我们可以为每个词获得嵌入，但我们需要整个推文的嵌入。作为最简单的方法，我们可以使用词嵌入的算术运算并获取所有向量的均值：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This works because words with similar meanings are converted into close vectors,
    and vice versa. With this method, we can convert all our tweets into embedding
    vectors:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为具有相似意义的词被转换成接近的向量，反之亦然。通过这种方法，我们可以将所有的推文转换成嵌入向量：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As we can see, Word2Vec’s output is much more memory-efficient compared to the
    TF-IDF approach. We have 300-dimensional vectors for each tweet, and the shape
    of the output matrix is 19,197x300 instead of 19,197x22,735 — a 75x difference
    in memory footprint!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，与TF-IDF方法相比，Word2Vec的输出在内存使用上要高效得多。我们为每条推文有300维的向量，输出矩阵的形状是19,197x300，而不是19,197x22,735——内存占用差异为75倍！
- en: '[Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html) is another model
    that can be more effective for making document embeddings compared to “naive”
    averaging; it was specially designed for vector representations of the documents.
    But at the time of writing this article, I was not able to find a pre-trained
    Doc2Vec model. Readers are welcome to try this on their own.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)是另一种模型，相比“简单”平均方法，它在生成文档嵌入方面可能更有效；它特别为文档的向量表示设计的。但在写这篇文章时，我没有找到预训练的Doc2Vec模型。读者可以自行尝试。'
- en: Sentence-BERT
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sentence-BERT
- en: 'In the previous step, we got word embeddings using Word2Vec. It works, but
    this approach has an obvious disadvantage. Word2Vec does not respect the context
    of the word; for example, the word “bank” in the sentence “the river bank” will
    get the same embedding as “the bank of England”. To fix that and get more accurate
    embeddings, we can try a different approach. The BERT (Bidirectional Encoder Representations
    from Transformer) language model was [introduced in 2018](https://arxiv.org/abs/1810.04805).
    It was trained on masked text sentences, in which the position and context of
    each word really matter. BERT was not originally made for calculating embeddings,
    but it turned out that extracting embeddings from BERT layers is an effective
    approach (those TDS articles from 2019 and 2020 can provide more details: [1](https://medium.com/towards-data-science/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b),
    [2](https://medium.com/towards-data-science/text-classification-with-no-model-training-935fe0e42180)).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步中，我们使用Word2Vec获得了词嵌入。它有效，但这种方法有一个明显的缺点。Word2Vec不考虑词的上下文；例如，“bank”这个词在句子“the
    river bank”和“the bank of England”中会得到相同的嵌入。为了解决这个问题并获得更准确的嵌入，我们可以尝试另一种方法。BERT（Bidirectional
    Encoder Representations from Transformer）语言模型于[2018年推出](https://arxiv.org/abs/1810.04805)。它是在被遮蔽的文本句子上训练的，其中每个词的位置和上下文确实很重要。BERT最初并不是为了计算嵌入而设计的，但事实证明，从BERT层中提取嵌入是一种有效的方法（2019年和2020年的那些TDS文章提供了更多细节：[1](https://medium.com/towards-data-science/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b)，[2](https://medium.com/towards-data-science/text-classification-with-no-model-training-935fe0e42180)）。
- en: 'Nowadays, several years later, the situation has improved, and we don’t need
    to extract raw embeddings from BERT manually; special projects like [Sentence
    Transformers](https://github.com/UKPLab/sentence-transformers) were specially
    designed for that. Before processing a real dataset, let’s consider a toy example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，几年后，情况已有所改善，我们不需要手动从BERT中提取原始嵌入；像[Sentence Transformers](https://github.com/UKPLab/sentence-transformers)这样的特别项目专门为此设计。在处理真实数据集之前，我们先考虑一个玩具示例：
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As an output, we got two 384-dimensional vectors for our sentences. As we can
    see, using the model is easy, and even removing the stop words is not required;
    the library is doing all this automatically.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输出，我们得到了两个 384 维度的向量用于我们的句子。如我们所见，使用模型很简单，甚至不需要移除停用词；该库会自动处理所有这些。
- en: 'Let’s now get embeddings for our tweets. Since BERT word embeddings are sensitive
    to word context and the library has its own cleaning and tokenization, I will
    not use the “text_clean” column as before. Instead, I will only convert tweet
    URLs and hashtags to text. The “partial_clean” method uses parts of the code from
    the original “text_clean” function, used at the beginning of this article:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们获取推文的嵌入。由于 BERT 词嵌入对词语上下文敏感，并且该库具有自己的清理和标记化，我不会像之前那样使用“text_clean”列。相反，我只会将推文
    URL 和标签转换为文本。“partial_clean”方法使用了本文开头“text_clean”函数的部分代码：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As an output of the sentence transformer, we got an array of 19,197x384 dimensionality.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作为句子变换器的输出，我们得到了一个 19,197x384 维度的数组。
- en: As a side note, it is important to mention that the BERT model is much more
    computationally “heavy” compared to Word2Vec. Calculating vectors for 19,197 tweets
    took about 80 seconds on a 12-core CPU, compared to only 1,8 seconds required
    by Word2Vec. It is not a problem for doing tests like this, but it can be more
    expensive to use in a cloud environment.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 附带说明一下，BERT 模型在计算上比 Word2Vec 要“重”得多。计算 19,197 条推文的向量在一个 12 核 CPU 上花费了大约 80 秒，而
    Word2Vec 只需 1.8 秒。这在进行这种测试时不是问题，但在云环境中使用可能会更昂贵。
- en: 4\. Clustering and Visualization
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 聚类与可视化
- en: 'Finally, we’re approaching the last part of this article. During previous steps,
    we got 3 versions of the “vectorized_docs” array, generated by using 3 methods:
    TF-IDF, Word2Vec, and Sentence-BERT. Let’s cluster these embeddings into groups
    and see what information we can extract.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们接近本文的最后部分。在之前的步骤中，我们获得了 3 个“vectorized_docs”数组的版本，这些版本通过使用 3 种方法生成：TF-IDF、Word2Vec
    和 Sentence-BERT。让我们将这些嵌入进行分组，并查看可以提取哪些信息。
- en: 'To do this, let’s first make several helper functions:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，让我们首先创建几个辅助函数：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: I am using [SciKit-learn KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
    to make **K-Means clustering**. A “make_clustered_dataframe” method creates a
    dataframe with original tweets and a new “cluster” column. When using K-Means,
    we also have two metrics that help us evaluate the results. **Inertia** can be
    used to measure clustering quality. It is calculated by measuring the distance
    between all cluster points and cluster centroids, and the lower the value, the
    better. Another useful metric is the **silhouette score**; this value has a range
    of [-1, 1]. If the value is close to 1, the clusters are well separated; if the
    value is about 0, the distance is not significant; and if the values are negative,
    the clusters are overlapping.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在使用 [SciKit-learn KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
    进行 **K-Means 聚类**。一个“make_clustered_dataframe”方法创建了一个包含原始推文和一个新“cluster”列的数据框。在使用
    K-Means 时，我们还有两个指标可以帮助我们评估结果。**惯性**可以用来衡量聚类质量。它是通过测量所有聚类点与聚类中心之间的距离来计算的，值越低越好。另一个有用的指标是
    **轮廓系数**；该值的范围为 [-1, 1]。如果值接近 1，则聚类分离良好；如果值接近 0，则距离不显著；如果值为负，则表示聚类重叠。
- en: 'The output of “make_clustered_dataframe” looks like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: “make_clustered_dataframe”的输出如下所示：
- en: '![](../Images/2bcc23cc40c33e609cd81d0207241a77.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bcc23cc40c33e609cd81d0207241a77.png)'
- en: 'It works, but only using this information, it’s hard to see if the clusters
    are good enough. Let’s add another helper method to display the **top clusters**,
    sorted by silhouette score. I use the [SciKit-learn silhouette_samples](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html)
    method to calculate this. I will also use a **word cloud** to visualize each cluster:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实有效，但仅使用这些信息，很难判断聚类是否足够好。让我们添加另一个辅助方法来显示 **最佳聚类**，按轮廓系数排序。我使用 [SciKit-learn
    silhouette_samples](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html)
    方法来计算这个。我还会使用 **词云** 来可视化每个聚类：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The next important question before using the K-Means method is choosing the
    “K”, optimal number of clusters. The [**Elbow method**](https://en.wikipedia.org/wiki/Elbow_method_(clustering))
    is a popular technique; the idea is to build the inertia value graph for different
    K-values. The “elbow” point on the graph is (at least in theory) the value of
    optimum K. Practically, it rarely works as expected, especially for badly structured
    datasets such as vectorized tweets, but the graph can give some insights. Let’s
    make a helper method to draw the elbow graph:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用K-Means方法之前，下一步重要的问题是选择“**K**”，即最佳簇数。[**肘部法**](https://en.wikipedia.org/wiki/Elbow_method_(clustering))是一种流行的技术；其思路是构建不同K值的惯性值图。图中的“肘部”点（至少理论上）是最佳K值。实际上，它很少如预期般工作，尤其是对于结构不佳的数据集，如向量化的推文，但该图可以提供一些见解。让我们制作一个辅助方法来绘制肘部图：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Visualization
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化
- en: 'As a bonus point, let’s add the last (I promise, it’s the last:) helper method
    to draw all clusters on a 2D plane. I suppose most readers cannot visualize 300-dimensional
    vectors in their heads yet ;) so I will use [**t-SNE**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)
    (T-distributed Stochastic Neighbor Embedding) dimensionality reduction methods
    to reduce the number of dimensions to 2, and [Bokeh](https://github.com/bokeh/bokeh)
    to draw the results:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个额外的点，让我们添加最后一个（我保证这是最后一个:) 辅助方法，将所有簇绘制在2D平面上。我想大多数读者还不能在脑海中可视化300维的向量 ;)
    所以我将使用[**t-SNE**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)（T-分布随机邻域嵌入）降维方法将维度减少到2，并使用[Bokeh](https://github.com/bokeh/bokeh)绘制结果：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now that we are ready to see the results, let’s see what we can get.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好查看结果了，让我们看看可以得到什么。
- en: Results
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: I was using three different (TF-IDF, Word2Vec, and Sentence-BERT) algorithms
    to convert text into embedding vectors, which are seriously different in architecture.
    Will all of them be able to find interesting patterns in all tweets? Let’s examine
    the results.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了三种不同的（TF-IDF、Word2Vec和Sentence-BERT）算法将文本转换为嵌入向量，它们在架构上差异很大。它们是否都能在所有推文中找到有趣的模式？让我们检查一下结果。
- en: TF-IDF
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF
- en: The major disadvantage of finding clusters in TF-IDF embeddings is a large amount
    of data. In my case, the matrix size was 19,197x22,735, because the text corpus
    contains 19,197 tweets and 22,735 unique tokens. Finding clusters in a matrix
    of that size is not fast, even for a modern PC.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在TF-IDF嵌入中寻找簇的主要缺点是数据量大。在我的例子中，矩阵大小为19,197x22,735，因为文本语料库包含19,197条推文和22,735个唯一标记。即使对于现代PC，在如此大的矩阵中寻找簇也并不快。
- en: 'In general, TF-IDF vectorization did not provide exceptional results, but K-Means
    was still able to find some interesting clusters. For example, from all 19,197
    tweets, a 200-tweet cluster was detected in which people were making posts about
    the international online forum:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，TF-IDF向量化没有提供特别出色的结果，但K-Means仍然能够找到一些有趣的簇。例如，从所有19,197条推文中，检测到一个包含200条推文的簇，其中人们在发帖关于国际在线论坛：
- en: '![](../Images/266a97b343bca55b3db9973d04697057.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/266a97b343bca55b3db9973d04697057.png)'
- en: Text cluster word cloud, Image by author
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 文本簇词云，图片由作者提供
- en: 'K-Means was also able to find some users who made a lot of similar posts:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means也能够找到一些发布了大量类似帖子用户：
- en: '![](../Images/8281db3edb3aaa7a201b3c8a7d1760ee.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8281db3edb3aaa7a201b3c8a7d1760ee.png)'
- en: In this case, a user with the nickname “**mickel” was probably trying to promote
    his online book (by the way, showing the message ids is useful for debugging;
    we can always open the original tweet in the browser), and he published a lot
    of similar posts about that. Those posts were not absolutely similar, but the
    algorithm was able to cluster them together. This approach can be useful, for
    example, in detecting accounts used for posting spam.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，昵称为“**mickel**”的用户可能在尝试推广他的在线书籍（顺便说一句，显示消息ID对调试很有用；我们可以随时在浏览器中打开原始推文），并且他发布了很多类似的帖子。这些帖子并不完全相似，但算法能够将它们聚类在一起。这种方法在检测用于发布垃圾邮件的账户时可能会很有用。
- en: 'Some interesting clusters were found in TF-IDF vectors, but most other clusters
    had silhouette values around zero. The t-SNE visualization shows the same result.
    There are some local groups in the picture, but most of the points overlap each
    other:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在TF-IDF向量中发现了一些有趣的簇，但大多数其他簇的轮廓值接近零。t-SNE可视化显示了相同的结果。图中有一些局部群体，但大多数点相互重叠：
- en: '![](../Images/d6e642264f5bc55f9624d44619e2d2d6.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6e642264f5bc55f9624d44619e2d2d6.png)'
- en: t-SNE visualization of TF-IDF generated vectors, Image by author
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE对TF-IDF生成的向量的可视化，图片由作者提供
- en: I saw some articles where authors got good results with TF-IDF embeddings, mostly
    in cases where texts belong to different domains. For example, posts about “politics”,
    “sports,” and “religion” will likely form more isolated clusters with higher silhouette
    values. But in our case, *all texts* are about climate, so the task is more challenging.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我看到一些文章中，作者使用TF-IDF嵌入取得了良好的结果，主要是在文本属于不同领域的情况下。例如，关于“政治”、“体育”和“宗教”的帖子可能会形成更多隔离的聚类，并具有较高的轮廓系数值。但在我们的案例中，*所有文本*都关于气候，因此任务更具挑战性。
- en: Word2Vec
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2Vec
- en: 'The first interesting result with Word2Vec — the Elbow method was able to produce
    a somehow visible “elbow” point:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Word2Vec得到的第一个有趣的结果是——肘部方法能够产生一个相对明显的“肘部”点：
- en: '![](../Images/c052dcd44bbf81039c4aca3869d7eddb.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c052dcd44bbf81039c4aca3869d7eddb.png)'
- en: The Elbow graph for Word2Vec embeddings, Image by author
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec嵌入的肘部图，图片由作者提供
- en: 'With K=8, the t-SNE visualization gave this result:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用K=8，t-SNE可视化得到了这个结果：
- en: '![](../Images/09e8fe35dc827e6cc226656e734fd893.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09e8fe35dc827e6cc226656e734fd893.png)'
- en: t-SNE visualization of Word2Vec generated vectors, Image by author
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE对Word2Vec生成的向量的可视化，图片由作者提供
- en: Most of the clusters are still overlapping, and silhouette values are, in general,
    low. But some interesting patterns can be found.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数聚类仍然重叠，并且轮廓系数值普遍较低。但可以发现一些有趣的模式。
- en: '“Climate change”. This cluster has the most popular words “climate”, “change”,
    “action,” and “global”:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: “气候变化”。这个聚类包含了最流行的词汇“气候”、“变化”、“行动”和“全球”：
- en: '![](../Images/42d34cb7c577fdffff743888fd06267a.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42d34cb7c577fdffff743888fd06267a.png)'
- en: Lots of people are obviously worried about climate change, so having this cluster
    is obvious.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 很多人显然对气候变化感到担忧，所以有这个聚类是显而易见的。
- en: '“Fuels”. This cluster has popular words like “energy”, “carbon”, “emission”,
    “fossil,” or “solar”:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: “燃料”。这个聚类包含了诸如“能源”、“碳”、“排放”、“化石”或“太阳能”等流行词汇：
- en: '![](../Images/04f478f33187e7029d78baa2f8924982.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04f478f33187e7029d78baa2f8924982.png)'
- en: “Environment”. Here we can see such words as “temperature”, “ocean”, “sea”,
    “ice,” and so on.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: “环境”。这里我们可以看到如“温度”、“海洋”、“海”、“冰”等词汇。
- en: '![](../Images/6de5482e2bac864c11443e2869943503.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6de5482e2bac864c11443e2869943503.png)'
- en: Sentence-BERT
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sentence-BERT
- en: 'In theory, these embeddings should provide the most accurate results; let’s
    see how it is going. A t-SNE cluster visualization looks like this:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，这些嵌入应该提供最准确的结果；让我们看看实际情况如何。t-SNE的聚类可视化如下：
- en: '![](../Images/2aa929309a6a4b97bdcd74fdf6b61ac8.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2aa929309a6a4b97bdcd74fdf6b61ac8.png)'
- en: t-SNE visualization of Sentence-BERT generated vectors, Image by author
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE对Sentence-BERT生成的向量的可视化，图片由作者提供
- en: As we can see, many local clusters can be found, and I will show some of the
    most interesting of them.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，可以找到许多本地聚类，我将展示其中一些最有趣的聚类。
- en: '“Ice melting”. A cluster with the most popular words “climate”, “ice”, “melt”,
    “glacier”, and “arctic”:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: “冰川融化”。一个包含了最流行词汇“气候”、“冰”、“融化”、“冰川”和“北极”的聚类：
- en: '![](../Images/c823d89f4569700b3ba9ad04d659ce95.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c823d89f4569700b3ba9ad04d659ce95.png)'
- en: '“Earth day”. This day is celebrated in April when this data was collected,
    and there is a cluster of messages with words like “earth”, “day”, “planet”, “happy”,
    or “action”:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: “地球日”。这一天在数据收集时是四月庆祝的，存在一个包含“地球”、“日”、“星球”、“快乐”或“行动”等词汇的消息聚类：
- en: '![](../Images/aaeb33156448bfbe516669c75d6ffed5.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aaeb33156448bfbe516669c75d6ffed5.png)'
- en: '“Global international forum”:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: “全球国际论坛”：
- en: '![](../Images/7c62d588ea5266104bade1a1dce5dfbd.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c62d588ea5266104bade1a1dce5dfbd.png)'
- en: 'This result is interesting for two reasons. Firstly, we saw this cluster before;
    the K-Means algorithm found it in the TF-IDF embeddings. Secondly, the “Word2Vec”
    model did not have the word “thereisa” in the dictionary, so it was just skipped.
    BERT has a better tokenization scheme, in which the unknown words are split into
    smaller tokens. We can easily see how it works:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果有两个有趣的原因。首先，我们之前见过这个聚类；K-Means算法在TF-IDF嵌入中发现了它。其次，“Word2Vec”模型的词典中没有“thereisa”这个词，因此它被跳过了。BERT有更好的分词方案，其中未知词被拆分成更小的标记。我们可以很容易地看到它是如何工作的：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can see that the words “online” and “forum” were converted into single tokens,
    but the word “thereisa” was converted into two words “there” and “##isa”. This
    not only allows BERT to deal with unknown words, but it is actually much closer
    to what we, as humans, often do: when we see unknown words, we often try to “split”
    them into pieces and guess the meaning.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，“online”和“forum”这些词被转换成了单一的标记，但“thereisa”被分解成了两个词“there”和“##isa”。这不仅使
    BERT 能处理未知词汇，而且实际上更接近我们作为人类的做法：当我们遇到未知词汇时，我们常常试图“拆分”它们并猜测其含义。
- en: 'But let’s go further. Another distinct group is related to protests; we can
    see here such words as “protest, “change”, “action”, “activism“, and so on:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们进一步探讨。另一个明显的组与抗议有关；我们可以看到如“protest”，“change”，“action”，“activism”等词汇：
- en: '![](../Images/51c6bed573cf08e0a636ad784b0160a7.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51c6bed573cf08e0a636ad784b0160a7.png)'
- en: 'And last but not least, another popular climate-related topic is electric transport.
    Here we can see words like “new”, “electric”, “car”, or “emission”:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，另一个受欢迎的气候相关话题是电动交通。我们可以看到像“new”，“electric”，“car”或“emission”等词：
- en: '![](../Images/aafbec2fc90927cd2b35d7131c9bbdc9.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aafbec2fc90927cd2b35d7131c9bbdc9.png)'
- en: Conclusion
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Using text clustering, we were able to process raw and unstructured text from
    social networks (in our case, Twitter, but this approach should also work with
    other platforms) and find interesting and distinctive patterns in the posts of
    tens of thousands of users. This can be important not only for purely academic
    reasons like cultural anthropology or sociology studies but also for “pragmatic”
    cases like detecting bots or users posting spam.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 通过文本聚类，我们能够处理来自社交网络的原始和非结构化文本（在我们的案例中是 Twitter，但这种方法也适用于其他平台），并发现数万用户帖子的有趣和独特的模式。这不仅对于像文化人类学或社会学研究这样的纯学术原因重要，也对像检测机器人或发布垃圾信息的用户这样的“实用”案例至关重要。
- en: From a natural language processing perspective, clustering social media data
    is an interesting and challenging topic. It is challenging because there are many
    ways of cleaning and transforming the data, and no way would be perfect. In our
    case, BERT embeddings unsurprisingly provided the best results compared to earlier
    TF-IDF and Word2Vec models. BERT is not only giving good results, but it is also
    better for dealing with unknown words, which can be a problem with Word2Vec. TF-IDF
    embeddings, in my opinion, did not show any exceptional results, but this approach
    still has an advantage. TF-IDF is based on pure statistics, and it does not require
    a pre-trained language model. So, in the case of rare languages for which pre-trained
    models are not available, TF-IDF can be used.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 从自然语言处理的角度来看，社交媒体数据的聚类是一个有趣且富有挑战性的课题。它具有挑战性的原因在于数据清洗和转换的方法多种多样，没有一种方法是完美的。在我们的案例中，BERT
    嵌入毫无悬念地提供了比早期的 TF-IDF 和 Word2Vec 模型更好的结果。BERT 不仅能提供良好的结果，而且在处理未知词汇方面也表现得更好，而这可能是
    Word2Vec 的一个问题。就我而言，TF-IDF 嵌入并未展示出任何卓越的结果，但这种方法仍然有其优势。TF-IDF 基于纯粹的统计数据，不需要预训练的语言模型。因此，在那些没有可用预训练模型的稀有语言情况下，可以使用
    TF-IDF。
- en: This article provides a sort of “low-level” approach, which is better for understanding
    how things work, and I encourage readers to do some experiments on their own;
    the source code in the article should be enough for that. At the same time, those
    who just want to get results in 10 lines of code without thinking about what is
    “under the hood”, are welcome to try ready-to-use libraries like [BERTopic](https://github.com/MaartenGr/BERTopic).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了一种“低级别”的方法，这种方法更适合理解事物的工作原理，我鼓励读者自己做一些实验；文章中的源代码应该足够进行这些实验。同时，那些只想在 10
    行代码中获得结果而不想考虑“底层”实现的人，可以尝试像 [BERTopic](https://github.com/MaartenGr/BERTopic)
    这样的现成库。
- en: If you enjoyed this story, feel free [to subscribe](https://medium.com/@dmitryelj/membership)
    to Medium, and you will get notifications when my new articles will be published,
    as well as full access to thousands of stories from other authors.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这个故事，可以随时 [订阅](https://medium.com/@dmitryelj/membership) Medium，你将会在我的新文章发布时收到通知，并且可以全面访问其他作者的成千上万篇故事。
- en: Thanks for reading.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。
