- en: How 25,000 Computers Trained ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-25-000-computers-trained-chatgpt-11104686a24d](https://towardsdatascience.com/how-25-000-computers-trained-chatgpt-11104686a24d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/fb497373dd0aa1eee593aec71f7c2eb5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The breakthrough behind ChatGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@JerryQu?source=post_page-----11104686a24d--------------------------------)[![Jerry
    Qu](../Images/f36a4a13d44c97923fa2b4b7b1290e1b.png)](https://medium.com/@JerryQu?source=post_page-----11104686a24d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----11104686a24d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----11104686a24d--------------------------------)
    [Jerry Qu](https://medium.com/@JerryQu?source=post_page-----11104686a24d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----11104686a24d--------------------------------)
    ·5 min read·Aug 29, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/017490466c0e04b2cbad388da6184527.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: What word comes after Good?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might think **Good Morning**, or **Good Bye**. But you definitely wouldn’t
    say ***Good Loud.*** That just doesn’t make sense. For decades, computer scientists
    have been training AI to solve this exact problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7dc5b0ba4db5b44c14f7e911d0c78e4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Given a word, our AI predicts the next word. Do this several times, & you’ve
    generated a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: This is how ChatGPT works.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Trained over the entire internet, ChatGPT has learned how to chat like a human.
    However, this immense feat was only made possible by a breakthrough in the late
    2010s. A breakthrough underpinning ChatGPT & forever shaping the world we live
    in.
  prefs: []
  type: TYPE_NORMAL
- en: This is the story of an AI that read & learned from every book, tweet, & website
    across the entire internet. And how it was made possible.
  prefs: []
  type: TYPE_NORMAL
- en: Sentences are long.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we move beyond a single word, next word prediction is a lot harder. Take
    this example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95206d3ccd94b8360832e3a957bbc553.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In this context, it makes no sense to say **I ate a good *Morning***. But our
    AI only looks at good, and spits out morning. In most cases even humans need many
    words to predict the next word. So an AI needs this extra information as well.
  prefs: []
  type: TYPE_NORMAL
- en: Our AI needs to read many words to predict the next word. ChatGPT can read more
    than [8**,000 previous words**](https://platform.openai.com/docs/models/gpt-4)
    at once. The natural way to do this would be to feed each word into the AI, one
    by one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/360c316240c0e46b9028be0312978d1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This is how AIs worked in the past. A **Recurrent Neural Network (RNN)** would
    take one word at a time, storing information as it reads a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: One of the problems with this AI is that it’s **incredibly slow**. Each word
    has to wait for the last, which is a problem at large scales. Imagine if your
    washing machine could only wash one shirt at a time. This **sequential** process
    would take days. But everyone knows we can throw in all our shirts at the same
    time, finishing in minutes. This is the idea of **parallelism.** By performing
    work in parallel instead of sequentially, we can dramatically speed up washing
    machines, computers, & AI.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs could only be trained on millions of words, nowhere near the trillions
    across the internet. We needed a faster, more efficient way of reading sentences.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer was the solution.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2017, a paper titled [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
    was published. This paper effectively turned sentences on their side. These researchers
    invented an AI that can read a whole sentence at once.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57482a64785dad9087764a67ed93030e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This new AI is called a **Transformer**, and its efficiency allowed it to learn
    from every book & website on the internet. To understand how it does this, we
    need to take a step back & understand how computers read text.
  prefs: []
  type: TYPE_NORMAL
- en: How can an AI *read* text?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computers work in 1s & 0s. Called Binary, these 1s & 0s make up numbers. Computer
    scientists needed a way to represent words as numbers. And this was made possible
    in 2013, when scientists at Google created [word2vec](https://arxiv.org/pdf/1301.3781.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Words contain semantic meaning. Dogs are related to cats. Kings are related
    to queens. Word2vec was able to represent these semantics in vectors or lists
    of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: With word2vec, you could take King, subtract Man, add Woman, and get the word
    Queen.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ad19072bc348084f2cb45c6c4cabb10.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This vector of numbers is called a **Word Embedding**. They embed the word’s
    meaning into this vector. When training an AI to process text, we actually feed
    it these word embeddings. The AI does some math, transforming these vectors, and
    spits out the next word. Transforming these vectors is what takes a lot of time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d522677b058694cb0e63b6d615679c19.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer does this all in parallel.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of waiting for the previous word to process, we transform all these
    word embeddings at the same time, performing an averaging to put them all together.
    This reduces the number of sequential operations from the length of the sentence,
    to a constant number.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c38eade80bd1eb798eba17d4f0092e89.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Lambda Labs [estimated](https://lambdalabs.com/blog/demystifying-gpt-3) that
    training ChatGPT on a single GPU would take **355 years.** But by exploiting its
    parallelism, ChatGPT was trained across **25,000 GPUs** & finished in a matter
    of **days**.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer sparked a paradigm shift in AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With increased parallelism, larger and larger AIs could be trained. While the
    largest sequential models in the past were trained on millions of words, ChatGPT
    was trained on nearly a **trillion**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a80c42f32564f938ca3dd7df397841e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author, Data from original papers
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT was trained on CommonCrawl, a collection of the entire internet since
    2008\. Using **over 25,000 computers**, this model has read & learned from every
    website over the entire internet. Imagine reading every book, tweet, & piece of
    code ever published.
  prefs: []
  type: TYPE_NORMAL
- en: Today, ChatGPT is being used to write code, [generate TV commercials](https://openai.com/customer-stories/waymark),
    & assist you with almost anything you can imagine! By turning sentences on their
    side, we’ve created a new era in AI, one that pushes the boundaries of what was
    once thought possible.
  prefs: []
  type: TYPE_NORMAL
- en: But we may have reached a limit.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the release of GPT-4, Sam Altman, OpenAI’s CEO said,
  prefs: []
  type: TYPE_NORMAL
- en: “I think we’re at the end of the era where it’s going to be these, like, giant,
    giant models…”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: After learning from the entire internet, what comes next? The impact of ChatGPT
    is trickling down into every industry. But like any breakthrough, progress plateaus.
    And AI’s next inflection point, only time will tell.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54adf62cdc3227169027831f8dd9b06c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author, data from original papers
  prefs: []
  type: TYPE_NORMAL
- en: '**If you enjoyed this article:**'
  prefs: []
  type: TYPE_NORMAL
- en: follow my Medium, [LinkedIn](https://www.linkedin.com/in/jerry-qu/), and [Twitter](https://twitter.com/JerryQu2)
    to stay updated with my progress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
