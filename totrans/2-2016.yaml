- en: Build a Segmentation Model with One Line of Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-essential-library-to-build-segmentation-models-6e17e81338e](https://towardsdatascience.com/the-essential-library-to-build-segmentation-models-6e17e81338e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Build and train a neural network model for image segmentation in the fastest
    way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mattiagatti.medium.com/?source=post_page-----6e17e81338e--------------------------------)[![Mattia
    Gatti](../Images/9d5aeb356ff01ed9e4ead66c18994595.png)](https://mattiagatti.medium.com/?source=post_page-----6e17e81338e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6e17e81338e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6e17e81338e--------------------------------)
    [Mattia Gatti](https://mattiagatti.medium.com/?source=post_page-----6e17e81338e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e17e81338e--------------------------------)
    ·6 min read·Mar 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad811a836b54548872ee8eafd0abbba0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[MartinThoma](https://commons.wikimedia.org/wiki/File:Image-segmentation-example-segmented.png),
    CC0, via Wikimedia Commons (edited)'
  prefs: []
  type: TYPE_NORMAL
- en: Neural network models have proven to be highly effective in solving segmentation
    problems, achieving state-of-the-art accuracy. They have led to significant improvements
    in various applications, including medical image analysis, autonomous driving,
    robotics, satellite imagery, video surveillance, and much more. However, building
    these models usually takes a long time, but after reading this guide you will
    be able to build one with just a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Table of content
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building blocks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Segmentation is the task of dividing an image into multiple segments or regions
    based on certain characteristics or properties. A segmentation model takes an
    image as input and returns a segmentation mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18664e6573f75bd5b860c3694ed659e2.png)'
  prefs: []
  type: TYPE_IMG
- en: (Left) An input image | (Right) Its segmentation mask. Both images by [PyTorch](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Segmentation neural network models consist of two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'An **encoder**: takes an input image and extracts features. Examples of encoders
    are ResNet, EfficentNet, and ViT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A **decoder**: takes the extracted features and generates a segmentation mask.
    The decoder varies on the architecture. Examples of architectures are U-Net, FPN,
    and DeepLab.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, when building a segmentation model for a specific application, you need
    to choose an architecture and an encoder. However, it is difficult to choose the
    best combination without testing several. This usually takes a long time because
    changing the model requires writing a lot of boilerplate code. The [Segmentation
    Models](https://github.com/qubvel/segmentation_models.pytorch) library solves
    this problem. It allows you to create a model in a single line by specifying the
    architecture and the encoder. Then you only need to modify that line to change
    either of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the latest version of Segmentation Models from PyPI use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Building Blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The library provides a class for most segmentation architectures and each of
    them can be used with any of the available encoders. In the next section, you
    will see that to build a model you need to instantiate the class of the chosen
    architecture and pass the string of the chosen encoder as a parameter. The figure
    below shows the class name of each architecture provided by the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b49a27e0de58532dca66f45a639efa55.png)'
  prefs: []
  type: TYPE_IMG
- en: Class names of all the architectures provided by the library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The figure below shows the names of the most common encoders provided by the
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1756893e86b8006ecc1b313fda9e6bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Names of the most common encoders provided by the library.
  prefs: []
  type: TYPE_NORMAL
- en: There are over 400 encoders, thus it’s not possible to show them all, but you
    can find a comprehensive list [here](https://github.com/qubvel/segmentation_models.pytorch#encoders).
  prefs: []
  type: TYPE_NORMAL
- en: Build a Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the architecture and the encoder have been chosen from the figures above,
    building the model is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`encoder_name` is the name of the chosen encoder (e.g. resnet50, efficentnet-b7,
    mit_b5).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_weights` is the dataset of the pre-trained. If `encoder_weights` is
    equal to `"imagenet"` the encoder weights are initialized by using the ImageNet
    pre-trained. All the encoders have at least one pre-trained and a comprehensive
    list is available [here](https://github.com/qubvel/segmentation_models.pytorch#encoders).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`in_channels` is the channel count of the input image (3 if RGB).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even if `in_channels` is not 3 an ImageNet pre-trained can be used: the first
    layer will be initialized by reusing the weights from the pre-trained first convolutional
    layer (the procedure is described [here](https://github.com/qubvel/segmentation_models.pytorch#input-channels)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`out_classes` is the number of classes in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation` is the activation function for the output layer. The possible
    choices are `None` (default), `sigmoid` and `softmax` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note:** when using a loss function that expects logits as input, the activation
    function must be None.For example, when using the `CrossEntropyLoss` function,
    `activation` must be `None` .'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Train the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section shows all the code required to perform training. However, this
    library doesn’t change the usual pipeline for training and validating a model.
    To simplify the process, the library provides the implementation of many loss
    functions such as *Jaccard Loss, Dice Loss, Dice Cross-Entropy Loss, Focal Loss,*
    and metrics such as *Accuracy, Precision, Recall, F1Score, and IOUScore*. For
    a complete list of them and their parameters, check their documentation in the
    [Losses](https://smp.readthedocs.io/en/latest/losses.html) and [Metrics](https://smp.readthedocs.io/en/latest/metrics.html)
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The proposed training example is a binary segmentation using the [Oxford-IIIT
    Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) (it will be downloaded
    by code). These are two samples from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3778e77e2af3b35453e7c23c7171992.png)'
  prefs: []
  type: TYPE_IMG
- en: A sample of a cat from the Oxford-IIIT Pet Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9bc5248992644cf2eca7c0358331308e.png)'
  prefs: []
  type: TYPE_IMG
- en: A sample of a dog from the Oxford-IIIT Pet Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, these are all steps to perform this type of segmentation task:'
  prefs: []
  type: TYPE_NORMAL
- en: Build the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the activation function of the last layer depending on the loss function
    you are going to use.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Define the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that when using a pre-trained, the input should be normalized by using
    the mean and standard deviation of the data used to train the pre-trained.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Define the train function.
  prefs: []
  type: TYPE_NORMAL
- en: Nothing changes here from the train function you would have written to train
    a model without using the library.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Define the validation function.
  prefs: []
  type: TYPE_NORMAL
- en: True positives, false positives, false negatives and true negatives from batches
    are all summed together to calculate metrics only at the end of batches. Note
    that logits must be converted to classes before metrics can be calculated. Call
    the train function to start training.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Use the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are some segmentations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4eb0f727a6a5f922ae61e421c080d9f0.png)![](../Images/54b4f642093a462f5e98794c8d56f3f7.png)![](../Images/6d7a007d8d96edcf33c0b1fc4abcf432.png)'
  prefs: []
  type: TYPE_IMG
- en: Concluding remarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This library has everything you need to experiment with segmentation. It’s very
    easy to build a model and apply changes, and most loss functions and metrics are
    provided. In addition, using this library doesn’t change the pipeline we’re used
    to. See the [official documentation](https://smp.readthedocs.io/en/latest/) for
    more information. I have also included some of the most common encoders and architectures
    in the references.
  prefs: []
  type: TYPE_NORMAL
- en: '[The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)
    is available to download for commercial/research purposes under a [Creative Commons
    Attribution-ShareAlike 4.0 International License.](https://creativecommons.org/licenses/by-sa/4.0/)
    The copyright remains with the original owners of the images.'
  prefs: []
  type: TYPE_NORMAL
- en: All images, unless otherwise noted, are by the Author. Thanks for reading, I
    hope you have found this useful.
  prefs: []
  type: TYPE_NORMAL
- en: '[1] O. Ronneberger, P. Fischer and T. Brox, [U-Net: Convolutional Networks
    for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) (2015)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Z. Zhou, Md. M. R. Siddiquee, N. Tajbakhsh and J. Liang, [UNet++: A Nested
    U-Net Architecture for Medical Image Segmentation](https://arxiv.org/abs/1807.10165)
    (2018)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] L. Chen, G. Papandreou, F. Schroff, H. Adam, [Rethinking Atrous Convolution
    for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587) (2017)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] L. Chen, Y. Zhu, G. Papandreou, F. Schroff, H. Adam, [Encoder-Decoder with
    Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1802.02611)
    (2018)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] R. Li, S. Zheng, C. Duan, C. Zhang, J. Su, P.M. Atkinson, [Multi-Attention-Network
    for Semantic Segmentation of Fine Resolution Remote Sensing Images](https://arxiv.org/abs/2009.02130)
    (2020)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] A. Chaurasia, E. Culurciello, [LinkNet: Exploiting Encoder Representations
    for Efficient Semantic Segmentation](https://arxiv.org/abs/1707.03718) (2017)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] T. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, S. Belongie, [Feature
    Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144) (2017)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] H. Zhao, J. Shi, X. Qi, X. Wang, J. Jia, [Pyramid Scene Parsing Network](https://arxiv.org/abs/1612.01105)
    (2016)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] H. Li, P. Xiong, J. An, L. Wang, [Pyramid Attention Network for Semantic
    Segmentation](https://arxiv.org/abs/1805.10180) (2018)'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] K. Simonyan, A. Zisserman, [Very Deep Convolutional Networks for Large-Scale
    Image Recognition](https://arxiv.org/abs/1409.1556) (2014)'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, [Deep Residual Learning
    for Image Recognition](https://arxiv.org/abs/1512.03385) (2015)'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] S. Xie, R. Girshick, P. Dollár, Z. Tu, K. He, [Aggregated Residual Transformations
    for Deep Neural Networks](https://arxiv.org/abs/1611.05431) (2016)'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] J. Hu, L. Shen, S. Albanie, G. Sun, E. Wu, [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)
    (2017)'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] G. Huang, Z. Liu, L. van der Maaten, K. Q. Weinberger, [Densely Connected
    Convolutional Networks](https://arxiv.org/abs/1608.06993) (2016)'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] M. Tan, Q. V. Le, [EfficientNet: Rethinking Model Scaling for Convolutional
    Neural Networks](https://arxiv.org/abs/1905.11946) (2019)'
  prefs: []
  type: TYPE_NORMAL
- en: '[16] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, P. Luo, [SegFormer:
    Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203)
    (2021)'
  prefs: []
  type: TYPE_NORMAL
