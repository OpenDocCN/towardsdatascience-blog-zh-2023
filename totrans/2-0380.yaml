- en: 'Beyond LLaMA: The Power of Open LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越 LLaMA：开源 LLMs 的力量
- en: 原文：[https://towardsdatascience.com/beyond-llama-the-power-of-open-llms-cef807a54a4f](https://towardsdatascience.com/beyond-llama-the-power-of-open-llms-cef807a54a4f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/beyond-llama-the-power-of-open-llms-cef807a54a4f](https://towardsdatascience.com/beyond-llama-the-power-of-open-llms-cef807a54a4f)
- en: How LLaMA is making open-source cool again
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLaMA 如何让开源再次变得酷炫
- en: '[](https://wolfecameron.medium.com/?source=post_page-----cef807a54a4f--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----cef807a54a4f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cef807a54a4f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cef807a54a4f--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----cef807a54a4f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----cef807a54a4f--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----cef807a54a4f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cef807a54a4f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cef807a54a4f--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----cef807a54a4f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cef807a54a4f--------------------------------)
    ·18 min read·Jul 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cef807a54a4f--------------------------------)
    ·18 min read·2023年7月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/29a535e5c06be6b564ccfb3b42238ab5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29a535e5c06be6b564ccfb3b42238ab5.png)'
- en: (Photo by [Paz Arando](https://unsplash.com/@pazarando?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/llama?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由 [Paz Arando](https://unsplash.com/@pazarando?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/s/photos/llama?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: Despite recent advances in large language models (LLMs), many of the most powerful
    models are only accessible via [paid APIs](https://console.anthropic.com/docs/api)
    and trained using large amounts of [proprietary data](https://openai.com/research/gpt-4),
    thus limiting the research community from accessing or reproducing such models.
    This trend raises serious concerns about whether LLMs will be mostly controlled
    by a small number of centralized groups that force others to pay for interaction
    with these models. Such a scenario strictly prevents most researchers from directly
    accessing or improving LLMs on their own.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）近期取得了进展，但许多最强大的模型仍然只能通过 [付费 API](https://console.anthropic.com/docs/api)
    访问，并且使用大量的 [专有数据](https://openai.com/research/gpt-4) 进行训练，从而限制了研究社区对这些模型的访问或复制。这一趋势引发了严重的担忧，即
    LLMs 是否将主要由少数几个集中化的组织控制，这些组织迫使他人支付费用以与这些模型互动。这种情况严格阻止了大多数研究人员直接访问或自行改进 LLMs。
- en: “[Many] LLMs require huge computational resources to train, and oftentimes use
    large and proprietary datasets. This suggests that in the future, highly capable
    LLMs will be largely controlled by a small number of organizations.” *— from [5]*
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “[许多] LLMs 需要巨大的计算资源进行训练，而且通常使用大型且专有的数据集。这表明未来，高能力的 LLMs 将主要由少数几个组织控制。” *— 摘自
    [5]*
- en: Given the computational burden of training and hosting LLMs, we might wonder
    whether open-sourcing these models is even helpful for the research community.
    If we are not part of a massive organization with extensive compute resources,
    *can we even do useful research with LLMs?* If not,maybe we are doomed to a world
    of centralized control of and access to LLMs. These models seem to have too much
    “gravity” (i.e., require access to tons of data and compute) for most people to
    easily work with them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于训练和托管大型语言模型（LLMs）的计算负担，我们可能会质疑开源这些模型对研究社区是否真的有帮助。如果我们不是拥有大量计算资源的大型组织的一部分，*我们甚至能用LLMs进行有意义的研究吗？*
    如果不能，也许我们注定要面对一个中央控制和访问LLMs的世界。这些模型似乎具有过强的“引力”（即需要大量的数据和计算资源），让大多数人很难轻松使用它们。
- en: The proposal of [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)
    (and subsequent leak to the public) moves in an opposite direction by open-sourcing
    a suite of powerful (but smaller) LLMs. Following the release of LLaMA to a public,
    we saw a massive wave of open research on LLMs. Such research produced a variety
    of different models, some of which were of comparable quality to ChatGPT. Most
    notably, however, these models were produced at minimal cost (i.e., <$500 in most
    cases) and with modest compute resources (i.e., some of these models can be run
    on a normal macbook!). Here, we will survey some of these post-LLaMA models that
    have been recently proposed and explore how open-source research on LLMs has made
    the topic more accessible.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) 的提议（以及随后向公众泄露）通过开源一套强大的（但较小的）LLM，走向了相反的方向。在
    LLaMA 向公众发布之后，我们见证了一波大规模的 LLM 开放研究。这些研究产生了各种不同的模型，其中一些与 ChatGPT 的质量相当。然而，最显著的是，这些模型的生产成本极低（即，大多数情况下低于
    $500）且计算资源 modest（即，部分模型可在普通 macbook 上运行！）。在这里，我们将调查一些最近提出的后 LLaMA 模型，并探索开源 LLM
    研究如何使这一主题变得更易接触。'
- en: '![](../Images/1ff7f48e43193f32d3ca43090b8120c4.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ff7f48e43193f32d3ca43090b8120c4.png)'
- en: (from [3, 4, 5])
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [3, 4, 5]）
- en: Core Concepts
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心概念
- en: In a [previous post](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone),
    we learned all about LLaMA, a suite of open-source, high-performing LLMs with
    a variety of sizes. LLaMA models are trained only on public data, making them
    compatible with open-source and reproducible without access to proprietary data.
    But, the story of LLaMA doesn’t end here! These models have recently become a
    hot topic in deep learning. In this overview, we are going to examine the research
    enabled by LLaMA and gain an understanding of why/how these models became popular.
    First, we will provide a little more background on LLaMA, followed by an overview
    of important ideas to understand for this overview.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [之前的一篇文章](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)中，我们了解了
    LLaMA，这是一套开源的高性能 LLM，具有多种规模。LLaMA 模型仅在公共数据上进行训练，使其与开源兼容，并且无需访问专有数据即可重复生成。然而，LLaMA
    的故事并未止步于此！这些模型最近已成为深度学习的热门话题。在本概述中，我们将探讨 LLaMA 使研究得以进行的原因，并了解这些模型为何以及如何变得流行。首先，我们将提供更多有关
    LLaMA 的背景信息，然后概述本概述所需理解的重要思想。
- en: How LLaMA was (or wasn’t?) open-sourced…
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLaMA 是如何（或未能）开源的……
- en: The deep learning community has embraced open-source for quite some time, and
    certain areas of research still do (e.g., see [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)).
    However, the LLM landscape is quite different, as the most popular/powerful models
    are only available behind paid APIs (e.g., [GPT-4](https://openai.com/research/gpt-4)
    [6], [Claude](https://www.anthropic.com/index/introducing-claude), and [Cohere](https://cohere.ai/)).
    The open-sourcing of [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)
    [1], a suite of smaller LLM foundation models of impressive quality, went against
    this trend. However, LLaMA wasn’t *exactly* open-sourced… the story is a bit more
    complicated.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习社区已经接受了开源一段时间，某些研究领域仍然如此（例如，请参见 [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)）。然而，LLM
    领域却大相径庭，因为最受欢迎/强大的模型仅通过付费 API 提供（例如，[GPT-4](https://openai.com/research/gpt-4)
    [6]、[Claude](https://www.anthropic.com/index/introducing-claude) 和 [Cohere](https://cohere.ai/)）。[LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)
    [1] 的开源，即一套质量卓越的较小 LLM 基础模型，打破了这一趋势。然而，LLaMA 并没有 *完全* 开源……故事要复杂一些。
- en: First, LLaMA was [announced by Meta](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    with full details, including an in-depth, helpful [publication](https://arxiv.org/abs/2302.13971),
    a form to apply for access to LLaMA, and a [simple repo](https://github.com/facebookresearch/llama)
    to run inference and tokenization with LLaMA after gaining model access. To gain
    access to the model, one had to agree to a laundry list of requirements, such
    as not using LLaMA commercially and ensuring any [derivative models](https://www.law.cornell.edu/wex/derivative_work)
    created with LLaMA follow the same license. But, all of this went out the window
    when, about a week after this release, weights for all LLaMA models were posted
    publicly to 4chan for anyone to download.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，LLaMA 被 [Meta](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    公布，详细信息包括深入、有用的 [出版物](https://arxiv.org/abs/2302.13971)、申请访问 LLaMA 的表单以及一个 [简单的仓库](https://github.com/facebookresearch/llama)，在获得模型访问权限后可用于运行推理和标记化。为了获得模型访问权限，必须同意一系列要求，例如不将
    LLaMA 用于商业目的，并确保用 LLaMA 创建的任何 [衍生模型](https://www.law.cornell.edu/wex/derivative_work)
    遵循相同的许可证。但这些要求都被抛到一边了，因为在发布大约一周后，所有 LLaMA 模型的权重被公开发布到 4chan，任何人都可以下载。
- en: Although the sharing of LLaMA was unexpected (and arguably harmful), it sparked
    thousands of downloads and has since enabled a massive amount of open research.
    Given that LLaMA is comprised of smaller models that are more accessible to researchers
    without extensive compute resources, these models are a perfect fit for this scenario.
    A massive number of incredible deep learning researchers went to work and produced
    a variety of projects powered by LLaMA within weeks, ranging from hosting multi-billion-parameter
    LLMs on a Macbook to reproducing ChatGPT for <$500.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LLaMA 的共享方式出乎意料（且可以说是有害的），但它引发了数千次下载，并且随后促进了大量的开放研究。鉴于 LLaMA 由更小的模型组成，这些模型对于没有大量计算资源的研究人员来说更为可及，这些模型非常适合这种情况。在几周内，大量令人惊叹的深度学习研究人员投入工作，利用
    LLaMA 开展了各种项目，从在 Macbook 上托管多亿参数的 LLM 到用不到 $500 复现 ChatGPT。
- en: Instruction Fine-Tuning
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指令微调
- en: '![](../Images/50575f03a407ba26facb633a46930cb1.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50575f03a407ba26facb633a46930cb1.png)'
- en: (from [10])
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [10]）
- en: Many of the models we will see within this overview are based upon the idea
    of instruction fine-tuning (or instruction tuning for short). Originally proposed
    by [FLAN](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html)
    [10], instruction fine-tuning is a form of training that makes language models
    better at solving language-based tasks in general, rather than just a single task;
    see above. In practice, this is done by fine-tuning a language model over sets
    of “instructions” that include fine-tuning examples combined with a description
    of the task being solved. Using this approach, we can fine-tune a language model
    to solve a variety of different tasks via textual prompting using different task
    templates; see below.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本概述中看到的许多模型都是基于指令微调（或简称为指令调优）的思想。指令微调最初由 [FLAN](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html)
    [10] 提出，它是一种训练形式，使语言模型在解决语言相关任务方面表现更好，而不仅仅是单一任务；见上文。实际上，这通过在一组“指令”上对语言模型进行微调来实现，这些指令包括与任务描述结合的微调示例。通过这种方法，我们可以通过使用不同的任务模板进行文本提示，微调语言模型以解决各种不同的任务；见下文。
- en: '![](../Images/aa0f322058f03a91778bb865e07a5b26.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa0f322058f03a91778bb865e07a5b26.png)'
- en: (from [10])
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [10]）
- en: Currently, one of the most popular variants of instruction fine-tuning is to
    fine-tune LLMs over examples of dialogue sessions, either from humans or generated
    by a chatbot. Given that many recent chatbots are specialized to follow instructions
    and perform information-seeking dialogue, these models, their output, and even
    the data used to train them contains a rich variety of instruction-following examples
    and behavior that can be directly leveraged for instruction tuning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，指令微调的一个最受欢迎的变体是通过对话示例对 LLM 进行微调，这些示例可以来自人类或由聊天机器人生成。鉴于许多最近的聊天机器人专门用于遵循指令并执行信息寻求对话，这些模型、它们的输出，甚至用于训练它们的数据都包含丰富的指令跟随示例和行为，可以直接用于指令微调。
- en: '![](../Images/d0dc117502e665675429d206d841bd70.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0dc117502e665675429d206d841bd70.png)'
- en: (from [2])
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: '**Self-instruct.** One form of instruction tuning that is relevant to this
    work is the self-instruct framework [2], which mitigates dependence on human-written
    instructions by generating instruction for fine-tuning with an LLM. In particular,
    this process starts with a small set of instruction data and iteratively *i)*
    uses an LLM to generate new data and *ii)* filters low quality data; see above.
    Such a technique produces high quality data for instruction tuning with minimal
    human annotation efforts.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**自我指导。** 与本工作相关的一种指令调整形式是自我指导框架[2]，它通过生成用于微调的指令来减少对人工编写指令的依赖。特别地，这一过程从一小部分指令数据开始，并迭代地*（i）*
    使用 LLM 生成新数据和*（ii）* 过滤低质量数据；见上文。这种技术能以最少的人力注释工作生成高质量的指令调整数据。'
- en: Knowledge Distillation
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识蒸馏
- en: '![](../Images/ec27e60e72e420ceba7fd768b6ffd393.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec27e60e72e420ceba7fd768b6ffd393.png)'
- en: (from [12])
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [12]）
- en: Proposed in [11], knowledge distillation uses a (large) fully-trained neural
    network as a training signal for another (small) neural network; see above. Many
    different types of knowledge distillation exist, but the idea behind them remains
    the same. Namely, if we train a neural network using both *i)* the normal training
    data and *ii)* the output of a larger, more powerful neural network over that
    data, then we will typically arrive at a better result than training a neural
    network over the data alone. By using its output as a training target, we can
    distill some of the information from a larger network into the smaller “student”
    network that is being trained. For more information on knowledge distillation
    and its many variants, check out the link [here](https://arxiv.org/abs/2006.05525).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在[11]中提出，知识蒸馏使用一个（大型）完全训练好的神经网络作为另一个（小型）神经网络的训练信号；见上文。虽然存在许多不同类型的知识蒸馏，但其背后的理念保持不变。即，如果我们使用*（i）*
    普通训练数据和*（ii）* 一个更大、更强大的神经网络对这些数据的输出来训练一个神经网络，那么通常会比仅使用数据来训练神经网络得到更好的结果。通过将其输出作为训练目标，我们可以将一些信息从更大的网络“蒸馏”到正在训练的小型“学生”网络中。有关知识蒸馏及其众多变体的更多信息，请查看[这里](https://arxiv.org/abs/2006.05525)的链接。
- en: Other Stuff…
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他内容……
- en: Beyond the information covered above, we will also need to have a baseline understanding
    of LLMs and how they work. To develop this understanding, check out the following
    resources.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了以上涵盖的信息，我们还需要对大型语言模型（LLMs）及其工作原理有一个基础的理解。要了解这些知识，请查看以下资源。
- en: Language Modeling Definition [[link](https://cameronrwolfe.substack.com/i/85568430/language-modeling)]
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模定义 [[链接](https://cameronrwolfe.substack.com/i/85568430/language-modeling)]
- en: Brief Overview of Language Modeling [[link](https://cameronrwolfe.substack.com/i/91134599/a-primer-on-language-modeling)]
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模简要概述 [[链接](https://cameronrwolfe.substack.com/i/91134599/a-primer-on-language-modeling)]
- en: Decoder-only Transformers [[link](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)]
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅解码器变换器 [[链接](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)]
- en: How LLMs Work [[link](https://twitter.com/cwolferesearch/status/1635693551584522256?s=20)]
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 如何工作 [[链接](https://twitter.com/cwolferesearch/status/1635693551584522256?s=20)]
- en: LLM Scaling Laws [[link](https://cameronrwolfe.substack.com/i/88082618/scaling-laws-for-neural-language-models)]
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 扩展定律 [[链接](https://cameronrwolfe.substack.com/i/88082618/scaling-laws-for-neural-language-models)]
- en: Self-attention in LLMs [[link](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20)]
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 中的自注意力 [[链接](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20)]
- en: Throughout the overview, we will also refer to the names of a few specific models
    within OpenAI’s catalog (e.g., `text-davinci-003`). See [here](https://platform.openai.com/docs/models)
    for a list of models (with associated descriptions) that are provided within the
    [OpenAI API](https://openai.com/blog/openai-api).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在概述中，我们还会提到 OpenAI 目录中的一些具体模型的名称（例如，`text-davinci-003`）。查看[这里](https://platform.openai.com/docs/models)可以找到提供的模型列表（及其相关描述），这些模型包含在[OpenAI
    API](https://openai.com/blog/openai-api)中。
- en: '[Alpaca: An Instruction-following LLaMA model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
    [3]'
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[Alpaca: 一个指令跟随的 LLaMA 模型](https://crfm.stanford.edu/2023/03/13/alpaca.html)
    [3]'
- en: “Doing research on instruction-following models in academia has been difficult,
    as there is no easily accessible model that comes close in capabilities to closed-source
    models such as OpenAI’s text-davinci-003.” *— from [3]*
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在学术界研究指令跟随模型一直很困难，因为没有一个容易获得的模型在能力上接近于封闭源模型，如 OpenAI 的 text-davinci-003。” *—
    来源于 [3]*
- en: Alpaca [3] is a fine-tuned version of the LLaMA-7B [1] LLM that performs similarly
    to OpenAI’s `text-davinci-003` (i.e., [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5)).
    The fine-tuning process for Alpaca is based on self-instruct [2], in which instruction-following
    data is collected from a higher-performing LLM (i.e., `text-davinci-003`) and
    used for SFT. Put simply, Alpaca demonstrates that the quality of small, open-source
    LLMs in an [instruction-following context](https://openai.com/research/instruction-following)
    can be drastically improved via fine-tuning over high-quality data. Plus, the
    entire fine-tuning process of Alpaca costs only $600 (including both data collection
    and fine-tuning), making such instruction-following LLMs easy and cheap to replicate
    for research purposes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Alpaca [3] 是LLaMA-7B [1] LLM的一个微调版本，其性能类似于OpenAI的`text-davinci-003`（即，[GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5)）。Alpaca的微调过程基于self-instruct
    [2]，其中从表现更好的LLM（即`text-davinci-003`）收集指令跟随数据，并用于SFT。简而言之，Alpaca表明，在[指令跟随背景](https://openai.com/research/instruction-following)中，通过高质量数据的微调可以显著提高小型开源LLM的质量。此外，整个Alpaca的微调过程费用仅为$600（包括数据收集和微调），使得这种指令跟随LLM易于且便宜地复制用于研究目的。
- en: '![](../Images/c341e85bf1841b3daa8651580ea427e2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c341e85bf1841b3daa8651580ea427e2.png)'
- en: Creating the Alpaca LLM (from [3])
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Alpaca LLM（来自[3]）
- en: '**Method.** To create an instruction-following LLM via SFT, we need *i)* a
    high-quality, pretrained language model and *ii)* instruction-following data to
    use for SFT. Luckily, the recent release of LLaMA provides easily-accessible,
    pretrained language models. Gaining access to instruction-following data is a
    bit more nuanced, but one method of doing this is self-instruct [2]. At a high
    level, self-instruct [bootstraps](https://carpentries-incubator.github.io/machine-learning-novice-python/07-bootstrapping/index.html#:~:text=In%20statistics%20and%20machine%20learning,our%20resampled%20dataset%20multiple%20times.)
    LLM-generated output for further training. In the case of Alpaca, we use `text-davinci-003`
    to generate instruction-following data by:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法。** 要通过SFT创建一个指令跟随的LLM，我们需要 *i)* 一个高质量的预训练语言模型和 *ii)* 用于SFT的指令跟随数据。幸运的是，最近发布的LLaMA提供了易于访问的预训练语言模型。获得指令跟随数据要复杂一些，但一种方法是使用self-instruct
    [2]。从高层次来看，self-instruct [bootstraps](https://carpentries-incubator.github.io/machine-learning-novice-python/07-bootstrapping/index.html#:~:text=In%20statistics%20and%20machine%20learning,our%20resampled%20dataset%20multiple%20times.)
    LLM生成的输出进行进一步训练。在Alpaca的案例中，我们使用`text-davinci-003`通过以下方式生成指令跟随数据：'
- en: Beginning with 175 instruction and output pairs from [self-instruct’s seed set](https://github.com/yizhongw/self-instruct/blob/main/data/seed_tasks.jsonl).
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[self-instruct的种子集](https://github.com/yizhongw/self-instruct/blob/main/data/seed_tasks.jsonl)开始，使用175个指令和输出对。
- en: Prompting the LLM to generate more instructions using the seed set as in-context
    examples for few-shot learning.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示LLM生成更多指令，使用种子集作为上下文示例进行少量学习。
- en: Authors of [3] also adopt a few [tricks](https://github.com/tatsu-lab/stanford_alpaca#data-generation-process)
    (e.g., a modified prompt and more efficient decoding/generation procedure) to
    make the data generation process cheaper and more efficient compared to the original
    self-instruct [2]. Overall, generating instruction-following data via the OpenAI
    API cost <$500 for 52K instruction-following examples.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]的作者也采用了一些[技巧](https://github.com/tatsu-lab/stanford_alpaca#data-generation-process)（例如，修改过的提示和更高效的解码/生成过程），使数据生成过程比原始self-instruct
    [2]更便宜、更高效。总体而言，通过OpenAI API生成指令跟随数据的费用不到$500，用于52K个指令跟随示例。'
- en: The LLaMA-7B model is then fine-tuned over this data using a HuggingFace-based
    training framework. By using [fully sharded data parallel (FSDP)](https://engineering.fb.com/2021/07/15/open-source/fsdp/)
    and [mixed precision training](https://cameronrwolfe.substack.com/i/73746315/what-can-we-use-in-practice)
    techniques, the fine-tuning process was reduced to 3 hours on 8 A100 GPUs, which
    costs <$100\. The code/data used for creating Alpaca is [available online](https://github.com/tatsu-lab/stanford_alpaca).
    However, commercial use of Alpaca is prohibited because *i)* LLaMA (which Alpaca
    is based upon) has a non-commercial license and *ii)* OpenAI [prohibits](https://openai.com/policies/terms-of-use)
    the use of its models to train competing LLMs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-7B 模型随后使用基于 HuggingFace 的训练框架在这些数据上进行微调。通过使用[完全分片的数据并行 (FSDP)](https://engineering.fb.com/2021/07/15/open-source/fsdp/)和[混合精度训练](https://cameronrwolfe.substack.com/i/73746315/what-can-we-use-in-practice)技术，微调过程在
    8 个 A100 GPU 上缩短至 3 小时，成本低于 $100。用于创建 Alpaca 的代码/数据[在线获取](https://github.com/tatsu-lab/stanford_alpaca)。然而，Alpaca
    的商业使用被禁止，因为 *i)* LLaMA（Alpaca 基于的模型）具有非商业许可证，*ii)* OpenAI [禁止](https://openai.com/policies/terms-of-use)
    使用其模型来训练竞争的 LLM。
- en: '**Results.** Alpaca is evaluated on instructions from the evaluation set used
    for self-instruct (i.e., mostly tasks related to email, social media, and productivity)
    and open-domain instructions that are hand-written by the authors. On such tasks,
    Alpaca is found to perform similarly to `text-davinci-003` (i.e., performs best
    in 50% of the ~180 cases that were tested). Although this evaluation is clearly
    limited in scope, *the performance of Alpaca is still quite impressive* given
    that is is a much smaller model than GPT-3.5 and relatively easy to replicate.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果。** Alpaca 在用于 self-instruct 的评估集上的指令（即，大多与电子邮件、社交媒体和生产力相关的任务）和由作者手工编写的开放领域指令上进行评估。在这些任务中，Alpaca
    的表现类似于 `text-davinci-003`（即，在测试的约 180 个案例中，表现最佳的占 50%）。尽管这种评估显然范围有限，*考虑到 Alpaca
    比 GPT-3.5 小得多且相对容易复制，其性能仍然非常令人印象深刻*。'
- en: '![](../Images/015ce7332940a09d690659bf84f51ad2.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/015ce7332940a09d690659bf84f51ad2.png)'
- en: Example of Alpaca Output (from [3])
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Alpaca 输出示例（来自 [3]）
- en: Similar to `text-davinci-003`, Alpaca’s outputs are typically shorter than those
    of ChatGPT. In other words, the model’s style reflects that of the LLM used to
    generate the instruction-following data used for fine-tuning.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 `text-davinci-003`，Alpaca 的输出通常比 ChatGPT 的要短。换句话说，模型的风格反映了用于生成指令跟随数据的 LLM
    的风格。
- en: '[Vicuna: An Open-Source Chatbot with 90% ChatGPT Quality](https://vicuna.lmsys.org/)
    [4]'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[Vicuna: 一种具有 90% ChatGPT 质量的开源聊天机器人](https://vicuna.lmsys.org/) [4]'
- en: '![](../Images/3188874ef9429218920a033d90491feb.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3188874ef9429218920a033d90491feb.png)'
- en: (from [4])
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [4]）
- en: Information-seeking dialogue agents (or chatbots) like ChatGPT are great, but
    the training framework and architecture of such models is unknown, which hinders
    open-source research. As a solution, authors of [4] propose Vicuna, an open-source
    chatbot that is created by fine-tuning LLaMA — 13B [1] (i.e., a smaller LLM with
    comparable performance to [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)).
    The fine-tuning data for Vicuna is examples of user conversations with ChatGPT,
    and the entire fine-tuning process can be replicated for <$300, thus making chatbots
    more accessible for research purposes. Compared to Alpaca, Vicuna is more comparable
    to ChatGPT and generates answers with more detail and structure.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索对话代理（或聊天机器人）如 ChatGPT 很出色，但此类模型的训练框架和架构未知，这阻碍了开源研究。作为解决方案，[4] 的作者提出了 Vicuna，一种通过微调
    LLaMA — 13B [1]（即，一个与 [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)
    性能相当的小型 LLM）创建的开源聊天机器人。Vicuna 的微调数据是与 ChatGPT 进行的用户对话示例，整个微调过程可以以不到 $300 的成本复制，从而使聊天机器人在研究中更加可及。与
    Alpaca 相比，Vicuna 更加接近 ChatGPT，生成的答案更具细节和结构。
- en: '![](../Images/9447003b1bd44bc098455a04e550a90a.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9447003b1bd44bc098455a04e550a90a.png)'
- en: (from [4])
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [4]）
- en: '**Method.** Data used for SFT with Vicuna is downloaded via public APIs from
    [ShareGPT](https://sharegpt.com/), a platform that allows users to share their
    conversations with ChatGPT. Prior to fine-tuning, authors filter inappropriate
    and low-quality data, as well as divide longer conversations into shorter chunks
    that fit within LLaMA-13B’s maximum context length. In total, 70K conversations
    are collected. Similar to Alpaca, the model is trained on 8 A100 GPUs using FSDP
    (with a few modifications to reduce cost and handle long sequences), which takes
    about one day; see above. Authors make [the code](https://github.com/lm-sys/FastChat)
    for both training [and hosting](https://arxiv.org/abs/2302.11665) Vicuna publicly
    available. A more comprehensive comparison of Vicuna to the open-source LLMs LLaMA
    and Alpaca is provided within the table below. We will get into how Vicuna is
    evaluated next.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法。** 用于Vicuna的SFT数据通过[ShareGPT](https://sharegpt.com/)的公共API下载，该平台允许用户分享与ChatGPT的对话。在微调之前，作者会过滤不适当和低质量的数据，并将较长的对话分割成适合LLaMA-13B最大上下文长度的较短片段。总共收集了70K个对话。类似于Alpaca，该模型在8个A100
    GPU上使用FSDP（经过一些修改以降低成本和处理长序列）进行训练，约需一天时间；见上文。作者公开了[代码](https://github.com/lm-sys/FastChat)，用于训练[和托管](https://arxiv.org/abs/2302.11665)
    Vicuna。下表提供了Vicuna与开源LLM LLaMA和Alpaca的更全面的比较。我们将接下来讨论Vicuna的评估方法。'
- en: '![](../Images/5b07d579c68a01a3d5d5226d394822dd.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b07d579c68a01a3d5d5226d394822dd.png)'
- en: (from [4])
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [4]）
- en: '**results.** Accurately evaluating chatbots is quite difficult, and it becomes
    harder as chatbots improve in quality. For example, authors in [4] claim that
    the self-instruct evaluation set (used to evaluate Alpaca) is solved effectively
    by recent chatbots, which makes differences between models difficult to discern.
    Given the limitations of existing benchmarks and the difficulty of creating new,
    comprehensive evaluation sets, authors in [4] opt for a different strategy: *using
    LLMs to perform the evaluation*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果。** 准确评估聊天机器人非常困难，随着聊天机器人质量的提高，这种困难会加剧。例如，[4]的作者声称，自我指导评估集（用于评估Alpaca）已被近期聊天机器人有效解决，这使得模型之间的差异难以分辨。鉴于现有基准的局限性和创建新的全面评估集的难度，[4]的作者选择了另一种策略：*使用LLMs进行评估*。'
- en: “With recent advancements in GPT-4, we are curious whether its capabilities
    have reached a human-like level that could enable an automated evaluation framework
    for benchmark generation and performance assessments.” *— from [4]*
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “随着GPT-4的最新进展，我们很好奇其能力是否已经达到类似人类的水平，这种水平是否可以支持一个自动化的评估框架用于基准生成和性能评估。” *— 摘自
    [4]*
- en: At this point, we might be thinking that there’s no way this can actually work.
    *Chat-ception?* Surprisingly, however, forming an evaluation framework based upon
    the recently-proposed [GPT-4 model](https://openai.com/research/gpt-4) [6] works
    well. First, authors of [4] devised eight categories of questions (e.g., roleplay
    scenarios and math tasks). Then, GPT-4 is prompted to generate a diverse set of
    questions within each category. Interestingly, GPT-4 is found capable of generating
    difficult questions that recent chatbots struggle to answer.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可能会认为这实际上不可能奏效。*聊天自指？* 然而，令人惊讶的是，基于最近提出的[GPT-4模型](https://openai.com/research/gpt-4)
    [6]形成的评估框架效果良好。首先，[4]的作者设计了八类问题（例如，角色扮演场景和数学任务）。然后，GPT-4被提示在每个类别中生成多样化的问题。有趣的是，GPT-4被发现能够生成近期聊天机器人难以回答的难题。
- en: In particular, GPT-4 is used to generate ten questions within each category
    and the output of five different chatbots (.e., LLaMA-13B, Alpaca-13B, Vicuna-13B,
    Bard, and ChatGPT) is evaluated. Going further, the quality of each model’s output
    is judged by asking GPT-4 to rate the quality of an answer based on detail, helpfulness,
    relevance, and accuracy. Although performing evaluation in this manner might seem
    like a stretch, GPT-4 ranks models pretty consistently an even [explains its reasoning](https://vicuna.lmsys.org/eval/).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，GPT-4用于在每个类别中生成十个问题，并评估五种不同聊天机器人的输出（即LLaMA-13B、Alpaca-13B、Vicuna-13B、Bard和ChatGPT）。进一步说，每个模型输出的质量通过要求GPT-4根据详细程度、帮助性、相关性和准确性对答案质量进行评分来判断。虽然以这种方式进行评估可能看起来有些牵强，但GPT-4对模型的排名相当一致，甚至[解释了其推理过程](https://vicuna.lmsys.org/eval/)。
- en: '![](../Images/a90950f6ac46cd081287c4c9f6df90fc.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a90950f6ac46cd081287c4c9f6df90fc.png)'
- en: (from [4])
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [4]）
- en: As judged by GPT-4, Vicuna’s produces output with 92% quality relative to ChatGPT;
    see above. This ratio is achieved by asking GPT-4 to assign a score to each of
    the models’ outputs. Then, relative performance between models can be assessed
    by computing their total quality score over all questions. Although this evaluation
    approach is not rigorous, it’s pretty interesting, relatively consistent, and
    forces us to think about interesting ways in which the LLM landscape will evolve
    moving forward.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 GPT-4 的判断，Vicuna 的输出质量相对于 ChatGPT 为 92%；见上文。这个比例是通过让 GPT-4 为每个模型的输出分配分数来实现的。然后，通过计算所有问题的总质量分数来评估模型之间的相对表现。尽管这种评估方法并不严格，但它相当有趣、相对一致，并迫使我们思考
    LLM 领域未来会如何演变。
- en: '![](../Images/fdf836585e3986a0f42ffffd2df2c18c.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdf836585e3986a0f42ffffd2df2c18c.png)'
- en: (from [4])
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [4]）
- en: Compared to other open-source models, we see that GPT-4 tends to prefer the
    output of Vicuna. Plus, Vicuna produces output that exceeds or matches ChatGPT
    quality on 45% of questions. *This level of quality is pretty impressive for a
    model that can be fine-tuned with only $300!*
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他开源模型相比，我们看到 GPT-4 更倾向于 Vicuna 的输出。此外，Vicuna 在 45% 的问题上产生的输出质量超过或匹配 ChatGPT。*这种质量水平对于一个只需
    $300 即可微调的模型来说相当令人印象深刻！*
- en: '[Koala: A Dialogue Model for Academic Research](https://bair.berkeley.edu/blog/2023/04/03/koala/)
    [5]'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[Koala: 一个用于学术研究的对话模型](https://bair.berkeley.edu/blog/2023/04/03/koala/) [5]'
- en: “Models that are small enough to be run locally can capture much of the performance
    of their larger cousins if trained on carefully sourced data.” *— from [5]*
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “足够小到可以在本地运行的模型，如果经过精心挑选的数据训练，可以捕捉到其较大同行的大部分性能。”*— 来自 [5]*
- en: At this point, we might start to wonder whether we are going to run out of animals
    to name LLMs after. Nonetheless, Koala is similar to Vicuna and Alpaca, as it
    continues to focus upon closing the gap in quality between proprietary and open-source
    LLMs. More specifically, Koala is a version of LLaMA-13B that has been fine-tuned
    on dialogue data from a variety of sources, ranging from public datasets to dialogues
    with other high-quality LLMs that are available on the internet.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可能开始怀疑是否会用尽用于为 LLM 命名的动物。尽管如此，Koala 与 Vicuna 和 Alpaca 类似，因为它继续致力于缩小专有和开源
    LLM 之间的质量差距。更具体地说，Koala 是 LLaMA-13B 的一个版本，经过在各种来源的对话数据上进行微调，从公共数据集到与互联网上其他高质量
    LLM 的对话。
- en: '![](../Images/19e8f10f3272380621c19273ac4905c6.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19e8f10f3272380621c19273ac4905c6.png)'
- en: Koala compared to related LLMs (from [5])
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Koala 与相关 LLM 的比较（来自 [5]）
- en: 'When evaluated on real-world prompts, Koala-13B is found to achieve competitive
    performance compared to ChatGPT and even outperform the related Alpaca model.
    As such, results from Koala continue to support a trend that we see in all work
    following LLaMA. Namely, we see that smaller models can achieve impressive quality
    given the correct data for fine-tuning. Findings like this might lead us to wonder:
    *are we focusing too much on model scale and not enough on the quality of our
    data?*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实世界的提示上进行评估时，Koala-13B 被发现相较于 ChatGPT 表现出具有竞争力的性能，甚至超过了相关的 Alpaca 模型。因此，Koala
    的结果继续支持我们在所有 LLaMA 后续工作中看到的趋势。即，我们看到较小的模型在获得正确的数据进行微调后可以取得令人印象深刻的质量。这样的发现可能会让我们想知道：*我们是否过于关注模型规模，而对数据质量关注不够？*
- en: '**Method.** Koala uses dialogue data both from public datasets and the internet
    for fine-tuning. However, authors in [5] heavily emphasize the importance of curating
    a high-quality dataset for fine-tuning. The data used for fine-tuning Koala can
    be roughly categorized as either distillation-based (i.e., dialogues from other
    LLMs) or open-source data (i.e., available in public datasets) and includes data
    from [ShareGPT](https://sharegpt.com/), [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3),
    [OIG](https://laion.ai/blog/oig-dataset/), [Anthropic HH](https://huggingface.co/datasets/Anthropic/hh-rlhf),
    and OpenAI [WebGPT](https://huggingface.co/datasets/openai/webgpt_comparisons)/[Summarization](https://huggingface.co/datasets/openai/summarize_from_feedback).
    Plus, the fine-tuning set even includes the data used for training the Alpaca
    [3] model.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法。** Koala使用来自公共数据集和互联网的对话数据进行微调。然而，[5]中的作者强调了为微调策划高质量数据集的重要性。用于微调Koala的数据大致可以分为蒸馏基础（即，来自其他LLM的对话）或开源数据（即，公开数据集中可用）两类，包括来自[ShareGPT](https://sharegpt.com/)、[HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)、[OIG](https://laion.ai/blog/oig-dataset/)、[Anthropic
    HH](https://huggingface.co/datasets/Anthropic/hh-rlhf)和OpenAI [WebGPT](https://huggingface.co/datasets/openai/webgpt_comparisons)/[Summarization](https://huggingface.co/datasets/openai/summarize_from_feedback)的数据。此外，微调集甚至包括用于训练Alpaca
    [3]模型的数据。'
- en: '![](../Images/1790825532b1ecd344f0762c4f970c28.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1790825532b1ecd344f0762c4f970c28.png)'
- en: (from [8])
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: （见[8]）
- en: All of this data is dialogue-based. Notably, however, some datasets contain
    multiple dialogues or responses for each question that are rated as good or bad.
    Interestingly, we can draw upon prior techniques [8] to incorporate this information
    into the fine-tuning process for the LLM. In particular, this is done via conditional
    training, where we can simply condition data over which the LLM is trained with
    human preference markers (e.g., just append textual information about whether
    the dialogue is good or bad); see above. Such an approach yields improved performance
    and enables us to use even low-quality dialogues for model training.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些数据都是基于对话的。然而，需要注意的是，一些数据集包含多个对话或对每个问题的响应，这些响应被评为好或坏。有趣的是，我们可以借鉴先前的技术[8]，将这些信息纳入LLM的微调过程中。特别地，这是通过条件训练来完成的，我们可以简单地将数据条件化，通过人类偏好标记来训练LLM（例如，只需附加有关对话是否好的文本信息）；见上文。这种方法可以提高性能，并使我们能够使用即使是低质量的对话进行模型训练。
- en: Authors in [5] make the training and hosting framework for Koala [publicly available](https://github.com/young-geng/EasyLM).
    The model is trained for two epochs using eight V100 GPUs, which takes about 6
    hours. In total, the compute cost of training this model is <$100 (assuming that
    we can use [preemptible/spot instances](https://cloud.google.com/compute/docs/instances/preemptible#what_is_a_preemptible_instance)),
    meaning that Koala is the cheapest model to reproduce of the models that we have
    seen so far!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[5]中的作者使Koala的训练和托管框架[公开可用](https://github.com/young-geng/EasyLM)。该模型使用八个V100
    GPU训练两个时期，耗时约6小时。总的来说，训练该模型的计算成本低于$100（假设我们可以使用[可抢占/临时实例](https://cloud.google.com/compute/docs/instances/preemptible#what_is_a_preemptible_instance)），这意味着Koala是迄今为止我们见过的模型中最便宜的再现模型！'
- en: '**results.** Authors in [5] train two different types of Koala models:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果。** [5]中的作者训练了两种不同类型的Koala模型：'
- en: '*Koala-distill*: fine-tuned only over distillation data (i.e., examples of
    dialogue from other chatbots)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Koala-distill*：仅在蒸馏数据上进行微调（即，来自其他聊天机器人的对话示例）'
- en: '*Koala-all*: fine-tuned using all of the data described above.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Koala-all*：使用上述所有数据进行微调。'
- en: Based on human trials and feedback, the quality of these Koala models is compared
    to that of Alpaca and ChatGPT. For evaluation, questions from the Alpaca [3] evaluation
    set and a set of real user queries from the internet are used. The authors choose
    to add more questions into the evaluation set because Alpaca’s evaluation set
    is quite similar to the data on which it was trained (i.e., both derived from
    [self-instruct](https://github.com/yizhongw/self-instruct) [2]).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 根据人类试验和反馈，这些Koala模型的质量与Alpaca和ChatGPT进行比较。评估中使用了来自Alpaca [3]评估集的问题和来自互联网的真实用户查询集。作者选择增加更多问题到评估集中，因为Alpaca的评估集与其训练数据非常相似（即，两者均源自[self-instruct](https://github.com/yizhongw/self-instruct)
    [2]）。
- en: '![](../Images/f70875cd88df8b36a2b49c90ba7f12be.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f70875cd88df8b36a2b49c90ba7f12be.png)'
- en: (from [5])
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: （见[5]）
- en: When humans judge the output of different LLMs in terms of quality and correctness,
    Koala-all is found to oftentimes exceed the performance of Alpaca and match or
    exceeding the quality of ChatGPT in a large number of cases. Plus, we see that
    Koala-distill actually outperforms Koala-all. This is a bit counterintuitive given
    that Koala-distill has a smaller fine-tuning dataset (i.e., just example dialogues
    from ChatGPT), but this tells us that the type and quality of data used for fine-tuning
    is incredibly important. Namely, using dialogues generated from a larger, better
    LLM for fine-tuning is incredibly effective.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们在质量和正确性方面评估不同LLM的输出时，发现Koala-all通常超越了Alpaca的表现，并在许多情况下达到或超过了ChatGPT的质量。此外，我们看到Koala-distill实际上表现优于Koala-all。这有点违反直觉，因为Koala-distill的微调数据集较小（即仅包含来自ChatGPT的示例对话），但这告诉我们，微调所用数据的类型和质量极为重要。也就是说，使用来自更大、更好的LLM生成的对话进行微调是非常有效的。
- en: “the key to building strong dialogue models may lie more in curating high-quality
    dialogue data that is diverse in user queries” *— from [5]*
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “构建强大对话模型的关键可能在于策划高质量、多样化的用户查询对话数据”*— 来自[5]*
- en: Going further…
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步探索…
- en: Even though LLaMA was proposed pretty recently, Alpaca, Vicuna, and Koala are
    not the only notable models that have been enabled (or inspired) by LLaMA. We
    can see below a list of other open-source language models that have been released
    recently.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLaMA提出的时间相对较短，但Alpaca、Vicuna和Koala并不是唯一受到LLaMA启发或支持的显著模型。以下是最近发布的其他开源语言模型的列表。
- en: '[Lit-LLaMA](https://github.com/Lightning-AI/lit-llama): a reproduction of LLaMA
    that is open-sourced under the [Apache-2.0 license](https://snyk.io/learn/apache-license/)
    (permits commercial use).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lit-LLaMA](https://github.com/Lightning-AI/lit-llama): 一个基于LLaMA的开源复现项目，遵循[Apache-2.0许可证](https://snyk.io/learn/apache-license/)（允许商业使用）。'
- en: '[ChatLLaMA](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama):
    make a personalized version of ChatGPT using LLaMA, your own data, and the least
    amount of compute possible.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatLLaMA](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama):
    使用LLaMA、你自己的数据以及尽可能少的计算资源来制作个性化版本的ChatGPT。'
- en: '[FreedomGPT](https://freedomgpt.com/): an open-source conversational Chatbot
    (based on Alpaca) that emphasizes lack of censorship.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FreedomGPT](https://freedomgpt.com/): 一个开源的对话型聊天机器人（基于Alpaca），强调没有审查。'
- en: '[ColossalChat](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b):
    an open-source ChatGPT replica that comes with a fully-implemented (and public)
    [RLHF pipeline](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)
    based on LLaMA (includes data collection, supervised fine-tuning, reward model
    training, and reinforcement learning fine-tuning; see below).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ColossalChat](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b):
    一个开源的ChatGPT复制品，配备了一个完全实现的（且公开的）[RLHF管道](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)，基于LLaMA（包括数据收集、监督微调、奖励模型训练和强化学习微调；详见下文）。'
- en: '[StackLLaMA](https://huggingface.co/blog/stackllama): provides an open implementation
    and discussion of RLHF-based fine-tuning for producing powerful chatbots (specifically
    using LLaMA as a starting point).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[StackLLaMA](https://huggingface.co/blog/stackllama): 提供了一个基于RLHF的微调开源实现和讨论，用于生成强大的聊天机器人（具体使用LLaMA作为起点）。'
- en: '[GPT4All](https://github.com/nomic-ai/gpt4all): demo, data, and code for training
    open-source LLMs based on LLaMA and GPT-J (has an Apache-2.0 license!).'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT4All](https://github.com/nomic-ai/gpt4all): 用于训练基于LLaMA和GPT-J的开源LLM的演示、数据和代码（拥有Apache-2.0许可证！）。'
- en: '[Baize](https://github.com/project-baize/baize-chatbot): an LLaMA-based, open-source
    chatbot that performs fine-tuning using [LoRA](https://github.com/microsoft/LoRA)
    (a parameter-efficient fine-tuning method).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Baize](https://github.com/project-baize/baize-chatbot): 一个基于LLaMA的开源聊天机器人，使用[LoRA](https://github.com/microsoft/LoRA)（一种参数高效的微调方法）进行微调。'
- en: '[Galpaca](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b):
    a version of [Galactica](https://cameronrwolfe.substack.com/i/93578656/galactica-a-large-language-model-for-science)
    (language model for science) that has been fine-tuned on the same dataset as Alpaca.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Galpaca](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b):
    一个[Galactica](https://cameronrwolfe.substack.com/i/93578656/galactica-a-large-language-model-for-science)（科学语言模型）版本，已在与Alpaca相同的数据集上进行了微调。'
- en: '[Dolly 2.0](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm):
    this model is not based on LLaMA, but is an open-source chatbot that has been
    instruction fine-tuned to ChatGPT-like quality and is open for commercial use.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dolly 2.0](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm):
    该模型不基于 LLaMA，但是一款开源聊天机器人，经过指令微调以达到类似 ChatGPT 的质量，并开放商业使用。'
- en: '[Open Assistant](https://github.com/LAION-AI/Open-Assistant): an open-source
    chatbot (comparable to ChatGPT) that can understand tasks, interact with third-party
    systems, and retrieve information.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Open Assistant](https://github.com/LAION-AI/Open-Assistant): 一个开源聊天机器人（与 ChatGPT
    相当），能够理解任务、与第三方系统互动并检索信息。'
- en: '![](../Images/94e36e8473e08ed1b0482230c5ef36d6.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94e36e8473e08ed1b0482230c5ef36d6.png)'
- en: (from [9])
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [9])
- en: Beyond the variety of proposed models, LLM research and use has also become
    more accessible as a result of LLaMA. LLaMA-13B could already be run using only
    a single GPU, but now we can even do this locally (e.g., on a macbook)!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提出的各种模型，LLM 的研究和使用也因为 LLaMA 而变得更加可及。LLaMA-13B 已经可以仅用一个 GPU 运行，但现在我们甚至可以在本地（例如，在
    macbook 上）完成这个操作！
- en: '[Alpaca.cpp](https://github.com/antimatter15/alpaca.cpp): run an open reproduction
    of Alpaca locally.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Alpaca.cpp](https://github.com/antimatter15/alpaca.cpp): 本地运行 Alpaca 的开源复刻版本。'
- en: '[GPTQ-4-LLaMA](https://github.com/qwopqwop200/GPTQ-for-LLaMa): a [4-bit quantized](https://cameronrwolfe.substack.com/p/quantized-training-with-deep-networks-82ea7f516dc6)
    version of LLaMA.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPTQ-4-LLaMA](https://github.com/qwopqwop200/GPTQ-for-LLaMa): 一个 [4-bit 量化](https://cameronrwolfe.substack.com/p/quantized-training-with-deep-networks-82ea7f516dc6)
    的 LLaMA 版本。'
- en: '[LLaMA.cpp](https://github.com/ggerganov/llama.cpp): inference of several open-source
    LLMs with 4-bit quantization, which enables local hosting (e.g., on a macbook).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLaMA.cpp](https://github.com/ggerganov/llama.cpp): 几个开源 LLM 的 4-bit 量化推理，这使得本地托管成为可能（例如，在
    macbook 上）。'
- en: It seems like LLMs will soon become available to more people than ever before.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来 LLMs 很快将比以往更多地向公众开放。
- en: Takeaways
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要点
- en: The main ideas that we can deduce from this work are *i)* LLaMA inspired a lot
    of open-source LLM research and *ii)* research/usage surrounding LLMs is becoming
    significantly more accessible because of LLaMA. If you told me a month ago that
    I would be able to run an LLM that is anywhere near the performance of ChatGPT
    on my macbook, I wouldn’t have believed you. These are exciting times to witness,
    and I’m grateful to be a small part of such an awesome community! A few basic
    takeaways are listed below.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这项工作中推断出的主要观点是 *i)* LLaMA 激发了大量开源 LLM 研究和 *ii)* 围绕 LLM 的研究/使用因为 LLaMA 而变得显著更为可及。如果一个月前你告诉我，我可以在我的
    macbook 上运行接近 ChatGPT 性能的 LLM，我是不会相信的。这是令人兴奋的时刻，我很感激能成为这样一个了不起的社区中的一员！以下列出了几个基本要点。
- en: '**LLMs are for everyone.** If we were questioning it before, we now know that
    the research community can indeed do valuable research on LLMs. A few weeks ago,
    most of us thought that LLMs were not very accessible due to extreme data and
    compute requirements. However, we can now train ChatGPT-quality models (or at
    least something close) for a few hundred dollars and even use these models to
    perform dialogue on our laptop!'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLMs 适合所有人。** 如果之前我们对此有所质疑，现在我们知道研究社区确实可以在 LLMs 上进行有价值的研究。几周前，我们大多数人认为由于极高的数据和计算需求，LLMs
    并不容易获得。然而，现在我们可以用几百美元训练出 ChatGPT 级别的模型（或至少接近的模型），甚至可以在我们的笔记本电脑上使用这些模型进行对话！'
- en: '**are smaller models enough?** For a long time, model scale has been an important
    component (along with large pretraining datasets) of high-performing LLMs. However,
    models such as Koala and Vicuna show us that smaller LLMs can actually perform
    incredibly well (and even match the performance of powerful LLMs like ChatGPT
    in some cases). Such a finding highlights the importance of data quality. Within
    the work we have seen here. the most effective techniques tend to use the output
    of larger LLMs as training data, indicates that knowledge distillation may be
    an important component of creating LLMs that are small but powerful.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**较小的模型是否足够？** 长期以来，模型规模一直是高性能 LLM 的一个重要组成部分（连同大规模的预训练数据集）。然而，像 Koala 和 Vicuna
    这样的模型告诉我们，较小的 LLM 实际上可以表现得非常出色（甚至在某些情况下与强大的 LLM 如 ChatGPT 的表现相匹配）。这样的发现突显了数据质量的重要性。在我们看到的工作中，最有效的技术往往使用较大
    LLM 的输出作为训练数据，这表明知识蒸馏可能是创建小而强大的 LLM 的重要组成部分。'
- en: '**commercially viable?** Although many of these techniques are cool, using
    them in commercial applications is difficult. For example, OpenAI prohibits the
    use of ChatGPT (or any other API model) for training competing models, thus preventing
    knowledge distillation approaches based on the OpenAI API. Plus, even LLaMA itself
    prohibits commercial use. As such, models like Alpaca, Koala, and Vicuna are only
    interesting from a research perspective, and their approach cannot be used for
    any model that is used commercially. With proposals like [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama),
    however, it seems like commercially-viable versions of these models may slowly
    become available.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**商业上可行？** 尽管这些技术都很酷，但在商业应用中使用它们却很困难。例如，OpenAI 禁止使用 ChatGPT（或任何其他 API 模型）来训练竞争模型，从而阻止了基于
    OpenAI API 的知识蒸馏方法。此外，即便是 LLaMA 本身也禁止商业使用。因此，像 Alpaca、Koala 和 Vicuna 这样的模型仅在研究层面上具有兴趣，它们的方法不能用于任何商业用途的模型。然而，随着像
    [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama) 这样的提案出现，这些模型的商业可行版本可能会逐渐出现。'
- en: Closing Remarks
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. You can also check out my [other
    writings](https://medium.com/@wolfecameron) on medium! If you liked it, please
    follow me on [twitter](https://twitter.com/cwolferesearch) or subscribe to my
    [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/), where
    I help readers build a deeper understanding of topics in AI research via understandable
    overviews of popular papers.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢您阅读本文。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的 AI 总监。我研究深度学习的经验和理论基础。您还可以查看我在 medium 上的 [其他文章](https://medium.com/@wolfecameron)！如果您喜欢这篇文章，请关注我的
    [twitter](https://twitter.com/cwolferesearch) 或订阅我的 [Deep (Learning) Focus 新闻通讯](https://cameronrwolfe.substack.com/)，在其中我通过对流行论文的易懂概述帮助读者深入理解
    AI 研究中的主题。
- en: Bibliography
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.”
    *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Touvron, Hugo 等人。“Llama：开放且高效的基础语言模型。” *arXiv 预印本 arXiv:2302.13971* (2023)。'
- en: '[2] Wang, Yizhong, et al. “Self-Instruct: Aligning Language Model with Self
    Generated Instructions.” *arXiv preprint arXiv:2212.10560* (2022).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Wang, Yizhong 等人。“Self-Instruct：将语言模型与自生成的指令对齐。” *arXiv 预印本 arXiv:2212.10560*
    (2022)。'
- en: '[3] Taori, Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.”
    (2023).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Taori, Rohan 等人。“斯坦福 Alpaca：一个遵循指令的 LLaMA 模型。” (2023)。'
- en: '[4] Chiang, Wei-Lin et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4
    with 90%* ChatGPT Quality.” (2023).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Chiang, Wei-Lin 等人。“Vicuna：一个开源聊天机器人，令人印象深刻的 GPT-4 质量达到 90%* ChatGPT。”
    (2023)。'
- en: '[5] Geng, Xinyang et al. “Koala: A Dialogue Model for Academic Research.” (2023).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Geng, Xinyang 等人。“Koala：一个用于学术研究的对话模型。” (2023)。'
- en: '[6] OpenAI (2023). “GPT-4 Technical Report.” *ArXiv, abs/2303.08774*.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] OpenAI (2023)。“GPT-4 技术报告。” *ArXiv, abs/2303.08774*。'
- en: '[7] Guo, Biyang, et al. “How Close is ChatGPT to Human Experts? Comparison
    Corpus, Evaluation, and Detection.” *arXiv preprint arXiv:2301.07597* (2023).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Guo, Biyang 等人。“ChatGPT 与人类专家有多接近？比较语料库、评估和检测。” *arXiv 预印本 arXiv:2301.07597*
    (2023)。'
- en: '[8] Liu, Hao et al. “Chain of Hindsight Aligns Language Models with Feedback.”
    *arXiv preprint arXiv:2302.02676* (2023)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Liu, Hao 等人。“事后链将语言模型与反馈对齐。” *arXiv 预印本 arXiv:2302.02676* (2023)'
- en: '[9] Ouyang, Long, et al. “Training language models to follow instructions with
    human feedback.” *Advances in Neural Information Processing Systems* 35 (2022):
    27730–27744.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Ouyang, Long 等人。“通过人类反馈训练语言模型以遵循指令。” *神经信息处理系统进展* 35 (2022)：27730–27744。'
- en: '[10] Wei, Jason, et al. “Finetuned language models are zero-shot learners.”
    *arXiv preprint arXiv:2109.01652* (2021).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Wei, Jason 等人。“微调语言模型是零样本学习者。” *arXiv 预印本 arXiv:2109.01652* (2021)。'
- en: '[11] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge
    in a neural network.” *arXiv preprint arXiv:1503.02531* (2015).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Hinton, Geoffrey, Oriol Vinyals 和 Jeff Dean。“在神经网络中蒸馏知识。” *arXiv 预印本 arXiv:1503.02531*
    (2015)。'
- en: '[12] Gou, Jianping, et al. “Knowledge distillation: A survey.” *International
    Journal of Computer Vision* 129 (2021): 1789–1819.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Gou, Jianping 等人。“知识蒸馏：综述。” *国际计算机视觉期刊* 129 (2021)：1789–1819。'
