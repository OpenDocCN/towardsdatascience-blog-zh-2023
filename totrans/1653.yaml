- en: Pitfalls in Product Experimentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pitfalls-in-product-experimentation-145d12bb139f](https://towardsdatascience.com/pitfalls-in-product-experimentation-145d12bb139f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/69c395fd20f7e52c33a86110cf56307b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by pikisuperstar on Freepik (www.freepik.com)
  prefs: []
  type: TYPE_NORMAL
- en: Common to-not-do-lists often overlooked in product experimentation causing poor
    and unreliable results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://tanuwidjajaolivia.medium.com/?source=post_page-----145d12bb139f--------------------------------)[![Olivia
    Tanuwidjaja](../Images/52a56de28da9b782b57f1c3928655cfb.png)](https://tanuwidjajaolivia.medium.com/?source=post_page-----145d12bb139f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----145d12bb139f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----145d12bb139f--------------------------------)
    [Olivia Tanuwidjaja](https://tanuwidjajaolivia.medium.com/?source=post_page-----145d12bb139f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----145d12bb139f--------------------------------)
    ·8 min read·Jan 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: We all know product experimentation is important, and **its benefits have largely
    been proven by organizations,** enabling data-driven decisions on products, features,
    and processes. [Google was testing 40 shades of blue](https://bambrick.com.au/blog/google-increased-revenue-200-million-just-finding-perfect-shade-blue/)
    on a link in the search results, and the right blue shade led to 200M in revenue.
    [Booking.com has acknowledged](https://hbr.org/2020/03/building-a-culture-of-experimentation)
    the scaling and transformation of the organization were made possible by numerous
    testing and experiments conducted there.
  prefs: []
  type: TYPE_NORMAL
- en: However, product experiments, like any other statistical testing or experimentation,
    **are prone to pitfalls**. These are design and/or execution flaws, which might
    be hidden or unsuspected throughout the process. It is the duty of the data team
    — Product Data Analysts/Data Scientists —to guardrail experimentations execution
    and analysis to get reliable results. And hence it is important to understand
    the common pitfalls and how to treat them, as they might mislead the analysis
    results and conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: If the experiment is not configured and analysed properly, it might lead to
    poor and unreliable results, defeating the initial purpose of the experiment —
    which is for testing out the treatments and gauging the impact.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Configuration pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before looking into the statistical strategy and analysis, it is essential to
    ensure **the** **planning and designing of the overall experimentation** are done
    right. While the things here seem basic, there is a high chance of it being overlooked
    (*again, as it is so basic*) and eventually making us miss out on the experiment
    if not done properly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizing for the wrong metrics.** Metrics selection drives the overall
    decision of whether the treatment changes are being rolled out or not. As a rule
    of thumb, [a metric for an experiment is ideally **relevant to business** and
    **movable/impacted by the treatment** given](https://medium.com/geekculture/selecting-the-right-metrics-to-be-tracked-5a427b94b725).
    (1) *If this metric goes up/down, would you be happy?* (2) *Suppose you’re a user
    that is given the treatment, would you do or not do activities that will impact
    the metric?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not maximizing the variations potential.** In the theoretical world, A/B
    testing (or split testing) is a common term used. It’s [comparing **two versions**
    of something to figure out which performs better](https://hbr.org/2017/06/a-refresher-on-ab-testing).
    In the practical world, this can be further extended to more than two versions
    ([**A/B/n testing**](https://www.optimizely.com/optimization-glossary/abn-testing/))
    or testing for a combination of variables ([**multivariate testing**](https://www.optimizely.com/optimization-glossary/multivariate-testing/)).
    Having more variations is great to **maximize resource utilization** and the **possibility
    of getting the best decision option** outof the experiment. They come with some
    [side statistical effects](/a-practical-guide-to-product-experimentation-f9a6252c0e9e)
    (i.e increase in sample size requirement; familywise error rate), but it’s still
    something worth exploring.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overlapping experiments.** There can be numerous experiments happening at
    the same time in the organization. Problems can occur when these different experiments
    are running on similar features as **they could interfere with each other** —
    affecting the same metrics on an overlapping subset of users. The metric increase
    from the experiment might actually not come from the treatment alone, but from
    another treatment from the overlapping experiment. **Organization-wide coordination
    (from experiment timing to targeting assignment)** can help to minimize this issue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Going directly to full rollout.** It might be tempting to run the experiment
    in full rollout right away to minimize the time needed and get the result as soon
    as possible. However, experiment changes are still “product releases” and things
    can go wrong in between. It is recommended to **approach the experiment with a
    staged rollout** to reduce the risk in these releases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/c429368f7fb0b25fcd5ff0b3aff3f6b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by pch.vector on Freepik (www.freepik.com)
  prefs: []
  type: TYPE_NORMAL
- en: Having a [**product experiment platform**](https://engineering.atspotify.com/2020/10/spotifys-new-experimentation-platform-part-1/)
    can be a potential solution to prevent these pitfalls, ensuring standardized metrics
    and best practices on the process are implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Product experiment is [the process of continually ***testing hypotheses*** for
    ways to improve your product](https://www.hotjar.com/product-experimentation/).
    Hypothesis testing itself is essentially a form of **statistical inference**,
    and hence there are statistical principles to be followed in order to do product
    experiments properly.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the product context and use cases, experiments might be statistically
    more complicated and require some extra measures to be looked out for. Below are
    some of the common ones.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment “peeking”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When running an experiment, it is quite tempting to check on the results shortly
    straight away after the deployment, and draw (premature) conclusions, especially
    if the results look good or aligned with our hypothesis. This is called the experiment
    “peeking” problem.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment “peeking” occurs when **the outcome is erroneously called *before*
    the proper sample size has been reached**. Even if the initial results show statistical
    significance, the inference might be **coming purely out of chance** and is a
    flawed inference if drawn before reaching the proper sample size.
  prefs: []
  type: TYPE_NORMAL
- en: The ideal way to tackle this is to confirm the sample size at the beginning
    of the test and **defer any conclusion before that sample value is reached**.
    However, in some cases reaching enough sample size might take too long and become
    not practical. One technique to explore in this case is [**sequential testing**](/unlocking-peeking-in-ab-tests-7847b9c2f6bb),
    where the final sample size is dynamic to the data we observe during the test.
    So if we observe more extreme results at the start, the test can be ended earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c990da67fe40299b430a591647d740e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Hexandcube](https://unsplash.com/@hexandcube?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Not setting the right null hypothesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In product experiments, we set up a [null hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis)
    to be tested — rejected or not rejected — with the treatment given. A common classic
    null hypothesis is *no difference in the variable of interests* between the datasets
    (control group vs treatment group) analyzed. This is called a [**superiority test**](https://www.analytics-toolkit.com/glossary/superiority-test/),
    in which we expect some **superior discrepancy between the treatment and control
    groups —** expecting a positive change in thevariable of interest (e.g. means,
    proportions) of the treatment group in order to proceed with implementing the
    treatment.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative for this is the [**non-inferiority test**](https://blog.analytics-toolkit.com/2017/case-non-inferiority-designs-ab-testing/),
    in which we have reason to implement a tested variant as long as it is not substantially
    worse than the control. The null hypothesis in this test would be something along
    the line of “*the variable of interest in the variant is X% worse than the control,
    or more*”. In this test, we are good to proceed with implementing the treatment
    even if it is performing worse than the control, as long as it is still within
    the “margin of caring” range.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/deffec2475ea5accd4f8115c8a68f81e.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of superiority vs non-inferiority test (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: This non-inferiority test can be useful for changes that might cause some negative
    impacts (i.e testing the impact of removing a feature on booking conversion) or
    to check secondary metrics on an experiment that we can accept decreasing to a
    certain threshold for an increase in the primary metric.
  prefs: []
  type: TYPE_NORMAL
- en: Contamination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The commonly used hypothesis tests — z-test and t-test — runs under the [assumption](https://www.mathworks.com/help/stats/hypothesis-test-assumptions.html)
    that the data are ***independently*** sampled from a normal distribution. While
    in most cases this can be easily fulfilled by ensuring randomized non-duplicate
    assignments, it can be tricky in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: For example, [experimenting with delivery pricing in an on-demand delivery app](https://medium.com/@DoorDash/switchback-tests-and-randomized-experimentation-under-network-effects-at-doordash-f1d938ab7c2a).
    Though treatment is isolated to selected users, there might be some impact on
    the non-treatment group as well, as the delivery fleet is shared across the area
    (instead of per customer). This is called contamination or network effect, in
    which **different treatments of an experiment interfere with each other.**
  prefs: []
  type: TYPE_NORMAL
- en: One common solution is to utilize a “**switchback experiment**”. In this case,
    all the users in the experiment will be exposed to the same experience where randomization
    happens on a time interval and region (or other granularity where the treatment
    effect can be isolated). The metrics of interest will then be averaged across
    time intervals.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple comparison problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The multiple-comparison problem is a well-known issue in statistics. It occurs
    when one **considers a set of statistical inferences simultaneously** or infers
    a subset of parameters selected based on the observed values.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we’re experimenting with the new UI page (treatment) compared to
    the old UI page (control) of an e-commerce platform. Instead of mainly testing
    on the booking conversion impact, we’re also checking it against numerous other
    (not-so-relevant) metrics like search-bar clicks, per-categories clicks, session
    duration, coupon usage rate, and so on. **As more attributes are compared, it
    becomes increasingly likely that the treatment and control groups will appear
    to differ on at least one attribute due to random** [**sampling error**](https://en.wikipedia.org/wiki/Sampling_error)
    **alone.**
  prefs: []
  type: TYPE_NORMAL
- en: To control this problem statistically, there are some approaches that can be
    used, like [**Bonferroni correction**](https://en.wikipedia.org/wiki/Bonferroni_correction)
    which lowers the p-value threshold that is needed to call a result significant.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df9a83e8e6979f626cb504dec527e030.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Clay Banks](https://unsplash.com/@claybanks?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Taking it to the next level
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The common pitfalls above aside, product experiment results might still be not
    very ideal and reliable. There are still some caveats to consider and keep in
    mind when analyzing the experiment results.
  prefs: []
  type: TYPE_NORMAL
- en: '**Novelty effect**. When some changes are introduced in the product, users
    are typically curious to explore more, and hence drive the change in business
    metrics. However, this effect is temporary as the interest might normalize after
    a while as the change becomes less novel. With this in mind, it is often a good
    idea to establish a “**burn-in period**” in experiments and ignore the data collected
    in the initial period of the experiment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consider seasonality**. Some product/feature usage might have a certain seasonal
    lifecycle that might impact experiments. For example, an entertainment site might
    see much higher traffic on weekends compared to weekdays. When running an experiment
    on the product, we can try to cover both weekends and weekdays to get holistic
    estimates of the treatment impact.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no single perfect easy way to run a product experiment, as it varies
    by population and treatment contexts. Also, not every real-world impact can be
    easily quantifiable. But still, ***product experiments done in a statistically
    right way can help bring scientific reasoning to a business decision***. It can
    be considered hand-in-hand with user research and product/business sense grained
    in the product domain experts — you as the PM or Data Analyst.
  prefs: []
  type: TYPE_NORMAL
