["```py\n**Table of Contents:**\n\nWhat do we mean by \"regression analysis\"?\nUnderstanding correlation\nThe difference between correlation and regression\nThe Linear Regression model\nAssumptions for the Linear Regression model\nFinding the line that best fits the data\nGraphical methods to validate your model\nAn example in Python\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create random linear data\na = 130\n\nx = 6*np.random.rand(a,1)-3\ny = 0.5*x+5+np.random.rand(a,1)\n\n# Labels\nplt.xlabel('x')\nplt.ylabel('y')\n\n# Plot a scatterplot\nplt.scatter(x,y)\n```", "```py\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create data\nx = np.array([1, 1, 2, 3, 4, 4, 5, 6, 7, 7, 8, 9])\ny = np.array([13, 14, 17, 12, 23, 24, 25, 25, 24, 28, 32, 33])\n\n# Create the dataframe \ndf = pd.DataFrame({'x':x, 'y':y})\n\n# Plot heat map for correlation coefficient\nsns.heatmap(df.corr(), annot=True, fmt=\"0.2\")\n```", "```py\ny = wx + b\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create data\nx = np.array([1, 1, 2, 3, 4, 4, 5, 6, 7, 7, 8, 9])\ny = np.array([13, 14, 17, 12, 23, 24, 25, 25, 24, 28, 32, 33])\n\n# Show scatterplot\nplt.scatter(x, y)\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create data\nx = np.array([1, 1, 2, 3, 4, 4, 5, 6, 7, 7, 8, 9])\ny = np.array([13, 14, 17, 12, 23, 24, 25, 25, 24, 28, 32, 33])\n\n# Create basic scatterplot\nplt.plot(x, y, 'o')\n\n# Obtain m (slope) and b (intercept) of a line\nm, b = np.polyfit(x, y, 1)\n\n# Add linear regression line to scatterplot \nplt.plot(x, m*x+b)\n\n# Labels\nplt.xlabel('x variable')\nplt.ylabel('y variable')\n```", "```py\nimport matplotlib.pyplot as plt\n\n # Scatterplot of y_train and y_train_pred\n plt.scatter(y_train, y_train_pred)\n plt.plot(y_test, y_test, color='r') # Plot the line\n\n # Labels\n plt.title('ACTUAL VS PREDICTED VALUES')\n plt.xlabel('ACTUAL VALUES')\n plt.ylabel('PREDICTED VALUES')\n```", "```py\nimport pandas as pd\n\n# Define the columns\ncolumns = ['Overall Qual', 'Overall Cond', 'Gr Liv Area',\n          'Total Bsmt SF', 'SalePrice']\n\n# Create dataframe\ndf = pd.read_csv('http://jse.amstat.org/v19n3/decock/AmesHousing.txt',\n                sep='\\t', usecols=columns)\n\n# Show statistics\ndf.describe()\n```", "```py\nimport numpy as np\n\n# Create a list of numbers\ndata = [1, 2, 3, 4, 5]\n\n# Find min and max values\ndata_min = min(data)\ndata_max = max(data)\n\n# Normalize the data\ndata_normalized = [(x - data_min) / (data_max - data_min) for x in data]\n\n# Print the normalized data\nprint(f'normalized data: {data_normalized}')\n\n>>>\n\n     normalized data: [0.0, 0.25, 0.5, 0.75, 1.0]\n```", "```py\nimport numpy as np\n\n# Original data\ndata = [1, 2, 3, 4, 5]\n\n# Calculate mean and standard deviation\nmean = np.mean(data)\nstd = np.std(data)\n\n# Standardize the data\ndata_standardized = [(x - mean) / std for x in data]\n\n# Print the standardized data\nprint(f'standardized values: {data_standardized}')\nprint(f'mean of standardized values: {np.mean(data_standardized)}')\nprint(f'std. dev. of standardized values: {np.std(data_standardized): .2f}')\n\n>>>\n\nstandardized values: [-1.414213562373095, -0.7071067811865475, 0.0, 0.7071067811865475, 1.414213562373095]\nmean of standardized values: 0.0\nstd. dev. of standardized values:  1.00 \n```", "```py\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Drop NaNs from dataframe\ndf = df.dropna(axis=0)\n\n# Apply mask\nmask = np.triu(np.ones_like(df.corr()))\n\n# Heat map for correlation coefficient\nsns.heatmap(df.corr(), annot=True, fmt=\"0.1\", mask=mask)\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\n\n# Define the features\nX = df.iloc[:,:-1]\n\n# Define the label\ny = df.iloc[:,-1]\n\n# Scale the features\nscaler = StandardScaler() # Call the scaler\nX = scaler.fit_transform(X) # Fit the features to scale them\n```", "```py\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Fit the LR model\nreg = LinearRegression().fit(X_train, y_train)\n\n# Calculate R^2\ncoeff_det_train = reg.score(X_train, y_train)\ncoeff_det_test = reg.score(X_test, y_test)\n\n# Print metrics\nprint(f\" R^2 for training set: {coeff_det_train}\")\nprint(f\" R^2 for test set: {coeff_det_test}\")\n\n>>>\n\n   R^2 for training set:  0.77\n   R^2 for test set:  0.73\n```", "```py\n**Notes:**\n1) your results can be slightly different due to the stocastical\nnature of the ML models.\n\n2) here we can see generalization on action: \nwe fitted the Linear Regression model to the train set with\n*reg = LinearRegression().fit(X_train, y_train)*.\nThe, we've calculated R^2 on the training and test sets with:\n*coeff_det_train = reg.score(X_train, y_train)\ncoeff_det_test = reg.score(X_test, y_test*\n\nIn other words: we don't fit the data to the test set.\nWe fit the data to the training set and we calculate the scores\nand predictions (see next snippet of code with KDE) on both sets\nto see the generalization of our modelon new unseen data\n(the data of the test set).\n```", "```py\n# Calculate predictions\ny_train_pred = reg.predict(X_train) # train set\ny_test_pred = reg.predict(X_test) # test set\n\n# KDE train set\nax = sns.kdeplot(y_train, color='r', label='Actual Values') #actual values\nsns.kdeplot(y_train_pred, color='b', label='Predicted Values', ax=ax) #predicted values\n\n# Show title\nplt.title('Actual vs Predicted values')\n# Show legend\nplt.legend()\n```", "```py\n# KDE test set\nax = sns.kdeplot(y_test, color='r', label='Actual Values') #actual values\nsns.kdeplot(y_test_pred, color='b', label='Predicted Values', ax=ax) #predicted values\n\n# Show title\nplt.title('Actual vs Predicted values')\n# Show legend\nplt.legend()\n```"]