- en: 3D Point Cloud Shape Detection for Indoor Modelling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/3d-point-cloud-shape-detection-for-indoor-modelling-70e36e5f2511](https://towardsdatascience.com/3d-point-cloud-shape-detection-for-indoor-modelling-70e36e5f2511)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Hands-on Tutorials](https://towardsdatascience.com/tagged/hands-on-tutorials),
    3D Python'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A 10-step Python Guide to Automate 3D Shape Detection, Segmentation, Clustering,
    and Voxelization for Space Occupancy 3D Modeling of Indoor Point Cloud Datasets.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@florentpoux?source=post_page-----70e36e5f2511--------------------------------)[![Florent
    Poux, Ph.D.](../Images/74df1e559b2edefba71ffd0d1294a251.png)](https://medium.com/@florentpoux?source=post_page-----70e36e5f2511--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70e36e5f2511--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70e36e5f2511--------------------------------)
    [Florent Poux, Ph.D.](https://medium.com/@florentpoux?source=post_page-----70e36e5f2511--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----70e36e5f2511--------------------------------)
    ¬∑28 min read¬∑Sep 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: If you have experience with point clouds or data analysis, you know how crucial
    it is to spot patterns. Recognizing data points with similar patterns, or "objects,"
    is important to gain more valuable insights. Our visual cognitive system accomplishes
    this task easily, but replicating this human ability through computational methods
    is a significant challenge.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to utilize the natural tendency of the human visual system to group
    sets of elements. üëÄ
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7a3a366e3f8d7cc6c3e1b7a11307505.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a result of the Segmentation phase on the 3D Point Cloud. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: But why is it useful?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, it lets you easily access and work with specific parts of the data by
    grouping them into segments. Secondly, it makes the data processing faster by
    looking at regions instead of individual points. This can save a lot of time and
    energy. And finally, segmentation can help you find patterns and relationships
    you wouldn‚Äôt be able to see just by looking at the raw data. üîç Overall, segmentation
    is crucial for getting useful information from point cloud data. If you are unsure
    how to do it, do not worry ‚Äî We will figure this out together! ü§ø
  prefs: []
  type: TYPE_NORMAL
- en: The Strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us frame the overall approach before approaching the project with an efficient
    solution. This tutorial follows a strategy comprising ten straightforward steps,
    as illustrated in our strategy diagram below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49c746a4375f4f284fe81f501c11fb4f.png)'
  prefs: []
  type: TYPE_IMG
- en: The workflow for 3D Point Cloud Indoor Modelling shown in this guide. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy is laid out, and below, you can find the quick links to the different
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now that we are set up, let us jump right in.
  prefs: []
  type: TYPE_NORMAL
- en: 'üéµ**Note to Readers***: This hands-on guide is part of a* [***UTWENTE***](https://www.itc.nl/)
    *joint work, with co-authors* ***F. Poux*** *and* ***V. Lehtola****. We acknowledge
    the financial contribution from the digital twins* [*@ITC*](http://twitter.com/ITC)
    *-project granted by the ITC faculty of the University of Twente.* ***All images
    are ¬© Florent Poux****.*'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Set up your Python environment.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b862dfa664dd54db82f77e3449a9a117.png)'
  prefs: []
  type: TYPE_IMG
- en: In a previous article below, we saw how to quickly set up an environment with
    Anaconda and use the IDE JupyterLab to manage your code. Continuing in this fashion
    if you set yourself up to become a fully-fledged Python app developer üòÜ.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----70e36e5f2511--------------------------------)
    [## 3D Python Workflows for LiDAR City Models: A Step-by-Step Guide'
  prefs: []
  type: TYPE_NORMAL
- en: The Ultimate Guide to unlocking a streamlined workflow for 3D City Modelling
    Applications. The tutorial covers Python‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----70e36e5f2511--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ä **Florent***: I highly recommend using a desktop setup or IDE and avoiding
    Google Colab IF you need to visualize 3D point clouds using the libraries provided,
    as they will be unstable at best or not working at worse (unfortunately‚Ä¶).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§† **Ville**: *We guess that you are running on Windows? This is fine, but if
    you want to get into computational methods, Linux is the go-to choice!*'
  prefs: []
  type: TYPE_NORMAL
- en: Well, we will take a ‚Äúparti-pris‚Äù to quickly get results. Indeed, we will accomplish
    excellent segmentation by following a minimalistic approach to coding üíª. That
    means being very picky about the underlying libraries! We will use three very
    robust ones, namely. `numpy`, `matplotlib`, and `open3d`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, to install the library package above in a fresh virtual environment,
    we suggest you run the following command from the `cmd` terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: üçú ***Disclaimer Note****:* We choose Python, not C++ nor Julia, so performances
    are what they are üòÑ. Hopefully, it will be enough for your application üòâ, for
    what we call ‚Äúoffline‚Äù processes (not real-time).
  prefs: []
  type: TYPE_NORMAL
- en: 'Within your Python IDE, make sure to import the three libraries that will be
    under heavy use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: And that is it! We are ready to rock indoor point cloud modeling workflows!
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Point Cloud Data Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e18d10ffc4d22a7dfa415f9b85211b8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In previous tutorials, we illustrated point cloud processing and meshing over
    a 3D dataset obtained using an aerial LiDAR from the AHN4 LiDAR Campaign. This
    time, we will use a dataset gathered using a Terrestrial Laser Scanner: the ITC
    new building. It was organized into three different sets for you to experiment
    on. In purple, you have an outdoor part. In red is the ground level, and in green
    is the first floor, as illustrated below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3a4446fe2fae77b40da248fa8e3081b.png)'
  prefs: []
  type: TYPE_IMG
- en: The 3D Point Cloud parts of the ITC Dataset used. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the data from the Drive Folder here: [ITC Datasets](https://drive.google.com/drive/folders/1sCBT1lc9A8Zn4grpxwFrBrvos86c0HZR?usp=share_link).
    Once you have a firm grasp on the data locally, you can load the dataset in your
    Python execution runtime with two simple lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ä **Florent**: *This Python code snippet uses the* `*Open3D*` *library to read
    a point cloud data file* `*ITC_groundfloor.ply*` *located in the directory. ‚Äú*`*../DATA/*`*‚Äù
    and assign it to the variable* `*pcd*`*.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The variable now holds your point cloud `pcd` you will play with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ac078d0d708734f455c9e8907b18642.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the point cloud data has been successfully loaded using Open3D, the next
    step is to apply various pre-processing techniques to enhance its quality and
    extract meaningful information.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Point Cloud Pre-Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/0bb8b750bb4d32115dac7abbc406fbde.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you intend on visualizing the point cloud within `open3d`It is good practice
    to shift your point cloud to bypass the large coordinates approximation, which
    creates shaky visualization effects. To apply such a shift to your `pcd` point
    cloud, first get the center of the point cloud, then translate it by subtracting
    it from the original variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Which can now be interactively visualized with the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö **Note**: `o3d.visualization.draw_geometries([pcd])` *calls the* `*draw_geometries()*`
    *function from the* `*visualization*` *Module in Open3D. The function takes a
    list of geometries as an argument and displays them in a visualization window.
    In this case, the list contains a single geometry, which is the* `*pcd*` *variable
    representing the point cloud. The* `*draw_geometries()*` *function creates a 3D
    visualization window and renders the point cloud. You can interact with the visualization
    window to rotate, zoom, and explore the point cloud from different perspectives.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38196ff8a2b3f0d77fa32690deefe8ba.png)![](../Images/1b23848831179d63eccaa3cf24969e6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Greatüëå, we are all set up to test some sampling strategies to unify our downward
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Point Cloud Random Sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us consider random sampling methods that can effectively reduce point cloud
    size while preserving overall structural integrity and representativeness. If
    we define a point cloud as a matrix (m x n), then a **decimated cloud** is obtained
    by keeping one row out of n of this matrix :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43048e0ad321a1ec9cb3b8607d68133a.png)'
  prefs: []
  type: TYPE_IMG
- en: A Point Cloud Decimation Strategy. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'At the matrix level, the decimation acts by keeping points for every n-th row
    depending on the n factor. Of course, this is made based on how the points are
    stored in the file. Slicing a point cloud with `open3d` It is pretty straightforward.
    To shorten and parametrize the expression, you can write the lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö **Note**: `sampled_pcd = pcd.random_down_sample(retained_ratio)` *applies
    random downsampling to the original point cloud* `*pcd*` *using the* `random_down_sample()`
    *function provided by Open3D. The* `retained_ratio` *parameter determines the
    proportion of points to be retained after downsampling. For example, if* `retained_ratio`
    *it is set to 0.5, approximately 50% of the points will be randomly selected and
    retained in the sampled point cloud.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f4d15aa96e162454499be387b499b6e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Results of the subsampling of the point cloud. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'üå± **Growing**: *When studying 3D point clouds, random sampling has limitations
    that could result in missing important information and inaccurate analysis. It
    doesn‚Äôt consider the spatial component or relationships between the points. Therefore,
    it‚Äôs essential to use other methods to ensure a more comprehensive analysis.*'
  prefs: []
  type: TYPE_NORMAL
- en: While this strategy is quick, random sampling may not be most adapted to a ‚Äústandardization‚Äù
    use case. The next step is to address potential outliers through statistical outlier
    removal techniques, ensuring the data quality and reliability for subsequent analysis
    and processing.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Statistical outlier removal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using an outlier filter on 3D point cloud data can help identify and remove
    any data points significantly different from the rest of the dataset. These outliers
    could result from measurement errors or other factors that can skew the analysis.
    By removing these outliers, we can get a more valid representation of the data
    and better adjust algorithms. However, we need to be careful not to delete valuable
    points.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will define a `statistical_outlier_removal` filter to remove points that
    are further away from their neighbors compared to the average for the point cloud.
    It takes two input parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nb_neighbors`, which specifies how many neighbors are considered to calculate
    the average distance for a given point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std_ratio`, which allows setting the threshold level based on the standard
    deviation of the average distances across the point cloud. The lower this number,
    the more aggressive the filter will be.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This amount to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö **Note**: *the* `*remove_statistical_outlier()*` *the function applies statistical
    outlier removal to the point cloud using a specified number of nearest neighbors
    (*`*nn*`*) and a standard deviation multiplier (*`*std_multiplier*`*). The function
    returns two values:* `*filtered_pcd*` *and* `*filtered_idx*`*.* `*filtered_pcd*`
    *represents the filtered point cloud, where statistical outliers have been removed.*
    `*filtered_idx*` *is an array of indices corresponding to the points in the original
    point cloud* `*pcd*` *that were retained after the outlier removal process.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize the results of this filtering technique, we color the outliers
    in red and add them to the list of point cloud objects we want to visualize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6ef3e983fc62a7eda4eaf143ce2330a5.png)'
  prefs: []
  type: TYPE_IMG
- en: The outliers are in tagged and visualized in red in the point cloud. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: After removing statistical outliers from the point cloud, the next step involves
    applying voxel-based sampling techniques to downsample the data further, facilitating
    efficient processing and analysis while preserving the essential structural characteristics
    of the point cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Point Cloud Voxel (Grid) Sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The grid subsampling strategy is based on the division of the 3D space in regular
    cubic cells called voxels. For each cell of this grid, we only keep one representative
    point, and this point, the representative of the cell, can be chosen in different
    ways. When subsampling, we keep that cell's closest point to the barycenter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/802a55909c5aa28b959e4357b8e2f9e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Voxel Grid Sampling. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'Concretely, we define a `voxel_size` that we then use to filter our point cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö **Note**: *This line performs voxel downsampling on the filtered point cloud,*
    `*filtered_pcd*`*, using the* `*voxel_down_sample()*` *function. The* `*voxel_size*`
    *parameter specifies the size of each voxel for downsampling the point cloud.
    Larger voxel sizes result in a more significant reduction in point cloud density.
    The result of the downsampling operation is assigned to the variable* `*pcd_downsampled*`*,
    representing the downsampled point cloud*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Time to visualize the repartition closely after our downsampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f65a10e76a7e48feae0b94ffc246e5db.png)'
  prefs: []
  type: TYPE_IMG
- en: The result of the sampling strategy applied on the point cloud. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, we have an outlier point set left untouched in further processing
    and a downsampled point cloud that constitutes the new subject of the subsequent
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Point Cloud Normals Extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A point cloud normal refers to the direction of a surface at a specific point
    in a 3D point cloud. It can be used for segmentation by dividing the point cloud
    into regions with similar normals, for example. In our case, normals will help
    identify objects and surfaces within the point cloud, making it easier to visualize.
    And it is an excellent opportunity to introduce a way to compute such normals
    semi-automatically. We first define the average distance between each point in
    the point cloud and its neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we use this information to extract a limited `max_nn` points within a
    radius `radius_normals` to compute a normal for each point in the 3D point cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The pcd_downsampled point cloud object is now proud to have normals, ready
    to display its prettiest side üòä. You know the drill at this stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/92aa19172478994bb05bcd86ef4bd146.png)'
  prefs: []
  type: TYPE_IMG
- en: The point cloud used for the follow-up experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Upon completing the voxel downsampling of the point cloud, the subsequent step
    involves configuring the parameters for point cloud shape detection and clustering,
    which plays a crucial role in grouping similar points together and extracting
    meaningful structures or objects from the downsampled point cloud data.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Point Cloud Parameter setting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/4d43adb22c40912f783a2cb44c0064b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this tutorial, we have selected two of the most effective and reliable methods
    for 3D Shape detection and clustering for you to master: RANSAC and Euclidean
    Clustering using DBSCAN. However, before utilizing these approaches, hence understanding
    the parameters, it is crucial to comprehend the fundamental concepts in simple
    terms.'
  prefs: []
  type: TYPE_NORMAL
- en: RANSAC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The RANSAC algorithm, short for RANdom SAmple Consensus, is a powerful tool
    for handling data that contains outliers, which is often the case when working
    with real-world sensors. The algorithm works by grouping data points into two
    categories: inliers and outliers. By identifying and ignoring the outliers, you
    can focus on working with reliable inliers, making your analysis more effective.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5c122fcaa8c0f6453ae2b2363de65b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Planar detection with RANSAC in a 3D Point Cloud. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: So let me use a tiny but simple example to illustrate how RANSAC works. Let
    us say that we want to fit a plane through the point cloud below. How can we do
    that?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb85346e13ae187ec137bef80050cce3.png)'
  prefs: []
  type: TYPE_IMG
- en: RANSAC Plane detection simulation in a random point cloud. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: First, we create a plane from the data, and for this, we randomly select 3 points
    from the point cloud necessary to establish a plane. And then, we simply check
    how many of the remaining points kind of fall on the plane (to a certain threshold),
    which will give a score to the proposal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4162c310eb51619b469d062fc8078732.png)'
  prefs: []
  type: TYPE_IMG
- en: RANSAC Scoring system illustrated. You can see that each iteration samples 3
    random points from which it will create a plan and then select the points that
    would fall on it. Here, iteration 159 would be the best candidate. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we repeat the process with 3 new random points and see how we are doing.
    Is it better? Is it worse? And again, we repeat this process over and over again,
    let‚Äôs say 10 times, 100 times, 1000 times, and then we select the plane model
    with the highest score (i.e. which has the best ‚Äúsupport‚Äù of the remaining data
    points). And that will be our solution: the supporting points plus the three points
    that we have sampled constitute our **inlier point set**, and the rest is our
    **outlier point set**. Easy enough, hun üòÅ?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Haha, but for the skeptics, don‚Äôt you have a rising question? How do we actually
    determine how many times we should repeat the process? How often should we try
    that? Well, that is actually something that we can compute, but let''s put it
    aside, for now, to focus on the matter at hand: point cloud segmentation üòâ.'
  prefs: []
  type: TYPE_NORMAL
- en: RANSAC Parameter Setting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We want to use RANSAC for detecting 3D planar shapes in our point cloud. To
    use RANSAC, we need to define three parameters: a distance threshold `distance_threshold`
    that allows tagging a point inlier or outlier to the 3D shape; a minimal number
    of point `ransac_n` selected to fit the geometric model; a number of iterations
    `num_iterations`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Determination of the Distance threshold:** We may be a bit limited by needing
    some domain knowledge to set up future segmentation and modeling threshold. Therefore,
    it would be exciting to try and bypass this to open the approach to non-experts.
    we will share with you a straightforward thought that could be useful. What if
    we were to compute the mean distance between points in our datasets and use this
    as a base to set up our threshold?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, it is an idea worth exploring. To determine such a value, we use a `KD-Tree`
    to speed up the process of querying the nearest neighbors for each point. From
    there, we can then query the k-nearest neighbors for each point in the point cloud,
    which is then packed in the `open3d` function shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: without much surprise, it is close to 5 cm as we sampled our point cloud to
    this value. It means that if we reasoned by considering the nearest neighbor,
    we would have an average distance of 51 mm.
  prefs: []
  type: TYPE_NORMAL
- en: 'üå± **Growing**: *From there, what we could do it to set the RANSAC parameter
    to a value derived from the nn_distance variable, which would then be adapted
    to the considered dataset, independently from domain knowledge. How would you
    approach this?*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Determination of the point number**: Here, it is quick. We want to find planes,
    so we will take the minimum number of points needed to define a 3D plane: 3.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Determination of the iteration number**: The more iteration you have, the
    more robust your 3D shape detection works, but the longer it takes. For now, we
    can leave that to 1000, which yields good results. We will explore more ingenious
    ways to find the noise ratio of a point cloud in future tutorials. üòâ'
  prefs: []
  type: TYPE_NORMAL
- en: 'üßô‚Äç‚ôÇÔ∏è **Experts**: *There exists an automatic way to get the iteration number
    right every time. If we want to succeed with a probability p (e.g., 99%), the
    outlier ratio in our data is e (e.g., 60%), and we need s point to define our
    model (here 3). The formula below gives us the expected number of iterations:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f34e9a8ff5e9d1dd5f22f9c860ef44df.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that our RANSAC parameters are defined, we can study a first segmentation
    pass.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Point Cloud Segmentation with RANSAC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/17e07a01da7ba28e3c757053c097f9b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us first set the different thresholds to non-automatic values for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, we can segment the point cloud using RANSAC to detect planes with
    the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We gather results in two variables: `plane_model`, which holds the parameters
    `a`,`b`,`c`,`d` of a plane, and the `inliers` as point indexes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows to use the indexes to segment the point cloud in a `inlier_cloud`
    point set (that we color in red), and `outlier_cloud` point set (that we color
    in grey:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ä **Florent***: The argument* `*invert=True*` *permits to select the opposite
    of the first argument, which means all indexes not present in* `*inliers*`*. If
    you are lacking the shading, remember to compute the normals, as shown above.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36f15121cce1019bae4e2e1fe3e1ebcc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'üå± **Growing**: *Try and adjust the various parameters, and study the impact
    with a qualitative analysis. Remember first to decorrelate changes (one variable
    at a time); else your analysis may be biased* üòä.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/385ce6263c78e78ca2ce72c1d56b7849.png)![](../Images/d0cf065a6a1ee3ea0f54091a3c00d1fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Great! You know how to segment your point cloud in an inlier point set and an
    outlier point set ü•≥! Now, let us study how to find some clusters close to one
    another. So let us imagine that once we detected the big planar portions, we have
    some ‚Äúfloating‚Äù objects that we want to delineate. How to do this? (yes, it is
    a false question, we have the answer for you üòÄ)
  prefs: []
  type: TYPE_NORMAL
- en: '6\. Scaling 3D Segmentation: Multi-Order RANSAC'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/24389692747d404df62d0d88855514cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Our philosophy will be very simple. We will first run RANSAC multiple times
    (let say `n` times) to extract the different planar regions constituting the scene.
    Then we will deal with the ‚Äúfloating elements‚Äù through Euclidean Clustering (DBSCAN).
    It means that we have to make sure we have a way to store the results during iterations.
    Ready?
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, let us instantiate an empty dictionary that will hold the results of
    the iterations (the plane parameters in `segment_models`, and the planar regions
    from the point cloud in `segments`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we want to make sure that we can influence later on the number of times
    we want to iterate for detecting the planes. To this end, let us create a variable
    `max_plane_idx` that holds the number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ä **Florent***: Here, we say that we want to iterate 10 times to find 10 planes,
    but there are smarter ways to define such a parameter. It actually extends the
    scope of the article and will be covered in another session.*'
  prefs: []
  type: TYPE_NORMAL
- en: Now let us go into a working loopy-loopy üòÅ, that I will first quickly illustrate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75431eb5d58c5a9972a73c461b4868ee.png)'
  prefs: []
  type: TYPE_IMG
- en: The loop to be executed to perform the segmentation within the RANSAC pass.
    ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first pass (loop `i=0`), we separate the inliers from the outliers.
    We store the inliers in `segments`, and then we want to pursue with only the remaining
    points stored in `rest`, that becomes the subject of interest for the loop n+1
    (loop `i=1`). That means that we want to consider the outliers from the previous
    step as the base point cloud until reaching the above threshold of iterations
    (not to be confused with RANSAC iterations). This translates into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And that is pretty much it! Now, for visualizing the ensemble, as we paint
    each segment detected with a color from `tab20` through the first line in the
    loop (`colors = plt.get_cmap("tab20")(i)`), you just need to write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: ü¶ö **Note***:* The list `[segments[i] for i in range(max_plane_idx)]` that we
    pass to the function `o3d.visualization.draw_geometries()` is actually a ‚Äúlist
    comprehension‚Äù ü§î. It is equivalent to writing a `for` loop that appends the first
    element `segments[i]` to a list. Conveniently, we can then add the `[rest]` to
    this list and the `draw.geometries()` method will understand we want to consider
    one point cloud to draw. How cool is that?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/149829534a11885af97f3ff19dea8f8f.png)'
  prefs: []
  type: TYPE_IMG
- en: The result of the DBSCAN first pass on the 3D Point Cloud
  prefs: []
  type: TYPE_NORMAL
- en: Ha! We think we are done‚Ä¶ But are we? Do you notice something strange here?
    If you look closely, there are some strange artifacts, like ‚Äúred lines/planes‚Äù
    that actually cut some planar elements. Why? üßê
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c67e5140011a746f7e66524ef332420.png)'
  prefs: []
  type: TYPE_IMG
- en: In fact, because we fit all the points to RANSAC plane candidates (which have
    no limit extent in the Euclidean space) independently of the point's density continuity,
    then we have these ‚Äúlines‚Äù artifacts depending on the order in which the planes
    are detected. So the next step is to prevent such behavior! For this, I propose
    to include in the iterative process a condition based on Euclidean clustering
    to refine inlier point sets in contiguous clusters. To this end, we will rely
    on the DBSCAN algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean Clustering (DBSCAN)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With point cloud datasets, we often need to group sets of points spatially contiguous
    (i.e. that are physically close or adjacent to each other in 3D space), as illustrated
    below. But how can we do this efficiently?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d3ff3a05173c7b35dfab22f6883f9a2.png)'
  prefs: []
  type: TYPE_IMG
- en: In this image, it seems obvious that we want to group points that are closed
    to one another, finding 5 sets of points. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: The DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm
    was introduced in 1996 for this purpose. This algorithm is widely used, which
    is why it was awarded a scientific contribution award in 2014 that has stood the
    test of time.
  prefs: []
  type: TYPE_NORMAL
- en: The DBSCAN algorithm involves scanning through each point in the dataset and
    constructing a set of reachable points based on density. This is achieved by analyzing
    the neighborhood of each point and including it in the region if it contains enough
    points. The process is repeated for each neighboring point until the cluster can
    no longer expand. Points that do not have enough neighbors are labeled as noise,
    making the algorithm robust to outliers. It‚Äôs pretty impressive, isn‚Äôt it? üòÜ
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f884ed20535463901f44913fbc652f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of the DBSCAN algorithm process and influence of the two parameters
    œµ and min_points on the results. You can see that the bigger the value, the fewer
    clusters are constituted. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'Ah, we almost forgot. The choice of parameters (œµ for the neighborhood and
    n_min for the minimal number of points) can also be tricky: One must take great
    care when setting parameters to create enough interior points (which will not
    happen if n_min is too large or œµ too small). In particular, this means that DBSCAN
    will have trouble finding clusters of different densities. BUT, DBSCAN has the
    great advantage of being computationally efficient without requiring to predefine
    the number of clusters, unlike Kmeans, for example. Finally, it allows for finding
    clusters of arbitrary shapes. We are now ready to dive in the parameter dark side
    of things üíª'
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN for 3D Point Cloud Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let me detail the logical process (Activate the beast modeüëπ). First, we need
    to define the parameters to run DBSCAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'üå± **Growing**: *The definition of these parameters is something to explore.
    You have to find a way to balance over-segmentation and under-segmentation problematics.
    Eventually, you can use some heuristics determination based on the initial distance
    definition of RANSAC. Something to explore* üòâ.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the for loop that we defined before, we will run DBSCAN just after the
    assignment of the inliers (`segments[i]=rest.select_by_index(inliers)`), by adding
    the following line right after:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will count how many points each cluster that we found holds, using
    a weird notation that makes use of a list comprehension. The result is then stored
    in the variable `candidates`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'And now? We have to find the ‚Äúbest candidate‚Äù, which is normally the cluster
    that holds the more points! And for this, here is the line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Okay, many tricks are happening under the hood here, but essentially, we use
    our Numpy proficiency to search and return the index of the points that belong
    to the biggest cluster. From here, it is downhill skiing, and we just need to
    ensure that we include any remaining clusters from each iteration for consideration
    in subsequent RANSAC iterations (üî• recommendation to to read 5 times the sentence
    to digest!):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö **Note***: the* `rest` *variable now makes sure to hold both the remaining
    points from RANSAC and DBSCAN. And of course, the inliers are now filtered to
    the biggest cluster present in the raw RANSAC inlier set*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the loop is over, you get a clean set of segments holding spatially contiguous
    point sets that follow planar shapes, that you can visualize with different colors
    using the following lines of code in the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The results should be something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e91a792806f2bc948d988ded50f423c9.png)![](../Images/111103f10b8da9050953ae4423df66d6.png)'
  prefs: []
  type: TYPE_IMG
- en: But is this the end? Noooo, never üòÑ! Once the multi-order RANSAC segmentation
    has been applied to the point cloud, the next stage involves refining the remaining
    non-segmented points through the utilization of DBSCAN, which aids in further
    enhancing the granularity of the point cloud analysis with additional clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ö **Note**: *Granularity is a useful academic word that is used a lot in data
    science. The granularity of data means the level of detail in how it is organized
    or modeled. Here, the smaller objects we can model from the point cloud, the finer
    granularity our representation has. (fancy, right?!)*'
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Euclidean Clustering Refinement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/664bd457a80f786f5d09aa8239aaeca7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Okay, time to evade the loop, and work on the remaining points assigned to
    the`rest` variable, that are not yet attributed to any segment. Let us first get
    a visual grasp on what we are talking about:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/76fdada74949c1d91e41829464bb87af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can apply a simple pass of Euclidean clustering with DBSCAN and capture
    the results in a `labels` variable. You know the drill:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: üå± **Growing:** *We use a radius of 10 cm for ‚Äúgrowing‚Äù clusters and consider
    one only if, after this step, we have at least 10 points. But is this the right
    choice?* ü§î*Feel free to experiment to find a good balance and, ideally, a way
    to automate this*üòÄ.
  prefs: []
  type: TYPE_NORMAL
- en: The labels vary between `-1` and `n`, where `-1` indicate it is a ‚Äúnoise‚Äù point
    and values `0` to `n` are then the cluster labels given to the corresponding point.
    Note that we want to get the labels as a NumPy array thereafter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3eb8a5f384d733521bb73466db0acea9.png)![](../Images/4f479ccf0472670ad4b5a84553201700.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the left: parameters are not well defined. On the right, we have a better
    delineation of objects for downward processes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nice. Now that we have groups of points defined with a label per point, let
    us color the results. This is optional, but it is handy for iterative processes
    to search for the right parameter‚Äôs values. To this end, we propose to use the
    Matplotlib library to get specific [color ranges](https://matplotlib.org/stable/tutorials/colors/colormaps.html),
    such as the tab20:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö **Note***: The* `max_label` *should be intuitive: it stores the maximal value
    in the labels list. This permits to use it as a denominator for the coloring scheme
    while treating with an ‚Äú*`if`*‚Äù statement the special case where the clustering
    is skewed and delivers only noise + one cluster. After, we make sure to set these
    noisy points with the label* `-1` *to black* (`0`). *Then, we give to the attribute*
    `colors` *of the point cloud* `pcd` *the 2D NumPy array of 3 ‚Äúcolumns‚Äù, representing
    R, G, B.*'
  prefs: []
  type: TYPE_NORMAL
- en: Et voil√†! I employ the same methodology as before, no sorcery! I just make sure
    to use coherent parameters to have a refined clustering to get the beautiful rainbow
    scene you always dreamed of ü•≥!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e7ebe11fc52adac83a2b93374501062.png)![](../Images/727dec15d27cdf31f1fcf08a86dd71b1.png)![](../Images/a4fd6c629333ca0ddd5943b6fd1c602b.png)'
  prefs: []
  type: TYPE_IMG
- en: After refining the point cloud clustering results using DBSCAN, the focus shifts
    to the voxelization technique, which involves organizing the data into a meaningful
    spatial structure, thereby enabling efficient modeling of the point cloud information.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Voxelization and Labelling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b83044b78cead7000efc9944696472ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have a point cloud with segment labels, it would be very interesting
    to see if we could fit indoor modeling workflows. One way to approach this is
    the use of voxels to accommodate for O-Space and R-Space. By dividing a point
    cloud into small cubes, it becomes easier to understand a model's occupied and
    empty spaces. Let us get into it.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1\. Voxel Grid Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating accurate and detailed 3D models of the space means generating a nice
    and tight voxel grid. This technique divides a point cloud into small cubes or
    voxels with their own coordinate system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ddeb653af9fa95611230f33d792e393.png)'
  prefs: []
  type: TYPE_IMG
- en: Voxel Grid Generation to structure the 3D Point Cloud with the segment information.
    ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'To create such a structure, we first define the size of our new entity: the
    voxel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we want to know how many of these cubes we must stack onto one another
    to fill the bounding box defined by our point cloud. This means that we have first
    to compute the extent of our point cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Great, now we use the `o3d.geometry.VoxelGrid.create_from_point_cloud()` function
    onto any point cloud of choice to fit a voxel grid on it. but wait. Which point
    cloud do we want to distinguish for further processes?
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, let us illustrate the case where you want to have voxels of ‚Äústructural‚Äù
    elements vs voxels of clutter that do not belong to structural elements. Without
    labeling, we could guide our choice based on whether or not they belong to RANSAC
    segments or other segments; This means first concatenating the segments from the
    RANSAC pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö **Note**: *At this stage, you have the ability to color the point clouds
    with a uniform color later picked up by the voxels. If this is something you would
    like, you can use:* `pcd_ransac.paint_uniform_color([1, 0, 0])`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can simply extract our voxel grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'and do the same thing for the remaining elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step is to visualize our result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a710cdc6698553309602a38d1349b361.png)'
  prefs: []
  type: TYPE_IMG
- en: A semantic representation of the space using voxel representation. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: This is awesome! It looks like one of those computer games where the whole world
    is constructed out of cubes! And we can actually use the segment labels to guide
    our voxel modeling! This opens up many perspectives! But an issue remains. With
    Open3D, it is hard to extract the voxels that are not filled; So, having achieved
    the structuration of the voxelized point cloud, the subsequent step involves exploring
    voxel space modeling techniques to provide alternative perspectives for analyzing
    the spatial relationships and properties of the voxelized data, opening up new
    avenues for advanced point cloud modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§† **Ville**: *Voxelization is a different spatial representation of the same
    point cloud data. Great for robots if they need to avoid collisions! And for great
    for simulating‚Ä¶ fire drills, for example!*'
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Spatial Modelling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/8f45765461494fbd3c658993a1b48144.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In indoor modeling applications, the voxel-based representation of point clouds
    plays a pivotal role in capturing and analyzing the geometric properties of complex
    environments. As the scale and complexity of point cloud datasets increase, it
    becomes essential to delve deeper into voxel segmentation techniques to extract
    meaningful structures and facilitate higher-level analysis. Let us thus define
    a function that fits a voxel grid and return both filled and empty spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Within the function, we will (1) Determine the minimum and maximum coordinates
    of the point cloud, (2) Calculate the dimensions of the voxel grid, (3) Create
    an empty voxel grid, (4) Calculate the indices of the occupied voxels and (5)
    Mark occupied voxels as True.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c8495062282952b988f871455aef9eb1.png)'
  prefs: []
  type: TYPE_IMG
- en: Algorithm workflow to create an occupancy grid from the point cloud. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'This translates into the following code that we want to have inside this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Our function is defined, and we can now use it to extract the voxels, segmented
    between structural, clutter, and empty ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö **Note**: *The* `*nonzero()*` *function from NumPy finds the indices of the
    nonzero elements in the* `*ransac_voxels*` *variable. The* `*nonzero()*` *function
    returns a tuple of arrays, where each array corresponds to the indices along a
    specific axis where the elements are nonzero. then we apply the* `*np.transpose()*`
    *NumPy function to the result obtained from* `*np.nonzero(ransac_voxels)*`*. The*
    `*transpose()*` *function finally permutes the axes of the array. (effectively
    swapping the rows with the columns). By combining these operations, the code line
    transposes the indices of the nonzero elements of* `*ransac_voxels*`*, resulting
    in a transposed array where each row represents the coordinates or indices of
    a nonzero element in the original* `*ransac_voxels*` *array*. üòä'
  prefs: []
  type: TYPE_NORMAL
- en: This voxel modeling approach offers valuable insights into the spatial relationships
    and properties of the voxelized data. To visualize the results as shown below
    outside of Python with transparency, we need to export our data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26688159b8fd845b7e77716526a41603.png)![](../Images/b5cf1042943b2491366570bf5785fea4.png)![](../Images/ac7654c650a1171f5b0bc0629506989d.png)'
  prefs: []
  type: TYPE_IMG
- en: Results of the Occupancy grid matching. On the left are both the filled voxels
    and empty voxels, in the middle are the filled voxels, and in the right are the
    empty voxels. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: After achieving the structuration of the voxelized point cloud, the next step
    involves exporting both the point cloud and voxel data to external formats, facilitating
    interoperability and enabling seamless integration with other software tools and
    workflows. This export process ensures that the structured voxel data and the
    original point cloud can be easily shared, visualized, or utilized for further
    analysis in various applications, fostering a collaborative and versatile approach
    to point cloud data utilization.
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Exporting 3D Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/61eb9b7f6baa27577836ecc3290f343b.png)'
  prefs: []
  type: TYPE_IMG
- en: Let us first focus on exporting the point cloud segmented datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1\. Segmented point cloud export
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To export the segmented point cloud, we must ensure that we can write the label
    per point within a readable ASCII file. To do this, we will create a list of XYZ
    segments to which we will append the label feature. This can be done with the
    following for loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, we want not to forget the remaining elements from the DBSCAN clustering,
    on which we apply the same principle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'finally, we append this to the segments list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'all that we have to do is then to use numpy to export the dataset and visualize
    it externally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9a6913b5b3158a617aa100ac50f293b6.png)![](../Images/3f317d5083906d7a4c6ef6e961bc04df.png)![](../Images/3d6ef345ce0c619c4c2e337ea05070c8.png)![](../Images/678224d1204a44582879919bd30270e1.png)![](../Images/a40abe9f177b691d00b9bc7991871a61.png)'
  prefs: []
  type: TYPE_IMG
- en: Following the successful export of the segmented point cloud dataset, the focus
    now shifts to exporting the voxel dataset as a `.obj` file.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2\. Voxel Model Export
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To export the voxels, we first have to generate a cube for each voxel; These
    cubes are then combined in a `voxel_assembly`, stacked together to generate the
    final file. We create the `voxel_modelling(filename, indices, voxel_size)` function
    to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö **Note**: *the function* `[cube()](https://drive.google.com/file/d/1kPu85YHl66gQH8Qumxlyd-Sp4PjgvBVm/view?usp=sharing)`
    *reads the provided indices, generates voxel cubes based on the indices and voxel
    size, writes the voxel cubes to a file, and keeps track of the assembled voxel
    cubes in the* `*voxel_assembly*` *list, which is ultimately returned by the function.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is then used to export three different voxel assemblies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2d627d5ec536ba3c638629a25ce4f5f5.png)![](../Images/7860e71bb83efd674302b5c044716254.png)![](../Images/442975beca59c66312b3cdbbcfc21310.png)'
  prefs: []
  type: TYPE_IMG
- en: The results of the Segmentation and voxel modeling. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: 'üíª Get Access to the Code here: [Code Samples](https://drive.google.com/drive/folders/1-sGlVvsPcyp9VZ8cw-J8kbhj4-aK_8p8?usp=sharing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'üçá Get Access to the Data here: [3D Datasets](https://drive.google.com/drive/folders/1sCBT1lc9A8Zn4grpxwFrBrvos86c0HZR?usp=sharing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'üë®‚Äçüè´3D Data Processing and AI Courses: [3D Academy](https://learngeodata.eu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'üìñ Subscribe to get early access to 3D Tutorials: [3D AI Automation](https://medium.com/@florentpoux/subscribe)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ceab061ce5f1ea698c109d269cc41fd2.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ü¶ä **Florent**: Massive congratulations üéâ! You just learned how to develop an
    automatic shape detection, clustering, voxelization, and indoor modeling program
    for 3D point clouds composed of millions of points with different strategies!
    Sincerely, well done! But the path certainly does not end here because you just
    unlocked a tremendous potential for intelligent processes that reason at a segment
    level!'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§† **Ville**: So, now you‚Äôre wondering if we can make 3D modeling a completely
    hands-off process? We‚Äôre halfway there with the technique we‚Äôve got. Why? Our
    technique can find parameters, for example, for the plane models. This is why
    the folks in academia call it parametric modeling. However, we still need to carefully
    choose some of the other parameters, such as the ones for RANSAC. I encourage
    you to experiment by applying your code on a different point cloud!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b6c35c1665bd5dbdfac5fc7031fb72a.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of 3D meshing based on the semantics of the ground. ¬© F. Poux
  prefs: []
  type: TYPE_NORMAL
- en: Going Further
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The learning journey does not end here. Our lifelong search begins, and future
    steps will dive into deepening 3D Voxel work, exploring semantics, and point cloud
    analysis with deep learning techniques. We will unlock advanced 3D LiDAR analytical
    workflows. A lot to be excited about!
  prefs: []
  type: TYPE_NORMAL
- en: '**Lehtola, V.**, Nikoohemat, S., & N√ºchter, A. (2020). Indoor 3D: Overview
    on scanning and reconstruction methods. Handbook of Big Geospatial Data, 55‚Äì97\.
    [https://doi.org/10.1007/978-3-030-55462-0_3](https://doi.org/10.1007/978-3-030-55462-0_3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Poux, F.**, & Billen, R. (2019). Voxel-based 3D point cloud semantic segmentation:
    unsupervised geometric and relationship featuring vs deep learning methods. *ISPRS
    International Journal of Geo-Information*. 8(5), 213; [https://doi.org/10.3390/ijgi8050213](https://doi.org/10.3390/ijgi8050213)
    ‚Äî Jack Dangermond Award ([Link to press coverage](https://www.geographie.uliege.be/cms/c_5724437/en/florent-poux-and-roland-billen-winners-of-the-2019-jack-dangermond-award))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bassier, M., Vergauwen, M., **Poux, F.**, (2020). Point Cloud vs. Mesh Features
    for Building Interior Classification. *Remote Sensing*. 12, 2224\. https://doi:10.3390/rs12142224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
