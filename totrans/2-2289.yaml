- en: Vector Search Is Not All You Need
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/vector-search-is-not-all-you-need-ecd0f16ad65e](https://towardsdatascience.com/vector-search-is-not-all-you-need-ecd0f16ad65e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alcarazanthony1?source=post_page-----ecd0f16ad65e--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----ecd0f16ad65e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ecd0f16ad65e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ecd0f16ad65e--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----ecd0f16ad65e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ecd0f16ad65e--------------------------------)
    ·6 min read·Sep 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Retrieval Augmented Generation (RAG) has revolutionized open-domain question
    answering, enabling systems to produce human-like responses to a wide array of
    queries. At the heart of RAG lies a retrieval module that scans a vast corpus
    to find relevant context passages, which are then processed by a neural generative
    module — often a pre-trained language model like GPT-3 — to formulate a final
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: While this approach has been highly effective, it’s not without its limitations.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most critical components, the vector search over embedded passages,
    has inherent constraints that can hamper the system’s ability to reason in a nuanced
    manner. This is particularly evident when questions require complex multi-hop
    reasoning across multiple documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vector search refers to searching for information using vector representations
    of data. It involves two key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoding data into vectors**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, the data being searched is encoded into numeric vector representations.
    For text data like passages or documents, this is done using embedding models
    like BERT or RoBERTa. These models convert text into dense vectors of continuous
    numbers that represent the semantic meaning. Images, audio, and other formats
    can also be encoded into vectors using appropriate deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. **Searching using vector similarity**
  prefs: []
  type: TYPE_NORMAL
- en: Once data is encoded into vectors, searching involves finding vectors similar
    to the vector representation of the search query. This relies on distance metrics
    like cosine similarity to quantify how close two vectors are and rank results.
    The vectors with the smallest distance (highest similarity) are returned as the
    most relevant search hits.
  prefs: []
  type: TYPE_NORMAL
- en: The key advantage of vector search is the ability to search for semantic similarity,
    not just literal keyword matches. The vector representations capture conceptual
    meaning, allowing more relevant yet linguistically distinct results to be identified.
    This enables a higher quality of search compared to traditional keyword matching.
  prefs: []
  type: TYPE_NORMAL
- en: However, transforming data into vectors and searching in high-dimensional semantic
    space also comes with limitations. Balancing the tradeoffs of vector search is
    an active area of research.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll dissect the limitations of vector search, exploring why
    it struggles to capture diverse relationships and intricate interconnections between
    documents. We’ll also delve into alternative techniques, such as knowledge graph
    prompting, that promise to overcome these shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the strengths and weaknesses of our current AI tools is essential
    as they become increasingly integrated into our lives. This article aims to provide
    a balanced view of where vector search shines and where it falls short in augmenting
    the reasoning capabilities of large language models.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Gap Between Questions and Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In vector search, both the input question and passages in the corpus are encoded
    as dense vector representations. Relevant context is retrieved by finding passages
    with the highest semantic similarity to the question vector.
  prefs: []
  type: TYPE_NORMAL
- en: However, questions often have an indirect relationship to the actual answers
    they seek.
  prefs: []
  type: TYPE_NORMAL
- en: The vector for “What is the capital of France?” may not necessarily have high
    similarity to a passage stating “Paris is the most populous city in France”.
  prefs: []
  type: TYPE_NORMAL
- en: This semantic gap means that passages containing the answer may be overlooked.
  prefs: []
  type: TYPE_NORMAL
- en: The embeddings fail to capture the inferential link between the question and
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: Passage Granularity Matters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In vector search systems, passages are typically represented by a single embedding
    vector. The granularity of these passages can vary.
  prefs: []
  type: TYPE_NORMAL
- en: If the passage is very large, like an entire document, it may encompass multiple
    concepts. Parts of the passage may be relevant, while other parts are not.
  prefs: []
  type: TYPE_NORMAL
- en: But with a single vector representing the entire passage, it is impossible to
    distinguish relevant sections from irrelevant ones. The passage as a whole may
    exhibit only weak similarity to the question vector.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, using sentence-level chunks can help isolate concepts. But this
    increases the number of vectors in the index, adding computational overhead.
  prefs: []
  type: TYPE_NORMAL
- en: There are inherent trade-offs between precision and tractability when choosing
    passage sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Struggle With Complex Reasoning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some questions demand synthesizing facts spread across multiple documents.
  prefs: []
  type: TYPE_NORMAL
- en: For example, “What is the earliest historical record of winemaking?” may require
    piecing together dates from various sources.
  prefs: []
  type: TYPE_NORMAL
- en: Vector search is ill-equipped for such multi-hop reasoning. Each passage is
    scored independently against the question. There is no mechanism to jointly analyze
    or connect information across separate results.
  prefs: []
  type: TYPE_NORMAL
- en: As questions get more complex, simple similarity search reaches its limits.
    The system struggles to collect and contextualize facts from different passages.
  prefs: []
  type: TYPE_NORMAL
- en: Black Box Model Workings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In standard vector search pipelines, it is opaque how initial retrieved passages
    are selected. The rankings depend on the inner workings of the semantic similarity
    model.
  prefs: []
  type: TYPE_NORMAL
- en: This lack of transparency makes results difficult to explain, verify, and improve.
    It also limits deployability for business-critical applications.
  prefs: []
  type: TYPE_NORMAL
- en: For increased oversight, the ranking algorithms should provide some interpretability
    into why certain passages are deemed relevant.
  prefs: []
  type: TYPE_NORMAL
- en: '**Modeling Diverse Relationships**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A core limitation of standard vector search is its singular focus on semantic
    similarity.
  prefs: []
  type: TYPE_NORMAL
- en: However, real-world reasoning requires modeling diverse relationships between
    content.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://blog.gopenai.com/knowledge-graph-prompting-a-new-approach-for-multi-document-question-answering-ab5c4006a429?source=post_page-----ecd0f16ad65e--------------------------------)
    [## Knowledge Graph Prompting: A New Approach for Multi-Document Question Answering'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-document question answering (MD-QA) involves answering questions that
    require synthesizing information across…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: blog.gopenai.com](https://blog.gopenai.com/knowledge-graph-prompting-a-new-approach-for-multi-document-question-answering-ab5c4006a429?source=post_page-----ecd0f16ad65e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowledge graph overcomes this by explicitly encoding various connections into
    an interconnected graph structure. Specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Topical relationships* — Passages are linked if they share rare or key keywords.
    This captures similarity in the topics discussed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Semantic relationships* — Passage embeddings are compared to connect those
    with semantic proximity, even if they do not share terms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This goes beyond surface-level topic matching.
  prefs: []
  type: TYPE_NORMAL
- en: '*Structural relationships* — Passages are connected to the specific sections,
    pages, or documents they appear in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This encodes the contextual hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: '*Temporal relationships* — Passages discussing time-ordered events are linked
    chronologically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This represents the flow of events.
  prefs: []
  type: TYPE_NORMAL
- en: '*Entity relationships* — Coreference links are added between passages referencing
    the same real-world entities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This allows entity-centric reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: By incorporating these diverse signals beyond just semantic similarity, KGP
    provides a richer substrate for reasoning about interconnected information.
  prefs: []
  type: TYPE_NORMAL
- en: '**Structural Relationships**'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, standard vector search lacks any notion of these structural relationships.
    Passages are treated atomically without any surrounding context.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graph’s modeling of structure relationships merits further discussion.
    By linking passages to the specific documents or sections they appear in, the
    contextual hierarchy of the information is encoded.
  prefs: []
  type: TYPE_NORMAL
- en: This enables explicitly reasoning about the section a certain fact is contained
    in, the document it originates from, and the site it was published on.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding the hierarchical document structure provides useful inductive biases
    for determining importance, validity, and relevance when reasoning across passages.
  prefs: []
  type: TYPE_NORMAL
- en: '**Temporal Relationships**'
  prefs: []
  type: TYPE_NORMAL
- en: This inductive bias is wholly absent in isolated vector search. Vector similarity
    scores do not factor temporal dynamics in any way. Retrieved passages are disconnected
    snapshots lacking narrative flow.
  prefs: []
  type: TYPE_NORMAL
- en: The explicit modeling of temporal relationships in KGP also provides significant
    advantages. Ordering passages chronologically based on the events they describe
    enables reasoning about unfolding narratives and timelines.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graph overcomes this limitation by chaining events based on their
    relative timing. This unlocks richer reasoning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '**Entity Relationships**'
  prefs: []
  type: TYPE_NORMAL
- en: With standard vector search, these entity links are not directly modeled. Valuable
    knowledge around entities is lost in the passage embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graph’s ability to connect entity references is a powerful asset.
    Linking passages that discuss the same real-world entities, concepts, or people
    allows focused reasoning around those shared elements.
  prefs: []
  type: TYPE_NORMAL
- en: KGP preserves this signal, enabling entity-centric exploration of the knowledge
    graph. This provides structural advantages when aggregating facts about specific
    entities across documents.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vector search enables efficient approximate matching based on semantic similarity.
    However, it has clear limitations when used in isolation for the retrieval step
    in RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: Employing hybrid approaches that combine vector search with graph-based knowledge
    representation, multi-step reasoning modules, and transparent ranking algorithms
    can help overcome these weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: As always, there is no single solution — leveraging a diverse toolkit of techniques
    is key to robust retrieval for real-world question answering.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45a8af157d31577aa016dfeff183c404.png)'
  prefs: []
  type: TYPE_IMG
- en: This image was created using an AI image generation model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sources :'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://medium.com/thirdai-blog/understanding-the-fundamental-limitations-of-vector-based-retrieval-for-building-llm-powered-48bb7b5a57b3](https://medium.com/thirdai-blog/understanding-the-fundamental-limitations-of-vector-based-retrieval-for-building-llm-powered-48bb7b5a57b3)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://labelbox.com/blog/how-vector-similarity-search-works/](https://labelbox.com/blog/how-vector-similarity-search-works/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.elastic.co/what-is/vector-search](https://www.elastic.co/what-is/vector-search)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://kaushikshakkari.medium.com/open-domain-question-answering-series-part-7-the-rise-of-vector-databases-in-the-world-of-9d848a3f47d5](https://kaushikshakkari.medium.com/open-domain-question-answering-series-part-7-the-rise-of-vector-databases-in-the-world-of-9d848a3f47d5)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://medium.com/@PolonioliAI/limitations-of-vectors-and-neural-search-4d81fd64482f](https://medium.com/@PolonioliAI/limitations-of-vectors-and-neural-search-4d81fd64482f)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://medium.com/vector-database/frustrated-with-new-data-our-vector-database-can-help-e5c430b29be7](https://medium.com/vector-database/frustrated-with-new-data-our-vector-database-can-help-e5c430b29be7)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.singlestore.com/blog/why-your-vector-database-should-not-be-a-vector-database/](https://www.singlestore.com/blog/why-your-vector-database-should-not-be-a-vector-database/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://clickhouse.com/blog/vector-search-clickhouse-p1](https://clickhouse.com/blog/vector-search-clickhouse-p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.searchenginejournal.com/semantic-search-with-vectors/467574/](https://www.searchenginejournal.com/semantic-search-with-vectors/467574/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.usenix.org/system/files/osdi23-zhang-qianxi_1.pdf](https://www.usenix.org/system/files/osdi23-zhang-qianxi_1.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.infoworld.com/article/3651360/solving-complex-problems-with-vector-databases.html](https://www.infoworld.com/article/3651360/solving-complex-problems-with-vector-databases.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://people.eecs.berkeley.edu/~matei/papers/2020/sigir_colbert.pdf](https://people.eecs.berkeley.edu/~matei/papers/2020/sigir_colbert.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://blog.futuresmart.ai/gpt-4-semantic-search-and-vector-databases-revolutionizing-question-answering](https://blog.futuresmart.ai/gpt-4-semantic-search-and-vector-databases-revolutionizing-question-answering)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://blog.vespa.ai/constrained-approximate-nearest-neighbor-search/](https://blog.vespa.ai/constrained-approximate-nearest-neighbor-search/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.pinecone.io/learn/vector-search-filtering/](https://www.pinecone.io/learn/vector-search-filtering/)'
  prefs: []
  type: TYPE_NORMAL
