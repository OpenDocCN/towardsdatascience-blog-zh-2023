["```py\nmodel = LinearRegressor()\nmodel.fit(X_train, y_train)\ny_predicted = model.predict(X_test)\ntest_score = some_score_function(y_predicted, y_test)\n```", "```py\n# the model will computed the predicted y-value from X_test, \n# and compare it to y_test with a score function\ntest_score = model.score(X_test, y_test)\ntrain_score = model.score(X_train, y_train)\n```", "```py\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# See my previous post for why we split the input data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# create and train a linear regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# Compute predicted values\ny_pred = lr.predict(X_test)\n\nprint(\"Model Score (R-squared):\",\n      lr.score(X_test, y_test)) # use the .score method of the model\nprint(\"R-squared Score:\", \n      r2_score(y_test, y_pred)) # use the the same function, but from sklearn.metrics\n\n# use other score functions\nprint(\"Mean Absolute Error:\",\n      mean_absolute_error(y_test, y_pred))\nprint(\"Mean Squared Error:\",\n       mean_squared_error(y_test, y_pred))\n```", "```py\nModel Score (R-squared): 0.8072059636181392\nR-squared Score: 0.8072059636181392\nMean Absolute Error: 0.5913425779189776\nMean Squared Error: 0.6536995137170021\n```", "```py\n%matplotlib qt\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\ndef truth(X):\n    return 0.5 * X**2 + X + 2 \n\nX = 6 * np.random.rand(100, 1) - 3\ny = truth(X) + np.random.randn(100, 1)*2\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\ndef fitted_model(degree):\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(X_train, y_train)\n    return model\n\nfig, axes = plt.subplots(1, 5, sharex=True, sharey=True)\nxs = np.linspace(X.min(), X.max())\n\ndegrees = [0, 1, 2, 10, 25]\n\nfor deg, ax in zip(degrees, axes):\n\n    model = fitted_model(deg)\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    y_train_score = model.score(X_train, y_train)\n    y_test_score = model.score(X_test, y_test)\n\n    ax.plot(xs, truth(xs), '--', alpha=0.2)\n    ax.scatter(X, y, alpha=0.5, color=\"gray\")\n    ax.scatter(X_train, y_train_pred, label=f\"train set\", alpha=0.5)\n    ax.scatter(X_test, y_test_pred, label=f\"test set\", alpha=0.5)\n    ax.set_xlabel(f\"train={y_train_score:.2f}/test={y_test_score:.2f}\")\n    ax.legend()\n    ax.set_title(f\"degree={deg}\")\nfig.tight_layout()\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, validation_curve\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import ValidationCurveDisplay\n\ndef truth(X):\n    return 0.5 * X**2 + X + 2 \n\nX = 6 * np.random.rand(100, 1) - 3\ny = truth(X) + np.random.randn(100, 1) * 2\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\ndegrees = np.arange(0, 12, 1)\n\n# Plotting the validation curve\nValidationCurveDisplay.from_estimator(\n    make_pipeline(PolynomialFeatures(), LinearRegression()),\n    X, y,\n    param_name='polynomialfeatures__degree',\n    param_range=degrees,\n)\nplt.xlabel('Degree of Polynomial Features')\nplt.ylabel('Score')\nplt.title('Validation Curve for polynomial fit')\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import ValidationCurveDisplay, LearningCurveDisplay\n\ndef truth(X):\n    return 0.5 * X**2 + X + 2 \n\nN = 1000\nX = 6 * np.random.rand(N, 1) - 3\ny = truth(X) + np.random.randn(N, 1) * 2\n\nLearningCurveDisplay.from_estimator(\n    make_pipeline(PolynomialFeatures(2), LinearRegression()),\n    X, y,\n    train_sizes=np.logspace(-2, 0, 20), \n)\nplt.xlabel('# of samples')\nplt.ylabel('Score')\nplt.title('Learning Curve for polynomial fit')\n```", "```py\n%matplotlib qt\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndegree_low = 2\ndegree_high = 15\ntrue_coefs = np.zeros(degree_high)\n\ntrue_coefs[4] = 0.5\ntrue_coefs[1] = 1\ntrue_coefs[0] = 2\n\nN = 100\nX = 6 * np.random.rand(N, 1) - 3\ny = 0.5 * X**3 + X + 2 + np.random.randn(N, 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\ndef compute_results(X_train, X_test, y_train):\n    model_underfit = make_pipeline(PolynomialFeatures(degree=degree_low), LinearRegression())\n    model_underfit.fit(X_train, y_train)\n    y_pred_train_underfit = model_underfit.predict(X_train)\n    y_pred_test_underfit = model_underfit.predict(X_test)\n    model_overfit = make_pipeline(PolynomialFeatures(degree=degree_high), LinearRegression())\n    model_overfit.fit(X_train, y_train)\n    y_pred_train_overfit = model_overfit.predict(X_train)\n    y_pred_test_overfit = model_overfit.predict(X_test)\n    return model_underfit, model_overfit, y_pred_train_underfit, y_pred_test_underfit, y_pred_train_overfit, y_pred_test_overfit\n\n# Plot the results\nfig, axes = plt.subplots(3, 3, figsize=(12, 8))\naxes[0,0].set_title(\"RAW input data\")\naxes[0,0].set_ylabel('First split')\naxes[2,0].set_ylabel('Second split')\naxes[1,0].set_ylabel('Poly. coefs.')\naxes[0,0].set_title(\"Input data\")\naxes[0,1].set_title(\"Low-degree, biased model (degree=1)\")\naxes[0,2].set_title(\"High-degree, high variance (degree=15)\")\naxes[1,0].bar(np.arange(len(true_coefs)), true_coefs, alpha=0.3, label=\"True coefs\")\n\nmodel_underfit, model_overfit, y_pred_train_underfit, y_pred_test_underfit, y_pred_train_overfit, y_pred_test_overfit = compute_results(X_train, X_test, y_train)\n\naxes[0,0].scatter(X_train, y_train, label='Training Data', color='red', alpha=0.7)\naxes[0,0].scatter(X_test,  y_test, label='Test Data', color='blue', alpha=0.7)\naxes[0,1].scatter(X, y, color=\"gray\", alpha=0.2)\naxes[0,1].scatter(X_train, y_pred_train_underfit, color='red', label='Train', alpha=0.7)\naxes[0,1].scatter(X_test,  y_pred_test_underfit, color='blue', label='Test', alpha=0.7)\naxes[0,2].scatter(X, y, color=\"gray\", alpha=0.2)\naxes[0,2].scatter(X_train, y_pred_train_overfit, color='red', label='Train', alpha=0.7)\naxes[0,2].scatter(X_test,  y_pred_test_overfit, color='blue', label='Test', alpha=0.7)\naxes[1,1].bar(np.arange(degree_low+1), model_underfit.named_steps['linearregression'].coef_.flatten(),  label=\"1st split\", alpha=0.3)\naxes[1,2].bar(np.arange(degree_high+1), model_overfit.named_steps['linearregression'].coef_.flatten(),  label=\"1st split\", alpha=0.3)\naxes[1,0].set_xticks(np.arange(len(true_coefs))); axes[1,0].set_xlim(-1, 15); axes[1,0].set_ylim(-5, 5)\naxes[1,1].set_xticks(np.arange(len(true_coefs))); axes[1,1].set_xlim(-1, 15); axes[1,1].set_ylim(-5, 5)\naxes[1,2].set_xticks(np.arange(len(true_coefs))); axes[1,2].set_xlim(-1, 15); axes[1,2].set_ylim(-5, 5)\naxes[0,1].set_xlabel(f'Train score={model_underfit.score(X_train, y_train):.2f} / Test score={model_underfit.score(X_test, y_test):.2f}')\naxes[0,2].set_xlabel(f'Train score={model_underfit.score(X_train, y_train):.2f} / Test score={model_underfit.score(X_test, y_test):.2f}')\n\n# Switch train and test sets...\nX_train, X_test, y_train, y_test = X_test, X_train, y_test, y_train \n\n# ... and start over\nmodel_underfit, model_overfit, y_pred_train_underfit, y_pred_test_underfit, y_pred_train_overfit, y_pred_test_overfit = compute_results(X_train, X_test, y_train)\n\naxes[2,0].scatter(X_train, y_train, label='Training Data', color='red', alpha=0.7)\naxes[2,0].scatter(X_test,  y_test, label='Test Data', color='blue', alpha=0.7)\naxes[2,1].scatter(X, y, color=\"gray\", alpha=0.2)\naxes[2,1].scatter(X_train, y_pred_train_underfit,  color='red', label='Train', alpha=0.7)\naxes[2,1].scatter(X_test,  y_pred_test_underfit, color='blue', label='Test', alpha=0.7)\naxes[2,2].scatter(X, y, color=\"gray\", alpha=0.2)\naxes[2,2].scatter(X_train, y_pred_train_overfit, color='red', label='Train', alpha=0.7)\naxes[2,2].scatter(X_test,  y_pred_test_overfit, color='blue', label='Test', alpha=0.7)\naxes[1,1].bar(np.arange(degree_low+1), model_underfit.named_steps['linearregression'].coef_.flatten(),  label=\"2nd split\", alpha=0.3)\naxes[1,2].bar(np.arange(degree_high+1), model_overfit.named_steps['linearregression'].coef_.flatten(),  label=\"2nd split\", alpha=0.3)\naxes[1,0].set_xticks(np.arange(len(true_coefs))); axes[1,0].set_xlim(-1, 15); axes[1,0].set_ylim(-5, 5)\naxes[1,1].set_xticks(np.arange(len(true_coefs))); axes[1,1].set_xlim(-1, 15); axes[1,1].set_ylim(-5, 5)\naxes[1,2].set_xticks(np.arange(len(true_coefs))); axes[1,2].set_xlim(-1, 15); axes[1,2].set_ylim(-5, 5)\naxes[2,1].set_xlabel(f'Train score={model_underfit.score(X_train, y_train):.2f} / Test score={model_underfit.score(X_test, y_test):.2f}')\naxes[2,2].set_xlabel(f'Train score={model_underfit.score(X_train, y_train):.2f} / Test score={model_underfit.score(X_test, y_test):.2f}')\n\nfig.suptitle('Comparison for 2 splits of low/high degree polynomial models')\nfor ax in axes.flatten(): ax.legend()\n\nplt.tight_layout()\n```"]