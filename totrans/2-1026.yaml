- en: Hands-On Deep Q-Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hands-on-deep-q-learning-9073040ce841](https://towardsdatascience.com/hands-on-deep-q-learning-9073040ce841)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Reinforcement Learning](https://medium.com/tag/reinforcement-learning)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Level up your agent to win more difficult games!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----9073040ce841--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----9073040ce841--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9073040ce841--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9073040ce841--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page-----9073040ce841--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9073040ce841--------------------------------)
    ·14 min read·Nov 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8bdefc9ca92e8dc70018967dbcd36267.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sean Stratton](https://unsplash.com/@seanstratton?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning is one of the most fascinating fields of machine learning.
    Unlike supervised learning, reinforcement learning models can learn complex processes
    independently, even without beautifully tabulated data.
  prefs: []
  type: TYPE_NORMAL
- en: For me, it is most fun to see AI agents win video games, but you can also use
    reinforcement learning to solve business problems. Just phrase it as a game, and
    off you go! You only have to define…
  prefs: []
  type: TYPE_NORMAL
- en: the environment your agent lives in,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: what decisions your agent can take, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: what success and failure look like.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/86ef974389efe4be3138d88a39347db3.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of an AI agent mastering a game. Pick up a customer and bring them to
    the hotel. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Before you continue, please read my introductory article about reinforcement
    learning. It gives you some more context and shows you how to conduct a simple,
    yet effective form of reinforcement learning yourself. It also serves as a basis
    for this article.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-practitioners-guide-to-reinforcement-learning-1f1e249f8fa5?source=post_page-----9073040ce841--------------------------------)
    [## A Practitioner’s Guide to Reinforcement Learning'
  prefs: []
  type: TYPE_NORMAL
- en: Take your first steps in writing game-winning AI agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-practitioners-guide-to-reinforcement-learning-1f1e249f8fa5?source=post_page-----9073040ce841--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In this article, you will learn about deep Q-learning, why we need it, and how
    to implement it yourself to master a game that looks much more difficult than
    the ones in my other article.
  prefs: []
  type: TYPE_NORMAL
- en: '**You can find the code in** [**my Github**](https://github.com/Garve/towards_data_science/blob/main/Hands-On%20Deep%20Q-Learning/Hands-On%20Deep%20Q-Learning.ipynb)**.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Large Observation Spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the article linked above, we conducted Q-learning to make an agent play some
    simple games with small **discrete observation spaces**. In the Frozen Lake game,
    as an example, you have 16 fields (=states or observations, I use these terms
    interchangeably from now on.) you can stand on in the 4x4 map. In the [gymnasium
    version of the card game Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/),
    there are 32 · 11 · 2 = 704 states.
  prefs: []
  type: TYPE_NORMAL
- en: Q-Learning inefficiencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In simple games such as the ones I mentioned, Q-learning works very well since
    the Q-tables stay quite small. However, bigger tables mean more work for the algorithm,
    hence Q-learning becomes more inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: This typically happens when you have **too many observations** rather than too
    many actions because the action space is usually tiny compared to the observation
    space. In the Blackjack environment, you only have 2 actions but 704 states, for
    example.
  prefs: []
  type: TYPE_NORMAL
- en: The cart pole game
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Or even worse: imagine that your observation space is **continuous**, such
    as in the [cart pole game](https://gymnasium.farama.org/environments/classic_control/cart_pole/)
    that gymnasium offers. In this game, the agent has to learn how to balance a pole
    by steering a cart left or right.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbcfb671e985c4768fe4f3d0d861e5db.png)'
  prefs: []
  type: TYPE_IMG
- en: Balance that pole. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: While the action space of this game only consists of **two actions** — going
    left or right — the observation space consists of
  prefs: []
  type: TYPE_NORMAL
- en: the position of the car,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: its velocity,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the pole angle and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the pole angular velocity,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: each of them being **real numbers**. This means that your observation space
    is **infinite**, and creating a Q-table for it… is challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Discretized Q-learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a workaround, you could **discretize** the space, i.e., slicing it into a
    **finite number** of buckets and mapping each continuous state to one of these
    buckets. Then you do normal Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d71791766cfccb16dfea9deccaaec24.png)'
  prefs: []
  type: TYPE_IMG
- en: Four buckets for the cart position, not a political spectrum. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: This poses the problem that you have to decide how many buckets you want to
    use. If you use too many, the Q-table will be too large again. If there are too
    few, very different states will be treated equally, which can lead to poor agent
    performance. This might happen in the image above, where all cart positions between
    0 and 3 are treated the same, for example. Being in position 0.1 is the same as
    being int he position 2.9, as far as the model is concerned.
  prefs: []
  type: TYPE_NORMAL
- en: We might still try out this approach in another article, but be aware that it
    is basically the Q-learning that we know with an extra discretization step for
    the observations. However, in this article, we want to apply **deep Q-learning**
    that can **deal with continuous observation spaces** out of the box!
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before, we reserved a cell for each Q-value *Q*(*s*, *a*) for each state *s*
    and action *a* in the Q-table. If these tables get too big, let us try a different
    strategy: **modeling the Q-values via functions**!'
  prefs: []
  type: TYPE_NORMAL
- en: For a given state *s* and action *a*, we would like an output that is (close
    to) the Q-value. Since we can’t make up the function on our own — at least I can’t
    — let us use a neural network with learnable weights as an approximation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ece725a20c5708d290889e20dab7fed1.png)'
  prefs: []
  type: TYPE_IMG
- en: Q-learning and deep Q-learning compared. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: This is nothing that I have made up, it was described as early as 1993 under
    the name **QCON** (connectionist Q-learning)in the dissertation “[Reinforcement
    Learning for Robots Using Neural Networks](https://isl.anthropomatik.kit.edu/pdf/Lin1993.pdf)”
    by Long-Ji Lin. *Happy 30th anniversary!*
  prefs: []
  type: TYPE_NORMAL
- en: The author also introduces the concept of **experience replay** in its current
    form that is used in the famous DeepMind paper [Playing Atari with Deep Reinforcement
    Learning](https://arxiv.org/abs/1312.5602).
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** You might wonder why the network doesn’t use the state *s* and action
    *a* as input since this would be the straightforward way to replace the table
    with a model. Input the state and action, receive a Q-value. The DeepMind paper
    authors say the following:'
  prefs: []
  type: TYPE_NORMAL
- en: There are several possible ways of parameterizing Q using a neural network.
    Since Q maps history-action pairs to scalar estimates of their Q-value, the history
    and the action have been used as inputs to the neural network by some previous
    approaches […]. The main drawback of this type of architecture is that a separate
    forward pass is required to compute the Q-value of each action, resulting in a
    cost that scales linearly with the number of actions. […] The main advantage of
    [our] type of architecture is the ability to compute Q-values for all possible
    actions in a given state with only a single forward pass through the network.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Intuition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In my other article, I told you that the updates in the Q-table are done via
    the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14044cb86ca0795fd4f22516a755c5d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In words, it means that we shift our old value *Q*(*s*, *a*) slightly into the
    direction of the target value *R*(*s*, *a*) + *γ*·max*ₐ* *Q*(*s*’, *a*), where
    *s* is a state, and *s’* the next state after taking action *a*, *R* is the reward
    function, and *γ* < 1 is a discount factor. *α* is the learning rate that tells
    us how far we shift our old Q-value *Q*(*s*, *a*) into the direction *R*(*s*,
    *a*) + *γ*·max*ₐ* *Q*(*s*’, *a*).
  prefs: []
  type: TYPE_NORMAL
- en: '**Small example.** Let us assume that *Q*(*s*, *a*) = 3, *R*(*s*, *a*) + *γ*·max*ₐ*
    *Q*(*s*’, *a*) = 4, and *α =* 0.1*.* Then our old value of 3 would get updated
    to 3 + 0.1·(4–3) = 3.1, a little bit further away from 3 into the direction of
    4.'
  prefs: []
  type: TYPE_NORMAL
- en: This means that this update rule changes the entries step by step, such that
    at some point we have *Q*(*s*, *a*) = *R*(*s*, *a*) + *γ*·max*ₐ* *Q*(*s*’, *a*),
    which is also called the **Bellman equation**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The brilliant observation is that we can translate this goal into a simple
    machine learning task for our neural network. So, let us assume that we do a step
    in our environment. We start in state *s,* do some action *a,* and end up in a
    new state *s’.* We also got a reward *r* = *R*(*s*, *a*) from the environment.
    Then we proceed with a training step as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute *N*(*s*) and *N*(*s*’), which are both vectors of real numbers, one
    for each action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check out the *a*-th action of *N*(*s*), let’s call it *N*(*s*)[*a*].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure that *N*(*s*)[*a*] gets closer to *R*(*s*, *a*) + *γ*·max*ₐ* *N*(*s*’)
    by performing a gradient update step minimizing the mean squared error, which
    means (*R*(*s*, *a*) + *γ*·max*ₐ* *N*(*s*’) - *Q*(*s*, *a*))².
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can then take another step in the environment, get new data (*s*, *a*, *s’*,
    *r*) (**read:** the agent was in state *s*, did *a*, ended up in *s’,* and got
    reward *r*), and do steps 1–3 all over again. We do this until we have an agent
    that is good enough, or we run out of patience or money to train.
  prefs: []
  type: TYPE_NORMAL
- en: Experience replay
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The intuition from above has a flaw: *it does not work very well like that*.
    When I first started, I implemented this logic and never got any good model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason was that I updated the model with only a **single training example**
    each time. This is like performing pure stochastic gradient descent. It might
    work, but during optimization, the model parameters jump around a lot and it might
    be hard for them to converge. Another problem is that **subsequent actions are
    highly correlated** since the state usually does not change much with a single
    action. Nowadays, we know an easy way to fix both problems: **use mini-batches**!'
  prefs: []
  type: TYPE_NORMAL
- en: The fancy term *experience replay* is not much more than that.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You don’t do a single step in the environment, but several ones, and you store
    the **memories** (*s*, *a*, *s’*, *r*) away into some data structure, the **replay
    memory**. It can just be an array, but usually, people use [deques](https://en.wikipedia.org/wiki/Double-ended_queue)
    (double-ended queues) to realize a **limited** replay memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f5ea3399d5de8e8fd65dbf6b49c0948.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Newer memories kick older ones out, once the size limit of the replay memory
    is reached. This makes sense since very old memories shouldn’t have much impact
    on what happens now anymore. This also enables the model to adapt to new rules
    in the game.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3ed1f5030d692784cf9b2127c71644b.png)'
  prefs: []
  type: TYPE_IMG
- en: 7 kicks out 1\. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Pseudocode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alright, with this knowledge, let us take a look at the pseudocode from the
    Playing Atari with Deep Reinforcement Learning paper. The only bits that differ
    are that they potentially preprocess the state *s* with some function φ (which
    we don’t do here) and that they remove the max term when the agent reaches a terminal
    state, i.e., the game ends.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebad2f580c430a3c52d916896a03cf7d.png)'
  prefs: []
  type: TYPE_IMG
- en: From their paper.
  prefs: []
  type: TYPE_NORMAL
- en: We can now implement this idea in Python!
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let us import some libraries and define some parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Nothing wild going on here, but please read the comments if something is unclear.
  prefs: []
  type: TYPE_NORMAL
- en: Replay memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we define the replay, which is only a thin wrapper around a Python deque.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, we defined the memory replay in a way that is easy to use. Let’s play
    around with it before we use it in action. We can do
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If we add another memory, we will lose the first one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us define a simple model! We will use TensorFlow here, but feel free to
    use PyTorch, [JAX](https://github.com/google/jax), or whatever.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With these preparations, let us conduct the training now.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! It kind of looks like the code for Q-learning, but the simple
    update of a cell in the table got replaced with one step of gradient descent in
    the form of a single `fit` .
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code might run for some hours. You can also log some data during training,
    such as the reward per episode. Another common practice is to **log the model
    every few episodes,** so you don’t lose too work much when the training crashes.
    However, saving models has another benefit: **very often, the best model is not
    the latest one**. You can see this when you take a look at my reward plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80e4eb25687823fc3ae665262219405a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: You can see that at first, the agent was quite bad. Around episode 100, the
    agent got rewards of around 200 already before it dropped again. Around episode
    230, the model became exceptional, just to dumb down again until episode 1000,
    where I ended the training. It is weird, but a common pattern that I also see
    when other people do reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s play!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What could be better than seeing the model we have trained so painstakingly
    in action? Right, *nothing*, so let’s get started! First, let us see how a random
    agent competes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55259aa459a1311b4c538892d2a7bf11.png)![](../Images/63493a6d3a03e735fd789209597a15b4.png)![](../Images/e72ccb63fbe8908731feb854a5042f52.png)'
  prefs: []
  type: TYPE_IMG
- en: Three different runs of a random agent. White flashes indicate a game over.
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Wow, the random agent **really** sucks. The run in the middle looked okay in
    the beginning, but then the randomness kicked it and prevented the agent from
    going left. You can recreate sad runs like this via
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: But you are here for something else, right?
  prefs: []
  type: TYPE_NORMAL
- en: A new challenger appears
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us load the model of episode 230 to see how it performs in a live test!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ebcf2a47b21c58dc85e62200ea844867.png)'
  prefs: []
  type: TYPE_IMG
- en: Smooth moves by our agent of episode 230\. After 500 timesteps, I cut the animation,
    but the agent did not fail. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, look at that! The agent can effortlessly balance the pole with minimal
    movement. However, you can see that it is slightly drifting right, which *might*
    be a problem in a long run. Let us increase the artificial restriction of 500
    time steps to several thousands and look at a sped-up version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e95b77d67a30262f38b36642ec0158a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Agent 230 is at it again in 60fps, gamer style. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: It seems like the agent is doing fine, even after 500 steps. Awesome!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have explained why vanilla Q-learning fails when dealing
    with large or even potentially infinite observation spaces. Discretized Q-learning
    can help in this case, but you have to think about how much you discretize to
    find a balance between the agent’s performance and training time.
  prefs: []
  type: TYPE_NORMAL
- en: The discretized Q-learning approach is perfectly fine, but we went for another
    technique that was successfully applied to learn more complex games — **deep Q-learning**.
    Instead of updating entries of a potentially large Q-table, we train a model with
    a **fixed amount of parameters**.
  prefs: []
  type: TYPE_NORMAL
- en: '*Let us be honest though*. Deep Q-learning is *also* a trade-off between training
    time and agent performance. A small network such as linear regression (no hidden
    layers) will probably perform horribly. If you have millions of parameters for
    the cart pole game, it is overkill, and training takes ages.'
  prefs: []
  type: TYPE_NORMAL
- en: You have to find a balance as well, it doesn’t come for free.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, judging from the papers, it seems that deep Q-learning is still more
    promising than discretized Q-learning for more complex games such as [Atari games](https://gymnasium.farama.org/environments/atari/).
  prefs: []
  type: TYPE_NORMAL
- en: Learning from sensory input
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have used the **internal state** of the environment. In the cart
    pole game, we were given the position of the car, the speed, angles, and so on.
    When we, as humans, play games, usually **we do not have that luxury**. I mean,
    we couldn’t process this information fast enough anyway to be good at the game.
    We have to rely on visual feedback — the screen — to make quick decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if the agent could only use the screen output as well? As the authors
    from the Playing Atari with Deep Reinforcement Learning paper show, it works quite
    well! As they write:'
  prefs: []
  type: TYPE_NORMAL
- en: We apply our method to seven Atari 2600 games from the Arcade Learning Environment,
    with no adjustment of the architecture or learning algorithm. We find that it
    outperforms all previous approaches on six of the games and surpasses a human
    expert on three of them.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The authors use a deep convolutional neural network that takes the screen, processes
    the image it sees, and outputs the Q-values. To be precise, they **don’t use a
    single frame**, but the **last 4 frames** of the game. This allows the model to
    learn the movement speed and directions of the objects on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: But I will leave it like this for now, we can dive into this topic with a fresh
    mind in another article!
  prefs: []
  type: TYPE_NORMAL
- en: I hope that you learned something new, interesting, and valuable today. Thanks
    for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '*If you have any questions, write me on* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*!*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And if you want to dive deeper into the world of algorithms, give my new publication
    All About Algorithms a try! I’m still searching for writers!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://allaboutalgorithms.com/?source=post_page-----9073040ce841--------------------------------)
    [## All About Algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: From intuitive explanations to in-depth analysis, algorithms come to life with
    examples, code, and awesome…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: allaboutalgorithms.com](https://allaboutalgorithms.com/?source=post_page-----9073040ce841--------------------------------)
  prefs: []
  type: TYPE_NORMAL
