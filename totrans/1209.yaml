- en: How to Leverage Pre-Trained Transformer Models for Custom Text Categorisation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/how-to-leverage-pre-trained-transformer-models-for-custom-text-categorisation-3757c517bd65](https://towardsdatascience.com/how-to-leverage-pre-trained-transformer-models-for-custom-text-categorisation-3757c517bd65)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, you have some custom text dataset that you wish to categorise, but wondering
    how? Well, let me show you how, using pre-trained state of the art language models.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)[![Saed
    Hussain](../Images/cb8d313c1c1a15fd1632979dc3c080a7.png)](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)
    [Saed Hussain](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)
    ¬∑11 min read¬∑Mar 31, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/948131b577b7ca3d0260d3ee6b5e9654.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Meagan Carsience](https://unsplash.com/@mcarsience_photography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Ok let‚Äôs get straight to the point! You have some custom data and now you want
    to categorise it into your custom classes. In this article, I will show you how
    you can use 2 approaches to achieve this objective. Both of them utilise pre-trained
    state of the art transformed based models.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the goal of this article is to share with you the approaches
    and how to use them. This is in not a complete data science tutorial with best
    practices. Unfortunately that is outside the scope of this article.
  prefs: []
  type: TYPE_NORMAL
- en: All the code from this article can be found in this [GitHub repo](https://github.com/saedhussain/medium/tree/main/text_category/notebooks).
  prefs: []
  type: TYPE_NORMAL
- en: '1: Zero Shot Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Zero-shot classification is a technique that allows you to classify text into
    categories without training a specific model for that task. Instead, it uses pre-trained
    models that have been trained on a large amount of data to perform this classification.
    The models are typically trained on a variety of tasks, including language modelling,
    text completion, and text entailment, among others.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af18cccb1f07546070ca552ef5d34bde.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Zero-shot text classification using pre-trained LLMs (source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: To perform zero-shot classification, you simply need to provide the pre-trained
    model with some text and a list of possible categories.
  prefs: []
  type: TYPE_NORMAL
- en: The model will then use its understanding of language and its pre-existing knowledge
    to classify the text into one of the provided categories. This approach is particularly
    useful when you have limited data available for a specific classification task,
    as it allows you to leverage the pre-existing knowledge of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Since it does the classification without any training on that particular task,
    it‚Äôs know as zero-shot.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All we need to implement this is to install the [hugging face transformers library](https://github.com/huggingface/transformers)
    using `pip install transformers` . We will use the pre-trained Facebook BART (Bidirectional
    and Auto-Regressive Transformers) model for this task.
  prefs: []
  type: TYPE_NORMAL
- en: '***Side Note****: on first use, it will take some time to download the model.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is a dictionary with 3 keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '**sequence**: input text that was classified by the pipeline'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels**: list of candidate (category) labels provided to the pipeline, ordered
    based on their probability scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scores**: probabilities scores assigned to each candidate label based on
    the model‚Äôs prediction of how likely the input text belongs to that label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, without any training, the model has correctly classified the
    given text into the ‚Äúgroceries‚Äù category. Because the model was trained on a large
    corpus of text in a given language, it can understand that language and draw inference.
    it understood the text and identified a suitable category from the list of candidate
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, it‚Äôs brilliant!! üòä
  prefs: []
  type: TYPE_NORMAL
- en: The larger the model is, the better it is at zero-shot classification tasks.
    For more info, have a look at the [zero-shot classification page](https://huggingface.co/tasks/zero-shot-classification)
    on hugging face.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ö°Ô∏è Checkout this [notebook](https://github.com/saedhussain/medium/blob/48db9652f83cbb83b547a0a55ff5bfb8355e0d26/text_category/notebooks/zero_shot_classification.ipynb)
    for more examples.
  prefs: []
  type: TYPE_NORMAL
- en: When to use it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given its extraordinary performance without any training, I highly recommend
    trying this first if you know the descriptions of the categories. It leverages
    the state of the art pre-trained models and gives excellent results without any
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a non exhaustive list of when this can be your go to approach:'
  prefs: []
  type: TYPE_NORMAL
- en: When you have limited labeled training data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you need to quickly prototype a solution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you need to classify new or rare categories
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you need to classify instances into multiple categories
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you want to leverage pre-trained models to classify instances without additional
    training
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you want to classify instances into categories that are defined by natural
    language descriptions rather than predefined labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the limitations?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some potential drawbacks and limitations to consider when using the
    zero-shot classification approach:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Limited training data augmentation: In zero-shot classification, there is limited
    scope for augmenting the training data to improve model performance, unlike in
    traditional supervised learning approaches.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Limited control over model behaviour: Zero-shot classification relies on a
    pre-trained model, which means that you have limited control over its behaviour
    and the patterns it has learned. This can lead to unexpected results, especially
    if the model was not trained on data similar to your task.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Limited customisation: Because zero-shot classification relies on a pre-trained
    model, there is limited scope for customisation or fine-tuning to the specific
    nuances of your task. This can limit the accuracy and performance of the model,
    especially if the task involves complex or domain-specific language.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'üöÄ Pro Tip: Always Clean Your Text!!'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although transformer-based LLMs are significantly better at handling noisy text
    compared to other models, it is still highly recommended to clean the text before
    feeding it into the model.
  prefs: []
  type: TYPE_NORMAL
- en: Not only is it a good data science practice, but it also makes a huge difference.
    The embeddings generated with noisy text data might have lower similarity scores
    to the correct category when compared against a clean text input. This undoubtably
    will reduce the category classification scores.
  prefs: []
  type: TYPE_NORMAL
- en: For example, below is a comparison of categorising this text ‚Äî ‚Äú*Tesco Semi
    Skimmed Milk 1.13L/2 Pints ‚Ä¶‚Ä¶ ¬£1.30*‚Äù, before and after removing any special characters
    and numbers. Why not try it out on the [Hugging Face zero-shot classification](https://huggingface.co/tasks/zero-shot-classification)
    page?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/233b993b53778d7ee285df8bb794c1b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Text categorisation probability comparison, using zero-shot method before and
    after cleaning text. (source:author)
  prefs: []
  type: TYPE_NORMAL
- en: ‚òïÔ∏è Let‚Äôs take a break before we go any further‚Ä¶
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/d8f8e20370669c8ee9b38864a0a45b0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Victoria Tronina](https://unsplash.com/@victoriaorvicky?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: ‚òïÔ∏è 5 minutes later‚Ä¶.alright, let‚Äôs do this üèÉ
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '2: Transfer Learning (pre-trained model + classification model)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the next level up from the previous approach. As briefly mentioned earlier,
    the zero-shot classification does not allow for customisation to a particular
    task. This is where this approach comes in as a **long term solution**.
  prefs: []
  type: TYPE_NORMAL
- en: In this approach, you use a pre-trained transformer model to create text embeddings,
    and then train a classification model to categorise these embeddings into their
    respective categories.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10998d7b476a0bcafa476ba46f9777c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Solution overview: Converting item description text to text embeddings using
    pre-trained models and using a classification model to classify the text into
    categories. (source:author)'
  prefs: []
  type: TYPE_NORMAL
- en: To demo this example, we will be using this [e-commerce products dataset](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification)
    from Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset contains 4 product categories and text description of the product.
    The objective is to build a model that can classify product description text into
    these 4 categories.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3683e4c4cac6df55adb8d729f28f0595.png)'
  prefs: []
  type: TYPE_IMG
- en: 'E-commerce product description and category data from [Kaggle](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification).
    (source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cd8a80a48ad90ec2d0b5f3d46fdd287.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are 4 categories in this [dataset](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification).
    (source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The implementation of this approach can be broken down into 4 steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Clean the text before feeding it to the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate text embeddings using a pre-trained large language model (LLM).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a classification model on the custom classes using the embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run predictions using the trained model and preprocessing pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By the end of this, you will have a classification model that leverages the
    high-quality embeddings learned by the LLM training on an extraordinary amount
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: These embeddings are the key factor in the success of the LLMs, as they capture
    rich representations of language that can effectively capture the nuances of meaning
    and context in a text.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ö°Ô∏è Note that the full code for this can be found at the following [notebook](https://github.com/saedhussain/medium/blob/53453650d6b5281686796cdbf8e4d9592b376c85/text_category/notebooks/ecommerce_text_categorisation.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Clean the Text'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the previous approach, it‚Äôs always a good practice to clean
    the text before using it for a model. Below the code used for cleaning the text
    in this article.
  prefs: []
  type: TYPE_NORMAL
- en: '***Side Note****: Cleaning is highly task-specific. In this example, I have
    only implemented some basic cleaning. As a best practice, it is always a good
    idea to first understand the data and then implement some task-specific cleaning.
    For example, you may need to decide whether to keep or remove numbers, depending
    on the requirements of your specific task. Additionally, reducing the text by
    removing unnecessary words/symbols can decrease processing time in downstream
    processes. Happy cleaning!* üòÑ'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Generate Text Embeddings'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to use the [SentenceTransformer](https://huggingface.co/sentence-transformers)
    library from Hugging Face to create text embeddings. This library contains transformer-based
    models pre-trained to generate fixed-length vector representations of textual
    data, such as paragraphs or sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we‚Äôll use the *‚Äúparaphrase-mpnet-base-v2‚Äù* pre-trained model from
    the library for this example, which produces fixed-length vectors of length 768.
  prefs: []
  type: TYPE_NORMAL
- en: '***Side Note****: in order to save time, I have reduced the sample size of
    both the training and test dataset. Generating embeddings can take a while, especially
    on a local machine. This can effect the performance of the model.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Train the Classification Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the embeddings, we are ready to train a classification model
    to take in these embeddings and classify it into one of the 4 product categories.
  prefs: []
  type: TYPE_NORMAL
- en: In this demo, I am using the XGBoost model, but feel free to use what ever you
    fancy!
  prefs: []
  type: TYPE_NORMAL
- en: '***Side Note****: in order to save time during training, limited hyper-parameters
    were used in the grid search. Also, to keep things simple, we are using the accuracy
    as a measure of performance. Make sure to use the appropriate metrics to measure
    the performance for your categorisation task.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4: Run Predictions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, package the text processing steps, load the trained classifier model
    and you are good to run predictions on your test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: And there you have it folks!! üòÉ
  prefs: []
  type: TYPE_NORMAL
- en: Without much task specific text cleaning, we used the embeddings from the pre-trained
    transformer model and build a classifier to categorise it into 4 categories, with
    91% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfb928bd622c8cf831b47c25500291b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Predictions (preds) on the test dataset. (source:author)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/417c5d0a2b319d3987138a9bcda81370.png)'
  prefs: []
  type: TYPE_IMG
- en: Cases where the predictions were wrong on the test dataset. (source:author)
  prefs: []
  type: TYPE_NORMAL
- en: The cases where the model failed to make correct predictions can be improved
    by using more data during training and doing task specific text cleaning. For
    example, leave dimension numbers in, as it helps differentiate household products
    from the rest.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ö°Ô∏è Another reminder that the full code is available in this [notebook](https://github.com/saedhussain/medium/blob/53453650d6b5281686796cdbf8e4d9592b376c85/text_category/notebooks/ecommerce_text_categorisation.ipynb).
    ü§ó
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have explored two different approaches for harnessing the
    power of pre-trained LLMs to customise text categorisation. We can achieve impressive
    results with minimal effort thanks to the high-quality embeddings they generate.
  prefs: []
  type: TYPE_NORMAL
- en: As these LLMs have been trained on large corpora of data, they possess a deep
    understanding of the language and can effectively capture the nuances of meaning
    and context in texts written in that language.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know these two approaches, give it a go for your text categorisation
    task! üòÉ
  prefs: []
  type: TYPE_NORMAL
- en: üëâ **Don‚Äôt forget to follow for more articles like this.** ü§ó
  prefs: []
  type: TYPE_NORMAL
- en: üöÄ Hope you found this article helpful. Consider joining Medium using [my link](https://medium.com/@saedhussain/membership)
    for more great content from me and other amazing authors on this platform!
  prefs: []
  type: TYPE_NORMAL
- en: '*Other articles by me:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-extract-and-convert-tables-from-pdf-files-to-pandas-dataframe-cb2e4c596fa8?source=post_page-----3757c517bd65--------------------------------)
    [## How to Extract and Convert Tables From PDF Files to Pandas Dataframe?'
  prefs: []
  type: TYPE_NORMAL
- en: So you have some pdf files with tables in them and want to read them into a
    pandas data frame. Let me show you how.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-extract-and-convert-tables-from-pdf-files-to-pandas-dataframe-cb2e4c596fa8?source=post_page-----3757c517bd65--------------------------------)
    [](/how-to-schedule-a-serverless-google-cloud-function-to-run-periodically-249acf3a652e?source=post_page-----3757c517bd65--------------------------------)
    [## How to Schedule a Serverless Google Cloud Function to Run Periodically
  prefs: []
  type: TYPE_NORMAL
- en: Do you have some code that needs to be run regularly?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-schedule-a-serverless-google-cloud-function-to-run-periodically-249acf3a652e?source=post_page-----3757c517bd65--------------------------------)
    [](/how-to-develop-and-test-your-google-cloud-function-locally-96a970da456f?source=post_page-----3757c517bd65--------------------------------)
    [## How to Develop and Test Your Google Cloud Function Locally
  prefs: []
  type: TYPE_NORMAL
- en: So, you have written your serverless cloud function, but don‚Äôt want to waste
    time deploying it and hoping it works. Let‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-develop-and-test-your-google-cloud-function-locally-96a970da456f?source=post_page-----3757c517bd65--------------------------------)
    [](/machine-learning-model-as-a-serverless-endpoint-using-google-cloud-function-a5ad1080a59e?source=post_page-----3757c517bd65--------------------------------)
    [## Machine Learning Model as a Serverless Endpoint using Google Cloud Functions
  prefs: []
  type: TYPE_NORMAL
- en: So you have built a model and want to productionize it as a serverless solution
    on google cloud platform (GCP). Let me‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/machine-learning-model-as-a-serverless-endpoint-using-google-cloud-function-a5ad1080a59e?source=post_page-----3757c517bd65--------------------------------)
  prefs: []
  type: TYPE_NORMAL
