- en: 'GenAI for Better NLP Systems I: A Tool for Generating Synthetic Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GenAI 提升 NLP 系统 I：生成合成数据的工具
- en: 原文：[https://towardsdatascience.com/genai-for-better-nlp-systems-i-a-tool-for-generating-synthetic-data-4b862ef3f88a](https://towardsdatascience.com/genai-for-better-nlp-systems-i-a-tool-for-generating-synthetic-data-4b862ef3f88a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/genai-for-better-nlp-systems-i-a-tool-for-generating-synthetic-data-4b862ef3f88a](https://towardsdatascience.com/genai-for-better-nlp-systems-i-a-tool-for-generating-synthetic-data-4b862ef3f88a)
- en: An experiment on using GenAI for generating and augmenting synthetic data using
    Python for Prompt Engineering
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 GenAI 生成和扩充合成数据的实验，基于 Python 的提示工程
- en: '[](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)[![Nabanita
    Roy](../Images/83ab7766a28c79371ebf9517e1f273d2.png)](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)
    [Nabanita Roy](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)[![Nabanita
    Roy](../Images/83ab7766a28c79371ebf9517e1f273d2.png)](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)
    [Nabanita Roy](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)
    ·7 min read·Sep 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)
    ·阅读时间7分钟·2023年9月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2c632c9d6ff85d169081e0ad66f249ac.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c632c9d6ff85d169081e0ad66f249ac.png)'
- en: Photo by [SR](https://unsplash.com/@lemonmelon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[SR](https://unsplash.com/@lemonmelon?utm_source=medium&utm_medium=referral)
    于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: One of the key challenges of Machine Learning(ML) is unbalanced data and the
    biases they introduce in ML models. With the advent of powerful Generative AI
    (GenAI) models, we can augment imbalanced training data with synthetic data easily,
    especially for Natural Language Processing(NLP) tasks. As a result, we can train
    models using classic ML algorithms for better performances in scenarios where
    Deep Learning models or directly using LLMs are not an option for reasons like
    the cost of computation, memory, availability of infrastructure or model explainability.
    Besides, despite the great efficacy shown by LLMs, we still don’t fully trust
    them. However, we can use LLMs to aid our work as data professionals and overcome
    roadblocks in building NLP systems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）的一个主要挑战是数据不平衡及其引入的模型偏差。随着强大生成式人工智能（GenAI）模型的出现，我们可以轻松地用合成数据来扩充不平衡的训练数据，特别是在自然语言处理（NLP）任务中。因此，我们可以使用经典的
    ML 算法来训练模型，以在深度学习模型或直接使用大语言模型（LLMs）不可行的情况下获得更好的性能，例如计算成本、内存、基础设施的可用性或模型可解释性。此外，尽管
    LLMs 展现出了极大的效能，但我们仍然没有完全信任它们。然而，我们可以利用 LLMs 来辅助我们的数据专业工作，克服在构建 NLP 系统时遇到的障碍。
- en: In this article, I have demonstrated how we can improve model performance for
    minority classes in imbalanced datasets using GenAI and Python to generate synthetic
    data and how we can iteratively engineer prompts to generate the desired outcome.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我展示了如何利用 GenAI 和 Python 生成合成数据来改善不平衡数据集中少数类的模型性能，以及如何通过迭代优化提示词来生成期望的结果。
- en: The Balancing Act
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡之道
- en: '![](../Images/086715743b0108b8c9033b978701d277.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/086715743b0108b8c9033b978701d277.png)'
- en: Photo by [Tingey Injury Law Firm](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Tingey Injury Law Firm](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)
    于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Long story short, ML models need enough examples to learn patterns and predict
    accurately. If the data contains fewer examples, then the model does not generalise
    and consequently, perform well. In such scenarios, the model can overfit for classes
    that have more examples and underfit for classes with fewer examples. To tackle
    unbalanced data we traditionally use statistical sampling methods like over or
    under-sampling, typically using [SMOTE](https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 长话短说，机器学习模型需要足够的示例来学习模式和准确预测。如果数据包含较少的示例，模型则无法泛化，表现也会较差。在这种情况下，模型可能会对样本较多的类别过拟合，而对样本较少的类别欠拟合。为了应对不平衡的数据，我们传统上使用统计采样方法，如过采样或欠采样，通常使用[SMOTE](https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/)。
- en: Why balancing data in NLP is hard?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么平衡 NLP 中的数据很困难？
- en: There are several examples of classification tasks in NLP where data is imbalanced
    and we have to resort to under-sampling to overcome this challenge. Information
    loss is the fundamental problem with under-sampling. While SMOTE implements efficient
    over-sampling strategies for numerical datasets, it is not suitable for texts
    or any sort of vectorized representations or embeddings for texts. This is because
    numeric datasets can easily be replicated with random sampling strategies, drawing
    from a combination of features’ probability distributions. For texts, it is not
    as simple to generate the embedded patterns since it is difficult to capture syntactical
    and semantic diversity using these methods.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 中，有几个分类任务的数据是不平衡的，我们不得不采用欠采样来克服这一挑战。欠采样的根本问题是信息丢失。尽管 SMOTE 为数值数据集实施了高效的过采样策略，但它不适用于文本或任何形式的文本向量化表示或嵌入。这是因为数值数据集可以通过随机采样策略轻松复制，结合特征的概率分布。对于文本而言，由于难以使用这些方法捕捉句法和语义的多样性，因此生成嵌入模式并不简单。
- en: 'Generative AI: A Modern Tool for Synthetic Data for NLP'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式 AI：用于 NLP 的现代合成数据工具
- en: This year, the power of GenAI is unleashed and prompt engineering is revolutionizing
    the way we work, and AI leaders are rethinking their operational strategies. Here
    is a demonstration of how GenAI can fuel NLP systems with synthetic data using
    prompt engineering.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 今年，GenAI 的力量被释放，提示工程正在革新我们的工作方式，AI 领导者正在重新思考他们的操作策略。下面是 GenAI 如何通过提示工程为 NLP
    系统提供合成数据的演示。
- en: 'Use-Case: Enriching Emotions Dataset with Synthetic Data'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用案例：用合成数据丰富情感数据集
- en: '**Background**'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**背景**'
- en: 'In my previous blogs on comparing [Multiclass Text Classifications with Keras
    Embedding Layer vs Word2vec embeddings](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0)
    and [Fine-tuning ANNs using KerasTuner](/how-i-improved-the-performance-of-a-multiclass-text-classifier-using-kerastune-and-other-basic-data-161a22625009),
    I presented models that performed well on the emotions — ‘joy’ and ‘sadness’,
    as target classes. This is because they had a sizable number of samples among
    all the other emotions in the dataset. Below is the distribution of the train
    data where clearly the number of examples for ‘joy’ and ‘sadness’ (majority classes)
    are higher than ‘anger’, ‘love’, ‘surprise’, and ‘fear’ (minority classes):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前关于比较[使用 Keras 嵌入层与 Word2vec 嵌入的多分类文本分类](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0)和[使用
    KerasTuner 微调 ANNs](/how-i-improved-the-performance-of-a-multiclass-text-classifier-using-kerastune-and-other-basic-data-161a22625009)的博客中，我展示了在情感—‘喜悦’和‘悲伤’作为目标类别—上表现良好的模型。这是因为它们在数据集中样本数量相对较多。以下是训练数据的分布，其中‘喜悦’和‘悲伤’（多数类别）的示例数量明显高于‘愤怒’、‘爱’、‘惊讶’和‘恐惧’（少数类别）：
- en: From the above figure, we can conclude that this data is highly imbalanced for
    emotions apart from ‘joy’ and ‘sadness’. ‘anger’ and ‘fear’ have a similar number
    of samples and the performance of the trained models on this data was slightly
    poorer than ‘joy’ and ‘sadness’. The poorest performance was observed for ‘love’
    and ‘surprise’ with 78% and 71% f1-scores for the best-chosen model from the exercise.
    Below, is the results on the test dataset for reference.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图可以得出结论，这些数据在‘喜悦’和‘悲伤’之外的情感上高度不平衡。‘愤怒’和‘恐惧’的样本数量相似，而在这些数据上训练出的模型表现略逊于‘喜悦’和‘悲伤’。表现最差的是‘爱’和‘惊讶’，在最佳选择模型中，其
    f1 分数分别为 78% 和 71%。以下是测试数据集的结果供参考。
- en: '![](../Images/2a83bd0a20bcd5a2e7e2949e2a61fe7d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a83bd0a20bcd5a2e7e2949e2a61fe7d.png)'
- en: 'Confusion Matrix for Model Performance without Augmented Synthetic Data | Image
    source: Author'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能的混淆矩阵（没有增强合成数据）| 图片来源：作者
- en: The best-performing model (notebook [here](https://github.com/royn5618/Medium_Blog_Codes/blob/master/Emotion%20Detection/EmotionClassifier_KerasTuner_2.ipynb))
    was a fine-tuned model *without using word2vec* and directly passing the token
    representation to the Keras embedding layer. Evidently, the precision for ‘surprise’
    (class 5) is 84%, which is good but recall is 62%, which is quite low. Higher
    precision means that when the classifier predicts an emotion, it is more likely
    to be correct. In other words, it minimizes false positives. On the other hand,
    higher recall means that the classifier is better at capturing all instances of
    the target emotion, minimizing false negatives. A 62% recall means that 62% of
    all the texts with the emotion ‘surprise’ were correctly identified. In real-world
    applications, achieving a balance between precision and recall is recommended,
    prioritizing one over another depending on business objectives. Here, the scores
    for precision and recall is unbalanced.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表现最好的模型（笔记本 [这里](https://github.com/royn5618/Medium_Blog_Codes/blob/master/Emotion%20Detection/EmotionClassifier_KerasTuner_2.ipynb)）是一个经过微调的模型，*没有使用
    word2vec*，而是直接将令牌表示传递给 Keras 嵌入层。显然，‘惊讶’（类别 5）的精确度为 84%，这是不错的，但召回率为 62%，这相当低。较高的精确度意味着分类器预测情感时更有可能是正确的。换句话说，它最小化了假阳性。另一方面，较高的召回率意味着分类器更擅长捕捉目标情感的所有实例，最小化假阴性。62%的召回率意味着62%的所有带有‘惊讶’情感的文本被正确识别。在实际应用中，建议在精确度和召回率之间取得平衡，根据业务目标优先考虑其中一个。在这里，精确度和召回率的分数不平衡。
- en: '**Objective: Increase recall for the emotion ‘surprise’, which is a minor class
    in this imbalanced dataset, using synthetic data generated by GPT.**'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**目标：提高对情感‘惊讶’的召回率，该情感在这个不平衡的数据集中属于小类，使用由 GPT 生成的合成数据。**'
- en: 2\. Prompt Engineering and Synthetic Data Generation
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 提示工程和合成数据生成
- en: In this experiment, I will be re-writing the 572 samples for ‘surprise’ in the
    training data and augment them to increase the total number of samples to 1200
    for that emotion.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我将重新编写 572 个‘惊讶’样本，并对其进行扩充，将该情感的样本总数增加到 1200 个。
- en: '**Prompt Engineering:**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示工程：**'
- en: Prompt engineering is the art of crafting instructions for an AI model that
    allows it to produce the most accurate response. In this use case, I will next
    be crafting a prompt that enables me to generate synthetic data. Not just that,
    I will provide examples and an output structure as well so that I can easily parse
    the synthetic data into a Pandas data frame.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是为 AI 模型制作指令的艺术，使其能够产生最准确的回应。在这个用例中，我将接下来制作一个提示，使我能够生成合成数据。不仅如此，我还将提供示例和输出结构，以便我可以轻松地将合成数据解析到
    Pandas 数据框中。
- en: 'This experiment uses the following prompt:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验使用了以下提示：
- en: 'Here are a few lessons learnt about the process of prompting for this experiment:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于这个实验提示过程的一些经验教训：
- en: This is the 15th-ish version of the prompt structure.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是第十五版本左右的提示结构。
- en: I asked to “generate X samples” instead of re-writing. That did not work very
    well since it started being repetitive in the syntactic style of the texts, especially
    after generating the first 15-20 of the synthetic samples, the model stopped generating
    a variety of sentence structures.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我要求“生成 X 个样本”而不是重写。由于在生成前 15-20 个合成样本后，模型开始重复使用相同的句法风格，它没有效果特别好，模型停止生成各种句子结构。
- en: Note how I instructed GPT-4 to remove punctuations and lowercase as well. This
    is useful for text cleaning tasks.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意我如何指示 GPT-4 移除标点符号并转为小写。这对于文本清理任务非常有用。
- en: I tried saying “Generate text samples with emotion ‘fear’” and GPT-4 conveniently
    added the word ‘fear’ in all samples. I had to use ‘underlying’, or ‘embedded’
    emotion in the prompt while I was trying to generate the samples instead of rewriting.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我尝试说“生成带有情感‘恐惧’的文本样本”，GPT-4 方便地在所有样本中添加了‘恐惧’一词。我在尝试生成样本时不得不使用‘潜在的’或‘嵌入的’情感，而不是重写。
- en: I tried adding ‘using synonyms’ in the instructions already but in some texts,
    the synonyms did not match well. So I removed that and added the clause on preserving
    the emotion because, in some texts, the emotion in content appeared vague to me.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我已经尝试在指令中添加了‘使用同义词’，但在某些文本中，同义词的匹配效果不好。所以我去掉了这个要求，并添加了保持情感的条款，因为在一些文本中，内容中的情感对我来说显得模糊。
- en: '**Addressing Token Limits and Full Code**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决令牌限制和完整代码**'
- en: 'Like everyone, I have the same token limits for prompting GPT-4\. Here’s what
    I have done. I *chunk*-ified the dataset and ran through all the data points.
    Here is the full code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 像每个人一样，我也有相同的令牌限制来提示GPT-4。 我做了什么呢？我将数据集进行了*分块*处理，并运行了所有数据点。以下是完整的代码：
- en: 'Here are a few lessons learnt about the process of prompting for this part
    of the experiment:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这部分实验的提示过程，我学到了几个经验教训：
- en: I ran into “503 — The engine is currently overloaded, please try again later”
    error without giving the code some resting time using time.sleep()
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我遇到了“503 — 引擎当前过载，请稍后再试”的错误，未给代码一些休息时间（使用time.sleep()）
- en: I instructed the model to produce ‘;’ separated samples so that I can parse
    them easily into a data frame.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我指示模型生成‘;’分隔的样本，以便我可以轻松地将它们解析到数据框中。
- en: I placed the above instruction in the first line of the prompt. “Do not use
    any punctuation” instruction was placed where it is. This resulted into generating
    the samples without any separation at all. (Rookie mistake I know in 🙄)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我将上述说明放在了提示的第一行。 “不要使用任何标点符号”的说明被放在了它所在的位置。这导致生成的样本完全没有分隔。（新手错误，我知道🙄）
- en: 'Now that I straightened the synthetic data generation and augmentation, here
    is the updated result of the tuned classifier (trained [here](https://github.com/royn5618/Medium_Blog_Codes/blob/master/GenAI_4_NLP_Systems/EmotionClassifier_KerasTuner_2.ipynb))
    now:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我已理顺了合成数据生成和增强，这里是调整后的分类器的更新结果（[训练于此](https://github.com/royn5618/Medium_Blog_Codes/blob/master/GenAI_4_NLP_Systems/EmotionClassifier_KerasTuner_2.ipynb)）：
- en: '![](../Images/bea951e54bd2fe9008dda7e22f5f0147.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bea951e54bd2fe9008dda7e22f5f0147.png)'
- en: 'Confusion Matrix for Model Performance with Augmented Synthetic Data | Image
    source: Author'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 带有增强合成数据的模型性能混淆矩阵 | 图片来源：作者
- en: Clearly, the recall has increased from 62% to 79% for the emotion ‘surprise’
    (Class 5), but the precision decreased from 84% to 72%. However, having a balance
    in the precision and recall scores is important, which is definitely better than
    the model without the synthetic data. In fact, the f1-score has improved from
    71% to 75% using this strategy. The overall model performance remains the same
    but is likely to be boosted on augmenting other classes as well — that’s for a
    future experimnetation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，对于情感‘惊讶’（类别5），召回率从62%提高到79%，但精确率从84%下降到72%。然而，平衡精确率和召回率是重要的，这显然比没有合成数据的模型要好。实际上，使用这种策略，f1分数从71%提高到75%。整体模型性能保持不变，但在增强其他类别时可能会得到提升——这是未来的实验方向。
- en: Conclusion
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This was a brief demonstration of how we can use GenAI to generate synthetic
    data for NLP-based use cases which otherwise could be a more complex task.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个简要演示，说明我们如何使用生成式AI为基于NLP的用例生成合成数据，这在其他情况下可能是一个更复杂的任务。
- en: There is a crucial challenge here, however. GPT-4 produces data that might induce
    patterns or biases in the texts, leading models to overfit on the patterns in
    synthetic data and consequently impede the classifier’s performance. Therefore,
    quality testing of the generated data and the overall performance is essential.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一个关键的挑战。GPT-4生成的数据可能会在文本中引入模式或偏差，导致模型在合成数据的模式上过拟合，从而阻碍分类器的性能。因此，对生成数据和整体性能的质量测试是至关重要的。
- en: Despite this limitation, Generative AI is incredibly helpful in expediting the
    generation of synthetic data for balancing classes in imbalanced datasets.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这个限制，生成式AI在加速生成合成数据以平衡不平衡数据集中的类别方面非常有帮助。
- en: Hope you enjoyed this blog 🙂. Here are the [Google Colab Notebooks](https://github.com/royn5618/Medium_Blog_Codes/tree/master/GenAI_4_NLP_Systems)
    on my GitHub.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢这个博客🙂。以下是我的GitHub上的[Google Colab笔记本](https://github.com/royn5618/Medium_Blog_Codes/tree/master/GenAI_4_NLP_Systems)。
- en: '*Thanks for visiting!*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢访问！*'
- en: '**My Links:** [Medium](https://medium.com/@nroy0110) | [LinkedIn](https://www.linkedin.com/in/nabanita-roy/)
    | [GitHub](https://github.com/royn5618)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**我的链接：** [Medium](https://medium.com/@nroy0110) | [LinkedIn](https://www.linkedin.com/in/nabanita-roy/)
    | [GitHub](https://github.com/royn5618)'
