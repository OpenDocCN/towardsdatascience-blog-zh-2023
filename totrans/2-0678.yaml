- en: Debugging SageMaker Endpoints With Docker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Docker 调试 SageMaker 端点
- en: 原文：[https://towardsdatascience.com/debugging-sagemaker-endpoints-with-docker-7a703fae3a26](https://towardsdatascience.com/debugging-sagemaker-endpoints-with-docker-7a703fae3a26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/debugging-sagemaker-endpoints-with-docker-7a703fae3a26](https://towardsdatascience.com/debugging-sagemaker-endpoints-with-docker-7a703fae3a26)
- en: An Alternative To SageMaker Local Mode
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker 本地模式的替代方案
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----7a703fae3a26--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----7a703fae3a26--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a703fae3a26--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a703fae3a26--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----7a703fae3a26--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page-----7a703fae3a26--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----7a703fae3a26--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a703fae3a26--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a703fae3a26--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----7a703fae3a26--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a703fae3a26--------------------------------)
    ·6 min read·Jun 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a703fae3a26--------------------------------)
    ·阅读时长6分钟·2023年6月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0b957e3ecdec75d862d1a637b72d9fe3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b957e3ecdec75d862d1a637b72d9fe3.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/1VW6HLOQE5A) by [**Mohammad
    Rahmani**](https://unsplash.com/@afgprogrammer)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[Unsplash](https://unsplash.com/photos/1VW6HLOQE5A)，作者[**Mohammad Rahmani**](https://unsplash.com/@afgprogrammer)
- en: A pain point with getting started with [SageMaker Real-Time Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)
    is that it is hard to debug at times. When creating an endpoint there are a number
    of ingredients you need to make sure are baked properly for successful deployment.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 启动[SageMaker 实时推理](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)的痛点之一是有时很难调试。当创建端点时，需要确保有许多因素得到了妥善处理，以便成功部署。
- en: Proper file structuring of model artifacts depending on the Model Server and
    Container that you are utilizing. Essentially the model.tar.gz you provide must
    be in a format that is compliant with the Model Server.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据您使用的模型服务器和容器，正确的模型工件文件结构至关重要。本质上，您提供的 model.tar.gz 必须符合模型服务器的格式。
- en: If you have a custom inference script that implements pre and post processing
    for your model, you need to ensure that the handlers implemented are compliant
    with your model server and that there are no scripting errors at the code level
    either.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您有一个自定义推理脚本，实现了模型的前处理和后处理，您需要确保实现的处理程序与模型服务器兼容，并且代码级别没有脚本错误。
- en: Previously we have discussed [SageMaker Local Mode](/debugging-sagemaker-endpoints-quickly-with-local-mode-2975bd55f6f7),
    but at the time of this article Local Mode does not support all hosting options
    and model servers that are available for SageMaker Deployment.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们讨论了[SageMaker 本地模式](/debugging-sagemaker-endpoints-quickly-with-local-mode-2975bd55f6f7)，但在本文撰写时，本地模式不支持所有
    SageMaker 部署可用的托管选项和模型服务器。
- en: To overcome this limitation we take a look at using [Docker](https://www.docker.com/)
    with a sample model and how we can test/debug our model artifacts and inference
    script prior to SageMaker Deployment. In this specific example we will utilize
    the [BART Model](https://huggingface.co/facebook/bart-large) that I have covered
    in my [last article](/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c)
    and see how we can host it with Docker.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个限制，我们将探讨如何使用[Docker](https://www.docker.com/)及示例模型，以及如何在 SageMaker 部署之前测试/调试我们的模型工件和推理脚本。在这个具体示例中，我们将利用[**BART
    模型**](https://huggingface.co/facebook/bart-large)，这是我在[上一篇文章](/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c)中介绍过的，看看如何使用
    Docker 托管它。
- en: '**NOTE**: For those of you new to AWS, make sure you make an account at the
    following [link](https://aws.amazon.com/console/) if you want to follow along.
    The article also assumes an intermediate understanding of SageMaker Deployment,
    I would suggest following this [article](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)
    for understanding Deployment/Inference more in depth. An intermediate level of
    understanding of Docker will also be helpful to fully understand this example.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：对于那些刚接触 AWS 的用户，如果你想跟随本文，请确保在以下 [链接](https://aws.amazon.com/console/)
    上创建一个账户。本文还假设你对 SageMaker 部署有中级了解，我建议你阅读这篇 [文章](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)
    以更深入地理解部署/推理。对 Docker 有中级了解也将有助于你完全理解这个示例。'
- en: How Does SageMaker Hosting Work?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker 托管是如何工作的？
- en: 'Before we can get to the code portion of this article, let’s take a look at
    how SageMaker actually serves requests. At it’s core SageMaker Inference has two
    constructs:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入本文的代码部分之前，让我们先看看 SageMaker 实际是如何处理请求的。SageMaker Inference 的核心有两个构件：
- en: '**Container**: This establishes the runtime environment for the model, it is
    also integrated with the model server that you are utilizing. You can either utilize
    one of the existing [Deep Learning Containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)
    (DLCs) or [Build Your Own Container](/bring-your-own-container-with-amazon-sagemaker-37211d8412f4).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Container**：这建立了模型的运行时环境，它还与您正在使用的模型服务器集成。你可以使用现有的 [深度学习容器](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)（DLCs）之一，也可以
    [构建你自己的容器](/bring-your-own-container-with-amazon-sagemaker-37211d8412f4)。'
- en: '**Model Artifacts**: In the [CreateModel](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_model.html)
    API call we specify an S3 URL with the model data present in the format of a model.tar.gz
    (tarball). This model data is loaded into the opt/ml/model directory on the container,
    this also includes any inference script that you provide.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型工件**：在 [CreateModel](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_model.html)
    API 调用中，我们指定了一个包含模型数据的 S3 URL，格式为 model.tar.gz（tarball）。这些模型数据被加载到容器上的 opt/ml/model
    目录中，这也包括你提供的任何推理脚本。'
- en: The key here is that the container needs a web server implemented that responds
    to port 8080 on the /invocations and /ping paths. An example of a web server we
    have implemented with these paths is Flask during a [Bring Your Own Container](/bring-your-own-container-with-amazon-sagemaker-37211d8412f4)
    example.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于容器需要实现一个 Web 服务器，响应端口 8080 上的 /invocations 和 /ping 路径。我们实现的一个 Web 服务器示例是
    Flask，在 [自定义容器](/bring-your-own-container-with-amazon-sagemaker-37211d8412f4)
    示例中提供了这些路径。
- en: With Docker what we will do is expose this port and point towards our local
    script and model artifacts, this way we simulate the way a SageMaker Endpoint
    is expected to behave.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Docker 时，我们将暴露这个端口，并指向我们的本地脚本和模型工件，这样我们就可以模拟 SageMaker Endpoint 预期的行为。
- en: Testing With Docker
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Docker 进行测试
- en: 'For simplicity’s sake we will utilize my BART example from my last article,
    you can grab the artifacts from this [repository](https://github.com/RamVegiraju/SageMaker-Docker-Local).
    Here you should see the following files:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们将使用我上一篇文章中的 BART 示例，你可以从这个 [仓库](https://github.com/RamVegiraju/SageMaker-Docker-Local)
    获取相关工件。在这里，你应该能看到以下文件：
- en: '![](../Images/1a559972afa9346ef4d835a2cc816ca9.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a559972afa9346ef4d835a2cc816ca9.png)'
- en: '**model.py**: This is the inference script that we are working with. In this
    case we are utilizing DJL Serving which expects a model.py with a handler function
    implementing inference. **Your inference script still needs to be compatible with
    the format that the model server expects.**'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**model.py**：这是我们正在使用的推理脚本。在这种情况下，我们使用 DJL Serving，它期望一个包含处理推理的 handler 函数的
    model.py。**你的推理脚本仍然需要与模型服务器期望的格式兼容。**'
- en: '**requirements.txt**: Any additional dependencies that your model.py script
    requires. For DJL Serving PyTorch is already installed beforehand, we use numpy
    for data processing.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**requirements.txt**：任何你的 model.py 脚本所需的额外依赖项。对于 DJL Serving，PyTorch 已经预先安装，我们使用
    numpy 进行数据处理。'
- en: '**serving.properties**: This is also a DJL specific file, here you can define
    any configuration at the model level (ex: workers per model)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**serving.properties**：这是一个 DJL 特有的文件，你可以在这里定义模型级别的任何配置（例如：每个模型的工作线程数）。'
- en: We have our model artifacts, now we need the container that we will be utilizing.
    In this case we can retrieve the existing DJL DeepSpeed image. For an extensive
    list of the images that are already provided by AWS please reference this [guide](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).
    You can also build your own image locally and point towards that. In this case
    we are operating in a [SageMaker Classic Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html)
    environment which comes with Docker pre-installed as well.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了模型工件，现在需要我们将要使用的容器。在这种情况下，我们可以检索现有的 DJL DeepSpeed 镜像。有关 AWS 已提供的镜像的详细列表，请参考这个[指南](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)。你也可以在本地构建自己的镜像并指向它。在这种情况下，我们在一个[
    SageMaker Classic Notebook 实例](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html)环境中操作，该环境中也预装了
    Docker。
- en: To work with existing AWS provided images we first need to login to [AWS Elastic
    Container Registry (ECR)](/pushing-docker-images-to-amazon-elastic-container-registry-830c301b8971)
    to retrieve the image, you can do that with the following shell command.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 AWS 提供的现有镜像，我们首先需要登录到[AWS Elastic Container Registry (ECR)](/pushing-docker-images-to-amazon-elastic-container-registry-830c301b8971)以检索镜像，你可以使用以下
    shell 命令来完成这一步。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You should see a login succeeded message similar to the following.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似于以下的登录成功消息。
- en: '![](../Images/d3ce47666e9dc95565e62207f8ab99e6.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3ce47666e9dc95565e62207f8ab99e6.png)'
- en: Login Succeeded (Screenshot by Author)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 登录成功（作者截图）
- en: Once logged in we can get to the path where our model artifacts are stored and
    run the following command which will launch the model server. If you have not
    already retrieved the image, this will also be pulled from ECR.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦登录成功，我们可以进入存储模型工件的路径，并运行以下命令来启动模型服务器。如果你还没有检索镜像，这也会从 ECR 中拉取镜像。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A few key points here:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个关键点：
- en: We are exposing port 8080 as SageMaker Inference expects.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们暴露了 8080 端口，因为 SageMaker 推理期望如此。
- en: We also point towards the existing image. This string is dependent on the region
    and model you are operating in. You can also utilize the SageMaker Python SDK
    [retrieve image_uri API call](https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html)
    to identify the appropriate image to pull here.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还指向现有的镜像。这个字符串取决于你所在的区域和操作的模型。你还可以使用 SageMaker Python SDK 的[retrieve image_uri
    API 调用](https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html)来识别合适的镜像以进行拉取。
- en: '![](../Images/80edbf854b73b7dc1e6d507a14c38b3e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80edbf854b73b7dc1e6d507a14c38b3e.png)'
- en: Image being retrieved (Screenshot by Author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图像正在检索中（作者截图）
- en: After the image has been pulled you see that the model server has been started.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在镜像拉取完成后，你会看到模型服务器已启动。
- en: '![](../Images/83e5eb09c3d3f4933b06c64e739c025d.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83e5eb09c3d3f4933b06c64e739c025d.png)'
- en: DJL Server Started (Screenshot by Author)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: DJL 服务器已启动（作者截图）
- en: We can also verify this container is running by utilizing the following Docker
    command.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过使用以下 Docker 命令来验证容器是否正在运行。
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/ff955e4533bcb8ba0705f8c14abf5cf0.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff955e4533bcb8ba0705f8c14abf5cf0.png)'
- en: Container Started (Screenshot by Author)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 容器已启动（作者截图）
- en: We see that the API is exposed via port 8080 which we can send sample requests
    to via curl. Notice we specify the /invocations path that SageMaker Containers
    expect.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 API 通过 8080 端口暴露，我们可以通过 curl 向其发送示例请求。注意，我们指定了 SageMaker 容器期望的 /invocations
    路径。
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We then see inference returned for the request and also the model server tracking
    the response and emitting our logging statements from our inference script.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看到推理返回了请求，并且模型服务器正在跟踪响应并从我们的推理脚本中发出日志语句。
- en: '![](../Images/2d1771223fea354eb27a90380051f9d0.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d1771223fea354eb27a90380051f9d0.png)'
- en: Sample Request (Screenshot by Author)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 示例请求（作者截图）
- en: Let’s break our model.py and see if we can catch the error early with Docker.
    Here in the inference function I add a syntactically incorrect print statement
    and restart my model server to see if this error is captured.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拆解 `model.py`，看看是否能通过 Docker 早期捕获到错误。在推理函数中，我添加了一个语法错误的打印语句，并重启模型服务器以查看是否能捕获到这个错误。
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can then see this error captured by the model server when we execute the
    docker run command.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以看到，当我们执行 docker run 命令时，模型服务器捕获了这个错误。
- en: '![](../Images/7f75fef77d00382b2822a06edc0e80f1.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f75fef77d00382b2822a06edc0e80f1.png)'
- en: Error captured by Model Server (Screenshot by Author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务器捕获的错误（作者截图）
- en: 'Note that you are not limited to utilizing just curl to test your container.
    We can also use something like the [Python requests](https://pypi.org/project/requests/)
    library to interface and work with the container. A sample request would look
    like the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你不仅限于使用 curl 来测试你的容器。我们还可以使用类似于 [Python requests](https://pypi.org/project/requests/)
    库来与容器进行交互和操作。一个示例请求可能如下所示：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Utilizing something like requests you can run larger scale load tests on the
    container. Note that the hardware you are running the container on is what is
    being utilized (think of this as your equivalent to the instance behind a SageMaker
    Endpoint).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 利用类似 requests 的工具，你可以对容器进行大规模负载测试。请注意，你运行容器的硬件就是正在被利用的（可以将其视为 SageMaker Endpoint
    后面的实例）。
- en: Additional Resources & Conclusion
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外资源与结论
- en: '[](https://github.com/RamVegiraju/SageMaker-Docker-Local/tree/master?source=post_page-----7a703fae3a26--------------------------------)
    [## GitHub - RamVegiraju/SageMaker-Docker-Local: How to locally test SageMaker
    Inference with Docker'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - RamVegiraju/SageMaker-Docker-Local: 如何使用 Docker 本地测试 SageMaker
    推理](https://github.com/RamVegiraju/SageMaker-Docker-Local/tree/master?source=post_page-----7a703fae3a26--------------------------------)'
- en: 'How to locally test SageMaker Inference with Docker - GitHub - RamVegiraju/SageMaker-Docker-Local:
    How to locally test…'
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用 Docker 本地测试 SageMaker 推理 - GitHub - RamVegiraju/SageMaker-Docker-Local：如何本地测试…
- en: github.com](https://github.com/RamVegiraju/SageMaker-Docker-Local/tree/master?source=post_page-----7a703fae3a26--------------------------------)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/RamVegiraju/SageMaker-Docker-Local/tree/master?source=post_page-----7a703fae3a26--------------------------------)'
- en: You can find the code for the entire example at the link above. With SageMaker
    Inference you want to avoid the pain of waiting for the endpoint to create to
    capture any errors. Utilizing this approach you can work with any SageMaker container
    to test and debug your model artifacts and inference scripts.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在上面的链接找到整个示例的代码。使用 SageMaker Inference，你希望避免等待端点创建以捕捉错误的痛苦。通过这种方法，你可以使用任何
    SageMaker 容器来测试和调试你的模型工件和推理脚本。
- en: As always feel free to leave any feedback or questions, thank you for reading!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 随时欢迎留下反馈或提问，感谢阅读！
- en: '*If you enjoyed this article feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *and subscribe to my Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*.
    If you’re new to Medium, sign up using my* [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*.*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，欢迎通过* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *与我联系，并订阅我的 Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*。如果你是
    Medium 的新用户，可以通过我的* [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*注册。*'
