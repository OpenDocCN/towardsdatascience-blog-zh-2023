- en: Kaiming He Initialization in Neural Networks â€” Math Proof
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kaiming He åˆå§‹åŒ–åœ¨ç¥ç»ç½‘ç»œä¸­çš„æ•°å­¦è¯æ˜
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4](https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4](https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4)
- en: Deriving optimal initial variance of weight matrices in neural network layers
    with ReLU activation function
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨å¯¼å…·æœ‰ ReLU æ¿€æ´»å‡½æ•°çš„ç¥ç»ç½‘ç»œå±‚ä¸­æƒé‡çŸ©é˜µçš„æœ€ä¼˜åˆå§‹æ–¹å·®
- en: '[](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[![Ester
    Hlav](../Images/5fe679b93c42d568d0d5b331a8bf92b9.png)](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)
    [Ester Hlav](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[![Ester
    Hlav](../Images/5fe679b93c42d568d0d5b331a8bf92b9.png)](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)
    [Ester Hlav](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)
    Â·10 min readÂ·Feb 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)
    Â·é˜…è¯»æ—¶é—´çº¦ 10 åˆ†é’ŸÂ·2023 å¹´ 2 æœˆ 15 æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0e072a98caece16e7471a537515610cc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e072a98caece16e7471a537515610cc.png)'
- en: Initialization techniques are one of the prerequisites for successfully training
    a deep learning architecture. Traditionally, weight initialization methods need
    to be compatible with the choice of an activation function as a mismatch can potentially
    affect training negatively.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–æŠ€æœ¯æ˜¯æˆåŠŸè®­ç»ƒæ·±åº¦å­¦ä¹ æ¶æ„çš„å‰ææ¡ä»¶ä¹‹ä¸€ã€‚ä¼ ç»Ÿä¸Šï¼Œæƒé‡åˆå§‹åŒ–æ–¹æ³•éœ€è¦ä¸æ¿€æ´»å‡½æ•°çš„é€‰æ‹©å…¼å®¹ï¼Œå› ä¸ºä¸åŒ¹é…å¯èƒ½ä¼šå¯¹è®­ç»ƒäº§ç”Ÿè´Ÿé¢å½±å“ã€‚
- en: ReLU is one of the most commonly used activation functions in deep learning.
    Its properties make it a very convenient choice for scaling to large neural networks.
    On one hand, it is inexpensive to calculate the derivative during backpropagation
    because it is a linear function with a step-function derivative. On the other
    hand, ReLU helps reduce feature correlation as it is a non-negative activation
    function, *i.e.* features can only contribute positively to subsequent layers.
    It is a prevalent choice in convolutional architectures where the input dimension
    is large and neural networks tend to be very deep.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ä¹‹ä¸€ã€‚å®ƒçš„ç‰¹æ€§ä½¿å…¶æˆä¸ºæ‰©å±•åˆ°å¤§å‹ç¥ç»ç½‘ç»œçš„éå¸¸æ–¹ä¾¿çš„é€‰æ‹©ã€‚ä¸€æ–¹é¢ï¼Œç”±äºå®ƒæ˜¯ä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼Œå…·æœ‰æ­¥è¿›å‡½æ•°å¯¼æ•°ï¼Œåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è®¡ç®—å¯¼æ•°çš„å¼€é”€è¾ƒå°ã€‚å¦ä¸€æ–¹é¢ï¼ŒReLU
    æœ‰åŠ©äºå‡å°‘ç‰¹å¾ç›¸å…³æ€§ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªéè´Ÿæ¿€æ´»å‡½æ•°ï¼Œ*å³* ç‰¹å¾åªèƒ½å¯¹åç»­å±‚äº§ç”Ÿç§¯æè´¡çŒ®ã€‚å®ƒåœ¨è¾“å…¥ç»´åº¦è¾ƒå¤§ä¸”ç¥ç»ç½‘ç»œå¾€å¾€éå¸¸æ·±çš„å·ç§¯æ¶æ„ä¸­æ˜¯ä¸€ç§æµè¡Œçš„é€‰æ‹©ã€‚
- en: 'In *â€œDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classificationâ€* â½*Â¹* â¾ by He *et al.* (2015), the authors present a methodology
    to optimally initialize neural network layers using a ReLU activation function.
    This technique allows the neural network to start in a regime with constant variance
    between inputs and outputs both in terms of forward and backward passes, which
    empirically showed meaningful improvement in training stability and speed. In
    the following sections, we provide a detailed and complete derivation behind the
    He initialization technique.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ He *et al.*ï¼ˆ2015ï¼‰çš„*â€œæ·±å…¥æ¢è®¨æ•´æµå™¨ï¼šè¶…è¶Šäººç±»æ°´å¹³çš„ ImageNet åˆ†ç±»â€* â½*Â¹* â¾ ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä½¿ç”¨ ReLU
    æ¿€æ´»å‡½æ•°æ¥æœ€ä¼˜åˆå§‹åŒ–ç¥ç»ç½‘ç»œå±‚çš„æ–¹æ³•ã€‚è¿™ç§æŠ€æœ¯ä½¿ç¥ç»ç½‘ç»œåœ¨å‰å‘å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­åœ¨è¾“å…¥å’Œè¾“å‡ºä¹‹é—´ä¿æŒå¸¸æ•°æ–¹å·®ï¼Œè¿™åœ¨ç»éªŒä¸Šæ˜¾ç¤ºå‡ºè®­ç»ƒç¨³å®šæ€§å’Œé€Ÿåº¦çš„æ˜¾è‘—æ”¹å–„ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æä¾›
    He åˆå§‹åŒ–æŠ€æœ¯èƒŒåçš„è¯¦ç»†å’Œå®Œæ•´çš„æ¨å¯¼è¿‡ç¨‹ã€‚
- en: Notation
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¦å·
- en: A layer in a neural network, composed of a weight matrix *Wâ‚–* and bias vector
    *bâ‚–*, undergoes two consecutive transformations. The first transformation is *yâ‚–
    = xâ‚– Wâ‚– + bâ‚–*, and the second is *xâ‚– â‚Š â‚ = f(yâ‚–)*
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œä¸­çš„ä¸€å±‚ï¼Œç”±æƒé‡çŸ©é˜µ *Wâ‚–* å’Œåç½®å‘é‡ *bâ‚–* ç»„æˆï¼Œç»è¿‡ä¸¤ä¸ªè¿ç»­çš„å˜æ¢ã€‚ç¬¬ä¸€ä¸ªå˜æ¢æ˜¯ *yâ‚– = xâ‚– Wâ‚– + bâ‚–*ï¼Œç¬¬äºŒä¸ªå˜æ¢æ˜¯
    *xâ‚– â‚Š â‚ = f(yâ‚–)*
- en: '*xâ‚–* is the actual layer and *yâ‚–* is the pre-activation layer'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*xâ‚–* æ˜¯å®é™…å±‚ï¼Œ*yâ‚–* æ˜¯é¢„æ¿€æ´»å±‚'
- en: A layer has *nâ‚–* units, thus *xâ‚– âˆˆ â„â¿â½* áµ*â¾, Wâ‚– âˆˆ â„â¿â½* áµ*â¾*Ë™*â¿â½* áµ âº Â¹*â¾, bâ‚–
    âˆˆâ„â¿â½* áµ âº Â¹*â¾*
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€å±‚æœ‰*nâ‚–*ä¸ªå•å…ƒï¼Œå› æ­¤*xâ‚– âˆˆ â„â¿â½* áµ*â¾, Wâ‚– âˆˆ â„â¿â½* áµ*â¾*Ë™*â¿â½* áµ âº Â¹*â¾, bâ‚– âˆˆâ„â¿â½* áµ âº Â¹*â¾*
- en: '*xâ‚–Wâ‚– + bâ‚–* has dimension ( *1 Ã— nâ‚– ) Ã— ( nâ‚– Ã— nâ‚– â‚Š* â‚*) + 1 Ã— nâ‚– â‚Š* â‚— *= 1
    Ã— nâ‚– â‚Š* â‚'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*xâ‚–Wâ‚– + bâ‚–* çš„ç»´åº¦æ˜¯ï¼ˆ*1 Ã— nâ‚–*ï¼‰Ã—ï¼ˆ*nâ‚– Ã— nâ‚– â‚Š* â‚*ï¼‰+ 1 Ã— nâ‚– â‚Š* â‚—* = 1 Ã— nâ‚– â‚Š* â‚'
- en: The activation function *f* is applied element-wise and does not change the
    shape of a vector. As a result, *xâ‚– â‚Š* â‚*= f(xâ‚– Wâ‚–+ bâ‚–)âˆˆ â„â¿â½* áµ âº Â¹*â¾*
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°*f*æŒ‰å…ƒç´ åº”ç”¨ï¼Œä¸æ”¹å˜å‘é‡çš„å½¢çŠ¶ã€‚å› æ­¤ï¼Œ*xâ‚– â‚Š* â‚*= f(xâ‚– Wâ‚–+ bâ‚–)âˆˆ â„â¿â½* áµ âº Â¹*â¾*
- en: For a neural network of depth *n*, the input layer is represented by *xâ‚€* and
    the output layer by *xâ‚™*
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ·±åº¦ä¸º*n*çš„ç¥ç»ç½‘ç»œï¼Œè¾“å…¥å±‚ç”±*xâ‚€*è¡¨ç¤ºï¼Œè¾“å‡ºå±‚ç”±*xâ‚™*è¡¨ç¤º
- en: The loss function of the network is represented by *L*
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç½‘ç»œçš„æŸå¤±å‡½æ•°ç”±*L*è¡¨ç¤º
- en: '*Î”x = âˆ‚L/âˆ‚x* denotes gradients of the loss function with respect to vector
    *x*'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Î”x = âˆ‚L/âˆ‚x* è¡¨ç¤ºæŸå¤±å‡½æ•°å…³äºå‘é‡*x*çš„æ¢¯åº¦'
- en: Assumptions
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡è®¾
- en: '*Assumption 1*:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å‡è®¾ 1*ï¼š'
- en: We assume for this initialization setup a *non-linear* activation function ReLU
    defined as *f(x) =* *ReLU(x) = max(0, x).* As a function defined separately on
    two intervals, its derivative has a value of 1 on the strictly positive half of
    *â„* and 0 on the strictly negative half. Technically, the derivative of ReLU is
    not defined in 0 due to the limits of both sides not being equal, that is *fâ€™(0â»*â»*)
    = 0 â‰  1 = fâ€™(0âº).* In practice, for backpropagationâ€™s purpose, ReLUâ€™(0) is assumed
    to be 0.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªåˆå§‹åŒ–è®¾ç½®ï¼Œæˆ‘ä»¬å‡è®¾ä¸€ä¸ª*éçº¿æ€§*æ¿€æ´»å‡½æ•°ReLUå®šä¹‰ä¸º*f(x) =* *ReLU(x) = max(0, x).* ä½œä¸ºåœ¨ä¸¤ä¸ªåŒºé—´ä¸Šåˆ†åˆ«å®šä¹‰çš„å‡½æ•°ï¼Œå®ƒçš„å¯¼æ•°åœ¨*â„*çš„ä¸¥æ ¼æ­£åŠè½´ä¸Šå€¼ä¸º1ï¼Œåœ¨ä¸¥æ ¼è´ŸåŠè½´ä¸Šå€¼ä¸º0ã€‚æŠ€æœ¯ä¸Šï¼Œç”±äºä¸¤ä¾§çš„æé™ä¸ç›¸ç­‰ï¼ŒReLUåœ¨0å¤„çš„å¯¼æ•°æœªå®šä¹‰ï¼Œå³*fâ€™(0â»*â»*)
    = 0 â‰  1 = fâ€™(0âº).* å®é™…ä¸Šï¼Œä¸ºäº†åå‘ä¼ æ’­çš„ç›®çš„ï¼Œå‡è®¾ReLUâ€™(0)ä¸º0ã€‚
- en: '*Assumption 2:*'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å‡è®¾ 2*ï¼š'
- en: It is assumed that all inputs, weights, and layers in the neural network are
    independent and identically distributed (*iid*) at initialization, as well as
    the gradients.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å‡è®¾ç¥ç»ç½‘ç»œä¸­çš„æ‰€æœ‰è¾“å…¥ã€æƒé‡å’Œå±‚åœ¨åˆå§‹åŒ–æ—¶éƒ½æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼ˆ*iid*ï¼‰ï¼Œæ¢¯åº¦ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: '*Assumption 3:*'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å‡è®¾ 3*ï¼š'
- en: The inputs are assumed to be normalized with zero mean and the weights and biases
    are initialized from a symmetric distribution centered at zero, *i.e. ğ”¼[xâ‚€] =
    ğ”¼[Wâ‚–] = ğ”¼[bâ‚–] = 0*. This means that both *xâ‚–* and yâ‚– have an expectation of zero
    at initialization, and *yâ‚–* has a symmetric distribution at initialization due
    to *f(0) = 0*.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å‡è®¾è¾“å…¥å·²å½’ä¸€åŒ–ä¸ºé›¶å‡å€¼ï¼Œæƒé‡å’Œåç½®ä»ä»¥é›¶ä¸ºä¸­å¿ƒçš„å¯¹ç§°åˆ†å¸ƒä¸­åˆå§‹åŒ–ï¼Œå³*ğ”¼[xâ‚€] = ğ”¼[Wâ‚–] = ğ”¼[bâ‚–] = 0*ã€‚è¿™æ„å‘³ç€*xâ‚–*å’Œyâ‚–åœ¨åˆå§‹åŒ–æ—¶éƒ½æœ‰é›¶çš„æœŸæœ›ï¼Œç”±äº*f(0)
    = 0*ï¼Œ*yâ‚–*åœ¨åˆå§‹åŒ–æ—¶å…·æœ‰å¯¹ç§°åˆ†å¸ƒã€‚
- en: Motivation
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ¨æœº
- en: 'The aim of this proof is to determine the distribution of the weight matrix
    by finding *Var[W]* given two constraints:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è¯æ˜çš„ç›®çš„æ˜¯é€šè¿‡åœ¨ä¸¤ä¸ªçº¦æŸæ¡ä»¶ä¸‹æ‰¾åˆ°*Var[W]*æ¥ç¡®å®šæƒé‡çŸ©é˜µçš„åˆ†å¸ƒï¼š
- en: âˆ€*k, Var[yâ‚–] = Var[yâ‚– â‚‹* â‚*]*, *i.e.* constant variance in the forward signal
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: âˆ€*k, Var[yâ‚–] = Var[yâ‚– â‚‹* â‚*]*ï¼Œ*å³*å‰å‘ä¿¡å·ä¸­çš„æ–¹å·®æ˜¯æ’å®šçš„
- en: âˆ€*k, Var[Î”xâ‚–] = Var[Î”xâ‚– â‚Š* â‚*], i.e.* constant variance in the backward signal
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: âˆ€*k, Var[Î”xâ‚–] = Var[Î”xâ‚– â‚Š* â‚*]ï¼Œå³*åå‘ä¿¡å·ä¸­çš„æ–¹å·®æ˜¯æ’å®šçš„
- en: Ensuring that the variance of both layers and gradients is constant throughout
    the network at initialization helps prevent exploding and vanishing gradients
    in neural networks. If the gain is *above* one, it will result in exploding gradients
    and optimization divergence, while if the gain is *below* one, it will result
    in vanishing gradients and halt learning. The above two equations ensure that
    the signal gain is precisely one.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿ç½‘ç»œä¸­æ‰€æœ‰å±‚å’Œæ¢¯åº¦çš„æ–¹å·®åœ¨åˆå§‹åŒ–æ—¶ä¿æŒæ’å®šï¼Œæœ‰åŠ©äºé˜²æ­¢ç¥ç»ç½‘ç»œä¸­çš„æ¢¯åº¦çˆ†ç‚¸å’Œæ¢¯åº¦æ¶ˆå¤±ã€‚å¦‚æœå¢ç›Š*å¤§äº* ä¸€ï¼Œåˆ™ä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸å’Œä¼˜åŒ–å‘æ•£ï¼›å¦‚æœå¢ç›Š*å°äº*
    ä¸€ï¼Œåˆ™ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±å¹¶åœæ­¢å­¦ä¹ ã€‚ä¸Šè¿°ä¸¤ä¸ªæ–¹ç¨‹ç¡®ä¿ä¿¡å·å¢ç›Šæ°å¥½ä¸ºä¸€ã€‚
- en: The motivation as well as the derivations in this paper are following the Xavier
    Glorot initializationâ½Â²â¾ paper published five years prior. While the previous
    work uses post-activation layers for constant variance in the forward signal,
    the He initialization proof uses pre-activation layers. Similarly, for the backward
    signal, Heâ€™s derivation uses post-activation layers instead of pre-activation
    layers in Glorotâ€™s initialization. Given that these two proofs share some similarities,
    looking at both helps gain insights into why controlling for weightsâ€™ variance
    is so important in *any* neural network. (See â€œ[Xavier Glorot Initialization in
    Neural Networks â€” Math Proof](https://medium.com/towards-data-science/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3)â€
    for more details)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„åŠ¨æœºåŠæ¨å¯¼åŸºäºäº”å¹´å‰å‘è¡¨çš„ Xavier Glorot åˆå§‹åŒ–â½Â²â¾ è®ºæ–‡ã€‚è™½ç„¶å‰è¿°å·¥ä½œä½¿ç”¨äº†åæ¿€æ´»å±‚æ¥ä¿æŒå‰å‘ä¿¡å·çš„æ’å®šæ–¹å·®ï¼Œä½† He åˆå§‹åŒ–è¯æ˜ä½¿ç”¨äº†å‰æ¿€æ´»å±‚ã€‚åŒæ ·åœ°ï¼Œå¯¹äºåå‘ä¿¡å·ï¼ŒHe
    çš„æ¨å¯¼ä½¿ç”¨äº†åæ¿€æ´»å±‚ï¼Œè€Œé Glorot åˆå§‹åŒ–ä¸­çš„å‰æ¿€æ´»å±‚ã€‚é‰´äºè¿™ä¸¤ä¸ªè¯æ˜æœ‰ä¸€äº›ç›¸ä¼¼ä¹‹å¤„ï¼Œæ¯”è¾ƒè¿™ä¸¤è€…æœ‰åŠ©äºç†è§£ä¸ºä»€ä¹ˆæ§åˆ¶æƒé‡çš„æ–¹å·®åœ¨*ä»»ä½•*ç¥ç»ç½‘ç»œä¸­å¦‚æ­¤é‡è¦ã€‚ï¼ˆæ›´å¤šç»†èŠ‚è§
    â€œ[Xavier Glorot Initialization in Neural Networks â€” Math Proof](https://medium.com/towards-data-science/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3)â€ï¼‰
- en: '[](/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=post_page-----73b9a0d845c4--------------------------------)
    [## Xavier Glorot Initialization in Neural Networks â€” Math Proof'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=post_page-----73b9a0d845c4--------------------------------)
    [## Xavier Glorot Initialization in Neural Networks â€” Math Proof'
- en: Detailed derivation for finding optimal initial distributions of weight matrices
    in deep learning layers with tanhâ€¦
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¯¦ç»†æ¨å¯¼äº†åœ¨æ·±åº¦å­¦ä¹ å±‚ä¸­ä½¿ç”¨ tanh æ¿€æ´»å‡½æ•°æ—¶å¯»æ‰¾æƒé‡çŸ©é˜µçš„æœ€ä¼˜åˆå§‹åˆ†å¸ƒâ€¦â€¦
- en: towardsdatascience.com](/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=post_page-----73b9a0d845c4--------------------------------)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=post_page-----73b9a0d845c4--------------------------------)
- en: 'Math Proof: Kaiming He Initialization'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°å­¦è¯æ˜ï¼šKaiming He åˆå§‹åŒ–
- en: I. Forward Pass
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I. å‰å‘ä¼ æ’­
- en: We are looking for *Wâ‚–* such that the variance of each subsequent pre-activation
    layer *y* is equal, *i.e.* *Var[yâ‚–] = Var[yâ‚– â‚‹* â‚*].*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ *Wâ‚–* ä½¿å¾—æ¯ä¸ªåç»­å‰æ¿€æ´»å±‚ *y* çš„æ–¹å·®ç›¸ç­‰ï¼Œ*å³* *Var[yâ‚–] = Var[yâ‚– â‚‹* â‚*].*
- en: We know that *yâ‚– = xâ‚– Wâ‚–+ bâ‚–.*
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çŸ¥é“ *yâ‚– = xâ‚– Wâ‚–+ bâ‚–*ã€‚
- en: For simplicity, we look at the *i-th* element of the pre-activation layer *yâ‚–*
    and apply the variance operator on both sides of the previous equation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬æŸ¥çœ‹å‰æ¿€æ´»å±‚çš„*i-th* å…ƒç´  *yâ‚–* å¹¶å¯¹å‰é¢çš„æ–¹ç¨‹ä¸¤è¾¹åº”ç”¨æ–¹å·®ç®—å­ã€‚
- en: '![](../Images/16cb584b879d9662cbbfbfc8a3ac57bc.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16cb584b879d9662cbbfbfc8a3ac57bc.png)'
- en: In the first step, we remove *bâ‚–* entirely, as following *Assumption 1* it is
    initialized at value zero. Additionally, we leverage the independence of *W* and
    *x* to transform the variance of the sum into a sum of variances, following *Var[X+Y]
    = Var[X] + Var[Y] with X* âŸ‚ *Y*.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å®Œå…¨å»é™¤ *bâ‚–*ï¼Œå› ä¸ºæ ¹æ®*å‡è®¾1*ï¼Œå®ƒåˆå§‹åŒ–ä¸ºé›¶å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨ *W* å’Œ *x* çš„ç‹¬ç«‹æ€§ï¼Œå°†å’Œçš„æ–¹å·®è½¬æ¢ä¸ºæ–¹å·®ä¹‹å’Œï¼Œä¾æ®*Var[X+Y]
    = Var[X] + Var[Y]ï¼Œå…¶ä¸­ X* âŸ‚ *Y*ã€‚
- en: In the second step, as *W* and *x* are *i.i.d.,* each term in the sum is equal,
    hence the sum is simply a *nâ‚–* times repetition of *Var[xW]*.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬äºŒæ­¥ä¸­ï¼Œç”±äº *W* å’Œ *x* æ˜¯ *i.i.d.*ï¼Œå› æ­¤å’Œä¸­çš„æ¯ä¸€é¡¹éƒ½æ˜¯ç›¸ç­‰çš„ï¼Œå› æ­¤å’Œä»…ä»…æ˜¯ *Var[xW]* çš„ *nâ‚–* æ¬¡é‡å¤ã€‚
- en: In the third step, we follow the formula for *X* âŸ‚ *Y* which implies that *Var[XY]
    = E[XÂ²]E[YÂ²] - E[X]Â²E[Y]Â²*. This allows us to separate *W* and *x* contributions
    to the pre-activation layerâ€™s variance.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸‰æ­¥ä¸­ï¼Œæˆ‘ä»¬éµå¾ª *X* âŸ‚ *Y* çš„å…¬å¼ï¼Œè¿™æ„å‘³ç€ *Var[XY] = E[XÂ²]E[YÂ²] - E[X]Â²E[Y]Â²*ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°† *W*
    å’Œ *x* å¯¹å‰æ¿€æ´»å±‚æ–¹å·®çš„è´¡çŒ®åˆ†å¼€ã€‚
- en: '![](../Images/dd865a2fc54dec78238b3aa7a6ccab66.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd865a2fc54dec78238b3aa7a6ccab66.png)'
- en: In the fourth step, we leverage *Assumption 3* of zero expectation for weights
    and layers at initialization. This leaves us with a single term involving a squared
    expectation.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬å››æ­¥ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†æƒé‡å’Œå±‚åœ¨åˆå§‹åŒ–æ—¶çš„é›¶æœŸæœ›çš„*å‡è®¾3*ã€‚è¿™ä½¿æˆ‘ä»¬å‰©ä¸‹ä¸€ä¸ªæ¶‰åŠå¹³æ–¹æœŸæœ›çš„å•ä¸€é¡¹ã€‚
- en: In the fifth step, we transform the squared expectation into a variance since
    *Var[X] = E[( X - E[X])Â²] = E[XÂ²]* if *X* has a zero mean. Now we can express
    the pre-activation layerâ€™s variance as a separate product of layer and weight
    variance.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬äº”æ­¥ä¸­ï¼Œæˆ‘ä»¬å°†å¹³æ–¹æœŸæœ›è½¬æ¢ä¸ºæ–¹å·®ï¼Œå› ä¸º*Var[X] = E[( X - E[X])Â²] = E[XÂ²]*ï¼Œå¦‚æœ*X*çš„å‡å€¼ä¸ºé›¶ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†å‰æ¿€æ´»å±‚çš„æ–¹å·®è¡¨ç¤ºä¸ºå±‚æ–¹å·®å’Œæƒé‡æ–¹å·®çš„å•ç‹¬ä¹˜ç§¯ã€‚
- en: Finally, in order to link *Var[yâ‚–] to Var[yâ‚– â‚‹* â‚*],* we express the squared
    expectation *E[xâ‚–Â²]* in terms of *Var[yâ‚– â‚‹* â‚*]* in the following steps using
    the *Law of the Unconscious Statistician*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä¸ºäº†å°† *Var[yâ‚–] ä¸ Var[yâ‚– â‚‹* â‚*]* å…³è”èµ·æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ *æ— æ„è¯†ç»Ÿè®¡å­¦å®¶å®šå¾‹* å°†å¹³æ–¹æœŸæœ› *E[xâ‚–Â²]* è¡¨è¾¾ä¸º *Var[yâ‚–
    â‚‹* â‚*]*ã€‚
- en: '![](../Images/438ceff83f96e7fda4ee45850bfbeb9e.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/438ceff83f96e7fda4ee45850bfbeb9e.png)'
- en: The theorem states that we can formulate any expectation of the function of
    a random variable as an integral of its function and probability density *p*.
    As we know that *xâ‚– = max(0, yâ‚– â‚‹* â‚*)*, we can rewrite the squared expectation
    of *xâ‚–* as an integral on *â„* of *y*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å®šç†æŒ‡å‡ºï¼Œæˆ‘ä»¬å¯ä»¥å°†éšæœºå˜é‡å‡½æ•°çš„ä»»ä½•æœŸæœ›è¡¨ç¤ºä¸ºå…¶å‡½æ•°å’Œæ¦‚ç‡å¯†åº¦ *p* çš„ç§¯åˆ†ã€‚ç”±äºæˆ‘ä»¬çŸ¥é“ *xâ‚– = max(0, yâ‚– â‚‹* â‚*)*ï¼Œæˆ‘ä»¬å¯ä»¥å°†
    *xâ‚–* çš„å¹³æ–¹æœŸæœ›é‡å†™ä¸ºåœ¨ *â„* ä¸Šå…³äº *y* çš„ç§¯åˆ†ã€‚
- en: '![](../Images/ff34a215683d295eb7d4e630f9029d68.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff34a215683d295eb7d4e630f9029d68.png)'
- en: In the sixth step, we simplify the integral using that *y* is zero on â„â».
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬å…­æ­¥ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ *y* åœ¨ â„â» ä¸Šä¸ºé›¶æ¥ç®€åŒ–ç§¯åˆ†ã€‚
- en: In the seventh step, we leverage the statistical property of *y* as a symmetric
    random variable, which hence has a symmetric density function *p*, and note that
    the entire integralâ€™s term is an even function. Even functions are symmetric with
    respect to 0 on â„, which means that integrating from 0 to *a* is the same as from
    *-a* to 0\. We use this trick to reformulate back the integral as an integral
    over â„.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸ƒæ­¥ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ *y* ä½œä¸ºå¯¹ç§°éšæœºå˜é‡çš„ç»Ÿè®¡ç‰¹æ€§ï¼Œå› æ­¤å®ƒå…·æœ‰å¯¹ç§°çš„å¯†åº¦å‡½æ•° *p*ï¼Œå¹¶æ³¨æ„åˆ°æ•´ä¸ªç§¯åˆ†é¡¹æ˜¯å¶å‡½æ•°ã€‚å¶å‡½æ•°ç›¸å¯¹äº â„ ä¸Šçš„ 0 æ˜¯å¯¹ç§°çš„ï¼Œè¿™æ„å‘³ç€ä»
    0 åˆ° *a* çš„ç§¯åˆ†ä¸ä» *-a* åˆ° 0 çš„ç§¯åˆ†æ˜¯ç›¸åŒçš„ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæŠ€å·§å°†ç§¯åˆ†é‡æ–°è¡¨è¿°ä¸ºåœ¨ â„ ä¸Šçš„ç§¯åˆ†ã€‚
- en: '![](../Images/f0613a7a42c13e34c3e7a65b014cffbb.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0613a7a42c13e34c3e7a65b014cffbb.png)'
- en: In the ninth and tenth steps, we rewrite this integral as an integral of a function
    of a random variable. By applying the LOTUS â€” this time from right to left â€” we
    can change this integral to an expectation of the function over the random variable
    *y*. As a squared expectation of a zero mean variable, this is essentially a variance.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¹å’Œç¬¬åæ­¥ä¸­ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªç§¯åˆ†é‡å†™ä¸ºéšæœºå˜é‡çš„å‡½æ•°çš„ç§¯åˆ†ã€‚é€šè¿‡åº”ç”¨ LOTUSâ€”â€”è¿™æ¬¡æ˜¯ä»å³åˆ°å·¦â€”â€”æˆ‘ä»¬å¯ä»¥å°†æ­¤ç§¯åˆ†è½¬æ¢ä¸ºå…³äºéšæœºå˜é‡ *y* çš„å‡½æ•°çš„æœŸæœ›ã€‚ä½œä¸ºé›¶å‡å€¼å˜é‡çš„å¹³æ–¹æœŸæœ›ï¼Œè¿™æœ¬è´¨ä¸Šæ˜¯æ–¹å·®ã€‚
- en: '![](../Images/8a86c6fac095d23f1346be8be0d7bfa2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a86c6fac095d23f1346be8be0d7bfa2.png)'
- en: We finally get to put it all together using the results of steps five and ten
    â€” the variance of a pre-activation layer is directly linked to its previous pre-activation
    variance as well as the variance of the layerâ€™s weights. Since we require that
    *Var[yâ‚–] = Var[yâ‚– â‚‹* â‚*]*, it allows us to confirm that a layerâ€™s weights variance
    *Var[Wâ‚–]* should be 2/*nâ‚–* .
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæˆ‘ä»¬å°†æ­¥éª¤äº”å’Œæ­¥éª¤åçš„ç»“æœç»¼åˆèµ·æ¥â€”â€”å‰ç½®æ¿€æ´»å±‚çš„æ–¹å·®ä¸å…¶å‰ç½®æ¿€æ´»æ–¹å·®ä»¥åŠè¯¥å±‚æƒé‡çš„æ–¹å·®ç›´æ¥ç›¸å…³ã€‚ç”±äºæˆ‘ä»¬è¦æ±‚ *Var[yâ‚–] = Var[yâ‚–
    â‚‹* â‚*]*ï¼Œè¿™ä½¿æˆ‘ä»¬å¯ä»¥ç¡®è®¤è¯¥å±‚æƒé‡çš„æ–¹å·® *Var[Wâ‚–]* åº”è¯¥æ˜¯ 2/*nâ‚–*ã€‚
- en: 'In summary, here is again the whole derivation of the forward propagation reviewed
    in this section:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œè¿™é‡Œæ˜¯æœ¬èŠ‚å›é¡¾çš„å‰å‘ä¼ æ’­çš„å®Œæ•´æ¨å¯¼ï¼š
- en: '![](../Images/b3525ec2d5fdd6d851a28475d664d725.png)![](../Images/d09f218de73be6e53ae2a95eafb5b2b2.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3525ec2d5fdd6d851a28475d664d725.png)![](../Images/d09f218de73be6e53ae2a95eafb5b2b2.png)'
- en: II. Backward Pass
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II. åå‘ä¼ æ’­
- en: We are looking for *Wâ‚–* such that *Var[Î”xâ‚–] = Var[Î”xâ‚– â‚Š* â‚*].*
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ *Wâ‚–*ï¼Œä½¿å¾— *Var[Î”xâ‚–] = Var[Î”xâ‚– â‚Š* â‚*]*ã€‚
- en: Here*, xâ‚– â‚Š* â‚*= f (yâ‚–)* and *yâ‚– = xâ‚– Wâ‚– + bâ‚–.*
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œ*ï¼Œ xâ‚– â‚Š* â‚*= f (yâ‚–)* å’Œ *yâ‚– = xâ‚– Wâ‚– + bâ‚–*ã€‚
- en: 'Before applying the variance operator, let us first calculate the partial derivatives
    of the loss *L* with respect to *x* and *y* : *Î”xâ‚–* and *Î”yâ‚–* .'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åº”ç”¨æ–¹å·®è¿ç®—ç¬¦ä¹‹å‰ï¼Œè®©æˆ‘ä»¬é¦–å…ˆè®¡ç®—æŸå¤± *L* ç›¸å¯¹äº *x* å’Œ *y* çš„åå¯¼æ•°ï¼š*Î”xâ‚–* å’Œ *Î”yâ‚–*ã€‚
- en: '![](../Images/9c17c3bc893544ffa5202e2901d6cead.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c17c3bc893544ffa5202e2901d6cead.png)'
- en: First, we use the chain rule and the fact that the derivative of a linear product
    is its linear coefficient â€” in this case, *Wâ‚–*.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨é“¾å¼æ³•åˆ™å’Œçº¿æ€§ä¹˜ç§¯çš„å¯¼æ•°æ˜¯å…¶çº¿æ€§ç³»æ•°çš„äº‹å®â€”â€”åœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯ *Wâ‚–*ã€‚
- en: Second, we leverage *Assumption 2* stating that gradients and weights are independent
    of each other. Using independence, the variance of the product becomes the product
    of variances, which is equal to zero since the weights are assumed to be initialized
    with zero means. Hence, the expectation of the gradient of *L w.r.t. x* is zero.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨ *å‡è®¾ 2* è¯´æ˜æ¢¯åº¦å’Œæƒé‡å½¼æ­¤ç‹¬ç«‹ã€‚åˆ©ç”¨ç‹¬ç«‹æ€§ï¼Œç§¯çš„æ–¹å·®ç­‰äºæ–¹å·®çš„ç§¯ï¼Œå› ä¸ºæƒé‡å‡è®¾ä¸ºé›¶å‡å€¼åˆå§‹åŒ–ã€‚å› æ­¤ï¼Œ*L w.r.t. x* çš„æ¢¯åº¦æœŸæœ›ä¸ºé›¶ã€‚
- en: Third, we use the chain rule to link *Î”yâ‚–* and *Î”xâ‚– â‚Š* â‚ as the partial derivative
    of *x* *w.r.t. y* is ReLUâ€™s derivative taken in *y*.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ï¼Œæˆ‘ä»¬ä½¿ç”¨é“¾å¼æ³•åˆ™å°† *Î”yâ‚–* å’Œ *Î”xâ‚– â‚Š* â‚ å…³è”èµ·æ¥ï¼Œå› ä¸º *x* ç›¸å¯¹äº *y* çš„åå¯¼æ•°æ˜¯ ReLU å¯¹ *y* çš„å¯¼æ•°ã€‚
- en: '![](../Images/9b88943311d1a917e2bc3bc18529d10c.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b88943311d1a917e2bc3bc18529d10c.png)'
- en: 'Fourth, recalling the derivative of ReLU, we compute the expectation of *Î”yâ‚–*
    using the previous equation. As *fâ€™(x)* is split into two parts with an equal
    probability of *Â½*, we can write it as a sum of two terms: expectation over â„âº
    and â„â», respectively. From previous calculations, we know that the expectation
    of *Î”xâ‚–* is zero, and we can thus confirm that both gradients have a mean of 0.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬å››ï¼Œå›é¡¾ReLUçš„å¯¼æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¹‹å‰çš„æ–¹ç¨‹è®¡ç®—*Î”yâ‚–*çš„æœŸæœ›ã€‚ç”±äº*fâ€™(x)*è¢«åˆ†ä¸ºä¸¤ä¸ªç­‰æ¦‚ç‡çš„éƒ¨åˆ†*Â½*ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶å†™ä½œä¸¤ä¸ªé¡¹çš„å’Œï¼šåˆ†åˆ«å¯¹â„âºå’Œâ„â»çš„æœŸæœ›ã€‚ä»ä¹‹å‰çš„è®¡ç®—ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“*Î”xâ‚–*çš„æœŸæœ›ä¸ºé›¶ï¼Œå› æ­¤å¯ä»¥ç¡®è®¤ä¸¤ä¸ªæ¢¯åº¦çš„å‡å€¼å‡ä¸º0ã€‚
- en: '![](../Images/197febab4ff1f2ccdf31069750903908.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/197febab4ff1f2ccdf31069750903908.png)'
- en: Fifth, we use the same rule as before to write a squared expectation as a variance,
    here with *Î”yâ‚–* .
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬äº”ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¹‹å‰ç›¸åŒçš„è§„åˆ™ï¼Œå°†å¹³æ–¹æœŸæœ›å†™ä½œæ–¹å·®ï¼Œè¿™é‡Œæ˜¯*Î”yâ‚–*ã€‚
- en: Sixth, we leverage *Assumption 2* stating gradients are independent at initialization
    to split the variance of the two gradients *Î”xâ‚– â‚Š* â‚ and *fâ€™(yâ‚–).* Further simplification
    stems from *Assumption 3* and we can finally compute ReLUâ€™s squared expectation
    given its even split between positive and negative intervals.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬å…­ï¼Œæˆ‘ä»¬åˆ©ç”¨*å‡è®¾2*æŒ‡å‡ºæ¢¯åº¦åœ¨åˆå§‹åŒ–æ—¶æ˜¯ç‹¬ç«‹çš„ï¼Œä»¥æ­¤åˆ†ç¦»ä¸¤ä¸ªæ¢¯åº¦*Î”xâ‚– â‚Š* â‚å’Œ*fâ€™(yâ‚–)*çš„æ–¹å·®ã€‚è¿›ä¸€æ­¥ç®€åŒ–æºè‡ª*å‡è®¾3*ï¼Œæˆ‘ä»¬æœ€ç»ˆå¯ä»¥è®¡ç®—ReLUçš„å¹³æ–¹æœŸæœ›ï¼Œå› ä¸ºå®ƒåœ¨æ­£è´ŸåŒºé—´ä¹‹é—´æ˜¯å‡åŒ€åˆ†å¸ƒçš„ã€‚
- en: '![](../Images/313e8009108a1814e9b693539fcf9548.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/313e8009108a1814e9b693539fcf9548.png)'
- en: Finally, using the gathered results from the above sections, and reapplying
    the assumption of *iid,* we conclude the result of the backpropagation pass to
    be similar to the forward pass, *i.e.* given *Var[Î”xâ‚–] = Var[Î”xâ‚– â‚Š* â‚*],* the
    variance of any layerâ€™s weights *Var[Wâ‚–]* is equal to 2/*nâ‚– .*
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåˆ©ç”¨ä¸Šè¿°å„èŠ‚ä¸­æ”¶é›†åˆ°çš„ç»“æœï¼Œå¹¶é‡æ–°åº”ç”¨*iid*å‡è®¾ï¼Œæˆ‘ä»¬å¾—å‡ºåå‘ä¼ æ’­çš„ç»“æœä¸æ­£å‘ä¼ æ’­ç±»ä¼¼ï¼Œå³ç»™å®š*Var[Î”xâ‚–] = Var[Î”xâ‚– â‚Š* â‚*]*ï¼Œä»»ä½•ä¸€å±‚çš„æƒé‡çš„æ–¹å·®*Var[Wâ‚–]*ç­‰äº2/*nâ‚–*ã€‚
- en: 'To summarize, here is a reminder of the important step-by-step calculations
    included within this backward pass section:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œè¿™é‡Œæ˜¯åå‘ä¼ æ’­éƒ¨åˆ†åŒ…å«çš„é‡è¦é€æ­¥è®¡ç®—çš„æé†’ï¼š
- en: '![](../Images/be426dc6afa5c2a15f929bc66627b32c.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be426dc6afa5c2a15f929bc66627b32c.png)'
- en: III. Weight Distribution
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: III. æƒé‡åˆ†å¸ƒ
- en: 'In the two previous sections, we concluded the following for both backward
    and forward setups:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰ä¸¤èŠ‚ä¸­ï¼Œæˆ‘ä»¬å¯¹æ­£å‘å’Œåå‘è®¾ç½®å¾—å‡ºäº†ä»¥ä¸‹ç»“è®ºï¼š
- en: '![](../Images/0f745c22499c3eca5a1fafb02cc475cb.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f745c22499c3eca5a1fafb02cc475cb.png)'
- en: It is interesting to note that this result is different from the Glorot initializationâ½Â²â¾,
    where the authors essentially have to average the *two* distinct results obtained
    in the forward and backward passes. Furthermore, we observe that the variance
    in the He method is doubled, which, intuitively, is due to the fact that ReLUâ€™s
    zero negative section reduces variance by a factor of two.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œè¿™ä¸ªç»“æœä¸Glorotåˆå§‹åŒ–æ–¹æ³•ä¸åŒï¼Œå…¶ä¸­ä½œè€…å®é™…ä¸Šå¿…é¡»å¯¹æ­£å‘å’Œåå‘ä¼ æ’­ä¸­å¾—åˆ°çš„*ä¸¤ä¸ª*ä¸åŒç»“æœè¿›è¡Œå¹³å‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°Heæ–¹æ³•ä¸­çš„æ–¹å·®æ˜¯ç¿»å€çš„ï¼Œè¿™ç›´è§‚ä¸Šæ˜¯å› ä¸ºReLUçš„é›¶è´ŸåŒºåŸŸå°†æ–¹å·®å‡å°‘äº†ä¸€åŠã€‚
- en: Subsequently, knowing the variance of the distribution, we can now initialize
    the weights with either normal distribution *N(0, ğœÂ²)* or uniform distribution
    *U(-a, a)*. Empirically, there is no evidence that one distribution is superior
    to the other, and it seems that the performance improvement comes down solely
    to the symmetry and scale properties of a chosen distribution. Furthermore, we
    do need to keep in mind *Assumption 3*, restricting the distribution choice to
    be symmetric and centered in 0.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: éšåï¼Œäº†è§£äº†åˆ†å¸ƒçš„æ–¹å·®åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ­£æ€åˆ†å¸ƒ*N(0, ğœÂ²)*æˆ–å‡åŒ€åˆ†å¸ƒ*U(-a, a)*æ¥åˆå§‹åŒ–æƒé‡ã€‚æ ¹æ®ç»éªŒï¼Œæ²¡æœ‰è¯æ®è¡¨æ˜ä¸€ç§åˆ†å¸ƒä¼˜äºå¦ä¸€ç§åˆ†å¸ƒï¼Œä¼¼ä¹æ€§èƒ½çš„æå‡å®Œå…¨å½’ç»“ä¸ºæ‰€é€‰åˆ†å¸ƒçš„å¯¹ç§°æ€§å’Œå°ºåº¦å±æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç¡®å®éœ€è¦è®°ä½*å‡è®¾3*ï¼Œé™åˆ¶åˆ†å¸ƒé€‰æ‹©ä¸ºå¯¹ç§°ä¸”ä»¥0ä¸ºä¸­å¿ƒã€‚
- en: '**For Normal distribution *N(0, ğœÂ²)***'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¯¹äºæ­£æ€åˆ†å¸ƒ *N(0, ğœÂ²)***'
- en: 'If *X ~ N(0, ğœÂ²),* then *Var[X] = ğœÂ²*, thus the variance and standard deviation
    of the weight matrix can be written as:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ*X ~ N(0, ğœÂ²)*ï¼Œåˆ™*Var[X] = ğœÂ²*ï¼Œå› æ­¤æƒé‡çŸ©é˜µçš„æ–¹å·®å’Œæ ‡å‡†å·®å¯ä»¥å†™ä½œï¼š
- en: '![](../Images/e1085f0b569de97721c8dd5bb2bc15f7.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1085f0b569de97721c8dd5bb2bc15f7.png)'
- en: 'We can therefore conclude that *Wâ‚–* follows a normal distribution with coefficients:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤æˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œ*Wâ‚–*éµå¾ªä»¥ä¸‹ç³»æ•°çš„æ­£æ€åˆ†å¸ƒï¼š
- en: '![](../Images/2ba793a077bfd84a7661bbde1ee61e1b.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ba793a077bfd84a7661bbde1ee61e1b.png)'
- en: As a reminder, *nâ‚–* is the number of inputs of the layer *k*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæé†’ï¼Œ*nâ‚–*æ˜¯å±‚*k*çš„è¾“å…¥æ•°é‡ã€‚
- en: '**For Uniform distribution *U(-a, a)***'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¯¹äºå‡åŒ€åˆ†å¸ƒ *U(-a, a)***'
- en: 'If *X ~ U(-a, a)*, then using the below formula of a variance for a random
    variable following a uniform distribution, we can find the bound *a*:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ*X ~ U(-a, a)*ï¼Œåˆ™ä½¿ç”¨ä»¥ä¸‹å‡åŒ€åˆ†å¸ƒéšæœºå˜é‡æ–¹å·®çš„å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°ç•Œé™ *a*ï¼š
- en: '![](../Images/2adb9197e433a65dc09424df786bfce6.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2adb9197e433a65dc09424df786bfce6.png)'
- en: 'Finally, we can conclude that *Wâ‚–* follows a uniform distribution with coefficients:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œ*Wâ‚–* éµå¾ªå…·æœ‰ç³»æ•°çš„å‡åŒ€åˆ†å¸ƒï¼š
- en: '![](../Images/a689c2c60b45301651c2fd13a4d62a25.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a689c2c60b45301651c2fd13a4d62a25.png)'
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: This article provides a step-by-step derivation of why *He initialization method*
    is optimal for neural networks that use ReLU activation functions, given the constraints
    on forward and backward passes to have constant variances.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æä¾›äº†ä¸ºä»€ä¹ˆ*He åˆå§‹åŒ–æ–¹æ³•*å¯¹äºä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°çš„ç¥ç»ç½‘ç»œæ˜¯æœ€ä¼˜çš„çš„é€æ­¥æ¨å¯¼ï¼Œå‰ææ˜¯å‰å‘å’Œåå‘ä¼ æ’­çš„æ–¹å·®ä¿æŒæ’å®šã€‚
- en: The methodology of this proof also extends to the broader family of linear rectifiers,
    like PReLU (discussed in (1) by He *et al.*) or Leaky ReLU (allowing for a minuscule
    gradient to flow in the negative interval). Similar optimal variance formulas
    can be derived for these variants of the ReLU activation function.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è¯æ˜çš„æ–¹æ³•è®ºè¿˜æ‰©å±•åˆ°çº¿æ€§æ¿€æ´»å‡½æ•°çš„æ›´å¹¿æ³›å®¶æ—ï¼Œå¦‚ PReLUï¼ˆåœ¨ (1) ä¸­ç”± He *ç­‰äºº* è®¨è®ºï¼‰æˆ– Leaky ReLUï¼ˆå…è®¸åœ¨è´ŸåŒºé—´ä¸­æœ‰å¾®å°çš„æ¢¯åº¦æµåŠ¨ï¼‰ã€‚å¯¹äºè¿™äº›
    ReLU æ¿€æ´»å‡½æ•°çš„å˜ä½“ï¼Œä¹Ÿå¯ä»¥æ¨å¯¼å‡ºç±»ä¼¼çš„æœ€ä¼˜æ–¹å·®å…¬å¼ã€‚
- en: '**Citations**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¼•ç”¨**'
- en: '(1)[*Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification*](https://arxiv.org/abs/1502.01852), He *et al.* (2015)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: (1)[*æ·±å…¥æ¢è®¨æ¿€æ´»å‡½æ•°ï¼šè¶…è¶Šäººç±»æ°´å¹³çš„ ImageNet åˆ†ç±»è¡¨ç°*](https://arxiv.org/abs/1502.01852)ï¼ŒHe *ç­‰äºº*
    (2015)
- en: (2)[*Understanding the difficulty of training deep feedforward neural networks*](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf),
    Glorot *et al.* (2010)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (2)[*ç†è§£è®­ç»ƒæ·±åº¦å‰é¦ˆç¥ç»ç½‘ç»œçš„éš¾åº¦*](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)ï¼ŒGlorot
    *ç­‰äºº* (2010)
- en: '*Source: All the above equations and images are my own.*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ¥æºï¼šä»¥ä¸Šæ‰€æœ‰æ–¹ç¨‹å’Œå›¾åƒå‡ä¸ºæˆ‘ä¸ªäººåˆ¶ä½œã€‚*'
