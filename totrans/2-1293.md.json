["```py\nimport osmnx as ox # version: 1.0.1\n\ncity = 'Budapest'\nadmin = ox.geocode_to_gdf(city)\nadmin.plot()\n```", "```py\nimport overpy # version: 0.6\nfrom shapely.geometry import Point # version: 1.7.1\nimport geopandas as gpd # version: 0.9.0\n\n# start the api\napi = overpy.Overpass()\n\n# get the enclosing bounding box\nminx, miny, maxx, maxy = admin.to_crs(4326).bounds.T[0]\nbbox = ','.join([str(miny), str(minx), str(maxy), str(maxx)])\n\n# define the OSM categories of interest\namenity_mapping = [\n    (\"amenity\", \"cafe\"),\n    (\"tourism\", \"gallery\"),\n    (\"amenity\", \"pub\"),\n    (\"amenity\", \"bar\"),\n    (\"amenity\", \"marketplace\"),\n    (\"sport\", \"yoga\"),\n    (\"amenity\", \"studio\"),\n    (\"shop\", \"music\"),\n    (\"shop\", \"second_hand\"),\n    (\"amenity\", \"foodtruck\"),\n    (\"amenity\", \"music_venue\"),\n    (\"shop\", \"books\"),\n]\n\n# iterate over all categories, call the overpass api, \n# and add the results to the poi_data list\npoi_data  = []\n\nfor idx, (amenity_cat, amenity) in enumerate(amenity_mapping):\n    query = f\"\"\"node[\"{amenity_cat}\"=\"{amenity}\"]({bbox});out;\"\"\"\n    result = api.query(query)\n    print(amenity, len(result.nodes))\n\n    for node in result.nodes:\n        data = {}\n        name = node.tags.get('name', 'N/A')\n        data['name'] = name\n        data['amenity'] = amenity_cat + '__' + amenity\n        data['geometry'] = Point(node.lon, node.lat)\n        poi_data.append(data)\n\n# transform the results into a geodataframe\ngdf_poi = gpd.GeoDataFrame(poi_data)\nprint(len(gdf_poi))\ngdf_poi = gpd.overlay(gdf_poi, admin[['geometry']])\ngdf_poi.crs = 4326\nprint(len(gdf_poi))\n```", "```py\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(1,1,figsize=(10,10))\nadmin.plot(ax=ax, color = 'none', edgecolor = 'k', linewidth = 2)\ngdf_poi.plot(column = 'amenity', ax=ax, legend = True, alpha = 0.3)\n```", "```py\nimport folium\nimport branca.colormap as cm\n\n# get the centroid of the city and set up the map\nx, y = admin.geometry.to_list()[0].centroid.xy\nm = folium.Map(location=[y[0], x[0]], zoom_start=12, tiles='CartoDB Dark_Matter')\ncolors = ['blue', 'green', 'red', 'purple', 'orange', 'pink', 'gray', 'cyan', 'magenta', 'yellow', 'lightblue', 'lime']\n\n# transform the gdf_poi\namenity_colors = {}\nunique_amenities = gdf_poi['amenity'].unique()\nfor i, amenity in enumerate(unique_amenities):\n    amenity_colors[amenity] = colors[i % len(colors)]\n\n# visualize the pois with a scatter plot\nfor idx, row in gdf_poi.iterrows():\n    amenity = row['amenity']\n    lat = row['geometry'].y\n    lon = row['geometry'].x\n    color = amenity_colors.get(amenity, 'gray')  # default to gray if not in the colormap\n\n    folium.CircleMarker(\n        location=[lat, lon],\n        radius=3,  \n        color=color,\n        fill=True,\n        fill_color=color,\n        fill_opacity=1.0,  # No transparency for dot markers\n        popup=amenity,\n    ).add_to(m)\n\n# show the map\nm\n```", "```py\nfrom sklearn.cluster import DBSCAN # version: 0.24.1\nfrom collections import Counter\n\n# do the clusteirng\ndef apply_dbscan_clustering(gdf_poi, eps):\n\n    feature_matrix = gdf_poi['geometry'].apply(lambda geom: (geom.x, geom.y)).tolist()\n    dbscan = DBSCAN(eps=eps, min_samples=1)  # You can adjust min_samples as needed\n    cluster_labels = dbscan.fit_predict(feature_matrix)\n    gdf_poi['cluster_id'] = cluster_labels\n\n    return gdf_poi\n\n# transforming to local crs\ngdf_poi_filt = gdf_poi.to_crs(23700)    \n\n# do the clustering\neps_value = 50  \nclustered_gdf_poi = apply_dbscan_clustering(gdf_poi_filt, eps_value)\n\n# Print the GeoDataFrame with cluster IDs\nprint('Number of clusters found: ', len(set(clustered_gdf_poi.cluster_id)))\nclustered_gdf_poi\n```", "```py\nclusters = clustered_gdf_poi.cluster_id.to_list()\nclusters_cnt = Counter(clusters).most_common()\n\nf, ax = plt.subplots(1,1,figsize=(8,4))\nax.hist([cnt for c, cnt in clusters_cnt], bins = 20)\nax.set_yscale('log')\nax.set_xlabel('Cluster size', fontsize = 14)\nax.set_ylabel('Number of clusters', fontsize = 14)\n```", "```py\nto_keep = [c for c, cnt in Counter(clusters).most_common() if cnt>9]\nclustered_gdf_poi = clustered_gdf_poi[clustered_gdf_poi.cluster_id.isin(to_keep)]\nclustered_gdf_poi = clustered_gdf_poi.to_crs(4326)\nlen(to_keep)\n```", "```py\nimport folium\nimport random\n\n# get the centroid of the city and set up the map\nmin_longitude, min_latitude, max_longitude, max_latitude = clustered_gdf_poi.total_bounds\nm = folium.Map(location=[(min_latitude+max_latitude)/2, (min_longitude+max_longitude)/2], zoom_start=14, tiles='CartoDB Dark_Matter')\n\n# get unique, random colors for each cluster\nunique_clusters = clustered_gdf_poi['cluster_id'].unique()\ncluster_colors = {cluster: \"#{:02x}{:02x}{:02x}\".format(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for cluster in unique_clusters}\n\n# visualize the pois\nfor idx, row in clustered_gdf_poi.iterrows():\n    lat = row['geometry'].y\n    lon = row['geometry'].x\n    cluster_id = row['cluster_id']\n    color = cluster_colors[cluster_id]\n\n    # create a dot marker \n    folium.CircleMarker(\n        location=[lat, lon],\n        radius=3, \n        color=color,\n        fill=True,\n        fill_color=color,\n        fill_opacity=0.9,  \n        popup=row['amenity'], \n    ).add_to(m)\n\n# show the map\nm\n```", "```py\nimport math\nimport pandas as pd\n\ndef get_entropy_score(tags):\n    tag_counts = {}\n    total_tags = len(tags)\n    for tag in tags:\n        if tag in tag_counts:\n            tag_counts[tag] += 1\n        else:\n            tag_counts[tag] = 1\n\n    tag_probabilities = [count / total_tags for count in tag_counts.values()]\n    shannon_entropy = -sum(p * math.log(p) for p in tag_probabilities)\n    return shannon_entropy\n\n# create a dict where each cluster has its own list of amenitiy\nclusters_amenities = clustered_gdf_poi.groupby(by = 'cluster_id')['amenity'].apply(list).to_dict()\n\n# compute and store the entropy scores\nentropy_data = []\nfor cluster, amenities in clusters_amenities.items():\n    E = get_entropy_score(amenities)\n    entropy_data.append({'cluster' : cluster, 'size' :len(amenities), 'entropy' : E})\n\n# add the entropy scores to a dataframe\nentropy_data = pd.DataFrame(entropy_data)\nentropy_data\n```", "```py\nentropy_data.corr()\n```", "```py\n# packing the poi profiles into dictionaries\nclusters = sorted(list(set(clustered_gdf_poi.cluster_id)))\namenity_profile_all = dict(Counter(clustered_gdf_poi.amenity).most_common())\namenity_profile_all = {k : v / sum(amenity_profile_all.values()) for k, v in amenity_profile_all.items()}\n\n# computing the relative frequency of each category\n# and keeping only the above-average (>1) and top 3 candidates\nclusters_top_profile = {}\nfor cluster in clusters:\n\n    amenity_profile_cls = dict(Counter(clustered_gdf_poi[clustered_gdf_poi.cluster_id == cluster].amenity).most_common() )\n    amenity_profile_cls = {k : v / sum(amenity_profile_cls.values()) for k, v in amenity_profile_cls.items()}\n\n    clusters_top_amenities = []\n    for a, cnt in amenity_profile_cls.items():\n        ratio = cnt / amenity_profile_all[a]\n        if ratio>1: clusters_top_amenities.append((a, ratio))\n        clusters_top_amenities = sorted(clusters_top_amenities, key=lambda tup: tup[1], reverse=True)\n        clusters_top_amenities = clusters_top_amenities[0:min([3,len(clusters_top_amenities)])]\n\n    clusters_top_profile[cluster] = [c[0] for c in clusters_top_amenities]\n\n# print, for each cluster, its top categories:\nfor cluster, top_amenities in clusters_top_profile.items():\n    print(cluster, top_amenities)\n```"]