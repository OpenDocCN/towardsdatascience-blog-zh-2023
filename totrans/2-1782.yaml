- en: Reshaping the Model’s Memory without the Need for Retraining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296](https://towardsdatascience.com/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| AI | LARGE LANGUAGE MODELS| MACHINE UNLEARNING|'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Erasing any echo of problematic content a large language model has learned
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)
    ·11 min read·Oct 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d98c180635de8d1ec4607a94fdfa029.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Drew Saurus](https://unsplash.com/@drew_saurus?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: “To forgive is wisdom, to forget is genius. ”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ― **Joyce Cary**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Large language models](https://en.wikipedia.org/wiki/Large_language_model)
    (LLMs) have taken the world by storm. In less than a year they are ubiquitous
    and are now used by millions of users. These models are often trained with huge
    amounts of text (including problematic material and sensitive data). How do you
    make a model forget? The same that could store the entirety of human knowledge?'
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to forget
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c7c7cf08a9381a67f4ce29c3f641e677.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Paul Pastourmatzis](https://unsplash.com/@pueblovista?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: LLMs stand as a testament to both our accomplishments and the challenges that
    lie ahead — [source](https://arxiv.org/pdf/2310.02238.pdf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LLMs have surprised both users and researchers with their ability to learn from
    huge amounts of text and identify language patterns and cultural nuances. While
    they could be the basis for a new application and scientific revolution, they
    have a dark side.
  prefs: []
  type: TYPE_NORMAL
- en: 'Huge [corpora](https://it.wikipedia.org/wiki/Corpus) must be used to train
    these patterns. While it is true that the greater the amount of data used the
    better the performance of an LLM, collecting this data is expensive. To limit
    costs, indiscriminate scraping of data from the Internet is often used. These
    corpora therefore also contain [extremely problematic data](https://hiddenlayer.com/research/the-dark-side-of-large-language-models/):
    copyrighted texts, toxic or malicious data, inaccurate or fake content, personal
    data, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b63f4540244b687eff11ef74d8cfc459.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2307.03941.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/machine-unlearning-the-duty-of-forgetting-3666e5b9f6e5?source=post_page-----9ade69f56296--------------------------------)
    [## Machine unlearning: The duty of forgetting'
  prefs: []
  type: TYPE_NORMAL
- en: How and why it is important to erase data point information from an AI model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/machine-unlearning-the-duty-of-forgetting-3666e5b9f6e5?source=post_page-----9ade69f56296--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have the ability to store all this information and [to leak them once they
    are queried](https://arxiv.org/abs/2307.10476). This opens up enormous ethical
    and even legal risks. In addition, this has led to lawsuits, public pressure,
    and the focus of legislative discussions.
  prefs: []
  type: TYPE_NORMAL
- en: To date, through [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)),
    we know that we can reinforce the specific knowledge of a model. However, if we
    wanted a model to forget specific information, we would have to retrain the model.
    The problem is that training an [LLM costs millions of dollars](https://www.forbes.com/sites/craigsmith/2023/09/08/what-large-models-cost-you--there-is-no-free-ai-lunch/)
    and is time-intensive.
  prefs: []
  type: TYPE_NORMAL
- en: '**How do you get an LLM to forget?**'
  prefs: []
  type: TYPE_NORMAL
- en: In general, [machine unlearning](https://arxiv.org/abs/1912.03817) is an active
    field of research. Most studies focus on classification tasks and only a few studies
    are on generative AI or LLMs. [LLMs are particularly problematic](https://arxiv.org/abs/2307.03941)
    because it is difficult to understand from where personal data (chat history or
    training data) were acquired and in what parameters they are stored. Removing
    data from a trained model is extremely complex, as model weights are a complex
    integration of the whole collection of [training data](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets).
  prefs: []
  type: TYPE_NORMAL
- en: '[An interesting approach that has recently been proposed](https://arxiv.org/abs/2210.01504)
    is that we fine-tune the model with the text we want to forget. In this case,
    we negate the [loss function](https://en.wikipedia.org/wiki/Loss_function), in
    other words, we penalize the model when it predicts as the next word in the text
    what we want to forget.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a95a62e4153b7ca7be31ba53c4bd67d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2210.01504.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**As simple and effective as this approach seems, it actually has limitations.**
    For example, if the text we want to forget is my bio: “*My name is Salvatore…*”
    the model will forget not only “*Salvatore*” but also “*my name is.*” In other
    words, this model forgets general knowledge about language.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we are interested in looking for an approach that instead of penalizing
    some text, shifts the model from predicting personal data to giving a generic
    answer (as if it had never encountered) personal data.
  prefs: []
  type: TYPE_NORMAL
- en: So we want a model that is able to effectively forget about the problem text,
    but at the same time retain its skills and the rest of its knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: How to forget Harry Potter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/5023274902f825b5d9c14ca8a3d8e695.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Dollar Gill](https://unsplash.com/@dollargill?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '**“It does not do to dwell on dreams and forget to live” — Albus Dumbledore
    in the** *Sorcerer’s Stone*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Recently an article dealt with how you can make a model forget an entire book
    without impacting LLM performance. The authors show how a model can forget the
    complex plot of [Harry Potter](https://en.wikipedia.org/wiki/Harry_Potter) and
    at the same time manage to maintain performance in benchmark datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2310.02238?source=post_page-----9ade69f56296--------------------------------)
    [## Who''s Harry Potter? Approximate Unlearning in LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) are trained on massive internet corpora that often
    contain copyrighted content. This poses…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2310.02238?source=post_page-----9ade69f56296--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: We can consider that an LLM is trained on text dataset X and we want it to forget
    text subset Y. Through fine tuning we can obtain a model that has enhanced knowledge
    of Y. This model will be an expert on subject Y. The traditional method would
    be to retrain the LLM on X-Y but this would require a lot of time and computational
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: We want a model that preserves its general knowledge and understanding of the
    language. Therefore, the authors decided to exploit the expert model to help an
    LLM to forget.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to understand what a generic prediction would be. For the
    authors of this paper, a generic prediction for a sentence such as “*He looks
    at the scar on his__*” is the difference between an expert model (which has a
    thorough understanding of what we want to forget) and the baseline model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In simple words, the authors took an LLM ([LLaMA-7B](https://ai.meta.com/blog/large-language-model-llama-meta-ai/))
    as a baseline and fine-tuned it to Harry Potter (expert model). After that, a
    prompt is given to the two models (“*He looks at the scar on his__*”), and a vector
    v of predictions (logit) is obtained for each, the generic prediction is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Using [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) and
    a constant α allows us to extract only the predictions specific to the expert
    model. This is to prevent the model from forgetting “*He looks at the scar on
    his*” but only “*forehead*” (i.e., where Harry Potter has his scar).
  prefs: []
  type: TYPE_NORMAL
- en: Is it enough?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: No, because forgetting a book is not just forgetting the name of a protagonist
    or a specific term (also because by varying the prompts one could still access
    this knowledge). The idea is that our model forgets in a deeper way. For authors,
    this can be achieved by destroying links between entities in the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, the authors extracted the various entities in the book with
    [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and translated them with names or
    entities that are idiosyncratic to the text. These are terms that are consistent
    but not specific to the book, as you can see from the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0eb755e98a512c0ad10f11b6c09cb4c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2310.02238.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: This serves to steer the model away conceptually from predicting Harry Potter-related
    content toward more general texts that are at the same time consistent with the
    textual input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Combining these two elements together, the process is divided into four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to create a dictionary where we map specific elements of the text to
    generic translations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We get blocks of text (depending on the [context length](https://www.linkedin.com/pulse/expanding-context-lengths-llms-towards-causalgpt-vs-bard-butvinik/)
    of the chosen LLM). We do the block mapping with our dictionary we get the prediction
    with the expert model for the original text and the model baseline prediction
    for the mapped text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We combine the predictions of these two models with the equation described above
    (eq .1) and thus obtain the generic predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the last step we conduct fine-tuning of the baseline model using the original
    test as input and the generic labels as target tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/c30df9ffab685b871f614590e2d73b69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2310.02238.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Did our model forget about magic?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c66142d8e57dc9420cf65370acb3e602.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Artem Maltsev](https://unsplash.com/@art_maltsev?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: “The trick to forgetting the big picture is to look at everything close-up.”
    — Chuck Palahniuk
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The authors chose LLaMA-2 in the 7B version as the model, both because it was
    open-source and because it showed excellent capabilities despite its limited size.
    The training of the original model (pretraining phase with a huge corpus of text)
    required 184K GPU hours, while the forgetting process proposed by the authors
    requires only 1 GPU hour (thus definitely inexpensive in terms of resources and
    affordable for anyone).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/meta-llama-2-0-the-most-disruptive-ainimal-d465ef187f2?source=post_page-----9ade69f56296--------------------------------)
    [## META LLaMA 2.0: the most disruptive AInimal'
  prefs: []
  type: TYPE_NORMAL
- en: Meta LLaMA can reshape the chatbot and LLM usage landscape
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/meta-llama-2-0-the-most-disruptive-ainimal-d465ef187f2?source=post_page-----9ade69f56296--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to assess whether the model actually retained information
    about the Harry Potter book (e.g., “*When Harry returned to class, he observed
    his best friends__*”). To be sure of this, the authors created a series of [textual
    prompts](https://en.wikipedia.org/wiki/Prompt_engineering) that the model had
    to complete based on its internal knowledge. In addition, they created prompts
    to check whether the model was familiar with what was described in the books (e.g.,
    “*Draft a brief narrative in the style of Harry Potter. Short story:*”). As can
    be seen, the model that went through the forgetting process seems to no longer
    be able to recall elements from the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8866a1bf08d6d33850a11304d9cf20de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2310.02238.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors manually evaluated not only how the model had completed the sentences
    but also the probabilities associated with a given [token](https://en.wikipedia.org/wiki/Lexical_analysis#Token).
    For example, considering the sentence “*Harry Potter studies__”* the authors saw
    whether the words “*magic*” or “*wizardry*” were among the highest probability
    tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The results show that the probability of the next token decreases significantly
    with each fine-tuning step. The lower the probability of a token the less likely
    it will be selected, even by changing prompts. According to the authors, only
    120 [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) fine-tuning
    steps are needed for optimal results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f6fffe8ff9c61a322787f303eb6a1896.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2310.02238.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model seems to have forgotten the book and provided generic answers. The
    question remains: **has the forgetting process impacted the model’s general skills
    and knowledge?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, the authors used three benchmark datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**WinoGrande**](https://arxiv.org/abs/1907.10641)is a benchmark for commonsense
    reasoning (273 expert-crafted resolution problems).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**HellaSwag**](https://arxiv.org/abs/1905.07830) is a dataset of sentences
    to complete that are trivial to humans but not to computers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**PIQA**](https://arxiv.org/abs/1911.11641), a dataset for commonsense reasoning
    created to investigate the physical knowledge of existing LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**BoolQ**](https://arxiv.org/abs/1905.10044) is a large question-answering
    dataset (yes/no) where the model is provided a question, and the context, and
    has to provide an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**OpenBookQA**](https://arxiv.org/abs/1809.02789), a question-answering dataset
    modeled after open book exams for assessing human understanding of a subject.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**ARC**](https://arxiv.org/abs/1803.05457), multiple-choice question-answering
    dataset containing questions from science exams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/7782eae7dfd36ac960f4663d2c36bf69.png)'
  prefs: []
  type: TYPE_IMG
- en: example of questions in the datasets. adapted from the original articles ([here](https://arxiv.org/abs/1905.10044),
    [here](https://arxiv.org/abs/1809.02789), and [here](https://arxiv.org/abs/1905.07830))
  prefs: []
  type: TYPE_NORMAL
- en: The results show that show that performance is minimally impacted by the unlearning
    process. Obviously, a greater number of [gradient steps](https://en.wikipedia.org/wiki/Gradient_descent)
    decreases familiarity with the topic but also impacts performance more.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/715e169c5137ecb5a8293c67986f64c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2310.02238.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this study has limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: There are occasional leaks (if you ask the model for the names of magic schools
    it suggests Hogwarts). Since the authors used the books as text (but there are
    also movies and theme parks dedicated to the world of Harry Potter) this could
    simply mean Wikipedia-level knowledge rather than actual leaks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, more sophisticated prompting techniques could lead the model to reveal
    information. It should therefore be tested with adversarial attacks or other prompting
    techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The method uses [GPT-4](https://openai.com/research/gpt-4) and thus its knowledge
    of Harry Potter, but in other cases, this is not possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Harry Potter books have a universe rich in characters, peculiar expressions,
    and precise themes. While the method seems to work well with a fictional topic,
    other topics do not have such rich lexical content or are much more abstruse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The authors aware of the limitations invite the community to try and test the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing the intrinsic limitations of automated benchmarks and internal evaluations,
    we believe that unlearning verification parallels endeavors like jailbreaking
    in adversarial nature. Therefore, we open-sourced the model, encouraging the broader
    community to challenge it, providing a more diverse and extensive set of tests
    to discern if any remnants of the targeted knowledge persist. (source)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The model is stored on HuggingFace and is available here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[## microsoft/Llama2-7b-WhoIsHarryPotter · Hugging Face'
  prefs: []
  type: TYPE_NORMAL
- en: We're on a journey to advance and democratize artificial intelligence through
    open source and open science.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: huggingface.co](https://huggingface.co/microsoft/Llama2-7b-WhoIsHarryPotter?source=post_page-----9ade69f56296--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Parting thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/95710cc7ff409c95285f071c4b038114.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Saif71.com](https://unsplash.com/@saif71?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: “The advantage of a bad memory is that one enjoys several times the same good
    things for the first time.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ― **Friedrich Nietzsche**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Forgetting something intentionally is a difficult challenge even for humans.
    This is also difficult for LLMs. As the study of grokking showed, there is a difference
    between memorizing and learning.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/grokking-learning-is-generalization-and-not-memorization-52c43c9025e4?source=post_page-----9ade69f56296--------------------------------)
    [## Grokking: Learning Is Generalization and Not Memorization'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how a neural network learns helps us to avoid that the model from
    forgetting what it learns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/grokking-learning-is-generalization-and-not-memorization-52c43c9025e4?source=post_page-----9ade69f56296--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Initial studies tried to make the model forget by eliminating what it memorized.
    This impacted his general knowledge and his understanding of language itself.
    This new study shows how it is not enough to focus on the key terms of a concept
    (for example, the main characters in Harry Potter) but also on the concept itself
    (the plot for example).
  prefs: []
  type: TYPE_NORMAL
- en: The authors show how the model loses familiarity with Harry Potter and at the
    same time maintains its performance in reasoning benchmarks. Although this method
    is not perfect and has only been tested on a limited case, it opens up some very
    interesting perspectives. Indeed, pretraining datasets are full of toxic comments,
    stereotypes, biases, and hateful speech. This is the first step in being able
    to allow a model to unlearn this content without re-training.
  prefs: []
  type: TYPE_NORMAL
- en: What do think? Let me know in the comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have found this interesting:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, you can* [***become a Medium member***](https://medium.com/@salvatore-raieli/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----9ade69f56296--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----9ade69f56296--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*or you may be interested in one of my recent articles:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/scaling-data-scaling-bias-a-deep-dive-into-hateful-content-and-racial-bias-in-generative-ai-70d8aa27a631?source=post_page-----9ade69f56296--------------------------------)
    [## Scaling Data, Scaling Bias: A Deep Dive into Hateful Content and Racial Bias
    in Generative AI'
  prefs: []
  type: TYPE_NORMAL
- en: 'scaling seems the solution for every issue in machine learning: but it is true?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/scaling-data-scaling-bias-a-deep-dive-into-hateful-content-and-racial-bias-in-generative-ai-70d8aa27a631?source=post_page-----9ade69f56296--------------------------------)
    [](https://levelup.gitconnected.com/tabula-rasa-why-do-tree-based-algorithms-outperform-neural-networks-db641862859b?source=post_page-----9ade69f56296--------------------------------)
    [## Tabula Rasa: Why Do Tree-Based Algorithms Outperform Neural Networks'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tree-based algorithms are the winner in tabular data: Why?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/tabula-rasa-why-do-tree-based-algorithms-outperform-neural-networks-db641862859b?source=post_page-----9ade69f56296--------------------------------)
  prefs: []
  type: TYPE_NORMAL
