- en: Turn GPT-4 into a Poker Coach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/turn-gpt-4-into-a-poker-coach-4a28ba5e9541](https://towardsdatascience.com/turn-gpt-4-into-a-poker-coach-4a28ba5e9541)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unleashing Creativity Beyond Chatbot Boundaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jacky.kaub?source=post_page-----4a28ba5e9541--------------------------------)[![Jacky
    Kaub](../Images/e66c699ee5a9d5bbd58a1a72d688234a.png)](https://medium.com/@jacky.kaub?source=post_page-----4a28ba5e9541--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4a28ba5e9541--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4a28ba5e9541--------------------------------)
    [Jacky Kaub](https://medium.com/@jacky.kaub?source=post_page-----4a28ba5e9541--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4a28ba5e9541--------------------------------)
    ·13 min read·May 5, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68f895c49f1100cc14bb864bc37349e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Michał Parzuchowski](https://unsplash.com/@mparzuchowski?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will not talk about how LLM models can pass a law exam or
    replace a developer.
  prefs: []
  type: TYPE_NORMAL
- en: We will not look at hints on optimizing prompts for making GPT do motivation
    letters or marketing content.
  prefs: []
  type: TYPE_NORMAL
- en: Like many people, I think that the emergence of the LLM like GPT4 is a little
    revolution from which a lot of new applications will emerge. I also think that
    we should not reduce their use to simple “chatbot assistants” and that with the
    appropriate backend and UX, those models can be leveraged to incredible next-level
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: This is why, in this article, we are going to think a bit out of the box and
    create a real application around the GPT API that could not be accessed simply
    via the chatbot interface and how a proper app design could serve a better user
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up some context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leveraging GPT4 in businesses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I played a lot with GPT4 since its release and I think there is globally two
    main families of use cases for using the model to generate a business.
  prefs: []
  type: TYPE_NORMAL
- en: The first way is to use GPT4 to generate static content. Say you want to write
    a cooking book with a particular theme (for example Italian food). You can make
    detailed prompts, generate a few recipes from GPT, try them yourself, and integrate
    the one you like in your book. In that case “prompting” will have a fixed cost
    and once the recipes are generated you don’t need GPT anymore. This type of use
    case can find a lot of variation (Marketing content, website content, or even
    generating some datasets for other uses), but is not as interesting if we want
    to focus on AI-oriented apps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89df1ab72515de113e4265531f798b4f.png)'
  prefs: []
  type: TYPE_IMG
- en: The logic of generating the content is outside the application, Author Illustration
  prefs: []
  type: TYPE_NORMAL
- en: 'The second use case is live prompting through an interface of your design.
    Going back to the cooking field: we could imagine a well-suited interface in which
    a user can pick up a few ingredients, a specialty, and ask the application to
    generate directly the recipe. Unlike in the first case, the content generated
    can be potentially infinite and suit better the needs of your users.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f109dfd3cc45954a6ddfb8e3ba164d3.png)'
  prefs: []
  type: TYPE_IMG
- en: In this scenario, the user interacts directly with the LLM via a well-designed
    UX which will generate prompts and content, Author Illustration
  prefs: []
  type: TYPE_NORMAL
- en: The drawback of this is that the number of calls to the LLM will be potentially
    infinite and grow with the number of users, unlike before where the amount of
    calls to the LLM was finite and controlled. This implies that you will have to
    design properly your business model and take a lot of care into including the
    cost of prompts in your business model.
  prefs: []
  type: TYPE_NORMAL
- en: As of when I am writing these lines, GPT4 “prompt” costs 0.03$/1000 tokens (with
    both request and answer tokens counted in the pricing). It does not seem like
    a lot, but could quickly escalate if you don’t pay attention to it. To work around
    this, you could for example propose to your user a subscription depending on the
    amount of prompts or limited the amount of prompts per user (via a login system
    etc…). We will talk a bit more in detail about pricing later in this article.
  prefs: []
  type: TYPE_NORMAL
- en: Why a use-case around Poker?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I thought for some time of the perfect use case to try around LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: First, poker analysis is theoretically a field in which LLM should perform well.
    In fact, every poker hand played can be translated into a standardized simple
    text describing the evolution of the hand. For example, the hand below describes
    a sequence in which “player1” win the pot after making a raise on the bet of “player2”
    after the “flop” action.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This standardization is important because it will make the development more
    simple. We will be able to simulate hands, translate them into this kind of prompt
    message, and “force” the answer of the LLM to continue the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of theoretical content is available in books, online, etc… Making it likely
    that GPT has “learned” things around the game and good moves.
  prefs: []
  type: TYPE_NORMAL
- en: Also, a lot of added value will come from the app engine and the UX, and not
    only from the LLM itself (for example we will have to design our own poker engine
    to simulate a game), which will make the application harder to replicate, or to
    simply “reproduce” via GPTChat.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the use case might adapt well to the second case scenario described
    above, where the LLM and a good UX can bring a completely new experience to users.
    We could imagine our application playing hands again a real user, analyzing hands
    and also giving rates and areas of improvement. The price per request should not
    be a problem as poker learners are used to paying for this kind of service, so
    a “pay as you use” might be possible in this particular use case (unlike the recipe
    concept app mentioned earlier for example)
  prefs: []
  type: TYPE_NORMAL
- en: About GPT4 API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I decided to build this article around [GPT4 API](https://openai.com/product/gpt-4)
    for its accuracy in comparison to GPT3.5\. OpenAI provides a simple Python wrapper
    that can be used to send your inputs and receive your outputs from the model.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The “pre-prompt” used with the role “system” will help the model to act the
    way you want him to act (you can use it typically to enforce a response format),
    the role “user” is used to add the message from the user. In our case, those messages
    will be pre-designed by our engine, for example, passing a particular poker hand
    to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Note that all the tokens from “system”, “user” and from the answer are counted
    in the price scheme, so it is really important to optimize those queries as much
    as you can.
  prefs: []
  type: TYPE_NORMAL
- en: Initial Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing to assess is the general quality of GPT for the different tasks
    related to poker. I want to evaluate quickly its ability to realize different
    tasks that would be part of the core of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing a hand with a realistic move and explanations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the core ideas of the application will be to make one or several instances
    of GPT play against a user. We want to assess how well GPT plays. In order to
    do so, I provided the model samples of hands I played for which I wanted an analysis
    regarding the next move and the explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: I performed several tests like this, validating the fact that the model would
    be able to play decently.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, by modifying the pre-prompt, I could control the behavior of
    the model making him play more or less aggressively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'would lead, for the same action, to a different move from the AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This means we could potentially code different “AI” playing with different styles
    to simulate a real poker field with players with different levels and styles.
  prefs: []
  type: TYPE_NORMAL
- en: Rating the actions of a player from a full-hand history
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A feature we might want to integrate into our application is the possibility
    to rate the user. Grades are always a good indicator of progress or a good way
    to target particular weaknesses when learning something.
  prefs: []
  type: TYPE_NORMAL
- en: In order to be processed correctly by our application, we would have to “enforce”
    the answer of the bot so that it can be simply parsed and used in the application
    (for example averaging over sessions or over thematics).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following combination of prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Would generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is nice for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: We can control the number of tokens displayed by the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can easily parse the answer in a dictionary to be used by our app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming the idea into a concrete application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After validating the two points above, it is time to code something bigger to
    demonstrate the whole concept and see how the LLM can be integrated into a larger
    application that diverges from a simple chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of this article, we will go with something fairly simple but
    it should give you a hint about all the capabilities behind LLMs when used out
    of the box.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of our demonstrator will be to make GPT play a hand against us, and
    then, once the hand has been completely played, ask the model to give us some
    hints on how to improve based on that hand only.
  prefs: []
  type: TYPE_NORMAL
- en: For this purpose, I quickly coded a simple poker engine that will help simulate
    a hand and play against an AI opponent. I will not go too much into the detail
    of the engine here (which is not the point of the article), but simply give you
    a brief overview of its design.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0cb21081d6bd88ce88d410e40d6ecd06.png)'
  prefs: []
  type: TYPE_IMG
- en: Schematic for the Poker Coach proof of concept
  prefs: []
  type: TYPE_NORMAL
- en: The role of the poker engine is to keep track of the game's general metadata
    (number of players, stacks, cards distributed, player turn…). The next action
    (both from a player or an AI) is added to the engine as a standard text message
    such as “call”, “fold”, or “raise 60”, etc… which is parsed and transcribed as
    new input for the engine to run the next step.
  prefs: []
  type: TYPE_NORMAL
- en: The poker engine also transcribes the sequence of action into a text file which
    is used to feed the prompt used by the AI to take a decision on the next move.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two pre-prompts will be used: one for deciding on the next action to take,
    and one, used at the end of the hand, to rate the actions of the human player.'
  prefs: []
  type: TYPE_NORMAL
- en: Playing the hand
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order for our application to understand the action from GPT, we need to
    make sure that the message of GPT is standardized. This can be enforced with the
    pre-prompt. In my case I expect GPT to answer me with a maximum of two words “ACTION
    AMOUNT”, and I am enforcing this behavior by providing an example of the model
    to which it would adapt with this simple playing pre-prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'I will combine this pre-prompt with the actual hand history generated by the
    poker engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This leads to a suitable answer that can be parsed and integrated into the
    poker engine to update the game and continue with the next action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The application in action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Put in snapshots, it might not look impressive, so I recorded a little video
    about the AI in action in a notebook. In the context of the video, I also asked
    the model to justify its move before taking action. The cards of all the players
    are visible in the widget for debugging purposes, but in real conditions, they
    would be hidden from the player. When the AI takes the action, only its own cards
    are passed to the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: My message parser simply takes the action after the “//” separator and I used
    the input() function to register the user input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/647e4c78ec1819bc40ba998dd0149e8e.png)'
  prefs: []
  type: TYPE_IMG
- en: GPT playing a hand of poker against me, Author illustration
  prefs: []
  type: TYPE_NORMAL
- en: Hand review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the head is completed, the engine has generated the whole hand that can
    be passed to our “evaluation module” where the LLM will rate the hand.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the prompt designed earlier, this is the kind of answer, easy to parse,
    that we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This output could be easily parsed and data stored in a DB from which we could
    extract all sorts of analysis for the player like identifying weaknesses depending
    on hand, position, configuration, etc… based on average grades provided by the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we proved with an example that new concepts centered on LLMs
    could emerge when coupled with well-suited app designs which diverge from the
    simple chatbot assistant.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the example above would be only the first step out of many before
    having a fully ready application, but it should be sufficient to bring food for
    thoughts and creativity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extrapolating a bit about how would look the next steps toward the development
    of a real application for a poker coach, we would have to take several actions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt token optimization: a large part of the costs of our application would
    come from prompt pricing. Optimizing the prompts in terms of quantity is essential
    to be able to reduce costs and be competitive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt content optimization: the quality of the output (rating, next action)
    can vary a lot depending on the context you provide to the model. For example,
    asking the model to first make an analysis before taking an action can improve
    a lot the coherence of the action taken. Many tests and iterations, in partnership
    with real players, should be needed to make sure the quality of the output is
    sufficient for a production-level application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Error handling: even if the outputs provided by the LLM fit your template most
    of the time, it is also important to handle the cases in which the model will
    provide an answer which does not fit your parser. Unlike all your functions, this
    part of the application can stay unpredictable, and it would be important to add
    an extra layer of control to make sure no bugs happen due to bad formats or impossible
    answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'User interface: while playing in a notebook with text inputs is enough for
    exploration like in this article, a core element would be a clean UX to sublime
    the user experience and make the interaction with the model and the engine smooth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While prices for this family of applications might be still high, I am sure
    that mass adoption and future improvements will tend to reduce their cost. We
    are only at the beginning of the development of those new technologies and it’s
    up to us to be creative and embrace the potential of LLMs, transforming them into
    powerful tools for innovative and user-centric experiences beyond traditional
    chatbot limitations.
  prefs: []
  type: TYPE_NORMAL
