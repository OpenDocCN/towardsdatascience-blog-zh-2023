- en: 'Retro-Engineering a Database Schema: GPT vs. Bard vs. LLama2 (Episode 2)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据库模式的逆向工程：GPT 与 Bard 与 Llama2（第2集）
- en: 原文：[https://towardsdatascience.com/retro-engineering-a-database-schema-gpt-vs-bard-vs-llama2-episode-2-e7f144a6753b](https://towardsdatascience.com/retro-engineering-a-database-schema-gpt-vs-bard-vs-llama2-episode-2-e7f144a6753b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/retro-engineering-a-database-schema-gpt-vs-bard-vs-llama2-episode-2-e7f144a6753b](https://towardsdatascience.com/retro-engineering-a-database-schema-gpt-vs-bard-vs-llama2-episode-2-e7f144a6753b)
- en: In my previous article, I benchmarked GPT-4 model against Bard. Now Llama-2
    enters the arena and it’s high time we see how it performs against its competitors!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在我之前的文章中，我对比了 GPT-4 模型和 Bard。现在 Llama-2 进入了竞技场，是时候看看它与竞争对手的表现了！
- en: '[](https://pl-bescond.medium.com/?source=post_page-----e7f144a6753b--------------------------------)[![Pierre-Louis
    Bescond](../Images/bb236055962b420fb3ab22088ab28f11.png)](https://pl-bescond.medium.com/?source=post_page-----e7f144a6753b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e7f144a6753b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e7f144a6753b--------------------------------)
    [Pierre-Louis Bescond](https://pl-bescond.medium.com/?source=post_page-----e7f144a6753b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pl-bescond.medium.com/?source=post_page-----e7f144a6753b--------------------------------)[![皮埃尔-路易斯·贝斯孔](../Images/bb236055962b420fb3ab22088ab28f11.png)](https://pl-bescond.medium.com/?source=post_page-----e7f144a6753b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e7f144a6753b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e7f144a6753b--------------------------------)
    [皮埃尔-路易斯·贝斯孔](https://pl-bescond.medium.com/?source=post_page-----e7f144a6753b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e7f144a6753b--------------------------------)
    ·6 min read·Oct 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e7f144a6753b--------------------------------)
    ·6 分钟阅读·2023 年 10 月 6 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2941f397ecb8ea1cf96ab9c325122f57.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2941f397ecb8ea1cf96ab9c325122f57.png)'
- en: Photo by [Dustin Humes](https://unsplash.com/@dustinhumes_photography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [达斯汀·休姆斯](https://unsplash.com/@dustinhumes_photography?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The Initial (and Final) Dataset
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始（和最终）数据集
- en: As explained [in this first article](https://medium.com/p/2e2776e8af86), we’ll
    start with a fake AI-generated dataset containing employees’ information.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [这篇文章](https://medium.com/p/2e2776e8af86) 中所述，我们将从一个包含员工信息的假 AI 生成数据集开始。
- en: '[](/retro-engineering-a-database-schema-and-quality-checks-gpt-vs-bard-2e2776e8af86?source=post_page-----e7f144a6753b--------------------------------)
    [## Retro-engineering a database schema and quality checks: GPT vs. Bard'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[链接](https://retro-engineering-a-database-schema-and-quality-checks-gpt-vs-bard-2e2776e8af86?source=post_page-----e7f144a6753b--------------------------------)
    [## 数据库模式及质量检查的逆向工程：GPT 与 Bard'
- en: Can LLMs retro-engineer a consolidated dataset to design the original database
    and suggest the corresponding data…
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM 是否可以逆向工程一个合并的数据集来设计原始数据库，并提出相应的数据……
- en: towardsdatascience.com](/retro-engineering-a-database-schema-and-quality-checks-gpt-vs-bard-2e2776e8af86?source=post_page-----e7f144a6753b--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[链接](https://towardsdatascience.com/retro-engineering-a-database-schema-and-quality-checks-gpt-vs-bard-2e2776e8af86?source=post_page-----e7f144a6753b--------------------------------)'
- en: The original table has 11 columns x 7688 rows but we’ll limit the extract to
    a sample of 50 rows to accommodate current LLMs token limitations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 原始表格有 11 列 x 7688 行，但我们将提取 50 行样本，以适应当前 LLM 的令牌限制。
- en: '![](../Images/0c36c37aab2a4cf97caf94e274d64dcd.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c36c37aab2a4cf97caf94e274d64dcd.png)'
- en: Sample of the source data (Image by Author)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 来源数据样本（图片由作者提供）
- en: '*(Note: the notebook and data source are available at the end of the article)*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*(注意：笔记本和数据源在文章末尾提供)*'
- en: Retro-Engineering the Data Model
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据模型的逆向工程
- en: The idea here is to ask each LLM to analyze this sample data and provide some
    insight into what the initial data scheme might look like.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是让每个 LLM 分析这个样本数据，并提供有关初始数据方案可能是什么样的见解。
- en: 'We’ll keep the same prompt as the one we used for GPT-4 and Bard:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保持与 GPT-4 和 Bard 使用的相同提示：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Ok great! … But now the question is “Where can I test Llama-2?”
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！……但现在的问题是“我在哪里可以测试 Llama-2？”
- en: 'There are several options available:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种可用的选项：
- en: The most obvious one (but also more complex and expensive 💸) is to host the
    model on a dedicated server in your cloud architecture. This is usually a good
    option if you intend to serve heavy-duty applications.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最明显的一个（但也更复杂和昂贵 💸）是将模型托管在你云架构中的专用服务器上。如果你打算服务于重型应用程序，这通常是一个不错的选择。
- en: Keep in mind that the virtual machines required to run an LLM can go from 2–3$/hour
    for a small model… up to 18$/hour if you want to host LLama2–70b properly, according
    to Azure 😨
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，运行 LLM 所需的虚拟机从小模型的 2–3$/小时... 到要妥善托管 LLama2–70b 的 18$/小时不等，根据 Azure 的信息
    😨
- en: An intermediary solution — recommended [by Yann Lecun himself](https://www.linkedin.com/feed/update/urn:li:activity:7109561666324885504?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7109561666324885504%2C7111453478513733632%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287111453478513733632%2Curn%3Ali%3Aactivity%3A7109561666324885504%29)
    — is to use platforms like [Anyscale](https://www.anyscale.com/) which, for example,
    offers $1 for 1M token on Llama-2–70b-chat (when Azure pricing is between $30
    and $60 for GPT-4)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个中间解决方案——由 [Yann Lecun 本人](https://www.linkedin.com/feed/update/urn:li:activity:7109561666324885504?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7109561666324885504%2C7111453478513733632%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287111453478513733632%2Curn%3Ali%3Aactivity%3A7109561666324885504%29)
    推荐——是使用像 [Anyscale](https://www.anyscale.com/) 这样的平台，例如，它在 Llama-2–70b-chat 上提供
    1M 标记 $1（而 Azure 对 GPT-4 的定价在 $30 和 $60 之间）
- en: The awesome Hugging Face platform also offers us a testbed. A good way, for
    example, to keep our finances straight!
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 令人惊叹的 Hugging Face 平台还为我们提供了一个测试平台。例如，这是一个保持财务清晰的好方法！
- en: '[](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat?source=post_page-----e7f144a6753b--------------------------------)
    [## Llama 2 7B Chat - a Hugging Face Space by huggingface-projects'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat?source=post_page-----e7f144a6753b--------------------------------)
    [## Llama 2 7B Chat - 由 huggingface-projects 提供的 Hugging Face 空间'
- en: Discover amazing ML apps made by the community
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发现社区制作的令人惊叹的 ML 应用
- en: huggingface.co](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat?source=post_page-----e7f144a6753b--------------------------------)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: huggingface.co](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat?source=post_page-----e7f144a6753b--------------------------------)
- en: While writing this second episode, I also discovered [https://chat.lmsys.org/](https://chat.lmsys.org/)
    that allows you to execute one single prompt with two models in parallel. Perfect
    to benchmark their performance. This is the tool I’ll use for this article.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在写这第二集时，我还发现了 [https://chat.lmsys.org/](https://chat.lmsys.org/)，它允许你用两个模型并行执行一个单一的提示。非常适合基准测试它们的性能。这是我将在本文中使用的工具。
- en: '![](../Images/d98758be443f6eec0e3aabe30a2c800d.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d98758be443f6eec0e3aabe30a2c800d.png)'
- en: Photo by [Paz Arando](https://unsplash.com/@pazarando?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Paz Arando](https://unsplash.com/@pazarando?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Analyzing results (LLama-2)
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析结果（LLama-2）
- en: 'When running the benchmark on ChatGPT, we chose the most advanced and generally
    available version: 4.0.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在ChatGPT上运行基准测试时，我们选择了最先进且普遍可用的版本：4.0。
- en: In the same spirit, we’ll run our query on the “[LLama-2–70b-chat](https://ai.meta.com/llama/)”
    version which, as its name suggests, has a size of 70 billion parameters and a
    context window of 4096 tokens.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 出于相同的精神，我们将在“[LLama-2–70b-chat](https://ai.meta.com/llama/)”版本上运行我们的查询，该版本如其名称所示，具有70亿个参数和4096个标记的上下文窗口。
- en: Identifying Categorical and Confidential Data
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 识别分类和机密数据
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Even if one could challenge the fact that “Employee_ID” actually belongs to
    confidential data (but the more I think about it, the more it makes sense), LLama-2
    managed to find all categorical and confidential columns, by order of appearance
    ✅
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有人可能会质疑“Employee_ID”是否确实属于机密数据（但我越想越觉得有道理），LLama-2 还是成功找出了所有分类和机密列，按出现顺序 ✅
- en: Suggesting a database model
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提议一个数据库模型
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Again, Llama-2 does pretty well by properly converting categorical columns into
    sub-tables with key/label pairs. ✅
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，Llama-2 做得很好，将分类列正确转换为包含键/标签对的子表。 ✅
- en: Separating the confidential data into another table like ChatGPT did would have
    been a plus but that’s already a near result! 🔶
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将机密数据分离到另一个表中，就像 ChatGPT 所做的那样，会是一个加分项，但这已经是一个接近的结果！ 🔶
- en: Finally, the “age” column has disappeared from the suggested schema. ❌
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，“年龄”列已从建议的模式中消失。 ❌
- en: SQL Scripts to create tables
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建表的 SQL 脚本
- en: 'I’ll continue here with the “acid test” 🧪 of simply running these queries in
    Snowflake and checking what the result looks like:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在这里继续进行“酸性测试”🧪，即简单地在 Snowflake 中运行这些查询并检查结果是什么样的：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Besides the fact that “age” still missing, everything runs like a charm and
    all tables are created seamlessly in Snowflake! ✅
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了“年龄”仍然缺失之外，一切运行得非常顺利，所有表格在 Snowflake 中无缝创建！ ✅
- en: Data Quality Checks
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量检查
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, Llama-2 suggests some common wisdom:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Llama-2 提出了些常见的见解：
- en: Check for duplicates ✅
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查重复 ✅
- en: Check invalid characters ✅
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查无效字符 ✅
- en: '… and also tries to adapt to the nature of some fields:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: … 还尝试适应某些字段的性质：
- en: A valid email address format ✅
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效的电子邮件地址格式 ✅
- en: Negative or non-numeric values for “salary” & “annual_evaluation” ✅
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “薪资”与“年度评估”的负值或非数字值 ✅
- en: '… but also produces some mistakes:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: … 但也会产生一些错误：
- en: For example, it won’t be unusual to find identical first (or even last) names
    in the table so duplicates should be accepted ❌
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，在表中找到相同的名字（甚至是姓氏）并不罕见，因此应接受重复 ❌
- en: Looking back at what ChatGPT did, identifying ranges for columns like salary,
    age or even defining the acceptable values for the annual evaluation, Llama-2
    provides an “average” answer.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾 ChatGPT 的表现，识别诸如薪资、年龄等列的范围，甚至定义年度评估的可接受值，Llama-2 提供了一个“平均”的答案。
- en: 'Conclusion for Llama-2–70b:'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Llama-2–70b 的结论：
- en: Identifying Categorical and Confidential Data ✅
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别分类数据和机密数据 ✅
- en: Suggesting a database model ✅
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建议一个数据库模型 ✅
- en: SQL Scripts to create tables ✅
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建表的 SQL 脚本 ✅
- en: Data Quality Checks 🔶
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量检查 🔶
- en: Conclusion (Updated with Llama-2)
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论（更新了 Llama-2）
- en: GPT-4k clearly outperforms Bard and Llama-2 when it comes to understanding a
    dataset, designing a proper data model (here under the 3rd Normal Form (3NF)),
    writing the corresponding SQL queries, and suggesting data quality checks.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4k 在理解数据集、设计合适的数据模型（此处为第三范式 (3NF)）、编写相应的 SQL 查询以及建议数据质量检查方面明显优于 Bard 和 Llama-2。
- en: For this specific task (database retro-engineering), Llama-2 achieves a higher
    performance than Bard.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的任务（数据库逆向工程），Llama-2 的表现优于 Bard。
- en: Looking at the answers above, I believe that the insights provided by Llama-2
    are much more valuable than those provided by Bard.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 看看上面的答案，我认为 Llama-2 提供的见解比 Bard 提供的要有价值得多。
- en: '![](../Images/d6dbaa44d89f44de9848183352e5cb06.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6dbaa44d89f44de9848183352e5cb06.png)'
- en: 'Notes:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：
- en: In late August, Meta released a “Code Llama” model that we could also benchmark,
    perhaps in a future article.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在八月底，Meta 发布了一个“Code Llama”模型，我们也可以对其进行基准测试，也许在未来的文章中。
- en: I used the predefined temperature of 0.7 for the inference, meaning that executing
    the same queries with the same model could produce different results. I decided
    to simply keep the first result provided by the model, not to bias the analysis.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我使用了预定义的温度 0.7 进行推理，这意味着执行相同的查询可能会得到不同的结果。我决定仅保留模型提供的第一个结果，以避免偏倚分析。
- en: ⏩ Links to the corresponding [Jupyter Notebook](https://github.com/pierrelouisbescond/medium_articles/blob/main/medium_llm_db_retro_eng_load_and_format_data.ipynb)
    and [CSV Data Source](https://github.com/pierrelouisbescond/medium_articles/blob/main/Employees_Base.csv).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ⏩ 相关的 [Jupyter Notebook](https://github.com/pierrelouisbescond/medium_articles/blob/main/medium_llm_db_retro_eng_load_and_format_data.ipynb)
    和 [CSV 数据源](https://github.com/pierrelouisbescond/medium_articles/blob/main/Employees_Base.csv)
    链接。
- en: As usual, I have tried to identify all the necessary steps if you’d like to
    reproduce the analysis, but do not hesitate to contact me if there are any missing
    instructions in this tutorial!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我已经尽力识别出所有必要的步骤，如果你想重复分析，但如果教程中有任何缺失的说明，请随时联系我！
- en: 'And feel free to check out my other posts on Medium:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以查看我在 Medium 上的其他文章：
- en: '[](https://pl-bescond.medium.com/pierre-louis-besconds-articles-on-medium-f6632a6895ad?source=post_page-----e7f144a6753b--------------------------------)
    [## Pierre-Louis Bescond’s articles on Medium'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pl-bescond.medium.com/pierre-louis-besconds-articles-on-medium-f6632a6895ad?source=post_page-----e7f144a6753b--------------------------------)
    [## Pierre-Louis Bescond 的 Medium 文章'
- en: Data Science, Machine Learning and Innovation
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据科学、机器学习与创新
- en: pl-bescond.medium.com](https://pl-bescond.medium.com/pierre-louis-besconds-articles-on-medium-f6632a6895ad?source=post_page-----e7f144a6753b--------------------------------)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: pl-bescond.medium.com](https://pl-bescond.medium.com/pierre-louis-besconds-articles-on-medium-f6632a6895ad?source=post_page-----e7f144a6753b--------------------------------)
