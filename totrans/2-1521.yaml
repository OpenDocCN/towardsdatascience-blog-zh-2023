- en: 'ML Basics (Part-4): Decision Trees'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ml-basics-part-4-decision-trees-cc37d07137b2](https://towardsdatascience.com/ml-basics-part-4-decision-trees-cc37d07137b2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**What are Decision Trees, How to Build and Apply Decision Trees for Different
    Classification Tasks**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://azad-wolf.medium.com/?source=post_page-----cc37d07137b2--------------------------------)[![J.
    Rafid Siddiqui, PhD](../Images/02280890ed87239c75cbcbfa7c5d686c.png)](https://azad-wolf.medium.com/?source=post_page-----cc37d07137b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cc37d07137b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cc37d07137b2--------------------------------)
    [J. Rafid Siddiqui, PhD](https://azad-wolf.medium.com/?source=post_page-----cc37d07137b2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cc37d07137b2--------------------------------)
    ·8 min read·Jan 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/854204edb68a795f1f9cc7135d68d19a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Decision Tree Classifiers (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous articles, we have explored the concepts of [*Regression*](https://azad-wolf.medium.com/ml-basics-part-1-regression-a-gateway-method-to-machine-learning-36d54d233907),
    [*Support Vector Machines*](https://azad-wolf.medium.com/ml-basics-part-2-support-vector-machines-ac4defba2615)*,
    and* [*Artificial Neural Networks*](https://azad-wolf.medium.com/ml-basics-part-3-artificial-neural-networks-879851bcd217)*.*
    In this article, we shall go through another Machine Learning concept called,
    *Decision Trees*. You can check-out the other methods from the aforementioned
    links.
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees are perhaps one of the simplest and the most intuitive classification
    methods in a Machine Learning toolbox. The first occurrence of Decision Trees
    appeared in a publication by *William Belson* in 1959\. Earlier uses of Decision
    Trees were limited to Taxonomy for their natural semblance for that type of data.
    Later adaptations resulted in Decision Tree Classifiers which were not limited
    to a particular type of data (e.g., nominal) and were equally suited for other
    data types (e.g., numerical). Decision Trees are a simple yet effective tool for
    many classification problems and in some cases, can outperform other more complicated
    methods which might turn out to be overkill for a simple dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision Nodes, Constraints and Leaf Nodes**'
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree — as the name suggests, is a tree data structure with a set
    of decision nodes and a set of edges connected in a directed graph. A Decision
    Node contains the decision criterion (i.e., a constraint) which determines the
    outgoing path from the node to the other node in the tree. Most commonly occurring
    decision trees are binary trees with only two edges at each decision node, however,
    there can be trees with more than two edges as well (e.g., trees built on nominal
    data). The constraint can be as simple as consisting of one variable (e.g., **X₁**
    **≤** C) or they can consist of a linear combination of multiple features (e.g.,
    **X₁** **≤ X** **₂ +** C). Similarly, constraints can also be non-linear. The
    leaf-nodes of the tree contain the classification labels. This means when we traverse
    a constructed tree from a trained dataset and reach a leaf node, we get the classification
    label for a test data point. In figure 2, we can see an example dataset and the
    respective learned decision tree from that data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/754897976473b07e3a965f21b4121f14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Left: A random set of data points from two classes forming a set
    of clusters, Right: A decision tree diagram learned from the data — Numbers only
    used as markers for later reference. (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning the Tree**'
  prefs: []
  type: TYPE_NORMAL
- en: As it was mentioned in the previous section, a decision tree is a collection
    of decision constraints arranged in the form of a tree whereby the leaf nodes
    provide classification for a particular data instance. The most important step
    then is to find out the best constraints that we should arrange in the form of
    a tree. This we do by finding the best constraint at each node that provides an
    optimum data split for its child nodes. More specifically, at any given node,
    we pick a feature, and we generate a set of candidate constraints by iteratively
    going through all the variable values in the training-set for that feature. For
    example, we if we pick a variable **X₀** then the candidate constraints would
    be generated by using this variable against all the values of that variable (i.e.,
    **X₀ ≤ C).** Where C is the constant value of that variable in the training-set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46b1c8f589e09f34d25e47e4bd0ee408.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Data Clusters, Decision Tree, and Visualization of Constraints with
    data splits (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The same process is repeated for all the feature variables and all the candidate
    constraints are obtained. In order to find out the optimum constraint, we apply
    the constraint on the data at that node and obtain two subsets of data: one for
    left branch, which satisfies the constraint and one for the right branch, which
    negates the constraint. We now need a measure to find out how well is the split.
    In other words, we want to know whether splitting the data improves our chances
    of finding the true data labels. For this we use an entropy measure.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae3ad3326d1535abe356fb4745bd12ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Equations of Entropy (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is the measure of uncertainty in the system. In information theory,
    entropy is the amount of information (surprise/uncertainty) present in the data.
    For example, a binary set representing the outcomes of a random variable **X ∈**
    [1,1,1,1,1,1] has zero entropy. Which means it has the least amount of information
    and therefore, we are certain about what would be the value for all the future
    occurrences in that set. However, for a set of values **X ∈** [1,1,1,0,0,0], the
    entropy would be *1.0* which means it has the highest amount of information, and
    we are not certain at all what would be the value of new events.
  prefs: []
  type: TYPE_NORMAL
- en: Now we measure the entropy of the classification labels in each data split and
    compare it against the entropy of the parent. More specifically, we compute a
    measure called *Information Gain* which measures the amount of information gained
    about a random variable by observing the outcome of another random variable. In
    the context of decision tree, this means the reduction in the amount of entropy
    of a child node in comparison to the parent node. Therefore, it can be computed
    by subtracting the entropy of the child node from the entropy of the parent node.
    In case of multiple child nodes, the weighted average of the child entropies is
    subtracted from the parent’s entropy. The weights can be computed by computing
    the probabilities of the left and right child with respect to the parent node.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21919f25a9df3cc289c0cbb3e46328b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Information Gain Computation (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: We compute the *Information Gain* for each candidate constraint and choose the
    constraint which has the maximum value for *Information Gain.* Intuitively, we
    are trying to find those data splits in which there is the least amount of variation
    in terms of class labels. We do this until the leaf node where we want to end
    up with class labels of only one class. However, depending on the depth of the
    tree we build, we may not have single class labels at the leaf nodes, in which
    case, we shall take the class labels of the majority class of the data split as
    a final classification label. In Figure 3, you can see the tree building process
    with the optimum constraints, data splits and the classification labels at the
    leaf nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f5ae8937f0bbae556a242a4dfa315dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Algorithm for a Decision Tree (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision Trees with Linear Constraints**'
  prefs: []
  type: TYPE_NORMAL
- en: Using the process described in the previous section we can construct a decision
    tree in python using *numpy* and apply it on various types of data. We start by
    creating a random dataset and split it into training and test-set as shown in
    figure 7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2123f86b9c1753899773e10652aca5e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A set of random data points split into training and test sets (Source:
    Author)'
  prefs: []
  type: TYPE_NORMAL
- en: We can train a decision tree classifier on the training-set. The resulting decision
    tree built from the training-set is shown in figure 8.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6fd311c87895323e1d499730f1efd97a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Single-Variable Decision Tree Classifier with an example prediction
    (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: Once a tree has been built, we can use it to predict a test point. For example,
    an instance of X=(0.5,-0.5) would be predicted as belonging to the ‘0’ class after
    traversing the tree. The prediction process only consists of a set of conditions
    applied on the data point. Training the tree is also a simple and intuitive process,
    however, it can become time consuming and cumbersome for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision Trees with Non-Linear Constraints**'
  prefs: []
  type: TYPE_NORMAL
- en: In the earlier section, we applied a Decision Tree classifier on linearly separable
    data. However, in most situations, data is not linearly separable. For example,
    the data in figure 9 is not linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/750a8fabca60be967e9eb2c25b31f985.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: An example of Non-Linear Data (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: We can build the tree using a single variable as before, but it would have problems
    in accurately finding the decision boundary with only orthogonal constraints.
    We will therefore build constraints from a linear combination of variables. The
    learned Decision Tree from such constraints is shown in figure 10\. Note that
    in this way we model each constraint as a line and can find slanted decision boundaries.
    This would also require less depth of the tree compared to a decision tree built
    with single variable constraints for the accurate classification of same data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6721da8de1ca74c2b4a8b3d79cabd98e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Multi-variable Decision Tree for Non-Linear Data (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, a Decision Tree built with linear constraints would be sufficient.
    Because multiple linear constraints can accurately create a non-linear decision
    boundary. However, in certain cases, you might want to try a non-linear constraint
    for more precise classification. Such Decision Tree with non-linear constraints
    is shown in figure 11.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4eb482af4c11a806e712b1c4cc98e7ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Decision Tree with Non-Linear Constraints (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: The results of Decision Trees, both the multivariate linear as well as the non-linear
    constraints, can be seen in figure 12\. As you might notice, there is a very small
    difference between the two classification boundaries. This is because a Decision
    Tree classifier built with multivariate linear constraints is already a non-linear
    classifier. However, it might be worth using for a different dataset where the
    classification performance be significantly different.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c635f844af8ff483a8850478cc9112b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Left: Classification with Multivariate Linear Constraints, Right:
    Classification with Non-Linear Constraints (Source: Author)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusions**'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, you have learned about the Decision Trees and how to build
    a Decision Tree. You have also learned how to apply a Decision Tree classifier
    for different data types. Decision Tree classifier is perhaps the simplest and
    most intuitive classifier in the Machine Learning Toolbox. However, in some cases,
    that is all what is needed to obtain accurate classification. You can find the
    code of Decision Tree implementation in python using *numpy* on the following
    github repository.
  prefs: []
  type: TYPE_NORMAL
- en: '**Code:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.github.com/azad-academy/MLBasics-DecisionTrees](https://www.github.com/azad-academy/MLBasics-DecisionTrees)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Become a Patreon Supporter:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.patreon.com/azadacademy](https://www.patreon.com/azadacademy)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Find me on Substack:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://azadwolf.substack.com](https://azadwolf.substack.com)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Follow Twitter for Updates:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://twitter.com/azaditech](https://twitter.com/azaditech)'
  prefs: []
  type: TYPE_NORMAL
