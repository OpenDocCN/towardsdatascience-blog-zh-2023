- en: How to Evaluate Learning to Rank Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-evaluate-learning-to-rank-models-d12cadb99d47](https://towardsdatascience.com/how-to-evaluate-learning-to-rank-models-d12cadb99d47)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical guide on how to evaluate LTR models in Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ransakaravihara.medium.com/?source=post_page-----d12cadb99d47--------------------------------)[![Ransaka
    Ravihara](../Images/ac09746938c10ad8f157d46ea0de27ca.png)](https://ransakaravihara.medium.com/?source=post_page-----d12cadb99d47--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d12cadb99d47--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d12cadb99d47--------------------------------)
    [Ransaka Ravihara](https://ransakaravihara.medium.com/?source=post_page-----d12cadb99d47--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d12cadb99d47--------------------------------)
    ·4 min read·Jan 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1829e4f1740ac9601d2d32d5ae7284a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: My previous article explained the three primary methodologies for approaching
    learning to Rank problems. In this article, we will be focused on how to evaluate
    an LTR model. Let's start.
  prefs: []
  type: TYPE_NORMAL
- en: We have a few selections for evaluating the LTR model. However, these options
    vary from the approach we are using. We should use ***binary relevance*** metrics
    if the goal is to assign a binary relevance score to each document. We should
    use ***graded relevance*** if the goal is to set a relevance score for each document
    on a continuous scale. Let's discuss the widely used three types of evaluation
    matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean Average Precision (MAP)**'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating MAP for ranked results can be tricky and often confusing. Let's
    take a tiny step at a time shown below figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e91450e54b5fb8ce6bba079cc716a8f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Steps of MAP | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: MAP has a few potential drawbacks,
  prefs: []
  type: TYPE_NORMAL
- en: It does not consider the ranking of the retrieved items, only the presence or
    absence of relevant documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It may not be appropriate for datasets where the relevance of items is not binary,
    as it does not consider an item's degree of relevance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will explore more on this later with examples.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean Reciprocal Rank (MRR)**'
  prefs: []
  type: TYPE_NORMAL
- en: A straightforward evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29fea9071752be09b7cfdfd7aa3c45bf.png)'
  prefs: []
  type: TYPE_IMG
- en: MRR Metric | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: One potential drawback of MRR is it considers only the first relevant document
    for the given query.
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalized Discounted Cumulative Gain (NDCG)**'
  prefs: []
  type: TYPE_NORMAL
- en: This is the most common and ideal evaluation metric for most use cases. It accounts
    for both the relevance and the position of the results. It can assign a higher
    score for highly relevant results and appears at the top of the list.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c8cf8d290a11ba3acb7a5d6c2885da4.png)'
  prefs: []
  type: TYPE_IMG
- en: Steps of NDCG | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: If you consider the DCG equation, the numerator increases when the document's
    relevance is high; the denominator increases when the position of the document
    increases. Altogether, DCG value will go high when highly relevant items are ranked
    higher.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the LTR model by hand
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assume we have two queries, *q1 and q2,* and our LTR model returned below two
    document lists per each query, respectively. (*Note the order of documents)*
  prefs: []
  type: TYPE_NORMAL
- en: '*q1* → *(d1,d2,d3)* q2 → *(d4,d5,d6)*'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, based on the ground truth labels, we noticed that only *d1,d5,*
    and *d6* are relevant for the given two queries.
  prefs: []
  type: TYPE_NORMAL
- en: Let's calculate each evaluation metric discussed above.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculate the *MAP***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The equation used here is,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96a2aa0c373da03f19b8e297530cbac2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Let's substitute values related to query *q1,*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d2b20330a7b85d7c7af92e18bc3ac9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: And the same for *q2.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1fe017ecc0fc5dfe8db431390bd0744b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can get the MAP for the given document- query pairs as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7af6bbb4c16a350a1903e3ff688c29bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Please note if you change the order of *d5* and *d6,* you still get the same
    MAP value for query *q2* since this can't take rank into calculation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculate the *MAR***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can calculate *MAR* as shown above. Not a complex equation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0fc58bbf41b06552e3d44fc06988747e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculate the NDCG** (for query *q2)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The equation to be used here is,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10764363c636d0beaed93db018338e11.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 01: Calculate the DCG(q2)*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/837d72ba272d3a2a0c0aaef59d08336a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 02: Calculate the IDCG(q2)*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afba39e1cad015505e1a67e78aeaf1f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 03: Calculate the NDCG(q2)*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5df6115890f710ea9f22c418c4c576fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '*Next in the Series:* [*How to Implement Learning to Rank Model using Python*](/how-to-implement-learning-to-rank-model-using-python-569cd9c49b08)'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-implement-learning-to-rank-model-using-python-569cd9c49b08?source=post_page-----d12cadb99d47--------------------------------)
    [## How to Implement Learning to Rank Model using Python'
  prefs: []
  type: TYPE_NORMAL
- en: The step-by-step guide on how to implement the lambdarank algorithm using Python
    and LightGBM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-implement-learning-to-rank-model-using-python-569cd9c49b08?source=post_page-----d12cadb99d47--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are other evaluation metrics for evaluating the LTR models. But here,
    I have explained widely used metrics. But choosing the right evaluation metric
    can often be tricky. Because it heavily depends on the dataset (mainly the target
    label) and the problem we are addressing.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs: []
  type: TYPE_NORMAL
