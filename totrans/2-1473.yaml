- en: Speeding up vision transformer prediction by 9 times with PyTorch, ONNX and
    TensorRT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch、ONNX和TensorRT将视觉变换器的预测速度提高9倍
- en: 原文：[https://towardsdatascience.com/making-vision-transformers-predict-9-times-faster-with-pytorch-onnx-tensorrt-and-multi-threading-dc1f09b6814](https://towardsdatascience.com/making-vision-transformers-predict-9-times-faster-with-pytorch-onnx-tensorrt-and-multi-threading-dc1f09b6814)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/making-vision-transformers-predict-9-times-faster-with-pytorch-onnx-tensorrt-and-multi-threading-dc1f09b6814](https://towardsdatascience.com/making-vision-transformers-predict-9-times-faster-with-pytorch-onnx-tensorrt-and-multi-threading-dc1f09b6814)
- en: How to use 16bit float, TensorRT, network rewriting and multi-threading to dramatically
    speed up deep learning model prediction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用16位浮点数、TensorRT、网络重写和多线程来显著加速深度学习模型预测
- en: '[](https://jasonweiyi.medium.com/?source=post_page-----dc1f09b6814--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page-----dc1f09b6814--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dc1f09b6814--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dc1f09b6814--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page-----dc1f09b6814--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jasonweiyi.medium.com/?source=post_page-----dc1f09b6814--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page-----dc1f09b6814--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dc1f09b6814--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dc1f09b6814--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page-----dc1f09b6814--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dc1f09b6814--------------------------------)
    ·11 min read·Jun 4, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----dc1f09b6814--------------------------------)
    ·阅读时间11分钟·2023年6月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8072ede2bfa4cf3d5e55d53741fc8600.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8072ede2bfa4cf3d5e55d53741fc8600.png)'
- en: Photo by [Sanjeevan SatheesKumar](https://unsplash.com/@sanjeevan_s?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sanjeevan SatheesKumar](https://unsplash.com/@sanjeevan_s?utm_source=medium&utm_medium=referral)拍摄于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Vision transformer such as [UNET](https://en.wikipedia.org/wiki/U-Net), [SwinUNETR](https://openaccess.thecvf.com/content/CVPR2022/html/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.html)
    are state-of-the-art in computer vision tasks, such as semantic segmentation.
    But it takes a lot of time for such models to make a prediction. This article
    shows how to speed up such model’s prediction by 9 times. This improvement paves
    the way for many real-time or near real-time applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如[UNET](https://en.wikipedia.org/wiki/U-Net)、[SwinUNETR](https://openaccess.thecvf.com/content/CVPR2022/html/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.html)这样的视觉变换器在计算机视觉任务中，如语义分割，都是最先进的。然而，这些模型进行预测需要大量时间。本文展示了如何将这种模型的预测速度提高9倍。这一改进为许多实时或接近实时的应用铺平了道路。
- en: The tumours segmentation task
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 肿瘤分割任务
- en: 'To set the scene, I’m using the SwinUNETR model to segment lung tumours from
    chest CT scan images, which are single channel grayscale 3D images. Here is an
    example:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置场景，我使用SwinUNETR模型从胸部CT扫描图像中分割肺肿瘤，这些图像是单通道灰度3D图像。以下是一个示例：
- en: '![](../Images/4b7a6181e9ed9bc45a923632e2cc2783.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b7a6181e9ed9bc45a923632e2cc2783.png)'
- en: Images from the public [NSCLC-Radiomics dataset](https://wiki.cancerimagingarchive.net/display/Public/NSCLC-Radiomics)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 来自公共[NSCLC-Radiomics数据集](https://wiki.cancerimagingarchive.net/display/Public/NSCLC-Radiomics)
- en: Left column shows a few 2D slices from a 3D CT scan image, at the axial plane.
    The two crescent black areas are lungs.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左侧列展示了来自3D CT扫描图像的几幅2D切片，处于轴向平面。两个弯月形的黑色区域是肺部。
- en: Right column shows the manual annotation of lung tumours.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右侧列显示了肺肿瘤的手动标注。
- en: Chest CT scans are typically sized 512×512×300, taking roughly 60 to 90 megabytes
    to store in disk. They are not small images.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 胸部CT扫描通常尺寸为512×512×300，存储在磁盘上大约需要60到90兆字节。它们不是小图像。
- en: I use PyTorch to train a SwinUNETR model to segment lung tumours. It takes around
    10 seconds for a trained model to make a prediction on a chest CT scan. So 10
    second per image is my starting point.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用PyTorch训练了一个SwinUNETR模型来分割肺肿瘤。经过训练的模型对胸部CT扫描进行预测大约需要10秒。因此，每张图像10秒是我的起点。
- en: Before we go into speed optimization, let’s look at the model’s input and output,
    and how it makes predictions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论速度优化之前，先来看一下模型的输入和输出，以及它是如何进行预测的。
- en: Model input and output shapes
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型输入和输出形状
- en: '![](../Images/48537d9d15553ca7beda4b1b64bbf528.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48537d9d15553ca7beda4b1b64bbf528.png)'
- en: Model input and output, by author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输入和输出，由作者提供
- en: The input is a 3D numpy array representing a chest CT scan.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入是一个表示胸部CT扫描的3D numpy数组。
- en: The SwinUNETR model couldn’t hold the whole image; it’s too big. A solution
    is to cut the image into smaller chunks, called Region of Interest (ROIs). In
    my setup, a region of interest is sized 96×96×96.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SwinUNETR模型无法处理整个图像，因为它太大了。一个解决方案是将图像切割成较小的块，称为感兴趣区域（ROIs）。在我的设置中，感兴趣区域的大小为96×96×96。
- en: The SwinUNETR model sees a single region of interest at a time, outputs two
    binary segmentation masks, one for the tumour class and the other for the background
    class. Both masks are of the region of interest size, so 96×96×96\. More precisely,
    SwinUNETR outputs two unnormalized class probability masks. In a later step, these
    unnormalized masks are normalized into proper probabilities between 0 and 1 via
    *softmax*, and then *argmax*-ed into binary masks.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SwinUNETR模型一次只看到一个感兴趣区域，输出两个二值分割掩码，一个用于肿瘤类别，另一个用于背景类别。这两个掩码的大小都是感兴趣区域的大小，即96×96×96。更准确地说，SwinUNETR输出两个未归一化的类别概率掩码。在后续步骤中，这些未归一化的掩码通过*softmax*归一化为0到1之间的概率，然后通过*argmax*转化为二值掩码。
- en: These masks are merged according to how the corresponding regions of interest
    are cut to deliver two full size segmentation masks — the tumour mask and the
    background mask — each mask has the size of the whole chest CT scan. Note that
    even though the model returns two segmentation masks, we are only interested in
    the tumour mask, and will ignore the background mask.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些掩码根据对应感兴趣区域的切割方式进行合并，以生成两个全尺寸的分割掩码——肿瘤掩码和背景掩码——每个掩码的大小与整个胸部CT扫描相同。请注意，尽管模型返回了两个分割掩码，但我们只关注肿瘤掩码，会忽略背景掩码。
- en: The input and output arrays, and the model, uses 32 bit floats.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入和输出数组，以及模型，使用的是32位浮点数。
- en: Sliding window inference
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滑动窗口推断
- en: The following pseudo code implements the above prediction idea.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下伪代码实现了上述预测思路。
- en: '![](../Images/18531397de70df9796e605ac3f62ad42.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18531397de70df9796e605ac3f62ad42.png)'
- en: Sliding window inference, image by author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 滑动窗口推断，作者提供的图像
- en: Note code snippets in this article are pseudo code to keeps them succinct. Following
    the same argument, methods whose implementation is obvious, such as *split_image*,
    are left to your imagination.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意本文中的代码片段是伪代码，以保持简洁。根据相同的逻辑，像*split_image*这样的实现显而易见的方法留给读者自行实现。
- en: The *sliding_window_inference* method accepts the full CT scan *image* and a
    PyTorch *model*. It also accepts a *batch_size* because a region of interest is
    small, and a GPU can hold multiple of them at a time for prediction. *batch_size*
    specifies how many regions of interest to send to the GPU. *sliding_window_inference*
    returns the binary tumour segmentation and background mask.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*sliding_window_inference*方法接受完整的CT扫描*image*和一个PyTorch *model*。它还接受一个*batch_size*，因为一个感兴趣区域很小，GPU可以同时处理多个区域进行预测。*batch_size*指定了发送到GPU的感兴趣区域的数量。*sliding_window_inference*返回二值肿瘤分割和背景掩码。'
- en: The method first splits the whole image into regions of interest and then groups
    them into *batches* with each group containing *batch_size* regions of interest.
    Here I assume the number of regions of interest is dividable by *batch_size* for
    code simplicity.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法首先将整个图像划分为感兴趣区域，然后将这些区域分组为*batch*，每组包含*batch_size*个感兴趣区域。在这里，我假设感兴趣区域的数量可以被*batch_size*整除，以简化代码。
- en: Each *batch* is sent to the *model* to make a batch of predictions. Each prediction
    is for a single region of interest.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个*batch*会被发送到*model*以生成一批预测结果。每个预测结果对应一个单独的感兴趣区域。
- en: Finally predictions for all regions of interest are merged to form two full
    sized segmentation masks. The merging also includes *softmax* and *argmax*.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终，所有感兴趣区域的预测结果被合并形成两个全尺寸的分割掩码。合并过程还包括*softmax*和*argmax*。
- en: Snippet to make prediction for an image
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对图像进行预测的代码片段
- en: 'The following snippets calls the *sliding_window_inference* method to make
    a prediction for an image file loaded into the the first GPU “cuda:0” as a PyTorch
    tensor:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段调用了*sliding_window_inference*方法，以对加载到第一个GPU “cuda:0” 的图像文件进行预测，图像作为PyTorch张量：
- en: '![](../Images/a60ac783a75437bfa02bdf1dff114420.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a60ac783a75437bfa02bdf1dff114420.png)'
- en: Snippets to invoke model prediction, by author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 调用模型预测的代码片段，由作者提供
- en: With the above setup, I now introduce a set of tactics to make the model predict
    faster.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述设置下，我现在介绍一组策略，以加快模型预测速度。
- en: 'Tactic 1: making prediction in 16bit floats'
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略 1：在 16 位浮点数中进行预测
- en: 'By default, the trained PyTorch model works with 32bit floating point. But
    often a 16bit float precision is enough to deliver very similar segmentation result.
    It is easy to turn a 32bit model into a 16bit one using just a single PyTorch
    API *half*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，训练好的 PyTorch 模型使用 32 位浮点数。但通常情况下，16 位浮点数精度足以提供非常相似的分割结果。使用单个 PyTorch API
    *half* 就可以轻松将 32 位模型转换为 16 位模型：
- en: '![](../Images/d7c347ea885b00b36a32fb87f9ab3ba9.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7c347ea885b00b36a32fb87f9ab3ba9.png)'
- en: Prediction in 16bit float precision, image by author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 16 位浮点数精度的预测，作者图片
- en: This tactic reduces the prediction time from 10 second to **7.7 second**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个策略将预测时间从 10 秒减少到 **7.7 秒**。
- en: 'Tactic 2: converting model to TensorRT'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略 2：将模型转换为 TensorRT
- en: '[TensorRT](https://developer.nvidia.com/tensorrt) is a software from Nvidia
    that aims at delivering fast inference for deep learning models. It achieves this
    by converting a general model, such as a PyTorch model, or a TensorFlow model,
    which runs in many hardware into a TensorRT model that only runs in one particular
    hardware — the hardware that you ran the model conversion on. During the conversion,
    TensorRT also performs many speed optimizations.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[TensorRT](https://developer.nvidia.com/tensorrt) 是 Nvidia 提供的一款软件，旨在为深度学习模型提供快速推理。它通过将通用模型（如
    PyTorch 模型或 TensorFlow 模型，能够在多种硬件上运行）转换为仅在特定硬件上运行的 TensorRT 模型来实现这一目标——即你进行模型转换的硬件。在转换过程中，TensorRT
    还进行许多速度优化。'
- en: The *trtexec* executable from the TensorRT installation performs the conversion.
    The problem is, sometimes, the conversion from a PyTorch model to a TensorRT model
    fails. It fails for me on the PyTorch SwinUNETR model. The particular failure
    message is not important, you will encounter your own errors.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 TensorRT 安装的 *trtexec* 可执行文件执行转换。问题是，有时将 PyTorch 模型转换为 TensorRT 模型会失败。在 PyTorch
    SwinUNETR 模型上失败。特定的失败消息并不重要，你会遇到自己的错误。
- en: The important thing is to know there is a walk-around. The walk-around is to
    first convert a PyTorch model into an intermediate format, [ONNX](https://onnx.ai/),
    and then convert the ONNX model into a TensorRT model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要知道有一个解决方法。解决方法是首先将 PyTorch 模型转换为中间格式，[ONNX](https://onnx.ai/)，然后再将 ONNX
    模型转换为 TensorRT 模型。
- en: ONNX is an open format built to represent machine learning models. ONNX defines
    a common set of operators — the building blocks of machine learning and deep learning
    models — and a common file format to enable AI developers to use models with a
    variety of frameworks, tools, runtimes, and compilers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX 是一种开源格式，用于表示机器学习模型。ONNX 定义了一组通用的操作符——机器学习和深度学习模型的构建块——以及一个通用的文件格式，使 AI
    开发者能够在各种框架、工具、运行时和编译器中使用模型。
- en: The good news is that the support to convert an ONNX model into a TensorRT model
    is better than converting a PyTorch model into a TensorRT model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，将 ONNX 模型转换为 TensorRT 模型的支持优于将 PyTorch 模型转换为 TensorRT 模型的支持。
- en: Converting a PyTorch model to an ONNX model
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 PyTorch 模型转换为 ONNX 模型
- en: 'The following snippet converts a PyTorch model into an ONNX model:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段将 PyTorch 模型转换为 ONNX 模型：
- en: '![](../Images/b0d23d3bf9d72cedbfbd246d6745644f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0d23d3bf9d72cedbfbd246d6745644f.png)'
- en: Converting a PyTorch model to an ONNX model, by author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将 PyTorch 模型转换为 ONNX 模型，作者提供
- en: It first creates random input for a single region of interest. Then uses the
    *export* method from the installed *onnx* Python package to perform the conversion.
    This conversion outputs a file called *swinunetr.onnx*. The argument *dynamic_axes*
    specifies that the TensorRT model should support dynamic size at the 0th dimension
    of the input, that is, the batch dimension.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 它首先为单个兴趣区域创建随机输入。然后使用已安装的*onnx* Python 包中的 *export* 方法来执行转换。此转换会生成一个名为 *swinunetr.onnx*
    的文件。参数 *dynamic_axes* 指定 TensorRT 模型应支持输入的第 0 维的动态大小，即批量维度。
- en: Converting a n ONNX model to a TensorRT model
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 ONNX 模型转换为 TensorRT 模型
- en: 'Now we can invoke the *trtexec* command line tool to convert the ONNX model
    to a TensorRT model:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以调用 *trtexec* 命令行工具将 ONNX 模型转换为 TensorRT 模型：
- en: '![](../Images/6412837fffb97b1533641a0719dedaba.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6412837fffb97b1533641a0719dedaba.png)'
- en: trtexec command line to convert an ONNX model to TensorRT model, by author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将 ONNX 模型转换为 TensorRT 模型的 trtexec 命令行，作者提供
- en: the *onnx=swinunetr.onnx* command line option specifies the location of the
    onnx model.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*onnx=swinunetr.onnx* 命令行选项指定了 onnx 模型的位置。'
- en: the *saveEngine=swinunetr_1_8_16.plan* option specifies the file name for the
    resulting TensorRT model, called a plan.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*saveEngine=swinunetr_1_8_16.plan* 选项指定生成的 TensorRT 模型的文件名，称为计划。'
- en: the *fp16* option requires that the converted model runs at 16 bit floating
    point precision.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*fp16* 选项要求转换后的模型以 16 位浮点精度运行。'
- en: the *minShapes=modelInput:1×1×96×96×96* specifies the minimal input size to
    the resulting TensorRT model.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*minShapes=modelInput:1×1×96×96×96* 指定生成的 TensorRT 模型的最小输入大小。'
- en: the *maxshapes=modelInput:16×1×96×96×96* specifies the maximal input size to
    the resulting TensorRT model. Since during the PyTorch to ONNX conversion, we
    only allow the 0th dimension, that is, the batch dimension, to support dynamic
    size, here in minShapes and maxShapes, only the first number can change. Together
    they tells the *trtexec* tool to output a model that can be used for an input
    with the batch size between 1 and 16.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*maxshapes=modelInput:16×1×96×96×96* 指定生成的 TensorRT 模型的最大输入大小。由于在 PyTorch 转换为
    ONNX 的过程中，我们只允许第 0 维，即批量维度，支持动态大小，因此在 minShapes 和 maxShapes 中，只有第一个数字可以改变。它们一起告诉
    *trtexec* 工具输出一个可以用于批量大小在 1 到 16 之间的输入的模型。'
- en: the *optShapes=modelInput:8×1×96×96×96* specifies that the resulting TensorRT
    model should run the fastest with a batch size of 8.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*optShapes=modelInput:8×1×96×96×96* 指定生成的 TensorRT 模型在批量大小为 8 时运行最快。'
- en: the *workspace=10240* option gives *trtexec* 10G of GPU memory to work on the
    model conversion.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*workspace=10240* 选项为 *trtexec* 提供 10G 的 GPU 内存来进行模型转换。'
- en: '*trtexec* will run for 10 to 20 minutes, and outputs the generated TensorRT
    plan file.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*trtexec* 将运行 10 到 20 分钟，并输出生成的 TensorRT 计划文件。'
- en: Making prediction using the TensorRT model
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorRT 模型进行预测
- en: 'The following snippet loads the TensorRT model plan file and uses the TrtModel
    that is adapted from [stackoverflow](https://stackoverflow.com/questions/59280745/inference-with-tensorrt-engine-file-on-python):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段加载 TensorRT 模型计划文件，并使用从[stackoverflow](https://stackoverflow.com/questions/59280745/inference-with-tensorrt-engine-file-on-python)中改编的
    TrtModel：
- en: '![](../Images/cb36cff770fc2bf922e5e7d45f344327.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb36cff770fc2bf922e5e7d45f344327.png)'
- en: Making prediction with a TensorRT model, by author
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorRT 模型进行预测，由作者提供
- en: Note that even though in the *trtexec* command line, we specified the *fp16*
    option, here when loading the plan, we still need to specify the 32 bit floating
    point. Strange.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使在 *trtexec* 命令行中，我们指定了 *fp16* 选项，在加载计划时，我们仍然需要指定 32 位浮点数。很奇怪。
- en: Minor adaptations are needed in the TrtModel you got from stackoverflow, but
    you will work it out. It is not that difficult.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对从 stackoverflow 获得的 TrtModel 需要进行一些小调整，但你会解决的。这并不难。
- en: With this tactic, the prediction time is **2.89 second**!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个战术，预测时间为 **2.89 秒**！
- en: 'Tactic 3: Wrapping model to return one mask'
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 战术 3：包装模型以返回一个掩码
- en: Our SwinUNETR model returns two segmentation masks, one for tumour and one for
    background, in the form of unnormalized probabilities. These two masks are first
    transferred from GPU back to CPU. Then in CPU, these unnormalized probabilities
    are *softmax*-ed to proper probabilities between 0 and 1, and finally *argmax*-ed
    to generate binary masks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 SwinUNETR 模型返回两个分割掩码，一个用于肿瘤，一个用于背景，以未归一化概率的形式。这两个掩码首先从 GPU 转移到 CPU。然后在 CPU
    中，这些未归一化的概率会被 *softmax* 转换为 0 和 1 之间的适当概率，最后通过 *argmax* 生成二值掩码。
- en: Since we only use the tumour mask, there is no need for the model to return
    the background mask. Transferring data between GPU and CPU takes time, and computations
    such as *softmax* takes time.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只使用肿瘤掩码，因此模型无需返回背景掩码。GPU 和 CPU 之间的数据传输需要时间，而 *softmax* 等计算也需要时间。
- en: 'To have a model that only returns a single mask, we can create a new class
    that wrappers the SwinUNETR model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得只返回单一掩码的模型，我们可以创建一个新的类来包装 SwinUNETR 模型：
- en: '![](../Images/d17758563e34fc125eca6f3c4cc99c54.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d17758563e34fc125eca6f3c4cc99c54.png)'
- en: SwinUNETR wrapper to return a single mask, by author
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: SwinUNETR 包装器以返回单一掩码，由作者提供
- en: 'The following figure illustrates the new model input output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了新的模型输入输出：
- en: '![](../Images/c6bfb66e608282bc7a1e2a0730d9e02c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6bfb66e608282bc7a1e2a0730d9e02c.png)'
- en: SwinWrapper input and output, by author
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: SwinWrapper 输入和输出，由作者提供
- en: 'The forward method pushes a batch of input regions of interest through the
    forward pass of the neural network to make prediction. In this method:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: forward 方法通过神经网络的前向传递处理一批感兴趣区域以进行预测。在这个方法中：
- en: the original model is first called on the passed-in input regions of interest
    to get the predictions of the two segmentation classes. The output is of shape
    Batch×2×Width×Height×Depth because in the current tumour segmentation task, there
    are two classes — tumour and background. Result is stored in the *out* variable.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始模型首先在传入的感兴趣区域上调用，以获取两个分割类别的预测。输出形状为Batch×2×Width×Height×Depth，因为在当前的肿瘤分割任务中，有两个类别——肿瘤和背景。结果存储在*out*变量中。
- en: Then *softmax* is applied to the two unnormalized segmentation masks to turn
    them into normalized probabilities between 0 and 1.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后对两个未归一化的分割掩码应用*softmax*，将它们转换为介于0和1之间的归一化概率。
- en: Then only the tumour class, that is, class 1, is selected to return to the caller.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后仅选择肿瘤类别，即类别1，返回给调用者。
- en: 'So, actually, this wrapper implements two optimizations:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这个封装器实现了两个优化：
- en: only returns a single segmentation mask, instead of two.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅返回一个分割掩码，而不是两个。
- en: moves the *softmax* operation from CPU into GPU.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将*softmax*操作从CPU移到GPU中。
- en: What about the *argmax* operation? Since only one segmentation mask is returned,
    there is no need for *argmax.* Instead, to create the original binary segmentation
    mask, we will do *tumour_segmentation_probability ≥ 0.5*, with *tumour_segmentation_probability*
    being the result from the *forward* method in SwinWrapper.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 那么*argmax*操作呢？由于只返回一个分割掩码，所以不需要*argmax*。相反，为了创建原始的二进制分割掩码，我们将进行*tumour_segmentation_probability
    ≥ 0.5*，其中*tumour_segmentation_probability*是SwinWrapper中的*forward*方法的结果。
- en: Since SwinWrapper is a PyTorch model, we need to do the PyTorch to ONNX, and
    ONNX to TensorRT conversion steps again.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SwinWrapper是一个PyTorch模型，我们需要再次进行PyTorch到ONNX和ONNX到TensorRT的转换步骤。
- en: 'When converting the SwinWapper model to an ONNX model, it only change needed
    is to use wrapped model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在将SwinWapper模型转换为ONNX模型时，唯一需要更改的是使用封装模型：
- en: '![](../Images/92fe03e45b2eec9f1413d361f917f822.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92fe03e45b2eec9f1413d361f917f822.png)'
- en: Converting wrapped SwinUNETR model to ONNX, by author
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 将封装的SwinUNETR模型转换为ONNX，由作者提供
- en: And the *trtexec* command line to convert an ONNX model to a TensorRT plan stay
    unchanged. So I won’t repeat it here.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 将ONNX模型转换为TensorRT计划的*trtexec*命令行保持不变，所以我不会在这里重复。
- en: This tactic reduced the prediction time from 2.89 second to **2.42 second**.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个策略将预测时间从2.89秒减少到**2.42秒**。
- en: 'Tactic 4: distributing regions of interest to multiple GPUs'
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略4：将感兴趣区域分配到多个GPU
- en: All the above tactics uses only one GPU, but sometimes we want to use a more
    expensive multiple GPUs machine to deliver even faster predictions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以上所有策略仅使用一个GPU，但有时我们希望使用更昂贵的多GPU机器以实现更快的预测。
- en: The idea is to load the same TensorRT model into *n* GPUs, and inside *sliding_window_inference*,
    we further split a batch of ROIs to *n* parts, and send each part to a different
    GPU. This way, the time-consuming forward pass of the SwinWrapper network can
    run concurrently for different parts.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其思路是将相同的TensorRT模型加载到*n*个GPU中，在*sliding_window_inference*中，我们进一步将一批ROIs分割为*n*个部分，并将每部分发送到不同的GPU。这种方式下，SwinWrapper网络的耗时前向传递可以并行处理不同部分。
- en: 'We need to change the *sliding_window_inference* method into the following
    *sliding_window_inference_multi_gpu*:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将*sliding_window_inference*方法改为以下的*sliding_window_inference_multi_gpu*：
- en: '![](../Images/942a38c35422ec4e60f95b4ab53d8ef7.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/942a38c35422ec4e60f95b4ab53d8ef7.png)'
- en: Multiple GPU sliding window inference, by author
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 多GPU滑动窗口推断，由作者提供
- en: Same as before, we group regions of interest in different batches.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 和之前一样，我们将感兴趣区域分成不同的批次。
- en: We split each batch into parts, depending on how many GPUs are given.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们根据提供的GPU数量将每个批次拆分成多个部分。
- en: For each part *batch_per_gpu*, we submit a task to into a ThreadPoolExecutor.
    The task performs model inference on the passed-in part.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个部分*batch_per_gpu*，我们将任务提交到ThreadPoolExecutor中。该任务对传入的部分执行模型推断。
- en: The *submit* method returns immediately with a future object, representing the
    result of the task when it finishes. It is crucial for the *submit* method to
    return immediately before the task finishes, so we can post other tasks to different
    threads without waiting, achieving parallelism.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*submit*方法立即返回一个future对象，表示任务完成时的结果。*submit*方法在任务完成前立即返回是至关重要的，这样我们可以在不等待的情况下将其他任务提交到不同的线程，实现并行处理。'
- en: After all tasks submittedin the inner *for loop*, wait for all future objects
    to complete.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在内层*for循环*中提交所有任务后，等待所有future对象完成。
- en: After the tasks’ completion, read results from the futures and merge results.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任务完成后，从期望中读取结果并合并结果。
- en: 'To invoke this new version of *sliding_window_inference_multi_gpu*, use the
    following snippet:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要调用这个新版本的*sliding_window_inference_multi_gpu*，请使用以下代码片段：
- en: '![](../Images/8b12d02411051657b2bff4b34aecd886.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b12d02411051657b2bff4b34aecd886.png)'
- en: Model prediction with multiple GPUs, by author
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个GPU进行模型预测，作者
- en: Here I used two GPUs, so I created two TensorRT models, each into a different
    GPU, “cuda:0” and “cuda:1”.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我使用了两个GPU，因此我创建了两个TensorRT模型，每个模型分别放在不同的GPU上，“cuda:0”和“cuda:1”。
- en: Then I created a ThreadPoolExecutor with two threads.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我创建了一个包含两个线程的ThreadPoolExecutor。
- en: I passed the models and the executor into the *sliding_window_inference_multi_gpu*
    method, similar to the case of a single GPU, to get the tumour class segmentation
    mask.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我将模型和执行器传递给*sliding_window_inference_multi_gpu*方法，类似于单GPU的情况，以获得肿瘤类别分割掩码。
- en: This tactic reduces the prediction time from 2.42 second to **1.38 second**!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个策略将预测时间从2.42秒减少到**1.38秒**！
- en: Now we have four tactics that improved the prediction speed of the SwinUNETR
    model by 9 times. That’s not too bad. But are we sacrificing prediction precision
    for speed?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有四种策略，将SwinUNETR模型的预测速度提高了9倍。还不错。但我们是否为了速度牺牲了预测精度？
- en: Are we sacrificing prediction precision for speed?
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们是否为了速度牺牲了预测精度？
- en: Note here the word “precision” means how well the final model segment tumours,
    it does not mean the floating point prediction, for example, 16bit, 32bit precision.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里“精度”一词指的是最终模型对肿瘤的分割效果，而不是浮点预测精度，例如16位、32位精度。
- en: To answer this question, we need to look at the DICE metric that measures the
    performance of a segmentation model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们需要查看DICE指标，这个指标用于衡量分割模型的性能。
- en: 'A DICE score is calculated as the proportion of overlapping between the predicted
    tumour and the ground truth tumour. DICE score is between 0 and 1; a larger DICE
    score means a better model prediction:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: DICE分数是通过预测的肿瘤与实际肿瘤重叠的比例来计算的。DICE分数介于0和1之间；分数越高，模型预测越好：
- en: DICE 1 is perfect prediction,
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DICE 1表示完美预测，
- en: DICE 0 is completely wrong prediction, or no prediction at all.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DICE 0表示完全错误的预测，或者根本没有预测。
- en: 'Let’s look at the DICE score for a test image:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看测试图像的DICE分数：
- en: '![](../Images/3619d69241e9d1feed45b76533760191.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3619d69241e9d1feed45b76533760191.png)'
- en: Dice score achieved by different tactics, by author
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 不同策略下的Dice分数，作者
- en: We can see that only when we turned a 32bit PyTorch model to a 16bit model in
    tactic 1, the DICE score slightly decreased from 0.93 to 0.91\. Other tactics
    don’t decrease the DICE score. This shows that the tactics can achieve much faster
    prediction speed with only a tiny bit of precision loss.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，只有当我们在策略1中将32位PyTorch模型转换为16位模型时，DICE分数从0.93略微下降到0.91。其他策略不会降低DICE分数。这表明这些策略可以在仅有微小精度损失的情况下实现更快的预测速度。
- en: Conclusions
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This article introduces four tactics that can make vision transformer predict
    at a much faster speed by using tools such as ONNX, TensorRT and multi-threading.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了四种策略，通过使用ONNX、TensorRT和多线程等工具，使视觉变换器预测速度大大提升。
- en: Acknowledgement
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: I would like to give a big thank you to my friend Chunyu Jin. He introduced
    me to the possibilities of fast deep learning model inferences. He made the first
    running TensorRT SwinUNETR model for me and suggested many of the tactics that
    I tried out here.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢我的朋友Chunyu Jin。他向我展示了快速深度学习模型推断的可能性。他为我制作了第一个运行的TensorRT SwinUNETR模型，并建议了我在这里尝试的许多策略。
- en: '[](https://medium.com/@jasonweiyi/membership?source=post_page-----dc1f09b6814--------------------------------)
    [## Join Medium with my referral link - Wei Yi'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jasonweiyi/membership?source=post_page-----dc1f09b6814--------------------------------)
    [## 通过我的推荐链接加入Medium - Wei Yi'
- en: Read every story from Wei Yi (and thousands of other writers on Medium). If
    you like my stories, please consider…
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读Wei Yi（以及Medium上的成千上万其他作者）的每一个故事。如果你喜欢我的故事，请考虑…
- en: medium.com](https://medium.com/@jasonweiyi/membership?source=post_page-----dc1f09b6814--------------------------------)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@jasonweiyi/membership?source=post_page-----dc1f09b6814--------------------------------)
