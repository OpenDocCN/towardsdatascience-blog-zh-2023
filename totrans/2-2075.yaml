- en: 'The Ultimate Guide to Training BERT from Scratch: Prepare the Dataset'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Data Preparation: Dive Deeper, Optimize your Process, and Discover How to Attack
    the Most Crucial Step'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dpoulopoulos.medium.com/?source=post_page-----beaae6febfd5--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page-----beaae6febfd5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----beaae6febfd5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----beaae6febfd5--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----beaae6febfd5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----beaae6febfd5--------------------------------)
    ·13 min read·Sep 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34a45dbd982191c4d64c0848ec5fbae1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[Part I](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f),
    [Part II](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822),
    and [Part IV](/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb)
    of this story are now live.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Imagine investing a full day fine-tuning BERT, only to hit a performance bottleneck
    that leaves you scratching your head. You dig into your code and discover the
    culprit: you just didn’t do a good job preparing your features and labels. Just
    like that, ten hours of precious GPU time evaporates into thin air.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s face it, *setting up your dataset isn’t just another step — it’s the
    engineering cornerstone of your entire training pipeline*. Some even argue that
    once your dataset is in good shape, the rest is mostly boilerplate: feed your
    model, calculate the loss, perform backpropagation, and update the model weights.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9ce304927937c39d5bde10ec7fd6689.png)'
  prefs: []
  type: TYPE_IMG
- en: The training pipeline — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'In this story, **we’ll get into the process of preparing your data for BERT,
    setting the stage for the ultimate goal: training a BERT model from scratch.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Welcome to the third installment of our comprehensive BERT series! In the first
    chapter, we introduced BERT — breaking down its objectives and demonstrating how
    to fine-tune it for a practical question-answering system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----beaae6febfd5--------------------------------)
    [## The Ultimate Guide to Training BERT from Scratch: Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: 'Demystifying BERT: The definition and various applications of the model that
    changed the NLP landscape.'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----beaae6febfd5--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in the second chapter, we dived deep into the world of tokenizers, exploring
    their mechanics and even creating a custom tokenizer for the Greek language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----beaae6febfd5--------------------------------)
    [## The Ultimate Guide to Training BERT from Scratch: The Tokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: 'From Text to Tokens: Your Step-by-Step Guide to BERT Tokenization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----beaae6febfd5--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we’re tackling one of the most pivotal stages of building a high-performing
    BERT model: *dataset preparation*. This guide will be a technical one, providing
    Python snippets and links to the GitHub repositories of popular open-source projects.
    Okay, we’re losing the light; let’s start!'
  prefs: []
  type: TYPE_NORMAL
- en: '[Learning Rate](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-dataset)
    is a newsletter for those who are curious about the world of ML and MLOps. If
    you want to learn more about topics like this subscribe [here](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-dataset).
    You’ll hear from me on the last Sunday of every month with updates and thoughts
    on the latest MLOps news and articles!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Download & Explore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First things first, we should choose a dataset. For the purpose of this tutorial,
    we’ll be working with the `[wikitext](https://huggingface.co/datasets/wikitext)`
    dataset, specifically the `wikitext-2-raw-v1` subset.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to its documentation, this dataset gathers over 100 million tokens
    extracted from Wikipedia’s verified articles. It’s the ideal playground for our
    BERT experiments, and thanks to Hugging Face, accessing it is a breeze:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `datasets` variable contains three splits: train, validation, and test.
    Let’s focus on the train split first.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The train split contains 36,718 rows of variable-length text. You’ll find entries
    ranging from empty strings to full-blown paragraphs. Here’s a peek using the Hugging
    Face dataset viewer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/455f492ae353ea3571db5ff4918618e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Wikitext dataset — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Note to self: Consistency in token lengths is non-negotiable.** This is crucial
    as our model knows how to handle sequences of specific length during training.
    We should keep that in mind during the data processing phase.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, what about the dataset’s features?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output reveals a rather minimalistic structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: That’s right, it’s just a `text` feature. So how do we shape this raw data into
    a robust training set for BERT? Get ready; we’re about to get our hands dirty
  prefs: []
  type: TYPE_NORMAL
- en: Just like Halloween
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create our dataset preparation blueprint, let’s take a moment to revisit
    BERT’s objectives. BERT training is a two-phase process, but today, we’re focusing
    only on the first phase: pre-training. The goal here is to teach the model what
    language is and how context changes the meaning of words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The pre-training phase has two key tasks: i) Masked Language Modeling (MLM)
    and ii) Next Sentence Prediction (NSP). During MLM, we purposely mask certain
    tokens in a sequence and train BERT to accurately predict them. With NSP, we present
    BERT with two adjacent sequences, asking it to predict whether the second naturally
    follows the first.'
  prefs: []
  type: TYPE_NORMAL
- en: So, our goal is to create a dataset that will support these tasks and teach
    BERT the concepts we want. Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: The foundations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Okay, first things first: we need to do two things before moving on to masking
    a sequence: We must tokenize the sentences and then create token sequences of
    equal length. Do you remember the mental note we made earlier?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88ac4835ee226280938f57b71003a539.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first load the pre-trained BERT tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, tokenizing a text sentence is really simple: We come up with a function
    that accepts an example and passes it through the tokenizer. Then, we map this
    function across the dataset. This is a common practice when working with Hugging
    Face datasets, and we’ll see this pattern again and again.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note the `num_proc` and `batched` arguments. These are essential to make the
    process run faster. They will make a huge difference as your datasets are getting
    bigger. For more information about the `map` function, check out the `datasets`
    library [docs](https://huggingface.co/docs/datasets/process#map). To understand
    more about these arguments specifically, take a closer look at the [multiprocessing](https://huggingface.co/docs/datasets/process#multiprocessing)
    and [batch-processing](https://huggingface.co/docs/datasets/process#batch-processing)
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Also, be mindful that we asked the tokenizer to omit adding any special tokens
    (`add_special_tokens=False`). You’ll see later why.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to create sequences of equal length. BERT usually accepts sequences
    of `512` tokens. This is mostly defined by the shape of the positional encodings
    matrix in the model’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, in this step, we will create sequences of `255` tokens. Why? Because
    later, we want to concatenate two sentences to create a dataset that supports
    the NSP tasks. This process will create sequences of `510` tokens (`255` + `255`).
    However, we should also add two special tokens: `[CLS]` to indicate the start
    of a sequence and `[SEP]` to mark where the first sentence ends and the second
    one begins.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, let’s create a helper function that does that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This function performs two transformations: First, it brings all the sequences
    together and then splits them again into small blocks of `255` tokens. Let’s map
    it over our tokenized dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: That concludes the preliminary transformations. Next, let’s concatenate two
    sequences together to create the effect we want for NSP.
  prefs: []
  type: TYPE_NORMAL
- en: Nest Sentence Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a reminder, the NSP task tries to predict if two given sentences are adjacent
    in the original text or not. Thus, we need a dataset where each entry should comprise
    a pair of sentences along with a label that indicates if they are related.
  prefs: []
  type: TYPE_NORMAL
- en: To create such a dataset, we will, once again, define a helper function. Admittedly,
    this function will be a bit more intricate than what we’ve worked with before,
    but don’t let that intimidate you. The most effective way to tackle complexity
    is often to dive in head first.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, that seems a lot! So, let’s visualize what this function does to make
    the code more digestible. This function aims to create something similar to the
    QnA scheme we saw in the first [introductory blog post](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db03129cde7b08c9322c885d8cd3e0bd.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT QnA Dataset Preparation — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'The `if/else` block works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Half of the time, create sequences of subsequent sentences, except if the sequence
    at hand is the last one, in which case, wrap around and create a sequence of two
    random sentences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other half of the time, create a sequence of two random sentences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Set the next sentence label accordingly: `0` for subsequent sequences, `1`
    if they have no connection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine the two sentences using the `[CLS]` and `[SEP]` special tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create the `token_type_ids` list to mark the two segments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, how did I come up with the names `token_type_ids` and `next_sentence_label`?
    Are they random names, and you can use whatever variable name you want? No! If
    you look at the model’s [forward](https://github.com/huggingface/transformers/blob/fa6107c97edf7cf725305a34735a57875b67d85e/src/transformers/models/bert/modeling_bert.py#L1077)
    method signature, you’ll find exactly these names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Pay close attention to this fact; you need to use exactly these names. If you
    don’t, the library will remove those features from your dataset before passing
    it to the model. Why? Because this is what it is supposed to do by [default](https://github.com/huggingface/transformers/blob/fa6107c97edf7cf725305a34735a57875b67d85e/src/transformers/trainer.py#L746).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, as always, let’s map this method. But first, let’s separate the different
    dataset splits so we have them ready later for training and map the function to
    each one separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'There is a subtle difference in how we map the function now: We are creating
    a lambda function that uses the one we defined, and we set `with_indices=True`
    so that our helper function has a handle on the index of each example that it
    operates on.'
  prefs: []
  type: TYPE_NORMAL
- en: Great! We have prepared our dataset to handle the NSP task. What about MLM?
    In the first blog, we’ve seen that we train BERT on the two tasks simultaneously.
    So, let’s tackle the MLM transformation next.
  prefs: []
  type: TYPE_NORMAL
- en: Masked Language Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Luckily, the `transformers` library offers a streamlined solution for the Masked
    Language Modeling (MLM) task. Simply instantiate the `DataCollatorForLanguageModeling`
    class with the appropriate parameters, and voila — most of the work is done for
    you.
  prefs: []
  type: TYPE_NORMAL
- en: But wait, you didn’t come here for the easy way out, did you? This guide is
    all about diving deep, so we won’t tolerate any black boxes. We’re going to pull
    back the curtain and explore how exactly to create a custom dataset for MLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how we’ll tackle it: First, we’ll paint a mental picture by visualizing
    the core concepts. Think of it as a virtual storyboard that brings everything
    into focus. Next, we’ll dissect the code, breaking it down step by step to get
    a grasp of each component.'
  prefs: []
  type: TYPE_NORMAL
- en: Our first order of business? Crafting labels for our input tokens. this is an
    easy one; all we need to do is clone the input array.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b88e658a7544a842588c33718e9fd3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Labels generation — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we forge the “mask” array, which mirrors the shape of our inputs and
    labels. Each entry in this mask array represents a probability value, dictating
    the likelihood that a toke at the corresponding position will be masked.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of demonstration, we’ll use a masking probability of `0.5`, even
    though in real-world applications, it’s usually set closer to `0.15`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff664b4a162b8cd72c35d13362bcc81f.png)'
  prefs: []
  type: TYPE_IMG
- en: Mask generation — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we know that there are some tokens we surely do not want to mask: the
    special `[CLS]` and `[SEP]` tokens. So, let’s set their corresponding probability
    values in the mask array to zero, effectively ruling out any chance of them being
    masked.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f548bd61e099a0fa57fd1f3385c1c1c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Mask special tokens — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for each position in the mask, let’s evaluate the probability and set
    the value to either `1.0` or `0.0`. Value `1.0` means that we will mask the token
    in the corresponding position while `0.0` means we will leave it unchanged:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89e70b742cf2428654661772c93b041b.png)'
  prefs: []
  type: TYPE_IMG
- en: Probability evaluation — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Great, we now have a map that points out which tokens we will mask. But here’s
    a twist: not all masked positions will be replaced by the `[MASK]` token. In fact,
    we’ll employ a more tricky strategy. Here’s how it breaks down: 50% of the masked
    tokens will get the special `[MASK]` token, 25% will be swapped out for a random
    token, and the remaining 25% will be left as is — though BERT will not know this.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In reality, we use an 80–10–10 scheme, but to make it work with our illustration,
    let’s employ the 50–25–25 strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc44b4ddc64ad7cfdda675d80469130b.png)'
  prefs: []
  type: TYPE_IMG
- en: Mask tokens — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it’s time to treat the labels. The labels at indices that should not
    contribute to the loss — i.e., the unmasked tokens — are set to `-100`. This is
    not a random negative number. It’s the default value for the `ignore_index` property
    in PyTorch’s [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).
    Thus, Pytorch will ignore these labels when calculating the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63492528b5b2fd0eeb5be3ed01dee92c.png)'
  prefs: []
  type: TYPE_IMG
- en: Process labels— Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s it! Let’s bring everything together in a final summarizing animation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/041fd60e99943b7c383bf241ea72ceb7.png)'
  prefs: []
  type: TYPE_IMG
- en: Masked Language Modeling — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a clear mental picture, let’s see the code. This is taken
    directly from `transformers` [library](https://github.com/huggingface/transformers/blob/41aef33758ae166291d72bc381477f2db84159cf/src/transformers/data/data_collator.py#L751):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We kick off by cloning the inputs to form the labels — old news by now! Next,
    we generate a probability matrix that’s the same shape as the labels and fill
    it with a chosen probability value. We then pinpoint the special tokens in the
    sequence and turn down their masking probabilities to a flat zero. Are you with
    me? Everything should look familiar!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s where it gets spicy: we evaluate the probability matrix using a Bernoulli
    distribution. Sounds a bit grande, but it’s basically a coin flip determining
    whether each matrix position gets a `1.0` or a `0.0`. Then we translate these
    ones and zeros to true and false values. Any ‘false’ hits in the matrix? We set
    their corresponding label values to `-100`, as we discussed previously.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we bring into play the 80–10–10 masking scheme we previously touched
    on, using the same Bernoulli logic. This effectively masks, replaces, or leaves
    tokens untouched in the input sequence, offering BERT a varied diet of data to
    chew on.
  prefs: []
  type: TYPE_NORMAL
- en: 'And that’s all! It’s a one-liner to go through all these steps using the `datasets`
    library. Instantiate a `DataCollatorForLanguageModeling` object and then pass
    it in the `Trainer` (more on the `Trainer` in the next episode):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the journey to train BERT from scratch, preparing the dataset is often the
    most labor-intensive and difficult step but also one of the most rewarding.
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve followed along, you’ve not just learned how to prepare the dataset
    but also the reason behind each decision in dataset preparation. From selecting
    the ideal dataset and breaking down its features to the fine art of masking tokens
    using a varied strategy, we’ve delved into the details that could make or break
    your model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll finally touch on the training process. See you there!
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My name is [Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-dataset),
    and I’m a machine learning engineer working for [HPE](https://www.hpe.com/us/en/home.html).
    I have designed and implemented AI and software solutions for major clients such
    as the European Commission, IMF, the European Central Bank, IKEA, Roblox and others.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in reading more posts about Machine Learning, Deep Learning,
    Data Science, and DataOps, follow me on [Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow),
    [LinkedIn](https://www.linkedin.com/in/dpoulopoulos/), or [@james2pl](https://twitter.com/james2pl)
    on Twitter.
  prefs: []
  type: TYPE_NORMAL
- en: Opinions expressed are solely my own and do not express the views or opinions
    of my employer.
  prefs: []
  type: TYPE_NORMAL
