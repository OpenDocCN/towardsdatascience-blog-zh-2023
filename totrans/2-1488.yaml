- en: 'Mastering Long Short-Term Memory with Python: Unleashing the Power of LSTM
    in NLP'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mastering-long-short-term-memory-with-python-unleashing-the-power-of-lstm-in-nlp-381ec3430f50](https://towardsdatascience.com/mastering-long-short-term-memory-with-python-unleashing-the-power-of-lstm-in-nlp-381ec3430f50)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A comprehensive guide to understanding and implementing LSTM layers for natural
    language processing with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eligijus-bujokas.medium.com/?source=post_page-----381ec3430f50--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----381ec3430f50--------------------------------)[](https://towardsdatascience.com/?source=post_page-----381ec3430f50--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----381ec3430f50--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----381ec3430f50--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----381ec3430f50--------------------------------)
    ·17 min read·Nov 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8590a68892231bb5a03f41691fc9f697.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sven Brandsma](https://unsplash.com/@seffen99?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This work is a continuation of my [article about RNNs and NLP with Python](https://medium.com/towards-data-science/mastering-nlp-in-depth-python-coding-for-deep-learning-models-a15055e989bf).
    A natural progression of a deep learning network with a simple recurrent layer
    is a deep learning network with a **L**ong **S**hort **T**erm **M**emory (**LSTM**
    for short) layer.
  prefs: []
  type: TYPE_NORMAL
- en: As with the RNN and NLP, I will try to explain the LSTM layer in great detail
    and code the forward pass of the layer from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the codes can be viewed here: [https://github.com/Eligijus112/NLP-python](https://github.com/Eligijus112/NLP-python)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will work with the same dataset¹ as in the previous article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bcb705d18dcb689363a428e44c88f127.png)'
  prefs: []
  type: TYPE_IMG
- en: Random rows from the dataset; Picture by author
  prefs: []
  type: TYPE_NORMAL
- en: Remember, that SENTIMENT=1 is a negative sentiment, and SENTIMENT=0 is a positive
    sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: We need to convert the text data into a sequence of integers. Unlike in the
    previous article though, we will now create a sequence not of words but of individual
    characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the text “*Nice Game*” could be converted to the following example
    vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[1, 2, 3, 4, 5, 6, 7, 8, 3]*'
  prefs: []
  type: TYPE_NORMAL
- en: Each individual character, including whitespaces and punctuations, will have
    an index.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us split our data into a train-test split and apply our created function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'There are 274 unique characters in our data. Let us print the top 10 entries
    in our **word2idx** dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us convert the texts to sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To recall, splitting the texts by word level led to the mean length of a sequence
    being equal to **~22** tokens. Now, we have sequences of length **~103** tokens.
    The standard deviation is very high, thus we will use the max sequence length
    of **200** in padding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The train and val datasets thus far look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/895432fe15cb4e3a0d476637def6883a.png)'
  prefs: []
  type: TYPE_IMG
- en: A snippet of data; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Why should we switch from a vanilla RNN to an LSTM network? The problems are
    twofold:'
  prefs: []
  type: TYPE_NORMAL
- en: A simple RNN has the so-called **vanishing gradient problem²** or the **exploding
    gradient problem** associated with the weights used in the ***for*** loop of the
    network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network tends to **“forget”** the initial steps input of a long sequence
    of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To illustrate the ***forgetness***, consider the example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our data, on average, there are 103 timesteps (the number of tokens in a
    text going from left to right). Recall the graph from the RNN article:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3de05ae2d60ac52ad02c3a95ad79fc0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Unrolled n step RNN; Picture by author
  prefs: []
  type: TYPE_NORMAL
- en: We have the same weight **W** that we multiply the output of the ReLU layer
    with. Then, we add that signal to the next time step, and so on. If we choose
    a relatively small value for **W** (let us say 0.5) and we have 103 steps of time
    series data, the impact from the first timestep input to the final output would
    be, roughly speaking, **0.5¹⁰³ * input1** which is approximately zero.
  prefs: []
  type: TYPE_NORMAL
- en: The signal from the second input would be **0.5¹⁰² * input2** and so on.
  prefs: []
  type: TYPE_NORMAL
- en: One can see, that the more timesteps we add, the less information is left to
    the final output from the initial time steps.
  prefs: []
  type: TYPE_NORMAL
- en: To battle this problem of forgetting the past, great minds have come up with
    an LSTM layer³ for use in time series problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Internally, an LSTM layer uses two activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanh function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Key facts to remember about these functions are:'
  prefs: []
  type: TYPE_NORMAL
- en: The **sigmoid** activation function takes in any value on a real number plane
    and outputs a value between **0 and 1**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **tanh** function takes in any value on a real number plane and outputs
    a value **between -1 and 1.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the sigmoid and tanh activation functions in our minds, let
    us return to the LSTM layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LSTM layer is made up of 2 parts (hence the name):'
  prefs: []
  type: TYPE_NORMAL
- en: Long-term memory block
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Short-term memory block
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At every time step (or token step), the LSTM layer outputs two predictions,
    the **long-term prediction** and the **short-term prediction**. A high-level diagram
    of an LSTM unit can be visualized like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/756d64f334e9b754acda7b6b6a6b85db.png)'
  prefs: []
  type: TYPE_IMG
- en: Unrolled simple LSTM network; Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: At each time step, the LSTM layer outputs a number and this is what we call
    the **short-term memory output.** It is usually just a scalar. Additionally, the
    **long-term memory** scalar is also calculated in the LSTM layer but it is not
    output and transferred to the second step in the sequence. It is very important
    to note that at each time step, both the short-term and the long-term memories
    are updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let us dive deep into the LSTM layer. The first part of an LSTM layer is
    the so-called **Forget Gate** operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04fd92b4546ccff1103dbc14fec6b8c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Forget gate; Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: The forget gate gets its name from the fact that we **calculate the percentage
    of the long-term memory that we want to keep.** This is due to the fact that the
    sigmoid activation function will output a number between 0 and 1 and we will multiply
    that number by the long-term memory and pass it along the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start to see the weights that will be updated at training time: **w1,
    w2,** and **b1**. These weights directly influence the amount of long-term memory
    to keep.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that at this step, the short-term memory is not adjusted and gets passed
    along to the second steps of the network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up in the LSTM layer is the input gate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4e654110995642ce727a2ad2d43c203.png)'
  prefs: []
  type: TYPE_IMG
- en: Input gate; Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: The input gate only adjusts the long-term memory part of the LSTM network, but
    in order to do that, it uses the current input and the current short-term memory
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the graph, just before the multiplication step, we have two outputs:
    one from the sigmoid activation function and another from the tanh activation
    layer. Loosely speaking, the sigmoid layer outputs the percentage of the memory
    to remember (0, 1) and the tanh outputs the potential memory to remember (-1,
    1).'
  prefs: []
  type: TYPE_NORMAL
- en: We then sum up the current long-term memory, which was a bit adjusted in the
    forget gate, with the input gate output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from the code snippet above, the only thing that has changed is
    the long-term memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last piece of the LSTM layer is the **output gate**. The output gate is
    the step where we will adjust the **short-term** memory of the layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7edebcc737e85b30b2762841029d4a03.png)'
  prefs: []
  type: TYPE_IMG
- en: Output gate; Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The logic is very similar to the logic that was present in the previous gates:
    the sigmoid activation calculates the percentage of memory to keep and the tanh
    function calculates the overall signal.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the output gate only adjusted the short-term memory scalar.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/577260ddbca493db6a1fcf95b06d7a75.png)'
  prefs: []
  type: TYPE_IMG
- en: LSTM layer; Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: The above graph shows the forget, input, and output gates on one graph⁴.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have an input sequence of x variables, the inner loop when using the
    LSTM layer is this:'
  prefs: []
  type: TYPE_NORMAL
- en: Initiate randomly short-term and long-term memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '2\. For each **x1** to **xn**:'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Forward propagate through the LSTM layer.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Output the short-term memory
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Save the long-term and short-term memories to the layer.
  prefs: []
  type: TYPE_NORMAL
- en: Let us wrap every gate to a class and create a Python example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now to wrap everything in a nice pytorch example with the LSTM layer. The syntax
    is very similar to a basic RNN model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This article went into the nitty gritty details about the inner workings of
    an LSTM cell. Some implementations of the LSTM layer may differ from the one presented
    here, but the overall parts of long-term and short-term memory are present throughout
    the vast majority of the implementations.
  prefs: []
  type: TYPE_NORMAL
- en: I hope the reader now has a better understanding of the LSTM layers and I hope
    he or she will start implementing it into their pipeline right away!
  prefs: []
  type: TYPE_NORMAL
- en: Special shoutout to the wonderful explainer video by StatQuest⁵.
  prefs: []
  type: TYPE_NORMAL
- en: '[1]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name:** Twitter Sentiment Analysis'
  prefs: []
  type: TYPE_NORMAL
- en: '**URL:** [https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset Licence:** [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name:** Vanishing Gradient Problem'
  prefs: []
  type: TYPE_NORMAL
- en: '**URL:** [https://k21academy.com/datascience-blog/machine-learning/recurrent-neural-networks/#:~:text=Two%20Issues%20of%20Standard%20RNNs&text=RNNs%20suffer%20from%20the%20matter,of%20long%20data%20sequences%20difficult](https://k21academy.com/datascience-blog/machine-learning/recurrent-neural-networks/#:~:text=Two%20Issues%20of%20Standard%20RNNs&text=RNNs%20suffer%20from%20the%20matter,of%20long%20data%20sequences%20difficult).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name:** LONG SHORT-TERM MEMORY'
  prefs: []
  type: TYPE_NORMAL
- en: '**URL:** [https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Year:** 1997'
  prefs: []
  type: TYPE_NORMAL
- en: '[4]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name:** Understanding LSTM Networks'
  prefs: []
  type: TYPE_NORMAL
- en: '**URL:** [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Year:** 2015'
  prefs: []
  type: TYPE_NORMAL
- en: '[5]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name:** Long Short-Term Memory (LSTM), Clearly Explained'
  prefs: []
  type: TYPE_NORMAL
- en: '**URL:** [https://www.youtube.com/watch?v=YCzL96nL7j0](https://www.youtube.com/watch?v=YCzL96nL7j0&t=358s)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Year:** 2022'
  prefs: []
  type: TYPE_NORMAL
