- en: A Requiem for the Transformer?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对变换器的悼歌？
- en: 原文：[https://towardsdatascience.com/a-requiem-for-the-transformer-297e6f14e189](https://towardsdatascience.com/a-requiem-for-the-transformer-297e6f14e189)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-requiem-for-the-transformer-297e6f14e189](https://towardsdatascience.com/a-requiem-for-the-transformer-297e6f14e189)
- en: '|PERSPECTIVES| AI| LARGE LANGUAGE MODELS|'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '|观点| AI| 大型语言模型|'
- en: Will be the transformer the model leading us to artificial general intelligence?
    Or will be replaced?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器会成为引领我们迈向人工通用智能的模型吗？还是会被替代？
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----297e6f14e189--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----297e6f14e189--------------------------------)[](https://towardsdatascience.com/?source=post_page-----297e6f14e189--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----297e6f14e189--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----297e6f14e189--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----297e6f14e189--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----297e6f14e189--------------------------------)[](https://towardsdatascience.com/?source=post_page-----297e6f14e189--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----297e6f14e189--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----297e6f14e189--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----297e6f14e189--------------------------------)
    ·18 min read·Dec 1, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----297e6f14e189--------------------------------)
    ·阅读时间18分钟·2023年12月1日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ab049cc38b8a0ad788f2871ab46d854d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab049cc38b8a0ad788f2871ab46d854d.png)'
- en: Photo by [Stefany Andrade](https://unsplash.com/@stefany_andrade?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于[Stefany Andrade](https://unsplash.com/@stefany_andrade?utm_source=medium&utm_medium=referral)
    在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The transformer has dominated the world of artificial intelligence for six years,
    achieving state-of-the-art in all subdomains of artificial intelligence. From
    [natural language processing (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing)
    to computer vision to sound and graphs, there are dedicated transformers with
    excellent performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器在人工智能领域主导了六年，成为所有人工智能子领域的最先进技术。从[自然语言处理 (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing)
    到计算机视觉，再到声音和图表，各个领域都有表现优异的专用变换器。
- en: How much longer will this domain last?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个领域还会持续多久？
- en: Is the transformer really the best architecture out there?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器真的还是最好的架构吗？
- en: Will it be replaced in the near future?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它会在不久的将来被替代吗？
- en: What are the threats to its dominance?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的主导地位面临什么威胁？
- en: This article attempts to answer these questions. Starting with why the transformer
    has been so successful and what elements have allowed it to establish itself in
    so many different domains, we will analyze whether it still has unchallenged dominance,
    what elements threaten its supremacy, and whether there are potential competitors.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文尝试回答这些问题。从变换器为何如此成功以及哪些因素使其在众多领域中立足开始，我们将分析它是否仍然拥有无可争议的主导地位，哪些因素威胁到它的霸主地位，以及是否存在潜在的竞争者。
- en: A brief history of an empire
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个帝国的简要历史
- en: '![](../Images/7bbb943855d46e605cb892ba837e1882.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bbb943855d46e605cb892ba837e1882.png)'
- en: Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于[Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: “All empires become arrogant. It is their nature.” ― Edward Rutherfurd
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “所有的帝国都会变得傲慢。这是它们的本性。” ― 爱德华·拉瑟福德
- en: ''
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Empires inevitably fall, and when they do, history judges them for the legacies
    they leave behind. — Noah Feldman
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 帝国不可避免地衰落，当它们衰落时，历史会根据它们留下的遗产来评判它们。—— 诺亚·费尔德曼
- en: '[“Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)” Is the
    basis of artificial intelligence as we know it today. The roots of generative
    AI and its success are in a single seed: **the transformer.**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[“Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)” 是我们今天所知的人工智能的基础。生成型AI及其成功的根源都在于一个种子：**变换器**。'
- en: The [transformer](https://en.wikipedia.org/wiki/Transformer_(machine-learning_model))
    was initially designed to solve the lack of parallelization of [RNNs](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    and to be able to model long-distance relationships among words in a sequence.
    The idea was to provide the model with a system to discriminate which parts of
    the sequence were most important (where to pay [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))).
    This was all designed to improve machine translation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformer](https://en.wikipedia.org/wiki/Transformer_(machine-learning_model))
    最初是为了解决 [RNNs](https://en.wikipedia.org/wiki/Recurrent_neural_network) 的平行化不足，并能够建模序列中单词之间的长距离关系。其理念是为模型提供一个系统，以区分序列中哪些部分最重要（即注意力的关注点，[attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))）。这一切都是为了改进机器翻译。'
- en: These elements, though, allowed the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    to understand the text better. In addition, parallelization allowed the model
    to scale both as a size and as on larger datasets. The rise of [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit)
    further showed the benefits of a parallelizable architecture like the Transformer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些因素使得 [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    更好地理解文本。此外，平行化使得模型在规模和更大的数据集上都能扩展。 [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)
    的兴起进一步展示了像 Transformer 这样的可平行化架构的好处。
- en: 'The Transformer thus emerged as the new king of AI. An empire grew in a very
    short time. In fact, today, all the popular models are Transformer: [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT),
    [Bard](https://en.wikipedia.org/wiki/Bard_(chatbot)), [GitHub Copilot](https://en.wikipedia.org/wiki/GitHub_Copilot),
    [Mistral](https://arxiv.org/abs/2310.06825), [LLaMA](https://en.wikipedia.org/wiki/LLaMA),
    Bing Chat, [stable diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion),
    [DALL-E](https://en.wikipedia.org/wiki/DALL-E), Midjourney, and so on.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Transformer 崛起成为了 AI 的新王者。一个帝国在极短的时间内建立起来。事实上，如今所有流行的模型都是 Transformer：[ChatGPT](https://en.wikipedia.org/wiki/ChatGPT)、[Bard](https://en.wikipedia.org/wiki/Bard_(chatbot))、[GitHub
    Copilot](https://en.wikipedia.org/wiki/GitHub_Copilot)、[Mistral](https://arxiv.org/abs/2310.06825)、[LLaMA](https://en.wikipedia.org/wiki/LLaMA)、Bing
    Chat、[稳定扩散](https://en.wikipedia.org/wiki/Stable_Diffusion)、[DALL-E](https://en.wikipedia.org/wiki/DALL-E)、Midjourney
    等等。
- en: '![](../Images/72edabb76414dca7b07a692dffd1cb2f.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72edabb76414dca7b07a692dffd1cb2f.png)'
- en: 'The trends of the cumulative numbers of arXiv papers that contain the keyphrases
    “language model” and “large language model”. image source: [here](https://arxiv.org/pdf/2303.18223.pdf)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 包含“语言模型”和“大型语言模型”关键短语的 arXiv 论文的累积数量趋势。图像来源：[这里](https://arxiv.org/pdf/2303.18223.pdf)
- en: This is because Transformer was quickly adapted to so many tasks beyond language.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为 Transformer 很快适应了许多超越语言的任务。
- en: Even the vastest empires fall at some point; what is happening to the Transformer
    dominion?
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 即使是最庞大的帝国也会在某个时刻衰落；Transformer 的统治正在发生什么？
- en: The giant with feet of clay
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “脚踏实地的巨人”
- en: '![](../Images/740eadf41176a62f613e0d5089cf5bd8.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/740eadf41176a62f613e0d5089cf5bd8.png)'
- en: 'image source: [here](https://huggingface.co/blog/large-language-models)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[这里](https://huggingface.co/blog/large-language-models)
- en: When the transformer was introduced, its performance shocked the world and gave
    rise to a parameter race. For a time we saw a kind of growth in models to the
    extent that it was called the “*new Moore law of AI.*” The growth continued until
    [Megatron](https://huggingface.co/docs/accelerate/usage_guides/megatron_lm) (530
    B) and [Google PaLM](https://ai.google/discover/palm2/) (540 B) were released
    in 2022\. **And yet we still haven’t seen the trillion parameters.**
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当 transformer 被引入时，它的性能震惊了世界，并引发了参数竞赛。曾经一度，我们看到模型的增长速度如此之快，以至于它被称为“*AI 的新摩尔定律*。”这种增长一直持续到
    2022 年 [Megatron](https://huggingface.co/docs/accelerate/usage_guides/megatron_lm)（530
    B）和 [Google PaLM](https://ai.google/discover/palm2/)（540 B）发布。**然而我们仍未见到万亿参数。**
- en: When deep convolutional networks showed their efficiency (VGG models), [ConvNets](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    went from 16 layers of [VGG16](https://arxiv.org/abs/1409.1556) to 201 layers
    of [DenseNet201](https://arxiv.org/abs/1608.06993) in a short time. Results and
    performance aside, it is a testament to the interest of the community. This pattern
    of horizontal and vertical growth (and incremental changes to the base model)
    stopped in 2021 when the community became convinced that [Vision Transformers
    (ViTs)](https://huggingface.co/docs/transformers/model_doc/vit) were superior
    to ConvNets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当深度卷积网络展示其高效性（VGG模型）时，[卷积神经网络](https://en.wikipedia.org/wiki/Convolutional_neural_network)在短时间内从16层的[VGG16](https://arxiv.org/abs/1409.1556)发展到201层的[DenseNet201](https://arxiv.org/abs/1608.06993)。除了结果和性能外，这也是社区兴趣的证明。这种水平和垂直增长（以及对基础模型的渐进性变化）的模式在2021年停止了，当时社区相信[视觉变换器（ViTs）](https://huggingface.co/docs/transformers/model_doc/vit)优于卷积神经网络。
- en: '![](../Images/1ca3f2c8609e3c259202ff0ceffcee10.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ca3f2c8609e3c259202ff0ceffcee10.png)'
- en: 'Vision Transformer. image source: [here](https://arxiv.org/pdf/2101.01169.pdf)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉变换器。图片来源：[这里](https://arxiv.org/pdf/2101.01169.pdf)
- en: Why did the growth of transformers stop? Were they replaced as well?
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么变换器的增长停止了？它们也被替代了吗？
- en: '**No, but some of the premises that led to transformer growth have disappeared.**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**不，但导致变换器增长的一些前提已经消失。**'
- en: This growth was motivated by the so-called [power law](https://en.wikipedia.org/wiki/Power_law).
    [According to OpenAI](https://arxiv.org/abs/2001.08361), by increasing the number
    of parameters, properties emerge abruptly. So scaling the models leads the model
    to develop properties that would not be observable below a certain scale. Too
    bad, that for Stanford researchers these properties are a mirage derived from
    a bias.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种增长受到了所谓的[幂律](https://en.wikipedia.org/wiki/Power_law)的激励。[根据OpenAI](https://arxiv.org/abs/2001.08361)的说法，通过增加参数数量，特性会突然显现。因此，扩大模型规模会使模型发展出在某一规模以下无法观察到的特性。遗憾的是，斯坦福研究人员认为这些特性是偏差导致的海市蜃楼。
- en: '[](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----297e6f14e189--------------------------------)
    [## Emergent Abilities in AI: Are We Chasing a Myth?'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----297e6f14e189--------------------------------)
    [## AI中的新兴能力：我们在追逐神话吗？'
- en: Changing Perspective on Large Language Models emerging properties
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于大语言模型新兴特性的视角改变
- en: towardsdatascience.com](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----297e6f14e189--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----297e6f14e189--------------------------------)'
- en: Scaling up a model means spending much more. More parameters, more computation,
    more infrastructure, more electrical consumption (and more carbon emissions).
    **Is it worth it?**
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 扩大模型意味着花费更多。更多参数、更多计算、更多基础设施、更高的电力消耗（以及更多的碳排放）。**这值得吗？**
- en: Actually, [DeepMind with Chinchilla](https://arxiv.org/abs/2203.15556) said
    that performance increases not only with the number of parameters but also with
    the amount of data. So if you want a model with billions of parameters you have
    to have enough tokens to train it. **Too bad, that we humans don’t produce enough
    to train a model from a trillion parameters.**
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，[DeepMind与Chinchilla](https://arxiv.org/abs/2203.15556)表示，性能的提升不仅与参数数量有关，还与数据量有关。因此，如果你想要一个拥有数十亿参数的模型，你必须拥有足够的令牌来训练它。**遗憾的是，我们人类无法产生足够的内容来训练一个万亿参数的模型。**
- en: '![](../Images/57aa796415fd7a367e260169c7427bec.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57aa796415fd7a367e260169c7427bec.png)'
- en: 'human data are a finite resource. image source: [here](https://arxiv.org/pdf/2211.04325.pdf)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 人类数据是有限资源。图片来源：[这里](https://arxiv.org/pdf/2211.04325.pdf)
- en: In addition, it is not just the quantity of text that impacts the performance
    of a model. **it is the quality of the text.** This is also a sore point because
    collecting huge amounts of text without filtering is not a good idea (aka downloading
    without criteria from the Internet).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，影响模型性能的不仅仅是文本的数量。**而是文本的质量。** 这也是一个痛点，因为收集大量未经筛选的文本并不是一个好主意（即从互联网上下载时不加选择）。
- en: '[](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----297e6f14e189--------------------------------)
    [## Say Once! Repeating Words Is Not Helping AI'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----297e6f14e189--------------------------------)
    [## 说一次！重复词汇并不能帮助AI'
- en: How and why is repeating tokens harming LLMs? Why is this a problem?
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重复令牌如何以及为何对大语言模型造成伤害？为什么这是个问题？
- en: towardsdatascience.com](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----297e6f14e189--------------------------------)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----297e6f14e189--------------------------------)'
- en: Also, generating text using artificial intelligence is not a good idea. In theory,
    one could use an LLM and ask it to produce text indefinitely. The problem is that
    the model trained with this text can only mimic another LLM, and certainly not
    outperform it.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，使用人工智能生成文本并不是一个好主意。理论上，人们可以使用一个LLM并要求它无限期地生成文本。问题在于，使用这些文本训练的模型只能模仿另一个LLM，肯定不能超越它。
- en: 'Overall, our key takeaway is that model imitation is not a free lunch: there
    exists a capabilities gap between today’s open-source LMs and their closed-source
    counterparts that cannot be closed by cheaply fine-tuning on imitation data. ([source](https://arxiv.org/pdf/2305.15717.pdf))'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 总体而言，我们的关键结论是，模型模仿并不是免费的午餐：今天的开源语言模型与其闭源对应物之间存在一个能力差距，这个差距无法通过廉价的模仿数据微调来弥合。
    ([source](https://arxiv.org/pdf/2305.15717.pdf))
- en: '[](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5?source=post_page-----297e6f14e189--------------------------------)
    [## The imitation game: Taming the gap between open source and proprietary models'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 模仿游戏：驾驭开源与专有模型之间的差距](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5?source=post_page-----297e6f14e189--------------------------------)'
- en: Can imitation models reach the performance of proprietary models like ChatGPT?
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模仿模型能否达到像ChatGPT这样的专有模型的性能？
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5?source=post_page-----297e6f14e189--------------------------------)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5?source=post_page-----297e6f14e189--------------------------------)'
- en: '**An additional point is that these huge models are also problematic for deployment.**
    Smaller models have good performance, especially for some tasks. One can distill
    and get much smaller models specialized for a specific task.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**另一个要点是，这些巨大的模型在部署上也存在问题。** 较小的模型性能很好，尤其是在某些任务中。可以将其提炼得到更小的专用模型。'
- en: '**Take home message: the huge transformer paradigm is in crisis. The idea that
    every year we will see a bigger and bigger model is over.**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**带回家的信息：巨大的变换器范式正面临危机。每年我们会看到更大模型的想法已经结束。**'
- en: After all, the issue is practicality (and cost). AI can cost a lot of money
    once it goes into production. For example, [Microsoft is reportedly losing huge
    amounts of money](https://www.techradar.com/pro/microsoft-is-reportedly-losing-huge-amounts-of-money-on-github-copilot)
    on GitHub Copilot ($20 per user per month). According to one report, [ChatGPT
    costs $700,000 per day](https://technext24.com/2023/08/14/chatgpt-costs-700000-daily-openai/),
    and investors may no longer cover the cost if ChatGPT does not become profitable.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 毕竟，问题在于实际性（和成本）。一旦进入生产阶段，AI的成本可能非常高。例如，[微软据说在GitHub Copilot上亏损巨额资金](https://www.techradar.com/pro/microsoft-is-reportedly-losing-huge-amounts-of-money-on-github-copilot)（每用户每月20美元）。据一份报告，[ChatGPT每天的成本为70万美元](https://technext24.com/2023/08/14/chatgpt-costs-700000-daily-openai/)，如果ChatGPT不能盈利，投资者可能不会继续承担这些成本。
- en: Therefore we can expect companies more interested in developing smaller models
    with a specific task and business in mind.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以期待公司更倾向于开发专注于特定任务和业务的较小模型。
- en: Okay, the transformer no longer grows, but is it still the best architecture
    in the game?
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 好吧，变换器不再增长了，但它仍然是游戏中最好的架构吗？
- en: well, let’s talk about that in the next section…
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们在下一节中讨论这个话题…
- en: Convolution is still on fire
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积仍然备受关注
- en: '![](../Images/55c3ef5870e540e3c2327c5ae1a51ef1.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55c3ef5870e540e3c2327c5ae1a51ef1.png)'
- en: Photo by [Ricardo Gomez Angel](https://unsplash.com/@rgaleriacom?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Ricardo Gomez Angel](https://unsplash.com/@rgaleriacom?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: First, why was the transformer successful everywhere?
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 首先，为什么变换器在各个领域都成功？
- en: 'In its initial description, the transformer brought together three basic concepts:
    starting with a position-aware representation of the sequence ([embedding](https://en.wikipedia.org/wiki/Embedding)
    + positional encoding), relating the elements of the sequence (self-attention),
    and constructing a hierarchical representation (layer stacking).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的描述中，变换器结合了三个基本概念：首先是序列的位置信息表示（[嵌入](https://en.wikipedia.org/wiki/Embedding)
    + 位置编码），然后是序列元素的关联（自注意力），最后是构建层级表示（层叠）。
- en: 'When the article [*Attention is All You Need*](https://arxiv.org/abs/1706.03762)
    was published it was based on a decade of research in NLP and put together the
    best of what had been published previously:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当文章[*Attention is All You Need*](https://arxiv.org/abs/1706.03762)发布时，它基于十年的NLP研究，并整合了之前发布的最佳成果：
- en: '[word embedding](https://arxiv.org/abs/1301.3781) was revolutionary in 2013
    in being able to transform words into vector representations. In addition, operations
    on embedding had logical and grammatical meaning.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[词嵌入](https://arxiv.org/abs/1301.3781)在2013年具有革命性，因为它能将词语转化为向量表示。此外，对嵌入的操作具有逻辑和语法上的意义。'
- en: Self-attention was an improvement on [the revolutionary idea that not all elements](https://arxiv.org/abs/1409.0473)
    of the sequence are important. Plus solving the long-standing problem of recurring
    neural networks and their [vanishing gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力在[并非所有序列元素](https://arxiv.org/abs/1409.0473)都重要的革命性思想上有所改进。此外，它还解决了循环神经网络长期存在的问题和[梯度消失](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)问题。
- en: The [hierarchical representation](https://distill.pub/2020/circuits/zoom-in/),
    on the other hand, came from twenty years of convolutional neural networks where
    we realized that by stacking layers the model learns an increasingly complex representation
    of the data.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[层级表示](https://distill.pub/2020/circuits/zoom-in/)则来自二十年的卷积神经网络研究，我们意识到通过堆叠层，模型可以学习到数据的越来越复杂的表示。'
- en: '![](../Images/18916ea2d147bd0b6dc0085e155c6035.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18916ea2d147bd0b6dc0085e155c6035.png)'
- en: 'The skeleton of its majesty, the transformer. Image source: [here](https://arxiv.org/abs/1706.03762)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其威严的骨架，变换器。图片来源：[这里](https://arxiv.org/abs/1706.03762)
- en: These elements made him successful in the NLP field, but at the same time, they
    were the key to winning in other fields as well. First, the fact that it had a
    very weak inductive bias made it adaptable to almost any type of data. Second,
    hierarchical representation and connecting elements of a sequence have applications
    far beyond NLP.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些元素使得他在NLP领域取得成功，但与此同时，它们也是在其他领域获胜的关键。首先，变换器具有非常弱的归纳偏差，使其能够适应几乎任何类型的数据。其次，层级表示和连接序列元素的能力在NLP之外也有广泛的应用。
- en: '**A story of success, except that the transformer has remained the same as
    it was in 2017 and is beginning to age badly.**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**成功的故事，除非变换器自2017年起未曾改变，开始逐渐老化。**'
- en: The beating heart of transformers is ultimately self-attention. But it is a
    heart that pumps too much blood. In fact, its quadratic computational cost is
    huge.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器的核心最终是自注意力。但这颗心脏泵送的血液过多。实际上，它的计算成本是巨大的。
- en: Therefore, several groups have tried to try to find a linear substitution to
    attention. However, all of these variants have been shown to have inferior performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，几个团队尝试寻找对自注意力的线性替代方法。然而，所有这些变体都被证明表现不如自注意力。
- en: '**And what seemed a good substitute?** Nothing less than the old convolution.
    As they showed in Hyena, by adapting convolution a little bit, you get a good
    model with transformer-like performance.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**而什么看起来是一个不错的替代品？** 无非就是古老的卷积。正如他们在Hyena中所展示的，通过稍微调整卷积，你可以获得一个表现类似变换器的好模型。'
- en: '[](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----297e6f14e189--------------------------------)
    [## Welcome Back 80s: Transformers Could Be Blown Away by Convolution'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----297e6f14e189--------------------------------)
    [## 欢迎回到80年代：卷积可能会击败变换器'
- en: The Hyena model shows how convolution could be faster than self-attention
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hyena模型展示了卷积如何比自注意力更快。
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----297e6f14e189--------------------------------)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----297e6f14e189--------------------------------)'
- en: This is ironic because since 2021 [Vision Transformers](https://en.wikipedia.org/wiki/Vision_transformer)
    (ViTs) have been believed to be superior to ConvNets in computer vision. This
    seemed to be the end of the uncodified dominance of convolutional networks ([ConvNets](https://en.wikipedia.org/wiki/Convolutional_neural_network))
    in what until recently was their realm. **But instead?**
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这颇具讽刺意味，因为自2021年以来，[视觉变换器](https://en.wikipedia.org/wiki/Vision_transformer)（ViTs）被认为在计算机视觉中优于ConvNets。这似乎标志着卷积网络（[ConvNets](https://en.wikipedia.org/wiki/Convolutional_neural_network)）在其领域中未被规范的霸主地位的终结。**但事实是？**
- en: It seems that the ConvNets have had their revenge. Astonishing, like thinking
    that dinosaurs would return to dominance over continents by driving out mammals.
    In truth, a [recent article published by DeepMind](https://arxiv.org/abs/2310.16764)
    basically states that the comparison between ViTs and ConvNets was not fair. By
    providing the same compute budget to ConvNets these have similar performance to
    ViTs on ImageNet.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎ConvNets已经实现了复仇。令人惊讶，就像认为恐龙会通过驱逐哺乳动物重新掌控大陆一样。实际上，[DeepMind发布的最新文章](https://arxiv.org/abs/2310.16764)基本上指出ViTs与ConvNets的比较并不公平。通过给ConvNets提供相同的计算预算，它们在ImageNet上的表现与ViTs相似。
- en: '[](https://levelup.gitconnected.com/have-convolutional-networks-become-obsolete-245969f6b9d9?source=post_page-----297e6f14e189--------------------------------)
    [## Have convolutional networks become obsolete'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/have-convolutional-networks-become-obsolete-245969f6b9d9?source=post_page-----297e6f14e189--------------------------------)
    [## 卷积网络是否已经过时'
- en: Vision transformers seem to have replaced convolutional networks, but are they
    really better?
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视觉变换器似乎取代了卷积网络，但它们真的更好吗？
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/have-convolutional-networks-become-obsolete-245969f6b9d9?source=post_page-----297e6f14e189--------------------------------)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/have-convolutional-networks-become-obsolete-245969f6b9d9?source=post_page-----297e6f14e189--------------------------------)
- en: '[Another article](https://arxiv.org/abs/2310.19909) seems to go in the same
    direction, convolutional networks seem to be competitive with transformers:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[另一篇文章](https://arxiv.org/abs/2310.19909)似乎也朝着同样的方向发展，卷积网络似乎与变换器具有竞争力：'
- en: The same winners also win at smaller scales. Among smaller backbones, ConvNeXt-Tiny
    and SwinV2-Tiny emerge victorious, followed by DINO ViT-Small. ([source](https://arxiv.org/abs/2310.19909))
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同样的获胜者在较小规模下也获胜。在较小的主干网络中，ConvNeXt-Tiny和SwinV2-Tiny表现突出，其次是DINO ViT-Small。（[source](https://arxiv.org/abs/2310.19909)）
- en: '![](../Images/7bf17cf03bb3da5aebdc66385982f6fe.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bf17cf03bb3da5aebdc66385982f6fe.png)'
- en: 'image source: [here](https://arxiv.org/abs/2310.19909)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2310.19909)
- en: 'There are three primary factors that influence the performance of such a model:
    its architecture, the pretraining algorithm, and the pretraining dataset. ([source](https://arxiv.org/abs/2310.19909))'
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 影响这种模型性能的主要因素有三个：其架构、预训练算法和预训练数据集。（[source](https://arxiv.org/abs/2310.19909)）
- en: 'Now if the pretraining algorithm and pretraining dataset are the same only
    the model architecture remains. **However, all things being equal, the alleged
    superiority of ViTs does not seem to emerge.** So much so that it seems like an
    admission of defeat what the DeepMind authors claim:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果预训练算法和预训练数据集相同，那么剩下的只有模型架构。**然而，在所有条件相等的情况下，ViTs的所谓优越性似乎并未显现。** 以至于DeepMind的作者所说的内容似乎像是承认了失败：
- en: Although the success of ViTs in computer vision is extremely impressive, in
    our view there is no strong evidence to suggest that pre-trained ViTs outperform
    pre-trained ConvNets when evaluated fairly. ([source](https://arxiv.org/abs/2310.16764))
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 尽管ViTs在计算机视觉领域的成功令人印象深刻，但在我们看来，尚无强有力的证据表明经过预训练的ViTs在公平评估时优于经过预训练的ConvNets。（[source](https://arxiv.org/abs/2310.16764)）
- en: '**Ouch. So we can say that ViTs are not superior to convolutional networks
    at least in computer vision. Is it?**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**哎呀。因此我们可以说，在计算机视觉领域，ViTs至少并不优于卷积网络。是吗？**'
- en: We note however that ViTs may have practical advantages in specific contexts,
    such as the ability to use similar model components across multiple modalities.
    ([source](https://arxiv.org/abs/2310.16764))
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们注意到，ViTs在特定背景下可能具有实际优势，例如在多个模态中使用相似的模型组件的能力。（[source](https://arxiv.org/abs/2310.16764)）
- en: The authors point out that they could potentially still be superior because
    useful when we are interested in multimodal models. Considering that features
    can also be extracted from a convolutional network, it is certainly more convenient
    to use the same model across multiple modalities.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 作者指出，它们可能仍然更优，因为在我们对多模态模型感兴趣时是有用的。考虑到特征也可以从卷积网络中提取，使用相同的模型在多个模态中显然更方便。
- en: However this is a very important point, empirical data show that at least in
    computer vision the transformer is not superior to other architectures. **This
    leads us to wonder whether its dominance will soon be questioned in other fields
    of artificial intelligence as well. For example, what is happening in the core
    field of the transformer? It is still the best model in natural language processing?**
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这是一个非常重要的点，实证数据表明，至少在计算机视觉中，transformer并不优于其他架构。**这让我们不禁思考它的主导地位是否会在其他人工智能领域受到挑战。例如，transformer核心领域的情况如何？它仍然是自然语言处理中的最佳模型吗？**
- en: The text Dominion has a fragile basis
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本Dominion的基础很脆弱
- en: '![](../Images/1229a84bd59851556c1e6cf876b3d236.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1229a84bd59851556c1e6cf876b3d236.png)'
- en: Photo by [Sigmund](https://unsplash.com/@sigmund?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Sigmund](https://unsplash.com/@sigmund?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Short answer: **yes, but its supremacy could end.** Let’s start with why it
    has been so successful in NLP.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 简短回答：**是的，但它的至高无上的地位可能会结束。** 让我们从它为何在NLP中如此成功开始。
- en: 'The initial advantage of the transformer on RNNs is that easily parallelized.
    This led to the initial euphoria and rush to the parameter. In the process, we
    realized what allowed the Transformer to win in NLP: **in-context learning**.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer相比RNN的初始优势在于容易并行化。这导致了最初的狂热和对参数的急切追求。在此过程中，我们意识到是什么使得Transformer在NLP中获胜：**上下文学习**。
- en: 'In-context learning is a very powerful concept: all it takes is a few examples
    and the model is capable of mapping a relationship between input and output. All
    this without even updating a single parameter.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习是一个非常强大的概念：只需几个示例，模型就能够映射输入和输出之间的关系。所有这一切无需更新任何参数。
- en: '[](/all-you-need-to-know-about-in-context-learning-55bde1180610?source=post_page-----297e6f14e189--------------------------------)
    [## All You Need to Know about In-Context Learning'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/all-you-need-to-know-about-in-context-learning-55bde1180610?source=post_page-----297e6f14e189--------------------------------)
    [## 你需要了解的所有关于上下文学习的内容'
- en: What is and how does it work what makes Large Language Models so powerful
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是及其工作原理是什么，让大型语言模型如此强大
- en: towardsdatascience.com](/all-you-need-to-know-about-in-context-learning-55bde1180610?source=post_page-----297e6f14e189--------------------------------)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/all-you-need-to-know-about-in-context-learning-55bde1180610?source=post_page-----297e6f14e189--------------------------------)
- en: Basically, this was an unanticipated (and not yet 100 % understood) effect of
    self-attention. [According to Anthropic](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html),
    there are induction heads that practically connect different parts of the model
    and allow this mapping.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这是自注意力的一个意外效果（且尚未完全理解）。[根据Anthropic](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)的说法，有一些引导头实际上连接了模型的不同部分，并允许这种映射。
- en: '**This miracle is the basis of the supposed reasoning capabilities of the models**.
    In addition, the fact that one could experiment so much with the prompt allowed
    for incredible results.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**这种奇迹是模型所谓推理能力的基础**。此外，能够如此多地实验提示也带来了令人难以置信的结果。'
- en: In practice, without having to train the model again, [prompting techniques](https://en.wikipedia.org/wiki/Prompt_engineering)
    could be created to improve the model’s capabilities in inference. [**Chain of
    thought**](https://arxiv.org/abs/2201.11903) is the best example of this approach.
    Using this ploy, an LLM is able to solve problems that require reasoning (math
    problems, coding problems, and so on).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，无需重新训练模型，[提示技术](https://en.wikipedia.org/wiki/Prompt_engineering)可以被创建以提升模型在推理方面的能力。[**思维链**](https://arxiv.org/abs/2201.11903)就是这种方法的最佳示例。利用这种策略，LLM能够解决需要推理的问题（数学问题、编程问题等）。
- en: '![](../Images/320d30d99027d11abcbc04beca7b57ef.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/320d30d99027d11abcbc04beca7b57ef.png)'
- en: 'image source: [here](https://arxiv.org/abs/2201.11903)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[here](https://arxiv.org/abs/2201.11903)
- en: 'However, one must take into account that:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，必须考虑到：
- en: However, this multi-step generation process does not inherently imply that LLMs
    possess strong reasoning capabilities, as they may merely emulate the superficial
    behavior of human reasoning without genuinely comprehending the underlying logic
    and rules necessary for precise reasoning. ([source](https://arxiv.org/abs/2310.20689v1))
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然而，这种多步骤生成过程并不意味着LLM具有强大的推理能力，因为它们可能仅仅模拟了人类推理的表面行为，而不是真正理解精确推理所需的基础逻辑和规则。[来源](https://arxiv.org/abs/2310.20689v1)
- en: Translated, we have created a parrot that has seen the entire human knowledge
    and can connect the question in the prompt with what it has seen during the training.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译过来，我们已经创造了一个见识过整个人类知识的鹦鹉，能够将提示中的问题与它在训练中看到的内容联系起来。
- en: Why is this advantage extremely precarious?
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么这种优势极其脆弱？
- en: '**Because the parrot does not have to be a transformer.** We need any model
    that takes natural language instructions as input and can do in-context learning,
    after which we can use the whole arsenal of prompt engineering techniques as if
    it were a transformer.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**因为鹦鹉不一定是变压器。** 我们需要任何能够接受自然语言指令作为输入并进行上下文学习的模型，然后我们可以像使用变压器一样使用所有的提示工程技术。'
- en: Ok, so if we do not need necessarily the transformer, where is our new “stochastic
    parrot”?
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 好吧，如果我们不一定需要变压器，那我们的新“随机鹦鹉”在哪里？
- en: Bureaucracy slows down innovation
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 官僚制度减缓了创新
- en: '![](../Images/6a6a4dca95a438143c96e5de5ba8f559.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a6a4dca95a438143c96e5de5ba8f559.png)'
- en: Photo by [Nam Anh](https://unsplash.com/@bepnamanh?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Nam Anh](https://unsplash.com/@bepnamanh?utm_source=medium&utm_medium=referral)提供，[来源于Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The main reason is that research in industry is currently focused on bringing
    the transformer (despite its flaws) into production. Also, it is risky to put
    a better architecture into production but whose behavior we know less about.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 主要原因是，目前工业界的研究集中在将变压器（尽管有其缺陷）投入生产。此外，将一个我们了解较少的更好架构投入生产也是有风险的。
- en: Let’s dig more about it…
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们深入了解一下…
- en: 'First, Google, META, Amazon, and other big tech have huge amounts of resources.
    Such large companies, however, are burdened by an elephantine internal bureaucracy:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，谷歌、META、亚马逊和其他大型科技公司拥有大量资源。然而，这些大公司却受到庞大的内部官僚制度的困扰：
- en: Google is a “once-great company” that has “slowly ceased to function” thanks
    to its bureaucratic “maze.” ([source](https://fortune.com/2023/02/16/alphabet-google-former-employee-praveen-seshadri-essay-criticizes-bureaucratic-maze/))
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 谷歌是一个“曾经伟大的公司”，由于其官僚化的“迷宫”，它“逐渐停止了运作”。[来源](https://fortune.com/2023/02/16/alphabet-google-former-employee-praveen-seshadri-essay-criticizes-bureaucratic-maze/)
- en: This increase in bureaucracy, results in reduced productivity and an overall
    slowdown. In order to implement a small change, one must have the approval of
    increasingly long chains of command and follow [increasingly complex protocols](https://www.theinformation.com/articles/aws-new-ceo-faces-a-fresh-challenge-bureaucracy).
    In short, it seems that big tech has the same problem that has plagued empires.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 官僚制度的增加导致了生产力的下降和整体放缓。为了实施一个小变化，必须获得越来越长的指挥链的批准，并遵循[日益复杂的协议](https://www.theinformation.com/articles/aws-new-ceo-faces-a-fresh-challenge-bureaucracy)。简而言之，大型科技公司似乎有着与帝国相同的问题。
- en: 'This obviously impacts innovation as well:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然也影响了创新：
- en: “If I had to summarize it, I would say that the signal to noise ratio is what
    wore me down. The innovation challenges … will only get worse as the risk tolerance
    will go down.” Noam Bardin, former Google executive. ([source](https://appleinsider.com/articles/21/06/21/google-is-risk-averse-has-paralyzing-bureaucracy-executives-say))
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “如果要总结一下，我会说信噪比是让我疲惫的原因。创新挑战… 随着风险容忍度的降低，只会变得更糟。”前谷歌高管诺姆·巴丁。[来源](https://appleinsider.com/articles/21/06/21/google-is-risk-averse-has-paralyzing-bureaucracy-executives-say)
- en: Of course, there are also well-founded reasons for companies like Google or
    Microsoft to be more cautious in their choices. For example, [Google lost billions
    in capitalization](https://fortune.com/2023/02/08/google-bard-ai-mistake-ad-stock-price-market-cap/)
    when Bard incorrectly answered a question about the James Webb Space Telescope.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，像谷歌或微软这样的公司在选择上更加谨慎也是有充分理由的。例如，[谷歌在Bard错误回答有关詹姆斯·韦布太空望远镜的问题时损失了数十亿美元](https://fortune.com/2023/02/08/google-bard-ai-mistake-ad-stock-price-market-cap/)。
- en: '**These reputational risks have turned into a giant brake on innovation**.
    A technology is adopted only when it is mature and not risky.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**这些声誉风险已经成为创新的巨大阻碍**。只有当技术成熟且风险较低时才会被采纳。'
- en: '**The best example is Apple.** The company adopts a technology only when it
    is mature and considers it profitable. In general, it has stopped being innovative
    in recent years (while still maintaining record profits). So far, it has kept
    out of the generative AI race because it does not consider it mature.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**最好的例子是苹果公司。** 该公司只有在技术成熟并且认为其有利可图时才会采用。一般来说，近年来它已停止创新（尽管仍保持创纪录的利润）。到目前为止，它一直没有参与生成性AI领域，因为它认为尚不成熟。'
- en: Apple is known as a “fast follower”; it likes to wait until new technologies
    have matured, then jump in with its own Apple-flavored version. ([source](https://www.fastcompany.com/90867819/apples-silence-on-generative-ai-grows-louder))
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 苹果被称为“快速跟随者”；它喜欢等到新技术成熟后，再推出自己的苹果版本。([来源](https://www.fastcompany.com/90867819/apples-silence-on-generative-ai-grows-louder))
- en: Shouldn’t this resistance to innovate then favor the transformer?
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 那么，这种对创新的抵制难道不应该有利于变压器技术吗？
- en: '**Yes, but we are forgetting open-source**. It is not only Big Tech doing research,
    there are many groups of researchers investigating AI. They may not have the resources
    of the FAANGs, but together independent research is a formidable force.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**是的，但我们忘记了开源**。不仅仅是大型科技公司在进行研究，还有许多研究小组在调查AI。他们可能没有FAANGs的资源，但独立研究合在一起是一股强大的力量。'
- en: '**Google itself admits this.** In fact, Mountain View is less afraid of Microsoft
    or OpenAI:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**谷歌自己也承认这一点。** 实际上，山景城对微软或OpenAI的恐惧较少：'
- en: “The uncomfortable truth is, we aren’t positioned to win this arms race and
    neither is OpenAI. While we’ve been squabbling, a third faction has been quietly
    eating our lunch,” Google’s leaked document, [source](https://www.theguardian.com/technology/2023/may/05/google-engineer-open-source-technology-ai-openai-chatgpt)
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “令人不安的真相是，我们并未处于赢得这场军备竞赛的位置，OpenAI也一样。在我们争吵的时候，第三方已经悄悄地抢占了我们的市场，”谷歌泄露的文件，[来源](https://www.theguardian.com/technology/2023/may/05/google-engineer-open-source-technology-ai-openai-chatgpt)
- en: '**This third faction is precisely open-source.** As soon as [LLaMA](https://ai.meta.com/llama/)
    was released it immediately received improved variants with instruction tuning,
    quantization, and [Reinforced learning from human feedback](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)
    (RLHF). Mistral 7B just came out and two groups extended the context length (first
    to [32K](https://huggingface.co/amazon/MistralLite) and then up to [128K](https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k)).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**第三方正是开源社区。** 一旦[LLaMA](https://ai.meta.com/llama/)发布，它立即收到了带有指令调整、量化和[人类反馈强化学习](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)（RLHF）的改进版本。Mistral
    7B刚刚推出，两组团队扩展了上下文长度（首先到[32K](https://huggingface.co/amazon/MistralLite)，然后到[128K](https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k)）。'
- en: Open-source models quickly become customizable and cheap, and gain support from
    a huge and active community. The open-source community immediately adopts each
    breakthrough and quickly improves it. Large companies waste time on internal bureaucracy
    and are restrained from adopting new technology because of the risk of reputational
    damage, risking falling behind.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模型迅速变得可定制且便宜，并获得了一个庞大且活跃的社区的支持。开源社区立即采用每一个突破，并迅速改进它。大型公司在内部官僚主义上浪费时间，并因声誉损害的风险而受到限制，导致可能落后。
- en: '**This outstanding community is fertile ground for the transformer’s successor.
    If a model showed to overcome the transformer’s limitations it could count on
    the force of a tsunami.**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**这个杰出的社区是变压器继任者的沃土。如果一个模型展示了克服变压器局限性的能力，它可能会获得如海啸般的力量。**'
- en: But let’s look at one last point as to why despite everything the transformer
    has not yet been replaced
  id: totrans-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但让我们看看最后一点，为什么尽管如此，变压器技术还没有被替代
- en: Theseus’ ship
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 叹息号的船
- en: '![](../Images/b866b91601b456d4007ca388e842eb84.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b866b91601b456d4007ca388e842eb84.png)'
- en: Photo by [Katherine McCormack](https://unsplash.com/@kathymack?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Katherine McCormack](https://unsplash.com/@kathymack?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'A question was raised by ancient philosophers: After several centuries of maintenance,
    if each individual part of the Ship of Theseus was replaced, one at a time, was
    it still the same ship? — [source](https://en.wikipedia.org/wiki/Ship_of_Theseus)'
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 古代哲学家提出了一个问题：经过数个世纪的维护，如果这些修补的 Theseus 号船的每一个部分都被逐个替换，它是否仍然是同一艘船？— [来源](https://en.wikipedia.org/wiki/Ship_of_Theseus)
- en: '**Has the transformer changed while remaining the same the transformer?** A
    brief search of the literature clearly shows that there are hundreds of variants
    of the transformer today (if one takes into account all the variations of positional
    encoding, attention, and so on). **So LLMs are not exactly the same model we have
    seen so far.**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**transformer 在保持不变的同时发生了变化吗？** 文献的简要搜索清楚地表明，如今有数百种 transformer 变体（如果考虑到所有位置编码、注意力等的变化）。**所以
    LLMs 并不是我们迄今为止看到的完全相同的模型。**'
- en: '![](../Images/5586a97ef672d3fc5a0cd316521b9d88.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5586a97ef672d3fc5a0cd316521b9d88.png)'
- en: 'Categorization of Transformer variants. image source: [here](https://www.sciencedirect.com/science/article/pii/S2666651022000146).
    Creative Commons, license: [here](https://creativecommons.org/licenses/by/4.0/)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 变体的分类。图片来源：[这里](https://www.sciencedirect.com/science/article/pii/S2666651022000146)。知识共享，许可证：[这里](https://creativecommons.org/licenses/by/4.0/)
- en: Right now we are in an [incremental phase of research](https://medium.com/mlearning-ai/the-decline-of-disruptive-science-730cc3fe28b1),
    where modifications of the same model continue to be proposed to try to overcome
    the limitations of a technology that is now old.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们处于[研究的增量阶段](https://medium.com/mlearning-ai/the-decline-of-disruptive-science-730cc3fe28b1)，在这个阶段，继续提出对相同模型的修改，以尝试克服现有技术的局限性。
- en: '**In a sense, we have reached the limits of the transformer.** There is no
    point in scaling the model because it does not bring benefits nor do we have enough
    tokens. Second, the transformer is computationally expensive, and **if in the
    past performance mattered today applications matter**. Third, COT and other techniques
    are patches to the real limit of the transformer: the model [is not really capable
    of understanding and generalizing](https://arxiv.org/abs/2311.00871). Self-attention
    allows the model to take advantage of the huge body of knowledge it has learned
    during training, but it still remains a stochastic parrot.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**在某种意义上，我们已经达到了 transformer 的极限。** 扩大模型没有意义，因为这不会带来好处，也没有足够的 tokens。其次，transformer
    的计算成本高昂，**如果说过去性能重要的话，如今应用才是关键**。第三，COT 和其他技术只是对 transformer 真实极限的补丁：模型[并不真正具备理解和概括的能力](https://arxiv.org/abs/2311.00871)。自注意力允许模型利用在训练过程中学到的大量知识，但它仍然是一个随机的鹦鹉。'
- en: The transformer is a way to capture interaction very quickly all at once between
    different parts of any input. It’s a general method that captures interactions
    between pieces in a sentence, or the notes in music, or pixels in an image, or
    parts of a protein. It can be purposed for any task.” — Ashish Vaswani, author
    of the transformer paper ([source](https://www.ft.com/content/37bb01af-ee46-4483-982f-ef3921436a50?accessToken=zwAGASoZXEbgkc83uwGv7kZEg9OYL-85IUNqUA.MEUCIGAX418D_QPFvxq-QLYReJE4g1m7wcZRXGk6pf1HqVQQAiEAn53oHV1zcVD3MYxeTbCWsSf_BIiSf_E_JcvZfhzlScw&sharetype=gift&token=4178b09d-f17c-4285-a18c-446dce2bdb54))
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: transformer 是一种快速捕捉任何输入不同部分之间交互的方式。它是一种通用方法，可以捕捉句子中片段之间的交互，或音乐中的音符，或图像中的像素，或蛋白质的部分。它可以用于任何任务。”
    — Ashish Vaswani，transformer 论文的作者（[来源](https://www.ft.com/content/37bb01af-ee46-4483-982f-ef3921436a50?accessToken=zwAGASoZXEbgkc83uwGv7kZEg9OYL-85IUNqUA.MEUCIGAX418D_QPFvxq-QLYReJE4g1m7wcZRXGk6pf1HqVQQAiEAn53oHV1zcVD3MYxeTbCWsSf_BIiSf_E_JcvZfhzlScw&sharetype=gift&token=4178b09d-f17c-4285-a18c-446dce2bdb54))
- en: '**In terms of cost, we will always see smaller and smaller specialized models
    for different applications.** In addition, much of today’s research is looking
    for a less expensive alternative to the transformer (examples are Hyena, [Monarch
    Mixer](https://together.ai/blog/monarch-mixer), [BiGS](https://arxiv.org/pdf/2212.10544.pdf),
    and [MEGA](https://arxiv.org/pdf/2209.10655.pdf)).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**从成本的角度来看，我们将会看到越来越多的小型专用模型用于不同的应用。** 此外，今天的研究大多在寻找一种比 transformer 更便宜的替代品（例如
    Hyena，[Monarch Mixer](https://together.ai/blog/monarch-mixer)，[BiGS](https://arxiv.org/pdf/2212.10544.pdf)
    和 [MEGA](https://arxiv.org/pdf/2209.10655.pdf)）。'
- en: In addition, transformer models [lack the ability to learn continuously](https://www.forbes.com/sites/robtoews/2021/06/01/what-artificial-intelligence-still-cant-do/?sh=234a649c66f6)
    (they have static parameters) and lack explainability. [Liquid neural networks](https://arxiv.org/pdf/2006.04439.pdf)
    (a model inspired by how worm brains work) aim to solve these two problems in
    particular.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，变换器模型[缺乏持续学习能力](https://www.forbes.com/sites/robtoews/2021/06/01/what-artificial-intelligence-still-cant-do/?sh=234a649c66f6)（它们具有静态参数）且缺乏解释性。[液体神经网络](https://arxiv.org/pdf/2006.04439.pdf)（一种受蠕虫大脑工作方式启发的模型）旨在特别解决这两个问题。
- en: Also, the idea of the monolithic transformer has been abandoned and more thought
    is given to the idea of having an ensemble of models working in this direction
    ([GPT-4 seems to be an ensemble of 8 models](https://pub.towardsai.net/gpt-4-8-models-in-one-the-secret-is-out-e3d16fd1eee0)).
    [Sakana AI aims at the same concept](https://www.bloomberg.com/news/articles/2023-08-17/ex-google-stability-ai-researchers-launch-ai-startup-that-takes-cues-from-fish#xj4y7vzkg)
    by drawing inspiration from the idea of collective intelligence.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，单一的变换器理念已被放弃，人们更多地考虑将多个模型组合在一起工作（[GPT-4似乎是由8个模型组成的](https://pub.towardsai.net/gpt-4-8-models-in-one-the-secret-is-out-e3d16fd1eee0)）。[Sakana
    AI也追求相同的概念](https://www.bloomberg.com/news/articles/2023-08-17/ex-google-stability-ai-researchers-launch-ai-startup-that-takes-cues-from-fish#xj4y7vzkg)，通过借鉴集体智能的理念。
- en: But so will these models replace the transformer?
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但这些模型会取代变换器吗？
- en: '**Probably not.** Or rather in some cases they will be adopted for specific
    necessities. For the time being, none of these models solve all the problems of
    the transformer, and for many applications the good old transformer is sufficient.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**可能不会。** 或者在某些情况下，它们会被采用以满足特定需求。现阶段，这些模型尚未解决变换器的所有问题，对于许多应用来说，老旧的变换器已经足够。'
- en: The transformer today has almost absolute dominance over many fields. Moreover,
    a lot of research and work has been based on this architecture and has led to
    its optimization. **So it is challenging to replace it.**
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，变换器在许多领域几乎具有绝对的主导地位。此外，许多研究和工作都基于这一架构并促成了其优化。**因此，替代它是具有挑战性的。**
- en: Ideas like liquid networks or swarm intelligence show that there is a search
    for alternatives beyond building more efficient and less expensive models. Then
    again, none of today’s research can overcome the real limitation of the transformer.
    Therefore, the transformer will be replaced but we do not yet know by what. It
    will be a new technology that will be based on new theoretical advances.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 像液体网络或群体智能这样的理念表明，人们在寻找超越构建更高效、更便宜模型的替代方案。然而，今天的研究仍未能克服变换器的真正限制。因此，变换器将被替代，但我们尚不清楚由什么来取代。它将是一项基于新理论进展的新技术。
- en: TL;DR
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TL;DR
- en: The transformer parallelization advantage that led to the inordinate growth
    of LLMs is over. Having realized that huge amounts of quality data are needed
    has dulled enthusiasm for creating ever larger models. Plus deployment costs cripple
    larger models.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器并行化的优势导致了大规模语言模型的过度增长，这种情况已经结束。意识到需要大量高质量数据使得对创建更大模型的热情减退。此外，部署成本使得更大的模型难以实现。
- en: Transformers no longer have absolute dominance over all branches of artificial
    intelligence. Recently some papers have shown that convolutional networks are
    competitive against transformers in computer vision when trained in the same way.
    their dominance in NLP is still undeniable but is based on fragile premises.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器在人工智能所有领域中的绝对主导地位已经不再。最近的一些论文表明，当以相同方式训练时，卷积网络在计算机视觉领域对变换器具有竞争力。它们在自然语言处理中的主导地位仍然不可否认，但基于脆弱的前提。
- en: Transformers could be replaced with a model capable of taking textual instructions
    and showing in-context learning. All prompt engineering techniques could be used
    the same.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器可能会被一种能够接受文本指令并在上下文中学习的模型取代。所有的提示工程技术可以保持不变。
- en: Large companies, weighed down by bureaucracy, are reluctant to innovate and
    are now focusing on application research. On the other hand, the open-source community
    is very active and a potential innovation would be adopted and pushed quickly.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型公司因官僚作风而不愿创新，现在专注于应用研究。另一方面，开源社区非常活跃，潜在的创新会被迅速采纳和推动。
- en: The real advantage of the transformer is that for years it had absolute dominance.
    So it has benefited from a lot of attention, research, and optimization. Being
    able to replace such a model is a very difficult but still necessary challenge.
    In fact, we have pretty much reached the limits of this technology and need a
    new one that will surpass it.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变压器的真正优势在于它多年拥有绝对主导地位。因此，它受到了大量关注、研究和优化。能够取代这样一个模型是非常困难但仍然必要的挑战。实际上，我们已经基本达到了这项技术的极限，需要一种能够超越它的新技术。
- en: While for the moment we do not have an alternative, research is looking for
    a model that can surpass it.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然目前我们没有替代方案，但研究正在寻找能够超越它的模型。
- en: Parting thoughts
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离别思考
- en: '![](../Images/99faa0da8e3de7239a32bfe3a7830d64.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99faa0da8e3de7239a32bfe3a7830d64.png)'
- en: Photo by [Saif71.com](https://unsplash.com/@saif71?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Saif71.com](https://unsplash.com/@saif71?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: At this time, research in AI has been incremental, especially in model architectures.
    Large companies at this time have been dedicated to finally putting these models
    into production and using them for commercial purposes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，人工智能的研究主要是增量性的，特别是在模型架构方面。大型公司现在致力于最终将这些模型投入生产并用于商业目的。
- en: is true that after a year of big announcements, the institutions have also moved
    and new laws are being prepared that will regulate artificial intelligence. These
    laws will also partly define the directions of new research. If these laws are
    too rigid they will throttle innovation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，在经历了一年的重大公告后，相关机构也开始行动，新法律正在准备中，将对人工智能进行规范。这些法律也将在一定程度上定义新的研究方向。如果这些法律过于僵化，将会抑制创新。
- en: “There are definitely large tech companies that would rather not have to try
    to compete with open source, so they’re creating fear of AI leading to human extinction.
    It’s been a weapon for lobbyists to argue for legislation that would be very damaging
    to the open-source community.” — Andrew Ng ([source](https://finance.yahoo.com/news/google-brain-cofounder-says-big-113049941.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAACCPVKuuJ47EM3W7q-arne9h3S_l8zr9D-HvVqTp0_5A4D4Co8SiIWFoZHyPteR9rAcLij4bq2MmZoKJ-dGXBNuJUmHToa-wshhX8xBxXysi4fjya2rsPgiCaatOi_8ViZkSPzdS0x-wPhW6AzvMIwlVzsiorSVkVB_XWoCU98xc))
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “确实有一些大型科技公司宁愿不与开源竞争，所以他们制造了对人工智能导致人类灭绝的恐惧。这已成为游说者用来推动对开源社区非常有害的立法的武器。” — Andrew
    Ng ([source](https://finance.yahoo.com/news/google-brain-cofounder-says-big-113049941.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAACCPVKuuJ47EM3W7q-arne9h3S_l8zr9D-HvVqTp0_5A4D4Co8SiIWFoZHyPteR9rAcLij4bq2MmZoKJ-dGXBNuJUmHToa-wshhX8xBxXysi4fjya2rsPgiCaatOi_8ViZkSPzdS0x-wPhW6AzvMIwlVzsiorSVkVB_XWoCU98xc))
- en: 'Yann LeCun also feels the same way. Right now the big companies are afraid
    of open-source:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Yann LeCun 也持相同看法。目前，大公司对开源感到恐惧：
- en: '“If your fear-mongering campaigns succeed, they will *inevitably* result in
    what you and I would identify as a catastrophe: a small number of companies will
    control AI.” — Yann LeCun ([source](https://www.telegraph.co.uk/business/2023/10/30/big-tech-stoking-fears-over-ai-warn-scientists/))'
  id: totrans-169
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “如果你的恐吓活动成功，它们将*不可避免地*导致你和我认为的灾难：少数公司将控制人工智能。” — Yann LeCun ([source](https://www.telegraph.co.uk/business/2023/10/30/big-tech-stoking-fears-over-ai-warn-scientists/))
- en: In any case, as we have seen no technology remains dominant forever, and the
    transformer is beginning to show the limits of its age. it is exciting to think
    about what will replace it, what theoretical advances, what elegant solutions
    it will implement, and what incredible capabilities it will have.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，正如我们所见，没有任何技术会永远主导，而变压器也开始显示出其时代的局限性。考虑到将来会取代它的技术、理论进展、优雅的解决方案以及它将拥有的惊人能力，令人兴奋。
- en: What do you think will replace the transformer? Have you tried and transformer
    alternative? Let me know in the comments
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你认为什么会取代变压器？你尝试过变压器的替代品吗？在评论中告诉我。
- en: 'If you have found this interesting:'
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得这些内容有趣：
- en: '*You can look for my other articles, and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***
    *Check* [***this repository***](https://github.com/SalvatoreRa/ML-news-of-the-week)
    *containing weekly updated ML & AI news.* ***I am open to collaborations and projects***
    *and you can reach me on LinkedIn.*'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以查看我的其他文章，也可以在*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***上联系我。***
    *查看* [***这个库***](https://github.com/SalvatoreRa/ML-news-of-the-week) *，其中包含每周更新的机器学习和人工智能新闻。*
    ***我对合作和项目持开放态度*** *，你可以在 LinkedIn 上联系我。*'
- en: '*Here is the link to my GitHub repository, where I am collecting code and many
    resources related to machine learning, artificial intelligence, and more.*'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*这里是我 GitHub 库的链接，我在这里收集了与机器学习、人工智能等相关的代码和许多资源。*'
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----297e6f14e189--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----297e6f14e189--------------------------------)
    [## GitHub - SalvatoreRa/tutorial：机器学习、人工智能、数据科学教程…'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习、人工智能、数据科学教程，包括数学解释和可重复使用的代码（用 Python 编写…
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----297e6f14e189--------------------------------)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----297e6f14e189--------------------------------)'
- en: '*or you may be interested in one of my recent articles:*'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*或者你可能对我的一篇近期文章感兴趣：*'
- en: '[](/teaching-is-hard-how-to-train-small-models-and-outperforming-large-counterparts-f131f9d463e1?source=post_page-----297e6f14e189--------------------------------)
    [## Teaching is Hard: How to Train Small Models and Outperforming Large Counterparts'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/teaching-is-hard-how-to-train-small-models-and-outperforming-large-counterparts-f131f9d463e1?source=post_page-----297e6f14e189--------------------------------)
    [## 教学是困难的：如何训练小型模型并超越大型模型'
- en: Distilling the knowledge of a large model is complex but a new method shows
    incredible performances
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取大型模型知识的过程复杂，但一种新方法展现了惊人的表现
- en: 'towardsdatascience.com](/teaching-is-hard-how-to-train-small-models-and-outperforming-large-counterparts-f131f9d463e1?source=post_page-----297e6f14e189--------------------------------)
    [](https://levelup.gitconnected.com/neural-ensemble-whats-better-than-a-neural-network-a-group-of-them-0c9e156fca15?source=post_page-----297e6f14e189--------------------------------)
    [## Neural Ensemble: what’s Better than a Neural Network? A group of them'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/teaching-is-hard-how-to-train-small-models-and-outperforming-large-counterparts-f131f9d463e1?source=post_page-----297e6f14e189--------------------------------)
    [](https://levelup.gitconnected.com/neural-ensemble-whats-better-than-a-neural-network-a-group-of-them-0c9e156fca15?source=post_page-----297e6f14e189--------------------------------)
    [## 神经网络集成：什么比神经网络更好？一组神经网络'
- en: 'Neural ensemble: how to combine different neural networks in a powerful model'
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络集成：如何将不同的神经网络结合成一个强大的模型
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/neural-ensemble-whats-better-than-a-neural-network-a-group-of-them-0c9e156fca15?source=post_page-----297e6f14e189--------------------------------)
    [](https://pub.towardsai.net/speak-only-about-what-you-have-read-can-llms-generalize-beyond-their-pretraining-data-041704e96cd5?source=post_page-----297e6f14e189--------------------------------)
    [## Speak Only About What You Have Read: Can LLMs Generalize Beyond Their Pretraining
    Data?'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[级别提升](https://levelup.gitconnected.com/neural-ensemble-whats-better-than-a-neural-network-a-group-of-them-0c9e156fca15?source=post_page-----297e6f14e189--------------------------------)
    [](https://pub.towardsai.net/speak-only-about-what-you-have-read-can-llms-generalize-beyond-their-pretraining-data-041704e96cd5?source=post_page-----297e6f14e189--------------------------------)
    [## 仅谈论你所读过的内容：LLMs 能否超越其预训练数据进行泛化？'
- en: Unveiling the Limits and Wonders of In-Context Learning in Large Language Models
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 揭示大型语言模型中上下文学习的局限性与奇迹
- en: 'pub.towardsai.net](https://pub.towardsai.net/speak-only-about-what-you-have-read-can-llms-generalize-beyond-their-pretraining-data-041704e96cd5?source=post_page-----297e6f14e189--------------------------------)
    [](https://salvatore-raieli.medium.com/ml-news-week-6-12-november-9878eb0a7005?source=post_page-----297e6f14e189--------------------------------)
    [## ML news: Week 6–12 November'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/speak-only-about-what-you-have-read-can-llms-generalize-beyond-their-pretraining-data-041704e96cd5?source=post_page-----297e6f14e189--------------------------------)
    [](https://salvatore-raieli.medium.com/ml-news-week-6-12-november-9878eb0a7005?source=post_page-----297e6f14e189--------------------------------)
    [## 机器学习新闻：11月6日至12日'
- en: OpenAI dev, TopicGPT, new chips, and much more
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenAI 开发，TopicGPT，新芯片等
- en: salvatore-raieli.medium.com](https://salvatore-raieli.medium.com/ml-news-week-6-12-november-9878eb0a7005?source=post_page-----297e6f14e189--------------------------------)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: salvatore-raieli.medium.com](https://salvatore-raieli.medium.com/ml-news-week-6-12-november-9878eb0a7005?source=post_page-----297e6f14e189--------------------------------)
- en: Reference
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Here is the list of the principal references I consulted to write this article,
    only the first name for an article is cited.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我撰写本文时参考的主要文献列表，仅引用了每篇文章的首名。
- en: Vaswani, 2017, Attention Is All You Need, [link](https://arxiv.org/abs/1706.03762)
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vaswani, 2017, 《注意力即是你所需要的一切》，[link](https://arxiv.org/abs/1706.03762)
- en: Huang, 2016, Densely Connected Convolutional Networks, [link](https://arxiv.org/abs/1608.06993)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Huang, 2016, 《密集连接卷积网络》，[link](https://arxiv.org/abs/1608.06993)
- en: Zhao, 2023, A Survey of Large Language Models, [link](https://arxiv.org/abs/2303.18223)
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhao, 2023, 《大型语言模型的综述》，[link](https://arxiv.org/abs/2303.18223)
- en: Smith, 2023, ConvNets Match Vision Transformers at Scale, [link](https://arxiv.org/abs/2310.16764)
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Smith, 2023, 《ConvNets在规模上匹配视觉变换器》，[link](https://arxiv.org/abs/2310.16764)
- en: 'Goldblum, 2023, Battle of the Backbones: A Large-Scale Comparison of Pretrained
    Models across Computer Vision Tasks, [link](https://arxiv.org/abs/2310.19909)'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Goldblum, 2023, 《骨干的战斗：预训练模型在计算机视觉任务中的大规模比较》，[link](https://arxiv.org/abs/2310.19909)
- en: Wang, 2023, Pretraining Without Attention, [link](https://arxiv.org/pdf/2212.10544.pdf)
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wang, 2023, 《无注意力的预训练》，[link](https://arxiv.org/pdf/2212.10544.pdf)
- en: 'Ma, 2023, Mega: Moving Average Equipped Gated Attention, [link](https://arxiv.org/pdf/2209.10655.pdf)'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ma, 2023, 《Mega：配备移动平均的门控注意力》，[link](https://arxiv.org/pdf/2209.10655.pdf)
- en: Hasani, 2020, Liquid Time-constant Networks, [link](https://arxiv.org/pdf/2006.04439.pdf)
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hasani, 2020, 《液态时间常数网络》，[link](https://arxiv.org/pdf/2006.04439.pdf)
- en: Yadlowsky, 2023, Pretraining Data Mixtures Enable Narrow Model Selection Capabilities
    in Transformer Models, [link](https://arxiv.org/abs/2311.00871)
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yadlowsky, 2023, 《预训练数据混合使变换器模型具备狭窄的模型选择能力》，[link](https://arxiv.org/abs/2311.00871)
- en: Kaplan, 2020, Scaling Laws for Neural Language Models, [link](https://arxiv.org/abs/2001.08361)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kaplan, 2020, 《神经语言模型的扩展规律》，[link](https://arxiv.org/abs/2001.08361)
- en: Hoffman, 2022, Training Compute-Optimal Large Language Models, [link](https://arxiv.org/abs/2203.15556)
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hoffman, 2022, 《训练计算最优的大型语言模型》，[link](https://arxiv.org/abs/2203.15556)
- en: Simonyan, 2014, Very Deep Convolutional Networks for Large-Scale Image Recognition,
    [link](https://arxiv.org/abs/1409.1556)
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Simonyan, 2014, 《用于大规模图像识别的非常深的卷积网络》，[link](https://arxiv.org/abs/1409.1556)
- en: 'Khan, 2022, Transformers in Vision: A Survey, [link](https://arxiv.org/pdf/2101.01169.pdf)'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Khan, 2022, 《视觉中的变换器：综述》，[link](https://arxiv.org/pdf/2101.01169.pdf)
- en: Wei, 2022, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,
    [link](https://arxiv.org/abs/2201.11903)
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wei, 2022, 《链式思考提示在大型语言模型中引发推理》，[link](https://arxiv.org/abs/2201.11903)
- en: 'Touvron, 2023, LLaMA: Open and Efficient Foundation Language Models, [link](https://arxiv.org/abs/2302.13971)'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Touvron, 2023, 《LLaMA：开放高效的基础语言模型》，[link](https://arxiv.org/abs/2302.13971)
