- en: A Requiem for the Transformer?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-requiem-for-the-transformer-297e6f14e189](https://towardsdatascience.com/a-requiem-for-the-transformer-297e6f14e189)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '|PERSPECTIVES| AI| LARGE LANGUAGE MODELS|'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Will be the transformer the model leading us to artificial general intelligence?
    Or will be replaced?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----297e6f14e189--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----297e6f14e189--------------------------------)[](https://towardsdatascience.com/?source=post_page-----297e6f14e189--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----297e6f14e189--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----297e6f14e189--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----297e6f14e189--------------------------------)
    ·18 min read·Dec 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab049cc38b8a0ad788f2871ab46d854d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Stefany Andrade](https://unsplash.com/@stefany_andrade?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The transformer has dominated the world of artificial intelligence for six years,
    achieving state-of-the-art in all subdomains of artificial intelligence. From
    [natural language processing (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing)
    to computer vision to sound and graphs, there are dedicated transformers with
    excellent performance.
  prefs: []
  type: TYPE_NORMAL
- en: How much longer will this domain last?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the transformer really the best architecture out there?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will it be replaced in the near future?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the threats to its dominance?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This article attempts to answer these questions. Starting with why the transformer
    has been so successful and what elements have allowed it to establish itself in
    so many different domains, we will analyze whether it still has unchallenged dominance,
    what elements threaten its supremacy, and whether there are potential competitors.
  prefs: []
  type: TYPE_NORMAL
- en: A brief history of an empire
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/7bbb943855d46e605cb892ba837e1882.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: “All empires become arrogant. It is their nature.” ― Edward Rutherfurd
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Empires inevitably fall, and when they do, history judges them for the legacies
    they leave behind. — Noah Feldman
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[“Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)” Is the
    basis of artificial intelligence as we know it today. The roots of generative
    AI and its success are in a single seed: **the transformer.**'
  prefs: []
  type: TYPE_NORMAL
- en: The [transformer](https://en.wikipedia.org/wiki/Transformer_(machine-learning_model))
    was initially designed to solve the lack of parallelization of [RNNs](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    and to be able to model long-distance relationships among words in a sequence.
    The idea was to provide the model with a system to discriminate which parts of
    the sequence were most important (where to pay [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))).
    This was all designed to improve machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: These elements, though, allowed the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    to understand the text better. In addition, parallelization allowed the model
    to scale both as a size and as on larger datasets. The rise of [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit)
    further showed the benefits of a parallelizable architecture like the Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Transformer thus emerged as the new king of AI. An empire grew in a very
    short time. In fact, today, all the popular models are Transformer: [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT),
    [Bard](https://en.wikipedia.org/wiki/Bard_(chatbot)), [GitHub Copilot](https://en.wikipedia.org/wiki/GitHub_Copilot),
    [Mistral](https://arxiv.org/abs/2310.06825), [LLaMA](https://en.wikipedia.org/wiki/LLaMA),
    Bing Chat, [stable diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion),
    [DALL-E](https://en.wikipedia.org/wiki/DALL-E), Midjourney, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72edabb76414dca7b07a692dffd1cb2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The trends of the cumulative numbers of arXiv papers that contain the keyphrases
    “language model” and “large language model”. image source: [here](https://arxiv.org/pdf/2303.18223.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: This is because Transformer was quickly adapted to so many tasks beyond language.
  prefs: []
  type: TYPE_NORMAL
- en: Even the vastest empires fall at some point; what is happening to the Transformer
    dominion?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The giant with feet of clay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/740eadf41176a62f613e0d5089cf5bd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://huggingface.co/blog/large-language-models)'
  prefs: []
  type: TYPE_NORMAL
- en: When the transformer was introduced, its performance shocked the world and gave
    rise to a parameter race. For a time we saw a kind of growth in models to the
    extent that it was called the “*new Moore law of AI.*” The growth continued until
    [Megatron](https://huggingface.co/docs/accelerate/usage_guides/megatron_lm) (530
    B) and [Google PaLM](https://ai.google/discover/palm2/) (540 B) were released
    in 2022\. **And yet we still haven’t seen the trillion parameters.**
  prefs: []
  type: TYPE_NORMAL
- en: When deep convolutional networks showed their efficiency (VGG models), [ConvNets](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    went from 16 layers of [VGG16](https://arxiv.org/abs/1409.1556) to 201 layers
    of [DenseNet201](https://arxiv.org/abs/1608.06993) in a short time. Results and
    performance aside, it is a testament to the interest of the community. This pattern
    of horizontal and vertical growth (and incremental changes to the base model)
    stopped in 2021 when the community became convinced that [Vision Transformers
    (ViTs)](https://huggingface.co/docs/transformers/model_doc/vit) were superior
    to ConvNets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ca3f2c8609e3c259202ff0ceffcee10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Vision Transformer. image source: [here](https://arxiv.org/pdf/2101.01169.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Why did the growth of transformers stop? Were they replaced as well?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**No, but some of the premises that led to transformer growth have disappeared.**'
  prefs: []
  type: TYPE_NORMAL
- en: This growth was motivated by the so-called [power law](https://en.wikipedia.org/wiki/Power_law).
    [According to OpenAI](https://arxiv.org/abs/2001.08361), by increasing the number
    of parameters, properties emerge abruptly. So scaling the models leads the model
    to develop properties that would not be observable below a certain scale. Too
    bad, that for Stanford researchers these properties are a mirage derived from
    a bias.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----297e6f14e189--------------------------------)
    [## Emergent Abilities in AI: Are We Chasing a Myth?'
  prefs: []
  type: TYPE_NORMAL
- en: Changing Perspective on Large Language Models emerging properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----297e6f14e189--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up a model means spending much more. More parameters, more computation,
    more infrastructure, more electrical consumption (and more carbon emissions).
    **Is it worth it?**
  prefs: []
  type: TYPE_NORMAL
- en: Actually, [DeepMind with Chinchilla](https://arxiv.org/abs/2203.15556) said
    that performance increases not only with the number of parameters but also with
    the amount of data. So if you want a model with billions of parameters you have
    to have enough tokens to train it. **Too bad, that we humans don’t produce enough
    to train a model from a trillion parameters.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57aa796415fd7a367e260169c7427bec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'human data are a finite resource. image source: [here](https://arxiv.org/pdf/2211.04325.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, it is not just the quantity of text that impacts the performance
    of a model. **it is the quality of the text.** This is also a sore point because
    collecting huge amounts of text without filtering is not a good idea (aka downloading
    without criteria from the Internet).
  prefs: []
  type: TYPE_NORMAL
- en: '[](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----297e6f14e189--------------------------------)
    [## Say Once! Repeating Words Is Not Helping AI'
  prefs: []
  type: TYPE_NORMAL
- en: How and why is repeating tokens harming LLMs? Why is this a problem?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----297e6f14e189--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Also, generating text using artificial intelligence is not a good idea. In theory,
    one could use an LLM and ask it to produce text indefinitely. The problem is that
    the model trained with this text can only mimic another LLM, and certainly not
    outperform it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, our key takeaway is that model imitation is not a free lunch: there
    exists a capabilities gap between today’s open-source LMs and their closed-source
    counterparts that cannot be closed by cheaply fine-tuning on imitation data. ([source](https://arxiv.org/pdf/2305.15717.pdf))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5?source=post_page-----297e6f14e189--------------------------------)
    [## The imitation game: Taming the gap between open source and proprietary models'
  prefs: []
  type: TYPE_NORMAL
- en: Can imitation models reach the performance of proprietary models like ChatGPT?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5?source=post_page-----297e6f14e189--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**An additional point is that these huge models are also problematic for deployment.**
    Smaller models have good performance, especially for some tasks. One can distill
    and get much smaller models specialized for a specific task.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Take home message: the huge transformer paradigm is in crisis. The idea that
    every year we will see a bigger and bigger model is over.**'
  prefs: []
  type: TYPE_NORMAL
- en: After all, the issue is practicality (and cost). AI can cost a lot of money
    once it goes into production. For example, [Microsoft is reportedly losing huge
    amounts of money](https://www.techradar.com/pro/microsoft-is-reportedly-losing-huge-amounts-of-money-on-github-copilot)
    on GitHub Copilot ($20 per user per month). According to one report, [ChatGPT
    costs $700,000 per day](https://technext24.com/2023/08/14/chatgpt-costs-700000-daily-openai/),
    and investors may no longer cover the cost if ChatGPT does not become profitable.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore we can expect companies more interested in developing smaller models
    with a specific task and business in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, the transformer no longer grows, but is it still the best architecture
    in the game?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: well, let’s talk about that in the next section…
  prefs: []
  type: TYPE_NORMAL
- en: Convolution is still on fire
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/55c3ef5870e540e3c2327c5ae1a51ef1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ricardo Gomez Angel](https://unsplash.com/@rgaleriacom?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: First, why was the transformer successful everywhere?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In its initial description, the transformer brought together three basic concepts:
    starting with a position-aware representation of the sequence ([embedding](https://en.wikipedia.org/wiki/Embedding)
    + positional encoding), relating the elements of the sequence (self-attention),
    and constructing a hierarchical representation (layer stacking).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the article [*Attention is All You Need*](https://arxiv.org/abs/1706.03762)
    was published it was based on a decade of research in NLP and put together the
    best of what had been published previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[word embedding](https://arxiv.org/abs/1301.3781) was revolutionary in 2013
    in being able to transform words into vector representations. In addition, operations
    on embedding had logical and grammatical meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-attention was an improvement on [the revolutionary idea that not all elements](https://arxiv.org/abs/1409.0473)
    of the sequence are important. Plus solving the long-standing problem of recurring
    neural networks and their [vanishing gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [hierarchical representation](https://distill.pub/2020/circuits/zoom-in/),
    on the other hand, came from twenty years of convolutional neural networks where
    we realized that by stacking layers the model learns an increasingly complex representation
    of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/18916ea2d147bd0b6dc0085e155c6035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The skeleton of its majesty, the transformer. Image source: [here](https://arxiv.org/abs/1706.03762)'
  prefs: []
  type: TYPE_NORMAL
- en: These elements made him successful in the NLP field, but at the same time, they
    were the key to winning in other fields as well. First, the fact that it had a
    very weak inductive bias made it adaptable to almost any type of data. Second,
    hierarchical representation and connecting elements of a sequence have applications
    far beyond NLP.
  prefs: []
  type: TYPE_NORMAL
- en: '**A story of success, except that the transformer has remained the same as
    it was in 2017 and is beginning to age badly.**'
  prefs: []
  type: TYPE_NORMAL
- en: The beating heart of transformers is ultimately self-attention. But it is a
    heart that pumps too much blood. In fact, its quadratic computational cost is
    huge.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, several groups have tried to try to find a linear substitution to
    attention. However, all of these variants have been shown to have inferior performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**And what seemed a good substitute?** Nothing less than the old convolution.
    As they showed in Hyena, by adapting convolution a little bit, you get a good
    model with transformer-like performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----297e6f14e189--------------------------------)
    [## Welcome Back 80s: Transformers Could Be Blown Away by Convolution'
  prefs: []
  type: TYPE_NORMAL
- en: The Hyena model shows how convolution could be faster than self-attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----297e6f14e189--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: This is ironic because since 2021 [Vision Transformers](https://en.wikipedia.org/wiki/Vision_transformer)
    (ViTs) have been believed to be superior to ConvNets in computer vision. This
    seemed to be the end of the uncodified dominance of convolutional networks ([ConvNets](https://en.wikipedia.org/wiki/Convolutional_neural_network))
    in what until recently was their realm. **But instead?**
  prefs: []
  type: TYPE_NORMAL
- en: It seems that the ConvNets have had their revenge. Astonishing, like thinking
    that dinosaurs would return to dominance over continents by driving out mammals.
    In truth, a [recent article published by DeepMind](https://arxiv.org/abs/2310.16764)
    basically states that the comparison between ViTs and ConvNets was not fair. By
    providing the same compute budget to ConvNets these have similar performance to
    ViTs on ImageNet.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/have-convolutional-networks-become-obsolete-245969f6b9d9?source=post_page-----297e6f14e189--------------------------------)
    [## Have convolutional networks become obsolete'
  prefs: []
  type: TYPE_NORMAL
- en: Vision transformers seem to have replaced convolutional networks, but are they
    really better?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/have-convolutional-networks-become-obsolete-245969f6b9d9?source=post_page-----297e6f14e189--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[Another article](https://arxiv.org/abs/2310.19909) seems to go in the same
    direction, convolutional networks seem to be competitive with transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: The same winners also win at smaller scales. Among smaller backbones, ConvNeXt-Tiny
    and SwinV2-Tiny emerge victorious, followed by DINO ViT-Small. ([source](https://arxiv.org/abs/2310.19909))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/7bf17cf03bb3da5aebdc66385982f6fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2310.19909)'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three primary factors that influence the performance of such a model:
    its architecture, the pretraining algorithm, and the pretraining dataset. ([source](https://arxiv.org/abs/2310.19909))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Now if the pretraining algorithm and pretraining dataset are the same only
    the model architecture remains. **However, all things being equal, the alleged
    superiority of ViTs does not seem to emerge.** So much so that it seems like an
    admission of defeat what the DeepMind authors claim:'
  prefs: []
  type: TYPE_NORMAL
- en: Although the success of ViTs in computer vision is extremely impressive, in
    our view there is no strong evidence to suggest that pre-trained ViTs outperform
    pre-trained ConvNets when evaluated fairly. ([source](https://arxiv.org/abs/2310.16764))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Ouch. So we can say that ViTs are not superior to convolutional networks
    at least in computer vision. Is it?**'
  prefs: []
  type: TYPE_NORMAL
- en: We note however that ViTs may have practical advantages in specific contexts,
    such as the ability to use similar model components across multiple modalities.
    ([source](https://arxiv.org/abs/2310.16764))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The authors point out that they could potentially still be superior because
    useful when we are interested in multimodal models. Considering that features
    can also be extracted from a convolutional network, it is certainly more convenient
    to use the same model across multiple modalities.
  prefs: []
  type: TYPE_NORMAL
- en: However this is a very important point, empirical data show that at least in
    computer vision the transformer is not superior to other architectures. **This
    leads us to wonder whether its dominance will soon be questioned in other fields
    of artificial intelligence as well. For example, what is happening in the core
    field of the transformer? It is still the best model in natural language processing?**
  prefs: []
  type: TYPE_NORMAL
- en: The text Dominion has a fragile basis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/1229a84bd59851556c1e6cf876b3d236.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sigmund](https://unsplash.com/@sigmund?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Short answer: **yes, but its supremacy could end.** Let’s start with why it
    has been so successful in NLP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial advantage of the transformer on RNNs is that easily parallelized.
    This led to the initial euphoria and rush to the parameter. In the process, we
    realized what allowed the Transformer to win in NLP: **in-context learning**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In-context learning is a very powerful concept: all it takes is a few examples
    and the model is capable of mapping a relationship between input and output. All
    this without even updating a single parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/all-you-need-to-know-about-in-context-learning-55bde1180610?source=post_page-----297e6f14e189--------------------------------)
    [## All You Need to Know about In-Context Learning'
  prefs: []
  type: TYPE_NORMAL
- en: What is and how does it work what makes Large Language Models so powerful
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/all-you-need-to-know-about-in-context-learning-55bde1180610?source=post_page-----297e6f14e189--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Basically, this was an unanticipated (and not yet 100 % understood) effect of
    self-attention. [According to Anthropic](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html),
    there are induction heads that practically connect different parts of the model
    and allow this mapping.
  prefs: []
  type: TYPE_NORMAL
- en: '**This miracle is the basis of the supposed reasoning capabilities of the models**.
    In addition, the fact that one could experiment so much with the prompt allowed
    for incredible results.'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, without having to train the model again, [prompting techniques](https://en.wikipedia.org/wiki/Prompt_engineering)
    could be created to improve the model’s capabilities in inference. [**Chain of
    thought**](https://arxiv.org/abs/2201.11903) is the best example of this approach.
    Using this ploy, an LLM is able to solve problems that require reasoning (math
    problems, coding problems, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/320d30d99027d11abcbc04beca7b57ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2201.11903)'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, one must take into account that:'
  prefs: []
  type: TYPE_NORMAL
- en: However, this multi-step generation process does not inherently imply that LLMs
    possess strong reasoning capabilities, as they may merely emulate the superficial
    behavior of human reasoning without genuinely comprehending the underlying logic
    and rules necessary for precise reasoning. ([source](https://arxiv.org/abs/2310.20689v1))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Translated, we have created a parrot that has seen the entire human knowledge
    and can connect the question in the prompt with what it has seen during the training.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this advantage extremely precarious?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Because the parrot does not have to be a transformer.** We need any model
    that takes natural language instructions as input and can do in-context learning,
    after which we can use the whole arsenal of prompt engineering techniques as if
    it were a transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: Ok, so if we do not need necessarily the transformer, where is our new “stochastic
    parrot”?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bureaucracy slows down innovation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/6a6a4dca95a438143c96e5de5ba8f559.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nam Anh](https://unsplash.com/@bepnamanh?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The main reason is that research in industry is currently focused on bringing
    the transformer (despite its flaws) into production. Also, it is risky to put
    a better architecture into production but whose behavior we know less about.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dig more about it…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'First, Google, META, Amazon, and other big tech have huge amounts of resources.
    Such large companies, however, are burdened by an elephantine internal bureaucracy:'
  prefs: []
  type: TYPE_NORMAL
- en: Google is a “once-great company” that has “slowly ceased to function” thanks
    to its bureaucratic “maze.” ([source](https://fortune.com/2023/02/16/alphabet-google-former-employee-praveen-seshadri-essay-criticizes-bureaucratic-maze/))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This increase in bureaucracy, results in reduced productivity and an overall
    slowdown. In order to implement a small change, one must have the approval of
    increasingly long chains of command and follow [increasingly complex protocols](https://www.theinformation.com/articles/aws-new-ceo-faces-a-fresh-challenge-bureaucracy).
    In short, it seems that big tech has the same problem that has plagued empires.
  prefs: []
  type: TYPE_NORMAL
- en: 'This obviously impacts innovation as well:'
  prefs: []
  type: TYPE_NORMAL
- en: “If I had to summarize it, I would say that the signal to noise ratio is what
    wore me down. The innovation challenges … will only get worse as the risk tolerance
    will go down.” Noam Bardin, former Google executive. ([source](https://appleinsider.com/articles/21/06/21/google-is-risk-averse-has-paralyzing-bureaucracy-executives-say))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Of course, there are also well-founded reasons for companies like Google or
    Microsoft to be more cautious in their choices. For example, [Google lost billions
    in capitalization](https://fortune.com/2023/02/08/google-bard-ai-mistake-ad-stock-price-market-cap/)
    when Bard incorrectly answered a question about the James Webb Space Telescope.
  prefs: []
  type: TYPE_NORMAL
- en: '**These reputational risks have turned into a giant brake on innovation**.
    A technology is adopted only when it is mature and not risky.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The best example is Apple.** The company adopts a technology only when it
    is mature and considers it profitable. In general, it has stopped being innovative
    in recent years (while still maintaining record profits). So far, it has kept
    out of the generative AI race because it does not consider it mature.'
  prefs: []
  type: TYPE_NORMAL
- en: Apple is known as a “fast follower”; it likes to wait until new technologies
    have matured, then jump in with its own Apple-flavored version. ([source](https://www.fastcompany.com/90867819/apples-silence-on-generative-ai-grows-louder))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shouldn’t this resistance to innovate then favor the transformer?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Yes, but we are forgetting open-source**. It is not only Big Tech doing research,
    there are many groups of researchers investigating AI. They may not have the resources
    of the FAANGs, but together independent research is a formidable force.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google itself admits this.** In fact, Mountain View is less afraid of Microsoft
    or OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: “The uncomfortable truth is, we aren’t positioned to win this arms race and
    neither is OpenAI. While we’ve been squabbling, a third faction has been quietly
    eating our lunch,” Google’s leaked document, [source](https://www.theguardian.com/technology/2023/may/05/google-engineer-open-source-technology-ai-openai-chatgpt)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**This third faction is precisely open-source.** As soon as [LLaMA](https://ai.meta.com/llama/)
    was released it immediately received improved variants with instruction tuning,
    quantization, and [Reinforced learning from human feedback](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)
    (RLHF). Mistral 7B just came out and two groups extended the context length (first
    to [32K](https://huggingface.co/amazon/MistralLite) and then up to [128K](https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k)).'
  prefs: []
  type: TYPE_NORMAL
- en: Open-source models quickly become customizable and cheap, and gain support from
    a huge and active community. The open-source community immediately adopts each
    breakthrough and quickly improves it. Large companies waste time on internal bureaucracy
    and are restrained from adopting new technology because of the risk of reputational
    damage, risking falling behind.
  prefs: []
  type: TYPE_NORMAL
- en: '**This outstanding community is fertile ground for the transformer’s successor.
    If a model showed to overcome the transformer’s limitations it could count on
    the force of a tsunami.**'
  prefs: []
  type: TYPE_NORMAL
- en: But let’s look at one last point as to why despite everything the transformer
    has not yet been replaced
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Theseus’ ship
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b866b91601b456d4007ca388e842eb84.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Katherine McCormack](https://unsplash.com/@kathymack?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'A question was raised by ancient philosophers: After several centuries of maintenance,
    if each individual part of the Ship of Theseus was replaced, one at a time, was
    it still the same ship? — [source](https://en.wikipedia.org/wiki/Ship_of_Theseus)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Has the transformer changed while remaining the same the transformer?** A
    brief search of the literature clearly shows that there are hundreds of variants
    of the transformer today (if one takes into account all the variations of positional
    encoding, attention, and so on). **So LLMs are not exactly the same model we have
    seen so far.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5586a97ef672d3fc5a0cd316521b9d88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Categorization of Transformer variants. image source: [here](https://www.sciencedirect.com/science/article/pii/S2666651022000146).
    Creative Commons, license: [here](https://creativecommons.org/licenses/by/4.0/)'
  prefs: []
  type: TYPE_NORMAL
- en: Right now we are in an [incremental phase of research](https://medium.com/mlearning-ai/the-decline-of-disruptive-science-730cc3fe28b1),
    where modifications of the same model continue to be proposed to try to overcome
    the limitations of a technology that is now old.
  prefs: []
  type: TYPE_NORMAL
- en: '**In a sense, we have reached the limits of the transformer.** There is no
    point in scaling the model because it does not bring benefits nor do we have enough
    tokens. Second, the transformer is computationally expensive, and **if in the
    past performance mattered today applications matter**. Third, COT and other techniques
    are patches to the real limit of the transformer: the model [is not really capable
    of understanding and generalizing](https://arxiv.org/abs/2311.00871). Self-attention
    allows the model to take advantage of the huge body of knowledge it has learned
    during training, but it still remains a stochastic parrot.'
  prefs: []
  type: TYPE_NORMAL
- en: The transformer is a way to capture interaction very quickly all at once between
    different parts of any input. It’s a general method that captures interactions
    between pieces in a sentence, or the notes in music, or pixels in an image, or
    parts of a protein. It can be purposed for any task.” — Ashish Vaswani, author
    of the transformer paper ([source](https://www.ft.com/content/37bb01af-ee46-4483-982f-ef3921436a50?accessToken=zwAGASoZXEbgkc83uwGv7kZEg9OYL-85IUNqUA.MEUCIGAX418D_QPFvxq-QLYReJE4g1m7wcZRXGk6pf1HqVQQAiEAn53oHV1zcVD3MYxeTbCWsSf_BIiSf_E_JcvZfhzlScw&sharetype=gift&token=4178b09d-f17c-4285-a18c-446dce2bdb54))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**In terms of cost, we will always see smaller and smaller specialized models
    for different applications.** In addition, much of today’s research is looking
    for a less expensive alternative to the transformer (examples are Hyena, [Monarch
    Mixer](https://together.ai/blog/monarch-mixer), [BiGS](https://arxiv.org/pdf/2212.10544.pdf),
    and [MEGA](https://arxiv.org/pdf/2209.10655.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, transformer models [lack the ability to learn continuously](https://www.forbes.com/sites/robtoews/2021/06/01/what-artificial-intelligence-still-cant-do/?sh=234a649c66f6)
    (they have static parameters) and lack explainability. [Liquid neural networks](https://arxiv.org/pdf/2006.04439.pdf)
    (a model inspired by how worm brains work) aim to solve these two problems in
    particular.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the idea of the monolithic transformer has been abandoned and more thought
    is given to the idea of having an ensemble of models working in this direction
    ([GPT-4 seems to be an ensemble of 8 models](https://pub.towardsai.net/gpt-4-8-models-in-one-the-secret-is-out-e3d16fd1eee0)).
    [Sakana AI aims at the same concept](https://www.bloomberg.com/news/articles/2023-08-17/ex-google-stability-ai-researchers-launch-ai-startup-that-takes-cues-from-fish#xj4y7vzkg)
    by drawing inspiration from the idea of collective intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: But so will these models replace the transformer?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Probably not.** Or rather in some cases they will be adopted for specific
    necessities. For the time being, none of these models solve all the problems of
    the transformer, and for many applications the good old transformer is sufficient.'
  prefs: []
  type: TYPE_NORMAL
- en: The transformer today has almost absolute dominance over many fields. Moreover,
    a lot of research and work has been based on this architecture and has led to
    its optimization. **So it is challenging to replace it.**
  prefs: []
  type: TYPE_NORMAL
- en: Ideas like liquid networks or swarm intelligence show that there is a search
    for alternatives beyond building more efficient and less expensive models. Then
    again, none of today’s research can overcome the real limitation of the transformer.
    Therefore, the transformer will be replaced but we do not yet know by what. It
    will be a new technology that will be based on new theoretical advances.
  prefs: []
  type: TYPE_NORMAL
- en: TL;DR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The transformer parallelization advantage that led to the inordinate growth
    of LLMs is over. Having realized that huge amounts of quality data are needed
    has dulled enthusiasm for creating ever larger models. Plus deployment costs cripple
    larger models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers no longer have absolute dominance over all branches of artificial
    intelligence. Recently some papers have shown that convolutional networks are
    competitive against transformers in computer vision when trained in the same way.
    their dominance in NLP is still undeniable but is based on fragile premises.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers could be replaced with a model capable of taking textual instructions
    and showing in-context learning. All prompt engineering techniques could be used
    the same.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large companies, weighed down by bureaucracy, are reluctant to innovate and
    are now focusing on application research. On the other hand, the open-source community
    is very active and a potential innovation would be adopted and pushed quickly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The real advantage of the transformer is that for years it had absolute dominance.
    So it has benefited from a lot of attention, research, and optimization. Being
    able to replace such a model is a very difficult but still necessary challenge.
    In fact, we have pretty much reached the limits of this technology and need a
    new one that will surpass it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While for the moment we do not have an alternative, research is looking for
    a model that can surpass it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parting thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/99faa0da8e3de7239a32bfe3a7830d64.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Saif71.com](https://unsplash.com/@saif71?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: At this time, research in AI has been incremental, especially in model architectures.
    Large companies at this time have been dedicated to finally putting these models
    into production and using them for commercial purposes.
  prefs: []
  type: TYPE_NORMAL
- en: is true that after a year of big announcements, the institutions have also moved
    and new laws are being prepared that will regulate artificial intelligence. These
    laws will also partly define the directions of new research. If these laws are
    too rigid they will throttle innovation.
  prefs: []
  type: TYPE_NORMAL
- en: “There are definitely large tech companies that would rather not have to try
    to compete with open source, so they’re creating fear of AI leading to human extinction.
    It’s been a weapon for lobbyists to argue for legislation that would be very damaging
    to the open-source community.” — Andrew Ng ([source](https://finance.yahoo.com/news/google-brain-cofounder-says-big-113049941.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAACCPVKuuJ47EM3W7q-arne9h3S_l8zr9D-HvVqTp0_5A4D4Co8SiIWFoZHyPteR9rAcLij4bq2MmZoKJ-dGXBNuJUmHToa-wshhX8xBxXysi4fjya2rsPgiCaatOi_8ViZkSPzdS0x-wPhW6AzvMIwlVzsiorSVkVB_XWoCU98xc))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Yann LeCun also feels the same way. Right now the big companies are afraid
    of open-source:'
  prefs: []
  type: TYPE_NORMAL
- en: '“If your fear-mongering campaigns succeed, they will *inevitably* result in
    what you and I would identify as a catastrophe: a small number of companies will
    control AI.” — Yann LeCun ([source](https://www.telegraph.co.uk/business/2023/10/30/big-tech-stoking-fears-over-ai-warn-scientists/))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In any case, as we have seen no technology remains dominant forever, and the
    transformer is beginning to show the limits of its age. it is exciting to think
    about what will replace it, what theoretical advances, what elegant solutions
    it will implement, and what incredible capabilities it will have.
  prefs: []
  type: TYPE_NORMAL
- en: What do you think will replace the transformer? Have you tried and transformer
    alternative? Let me know in the comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have found this interesting:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*You can look for my other articles, and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***
    *Check* [***this repository***](https://github.com/SalvatoreRa/ML-news-of-the-week)
    *containing weekly updated ML & AI news.* ***I am open to collaborations and projects***
    *and you can reach me on LinkedIn.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Here is the link to my GitHub repository, where I am collecting code and many
    resources related to machine learning, artificial intelligence, and more.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----297e6f14e189--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----297e6f14e189--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*or you may be interested in one of my recent articles:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/teaching-is-hard-how-to-train-small-models-and-outperforming-large-counterparts-f131f9d463e1?source=post_page-----297e6f14e189--------------------------------)
    [## Teaching is Hard: How to Train Small Models and Outperforming Large Counterparts'
  prefs: []
  type: TYPE_NORMAL
- en: Distilling the knowledge of a large model is complex but a new method shows
    incredible performances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/teaching-is-hard-how-to-train-small-models-and-outperforming-large-counterparts-f131f9d463e1?source=post_page-----297e6f14e189--------------------------------)
    [](https://levelup.gitconnected.com/neural-ensemble-whats-better-than-a-neural-network-a-group-of-them-0c9e156fca15?source=post_page-----297e6f14e189--------------------------------)
    [## Neural Ensemble: what’s Better than a Neural Network? A group of them'
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural ensemble: how to combine different neural networks in a powerful model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/neural-ensemble-whats-better-than-a-neural-network-a-group-of-them-0c9e156fca15?source=post_page-----297e6f14e189--------------------------------)
    [](https://pub.towardsai.net/speak-only-about-what-you-have-read-can-llms-generalize-beyond-their-pretraining-data-041704e96cd5?source=post_page-----297e6f14e189--------------------------------)
    [## Speak Only About What You Have Read: Can LLMs Generalize Beyond Their Pretraining
    Data?'
  prefs: []
  type: TYPE_NORMAL
- en: Unveiling the Limits and Wonders of In-Context Learning in Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'pub.towardsai.net](https://pub.towardsai.net/speak-only-about-what-you-have-read-can-llms-generalize-beyond-their-pretraining-data-041704e96cd5?source=post_page-----297e6f14e189--------------------------------)
    [](https://salvatore-raieli.medium.com/ml-news-week-6-12-november-9878eb0a7005?source=post_page-----297e6f14e189--------------------------------)
    [## ML news: Week 6–12 November'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI dev, TopicGPT, new chips, and much more
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: salvatore-raieli.medium.com](https://salvatore-raieli.medium.com/ml-news-week-6-12-november-9878eb0a7005?source=post_page-----297e6f14e189--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here is the list of the principal references I consulted to write this article,
    only the first name for an article is cited.
  prefs: []
  type: TYPE_NORMAL
- en: Vaswani, 2017, Attention Is All You Need, [link](https://arxiv.org/abs/1706.03762)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Huang, 2016, Densely Connected Convolutional Networks, [link](https://arxiv.org/abs/1608.06993)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zhao, 2023, A Survey of Large Language Models, [link](https://arxiv.org/abs/2303.18223)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Smith, 2023, ConvNets Match Vision Transformers at Scale, [link](https://arxiv.org/abs/2310.16764)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Goldblum, 2023, Battle of the Backbones: A Large-Scale Comparison of Pretrained
    Models across Computer Vision Tasks, [link](https://arxiv.org/abs/2310.19909)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wang, 2023, Pretraining Without Attention, [link](https://arxiv.org/pdf/2212.10544.pdf)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ma, 2023, Mega: Moving Average Equipped Gated Attention, [link](https://arxiv.org/pdf/2209.10655.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hasani, 2020, Liquid Time-constant Networks, [link](https://arxiv.org/pdf/2006.04439.pdf)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yadlowsky, 2023, Pretraining Data Mixtures Enable Narrow Model Selection Capabilities
    in Transformer Models, [link](https://arxiv.org/abs/2311.00871)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kaplan, 2020, Scaling Laws for Neural Language Models, [link](https://arxiv.org/abs/2001.08361)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hoffman, 2022, Training Compute-Optimal Large Language Models, [link](https://arxiv.org/abs/2203.15556)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simonyan, 2014, Very Deep Convolutional Networks for Large-Scale Image Recognition,
    [link](https://arxiv.org/abs/1409.1556)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Khan, 2022, Transformers in Vision: A Survey, [link](https://arxiv.org/pdf/2101.01169.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wei, 2022, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,
    [link](https://arxiv.org/abs/2201.11903)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Touvron, 2023, LLaMA: Open and Efficient Foundation Language Models, [link](https://arxiv.org/abs/2302.13971)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
