# 从零开始训练BERT的终极指南：最终篇

> 译文：[https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb)

## 最终前沿：构建和训练你的BERT模型

[](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)[![Dimitris Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------) [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------)·阅读时间7分钟·2023年12月18日

--

![](../Images/548c617f9d3d11635cc30ccc45e1d106.png)

图片来源：[Rob Laughter](https://unsplash.com/@roblaughter?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

> 本博客文章总结了我们关于从零开始训练BERT的系列内容。有关背景和完整理解，请参阅系列的[第一部分](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f)、[第二部分](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822)和[第三部分](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)。

当BERT在2018年登场时，它在自然语言处理（NLP）领域掀起了一场巨浪。许多人将其视为NLP的ImageNet时刻，类似于2012年深度神经网络对计算机视觉及更广泛的机器学习领域带来的变化。

五年过去了，预言依然准确。基于Transformer的大型语言模型（LLMs）不仅仅是新玩具；它们正在重塑整个格局。从改变我们的工作方式到革命性地获取信息，这些模型是无数新兴初创公司背后的核心技术，旨在挖掘它们尚未被充分利用的潜力。

这就是我决定撰写这一系列博客文章的原因，深入探讨BERT的世界以及如何从零开始训练自己的模型。重点不仅仅是完成任务——毕竟，你可以轻松找到在Hugging Face Hub上预训练的BERT模型。真正的魔力在于理解这个突破性模型的内部工作原理，并将这些知识应用于当前环境。

第一篇文章作为你的入门票，引入了BERT的核心概念、目标和潜在应用。我们甚至一起完成了微调过程，创建了一个问答系统：

[BERT训练的终极指南：介绍](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----eab78b0657bb--------------------------------) [## BERT训练的终极指南：介绍

### 解密BERT：改变NLP领域的模型的定义和各种应用。

[BERT训练的终极指南：介绍](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----eab78b0657bb--------------------------------)

第二部分作为你了解分词器这一经常被忽视领域的内部指南——解释它们的作用，展示它们如何将单词转换为数值，并指导你训练自己的分词器：

[BERT训练的终极指南：分词器](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----eab78b0657bb--------------------------------) [## BERT训练的终极指南：分词器

### 从文本到标记：你的BERT分词指南

[BERT训练的终极指南：分词器](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----eab78b0657bb--------------------------------)

在第三章中，我们讨论了我认为整个训练流程中最困难的步骤：数据集准备。我们*深入探讨*了为BERT的专用Masked ML和NSP任务准备数据集的细节，这些任务是它用来理解语言和上下文的代理。

[BERT训练的终极指南：准备数据集](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5?source=post_page-----eab78b0657bb--------------------------------) [## BERT训练的终极指南：准备数据集

### 数据准备：*深入探讨*，优化流程，并发现如何应对最关键的步骤

[BERT训练的终极指南：准备数据集](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5?source=post_page-----eab78b0657bb--------------------------------)

现在，最终的序幕拉开并落下：训练。在这篇文章中，我们将*挽起袖子*，训练一个新的BERT模型——你可以稍后针对特定的NLP需求进行微调。如果你已经涉足深度学习模型训练，这篇文章应该能让你毫无问题地跟上。所以，事不宜迟，让我们*深入探讨*吧！

> [学习速率](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-training)是一个面向对ML和MLOps世界感兴趣的人的通讯。如果你想了解更多类似的主题，请[点击这里](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-training)订阅。每个月的最后一个星期天，我会与你分享最新的MLOps新闻和文章的更新和看法！

# 训练循环

现代框架如PyTorch和TensorFlow简化了构建和训练深度神经网络的过程。训练循环就像编排，将数据集特征传递通过模型以获得预测，将模型的预测与标签（真实值）进行比较以计算损失，最后使用损失通过模型的层反向传播误差以更新模型的权重。

这就是在PyTorch中流程的样子：

[PRE0]

理论上，这看起来很简单，不是吗？实际上，它变得更简单而且更强大。我们将使用Hugging Face `transformers` 库来训练我们的BERT编码器。`Trainer` 对象为我们处理循环实现，并添加了许多其他功能。因此，我们可以将注意力转向代码的其他部分。现在，让我们首先定义我们的指标。

在纸面上，这个过程看起来真的很简单，你不觉得吗？现实更好：它不仅简单，而且非常强大，这要归功于像Hugging Face `transformers` 库这样的工具。

他们的 `Trainer` 对象承担了繁重的工作，同时增加了许多额外的功能。因此，我们可以将注意力转向代码的其他部分。

所以，接下来让我们定义我们的指标。

## 准确率

为了评估我们模型在训练期间的表现，我们需要关注两个关键数字：损失和对我们有意义的指标。损失是像梯度下降这样的算法在反向传播期间用来改进模型性能的，而指标则是更易于理解的东西。

本着简洁的精神，我们将选择一个直接的衡量标准：模型在掩码语言建模（MLM）和下一句预测（NSP）任务上的准确率。实质上，我们关心的是两个方面：我们的模型在填充那些掩码词时的准确度，以及它在识别两句逻辑上是否相互衔接方面的表现。

然而，我们必须说这不是验证我们模型性能的最佳方式。例如，考虑以下句子：

> 我在数学方面是 [MASK]。

你会如何填充掩码词？我是*擅长*数学，还是我是*糟糕*的数学？这两种预测在语境上似乎都有效，但它们完全改变了句子的意思。

因此，用只有一个正确答案的验证数据集来评估模型不是最好的主意。这可能并不能真正测试模型理解语言的能力，而是记忆能力。为了解决这个问题，我们可以计算另一个指标，比如训练后模型的困惑度。

现在，让我们通过从 `transformers` 库中加载相应模块来评估模型的准确性：

[PRE1]

## 训练

假设你在整个教程系列中一直跟随我，现在你正处于一个关键时刻，你已经准备好了数据集。数据科学家中有一句常见的说法：一旦你的数据集为模型雕琢完成，你的大部分工作就差不多完成了。由于深度学习框架的进步，接下来的步骤往往感觉像是简单的模板代码。

所以，带着期待，让我们开始实例化模型吧。

[PRE2]

现在迎来了高潮：启动训练脚本。首先，我们将定义训练参数，然后，在一切准备就绪后，我们将启动训练循环：

[PRE3]

两小时和20个周期后，在有限的Colab Notebook环境中，我在NSP任务上达到了80%的准确率，在Masked ML任务上达到了15%的准确率。对于一个从零开始训练的模型来说，这些数字还不错。

![](../Images/686ea6f168d6cb72d37ecbb6983d1ae2.png)

训练结果 — 作者提供的图片

然而，值得注意的是，在大多数实际场景中，你的起点并不是从零开始。通常，你会从一个预训练的模型开始，这往往能带来更好的结果。

同时，还有大量的空间进行实验和提升性能。调整训练过程中的超参数，特别是学习率，可以显著提升你的结果，即使你是从头开始构建模型。

# 结论

我们已经达到了从零开始训练BERT模型的最终目的地。在这段旅程中，我们了解了为什么BERT被认为是NLP领域的开创性模型，并深入探讨了分词器的复杂性，不仅理解了它们的机制，还掌握了训练我们自己分词器的技巧。

接下来，我们的旅程带我们经历了为两个并行运行的任务——NSP和Masked ML任务——准备数据集的细致过程。最终，我们来到了启动训练脚本的关键步骤，使用了`transformers`库。

这是一段漫长但充实的旅程，我希望你也喜欢它。你应该准备好将这些知识应用到自己的活动中，并扩展到更多领域。下次见，祝编程愉快！

# 关于作者

我的名字是 [Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-intro)，我是一个为 [HPE](https://www.hpe.com/us/en/home.html)工作的机器学习工程师。我为欧洲委员会、国际货币基金组织、欧洲中央银行、宜家、Roblox等主要客户设计和实施了AI和软件解决方案。

如果你有兴趣阅读更多关于机器学习、深度学习、数据科学和数据运维的文章，可以在 [Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow)、[LinkedIn](https://www.linkedin.com/in/dpoulopoulos/) 或在Twitter上的 [@james2pl](https://twitter.com/james2pl) 关注我。

所表达的意见仅代表我个人，并不代表我的雇主的观点或意见。
