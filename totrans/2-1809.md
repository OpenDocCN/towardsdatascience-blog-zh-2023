# Sb3，应用RL的瑞士军刀

> 原文：[https://towardsdatascience.com/sb3-the-swiss-army-knife-of-applied-rl-5548535d09cd](https://towardsdatascience.com/sb3-the-swiss-army-knife-of-applied-rl-5548535d09cd)

## 你的模型选择，适用于任何环境

[](https://medium.com/@byjameskoh?source=post_page-----5548535d09cd--------------------------------)[![詹姆斯·科，博士](../Images/8e7af8b567cdcf24805754801683b426.png)](https://medium.com/@byjameskoh?source=post_page-----5548535d09cd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5548535d09cd--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5548535d09cd--------------------------------) [詹姆斯·科，博士](https://medium.com/@byjameskoh?source=post_page-----5548535d09cd--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5548535d09cd--------------------------------) ·8 分钟阅读·2023年10月26日

--

![](../Images/3359413fa03ba1b4537646411e4dea66.png)

图片由 DALL·E 3 根据提示“创建一个现实主义风格的打开的瑞士军刀图像”生成。

Stablebaseline3 (sb3) 就像是一把瑞士军刀。它是一种多功能工具，可以用于许多目的。而且，就像瑞士军刀在你被困在丛林中时可以救命一样，sb3 可以在你在办公室中遇到看似不可能的截止日期时救你一命。

> 本指南使用 gymnasium=0.28.1 和 stable-baselines=2.1.0。如果你使用不同的版本，或许还参考了其他旧指南，可能不会得到下面的结果。但不要担心，这里也提供了安装指南。我保证只要按照我的说明操作，你就能获得结果。

# [1] 你将获得什么

Stablebaseline3 使用起来很简单。它也有很好的文档支持，你可以自行跟随教程。但…

+   你是否参考过旧的指南（可能是使用 `gym` 的指南），结果发现你的机器上存在错误？

+   你能始终确保兼容性吗？

+   如果你想使用 `gymnasium` 的环境并修改奖励，该怎么办？

+   你知道如何包装自己的任务，以便可以在几行代码中应用 SOTA 模型吗？

这就是本文的目标！在阅读了这篇指南之后，你将…

1.  使用 sb3 模型解决经典环境，视觉化结果，并在几行代码中保存（或加载）训练好的模型。**[第 3.1 节]**

1.  理解如何检查动作空间和观测空间的兼容性。**[第 3.2 节]**

1.  学习如何包装 `gymnasium` 环境，以便可以使用任何 sb3 模型，而不会对 `box` 或 `discrete` 有任何限制。**[第 4.1 节]**

1.  学会如何包装 `gymnasium` 环境以进行奖励塑形。**[第 4.2 节]**

1.  了解如何将自定义环境包装为与 sb3 兼容，同时对原始代码进行最小更改，原始代码可能遵循不同的结构。**[第5节]**

# [2] 安装

创建一个虚拟环境并设置相关依赖。我主要针对的是大多数人——这里的指南是在 Windows 系统上创建的，并且已经安装了 Anaconda。打开你的 Anaconda 提示符并执行以下操作：

[PRE0]

在这里，我们将使用 jupyter notebook，因为它是一个更用户友好的教学工具。

# [3] 成功的初步体验 — 查看你的训练 RL 代理

首先要导入所需的库。

[PRE1]

## [3.1] Cartpole 上的 DQN

我们从小的例子开始，比如 Cartpole 任务，目标是推动小车（向左或向右）以保持杆子直立。

你绝对需要的最低限度是什么？就是这个，用于训练。

[PRE2]

还有这个，用于评估。

[PRE3]

最后，这样做是为了可视化。

[PRE4]

只需 10 行以上的代码和几秒钟时间，我们就解决了一个经典的 RL 问题。这是 AI 已经被民主化到何种程度的一个好例子！

使用上面完全相同的代码训练并可视化的代理。图片由作者提供。

要保存你的 sb3 模型，只需在训练执行期间添加一个回调。

[PRE5]

你的模型随后可以用两行代码加载。

[PRE6]

## [3.2] 检查动作/观察空间

假设我们尝试不同的模型，比如使用 `model=SAC("MlpPolicy", env)`。这将导致一个错误。

这是因为 SAC（Soft Actor Critic）仅适用于连续动作空间，如官方 Stable Baselines3 [文档](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) 中所述，而 Cartpole 环境具有离散动作空间。

我将动作空间约束汇编成一个简单的函数如下：

[PRE7]

这样，`is_compatible(env,'DQN')` 返回 `True`，而 `is_compatible(env,'SAC')` 返回 `False`。

对于 sb3 中的任何模型，观察空间没有约束。

# [4] 包装 `gymnasium` 环境

如果我们想根据自己的规格修改 `gymnasium` 环境呢？我们应该从头编写代码？还是查看源代码并在那进行修改？

对这两个问题的回答是，**不**。

最好只是包装 `gymnasium` 对象。这样不仅快速简便，还使你的代码可读且可靠。

人们不需要逐行审查你的代码。他们只需查看你包装器中的修改（假设他们对 `gymnasium` 的正确性感到信服）。

## [4.1] 不考虑 `box` 或 `discrete`

在第 3.2 节中，我们看到 SAC 与 Cartpole 不兼容。

这是一个解决办法。实际上，任何 sb3 模型都可以用于任何环境；我们只需要一个简单的包装器。

[PRE8]

通过这样做，你可以使用像 SAC 这样的处理连续动作空间的模型来解决具有离散动作空间的环境。

[PRE9]

任何 sb3 模型都可以与任何经典的 gymnasium 环境兼容。不要仅仅听我的话。试试以下内容。

[PRE10]

请注意，这里的目的是*展示*环境可以被包装成兼容的形式。性能可能不是理想的，但这不是重点。

关键是要向你展示，如果你理解sb3如何与gymnasium配合使用，你能够将任何东西包装成通用兼容的形式。

## [4.2] 奖励塑形

假设我们想修改一个gymnasium环境，以尝试奖励塑形。例如，你可能已经玩过[Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/)，并观察到一个用默认超参数训练的智能体可能会悬停在顶部，以避免碰撞的风险。

Lunar Lander在顶部悬停。图片由作者提供。

在这种情况下，我们可以对智能体持续停留在顶部时施加惩罚。

[PRE11]

请记住，在用伪奖励进行训练后，智能体应使用实际环境和原始奖励进行微调。

[PRE12]

通过奖励塑形训练的智能体解决了Lunar Lander。图片由作者提供。

这看起来好多了！

# [5] 自定义任务的包装器

在这一最终部分，我将实现我的第5个承诺——*学习如何将自定义环境包装成与sb3兼容，同时对原始代码做最小的修改，原始代码可能遵循不同的结构*。

作为学习者，我们训练RL智能体解决知名的基准问题。然而，行业支付你的是解决实际问题，而不是玩具问题。如果你因为RL专长而被雇佣，你很可能需要解决对公司而言独特的问题。

然而，sb3和gymnasium仍然是你的好朋友！

为了说明问题，让我们考虑以下简单的GridWorld。

[PRE13]

请注意，这里的`transition`方法返回`reward`、`next_state`和`done`。Stable baselines3将不接受这种风格。

你需要重新编写你的环境吗？不需要！

相反，我们构建了一个简单的包装器。

[PRE14]

在上面，我们定义了一个`step`方法，它包裹了原始环境的`transition`，并返回sb3期望的内容。

与此同时，我利用这个机会展示了我们可以在不解剖原始环境的情况下进行修改。在这里，`CustomEnv`如果目标在50步内未达成，则终止回合（并施加大惩罚）。

我们怎么知道环境是否正确包装了呢？首先，它必须通过以下基本检查。

[PRE15]

接下来，我们可以使用sb3模型在包装后的环境上进行训练。你还可以在这里调整超参数，如下所示。

[PRE16]

# 结论

在这篇文章中，你已经学习了如何设置自己的环境以运行sb3和gymnasium。你现在有能力在***任何***你选择的环境中实现最先进的RL算法。

享受吧！
