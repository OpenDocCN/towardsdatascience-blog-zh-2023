- en: Solving The Taxi Environment With Q-Learning — A Tutorial
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Q 学习解决出租车环境——教程
- en: 原文：[https://towardsdatascience.com/solving-the-taxi-environment-with-q-learning-a-tutorial-c76c22fc5d8f](https://towardsdatascience.com/solving-the-taxi-environment-with-q-learning-a-tutorial-c76c22fc5d8f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/solving-the-taxi-environment-with-q-learning-a-tutorial-c76c22fc5d8f](https://towardsdatascience.com/solving-the-taxi-environment-with-q-learning-a-tutorial-c76c22fc5d8f)
- en: A Python implementation of Q-learning to solve the Taxi-v3 environment from
    OpenAI Gym in an animated Jupyter Notebook
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在动画 Jupyter Notebook 中实现 Q 学习，以解决 OpenAI Gym 的 Taxi-v3 环境
- en: '[](https://wvheeswijk.medium.com/?source=post_page-----c76c22fc5d8f--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----c76c22fc5d8f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c76c22fc5d8f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c76c22fc5d8f--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----c76c22fc5d8f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wvheeswijk.medium.com/?source=post_page-----c76c22fc5d8f--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----c76c22fc5d8f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c76c22fc5d8f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c76c22fc5d8f--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----c76c22fc5d8f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c76c22fc5d8f--------------------------------)
    ·8 min read·Mar 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c76c22fc5d8f--------------------------------)
    ·阅读时长8分钟·2023年3月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c60a0923b3fdf481b97b7be803bcebf7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c60a0923b3fdf481b97b7be803bcebf7.png)'
- en: Photo by [Alexander Redl](https://unsplash.com/@alexanderredl?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Alexander Redl](https://unsplash.com/@alexanderredl?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The goal of the Taxi Environment in OpenAI’s Gym — yes, from the [company behind
    ChatGPT](https://medium.com/datadriveninvestor/how-does-the-company-behind-chatgpt-and-dall-e-make-its-money-d6aa5121a849)
    and Dall⋅E — is simple and straightforward, making for an excellent introduction
    to the field of Reinforcement Learning (RL).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 中出租车环境的目标——是的，来自于 [ChatGPT 背后的公司](https://medium.com/datadriveninvestor/how-does-the-company-behind-chatgpt-and-dall-e-make-its-money-d6aa5121a849)
    和 Dall⋅E——是简单直接的，为强化学习（RL）领域提供了一个很好的入门介绍。
- en: This article provides a step-to-step guide to implement the environment, learn
    a policy using tabular Q-learning, and visualize the learned behavior in animations.
    If you are looking to make your first strides in RL, this tutorial might be just
    for you.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了一个逐步指南，介绍如何实现环境、使用表格 Q 学习学习策略，并通过动画可视化学习到的行为。如果你想在 RL 中迈出第一步，这个教程可能正适合你。
- en: Problem setting
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题设置
- en: In the Taxi environment, a taxi should pick up passengers and drop them at their
    destination on a small parking lot, driving the shortest path possible. How hard
    can it be to learn such a task with RL? Let’s find out.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在出租车环境中，一辆出租车需要接载乘客并将他们送到一个小停车场的目的地，尽可能地走最短路径。使用强化学习（RL）学习这样一个任务有多难呢？让我们来看看。
- en: Before diving into the implementation, it is good practice to first formulate
    the setting as a [Markov Decision Process](https://medium.com/towards-data-science/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7)
    (MDP), getting a grasp on the mathematical structure of the problem at hand.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入实现之前，最好先将设置表述为 [马尔可夫决策过程](https://medium.com/towards-data-science/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7)（MDP），以掌握问题的数学结构。
- en: State space
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态空间
- en: An MDP states contains the information needed to (i) determine actions, (ii)
    compute the action reward and (iii) compute the transition to the next state.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 MDP 状态包含以下信息：（i）确定动作，（ii）计算动作奖励，（iii）计算过渡到下一个状态。
- en: 'The most obvious state property is the location of the taxi; the 5*5 grid gives
    us 25 options. Furthermore, a passenger waiting for pickup can be waiting at 1
    or 4 points (labeled Y, R, G, B) or they can be in the taxi, giving us (4+1) options.
    Finally, the destination can also be Y, R, G or B. We denote the state by the
    following vector:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的状态属性是出租车的位置；5*5的网格给我们25个选项。此外，等待接送的乘客可以在1或4个点（标记为Y、R、G、B）等待，或者他们可以在出租车内，总共提供了（4+1）个选项。最后，目的地也可以是Y、R、G或B。我们用以下向量表示状态：
- en: '`State = [x_pos_taxi, y_pos_taxi, pos_passenger, dest_passenger]`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`State = [x_pos_taxi, y_pos_taxi, pos_passenger, dest_passenger]`'
- en: Although a very simple problem setting, we thus already deal with 5*5*(4+1)*4=500
    states! Scale the problem to thousands of locations, and it is easy to see how
    quickly the size of MDPs explodes (the so-called curse of dimensionality).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个非常简单的问题设置，但我们已经处理了5*5*(4+1)*4=500个状态！将问题规模扩大到数千个位置，就很容易看出MDP的规模如何迅速膨胀（所谓的维度诅咒）。
- en: The number of **reachable** states is slightly smaller than 500, e.g., a passenger
    will never have the same pickup point and destination. Due to modelling complexities,
    we typically focus on the full state space, representing the worst-case scenario.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**可达**的状态数量稍微小于500，例如，乘客不会有相同的接送点和目的地。由于建模复杂性，我们通常关注完整的状态空间，代表最坏情况。'
- en: Action space
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作空间
- en: 'The state space may be bigger than you’d expect at first glance, but the action
    space contains only six decisions:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间可能比你一开始预期的要大，但动作空间仅包含六个决策：
- en: Down
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下
- en: Up
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上
- en: Left
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左
- en: Right
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右
- en: Drop passenger
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接送乘客
- en: Pick up passenger
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接送乘客
- en: In case the action is not allowed (e.g., trying to drive through a wall, attempting
    to pick up a customer when there is none at the present location), the agent will
    remain still.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果动作不被允许（例如，尝试穿越墙壁，或在当前地点没有客户时尝试接客），代理将保持不动。
- en: The action_mask (found in `env.action_mask(state)`) informs us which actions
    are feasible, but this feasibility check is not automatically enforced.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: action_mask（在`env.action_mask(state)`中找到）告诉我们哪些动作是可行的，但这个可行性检查并不会自动强制执行。
- en: Reward function
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励函数
- en: 'Three types of reward can be distinguished in this environment:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中可以区分三种类型的奖励：
- en: '**Move: -1\.** Every move comes with a small penalty, to encourage following
    the shortest path from origin to destination.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移动：-1。** 每次移动都会带来小惩罚，以鼓励从起点到终点沿最短路径行驶。'
- en: '**Failed drop-off: -10\.** A passenger will naturally be unhappy when dropped
    at the wrong location, so a large penalty is appropriate.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**失败的接送：-10。** 当乘客被放置在错误的位置时，自然会感到不满，因此应该给予较大的惩罚。'
- en: '**Successful drop-off: 20\.** To encourage the desired behavior, a large bonus
    is incurred.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成功的接送：20。** 为了鼓励期望的行为，给予较大的奖励。'
- en: Designing rewards in a way that encourages desired behavior is known as reward
    shaping. For instance, we might simply end the game without a reward upon successful
    drop-off (thus ending the stream of costs), but a clear reward signal often eases
    learning considerably.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 设计奖励的方式以鼓励期望的行为称为奖励塑形。例如，我们可以在成功接送后简单地结束游戏而不给予奖励（从而结束成本流），但明确的奖励信号通常大大简化学习。
- en: Transition function
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转移函数
- en: Unlike most realistic environments, the taxi environment exhibits no randomness.
    This feature considerably simplifies learning, as the **next state is dictated
    solely by our action**. In case an infeasible action is selected — such as trying
    to drive through a wall or picking up a non-existent passenger — the agent simply
    remains in place.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数现实环境不同，出租车环境没有随机性。这个特性大大简化了学习，因为**下一个状态完全由我们的动作决定**。如果选择了不可行的动作——比如尝试穿越墙壁或接一个不存在的乘客——代理将会停在原地。
- en: 'In an abstract sense, we could denote the transition function as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从抽象的角度来看，我们可以如下表示转移函数：
- en: '`new_state = f(state,action)`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`new_state = f(state,action)`'
- en: An explicit definition would consider all grid moves, (in)feasible drop-offs
    and (in)feasible pickups, which is not necessary hard but requires some focus.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 明确的定义会考虑所有网格移动，（不）可行的接送和（不）可行的接客，这并不是特别困难，但需要一些专注。
- en: The game ends upon successfully dropping off a passenger at their destination,
    denoting the terminating state.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏在成功将乘客送到目的地时结束，表示终止状态。
- en: Initialization
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化
- en: Having mathematically defined the problem (although not in excruciating detail),
    we will move towards implementation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上定义了问题（尽管不是非常详细）之后，我们将转向实现。
- en: First, we install the necessary libraries and subsequently import them. Obviously
    we need to install the `gym` environment. Other than that, we just require some
    visual stuff and your typical data science libraries.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们安装必要的库，并随后导入它们。显然，我们需要安装`gym`环境。除此之外，我们只需要一些视觉相关的内容和你典型的数据科学库。
- en: Assuming all imports went correctly, it is time for a sanity check. Lets create
    and render a Taxi-v3 environment.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设所有导入都正确，现在是时候进行一下基本检查了。让我们创建并渲染一个Taxi-v3环境。
- en: We should see an action between 0 and 5, a state between 0 and 499, and an action
    mask to block infeasible actions. Most importantly, we should have rendered a
    still frame to affirm we are truly up and running!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到一个介于0到5之间的动作，一个介于0到499之间的状态，以及一个阻止不可行动作的动作掩码。最重要的是，我们应该渲染一个静态画面，以确认我们确实已经启动并运行！
- en: '![](../Images/78a49c24b3980432b5f0125f124f1447.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78a49c24b3980432b5f0125f124f1447.png)'
- en: Screenshot of initialization output [image by author, using [OpenAI Gym](https://github.com/openai/gym/blob/master/LICENSE.md)]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化输出的截图[图片由作者提供，使用[OpenAI Gym](https://github.com/openai/gym/blob/master/LICENSE.md)]
- en: 'Finally, let’s set up two functions : one for the console output and animation,
    one for storing GIFs. Note that the animation is simply a repeated plot of the
    RGB frame. It is also very time-consuming, and therefore not recommended to run
    during training.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们设置两个函数：一个用于控制台输出和动画，另一个用于存储GIF。请注意，动画只是RGB帧的重复绘图。它也非常耗时，因此不建议在训练期间运行。
- en: Testing a random agent
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试随机代理
- en: Having asserted that the environment works as expected, it is time to let a
    random agent run wild. As you guessed, this agent takes random action at every
    moment in time. This is essentially how the learning agent sets out, having **no
    reliable Q-values** to work with.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在确认环境按预期工作后，现在是时候让一个随机代理“肆意行动”了。正如你所猜测的，这个代理在每一个时刻都会采取随机行动。这本质上就是学习代理开始时的状况，因为它**没有可靠的Q值**可以使用。
- en: You will see the agent trying to drive through walls, pick up passengers at
    deserted sports, or drop off passenger at the wrong location. It might take thousands
    of actions before — strictly by accident — dropping the passenger at the right
    location.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到代理尝试穿墙、在荒凉的地方接乘客，或者在错误的地点放下乘客。在极少数情况下，代理可能需要几千个动作——纯粹是偶然——才会将乘客放到正确的位置。
- en: 'As you can imagine, the random agent frustrates the passenger to no end:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可以想象的那样，随机代理让乘客感到无比沮丧：
- en: '![](../Images/b47a4e00828fa23dd51d9fe2e3f6555b.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b47a4e00828fa23dd51d9fe2e3f6555b.png)'
- en: Animation of an untrained agent. The taxi selects random actions at each time
    step, including infeasible ones, until eventually dropping of the passenger at
    the correct stop by accident [image by author, using [OpenAI Gym](https://github.com/openai/gym/blob/master/LICENSE.md)]
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 未训练代理的动画。出租车在每个时间步选择随机动作，包括不可行的动作，直到最终偶然将乘客放到正确的站点[图片由作者提供，使用[OpenAI Gym](https://github.com/openai/gym/blob/master/LICENSE.md)]
- en: Why sit through this lengthy animation? Well, it really gives an impression
    how an untrained RL agent behaves, and how long it takes to get a meaningful reward
    signal.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要观看这么长的动画？好吧，这真的能给你一个未训练的RL代理是如何行为的印象，以及获取有意义的奖励信号需要多长时间。
- en: 'Note: by applying the **action mask** — a Boolean vector that essentially assigns
    a probability of 0 to selecting infeasible actions — we restrict ourselves to
    only sensible actions, typically substantially shortening the episode’s length.'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：通过应用**动作掩码**——一个布尔向量，它实质上将不可行的动作的选择概率分配为0——我们将自己限制在仅有的合理动作中，通常会大大缩短剧集的长度。
- en: Training the agent
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练代理
- en: Time to learn something useful.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是学习一些有用的东西的时候了。
- en: We will deploy a [basic Q-learning algorithm](https://medium.com/towards-data-science/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff)
    with ϵ-greedy exploration, selecting random actions with probability ϵ and use
    Q-values to select actions for the remainder of the time.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将部署一个[基础Q学习算法](https://medium.com/towards-data-science/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff)，采用ϵ-贪婪探索，选择ϵ概率的随机动作，其余时间使用Q值来选择动作。
- en: Q-values are updates using the following equation after making an observation.
    Note that with 500 states and 6 actions, we must fill a Q-table of size 500*6=3000,
    with each state-action pair requiring multiple observations to learn its value
    with any degree of accuracy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Q值在观察后使用以下公式更新。请注意，随着500个状态和6个动作的增加，我们必须填充一个大小为500*6=3000的Q表，每个状态-动作对需要多个观察来以任何程度的准确性学习其值。
- en: '![](../Images/5d1b28ed0b65945438e798622046ba67.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d1b28ed0b65945438e798622046ba67.png)'
- en: In Q-learning, we update values of state-action pairs after each observation,
    using off-policy learning.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在Q学习中，我们在每次观察后更新状态-动作对的值，使用脱离策略的学习。
- en: The code looks as follows. You can tinker with the hyper-parameters; in my experience
    this problem is fairly insensitive to them.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下。你可以调整超参数；根据我的经验，这个问题对它们的敏感性较低。
- en: Converge of training is visualized below. After 2000 episodes, it seems we have
    learned a pretty good and consistent policy.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的收敛情况如下图所示。经过2000个回合后，我们似乎学会了一个相当好的、稳定的策略。
- en: '![](../Images/b080cb5baa9ef3c3483892086b7f0af9.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b080cb5baa9ef3c3483892086b7f0af9.png)'
- en: Initially, the agent requires many steps to successfully complete an epsiode.
    Ultimately, positive rewards are recognized and the agent starts taking increasingly
    efficient actions, culminating in shortest path solutions [image by author]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，代理需要许多步骤才能成功完成一个回合。最终，正向奖励会被识别，代理开始采取越来越高效的动作，最终实现最短路径解决方案 [作者图片]
- en: 'Note: the code ignores the action mask and selects the first Q-value in case
    of a tie. Although alternatives for both issues are provided, they substantially
    increase the computational effort per episode, while providing no clear performance
    benefit in this setting.'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：代码忽略了动作掩码，并在出现平局时选择第一个Q值。虽然提供了两种问题的替代方案，但它们大大增加了每个回合的计算工作量，并且在此设置下没有明显的性能提升。
- en: Testing the policy
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试策略
- en: Let’s see what we learned. Depending on the state we are in, we look up the
    corresponding Q-values in the Q-table (i.e., six values for each state, corresponding
    to the actions) and select the action with the highest associated Q-value.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们学到了什么。根据我们所在的状态，我们在Q表中查找相应的Q值（即每个状态的六个值，对应于动作），并选择Q值最高的动作。
- en: Effectively, the Q-value captures the expected cumulative reward associated
    with the action. Note that we no longer select random actions with probability
    ϵ — this mechanism was required strictly for learning.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，Q值捕捉了与动作相关的预期累计奖励。注意，我们不再以概率ϵ选择随机动作——这一机制仅用于学习。
- en: If you performed sufficiently many iterations, you should see the taxi always
    drive directly to the passenger, take the shortest path to the destination, and
    successfully drop of the passenger. Indeed, we’ve come a long way since driving
    around at random.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你进行了足够多的迭代，你应该会看到出租车总是直接驶向乘客，选择最短路径到达目的地，并成功接送乘客。的确，自从随意驾驶以来，我们已经取得了长足的进步。
- en: '![](../Images/3d6fb42df9bb360b32658485e1d00b11.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d6fb42df9bb360b32658485e1d00b11.png)'
- en: Animation of a successfully learned policy. The taxi will follow the shortest
    path both to pick up and drop off the passenger, maximizing the reward [image
    by author, using [OpenAI Gym](https://github.com/openai/gym/blob/master/LICENSE.md)]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 成功学习到的策略动画。出租车将沿着最短路径接送乘客，最大化奖励 [作者图片，使用 [OpenAI Gym](https://github.com/openai/gym/blob/master/LICENSE.md)]
- en: Although it requires many observations to learn this policy in a seemingly trivial
    environment, you might appreciate the result more if you realize the algorithm
    learned all of this without knowing anything at all about the environment it operates
    in!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在一个看似简单的环境中需要大量观察来学习这一策略，但如果你意识到算法在完全不了解其操作环境的情况下学会了这些，你可能会更欣赏这个结果！
- en: Wrapping up
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The Taxi environment is a nice one to get started with Reinforcement Learning.
    The problem setting is simple and intuitive, yet could easily be extended towards
    something more realistic (think realistic urban networks, managing taxi fleets,
    uncertain travel times). It is computationally well-manageable, but also illustrates
    the computational effort required even to learn policies for trivial problem settings.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Taxi环境是一个很好的强化学习入门选择。问题设置简单直观，但可以很容易地扩展到更现实的场景（例如现实的城市网络、管理出租车队、不可预测的旅行时间）。它在计算上易于管理，但也展示了即使在微不足道的问题设置下学习策略所需的计算工作量。
- en: If you are new to Reinforcement Learning, playing around with the OpenAI Gym
    environments is highly recommended before moving to designing and solving your
    own problems.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是强化学习的新手，强烈建议你在设计和解决自己的问题之前，先在OpenAI Gym环境中进行尝试。
- en: '*The full Jupyter Notebook can be found on my GitHub:*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*完整的Jupyter Notebook可以在我的GitHub上找到：*'
- en: '[https://github.com/woutervanheeswijk/taxi_environment](https://github.com/woutervanheeswijk/taxi_environment)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/woutervanheeswijk/taxi_environment](https://github.com/woutervanheeswijk/taxi_environment)'
- en: '![](../Images/b940d5c28484743ce42d461519ae29cb.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b940d5c28484743ce42d461519ae29cb.png)'
- en: Photo by [Lexi Anderson](https://unsplash.com/@lexianderson?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Lexi Anderson](https://unsplash.com/@lexianderson?utm_source=medium&utm_medium=referral)
    拍摄，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Further reading
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*An introduction to Q-learning and SARSA:*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q-learning 和 SARSA 简介：*'
- en: '[](/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff?source=post_page-----c76c22fc5d8f--------------------------------)
    [## Walking Off The Cliff With Off-Policy Reinforcement Learning'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff?source=post_page-----c76c22fc5d8f--------------------------------)
    [## 使用非策略强化学习的悬崖行走'
- en: An in-depth comparison of off-policy and on-policy reinforcement learning
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对非策略和策略强化学习的深入比较
- en: towardsdatascience.com](/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff?source=post_page-----c76c22fc5d8f--------------------------------)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff?source=post_page-----c76c22fc5d8f--------------------------------)
- en: '*Structuring Markov Decision Processes:*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*构建马尔可夫决策过程：*'
- en: '[](/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7?source=post_page-----c76c22fc5d8f--------------------------------)
    [## The Five Building Blocks of Markov Decision Processes'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7?source=post_page-----c76c22fc5d8f--------------------------------)
    [## 马尔可夫决策过程的五大构建块'
- en: Define and communicate your Reinforcement Learning models by mastering the foundational
    principles of a Markov Decision…
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过掌握马尔可夫决策的基础原则来定义和沟通你的强化学习模型……
- en: towardsdatascience.com](/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7?source=post_page-----c76c22fc5d8f--------------------------------)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7?source=post_page-----c76c22fc5d8f--------------------------------)
- en: '*A tutorial for implementing Deep Q-learning:*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*实现 Deep Q-learning 的教程：*'
- en: '[](/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e?source=post_page-----c76c22fc5d8f--------------------------------)
    [## A Minimal Working Example for Deep Q-Learning in TensorFlow 2.0'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e?source=post_page-----c76c22fc5d8f--------------------------------)
    [## TensorFlow 2.0 中的深度 Q 学习的最小工作示例'
- en: A multi-armed bandit example to train a Q-network. The update procedure takes
    just a few lines of code using TensorFlow
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多臂老虎机示例用于训练 Q 网络。更新过程使用 TensorFlow 仅需几行代码
- en: towardsdatascience.com](/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e?source=post_page-----c76c22fc5d8f--------------------------------)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e?source=post_page-----c76c22fc5d8f--------------------------------)
- en: References
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'The notebook detailed in this article is partially based on and adapts code
    from the following sources:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中详细介绍的笔记本部分基于以下来源并改编了代码：
- en: '[1] OpenAI Gym. [Taxi-v3 environment](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py).
    The OpenAI Gym environment is available under the [MIT License](https://github.com/openai/gym/blob/master/LICENSE.md).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] OpenAI Gym. [Taxi-v3 环境](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py)。OpenAI
    Gym 环境在[MIT 许可证](https://github.com/openai/gym/blob/master/LICENSE.md)下提供。'
- en: '[2] LearnDataSci. [Reinforcement Q-Learning from Scratch in Python with OpenAI
    Gym](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/).
    Taxi-v2 implementation.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] LearnDataSci. [从头开始在 Python 中实现强化 Q-learning 与 OpenAI Gym](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/)。Taxi-v2
    实现。'
- en: '[3] Botforge. [Save OpenAI Gym renders as GIFS](https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553).
    Public GitHub Gist.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Botforge. [保存 OpenAI Gym 渲染为 GIFS](https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553)。公开
    GitHub Gist。'
