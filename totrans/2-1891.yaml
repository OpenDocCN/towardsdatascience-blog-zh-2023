- en: Solving The Taxi Environment With Q-Learning — A Tutorial
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Q 学习解决出租车环境——教程
- en: 原文：[https://towardsdatascience.com/solving-the-taxi-environment-with-q-learning-a-tutorial-c76c22fc5d8f](https://towardsdatascience.com/solving-the-taxi-environment-with-q-learning-a-tutorial-c76c22fc5d8f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/solving-the-taxi-environment-with-q-learning-a-tutorial-c76c22fc5d8f](https://towardsdatascience.com/solving-the-taxi-environment-with-q-learning-a-tutorial-c76c22fc5d8f)
- en: A Python implementation of Q-learning to solve the Taxi-v3 environment from
    OpenAI Gym in an animated Jupyter Notebook
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在动画 Jupyter Notebook 中实现 Q 学习，以解决 OpenAI Gym 的 Taxi-v3 环境
- en: '[](https://wvheeswijk.medium.com/?source=post_page-----c76c22fc5d8f--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----c76c22fc5d8f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c76c22fc5d8f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c76c22fc5d8f--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----c76c22fc5d8f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wvheeswijk.medium.com/?source=post_page-----c76c22fc5d8f--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----c76c22fc5d8f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c76c22fc5d8f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c76c22fc5d8f--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----c76c22fc5d8f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c76c22fc5d8f--------------------------------)
    ·8 min read·Mar 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c76c22fc5d8f--------------------------------)
    ·阅读时长8分钟·2023年3月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c60a0923b3fdf481b97b7be803bcebf7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c60a0923b3fdf481b97b7be803bcebf7.png)'
- en: Photo by [Alexander Redl](https://unsplash.com/@alexanderredl?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Alexander Redl](https://unsplash.com/@alexanderredl?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The goal of the Taxi Environment in OpenAI’s Gym — yes, from the [company behind
    ChatGPT](https://medium.com/datadriveninvestor/how-does-the-company-behind-chatgpt-and-dall-e-make-its-money-d6aa5121a849)
    and Dall⋅E — is simple and straightforward, making for an excellent introduction
    to the field of Reinforcement Learning (RL).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 中出租车环境的目标——是的，来自于 [ChatGPT 背后的公司](https://medium.com/datadriveninvestor/how-does-the-company-behind-chatgpt-and-dall-e-make-its-money-d6aa5121a849)
    和 Dall⋅E——是简单直接的，为强化学习（RL）领域提供了一个很好的入门介绍。
- en: This article provides a step-to-step guide to implement the environment, learn
    a policy using tabular Q-learning, and visualize the learned behavior in animations.
    If you are looking to make your first strides in RL, this tutorial might be just
    for you.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了一个逐步指南，介绍如何实现环境、使用表格 Q 学习学习策略，并通过动画可视化学习到的行为。如果你想在 RL 中迈出第一步，这个教程可能正适合你。
- en: Problem setting
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题设置
- en: In the Taxi environment, a taxi should pick up passengers and drop them at their
    destination on a small parking lot, driving the shortest path possible. How hard
    can it be to learn such a task with RL? Let’s find out.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在出租车环境中，一辆出租车需要接载乘客并将他们送到一个小停车场的目的地，尽可能地走最短路径。使用强化学习（RL）学习这样一个任务有多难呢？让我们来看看。
- en: Before diving into the implementation, it is good practice to first formulate
    the setting as a [Markov Decision Process](https://medium.com/towards-data-science/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7)
    (MDP), getting a grasp on the mathematical structure of the problem at hand.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入实现之前，最好先将设置表述为 [马尔可夫决策过程](https://medium.com/towards-data-science/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7)（MDP），以掌握问题的数学结构。
- en: State space
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态空间
- en: An MDP states contains the information needed to (i) determine actions, (ii)
    compute the action reward and (iii) compute the transition to the next state.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 MDP 状态包含以下信息：（i）确定动作，（ii）计算动作奖励，（iii）计算过渡到下一个状态。
- en: 'The most obvious state property is the location of the taxi; the 5*5 grid gives
    us 25 options. Furthermore, a passenger waiting for pickup can be waiting at 1
    or 4 points (labeled Y, R, G, B) or they can be in the taxi, giving us (4+1) options.
    Finally, the destination can also be Y, R, G or B. We denote the state by the
    following vector:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '`State = [x_pos_taxi, y_pos_taxi, pos_passenger, dest_passenger]`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Although a very simple problem setting, we thus already deal with 5*5*(4+1)*4=500
    states! Scale the problem to thousands of locations, and it is easy to see how
    quickly the size of MDPs explodes (the so-called curse of dimensionality).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The number of **reachable** states is slightly smaller than 500, e.g., a passenger
    will never have the same pickup point and destination. Due to modelling complexities,
    we typically focus on the full state space, representing the worst-case scenario.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Action space
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The state space may be bigger than you’d expect at first glance, but the action
    space contains only six decisions:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Down
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drop passenger
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pick up passenger
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In case the action is not allowed (e.g., trying to drive through a wall, attempting
    to pick up a customer when there is none at the present location), the agent will
    remain still.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The action_mask (found in `env.action_mask(state)`) informs us which actions
    are feasible, but this feasibility check is not automatically enforced.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Reward function
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Three types of reward can be distinguished in this environment:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '**Move: -1\.** Every move comes with a small penalty, to encourage following
    the shortest path from origin to destination.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failed drop-off: -10\.** A passenger will naturally be unhappy when dropped
    at the wrong location, so a large penalty is appropriate.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Successful drop-off: 20\.** To encourage the desired behavior, a large bonus
    is incurred.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing rewards in a way that encourages desired behavior is known as reward
    shaping. For instance, we might simply end the game without a reward upon successful
    drop-off (thus ending the stream of costs), but a clear reward signal often eases
    learning considerably.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Transition function
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike most realistic environments, the taxi environment exhibits no randomness.
    This feature considerably simplifies learning, as the **next state is dictated
    solely by our action**. In case an infeasible action is selected — such as trying
    to drive through a wall or picking up a non-existent passenger — the agent simply
    remains in place.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'In an abstract sense, we could denote the transition function as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '`new_state = f(state,action)`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: An explicit definition would consider all grid moves, (in)feasible drop-offs
    and (in)feasible pickups, which is not necessary hard but requires some focus.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The game ends upon successfully dropping off a passenger at their destination,
    denoting the terminating state.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Initialization
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having mathematically defined the problem (although not in excruciating detail),
    we will move towards implementation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: First, we install the necessary libraries and subsequently import them. Obviously
    we need to install the `gym` environment. Other than that, we just require some
    visual stuff and your typical data science libraries.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们安装必要的库，并随后导入它们。显然，我们需要安装`gym`环境。除此之外，我们只需要一些视觉相关的内容和你典型的数据科学库。
- en: Assuming all imports went correctly, it is time for a sanity check. Lets create
    and render a Taxi-v3 environment.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设所有导入都正确，现在是时候进行一下基本检查了。让我们创建并渲染一个Taxi-v3环境。
- en: We should see an action between 0 and 5, a state between 0 and 499, and an action
    mask to block infeasible actions. Most importantly, we should have rendered a
    still frame to affirm we are truly up and running!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到一个介于0到5之间的动作，一个介于0到499之间的状态，以及一个阻止不可行动作的动作掩码。最重要的是，我们应该渲染一个静态画面，以确认我们确实已经启动并运行！
- en: '![](../Images/78a49c24b3980432b5f0125f124f1447.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78a49c24b3980432b5f0125f124f1447.png)'
- en: Screenshot of initialization output [image by author, using [OpenAI Gym](https://github.com/openai/gym/blob/master/LICENSE.md)]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化输出的截图[图片由作者提供，使用[OpenAI Gym](https://github.com/openai/gym/blob/master/LICENSE.md)]
- en: 'Finally, let’s set up two functions : one for the console output and animation,
    one for storing GIFs. Note that the animation is simply a repeated plot of the
    RGB frame. It is also very time-consuming, and therefore not recommended to run
    during training.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们设置两个函数：一个用于控制台输出和动画，另一个用于存储GIF。请注意，动画只是RGB帧的重复绘图。它也非常耗时，因此不建议在训练期间运行。
- en: Testing a random agent
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试随机代理
- en: Having asserted that the environment works as expected, it is time to let a
    random agent run wild. As you guessed, this agent takes random action at every
    moment in time. This is essentially how the learning agent sets out, having **no
    reliable Q-values** to work with.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在确认环境按预期工作后，现在是时候让一个随机代理“肆意行动”了。正如你所猜测的，这个代理在每一个时刻都会采取随机行动。这本质上就是学习代理开始时的状况，因为它**没有可靠的Q值**可以使用。
- en: You will see the agent trying to drive through walls, pick up passengers at
    deserted sports, or drop off passenger at the wrong location. It might take thousands
    of actions before — strictly by accident — dropping the passenger at the right
    location.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到代理尝试穿墙、在荒凉的地方接乘客，或者在错误的地点放下乘客。在极少数情况下，代理可能需要几千个动作——纯粹是偶然——才会将乘客放到正确的位置。
- en: 'As you can imagine, the random agent frustrates the passenger to no end:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可以想象的那样，随机代理让乘客感到无比沮丧：
- en: '![](../Images/b47a4e00828fa23dd51d9fe2e3f6555b.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b47a4e00828fa23dd51d9fe2e3f6555b.png)'
- en: Animation of an untrained agent. The taxi selects random actions at each time
    step, including infeasible ones, until eventually dropping of the passenger at
    the correct stop by accident [image by author, using [OpenAI Gym](https://github.com/openai/gym/blob/master/LICENSE.md)]
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 未训练代理的动画。出租车在每个时间步选择随机动作，包括不可行的动作，直到最终偶然将乘客放到正确的站点[图片由作者提供，使用[OpenAI Gym](https://github.com/openai/gym/blob/master/LICENSE.md)]
- en: Why sit through this lengthy animation? Well, it really gives an impression
    how an untrained RL agent behaves, and how long it takes to get a meaningful reward
    signal.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要观看这么长的动画？好吧，这真的能给你一个未训练的RL代理是如何行为的印象，以及获取有意义的奖励信号需要多长时间。
- en: 'Note: by applying the **action mask** — a Boolean vector that essentially assigns
    a probability of 0 to selecting infeasible actions — we restrict ourselves to
    only sensible actions, typically substantially shortening the episode’s length.'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：通过应用**动作掩码**——一个布尔向量，它实质上将不可行的动作的选择概率分配为0——我们将自己限制在仅有的合理动作中，通常会大大缩短剧集的长度。
- en: Training the agent
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练代理
- en: Time to learn something useful.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是学习一些有用的东西的时候了。
- en: We will deploy a [basic Q-learning algorithm](https://medium.com/towards-data-science/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff)
    with ϵ-greedy exploration, selecting random actions with probability ϵ and use
    Q-values to select actions for the remainder of the time.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将部署一个[基础Q学习算法](https://medium.com/towards-data-science/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff)，采用ϵ-贪婪探索，选择ϵ概率的随机动作，其余时间使用Q值来选择动作。
- en: Q-values are updates using the following equation after making an observation.
    Note that with 500 states and 6 actions, we must fill a Q-table of size 500*6=3000,
    with each state-action pair requiring multiple observations to learn its value
    with any degree of accuracy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Q值在观察后使用以下公式更新。请注意，随着500个状态和6个动作的增加，我们必须填充一个大小为500*6=3000的Q表，每个状态-动作对需要多个观察来以任何程度的准确性学习其值。
- en: '![](../Images/5d1b28ed0b65945438e798622046ba67.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: In Q-learning, we update values of state-action pairs after each observation,
    using off-policy learning.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The code looks as follows. You can tinker with the hyper-parameters; in my experience
    this problem is fairly insensitive to them.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Converge of training is visualized below. After 2000 episodes, it seems we have
    learned a pretty good and consistent policy.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b080cb5baa9ef3c3483892086b7f0af9.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Initially, the agent requires many steps to successfully complete an epsiode.
    Ultimately, positive rewards are recognized and the agent starts taking increasingly
    efficient actions, culminating in shortest path solutions [image by author]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: the code ignores the action mask and selects the first Q-value in case
    of a tie. Although alternatives for both issues are provided, they substantially
    increase the computational effort per episode, while providing no clear performance
    benefit in this setting.'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Testing the policy
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s see what we learned. Depending on the state we are in, we look up the
    corresponding Q-values in the Q-table (i.e., six values for each state, corresponding
    to the actions) and select the action with the highest associated Q-value.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Effectively, the Q-value captures the expected cumulative reward associated
    with the action. Note that we no longer select random actions with probability
    ϵ — this mechanism was required strictly for learning.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: If you performed sufficiently many iterations, you should see the taxi always
    drive directly to the passenger, take the shortest path to the destination, and
    successfully drop of the passenger. Indeed, we’ve come a long way since driving
    around at random.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d6fb42df9bb360b32658485e1d00b11.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Animation of a successfully learned policy. The taxi will follow the shortest
    path both to pick up and drop off the passenger, maximizing the reward [image
    by author, using [OpenAI Gym](https://github.com/openai/gym/blob/master/LICENSE.md)]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Although it requires many observations to learn this policy in a seemingly trivial
    environment, you might appreciate the result more if you realize the algorithm
    learned all of this without knowing anything at all about the environment it operates
    in!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping up
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Taxi environment is a nice one to get started with Reinforcement Learning.
    The problem setting is simple and intuitive, yet could easily be extended towards
    something more realistic (think realistic urban networks, managing taxi fleets,
    uncertain travel times). It is computationally well-manageable, but also illustrates
    the computational effort required even to learn policies for trivial problem settings.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: If you are new to Reinforcement Learning, playing around with the OpenAI Gym
    environments is highly recommended before moving to designing and solving your
    own problems.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '*The full Jupyter Notebook can be found on my GitHub:*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/woutervanheeswijk/taxi_environment](https://github.com/woutervanheeswijk/taxi_environment)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b940d5c28484743ce42d461519ae29cb.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: Photo by [Lexi Anderson](https://unsplash.com/@lexianderson?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*An introduction to Q-learning and SARSA:*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[](/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff?source=post_page-----c76c22fc5d8f--------------------------------)
    [## Walking Off The Cliff With Off-Policy Reinforcement Learning'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: An in-depth comparison of off-policy and on-policy reinforcement learning
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff?source=post_page-----c76c22fc5d8f--------------------------------)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '*Structuring Markov Decision Processes:*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7?source=post_page-----c76c22fc5d8f--------------------------------)
    [## The Five Building Blocks of Markov Decision Processes'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Define and communicate your Reinforcement Learning models by mastering the foundational
    principles of a Markov Decision…
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7?source=post_page-----c76c22fc5d8f--------------------------------)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '*A tutorial for implementing Deep Q-learning:*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e?source=post_page-----c76c22fc5d8f--------------------------------)
    [## A Minimal Working Example for Deep Q-Learning in TensorFlow 2.0'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: A multi-armed bandit example to train a Q-network. The update procedure takes
    just a few lines of code using TensorFlow
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e?source=post_page-----c76c22fc5d8f--------------------------------)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The notebook detailed in this article is partially based on and adapts code
    from the following sources:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[1] OpenAI Gym. [Taxi-v3 environment](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py).
    The OpenAI Gym environment is available under the [MIT License](https://github.com/openai/gym/blob/master/LICENSE.md).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[2] LearnDataSci. [Reinforcement Q-Learning from Scratch in Python with OpenAI
    Gym](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/).
    Taxi-v2 implementation.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Botforge. [Save OpenAI Gym renders as GIFS](https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553).
    Public GitHub Gist.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
