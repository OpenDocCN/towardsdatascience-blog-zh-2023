["```py\n#Original Line from the first analysis\ndf['Label'] = [1 if x >=3 else 0 for x in df['Rating']]\n```", "```py\norig_reviews['Rating']=orig_reviews['Rating'].apply(lambda x: 0 if x < 4else 1)\n```", "```py\nD = 20 \ni = Input(shape = (T,))\nx = Embedding(V +1 , D)(i)\nx = Conv1D(16,2,activation='relu',)(x)\nx = MaxPooling1D(2)(x)\nx = Conv1D(32,2,activation='relu')(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(1,activation='sigmoid')(x)\nmodel = Model(i,x)\n```", "```py\n # Set the model name\nMODEL_NAME = 'bert-base-cased'\n\n# Build a BERT based tokenizer\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n\n# Store length of each review\ntoken_lens = []\n\n# Iterate through the content slide\nfor txt in df.content:\n    tokens = tokenizer.encode(txt, max_length=512)\n    token_lens.append(len(tokens))\n\nMAX_LEN = 160\n\nclass GPReviewDataset(Dataset):\n    # Constructor Function\n    def __init__(self, reviews, targets, tokenizer, max_len):\n        self.reviews = reviews\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    # Length magic method\n    def __len__(self):\n        return len(self.reviews)\n\n    # get item magic method\n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        target = self.targets[item]\n\n        # Encoded format to be returned\n        encoding = self.tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'review_text': review,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'targets': torch.tensor(target, dtype=torch.long)\n\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED,stratify=df.sentiment)\n\ndef create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = GPReviewDataset(\n        reviews=df.content.to_numpy(),\n        targets=df.sentiment.to_numpy(),\n        tokenizer=tokenizer,\n        max_len=max_len\n    )\n\n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        num_workers=0\n    )\n\nBATCH_SIZE = 16\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n\nprint(df_train.shape, df_test.shape)\n\nbert_model = BertModel.from_pretrained(MODEL_NAME,return_dict=False)\n\n# Build the Sentiment Classifier class\nclass SentimentClassifier(nn.Module):\n\n    # Constructor class\n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(MODEL_NAME,return_dict=False)\n        self.drop = nn.Dropout(p=0.5)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    # Forward propagaion class\n    def forward(self, input_ids, attention_mask,return_dict):\n        _, pooled_output = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask,\n          return_dict=False\n        )\n        #  Add a dropout layer\n        output = self.drop(pooled_output)\n        return self.out(output)\n\n# Instantiate the model and move to classifier\nmodel = SentimentClassifier(2)\nmodel = model.to(device)\n\n# Number of iterations\nEPOCHS = 10\n\n# Optimizer Adam\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)\n\n# Set the loss function\nloss_fn = nn.CrossEntropyLoss().to(device)\n\n# Function for a single training iteration\ndef train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n\n    for d in data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n        return_dict=True)\n\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n\n        # Backward prop\n        loss.backward()\n\n        # Gradient Descent\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n    return correct_predictions.double() / n_examples, np.mean(losses)\n\ndef eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n\n    losses = []\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n\n            # Get model ouptuts\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n            return_dict=True)\n\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n\n    return correct_predictions.double() / n_examples, np.mean(losses) \n```"]