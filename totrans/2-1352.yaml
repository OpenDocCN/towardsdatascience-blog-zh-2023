- en: 'Introduction to ML Deployment: Flask, Docker & Locust'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习部署简介：Flask、Docker与Locust
- en: 原文：[https://towardsdatascience.com/introduction-to-ml-deployment-flask-docker-locust-b87b5bd78a17](https://towardsdatascience.com/introduction-to-ml-deployment-flask-docker-locust-b87b5bd78a17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/introduction-to-ml-deployment-flask-docker-locust-b87b5bd78a17](https://towardsdatascience.com/introduction-to-ml-deployment-flask-docker-locust-b87b5bd78a17)
- en: Learn how to deploy your models in Python and measure the performance using
    Locust
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何在Python中部署你的模型并使用Locust测量性能
- en: '[](https://medium.com/@antonsruberts?source=post_page-----b87b5bd78a17--------------------------------)[![Antons
    Tocilins-Ruberts](../Images/363a4f32aa793cca7a67dea68e76e3cf.png)](https://medium.com/@antonsruberts?source=post_page-----b87b5bd78a17--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b87b5bd78a17--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b87b5bd78a17--------------------------------)
    [Antons Tocilins-Ruberts](https://medium.com/@antonsruberts?source=post_page-----b87b5bd78a17--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@antonsruberts?source=post_page-----b87b5bd78a17--------------------------------)[![Antons
    Tocilins-Ruberts](../Images/363a4f32aa793cca7a67dea68e76e3cf.png)](https://medium.com/@antonsruberts?source=post_page-----b87b5bd78a17--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b87b5bd78a17--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b87b5bd78a17--------------------------------)
    [Antons Tocilins-Ruberts](https://medium.com/@antonsruberts?source=post_page-----b87b5bd78a17--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b87b5bd78a17--------------------------------)
    ·9 min read·Feb 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b87b5bd78a17--------------------------------)
    ·阅读时间9分钟·2023年2月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b57994b78fc248ef65b80754cfd3deaf.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b57994b78fc248ef65b80754cfd3deaf.png)'
- en: Photo by [İsmail Enes Ayhan](https://unsplash.com/@ismailenesayhan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 摄影师 [İsmail Enes Ayhan](https://unsplash.com/@ismailenesayhan?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 上的照片
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: You’ve spent a lot of time on EDA, carefully crafted your features, tuned your
    model for days and finally have something that performs well on the test set.
    Now what? Now, my friend, we need to deploy the model. After all, any model that
    stays in the notebook has a value of zero, regardless of how good it is.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经在EDA上花费了大量时间，精心设计了特征，调优了模型好几天，终于在测试集上得到了表现良好的结果。那么接下来呢？现在，朋友，我们需要部署这个模型。毕竟，任何停留在笔记本中的模型都没有价值，不论它有多好。
- en: It might feel overwhelming to learn this part of the data science workflow,
    especially if you don’t have a lot of software engineering experience. Fear not,
    this post’s main purpose is to get you started by introducing one of the most
    popular frameworks for deployment in Python — Flask. In addition, you’ll learn
    how to containerise the deployment and measure its performance, two steps that
    are frequently overlooked.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 学习数据科学工作流的这一部分可能会让人感到不知所措，尤其是当你没有太多软件工程经验时。不过不用担心，这篇文章的主要目的是通过介绍Python中最流行的部署框架之一——Flask，帮助你入门。此外，你还将学习如何对部署进行容器化并测量其性能，这两个步骤常常被忽视。
- en: What is “deployment” anyway?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “部署”到底是什么？
- en: First thing first, let’s clarify what I mean by deployment in this post. ML
    deployment is the process of taking a trained model and integrating it into a
    production system (server in the diagram below), making it available for use by
    end-users or other systems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们澄清一下在这篇文章中“部署”是什么意思。机器学习部署是将训练好的模型整合到生产系统（下图中的服务器）中的过程，使其可以供最终用户或其他系统使用。
- en: '![](../Images/184017a1e42ba74800745abcd5f11ed5.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/184017a1e42ba74800745abcd5f11ed5.png)'
- en: Model deployment diagram. Image by author.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署图。图片来源：作者。
- en: Keep in mind that in reality, deployment process is much more complicated than
    simply making the model available to end-users. It also involves service integration
    with other systems, selection of an appropriate infrastructure, load balancing
    and optimisation, and robust testing of all of these components. Most of these
    steps are out-of-scope for this post and should ideally be handled by experienced
    software/ML engineers. Nevertheless, it’s important to have some understanding
    around these areas which is why this post will cover containerisation, inference
    speed testing, and load handling.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，实际的部署过程比简单地使模型对最终用户可用要复杂得多。它还涉及与其他系统的服务集成、选择适当的基础设施、负载均衡和优化，以及对所有这些组件的全面测试。这些步骤中的大多数超出了本帖的范围，理想情况下应由经验丰富的软件/ML
    工程师处理。然而，了解这些领域的一些内容还是很重要的，这就是为什么本帖将涵盖容器化、推理速度测试和负载处理。
- en: Setup
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置
- en: All the code can be found in this GitHub [repo](https://github.com/aruberts/tutorials/tree/main/deployment).
    I’ll show fragments from it, but make sure to pull it and experiment with it,
    that’s the best way to learn. To run the code you’ll need — `docker` , `flask`
    , `fastapi` , and `locust` installed. There might be some additional dependencies
    to install, depending on the environment you’re running this code in.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码可以在这个 GitHub [repo](https://github.com/aruberts/tutorials/tree/main/deployment)
    中找到。我将展示其中的一些片段，但请确保拉取并进行实验，这才是最好的学习方式。要运行代码，你需要安装 `docker`、`flask`、`fastapi`
    和 `locust`。根据你运行此代码的环境，可能还需要安装一些额外的依赖。
- en: Project Overview
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目概述
- en: To make the learning more practical, this post will show you a simple demo deployment
    of a loan default prediction model. The model training process is out of scope
    for this post, so already trained and serialised CatBoost model is available in
    the GitHub [repo](https://github.com/aruberts/tutorials/tree/main/deployment).
    The model was trained on the pre-processed [U.S. Small Business Administration
    dataset](https://www.kaggle.com/datasets/mirbektoktogaraev/should-this-loan-be-approved-or-denied)
    (CC BY-SA 4.0 license). Feel free to explore the data dictionary to understand
    what each of the columns mean.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使学习更加实际，本帖将展示一个简单的贷款违约预测模型的演示部署。模型训练过程不在本帖范围内，因此已经训练并序列化的 CatBoost 模型可以在 GitHub
    [repo](https://github.com/aruberts/tutorials/tree/main/deployment) 中找到。该模型在预处理过的
    [美国小企业管理局数据集](https://www.kaggle.com/datasets/mirbektoktogaraev/should-this-loan-be-approved-or-denied)（CC
    BY-SA 4.0 许可证）上训练。请随意浏览数据字典以了解每一列的含义。
- en: 'This project focuses mostly on the serving part i.e. making the model available
    to other systems. Hence, the model will actually be deployed on your local machine
    which is good for testing but is suboptimal for the real world. Here are the main
    steps that deployments for Flask and FastAPI will follow:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目主要集中在服务部分，即使模型对其他系统可用。因此，模型实际上将在你的本地机器上部署，这对于测试来说是好的，但在现实世界中并不理想。Flask 和
    FastAPI 的部署将遵循以下主要步骤：
- en: Create API endpoint (using Flask or FastAPI)
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 API 端点（使用 Flask 或 FastAPI）
- en: Containerise the application (endpoint) using Docker
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Docker 对应用程序（端点）进行容器化
- en: Run the Docker image locally, creating a server
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地运行 Docker 镜像，创建一个服务器
- en: Test the server performance
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试服务器性能
- en: '![](../Images/65fedce1efd4c8aa8517cdbb8b8f4f9d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65fedce1efd4c8aa8517cdbb8b8f4f9d.png)'
- en: Project flow diagram. Image by author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 项目流程图。图片由作者提供。
- en: Sounds exciting, right? Well, let’s get started then!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很激动人心，对吧？那么，我们开始吧！
- en: What is Flask?
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Flask？
- en: Flask is a popular and widely adopted web framework for Python due to its lightweight
    nature and minimal installation requirements. It offers a straightforward approach
    to developing REST APIs that are ideal for serving machine learning models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Flask 是一个流行且广泛采用的 Python 网络框架，由于其轻量级特性和最低的安装要求而受到青睐。它提供了一种简单的方法来开发 REST API，非常适合服务机器学习模型。
- en: The typical workflow for Flask involves defining a prediction HTTP endpoint
    and linking it to specific Python functions that receive data as input and generate
    predictions as output. This endpoint can then be accessed by users and other applications.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Flask 的典型工作流程包括定义一个预测 HTTP 端点，并将其链接到接收数据作为输入并生成预测作为输出的特定 Python 函数。用户和其他应用程序可以访问这个端点。
- en: Create Flask App
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 Flask 应用
- en: If you’re interested in simply creating a prediction endpoint, it’s going to
    be quite simple. All you need to do is to deserialise the model, create the `Flask`
    application object, and specify the prediction endpoint with `POST` method. More
    information about `POST` and other methods you can find [here](https://pythonbasics.org/flask-http-methods/).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只是想创建一个预测端点，那会非常简单。你需要做的就是反序列化模型，创建`Flask`应用对象，并用`POST`方法指定预测端点。关于`POST`和其他方法的更多信息，可以在[这里](https://pythonbasics.org/flask-http-methods/)找到。
- en: The most important part of the code above is the `predict` function. It reads
    the json input which in this case is a bunch of attributes describing a loan application.
    It then takes this data, transforms it to the DataFrame, and passes it through
    the model. The resulting probability of a default is then formatted back into
    json and returned. When this app is deployed locally, we can get the prediction
    by sending a request with json-formatted data to `http://0.0.0.0:8989/predict`
    url. Let’s try it out! To launch the server, we can simply run the Python file
    with the command below.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码中最重要的部分是`predict`函数。它读取json输入，在这个例子中是描述贷款申请的一堆属性。然后，它将这些数据转换为DataFrame，并通过模型处理。最终的违约概率被格式化回json并返回。当这个应用程序在本地部署时，我们可以通过向`http://0.0.0.0:8989/predict`网址发送json格式的数据来获取预测。我们来试试吧！要启动服务器，我们可以简单地运行下面的Python文件。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/ad4f024964d8a4c64e23625f5694ea5f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad4f024964d8a4c64e23625f5694ea5f.png)'
- en: Expected output. Screenshot by author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出。作者提供的截图。
- en: When this command is run you should the message that you app is running at the
    `[http://0.0.0.0:8989/](http://0.0.0.0:8989/)` address. For now, let’s ignore
    a big red warning and test the app. To check if the app is working as expected,
    we can send a test request (loan application data) to the app and see if we get
    a response (default probability prediction) in return.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行这个命令时，你应该会看到消息，显示你的应用程序正在`[http://0.0.0.0:8989/](http://0.0.0.0:8989/)`地址上运行。目前，我们暂时忽略那个大红色的警告，测试一下应用程序。为了检查应用程序是否按预期工作，我们可以向应用程序发送一个测试请求（贷款申请数据），看看是否会收到响应（违约概率预测）。
- en: If you managed to get a response with probability — congrats! You’ve deployed
    the model using your own computer as a server. Now, let’s kick it up a notch and
    package your deployment app using Docker.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你成功获得了概率响应——恭喜！你已经使用自己的计算机作为服务器部署了模型。现在，让我们更进一步，使用Docker打包你的部署应用。
- en: Containerise Flask App
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器化Flask应用
- en: Containerisation is the process of encapsulating your application and all of
    its dependencies (including Python) into a self-contained, isolated package that
    can run consistently across different environments (e.g. locally, in the cloud,
    on your friend’s laptop, etc.). You can achieve this with Docker, and all you
    need to do is to correctly specify the Dockerfile, build the image and then run
    it. Dockerfile gives instructions to your container e.g. which version of Python
    to use, which packages to install, and which commands to run. There’s a great
    [video tutorial](https://www.youtube.com/watch?v=0qG_0CPQhpg&t=138s) about Docker
    if you’re interested to find out more.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 容器化是将应用程序及其所有依赖项（包括Python）封装到一个自包含、隔离的包中，以便在不同环境（例如本地、云端、朋友的笔记本电脑等）中一致运行的过程。你可以使用Docker来实现这一点，你需要做的就是正确指定Dockerfile，构建镜像，然后运行它。Dockerfile给你的容器提供指令，例如使用哪个版本的Python、安装哪些包和运行哪些命令。如果你想了解更多，关于Docker有一个很棒的[视频教程](https://www.youtube.com/watch?v=0qG_0CPQhpg&t=138s)。
- en: Here’s how it can look like for the Flask application above.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是Flask应用程序的可能外观。
- en: Now, we can build the image using `docker build` command.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`docker build`命令构建镜像。
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`-t` gives you a option to name your docker image and provide a tag for it,
    so this image’s name is `deafult-service` with a tag of `v01` . The dot at the
    end refers to the PATH argument that needs to be provided. It’s the location of
    your model, application code, etc. Since I assume that you’re building this image
    in the directory with all the code, PATH is set to `.` which means current directory.
    It might take some time to build this image but once it’s done, you should be
    able to see it when you run `docker images` .'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`-t`选项允许你为你的docker镜像命名并提供一个标签，因此这个镜像的名称是`deafult-service`，标签是`v01`。末尾的点表示需要提供的PATH参数。它是模型、应用程序代码等的位置。由于我假设你是在包含所有代码的目录中构建这个镜像的，因此PATH设置为`.`，即当前目录。构建这个镜像可能需要一些时间，但完成后，你应该能够在运行`docker
    images`时看到它。'
- en: 'Let’s run the Dockerised app using the following command:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令运行 Docker 化的应用：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`-it` flag makes the Docker image run in an interactive mode, meaning that
    you’ll be able to see the code logs in the shell and to stop the image when needed
    using Ctrl+C. `--rm` ensures that the container is automatically removed when
    you stop the image. Finally, `-p` makes the ports from inside Docker image available
    outside of it. The command above maps port 8989 from within Docker to the localhost,
    making our endpoint available at the same address.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`-it`标志使 Docker 镜像以交互模式运行，这意味着你可以在 shell 中查看代码日志，并在需要时使用 Ctrl+C 停止镜像。`--rm`
    确保在你停止镜像时容器会被自动移除。最后，`-p` 使 Docker 镜像内的端口对外部可用。上面的命令将 Docker 内部的 8989 端口映射到本地主机，使我们的端点在相同的地址上可用。'
- en: Test Flask App
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试 Flask 应用
- en: Now that our model is successfully deployed using Flask and the deployment container
    is up and running (at least locally), it’s time to evaluate its performance. At
    this point, our focus is on serving metrics such as response time and the server’s
    capability to handle requests per second, rather than ML metrics like RMSE or
    F1 score.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已经成功部署，使用 Flask 部署容器已经启动并运行（至少在本地），是时候评估它的性能了。此时，我们的重点是服务指标，如响应时间和服务器处理请求的能力，而不是像
    RMSE 或 F1 分数这样的机器学习指标。
- en: Testing Using Script
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用脚本进行测试
- en: To obtain a rough estimation of response latency, we can create a script that
    sends several requests to the server and measure the time taken (usually in milliseconds)
    for the server to return a prediction. However, it’s important to note that the
    response time is not constant, so we need to measure the median latency to estimate
    the time users usually wait to receive a response, and the 95th latency percentile
    to measure the worst-case scenarios.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得响应延迟的大致估计，我们可以创建一个脚本，向服务器发送多个请求，并测量服务器返回预测的时间（通常以毫秒为单位）。然而，需要注意的是，响应时间不是恒定的，因此我们需要测量中位延迟，以估计用户通常等待响应的时间，以及
    95th 延迟百分位数，以测量最坏情况。
- en: This code resides in `measure_response.py` , so we can simply run this python
    file to measure these latency metrics.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码位于 `measure_response.py` 文件中，因此我们可以简单地运行此 Python 文件来测量这些延迟指标。
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/3cdd4664fe35f46545d8d239bdfda2e8.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cdd4664fe35f46545d8d239bdfda2e8.png)'
- en: Latency metrics. Screenshot by author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟指标。截图由作者提供。
- en: The median response time turned out to be 9 ms, but the worst case scenario
    is more than 10x this time. If this performance is satisfactory or not is up to
    you and the product manager but at least now you’re aware of these metrics and
    can work further to improve them.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 中位响应时间为 9 毫秒，但最坏的情况是这个时间的 10 倍以上。这个性能是否令人满意由你和产品经理决定，但至少现在你已经了解了这些指标，并可以进一步改进它们。
- en: Testing Using Locust
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Locust 进行测试
- en: '[Locust](https://docs.locust.io/en/stable/what-is-locust.html) is a Python
    package designed to test the performance and scalability of web applications.
    We’re going to use Locust to generate a more advanced testing scenario since it
    allows to configure parameters like the number of users (i.e. loan applicants)
    per second.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[Locust](https://docs.locust.io/en/stable/what-is-locust.html) 是一个用于测试 web
    应用性能和可扩展性的 Python 包。我们将使用 Locust 生成一个更高级的测试场景，因为它允许配置每秒用户数量（即贷款申请者）的参数。'
- en: First things first, the package can be installed by running `pip install locust`
    in your terminal. Then, we need to define a test scenario which will specify what
    our imaginary user will perform with our server. In our case it’s quite straightforward
    — the user will send us a request with the (json formatted) information about
    their loan application and will receive a response from our deployed model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，运行 `pip install locust` 可以安装该包。然后，我们需要定义一个测试场景，以指定我们的虚拟用户将如何与我们的服务器进行交互。在我们的案例中，这非常简单——用户将向我们发送一个包含其贷款申请的（JSON
    格式的）信息请求，并从我们部署的模型中接收响应。
- en: As you can see, the Locust task is very similar to a test ping that we did above.
    The only difference is that it needs to be wrapped in a class that inherits from
    `locust.HttpUser` and the performed task (send data and get response) needs to
    be decorated with `@task` .
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Locust 任务与我们之前做的测试 ping 非常相似。唯一的区别是它需要被封装在一个继承自`locust.HttpUser`的类中，并且执行的任务（发送数据并获取响应）需要用`@task`进行装饰。
- en: To statrt load testing we simply need to run the command below.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始负载测试，我们只需运行下面的命令。
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When it launches, you’ll be able to access the testing UI at `http://0.0.0.0:8089`
    where you’ll need to specify the application’s URL, number of users and spawn
    rate.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 启动后，你将能够访问测试 UI，网址为 `http://0.0.0.0:8089`，在这里你需要指定应用的 URL、用户数量和生成速率。
- en: '![](../Images/bb95bf598de56bb0f574b4d1665673a2.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb95bf598de56bb0f574b4d1665673a2.png)'
- en: Locust UI. Screenshot by author.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Locust UI。截图由作者提供。
- en: Spawn rate of 5 and with 100 users means that every second there will be 5 new
    users sends requests to your app, until their number reaches 100\. This means
    that at its peak, our app will need to handle 100 requests per second. Now, let’s
    click the **Start swarming** button and move to the charts section of the UI.
    Below I’m going to present results for my machine but they’ll certainly be different
    to yours, so make sure to run this on your own as well.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 生成速率为 5，用户数为 100，意味着每秒将有 5 个新用户向你的应用发送请求，直到用户数量达到 100。这意味着在峰值时，我们的应用需要处理每秒 100
    个请求。现在，让我们点击 **开始压力测试** 按钮并转到 UI 的图表部分。下面我将展示我机器上的结果，但它们肯定与你的不同，所以请确保也在你自己的机器上运行测试。
- en: '![](../Images/24e3d0775540ab8e8c6629b8fbeadd81.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24e3d0775540ab8e8c6629b8fbeadd81.png)'
- en: Locust 100 users test visualisation. Screenshot by author.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Locust 100 用户测试可视化。截图由作者提供。
- en: You’ll see that as the traffic builds up, your response time will get slower.
    There are going to be some occasional peaks as well, so it’s important to understand
    when they happen and why. Most importantly, Locust helps us understand that our
    local server can handle 100 requests per second with median response time of ~250ms.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，随着流量的增加，你的响应时间会变得越来越慢。还会有一些偶发的峰值，因此理解这些峰值发生的时间和原因是很重要的。最重要的是，Locust 帮助我们了解我们的本地服务器可以处理每秒
    100 个请求，响应时间中位数约为 ~250ms。
- en: We can keep stress testing our app and identify the load that it cannot manage.
    For this, let’s increase the number of users to 1000 to see what happens.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续对应用进行压力测试，并识别出它无法处理的负载。为此，我们将用户数量增加到 1000 以查看会发生什么。
- en: '![](../Images/3ee590e409dd2cdb7b504ea5118f2e3b.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ee590e409dd2cdb7b504ea5118f2e3b.png)'
- en: Locust 1000 users test visualisation. Screenshot by author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Locust 1000 用户测试可视化。截图由作者提供。
- en: Looks like the breaking point of my local server is ~ 180 concurrent users.
    This is an important piece of information that we were able to extract using Locust.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我本地服务器的崩溃点大约是 ~180 个并发用户。这是我们通过 Locust 提取出的重要信息。
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Good job for getting this far! I hope that this post has provided you with a
    practical and insightful introduction to model deployment. By following this project
    or adapting it to your specific model, you should now have a thorough comprehension
    of the essential steps involved in model deployment. Specifically, you have gained
    knowledge on creating REST API endpoints for your model using Flask, containerising
    them with Docker, and systematically testing these endpoints using Locust.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好，已经走到这一步了！我希望这篇文章为你提供了一个实用且富有见地的模型部署介绍。通过跟随这个项目或将其调整到你的特定模型，你现在应该对模型部署的关键步骤有了全面的理解。具体来说，你已经掌握了使用
    Flask 创建 REST API 端点、用 Docker 容器化这些端点，并使用 Locust 系统地测试这些端点的方法。
- en: In the next post, I’ll be covering FastAPI, BentoML, cloud deployment and much
    more so make sure to subscribe, clap, and leave a comment if something is unclear.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一篇文章中，我将介绍 FastAPI、BentoML、云部署以及更多内容，因此请确保订阅、点赞，并在有疑问时留下评论。
