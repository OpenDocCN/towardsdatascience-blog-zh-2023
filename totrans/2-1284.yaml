- en: Hyperparameter Optimization — Intro and Implementation of Grid Search, Random
    Search and Bayesian Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a](https://towardsdatascience.com/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Most common hyperparameter optimization methodologies to boost machine learning
    outcomes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)
    ·10 min read·Mar 13, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13056882b4fc4ae3b84019b4de503886.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jonas Jaeken](https://unsplash.com/@jonasjaekenmedia) on [Unsplash](https://unsplash.com/photos/Gg2ttawakqE)
  prefs: []
  type: TYPE_NORMAL
- en: Usually the first solution that comes to mind when trying to improve a machine
    learning model is to just add more training data. Additional data usually helps
    (barring certain situations) but generating high-quality data can be quite expensive.
    Hyperparameter optimization can save us time and resources by getting the best
    model performance using the existing data.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter optimization, as the name suggests, is the process of identifying
    the best combination of hyperparameters for a machine learning model to satisfy
    an optimization function (i.e. maximize the performance of the model, given the
    data set in study). In other words, each model comes with multiple knobs and levers
    that we can change, until we get to the optimized combination. A few examples
    of parameters that we can change during hyperparameter optimization can be learning
    rate, architecture of a neural network (e.g. number of hidden layers), regularization,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we are going to conceptually walk through the three most common
    hyperparameter optimization methodologies, namely Grid Search, Random Search and
    Bayesian Optimization, and then implement them.
  prefs: []
  type: TYPE_NORMAL
- en: I am going to include a high-level comparison table here for future reference
    and then each one will be further explored, explained and implemented in the remainder
    of the post.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b18f9d1f656e46b1e954d189de413ca5.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 1 — Hyperparameter Optimization Methodology Comparison
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: '*(All images, unless otherwise noted, are by the author.)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fmnobar/membership?source=post_page-----b2f16c00578a--------------------------------)
    [## Join Medium with my referral link - Farzad Mahmoodinobar'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Farzad (and other writers on Medium). Your membership
    fee directly supports Farzad and other…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@fmnobar/membership?source=post_page-----b2f16c00578a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Grid Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Grid search is probably the simplest and most intuitive method of hyperparameter
    optimization, which involves exhaustively searching for the best combination of
    hyperparameters in the defined search space. “Search Space” in this context is
    the entire hyperparameters and the values for such hyperparameters that are bing
    considered in the optimization process. Let’s walk through an example to better
    understand Grid Search.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume we have a machine learning model that has only three parameters,
    each of which can take the values provided in the lists below:'
  prefs: []
  type: TYPE_NORMAL
- en: parameter_1 = [1, 2, 3]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: parameter_2 = [a, b, c]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: parameter_3 = [x, y, z]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We do not know which combination of these parameters will optimize our model’s
    optimization function (i.e. provide the best output for our machine learning model).
    In Grid Search we simply try every single combination of these parameters, measure
    the performance of the model for each and then simply pick the combination generating
    the best performance! In this example, parameter_1 can take 3 values (i.e. 1,
    2 or 3), parameter_2 can take 3 values (i.e. a, b and c) and parameter_3 can take
    3 values (i.e. x, y and z). In other words, there are 3*3*3=27 combinations in
    total. Grid Search in this example will involve 27 rounds of evaluating the performance
    of the ML model to find the best performing combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, this approach is very simple (akin to a trial-and-error task)
    but it also has some limitations. Let’s summarize advantages and disadvantages
    together:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Easy to understand and implement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to parallelize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works well for discrete and continuous spaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Expensive in large and/or complex models with larger number of hyperparameters
    (since all combinations will have to be tried and evaluated)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memoryless — Does not learn from past observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May not find the optimal combination if the search space is too large
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: My recommendation is that if you have a simple model with a small search space,
    use the Grid Search. Otherwise, keep reading to find solutions better suited for
    larger search spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement Grid Search with a real example.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1\. Grid Search — Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To implement Grid Search, we will create a random forest classification model
    using the [Iris data set from scikit-learn](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html).
    This data set includes 3 different irises petal and sepal length, which will be
    used in this classification exercise. For the purposes of this post, model development
    is of the secondary importance, since the objective is to compare performance
    of various hyperparameter optimization strategies . I encourage you to focus on
    the model evaluation results and the time it takes for each of the hyperparameter
    optimization methodologies to reach their selected set of hyperparameters. I will
    describe the results and then will provide a summary comparison table for the
    three methodologies used in this post.
  prefs: []
  type: TYPE_NORMAL
- en: 'The search space, which includes all the values of the hyperparameters is defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Above search space consists of 4*5*3*3=180 total combinations of hyperparameters.
    We will use Grid Search to find the combination which optimizes the objective
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61c82cd7f6518083c1393774739732a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Grid Search Results
  prefs: []
  type: TYPE_NORMAL
- en: Here we can see the hyperparameter values selected using Grid Search. `best_score`
    describes the evaluation results using the selected set of hyperparameters and
    `elapsed_time` describes the time it took my local laptop to execute this hyperparameter
    optimization strategy. Keep in mind the evaluation results and the elapsed time
    for the sake of comparison when we get to the next methods. For now, let’s move
    on to Random Search.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Random Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random Search, as the name suggests, is the process of randomly sampling hyperparameters
    from a defined search space. As opposed to Grid Search which exhaustively goes
    through every single combination of hyperparameters’ values, Random Search only
    selects a random subset of hyperparameter values for a pre-defined number of iterations
    (depending on the available resources such as time, budget, objective, etc.),
    calculates the machine learning model’s performance for each and then selects
    the best one.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can imagine based on the approached described above, Random Search is
    less expensive compared to a full Grid Search but still has its own advantages
    and disadvantages, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Easy to understand and implement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to parallelize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works for discrete and continuous spaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less expensive than Grid Search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More likely to converge to the optimum compared to Grid Search with the same
    number of attempts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Memoryless — Does not learn from past observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May miss important hyperparameter values, given the random selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next method, we will tackle the “memoryless” disadvantage of Grid and
    Random Search, through Bayesian Optimization. But before getting there, let’s
    implement Random Search.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Random Search — Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the code snippet below, we will implement a Random Search hyperparameter
    optimization for the same problem described in the Grid Search implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f04c7a1231b6b6527e305b5be18eb388.png)'
  prefs: []
  type: TYPE_IMG
- en: Random Search Results
  prefs: []
  type: TYPE_NORMAL
- en: These results are quite interesting, when compared to the results of Grid Search.
    The `best_score` remains unchanged but the `elapsed_time` was decreased from 352.0
    to 75.5 seconds! That’s quite impressive! In other words, Random Search managed
    to find a set of hyperparameters that yielded the same level of performance as
    Grid Search, in around 21% of the time required for a Grid Search! That’s much
    more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s move on to our next approach, called Bayesian Optimization, which
    learns from each attempt during optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Bayesian Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayesian Optimization is a hyperparameter optimization methodology that uses
    probabilistic models to “learn” from previous attempts and to guide the search
    towards the optimum combination of hyperparameters in the search space that optimizes
    the machine learning model’s objective function.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Optimization approach can be broken down into 4 steps, which I will
    describe below. I encourage you to read through the steps for a better understanding
    of the process but knowledge is not a requirement for using this method.
  prefs: []
  type: TYPE_NORMAL
- en: Define a “Prior”, which is a probabilistic model about our belief at a point
    in time about the most likely combination of hyperparameters to optimize the objective
    function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the model for a sample of hyperparameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the knowledge gained in Step 2, update the probabilistic model from Step
    1 (i.e. what we called “Prior”) about where we believe the most likely combination
    of hyperparameters to optimize the objective function is. Our updated belief is
    called the “Posterior”. In other words, knowledge gained in Step 2 helped us have
    a better understanding about the search space and took us from Prior to Posterior,
    making Posterior our “latest” knowledge about the search space and the objective
    function, informed by Step 2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat Steps 2 and 3, until the performance of the model converges, resources
    are exhausted or some other pre-defined metric is met
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you are interested in learning more about the details of Bayesian Optimization,
    you can look at the following post:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fmnobar/conceptual-overview-of-bayesian-optimization-for-parameter-tuning-in-machine-learning-a3b1b4b9339f?source=post_page-----b2f16c00578a--------------------------------)
    [## Bayesian Optimization in Machine Learning'
  prefs: []
  type: TYPE_NORMAL
- en: This post is about hyperparameter optimization through Bayesian Optimization.
    The task intended to help choose a set of…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@fmnobar/conceptual-overview-of-bayesian-optimization-for-parameter-tuning-in-machine-learning-a3b1b4b9339f?source=post_page-----b2f16c00578a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how Bayesian Optimization works, let’s take a look at
    its advantages and disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Learns from past observations and is hence more efficient — in other words,
    it is expected to find a better set of hyperparameters in fewer iterations, compared
    to memoryless methodologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converges to an optimum, given certain assumptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Difficult to parallelize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computationally heavier than Grid and Random Search per iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choice of the initial probabilistic distribution of the Prior and functions
    used in Bayesian Optimization (e.g. acquisition function, etc.) can significantly
    impact the performance and its learning curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the details out of the way, let’s implement Bayesian Optimization and look
    at the results.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Bayesian Optimization — Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the previous section, we will use the code snippet below to implement
    a Bayesian Optimization hyperparameter optimization for the same problem described
    in the Grid Search implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee556232c9fa4edfb53602442440ce2a.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayesian Optimization Results
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting set of results! `best_score` remained the same as what we
    were able to achieve through Grid and Random Search but the results were achieved
    in only 23.1 seconds, compared to 75.5 seconds for Random Search and 352.0 seconds
    for Grid Search! In other words, the time required using Bayesian Optimization
    was around 93% less than what was required in a Grid Search. That is a large productivity
    gain, which becomes much more meaningful in larger and more complex models and
    search spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Note that Bayesian Optimization achieved these results using only 10 iterations,
    because it can learn from previous iterations (as opposed to Random and Grid Search).
  prefs: []
  type: TYPE_NORMAL
- en: Results Comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Below table is a comparison of the results of the three approaches discussed.
    “Methodology” column describes the hyperparameter optimization approach used.
    It is followed by the hyperparameters selected using each methodology. “Best Score”
    is the score achieved using that specific approach, followed by “Elapsed Time”,
    which indicates the time it took for the optimization strategy to run on my local
    laptop. Last column, “Gained Efficiency”, assumes Grid Search as the baseline
    and then calculates the efficiency gained (using elapsed time) for each of the
    other two approaches, compared to Grid Search. For example, since Random Search
    took 75.5 seconds, while Grid Search took 352.0 seconds, the gained efficiency
    for Random Search relative to the baseline of Grid Search is calculated as 1–75.5/352.0=78.5%.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d70ae1e3b7131f0d71d705dbee86243.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 2 — Methodology Performance Comparison Table
  prefs: []
  type: TYPE_NORMAL
- en: 'Two main takeaways from the comparison table above:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Efficiency:** We can see how learned approaches such as Bayesian Optimization
    can find an optimized set of hyperparameters in a shorter period of time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Parameter Selection:** There can be more than one correct answer. For example,
    the selected parameters for Bayesian Optimization are different than those for
    Grid and Random Search, although the evaluation metric (i.e. `best_score`) remain
    the same. This can be even more important in larger and more complex settings.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we talked about what hyperparameter optimization is and introduced
    three of the most common methodologies used for this optimization exercise. Then
    we walked through each of these three methodologies in detail and implemented
    them in a classification exercise. Lastly, we compared the results of implementing
    these three methodologies. We found out how approaches such as Bayesian Optimization
    that can learn from previous attempts can be significantly more efficient, which
    can be an important factor in large and complex models (e.g. deep neural networks),
    where efficiency can be a deciding factor.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for Reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you found this post helpful, please [follow me on Medium](https://medium.com/@fmnobar)
    and subscribe to receive my latest posts!
  prefs: []
  type: TYPE_NORMAL
