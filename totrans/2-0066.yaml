- en: '3D Deep Learning Python Tutorial: PointNet Data Preparation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3Dæ·±åº¦å­¦ä¹ Pythonæ•™ç¨‹ï¼šPointNetæ•°æ®å‡†å¤‡
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f](https://towardsdatascience.com/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f](https://towardsdatascience.com/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f)
- en: Hands-On Tutorial, Deep Dive, 3D Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®è·µæ•™ç¨‹ï¼Œæ·±åº¦æ¢è®¨ï¼Œ3D Python
- en: The Ultimate Python Guide to structure large LiDAR point cloud for training
    a 3D Deep Learning Semantic Segmentation Model with the PointNet Architecture.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ã€Šç»ˆæPythonæŒ‡å—ï¼šä¸ºPointNetæ¶æ„è®­ç»ƒ3Dæ·±åº¦å­¦ä¹ è¯­ä¹‰åˆ†å‰²æ¨¡å‹è€Œæ„å»ºå¤§å‹LiDARç‚¹äº‘ã€‹
- en: '[](https://medium.com/@florentpoux?source=post_page-----90398f880c9f--------------------------------)[![Florent
    Poux, Ph.D.](../Images/74df1e559b2edefba71ffd0d1294a251.png)](https://medium.com/@florentpoux?source=post_page-----90398f880c9f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----90398f880c9f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----90398f880c9f--------------------------------)
    [Florent Poux, Ph.D.](https://medium.com/@florentpoux?source=post_page-----90398f880c9f--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@florentpoux?source=post_page-----90398f880c9f--------------------------------)[![Florent
    Poux, Ph.D.](../Images/74df1e559b2edefba71ffd0d1294a251.png)](https://medium.com/@florentpoux?source=post_page-----90398f880c9f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----90398f880c9f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----90398f880c9f--------------------------------)
    [Florent Poux, Ph.D.](https://medium.com/@florentpoux?source=post_page-----90398f880c9f--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----90398f880c9f--------------------------------)
    Â·30 min readÂ·May 31, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----90398f880c9f--------------------------------)
    Â·30åˆ†é’Ÿé˜…è¯»Â·2023å¹´5æœˆ31æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/23e707b7b223e287573f055b8cbb2a60.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23e707b7b223e287573f055b8cbb2a60.png)'
- en: This creative illustration visually highlights how 3D Deep Learning could represent
    a top-down scene in a way it is easy to separate between classes. If you like
    these, contact [Marina TÃ¼nsmeyer](http://mimatelier.com/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¼ åˆ›æ„æ’å›¾ç›´è§‚åœ°çªå‡ºäº†3Dæ·±åº¦å­¦ä¹ å¦‚ä½•ä»¥æ˜“äºåˆ†ç±»çš„æ–¹å¼è¡¨ç°è‡ªä¸Šè€Œä¸‹çš„åœºæ™¯ã€‚å¦‚æœä½ å–œæ¬¢è¿™äº›ï¼Œè”ç³»[Marina TÃ¼nsmeyer](http://mimatelier.com/)ã€‚
- en: 'The application field of 3D deep learning has snowballed in recent years. We
    have superb applications in various areas, including robotics, autonomous driving
    & mapping, medical imaging, and entertainment. When we look at the results, we
    are often awed (but not all the time ğŸ˜), and we may think: â€œI will use this model
    right now for my application!â€. But unfortunately, the nightmare begins: 3D Deep
    Learning implementation. Even if new coding libraries aim at simplifying the process,
    implementing an end-to-end 3D Deep Learning model is a feat, especially if you
    are isolated in a dark corner of some gloomy cave.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼Œ3Dæ·±åº¦å­¦ä¹ çš„åº”ç”¨é¢†åŸŸè¿…é€Ÿæ‰©å±•ã€‚æˆ‘ä»¬åœ¨æœºå™¨äººæŠ€æœ¯ã€è‡ªåŠ¨é©¾é©¶ä¸åœ°å›¾åˆ¶ä½œã€åŒ»å­¦æˆåƒå’Œå¨±ä¹ç­‰å„ä¸ªé¢†åŸŸéƒ½æ‹¥æœ‰å“è¶Šçš„åº”ç”¨ã€‚çœ‹åˆ°è¿™äº›ç»“æœæ—¶ï¼Œæˆ‘ä»¬å¸¸å¸¸æ„Ÿåˆ°æƒŠå¹ï¼ˆä½†å¹¶éæ€»æ˜¯å¦‚æ­¤ğŸ˜ï¼‰ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šæƒ³ï¼šâ€œæˆ‘ç°åœ¨å°±è¦åœ¨æˆ‘çš„åº”ç”¨ä¸­ä½¿ç”¨è¿™ä¸ªæ¨¡å‹ï¼â€ä½†ä¸å¹¸çš„æ˜¯ï¼Œå™©æ¢¦å¼€å§‹äº†ï¼š3Dæ·±åº¦å­¦ä¹ çš„å®ç°ã€‚å³ä½¿æ–°çš„ç¼–ç åº“æ—¨åœ¨ç®€åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œå®ç°ä¸€ä¸ªç«¯åˆ°ç«¯çš„3Dæ·±åº¦å­¦ä¹ æ¨¡å‹ä»æ˜¯ä¸€é¡¹å£®ä¸¾ï¼Œå°¤å…¶æ˜¯å½“ä½ å­¤èº«ä¸€äººå¾…åœ¨æŸä¸ªé˜´æš—çš„è§’è½æ—¶ã€‚
- en: '![](../Images/a17e508f9b1bc782eb13b6b65a6cde85.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a17e508f9b1bc782eb13b6b65a6cde85.png)'
- en: This is how it feels to code 3D Deep Learning. Â© F. Poux
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ç¼–ç 3Dæ·±åº¦å­¦ä¹ çš„æ„Ÿè§‰ã€‚Â© F. Poux
- en: One of the most overlooked pain points in 3D deep learning frameworks is preparing
    the data to **be usable** by a selected learning architecture. I do not mean a
    nice research dataset but an actual (messy) data silo on which you want to develop
    an application. This is even steeper in the case of large and complex 3D point
    cloud datasets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨3Dæ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­ï¼Œæœ€è¢«å¿½è§†çš„ç—›ç‚¹ä¹‹ä¸€æ˜¯å°†æ•°æ®**å‡†å¤‡å¥½**ä»¥ä¾›é€‰å®šçš„å­¦ä¹ æ¶æ„ä½¿ç”¨ã€‚æˆ‘æŒ‡çš„ä¸æ˜¯ä¸€ä¸ªç²¾ç¾çš„ç ”ç©¶æ•°æ®é›†ï¼Œè€Œæ˜¯ä¸€ä¸ªå®é™…çš„ï¼ˆæ··ä¹±çš„ï¼‰æ•°æ®ä»“åº“ï¼Œä½ æƒ³åœ¨å…¶ä¸Šå¼€å‘åº”ç”¨ç¨‹åºã€‚åœ¨å¤§å‹ä¸”å¤æ‚çš„3Dç‚¹äº‘æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªé—®é¢˜å°¤ä¸ºä¸¥å³»ã€‚
- en: 'Oh, but do you see where we are going with this article? You dreamed it (Donâ€™t
    hide it, I know ğŸ˜‰), and we will cover it at the proper coding depth. This hands-on
    tutorial explores how to efficiently prepare 3D point clouds from an Aerial LiDAR
    campaign to be used with the most popular 3D deep learning point-based model:
    the PointNet Architecture.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ï¼Œä½ æ˜¯å¦æ˜ç™½æˆ‘ä»¬è¦åœ¨è¿™ç¯‡æ–‡ç« ä¸­æ¢è®¨ä»€ä¹ˆï¼Ÿä½ æ¢¦åˆ°äº†å®ƒï¼ˆä¸è¦éšè—ï¼Œæˆ‘çŸ¥é“ğŸ˜‰ï¼‰ï¼Œæˆ‘ä»¬å°†æ·±å…¥åˆ°é€‚å½“çš„ç¼–ç æ·±åº¦ã€‚è¿™ç¯‡å®è·µæ•™ç¨‹æ¢è®¨äº†å¦‚ä½•é«˜æ•ˆåœ°å‡†å¤‡ä»èˆªç©ºLiDARæ´»åŠ¨ä¸­è·å¾—çš„3Dç‚¹äº‘ï¼Œä»¥ç”¨äºæœ€æµè¡Œçš„åŸºäºç‚¹çš„3Dæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼šPointNetæ¶æ„ã€‚
- en: We cover the entire data preparation pipeline, from 3D data curation to feature
    extraction and normalization. It provides the knowledge and practical Python skills
    to tackle real-world 3D Deep Learning problems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ¶µç›–äº†æ•´ä¸ªæ•°æ®å‡†å¤‡æµç¨‹ï¼Œä» 3D æ•°æ®æ•´ç†åˆ°ç‰¹å¾æå–å’Œå½’ä¸€åŒ–ã€‚å®ƒæä¾›äº†çŸ¥è¯†å’Œå®é™…çš„ Python æŠ€èƒ½ï¼Œä»¥è§£å†³ç°å®ä¸–ç•Œçš„ 3D æ·±åº¦å­¦ä¹ é—®é¢˜ã€‚
- en: '![](../Images/5e3562ca370d6d278c11635d8b34757d.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e3562ca370d6d278c11635d8b34757d.png)'
- en: The PointNet Data Preparation Workflow for 3D Semantic Segmentation. Â© F. Poux
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: PointNet æ•°æ®å‡†å¤‡å·¥ä½œæµç¨‹ç”¨äº 3D è¯­ä¹‰åˆ†å‰²ã€‚Â© F. Poux
- en: By following this tutorial, you will be able to apply these techniques to your
    own 3D point cloud datasets and use any of them to train a PointNet Semantic Segmentation
    model. Are you ready?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è·Ÿéšæœ¬æ•™ç¨‹ï¼Œä½ å°†èƒ½å¤Ÿå°†è¿™äº›æŠ€æœ¯åº”ç”¨åˆ°ä½ è‡ªå·±çš„ 3D ç‚¹äº‘æ•°æ®é›†ä¸Šï¼Œå¹¶åˆ©ç”¨å®ƒä»¬æ¥è®­ç»ƒ PointNet è¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚ä½ å‡†å¤‡å¥½äº†å—ï¼Ÿ
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'ğŸµ**Note to Readers***: This hands-on guide is part of a* [***UTWENTE***](https://www.itc.nl/)
    *joint work with my dear colleague* [***Prof. Sander Oude Elberink***](https://people.utwente.nl/s.j.oudeelberink)*.
    We acknowledge the financial contribution from the digital twins* [*@ITC*](http://twitter.com/ITC)
    *-project granted by the ITC faculty of the University of Twente.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸµ**è¯»è€…æ³¨æ„**ï¼šæœ¬å®è·µæŒ‡å—æ˜¯ä¸æˆ‘çš„äº²çˆ±çš„åŒäº‹* [***UTWENTE***](https://www.itc.nl/) *åˆä½œçš„ä¸€éƒ¨åˆ†* [***æ¡‘å¾·Â·å¥¥å¾·Â·åŸƒå°”ä¼¯æ—å…‹æ•™æˆ***](https://people.utwente.nl/s.j.oudeelberink)*ã€‚æˆ‘ä»¬æ„Ÿè°¢æ¥è‡ªæ•°å­—åŒèƒèƒ*
    [*@ITC*](http://twitter.com/ITC) *é¡¹ç›®çš„è´¢åŠ¡æ”¯æŒï¼Œè¯¥é¡¹ç›®ç”±ç‰¹æ¸©ç‰¹å¤§å­¦ ITC å­¦é™¢èµ„åŠ©ã€‚*
- en: 3D Deep Learning Essentials
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3D æ·±åº¦å­¦ä¹ è¦ç‚¹
- en: 3D Semantic Segmentation VS Classification
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3D è¯­ä¹‰åˆ†å‰² VS åˆ†ç±»
- en: The fundamental difference between 3D semantic segmentation and classification
    for 3D point clouds is that segmentation aims to assign a label to each point
    in the point cloud. In contrast, classification seeks to assign a single label
    to the entire cloud.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 3D è¯­ä¹‰åˆ†å‰²å’Œ 3D ç‚¹äº‘åˆ†ç±»çš„æ ¹æœ¬åŒºåˆ«åœ¨äºï¼Œåˆ†å‰²æ—¨åœ¨ä¸ºç‚¹äº‘ä¸­çš„æ¯ä¸ªç‚¹åˆ†é…æ ‡ç­¾ã€‚è€Œåˆ†ç±»åˆ™è¯•å›¾ä¸ºæ•´ä¸ªç‚¹äº‘åˆ†é…ä¸€ä¸ªæ ‡ç­¾ã€‚
- en: '![](../Images/1f556b730b7907f9c25b9033d1c0c5b7.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f556b730b7907f9c25b9033d1c0c5b7.png)'
- en: The difference between the classification model and the semantic segmentation
    model. In both cases, we pass a point cloud, but for the classification task,
    the whole point cloud is the entity, whereas, in the Semantic Segmentation case,
    each point is an entity to classify. Â© F. Poux
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»æ¨¡å‹ä¸è¯­ä¹‰åˆ†å‰²æ¨¡å‹ä¹‹é—´çš„åŒºåˆ«ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½ä¼ é€’ä¸€ä¸ªç‚¹äº‘ï¼Œä½†å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œæ•´ä¸ªç‚¹äº‘æ˜¯ä¸€ä¸ªå®ä½“ï¼Œè€Œåœ¨è¯­ä¹‰åˆ†å‰²çš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ªç‚¹æ˜¯ä¸€ä¸ªéœ€è¦åˆ†ç±»çš„å®ä½“ã€‚Â©
    F. Poux
- en: For example, using the PointNet architecture, 3D point cloud classification
    involves passing the entire point cloud through the Network and outputting a single
    label representing the entire cloud. In contrast, the semantic segmentation â€œheaderâ€
    will assign a label to each point in the cloud. The difference in approach is
    because segmentation requires a more detailed understanding of the 3D space represented,
    as it seeks to identify and label individual objects or regions within the point
    cloud. In contrast, classification only requires a high-level understanding of
    the overall shape or composition of the point cloud.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä½¿ç”¨ PointNet æ¶æ„ï¼Œ3D ç‚¹äº‘åˆ†ç±»æ¶‰åŠå°†æ•´ä¸ªç‚¹äº‘é€šè¿‡ç½‘ç»œï¼Œå¹¶è¾“å‡ºä¸€ä¸ªä»£è¡¨æ•´ä¸ªç‚¹äº‘çš„æ ‡ç­¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¯­ä¹‰åˆ†å‰²çš„â€œå¤´éƒ¨â€å°†ä¸ºäº‘ä¸­çš„æ¯ä¸ªç‚¹åˆ†é…ä¸€ä¸ªæ ‡ç­¾ã€‚æ–¹æ³•çš„ä¸åŒåœ¨äºï¼Œåˆ†å‰²éœ€è¦å¯¹è¡¨ç¤ºçš„
    3D ç©ºé—´æœ‰æ›´è¯¦ç»†çš„ç†è§£ï¼Œå› ä¸ºå®ƒè¯•å›¾è¯†åˆ«å’Œæ ‡è®°ç‚¹äº‘ä¸­çš„ä¸ªä½“å¯¹è±¡æˆ–åŒºåŸŸã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåˆ†ç±»ä»…éœ€è¦å¯¹ç‚¹äº‘çš„æ•´ä½“å½¢çŠ¶æˆ–ç»„æˆæœ‰è¾ƒé«˜å±‚æ¬¡çš„ç†è§£ã€‚
- en: Overall, while 3D semantic segmentation and classification are essential tasks
    for analyzing 3D point cloud data, the main difference is the level of detail
    and granularity required in the labeling process.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œå°½ç®¡ 3D è¯­ä¹‰åˆ†å‰²å’Œåˆ†ç±»æ˜¯åˆ†æ 3D ç‚¹äº‘æ•°æ®çš„å…³é”®ä»»åŠ¡ï¼Œä½†ä¸»è¦åŒºåˆ«åœ¨äºæ ‡è®°è¿‡ç¨‹æ‰€éœ€çš„è¯¦ç»†ç¨‹åº¦å’Œç²’åº¦ã€‚
- en: And as you guessed it, we will attack semantic segmentation because it requires
    a more detailed understanding of the space being analyzed, which is so much fun
    ğŸ˜.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ çŒœåˆ°çš„ï¼Œæˆ‘ä»¬å°†æ”»å…‹è¯­ä¹‰åˆ†å‰²ï¼Œå› ä¸ºå®ƒéœ€è¦å¯¹è¢«åˆ†æçš„ç©ºé—´æœ‰æ›´è¯¦ç»†çš„ç†è§£ï¼Œè¿™éå¸¸æœ‰è¶£ ğŸ˜ã€‚
- en: But before that, let us take a tiny step back to better grasp how PointNet Architecture
    works, shall we?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡åœ¨æ­¤ä¹‹å‰ï¼Œè®©æˆ‘ä»¬ç¨å¾®å›é¡¾ä¸€ä¸‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£ PointNet æ¶æ„çš„å·¥ä½œåŸç†ï¼Œå¥½å—ï¼Ÿ
- en: 'PointNet: A Point-based 3D Deep Learning Architecture'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PointNetï¼šä¸€ç§åŸºäºç‚¹çš„ 3D æ·±åº¦å­¦ä¹ æ¶æ„
- en: Turning complex topics into small chunk-wise bits of knowledge is my specialty.
    But I must admit that, when touching upon 3D Deep Learning, the complexity of
    the function learned through the different processes within the neural Network
    and the empirical nature of hyper-parameter determination are important challenges.
    To overcome, hun? ğŸ˜
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å¤æ‚çš„ä¸»é¢˜æ‹†è§£æˆå°å—çŸ¥è¯†æ˜¯æˆ‘çš„ä¸“é•¿ã€‚ä½†æ˜¯æˆ‘å¿…é¡»æ‰¿è®¤ï¼Œå½“æ¶‰åŠåˆ° 3D æ·±åº¦å­¦ä¹ æ—¶ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œä¸­ä¸åŒè¿‡ç¨‹å­¦åˆ°çš„å‡½æ•°çš„å¤æ‚æ€§ä»¥åŠè¶…å‚æ•°ç¡®å®šçš„ç»éªŒæ€§ç‰¹å¾æ˜¯é‡è¦çš„æŒ‘æˆ˜ã€‚è¦å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œå—¯ï¼ŸğŸ˜
- en: First, let us recap what PointNet is. PointNet is one of the pioneers in Neural
    Networks for 3D deep learning. If you understand PointNet, you can use all the
    other advanced models. But, of course, understanding is only a part of the equation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹ PointNet æ˜¯ä»€ä¹ˆã€‚PointNet æ˜¯ 3D æ·±åº¦å­¦ä¹ ä¸­ç¥ç»ç½‘ç»œçš„å¼€åˆ›è€…ä¹‹ä¸€ã€‚å¦‚æœä½ ç†è§£äº† PointNetï¼Œä½ å°±å¯ä»¥ä½¿ç”¨æ‰€æœ‰å…¶ä»–é«˜çº§æ¨¡å‹ã€‚ä½†æ˜¯ï¼Œå½“ç„¶ï¼Œç†è§£åªæ˜¯æ–¹ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚
- en: '![](../Images/977877dcb2d06fa1636fdd3356d96303.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/977877dcb2d06fa1636fdd3356d96303.png)'
- en: 'The PointNet Architecture has the ability to attack three semantization applications:
    Classification, Part-Segmentation, and Semantic Segmentation. Â© F. Poux'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PointNet æ¶æ„èƒ½å¤Ÿå¤„ç†ä¸‰ç§è¯­ä¹‰åº”ç”¨ï¼šåˆ†ç±»ã€éƒ¨ä»¶åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²ã€‚Â© F. Poux
- en: The other part is making the scary thing work and extending it to use it with
    your data! And this is a challenging feat! Even for seasoned coders. Therefore,
    we divide into several parts the process. Today, it is about preparing the data
    so that we are sure we have something that works in real conditions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€éƒ¨åˆ†æ˜¯è®©è¿™äº›å¤æ‚çš„ä¸œè¥¿å‘æŒ¥ä½œç”¨ï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°ä½ çš„æ•°æ®ä¸Šï¼è¿™æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼å³ä½¿å¯¹äºç»éªŒä¸°å¯Œçš„ç¼–ç å‘˜ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªè¿‡ç¨‹åˆ†ä¸ºå‡ ä¸ªéƒ¨åˆ†ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬è®¨è®ºçš„æ˜¯å‡†å¤‡æ•°æ®ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬åœ¨å®é™…æ¡ä»¶ä¸‹æœ‰ç”¨çš„ä¸œè¥¿ã€‚
- en: To prepare the data correctly, it is essential to understand the building block
    of the Network. Let me give you the critical aspects of what to consider when
    preparing your data with the Network below.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ­£ç¡®å‡†å¤‡æ•°æ®ï¼Œç†è§£ç½‘ç»œçš„æ„å»ºå—æ˜¯è‡³å…³é‡è¦çš„ã€‚ä¸‹é¢æˆ‘å°†ä»‹ç»åœ¨ä½¿ç”¨ç½‘ç»œå‡†å¤‡æ•°æ®æ—¶éœ€è¦è€ƒè™‘çš„å…³é”®æ–¹é¢ã€‚
- en: '![](../Images/4f3210ed58bfea5e2afda6856cb79faa.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f3210ed58bfea5e2afda6856cb79faa.png)'
- en: 'The PoinNet Model Architecture as described by the authors of the paper: [ArXiv
    Paper](https://arxiv.org/pdf/1612.00593.pdf).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä½œè€…æè¿°çš„ PointNet æ¨¡å‹æ¶æ„ï¼š[ArXiv è®ºæ–‡](https://arxiv.org/pdf/1612.00593.pdf)ã€‚
- en: The architecture of PointNet consists of several layers of neural networks that
    process the point cloud data. The input to PointNet is a simple set of points,
    each represented by its 3D coordinates and additional features such as color or
    intensity. These points are fed into successive shared Multi-Layer Perceptron
    (MLP) network that learns to extract local features from each point.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: PointNet çš„æ¶æ„ç”±å‡ ä¸ªå¤„ç†ç‚¹äº‘æ•°æ®çš„ç¥ç»ç½‘ç»œå±‚ç»„æˆã€‚PointNet çš„è¾“å…¥æ˜¯ä¸€ä¸ªç®€å•çš„ç‚¹é›†ï¼Œæ¯ä¸ªç‚¹ç”±å…¶ 3D åæ ‡å’Œé™„åŠ ç‰¹å¾ï¼ˆå¦‚é¢œè‰²æˆ–å¼ºåº¦ï¼‰è¡¨ç¤ºã€‚è¿™äº›ç‚¹è¢«è¾“å…¥åˆ°è¿ç»­çš„å…±äº«å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ç½‘ç»œä¸­ï¼Œç½‘ç»œå­¦ä¹ ä»æ¯ä¸ªç‚¹ä¸­æå–å±€éƒ¨ç‰¹å¾ã€‚
- en: '![](../Images/2dd6409b2608bc0f614285a612ac89ed.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2dd6409b2608bc0f614285a612ac89ed.png)'
- en: 'The MLP in the PointNet Architecture as described by the authors of the paper:
    [ArXiv Paper](https://arxiv.org/pdf/1612.00593.pdf).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä½œè€…æè¿°çš„ PointNet æ¶æ„ä¸­çš„ MLPï¼š[ArXiv è®ºæ–‡](https://arxiv.org/pdf/1612.00593.pdf)ã€‚
- en: 'ğŸ¦š **Note**: *An MLP is a neural network of multiple layers of connected nodes
    or neurons. Each neuron in the MLP receives input from the neurons in the previous
    layer, applies a transformation to this input using weights and biases, and then
    passes the result to the neurons in the next layer. The weights and biases in
    the MLP are learned during training using backpropagation, which adjusts them
    to minimize the difference between the Networkâ€™s predictions and the true output.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š **æ³¨æ„**ï¼š*MLP æ˜¯ä¸€ä¸ªç”±å¤šä¸ªå±‚è¿æ¥çš„èŠ‚ç‚¹æˆ–ç¥ç»å…ƒæ„æˆçš„ç¥ç»ç½‘ç»œã€‚MLP ä¸­çš„æ¯ä¸ªç¥ç»å…ƒä»ä¸Šä¸€å±‚çš„ç¥ç»å…ƒæ¥æ”¶è¾“å…¥ï¼Œåˆ©ç”¨æƒé‡å’Œåç½®å¯¹è¯¥è¾“å…¥è¿›è¡Œå˜æ¢ï¼Œç„¶åå°†ç»“æœä¼ é€’ç»™ä¸‹ä¸€å±‚çš„ç¥ç»å…ƒã€‚MLP
    ä¸­çš„æƒé‡å’Œåç½®åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡åå‘ä¼ æ’­å­¦ä¹ ï¼Œä»¥æœ€å°åŒ–ç½‘ç»œé¢„æµ‹å€¼ä¸çœŸå®è¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚*
- en: These MLPs are fully connected layers, each followed by what we call â€œa non-linear
    activation functionâ€ (such as ReLU). The number of neurons in each layer (E.g.,
    64) and the number of layers themselves (E.g., 2) can be adjusted depending on
    the specific task and the complexity of the input point cloud data. As you can
    guess, the more neurons and layers, the more complex the targeted problem can
    be because of the combinatorial possibilities given by the architecture plasticity.
    If we continue to explore the PointNet architecture, we see that we describe the
    original n input points, with 1024 features that span from the initial ones provided
    (X, Y, and Z). This is where the architecture provides a global description of
    the input point cloud by using a max-pooling operation to the locally learned
    features to get a global feature vector that summarizes the entire point cloud.
    This global feature vector is then fed through several fully connected layers
    to produce the final output of the Classification head, i.e., the score for k
    classes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº› MLP æ˜¯å…¨è¿æ¥å±‚ï¼Œæ¯ä¸€å±‚åé¢è·Ÿç€æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œéçº¿æ€§æ¿€æ´»å‡½æ•°â€ï¼ˆå¦‚ ReLUï¼‰ã€‚æ¯å±‚çš„ç¥ç»å…ƒæ•°é‡ï¼ˆä¾‹å¦‚ 64ï¼‰å’Œå±‚æ•°ï¼ˆä¾‹å¦‚ 2ï¼‰å¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡å’Œè¾“å…¥ç‚¹äº‘æ•°æ®çš„å¤æ‚æ€§è¿›è¡Œè°ƒæ•´ã€‚æ­£å¦‚ä½ æ‰€çŒœæµ‹çš„ï¼Œç¥ç»å…ƒå’Œå±‚æ•°è¶Šå¤šï¼Œç›®æ ‡é—®é¢˜å¯èƒ½è¶Šå¤æ‚ï¼Œå› ä¸ºæ¶æ„çš„å¯å¡‘æ€§å¸¦æ¥äº†ç»„åˆå¯èƒ½æ€§ã€‚å¦‚æœæˆ‘ä»¬ç»§ç»­æ·±å…¥ç ”ç©¶
    PointNet æ¶æ„ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°æˆ‘ä»¬ç”¨ 1024 ä¸ªç‰¹å¾æè¿°åŸå§‹çš„ n ä¸ªè¾“å…¥ç‚¹ï¼Œè¿™äº›ç‰¹å¾ä»æœ€åˆæä¾›çš„ï¼ˆXã€Y å’Œ Zï¼‰ä¸­å»¶å±•å‡ºæ¥ã€‚è¿™æ˜¯æ¶æ„é€šè¿‡ä½¿ç”¨æœ€å¤§æ± åŒ–æ“ä½œå¯¹å±€éƒ¨å­¦ä¹ ç‰¹å¾è¿›è¡Œå…¨å±€æè¿°çš„åœ°æ–¹ï¼Œä»è€Œè·å¾—ä¸€ä¸ªæ€»ç»“æ•´ä¸ªç‚¹äº‘çš„å…¨å±€ç‰¹å¾å‘é‡ã€‚ç„¶åï¼Œè¿™ä¸ªå…¨å±€ç‰¹å¾å‘é‡ä¼šé€šè¿‡è‹¥å¹²å…¨è¿æ¥å±‚æ¥ç”Ÿæˆåˆ†ç±»å¤´çš„æœ€ç»ˆè¾“å‡ºï¼Œå³
    k ç±»çš„è¯„åˆ†ã€‚
- en: '![](../Images/cfccefae9c1356f71ebad1bb415fd7a1.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cfccefae9c1356f71ebad1bb415fd7a1.png)'
- en: 'The MaxPool and MLP of PoinNet Model Architecture as described by the authors
    of the paper: [ArXiv Paper](https://arxiv.org/pdf/1612.00593.pdf).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç”±è®ºæ–‡ä½œè€…æè¿°çš„ PoinNet æ¨¡å‹æ¶æ„çš„ MaxPool å’Œ MLP: [ArXiv Paper](https://arxiv.org/pdf/1612.00593.pdf)ã€‚'
- en: If you notice closely, the semantic segmentation head in PointNet is a fully
    connected network that concatenates the global feature vector and the local feature
    vectors to produce a per-point score or label for each point in the input point
    cloud data. The semantic segmentation head consists of several fully connected
    layers with ReLU activation functions and a final softmax layer. The output of
    the final softmax layer represents the per-point probability distribution over
    the different semantic labels or classes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»”ç»†è§‚å¯Ÿï¼ŒPointNet ä¸­çš„è¯­ä¹‰åˆ†å‰²å¤´æ˜¯ä¸€ä¸ªå…¨è¿æ¥ç½‘ç»œï¼Œå®ƒå°†å…¨å±€ç‰¹å¾å‘é‡å’Œå±€éƒ¨ç‰¹å¾å‘é‡ä¸²è”åœ¨ä¸€èµ·ï¼Œä¸ºè¾“å…¥ç‚¹äº‘æ•°æ®ä¸­çš„æ¯ä¸ªç‚¹ç”Ÿæˆä¸€ä¸ªæ¯ç‚¹è¯„åˆ†æˆ–æ ‡ç­¾ã€‚è¯­ä¹‰åˆ†å‰²å¤´ç”±è‹¥å¹²å…¨è¿æ¥å±‚ã€ReLU
    æ¿€æ´»å‡½æ•°å’Œä¸€ä¸ªæœ€ç»ˆçš„ softmax å±‚ç»„æˆã€‚æœ€ç»ˆ softmax å±‚çš„è¾“å‡ºä»£è¡¨ä¸åŒè¯­ä¹‰æ ‡ç­¾æˆ–ç±»åˆ«çš„æ¯ç‚¹æ¦‚ç‡åˆ†å¸ƒã€‚
- en: '![](../Images/91d55f8e974c16c23c108d75eb491b61.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91d55f8e974c16c23c108d75eb491b61.png)'
- en: 'The Segmentation Head of the PoinNet Model Architecture as described by the
    paper''s authors: [ArXiv Paper](https://arxiv.org/pdf/1612.00593.pdf).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç”±è®ºæ–‡ä½œè€…æè¿°çš„ PoinNet æ¨¡å‹æ¶æ„çš„åˆ†å‰²å¤´: [ArXiv Paper](https://arxiv.org/pdf/1612.00593.pdf)ã€‚'
- en: The PointNet Architecture can capture important geometric and contextual information
    for tasks such as object classification and segmentation in 3D data by learning
    local and global features from each point in the input point cloud. One of the
    critical innovations of PointNet is using a symmetric function in the max-pooling
    operation, which ensures that the output is invariant to the order of the input
    points. This makes PointNet robust to variations in the ordering of the input
    points, which is essential in 3D data analysis.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: PointNet æ¶æ„èƒ½å¤Ÿæ•æ‰ä»»åŠ¡å¦‚ 3D æ•°æ®ä¸­çš„å¯¹è±¡åˆ†ç±»å’Œåˆ†å‰²æ‰€éœ€çš„é‡è¦å‡ ä½•å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé€šè¿‡ä»è¾“å…¥ç‚¹äº‘ä¸­çš„æ¯ä¸ªç‚¹å­¦ä¹ å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚PointNet
    çš„ä¸€ä¸ªå…³é”®åˆ›æ–°æ˜¯åœ¨æœ€å¤§æ± åŒ–æ“ä½œä¸­ä½¿ç”¨å¯¹ç§°å‡½æ•°ï¼Œè¿™ç¡®ä¿äº†è¾“å‡ºå¯¹è¾“å…¥ç‚¹çš„é¡ºåºä¸å˜ã€‚è¿™ä½¿å¾— PointNet å¯¹è¾“å…¥ç‚¹é¡ºåºçš„å˜åŒ–å…·æœ‰é²æ£’æ€§ï¼Œè¿™åœ¨ 3D æ•°æ®åˆ†æä¸­è‡³å…³é‡è¦ã€‚
- en: Now, we are ready to attack heads on preparing the data for PointNet. Which
    point cloud do we mean in the beginning? Do we feed a complete point cloud?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å‡†å¤‡å…¨åŠ›ä»¥èµ´åœ°ä¸º PointNet å‡†å¤‡æ•°æ®ã€‚æœ€å¼€å§‹æˆ‘ä»¬æŒ‡çš„æ˜¯ä»€ä¹ˆç‚¹äº‘ï¼Ÿæˆ‘ä»¬æ˜¯å¦è¾“å…¥ä¸€ä¸ªå®Œæ•´çš„ç‚¹äº‘ï¼Ÿ
- en: 'PointNet: Data preparation key aspects'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'PointNet: æ•°æ®å‡†å¤‡çš„å…³é”®æ–¹é¢'
- en: 'On a high-level view, if we study the [original paper published](https://arxiv.org/pdf/1612.00593.pdf),
    we can see that PointNet functions in a very straightforward manner:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼Œå¦‚æœæˆ‘ä»¬ç ”ç©¶[åŸå§‹è®ºæ–‡](https://arxiv.org/pdf/1612.00593.pdf)ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° PointNet çš„åŠŸèƒ½éå¸¸ç›´æ¥ï¼š
- en: We take a point cloud and normalize the data to a canonical space.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ç‚¹äº‘æ•°æ®è§„èŒƒåŒ–åˆ°æ ‡å‡†ç©ºé—´ã€‚
- en: We compute a bunch of features (without ingesting our knowledge but by leveraging
    the network capabilities to create cool ones)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—ä¸€ç³»åˆ—ç‰¹å¾ï¼ˆä¸ä¾èµ–äºæˆ‘ä»¬å·²æœ‰çš„çŸ¥è¯†ï¼Œè€Œæ˜¯åˆ©ç”¨ç½‘ç»œçš„èƒ½åŠ›æ¥åˆ›å»ºæœ‰ç”¨çš„ç‰¹å¾ï¼‰
- en: We aggregate these features into a global signature for the considered cloud.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¿™äº›ç‰¹å¾æ±‡èšæˆè€ƒè™‘ä¸­çš„ç‚¹äº‘çš„å…¨å±€ç‰¹å¾ã€‚
- en: '*Option 1*: we use this global signature to classify the point cloud'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*é€‰é¡¹ 1*ï¼šæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå…¨å±€ç‰¹å¾æ¥å¯¹ç‚¹äº‘è¿›è¡Œåˆ†ç±»'
- en: '*Option 2*: we combine this global signature with the local signature and build
    even sharper features for Semantic Segmentation.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*é€‰é¡¹ 2*ï¼šæˆ‘ä»¬å°†è¿™ä¸ªå…¨å±€ç‰¹å¾ä¸å±€éƒ¨ç‰¹å¾ç»“åˆï¼Œæ„å»ºæ›´ç²¾ç¡®çš„è¯­ä¹‰åˆ†å‰²ç‰¹å¾ã€‚'
- en: '![](../Images/7b517b55b4f933bf61007f4e08ab20f5.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b517b55b4f933bf61007f4e08ab20f5.png)'
- en: The five steps of PointNet towards either Semantic Segmentation or Classification
    tasks. Â© F. Poux
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: PointNet åœ¨è¯­ä¹‰åˆ†å‰²æˆ–åˆ†ç±»ä»»åŠ¡ä¸­çš„äº”ä¸ªæ­¥éª¤ã€‚Â© F. Poux
- en: It is all about features, meaning the chunk we provide the Network should be
    very relevant. E.g., giving the entire point cloud will not work, giving a tiny
    sample will not work, and giving structured samples with a different distribution
    than what will be fed will not work. So how do we do that?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€åˆ‡éƒ½å›´ç»•ç‰¹å¾å±•å¼€ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æä¾›ç»™ç½‘ç»œçš„å—åº”è¯¥éå¸¸ç›¸å…³ã€‚ä¾‹å¦‚ï¼Œç»™å‡ºæ•´ä¸ªç‚¹äº‘æ˜¯è¡Œä¸é€šçš„ï¼Œç»™å‡ºä¸€ä¸ªå¾®å°çš„æ ·æœ¬ä¹Ÿæ˜¯è¡Œä¸é€šçš„ï¼Œæä¾›å…·æœ‰ä¸åŒåˆ†å¸ƒçš„ç»“æ„åŒ–æ ·æœ¬ä¹Ÿè¡Œä¸é€šã€‚é‚£ä¹ˆæˆ‘ä»¬æ€ä¹ˆåšå‘¢ï¼Ÿ
- en: Let us follow a linear ten-steps process to obtain well-thought 3D point cloud
    training/inference-ready datasets.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬éµå¾ªä¸€ä¸ªçº¿æ€§çš„åæ­¥æµç¨‹ï¼Œä»¥è·å¾—ç»è¿‡æ·±æ€ç†Ÿè™‘çš„ 3D ç‚¹äº‘è®­ç»ƒ/æ¨ç†å‡†å¤‡æ•°æ®é›†ã€‚
- en: '![](../Images/5e3562ca370d6d278c11635d8b34757d.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e3562ca370d6d278c11635d8b34757d.png)'
- en: The PointNet Data Preparation Workflow. Â© F. Poux
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PointNet æ•°æ®å‡†å¤‡å·¥ä½œæµç¨‹ã€‚Â© F. Poux
- en: 'Step 1: Prepare your working environment'
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡ä½ çš„å·¥ä½œç¯å¢ƒ
- en: '![](../Images/e5938db254abaaf0372ea2c43c7c6c0a.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5938db254abaaf0372ea2c43c7c6c0a.png)'
- en: 'In this article, we use two main components: CloudCompare and JupyterLab IDE
    (+ Python). For a detailed view of the best possible setup, I strongly encourage
    you to follow this article which goes into well-needed detail:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šCloudCompare å’Œ JupyterLab IDE (+ Python)ã€‚å¯¹äºæœ€ä½³è®¾ç½®çš„è¯¦ç»†è§†å›¾ï¼Œæˆ‘å¼ºçƒˆå»ºè®®ä½ å‚è€ƒè¿™ç¯‡æ–‡ç« ï¼Œå®ƒè¯¦ç»†ä»‹ç»äº†æ‰€éœ€å†…å®¹ï¼š
- en: '[](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----90398f880c9f--------------------------------)
    [## 3D Python Workflows for LiDAR City Models: A Step-by-Step Guide'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----90398f880c9f--------------------------------)
    [## 3D Python å·¥ä½œæµç¨‹ç”¨äº LiDAR åŸå¸‚æ¨¡å‹ï¼šé€æ­¥æŒ‡å—'
- en: The Ultimate Guide to unlocking a streamlined workflow for 3D City Modelling
    Applications. The tutorial covers Pythonâ€¦
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£é” 3D åŸå¸‚å»ºæ¨¡åº”ç”¨ç¨‹åºçš„ç»ˆææŒ‡å—ã€‚è¯¥æ•™ç¨‹æ¶µç›–äº† Pythonâ€¦
- en: towardsdatascience.com](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----90398f880c9f--------------------------------)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----90398f880c9f--------------------------------)'
- en: We will have a specific stack of libraries organized into main libraries, plotting
    libraries, and utility libraries.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æœ‰ä¸€ä¸ªç‰¹å®šçš„åº“æ ˆï¼Œåˆ†ä¸ºä¸»è¦åº“ã€ç»˜å›¾åº“å’Œå®ç”¨åº“ã€‚
- en: 'ğŸ¦š **Note**: *If you work in a local environment, I recommend for this tutorial
    to run pip for your package management* (pip install library_name)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š **æ³¨æ„**ï¼š*å¦‚æœä½ åœ¨æœ¬åœ°ç¯å¢ƒä¸­å·¥ä½œï¼Œæˆ‘å»ºè®®æœ¬æ•™ç¨‹ä½¿ç”¨ pip è¿›è¡ŒåŒ…ç®¡ç†*ï¼ˆpip install library_nameï¼‰
- en: 'The two main libraries that we will use are NumPy and Pytorch:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨çš„ä¸¤ä¸ªä¸»è¦åº“æ˜¯ NumPy å’Œ Pytorchï¼š
- en: '**Numpy**: NumPy is a Python library for working with numerical data, and it
    provides functions for manipulating arrays and matrices, mathematical operations,
    and linear algebra functions.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Numpy**ï¼šNumPy æ˜¯ä¸€ä¸ªç”¨äºå¤„ç†æ•°å€¼æ•°æ®çš„ Python åº“ï¼Œå®ƒæä¾›äº†æ“ä½œæ•°ç»„å’ŒçŸ©é˜µã€æ•°å­¦è¿ç®—å’Œçº¿æ€§ä»£æ•°å‡½æ•°çš„åŠŸèƒ½ã€‚'
- en: '**Pytorch**: Pytorch is a popular deep learning framework in Python. It provides
    tools for building and training neural networks and optimizing and evaluating
    models.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pytorch**ï¼šPytorch æ˜¯ä¸€ä¸ªæµè¡Œçš„ Python æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚å®ƒæä¾›äº†æ„å»ºå’Œè®­ç»ƒç¥ç»ç½‘ç»œä»¥åŠä¼˜åŒ–å’Œè¯„ä¼°æ¨¡å‹çš„å·¥å…·ã€‚'
- en: 'Then, we support these with two plotting libraries:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªç»˜å›¾åº“æ¥æ”¯æŒè¿™äº›ï¼š
- en: '**Matplotlib**: Matplotlib is a Python library for creating visualizations
    such as plots, charts, and graphs.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Matplotlib**ï¼šMatplotlib æ˜¯ä¸€ä¸ªç”¨äºåˆ›å»ºå¯è§†åŒ–å›¾è¡¨ã€å›¾å½¢å’Œå›¾åƒçš„ Python åº“ã€‚'
- en: '**Plotly**: Plotly is a Python library for creating **interactive** visualizations.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Plotly**ï¼šPlotly æ˜¯ä¸€ä¸ªç”¨äºåˆ›å»º**äº¤äº’å¼**å¯è§†åŒ–çš„ Python åº“ã€‚'
- en: 'And finally, we will also use three utility modules:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬è¿˜å°†ä½¿ç”¨ä¸‰ä¸ªå®ç”¨æ¨¡å—ï¼š
- en: '**os**: The os module in Python provides a way of using operating system-dependent
    functionality. It provides functions for interacting with the file system, such
    as creating, deleting, and renaming files and directories.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**os**: Pythonä¸­çš„osæ¨¡å—æä¾›äº†ä¸€ç§ä½¿ç”¨æ“ä½œç³»ç»Ÿç›¸å…³åŠŸèƒ½çš„æ–¹æ³•ã€‚å®ƒæä¾›äº†ä¸æ–‡ä»¶ç³»ç»Ÿäº¤äº’çš„å‡½æ•°ï¼Œä¾‹å¦‚åˆ›å»ºã€åˆ é™¤å’Œé‡å‘½åæ–‡ä»¶å’Œç›®å½•ã€‚'
- en: '**glob**: The glob module in Python provides a way of matching files and directories
    using patterns. For example, it can find all files with a specific extension in
    a directory.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**glob**: Pythonä¸­çš„globæ¨¡å—æä¾›äº†ä¸€ç§ä½¿ç”¨æ¨¡å¼åŒ¹é…æ–‡ä»¶å’Œç›®å½•çš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥åœ¨ç›®å½•ä¸­æ‰¾åˆ°æ‰€æœ‰å…·æœ‰ç‰¹å®šæ‰©å±•åçš„æ–‡ä»¶ã€‚'
- en: '**random** (Optional): The `random` library is a built-in module that provides
    functions for generating random numbers, selecting random items from a list, and
    shuffling sequences.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**random**ï¼ˆå¯é€‰ï¼‰ï¼š`random`åº“æ˜¯ä¸€ä¸ªå†…ç½®æ¨¡å—ï¼Œæä¾›ç”Ÿæˆéšæœºæ•°ã€ä»åˆ—è¡¨ä¸­é€‰æ‹©éšæœºé¡¹å’Œæ‰“ä¹±åºåˆ—çš„å‡½æ•°ã€‚'
- en: 'Once this is done, we are ready to get to the second aspect: Getting our hands
    on new 3D point cloud datasets!'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›å…¥ç¬¬äºŒä¸ªæ–¹é¢ï¼šè·å–æ–°çš„3Dç‚¹äº‘æ•°æ®é›†ï¼
- en: 'Step 2: 3D Data Curation'
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­¥éª¤2ï¼š3Dæ•°æ®æ•´ç†
- en: '![](../Images/a5e20e985dd57cccb438a4637463b0d9.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5e20e985dd57cccb438a4637463b0d9.png)'
- en: For this tutorial, we go east of the Netherlands, near Enschede, where the University
    of Twente shines ğŸŒ. Here, we select a part of the AHN4 dataset, which would have
    a good proportion of trees, ground, buildings, and a bit of water as well ğŸš¿. Let
    us say enough in a tile to have sufficient points for each class!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæœ¬æ•™ç¨‹ï¼Œæˆ‘ä»¬å‰å¾€è·å…°ä¸œéƒ¨ï¼Œé è¿‘æ©æ–¯èµ«å¾·ï¼Œé‚£é‡Œæœ‰ç‰¹æ¸©ç‰¹å¤§å­¦çš„å…‰èŠ’ğŸŒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€‰æ‹©äº†AHN4æ•°æ®é›†çš„ä¸€éƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†æ•°æ®åº”è¯¥æœ‰è¶³å¤Ÿçš„æ ‘æœ¨ã€åœ°é¢ã€å»ºç­‘ç‰©ä»¥åŠä¸€ç‚¹æ°´
    ğŸš¿ã€‚æˆ‘ä»¬å¯ä»¥è¯´æ¯ä¸ªç±»åˆ«éƒ½æœ‰è¶³å¤Ÿçš„ç‚¹ï¼
- en: 'ğŸ¦š **Note**: *We will train on imbalanced datasets, with a high predominance
    of ground points compared to the other classes. This is not an ideal scenario,
    where the MLP and semantic segmentation head may be biased towards predicting
    the majority class labels and ignore the minority class labels. This can result
    in inaccurate segmentation and misclassification of minority class points. Still,
    several techniques can be used to mitigate the effects of imbalanced classes,
    such as data augmentation, oversampling or undersampling of the minority class,
    and using weighted loss functions. This is for another time.* ğŸ˜‰'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¦š **æ³¨æ„**: *æˆ‘ä»¬å°†åœ¨ä¸å¹³è¡¡çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­åœ°é¢ç‚¹çš„æ¯”ä¾‹è¿œé«˜äºå…¶ä»–ç±»åˆ«ã€‚è¿™ä¸æ˜¯ç†æƒ³çš„æƒ…å†µï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒMLPå’Œè¯­ä¹‰åˆ†å‰²å¤´å¯èƒ½ä¼šåå‘äºé¢„æµ‹å¤šæ•°ç±»åˆ«æ ‡ç­¾ï¼Œè€Œå¿½ç•¥å°‘æ•°ç±»åˆ«æ ‡ç­¾ã€‚è¿™å¯èƒ½å¯¼è‡´ä¸å‡†ç¡®çš„åˆ†å‰²å’Œå°‘æ•°ç±»åˆ«ç‚¹çš„é”™è¯¯åˆ†ç±»ã€‚ä¸è¿‡ï¼Œå¯ä»¥ä½¿ç”¨å‡ ç§æŠ€æœ¯æ¥å‡è½»ä¸å¹³è¡¡ç±»åˆ«çš„å½±å“ï¼Œå¦‚æ•°æ®å¢å¼ºã€å°‘æ•°ç±»åˆ«çš„è¿‡é‡‡æ ·æˆ–æ¬ é‡‡æ ·ï¼Œä»¥åŠä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°ã€‚è¿™æ˜¯å¦ä¸€ä¸ªè¯é¢˜ã€‚*
    ğŸ˜‰'
- en: 'To gather the dataset, we access the open data portal [geotiles.nl](https://geotiles.nl/).
    We zoom in onto a part of interest waiting to have the _XX (to have a data size
    coherent), and then, we download the .laz dataset, as illustrated below:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ”¶é›†æ•°æ®é›†ï¼Œæˆ‘ä»¬è®¿é—®å¼€æ”¾æ•°æ®é—¨æˆ· [geotiles.nl](https://geotiles.nl/)ã€‚æˆ‘ä»¬ç¼©æ”¾åˆ°ä¸€ä¸ªæ„Ÿå…´è¶£çš„åŒºåŸŸï¼Œç­‰å¾…æœ‰_XXï¼ˆä»¥ä¾¿æ•°æ®é‡ä¸€è‡´ï¼‰ï¼Œç„¶åä¸‹è½½.lazæ•°æ®é›†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/b1652f394039a471da8eebb3ae293dd6.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1652f394039a471da8eebb3ae293dd6.png)'
- en: Gathering a Point Cloud Dataset from the AHN4 LiDAR Campaign in the Netherlands.
    Â© F. Poux
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è·å…°AHN4 LiDARæ´»åŠ¨ä¸­æ”¶é›†ç‚¹äº‘æ•°æ®é›†ã€‚Â© F. Poux
- en: Also, we can prepare some compelling use cases where you would like to test
    your model on tile(s) of interest later. This can be, for example, where you live
    if some open data is available there.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥å‡†å¤‡ä¸€äº›å¼•äººæ³¨ç›®çš„ç”¨ä¾‹ï¼Œä»¥ä¾¿ä½ ä»¥åå¯ä»¥åœ¨æ„Ÿå…´è¶£çš„åˆ‡ç‰‡ä¸Šæµ‹è¯•ä½ çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åœ¨ä½ æ‰€åœ¨çš„åœ°æ–¹ï¼Œå¦‚æœé‚£é‡Œæœ‰å¼€æ”¾æ•°æ®ã€‚
- en: 'ğŸ¦š **Note**: *If you want to put later on your model to a true challenge, downloading
    a tile in another land is a great generalization test! For example, you could
    download a* [*LiDAR HD*](https://geoservices.ign.fr/lidarhd) *point cloud tile
    to see the differences/improvements if used for training or testing.*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¦š **æ³¨æ„**: *å¦‚æœä½ æƒ³å¯¹ä½ çš„æ¨¡å‹è¿›è¡ŒçœŸæ­£çš„æŒ‘æˆ˜ï¼Œä¸‹è½½å¦ä¸€ä¸ªåœ°åŒºçš„åˆ‡ç‰‡æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ³›åŒ–æµ‹è¯•ï¼ä¾‹å¦‚ï¼Œä½ å¯ä»¥ä¸‹è½½ä¸€ä¸ª* [*LiDAR HD*](https://geoservices.ign.fr/lidarhd)
    *ç‚¹äº‘åˆ‡ç‰‡ï¼Œä»¥æŸ¥çœ‹å¦‚æœç”¨äºè®­ç»ƒæˆ–æµ‹è¯•ï¼Œæ˜¯å¦ä¼šæœ‰å·®å¼‚/æ”¹è¿›ã€‚*'
- en: 'Now that you have your point cloud in the .laz file format let us explore the
    characteristics given by the info file that you can also view or download:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»æœ‰äº†.lazæ–‡ä»¶æ ¼å¼çš„ç‚¹äº‘ï¼Œè®©æˆ‘ä»¬æ¢ç´¢infoæ–‡ä»¶æä¾›çš„ç‰¹æ€§ï¼Œä½ ä¹Ÿå¯ä»¥æŸ¥çœ‹æˆ–ä¸‹è½½ï¼š
- en: '![](../Images/8c7883e8cee2dad643b7fdd45639c294.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c7883e8cee2dad643b7fdd45639c294.png)'
- en: A Selected informational document on the selected 3D LiDAR Point Cloud dataset.
    Â© F. Poux
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä»½å…³äºé€‰å®š3D LiDARç‚¹äº‘æ•°æ®é›†çš„ä¿¡æ¯æ–‡ä»¶ã€‚Â© F. Poux
- en: This permits a good grasp of the data content, a crucial first step when building
    qualitative datasets.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æœ‰åŠ©äºæ·±å…¥ç†è§£æ•°æ®å†…å®¹ï¼Œè¿™æ˜¯æ„å»ºé«˜è´¨é‡æ•°æ®é›†æ—¶è‡³å…³é‡è¦çš„ç¬¬ä¸€æ­¥ã€‚
- en: '![](../Images/2cccf5613291347f8208381a21944c7b.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cccf5613291347f8208381a21944c7b.png)'
- en: This shows the content of the additional information on the point cloud. Â© F.
    Poux
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å±•ç¤ºäº†ç‚¹äº‘çš„é™„åŠ ä¿¡æ¯å†…å®¹ã€‚Â© F. Poux
- en: 'As you scroll through the various information points, several fields are interesting
    to note:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æµè§ˆå„ç§ä¿¡æ¯ç‚¹æ—¶ï¼Œæœ‰å‡ ä¸ªå­—æ®µå€¼å¾—æ³¨æ„ï¼š
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This small file selection hints that we will deal with around 32 million data
    points for our experiments, which have colors, intensity, and a Near Infrared
    Field if we want to steepen our model later on.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå°æ–‡ä»¶é€‰æ‹©æç¤ºæˆ‘ä»¬å°†å¤„ç†å¤§çº¦3200ä¸‡æ•°æ®ç‚¹ï¼Œè¿™äº›æ•°æ®ç‚¹å…·æœ‰é¢œè‰²ã€å¼ºåº¦ï¼Œå¹¶ä¸”å¦‚æœæˆ‘ä»¬å¸Œæœ›ç¨åæå‡æ¨¡å‹ï¼Œè¿˜å¯ä»¥å…·æœ‰è¿‘çº¢å¤–å­—æ®µã€‚
- en: Very nice! Now that we have the software stack installed and the 3D point cloud
    downloaded, we can jump onto a 3D Data Analysis to ensure the input fed to our
    model holds its promises.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: éå¸¸å¥½ï¼ç°åœ¨æˆ‘ä»¬å·²ç»å®‰è£…äº†è½¯ä»¶å †æ ˆå¹¶ä¸‹è½½äº†3Dç‚¹äº‘ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œ3Dæ•°æ®åˆ†æï¼Œä»¥ç¡®ä¿è¾“å…¥åˆ°æ¨¡å‹ä¸­çš„æ•°æ®ç¬¦åˆé¢„æœŸã€‚
- en: 'Step 3: 3D Data Analysis (CloudCompare)'
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­¥éª¤3ï¼š3Dæ•°æ®åˆ†æï¼ˆCloudCompareï¼‰
- en: '![](../Images/ce5da5ed1260e2c915501f876ca57766.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce5da5ed1260e2c915501f876ca57766.png)'
- en: It is time to load the 3D aerial point cloud file into the software [CloudCompare](https://cloudcompare.org/).
    First, open CloudCompare on your computer until an empty GUI appears, which functions
    as shown below.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯æ—¶å€™å°†3Dèˆªç©ºç‚¹äº‘æ–‡ä»¶åŠ è½½åˆ°è½¯ä»¶[CloudCompare](https://cloudcompare.org/)ä¸­äº†ã€‚é¦–å…ˆï¼Œåœ¨è®¡ç®—æœºä¸Šæ‰“å¼€CloudCompareï¼Œç›´åˆ°å‡ºç°ç©ºçš„GUIï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/5835e687656ac5c5a90468fe7fa06d51.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5835e687656ac5c5a90468fe7fa06d51.png)'
- en: 'The GUI of CloudCompare. Source: [learngeodata.eu](http://learngeodata.eu)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: CloudCompareçš„GUIã€‚æ¥æºï¼š[learngeodata.eu](http://learngeodata.eu)
- en: From there, we load the .laz file we downloaded by drag-drop and select some
    attributes from the menu that pops out on import, as illustrated below.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é‚£é‡Œï¼Œæˆ‘ä»¬é€šè¿‡æ‹–æ”¾çš„æ–¹å¼åŠ è½½ä¸‹è½½çš„.lazæ–‡ä»¶ï¼Œå¹¶åœ¨å¯¼å…¥æ—¶ä»å¼¹å‡ºçš„èœå•ä¸­é€‰æ‹©ä¸€äº›å±æ€§ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
- en: '![](../Images/b203996c036f86e53f4b5ac0b5fd6e28.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b203996c036f86e53f4b5ac0b5fd6e28.png)'
- en: Importing a 3D Point Cloud in CloudCompare. We make sure to select relevant
    features to load. Â© F. Poux
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨CloudCompareä¸­å¯¼å…¥3Dç‚¹äº‘ã€‚æˆ‘ä»¬ç¡®ä¿é€‰æ‹©ç›¸å…³ç‰¹å¾è¿›è¡ŒåŠ è½½ã€‚Â© F. Poux
- en: 'ğŸ¦š **Note**: *We unselect all fields to pre-select some features that bring
    uncorrelated or low-correlated information and the labels for each point that
    hint at possible ground truth. We will thus only keep the intensity and classification
    field. Indeed, as we target Aerial point clouds, we want something that can generalize
    quite efficiently. Therefore, we aim at features likely found in unlabelled data
    that we want our model to perform on later. On top, the point cloud has RGB information,
    which is also a sound choice.*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š **æ³¨æ„**ï¼š*æˆ‘ä»¬å–æ¶ˆé€‰æ‹©æ‰€æœ‰å­—æ®µä»¥é¢„é€‰ä¸€äº›å¸¦æ¥ä¸ç›¸å…³æˆ–ä½ç›¸å…³ä¿¡æ¯çš„ç‰¹å¾ä»¥åŠæ¯ä¸ªç‚¹çš„æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾å¯èƒ½æŒ‡ç¤ºçœŸå®æƒ…å†µã€‚å› æ­¤ï¼Œæˆ‘ä»¬åªä¿ç•™å¼ºåº¦å’Œåˆ†ç±»å­—æ®µã€‚ç¡®å®ï¼Œç”±äºæˆ‘ä»¬é’ˆå¯¹çš„æ˜¯èˆªç©ºç‚¹äº‘ï¼Œæˆ‘ä»¬å¸Œæœ›é€‰æ‹©èƒ½å¤Ÿé«˜æ•ˆæ³›åŒ–çš„ç‰¹å¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€‰æ‹©åœ¨æœªæ ‡è®°çš„æ•°æ®ä¸­å¯èƒ½æ‰¾åˆ°çš„ç‰¹å¾ï¼Œä»¥ä¾¿æˆ‘ä»¬åç»­çš„æ¨¡å‹èƒ½å¤Ÿåœ¨è¿™äº›æ•°æ®ä¸Šè¿›è¡Œè¡¨ç°ã€‚æ­¤å¤–ï¼Œç‚¹äº‘è¿˜å…·æœ‰RGBä¿¡æ¯ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚*
- en: 'At this stage, the **seven** selected features are the following: X, Y, and
    Z (spatial), R, G, B (radiometry), and intensity. On top, we keep the AHN4 labels
    per point from the Classification field of the .laz file. Once your 3D aerial
    point cloud is successfully imported into CloudCompare, we are ready for analysis
    and visualization. We can quickly review the two extra fields (intensity and classification)
    from the â€œ`Object Properties panel` (3)â€. If we study the intensity, we notice
    some outlier points that shift our feature vector a bit, as shown below.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€é˜¶æ®µï¼Œ**ä¸ƒ**ä¸ªé€‰å®šçš„ç‰¹å¾å¦‚ä¸‹ï¼šXã€Yå’ŒZï¼ˆç©ºé—´ï¼‰ï¼ŒRã€Gã€Bï¼ˆè¾å°„è®¡ï¼‰ï¼Œä»¥åŠå¼ºåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¿ç•™æ¥è‡ª.lazæ–‡ä»¶åˆ†ç±»å­—æ®µçš„AHN4æ ‡ç­¾ã€‚æˆåŠŸå°†3Dèˆªç©ºç‚¹äº‘å¯¼å…¥CloudCompareåï¼Œæˆ‘ä»¬å°±å‡†å¤‡å¥½è¿›è¡Œåˆ†æå’Œå¯è§†åŒ–äº†ã€‚æˆ‘ä»¬å¯ä»¥å¿«é€ŸæŸ¥çœ‹â€œ`å¯¹è±¡å±æ€§é¢æ¿`ï¼ˆ3ï¼‰â€ä¸­çš„ä¸¤ä¸ªé¢å¤–å­—æ®µï¼ˆå¼ºåº¦å’Œåˆ†ç±»ï¼‰ã€‚å¦‚æœæˆ‘ä»¬ç ”ç©¶å¼ºåº¦ï¼Œä¼šå‘ç°ä¸€äº›ç¦»ç¾¤ç‚¹ç¨å¾®åç§»äº†æˆ‘ä»¬çš„ç‰¹å¾å‘é‡ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/307997f24d50733e36cbacf3eab5ff5e.png)![](../Images/2f693937a99497d559776950f99ad7e3.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/307997f24d50733e36cbacf3eab5ff5e.png)![](../Images/2f693937a99497d559776950f99ad7e3.png)'
- en: The Intensity-colored point cloud and the histogram of the repartition. Â© F.
    Poux
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåº¦ç€è‰²çš„ç‚¹äº‘åŠå…¶åˆ†å¸ƒç›´æ–¹å›¾ã€‚Â© F. Poux
- en: This is the first observation we must address if we want to use this as an input
    feature for PointNet.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³å°†å…¶ç”¨ä½œPointNetçš„è¾“å…¥ç‰¹å¾ï¼Œè¿™æ˜¯æˆ‘ä»¬å¿…é¡»è§£å†³çš„ç¬¬ä¸€ä¸ªè§‚å¯Ÿç‚¹ã€‚
- en: Concerning the color values (Red, Green, Blue), they are obtained from another
    sensor, possibly at another time. Therefore, as they are merged from the available
    ortho imagery on the zone, we may have some precision/reprojection issues. But
    as you can imagine, having the ability to separate green elements from red ones
    should give us a clear indication of the probability a point belong to the vegetation
    classğŸ˜.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºé¢œè‰²å€¼ï¼ˆçº¢è‰²ã€ç»¿è‰²ã€è“è‰²ï¼‰ï¼Œå®ƒä»¬æ˜¯ä»å¦ä¸€ä¸ªä¼ æ„Ÿå™¨è·å¾—çš„ï¼Œå¯èƒ½æ˜¯åœ¨å¦ä¸€ä¸ªæ—¶é—´ã€‚å› æ­¤ï¼Œç”±äºå®ƒä»¬æ˜¯ä»è¯¥åŒºåŸŸçš„ç°æœ‰æ­£å°„å½±åƒä¸­åˆæˆçš„ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé‡åˆ°ä¸€äº›ç²¾åº¦/é‡æŠ•å½±é—®é¢˜ã€‚ä½†æ­£å¦‚ä½ æ‰€æƒ³ï¼Œèƒ½å¤Ÿå°†ç»¿è‰²å…ƒç´ ä¸çº¢è‰²å…ƒç´ åˆ†å¼€åº”è¯¥èƒ½ç»™æˆ‘ä»¬ä¸€ä¸ªæ¸…æ™°çš„æŒ‡ç¤ºï¼Œè¡¨æ˜ä¸€ä¸ªç‚¹å±äºæ¤è¢«ç±»åˆ«çš„æ¦‚ç‡ğŸ˜ã€‚
- en: '![](../Images/cf669bf4b9eaaa56b0230b3b0fae2209.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf669bf4b9eaaa56b0230b3b0fae2209.png)'
- en: The LiDAR dataset is colored with the ortho imagery to get R, G,B features.
    Â© F. Poux
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: LiDAR æ•°æ®é›†ä½¿ç”¨æ­£å°„å½±åƒç€è‰²ä»¥è·å– Rã€Gã€B ç‰¹å¾ã€‚Â© F. Poux
- en: We have a point cloud, with 32 million points expressed in a cartesian system
    (X, Y, Z), each having an intensity feature and colors (Red, Green, and Blue).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ä¸€ä¸ªç‚¹äº‘ï¼Œå…¶ä¸­åŒ…å«3200ä¸‡ä¸ªç‚¹ï¼Œä»¥ç¬›å¡å°”åæ ‡ç³»ï¼ˆXï¼ŒYï¼ŒZï¼‰è¡¨ç¤ºï¼Œæ¯ä¸ªç‚¹éƒ½æœ‰å¼ºåº¦ç‰¹å¾å’Œé¢œè‰²ï¼ˆçº¢è‰²ã€ç»¿è‰²å’Œè“è‰²ï¼‰ã€‚
- en: 'ğŸ¦š **Note**: *You can save this stage for later, as you may have a vast choice
    of features such as the one illustrated below, which is the Near InfraRed (NIR)
    Channel available on the dataset. For example, this is a convenient field that
    can highlight healthy (or not) vegetation.* ğŸ˜‰'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š **æ³¨æ„**ï¼š*ä½ å¯ä»¥å°†è¿™ä¸ªé˜¶æ®µç•™åˆ°åé¢ï¼Œå› ä¸ºä½ å¯èƒ½ä¼šæœ‰è®¸å¤šé€‰æ‹©çš„ç‰¹å¾ï¼Œä¾‹å¦‚ä¸‹é¢æ‰€ç¤ºçš„è¿‘çº¢å¤–ï¼ˆNIRï¼‰é€šé“ï¼Œå®ƒåœ¨æ•°æ®é›†ä¸­æ˜¯å¯ç”¨çš„ã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–¹ä¾¿çš„å­—æ®µï¼Œå¯ä»¥çªå‡ºæ˜¾ç¤ºå¥åº·ï¼ˆæˆ–ä¸å¥åº·ï¼‰çš„æ¤è¢«ã€‚*
    ğŸ˜‰
- en: '![](../Images/dfe86ac1fc5d20e6293eb31f7c43f894.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfe86ac1fc5d20e6293eb31f7c43f894.png)'
- en: The Near Infrared feature. Â© F. Poux
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘çº¢å¤–ç‰¹å¾ã€‚Â© F. Poux
- en: We have another last scalar field if you scroll the available ones. The Classification
    field, of course! And this is very handy to help us create a labeled dataset to
    avoid going from scratch (thank you OpenData ! ğŸ‘Œ)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ»šåŠ¨å¯ç”¨çš„å­—æ®µï¼Œæˆ‘ä»¬è¿˜æœ‰å¦ä¸€ä¸ªæœ€åçš„æ ‡é‡å­—æ®µã€‚åˆ†ç±»å­—æ®µï¼Œå½“ç„¶ï¼è¿™å¯¹äºå¸®åŠ©æˆ‘ä»¬åˆ›å»ºæ ‡æ³¨æ•°æ®é›†éå¸¸æ–¹ä¾¿ï¼Œä»¥é¿å…ä»é›¶å¼€å§‹ï¼ˆæ„Ÿè°¢å¼€æ”¾æ•°æ®ï¼ğŸ‘Œï¼‰
- en: '![](../Images/4c87a229f0f68f78c29dcf7b5429bcfc.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c87a229f0f68f78c29dcf7b5429bcfc.png)'
- en: The provided classification. Â© F. Poux
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æä¾›çš„åˆ†ç±»ã€‚Â© F. Poux
- en: 'ğŸ¦š **Note**: *For the sake of pedagogical training, we will consider the classification
    the ground truth for the rest of the tutorial. However, know that the classification
    was achieved with some uncertainty and that if you want the best-performing model,
    have to be fixed. Indeed, there is a famous saying with 3D Deep Learning: Garbage
    in = Garbage out. Therefore, the quality of your data should be paramount.*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š **æ³¨æ„**ï¼š*å‡ºäºæ•™å­¦åŸ¹è®­çš„ç›®çš„ï¼Œæˆ‘ä»¬å°†è€ƒè™‘å°†åˆ†ç±»ä½œä¸ºæ•™ç¨‹å‰©ä½™éƒ¨åˆ†çš„çœŸå®æƒ…å†µã€‚ç„¶è€Œï¼Œè¯·çŸ¥é“åˆ†ç±»æ˜¯æœ‰ä¸€å®šä¸ç¡®å®šæ€§çš„ï¼Œå¦‚æœä½ æƒ³è¦æœ€å¥½çš„æ¨¡å‹ï¼Œå¿…é¡»ä¿®æ­£å®ƒã€‚ç¡®å®ï¼Œæœ‰ä¸€å¥è‘—åçš„3Dæ·±åº¦å­¦ä¹ æ ¼è¨€ï¼šåƒåœ¾è¿›=åƒåœ¾å‡ºã€‚å› æ­¤ï¼Œæ•°æ®çš„è´¨é‡åº”è¯¥æ˜¯é¦–è¦çš„ã€‚*
- en: Letâ€™s focus on refining the labeling phase.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸“æ³¨äºç²¾ç‚¼æ ‡æ³¨é˜¶æ®µã€‚
- en: Step 4\. 3D Data Labelling (Labels Concatenation)
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬4æ­¥ï¼š3D æ•°æ®æ ‡æ³¨ï¼ˆæ ‡ç­¾è¿æ¥ï¼‰
- en: '![](../Images/b068f5f131c82160cb08b76f3f9f291b.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b068f5f131c82160cb08b76f3f9f291b.png)'
- en: Okay, so before jumping in on this step, I must say something. 3D point cloud
    labeling to train a supervised 3D semantic segmentation learning model is a (painfully)
    manual process. The goal is to assign labels to individual points in a 3D point
    cloud. The main critical objective of this process includes identifying the target
    objects in the point cloud, selecting the appropriate labeling technique, and
    ensuring the accuracy of the labeling process.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œåœ¨è¿›å…¥è¿™ä¸ªæ­¥éª¤ä¹‹å‰ï¼Œæˆ‘å¿…é¡»è¯´ç‚¹ä»€ä¹ˆã€‚3D ç‚¹äº‘æ ‡æ³¨ç”¨äºè®­ç»ƒæœ‰ç›‘ç£çš„ 3D è¯­ä¹‰åˆ†å‰²å­¦ä¹ æ¨¡å‹æ˜¯ä¸€ä¸ªï¼ˆç—›è‹¦çš„ï¼‰æ‰‹åŠ¨è¿‡ç¨‹ã€‚ç›®æ ‡æ˜¯ä¸º 3D ç‚¹äº‘ä¸­çš„å•ä¸ªç‚¹åˆ†é…æ ‡ç­¾ã€‚è¿™ä¸ªè¿‡ç¨‹çš„ä¸»è¦å…³é”®ç›®æ ‡åŒ…æ‹¬è¯†åˆ«ç‚¹äº‘ä¸­çš„ç›®æ ‡ç‰©ä½“ã€é€‰æ‹©åˆé€‚çš„æ ‡æ³¨æŠ€æœ¯ï¼Œå¹¶ç¡®ä¿æ ‡æ³¨è¿‡ç¨‹çš„å‡†ç¡®æ€§ã€‚
- en: '![](../Images/3e191ebfd0b0200319a3fa40a6bd9096.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e191ebfd0b0200319a3fa40a6bd9096.png)'
- en: 'An example of a labeling process: labeling clusters VS labeling individual
    points. Â© F. Poux'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡æ³¨è¿‡ç¨‹çš„ä¸€ä¸ªç¤ºä¾‹ï¼šæ ‡æ³¨ç°‡ä¸æ ‡æ³¨å•ä¸ªç‚¹ã€‚Â© F. Poux
- en: To identify the objects or regions in the point cloud that require labeling,
    we manually inspect the cloud or by using algorithms that automatically detect
    particular objects or regions based on their features, such as size, shape, or
    color.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¯†åˆ«ç‚¹äº‘ä¸­éœ€è¦æ ‡æ³¨çš„ç‰©ä½“æˆ–åŒºåŸŸï¼Œæˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨æ£€æŸ¥äº‘ï¼Œæˆ–è€…ä½¿ç”¨åŸºäºç‰¹å¾ï¼ˆå¦‚å¤§å°ã€å½¢çŠ¶æˆ–é¢œè‰²ï¼‰è‡ªåŠ¨æ£€æµ‹ç‰¹å®šç‰©ä½“æˆ–åŒºåŸŸçš„ç®—æ³•ã€‚
- en: '[](https://learngeodata.eu/?source=post_page-----90398f880c9f--------------------------------)
    [## 3D Academy - Point Cloud Online Course'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://learngeodata.eu/?source=post_page-----90398f880c9f--------------------------------)
    [## 3D Academy - ç‚¹äº‘åœ¨çº¿è¯¾ç¨‹'
- en: The best 3D Online Courses for Teachers, Researchers, Developers, and Engineers.
    Master 3D Point Cloud Processing andâ€¦
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸ºæ•™å¸ˆã€ç ”ç©¶äººå‘˜ã€å¼€å‘äººå‘˜å’Œå·¥ç¨‹å¸ˆæä¾›çš„æœ€ä½³3Dåœ¨çº¿è¯¾ç¨‹ã€‚æŒæ¡3Dç‚¹äº‘å¤„ç†åŠâ€¦â€¦
- en: learngeodata.eu](https://learngeodata.eu/?source=post_page-----90398f880c9f--------------------------------)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[learngeodata.eu](https://learngeodata.eu/?source=post_page-----90398f880c9f--------------------------------)'
- en: 'In our case, we start with an advantage: the point cloud is already classified.
    The first step is thus to extract each class as an independent point cloud, as
    illustrated below.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªä¼˜åŠ¿ï¼šç‚¹äº‘å·²ç»è¢«åˆ†ç±»ã€‚å› æ­¤ï¼Œç¬¬ä¸€æ­¥æ˜¯å°†æ¯ä¸ªç±»åˆ«æå–ä¸ºç‹¬ç«‹çš„ç‚¹äº‘ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
- en: '![](../Images/a96b31c90a448c3ff94dff8288789a7c.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a96b31c90a448c3ff94dff8288789a7c.png)'
- en: First, we select the point cloud and switch the â€œcolorsâ€ property from RGB to
    Scalar Field. We then ensure we visualize the Classification Scalar Field. From
    there, we go to EDIT > Scalar Field > Split Cloud by Integer Value, resulting
    in one point cloud per class in the point cloud.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬é€‰æ‹©ç‚¹äº‘å¹¶å°†â€œé¢œè‰²â€å±æ€§ä»RGBåˆ‡æ¢åˆ°æ ‡é‡åœºã€‚ç„¶åç¡®ä¿æˆ‘ä»¬å¯è§†åŒ–åˆ†ç±»æ ‡é‡åœºã€‚ä»é‚£é‡Œï¼Œæˆ‘ä»¬è½¬åˆ°EDIT > Scalar Field > Split
    Cloud by Integer Valueï¼Œä»è€Œåœ¨ç‚¹äº‘ä¸­ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆä¸€ä¸ªç‚¹äº‘ã€‚
- en: 'From the various classes that we get as clouds, we see that :'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æˆ‘ä»¬å¾—åˆ°çš„å„ç§ç‚¹äº‘ç±»åˆ«ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼š
- en: '[PRE2]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: From there, we can rework `class 1 = vegetation + clutter`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é‚£é‡Œï¼Œæˆ‘ä»¬å¯ä»¥é‡æ–°å¤„ç†`class 1 = vegetation + clutter`ã€‚
- en: The appropriate labeling technique must be selected based on the specific task
    and the available data. For example, we can use an unsupervised technique for
    more exploratory analysis and iteratively take some color thresholding by selecting
    candidate points in the vegetation, as illustrated below.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å¿…é¡»æ ¹æ®ç‰¹å®šä»»åŠ¡å’Œå¯ç”¨æ•°æ®é€‰æ‹©åˆé€‚çš„æ ‡è®°æŠ€æœ¯ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ— ç›‘ç£æŠ€æœ¯è¿›è¡Œæ›´å¤šçš„æ¢ç´¢æ€§åˆ†æï¼Œå¹¶é€šè¿‡é€‰æ‹©æ¤è¢«ä¸­çš„å€™é€‰ç‚¹æ¥è¿›è¡Œä¸€äº›é¢œè‰²é˜ˆå€¼å¤„ç†ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
- en: '![](../Images/522101de53d3a12b1147e36185ae82c9.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/522101de53d3a12b1147e36185ae82c9.png)'
- en: Segmenting the point cloud based on color information, in order to create sharper
    labels in a semi-automatic fashion. Â© F. Poux
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®é¢œè‰²ä¿¡æ¯å¯¹ç‚¹äº‘è¿›è¡Œåˆ†å‰²ï¼Œä»¥åŠè‡ªåŠ¨åŒ–çš„æ–¹å¼åˆ›å»ºæ›´ç²¾ç¡®çš„æ ‡ç­¾ã€‚Â© F. Poux
- en: This will give inaccurate results but may speed up manually selecting any point
    that belongs to the vegetation.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ç»™å‡ºä¸å‡†ç¡®çš„ç»“æœï¼Œä½†å¯èƒ½ä¼šåŠ å¿«æ‰‹åŠ¨é€‰æ‹©å±äºæ¤è¢«çš„ä»»ä½•ç‚¹ã€‚
- en: Finally, ensuring the accuracy of the labeling process is critical for producing
    reliable results. This can be achieved through manual verification or quality
    control techniques such as cross-validation or inter-annotator agreement.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œç¡®ä¿æ ‡ç­¾è¿‡ç¨‹çš„å‡†ç¡®æ€§å¯¹äºäº§ç”Ÿå¯é ç»“æœè‡³å…³é‡è¦ã€‚è¿™å¯ä»¥é€šè¿‡æ‰‹åŠ¨éªŒè¯æˆ–è´¨é‡æ§åˆ¶æŠ€æœ¯ï¼Œå¦‚äº¤å‰éªŒè¯æˆ–æ ‡æ³¨è€…ä¸€è‡´æ€§ï¼Œæ¥å®ç°ã€‚
- en: 'ğŸ¦š **Note**: *It is good to grasp the jargon, but do not be scared. These concepts
    can be covered at a later stage. One thing at a time.* ğŸ˜‰'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š **æ³¨æ„**ï¼š*äº†è§£æœ¯è¯­æ˜¯å¥½çš„ï¼Œä½†ä¸è¦å®³æ€•ã€‚è¿™äº›æ¦‚å¿µå¯ä»¥åœ¨åç»­é˜¶æ®µè¦†ç›–ã€‚ä¸€ä»¶äº‹ä¸€æ¬¡å®Œæˆã€‚* ğŸ˜‰
- en: 'Ultimately, the labeling processâ€™s accuracy will directly impact subsequent
    tasksâ€™ performance, incl. 3D Semantic Segmentation. In our case, we organize the
    data as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæ ‡ç­¾è¿‡ç¨‹çš„å‡†ç¡®æ€§å°†ç›´æ¥å½±å“åç»­ä»»åŠ¡çš„è¡¨ç°ï¼ŒåŒ…æ‹¬3Dè¯­ä¹‰åˆ†å‰²ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†æ•°æ®ç»„ç»‡å¦‚ä¸‹ï¼š
- en: '[PRE3]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We execute this within CloudCompare.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨CloudCompareä¸­æ‰§è¡Œæ­¤æ“ä½œã€‚
- en: '![](../Images/13bdd0ee2f63786bb858e28b0590edd6.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13bdd0ee2f63786bb858e28b0590edd6.png)'
- en: Organization of the various classes within CloudCompare. Â© F. Poux
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨CloudCompareä¸­ç»„ç»‡å„ç§ç±»åˆ«ã€‚Â© F. Poux
- en: After renaming for clarity our different clouds (initialization), we will (1)
    fuse the clutter in one single cloud, (2) delete the Classification field for
    all the clouds, (3) recreate a classification field with the new numbering, (4)
    clone all the clouds and (5) merge the cloned clouds, as shown below.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸ºæ¸…æ™°åº¦é‡å‘½åæˆ‘ä»¬çš„ä¸åŒç‚¹äº‘ï¼ˆåˆå§‹åŒ–ï¼‰åï¼Œæˆ‘ä»¬å°†ï¼ˆ1ï¼‰å°†æ‚ä¹±ç‰©ä½“åˆå¹¶ä¸ºä¸€ä¸ªå•ç‹¬çš„ç‚¹äº‘ï¼Œï¼ˆ2ï¼‰åˆ é™¤æ‰€æœ‰ç‚¹äº‘çš„åˆ†ç±»å­—æ®µï¼Œï¼ˆ3ï¼‰ç”¨æ–°çš„ç¼–å·é‡æ–°åˆ›å»ºåˆ†ç±»å­—æ®µï¼Œï¼ˆ4ï¼‰å…‹éš†æ‰€æœ‰ç‚¹äº‘ï¼Œï¼ˆ5ï¼‰åˆå¹¶å…‹éš†ç‚¹äº‘ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/9246ba132e80e03851d3b68b13f5ba5c.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9246ba132e80e03851d3b68b13f5ba5c.png)'
- en: Initial preparation of our labels into the new point cloud. Â© F. Poux
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: åˆæ­¥å‡†å¤‡æˆ‘ä»¬çš„æ ‡ç­¾è¿›å…¥æ–°çš„ç‚¹äº‘ã€‚Â© F. Poux
- en: '![](../Images/2984926c752b2b5391615bda185f70c3.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2984926c752b2b5391615bda185f70c3.png)'
- en: The Data Preparation Phase is Executed within CloudCompare. Â© F. Poux
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å‡†å¤‡é˜¶æ®µåœ¨CloudCompareä¸­æ‰§è¡Œã€‚Â© F. Poux
- en: Now, we have a labeled dataset with a specific point label repartition.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå¸¦æ ‡ç­¾çš„æ•°æ®é›†ï¼Œå¹¶å…·æœ‰ç‰¹å®šçš„ç‚¹æ ‡ç­¾åˆ†å¸ƒã€‚
- en: '![](../Images/f3252ee1a485053c68189023813c73c5.png)![](../Images/d15e1321ad392c59e82714c17eb58ac8.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3252ee1a485053c68189023813c73c5.png)![](../Images/d15e1321ad392c59e82714c17eb58ac8.png)'
- en: We notice that out of our 32,080,350 points, 23,131,067 belong to the ground
    (72%), 7,440,825 to the vegetation (23%), 1,146,575 to buildings (4%), 191,039
    to water (less than 1%), and the remaining 170,844 are not labeled (class 0).
    This will be very interesting because we are in this specific imbalance case with
    predominant classes.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œåœ¨32,080,350ä¸ªç‚¹ä¸­ï¼Œ23,131,067ä¸ªå±äºåœ°é¢ï¼ˆ72%ï¼‰ï¼Œ7,440,825ä¸ªå±äºæ¤è¢«ï¼ˆ23%ï¼‰ï¼Œ1,146,575ä¸ªå±äºå»ºç­‘ç‰©ï¼ˆ4%ï¼‰ï¼Œ191,039ä¸ªå±äºæ°´ä½“ï¼ˆä¸åˆ°1%ï¼‰ï¼Œå‰©ä½™çš„170,844ä¸ªæœªæ ‡è®°ï¼ˆç±»åˆ«0ï¼‰ã€‚è¿™å°†éå¸¸æœ‰è¶£ï¼Œå› ä¸ºæˆ‘ä»¬å¤„äºè¿™ä¸ªç‰¹å®šçš„ä¸å¹³è¡¡æƒ…å†µä¸­ï¼Œå…·æœ‰ä¸»å¯¼ç±»åˆ«ã€‚
- en: Now that we have analyzed what our point cloud contains and refined the labels,
    we can dive into feature selection.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»åˆ†æäº†ç‚¹äº‘çš„å†…å®¹å¹¶ç»†åŒ–äº†æ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥*æ·±å…¥*è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚
- en: Step 5\. 3D Feature Selection
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬5æ­¥ã€‚3Dç‰¹å¾é€‰æ‹©
- en: '![](../Images/e2b0c2ad5eedfee4eb23e276a88572b8.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2b0c2ad5eedfee4eb23e276a88572b8.png)'
- en: When using the PointNet architecture for 3D point cloud semantic segmentation,
    feature selection is essential in preparing the data for training.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨PointNetæ¶æ„è¿›è¡Œ3Dç‚¹äº‘è¯­ä¹‰åˆ†å‰²æ—¶ï¼Œç‰¹å¾é€‰æ‹©å¯¹äºå‡†å¤‡è®­ç»ƒæ•°æ®è‡³å…³é‡è¦ã€‚
- en: In traditional machine learning methods, feature engineering is often required
    to select and extract relevant features from the data. However, this step can
    be avoided with deep learning methods like PointNet since the model can learn
    to extract features from the data automatically.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ä¸­ï¼Œé€šå¸¸éœ€è¦ç‰¹å¾å·¥ç¨‹æ¥é€‰æ‹©å’Œæå–æ•°æ®ä¸­çš„ç›¸å…³ç‰¹å¾ã€‚ç„¶è€Œï¼Œä½¿ç”¨PointNetç­‰æ·±åº¦å­¦ä¹ æ–¹æ³•å¯ä»¥é¿å…è¿™ä¸€æ­¥éª¤ï¼Œå› ä¸ºæ¨¡å‹å¯ä»¥è‡ªåŠ¨å­¦ä¹ ä»æ•°æ®ä¸­æå–ç‰¹å¾ã€‚
- en: 'However, ensuring that the input data contains the necessary information for
    the model to learn relevant and deducted features is still essential. We use seven
    features: `X`, `Y`, `Z` (spatial attributes), `R`, `G`, `B` (radiometric attributes),
    and the intensity `I` (LiDAR-derived).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œç¡®ä¿è¾“å…¥æ•°æ®åŒ…å«æ¨¡å‹å­¦ä¹ ç›¸å…³å’Œæ¨å¯¼ç‰¹å¾æ‰€éœ€çš„ä¿¡æ¯ä»ç„¶å¾ˆé‡è¦ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸ƒä¸ªç‰¹å¾ï¼š`X`ã€`Y`ã€`Z`ï¼ˆç©ºé—´å±æ€§ï¼‰ã€`R`ã€`G`ã€`B`ï¼ˆè¾å°„å±æ€§ï¼‰å’Œå¼ºåº¦`I`ï¼ˆæ¿€å…‰é›·è¾¾è¡ç”Ÿï¼‰ã€‚
- en: '[PRE4]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is our reference. It means that we will build our model with this input,
    and any other dataset we would like to process with the trained PointNet model
    must contain these same features. Before moving within Python, the last step is
    to structure the data according to the Architecture specifications.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬çš„å‚è€ƒã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªè¾“å…¥æ¥æ„å»ºæ¨¡å‹ï¼Œä»»ä½•æˆ‘ä»¬å¸Œæœ›ç”¨è®­ç»ƒå¥½çš„PointNetæ¨¡å‹å¤„ç†çš„å…¶ä»–æ•°æ®é›†å¿…é¡»åŒ…å«è¿™äº›ç›¸åŒçš„ç‰¹å¾ã€‚åœ¨è¿›å…¥Pythonä¹‹å‰ï¼Œæœ€åä¸€æ­¥æ˜¯æ ¹æ®æ¶æ„è§„èŒƒç»“æ„åŒ–æ•°æ®ã€‚
- en: Step 6\. Data Structuration (Tiling)
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬6æ­¥ã€‚æ•°æ®ç»“æ„åŒ–ï¼ˆç“¦ç‰‡åŒ–ï¼‰
- en: '![](../Images/42c2dea08aaaf1a4b1414d386c5aa486.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42c2dea08aaaf1a4b1414d386c5aa486.png)'
- en: For several reasons, structuring a 3D point cloud into square tiles is essential
    when processing it with the neural network architecture PointNet.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå‡ ä¸ªåŸå› ï¼Œåœ¨ä½¿ç”¨ç¥ç»ç½‘ç»œæ¶æ„PointNetå¤„ç†3Dç‚¹äº‘æ—¶ï¼Œå°†å…¶ç»“æ„åŒ–ä¸ºæ­£æ–¹å½¢ç“¦ç‰‡æ˜¯å¿…è¦çš„ã€‚
- en: '![](../Images/201d5711e9e23692b603a0205f583a20.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/201d5711e9e23692b603a0205f583a20.png)'
- en: The tile definition within this workflow is to train PointNet 3D Deep Learning
    Architecture. Â© F. Poux
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤å·¥ä½œæµä¸­çš„ç“¦ç‰‡å®šä¹‰æ˜¯è®­ç»ƒPointNet 3Dæ·±åº¦å­¦ä¹ æ¶æ„ã€‚Â© F. Poux
- en: First, PointNet requires the input data to be of fixed size, meaning that all
    input samples should have the same number of points. By dividing a 3D point cloud
    into square tiles, we can ensure that each tile has a more homogeneous number
    of points, allowing PointNet to process them consistently and effectively without
    the extra overhead or unreversible loss when sampling to the final fixed-point
    number.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼ŒPointNetè¦æ±‚è¾“å…¥æ•°æ®ä¸ºå›ºå®šå¤§å°ï¼Œè¿™æ„å‘³ç€æ‰€æœ‰è¾“å…¥æ ·æœ¬åº”å…·æœ‰ç›¸åŒæ•°é‡çš„ç‚¹ã€‚é€šè¿‡å°†3Dç‚¹äº‘åˆ†å‰²æˆæ­£æ–¹å½¢ç“¦ç‰‡ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿æ¯ä¸ªç“¦ç‰‡å…·æœ‰æ›´å‡åŒ€çš„ç‚¹æ•°é‡ï¼Œä½¿PointNetèƒ½å¤Ÿä¸€è‡´æœ‰æ•ˆåœ°å¤„ç†å®ƒä»¬ï¼Œè€Œä¸ä¼šåœ¨é‡‡æ ·åˆ°æœ€ç»ˆå›ºå®šç‚¹æ•°æ—¶äº§ç”Ÿé¢å¤–å¼€é”€æˆ–ä¸å¯é€†æŸå¤±ã€‚
- en: '![](../Images/40b8d89038937bdcdf14757276160636.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40b8d89038937bdcdf14757276160636.png)'
- en: Example of the impact of sampling strategies on the 3D Point Cloud dataset.
    Â© F. Poux
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: é‡‡æ ·ç­–ç•¥å¯¹3Dç‚¹äº‘æ•°æ®é›†çš„å½±å“ç¤ºä¾‹ã€‚Â© F. Poux
- en: 'ğŸŒ± **Growing**: *With PointNet, we need to have the input tile to a fixed number
    of points, recommended at 4096 points by the original paperâ€™s authors. This means
    that a sampling strategy will be needed (****which is not done in CloudCompare****).
    As you can see from the illustration above, sampling the point cloud with different
    strategies will yield different results and object identification capabilities
    (E.,g. the electrical pole on the right). Do you think this impacts the 3D Deep
    Learning architecture performances?*'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŒ± **å¢é•¿**ï¼š*ä½¿ç”¨PointNetæ—¶ï¼Œæˆ‘ä»¬éœ€è¦å°†è¾“å…¥ç“¦ç‰‡å›ºå®šä¸ºæ¨èçš„4096ä¸ªç‚¹ã€‚è¿™æ„å‘³ç€éœ€è¦ä¸€ç§é‡‡æ ·ç­–ç•¥ï¼ˆ****CloudCompareä¸­æœªå®ç°****ï¼‰ã€‚å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œä½¿ç”¨ä¸åŒç­–ç•¥å¯¹ç‚¹äº‘è¿›è¡Œé‡‡æ ·å°†äº§ç”Ÿä¸åŒçš„ç»“æœå’Œç‰©ä½“è¯†åˆ«èƒ½åŠ›ï¼ˆä¾‹å¦‚å³ä¾§çš„ç”µçº¿æ†ï¼‰ã€‚ä½ è®¤ä¸ºè¿™ä¼šå½±å“3Dæ·±åº¦å­¦ä¹ æ¶æ„çš„æ€§èƒ½å—ï¼Ÿ*
- en: Secondly, PointNetâ€™s architecture involves a shared multi-layer perceptron (MLP)
    applied to each point independently, which means that the Network processes each
    point in isolation from its neighbors. By structuring the point cloud into tiles,
    we can preserve the local context of each point within its tile while still allowing
    the Network to process points independently, enabling it to extract some meaningful
    features from the data.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼ŒPointNetçš„æ¶æ„æ¶‰åŠåº”ç”¨äºæ¯ä¸ªç‚¹çš„å…±äº«å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œè¿™æ„å‘³ç€ç½‘ç»œåœ¨å¤„ç†æ¯ä¸ªç‚¹æ—¶æ˜¯ç‹¬ç«‹çš„ï¼Œä¸è€ƒè™‘å…¶é‚»å±…ã€‚é€šè¿‡å°†ç‚¹äº‘ç»“æ„åŒ–ä¸ºç“¦ç‰‡ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¿æŒæ¯ä¸ªç‚¹å±€éƒ¨ä¸Šä¸‹æ–‡çš„åŒæ—¶ï¼Œè®©ç½‘ç»œç‹¬ç«‹å¤„ç†ç‚¹ï¼Œä»è€Œä»æ•°æ®ä¸­æå–æœ‰æ„ä¹‰çš„ç‰¹å¾ã€‚
- en: '![](../Images/3ecdd116091d1b33e4c152f15903a4f1.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ecdd116091d1b33e4c152f15903a4f1.png)'
- en: The resulting 3D point cloud tile. Â© F. Poux
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆçš„3Dç‚¹äº‘ç“¦ç‰‡ã€‚Â© F. Poux
- en: Finally, structuring the 3D point cloud into tiles can also improve the computational
    efficiency of the neural Network, as it allows for parallel processing of the
    tiles, reducing the overall processing time required to analyze the entire point
    cloud (on GPU).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå°†3Dç‚¹äº‘ç»“æ„åŒ–ä¸ºç“¦ç‰‡ä¹Ÿå¯ä»¥æé«˜ç¥ç»ç½‘ç»œçš„è®¡ç®—æ•ˆç‡ï¼Œå› ä¸ºå®ƒå…è®¸å¯¹ç“¦ç‰‡è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œä»è€Œå‡å°‘åˆ†ææ•´ä¸ªç‚¹äº‘æ‰€éœ€çš„æ€»ä½“å¤„ç†æ—¶é—´ï¼ˆåœ¨GPUä¸Šï¼‰ã€‚
- en: We use the â€œCross Sectionâ€ tool (1) to achieve this feat. We set up the size
    to 100 meters (2), we then shift along X and Y (minus) to get as close as possible
    to the lowest corner of the initial tile (3), we use the multiple slice button
    (4), we repeat along and Y axis (5) and get the resulting square tiles (6), as
    shown below.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨â€œæ¨ªæˆªé¢â€å·¥å…·ï¼ˆ1ï¼‰æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚æˆ‘ä»¬å°†å¤§å°è®¾ç½®ä¸º100ç±³ï¼ˆ2ï¼‰ï¼Œç„¶åæ²¿Xè½´å’ŒYè½´ï¼ˆè´Ÿæ–¹å‘ï¼‰ç§»åŠ¨ï¼Œä»¥å°½å¯èƒ½æ¥è¿‘åˆå§‹ç“¦ç‰‡çš„æœ€åº•éƒ¨è§’è½ï¼ˆ3ï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šä¸ªåˆ‡ç‰‡æŒ‰é’®ï¼ˆ4ï¼‰ï¼Œæ²¿Xè½´å’ŒYè½´é‡å¤ï¼ˆ5ï¼‰ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ–¹å½¢ç“¦ç‰‡ï¼ˆ6ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/aa7854693120b84501c12c0d7faca466.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa7854693120b84501c12c0d7faca466.png)'
- en: The process to automate the tile creation within CloudCompare. Â© F. Poux
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨åŒ–ç“¦ç‰‡åˆ›å»ºè¿‡ç¨‹åœ¨CloudCompareä¸­ã€‚Â© F. Poux
- en: '![](../Images/3b33c2155b7469d98c831fd5aa8d380c.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b33c2155b7469d98c831fd5aa8d380c.png)'
- en: The live process to automate the tile creation within CloudCompare. Â© F. Poux
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨åŒ–ç“¦ç‰‡åˆ›å»ºè¿‡ç¨‹åœ¨CloudCompareä¸­ã€‚Â© F. Poux
- en: This allows defining tiles of approximately one hundred meters by one hundred
    meters along X and Y axes. We obtain 143 tiles, from which we discard the last
    13 tiles, as they could be more representative of what we want our input to be
    (i.e., they are not square because they are on the edge). With the remaining 130
    tiles, we choose a good (around 20%) of representative tiles (holding Shift +
    Selection), as shown below.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å…è®¸å®šä¹‰å¤§çº¦ä¸€ç™¾ç±³ä¹˜ä¸€ç™¾ç±³çš„ç“¦ç‰‡ï¼Œæ²¿Xè½´å’ŒYè½´ã€‚æˆ‘ä»¬è·å¾—äº†143ä¸ªç“¦ç‰‡ï¼Œå…¶ä¸­æŠ›å¼ƒäº†æœ€å13ä¸ªç“¦ç‰‡ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½æ›´èƒ½ä»£è¡¨æˆ‘ä»¬å¸Œæœ›è¾“å…¥çš„å†…å®¹ï¼ˆå³ï¼Œå®ƒä»¬ä¸æ˜¯æ–¹å½¢ï¼Œå› ä¸ºå®ƒä»¬ä½äºè¾¹ç¼˜ï¼‰ã€‚åœ¨å‰©ä¸‹çš„130ä¸ªç“¦ç‰‡ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©äº†å¤§çº¦20%å…·æœ‰ä»£è¡¨æ€§çš„ç“¦ç‰‡ï¼ˆæŒ‰ä½Shift
    + é€‰æ‹©ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/61b8548c1516f4385b7d17717237774f.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61b8548c1516f4385b7d17717237774f.png)'
- en: Selection and manual split into training and testing set for PointNet. Â© F.
    Poux
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹PointNetè¿›è¡Œçš„é€‰æ‹©å’Œæ‰‹åŠ¨åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚Â© F. Poux
- en: 'ğŸŒ± **Growing**: *We split our data between training and testing following an
    80/20 percent scheme. At this stage, what do you think about this approach? What
    would be, in your opinion, a good strategy?*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŒ± **å¢é•¿**ï¼š*æˆ‘ä»¬æŒ‰ç…§80/20çš„æ¯”ä¾‹å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œä½ æ€ä¹ˆçœ‹è¿™ç§æ–¹æ³•ï¼Ÿä½ è®¤ä¸ºä»€ä¹ˆæ ·çš„ç­–ç•¥æ¯”è¾ƒå¥½ï¼Ÿ*
- en: At the end of this process, we have around 100 tiles in the train set and 30
    tiles in the test set, each holding the original number of points. We then select
    one folder and export each tile as an ASCII file, as shown below.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†åˆ«æ‹¥æœ‰çº¦100ä¸ªç“¦ç‰‡å’Œ30ä¸ªç“¦ç‰‡ï¼Œæ¯ä¸ªç“¦ç‰‡éƒ½ä¿ç•™äº†åŸå§‹ç‚¹æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œå°†æ¯ä¸ªç“¦ç‰‡å¯¼å‡ºä¸ºASCIIæ–‡ä»¶ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/cc137efa069ddc07435b100aa870e872.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc137efa069ddc07435b100aa870e872.png)'
- en: Exporting the point cloud tiles to use with PointNet
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ç‚¹äº‘ç“¦ç‰‡å¯¼å‡ºä»¥ä¾›PointNetä½¿ç”¨
- en: 'ğŸ¦š **Note**: *CloudCompare allows to export all the point clouds independently
    within a directory when choosing to export as an ASCII file. He will automatically
    indent after the last character, using a â€œ*`*_*`*â€ character to ensure consistency.
    This is very handy and can be used/abused.*'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š **æ³¨æ„**ï¼š*CloudCompare å…è®¸åœ¨é€‰æ‹©ä»¥ ASCII æ–‡ä»¶æ ¼å¼å¯¼å‡ºæ—¶ï¼Œå°†æ‰€æœ‰ç‚¹äº‘ç‹¬ç«‹å¯¼å‡ºåˆ°ä¸€ä¸ªç›®å½•ä¸­ã€‚å®ƒä¼šåœ¨æœ€åä¸€ä¸ªå­—ç¬¦ä¹‹åè‡ªåŠ¨ç¼©è¿›ï¼Œä½¿ç”¨â€œ*`*_*`*â€å­—ç¬¦ä»¥ç¡®ä¿ä¸€è‡´æ€§ã€‚è¿™éå¸¸æ–¹ä¾¿ï¼Œå¯ä»¥ä½¿ç”¨/æ»¥ç”¨ã€‚*
- en: Structuring a 3D point cloud into square tiles is an essential preprocessing
    step when using PointNet. It allows for consistent input data size, preserves
    local context, and improves computational efficiency, all of which contribute
    to more accuracy and efficient processing of the data. This is the final step
    before moving on to 3D Python ğŸ‰.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: å°† 3D ç‚¹äº‘ç»“æ„åŒ–ä¸ºæ–¹å½¢ç“¦ç‰‡æ˜¯ä½¿ç”¨ PointNet æ—¶çš„ä¸€ä¸ªé‡è¦é¢„å¤„ç†æ­¥éª¤ã€‚å®ƒå…è®¸è¾“å…¥æ•°æ®å¤§å°çš„ä¸€è‡´æ€§ï¼Œä¿ç•™å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œå¹¶æé«˜è®¡ç®—æ•ˆç‡ï¼Œè¿™äº›éƒ½æœ‰åŠ©äºæ›´å‡†ç¡®å’Œé«˜æ•ˆçš„æ•°æ®å¤„ç†ã€‚è¿™æ˜¯è¿›å…¥
    3D Python ğŸ‰ä¹‹å‰çš„æœ€åä¸€æ­¥ã€‚
- en: Step 7\. 3D Python Data Loading
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬7æ­¥ã€‚3D Python æ•°æ®åŠ è½½
- en: '![](../Images/e1aae6522def0e1a19d8596c449407f2.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1aae6522def0e1a19d8596c449407f2.png)'
- en: It is time to ingest the point cloud tiles in Python.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯æ—¶å€™åœ¨ Python ä¸­å¤„ç†ç‚¹äº‘ç“¦ç‰‡äº†ã€‚
- en: 'For this, we import the libraries that we need. If you use the Google Colab
    version accessible here: ğŸ’» Google Colab Code, then it is important to run the
    first line as shown below:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¼å…¥æ‰€éœ€çš„åº“ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯å¯ä»¥åœ¨è¿™é‡Œè®¿é—®çš„ Google Colab ç‰ˆæœ¬ï¼šğŸ’» Google Colab Codeï¼Œé‚£ä¹ˆé‡è¦çš„æ˜¯è¦è¿è¡Œä¸‹é¢æ‰€ç¤ºçš„ç¬¬ä¸€è¡Œï¼š
- en: '[PRE5]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For any setup, we have to import the various libraries as illustrated below:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä»»ä½•è®¾ç½®ï¼Œæˆ‘ä»¬å¿…é¡»å¯¼å…¥ä¸‹è¿°å„ç§åº“ï¼š
- en: '[PRE6]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Great! From there, we split the datafile names in our respective folders in
    `pointcloud_train_files`, and `pointcloud_test_files`
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: å¤ªå¥½äº†ï¼ä»è¿™é‡Œå¼€å§‹ï¼Œæˆ‘ä»¬å°†æ•°æ®æ–‡ä»¶åæ‹†åˆ†åˆ°å„è‡ªçš„æ–‡ä»¶å¤¹ä¸­ï¼Œ`pointcloud_train_files`å’Œ`pointcloud_test_files`
- en: '[PRE7]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'ğŸ¦š **Note**: *we have two folders in our explorer: the train folder, and the
    test folder, both in the* `AHN4_33EZ2_12` *folder. What we do here is first to
    give the path to the root folder, and then, we will collect with glob all the
    files in train and test with the* `***` *that means â€œ*`select all.â€` *A convenient
    way to deal with multiple files!*'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š **æ³¨æ„**ï¼š*æˆ‘ä»¬åœ¨èµ„æºç®¡ç†å™¨ä¸­æœ‰ä¸¤ä¸ªæ–‡ä»¶å¤¹ï¼šè®­ç»ƒæ–‡ä»¶å¤¹å’Œæµ‹è¯•æ–‡ä»¶å¤¹ï¼Œå‡åœ¨* `AHN4_33EZ2_12` *æ–‡ä»¶å¤¹ä¸­ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œåšçš„æ˜¯é¦–å…ˆæŒ‡å®šæ ¹æ–‡ä»¶å¤¹çš„è·¯å¾„ï¼Œç„¶åç”¨
    glob æ”¶é›†è®­ç»ƒå’Œæµ‹è¯•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼Œä½¿ç”¨* `***` *è¡¨ç¤ºâ€œ*`é€‰æ‹©æ‰€æœ‰`â€ã€‚*ä¸€ç§å¤„ç†å¤šä¸ªæ–‡ä»¶çš„ä¾¿æ·æ–¹æ³•ï¼*
- en: 'At this step, two variables hold the paths to all the tiles we prepared. To
    ensure that this is correct, we can print one element taken randomly from a 0
    to 20 random distribution:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€æ­¥ï¼Œä¸¤ä¸ªå˜é‡ä¿å­˜äº†æˆ‘ä»¬å‡†å¤‡å¥½çš„æ‰€æœ‰ç“¦ç‰‡çš„è·¯å¾„ã€‚ä¸ºäº†ç¡®ä¿è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥æ‰“å°ä»0åˆ°20çš„éšæœºåˆ†å¸ƒä¸­éšæœºé€‰å–çš„ä¸€ä¸ªå…ƒç´ ï¼š
- en: '[PRE8]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Great, so what we can do, is thus to split further our dataset into three variables:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: å¤ªå¥½äº†ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥å°†æ•°æ®é›†æ‹†åˆ†æˆä¸‰ä¸ªå˜é‡ï¼š
- en: '**valid_list**: This holds the validation data paths. The validation split
    helps to improve the model performance by fine-tuning the model after each epoch.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**valid_list**ï¼šè¿™ä¿å­˜äº†éªŒè¯æ•°æ®è·¯å¾„ã€‚éªŒè¯æ‹†åˆ†é€šè¿‡åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸåå¾®è°ƒæ¨¡å‹æ¥å¸®åŠ©æé«˜æ¨¡å‹æ€§èƒ½ã€‚'
- en: '**train_list**: This holds the training data paths, which is the data set on
    which the training takes place.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**train_list**ï¼šè¿™ä¿å­˜äº†è®­ç»ƒæ•°æ®è·¯å¾„ï¼Œå³ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚'
- en: '**test_list**: This holds the test data paths. The test set informs us about
    the final accuracy of the model after completing the training phase'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**test_list**ï¼šè¿™ä¿å­˜äº†æµ‹è¯•æ•°æ®è·¯å¾„ã€‚æµ‹è¯•é›†å‘ŠçŸ¥æˆ‘ä»¬æ¨¡å‹åœ¨å®Œæˆè®­ç»ƒé˜¶æ®µåçš„æœ€ç»ˆå‡†ç¡®æ€§ã€‚'
- en: This is done using the friendly numpy functions that work on the array indexes
    from the list. Indeed, we randomly extract 20% from the `pointcloud_train_files`,
    then split what is retained for validation vs. what is not retained and constitutes
    the `train_list` variable.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯é€šè¿‡å‹å¥½çš„ numpy å‡½æ•°å®Œæˆçš„ï¼Œè¿™äº›å‡½æ•°ä½œç”¨äºåˆ—è¡¨ä¸­çš„æ•°ç»„ç´¢å¼•ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬éšæœºæå–äº†`pointcloud_train_files`ä¸­çš„20%ï¼Œç„¶åå°†ä¿ç•™çš„éƒ¨åˆ†ä¸æœªä¿ç•™çš„éƒ¨åˆ†è¿›è¡Œåˆ†å‰²ï¼Œåè€…æ„æˆäº†`train_list`å˜é‡ã€‚
- en: '[PRE9]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then randomly study the properties of one data file by looking at the median,
    the standard deviation, and the min-max values with the following snippet:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡æŸ¥çœ‹ä¸­ä½æ•°ã€æ ‡å‡†å·®å’Œæœ€å°-æœ€å¤§å€¼æ¥éšæœºç ”ç©¶ä¸€ä¸ªæ•°æ®æ–‡ä»¶çš„å±æ€§ï¼Œä½¿ç”¨ä»¥ä¸‹ä»£ç ç‰‡æ®µï¼š
- en: '[PRE10]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Which gives:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†äº§ç”Ÿï¼š
- en: '[PRE11]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As we can notice, there is one central element that we have to address: data
    normalization. Indeed, so to avoid any mismatch, we need to be in this â€œcanonical
    space,â€ which means we can replicate the same experimental context in the feature
    space. Using a T-Net would be like killing a fly ğŸª° with a bazooka. This is fine,
    but if we can avoid and use an actual coherent approach, it would be smarter ğŸ˜.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€æ³¨æ„åˆ°çš„ï¼Œæœ‰ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜éœ€è¦è§£å†³ï¼šæ•°æ®å½’ä¸€åŒ–ã€‚ç¡®å®ï¼Œä¸ºäº†é¿å…ä»»ä½•ä¸åŒ¹é…ï¼Œæˆ‘ä»¬éœ€è¦å¤„äºè¿™ç§â€œè§„èŒƒç©ºé—´â€ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥åœ¨ç‰¹å¾ç©ºé—´ä¸­å¤åˆ¶ç›¸åŒçš„å®éªŒç¯å¢ƒã€‚ä½¿ç”¨T-Netå°±åƒç”¨ç«ç®­ç­’ğŸª°æ€è‹è‡ã€‚è¿™æ˜¯å¯ä»¥çš„ï¼Œä½†å¦‚æœæˆ‘ä»¬å¯ä»¥é¿å…å¹¶ä½¿ç”¨å®é™…ä¸€è‡´çš„æ–¹æ³•ï¼Œé‚£å°†æ›´èªæ˜ğŸ˜ã€‚
- en: Step 8\. 3D Python Normalization
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 8\. 3D Python å½’ä¸€åŒ–
- en: '![](../Images/5ed3e236dc6c9533e86a3b7760f58254.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ed3e236dc6c9533e86a3b7760f58254.png)'
- en: Normalizing a 3D point cloud tile before feeding it to the PointNet architecture
    is crucial for three main reasons. First, normalization ensures that the input
    data is centered around the origin, which is essential for PointNetâ€™s architecture
    which applies an MLP to each point independently. The MLP is more effective when
    the input data is centered around the origin, which allows for more meaningful
    feature extraction and better overall performance.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å°†3Dç‚¹äº‘ç“¦ç‰‡è¾“å…¥åˆ°PointNetæ¶æ„ä¹‹å‰è¿›è¡Œå½’ä¸€åŒ–è‡³å…³é‡è¦ï¼Œä¸»è¦æœ‰ä¸‰ä¸ªåŸå› ã€‚é¦–å…ˆï¼Œå½’ä¸€åŒ–ç¡®ä¿è¾“å…¥æ•°æ®å›´ç»•åŸç‚¹ä¸­å¿ƒï¼Œè¿™å¯¹äºPointNetçš„æ¶æ„è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¯¹æ¯ä¸ªç‚¹ç‹¬ç«‹åº”ç”¨MLPã€‚å½“è¾“å…¥æ•°æ®å›´ç»•åŸç‚¹ä¸­å¿ƒæ—¶ï¼ŒMLPæ›´æœ‰æ•ˆï¼Œè¿™ä½¿å¾—ç‰¹å¾æå–æ›´æœ‰æ„ä¹‰ï¼Œæ•´ä½“æ€§èƒ½æ›´å¥½ã€‚
- en: '![](../Images/e7e59d8169a06fc63c88463815748d87.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7e59d8169a06fc63c88463815748d87.png)'
- en: Illustration of the normalization impact on the results of training 3D Deep
    Learning Models. Â© F. Poux
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: å½’ä¸€åŒ–å¯¹è®­ç»ƒ3Dæ·±åº¦å­¦ä¹ æ¨¡å‹ç»“æœçš„å½±å“ç¤ºæ„å›¾ã€‚Â© F. Poux
- en: 'ğŸŒ± **Growing**: *Some sort of intuition is also good before normalizing bluntly.
    For example, we predominantly use gravity-based scenes, meaning that the Z-axis
    is almost always colinear to the Z-axis. Therefore, how would you approach this
    normalization?*'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŒ± **æˆé•¿**ï¼š*åœ¨ç›²ç›®å½’ä¸€åŒ–ä¹‹å‰ï¼Œæœ‰äº›ç›´è§‰ä¹Ÿæ˜¯æœ‰ç›Šçš„ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬ä¸»è¦ä½¿ç”¨åŸºäºé‡åŠ›çš„åœºæ™¯ï¼Œè¿™æ„å‘³ç€Zè½´å‡ ä¹æ€»æ˜¯ä¸Zè½´å…±çº¿ã€‚å› æ­¤ï¼Œä½ ä¼šå¦‚ä½•å¤„ç†è¿™ç§å½’ä¸€åŒ–ï¼Ÿ*
- en: Secondly, normalization scales the point cloud data to a consistent range, which
    helps prevent saturation of the activation functions within the MLP. This allows
    the Network to learn from the entire range of input values, improving its ability
    to accurately classify or segment the data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œå½’ä¸€åŒ–å°†ç‚¹äº‘æ•°æ®ç¼©æ”¾åˆ°ä¸€è‡´çš„èŒƒå›´ï¼Œè¿™æœ‰åŠ©äºé˜²æ­¢MLPä¸­çš„æ¿€æ´»å‡½æ•°é¥±å’Œã€‚è¿™ä½¿å¾—ç½‘ç»œèƒ½å¤Ÿä»æ•´ä¸ªè¾“å…¥å€¼èŒƒå›´ä¸­å­¦ä¹ ï¼Œæé«˜äº†å‡†ç¡®åˆ†ç±»æˆ–åˆ†å‰²æ•°æ®çš„èƒ½åŠ›ã€‚
- en: '![](../Images/e4f256a4ace3ba7acdfac220fe4cdac1.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4f256a4ace3ba7acdfac220fe4cdac1.png)'
- en: The problem with [0,1] scaling illustrated on the intensity field of 3D point
    clouds. Â© F. Poux
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨3Dç‚¹äº‘çš„å¼ºåº¦åœºä¸Šè¯´æ˜[0,1]ç¼©æ”¾çš„é—®é¢˜ã€‚Â© F. Poux
- en: Finally, normalization can help reduce the impact of different scales in the
    point cloud data, which can be caused by differences in sensor resolutions or
    distances from the scanned objects (which is a bit flattened in the case of Aerial
    LiDAR data). This improves the consistency of the data and the Networkâ€™s ability
    to extract meaningful features from it.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå½’ä¸€åŒ–æœ‰åŠ©äºå‡å°‘ç‚¹äº‘æ•°æ®ä¸­ä¸åŒå°ºåº¦çš„å½±å“ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºä¼ æ„Ÿå™¨åˆ†è¾¨ç‡æˆ–æ‰«æç‰©ä½“çš„è·ç¦»å·®å¼‚ï¼ˆåœ¨èˆªæ‹LiDARæ•°æ®ä¸­æœ‰æ‰€å¹³å¦åŒ–ï¼‰ã€‚è¿™æé«˜äº†æ•°æ®çš„ä¸€è‡´æ€§å’Œç½‘ç»œä»ä¸­æå–æœ‰æ„ä¹‰ç‰¹å¾çš„èƒ½åŠ›ã€‚
- en: 'Okay, let us get on it. For our experiments, we will first capture the minimum
    value of the features in `min_f`, and the average in `mean_f`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ã€‚å¯¹äºæˆ‘ä»¬çš„å®éªŒï¼Œæˆ‘ä»¬å°†é¦–å…ˆæ•æ‰ç‰¹å¾çš„æœ€å°å€¼`min_f`å’Œå¹³å‡å€¼`mean_f`ï¼š
- en: '[PRE12]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'ğŸ¦š **Note**: *We transposed our dataset to handle the data and the indexes much
    more efficiently and conveniently. Therefore, to take the* `X-axis` *elements
    of the point cloud, we can just pass* `cloud_data[0]` *instead of* `cloud_data[:,0]`*,
    which involves a bit of overhead.*'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š **æ³¨æ„**ï¼š*æˆ‘ä»¬å¯¹æ•°æ®é›†è¿›è¡Œäº†è½¬ç½®ï¼Œä»¥æ›´é«˜æ•ˆã€ä¾¿æ·åœ°å¤„ç†æ•°æ®å’Œç´¢å¼•ã€‚å› æ­¤ï¼Œè¦è·å–ç‚¹äº‘çš„* `X-axis` *å…ƒç´ ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨* `cloud_data[0]`
    *è€Œä¸æ˜¯* `cloud_data[:,0]`*ï¼Œè¿™æ ·å¯ä»¥å‡å°‘ä¸€äº›å¼€é”€ã€‚*
- en: 'We will now normalize the different features to use in our PointNet networks.
    First, the spatial coordinates X, Y, and Z. We will center our data on the planimetric
    axis (X and Y) and ensure that we subtract the minimal value of Z to account for
    discrimination between roofs and ground, for example:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å°†å¯¹ä¸åŒçš„ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥ç”¨äºæˆ‘ä»¬çš„PointNetç½‘ç»œã€‚é¦–å…ˆæ˜¯ç©ºé—´åæ ‡Xã€Yå’ŒZã€‚æˆ‘ä»¬å°†æŠŠæ•°æ®ä¸­å¿ƒåŒ–åˆ°å¹³é¢åæ ‡è½´ï¼ˆXå’ŒYï¼‰ï¼Œå¹¶ç¡®ä¿å‡å»Zçš„æœ€å°å€¼ï¼Œä»¥åŒºåˆ†å±‹é¡¶å’Œåœ°é¢ï¼Œä¾‹å¦‚ï¼š
- en: '[PRE13]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Great, now we can scale our colors by ensuring we are in a [0,1] range. This
    is done by dividing the max value (255) for all our colors:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥é€šè¿‡ç¡®ä¿æˆ‘ä»¬åœ¨[0,1]èŒƒå›´å†…æ¥ç¼©æ”¾æˆ‘ä»¬çš„é¢œè‰²ã€‚è¿™æ˜¯é€šè¿‡å°†æ‰€æœ‰é¢œè‰²çš„æœ€å¤§å€¼ï¼ˆ255ï¼‰è¿›è¡Œé™¤æ³•å®ç°çš„ï¼š
- en: '[PRE14]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we will attack the normalization of the intensity feature. Here, we
    will work with quantiles to obtain a normalization robust to outliers, as we saw
    when exploring our data. This is done in a three-stage process. First, we compute
    the interquartile difference `IQR`, which is the difference between the 75th and
    25th quantile. Then we subtract the median from all the observations and divide
    by the interquartile difference. Finally, we subtract the minimum value of the
    intensity to have a significant normalization:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°†å¤„ç†å¼ºåº¦ç‰¹å¾çš„å½’ä¸€åŒ–ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åˆ†ä½æ•°æ¥è·å¾—å¯¹å¼‚å¸¸å€¼å…·æœ‰é²æ£’æ€§çš„å½’ä¸€åŒ–ï¼Œå°±åƒæˆ‘ä»¬åœ¨æ¢ç´¢æ•°æ®æ—¶çœ‹åˆ°çš„é‚£æ ·ã€‚è¿™æ˜¯é€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„è¿‡ç¨‹å®Œæˆçš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¡ç®—å››åˆ†ä½å·®`IQR`ï¼Œå³ç¬¬
    75 ä¸ªåˆ†ä½æ•°å’Œç¬¬ 25 ä¸ªåˆ†ä½æ•°ä¹‹é—´çš„å·®å€¼ã€‚ç„¶åæˆ‘ä»¬ä»æ‰€æœ‰è§‚æµ‹å€¼ä¸­å‡å»ä¸­ä½æ•°ï¼Œå¹¶é™¤ä»¥å››åˆ†ä½å·®ã€‚æœ€åï¼Œæˆ‘ä»¬å‡å»å¼ºåº¦çš„æœ€å°å€¼ï¼Œä»¥è·å¾—æ˜¾è‘—çš„å½’ä¸€åŒ–ï¼š
- en: '[PRE15]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Wonderful! At this stage, we have a point cloud normalized and ready to be fed
    to a PointNet architecture. But automating this process is the next logical step
    to execute this on all the tiles.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: å¤ªæ£’äº†ï¼åœ¨è¿™ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å·²ç»æœ‰äº†ä¸€ä¸ªå½’ä¸€åŒ–çš„ç‚¹äº‘ï¼Œå‡†å¤‡å¥½è¾“å…¥ PointNet æ¶æ„ã€‚ä½†æ˜¯è‡ªåŠ¨åŒ–è¿™ä¸ªè¿‡ç¨‹æ˜¯æ‰§è¡Œæ‰€æœ‰ç“¦ç‰‡çš„ä¸‹ä¸€æ­¥é€»è¾‘æ­¥éª¤ã€‚
- en: Creating a Point Cloud Tile Load and Normalize function
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªç‚¹äº‘ç“¦ç‰‡åŠ è½½å’Œå½’ä¸€åŒ–å‡½æ•°
- en: 'We create a function `cloud_loader` that takes as input the path to a tile
    `tile_path`, and a string of features used, `features_used`, and outputs a `cloud_data`
    variable, which holds the normalized features, along with its ground-truth variable
    `gt` that holds the labels of each point. The function will act as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°`cloud_loader`ï¼Œå®ƒä»¥ä¸€ä¸ªç“¦ç‰‡è·¯å¾„`tile_path`å’Œä¸€ä¸ªç”¨äºçš„ç‰¹å¾å­—ç¬¦ä¸²`features_used`ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ª`cloud_data`å˜é‡ï¼Œå…¶ä¸­åŒ…å«å½’ä¸€åŒ–ç‰¹å¾ï¼Œä»¥åŠä¸€ä¸ªçœŸå®æ ‡ç­¾å˜é‡`gt`ï¼Œå…¶ä¸­åŒ…å«æ¯ä¸ªç‚¹çš„æ ‡ç­¾ã€‚è¯¥å‡½æ•°å°†å¦‚ä¸‹æ“ä½œï¼š
- en: '![](../Images/8098b012294e7a76b4bb61349c8e7bbd.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8098b012294e7a76b4bb61349c8e7bbd.png)'
- en: The definition of a cloud loading function to process point cloud datasets and
    make them ready for training. Â© F. Poux
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰ä¸€ä¸ªäº‘åŠ è½½å‡½æ•°æ¥å¤„ç†ç‚¹äº‘æ•°æ®é›†ï¼Œå¹¶ä½¿å…¶å‡†å¤‡å¥½ç”¨äºè®­ç»ƒã€‚Â© F. Poux
- en: 'This translates into a simple `cloud_loader` function, as shown below:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è½¬æ¢ä¸ºä¸€ä¸ªç®€å•çš„`cloud_loader`å‡½æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE16]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This function is now used to obtain both point cloud features and labels as
    follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°ç°åœ¨ç”¨äºè·å–ç‚¹äº‘ç‰¹å¾å’Œæ ‡ç­¾ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE17]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'ğŸŒ± **Growing**: *As you can see, we pass a string for the features. This is
    very convenient for our different â€˜*`*if*`*â€™ tests indeed. However, note that
    we do not return errors if what is fed to the function is not expected. This is
    not a standard code practice, but this extends the scope of this tutorial*. *I
    recommend checking out PEP-8 guidelines if you want to start with beautiful code
    writing.*'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŒ± **æˆé•¿**ï¼š*å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬ä¼ é€’äº†ä¸€ä¸ªç‰¹å¾çš„å­—ç¬¦ä¸²ã€‚è¿™å¯¹äºæˆ‘ä»¬ä¸åŒçš„â€˜*`*if*`*â€™æµ‹è¯•éå¸¸æ–¹ä¾¿ã€‚ç„¶è€Œï¼Œè¯·æ³¨æ„ï¼Œå¦‚æœä¼ é€’ç»™å‡½æ•°çš„å†…å®¹ä¸ç¬¦åˆé¢„æœŸï¼Œæˆ‘ä»¬ä¸ä¼šè¿”å›é”™è¯¯ã€‚è¿™ä¸æ˜¯æ ‡å‡†çš„ä»£ç å®è·µï¼Œä½†è¿™æ‰©å±•äº†æœ¬æ•™ç¨‹çš„èŒƒå›´*ã€‚
    *å¦‚æœä½ æƒ³å¼€å§‹ç¼–å†™æ¼‚äº®çš„ä»£ç ï¼Œæˆ‘å»ºè®®æŸ¥çœ‹ PEP-8 æŒ‡å—ã€‚*
- en: Step 9\. 3D Python Interactive Visualization
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 9\. 3D Python äº¤äº’å¼å¯è§†åŒ–
- en: '![](../Images/38078309e7c97475a7442c021a49a0f2.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38078309e7c97475a7442c021a49a0f2.png)'
- en: 'If we want to parallel a previous article, accessible here:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³è¦å¹³è¡Œäºä»¥å‰çš„æ–‡ç« ï¼Œå¯ä»¥åœ¨è¿™é‡Œè®¿é—®ï¼š
- en: '[](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----90398f880c9f--------------------------------)
    [## 3D Python Workflows for LiDAR City Models: A Step-by-Step Guide'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----90398f880c9f--------------------------------)
    [## LiDAR åŸå¸‚æ¨¡å‹çš„ 3D Python å·¥ä½œæµï¼šé€æ­¥æŒ‡å—'
- en: The Ultimate Guide to unlocking a streamlined workflow for 3D City Modelling
    Applications. The tutorial covers Pythonâ€¦
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£é” 3D åŸå¸‚å»ºæ¨¡åº”ç”¨çš„**ç»ˆææŒ‡å—**ã€‚è¯¥æ•™ç¨‹æ¶µç›– Pythonâ€¦
- en: towardsdatascience.com](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----90398f880c9f--------------------------------)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----90398f880c9f--------------------------------)'
- en: 'We can visualize our dataset with Open3D. First, we need to install a specific
    version (if working on a Jupyter Notebook environment, such as Google Colab or
    the CRIB platform) and load it in our script:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Open3D å¯è§†åŒ–æˆ‘ä»¬çš„æ•°æ®é›†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…ä¸€ä¸ªç‰¹å®šç‰ˆæœ¬ï¼ˆå¦‚æœåœ¨ Jupyter Notebook ç¯å¢ƒä¸­å·¥ä½œï¼Œå¦‚ Google
    Colab æˆ– CRIB å¹³å°ï¼‰ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„è„šæœ¬ä¸­åŠ è½½å®ƒï¼š
- en: '[PRE18]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'ğŸ¦š **Note**: T*he â€œ*`!`*â€ before pip is used when you work on Google Colab to
    say it should use the environment console directly. If you work locally, you should
    delete this character and use* `pip install open3d==0.16` *directly.*'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š **æ³¨æ„**ï¼š*åœ¨ pip ä¹‹å‰çš„â€œ*`!`*â€æ˜¯åœ¨ Google Colab ä¸Šå·¥ä½œæ—¶ä½¿ç”¨çš„ï¼Œè¡¨ç¤ºå®ƒåº”è¯¥ç›´æ¥ä½¿ç”¨ç¯å¢ƒæ§åˆ¶å°ã€‚å¦‚æœä½ åœ¨æœ¬åœ°å·¥ä½œï¼Œåº”åˆ é™¤æ­¤å­—ç¬¦å¹¶ç›´æ¥ä½¿ç”¨*
    `pip install open3d==0.16` *ã€‚*
- en: 'Then we run the following successive steps :'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æ‰§è¡Œä»¥ä¸‹è¿ç»­æ­¥éª¤ï¼š
- en: '![](../Images/f4c17ac7191f63ac5111517f2b34dcc5.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4c17ac7191f63ac5111517f2b34dcc5.png)'
- en: The drawing function to plot interactive 3D scenes directly within Google Colab.
    Â© F. Poux
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ç»˜åˆ¶å‡½æ•°ä»¥ç›´æ¥åœ¨ Google Colab ä¸­ç»˜åˆ¶äº¤äº’å¼ 3D åœºæ™¯ã€‚Â© F. Poux
- en: 'Which translates into the following code lines:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è½¬åŒ–ä¸ºä»¥ä¸‹ä»£ç è¡Œï¼š
- en: '[PRE19]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'ğŸ¦š**Note**: *As our pc variable capturing the* `cloud_data` *output of our*
    `cloud_loader` *function is transposed, we must not forget to transpose it back
    when plotting with* `open3d`*.*'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š**æ³¨æ„**ï¼š*ç”±äºæˆ‘ä»¬çš„ pc å˜é‡æ•è·äº†* `cloud_data` *æ¥è‡ª* `cloud_loader` *å‡½æ•°çš„è¾“å‡ºè¢«è½¬ç½®ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨* `open3d`
    *ç»˜å›¾æ—¶å¿…é¡»è®°å¾—å°†å…¶è½¬ç½®å›å»ã€‚*
- en: 'The previous code snippet will output the following visualization:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ä»£ç ç‰‡æ®µå°†è¾“å‡ºä»¥ä¸‹å¯è§†åŒ–ï¼š
- en: '![](../Images/4bdc90b04fb717cd5e4223afe246cdbd.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bdc90b04fb717cd5e4223afe246cdbd.png)'
- en: The results of plotting scenes with plotly and R,G,B fields. Â© F. Poux
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ plotly ç»˜åˆ¶åœºæ™¯å’Œ Rã€Gã€B å­—æ®µçš„ç»“æœã€‚Â© F. Poux
- en: 'ğŸ¦š **Note**: *when using the* `draw_plotly` *function, we do not have a direct
    hand in the scaling of the plot, and we can notice that we have non-equal scales
    for* `*X*`*,* `*Y,*` *and* `*Z*`*, which emphasizes the Z largely in that case.*
    ğŸ˜'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š **æ³¨æ„**ï¼š*ä½¿ç”¨* `draw_plotly` *å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥æ§åˆ¶å›¾è¡¨çš„ç¼©æ”¾ï¼Œå¹¶ä¸”å¯ä»¥æ³¨æ„åˆ°* `*X*`*,* `*Y,*` *å’Œ*
    `*Z*`*çš„æ¯”ä¾‹ä¸ç­‰ï¼Œè¿™åœ¨è¿™ç§æƒ…å†µä¸‹å¼ºè°ƒäº† Zã€‚* ğŸ˜
- en: 'Due to the limitations that you can notice, we create a custom visualization
    function to visualize a random tile so that running the function: `visualize_input_tile`
    outputs an interactive `plotly` visualization that lets us switch the rendering
    mode.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºä½ å¯ä»¥æ³¨æ„åˆ°çš„é™åˆ¶ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªè‡ªå®šä¹‰å¯è§†åŒ–å‡½æ•°æ¥å¯è§†åŒ–éšæœºåˆ‡ç‰‡ï¼Œä»¥ä¾¿è¿è¡Œå‡½æ•°ï¼š`visualize_input_tile` è¾“å‡ºä¸€ä¸ªäº¤äº’å¼çš„ `plotly`
    å¯è§†åŒ–ï¼Œè®©æˆ‘ä»¬åˆ‡æ¢æ¸²æŸ“æ¨¡å¼ã€‚
- en: 'To test the provided function, we first need to define the class names in our
    experiments: `class_names = [â€˜unclassifiedâ€™, â€˜groundâ€™, â€˜vegetationâ€™, â€˜buildingsâ€™,
    â€˜waterâ€™]`. Then, we provide the cloud features `cloud_features=â€™xyziâ€™`, randomly
    select a point cloud captured in the variable `selection`, and visualize the tile.
    This translates into the following code snippet:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æµ‹è¯•æä¾›çš„å‡½æ•°ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦åœ¨å®éªŒä¸­å®šä¹‰ç±»åç§°ï¼š`class_names = [â€˜unclassifiedâ€™, â€˜groundâ€™, â€˜vegetationâ€™,
    â€˜buildingsâ€™, â€˜waterâ€™]`ã€‚ç„¶åï¼Œæˆ‘ä»¬æä¾›äº‘ç‰¹å¾`cloud_features=â€™xyziâ€™`ï¼Œéšæœºé€‰æ‹©å˜é‡`selection`ä¸­æ•è·çš„ç‚¹äº‘ï¼Œå¹¶å¯è§†åŒ–åˆ‡ç‰‡ã€‚è¿™è½¬åŒ–ä¸ºä»¥ä¸‹ä»£ç ç‰‡æ®µï¼š
- en: '[PRE20]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: which outputs the interactive scene below.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è¾“å‡ºå¦‚ä¸‹äº¤äº’å¼åœºæ™¯ã€‚
- en: '![](../Images/30e3f33bf92ffaab3aa2cf9ac4ee8593.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30e3f33bf92ffaab3aa2cf9ac4ee8593.png)'
- en: interactive 3D scene within Google Colab. Â© F. Poux
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Google Colab ä¸­çš„äº¤äº’å¼ 3D åœºæ™¯ã€‚Â© F. Poux
- en: 'ğŸ¦š **Note**: *You can use the button to switch rendering modes between the feature
    intensity and the labels from the loaded features of interest.*'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š **æ³¨æ„**ï¼š*ä½ å¯ä»¥ä½¿ç”¨æŒ‰é’®åœ¨ç‰¹å¾å¼ºåº¦å’Œä»åŠ è½½çš„æ„Ÿå…´è¶£ç‰¹å¾çš„æ ‡ç­¾ä¹‹é—´åˆ‡æ¢æ¸²æŸ“æ¨¡å¼ã€‚*
- en: We have a working solution for loading, normalizing, and visualizing a single
    tile in Python. The last step is to create what we call a tensor for use with
    the PointNet Architecture.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ä¸€ä¸ªå·¥ä½œè§£å†³æ–¹æ¡ˆç”¨äºåŠ è½½ã€è§„èŒƒåŒ–å’Œå¯è§†åŒ– Python ä¸­çš„å•ä¸ªåˆ‡ç‰‡ã€‚æœ€åä¸€æ­¥æ˜¯åˆ›å»ºæˆ‘ä»¬ç§°ä¹‹ä¸ºå¼ é‡çš„å†…å®¹ï¼Œä»¥ç”¨äº PointNet æ¶æ„ã€‚
- en: Step 10\. Tensor Creation
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ 10 æ­¥ã€‚å¼ é‡åˆ›å»º
- en: '![](../Images/5a4bea8711a8eb6399d3d4d008598f37.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a4bea8711a8eb6399d3d4d008598f37.png)'
- en: 'I want to show you how we can use PyTorch as an initiation. For clarity concerns,
    let me quickly define the primary Python object type we manipulate with this library:
    a tensor.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨ PyTorch è¿›è¡Œåˆæ­¥æ“ä½œã€‚ä¸ºæ¸…æ™°èµ·è§ï¼Œè®©æˆ‘å¿«é€Ÿå®šä¹‰æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªåº“æ—¶æ“ä½œçš„ä¸»è¦ Python å¯¹è±¡ç±»å‹ï¼šå¼ é‡ã€‚
- en: 'A PyTorch tensor is a multi-dimensional array used for storing and manipulating
    data in PyTorch. It is similar to a NumPy array but with the added benefit of
    being optimized for use with deep learning models. Tensors can be created using
    the `torch.tensor()` function and initialized with data or created as an empty
    tensor with a specified shape. For example, to create a 3x3 tensor with random
    data:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch å¼ é‡æ˜¯ä¸€ä¸ªå¤šç»´æ•°ç»„ï¼Œç”¨äºåœ¨ PyTorch ä¸­å­˜å‚¨å’Œæ“ä½œæ•°æ®ã€‚å®ƒç±»ä¼¼äº NumPy æ•°ç»„ï¼Œä½†å…·æœ‰é’ˆå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹ä¼˜åŒ–çš„é¢å¤–å¥½å¤„ã€‚å¼ é‡å¯ä»¥ä½¿ç”¨
    `torch.tensor()` å‡½æ•°åˆ›å»ºï¼Œå¹¶ç”¨æ•°æ®åˆå§‹åŒ–ï¼Œæˆ–è€…åˆ›å»ºä¸€ä¸ªå…·æœ‰æŒ‡å®šå½¢çŠ¶çš„ç©ºå¼ é‡ã€‚ä¾‹å¦‚ï¼Œè¦åˆ›å»ºä¸€ä¸ª 3x3 çš„å¼ é‡å¹¶å¡«å……éšæœºæ•°æ®ï¼š
- en: '[PRE21]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'which will output:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è¾“å‡ºï¼š
- en: '[PRE22]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Pretty straightforward, hun? Now, to make things easier for us, there is also
    a tiny Pytorch library that we can use to prepare lists of datasets. This library
    is called `TorchNet`. `TorchNet` is designed to simplify the process of building
    and training complex neural network architectures by providing a set of predefined
    modules and helper functions for everyday tasks such as data loading, validation,
    and testing.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸å½“ç®€å•ï¼Œå¯¹å§ï¼Ÿç°åœ¨ï¼Œä¸ºäº†ç®€åŒ–æ“ä½œï¼Œè¿˜æœ‰ä¸€ä¸ªå°å‹çš„ Pytorch åº“ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ¥å‡†å¤‡æ•°æ®é›†åˆ—è¡¨ã€‚è¿™ä¸ªåº“å«åš`TorchNet`ã€‚`TorchNet`
    æ—¨åœ¨é€šè¿‡æä¾›ä¸€ç»„é¢„å®šä¹‰çš„æ¨¡å—å’ŒåŠ©æ‰‹å‡½æ•°æ¥ç®€åŒ–æ„å»ºå’Œè®­ç»ƒå¤æ‚ç¥ç»ç½‘ç»œæ¶æ„çš„è¿‡ç¨‹ï¼Œè¿™äº›æ¨¡å—å’ŒåŠ©æ‰‹å‡½æ•°é€‚ç”¨äºæ•°æ®åŠ è½½ã€éªŒè¯å’Œæµ‹è¯•ç­‰æ—¥å¸¸ä»»åŠ¡ã€‚
- en: One of the main advantages of `TorchNet` is its modular design, which allows
    users to easily construct complex neural network architectures by combining a
    series of pre-built modules. This can save significant time and effort compared
    to building neural networks from scratch, especially when new to deep learning.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`TorchNet` çš„ä¸»è¦ä¼˜åŠ¿ä¹‹ä¸€æ˜¯å…¶æ¨¡å—åŒ–è®¾è®¡ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡ç»„åˆä¸€ç³»åˆ—é¢„æ„å»ºæ¨¡å—æ¥è½»æ¾æ„å»ºå¤æ‚çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚è¿™å¯ä»¥èŠ‚çœå¤§é‡æ—¶é—´å’Œç²¾åŠ›ï¼Œç›¸æ¯”ä»å¤´å¼€å§‹æ„å»ºç¥ç»ç½‘ç»œï¼Œå°¤å…¶æ˜¯åœ¨åˆšæ¥è§¦æ·±åº¦å­¦ä¹ æ—¶ã€‚'
- en: 'ğŸ¦š **Note**: *Besides its modular design, TorchNet provides several helper functions
    for common deep learning tasks, such as data augmentation, early stopping, and
    model checkpointing. This can help users to achieve better results and optimize
    their neural network architectures more efficiently*.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦š **æ³¨æ„**ï¼š*é™¤äº†å…¶æ¨¡å—åŒ–è®¾è®¡å¤–ï¼ŒTorchNet è¿˜æä¾›äº†å¤šä¸ªç”¨äºå¸¸è§æ·±åº¦å­¦ä¹ ä»»åŠ¡çš„åŠ©æ‰‹å‡½æ•°ï¼Œä¾‹å¦‚æ•°æ®å¢å¼ºã€æ—©æœŸåœæ­¢å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚è¿™å¯ä»¥å¸®åŠ©ç”¨æˆ·è·å¾—æ›´å¥½çš„ç»“æœï¼Œå¹¶æ›´é«˜æ•ˆåœ°ä¼˜åŒ–ä»–ä»¬çš„ç¥ç»ç½‘ç»œæ¶æ„*ã€‚
- en: 'To install `torchnet` version `0.0.4` and import it into our script, we can
    do the following:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å®‰è£…`torchnet`ç‰ˆæœ¬`0.0.4`å¹¶å°†å…¶å¯¼å…¥åˆ°æˆ‘ä»¬çš„è„šæœ¬ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: '[PRE23]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We also import another utility module called `functools`. This module is for
    higher-order functions that act on or return other functions. For this, add to
    the import statements `import functools`.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯¼å…¥äº†å¦ä¸€ä¸ªåä¸º `functools` çš„å®ç”¨æ¨¡å—ã€‚è¯¥æ¨¡å—ç”¨äºå¤„ç†æˆ–è¿”å›å…¶ä»–å‡½æ•°çš„é«˜é˜¶å‡½æ•°ã€‚ä¸ºæ­¤ï¼Œå°† `import functools` æ·»åŠ åˆ°å¯¼å…¥è¯­å¥ä¸­ã€‚
- en: 'In general, any callable object can be treated as a function for the purposes
    of this module. From this additional setup, it is straightforward to generate
    the train, validation, and test sets with the following four lines of code:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œä»»ä½•å¯è°ƒç”¨çš„å¯¹è±¡éƒ½å¯ä»¥è¢«è§†ä¸ºæ­¤æ¨¡å—çš„å‡½æ•°ã€‚é€šè¿‡è¿™äº›é¢å¤–çš„è®¾ç½®ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å››è¡Œä»£ç è½»æ¾ç”Ÿæˆè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼š
- en: '[PRE24]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, if we would like to explore, you can use indexes like a classical numpy
    array to retrieve a tensor on a specific position, such as `train_set[1]`, which
    outputs:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦æ¢ç´¢ï¼Œå¯ä»¥ä½¿ç”¨åƒç»å…¸çš„ numpy æ•°ç»„ä¸€æ ·çš„ç´¢å¼•æ¥æ£€ç´¢ç‰¹å®šä½ç½®çš„å¼ é‡ï¼Œä¾‹å¦‚`train_set[1]`ï¼Œå…¶è¾“å‡ºä¸ºï¼š
- en: '![](../Images/8de3614b4899010c759ea2104f9c0e0a.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8de3614b4899010c759ea2104f9c0e0a.png)'
- en: 'Finally, we have to save our results to a Python object to use straight out
    of the box for the following steps, such as PointNet training. We are using the
    library pickle, which is handy for saving Python objects. To save an object, just
    run the following:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¿…é¡»å°†ç»“æœä¿å­˜åˆ° Python å¯¹è±¡ä¸­ï¼Œä»¥ä¾¿åœ¨æ¥ä¸‹æ¥çš„æ­¥éª¤ä¸­ç›´æ¥ä½¿ç”¨ï¼Œä¾‹å¦‚ PointNet è®­ç»ƒã€‚æˆ‘ä»¬ä½¿ç”¨çš„åº“æ˜¯ pickleï¼Œå®ƒéå¸¸é€‚åˆä¿å­˜
    Python å¯¹è±¡ã€‚è¦ä¿å­˜ä¸€ä¸ªå¯¹è±¡ï¼Œåªéœ€è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
- en: '[PRE25]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If you want to test your setup, you can also run the following lines of code
    and ensure that you retrieve back what you intend:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³æµ‹è¯•ä½ çš„è®¾ç½®ï¼Œè¿˜å¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç è¡Œï¼Œç¡®ä¿ä½ æ£€ç´¢åˆ°ä½ æƒ³è¦çš„å†…å®¹ï¼š
- en: '[PRE26]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'ğŸ’» Get Access to the Code here: [Google Colab](https://colab.research.google.com/drive/1pqBqGPV36_gxi4yjPiTUR3fb5IldpdaS?usp=sharing)'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ’» åœ¨è¿™é‡Œè·å–ä»£ç è®¿é—®ï¼š[Google Colab](https://colab.research.google.com/drive/1pqBqGPV36_gxi4yjPiTUR3fb5IldpdaS?usp=sharing)
- en: 'ğŸ‡ Get Access to the Data here: [3D Datasets](https://drive.google.com/drive/folders/1RPCX2NCBn24g4lC3qS_xhuS1peR_EnxM?usp=sharing)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ‡ åœ¨è¿™é‡Œè·å–æ•°æ®è®¿é—®ï¼š[3D æ•°æ®é›†](https://drive.google.com/drive/folders/1RPCX2NCBn24g4lC3qS_xhuS1peR_EnxM?usp=sharing)
- en: 'ğŸ‘¨â€ğŸ«3D Data Processing and AI Courses: [3D Academy](https://learngeodata.eu/)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ‘¨â€ğŸ«3D æ•°æ®å¤„ç†å’Œ AI è¯¾ç¨‹ï¼š[3D å­¦é™¢](https://learngeodata.eu/)
- en: 'ğŸ“– Subscribe to get early access to 3D Tutorials: [3D AI Automation](https://medium.com/@florentpoux/subscribe)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ“– è®¢é˜…ä»¥è·å¾— 3D æ•™ç¨‹çš„æ—©æœŸè®¿é—®ï¼š[3D AI è‡ªåŠ¨åŒ–](https://medium.com/@florentpoux/subscribe)
- en: 'ğŸ’ Support my work with Medium ğŸ¤Ÿ: [Medium Subscription](https://medium.com/@florentpoux/membership)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ’ æ”¯æŒæˆ‘çš„å·¥ä½œä¸ Medium ğŸ¤Ÿï¼š[Medium è®¢é˜…](https://medium.com/@florentpoux/membership)
- en: '![](../Images/9d299f0b910437eb61e9167de8868a19.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d299f0b910437eb61e9167de8868a19.png)'
- en: ğŸ”® Conclusion
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ”® ç»“è®º
- en: Congratulations! In this hands-on tutorial, we explored the critical steps in
    preparing 3D point cloud data from aerial LiDAR scans for use with the PointNet
    architecture.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œï¼åœ¨è¿™ä¸ªå®è·µæ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å‡†å¤‡ç”¨äº PointNet æ¶æ„çš„ 3D ç‚¹äº‘æ•°æ®çš„å…³é”®æ­¥éª¤ã€‚
- en: '![](../Images/5e3562ca370d6d278c11635d8b34757d.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e3562ca370d6d278c11635d8b34757d.png)'
- en: The 3D Deep Learning Data Preparation Workflow for PointNet. Â© F. Poux
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: PointNet çš„ 3D æ·±åº¦å­¦ä¹ æ•°æ®å‡†å¤‡å·¥ä½œæµç¨‹ã€‚Â© F. Poux
- en: By following this step-by-step guide, you have learned how to clean, process
    LiDAR point clouds, extract relevant features, and normalize the data for 3D deep
    learning models. We have also discussed some key considerations in working with
    3D point cloud data, such as tile size, normalization, and data augmentation.
    You can apply these techniques to your 3D point cloud datasets and use them for
    training and testing PointNet models for object classification and segmentation.
    The field of 3D deep learning is rapidly evolving, and this tutorial is a cornerstone
    that provides a solid foundation for you to explore this exciting area further.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡éµå¾ªè¿™ä¸ªé€æ­¥æŒ‡å—ï¼Œä½ å·²ç»å­¦ä¼šäº†å¦‚ä½•æ¸…ç†ã€å¤„ç† LiDAR ç‚¹äº‘ï¼Œæå–ç›¸å…³ç‰¹å¾ï¼Œå¹¶ä¸º 3D æ·±åº¦å­¦ä¹ æ¨¡å‹è§„èŒƒåŒ–æ•°æ®ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†ä¸€äº›å¤„ç† 3D ç‚¹äº‘æ•°æ®çš„å…³é”®æ³¨æ„äº‹é¡¹ï¼Œå¦‚ç“¦ç‰‡å¤§å°ã€è§„èŒƒåŒ–å’Œæ•°æ®å¢å¼ºã€‚ä½ å¯ä»¥å°†è¿™äº›æŠ€æœ¯åº”ç”¨äºä½ çš„
    3D ç‚¹äº‘æ•°æ®é›†ï¼Œå¹¶ç”¨å®ƒä»¬æ¥è®­ç»ƒå’Œæµ‹è¯• PointNet æ¨¡å‹ï¼Œç”¨äºå¯¹è±¡åˆ†ç±»å’Œåˆ†å‰²ã€‚3D æ·±åº¦å­¦ä¹ é¢†åŸŸæ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œè¿™ä¸ªæ•™ç¨‹æ˜¯ä¸€ä¸ªåŸºçŸ³ï¼Œä¸ºä½ è¿›ä¸€æ­¥æ¢ç´¢è¿™ä¸€æ¿€åŠ¨äººå¿ƒçš„é¢†åŸŸæä¾›äº†åšå®çš„åŸºç¡€ã€‚
- en: ğŸ¤¿ Going Further
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¤¿ è¿›ä¸€æ­¥æ¢ç´¢
- en: But the learning journey does not end here. Our lifelong search begins, and
    future steps will dive into deepening 3D Voxel work, Artificial Intelligence for
    3D data, exploring semantics, and digital twinning. On top, we will analyze point
    clouds with deep learning techniques and unlock advanced 3D LiDAR analytical workflows.
    A lot to be excited about!
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å­¦ä¹ ä¹‹æ—…å¹¶æœªæ­¢æ­¥äºæ­¤ã€‚æˆ‘ä»¬çš„ç»ˆèº«æ¢ç´¢æ‰åˆšåˆšå¼€å§‹ï¼Œæœªæ¥çš„æ­¥éª¤å°†æ·±å…¥æ¢è®¨ 3D ä½“ç´ å·¥ä½œã€3D æ•°æ®çš„äººå·¥æ™ºèƒ½ã€è¯­ä¹‰åˆ†æå’Œæ•°å­—åŒèƒèƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯åˆ†æç‚¹äº‘ï¼Œè§£é”é«˜çº§
    3D LiDAR åˆ†æå·¥ä½œæµç¨‹ã€‚è¿˜æœ‰å¾ˆå¤šä»¤äººå…´å¥‹çš„å†…å®¹ï¼
- en: References
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnet: Deep learning
    on point sets for 3d classification and segmentation. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition* (pp. 652â€“660).'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnetï¼šç‚¹é›†ä¸Šçš„æ·±åº¦å­¦ä¹ ç”¨äº 3D åˆ†ç±»å’Œåˆ†å‰²ã€‚æ”¶å½•äº
    *IEEE è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†* (ç¬¬ 652â€“660 é¡µ)ã€‚
- en: 'Poux, F., & Billen, R. (2019). Voxel-based 3D point cloud semantic segmentation:
    Unsupervised geometric and relationship featuring vs deep learning methods. *ISPRS
    International Journal of Geo-Information*, *8*(5), 213.'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Poux, F., & Billen, R. (2019). åŸºäºä½“ç´ çš„ 3D ç‚¹äº‘è¯­ä¹‰åˆ†å‰²ï¼šæ— ç›‘ç£å‡ ä½•å’Œå…³ç³»ç‰¹å¾ä¸æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚*ISPRS å›½é™…åœ°ç†ä¿¡æ¯å­¦æ‚å¿—*,
    *8*(5), 213ã€‚
- en: Xu, S., Vosselman, G., & Elberink, S. O. (2014). Multiple-entity based classification
    of airborne laser scanning data in urban areas. *ISPRS Journal of photogrammetry
    and remote sensing*, *88*, 1â€“15.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Xu, S., Vosselman, G., & Elberink, S. O. (2014). åŸºäºå¤šå®ä½“çš„åŸå¸‚åŒºåŸŸèˆªç©ºæ¿€å…‰æ‰«ææ•°æ®åˆ†ç±»ã€‚*ISPRS
    æ‘„å½±æµ‹é‡ä¸é¥æ„Ÿæ‚å¿—*, *88*, 1â€“15ã€‚
