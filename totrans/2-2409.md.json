["```py\nimport numpy as np\n\nclass XGBNode:\n    \"\"\"A node object that recursively builds itself to construct a regression tree\n    \"\"\"\n    def __init__(self):\n        self.is_leaf: bool = False\n        self.left_child: XGBNode = None\n        self.right_child: XGBNode = None\n```", "```py\ndef build(self, X, grads, hessians, curr_depth, max_depth, reg_lambda, gamma):\n    \"\"\"Recursively build the node until a stopping criterion is reached\n    \"\"\"\n    if len(X) == 1 or curr_depth >= max_depth:\n        # Stopping criterion (1): there is only one sample left at this node\n        # Stopping criterion (2): max depth of the tree has been reached \n        self.is_leaf = True\n        self.weight = self.calc_leaf_weight(grads, hessians, reg_lambda)\n        return\n\n    best_gain, best_split = self.find_best_split(X, grads, hessians, reg_lambda)\n\n    if best_gain < gamma:\n        # Stopping criterion (3): the best gain is less than the minimum split gain \n        self.is_leaf = True\n        self.weight = self.calc_leaf_weight(grads, hessians, reg_lambda)\n        return        \n    else:\n        # Split the node according to the best split found\n        feature_idx, threshold, left_samples_idx, right_samples_idx = best_split\n\n        self.split_feature_idx = feature_idx\n        self.split_threshold = threshold\n\n        self.left_child = XGBNode()\n        self.left_child.build(X[left_samples_idx],\n                            grads[left_samples_idx],\n                            hessians[left_samples_idx],\n                            curr_depth + 1,\n                            max_depth, reg_lambda, gamma)\n\n        self.right_child = XGBNode()\n        self.right_child.build(X[right_samples_idx],\n                            grads[right_samples_idx],\n                            hessians[right_samples_idx],\n                            curr_depth + 1,\n                            max_depth, reg_lambda, gamma)\n```", "```py\ndef calc_leaf_weight(self, grads, hessians, reg_lambda):\n    \"\"\"Calculate the optimal weight of this leaf node (eq.(5) in [1])\n    \"\"\"\n    return -np.sum(grads) / (np.sum(hessians) + reg_lambda)\n```", "```py\ndef find_best_split(self, X, grads, hessians, reg_lambda):\n    \"\"\"Scan through every feature and find the best split point (Algorithm 1 in [1])\n    \"\"\"\n    G = np.sum(grads)\n    H = np.sum(hessians)\n\n    best_gain = float('-inf')   \n    best_split = None\n\n    # Iterate over all the possible features\n    for j in range(X.shape[1]):\n        G_left, H_left = 0, 0\n\n        # Sort the samples according to their value in the current feature\n        sorted_samples_idx = np.argsort(X[:, j])\n\n        # Calculate the gain of every possible split point\n        for i in range(X.shape[0] - 1):   \n            G_left += grads[sorted_samples_idx[i]]\n            H_left += hessians[sorted_samples_idx[i]]\n\n            G_right = G - G_left\n            H_right = H - H_left\n            curr_gain = self.calc_split_gain(G, H, G_left, H_left, G_right, H_right, reg_lambda)\n\n            if curr_gain > best_gain:\n                # Update the properties of the best split\n                best_gain = curr_gain     \n                feature_idx = j \n                threshold = X[sorted_samples_idx[i]][j]\n                left_samples_idx = sorted_samples_idx[:i + 1]\n                right_samples_idx = sorted_samples_idx[i + 1:]\n                best_split = (feature_idx, threshold, left_samples_idx, right_samples_idx)\n\n    return best_gain, best_split\n```", "```py\ndef calc_split_gain(self, G, H, G_left, H_left, G_right, H_right, reg_lambda):\n    \"\"\"Compute the loss reduction (eq. (7) in [1])\n    \"\"\"\n    def calc_term(g, h):\n        return g**2 / (h + reg_lambda)\n\n    gain = calc_term(G_left, H_left) + calc_term(G_right, H_right) - calc_term(G, H)\n    return 0.5 * gain\n```", "```py\ndef predict(self, x):\n    \"\"\"Return the score of a given sample x\n    \"\"\"\n    if self.is_leaf:\n        return self.weight\n    else:\n        if x[self.split_feature_idx] <= self.split_threshold:\n            return self.left_child.predict(x)\n        else:\n            return self.right_child.predict(x)\n```", "```py\nfrom xgb_node import XGBNode\n\nclass XGBTree:\n    \"\"\"A single tree object that will be used for gradient boosting\n    \"\"\"\n    def __init__(self):\n        self.root: XGBNode = None\n```", "```py\ndef build(self, X, grads, hessians, max_depth, reg_lambda, gamma):\n    \"\"\"Recursively build the root node of the tree \n    \"\"\"\n    self.root = XGBNode()\n    curr_depth = 0\n    self.root.build(X, grads, hessians, curr_depth, max_depth, reg_lambda, gamma)\n```", "```py\ndef predict(self, x):\n    \"\"\"Return the score of a given sample x\n    \"\"\"\n    return self.root.predict(x)\n```", "```py\nimport numpy as np\n\nfrom sklearn.base import BaseEstimator\nfrom xgb_tree import XGBTree\nfrom typing import List\nfrom abc import ABC, abstractmethod\n```", "```py\nclass XGBBaseModel(ABC, BaseEstimator):\n    \"\"\"Base class for the XGBoost estimators\n    \"\"\"\n    def __init__(\n        self,\n        n_estimators=100,     # The number of trees (boosting rounds)\n        max_depth=6,          # Maximum depth of a tree        \n        learning_rate=0.3,    # Step size shrinkage applied to the leaf weights\n        reg_lambda=1,         # L2 regularization term on the leaf weights\n        gamma=0,              # Minimum loss reduction required to split a node\n        verbose=0             # Verbosity of the log messages (change to 1 for debug mode)\n    ):\n        self.n_estimators = n_estimators        \n        self.max_depth = max_depth       \n        self.learning_rate = learning_rate\n        self.reg_lambda = reg_lambda\n        self.gamma = gamma\n        self.verbose = verbose\n```", "```py\ndef fit(self, X, y):\n    \"\"\"Build an ensemble of trees for the given training set\n    \"\"\"\n    # Get the initial prediction of the ensemble\n    self.base_pred = self.get_base_prediction(y)\n\n    self.estimators: List[XGBTree] = []\n    for i in range(self.n_estimators):\n        # Compute the first and second order gradients of the loss function with respect\n        # to the predictions of the current ensemble\n        out = self.get_output_values(X)\n        grads = self.calc_gradients(y, out)\n        hessians = self.calc_hessians(y, out)\n\n        # Add a new tree to the ensemble\n        tree = XGBTree()\n        tree.build(X, grads, hessians, self.max_depth, self.reg_lambda, self.gamma)\n        self.estimators.append(tree)\n\n        if self.verbose and i % 10 == 0:\n            print(f'Boosting iteration {i}')\n    return self\n```", "```py\ndef get_output_values(self, X):\n    \"\"\"Return the predicted values of the ensemble for the given data set\n    \"\"\"\n    # Initialize the output values with the base prediction\n    output = np.full(X.shape[0], self.base_pred)\n\n    # Add the predictions of the base trees scaled by the learning rate\n    if len(self.estimators) > 0:\n        for i in range(len(X)):            \n            output[i] += np.sum(self.learning_rate * estimator.predict(X[i]) \n                                    for estimator in self.estimators)\n    return output\n```", "```py\n@abstractmethod\ndef get_base_prediction(self, y):\n    \"\"\"Return the initial prediction of the model\"\"\"\n    pass\n\n@abstractmethod\ndef calc_gradients(self, y, out):\n    \"\"\"Calculate the first order gradients\"\"\" \n    pass\n\n@abstractmethod\ndef calc_hessians(self, y, out):\n    \"\"\"Calculate the second order gradients\"\"\"\n    pass\n\n@abstractmethod\ndef predict(self, X):\n    \"\"\"Return the final predicted labels for the given samples\"\"\"\n    pass\n```", "```py\nimport numpy as np\n\nfrom xgb_base_model import XGBBaseModel\nfrom sklearn.metrics import r2_score\n```", "```py\nclass XGBRegressor(XGBBaseModel):\n    \"\"\"An XGBoost estimator for regression tasks\n    \"\"\"\n    def __init__(\n        self, \n        n_estimators=100, \n        max_depth=6,         \n        learning_rate=0.3, \n        reg_lambda=1, \n        gamma=0,\n        verbose=0\n    ):\n        super().__init__(n_estimators, max_depth, learning_rate, reg_lambda, gamma, verbose)\n```", "```py\ndef get_base_prediction(self, y):\n    # The initial prediction is the mean of the targets\n    return np.mean(y)\n```", "```py\ndef calc_gradients(self, y, out):\n    # The first order gradients are twice the residuals\n    grads = 2 * (out - y)\n    return grads\n\ndef calc_hessians(self, y, out):\n    # The second order gradients are equal to the constant 2\n    hessians = np.full(len(y), 2)\n    return hessians\n```", "```py\ndef predict(self, X):\n    # The predicted labels are the same as the output values\n    y_pred = self.get_output_values(X)\n    return y_pred\n```", "```py\ndef score(self, X, y):\n    y_pred = self.predict(X)\n    return r2_score(y, y_pred)\n```", "```py\nfrom sklearn.datasets import make_regression\nX, y = make_regression(random_state=0)\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n```", "```py\ndef evaluate_model(name, model, X_train, y_train, X_test, y_test):\n    print(name)\n    print('-' * 30)    \n    train_start_time = time.time()\n    model.fit(X_train, y_train)\n    elapsed = time.time() - train_start_time\n    print(f'Training time: {elapsed:.3f} sec')\n\n    train_score = model.score(X_train ,y_train)\n    print(f'R2 score (train): {train_score:.4f}')\n    test_score = model.score(X_test, y_test)\n    print(f'R2 score (test): {test_score:.4f}')\n```", "```py\nfrom xgb_regressor import XGBRegressor\n\nmy_xgb_reg = XGBRegressor()\nevaluate_model('XGBoost (custom)', my_xgb_reg, X_train, y_train, X_test, y_test)\n```", "```py\nXGBoost (custom)\n------------------------------\nTraining time: 14.522 sec\nR2 score (train): 1.0000\nR2 score (test): 0.3218\n```", "```py\npip install xgboost\n```", "```py\nimport xgboost\n\noriginal_xgb_reg = xgboost.XGBRegressor()\nevaluate_model('XGBoost (original)', original_xgb_reg, X_train, y_train, X_test, y_test)\n```", "```py\nXGBoost (original)\n------------------------------\nTraining time: 1.074 sec\nR2 score (train): 1.0000\nR2 score (test): 0.3077\n```", "```py\nfrom sklearn.ensemble import GradientBoostingRegressor\ngboost_reg = GradientBoostingRegressor(random_state=0)\nevaluate_model('Gradient boosting', gboost_reg, X_train, y_train, X_test, y_test)\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nhist_gboost_reg = HistGradientBoostingRegressor(random_state=0)\nevaluate_model('Histogram-based gradient boosting', hist_gboost_reg, X_train, y_train, X_test, y_test)\n```", "```py\nGradient boosting\n-----------------------------------\nTraining time: 0.129 sec\nR2 score (train): 1.0000\nR2 score (test): 0.4376\n\nHistogram-based gradient boosting\n-----------------------------------\nTraining time: 0.188 sec\nR2 score (train): 0.9756\nR2 score (test): 0.3984\n```", "```py\nfrom sklearn.datasets import fetch_california_housing\n\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nnames = ['XGBoost (custom)', 'XGBoost (original)', 'Gradient boosting', 'Histogram-based gradient boosting', \n         'Linear regression']\nmodels = [XGBRegressor(), \n          xgboost.XGBRegressor(), \n          GradientBoostingRegressor(random_state=0), \n          HistGradientBoostingRegressor(random_state=0),\n          LinearRegression()]\n\nfor name, model in zip(names, models):\n    evaluate_model(name, model, X_train, y_train, X_test, y_test)\n```", "```py\nXGBoost (custom)\n-----------------------------------\nTraining time: 434.099 sec\nR2 score (train): 0.9026\nR2 score (test): 0.8253\n\nXGBoost (original)\n-----------------------------------\nTraining time: 1.003 sec\nR2 score (train): 0.9396\nR2 score (test): 0.8360\n\nGradient boosting\n-----------------------------------\nTraining time: 2.918 sec\nR2 score (train): 0.8027\nR2 score (test): 0.7774\n\nHistogram-based gradient boosting\n-----------------------------------\nTraining time: 0.975 sec\nR2 score (train): 0.8801\nR2 score (test): 0.8381\n\nLinear regression\n-----------------------------------\nTraining time: 0.007 sec\nR2 score (train): 0.6089\nR2 score (test): 0.5943\n```", "```py\nimport numpy as np\n\nfrom xgb_base_model import XGBBaseModel\nfrom sklearn.metrics import accuracy_score\n```", "```py\nclass XGBClassifier(XGBBaseModel):\n    \"\"\"An XGBoost estimator for binary classification tasks\n    \"\"\"\n    def __init__(\n        self, \n        n_estimators=100, \n        max_depth=6,         \n        learning_rate=0.3, \n        reg_lambda=1, \n        gamma=0,\n        verbose=0\n    ):\n        super().__init__(n_estimators, max_depth, learning_rate, reg_lambda, gamma, verbose)\n```", "```py\ndef get_base_prediction(self, y):\n    # The initial prediction is the log odds of the positive class\n    prob = np.sum(y == 1) / len(y)\n    return np.log(prob / (1 - prob))\n```", "```py\ndef sigmoid(self, x):\n    return 1 / (1 + np.exp(-x))\n```", "```py\ndef calc_gradients(self, y, out):\n    # The first order gradients are the residuals between the predicted probabilities \n    # (sigmoid of the log odds) and the true labels\n    prob = self.sigmoid(out)    \n    grads = prob - y\n    return grads\n\ndef calc_hessians(self, y, out):\n    # The second order gradients are p(1 - p) where p is the predicted probability\n    prob = self.sigmoid(out)\n    hessians = prob * (1 - prob)\n    return hessians\n```", "```py\ndef predict_proba(self, X):\n    # The predicted probability is the sigmoid of the log odds\n    log_odds = self.get_output_values(X)\n    prob = self.sigmoid(log_odds)\n    return prob\n```", "```py\ndef predict(self, X):\n    # Assign a label of 1 if the probability of the positive class is > 0.5,\n    # otherwise assign a label of 0\n    prob = self.predict_proba(X)\n    y_pred = np.where(prob > 0.5, 1, 0)\n    return y_pred\n```", "```py\ndef score(self, X, y):\n    y_pred = self.predict(X)\n    return accuracy_score(y, y_pred)\n```", "```py\nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=1000, class_sep=0.1, random_state=0)\n```", "```py\ndef evaluate_model(name, model, X_train, y_train, X_test, y_test):\n    print(name)\n    print('-' * 35)\n    train_start_time = time.time()\n    model.fit(X_train, y_train)\n    elapsed = time.time() - train_start_time\n    print(f'Training time: {elapsed:.3f} sec')\n\n    train_score = model.score(X_train ,y_train)\n    print(f'Accuracy (train): {train_score:.4f}')\n    test_score = model.score(X_test, y_test)\n    print(f'Accuracy (test): {test_score:.4f}')\n    print()\n```", "```py\nnames = ['XGBoost (custom)', 'XGBoost (original)', 'Gradient boosting', 'Histogram-based gradient boosting', \n         'Logistic regression']\nmodels = [XGBClassifier(), \n          xgboost.XGBClassifier(), \n          GradientBoostingClassifier(random_state=0), \n          HistGradientBoostingClassifier(random_state=0),\n          LogisticRegression()]\n\nfor name, model in zip(names, models):\n    evaluate_model(name, model, X_train, y_train, X_test, y_test)\n```", "```py\nXGBoost (custom)\n-----------------------------------\nTraining time: 39.782 sec\nAccuracy (train): 1.0000\nAccuracy (test): 0.6900\n\nXGBoost (original)\n-----------------------------------\nTraining time: 1.111 sec\nAccuracy (train): 1.0000\nAccuracy (test): 0.6850\n\nGradient boosting\n-----------------------------------\nTraining time: 0.409 sec\nAccuracy (train): 0.9213\nAccuracy (test): 0.6550\n\nHistogram-based gradient boosting\n-----------------------------------\nTraining time: 0.810 sec\nAccuracy (train): 1.0000\nAccuracy (test): 0.6600\n\nLogistic regression\n-----------------------------------\nTraining time: 0.007 sec\nAccuracy (train): 0.5962\nAccuracy (test): 0.5300\n```"]