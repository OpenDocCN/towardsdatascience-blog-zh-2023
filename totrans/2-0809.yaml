- en: Enhanced Large Language Models as Reasoning Engines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强的大型语言模型作为推理引擎
- en: 原文：[https://towardsdatascience.com/enhanced-large-language-models-as-reasoning-engines-582bff782113](https://towardsdatascience.com/enhanced-large-language-models-as-reasoning-engines-582bff782113)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/enhanced-large-language-models-as-reasoning-engines-582bff782113](https://towardsdatascience.com/enhanced-large-language-models-as-reasoning-engines-582bff782113)
- en: '[](https://medium.com/@alcarazanthony1?source=post_page-----582bff782113--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----582bff782113--------------------------------)[](https://towardsdatascience.com/?source=post_page-----582bff782113--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----582bff782113--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----582bff782113--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alcarazanthony1?source=post_page-----582bff782113--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----582bff782113--------------------------------)[](https://towardsdatascience.com/?source=post_page-----582bff782113--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----582bff782113--------------------------------)
    [安东尼·阿尔卡拉斯](https://medium.com/@alcarazanthony1?source=post_page-----582bff782113--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----582bff782113--------------------------------)
    ·12 min read·Dec 23, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----582bff782113--------------------------------)
    ·阅读时长12分钟·2023年12月23日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工智能软件被用来增强本文文本的语法、流畅性和可读性。*'
- en: The recent exponential advances in natural language processing capabilities
    from large language models (LLMs) have stirred tremendous excitement about their
    potential to achieve human-level intelligence. Their ability to produce remarkably
    coherent text and engage in dialogue after exposure to vast datasets seems to
    point towards flexible, general purpose reasoning skills.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）在自然语言处理能力方面的指数级进步引起了人们对其实现人类水平智能的巨大兴奋。它们在暴露于大量数据集后，能够生成异常连贯的文本并进行对话，这似乎表明了灵活的通用推理能力。
- en: However, a growing chorus of voices urges caution against unchecked optimism
    by highlighting fundamental blindspots that limit neural approaches. LLMs still
    frequently make basic logical and mathematical mistakes that reveal a lack of
    systematicity behind their responses. Their knowledge remains intrinsically statistical
    without deeper semantic structures.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，越来越多的声音敦促对未受限制的乐观情绪保持谨慎，突出了限制神经方法的基本盲点。大型语言模型仍然经常犯基本的逻辑和数学错误，暴露了其回应背后缺乏系统性的问题。它们的知识仍然本质上是统计性的，没有更深层次的语义结构。
- en: More complex reasoning tasks further expose these limitations. LLMs struggle
    with causal, counterfactual, and compositional reasoning challenges that require
    going beyond surface pattern recognition. Unlike humans who learn abstract schemas
    to flexibly recombine modular concepts, neural networks memorize correlations
    between co-occurring terms. This results in brittle generalization outside narrow
    training distributions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的推理任务进一步暴露了这些局限性。大型语言模型在因果、反事实和组合推理挑战中挣扎，这些挑战需要超越表面模式识别。与人类通过学习抽象的模式来灵活地重新组合模块概念不同，神经网络记忆的是共同出现的术语之间的相关性。这导致了在狭窄的训练分布之外的脆弱泛化。
- en: The chasm underscores how human cognition employs structured symbolic representations
    to enable systematic composability and causal models for conceptualizing dynamics.
    We reason by manipulating modular symbolic concepts based on valid inference rules,
    chaining logical dependencies, leveraging mental simulations, and postulating
    mechanisms relating variables. The inherently statistical nature of neural networks
    precludes developing such structured reasoning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这种鸿沟凸显了人类认知如何利用结构化的符号表示来实现系统的可组合性和因果模型以概念化动态。我们通过基于有效推理规则操控模块化的符号概念、链式逻辑依赖、利用心理模拟和假设变量关系机制来推理。神经网络固有的统计性质使得难以发展这种结构化推理。
- en: It remains mysterious how symbolic-like phenomena emerge in LLMs despite their
    subsymbolic substrate. But clearer acknowledgement of this “hybridity gap” is
    imperative. True progress requires embracing complementary strengths — the flexibility
    of neural approaches with structured knowledge representations and causal reasoning
    techniques — to create integrated reasoning systems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大语言模型（LLMs）具有亚符号基础，但符号类似现象的出现仍然令人费解。然而，更清晰地认识到这种“混合差距”是必要的。真正的进步需要融合互补的优势——神经方法的灵活性与结构化知识表示和因果推理技术——以创建集成推理系统。
- en: We first outline the growing chorus of analyses exposing neural networks’ lack
    of systematicity, causal comprehension, and compositional generalization — underscoring
    differences from innate human faculties.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先概述了越来越多的分析揭示神经网络在系统性、因果理解和组合泛化方面的不足——强调与先天人类能力的差异。
- en: Next, we detail salient facets of the “reasoning gap”, including struggles with
    modular skill orchestration, unraveling dynamics, and counterfactual simulation.
    We surface innate human capacities contemporary ML lacks, explaining resulting
    brittleness.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们详细介绍“推理差距”的显著方面，包括模块化技能协调的困难、解开动态和反事实模拟。我们揭示了当代机器学习所缺乏的先天人类能力，并解释了由此产生的脆弱性。
- en: Seeking remedies, we discuss knowledge graphs as scaffolds for explicit conceptual
    relationships missing from statistical learning. We highlight approaches for structured
    knowledge injection — querying interfaces and vectorized graph embeddings — to
    contextualize neural generation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 寻求解决方案时，我们讨论了知识图谱作为明确概念关系的支架，这些关系在统计学习中缺失。我们强调了结构化知识注入的方法——查询接口和向量化图谱嵌入——以将神经生成上下文化。
- en: We present techniques like dimensional typing in embeddings and parallel knowledge
    retrieval to improve inductive biases for logical deduction and efficient inference.
    Finally, we make the case for patiently cultivating high-quality knowledge graphs
    as strategic assets for enterprises pursuing substantive AI progress.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了诸如嵌入中的维度类型和并行知识检索等技术，以改善逻辑推理和高效推断的归纳偏差。最后，我们主张耐心培养高质量知识图谱，作为追求实质性人工智能进展的战略资产。
- en: 'The Reasoning Gap :'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理差距：
- en: '[](https://arxiv.org/abs/2311.18751?source=post_page-----582bff782113--------------------------------)
    [## Language Model Agents Suffer from Compositional Generalization in Web Automation'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2311.18751?source=post_page-----582bff782113--------------------------------)
    [## 语言模型代理在网络自动化中的组合泛化问题'
- en: Language model agents (LMA) recently emerged as a promising paradigm on muti-step
    decision making tasks, often…
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言模型代理（LMA）最近成为多步骤决策任务中的一种有前景的范式，通常…
- en: arxiv.org](https://arxiv.org/abs/2311.18751?source=post_page-----582bff782113--------------------------------)
    [](https://arxiv.org/abs/2305.20050?source=post_page-----582bff782113--------------------------------)
    [## Let's Verify Step by Step
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2311.18751?source=post_page-----582bff782113--------------------------------)
    [](https://arxiv.org/abs/2305.20050?source=post_page-----582bff782113--------------------------------)
    [## 让我们逐步验证'
- en: In recent years, large language models have greatly improved in their ability
    to perform complex multi-step reasoning…
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型在执行复杂多步骤推理的能力上有了显著提升…
- en: arxiv.org](https://arxiv.org/abs/2305.20050?source=post_page-----582bff782113--------------------------------)
    [](https://arxiv.org/abs/2304.03843?source=post_page-----582bff782113--------------------------------)
    [## Why think step by step? Reasoning emerges from the locality of experience
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2305.20050?source=post_page-----582bff782113--------------------------------)
    [](https://arxiv.org/abs/2304.03843?source=post_page-----582bff782113--------------------------------)
    [## 为什么要逐步思考？推理源于经验的局部性'
- en: Humans have a powerful and mysterious capacity to reason. Working through a
    set of mental steps enables us to make…
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人类具有强大而神秘的推理能力。通过一系列心理步骤，我们能够进行…
- en: arxiv.org](https://arxiv.org/abs/2304.03843?source=post_page-----582bff782113--------------------------------)
    [](https://pub.aimind.so/can-llm-learn-to-reason-about-cause-and-effect-82a5f7b8cf5c?source=post_page-----582bff782113--------------------------------)
    [## Can LLM Learn to Reason About Cause and Effect?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2304.03843?source=post_page-----582bff782113--------------------------------)
    [](https://pub.aimind.so/can-llm-learn-to-reason-about-cause-and-effect-82a5f7b8cf5c?source=post_page-----582bff782113--------------------------------)
    [## 大语言模型能否学会因果推理？'
- en: Causal reasoning — the ability to understand and analyze the causal relationships
    between events — has long been…
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因果推理——理解和分析事件之间因果关系的能力——长期以来一直是…
- en: pub.aimind.so](https://pub.aimind.so/can-llm-learn-to-reason-about-cause-and-effect-82a5f7b8cf5c?source=post_page-----582bff782113--------------------------------)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.aimind.so](https://pub.aimind.so/can-llm-learn-to-reason-about-cause-and-effect-82a5f7b8cf5c?source=post_page-----582bff782113--------------------------------)'
- en: An emerging perspective explains many of the logical contradictions and lack
    of systematic generalization exhibited by LLMs as arising from a fundamental “reasoning
    gap” — the inability to spontaneously chain facts and inferences through intermediate
    reasoning steps.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一种新兴观点解释了LLM表现出的许多逻辑矛盾和系统性泛化不足，认为这是由于基本的“推理差距”——即无法自发地通过中间推理步骤将事实和推论串联起来。
- en: Humans possess an intuitive capacity to make conceptual leaps using structured
    background knowledge and directing attention towards relevant pathways. We leverage
    existing schema and causal models linking concepts to stitch coherent narratives.
    For example, humans can reliably determine the climate in a country’s capital
    by using the capital relation to connect isolated facts — that Paris is the capital
    of France, and France has a temperate climate.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 人类具有使用结构化背景知识和将注意力指向相关路径的直觉能力。我们利用现有的模式和因果模型，将概念连接起来，以编织连贯的叙事。例如，人类可以通过使用首都关系连接孤立的事实——巴黎是法国的首都，法国气候温和，从而可靠地确定一个国家首都的气候。
- en: However, neural networks like LLMs accumulate only statistical associations
    between terms that co-occur frequently together in text corpora. Their knowledge
    remains implicit and unstructured. With no appreciation of higher order semantic
    relationships between concepts, they struggle to bridge concepts never seen directly
    linked during pre-training. Questions requiring reasoning through intermediate
    steps reveal this brittleness.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，像LLM这样的神经网络仅积累了文本语料库中经常一起出现的术语之间的统计关联。它们的知识依然是隐性的和无结构的。由于没有对概念之间高阶语义关系的理解，它们难以弥合预训练过程中从未直接链接过的概念。需要通过中间步骤进行推理的问题揭示了这种脆弱性。
- en: Newbenchmark tests specifically assess systematicity — whether models can combine
    known building blocks in novel ways. While LLMs achieve high accuracy when concepts
    they must relate already co-occur in training data, performance drops significantly
    when concepts appear separately despite influencing each other. The models fail
    to chain together their isolated statistical knowledge, unlike humans who can
    link concepts through indirect relationships.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 新基准测试特别评估系统性——模型是否能够以新颖的方式组合已知的构建块。虽然LLM在概念必须相关且已经出现在训练数据中的情况下可以达到高准确率，但当概念分开出现并相互影响时，性能显著下降。这些模型无法将孤立的统计知识串联起来，与人类通过间接关系链接概念不同。
- en: This manifests empirically in LLMs’ inability to provide consistent logical
    narratives when queried through incremental prompt elaborations. Slight perturbations
    to explore new directions trigger unpredictable model failures as learned statistical
    associations break down. Without structured representations or programmatic operators,
    LLMs cannot sustain coherent, goal-oriented reasoning chains — revealing deficits
    in compositional generalization.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象在LLM无法在通过逐步提示展开的查询中提供一致的逻辑叙事中得到了实证。当轻微扰动探索新方向时，会触发不可预测的模型失败，因为学习到的统计关联会崩溃。没有结构化的表示或程序化操作符，LLM无法维持连贯的、目标导向的推理链——暴露了组合泛化的缺陷。
- en: The “reasoning gap” helps explain counterintuitive LLM behavior. Their knowledge,
    encoded across billions of parameters through deep learning from surface statistical
    patterns alone, lacks frameworks for disciplined, interpretable logical reasoning
    that humans innately employ for conceptual consistency and exploration. This illuminates
    priorities for hybrid neurosymbolic approaches.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: “推理差距”有助于解释LLM（大型语言模型）的反直觉行为。它们的知识通过深度学习编码在数十亿个参数中，仅从表面统计模式中获取，缺乏人类在概念一致性和探索中固有的有纪律、可解释的逻辑推理框架。这为混合神经符号方法的优先事项提供了启示。
- en: '**Compositional Reasoning**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**组合推理**'
- en: LLMs still struggle with systematic combinatorial generalization — flexibly
    assembling novel solutions by recombining known skills. For example, separately
    learning to make coffee and toast does not directly enable orchestrating the joint
    routine. Humans intrinsically develop far richer modular, hierarchical representations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在系统性组合泛化方面仍然存在困难——灵活地通过重新组合已知技能来组装新颖的解决方案。例如，分别学习制作咖啡和吐司并不能直接使你能够协调这两者的联合例程。人类本质上发展出更丰富的模块化、层次化表示。
- en: '**Causal Reasoning**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**因果推理**'
- en: While correlation comes naturally to statistics-driven models, unraveling causal
    mechanisms involving experiments, interventions and counterfactuals remains elusive
    without explicit conceptual frameworks. We possess innate construals of objects,
    agents and dynamics.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然相关性对统计驱动模型来说很自然，但没有明确的概念框架，揭示涉及实验、干预和反事实的因果机制仍然难以捉摸。我们具备对对象、代理和动态的天生理解。
- en: '**Temporal Reasoning**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**时间推理**'
- en: The sequential, transient nature of events, plans and narratives requires maintaining
    internal timelines, projecting into horizons. Yet LLMs display limited episodic
    memory to sustain coherence, lacking mental situational modeling.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 事件、计划和叙事的顺序性和瞬态特征要求维持内部时间线，并展望未来。然而，LLMs 展现出的有限情节记忆无法维持连贯性，缺乏心理情境建模。
- en: '**Common Sense**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**常识**'
- en: Our expansive everyday frameworks encompassing objects, spaces and intuitive
    psychology provide testimony on plausibility when navigating the world. Failing
    to emulate this understanding of naive physics, pragmatics and social dynamics
    restricts exposed statistical knowledge.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们广泛的日常框架，包括对象、空间和直观心理学，提供了在探索世界时的可能性证据。未能模拟这种对朴素物理学、语用学和社会动态的理解限制了暴露的统计知识。
- en: '**Meta-learning**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**元学习**'
- en: Humans demonstrate meta-cognition around our own reasoning gaps, directing attention
    and deliberately seeking information to strengthen models. The opacity and lack
    of higher-order uncertainty or self-reflection limits controlled, strategic model
    improvement in neural networks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在处理自身推理漏洞时展示了元认知，能够引导注意力并有意识地寻求信息以增强模型。而神经网络中的模糊性和缺乏更高阶的不确定性或自我反思限制了对模型的控制性、战略性改进。
- en: In totality, LLMs do not possess the schema induction, causal representations,
    simulation capacities, autoregressive prediction, social heuristics or metacognitive
    architectures underpinning versatile systematic generalization. Lacking structured
    inductive biases, combinatorial explosions render learning explicit compositions
    infeasible. The multitude of reasoning gaps hampers reaching human-like flexibility
    guided by understanding.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，LLMs 不具备支持多样化系统化归纳的模式归纳、因果表示、模拟能力、自回归预测、社会启发式或元认知架构。缺乏结构化的归纳偏见，组合爆炸使得学习明确的组合变得不可行。推理漏洞的众多阻碍了达到由理解引导的人类般的灵活性。
- en: This further spotlights the urgent imperative of hybrid neurosymbolic paradigms
    that symbiotically combine innate neural capacities with structured compositionality
    and causal constraints. Such integration promises more robust, trustworthy systems
    that reason fluidly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这进一步突显了混合神经符号范式的紧迫性，这些范式将天生的神经能力与结构化的组合性和因果约束相结合。这样的整合有望实现更强大、可信的系统，能够流畅地推理。
- en: 'Enhancing Reasoning with Knowledge Graphs :'
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过知识图谱增强推理：
- en: '[](/vector-search-is-not-all-you-need-ecd0f16ad65e?source=post_page-----582bff782113--------------------------------)
    [## Vector Search Is Not All You Need'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/vector-search-is-not-all-you-need-ecd0f16ad65e?source=post_page-----582bff782113--------------------------------)
    [## 向量搜索并非你所需的一切'
- en: Introduction
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引言
- en: towardsdatascience.com](/vector-search-is-not-all-you-need-ecd0f16ad65e?source=post_page-----582bff782113--------------------------------)
    [](https://medium.com/@alcarazanthony1/knowledge-graph-embeddings-as-a-bridge-between-symbolic-and-subsymbolic-ai-0f37c55de845?source=post_page-----582bff782113--------------------------------)
    [## Knowledge Graph Embeddings as a Bridge between Symbolic and Subsymbolic AI
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/vector-search-is-not-all-you-need-ecd0f16ad65e?source=post_page-----582bff782113--------------------------------)
    [](https://medium.com/@alcarazanthony1/knowledge-graph-embeddings-as-a-bridge-between-symbolic-and-subsymbolic-ai-0f37c55de845?source=post_page-----582bff782113--------------------------------)
    [## 知识图谱嵌入作为符号与子符号 AI 之间的桥梁
- en: The Resurgence of Structure
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构的复兴
- en: medium.com](https://medium.com/@alcarazanthony1/knowledge-graph-embeddings-as-a-bridge-between-symbolic-and-subsymbolic-ai-0f37c55de845?source=post_page-----582bff782113--------------------------------)
    [](https://ai.plainenglish.io/augmenting-large-language-models-with-hybrid-knowledge-architectures-24cd322b5be7?source=post_page-----582bff782113--------------------------------)
    [## Augmenting Large Language Models with Hybrid Knowledge Architectures
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@alcarazanthony1/knowledge-graph-embeddings-as-a-bridge-between-symbolic-and-subsymbolic-ai-0f37c55de845?source=post_page-----582bff782113--------------------------------)
    [](https://ai.plainenglish.io/augmenting-large-language-models-with-hybrid-knowledge-architectures-24cd322b5be7?source=post_page-----582bff782113--------------------------------)
    [## 使用混合知识架构增强大型语言模型'
- en: 'Tracing Vector Relevance with Symbolic Chains: A Balanced Approach for Robust
    Reasoning in Retrieval-Augmented…'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用符号链跟踪向量相关性：一种平衡的方法，用于增强检索增强型推理中的稳健性……
- en: ai.plainenglish.io](https://ai.plainenglish.io/augmenting-large-language-models-with-hybrid-knowledge-architectures-24cd322b5be7?source=post_page-----582bff782113--------------------------------)
    ![](../Images/2e2201b8836e2cddf909019bd6d4539a.png)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[ai.plainenglish.io](https://ai.plainenglish.io/augmenting-large-language-models-with-hybrid-knowledge-architectures-24cd322b5be7?source=post_page-----582bff782113--------------------------------)
    ![](../Images/2e2201b8836e2cddf909019bd6d4539a.png)'
- en: Schema by the author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的模式
- en: Knowledge graphs offer a promising method for overcoming the “reasoning gap”
    plaguing modern large language models (LLMs). By explicitly modeling concepts
    as nodes and relationships as edges, knowledge graphs provide structured symbolic
    representations that can augment the flexible statistical knowledge within LLMs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱提供了一种有前景的方法来克服现代大型语言模型（LLMs）所面临的“推理差距”。通过将概念明确建模为节点，将关系建模为边，知识图谱提供了结构化的符号表示，这可以增强LLMs中的灵活统计知识。
- en: Establishing explanatory connections between concepts empowers more systematic,
    interpretable reasoning across distant domains. LLMs struggle to link disparate
    concepts purely through learned data patterns. But knowledge graphs can effectively
    relate concepts not directly co-occurring in text corpora by providing relevant
    intermediate nodes and relationships. This scaffolding bridges gaps in statistical
    knowledge, enabling logical chaining.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 建立概念之间的解释性连接可以增强跨遥远领域的系统化、可解释的推理。LLMs很难仅通过学习的数据模式来连接不同的概念。但知识图谱可以通过提供相关的中介节点和关系，有效地关联文本语料中没有直接共现的概念。这种支撑结构弥合了统计知识的空白，实现逻辑链条。
- en: Such knowledge graphs also increase transparency and trust in LLM-based inference.
    Requiring models to display full reasoning chains over explicit graph relations
    mitigates risks from unprincipled statistical hallucinations. Exposing the graph
    paths makes statistical outputs grounded in validated connections.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种知识图谱还增加了对LLM基础推理的透明度和信任度。要求模型展示基于显式图关系的完整推理链可以减轻不当统计幻觉带来的风险。暴露图路径使统计输出建立在经过验证的连接上。
- en: Constructing clean interfaces between innately statistical LLMs and structured
    causal representations shows promise for overcoming today’s brittleness. Combining
    neural knowledge breadth with external knowledge depth can nurture the development
    of AI systems that learn and reason both flexibly and systematically.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上统计的LLMs与结构化因果表示之间构建干净的接口显示出克服当前脆弱性的潜力。将神经知识的广度与外部知识的深度相结合，可以促进AI系统的开发，使其在学习和推理方面既灵活又系统化。
- en: '**Knowledge Graph Querying and Graph Algorithms**'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**知识图谱查询和图算法**'
- en: 'Knowledge graph querying and graph algorithms are powerful tools for extracting
    and analyzing complex relationships from large datasets. Here’s how they work
    and what they can achieve:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱查询和图算法是从大数据集中提取和分析复杂关系的强大工具。以下是它们的工作原理及其可能达到的效果：
- en: 'Knowledge Graph Querying:'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识图谱查询：
- en: Knowledge graphs organize information as entities (like books, people, or concepts)
    and relationships (like authorship, kinship, or thematic connection). Querying
    languages like SPARQL and Cypher enable the formulation of queries to extract
    specific information from these graphs. For example, the query mentioned in your
    example finds books related to “Artificial Intelligence” by matching book nodes
    connected to relevant topic nodes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱将信息组织为实体（如书籍、人物或概念）和关系（如著作关系、亲属关系或主题连接）。查询语言如 SPARQL 和 Cypher 使得可以构造查询，从这些图谱中提取特定信息。例如，你示例中的查询通过匹配与相关主题节点连接的书籍节点，找到与“人工智能”相关的书籍。
- en: 'Graph Algorithms:'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图算法：
- en: 'Beyond querying, graph algorithms can analyze these structures in more profound
    ways. Some typical graph algorithms include:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了查询，图算法还可以以更深层次的方式分析这些结构。一些典型的图算法包括：
- en: 'Pathfinding Algorithms (e.g., Dijkstra’s, A*): Find the shortest path between
    two nodes, useful in route planning and network analysis.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路径寻找算法（如 Dijkstra 算法、A* 算法）：找到两个节点之间的最短路径，适用于路线规划和网络分析。
- en: 'Community Detection Algorithms (e.g., Louvain Method): Identify clusters or
    communities within graphs, helping in social network analysis and market segmentation.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社区检测算法（如 Louvain 方法）：识别图中的聚类或社区，有助于社交网络分析和市场细分。
- en: 'Centrality Measures (e.g., PageRank, Betweenness Centrality): Determine the
    importance of different nodes in a network, applicable in analyzing influence
    in social networks or key infrastructure in transportation networks.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中心性测量（如PageRank、介数中心性）：确定网络中不同节点的重要性，适用于分析社交网络中的影响力或交通网络中的关键基础设施。
- en: 'Recommendation Systems: By analyzing user-item graphs, these systems can make
    personalized recommendations based on past interactions.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统：通过分析用户-项目图，这些系统可以基于过去的互动进行个性化推荐。
- en: 'Large Language Models (LLMs) can generate queries for knowledge graphs based
    on natural language input. While they excel in understanding and generating human-like
    text, their statistical nature means they’re less adept at structured logical
    reasoning. Therefore, pairing them with knowledge graphs and structured querying
    interfaces can leverage the strengths of both: the LLM for understanding and contextualizing
    user input and the knowledge graph for precise, logical data retrieval.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）可以基于自然语言输入生成对知识图谱的查询。尽管它们在理解和生成类似人类的文本方面表现出色，但其统计性质意味着它们在结构化逻辑推理方面较弱。因此，将它们与知识图谱和结构化查询接口配对，可以利用两者的优势：LLM用于理解和上下文化用户输入，知识图谱用于精确、逻辑的数据检索。
- en: Incorporating graph algorithms into the mix can further enhance this synergy.
    For instance, an LLM could suggest a community detection algorithm on a social
    network graph to identify influential figures within a specific interest group.
    However, the challenge lies in integrating these disparate systems in a way that
    is both efficient and interpretable.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将图算法纳入其中可以进一步增强这种协同。例如，LLM可以建议在社交网络图上使用社区检测算法，以识别特定兴趣小组中的影响力人物。然而，挑战在于以高效且可解释的方式集成这些不同的系统。
- en: '![](../Images/1501f90a1aa1ef4c30bb76f3a95fe084.png)![](../Images/23375e373e6709e89610326af2e845ea.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1501f90a1aa1ef4c30bb76f3a95fe084.png)![](../Images/23375e373e6709e89610326af2e845ea.png)'
- en: '**Knowledge Graph Embeddings**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识图谱嵌入**'
- en: Knowledge graph embeddings encode entities and relations as dense vector representations.
    These vectors can be dynamically integrated within LLMs using fused models.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱嵌入将实体和关系编码为密集的向量表示。这些向量可以在LLM中动态集成，使用融合模型。
- en: For example, a cross-attention mechanism can contextualize language model token
    embeddings by matching them against retrieved graph embeddings. This injects relevant
    external knowledge.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，交叉注意机制可以通过将语言模型的令牌嵌入与检索到的图嵌入进行匹配，从而为语言模型的令牌嵌入提供上下文。这注入了相关的外部知识。
- en: Mathematically fusing these complementary vectors grounds the model, while allowing
    gradient flows across both components. The LLM inherits relational patterns, improving
    reasoning.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上将这些互补向量融合，既为模型提供了基础，又允许在两个组件之间进行梯度流动。LLM继承了关系模式，改善了推理能力。
- en: So both querying and embeddings provide mechanisms for connecting structured
    knowledge graphs with the statistical capacities of LLMs. This facilitates interpretable,
    contextual responses informed by curated facts.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，查询和嵌入机制都提供了将结构化知识图谱与LLMs的统计能力连接起来的方法。这有助于生成可解释的、基于策划事实的上下文响应。
- en: The path towards safe, performant, and explainable AI undoubtedly lies in architecting
    hybrid systems with distinct reasoning modules suited to their strengths while
    mitigating individual weaknesses through symbiotic integration. Knowledge graphs
    offer the structural scaffolding to elevate LLMs from pattern recognizers to context-aware,
    disciplined reasoners.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通往安全、高效和可解释AI的道路无疑在于设计具有不同推理模块的混合系统，这些模块适合其各自的优势，同时通过共生集成来减轻各自的弱点。知识图谱提供了结构性的支撑，将LLMs从模式识别器提升为具有上下文感知和严格推理能力的推理器。
- en: Knowledge graph embeddings can be further enhanced by incorporating additional
    constraints and structure beyond just encoding factual entities and relationships.
    This provides useful inductive biases to orient semantic similarity and reasoning
    in more reliable ways.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱嵌入可以通过加入额外的约束和结构进一步增强，而不仅仅是编码事实实体和关系。这提供了有用的归纳偏差，以更可靠的方式定向语义相似性和推理。
- en: 'Some examples include:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一些例子包括：
- en: '**Dimensional Typing**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**维度类型**'
- en: Assigning dedicated dimensions in the embedding space to model specific hierarchical
    knowledge categories (like types, attributes, temporal bins etc.) allows interpreting
    vector arithmetic and symmetry operations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入空间中分配专门的维度，以建模特定的层次知识类别（如类型、属性、时间段等），可以解释向量运算和对称操作。
- en: '**Logical Rules as Vector Equations**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑规则作为向量方程**'
- en: Modeling logical rules like transitivity as vector equations over relation embeddings
    bakes in compliance with first-order logic when querying the vector space.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将逻辑规则如传递性建模为关系嵌入上的向量方程，使得在查询向量空间时符合一阶逻辑。
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Entity Linking Regularization**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**实体链接正则化**'
- en: Adding a linkage loss that pulls together vector representations for the same
    real-world entities improves generalization across surface forms.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 添加链接损失将相同真实世界实体的向量表示拉近，改善跨表面形式的泛化能力。
- en: '**Temporal Ordering**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**时间排序**'
- en: Encoding time series knowledge by chronologically positioning entity embeddings
    assists in analogical reasoning over time.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过按时间顺序排列实体嵌入来编码时间序列知识，有助于进行时间上的类比推理。
- en: Overall, embellishing knowledge graph embeddings with structured inductive biases
    — whether through typed dimensions, vector logic, or temporal ordering — makes
    the vector arithmetic better comport with real-world constraints. This strengthens
    their ability to tackle complex reasoning tasks, providing useful scaffolds that
    can likewise elevate the performance of integrated language models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，通过结构化的归纳偏差——无论是通过类型维度、向量逻辑还是时间排序——美化知识图谱嵌入，使得向量运算更好地符合现实世界约束。这增强了它们解决复杂推理任务的能力，提供了有用的支撑，同时提升了集成语言模型的性能。
- en: The core benefit is infusing domain knowledge to orient the latent geometry
    in precise ways amplifies the reasoning capacity of connectionist models interacting
    with the vector space. Guiding the primitives the neural engine operates upon
    through structured initialization fosters more systematic compositional computation
    accessible through queries.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 核心好处在于将领域知识注入以精确方式定向潜在几何，从而增强连接主义模型在与向量空间互动时的推理能力。通过结构化初始化指导神经引擎操作的原始数据，促进了通过查询进行更系统的组合计算。
- en: 'Complementary approches:'
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 互补方法：
- en: 'First retrieving relevant knowledge graph embeddings for a query before querying
    the full knowledge graph. This two-step approach allows efficient focus of graphical
    operations:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先在查询完整知识图谱之前检索相关的知识图谱嵌入。这种两步法允许高效集中图形操作：
- en: '**Step 1: Vector Embedding Retrieval**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：向量嵌入检索**'
- en: Given a natural language query, relevant knowledge graph embeddings can be quickly
    retrieved using approximate nearest neighbor search over indexed vectors.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 给定自然语言查询，相关的知识图谱嵌入可以通过对索引向量的近似最近邻搜索迅速检索。
- en: For example, using a query like “Which books discuss artificial intelligence”,
    vector search would identify embeddings of the Book, Topic, and AI Concept entities.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用类似“哪些书讨论了人工智能”的查询，向量搜索会识别出Book、Topic和AI Concept实体的嵌入。
- en: This focuses the search without needing to scan the entire graph, improving
    latency. The embeddings supply useful query expansion signals for the next step.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这在不需要扫描整个图谱的情况下聚焦搜索，改善延迟。嵌入提供了有用的查询扩展信号，用于下一步。
- en: '**Step 2: Graph Query/Algorithm Execution**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：图查询/算法执行**'
- en: The selected entity embeddings suggest useful entry points and relationships
    for structured graphical queries and algorithms.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 选定的实体嵌入建议有用的切入点和关系，用于结构化的图形查询和算法。
- en: In our example, the matches for Book, Topic, and AI Concept cues exploration
    of BOOK-TOPIC and TOPIC-CONCEPT connections.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，Book、Topic和AI Concept的匹配引导了BOOK-TOPIC和TOPIC-CONCEPT连接的探索。
- en: 'Executing a query like:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 执行如下查询：
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This traverses booked linked to AI topics to produce relevant results.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这遍历了与AI主题相关的书籍以生成相关结果。
- en: 'Overall, the high-level flow is:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，高层次流程是：
- en: Use vector search to identify useful symbolic handles
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用向量搜索识别有用的符号句柄
- en: Execute graph algorithms seeded by these handles
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行由这些句柄种子生成的图算法
- en: This tight coupling connects the strength of similarity search with multi-hop
    reasoning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这种紧密耦合将相似性搜索的强度与多跳推理相结合。
- en: The key benefit is focusing complex graph algorithms using fast initial embedding
    matches. This improves latency and relevance by avoiding exhaustive graph scans
    for each query. The combination enables scalable, efficient semantic search and
    reasoning over vast knowledge.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 主要好处在于使用快速的初始嵌入匹配来聚焦复杂的图算法。这通过避免对每个查询进行详尽的图扫描，改善了延迟和相关性。这种组合使得对广泛知识的可扩展、高效的语义搜索和推理成为可能。
- en: Paralell Querying of Multiple Graphs or the same Graph
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行查询多个图谱或相同图谱
- en: '[](/achieving-structured-reasoning-with-llms-in-chaotic-contexts-with-thread-of-thought-prompting-and-a4b8018b619a?source=post_page-----582bff782113--------------------------------)
    [## Achieving Structured Reasoning with LLMs in Chaotic Contexts with Thread of
    Thought Prompting and…'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/achieving-structured-reasoning-with-llms-in-chaotic-contexts-with-thread-of-thought-prompting-and-a4b8018b619a?source=post_page-----582bff782113--------------------------------)
    [## 在混乱的背景下通过思维线索提示和…实现结构化推理]'
- en: Large language models (LLMs) demonstrated impressive few-shot learning capabilities,
    rapidly adapting to new tasks with…
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）展示了令人印象深刻的少量学习能力，能够迅速适应新任务...
- en: towardsdatascience.com](/achieving-structured-reasoning-with-llms-in-chaotic-contexts-with-thread-of-thought-prompting-and-a4b8018b619a?source=post_page-----582bff782113--------------------------------)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/achieving-structured-reasoning-with-llms-in-chaotic-contexts-with-thread-of-thought-prompting-and-a4b8018b619a?source=post_page-----582bff782113--------------------------------)'
- en: 'The key idea behind using multiple knowledge graphs in parallel is to provide
    the language model with a broader scope of structured knowledge to draw from during
    the reasoning process. Let me expand on the rationale:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个知识图谱并行的关键思想是为语言模型提供更广泛的结构化知识来源，以便在推理过程中进行借鉴。让我进一步阐述一下其合理性：
- en: '**Knowledge Breadth**: No single knowledge graph can encapsulate all of humanity’s
    accrued knowledge across every domain. By querying multiple knowledge graphs in
    parallel, we maximize the factual information available for the language model
    to leverage.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**知识广度**：没有任何一个知识图谱可以囊括所有人类在各个领域的积累的知识。通过并行查询多个知识图谱，我们可以最大化语言模型可利用的事实信息。'
- en: '**Reasoning Diversity**: Different knowledge graphs may model domains using
    different ontologies, rules, constraints etc. This diversity of knowledge representation
    exposes the language model to a wider array of reasoning patterns to learn.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**推理多样性**：不同的知识图谱可能使用不同的本体、规则、约束等来建模领域。这种知识表示的多样性使语言模型能够接触到更多样的推理模式进行学习。'
- en: '**Efficiency**: Querying knowledge graphs in parallel allows retrieving relevant
    information simultaneously. This improves latency compared to sequential queries.
    Parallel search allows more rapid gathering of contextual details to analyze.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**效率**：并行查询知识图谱允许同时检索相关信息。这比顺序查询提高了延迟效率。并行搜索允许更快速地收集上下文细节进行分析。'
- en: '**Robustness**: Having multiple knowledge sources provides redundancy in cases
    where a particular graph is unavailable or lacks information on a specific reasoning
    chain.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**鲁棒性**：拥有多个知识来源提供了冗余，以防某个特定图谱不可用或缺乏特定推理链的信息。'
- en: '**Transfer Learning**: Being exposed to a multitude of reasoning approaches
    provides more transferable learning examples for the language model. This enhances
    few-shot adaptation abilities.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迁移学习**：接触到多种推理方法为语言模型提供了更多可迁移的学习示例。这增强了少量学习的适应能力。'
- en: So in summary, orchestrating a chorus of knowledge graphs provides breadth and
    diversity of grounded knowledge to overcome limitations of individual knowledge
    bases. Parallel retrieval improves efficiency and robustness. Transfer learning
    across diverse reasoning patterns also accelerates language model adaption. This
    combination aims to scale structured knowledge injection towards more human-like
    versatile understanding.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，协调多个知识图谱提供了广泛和多样化的基础知识，以克服单一知识库的局限性。并行检索提高了效率和鲁棒性。跨多样化推理模式的迁移学习也加速了语言模型的适应。这种组合旨在扩展结构化知识注入，实现更接近人类的多功能理解。
- en: Large Language Models as a fluid semantic glue between structured modules
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型作为结构化模块之间流动的语义粘合剂
- en: While vector search and knowledge graphs provide structured symbolic representations,
    large language models (LLMs) like GPT-3 offer unstructured yet adaptive semantic
    knowledge. LLMs have demonstrated remarkable few-shot learning abilities, quickly
    adapting to new domains with only a handful of examples.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然向量搜索和知识图谱提供了结构化的符号表示，但像GPT-3这样的语言模型（LLMs）提供了非结构化但适应性强的语义知识。LLMs已经展示了卓越的少量学习能力，能够在只有少量示例的情况下快速适应新领域。
- en: This makes LLMs well-suited to act as a fluid semantic glue between structured
    modules — ingesting the symbolic knowledge, interpreting instructions, handling
    edge cases through generalization, and producing contextual outputs. They leverage
    their vast parametric knowledge to rapidly integrate with external programs and
    data representations.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得LLMs非常适合在结构化模块之间充当流动的语义胶——吸收符号知识，解释指令，通过泛化处理边缘情况，并生成上下文输出。它们利用其广泛的参数知识迅速与外部程序和数据表示集成。
- en: We can thus conceive of LLMs as a dynamic, ever-optimizing semantic layer. They
    ingest forms of structured knowledge and adapt on the fly based on new inputs
    and querying contexts. Rather than replacing symbolic approaches, LLMs amplify
    them through rapid binding and contextual response generation. This fluid integration
    saves the effort of manually handling all symbol grounding and edge cases explicitly.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将LLMs视为动态的、不断优化的语义层。它们吸收结构化知识的形式，并根据新的输入和查询上下文进行即时适应。LLMs不是替代符号方法，而是通过快速绑定和上下文响应生成来增强它们。这种流动的整合节省了手动处理所有符号基础和边缘情况的努力。
- en: Leveraging the innate capacities of LLMs for semantic generalization allows
    structured programs to focus on providing logical constraints and clean interfaces.
    The LLM then handles inconsistencies and gaps through adaptive few-shot learning.
    This symbiotic approach underscores architecting AI systems with distinct reasoning
    faculties suited for their inherent strengths.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 利用LLMs的固有语义泛化能力，使结构化程序能够专注于提供逻辑约束和清晰接口。LLM随后通过自适应少量学习处理不一致性和空白。这种共生的方法强调了以适应其固有优势的独特推理能力来构建AI系统。
- en: Structured Knowledge as AI’s Bedrock
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化知识作为AI的基石
- en: The exponential hype around artificial intelligence risks organizations pursuing
    short-sighted scripts promising quick returns. But meaningful progress requires
    patient cultivation of high-quality knowledge foundations. This manifests in structured
    knowledge graphs methodically encoding human expertise as networked representations
    over time.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 关于人工智能的指数级炒作风险让组织追求短视的脚本，承诺快速回报。但有意义的进展需要耐心地培养高质量的知识基础。这体现在结构化知识图谱中，系统地将人类专业知识编码为网络化的表示。
- en: Curating clean abstractions of complex domains as interconnected entities, constraints
    and rules is no trivial investment. It demands deliberate ontology engineering,
    disciplined data governance and iterative enhancement. The incremental nature
    can frustrate business leaders accustomed to rapid software cycles.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 将复杂领域策划成相互关联的实体、约束和规则并非小事。这需要有意的本体工程、严谨的数据治理和迭代的改进。这种渐进性可能会让习惯于快速软件周期的商业领袖感到沮丧。
- en: However, structured knowledge is AI’s missing pillar — curbing unbridled statistical
    models through grounding signals. Knowledge graphs provide the scaffolding for
    injectable domain knowledge, while enabling transparent querying and analysis.
    Their composable nature also allows interoperating with diverse systems.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，结构化知识是AI缺失的支柱——通过基础信号遏制不受控制的统计模型。知识图谱提供了注入领域知识的支撑结构，同时实现了透明的查询和分析。其可组合的特性也允许与不同系统的互操作。
- en: All this makes a compelling case for enterprise knowledge graphs as strategic
    assets. Much like databases evolved from flexible spreadsheets, the constraints
    of structure ultimately multiply capability. The entities and relationships within
    enterprise knowledge graphs become reliable touchpoints for driving everything
    from conversational assistants to analytics.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都使得企业知识图谱作为战略资产具有了令人信服的理由。正如数据库从灵活的电子表格发展而来，结构的约束最终乘以能力。企业知识图谱中的实体和关系成为推动从对话助手到分析的一切的可靠接触点。
- en: In the rush to the AI frontier, it is tempting to let unconstrained models loose
    on processes. But as with every past wave of automation, thoughtfully encoding
    human knowledge to elevate machine potential remains imperative. Managed well,
    maintaining this structured advantage compounds over time across applications,
    cementing market leadership. Knowledge powers better decisions — making enterprise
    knowledge graphs indispensable AI foundations.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在冲向AI前沿的过程中，放任不受约束的模型进行处理是很诱人的。但正如过去每一波自动化浪潮一样，精心编码人类知识以提升机器潜力仍然是至关重要的。如果管理得当，保持这种结构优势在应用中会随着时间的推移而不断增加，巩固市场领导地位。知识推动更好的决策——使得企业知识图谱成为不可或缺的AI基础。
- en: '![](../Images/f53b4b5d285671961d8cc6dfaf29b3df.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f53b4b5d285671961d8cc6dfaf29b3df.png)'
- en: Image generated by Dall-E-3 by the Author
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用Dall-E-3生成的图像
