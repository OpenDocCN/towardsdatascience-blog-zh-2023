- en: Retrieval-Augmented Generation (RAG)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）
- en: 原文：[https://towardsdatascience.com/add-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a](https://towardsdatascience.com/add-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/add-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a](https://towardsdatascience.com/add-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a)
- en: Learn how to add your own proprietary data to a pre-trained LLM using a prompt-based
    technique called Retrieval-Augmented Generation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何使用一种基于提示的技术，即检索增强生成，将你自己的专有数据添加到预训练的LLM中
- en: '[](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)
    ·21 min read·Sep 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)
    ·21分钟阅读·2023年9月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/cd0c3b0f21d72762c8a25dcf3ac3c039.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd0c3b0f21d72762c8a25dcf3ac3c039.png)'
- en: Photo by [Joshua Sortino](https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Joshua Sortino](https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Introduction**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: Large Language Models (LLMs) know a lot about the world, but they don’t know
    everything. Since training these models takes a long time, the data they were
    last trained on can be pretty old. And although LLMs know about general-purpose
    facts available on the internet, they don’t know about your proprietary data,
    which is often the data you need in your AI-based application. So it’s no surprise
    that extending LLMs with new data has been a considerable area of focus lately,
    both in academia and in the industry.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）了解很多关于世界的知识，但它们并不知晓所有信息。由于训练这些模型需要很长时间，因此它们最后一次训练的数据可能相当陈旧。尽管LLMs了解互联网上的通用信息，但它们不知道你的专有数据，而这通常是你在基于AI的应用中所需的数据。因此，将LLMs扩展到新数据上，最近在学术界和行业中成为了一个重要的研究领域也就不足为奇了。
- en: 'Before this new era of large language models, we would often extend models
    with new data by simply fine-tuning them. But now that our models are much larger
    and have been trained with much more data, fine-tuning is only appropriate for
    a few scenarios. Fine-tuning performs particularly well when we want to make our
    LLM communicate in a different style or tone. One great example of fine-tuning
    is OpenAI’s adaptation of their older completion-style GPT-3.5 models into their
    newer chat-style GPT-3.5-turbo (ChatGPT) models. Their completion-style models,
    if given the input “Can you tell me about cold-weather tents”, might reply by
    extending the prompt: “and any other cold-weather camping equipment?” On the other
    hand, their chat-style models might reply instead with an answer: “Certainly!
    They’re designed to withstand low temperatures, high winds, and snow by…” In this
    case, OpenAI’s focus was not to update the information the model had access to,
    but rather to change the way it converses with users. And for that, fine-tuning
    does wonders!'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: However, fine-tuning doesn’t perform as well for adding new data to large models,
    which I have found to be a much more common business scenario. In addition, fine-tuning
    LLMs requires a large amount of high quality data, a big budget to spend on compute
    resources, and a lot of time — all of which are scarce resources for most users.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we’ll cover an alternative technique called “Retrieval-Augmented
    Generation” (RAG). This approach is based on prompting and it was introduced by
    Facebook AI Research (FAIR) and collaborators in 2021\. The RAG concept is powerful
    enough that it’s used by Bing search and other high-traffic sites to incorporate
    current data into their models, and yet simple enough that I can explain it to
    you in depth in this blog post. It’s also effective when you don’t have a large
    amount of new data, a big budget, or a lot of time.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find three code implementations for a simple RAG scenario in the [GitHub
    repo](https://github.com/bstollnitz/rag/) associated with this blog post: one
    that uses the OpenAI APIs directly, another that uses the open-source LangChain
    API, and a third implementation that uses the open-source Semantic Kernel API.
    I will show and explain the code for the first scenario in this blog post, and
    encourage you to browse the others on your own time.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: But before we dive into the code, let’s learn the fundamental concepts of RAG.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '**An overview of RAG**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Retrieval-Augmented Generation can vary in its implementation, but at a conceptual
    level, using RAG in an AI-based application involves the following steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The user inputs a question.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system searches for relevant documents that might answer the question. These
    documents often consist of proprietary data, and are stored in some kind of document
    index.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system creates an LLM prompt that combines the user input, the related documents,
    and instructions for the LLM to answer the user’s question using the documents
    provided.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统创建一个 LLM 提示，将用户输入、相关文档和 LLM 的指令结合起来，让 LLM 使用提供的文档回答用户的问题。
- en: The system sends the prompt to an LLM.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统将提示发送到 LLM。
- en: The LLM returns an answer to the user’s question, grounded by the context we
    provided. This is the output of our system.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 根据我们提供的上下文向用户的问题返回答案。这是我们系统的输出。
- en: 'Here’s a diagram of this general idea:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这个一般概念的图示：
- en: '![](../Images/fcc60dfb9df9e2f70e036ce0b0b10e31.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcc60dfb9df9e2f70e036ce0b0b10e31.png)'
- en: I’ve given you a colloquial meaning for RAG, but it’s somewhat lacking in implementation
    details. Let’s take a closer look at the paper that introduced this idea to start
    grasping the specifics.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经给你提供了 RAG 的口语含义，但在实现细节上有所欠缺。让我们深入研究一下介绍这一概念的论文，以开始掌握具体细节。
- en: '**The RAG research paper**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**RAG 研究论文**'
- en: The term RAG was first introduced by FAIR and academic collaborators in 2021,
    in their paper titled [Retrieval-Augmented Generation for Knowledge-Intensive
    NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf). The ideas presented by the authors
    had a tremendous impact in the industry solutions we use today, so they’re worth
    getting familiar with.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 这个术语首次由 FAIR 和学术合作伙伴于 2021 年引入，在他们题为 [Retrieval-Augmented Generation for
    Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf) 的论文中提出。作者提出的这些理念对我们今天使用的行业解决方案产生了巨大影响，因此值得了解。
- en: 'Here’s an overview of the architecture presented in the paper:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是论文中介绍的架构概览：
- en: '![](../Images/ca9ba59b9aead48ce16a2087abad6c8c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca9ba59b9aead48ce16a2087abad6c8c.png)'
- en: 'We’ll look at each piece of this architecture in depth throughout this post.
    At a high level, the proposed structure is composed of two components: a retriever
    and a generator. The retriever component transforms the input text into a sequence
    of floating point numbers (a vector) using the query encoder, transforms each
    document in the same way using the document encoder, and stores the document encodings
    in a search index. It then searches the search index for document vectors that
    are related to the input vector, converts the document vectors back into text,
    and returns their text as output. The generator then takes the user’s input text
    and matched documents, combines them into a prompt, and asks an LLM for a reply
    to the user’s input given the information in the documents. The output of that
    LLM is the output of the system.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这篇文章中深入探讨这一架构的每一部分。从高层次来看，提出的结构由两个组件组成：检索器和生成器。检索器组件使用查询编码器将输入文本转换为浮点数序列（向量），使用文档编码器以相同方式转换每个文档，并将文档编码存储在搜索索引中。然后，它在搜索索引中搜索与输入向量相关的文档向量，将文档向量转换回文本，并将这些文本作为输出返回。生成器然后接受用户的输入文本和匹配的文档，将它们合并为一个提示，并根据文档中的信息请求
    LLM 对用户输入的回复。该 LLM 的输出即为系统的输出。
- en: 'You may have noticed that the query encoder, document encoder, and LLM are
    all represented similarly in the image above — that’s because they’re all implemented
    using transformers. Traditional transformers are composed of two parts: an encoder
    and a decoder. The encoder is responsible for transforming the input text into
    a vector (or sequence of vectors) that roughly captures the meaning of the words;
    and the decoder is responsible for generating new text based on the input text.
    In the paper’s architecture, the query encoder and document encoder are implemented
    using encoder-only transformers because they just need to transform pieces of
    text into vectors of numbers. The LLM in the generator is implemented using a
    traditional encoder-decoder transformer.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到查询编码器、文档编码器和 LLM 在上图中都以类似方式表示——这是因为它们都是使用 transformers 实现的。传统的 transformers
    由两部分组成：编码器和解码器。编码器负责将输入文本转换为大致捕捉单词含义的向量（或向量序列）；解码器负责根据输入文本生成新文本。在论文的架构中，查询编码器和文档编码器使用仅编码器
    transformers 实现，因为它们只需将文本片段转换为数字向量。生成器中的 LLM 使用传统的编码器-解码器 transformers 实现。
- en: How is this architecture trained? The paper proposes using pre-trained transformers,
    and jointly fine-tuning the query encoder and generator LLM only. This fine-tuning
    is done using pairs of user inputs and the corresponding outputs expected from
    the LLM. The document encoder is not fine-tuned because that would be costly and
    the authors found that it’s not necessary for good performance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构是如何训练的？论文建议使用预训练的变换器，并仅对查询编码器和生成器LLM进行联合微调。这种微调是使用用户输入和LLM预期输出的对应对进行的。文档编码器没有进行微调，因为这样做成本较高，而且作者发现对于良好的性能来说并不必要。
- en: 'The paper proposes two approaches for the implementation of this architecture:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 论文提出了实现这种架构的两种方法：
- en: '**RAG-sequence** — We retrieve *k* documents, and use them to generate all
    the output tokens that answer a user query.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RAG-sequence** — 我们检索*k*个文档，并使用它们生成回答用户查询的所有输出标记。'
- en: '**RAG-token**— We retrieve *k* documents, use them to generate the next token,
    then retrieve *k* more documents, use them to generate the next token, and so
    on. This means that we could end up retrieving several different sets of documents
    in the generation of a single answer to a user’s query.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RAG-token** — 我们检索*k*个文档，使用它们生成下一个标记，然后再检索*k*个文档，使用它们生成下一个标记，依此类推。这意味着在生成单个答案时，我们可能会检索多个不同的文档集。'
- en: You now have a good high-level understanding of the architecture in the RAG
    paper. This pattern is very common in the industry; however, it turns out that
    not every detail in the paper is implemented exactly as proposed.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在对RAG论文中的架构有了很好的高层次理解。这种模式在行业中非常常见；然而，结果是论文中的每一个细节并不总是按照建议的方式实现。
- en: '**RAG as used in the industry**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**行业中使用的RAG**'
- en: 'In practice, the RAG implementations that are common in the industry have been
    adapted from the paper in the following ways:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，行业内常见的RAG实现方式已从论文中进行了如下适配：
- en: Of the two approaches proposed in the paper, the RAG-sequence implementation
    is pretty much always used in the industry. It’s cheaper and simpler to run than
    the alternative, and it produces great results.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在论文中提出的两种方法中，RAG-sequence的实现几乎在行业中总是被使用。它比另一种方法便宜且更简单，且能产生很好的结果。
- en: We don’t generally fine-tune any of the transformers. The pre-trained LLMs available
    to us these days are good enough to use as is, and too costly to fine-tune ourselves.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通常不会对任何变换器进行微调。现有的预训练LLM已经足够好，不需要进一步微调，而且自己微调的成本也过高。
- en: 'In addition, the method for searching documents isn’t always done exactly as
    proposed by the paper. Search is commonly done with the help of a search service,
    such as [FAISS](https://github.com/facebookresearch/faiss) or [Azure Cognitive
    Search](https://azure.microsoft.com/en-us/products/ai-services/cognitive-search),
    and these services support different search techniques that pair well with RAG.
    A search service is generally composed of the following two steps of execution:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，文档搜索的方法也并不总是完全按照论文中的建议进行。搜索通常借助搜索服务进行，如[FAISS](https://github.com/facebookresearch/faiss)或[Azure
    Cognitive Search](https://azure.microsoft.com/en-us/products/ai-services/cognitive-search)，这些服务支持与RAG配对良好的不同搜索技术。搜索服务通常由以下两个执行步骤组成：
- en: '**Retrieval**: This step compares the user’s query with the documents in the
    search index, and retrieves the most relevant documents. There are three common
    retrieval techniques: keyword search, vector search, and hybrid search.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索**：此步骤将用户的查询与搜索索引中的文档进行比较，并检索出最相关的文档。常见的检索技术有三种：关键词搜索、向量搜索和混合搜索。'
- en: '**Ranking**: This is an optional step that follows retrieval. It takes the
    list of documents that were found relevant by retrieval and improves on the order
    in which they’re ranked.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排名**：这是一个可选步骤，跟随检索步骤进行。它对检索到的相关文档列表进行排序优化。'
- en: Let’s learn about each of these in more detail, starting with the three types
    of retrieval.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解这些内容，从三种检索类型开始。
- en: '**Keyword search**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键词搜索**'
- en: 'The simplest way to find documents that are related to a user query is to do
    a “keyword search” (also known as “full-text search”). Keyword search uses the
    exact terms in the user input text to search an index for documents with matching
    text. The matching is done based on text only, with no vectors involved. Even
    though this technique has been around for a while, it’s still relevant today.
    This type of search is very useful when you’re searching for user IDs, product
    codes, addresses, and any other data that requires high precision matches. Here’s
    a high-level diagram of this implementation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 查找与用户查询相关的文档的最简单方法是进行“关键词搜索”（也称为“全文搜索”）。关键词搜索使用用户输入文本中的精确术语来搜索索引中的匹配文档。匹配仅基于文本进行，不涉及向量。尽管这种技术已经存在了一段时间，但今天仍然相关。这种搜索在寻找用户ID、产品代码、地址和任何需要高精度匹配的数据时非常有用。以下是这种实现的高层次图示：
- en: '![](../Images/e20c2107f7aba33e20f921f9a31a90a5.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e20c2107f7aba33e20f921f9a31a90a5.png)'
- en: In this scenario, our search service keeps an inverted index that maps from
    words to documents that use those words. The user’s text input is parsed to extract
    search terms, and analyzed to find standard forms of those terms. The inverted
    index is then scanned for the search terms, each match is scored, and the most
    relevant matching documents are returned from the search service.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的搜索服务保留一个反向索引，该索引将单词映射到使用这些单词的文档。用户的文本输入会被解析以提取搜索词，并分析以找到这些词的标准形式。然后扫描反向索引中的搜索词，每个匹配项都会被评分，最后从搜索服务中返回最相关的匹配文档。
- en: '**Vector search**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**Vector search**'
- en: “Vector search” (also known as “dense retrieval”) differs from keyword search
    in the sense that it can find matches when no search terms are present in the
    documents, but the general ideas are similar. For example, suppose you’re building
    a chatbot to support a property rental site. If the user asks, “Do you have recommendations
    for a spacious apartment close to the sea?” and the document for a particular
    property contains the text “4000 sq ft home with ocean view,” keyword search won’t
    identify that as a match, but vector search will. Vector search works best when
    we’re searching unstructured text for general ideas, rather than precise keywords.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: “向量搜索”（也称为“密集检索”）与关键词搜索的不同之处在于，即使文档中没有搜索词，它也能找到匹配项，但总体思路类似。例如，假设你正在构建一个支持房产租赁网站的聊天机器人。如果用户询问“你有没有推荐靠近海边的宽敞公寓？”而某个特定房产的文档包含“4000平方英尺的海景房”这一文本，关键词搜索将无法识别为匹配，但向量搜索会识别。向量搜索在我们搜索非结构化文本以获取一般概念时效果最佳，而不是精确的关键词。
- en: 'Here’s a high-level overview of RAG with vector search:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 RAG 与向量搜索的高层次概述：
- en: '![](../Images/022561c5321ba663db9f1ca5adb1d144.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/022561c5321ba663db9f1ca5adb1d144.png)'
- en: As you can see in the diagram, vector search as used in the industry is pretty
    much the same as the RAG paper proposal. Except that in this case we don’t fine-tune
    the transformers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，行业中使用的向量搜索与 RAG 论文中的提议基本相同。只是在这种情况下，我们不对变换器进行微调。
- en: We typically use a pre-trained embedding model such as OpenAI’s *text-embedding-ada-002*
    to encode the query and documents, and a pre-trained LLM such as OpenAI’s *gpt-35-turbo*
    (ChatGPT) to produce the final output. The embedding model is used to translate
    the input text and each of our documents into a corresponding “embedding.” What’s
    an embedding? It’s a vector of floating-point numbers that roughly captures the
    general idea of the text it encodes. If two pieces of text are related, then we
    can assume that the corresponding embedding vectors are similar.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用预训练的嵌入模型，如 OpenAI 的 *text-embedding-ada-002* 来编码查询和文档，并使用预训练的 LLM，如 OpenAI
    的 *gpt-35-turbo*（ChatGPT）来生成最终输出。嵌入模型用于将输入文本和每个文档转换为相应的“嵌入”。什么是嵌入？它是一个浮点数向量，大致捕捉了它所编码文本的一般思想。如果两个文本相关，我们可以假设它们的嵌入向量是相似的。
- en: 'How can we determine if two vectors are similar? Let’s look at an example to
    answer that question. We’ll assume that we used our embedding model to compute
    the following embedding vectors:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何确定两个向量是否相似？我们来通过一个例子来解答这个问题。我们假设使用了我们的嵌入模型来计算以下嵌入向量：
- en: '**a** = (0, 1) represents “Do you have recommendations for a spacious apartment
    close to the sea?”'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**a** = (0, 1) 表示“你有没有推荐靠近海边的宽敞公寓？”'
- en: '**b** = (0.12, 0.99) represents “4000 sq ft home with ocean view”'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**b** = (0.12, 0.99) 表示“4000平方英尺的海景房”'
- en: '**c** = (0.96, 0.26) represents “I want a donut”'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**c** = (0.96, 0.26) 表示“我想要一个甜甜圈”'
- en: 'We can plot these in a graph:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些向量绘制在图表中：
- en: '![](../Images/ffcbc89b567a4f9b952f8a15541a7f87.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffcbc89b567a4f9b952f8a15541a7f87.png)'
- en: 'Looking at the image, you can intuitively tell that the vector that’s most
    simlar to **a** is **b** (rather than **c**). Progammatically speaking, there
    are three common methods for calculating vector similarity: dot product, cosine
    similarity, and Euclidean distance. Let’s calculate similarities using these methods,
    and see if we can validate our intuition.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像中，你可以直观地判断出最接近**a**的向量是**b**（而不是**c**）。从编程的角度看，计算向量相似性有三种常见的方法：点积、余弦相似性和欧几里得距离。让我们使用这些方法计算相似性，并看看是否能验证我们的直觉。
- en: 'The dot product method, as you’d expect, simply calculates the dot product
    (also known as the inner product) between two vectors. The larger the result,
    the more similar the vectors are:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 点积方法，如你所料，简单地计算两个向量之间的点积（也称为内积）。结果越大，向量之间的相似性就越高：
- en: '![](../Images/0db77ddcf6f8d56f89562d50616b5d80.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0db77ddcf6f8d56f89562d50616b5d80.png)'
- en: The dot product between **a** and **b** is larger than the dot product between
    **a** and **c**, which confirms our intuition that **a** and **b** are more similar.
    Keep in mind that the dot product is strongly influenced by the lengths of the
    vectors, and really only measures similarity if the vectors all have the same
    length. OpenAI embeddings always have length one, and to be consistent with OpenAI
    our sample embeddings also have length one.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**a**和**b**之间的点积大于**a**和**c**之间的点积，这确认了我们的直觉，即**a**和**b**更为相似。请记住，点积受到向量长度的强烈影响，实际上只有在所有向量长度相同的情况下才测量相似性。OpenAI的嵌入向量长度总是为一，为了与OpenAI保持一致，我们的示例嵌入向量也具有相同的长度。'
- en: Cosine similarity calculates the cosine of the angle between two vectors, and
    the larger the cosine, the more similar the vectors are. The formula to calculate
    it starts out similar to the dot product, but the vectors are then divided by
    the product of their lengths. As a result of this extra normalization step, cosine
    similarity effectively measures the similarity between two vectors by accounting
    for their directions while ignoring their lengths.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似性计算两个向量之间夹角的余弦值，余弦值越大，向量越相似。计算公式类似于点积，但向量在计算后会被其长度的乘积除以。因此，余弦相似性通过考虑方向而忽略长度，实质上测量了两个向量之间的相似性。
- en: '![](../Images/9d1c9ca3b09173447a6bb6394b711aac.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d1c9ca3b09173447a6bb6394b711aac.png)'
- en: Because our vectors have length one, dot product and cosine similarity produce
    exactly the same results, as you can see in the calculations above.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的向量长度为一，点积和余弦相似性产生完全相同的结果，如上面的计算所示。
- en: Euclidean distance measures distance between two vectors, in the usual sense.
    The smaller the distance between two vectors, the more similar they are. Unlike
    cosine similarity, Euclidean distance takes into account the length and direction
    of the vectors.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离测量两个向量之间的距离，按照通常的定义。两个向量之间的距离越小，它们的相似性就越高。与余弦相似性不同，欧几里得距离考虑了向量的长度和方向。
- en: '![](../Images/332fa815424cb03da4f3f91f3ed840e9.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/332fa815424cb03da4f3f91f3ed840e9.png)'
- en: As you can see, the distance between **a** and **b** is smaller than **a** and
    **c**, which means that **a** and **b** are most similar.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，**a**和**b**之间的距离小于**a**和**c**之间的距离，这意味着**a**和**b**最为相似。
- en: Our sample uses 2D vectors so that we can get some visual intuition, but it’s
    common for embedding vectors to have many more dimensions. For example, the *text-embedding-ada-002*
    model from OpenAI generates vectors with 1536 dimensions. Keep in mind that the
    number of dimensions used for embeddings doesn’t depend on the length of the input
    text, so a short query and a long document will both result in an embedding vector
    with the same number of dimensions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例使用二维向量，以便获得一些直观的视觉效果，但嵌入向量通常具有更多的维度。例如，OpenAI的*text-embedding-ada-002*模型生成1536维的向量。请记住，用于嵌入的维度数量与输入文本的长度无关，因此短查询和长文档都将生成具有相同维度数量的嵌入向量。
- en: Most search services available today support the three similarity methods we
    discussed, and allow you to choose the method you want to use. Which similarity
    technique should you choose?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 目前大多数搜索服务都支持我们讨论的三种相似性方法，并允许你选择想要使用的方法。你应该选择哪种相似性技术？
- en: If your embeddings have different lengths and you want to take those lengths
    into account, then Euclidean distance is the best choice, because it considers
    both length and direction.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your embeddings have all been normalized to unit length, then all three solutions
    will give you the same ranking, as you saw. However, dot product is slightly cheaper
    to compute. If an app or service you’re working with knows it’s dealing with unit
    length vectors, then most likely its cosine similarity implementation has been
    optimized to use the same calculations as dot product. So most likely cosine similarity
    is as cheap to compute as dot product in this case.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to find the top document vectors for a given input vector, our search
    service could simply use brute force to calculate the similarity between the input
    vector and each document vector, and then select the top matches. However, this
    simple algorithm wouldn’t scale to large enterprise applications with lots of
    document vectors. Therefore search services generally use some type of Approximate
    Nearest Neighbor (ANN) algorithm, which use clever optimizations to give approximate
    results in much less time. One popular implementation of ANN is the Hierarchical
    Navigable Small World (HNSW) algorithm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '**Hybrid search**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid search consists of the simultaneous use of keyword and vector search.
    For example, let’s consider a scenario where you have a customer ID and a text
    input query, and you want to do a search that captures both the high-precision
    of the customer ID and the general meaning of the user text. This is the perfect
    scenario for hybrid search. Hybrid search performs the two types of search separately,
    and then it combines the outputs using an algorithm that selects the best results
    of each technique. This method is used often in the industry, especially in more
    complex applications.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic ranking**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Semantic ranking (also known as “rerank”) is an optional step that follows the
    retrieval of documents. The retrieval step does its best to rank the returned
    documents based on how relevant they are to the user’s query, but the semantic
    ranking step can often improve on that result. It takes a subset of the documents
    returned by the retrieval, computes higher quality relevance scores using an LLM
    that is trained just for that task, and reranks the documents based on those scores.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6789bdec7e36798f319a395d4d58f9be.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: In the image above I show semantic ranking combined with vector search, but
    you could easily combine it with keyword search instead. I decided to show semantic
    ranking with vector search in the diagram because that’s the solution I’ve implemented
    in the code for this post, which we’ll look at next.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '**A simple implementation of RAG**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we’ll take a look at code that adds our own custom data to
    ChatGPT, using RAG and Azure Cognitive Search. The code I’ll show here uses OpenAI
    APIs to interact with ChatGPT directly, but in the same [GitHub project](https://github.com/bstollnitz/rag/),
    you’ll find two other similar implementations: one that uses LangChain and another
    that uses Semantic Kernel. These are two popular open-source frameworks that help
    developers build applications with LLMs.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将查看将自定义数据添加到 ChatGPT 的代码，使用 RAG 和 Azure Cognitive Search。我在这里展示的代码使用
    OpenAI API 直接与 ChatGPT 交互，但在同一个 [GitHub 项目](https://github.com/bstollnitz/rag/)
    中，你还会找到两个类似的实现：一个使用 LangChain，另一个使用 Semantic Kernel。这两个是帮助开发者使用大型语言模型构建应用程序的流行开源框架。
- en: The goal for this project is to create a chatbot that our users can leverage
    to get more information about the products our company sells. The [data](https://github.com/bstollnitz/rag/tree/main/data)
    we’ll use consists of several markdown files containing details about our products.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的目标是创建一个聊天机器人，供用户利用它获取有关我们公司销售的产品的更多信息。我们将使用的 [数据](https://github.com/bstollnitz/rag/tree/main/data)
    包含了若干个关于我们产品的 markdown 文件。
- en: 'Let’s start by looking at [init_search_1.py](https://github.com/bstollnitz/rag/blob/main/src/1_openai/init_search_1.py):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先查看 [init_search_1.py](https://github.com/bstollnitz/rag/blob/main/src/1_openai/init_search_1.py)：
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Our first task is to load the markdown data files and split them into chunks
    of 6000 characters, with 100 characters of overlap between chunks. Each of these
    data chunks will later be encoded by a single embedding and added to the search
    index. If our files are small enough, we don’t need to break them into chunks,
    and can encode each file as is. But if they’re big, it’s better to break them
    up, because our embedding has a fixed size and won’t be able to capture all the
    information as well. We add some overlap to the chunks so that we don’t lose ideas
    that cross chunk boundaries.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个任务是加载 markdown 数据文件，并将其拆分为 6000 个字符的块，每块之间有 100 个字符的重叠。每个数据块将在后续由单个嵌入编码，并添加到搜索索引中。如果我们的文件足够小，我们可以不拆分它们，直接对每个文件进行编码。但如果文件很大，最好将其拆分，因为我们的嵌入有固定大小，无法很好地捕捉所有信息。我们在块之间添加一些重叠，以免丢失跨块的想法。
- en: Next we create a list that contains a dictionary per chunk, specifying the text
    of the chunk, a unique ID, and the name of the source file. We then calculate
    the embedding for each chunk using OpenAI’s *text-embedding-ada-002* model, and
    insert that into the chunk’s dictionary as well. This list of dictionaries contains
    all the information needed to populate a search index.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含每个块的字典的列表，字典中指定了块的文本、唯一 ID 和源文件的名称。然后，我们使用 OpenAI 的 *text-embedding-ada-002*
    模型计算每个块的嵌入，并将其也插入到块的字典中。这个字典列表包含了填充搜索索引所需的所有信息。
- en: Next, we create the search index using the Azure Cognitive Search API. We configure
    the fields that will be created in the index, taking care to specify that the
    field with the embedding needs to support vector search. We configure our vector
    search by saying that the embedding size is 1536 (which is always the case for
    the OpenAI model we’re using), that we want to use the cosine method for vector
    similarity comparison (as recommended by OpenAI), and that we want to use the
    Hierarchical Navigable Small World (HNSW) algorithm to speed up the comparison
    search. We also specify that we want to enable semantic ranking for our context
    text field. We give our index a name, and save it.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 Azure Cognitive Search API 创建搜索索引。我们配置将要在索引中创建的字段，特别指定需要支持向量搜索的嵌入字段。我们通过设置嵌入大小为
    1536（这是我们使用的 OpenAI 模型的固定值），指定要使用余弦方法进行向量相似性比较（根据 OpenAI 的推荐），并且使用 Hierarchical
    Navigable Small World (HNSW) 算法来加快比较搜索速度，来配置我们的向量搜索。我们还指定要为我们的上下文文本字段启用语义排序。我们给索引命名并保存。
- en: 'Last, we upload our data to the search index we just created. You can confirm
    that all the chunks have been added to the index by going to [the Azure portal](https://portal.azure.com),
    clicking on your Cognitive Search resource, then on the name of your resource,
    and then on “Indexes.” You should see your index listed on the right side, together
    with the document count, which is the number of chunks that were uploaded to the
    index (I have 45 chunks in my index). If you click on the index name, you can
    see the data that was uploaded to it by clicking the “Search” button within “Search
    Explorer”:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将数据上传到刚刚创建的搜索索引中。你可以通过访问 [Azure 门户](https://portal.azure.com)，点击你的 Cognitive
    Search 资源，然后点击资源名称，再点击“Indexes”来确认所有的块是否已被添加到索引中。你应该会在右侧看到你的索引列表及其文档计数，即上传到索引中的块的数量（我在我的索引中有
    45 个块）。如果你点击索引名称，可以通过点击“Search Explorer”中的“Search”按钮查看上传到其中的数据。
- en: '![](../Images/ddf912a287aaaba5cd9965ebcb96bb89.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddf912a287aaaba5cd9965ebcb96bb89.png)'
- en: If you look at the equivalent *init_search* files in the LangChain and Semantic
    Kernel folders, you’ll see that they contain much less code. Using the Azure Cognitive
    Search APIs directly gives you more control over the configuration of the index,
    and using a library as an intermediary hides away a lot of the complexity. The
    best option really depends on what you’re looking for.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看 LangChain 和 Semantic Kernel 文件夹中的等效 *init_search* 文件，你会发现它们包含的代码要少得多。直接使用
    Azure Cognitive Search APIs 给你更多的索引配置控制权，而使用库作为中介则隐藏了许多复杂性。最佳选项实际上取决于你的需求。
- en: 'Now that we’ve established our search index, we’ll see how the index can be
    used in a chatbot. Let’s look at the [main_1.py](https://github.com/bstollnitz/rag/blob/main/src/1_openai/main_1.py)
    file, which is our main executable file for the chatbot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了搜索索引，接下来我们将看到如何在聊天机器人中使用该索引。让我们看看 [main_1.py](https://github.com/bstollnitz/rag/blob/main/src/1_openai/main_1.py)
    文件，这是我们聊天机器人的主要可执行文件：
- en: '[PRE1]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The *main* function pretends to be a user, asking the chatbot a sequence of
    questions. For each question, the code simply invokes the *ask* function of the
    chatbot. The equivalent files in the LangChain and Semantic Kernel versions look
    essentially the same.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*main* 函数模拟用户，向聊天机器人提出一系列问题。对于每个问题，代码简单地调用聊天机器人的 *ask* 函数。LangChain 和 Semantic
    Kernel 版本中的等效文件看起来基本相同。'
- en: 'Last, let’s look at the [chatbot_1.py](https://github.com/bstollnitz/rag/blob/main/src/1_openai/chatbot_1.py)
    file, which contains the code for a chatbot that remembers the conversation history
    and knows how to implement RAG:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看 [chatbot_1.py](https://github.com/bstollnitz/rag/blob/main/src/1_openai/chatbot_1.py)
    文件，其中包含了一个记住对话历史并知道如何实现 RAG 的聊天机器人代码：
- en: '[PRE2]{chat_history_str}[PRE3]{query}[PRE4]{context}[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE2]{chat_history_str}[PRE3]{query}[PRE4]{context}[PRE5]'
- en: 'Our chatbot keeps a *chat_history* list of questions and answers in memory,
    so that it can make sense of questions in the context of the entire conversation.
    The *Chatbot* class has a single public *ask* function, which contains the following
    steps:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的聊天机器人会在内存中保留一个 *chat_history* 问答列表，以便在整个对话的背景下理解问题。 *Chatbot* 类有一个公共的 *ask*
    函数，包含以下步骤：
- en: The *_summarize_user_intent* function uses an LLM to rephrase the user’s question
    while taking into account any previous chat history. Why do we need this step?
    Suppose the user asks a question that doesn’t quite make sense on its own, but
    makes sense when seen in the context of the chat history; for example, if a question
    mentions “it” in reference to a previous topic. If we search our index for documents
    related to the user’s question on its own, we probably won’t get good results.
    But if we rephrase the user’s question to incorporate the missing history, we’ll
    get a much better set of documents. You’ll see an example of this in the output
    printout I’ll show soon.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*_summarize_user_intent* 函数使用 LLM 重新措辞用户的问题，同时考虑任何先前的聊天历史。为什么我们需要这一步？假设用户提出的问题本身不太有意义，但在聊天历史的背景下却很有意义；例如，如果一个问题提到“它”指代先前的主题。如果我们仅搜索与用户问题相关的文档，可能不会得到好的结果。但如果我们重新措辞用户的问题以融入缺失的历史，我们将获得更好的文档集。你会在我稍后展示的输出打印中看到一个例子。'
- en: '*_get_context* searches the index that we created earlier, looking for documents
    similar to the user intent obtained in the previous step.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*_get_context* 搜索我们之前创建的索引，寻找与前一步获得的用户意图相似的文档。'
- en: '*_rag* asks an LLM for an answer to the user query, based on the documents
    returned from our search and the chat history. We take care to update the chat
    history with the user and assistant messages in this step.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*_rag* 根据从我们的搜索和聊天历史中返回的文档向 LLM 请求用户查询的答案。在此步骤中，我们会更新聊天历史记录，包括用户和助手的消息。'
- en: If you look at the equivalent files in the LangChain and Semantic Kernel folders,
    you’ll see that they both use templating APIs to construct the prompt sent to
    the LLM. You’ll also notice that LangChain has built-in support for keeping chat
    history. Semantic Kernel, on the other hand, is built around the concept of functions
    (reusable pieces of code) and plugins (collections of functions that can be called
    by external apps in a standardized way).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看 LangChain 和 Semantic Kernel 文件夹中的等效文件，你会发现它们都使用模板 API 来构造发送给 LLM 的提示。你还会注意到
    LangChain 内置了保持聊天历史记录的支持。另一方面，Semantic Kernel 是围绕函数（可重用的代码片段）和插件（可以被外部应用以标准化方式调用的函数集合）这一概念构建的。
- en: 'You should get output similar to the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到类似于以下的输出：
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In addition to the question and response, I’m also printing the user intent,
    so that you get a feel for why it’s useful. As you can see, the second question
    is ambiguous without chat history, but the corresponding user intent stands on
    its own by incorporating knowledge from the first question.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了问题和回答，我还会打印用户意图，以便你了解它的实用性。如你所见，第二个问题在没有聊天历史记录的情况下是模糊的，但对应的用户意图通过融入第一个问题的知识独立存在。
- en: The last response demonstrates that the system answers our questions by relying
    only on the documents we provided, and not on information that the LLM may have
    learned during training.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的响应展示了系统仅依靠我们提供的文档回答问题，而不是依赖 LLM 在训练过程中学到的信息。
- en: '**Conclusion**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In this post, we covered the RAG pattern for extending a pre-trained LLM with
    custom data. We talked about the research paper where the RAG concept was first
    introduced, how it’s been adapted for the industry, and the different search techniques
    comonly used in conjunction with it. We finished by discussing a code sample that
    shows a RAG implementation using OpenAI and Azure Cognitive Search.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了如何使用自定义数据扩展预训练 LLM 的 RAG 模式。我们讨论了首次提出 RAG 概念的研究论文，它如何适应行业需求，以及常用的搜索技术。最后，我们讨论了一个代码示例，展示了如何使用
    OpenAI 和 Azure Cognitive Search 实现 RAG。
- en: If you’re looking for a full RAG-based chat application, including client code
    and enterprise-level best-practices, I recommend that you take a look at the [Azure
    Chat repo](https://github.com/microsoft/azurechat) created by my colleagues at
    Microsoft. This application provides solutions to many common issues, and is sure
    to save you time.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在寻找一个完整的基于 RAG 的聊天应用程序，包括客户端代码和企业级最佳实践，我推荐你查看我在微软的同事们创建的 [Azure Chat repo](https://github.com/microsoft/azurechat)。这个应用程序提供了许多常见问题的解决方案，肯定能为你节省时间。
- en: Hopefully this has inspired you to use the RAG pattern in your work. Thank you
    for reading, and good luck with your AI projects!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这能激励你在工作中使用 RAG 模式。感谢阅读，祝你的 AI 项目好运！
- en: '**Note**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: All images are by the author unless otherwise noted. You can use any of the
    original images in this blog post for any purpose, with attribution (a link to
    this article).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由作者提供，除非另有说明。你可以在任何用途下使用这篇博客文章中的任何原始图片，并附上出处链接（指向本文）。
