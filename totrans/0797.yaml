- en: 'Elliot Activation Function: What Is It and Is It Effective?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a](https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is the Elliot Activation Function and is it a good alternative to the other
    activation functions used in neural networks?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![Benjamin
    McCloskey](../Images/7118f5933f2affe2a7a4d3375452fa4c.png)](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)
    [Benjamin McCloskey](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)
    ·7 min read·Feb 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39a7ac7751ff21380e8ecc69c64edfb1.png)'
  prefs: []
  type: TYPE_IMG
- en: Elliot Activation Function (Image from Author)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Are you in the middle of creating a new machine-learning model and unsure of
    what activation function you should be using?
  prefs: []
  type: TYPE_NORMAL
- en: '**But wait, *what is an activation function?***'
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions allow machine learning models to understand and solve *nonlinear*
    problems. Using an activation function in neural networks specifically helps with
    the passing of the most important information from each neuron to the next. Today,
    the ReLU Activation Function is generally used in the architecture of Neural Networks,
    however, that does not necessarily mean it is always the best choice. (Check out
    my post below on the ReLU and LReLU Activations).
  prefs: []
  type: TYPE_NORMAL
- en: '[](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------)
    [## Leaky ReLU vs. ReLU Activation Functions: Which is Better?'
  prefs: []
  type: TYPE_NORMAL
- en: An experiment to investigate if there is a noticeable difference in a model’s
    performance when using a ReLU Activation…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: I recently came across the **Elliot Activation Function** which was praised
    as being a possible alternative to various activation functions, including the
    Sigmoid and Hyperbolic Tagenet. Today we will run an experiment to test the Elliot
    Activation Function’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment 1:** *Test the Elliot Activation Function’s performance against
    the Sigmoid Activation Function and Hyperbolic Tangent Activation Function.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment 2:** *Test the Elliot Activation Function’s performance against
    the ReLU Activation Function.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to answer the question: I***s the Elliot Activation Function effective
    or not?***'
  prefs: []
  type: TYPE_NORMAL
- en: Elliot Activation Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/77e52168c1320307627e828d342e28a2.png)'
  prefs: []
  type: TYPE_IMG
- en: The Elliot Activation Function will result in an approximation that is relatively
    close to the Sigmoid and Hyperbolic Tangent Activation Functions. Some have found
    that Elliot performs calculations **2x faster** than the Sigmoid Activation Function
    [3]. Just like the Sigmoid Activation function, the Elliot Activation Function
    is constrained between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d6d284c49d6dfde0ab3cd3d7f19a2f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Elliot Activation Function (Image from Author)
  prefs: []
  type: TYPE_NORMAL
- en: Experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**PROBLEM:** Keras currently does not have the Elliot Activation Function in
    its repository.'
  prefs: []
  type: TYPE_NORMAL
- en: '**SOLUTION:** We can use the Keras backend and create it ourselves!'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For this experiment, let's see how the Elliot Activation Function compares to
    its similar counterparts as well as the ReLU Activation, a fundamental activation
    function used in neural networks today.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset and Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step of all Python projects is to import your packages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The dataset used today is the iris dataset, which can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data).
    This dataset is publicly available and is allowed for public use (there is an
    option to load it into Python through *sklearn*).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s create the four models. These will be fairly simple models. Each
    will have one layer of 8 neurons and an activation function. The final layer will
    have 3 neurons and use a Softmax Activation Function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Next, simply train the models and analyze the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The results were… surprising, actually. As expected, the Elliot Activation Function
    produced models with similar performances to those adopting the Sigmoid And Hyperbolic
    Tangent Activation Functions.
  prefs: []
  type: TYPE_NORMAL
- en: '**1 Epoch**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sigmoid @ 1*: Accuracy: 0.3109 | Loss: 2.0030'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 1*: Accuracy: 0.3361 | Loss: 1.0866'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 1 Epoch, the Elliot Activation Function model outperformed the Sigmoid Activation
    Function model with a 2.61% higher accuracy and an *almost 100% decrease in the
    amount of loss.*
  prefs: []
  type: TYPE_NORMAL
- en: '**10 Epochs**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sigmoid @ 10*: Accuracy: 0.3529 | Loss: 1.0932'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 10*: Accuracy: 0.6891 | Loss: 0.9434'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 10 epochs, the model with the Elliot Activation Function had an almost 30%
    higher accuracy with a lower loss compared to the model using the Sigmoid Activation
    Function.
  prefs: []
  type: TYPE_NORMAL
- en: '**100 Epochs**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sigmoid @ 100*: Accuracy: 0.9496 | Loss: 0.4596'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 100*: Accuracy: 0.9580 | Loss: 0.5485'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Sigmoid Model outperformed the Elliot Model, however, it should be noted
    that their performances were almost exactly the same.
  prefs: []
  type: TYPE_NORMAL
- en: '**1000 Epochs**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sigmoid @ 1000*: Accuracy: 0.9832 | Loss: 0.0584'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 1000*: Accuracy: 0.9832 | loss: 0.0433'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 1000 Epochs, the performances of the two different models were almost exactly
    the same.
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall, the model using the Elliot Activation Function performed slightly
    better than the model using the Sigmoid Activation Function.**'
  prefs: []
  type: TYPE_NORMAL
- en: Elliot versus Hyperbolic Tangent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**1 Epoch**'
  prefs: []
  type: TYPE_NORMAL
- en: '*tanh @ 1* : Accuracy: 0.3361 | loss: 1.1578'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 1*: Accuracy: 0.3361 | Loss: 1.0866'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 1 Epoch, the Elliot Activation Function Model performs the same as the Hyperbolic
    Tangent Activation Function Model. I expected these functions to produce similar
    performing models since each similarly constrains the values passed on to the
    next layer of a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '**10 Epochs**'
  prefs: []
  type: TYPE_NORMAL
- en: '*tanh @ 10*: Accuracy: 0.3277 | Loss: 0.9981'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 10*: Accuracy: 0.6891 | Loss: 0.9434'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model with the Elliot Activation Function greatly outperforms the model
    with the Hyperbolic Tangent Activation Function, just as the Elliot Model did
    at 10 Epochs when compared to the Sigmoid Model.
  prefs: []
  type: TYPE_NORMAL
- en: '**100 Epochs**'
  prefs: []
  type: TYPE_NORMAL
- en: '*tanh @ 100:* Accuracy: 0.9916 | Loss: 0.2325'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 100*: Accuracy: 0.9580 | Loss: 0.5485'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 100 epochs, the Hyperbolic Tangent model performs much better than the Elliot
    model. At higher epochs, the Elliot Activation Function seems to underperform
    compared to the *tanh* activation function but let’s see how their performances
    differ at 1000 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '**1000 Epochs**'
  prefs: []
  type: TYPE_NORMAL
- en: '*tanh @ 1000:* Accuracy: 0.9748 | Loss: 0.0495'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 1000*: Accuracy: 0.9832 | Loss: 0.0433'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well, at 1000 epochs, the Elliot Activation Function model slightly outperforms
    the Hyperbolic Tangent Activation Function model.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, I would say that the Hyperbolic Tangent and Elliot Activation Function
    Models work almost the same in the layers of a neural network. **There could be
    a difference in the time to train a model**, however, these models were very simple
    and time may become a bigger factor with the more data one has as well as the
    size of the network they are creating.
  prefs: []
  type: TYPE_NORMAL
- en: Elliot versus ReLU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**1 Epoch**'
  prefs: []
  type: TYPE_NORMAL
- en: '*ReLU @ 1:* Accuracy: 0.6639 | Loss: 1.0221'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 1*: Accuracy: 0.3361 | Loss: 1.0866'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 1 epoch, the model with the ReLU Activation Function does *much* better which
    outlines one observation that the Elliot Activation Function is causing the model
    to train slower.
  prefs: []
  type: TYPE_NORMAL
- en: '**10 Epochs**'
  prefs: []
  type: TYPE_NORMAL
- en: '*ReLU @ 10*: Accuracy: 0.6471 | Loss: 0.9413'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 10*: Accuracy: 0.6891 | Loss: 0.9434'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wow! The model containing the Elliot activation actually performed *better*
    than the model with the ReLU Activation Function with a 4.2% higher accuracy and
    0.21% lower loss.
  prefs: []
  type: TYPE_NORMAL
- en: '**100 Epochs**'
  prefs: []
  type: TYPE_NORMAL
- en: '*ReLU @ 100* : Accuracy: 0.9160 | Loss: 0.4749'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 100*: Accuracy: 0.9580 | Loss: 0.5485'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though the model which adopted the Elliot Activation Function had a higher
    loss, it was able to achieve higher accuracy by 4.2%. *Again*, this shows the
    strength of the Elliot Activation Function when placed within neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**1000 Epochs**'
  prefs: []
  type: TYPE_NORMAL
- en: '*ReLU @ 1000*: Accuracy: 0.9916 | Loss: 0.0494'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elliot @ 1000*: Accuracy: 0.9832 | Loss: 0.0433'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the model with the Elliot Activation Function did not do better in terms
    of accuracy, the loss was lower and I was still happy with the results. As shown
    at 1000 epochs, the Elliot Activation Function was almost just as good as the
    ReLU Activation Function and with the right problem and hyperparameter tuning,
    the Elliot Activation Function could the more optimal choice.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Today, we looked at a lesser-known activation function: **The Elliot Activation**.
    To test its performance, it was compared against two activation functions that
    are similar in their shape: the Sigmoid and Hyperbolic Tangent Activation Functions.
    That trial resulted in the Elliot Function performing the same if not better than
    using either of those two functions within the body of a neural network. Next,
    we compared the performance of an Elliot Activation Function to the standard ReLU
    Activation Function used in Neural Networks today. Of the 4 trials, the model
    adopting the Elliot Activation Function performed better 50% of the time. For
    the other trials it underperformed, its performance was still almost exactly the
    same as the model which was deployed with the ReLU Activation Function. I recommend
    trying the Elliot Activation Function in your next Neural Network because there
    is a chance it may perform better!'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you enjoyed today’s reading, PLEASE give me a follow and let me know if
    there is another topic you would like me to explore! If you do not have a Medium
    account, sign up through my link** [**here**](https://ben-mccloskey20.medium.com/membership)**!
    I will receive a small commission when you use my link. Additionally, add me on**
    [**LinkedIn**](https://www.linkedin.com/in/benjamin-mccloskey-169975a8/), **or
    feel free to reach out! Thanks for reading!**'
  prefs: []
  type: TYPE_NORMAL
- en: Dubey, Shiv Ram, Satish Kumar Singh, and Bidyut Baran Chaudhuri. “A comprehensive
    survey and performance analysis of activation functions in deep learning.” *arXiv
    preprint arXiv:2109.14545* (2021).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sharma, Sagar, Simone Sharma, and Anidhya Athaiya. “Activation functions in
    neural networks.” *towards data science* 6.12 (2017): 310–316.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.gallamine.com/2013/01/a-sigmoid-function-without-exponential_31.html](https://www.gallamine.com/2013/01/a-sigmoid-function-without-exponential_31.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
