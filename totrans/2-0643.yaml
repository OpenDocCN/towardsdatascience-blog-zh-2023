- en: Data Integration Strategies for Time Series Databases
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列数据库的数据集成策略
- en: 原文：[https://towardsdatascience.com/data-integration-strategies-for-time-series-databases-f96cab274820](https://towardsdatascience.com/data-integration-strategies-for-time-series-databases-f96cab274820)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-integration-strategies-for-time-series-databases-f96cab274820](https://towardsdatascience.com/data-integration-strategies-for-time-series-databases-f96cab274820)
- en: Exploring popular data integration strategies for TSDBs including ETL, ELT,
    and CDC
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探讨用于时间序列数据库（TSDB）的流行数据集成策略，包括ETL、ELT和CDC
- en: '[](https://yitaek.medium.com/?source=post_page-----f96cab274820--------------------------------)[![Yitaek
    Hwang](../Images/8e85bfb443a1df284607fb5e5fb2afa5.png)](https://yitaek.medium.com/?source=post_page-----f96cab274820--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f96cab274820--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f96cab274820--------------------------------)
    [Yitaek Hwang](https://yitaek.medium.com/?source=post_page-----f96cab274820--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://yitaek.medium.com/?source=post_page-----f96cab274820--------------------------------)[![Yitaek
    Hwang](../Images/8e85bfb443a1df284607fb5e5fb2afa5.png)](https://yitaek.medium.com/?source=post_page-----f96cab274820--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f96cab274820--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f96cab274820--------------------------------)
    [Yitaek Hwang](https://yitaek.medium.com/?source=post_page-----f96cab274820--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f96cab274820--------------------------------)
    ·6 min read·Jan 31, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----f96cab274820--------------------------------)
    ·6分钟阅读·2023年1月31日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/58b09c28f46fa6ad8943b13548433aec.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58b09c28f46fa6ad8943b13548433aec.png)'
- en: Photo by [fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)
    于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: As digital transformation reaches more industries, the number of data points
    generated is growing exponentially. As such, data integration strategies to collect
    such large volumes of data from different sources in varying formats and structures
    are now a primary concern for data engineering teams. Traditional approaches to
    data integration, which have largely focused on curating highly structured data
    into data warehouses, struggle to deal with the volume and heterogeneity of new
    data sets.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数字化转型扩展到更多行业，生成的数据点数量正呈指数增长。因此，如何从不同来源以各种格式和结构收集如此大量的数据成为数据工程团队的主要关注点。传统的数据集成方法主要集中在将高度结构化的数据整理到数据仓库中，这些方法难以应对新数据集的体量和异质性。
- en: Time series data present an additional layer of complexity. By nature, the value
    of each time series data point diminishes over time as the granularity of the
    data loses relevance as it gets stale. So it is crucial for teams to carefully
    plan data integration strategies into time series databases (TSDBs) to ensure
    that the analysis reflects the trends and situation in near real-time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据增加了额外的复杂性。由于时间的推移，每个时间序列数据点的价值会减少，因为数据的粒度在数据陈旧时会失去相关性。因此，团队必须仔细规划数据集成策略到时间序列数据库（TSDB）中，以确保分析能够反映接近实时的趋势和情况。
- en: 'In this article, we’ll examine some of the most popular approaches to data
    integration for TSDBs:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将审视一些最流行的时间序列数据库数据集成方法：
- en: '**ETL** (Extract, Transform, Load)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ETL**（提取、转换、加载）'
- en: '**ELT** (Extract, Load, Transform)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提取、加载、转换**（ELT）'
- en: '**Data Streaming with CDC** (Change Data Capture)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据流处理与CDC**（变更数据捕捉）'
- en: Given the need for real-time insights for time series data, most modern event-driven
    architectures now implement data streaming with CDC. To illustrate how it works
    in practice, we will walk through a reference implementation with [QuestDB](https://questdb.io/)
    (a fast TSDB) to show that CDC can flexibly handle the needs of a time series
    data source.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于时间序列数据对实时洞察的需求，现代事件驱动架构通常采用CDC进行数据流处理。为了展示其实际操作，我们将通过[QuestDB](https://questdb.io/)（一个快速的TSDB）来展示CDC如何灵活地满足时间序列数据源的需求。
- en: Extract, Transform, Load (ETL)
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取、转换、加载（ETL）
- en: ETL is a traditional and popular data integration strategy that involves first
    transforming the data into a predetermined structure, before loading the data
    into the target system (typically a data warehouse).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**ETL** 是一种传统且流行的数据集成策略，它首先将数据转换成预定结构，然后再将数据加载到目标系统（通常是数据仓库）中。'
- en: '![](../Images/c7d5a31d9b7a8cd6137b9e2c0d17d0f7.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7d5a31d9b7a8cd6137b9e2c0d17d0f7.png)'
- en: Image by the author
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: One of the main advantages of ETL is that it provides the highest degree of
    customization. Since the data is first extracted to a staging area where it is
    transformed into a clean, standardized format, ETL systems can handle a wide range
    of formats and structures. Also, once the data is loaded into the data warehouse,
    data science teams can run efficient queries and analyses. Finally, given the
    maturity of the ETL ecosystem, there is a plethora of enterprise-grade tools to
    choose from.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**ETL** 的主要优势之一是它提供了最高程度的自定义。由于数据首先被提取到一个临时区域，在那里被转换为干净、标准化的格式，**ETL** 系统可以处理各种格式和结构。此外，一旦数据加载到数据仓库中，数据科学团队可以运行高效的查询和分析。最后，鉴于
    **ETL** 生态系统的成熟，选择企业级工具的选项非常丰富。'
- en: On the other hand, ETL is both time- and resource-intensive to maintain. The
    logic for sanitizing and transforming the data can be complex and computationally
    expensive. This is why most ETL systems are typically batch-oriented, only loading
    the data into the warehouse periodically. As the volume of data and the sources
    of data grows, this can become a bottleneck.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**ETL** 维护起来既耗时又费资源。数据清理和转换的逻辑可能复杂且计算开销大。这也是为什么大多数 **ETL** 系统通常是批处理导向的，只在周期性地将数据加载到仓库中。随着数据量和数据源的增加，这可能会成为瓶颈。
- en: Given these qualities, ETL systems are most used for datasets that require complex
    transformation logic before analysis. It can also work well for datasets that
    do not require real-time insights and can be stored for long-term trend analysis.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些特性，**ETL** 系统通常用于需要在分析之前进行复杂转换逻辑的数据集。它也适合那些不需要实时洞察并可以长期存储用于趋势分析的数据集。
- en: Extract, Load, Transform (ELT)
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取、加载、转换（**ELT**）
- en: ELT, as the name suggests, loads the data first into the target system (typically
    a data lake) and performs the transformation within the system itself. Given the
    responsibilities of the target system to handle both fast loads and transformations,
    ELT pipelines usually leverage modern, cloud-based data lakes that can deal with
    the processing requirements
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名称所示，**ELT** 首先将数据加载到目标系统（通常是数据湖）中，并在系统内部进行转换。由于目标系统负责处理快速加载和转换，**ELT** 流水线通常利用现代的、基于云的数据湖，以应对处理需求。
- en: '![](../Images/d4a554c0daf7d71f8254d42f4ced5db5.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4a554c0daf7d71f8254d42f4ced5db5.png)'
- en: Image by the author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Compared to ETL pipelines, ELT systems can provide more real-time analysis of
    the data since raw data is ingested and transformed on the fly. Most cloud-based
    data lakes provide SDKs or endpoints to efficiently ingest data in micro-batches
    and provide almost limitless scalability. However, ELT is not without downsides.
    Since transformation is done by the target system, such operations are limited
    by the capabilities supported by the data lakes. If you need a more complex transformation
    logic, additional steps may be needed to re-extract data and store it in a more
    friendly format.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与 **ETL** 流水线相比，**ELT** 系统可以提供更多实时的数据分析，因为原始数据会被实时摄取和转换。大多数基于云的数据湖提供 SDK 或端点，以高效摄取微批量数据并提供几乎无限的可扩展性。然而，**ELT**
    并非没有缺点。由于转换由目标系统完成，这些操作受限于数据湖支持的功能。如果需要更复杂的转换逻辑，可能需要额外的步骤来重新提取数据并以更友好的格式存储。
- en: For most use cases, ELT is a more efficient data integration strategy than ETL.
    If your application can leverage cloud-based tools and does not require complex
    processing, ELT can be a great choice to handle large amounts of data in near
    real-time.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数使用场景而言，**ELT** 是比 **ETL** 更高效的数据集成策略。如果你的应用程序可以利用基于云的工具并且不需要复杂的处理，**ELT**
    可以是处理大量数据的极佳选择，接近实时完成处理。
- en: Change Data Capture (CDC)
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更改数据捕获（**CDC**）
- en: For new projects, teams can plan to utilize TSDBs as one of the target systems
    in an ETL or ELT pipeline. However, for existing projects, either migrating to
    or adding a TSDB into the mix can be a challenge. For one, work may be required
    to either modify or use a new driver/SDK to stream data into the TSDB. Even if
    the same drivers are supported, data formats may also need to change to fully
    take advantage of TSDB capabilities. CDC tools can be useful to bridge this gap.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新项目，团队可以计划将 TSDB 作为 ETL 或 ELT 管道中的目标系统之一。然而，对于现有项目，无论是迁移到 TSDB 还是将 TSDB 添加到系统中都可能面临挑战。一方面，可能需要修改或使用新的驱动程序/SDK
    来将数据流式传输到 TSDB。即使支持相同的驱动程序，数据格式也可能需要更改以充分利用 TSDB 的功能。CDC 工具可以有效地弥合这一差距。
- en: 'CDC is simple in principle: CDC tools such as Debezium continuously monitor
    changes in the source system and notify your data pipeline whenever there is a
    change. The application causing the change is often not even aware there is a
    CDC process listening on changes. This makes CDC a good fit for integrating new
    real-time data pipelines into existing architectures because it requires small
    or no changes in the existing applications. As such, CDC can be used in conjunction
    with either ETL or ELT pipelines. For example, the source system can be a SQL
    RDBMS (e.g., MySQL, PostgreSQL, etc) or NoSQL DB (e.g., MongoDB, Casandra) and
    one of the target systems can be a TSDB along with other data lakes or warehouses.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: CDC 在原则上很简单：CDC 工具如 Debezium 持续监控源系统中的变化，并在发生变化时通知你的数据管道。引起变化的应用程序通常甚至不知道有 CDC
    进程在监听变化。这使得 CDC 非常适合将新的实时数据管道集成到现有架构中，因为它对现有应用程序的更改很少或没有。因此，CDC 可以与 ETL 或 ELT
    管道一起使用。例如，源系统可以是 SQL RDBMS（例如 MySQL、PostgreSQL 等）或 NoSQL 数据库（例如 MongoDB、Cassandra），而目标系统之一可以是
    TSDB 以及其他数据湖或数据仓库。
- en: '![](../Images/fd1e89ed545b347b5ace7779fc175dc8.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd1e89ed545b347b5ace7779fc175dc8.png)'
- en: Image by the author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: The main advantage of using CDC for data integration is that it provides real-time
    data replication. Unlike traditional ETL and ELT systems that work with batches,
    the changes to the source system are continuously streamed into one or more target
    systems. This can be useful to replicate either a subset or the entire data across
    multiple databases in near real-time. The target databases may even be in different
    geographical regions or serve different purposes (i.e., long-term storage vs.
    real-time analytics). For time series data where the changes in value over time
    are often most useful, CDC efficiently captures that delta for real-time insights.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CDC（Change Data Capture）进行数据集成的主要优势在于它提供了实时数据复制。与传统的批量处理 ETL 和 ELT 系统不同，CDC
    会将源系统的变化持续流式传输到一个或多个目标系统。这对于在多个数据库之间近实时地复制部分数据或全部数据非常有用。目标数据库甚至可能位于不同的地理区域或服务于不同的目的（即长期存储与实时分析）。对于时间序列数据，随着时间变化的数值通常最为有用，CDC
    能有效捕捉这些增量数据，以便提供实时洞察。
- en: Reference Implementation for CDC
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CDC 的参考实现
- en: To illustrate how CDC works more concretely, let’s take the [reference implementation](https://github.com/questdb/kafka-questdb-connector)
    I wrote about recently to stream stock prices into QuestDB.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更具体地说明 CDC 如何工作，我们可以看一下我最近写的[参考实现](https://github.com/questdb/kafka-questdb-connector)，它将股票价格流式传输到
    QuestDB。
- en: '![](../Images/2818fe615a4f7ec0ffd9fba5adeaebec.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2818fe615a4f7ec0ffd9fba5adeaebec.png)'
- en: Image by the author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: At a high level, a Java Spring App publishes stock price information into PostgreSQL.
    Debezium Connector then reads the changes from PostgreSQL and publishes them onto
    Kafka. On the other side, QuestDB’s Kafka Connector reads from the Kafka topics
    and streams them onto QuestDB.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，一个 Java Spring 应用将股票价格信息发布到 PostgreSQL。然后 Debezium Connector 从 PostgreSQL
    中读取这些变化，并将其发布到 Kafka。另一方面，QuestDB 的 Kafka Connector 从 Kafka 主题中读取数据，并将其流式传输到 QuestDB。
- en: 'For a deeper dive, please refer to:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解，请参阅：
- en: '[](https://questdb.io/blog/2023/01/03/change-data-capture-with-questdb-and-debezium?source=post_page-----f96cab274820--------------------------------)
    [## Change Data Capture with QuestDB and Debezium | QuestDB: the database for
    time series'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://questdb.io/blog/2023/01/03/change-data-capture-with-questdb-and-debezium?source=post_page-----f96cab274820--------------------------------)
    [## 使用 QuestDB 和 Debezium 进行变更数据捕获 | QuestDB：时间序列数据库]'
- en: In this tutorial, our community contributor, Yitaek Hwang, shows us how to stream
    data into QuestDB with change data…
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在本教程中，我们的社区贡献者 Yitaek Hwang 向我们展示了如何使用变更数据流将数据传输到 QuestDB。
- en: questdb.io](https://questdb.io/blog/2023/01/03/change-data-capture-with-questdb-and-debezium?source=post_page-----f96cab274820--------------------------------)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[questdb.io](https://questdb.io/blog/2023/01/03/change-data-capture-with-questdb-and-debezium?source=post_page-----f96cab274820--------------------------------)'
- en: In this reference architecture, the Java Spring App is transforming and loading
    the data onto PostgreSQL before it’s replicated to TSDB for further analysis.
    If a more ELT-like pipeline is desired, raw data from another provider could have
    been loaded directly onto PostgreSQL and transformed later in QuestDB as well.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个参考架构中，Java Spring应用程序在将数据复制到TSDB进行进一步分析之前，将数据转换并加载到PostgreSQL中。如果需要更类似ELT的管道，可以将来自其他提供商的原始数据直接加载到PostgreSQL中，并在QuestDB中进行转换。
- en: The important thing to note with this architecture is that CDC can seamlessly
    integrate with existing systems. From the application standpoint, it can retain
    the transactional guarantees of PostgreSQL, while adding a new TSDB component
    down the pipeline.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个架构中需要注意的关键点是，CDC可以无缝集成到现有系统中。从应用程序的角度来看，它可以保留PostgreSQL的事务保证，同时在管道中添加一个新的TSDB组件。
- en: Conclusion
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Data integration plays an important role for organizations that use a TSDB to
    store and analyze time series data. In this article, we looked at some advantages
    and disadvantages of using ETL or ELT. We also examined how CDC can be used in
    conjunction with those pipelines to provide real-time replication into TSDBs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集成在使用TSDB来存储和分析时间序列数据的组织中扮演着重要角色。在本文中，我们探讨了使用ETL或ELT的一些优缺点。我们还研究了如何将CDC与这些管道结合使用，以实现对TSDB的实时复制。
- en: Given the special qualities of time series data, using a TSDB to properly store
    and analyze them is important. If you are starting fresh, look to build an ELT
    pipeline to stream data into a TSDB. To integrate with an existing system, look
    at utilizing CDC tools to limit the disruption to the current architecture.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到时间序列数据的特殊性质，使用TSDB来正确存储和分析这些数据非常重要。如果你是从零开始，考虑建立一个ELT管道，将数据流入TSDB。要与现有系统集成，可以利用CDC工具以减少对当前架构的干扰。
