- en: 'Machine Learning, Illustrated: Evaluation Metrics for Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/machine-learning-illustrated-classification-evaluation-metrics-dfc33b373c43](https://towardsdatascience.com/machine-learning-illustrated-classification-evaluation-metrics-dfc33b373c43)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A comprehensive (and colorful) guide to everything you need to know about evaluating
    classification models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shreya.rao?source=post_page-----dfc33b373c43--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page-----dfc33b373c43--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dfc33b373c43--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dfc33b373c43--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----dfc33b373c43--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dfc33b373c43--------------------------------)
    ·12 min read·Apr 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'I realized through my learning journey that I’m an incredibly visual learner
    and I appreciate the use of color and fun illustrations to learn new concepts,
    especially scientific ones that are typically explained like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e5178e7df4a16c4cbf60390074b5bcc.png)'
  prefs: []
  type: TYPE_IMG
- en: From my previous articles, through tons of lovely comments and messages (thank
    you for all the support!), I found that several people resonated with this sentiment.
    So I decided to start a new series where I’m going to attempt to illustrate machine
    learning and computer science concepts to hopefully make learning them fun. So,
    buckle up and enjoy the ride!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin this series by exploring a fundamental question in machine learning:
    **how do we evaluate the performance of classification models**?'
  prefs: []
  type: TYPE_NORMAL
- en: 'In previous articles such as [Decision Tree Classification](https://medium.com/towards-artificial-intelligence/decision-tree-classification-explain-it-to-me-like-im-10-59a53c0b338f)
    and [Logistic Regression](https://medium.com/towards-data-science/back-to-basics-part-tres-logistic-regression-e309de76bd66),
    we discussed how to build classification models. However, it’s crucial to quantify
    how well these models are performing, which begs the question: what metrics should
    we use to do so?'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this concept, let’s build a **loan repayment classification model**.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to predict whether an individual is likely to repay their loan based
    on their credit score. While other variables like age, salary, loan amount, loan
    type, occupation, and credit history may also factor into such a classifier, for
    the sake of simplicity, we’ll only consider credit score as the primary determinant
    in our model.
  prefs: []
  type: TYPE_NORMAL
- en: Following the steps laid out in the [Logistic Regression](https://medium.com/towards-data-science/back-to-basics-part-tres-logistic-regression-e309de76bd66)
    article, we build a classifier that predicts the probability that someone will
    repay the loan based on their credit score.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed28a0b38f1d241ca277fa3865d2fe78.png)'
  prefs: []
  type: TYPE_IMG
- en: From this, we see that the lower the credit score, the more likely it is that
    the person is not going to repay their loan and vice-versa.
  prefs: []
  type: TYPE_NORMAL
- en: Right now, the output of this model is the **probability** that a person is
    going to repay their loan. However, if we want to classify the loan as *going
    to repay* or *not going to repay*, then we need to find a way to turn these probabilities
    into a classification.
  prefs: []
  type: TYPE_NORMAL
- en: One way to do that is to set a threshold of 0.5 and classify any people below
    that threshold as *not going to repay* and any above it as *going to repay.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de1572c26b231957cede19ce6b788bd6.png)'
  prefs: []
  type: TYPE_IMG
- en: From this, we deduce that this model will classify anyone with a credit score
    below 600 as *not going to repay* (pink) and above 600 as *going to repay* (blue).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4770ed8570d10571800ae847f908c6ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Using 0.5 as a cutoff, we classify this person with a credit score of 420 as…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a023359f3f9f5f9fa5ea3e03f75a051e.png)'
  prefs: []
  type: TYPE_IMG
- en: …*not going to repay*. And this person with a credit score of 700 as...
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23bef7ee38242f4273bac32a04ea6e78.png)'
  prefs: []
  type: TYPE_IMG
- en: '*…going to repay*.'
  prefs: []
  type: TYPE_NORMAL
- en: Now to ***test*** out how effective our model is, we need way more than 2 people.
    So let’s dig through past records and collect information about 10,000 people’s
    credit scores and if they repaid or did not repay their loans.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b444ea724520e5dce6bd7935395de26c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'NOTE: In our records, we have 9500 people that repaid their loan and only 500
    that didn’t.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We then run our classifier on each person and based on their credit scores we
    predict if the person is going to repay their loan or not.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To better visualize how our predictions compared with the truth, we create something
    called a confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30d416e9cdbfd292afc254f1dc8f0b5e.png)'
  prefs: []
  type: TYPE_IMG
- en: In this specific confusion matrix, we consider an individual who repaid their
    loan as a **Positive** label and an individual who did not repay their loan as
    a **Negative** label.
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positives (TP)**: People that actually repaid their loan and were ***correctly***
    classified by the model as *going to repay*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negatives (FP)**: People that actually repaid their loan, but were
    ***incorrectly*** classified by the model as *not going to repay*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negatives (TN)**: People that in reality didn’t repay their loans and
    were ***correctly*** classified by the model as *not going to repay*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positives (FP)**: People that in reality didn’t repay their loans,
    but were ***incorrectly*** classified by the model as *going to repay*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now imagine, we passed information about the 10,000 people through our model.
    We end up with a confusion matrix that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29dbb8156c5439564198fafbe34a931d.png)'
  prefs: []
  type: TYPE_IMG
- en: From this, we can deduce that —
  prefs: []
  type: TYPE_NORMAL
- en: Out of 9500 people that repaid their loan — 9000 were correctly classified (TP)
    and 500 were incorrectly classified (FN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of the 500 people that didn't repay their loan — 200 (TN) were correctly
    classified and 300 (FP) were incorrectly classified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Intuitively, the first thing we ask ourselves is: **how accurate is our model?**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8e39c5c68699c535163b20623cad996a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In our case, accuracy is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cd8bbf988c24d216c6a1876089306896.png)'
  prefs: []
  type: TYPE_IMG
- en: 92% accuracy is certainly impressive, but it’s important to note that accuracy
    is often a simplistic metric for evaluating model performance.
  prefs: []
  type: TYPE_NORMAL
- en: If we take a closer look at the confusion matrix, we can see that while many
    individuals who repaid their loans were correctly classified, only 200 out of
    the 500 individuals who did not repay were correctly labeled by the model, with
    the other 300 being incorrectly classified.
  prefs: []
  type: TYPE_NORMAL
- en: So let’s explore some other commonly used metrics that we can use to assess
    the performance of our model.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another question we can ask is: **what percentage of individuals predicted
    as *going to repay* actually repaid their loans**?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d8d9475cccf3d6943afe61447496a4a7.png)'
  prefs: []
  type: TYPE_IMG
- en: To calculate precision, we can divide the True Positives by the total number
    of predicted Positives (i.e., individuals classified as *going to repay*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dcbe14cb2928f07efca5126e4c634a0b.png)'
  prefs: []
  type: TYPE_IMG
- en: So when our classifier predicts that a person is *going to repay*, our classifier
    is right 96.8% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity (aka Recall)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, can ask ourselves: **what % of individuals who actually repaid their
    loan does our model correctly identify as *going to repay***?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46172305710ec90061414dc22ef100c0.png)'
  prefs: []
  type: TYPE_IMG
- en: To compute sensitivity, we can take the True Positives and divide them by the
    total number of individuals who actually repaid their loans.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5cbafb4937027fdcbb82636f48629e9c.png)'
  prefs: []
  type: TYPE_IMG
- en: The classier correctly labeled 94.7% of people that actually repaid their loan
    and the rest it incorrectly labeled *not going to repay*.
  prefs: []
  type: TYPE_NORMAL
- en: '*NOTE: The terms used in Precision and Sensitivity formulas can be confusing
    at times. One simple mnemonic to differentiate between the two is to remember
    that both formulas use TP (True Positive), but the denominators differ. Precision
    has (TP + FP) in the denominator, while Sensitivity has (TP + FN).*'
  prefs: []
  type: TYPE_NORMAL
- en: '*To remember this difference, think of the “P” in FP matching the “P” in Precision:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/937833611ff903d0ed8507cb3dbbee95.png)'
  prefs: []
  type: TYPE_IMG
- en: '*and that leaves FN, which we find in the denominator of Sensitivity:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/727de77c429d136b800f3d8a3947201e.png)'
  prefs: []
  type: TYPE_IMG
- en: F1 Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another useful metric that combines sensitivity and precision is the F1 score,
    which calculates the harmonic mean of precision and sensitivity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7fa1e663f7d6e69639c6ad9f1599b8c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'F1 Score in our case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b558556c3e09dec686dae4d75984eee0.png)'
  prefs: []
  type: TYPE_IMG
- en: Usually the F1 score provides a more comprehensive evaluation of model performance.
    Thus, the F1 score is typically a more useful metric than accuracy in practice.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Specificity**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another critical question to consider is specificity, which asks the question:
    **what % of individuals who did not repay their loans were correctly identified
    as *not going to repay***?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/989870a85d98621c22579406acd29712.png)'
  prefs: []
  type: TYPE_IMG
- en: To calculate specificity, we divide the True Negatives by the total number of
    individuals who didn’t repay their loans.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f53ce590c057ee023d5ff559967015eb.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that our classifier only correctly identifies 40% of individuals
    who didn’t repay their loans.
  prefs: []
  type: TYPE_NORMAL
- en: This stark difference between specificity and the other evaluation metrics emphasizes
    the significance of selecting the appropriate metrics to assess model performance.
    It is crucial to consider all evaluation metrics and interpret them appropriately,
    as each may provide a distinct perspective on the model’s effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: I often find it helpful to combine various metrics or devise my own metric
    based on the problem at hand'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In our scenario, accurately identifying individuals who will not repay their
    loans is more critical, as providing loans to such individuals can incur significant
    costs compared to rejecting loans for those who will repay. So we need to think
    about ways to improve its performance to do that.
  prefs: []
  type: TYPE_NORMAL
- en: One way to achieve this is by **adjusting the threshold value for classification**.
  prefs: []
  type: TYPE_NORMAL
- en: Although doing so may seem counterintuitive, what is important to us is to correctly
    identify the individuals who are not going to repay their loans. Thus, incorrectly
    labeling people who are actually going to repay is not as essential to us.
  prefs: []
  type: TYPE_NORMAL
- en: By adjusting the threshold value, we can make our model more sensitive to the
    Negative class (people who aren’t going to repay) at the expense of the Positive
    class (people who are going to repay). This may increase the number of False Negatives
    (classifying people who repaid as *not going to repay*), but can potentially reduce
    False Positives (failing to correctly identify people who didn’t repay).
  prefs: []
  type: TYPE_NORMAL
- en: Until now we used a threshold of 0.5, but let’s try changing it around to see
    if our model performs better.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by setting the threshold to 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3bf59be592368a83977f96ebfd7c6f8e.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that every person will be classified as *going to repay* (represented
    by blue)*:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c81e3645f5594a12a64e392bbfd86f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This will result in this confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47e9201e1431b029ed4185f8c46e7446.png)'
  prefs: []
  type: TYPE_IMG
- en: everyone is classified as *going to repay*
  prefs: []
  type: TYPE_NORMAL
- en: '…with accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d76d4486b3f46136473dfcea4e88559.png)'
  prefs: []
  type: TYPE_IMG
- en: '…sensitivity and precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f7742b1c6283bcedd027e882af15ba3.png)'
  prefs: []
  type: TYPE_IMG
- en: '…and specificity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5ed1376b43a05d55d504b7687a1e08d.png)'
  prefs: []
  type: TYPE_IMG
- en: At threshold = 0, our classifier is not able to correctly classify any of the
    individuals who didn’t repay their loans, rendering it ineffective even though
    the accuracy and sensitivity may seem impressive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try a threshold of 0.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e814234954301ddec65e1058ed7e90db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So any person with a credit score of below 420 will be classified as *not going
    to repay*. This results in this confusion matrix and metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f69d13f7e99fdfd41c5ad290fbd3c11.png)![](../Images/2b098f950c0c25bcd237cf09216cb153.png)'
  prefs: []
  type: TYPE_IMG
- en: Again we see that all the metrics except specificity are pretty great.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s go to the other extreme and set the threshold to 0.9:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a9c8890155630285fd6bf61e99b8209.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So any person below a credit score of 760 is going to be labeled *not going
    to repay*. This will result in this confusion matrix and metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/187f1986a07b8059a5720978f27cdda1.png)![](../Images/9ac5dc8d336443c991f2a54886a1946f.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we see the metrics are almost flipped. The specificity and precision are
    great, but the accuracy and sensitivity are terrible.
  prefs: []
  type: TYPE_NORMAL
- en: You get the idea. We can do this for a bunch more thresholds (0.004, 0.3, 0.6,
    0.875…). But doing so will result in a staggering number of confusion matrices
    and metrics. And this will cause a lot of confusion. *Pun definitely intended.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/529b0720b99251e39a534191dca2b7b2.png)'
  prefs: []
  type: TYPE_IMG
- en: ROC Curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is where Receiver Operating Characteristics (ROC) curve comes in to dispel
    this confusion.
  prefs: []
  type: TYPE_NORMAL
- en: '**The ROC curve summarizes and allows us to visualize the classifier’s performance
    over all possible thresholds**.'
  prefs: []
  type: TYPE_NORMAL
- en: The y-axis of the curve is the True Positive Rate, which is the same as Sensitivity.
    And the x-axis is the False Positive Rate, which is 1-Specificity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27204dad4436b333d3ab4920d01cbf29.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The False Positive Rate tells us the proportion of people that didn''t repay
    that were incorrectly classified as* going to repay *(FP).*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'So when threshold = 0, from earlier we saw that our confusion matrix and metrics
    were:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95296a4e3cef47809c77112a7067c46d.png)'
  prefs: []
  type: TYPE_IMG
- en: We know that the and the `True Positive Rate = Sensitivity = 1` and the `False
    Positive Rate = 1 — Specificty = 1 — 0 = 1.`
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s plot this information on the ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3ef24c3bf5f88b4006d4a04eb521f84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This dotted blue line shows us where the True Positive Rate = False Positive
    Rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9029ba017ea0c59ba5c2799a5dd70f9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Any point on this line means that the proportion of correctly classified people
    that repaid is the same as the proportion of incorrectly classified people that
    didn’t repay.
  prefs: []
  type: TYPE_NORMAL
- en: The key is that we want our threshold point to be as far away from the line
    to the left as possible and we don't want any point below this line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now when threshold = 0.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9475dcbfad7e9a03cd8c0964d892e4f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plotting this threshold on the ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68c6836511dad7f44d885ecf4e1c2ec1.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the new point (0.84, 0.989) is to the left of the blue dotted line, we
    know that the proportion of correctly classified people that repaid is greater
    than the proportion of incorrectly classified people that didn’t repay.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the new threshold is better than the first one on the blue dotted
    line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s increase the threshold to 0.2\. We calculate the True Positive Rate
    and False Positive Rate for this threshold and plot it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66675b3868dd0bd4ce8a2f59886c0c77.png)'
  prefs: []
  type: TYPE_IMG
- en: The new point (0.75, 0.98) is even further to the left of the dotted blue line,
    showing that the new threshold is better than the previous one.
  prefs: []
  type: TYPE_NORMAL
- en: And now we keep repeating the same process with a couple of other thresholds
    (=0.35, 0.5, 0.65, 0.7, 0.8, 1) until threshold = 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14e9614576f107f4cf0643ccd8c0a930.png)'
  prefs: []
  type: TYPE_IMG
- en: At threshold = 1, we are at the point (0, 0) where True Positive Rate = False
    Negative Rate = 0 since the classifier classifies all the points as *not going
    to repay.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Now without having to sort through all the confusing matrices and metrics,
    I can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ccdfa1f8a6d1d4d55c30147dda3b38cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Because at the purple point when TPR = 0.8 and FPR = 0,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f5c80b0ae3d95bbdb0dd58a703d5ab0.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, this threshold resulted in no False Positives. Whereas at the
    blue point, although 80% of the people that repaid are correctly classified only
    80% of the people that did not repay are correctly classified (as opposed to 100%
    for the previous threshold).
  prefs: []
  type: TYPE_NORMAL
- en: Now if we connect all these dots…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e216586da142a83faccc432382930370.png)'
  prefs: []
  type: TYPE_IMG
- en: …we end up with the ROC curve.
  prefs: []
  type: TYPE_NORMAL
- en: AUC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let’s say we want to compare two different classifiers that we build. For
    instance, the first classifier is the logistic regression one we were looking
    at so far which resulted in this ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d12036c4f587bce57968c85b86473764.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And we decided to build another decision tree classifier that resulted in this
    ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5924cf719379910fd65a10d887f99779.png)'
  prefs: []
  type: TYPE_IMG
- en: Then a way to compare both classifiers is to calculate the areas under their
    respective curves or AUCs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/931cb5587b60b3838124588494d03bb3.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the AUC of the logistic regression curve is greater, we conclude that
    it is a better classifier.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we discussed commonly used metrics to evaluate classification models.
    However, it is important to note that the selection of metrics is subjective and
    depends on understanding the problem at hand and the business requirements. It
    may also be useful to use a combination of these metrics or even create new ones
    that are more appropriate for the specific model’s needs.
  prefs: []
  type: TYPE_NORMAL
- en: Massive shoutout to StatQuest, my favorite statistics and machine learning resource.
    And please feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/shreyarao24)
    or shoot me an email at *shreya.statistics@gmail.com*.
  prefs: []
  type: TYPE_NORMAL
