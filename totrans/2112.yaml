- en: 'TimeGPT: The First Foundation Model for Time Series Forecasting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/timegpt-the-first-foundation-model-for-time-series-forecasting-bf0a75e63b3a](https://towardsdatascience.com/timegpt-the-first-foundation-model-for-time-series-forecasting-bf0a75e63b3a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explore the first generative pre-trained forecasting model and apply it in a
    project with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcopeixeiro?source=post_page-----bf0a75e63b3a--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----bf0a75e63b3a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bf0a75e63b3a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bf0a75e63b3a--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----bf0a75e63b3a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bf0a75e63b3a--------------------------------)
    ·12 min read·Oct 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69c006f4feeba3f9fe392aff3b2a35f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Boris Smokrovic](https://unsplash.com/@borisworkshop?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The field of time series forecasting is going through a very exciting period.
    In only the last three years, we have seen many important contributions, like
    [N-BEATS](https://medium.com/towards-data-science/the-easiest-way-to-forecast-time-series-using-n-beats-d778fcc2ba60),
    [N-HiTS](https://medium.com/towards-data-science/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5),
    [PatchTST](https://medium.com/towards-data-science/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc)
    and [TimesNet](https://medium.com/towards-data-science/timesnet-the-latest-advance-in-time-series-forecasting-745b69068c9c).
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, [large language models (LLMs)](https://medium.com/towards-data-science/catch-up-on-large-language-models-8daf784f46f8)
    have gained a lot of popularity lately, with applications like ChatGPT, as they
    can adapt to a wide variety of tasks without further training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Which leads to the question: can foundation models exist for time series like
    they exist for natural language processing? Is it possible that a large model
    pre-trained on massive amounts of time series data can then produce accurate predictions
    on unseen data?'
  prefs: []
  type: TYPE_NORMAL
- en: With [TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf), proposed by Azul Garza
    and Max Mergenthaler-Canseco, the authors adapt the techniques and architecture
    behind LLMs to the field of forecasting, successfully building the first time
    series foundation model capable of zero-shot inference.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we first explore the architecture behind TimeGPT and how the
    model was trained. Then, we apply it in a forecasting project to evaluate its
    performance against other state-of-the-art methods, like N-BEATS, N-HiTS and PatchTST.
  prefs: []
  type: TYPE_NORMAL
- en: For more details, make sure to read the [original paper](https://arxiv.org/pdf/2310.03589.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '***Learn the latest time series analysis techniques with my*** [***free time
    series cheat sheet***](https://www.datasciencewithmarco.com/pl/2147608294) ***in
    Python! Get the implementation of statistical and deep learning techniques, all
    in Python and TensorFlow!***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Explore TimeGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, TimeGPT is a first attempt at creating a foundation model
    for time series forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a18b137f0b737e2e4df634cbc8111b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of how TimeGPT was trained to make inference on unseen data. Image
    by Azul Garza and Max Mergenthaler-Canseco from [TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can see that the general idea behind TimeGPT is to
    train a model on massive amounts of data from different domains to then produce
    zero-shot inference on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this method relies on **transfer learning**, which is the capacity
    of a model to solve a new task using its knowledge gained during training.
  prefs: []
  type: TYPE_NORMAL
- en: Now, this only works if the model is large enough, and if it is trained on lots
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: Training TimeGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To that end, the authors trained TimeGPT on more than 100 billion data points
    all coming from open-source time series data. The dataset spans a wide array of
    domains, from finance, economics and weather, to web traffic, energy and sales.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the authors do not disclose the sources of public data used to curate
    100 billion data points.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This variety is critical for the success of a foundation model, as it can learn
    different temporal patterns and therefore generalize better.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can expect weather data to have a daily (hotter during the day
    than at night) and yearly seasonality, while car traffic data can have a daily
    seasonality (more cars on the road during the day than at night) and a weekly
    seasonality (more cars on the road during the week than on weekends).
  prefs: []
  type: TYPE_NORMAL
- en: To ensure the robustness and generalization capabilities of the model, preprocessing
    was kept to a minimum. In fact, only missing values were filled, and the rest
    was kept in its raw form. While the authors do not specify the method for data
    imputation, I suspect that some kind of interpolation technique was used, like
    linear, spline or moving average interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: The model was then trained over multiple days, during which hyperparameters
    and learning rates were optimized. While the authors do not disclose how many
    days and GPUs were required for training, we do know that the model is implemented
    in PyTorch, and it uses the Adam optimizer and a learning rate decay strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of TimeGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TimeGPT leverages the Transformer architecture with self-attention mechanism
    based on the seminal work of Google and the University of Toronto in 2017.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf13280001ad8725c402657bb8fe31eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Architecture of TimeGPT. The input series, along with exogenous variables, is
    fed to the encoder of the Transfomer, and the decoder then generates forecasts.
    Image by Azul Garza and Max Mergenthaler-Canseco from [TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can see that TimeGPT uses the full encoder-decoder
    Transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The inputs can consist of a window of historical data, as well as exogenous
    data, like punctual events or another series.
  prefs: []
  type: TYPE_NORMAL
- en: The inputs are fed to the encoder portion of the model. The attention mechanism
    inside the encoder then learns different properties from the inputs. This is then
    fed to the decoder, which uses the learned information to produce forecasts. Of
    course, the sequence of predictions ends when it reaches the length of the forecast
    horizon set by the user.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the authors have implemented conformal predictions
    in TimeGPT, allowing the model to estimate prediction intervals based on historic
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: The capabilities of TimeGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TimeGPT comes with a wide array of capabilities considering that it is a first
    attempt at building a foundation model for time series.
  prefs: []
  type: TYPE_NORMAL
- en: First, with TimeGPT being a pre-trained model, it means that we can generate
    predictions without training it on our data specifically. Still, it is possible
    to fine-tune the model to our data.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the model supports exogenous variables to forecast our target, and it
    can handle multivariate forecasting tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, with the use of conformal prediction, TimeGPT can estimate prediction
    intervals. This in turn allows the model to perform anomaly detection. Basically,
    if a data point falls outside of a 99% confidence interval, then the model labels
    it as an anomaly.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that all those tasks are possible with zero-shot inference, or
    with some fine-tuning, which is a radical shift in paradigm for the field of time
    series forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a more solid understanding of TimeGPT, how it works and how
    it was trained, let’s see the model in action.
  prefs: []
  type: TYPE_NORMAL
- en: Forecast with TimeGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now apply TimeGPT on a forecasting task and compare its performance to
    other models.
  prefs: []
  type: TYPE_NORMAL
- en: Note that at the time of writing this article, TimeGPT is only accessible by
    API, and it is in closed beta. I submitted a request and was granted free access
    to the model for two weeks. To get a token and access the model, you have to visit
    their [website](https://www.nixtla.io/).
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, the model was trained on 100 billion data points coming
    from publicly available data. Since the authors do not specify the actual datasets
    used, I think it is unreasonable to test the model on known benchmark datasets,
    like [ETT](https://paperswithcode.com/dataset/ett) or [weather](https://www.kaggle.com/datasets/mnassrib/jena-climate),
    as the model has likely seen this data during training.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, I compiled and open-sourced my own dataset for this article.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, I curated the daily views on my blog from January 1st 2020, to
    October 12th 2023\. I also added two exogenous variables: one to signal a day
    where a new article was published, and the other to flag a day where it is a holiday
    in the United States, as the majority of my audience lives there.'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is now publicly available on [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/data/medium_views_published_holidays.csv),
    and most importantly, we are sure that TimeGPT did not train on this data.
  prefs: []
  type: TYPE_NORMAL
- en: As always, you can access the full notebook on [GitHub](https://github.com/marcopeix/time-series-analysis/blob/master/TimeGPT.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Import libraries and read the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The natural first step is to import the libraries for this experiment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then, to access the TimeGPT model, we read the API key from a file. Note that
    I did not assign the API key to an environment variable, because the access was
    limited to two weeks only.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can read the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/dfd146f8cdfc8f777aac50ca4a8472e6.png)'
  prefs: []
  type: TYPE_IMG
- en: The first five rows of our dataset. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can see that the format of the dataset is the same
    as when we work with other open-source libraries from Nixtla.
  prefs: []
  type: TYPE_NORMAL
- en: We have a *unique_id* column to label different time series, but in our case,
    we only have one series.
  prefs: []
  type: TYPE_NORMAL
- en: The column *y* represents the daily views on my blog, and *published* is a simple
    flag to label a day where a new article was published (1) or no article was published
    (0). Intuitively, we know that when new content is released, the views usually
    increase for a period of time.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the column *is_holiday* indicates if there is a holiday or not in the
    United States. The intuition is that on holidays, fewer people will visit my blog.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s visualize our data and look for discerning patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2bcfd0a41d8a036f517aecf0be949731.png)'
  prefs: []
  type: TYPE_IMG
- en: Daily views on my blog. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can already see some interesting behaviour. First,
    notice that the red dots indicate a new published article, and they are almost
    immediately followed by peaks in visits.
  prefs: []
  type: TYPE_NORMAL
- en: We also notice less activity in 2021 which is reflected in fewer daily views
    on my blog. Finally, in 2023, we notice some anomalous peaks in visits after an
    article is published.
  prefs: []
  type: TYPE_NORMAL
- en: Zooming in on the data, we also uncover a clear weekly seasonality.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a9cb296a60b240f8c5df3f584f588dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Daily views on my blog. Here, we see a clear weekly seasonality with less people
    visiting on the weekend. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can now see that fewer visitors come to the blog during
    the weekend than during the week.
  prefs: []
  type: TYPE_NORMAL
- en: With all of that in mind, let’s see how we can work with TimeGPT to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Predict with TimeGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let’s split the dataset into a training set and a test set. Here, I will
    keep 168 time steps for the test set, which corresponds to 24 weeks of daily data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then, we work with a forecast horizon of seven days, as I am interested in predicting
    the daily views for a full week.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the API does not come with an implementation of cross-validation. Therefore,
    we create our own loop to generate seven predictions at a time, until we have
    predictions for the entire test set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the code block above, notice that we have to pass the future values of our
    exogenous variables. This is fine, because they are static variables. We know
    the future dates of holidays, and the blog author personally knows when he plans
    on publishing an article.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that we fine-tune TimeGPT to our data using the *finetune_steps* parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Once the loop is done, we can add the predictions to the test set. Again, TimeGPT
    generated seven predictions at a time until 168 predictions were obtained, so
    that we can evaluate its capacity in forecasting the daily views for next week.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7568508f0602da67a14bc017d7b8893c.png)'
  prefs: []
  type: TYPE_IMG
- en: Predictions from TimeGPT. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting with N-BEATS, N-HiTS and PatchTST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s apply other methods to see if training these models specifically
    on our dataset can produce better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: For this experiment, as mentioned before, we use N-BEATS, N-HiTS and PatchTST.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Then, we initialize the *NeuralForecast* object and specify the frequency of
    our data, which is daily in this case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Then, we run perform cross-validation over 24 windows of 7 time steps to have
    predictions that align with the test set used for TimeGPT.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can simply add the predictions from TimeGPT to this new *preds_df*
    DataFrame to have a single DataFrame with the predictions from all models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/88ac6201d63fccff2ba447acfcdfb5e3.png)'
  prefs: []
  type: TYPE_IMG
- en: DataFrame with predictions from all models. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Great! We are now ready to evaluate the performance of each model.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before measuring performance metrics, let’s visualize the predictions of each
    model on our test set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1dae3bf9da71a92143bbb581ee80e438.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing the predictions from each model. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: First, we see a lot of overlapping between each model. However, we do notice
    that N-HiTS predicted two peaks that were not realized in real life. Also, it
    seems that PatchTST is often under-forecasting. However, TimeGPT seem to generally
    overlap the actual data quite well.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the only way to assess each model’s performance is to measure performance
    metrics. Here, we use the mean absolute error (MAE) and mean squared error (MSE).
    Also, we round the predictions to whole numbers, as a decimal number makes no
    sense in the context of daily visitors to a blog.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b5f5589624a057c73f2c8b3685b85464.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance metrics of each model. Here, TimeGPT is the champion model as it
    achieves the lowest MAE and MSE. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we see that TimeGPT is the champion model as it achieves
    the lowest MAE and MSE, followed by N-BEATS, PatchTST and N-HiTS.
  prefs: []
  type: TYPE_NORMAL
- en: This is an exciting result, as TimeGPT has never seen this dataset and was only
    fine-tuned for a few steps. While this is not an exhaustive experiment, I believe
    it does show a glimpse of the potential foundational models can have in the field
    of forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: My personal opinion on TimeGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While my short experiment with TimeGPT proved to be exciting, I must point out
    that the [original paper](https://arxiv.org/pdf/2310.03589.pdf) remains vague
    in many important areas.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we do not know what datasets were used to train and test the model, so
    we cannot really verify the performance results of TimeGPT, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d0b4e177853479b015c216be4d57815.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance result of TimeGPT as reported in the [original paper](https://arxiv.org/pdf/2310.03589.pdf)
    by Azul Garza and Max Mergenthaler-Canseco
  prefs: []
  type: TYPE_NORMAL
- en: From the table above, we can see that TimeGPT performs best for the monthly
    and weekly frequencies, with N-HiTS and Temporal Fusion Transformer (TFT) usually
    ranking 2nd or 3rd. Then again, because we do not know what data was used, we
    cannot verify these metrics.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a lack of transparency when it comes to how the model was trained
    and how it was adapted to handle time series data.
  prefs: []
  type: TYPE_NORMAL
- en: I believe that the model is intended for commercial use, which explains why
    the paper lacks the details to reproduce TimeGPT. There is nothing wrong with
    that, but the lack of reproducibility of the paper is a concern for the scientific
    community.
  prefs: []
  type: TYPE_NORMAL
- en: Still, I hope that this sparks new work and research in foundation models for
    time series, and that we eventually see an open-source version of these models,
    much like we see it happening for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TimeGPT is the first foundation model for time series forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: It leverages the Transformer architecture and was pre-trained on 100 billion
    data points to make zero-shot inference on new unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Combined with the technique of conformal prediction, the model can generate
    prediction intervals and perform anomaly detection without being trained on a
    specific dataset.
  prefs: []
  type: TYPE_NORMAL
- en: I still believe that each forecasting problem requires a unique approach, so
    make sure to test out TimeGPT as well as other models.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading! I hope that you enjoyed it and that you learned something
    new!
  prefs: []
  type: TYPE_NORMAL
- en: Looking to master time series forecasting? Then check out my course [Applied
    Time Series Forecasting in Python](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10).
    This is the only course that uses Python to implement statistical, deep learning
    and state-of-the-art models in 16 guided hands-on projects.
  prefs: []
  type: TYPE_NORMAL
- en: Cheers 🍻
  prefs: []
  type: TYPE_NORMAL
- en: Support me
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enjoying my work? Show your support with [Buy me a coffee](http://buymeacoffee.com/dswm),
    a simple way for you to encourage me, and I get to enjoy a cup of coffee! If you
    feel like it, just click the button below 👇
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a90a701107c4ea11414ef27bd59465af.png)'
  prefs: []
  type: TYPE_IMG
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[TimeGPT-1](https://arxiv.org/pdf/2310.03589.pdf) by Azul Garza and Max Mergenthaler-Canseco'
  prefs: []
  type: TYPE_NORMAL
