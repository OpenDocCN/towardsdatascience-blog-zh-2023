- en: Loss Functions in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/loss-functions-in-machine-learning-9977e810ac02](https://towardsdatascience.com/loss-functions-in-machine-learning-9977e810ac02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understand the most common loss functions and when to use each one
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roiyeho?source=post_page-----9977e810ac02--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----9977e810ac02--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9977e810ac02--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9977e810ac02--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----9977e810ac02--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9977e810ac02--------------------------------)
    ·11 min read·May 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/075f0e8f6ffba336b07154d146ab466c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/XWar9MbNGUY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions have an important role in machine learning as they guide the
    learning process of the model and define its objective.
  prefs: []
  type: TYPE_NORMAL
- en: There is a large number of loss functions available and choosing the proper
    one is crucial for training an accurate model. Different choices of a loss function
    can lead to different classification or regression models.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we will discuss the most commonly used loss functions, how they
    operate, their pros and cons, and when to use each one.
  prefs: []
  type: TYPE_NORMAL
- en: What is a Loss Function?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that in [supervised machine learning](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)
    problems, we are given a training set of *n* labeled samples: *D* = {(**x**₁,
    *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ* represents the **features**
    of sample *i* and *yᵢ* represents the **label** of that sample. Our goal is to
    build a model whose predictions are as close as possible to the true labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A **loss function** measures the model’s prediction error for a given sample,
    i.e., the difference between the model’s predicted value and the true value for
    that sample. It takes two parameters: the true label of the sample *y* and the
    model’s prediction *ŷ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d967248ab329e423346312152b020a1.png)'
  prefs: []
  type: TYPE_IMG
- en: During the training of the model, we tune its parameters so as to minimize the
    loss function on the given training samples.
  prefs: []
  type: TYPE_NORMAL
- en: Note that a loss functioncalculates the error per sample, while a **cost function**
    calculates the error over the whole data set (although these two terms are sometimes
    used interchangeably).
  prefs: []
  type: TYPE_NORMAL
- en: Desired Properties of a Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ideally, we would like the loss function to have the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss function should reflect the objective the model is trying to achieve.
    For example, in regression problems our goal is to minimize the differences between
    the predictions and the target values, while in classification our goal is to
    minimize the number of misclassification errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous and differentiable everywhere. Most optimization algorithms, such
    as gradient descent, require the loss function to be differentiable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convex. A convex function has only one global minimum point, thus optimization
    methods like gradient descent are guaranteed to return the globally optimal solution.
    In practice, this property is hard to achieve, and most loss functions are non-convex
    (i.e., they have multiple local minima).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Symmetric, i.e., the error above the target should cause the same loss as the
    same error below the target.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast to compute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss Functions and Maximum Likelihood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many of the loss functions used in machine learning can be derived from the
    maximum likelihood principle (see [my previous article](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43)
    for explanation on maximum likelihood).
  prefs: []
  type: TYPE_NORMAL
- en: 'In maximum likelihood estimation (MLE) we are trying to find a model with parameters
    *θ* that maximizes the probability of the observed data given the model: *P*(*D*|*θ*).
    To simplify the likelihood function, we typically take its logarithm, and then
    we try to maximize the log likelihood: log *P*(*D*|*θ*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can define a loss function for a given sample (**x**, *y*) as
    the negative log likelihood of observing its true label given the prediction of
    our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34e04997a93736017db26d842aba3b4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Loss function as the negative log likelihood
  prefs: []
  type: TYPE_NORMAL
- en: Because negative logarithm is a monotonically decreasing function, maximizing
    the likelihood is equivalent to minimizing the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Note that to use this technique to define loss functions, we need to assume
    that the data set is generated from some known probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections we will discuss the most common loss functions used in
    different types of problems (regression, binary classification and multi-class
    classification).
  prefs: []
  type: TYPE_NORMAL
- en: Regression Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In regression problem both the target label and the model’s prediction take
    continuous values. The three most commonly used loss functions in regression problems
    are: squared loss, absolute loss and Huber loss.'
  prefs: []
  type: TYPE_NORMAL
- en: Squared Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Squared loss is defined as the squared difference between the target label
    and its predicted value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc58379fa07e6476689d9f3d59387303.png)'
  prefs: []
  type: TYPE_IMG
- en: Squared loss
  prefs: []
  type: TYPE_NORMAL
- en: This loss function is used in ordinary least squares (OLS), which is the most
    common method for solving [linear regression](https://medium.com/towards-data-science/linear-regression-in-depth-part-1-485f997fd611)
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous and differentiable everywhere
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convex (has only one global minimum)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to compute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the assumption that the labels have a Gaussian noise, squared loss is
    the negative maximum likelihood of the model given the data. You can find a proof
    of this statement in [my previous article](https://medium.com/towards-data-science/linear-regression-in-depth-part-1-485f997fd611).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**:'
  prefs: []
  type: TYPE_NORMAL
- en: Sensitive to outliers due to the squaring of the errors. A small number of samples
    that are distant from the other samples can cause a large change in the model
    (as will be demonstrated later).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Absolute Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The absolute loss is defined as the absolute difference between the true label
    and the model’s prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7287270f7cf523465b0126f9956a8382.png)'
  prefs: []
  type: TYPE_IMG
- en: Absolute loss
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**:'
  prefs: []
  type: TYPE_NORMAL
- en: Not overly affected by outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to compute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**:'
  prefs: []
  type: TYPE_NORMAL
- en: Non-differentiable at 0, which makes it hard use it in optimization methods
    such as gradient descent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not have a maximum likelihood interpretation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huber Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Huber loss is a combination of squared loss and absolute loss. For loss values
    that are less than a predefined parameter called *δ*, it uses the squared error,
    and for values greater than *δ* it uses the absolute error.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical definition of Huber loss is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcdf839bed49d497927cc1de2cbea11f.png)'
  prefs: []
  type: TYPE_IMG
- en: Huber loss
  prefs: []
  type: TYPE_NORMAL
- en: '*δ* is typically set to 1.'
  prefs: []
  type: TYPE_NORMAL
- en: Huber loss is commonly used in deep learning where it helps to avoid the exploding
    gradient problem due to its insensitivity to large errors.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous and differentiable everywhere
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less sensitive to outliers than squared loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**:'
  prefs: []
  type: TYPE_NORMAL
- en: Slower to compute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires tuning of the hyperparameter *δ*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not have a maximum likelihood interpretation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following graph shows the three regression loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a052f714dc9c52381fe4d4a534cc37c.png)'
  prefs: []
  type: TYPE_IMG
- en: Loss functions for regression problems
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-Learn Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)
    class fits a [linear regression](https://medium.com/towards-data-science/linear-regression-in-depth-part-1-485f997fd611)
    model to a given data set using stochastic gradient descent (SGD). Its *loss*
    parameter can be used to choose the loss function for the optimization. The options
    of this parameter are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*squared_error* (squared loss). This is the default option.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*huber* (Huber loss)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*epsilon_intensive* (the loss function used in Support Vector Regression)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s examine the effect of using the Huber loss instead of squared loss on
    a sample data set that contains an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define our data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3e1fedad8a0563309e41023bcba2287f.png)'
  prefs: []
  type: TYPE_IMG
- en: The training set
  prefs: []
  type: TYPE_NORMAL
- en: Clearly the point (10, 10) is an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we fit two SGDRegressor models to this data set: one with a squared loss
    function and another with a Huber loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the two regression lines found by these models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9266c1c43c25ea66806e77b07930eebf.png)'
  prefs: []
  type: TYPE_IMG
- en: The regression lines found by squared and Huber losses
  prefs: []
  type: TYPE_NORMAL
- en: It is clearly evident that the model trained with squared loss was much more
    affected by the outlier than the model trained with Huber loss.
  prefs: []
  type: TYPE_NORMAL
- en: Binary Classification Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In binary classification problems, the ground truth labels are binary (1/0 or
    1/-1). The predicted value of the model can be either binary (a **hard label**)
    or a probability estimate that the given sample belongs to the positive class
    (a **soft label**).
  prefs: []
  type: TYPE_NORMAL
- en: Examples for classification models that provide only hard labels include support
    vector machines (SVMs) and [K-nearest neighbors](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad)
    (KNNs), while models such as [logistic regression](https://medium.com/towards-data-science/mastering-logistic-regression-3e502686f0ae)
    and [neural networks](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)
    (with a sigmoid output) also provide a probability estimate.
  prefs: []
  type: TYPE_NORMAL
- en: 0–1 Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The simplest loss function is the zero-one loss function (also called **misclassification
    error**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c97cbd97070d64dff379c1358492dd53.png)'
  prefs: []
  type: TYPE_IMG
- en: Zero-one loss
  prefs: []
  type: TYPE_NORMAL
- en: '*I* is the indicator function that returns 1 if its input is true, and 0 otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: For every sample that the classifier gets wrong (misclassifies) a loss of 1
    is incurred, whereas correctly classified samples lead to 0 loss.
  prefs: []
  type: TYPE_NORMAL
- en: The 0–1 loss function is often used to evaluate classifiers, but is not useful
    in guiding optimization since it is non-differentiable and non-continuous.
  prefs: []
  type: TYPE_NORMAL
- en: Log Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Log loss (also called **logistic loss** or **binary cross-entropy loss**) is
    used to train models that provide class probability estimates such as [logistic
    regression](https://medium.com/towards-data-science/mastering-logistic-regression-3e502686f0ae).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us denote the probability estimate given by the model that the sample belongs
    to the positive class by *p*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/717ee211ea04947413c6c9e06dbc5a8c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then log loss is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/204fff1b75019188c1b25ebf1249e8e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Log loss
  prefs: []
  type: TYPE_NORMAL
- en: 'How did we get to this loss function? Again we are going to use the maximum
    likelihood principle. More specifically, we will show that log loss is the negative
    log likelihood under the assumption that the labels have a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution)
    (a probability distribution of a binary random variable that takes 1 with probability
    *p* and 0 with probability 1 − *p*). Mathematically, this can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/615eb356f5860b57e640f5c6bb4f2830.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Proof:**'
  prefs: []
  type: TYPE_NORMAL
- en: Given a model of the data (the labels) as a Bernoulli distribution with parameter
    *p*, the probability that a sample belongs to the positive class is simply *p*,
    i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/981280d9c7d1049cf83134177090d825.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the probability that the sample belongs to the negative class is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9bf76a672f4e7b4c863d63c189ce3f60.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can write these two equations more compactly as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c658cf69f1834acc67a0262ac647d9d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Explanation: when y = *1*, pʸ = p and *(1* − p)*¹*⁻ʸ= *1*, therefore P(y|p)
    = p. Similarly, when y = 0, pʸ = *1* and (*1* − p)*¹*⁻ʸ = *1* − p, therefore P(y|p)
    = *1* − p.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Therefore the log likelihood of the data is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf2cbf2a69b101936b74249690535fb9.png)'
  prefs: []
  type: TYPE_IMG
- en: The log loss is exactly the negative of this function!
  prefs: []
  type: TYPE_NORMAL
- en: The log loss function is differentiable and convex, i.e., it has a unique global
    minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Hinge Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hinge loss is used for training support vector machines (SVMs), where the goal
    is to maximize the margin of the area that separates the two classes while minimizing
    the margin violations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b380c4b177bbd25387d3c940a683d61a.png)'
  prefs: []
  type: TYPE_IMG
- en: SVM
  prefs: []
  type: TYPE_NORMAL
- en: 'The hinge loss is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bec472d3df7b22f8f186c7f12fe1e284.png)'
  prefs: []
  type: TYPE_IMG
- en: Hinge loss
  prefs: []
  type: TYPE_NORMAL
- en: Note that *ŷ* here is the raw output of the classifier’s decision function,
    i.e., *ŷ* = **w***ᵗ***x** (SVM does not provide probability estimates).
  prefs: []
  type: TYPE_NORMAL
- en: When *y* and *ŷ* have the same sign (i.e., the model predicts the correct class)
    and |*ŷ*| ≥ 1, the hinge loss is 0\. This means that correctly classified samples
    that are outside the margin do not contribute to the loss (the solution will be
    the same with these samples removed). However, for samples that are inside the
    margins (|*ŷ*| < 1), even if the model’s prediction is correct, there will still
    be a small loss. When *y* and *ŷ* have opposite signs, the hinge loss grows linearly
    with *ŷ.*
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines will be covered in more detail in a future article.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows the three classification loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d67850a47efcb3cadc3066ba1d985f8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Loss functions for binary classification problems
  prefs: []
  type: TYPE_NORMAL
- en: Both log loss and hinge loss can be seen as continuous approximations to the
    0–1 loss.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Class Classification Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In multi-class classification problems, the target label is 1 out of *k* classes.
    The label is usually encoded using one-hot encoding, i.e., as a binary *k*-dimensional
    vector **y** = (*y*₁, …, *yₖ*)*ᵗ*, where *yᵢ* = 1 for the true class *i* and 0
    elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'A probabilistic classifier outputs for each sample a *k*-dimensional vector
    with probability estimates of each class: **p** = (*p*₁, …, *pₖ*)*ᵗ*. These probabilities
    sum to 1, i.e., *p*₁ + … + *pₖ* = 1.'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Entropy Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The loss function used to train such a classifier is called **cross-entropy
    loss**, which is an extension of log loss to the multi-class case. It is defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e427f6ddcc9387b6b90580074048758.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross-entropy loss
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, assume that we have a three-class problem, the true class of our
    sample is class 2 (i.e., **y** = [0, 1, 0]), and the prediction of our model is
    **p** = [0.3, 0.6, 0.1]. Then the cross-entropy loss induced by this sample is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1891f3a6b5b5e64882b4c8a882e70e41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To see how the cross-entropy loss generalizes log loss, notice that in the
    binary case *p*₁ = 1 - *p*₀ and *y*₁ = 1 - *y*₀, therefore we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd6a9d5681ba8fe5f17b7d8b13a405b1.png)'
  prefs: []
  type: TYPE_IMG
- en: which is exactly the log loss for *p* = *p*₀ and *y* = *y*₀.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to log loss, we can show that cross-entropy loss is the negative of
    the log-likelihood of the model, under the assumption that the labels are sampled
    from a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution)
    (a generalization of Bernoulli distribution to *k* possible outcomes).
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a model of the data (the labels) as a categorical distribution with probabilities
    **p** = (*p*₁, …, *pₖ*), the probability that a given sample belongs to class
    *i* is *pᵢ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f942e0a2fc1a0df23c29397166fc2d0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the probability that the true label of the sample is **y** is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d89058ed69762eaf26ec90e40a76d4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Explanation: if the correct class of the given sample is i, then yᵢ = *1*,
    and for all j ≠ i, yⱼ = *0*. Hence, P*(****y***|***p****)* = pᵢ, which is the
    probability that the sample belongs to class i.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Therefore, the log likelihood of our model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94d51e236487cec80bc12c7e7b14f80e.png)'
  prefs: []
  type: TYPE_IMG
- en: The cross-entropy loss is exactly the negative of this function!
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article we have discussed various loss functions and showed how they
    are derived from basic principles such as maximum likelihood.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In regression problems, squared loss is the most common loss function. However,
    if you suspect that your data set contains outliers, using Huber loss may be a
    better choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In binary classification problems, the choice of a different loss function leads
    to a different classifier (logistic regression uses log loss while SVM uses hinge
    loss).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In multi-class classification problems, cross-entropy loss is the most common
    loss function and is extension of log loss to the multi-class case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Final Notes**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code examples of this article can be found on my github: [https://github.com/roiyeho/medium/tree/main/loss_functions](https://github.com/roiyeho/medium/tree/main/loss_functions)'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
