- en: Loss Functions in Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的损失函数
- en: 原文：[https://towardsdatascience.com/loss-functions-in-machine-learning-9977e810ac02](https://towardsdatascience.com/loss-functions-in-machine-learning-9977e810ac02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/loss-functions-in-machine-learning-9977e810ac02](https://towardsdatascience.com/loss-functions-in-machine-learning-9977e810ac02)
- en: Understand the most common loss functions and when to use each one
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解最常见的损失函数及其使用场景
- en: '[](https://medium.com/@roiyeho?source=post_page-----9977e810ac02--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----9977e810ac02--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9977e810ac02--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9977e810ac02--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----9977e810ac02--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----9977e810ac02--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----9977e810ac02--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9977e810ac02--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9977e810ac02--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----9977e810ac02--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9977e810ac02--------------------------------)
    ·11 min read·May 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----9977e810ac02--------------------------------)
    ·阅读时间：11分钟·2023年5月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/075f0e8f6ffba336b07154d146ab466c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/075f0e8f6ffba336b07154d146ab466c.png)'
- en: Photo by [Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/XWar9MbNGUY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，来源于[Unsplash](https://unsplash.com/photos/XWar9MbNGUY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Loss functions have an important role in machine learning as they guide the
    learning process of the model and define its objective.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数在机器学习中扮演着重要角色，因为它们引导模型的学习过程并定义其目标。
- en: There is a large number of loss functions available and choosing the proper
    one is crucial for training an accurate model. Different choices of a loss function
    can lead to different classification or regression models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 存在大量的损失函数，选择合适的损失函数对于训练一个准确的模型至关重要。不同的损失函数选择可以导致不同的分类或回归模型。
- en: In this article we will discuss the most commonly used loss functions, how they
    operate, their pros and cons, and when to use each one.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论最常用的损失函数，它们的运作方式、优缺点以及每种函数的适用场景。
- en: What is a Loss Function?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是损失函数？
- en: 'Recall that in [supervised machine learning](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)
    problems, we are given a training set of *n* labeled samples: *D* = {(**x**₁,
    *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ* represents the **features**
    of sample *i* and *yᵢ* represents the **label** of that sample. Our goal is to
    build a model whose predictions are as close as possible to the true labels.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，在[监督式机器学习](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)问题中，我们给定了一个包含*n*个标签样本的训练集：*D*
    = {(**x**₁, *y*₁), (**x**₂, *y*₂), … , (**x**ₙ, *y*ₙ*)}，其中**x**ᵢ表示样本*i*的**特征**，*yᵢ*表示该样本的**标签**。我们的目标是构建一个预测尽可能接近真实标签的模型。
- en: 'A **loss function** measures the model’s prediction error for a given sample,
    i.e., the difference between the model’s predicted value and the true value for
    that sample. It takes two parameters: the true label of the sample *y* and the
    model’s prediction *ŷ*:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失函数**度量模型对给定样本的预测误差，即模型预测值与该样本真实值之间的差异。它有两个参数：样本的真实标签 *y* 和模型的预测值 *ŷ*：'
- en: '![](../Images/9d967248ab329e423346312152b020a1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d967248ab329e423346312152b020a1.png)'
- en: During the training of the model, we tune its parameters so as to minimize the
    loss function on the given training samples.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练过程中，我们调整其参数，以最小化在给定训练样本上的损失函数。
- en: Note that a loss functioncalculates the error per sample, while a **cost function**
    calculates the error over the whole data set (although these two terms are sometimes
    used interchangeably).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，损失函数计算每个样本的误差，而**成本函数**计算整个数据集的误差（尽管这两个术语有时可以互换使用）。
- en: Desired Properties of a Loss Function
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数的期望属性
- en: 'Ideally, we would like the loss function to have the following properties:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望损失函数具有以下属性：
- en: The loss function should reflect the objective the model is trying to achieve.
    For example, in regression problems our goal is to minimize the differences between
    the predictions and the target values, while in classification our goal is to
    minimize the number of misclassification errors.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数应反映模型试图实现的目标。例如，在回归问题中，我们的目标是最小化预测值与目标值之间的差异，而在分类中，我们的目标是最小化误分类错误的数量。
- en: Continuous and differentiable everywhere. Most optimization algorithms, such
    as gradient descent, require the loss function to be differentiable.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有地方都是连续和可导的。大多数优化算法，如梯度下降，需要损失函数是可导的。
- en: Convex. A convex function has only one global minimum point, thus optimization
    methods like gradient descent are guaranteed to return the globally optimal solution.
    In practice, this property is hard to achieve, and most loss functions are non-convex
    (i.e., they have multiple local minima).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凸性。凸函数只有一个全局最小点，因此像梯度下降这样的优化方法可以保证返回全局最优解。在实践中，这种属性很难实现，大多数损失函数是非凸的（即，它们具有多个局部最小值）。
- en: Symmetric, i.e., the error above the target should cause the same loss as the
    same error below the target.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对称，即，目标上方的误差应导致与目标下方相同的损失。
- en: Fast to compute
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算快速
- en: Loss Functions and Maximum Likelihood
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数和最大似然
- en: Many of the loss functions used in machine learning can be derived from the
    maximum likelihood principle (see [my previous article](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43)
    for explanation on maximum likelihood).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 许多在机器学习中使用的损失函数可以从最大似然原则推导出来（有关最大似然的解释，请参见[我的上一篇文章](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43)）。
- en: 'In maximum likelihood estimation (MLE) we are trying to find a model with parameters
    *θ* that maximizes the probability of the observed data given the model: *P*(*D*|*θ*).
    To simplify the likelihood function, we typically take its logarithm, and then
    we try to maximize the log likelihood: log *P*(*D*|*θ*).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在最大似然估计（MLE）中，我们试图找到一个具有参数*θ*的模型，该模型最大化给定模型的观察数据的概率：*P*(*D*|*θ*)。为了简化似然函数，我们通常取其对数，然后我们尝试最大化对数似然：log
    *P*(*D*|*θ*)。
- en: 'Therefore, we can define a loss function for a given sample (**x**, *y*) as
    the negative log likelihood of observing its true label given the prediction of
    our model:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将给定样本（**x**，*y*）的损失函数定义为给定我们模型预测的真实标签的负对数似然：
- en: '![](../Images/34e04997a93736017db26d842aba3b4e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34e04997a93736017db26d842aba3b4e.png)'
- en: Loss function as the negative log likelihood
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作为负对数似然的损失函数
- en: Because negative logarithm is a monotonically decreasing function, maximizing
    the likelihood is equivalent to minimizing the loss.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因为负对数是一个单调递减函数，所以最大化似然等同于最小化损失。
- en: Note that to use this technique to define loss functions, we need to assume
    that the data set is generated from some known probability distribution.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，要使用这种技术定义损失函数，我们需要假设数据集是从某种已知的概率分布生成的。
- en: In the next sections we will discuss the most common loss functions used in
    different types of problems (regression, binary classification and multi-class
    classification).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将讨论在不同类型的问题（回归、二分类和多分类）中使用的最常见的损失函数。
- en: Regression Problems
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归问题
- en: 'In regression problem both the target label and the model’s prediction take
    continuous values. The three most commonly used loss functions in regression problems
    are: squared loss, absolute loss and Huber loss.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，目标标签和模型预测值都取连续值。回归问题中最常用的三种损失函数是：平方损失、绝对损失和Huber损失。
- en: Squared Loss
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平方损失
- en: 'Squared loss is defined as the squared difference between the target label
    and its predicted value:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 平方损失定义为目标标签与其预测值之间的平方差：
- en: '![](../Images/dc58379fa07e6476689d9f3d59387303.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc58379fa07e6476689d9f3d59387303.png)'
- en: Squared loss
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 平方损失
- en: This loss function is used in ordinary least squares (OLS), which is the most
    common method for solving [linear regression](https://medium.com/towards-data-science/linear-regression-in-depth-part-1-485f997fd611)
    problems.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 该损失函数用于普通最小二乘法（OLS），这是解决[线性回归](https://medium.com/towards-data-science/linear-regression-in-depth-part-1-485f997fd611)问题的最常见方法。
- en: '**Pros**:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: Continuous and differentiable everywhere
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有地方都是连续和可导的
- en: Convex (has only one global minimum)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凸性（只有一个全局最小值）
- en: Easy to compute
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于计算
- en: Under the assumption that the labels have a Gaussian noise, squared loss is
    the negative maximum likelihood of the model given the data. You can find a proof
    of this statement in [my previous article](https://medium.com/towards-data-science/linear-regression-in-depth-part-1-485f997fd611).
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在假设标签有高斯噪声的情况下，平方损失是给定数据下模型的负最大似然。你可以在[我之前的文章](https://medium.com/towards-data-science/linear-regression-in-depth-part-1-485f997fd611)中找到这个陈述的证明。
- en: '**Cons**:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: Sensitive to outliers due to the squaring of the errors. A small number of samples
    that are distant from the other samples can cause a large change in the model
    (as will be demonstrated later).
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于误差的平方，对离群点敏感。少量距离其他样本较远的样本可能会导致模型发生较大变化（稍后会演示）。
- en: Absolute Loss
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绝对损失
- en: 'The absolute loss is defined as the absolute difference between the true label
    and the model’s prediction:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对损失定义为真实标签和模型预测之间的绝对差异：
- en: '![](../Images/7287270f7cf523465b0126f9956a8382.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7287270f7cf523465b0126f9956a8382.png)'
- en: Absolute loss
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对损失
- en: '**Pros**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: Not overly affected by outliers
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对离群点的影响不大
- en: Easy to compute
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算简单
- en: '**Cons**:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: Non-differentiable at 0, which makes it hard use it in optimization methods
    such as gradient descent.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 0 处不可微分，这使得在梯度下降等优化方法中使用它变得困难。
- en: Does not have a maximum likelihood interpretation
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有最大似然解释
- en: Huber Loss
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Huber 损失
- en: Huber loss is a combination of squared loss and absolute loss. For loss values
    that are less than a predefined parameter called *δ*, it uses the squared error,
    and for values greater than *δ* it uses the absolute error.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Huber 损失是平方损失和绝对损失的组合。对于小于预定义参数 *δ* 的损失值，它使用平方误差，对于大于 *δ* 的值，它使用绝对误差。
- en: 'The mathematical definition of Huber loss is:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Huber 损失的数学定义是：
- en: '![](../Images/fcdf839bed49d497927cc1de2cbea11f.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcdf839bed49d497927cc1de2cbea11f.png)'
- en: Huber loss
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Huber 损失
- en: '*δ* is typically set to 1.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*δ* 通常设置为 1。'
- en: Huber loss is commonly used in deep learning where it helps to avoid the exploding
    gradient problem due to its insensitivity to large errors.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Huber 损失在深度学习中常用于避免梯度爆炸问题，因为它对大错误不敏感。
- en: '**Pros**:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: Continuous and differentiable everywhere
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有地方都连续且可微分
- en: Less sensitive to outliers than squared loss
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比平方损失对离群点的敏感度低
- en: '**Cons**:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: Slower to compute
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算较慢
- en: Requires tuning of the hyperparameter *δ*
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要调整超参数 *δ*
- en: Does not have a maximum likelihood interpretation
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有最大似然解释
- en: 'The following graph shows the three regression loss functions:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了三种回归损失函数：
- en: '![](../Images/7a052f714dc9c52381fe4d4a534cc37c.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a052f714dc9c52381fe4d4a534cc37c.png)'
- en: Loss functions for regression problems
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题的损失函数
- en: Scikit-Learn Example
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scikit-Learn 示例
- en: 'The [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)
    class fits a [linear regression](https://medium.com/towards-data-science/linear-regression-in-depth-part-1-485f997fd611)
    model to a given data set using stochastic gradient descent (SGD). Its *loss*
    parameter can be used to choose the loss function for the optimization. The options
    of this parameter are:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)
    类通过随机梯度下降（SGD）将[线性回归](https://medium.com/towards-data-science/linear-regression-in-depth-part-1-485f997fd611)模型拟合到给定的数据集。其
    *loss* 参数可用于选择优化的损失函数。该参数的选项包括：'
- en: '*squared_error* (squared loss). This is the default option.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*squared_error*（平方损失）。这是默认选项。'
- en: '*huber* (Huber loss)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*huber*（Huber 损失）'
- en: '*epsilon_intensive* (the loss function used in Support Vector Regression)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*epsilon_intensive*（在支持向量回归中使用的损失函数）'
- en: Let’s examine the effect of using the Huber loss instead of squared loss on
    a sample data set that contains an outlier.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来检查使用 Huber 损失而非平方损失对包含离群点的样本数据集的影响。
- en: 'We first define our data set:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义我们的数据集：
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s plot the data points:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来绘制数据点：
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/3e1fedad8a0563309e41023bcba2287f.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e1fedad8a0563309e41023bcba2287f.png)'
- en: The training set
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集
- en: Clearly the point (10, 10) is an outlier.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，点 (10, 10) 是一个离群点。
- en: 'Next, we fit two SGDRegressor models to this data set: one with a squared loss
    function and another with a Huber loss.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将两个 SGDRegressor 模型拟合到此数据集中：一个使用平方损失函数，另一个使用 Huber 损失。
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s plot the two regression lines found by these models:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来绘制这些模型找到的两条回归线：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/9266c1c43c25ea66806e77b07930eebf.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9266c1c43c25ea66806e77b07930eebf.png)'
- en: The regression lines found by squared and Huber losses
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 平方损失和 Huber 损失找到的回归线
- en: It is clearly evident that the model trained with squared loss was much more
    affected by the outlier than the model trained with Huber loss.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，使用平方损失训练的模型比使用Huber损失训练的模型更受离群值的影响。
- en: Binary Classification Problems
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二元分类问题
- en: In binary classification problems, the ground truth labels are binary (1/0 or
    1/-1). The predicted value of the model can be either binary (a **hard label**)
    or a probability estimate that the given sample belongs to the positive class
    (a **soft label**).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元分类问题中，真实标签是二元的（1/0 或 1/-1）。模型的预测值可以是二元的（**硬标签**）或样本属于正类的概率估计（**软标签**）。
- en: Examples for classification models that provide only hard labels include support
    vector machines (SVMs) and [K-nearest neighbors](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad)
    (KNNs), while models such as [logistic regression](https://medium.com/towards-data-science/mastering-logistic-regression-3e502686f0ae)
    and [neural networks](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)
    (with a sigmoid output) also provide a probability estimate.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 仅提供硬标签的分类模型的示例包括支持向量机（SVM）和[K近邻](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad)（KNN），而像[逻辑回归](https://medium.com/towards-data-science/mastering-logistic-regression-3e502686f0ae)和[神经网络](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)（具有sigmoid输出）这样的模型也提供概率估计。
- en: 0–1 Loss
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0–1 损失
- en: 'The simplest loss function is the zero-one loss function (also called **misclassification
    error**):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的损失函数是零一损失函数（也称为**误分类错误**）：
- en: '![](../Images/c97cbd97070d64dff379c1358492dd53.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c97cbd97070d64dff379c1358492dd53.png)'
- en: Zero-one loss
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 零一损失
- en: '*I* is the indicator function that returns 1 if its input is true, and 0 otherwise.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*I* 是指示函数，如果其输入为真则返回1，否则返回0。'
- en: For every sample that the classifier gets wrong (misclassifies) a loss of 1
    is incurred, whereas correctly classified samples lead to 0 loss.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个分类器错误分类的样本（误分类），会产生1的损失，而正确分类的样本则导致0损失。
- en: The 0–1 loss function is often used to evaluate classifiers, but is not useful
    in guiding optimization since it is non-differentiable and non-continuous.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 0–1损失函数常用于评估分类器，但在优化指导中并不实用，因为它是不可微分且不连续的。
- en: Log Loss
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对数损失
- en: Log loss (also called **logistic loss** or **binary cross-entropy loss**) is
    used to train models that provide class probability estimates such as [logistic
    regression](https://medium.com/towards-data-science/mastering-logistic-regression-3e502686f0ae).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失（也称为**逻辑损失**或**二元交叉熵损失**）用于训练提供类别概率估计的模型，如[逻辑回归](https://medium.com/towards-data-science/mastering-logistic-regression-3e502686f0ae)。
- en: 'Let us denote the probability estimate given by the model that the sample belongs
    to the positive class by *p*:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 设我们用*p*表示模型给出的样本属于正类的概率估计：
- en: '![](../Images/717ee211ea04947413c6c9e06dbc5a8c.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/717ee211ea04947413c6c9e06dbc5a8c.png)'
- en: 'Then log loss is defined as:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对数损失被定义为：
- en: '![](../Images/204fff1b75019188c1b25ebf1249e8e7.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/204fff1b75019188c1b25ebf1249e8e7.png)'
- en: Log loss
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失
- en: 'How did we get to this loss function? Again we are going to use the maximum
    likelihood principle. More specifically, we will show that log loss is the negative
    log likelihood under the assumption that the labels have a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution)
    (a probability distribution of a binary random variable that takes 1 with probability
    *p* and 0 with probability 1 − *p*). Mathematically, this can be written as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是如何得到这个损失函数的？我们将再次使用最大似然原理。更具体地说，我们将展示对数损失是在标签假设为[伯努利分布](https://en.wikipedia.org/wiki/Bernoulli_distribution)（一种二元随机变量的概率分布，取1的概率为*p*，取0的概率为1
    − *p*）下的负对数似然。数学上，可以写成如下形式：
- en: '![](../Images/615eb356f5860b57e640f5c6bb4f2830.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/615eb356f5860b57e640f5c6bb4f2830.png)'
- en: '**Proof:**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明：**'
- en: Given a model of the data (the labels) as a Bernoulli distribution with parameter
    *p*, the probability that a sample belongs to the positive class is simply *p*,
    i.e.,
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 设数据（标签）模型为伯努利分布，参数为*p*，样本属于正类的概率就是*p*，即：
- en: '![](../Images/981280d9c7d1049cf83134177090d825.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/981280d9c7d1049cf83134177090d825.png)'
- en: 'Similarly, the probability that the sample belongs to the negative class is:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，样本属于负类的概率是：
- en: '![](../Images/9bf76a672f4e7b4c863d63c189ce3f60.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9bf76a672f4e7b4c863d63c189ce3f60.png)'
- en: 'We can write these two equations more compactly as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更紧凑地写出这两个方程：
- en: '![](../Images/c658cf69f1834acc67a0262ac647d9d5.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c658cf69f1834acc67a0262ac647d9d5.png)'
- en: 'Explanation: when y = *1*, pʸ = p and *(1* − p)*¹*⁻ʸ= *1*, therefore P(y|p)
    = p. Similarly, when y = 0, pʸ = *1* and (*1* − p)*¹*⁻ʸ = *1* − p, therefore P(y|p)
    = *1* − p.'
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 解释：当y = *1*时，pʸ = p且*(1* − p)*¹*⁻ʸ= *1*，因此P(y|p) = p。类似地，当y = 0时，pʸ = *1*且(*1*
    − p)*¹*⁻ʸ = *1* − p，因此P(y|p) = *1* − p。
- en: 'Therefore the log likelihood of the data is:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此数据的对数似然是：
- en: '![](../Images/cf2cbf2a69b101936b74249690535fb9.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf2cbf2a69b101936b74249690535fb9.png)'
- en: The log loss is exactly the negative of this function!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失正好是该函数的负数！
- en: The log loss function is differentiable and convex, i.e., it has a unique global
    minimum.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失函数是可微的且凸的，即具有唯一的全局最小值。
- en: Hinge Loss
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 铰链损失
- en: Hinge loss is used for training support vector machines (SVMs), where the goal
    is to maximize the margin of the area that separates the two classes while minimizing
    the margin violations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 铰链损失用于训练支持向量机（SVM），其目标是最大化分隔两个类别的区域的边界，同时最小化边界违例。
- en: '![](../Images/b380c4b177bbd25387d3c940a683d61a.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b380c4b177bbd25387d3c940a683d61a.png)'
- en: SVM
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: SVM
- en: 'The hinge loss is defined as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 铰链损失定义如下：
- en: '![](../Images/bec472d3df7b22f8f186c7f12fe1e284.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bec472d3df7b22f8f186c7f12fe1e284.png)'
- en: Hinge loss
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 铰链损失
- en: Note that *ŷ* here is the raw output of the classifier’s decision function,
    i.e., *ŷ* = **w***ᵗ***x** (SVM does not provide probability estimates).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里的*ŷ*是分类器决策函数的原始输出，即*ŷ* = **w***ᵗ***x**（SVM不提供概率估计）。
- en: When *y* and *ŷ* have the same sign (i.e., the model predicts the correct class)
    and |*ŷ*| ≥ 1, the hinge loss is 0\. This means that correctly classified samples
    that are outside the margin do not contribute to the loss (the solution will be
    the same with these samples removed). However, for samples that are inside the
    margins (|*ŷ*| < 1), even if the model’s prediction is correct, there will still
    be a small loss. When *y* and *ŷ* have opposite signs, the hinge loss grows linearly
    with *ŷ.*
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当*y*和*ŷ*具有相同的符号（即模型预测正确的类别）且|*ŷ*| ≥ 1时，铰链损失为0。这意味着正确分类且在边界之外的样本不会对损失产生影响（即使去掉这些样本，结果也会相同）。然而，对于边界内的样本（|*ŷ*|
    < 1），即使模型的预测是正确的，仍会有少量损失。当*y*和*ŷ*具有相反的符号时，铰链损失会随着*ŷ*的增加而线性增长。
- en: Support vector machines will be covered in more detail in a future article.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机将在未来的文章中详细讨论。
- en: 'The following graph shows the three classification loss functions:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了三种分类损失函数：
- en: '![](../Images/d67850a47efcb3cadc3066ba1d985f8f.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d67850a47efcb3cadc3066ba1d985f8f.png)'
- en: Loss functions for binary classification problems
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类问题的损失函数
- en: Both log loss and hinge loss can be seen as continuous approximations to the
    0–1 loss.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失和铰链损失都可以看作是对0–1损失的连续近似。
- en: Multi-Class Classification Problems
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类分类问题
- en: In multi-class classification problems, the target label is 1 out of *k* classes.
    The label is usually encoded using one-hot encoding, i.e., as a binary *k*-dimensional
    vector **y** = (*y*₁, …, *yₖ*)*ᵗ*, where *yᵢ* = 1 for the true class *i* and 0
    elsewhere.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在多类分类问题中，目标标签是*k*个类别中的1个。标签通常使用独热编码进行编码，即作为一个二进制*k*维向量**y** = (*y*₁, …, *yₖ*)*ᵗ*，其中*yᵢ*
    = 1表示真实类别*i*，其余为0。
- en: 'A probabilistic classifier outputs for each sample a *k*-dimensional vector
    with probability estimates of each class: **p** = (*p*₁, …, *pₖ*)*ᵗ*. These probabilities
    sum to 1, i.e., *p*₁ + … + *pₖ* = 1.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 概率分类器为每个样本输出一个*k*维向量，其中包含每个类别的概率估计：**p** = (*p*₁, …, *pₖ*)*ᵗ*。这些概率的总和为1，即*p*₁
    + … + *pₖ* = 1。
- en: Cross-Entropy Loss
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: 'The loss function used to train such a classifier is called **cross-entropy
    loss**, which is an extension of log loss to the multi-class case. It is defined
    as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练这种分类器的损失函数称为**交叉熵损失**，它是对数损失在多类情况中的扩展。其定义如下：
- en: '![](../Images/2e427f6ddcc9387b6b90580074048758.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e427f6ddcc9387b6b90580074048758.png)'
- en: Cross-entropy loss
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: 'For example, assume that we have a three-class problem, the true class of our
    sample is class 2 (i.e., **y** = [0, 1, 0]), and the prediction of our model is
    **p** = [0.3, 0.6, 0.1]. Then the cross-entropy loss induced by this sample is:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个三类问题，我们的样本的真实类别是类别2（即**y** = [0, 1, 0]），我们的模型的预测是**p** = [0.3, 0.6,
    0.1]。那么该样本引发的交叉熵损失是：
- en: '![](../Images/1891f3a6b5b5e64882b4c8a882e70e41.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1891f3a6b5b5e64882b4c8a882e70e41.png)'
- en: 'To see how the cross-entropy loss generalizes log loss, notice that in the
    binary case *p*₁ = 1 - *p*₀ and *y*₁ = 1 - *y*₀, therefore we get:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解交叉熵损失如何推广对数损失，请注意在二分类情况下*p*₁ = 1 - *p*₀和*y*₁ = 1 - *y*₀，因此我们得到：
- en: '![](../Images/dd6a9d5681ba8fe5f17b7d8b13a405b1.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd6a9d5681ba8fe5f17b7d8b13a405b1.png)'
- en: which is exactly the log loss for *p* = *p*₀ and *y* = *y*₀.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是 *p* = *p*₀ 和 *y* = *y*₀ 时的对数损失。
- en: Similar to log loss, we can show that cross-entropy loss is the negative of
    the log-likelihood of the model, under the assumption that the labels are sampled
    from a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution)
    (a generalization of Bernoulli distribution to *k* possible outcomes).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于对数损失，我们可以证明交叉熵损失是模型对数似然的负值，前提是标签是从[类别分布](https://en.wikipedia.org/wiki/Categorical_distribution)（伯努利分布到
    *k* 个可能结果的推广）中抽样的。
- en: '**Proof**:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明**：'
- en: 'Given a model of the data (the labels) as a categorical distribution with probabilities
    **p** = (*p*₁, …, *pₖ*), the probability that a given sample belongs to class
    *i* is *pᵢ*:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个数据（标签）模型作为具有概率 **p** = (*p*₁, …, *pₖ*) 的类别分布，则给定样本属于类 *i* 的概率是 *pᵢ*：
- en: '![](../Images/f942e0a2fc1a0df23c29397166fc2d0e.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f942e0a2fc1a0df23c29397166fc2d0e.png)'
- en: 'Therefore, the probability that the true label of the sample is **y** is:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，样本的真实标签为**y**的概率是：
- en: '![](../Images/0d89058ed69762eaf26ec90e40a76d4d.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d89058ed69762eaf26ec90e40a76d4d.png)'
- en: 'Explanation: if the correct class of the given sample is i, then yᵢ = *1*,
    and for all j ≠ i, yⱼ = *0*. Hence, P*(****y***|***p****)* = pᵢ, which is the
    probability that the sample belongs to class i.'
  id: totrans-159
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 解释：如果给定样本的正确类别是 i，则 yᵢ = *1*，对于所有 j ≠ i，yⱼ = *0*。因此，P*(****y***|***p****)* =
    pᵢ，这就是样本属于类 i 的概率。
- en: 'Therefore, the log likelihood of our model is:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们模型的对数似然是：
- en: '![](../Images/94d51e236487cec80bc12c7e7b14f80e.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94d51e236487cec80bc12c7e7b14f80e.png)'
- en: The cross-entropy loss is exactly the negative of this function!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失正是这个函数的负值！
- en: Key Takeaways
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键要点
- en: In this article we have discussed various loss functions and showed how they
    are derived from basic principles such as maximum likelihood.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了各种损失函数，并展示了它们如何从最大似然等基本原理推导出来。
- en: In regression problems, squared loss is the most common loss function. However,
    if you suspect that your data set contains outliers, using Huber loss may be a
    better choice.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在回归问题中，平方损失是最常见的损失函数。然而，如果你怀疑数据集中存在异常值，使用Huber损失可能是更好的选择。
- en: In binary classification problems, the choice of a different loss function leads
    to a different classifier (logistic regression uses log loss while SVM uses hinge
    loss).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二分类问题中，选择不同的损失函数会导致不同的分类器（逻辑回归使用对数损失，而SVM使用铰链损失）。
- en: In multi-class classification problems, cross-entropy loss is the most common
    loss function and is extension of log loss to the multi-class case.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多类分类问题中，交叉熵损失是最常见的损失函数，并且是对数损失在多类情况下的扩展。
- en: '**Final Notes**'
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**最终说明**'
- en: All images unless otherwise noted are by the author.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则所有图片均由作者提供。
- en: 'The code examples of this article can be found on my github: [https://github.com/roiyeho/medium/tree/main/loss_functions](https://github.com/roiyeho/medium/tree/main/loss_functions)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的代码示例可以在我的 GitHub 上找到：[https://github.com/roiyeho/medium/tree/main/loss_functions](https://github.com/roiyeho/medium/tree/main/loss_functions)
- en: Thanks for reading!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
