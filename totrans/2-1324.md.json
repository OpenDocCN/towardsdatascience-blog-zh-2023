["```py\nfrom transformers import CLIPProcessor, CLIPModel\nimport torch\nimport requests\nfrom PIL import Image\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\nurl = \"https://images.freeimages.com/images/large-previews/342/green-tree-frog2-1616738.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a tree frog\", \"a photo of a tailed frog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image  # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\nprint(probs)\n\n\"\"\"\nOutput:\ntensor([[0.3164, 0.6836]], grad_fn=<SoftmaxBackward0>)\n\"\"\"\n```", "```py\n# define features description for each class\nfeatures = {\"tree frog\": [\n        \"protruding eyes\", \"large mouth\", \"without a tail\",  \"bright green colour\"\n    ],\n    \"tailed frog\": [\n        \"tiny eyes\", \"small mouth\", \"has long tail\", \"dark colour\"\n    ]}\n\n# image embedding\nimage_features = model.visual_projection(model.vision_model(inputs['pixel_values']).pooler_output)\n\ntree_frog_vector = model.text_model(processor(features['tree frog'], return_tensors=\"pt\", padding=True)['input_ids']).pooler_output\n# take the mean prompt embedding\ntree_frog_vector = tree_frog_vector.mean(dim=0, keepdims=True)\n# final projection \ntree_frog_vector = model.text_projection(tree_frog_vector)\n\ntailed_frog_vector = model.text_model(processor(features['tailed frog'], return_tensors=\"pt\", padding=True)['input_ids']).pooler_output\n# take the mean prompt embedding\ntailed_frog_vector = tailed_frog_vector.mean(dim=0, keepdims=True)\n# final projection\ntailed_frog_vector = model.text_projection(tailed_frog_vector)\n\n# concatenate \ntext_features = torch.cat([tree_frog_vector, tailed_frog_vector], dim=0)\n\n# normalize features\nimage_features = image_features / image_features.norm(dim=-1, keepdim=True)\ntext_features = text_features / text_features.norm(dim=-1, keepdim=True)\n\n# cosine similarity as logits\nlogit_scale = model.logit_scale.exp()\nlogits_per_image = logit_scale * image_features @ text_features.t()\nlogits_per_image.softmax(dim=1)\n\n\"\"\"\nOutput:\ntensor([[0.8901, 0.1099]], grad_fn=<SoftmaxBackward0>)\n\"\"\"\n```", "```py\n# here we don't average the textual features as we want to see\n# the score for each feature separately\ntree_frog_vector = model.text_model(processor(features['tree frog'], return_tensors=\"pt\", padding=True)['input_ids']).pooler_output\ntree_frog_vector = model.text_projection(tree_frog_vector)\ntext_features_tree_frog = tree_frog_vector\n\ntext_features_tree_frog = text_features_tree_frog / text_features_tree_frog.norm(dim=-1, keepdim=True)\nlogit_scale = model.logit_scale.exp()\nlogits_per_image = logit_scale * image_features @ text_features_tree_frog.t()\nlogits_per_image\n\n\"\"\"\nOutput:\ntensor([[25.5400, 22.6840, 21.3895, 25.9017]], grad_fn=<MmBackward0>\n\"\"\"\n\ntailed_frog_vector = model.text_model(processor(features['tailed frog'], return_tensors=\"pt\", padding=True)['input_ids']).pooler_output\ntailed_frog_vector = model.text_projection(tailed_frog_vector)\ntext_features_tailed_frog = tailed_frog_vector\n\ntext_features_tailed_frog = text_features_tailed_frog / text_features_tailed_frog.norm(dim=-1, keepdim=True)\nlogit_scale = model.logit_scale.exp()\nlogits_per_image = logit_scale * image_features @ text_features_tailed_frog.t()\nlogits_per_image\n\n\"\"\"\nOutput:\ntensor([[24.0911, 22.3996, 21.2813, 21.0066]], grad_fn=<MmBackward0>\n\"\"\" \n```"]