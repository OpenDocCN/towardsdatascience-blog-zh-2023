["```py\n!pip install -q bitsandbytes datasets accelerate loralib\n!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git\n```", "```py\n\"\"\"Importing dependencies and downloading pre-trained bloom model\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n\n#loading model\nmodel = AutoModelForCausalLM.from_pretrained(\n    # \"bigscience/bloom-3b\",\n    # \"bigscience/bloom-1b1\",\n    \"bigscience/bloom-560m\",\n    torch_dtype=torch.float16,\n    device_map='auto',\n)\n\n#loading tokenizer for this model (which turns text into an input for the model)\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/tokenizer\")\n```", "```py\n\"\"\"Setting up LoRA using parameter efficient fine tuning\n\"\"\"\n\nfrom peft import LoraConfig, get_peft_model\n\n#defining how LoRA will work in this particular example\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=8,\n    target_modules=[\"query_key_value\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n#this actually overwrites the model in memory, so\n#the rename is only for ledgibility.\npeft_model = get_peft_model(model, config)\n```", "```py\n\"\"\"Comparing parameters before and after LoRA\n\"\"\"\n\ntrainable_params = 0\nall_param = 0\n\n#iterating over all parameters\nfor _, param in peft_model.named_parameters():\n    #adding parameters to total\n    all_param += param.numel()\n    #adding parameters to trainable if they require a graident\n    if param.requires_grad:\n        trainable_params += param.numel()\n\n#printing results\nprint(f\"trainable params: {trainable_params}\")\nprint(f\"all params: {all_param}\")\nprint(f\"trainable: {100 * trainable_params / all_param:.2f}%\")\n```", "```py\n\"\"\"Loading SQUAD dataset\n\"\"\"\n\nfrom datasets import load_dataset\nqa_dataset = load_dataset(\"squad_v2\")\n```", "```py\n**CONTEXT:**\n{context}\n\n**QUESTION:**\n{question}\n\n**ANSWER:**\n{answer}</s>\n```", "```py\n\"\"\"Reformatting SQUAD to respect our defined structure\n\"\"\"\n\n#defining a function for reformatting\ndef create_prompt(context, question, answer):\n  if len(answer[\"text\"]) < 1:\n    answer = \"Cannot Find Answer\"\n  else:\n    answer = answer[\"text\"][0]\n  prompt_template = f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{question}\\n\\nANSWER:\\n{answer}</s>\"\n  return prompt_template\n\n#applying the reformatting function to the entire dataset\nmapped_qa_dataset = qa_dataset.map(lambda samples: tokenizer(create_prompt(samples['context'], samples['question'], samples['answers'])))\n```", "```py\n\"\"\"Fine Tuning\nThis code is largly co-opted. In the absence of a rigid validation\nprocedure, the best practice is to just copy a successful tutorial or,\nbetter yet, directly from the documentation.\n\"\"\"\n\nimport transformers\n\ntrainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=mapped_qa_dataset[\"train\"],\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        warmup_steps=100,\n        max_steps=100,\n        learning_rate=1e-3,\n        fp16=True,\n        logging_steps=1,\n        output_dir='outputs',\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\npeft_model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()\n```", "```py\n\"\"\"Saving the LoRA fine tuning locally\n\"\"\"\nmodel_id = \"BLOOM-560m-LoRA\"\npeft_model.save_pretrained(model_id)\n```", "```py\n!ls -lh {model_id}\n```", "```py\n\"\"\"Helper Function for Comparing Results\n\"\"\"\n\nfrom IPython.display import display, Markdown\n\ndef make_inference(context, question):\n\n    #turn the input into tokens\n    batch = tokenizer(f\"**CONTEXT:**\\n{context}\\n\\n**QUESTION:**\\n{question}\\n\\n**ANSWER:**\\n\", return_tensors='pt', return_token_type_ids=False)\n    #move the tokens onto the GPU, for inference\n    batch = batch.to(device='cuda')\n\n    #make an inference with both the fine tuned model and the raw model\n    with torch.cuda.amp.autocast():\n        #I think inference time would be faster if these were applied,\n        #but the fact that LoRA is not applied allows me to experiment\n        #with before and after fine tuning simultaniously\n\n        #raw model\n        peft_model.disable_adapter_layers()\n        output_tokens_raw = model.generate(**batch, max_new_tokens=200)\n\n        #LoRA model\n        peft_model.enable_adapter_layers()\n        output_tokens_qa = peft_model.generate(**batch, max_new_tokens=200)\n\n    #display results\n    display(Markdown(\"# Raw Model\\n\"))\n    display(Markdown((tokenizer.decode(output_tokens_raw[0], skip_special_tokens=True))))\n    display(Markdown(\"\\n# QA Model\\n\"))\n    display(Markdown((tokenizer.decode(output_tokens_qa[0], skip_special_tokens=True))))\n```", "```py\ncontext = \"You are a monster, and you eat yellow legos.\"\nquestion = \"What is the best food?\"\n\nmake_inference(context, question)\n```", "```py\ncontext = \"you are a math wizard\"\nquestion = \"what is 1+1 equal to?\"\n\nmake_inference(context, question)\n```", "```py\ncontext = \"Answer the riddle\"\nquestion = \"What gets bigger the more you take away?\"\n\nmake_inference(context, question)\n```"]