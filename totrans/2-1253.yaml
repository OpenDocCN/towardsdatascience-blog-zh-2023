- en: How to Train a Word2Vec Model from Scratch with Gensim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031](https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*In this article we will explore Gensim, a very popular Python library for
    training text-based machine learning models, to train a Word2Vec model from scratch*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)[![Andrea
    D''Agostino](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)
    [Andrea D''Agostino](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)
    ·9 min read·Feb 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5bacd1ddee7fde33777e0a5a0db4449.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec is a machine learning algorithm that allows you to create vector representations
    of words.
  prefs: []
  type: TYPE_NORMAL
- en: These representations, called **embeddings**, are used in many natural language
    processing tasks, such as word clustering, classification, and text generation.
  prefs: []
  type: TYPE_NORMAL
- en: The Word2Vec algorithm marked the beginning of an era in the NLP world when
    it was first introduced by Google in 2013.
  prefs: []
  type: TYPE_NORMAL
- en: It is based on word representations created by a neural network trained on very
    large data corpuses.
  prefs: []
  type: TYPE_NORMAL
- en: '**The output of Word2Vec are vectors**, one for each word in the training dictionary,
    **that effectively capture relationships between words.**'
  prefs: []
  type: TYPE_NORMAL
- en: Vectors that are close together in vector space have similar meanings based
    on context, and vectors that are far apart have different meanings. For example,
    the words “strong” and “mighty” would be close together while “strong” and “Paris”
    would be relatively far away within the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: This is a significant improvement over the performance of the bag-of-words model,
    which is based on simply counting the tokens present in a textual data corpus.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we will explore Gensim, a popular Python library for training
    text-based machine learning models, **to train a Word2Vec model from scratch.**
  prefs: []
  type: TYPE_NORMAL
- en: '**I will use the articles from my from my personal blog in Italian to act as
    a textual corpus for this project.** Feel free to use whatever corpus you wish
    — the pipeline is extendable.'
  prefs: []
  type: TYPE_NORMAL
- en: This approach is adaptable to any textual dataset. You’ll be able to create
    the embeddings yourself and visualize them.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: Project requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s draw up a list of actions to do that serve as foundations of the project.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll create a new virtual environment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(read here to understand how: [How to Set Up a Development Environment for
    Machine Learning](https://medium.com/towards-data-science/how-to-set-up-a-development-environment-for-machine-learning-b015a91bda8a))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Install the dependencies, among which Gensim
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare our corpus to deliver to Word2Vec
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model and save it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use TSNE and Plotly to visualize embeddings to visually understand the vector
    space generated by Word2Vec
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'BONUS: Use the Datapane library to create an interactive HTML report to share
    with whoever we want'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By the end of the article we will have in our hands an excellent basis for developing
    more complex reasoning, such as clustering of embeddings and more.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll assume you’ve already configured your environment correctly, so I won’t
    explain how to do it in this article. Let’s start right away with downloading
    the blog data.
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we begin let’s make sure to install the following project level dependencies
    by running `pip install XXXXX` in the terminal.
  prefs: []
  type: TYPE_NORMAL
- en: '`trafilatura`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gensim`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nltk`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tqdm`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plotly`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`datapane`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also initialize a `logger` object to receive Gensim messages in the
    terminal.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve the corpus data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned we will use the articles of my personal blog in Italian (diariodiunanalista.it)
    for our corpus data.
  prefs: []
  type: TYPE_NORMAL
- en: Here is how it appears in Deepnote.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0be5f55ccea2ca2d8e8545a200e88d73.png)'
  prefs: []
  type: TYPE_IMG
- en: The data we collected in pandas dataframe format. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The textual data that we are going to use is under the *article* column. Let’s
    see what a random text looks like
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41a7e00873bdb188020b356d98d410d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Regardless of language, this should be processed before being delivered to the
    Word2Vec model. We have to go and remove the Italian stopwords, clean up punctuation,
    numbers and other symbols. This will be the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation of the data corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing to do is to import some fundamental dependencies for preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s create a `preprocess_text` function that takes some text as input
    and returns a clean version of it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let’s apply this function to the Pandas dataframe by using a lambda function
    with `.apply`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We get a clean series.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/feb2922485c7bfe6346ff48ac57b5166.png)'
  prefs: []
  type: TYPE_IMG
- en: Each article has been cleaned and tokenized. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine a text to see the effect of our preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/224362b3e5ce4d5140ac7a71723ce7f6.png)'
  prefs: []
  type: TYPE_IMG
- en: How a single cleaned text appears. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The text now appears to be ready to be processed by Gensim. Let’s carry on.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing to do is create a variable `texts` that will contain our texts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to train the model. Word2Vec can accept many parameters, but
    let’s not worry about that for now. Training the model is straightforward, and
    requires one line of code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6f93c773e91a14eaabaa83292e1007ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Word2Vec training process. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Our model is ready and the embeddings have been created. To test this, let’s
    try to find the vector for the word *overfitting*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ba1d4e5abac985e48d348357263fe61.png)'
  prefs: []
  type: TYPE_IMG
- en: Word embeddings for the word “overfitting”. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Word2Vec creates 100-dimensional vectors. This parameter can be
    changed, along with many others, when we instantiate the class. In any case, the
    more dimensions associated with a word, **the more information the neural network
    will have about the word itself and its relationship to the others.**
  prefs: []
  type: TYPE_NORMAL
- en: Obviously this **has a higher computational and memory cost**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note: one of the most important limitations of Word2Vec **is the inability
    to generate vectors for words not present in the vocabulary** (called OOV — out
    of vocabulary words).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4a0288cbf1c6967704a7d7432d26fde.png)'
  prefs: []
  type: TYPE_IMG
- en: A major limitation of W2V is the inability to map embeddings for out of vocabulary
    words. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: To handle new words, therefore, we’ll have to either train a new model or add
    vectors manually.
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the similarity between two words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the cosine similarity we can calculate how far apart the vectors are in
    space.
  prefs: []
  type: TYPE_NORMAL
- en: With the command below we instruct Gensim to find the first 3 words most similar
    to *overfitting*
  prefs: []
  type: TYPE_NORMAL
- en: '`model.wv.most_similar(positive=[''overfitting''], topn=3))`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ad8cacaf04f80ca5b292fe6366dbf5d.png)'
  prefs: []
  type: TYPE_IMG
- en: The most similar words to “*overfitting”. Image by author.*
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how the word “when” (*quando* in Italian)is present in this result.
    It will be appropriate to include similar adverbs in the stop words to clean up
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: To save the model, just do `model.save("./path/to/model")`.
  prefs: []
  type: TYPE_NORMAL
- en: Visualize embeddings with TSNE and Plotly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our vectors are 100-dimensional. It’s a problem to visualize them unless we
    do something to **reduce their dimensionality.**
  prefs: []
  type: TYPE_NORMAL
- en: We will use the TSNE, a technique to reduce the dimensionality of the vectors
    and create two components, one for the X axis and one for the Y axis on a scatterplot.
  prefs: []
  type: TYPE_NORMAL
- en: In the .gif below you can see the words embedded in the space thanks to the
    Plotly features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfda99275b8d7c9ac1e38fcb2e3c86f4.png)'
  prefs: []
  type: TYPE_IMG
- en: How embeddings of my Italian blog appear in TSNE projection. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the code to generate this image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This visualization can be useful for noticing semantic and syntactic tendencies
    in your data.
  prefs: []
  type: TYPE_NORMAL
- en: '**For example, it’s very useful for pointing out anomalies**, such as groups
    of words that tend to clump together for some reason.'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters of Word2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By checking on the Gensim website we see that there are many parameters that
    Word2Vec accepts. The most important ones are `vectors_size`, `min_count`, `window`
    and `sg`.
  prefs: []
  type: TYPE_NORMAL
- en: '**vectors_size** : defines the dimensions of our vector space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**min_count**: Words below the min_count frequency are removed from the vocabulary
    before training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**window**: maximum distance between the current and the expected word within
    a sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sg**: defines the training algorithm. 0 = CBOW (continuous bag of words),
    1 = Skip-Gram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We won’t go into detail on each of these. I suggest the interested reader to
    take a look at the [Gensim documentation](https://radimrehurek.com/gensim/models/word2vec.html).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to retrain our model with the following parameters
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/91cc372f6337a96de2699621933721b3.png)'
  prefs: []
  type: TYPE_IMG
- en: A new projection based on the new parameters for Word2Vec. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The representation changes a lot. The number of vectors is the same as before
    (Word2Vec defaults to 100), while `min_count`, `window` and `sg` have been changed
    from their defaults.
  prefs: []
  type: TYPE_NORMAL
- en: I suggest to the reader to change these parameters in order to understand which
    representation is more suitable for his own case.
  prefs: []
  type: TYPE_NORMAL
- en: 'BONUS: Create an interactive report with Datapane'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have reached the end of the article. We conclude the project by creating
    **an interactive report in HTML with Datapane, which will allow the user to view
    the graph previously created with Plotly directly in the browser.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91aad09bfe32588f15ed2eb21f4d9e82.png)'
  prefs: []
  type: TYPE_IMG
- en: Creation of an interactive report with Datapane. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: This is the Python code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Datapane is highly customizable. I advise the reader to study the documentation
    to integrate aesthetics and other features.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how to build embeddings from scratch using Gensim and Word2Vec.
    This is very simple to do if you have a structured dataset and if you know the
    Gensim API.
  prefs: []
  type: TYPE_NORMAL
- en: With embeddings we can really do many things, for example
  prefs: []
  type: TYPE_NORMAL
- en: do **document clustering**, displaying these clusters in vector space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**research similarities** between words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use embeddings **as features in a machine learning model**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lay the foundations for **machine translation**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and so on. If you are interested in a topic that extends the one covered here,
    leave a comment and let me know 👍
  prefs: []
  type: TYPE_NORMAL
- en: With this project you can enrich your portfolio of NLP templates and communicate
    to a stakeholder expertise in dealing with textual documents in the context of
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: To the next article 👋
  prefs: []
  type: TYPE_NORMAL
- en: '**If you want to support my content creation activity, feel free to follow
    my referral link below and join Medium’s membership program**. I will receive
    a portion of your investment and you’ll be able to access Medium’s plethora of
    articles on data science and more in a seamless way.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@theDrewDag/membership?source=post_page-----c457d587e031--------------------------------)
    [## Join Medium with my referral link - Andrea D''Agostino'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Andrea D'Agostino (and thousands of other writers on Medium).
    Your membership fee directly…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@theDrewDag/membership?source=post_page-----c457d587e031--------------------------------)
  prefs: []
  type: TYPE_NORMAL
