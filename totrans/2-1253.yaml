- en: How to Train a Word2Vec Model from Scratch with Gensim
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä½¿ç”¨Gensimä»å¤´å¼€å§‹è®­ç»ƒWord2Vecæ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031](https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031](https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031)
- en: '*In this article we will explore Gensim, a very popular Python library for
    training text-based machine learning models, to train a Word2Vec model from scratch*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢Gensimï¼Œè¿™æ˜¯ä¸€ç§éå¸¸æµè¡Œçš„Pythonåº“ï¼Œç”¨äºè®­ç»ƒåŸºäºæ–‡æœ¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»¥ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªWord2Vecæ¨¡å‹*'
- en: '[](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)[![Andrea
    D''Agostino](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)
    [Andrea D''Agostino](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)[![Andrea
    D''Agostino](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)
    [Andrea D''Agostino](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)
    Â·9 min readÂ·Feb 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)
    Â·9åˆ†é’Ÿé˜…è¯»Â·2023å¹´2æœˆ6æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f5bacd1ddee7fde33777e0a5a0db4449.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5bacd1ddee7fde33777e0a5a0db4449.png)'
- en: Image by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒã€‚
- en: Word2Vec is a machine learning algorithm that allows you to create vector representations
    of words.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vecæ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå®ƒå…è®¸ä½ åˆ›å»ºå•è¯çš„å‘é‡è¡¨ç¤ºã€‚
- en: These representations, called **embeddings**, are used in many natural language
    processing tasks, such as word clustering, classification, and text generation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è¡¨ç¤ºè¢«ç§°ä¸º**åµŒå…¥**ï¼Œåœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ä½¿ç”¨ï¼Œå¦‚å•è¯èšç±»ã€åˆ†ç±»å’Œæ–‡æœ¬ç”Ÿæˆã€‚
- en: The Word2Vec algorithm marked the beginning of an era in the NLP world when
    it was first introduced by Google in 2013.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vecç®—æ³•æ ‡å¿—ç€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸–ç•Œçš„ä¸€ä¸ªæ—¶ä»£çš„å¼€å§‹ï¼Œå®ƒåœ¨2013å¹´ç”±Googleé¦–æ¬¡ä»‹ç»ã€‚
- en: It is based on word representations created by a neural network trained on very
    large data corpuses.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåŸºäºé€šè¿‡ç¥ç»ç½‘ç»œè®­ç»ƒçš„éå¸¸å¤§çš„æ•°æ®è¯­æ–™åº“åˆ›å»ºçš„å•è¯è¡¨ç¤ºã€‚
- en: '**The output of Word2Vec are vectors**, one for each word in the training dictionary,
    **that effectively capture relationships between words.**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**Word2Vecçš„è¾“å‡ºæ˜¯å‘é‡**ï¼Œæ¯ä¸ªå‘é‡å¯¹åº”è®­ç»ƒè¯å…¸ä¸­çš„ä¸€ä¸ªå•è¯ï¼Œ**æœ‰æ•ˆåœ°æ•æ‰äº†å•è¯ä¹‹é—´çš„å…³ç³»ã€‚**'
- en: Vectors that are close together in vector space have similar meanings based
    on context, and vectors that are far apart have different meanings. For example,
    the words â€œstrongâ€ and â€œmightyâ€ would be close together while â€œstrongâ€ and â€œParisâ€
    would be relatively far away within the vector space.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‘é‡ç©ºé—´ä¸­ç›¸äº’æ¥è¿‘çš„å‘é‡åŸºäºä¸Šä¸‹æ–‡å…·æœ‰ç›¸ä¼¼çš„å«ä¹‰ï¼Œè€Œç›¸è·è¾ƒè¿œçš„å‘é‡å…·æœ‰ä¸åŒçš„å«ä¹‰ã€‚ä¾‹å¦‚ï¼Œâ€œå¼ºå¤§â€å’Œâ€œé›„ä¼Ÿâ€è¿™ä¸¤ä¸ªè¯åœ¨å‘é‡ç©ºé—´ä¸­ä¼šå¾ˆæ¥è¿‘ï¼Œè€Œâ€œå¼ºå¤§â€å’Œâ€œå·´é»â€è¿™ä¸¤ä¸ªè¯åˆ™ä¼šç›¸å¯¹è¿œç¦»ã€‚
- en: This is a significant improvement over the performance of the bag-of-words model,
    which is based on simply counting the tokens present in a textual data corpus.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¯”åŸºäºç®€å•è®¡ç®—æ–‡æœ¬æ•°æ®è¯­æ–™åº“ä¸­å­˜åœ¨çš„æ ‡è®°çš„è¢‹è£…å•è¯æ¨¡å‹æ€§èƒ½æœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚
- en: In this article we will explore Gensim, a popular Python library for training
    text-based machine learning models, **to train a Word2Vec model from scratch.**
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢Gensimï¼Œè¿™æ˜¯ä¸€ç§æµè¡Œçš„Pythonåº“ï¼Œç”¨äºè®­ç»ƒåŸºäºæ–‡æœ¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œ**ä»¥ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªWord2Vecæ¨¡å‹ã€‚**
- en: '**I will use the articles from my from my personal blog in Italian to act as
    a textual corpus for this project.** Feel free to use whatever corpus you wish
    â€” the pipeline is extendable.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**æˆ‘å°†ä½¿ç”¨æˆ‘ä¸ªäººåšå®¢ä¸­çš„æ„å¤§åˆ©è¯­æ–‡ç« ä½œä¸ºæœ¬é¡¹ç›®çš„æ–‡æœ¬è¯­æ–™åº“ã€‚** éšæ„ä½¿ç”¨ä½ å¸Œæœ›çš„ä»»ä½•è¯­æ–™åº“â€”â€”è¿™ä¸ªæµç¨‹æ˜¯å¯æ‰©å±•çš„ã€‚'
- en: This approach is adaptable to any textual dataset. Youâ€™ll be able to create
    the embeddings yourself and visualize them.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•é€‚ç”¨äºä»»ä½•æ–‡æœ¬æ•°æ®é›†ã€‚ä½ å°†èƒ½å¤Ÿè‡ªå·±åˆ›å»ºåµŒå…¥å‘é‡å¹¶è¿›è¡Œå¯è§†åŒ–ã€‚
- en: Letâ€™s begin!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: Project requirements
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¡¹ç›®è¦æ±‚
- en: Letâ€™s draw up a list of actions to do that serve as foundations of the project.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ—å‡ºä¸€äº›è¡ŒåŠ¨æ­¥éª¤ï¼Œè¿™äº›æ­¥éª¤å°†ä½œä¸ºé¡¹ç›®çš„åŸºç¡€ã€‚
- en: Weâ€™ll create a new virtual environment
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ–°çš„è™šæ‹Ÿç¯å¢ƒã€‚
- en: '(read here to understand how: [How to Set Up a Development Environment for
    Machine Learning](https://medium.com/towards-data-science/how-to-set-up-a-development-environment-for-machine-learning-b015a91bda8a))'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ï¼ˆé˜…è¯»è¿™é‡Œä»¥äº†è§£å¦‚ä½•æ“ä½œï¼š[å¦‚ä½•ä¸ºæœºå™¨å­¦ä¹ è®¾ç½®å¼€å‘ç¯å¢ƒ](https://medium.com/towards-data-science/how-to-set-up-a-development-environment-for-machine-learning-b015a91bda8a)ï¼‰
- en: Install the dependencies, among which Gensim
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®‰è£…ä¾èµ–é¡¹ï¼Œå…¶ä¸­åŒ…æ‹¬Gensim
- en: Prepare our corpus to deliver to Word2Vec
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‡†å¤‡æˆ‘ä»¬çš„è¯­æ–™åº“ä»¥äº¤ç»™Word2Vec
- en: Train the model and save it
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜å®ƒ
- en: Use TSNE and Plotly to visualize embeddings to visually understand the vector
    space generated by Word2Vec
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨TSNEå’ŒPlotlyæ¥å¯è§†åŒ–åµŒå…¥ï¼Œä»¥ä¾¿ç›´è§‚ç†è§£Word2Vecç”Ÿæˆçš„å‘é‡ç©ºé—´ã€‚
- en: 'BONUS: Use the Datapane library to create an interactive HTML report to share
    with whoever we want'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é™„åŠ ï¼šä½¿ç”¨Datapaneåº“åˆ›å»ºä¸€ä¸ªäº’åŠ¨HTMLæŠ¥å‘Šï¼Œä¸æˆ‘ä»¬æƒ³è¦åˆ†äº«çš„äººå…±äº«
- en: By the end of the article we will have in our hands an excellent basis for developing
    more complex reasoning, such as clustering of embeddings and more.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æ–‡ç« ç»“å°¾æ—¶ï¼Œæˆ‘ä»¬å°†æ‹¥æœ‰ä¸€ä¸ªå¾ˆå¥½çš„åŸºç¡€ï¼Œä»¥ä¾¿å¼€å±•æ›´å¤æ‚çš„æ¨ç†ï¼Œä¾‹å¦‚åµŒå…¥çš„èšç±»ç­‰ã€‚
- en: Iâ€™ll assume youâ€™ve already configured your environment correctly, so I wonâ€™t
    explain how to do it in this article. Letâ€™s start right away with downloading
    the blog data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å‡è®¾ä½ å·²ç»æ­£ç¡®é…ç½®äº†ä½ çš„ç¯å¢ƒï¼Œæ‰€ä»¥åœ¨è¿™ç¯‡æ–‡ç« ä¸­æˆ‘ä¸ä¼šè§£é‡Šå¦‚ä½•æ“ä½œã€‚æˆ‘ä»¬ç›´æ¥å¼€å§‹ä¸‹è½½åšå®¢æ•°æ®å§ã€‚
- en: Dependencies
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¾èµ–é¡¹
- en: Before we begin letâ€™s make sure to install the following project level dependencies
    by running `pip install XXXXX` in the terminal.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡åœ¨ç»ˆç«¯ä¸­è¿è¡Œ`pip install XXXXX`æ¥ç¡®ä¿å®‰è£…ä»¥ä¸‹é¡¹ç›®çº§åˆ«çš„ä¾èµ–ã€‚
- en: '`trafilatura`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trafilatura`'
- en: '`pandas`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`gensim`'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gensim`'
- en: '`nltk`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nltk`'
- en: '`tqdm`'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tqdm`'
- en: '`scikit-learn`'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`'
- en: '`plotly`'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plotly`'
- en: '`datapane`'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datapane`'
- en: We will also initialize a `logger` object to receive Gensim messages in the
    terminal.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å°†åˆå§‹åŒ–ä¸€ä¸ª`logger`å¯¹è±¡ï¼Œä»¥ä¾¿åœ¨ç»ˆç«¯æ¥æ”¶Gensimæ¶ˆæ¯ã€‚
- en: Retrieve the corpus data
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è·å–è¯­æ–™åº“æ•°æ®
- en: As mentioned we will use the articles of my personal blog in Italian (diariodiunanalista.it)
    for our corpus data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æˆ‘ä¸ªäººåšå®¢ä¸­çš„æ„å¤§åˆ©è¯­æ–‡ç« ï¼ˆdiariodiunanalista.itï¼‰ä½œä¸ºæˆ‘ä»¬çš„è¯­æ–™åº“æ•°æ®ã€‚
- en: Here is how it appears in Deepnote.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Deepnoteä¸­å®ƒçš„æ ·å­æ˜¯è¿™æ ·çš„ã€‚
- en: '![](../Images/0be5f55ccea2ca2d8e8545a200e88d73.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0be5f55ccea2ca2d8e8545a200e88d73.png)'
- en: The data we collected in pandas dataframe format. Image by author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»¥pandasæ•°æ®æ¡†æ ¼å¼æ”¶é›†çš„æ•°æ®ã€‚å›¾ç‰‡æ¥æºäºä½œè€…ã€‚
- en: The textual data that we are going to use is under the *article* column. Letâ€™s
    see what a random text looks like
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¦ä½¿ç”¨çš„æ–‡æœ¬æ•°æ®ä½äº*article*åˆ—ä¸­ã€‚è®©æˆ‘ä»¬çœ‹çœ‹éšæœºæ–‡æœ¬çš„æ ·å­ã€‚
- en: '![](../Images/41a7e00873bdb188020b356d98d410d9.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41a7e00873bdb188020b356d98d410d9.png)'
- en: Regardless of language, this should be processed before being delivered to the
    Word2Vec model. We have to go and remove the Italian stopwords, clean up punctuation,
    numbers and other symbols. This will be the next step.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºè¯­è¨€å¦‚ä½•ï¼Œè¿™äº›æ•°æ®åœ¨äº¤ç»™Word2Vecæ¨¡å‹ä¹‹å‰éƒ½åº”è¿›è¡Œå¤„ç†ã€‚æˆ‘ä»¬éœ€è¦å»é™¤æ„å¤§åˆ©è¯­åœç”¨è¯ï¼Œæ¸…ç†æ ‡ç‚¹ç¬¦å·ã€æ•°å­—å’Œå…¶ä»–ç¬¦å·ã€‚è¿™å°†æ˜¯ä¸‹ä¸€æ­¥ã€‚
- en: Preparation of the data corpus
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®è¯­æ–™åº“çš„å‡†å¤‡
- en: The first thing to do is to import some fundamental dependencies for preprocessing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆéœ€è¦å¯¼å…¥ä¸€äº›ç”¨äºé¢„å¤„ç†çš„åŸºæœ¬ä¾èµ–ã€‚
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now letâ€™s create a `preprocess_text` function that takes some text as input
    and returns a clean version of it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª`preprocess_text`å‡½æ•°ï¼Œå®ƒæ¥æ”¶ä¸€äº›æ–‡æœ¬ä½œä¸ºè¾“å…¥å¹¶è¿”å›ä¸€ä¸ªå¹²å‡€çš„ç‰ˆæœ¬ã€‚
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Letâ€™s apply this function to the Pandas dataframe by using a lambda function
    with `.apply`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ä½¿ç”¨å¸¦æœ‰`.apply`çš„lambdaå‡½æ•°å°†è¿™ä¸ªå‡½æ•°åº”ç”¨åˆ°Pandasæ•°æ®æ¡†ä¸­ã€‚
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We get a clean series.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªå¹²å‡€çš„ç³»åˆ—ã€‚
- en: '![](../Images/feb2922485c7bfe6346ff48ac57b5166.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/feb2922485c7bfe6346ff48ac57b5166.png)'
- en: Each article has been cleaned and tokenized. Image by author.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ç¯‡æ–‡ç« éƒ½ç»è¿‡äº†æ¸…ç†å’Œåˆ†è¯å¤„ç†ã€‚å›¾ç‰‡æ¥æºäºä½œè€…ã€‚
- en: Letâ€™s examine a text to see the effect of our preprocessing.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ–‡æœ¬ï¼Œçœ‹çœ‹æˆ‘ä»¬é¢„å¤„ç†çš„æ•ˆæœã€‚
- en: '![](../Images/224362b3e5ce4d5140ac7a71723ce7f6.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/224362b3e5ce4d5140ac7a71723ce7f6.png)'
- en: How a single cleaned text appears. Image by author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å•ä¸ªæ¸…ç†åçš„æ–‡æœ¬çš„æ ·å­ã€‚å›¾ç‰‡æ¥æºäºä½œè€…ã€‚
- en: The text now appears to be ready to be processed by Gensim. Letâ€™s carry on.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ–‡æœ¬çœ‹èµ·æ¥å·²ç»å‡†å¤‡å¥½è¢«Gensimå¤„ç†äº†ã€‚æˆ‘ä»¬ç»§ç»­ã€‚
- en: Word2Vec training
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vecè®­ç»ƒ
- en: The first thing to do is create a variable `texts` that will contain our texts.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆè¦åšçš„æ˜¯åˆ›å»ºä¸€ä¸ªå˜é‡`texts`ï¼Œå®ƒå°†åŒ…å«æˆ‘ä»¬çš„æ–‡æœ¬ã€‚
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We are now ready to train the model. Word2Vec can accept many parameters, but
    letâ€™s not worry about that for now. Training the model is straightforward, and
    requires one line of code.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½è®­ç»ƒæ¨¡å‹äº†ã€‚Word2Vecå¯ä»¥æ¥å—è®¸å¤šå‚æ•°ï¼Œä½†ç°åœ¨æˆ‘ä»¬ä¸å¿…æ‹…å¿ƒè¿™äº›ã€‚è®­ç»ƒæ¨¡å‹éå¸¸ç®€å•ï¼Œåªéœ€è¦ä¸€è¡Œä»£ç ã€‚
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/6f93c773e91a14eaabaa83292e1007ba.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f93c773e91a14eaabaa83292e1007ba.png)'
- en: Word2Vec training process. Image by author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vecè®­ç»ƒè¿‡ç¨‹ã€‚å›¾ç‰‡æ¥æºäºä½œè€…ã€‚
- en: Our model is ready and the embeddings have been created. To test this, letâ€™s
    try to find the vector for the word *overfitting*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ¨¡å‹å·²ç»å‡†å¤‡å¥½ï¼Œå¹¶ä¸”åµŒå…¥å·²ç»åˆ›å»ºã€‚ä¸ºäº†æµ‹è¯•è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬å°è¯•æ‰¾å‡ºå•è¯*overfitting*çš„å‘é‡ã€‚
- en: '![](../Images/0ba1d4e5abac985e48d348357263fe61.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ba1d4e5abac985e48d348357263fe61.png)'
- en: Word embeddings for the word â€œoverfittingâ€. Image by author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: â€œoverfittingâ€ä¸€è¯çš„è¯åµŒå…¥ã€‚å›¾åƒæ¥æºï¼šä½œè€…ã€‚
- en: By default, Word2Vec creates 100-dimensional vectors. This parameter can be
    changed, along with many others, when we instantiate the class. In any case, the
    more dimensions associated with a word, **the more information the neural network
    will have about the word itself and its relationship to the others.**
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼ŒWord2Vecåˆ›å»º100ç»´çš„å‘é‡ã€‚è¿™ä¸ªå‚æ•°å¯ä»¥æ›´æ”¹ï¼Œè¿˜æœ‰è®¸å¤šå…¶ä»–å‚æ•°ï¼Œåœ¨æˆ‘ä»¬å®ä¾‹åŒ–ç±»æ—¶è¿›è¡Œè®¾ç½®ã€‚æ— è®ºå¦‚ä½•ï¼Œå…³è”çš„ç»´åº¦è¶Šå¤šï¼Œ**ç¥ç»ç½‘ç»œå¯¹å•è¯æœ¬èº«åŠå…¶ä¸å…¶ä»–å•è¯å…³ç³»çš„äº†è§£å°±è¶Šå¤šã€‚**
- en: Obviously this **has a higher computational and memory cost**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œè¿™**å…·æœ‰æ›´é«˜çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬**ã€‚
- en: 'Please note: one of the most important limitations of Word2Vec **is the inability
    to generate vectors for words not present in the vocabulary** (called OOV â€” out
    of vocabulary words).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼šWord2Vec**æ— æ³•ä¸ºè¯æ±‡è¡¨ä¸­æœªå‡ºç°çš„å•è¯ç”Ÿæˆå‘é‡**ï¼ˆç§°ä¸ºOOV â€” è¯æ±‡å¤–å•è¯ï¼‰æ˜¯å…¶æœ€é‡è¦çš„é™åˆ¶ä¹‹ä¸€ã€‚
- en: '![](../Images/e4a0288cbf1c6967704a7d7432d26fde.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4a0288cbf1c6967704a7d7432d26fde.png)'
- en: A major limitation of W2V is the inability to map embeddings for out of vocabulary
    words. Image by author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: W2Vçš„ä¸€ä¸ªä¸»è¦é™åˆ¶æ˜¯æ— æ³•ä¸ºè¯æ±‡å¤–å•è¯æ˜ å°„åµŒå…¥ã€‚å›¾åƒæ¥æºï¼šä½œè€…ã€‚
- en: To handle new words, therefore, weâ€™ll have to either train a new model or add
    vectors manually.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸ºäº†å¤„ç†æ–°å•è¯ï¼Œæˆ‘ä»¬å¿…é¡»è®­ç»ƒä¸€ä¸ªæ–°æ¨¡å‹æˆ–æ‰‹åŠ¨æ·»åŠ å‘é‡ã€‚
- en: Calculate the similarity between two words
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®¡ç®—ä¸¤ä¸ªå•è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦
- en: With the cosine similarity we can calculate how far apart the vectors are in
    space.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‘é‡åœ¨ç©ºé—´ä¸­çš„è·ç¦»ã€‚
- en: With the command below we instruct Gensim to find the first 3 words most similar
    to *overfitting*
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤ï¼Œæˆ‘ä»¬æŒ‡ç¤ºGensimæŸ¥æ‰¾ä¸*overfitting*æœ€ç›¸ä¼¼çš„å‰ä¸‰ä¸ªå•è¯
- en: '`model.wv.most_similar(positive=[''overfitting''], topn=3))`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.wv.most_similar(positive=[''overfitting''], topn=3))`'
- en: '![](../Images/1ad8cacaf04f80ca5b292fe6366dbf5d.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ad8cacaf04f80ca5b292fe6366dbf5d.png)'
- en: The most similar words to â€œ*overfittingâ€. Image by author.*
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸â€œ*overfitting*â€æœ€ç›¸ä¼¼çš„è¯ã€‚å›¾åƒæ¥æºï¼šä½œè€…ã€‚
- en: Letâ€™s see how the word â€œwhenâ€ (*quando* in Italian)is present in this result.
    It will be appropriate to include similar adverbs in the stop words to clean up
    the results.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å•è¯â€œwhenâ€(*quando* åœ¨æ„å¤§åˆ©è¯­ä¸­)åœ¨è¿™ä¸ªç»“æœä¸­æ˜¯å¦‚ä½•å‘ˆç°çš„ã€‚å°†ç±»ä¼¼çš„å‰¯è¯åŒ…å«åœ¨åœç”¨è¯ä¸­ä»¥æ¸…ç†ç»“æœä¼šæ¯”è¾ƒåˆé€‚ã€‚
- en: To save the model, just do `model.save("./path/to/model")`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä¿å­˜æ¨¡å‹ï¼Œåªéœ€æ‰§è¡Œ`model.save("./path/to/model")`ã€‚
- en: Visualize embeddings with TSNE and Plotly
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨TSNEå’ŒPlotlyå¯è§†åŒ–åµŒå…¥
- en: Our vectors are 100-dimensional. Itâ€™s a problem to visualize them unless we
    do something to **reduce their dimensionality.**
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å‘é‡æ˜¯100ç»´çš„ã€‚é™¤éæˆ‘ä»¬åšäº›äº‹æƒ…æ¥**å‡å°‘å®ƒä»¬çš„ç»´åº¦**ï¼Œå¦åˆ™å¯è§†åŒ–å®ƒä»¬æ˜¯ä¸ªé—®é¢˜ã€‚
- en: We will use the TSNE, a technique to reduce the dimensionality of the vectors
    and create two components, one for the X axis and one for the Y axis on a scatterplot.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨TSNEï¼Œè¿™æ˜¯ä¸€ç§é™ä½å‘é‡ç»´åº¦çš„æŠ€æœ¯ï¼Œå¹¶åˆ›å»ºä¸¤ä¸ªç»„ä»¶ï¼Œä¸€ä¸ªç”¨äºXè½´ï¼Œå¦ä¸€ä¸ªç”¨äºYè½´ä¸Šçš„æ•£ç‚¹å›¾ã€‚
- en: In the .gif below you can see the words embedded in the space thanks to the
    Plotly features.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹é¢çš„.gifä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°æ„Ÿè°¢PlotlyåŠŸèƒ½åµŒå…¥åœ¨ç©ºé—´ä¸­çš„å•è¯ã€‚
- en: '![](../Images/bfda99275b8d7c9ac1e38fcb2e3c86f4.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfda99275b8d7c9ac1e38fcb2e3c86f4.png)'
- en: How embeddings of my Italian blog appear in TSNE projection. Image by author.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ„å¤§åˆ©åšå®¢çš„åµŒå…¥åœ¨TSNEæŠ•å½±ä¸­å‘ˆç°çš„æ•ˆæœã€‚å›¾åƒæ¥æºï¼šä½œè€…ã€‚
- en: Here is the code to generate this image.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”Ÿæˆæ­¤å›¾åƒçš„ä»£ç ã€‚
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This visualization can be useful for noticing semantic and syntactic tendencies
    in your data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å¯è§†åŒ–æœ‰åŠ©äºè¯†åˆ«æ•°æ®ä¸­çš„è¯­ä¹‰å’Œå¥æ³•å€¾å‘ã€‚
- en: '**For example, itâ€™s very useful for pointing out anomalies**, such as groups
    of words that tend to clump together for some reason.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¾‹å¦‚ï¼Œè¿™éå¸¸æœ‰åŠ©äºæŒ‡å‡ºå¼‚å¸¸**ï¼Œå¦‚å› æŸç§åŸå› å€¾å‘äºèšé›†åœ¨ä¸€èµ·çš„å•è¯ç»„ã€‚'
- en: Parameters of Word2Vec
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vecçš„å‚æ•°
- en: By checking on the Gensim website we see that there are many parameters that
    Word2Vec accepts. The most important ones are `vectors_size`, `min_count`, `window`
    and `sg`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ£€æŸ¥Gensimç½‘ç«™ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°Word2Vecæ¥å—è®¸å¤šå‚æ•°ã€‚æœ€é‡è¦çš„å‚æ•°åŒ…æ‹¬`vectors_size`ã€`min_count`ã€`window`å’Œ`sg`ã€‚
- en: '**vectors_size** : defines the dimensions of our vector space.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vectors_size** : å®šä¹‰äº†æˆ‘ä»¬å‘é‡ç©ºé—´çš„ç»´åº¦ã€‚'
- en: '**min_count**: Words below the min_count frequency are removed from the vocabulary
    before training.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_count**: è®­ç»ƒå‰ï¼Œä¼šå°†é¢‘ç‡ä½äºmin_countçš„å•è¯ä»è¯æ±‡è¡¨ä¸­ç§»é™¤ã€‚'
- en: '**window**: maximum distance between the current and the expected word within
    a sentence.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**window**: å¥å­ä¸­å½“å‰è¯ä¸æœŸæœ›è¯ä¹‹é—´çš„æœ€å¤§è·ç¦»ã€‚'
- en: '**sg**: defines the training algorithm. 0 = CBOW (continuous bag of words),
    1 = Skip-Gram.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sg**ï¼šå®šä¹‰è®­ç»ƒç®—æ³•ã€‚0 = CBOWï¼ˆè¿ç»­è¯è¢‹ï¼‰ï¼Œ1 = Skip-Gramã€‚'
- en: We wonâ€™t go into detail on each of these. I suggest the interested reader to
    take a look at the [Gensim documentation](https://radimrehurek.com/gensim/models/word2vec.html).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä¼šè¯¦ç»†è®¨è®ºæ¯ä¸€ä¸ªã€‚æˆ‘å»ºè®®æ„Ÿå…´è¶£çš„è¯»è€…æŸ¥çœ‹[Gensimæ–‡æ¡£](https://radimrehurek.com/gensim/models/word2vec.html)ã€‚
- en: Letâ€™s try to retrain our model with the following parameters
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°è¯•ä½¿ç”¨ä»¥ä¸‹å‚æ•°é‡æ–°è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/91cc372f6337a96de2699621933721b3.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91cc372f6337a96de2699621933721b3.png)'
- en: A new projection based on the new parameters for Word2Vec. Image by author.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºWord2Vecæ–°å‚æ•°çš„æ–°æŠ•å½±ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: The representation changes a lot. The number of vectors is the same as before
    (Word2Vec defaults to 100), while `min_count`, `window` and `sg` have been changed
    from their defaults.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºå‘ç”Ÿäº†å¾ˆå¤§å˜åŒ–ã€‚å‘é‡çš„æ•°é‡ä¸ä¹‹å‰ç›¸åŒï¼ˆWord2Vecçš„é»˜è®¤å€¼ä¸º100ï¼‰ï¼Œè€Œ`min_count`ã€`window`å’Œ`sg`å·²ä»é»˜è®¤å€¼è¿›è¡Œäº†æ›´æ”¹ã€‚
- en: I suggest to the reader to change these parameters in order to understand which
    representation is more suitable for his own case.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å»ºè®®è¯»è€…æ›´æ”¹è¿™äº›å‚æ•°ï¼Œä»¥äº†è§£å“ªç§è¡¨ç¤ºæ›´é€‚åˆè‡ªå·±çš„æƒ…å†µã€‚
- en: 'BONUS: Create an interactive report with Datapane'
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¥–é‡‘ï¼šä½¿ç”¨Datapaneåˆ›å»ºä¸€ä¸ªäº’åŠ¨æŠ¥å‘Š
- en: We have reached the end of the article. We conclude the project by creating
    **an interactive report in HTML with Datapane, which will allow the user to view
    the graph previously created with Plotly directly in the browser.**
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»åˆ°äº†æ–‡ç« çš„ç»“å°¾ã€‚æˆ‘ä»¬é€šè¿‡åˆ›å»º**ä¸€ä¸ªHTMLæ ¼å¼çš„äº’åŠ¨æŠ¥å‘Šï¼Œåˆ©ç”¨Datapaneä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨æµè§ˆå™¨ä¸­ç›´æ¥æŸ¥çœ‹ä¹‹å‰ç”¨Plotlyåˆ›å»ºçš„å›¾è¡¨**æ¥ç»“æŸè¿™ä¸ªé¡¹ç›®ã€‚
- en: '![](../Images/91aad09bfe32588f15ed2eb21f4d9e82.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91aad09bfe32588f15ed2eb21f4d9e82.png)'
- en: Creation of an interactive report with Datapane. Image by author.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Datapaneåˆ›å»ºçš„äº’åŠ¨æŠ¥å‘Šã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: This is the Python code
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯Pythonä»£ç 
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Datapane is highly customizable. I advise the reader to study the documentation
    to integrate aesthetics and other features.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Datapaneé«˜åº¦å¯å®šåˆ¶ã€‚æˆ‘å»ºè®®è¯»è€…æŸ¥é˜…æ–‡æ¡£ï¼Œä»¥ä¾¿æ•´åˆç¾å­¦å’Œå…¶ä»–åŠŸèƒ½ã€‚
- en: Conclusion
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: We have seen how to build embeddings from scratch using Gensim and Word2Vec.
    This is very simple to do if you have a structured dataset and if you know the
    Gensim API.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹åˆ°å¦‚ä½•ä½¿ç”¨Gensimå’ŒWord2Vecä»å¤´æ„å»ºåµŒå…¥ã€‚å¦‚æœä½ æœ‰ç»“æ„åŒ–çš„æ•°æ®é›†ï¼Œå¹¶ä¸”çŸ¥é“Gensimçš„APIï¼Œè¿™ä¸ªè¿‡ç¨‹éå¸¸ç®€å•ã€‚
- en: With embeddings we can really do many things, for example
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åµŒå…¥æˆ‘ä»¬å¯ä»¥åšå¾ˆå¤šäº‹æƒ…ï¼Œä¾‹å¦‚
- en: do **document clustering**, displaying these clusters in vector space
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åš**æ–‡æ¡£èšç±»**ï¼Œåœ¨å‘é‡ç©ºé—´ä¸­å±•ç¤ºè¿™äº›èšç±»
- en: '**research similarities** between words'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç ”ç©¶å•è¯ä¹‹é—´çš„ç›¸ä¼¼æ€§**'
- en: use embeddings **as features in a machine learning model**
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†åµŒå…¥**ä½œä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„ç‰¹å¾**
- en: lay the foundations for **machine translation**
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸º**æœºå™¨ç¿»è¯‘**å¥ å®šåŸºç¡€
- en: and so on. If you are interested in a topic that extends the one covered here,
    leave a comment and let me know ğŸ‘
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ç­‰ç­‰ã€‚å¦‚æœä½ å¯¹å»¶ä¼¸è‡³è¿™é‡Œè®¨è®ºä¸»é¢˜çš„å†…å®¹æ„Ÿå…´è¶£ï¼Œè¯·ç•™è¨€å‘Šè¯‰æˆ‘ ğŸ‘
- en: With this project you can enrich your portfolio of NLP templates and communicate
    to a stakeholder expertise in dealing with textual documents in the context of
    machine learning.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å¯ä»¥ä¸°å¯Œä½ çš„NLPæ¨¡æ¿ç»„åˆï¼Œå¹¶å‘åˆ©ç›Šç›¸å…³è€…å±•ç¤ºä½ åœ¨æœºå™¨å­¦ä¹ èƒŒæ™¯ä¸‹å¤„ç†æ–‡æœ¬æ–‡ä»¶çš„ä¸“ä¸šçŸ¥è¯†ã€‚
- en: To the next article ğŸ‘‹
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ç¯‡æ–‡ç«  ğŸ‘‹
- en: '**If you want to support my content creation activity, feel free to follow
    my referral link below and join Mediumâ€™s membership program**. I will receive
    a portion of your investment and youâ€™ll be able to access Mediumâ€™s plethora of
    articles on data science and more in a seamless way.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚æœä½ æƒ³æ”¯æŒæˆ‘çš„å†…å®¹åˆ›ä½œæ´»åŠ¨ï¼Œè¯·éšæ—¶é€šè¿‡ä¸‹é¢çš„æ¨èé“¾æ¥å…³æ³¨æˆ‘å¹¶åŠ å…¥Mediumä¼šå‘˜è®¡åˆ’**ã€‚æˆ‘å°†è·å¾—ä½ æŠ•èµ„çš„ä¸€éƒ¨åˆ†ï¼ŒåŒæ—¶ä½ ä¹Ÿèƒ½æ— ç¼è®¿é—®Mediumä¸Šå¤§é‡çš„æ•°æ®ç§‘å­¦ç­‰ç›¸å…³æ–‡ç« ã€‚'
- en: '[](https://medium.com/@theDrewDag/membership?source=post_page-----c457d587e031--------------------------------)
    [## Join Medium with my referral link - Andrea D''Agostino'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@theDrewDag/membership?source=post_page-----c457d587e031--------------------------------)
    [## ä½¿ç”¨æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥Medium - Andrea D''Agostino'
- en: Read every story from Andrea D'Agostino (and thousands of other writers on Medium).
    Your membership fee directlyâ€¦
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é˜…è¯»Andrea D'Agostinoï¼ˆä»¥åŠMediumä¸Šå…¶ä»–æ•°åƒåä½œå®¶çš„ï¼‰æ¯ä¸ªæ•…äº‹ã€‚ä½ çš„ä¼šå‘˜è´¹ç”¨ç›´æ¥â€¦
- en: medium.com](https://medium.com/@theDrewDag/membership?source=post_page-----c457d587e031--------------------------------)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@theDrewDag/membership?source=post_page-----c457d587e031--------------------------------)
