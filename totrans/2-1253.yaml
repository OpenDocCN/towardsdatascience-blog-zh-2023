- en: How to Train a Word2Vec Model from Scratch with Gensim
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用Gensim从头开始训练Word2Vec模型
- en: 原文：[https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031](https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031](https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031)
- en: '*In this article we will explore Gensim, a very popular Python library for
    training text-based machine learning models, to train a Word2Vec model from scratch*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*在本文中，我们将探索Gensim，这是一种非常流行的Python库，用于训练基于文本的机器学习模型，以从头开始训练一个Word2Vec模型*'
- en: '[](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)[![Andrea
    D''Agostino](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)
    [Andrea D''Agostino](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)[![Andrea
    D''Agostino](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)
    [Andrea D''Agostino](https://medium.com/@theDrewDag?source=post_page-----c457d587e031--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)
    ·9 min read·Feb 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c457d587e031--------------------------------)
    ·9分钟阅读·2023年2月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f5bacd1ddee7fde33777e0a5a0db4449.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5bacd1ddee7fde33777e0a5a0db4449.png)'
- en: Image by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像。
- en: Word2Vec is a machine learning algorithm that allows you to create vector representations
    of words.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec是一个机器学习算法，它允许你创建单词的向量表示。
- en: These representations, called **embeddings**, are used in many natural language
    processing tasks, such as word clustering, classification, and text generation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表示被称为**嵌入**，在许多自然语言处理任务中使用，如单词聚类、分类和文本生成。
- en: The Word2Vec algorithm marked the beginning of an era in the NLP world when
    it was first introduced by Google in 2013.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec算法标志着自然语言处理（NLP）世界的一个时代的开始，它在2013年由Google首次介绍。
- en: It is based on word representations created by a neural network trained on very
    large data corpuses.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 它基于通过神经网络训练的非常大的数据语料库创建的单词表示。
- en: '**The output of Word2Vec are vectors**, one for each word in the training dictionary,
    **that effectively capture relationships between words.**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**Word2Vec的输出是向量**，每个向量对应训练词典中的一个单词，**有效地捕捉了单词之间的关系。**'
- en: Vectors that are close together in vector space have similar meanings based
    on context, and vectors that are far apart have different meanings. For example,
    the words “strong” and “mighty” would be close together while “strong” and “Paris”
    would be relatively far away within the vector space.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在向量空间中相互接近的向量基于上下文具有相似的含义，而相距较远的向量具有不同的含义。例如，“强大”和“雄伟”这两个词在向量空间中会很接近，而“强大”和“巴黎”这两个词则会相对远离。
- en: This is a significant improvement over the performance of the bag-of-words model,
    which is based on simply counting the tokens present in a textual data corpus.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这比基于简单计算文本数据语料库中存在的标记的袋装单词模型性能有了显著改进。
- en: In this article we will explore Gensim, a popular Python library for training
    text-based machine learning models, **to train a Word2Vec model from scratch.**
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探索Gensim，这是一种流行的Python库，用于训练基于文本的机器学习模型，**以从头开始训练一个Word2Vec模型。**
- en: '**I will use the articles from my from my personal blog in Italian to act as
    a textual corpus for this project.** Feel free to use whatever corpus you wish
    — the pipeline is extendable.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**我将使用我个人博客中的意大利语文章作为本项目的文本语料库。** 随意使用你希望的任何语料库——这个流程是可扩展的。'
- en: This approach is adaptable to any textual dataset. You’ll be able to create
    the embeddings yourself and visualize them.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法适用于任何文本数据集。你将能够自己创建嵌入向量并进行可视化。
- en: Let’s begin!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Project requirements
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目要求
- en: Let’s draw up a list of actions to do that serve as foundations of the project.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出一些行动步骤，这些步骤将作为项目的基础。
- en: We’ll create a new virtual environment
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将创建一个新的虚拟环境。
- en: '(read here to understand how: [How to Set Up a Development Environment for
    Machine Learning](https://medium.com/towards-data-science/how-to-set-up-a-development-environment-for-machine-learning-b015a91bda8a))'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （阅读这里以了解如何操作：[如何为机器学习设置开发环境](https://medium.com/towards-data-science/how-to-set-up-a-development-environment-for-machine-learning-b015a91bda8a)）
- en: Install the dependencies, among which Gensim
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装依赖项，其中包括Gensim
- en: Prepare our corpus to deliver to Word2Vec
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备我们的语料库以交给Word2Vec
- en: Train the model and save it
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型并保存它
- en: Use TSNE and Plotly to visualize embeddings to visually understand the vector
    space generated by Word2Vec
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TSNE和Plotly来可视化嵌入，以便直观理解Word2Vec生成的向量空间。
- en: 'BONUS: Use the Datapane library to create an interactive HTML report to share
    with whoever we want'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 附加：使用Datapane库创建一个互动HTML报告，与我们想要分享的人共享
- en: By the end of the article we will have in our hands an excellent basis for developing
    more complex reasoning, such as clustering of embeddings and more.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 到文章结尾时，我们将拥有一个很好的基础，以便开展更复杂的推理，例如嵌入的聚类等。
- en: I’ll assume you’ve already configured your environment correctly, so I won’t
    explain how to do it in this article. Let’s start right away with downloading
    the blog data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设你已经正确配置了你的环境，所以在这篇文章中我不会解释如何操作。我们直接开始下载博客数据吧。
- en: Dependencies
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依赖项
- en: Before we begin let’s make sure to install the following project level dependencies
    by running `pip install XXXXX` in the terminal.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要通过在终端中运行`pip install XXXXX`来确保安装以下项目级别的依赖。
- en: '`trafilatura`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trafilatura`'
- en: '`pandas`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`gensim`'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gensim`'
- en: '`nltk`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nltk`'
- en: '`tqdm`'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tqdm`'
- en: '`scikit-learn`'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`'
- en: '`plotly`'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plotly`'
- en: '`datapane`'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datapane`'
- en: We will also initialize a `logger` object to receive Gensim messages in the
    terminal.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将初始化一个`logger`对象，以便在终端接收Gensim消息。
- en: Retrieve the corpus data
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取语料库数据
- en: As mentioned we will use the articles of my personal blog in Italian (diariodiunanalista.it)
    for our corpus data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将使用我个人博客中的意大利语文章（diariodiunanalista.it）作为我们的语料库数据。
- en: Here is how it appears in Deepnote.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在Deepnote中它的样子是这样的。
- en: '![](../Images/0be5f55ccea2ca2d8e8545a200e88d73.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0be5f55ccea2ca2d8e8545a200e88d73.png)'
- en: The data we collected in pandas dataframe format. Image by author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以pandas数据框格式收集的数据。图片来源于作者。
- en: The textual data that we are going to use is under the *article* column. Let’s
    see what a random text looks like
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要使用的文本数据位于*article*列中。让我们看看随机文本的样子。
- en: '![](../Images/41a7e00873bdb188020b356d98d410d9.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41a7e00873bdb188020b356d98d410d9.png)'
- en: Regardless of language, this should be processed before being delivered to the
    Word2Vec model. We have to go and remove the Italian stopwords, clean up punctuation,
    numbers and other symbols. This will be the next step.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 无论语言如何，这些数据在交给Word2Vec模型之前都应进行处理。我们需要去除意大利语停用词，清理标点符号、数字和其他符号。这将是下一步。
- en: Preparation of the data corpus
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据语料库的准备
- en: The first thing to do is to import some fundamental dependencies for preprocessing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要导入一些用于预处理的基本依赖。
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now let’s create a `preprocess_text` function that takes some text as input
    and returns a clean version of it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个`preprocess_text`函数，它接收一些文本作为输入并返回一个干净的版本。
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s apply this function to the Pandas dataframe by using a lambda function
    with `.apply`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用带有`.apply`的lambda函数将这个函数应用到Pandas数据框中。
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We get a clean series.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个干净的系列。
- en: '![](../Images/feb2922485c7bfe6346ff48ac57b5166.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/feb2922485c7bfe6346ff48ac57b5166.png)'
- en: Each article has been cleaned and tokenized. Image by author.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 每篇文章都经过了清理和分词处理。图片来源于作者。
- en: Let’s examine a text to see the effect of our preprocessing.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下文本，看看我们预处理的效果。
- en: '![](../Images/224362b3e5ce4d5140ac7a71723ce7f6.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/224362b3e5ce4d5140ac7a71723ce7f6.png)'
- en: How a single cleaned text appears. Image by author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 单个清理后的文本的样子。图片来源于作者。
- en: The text now appears to be ready to be processed by Gensim. Let’s carry on.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在文本看起来已经准备好被Gensim处理了。我们继续。
- en: Word2Vec training
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vec训练
- en: The first thing to do is create a variable `texts` that will contain our texts.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是创建一个变量`texts`，它将包含我们的文本。
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We are now ready to train the model. Word2Vec can accept many parameters, but
    let’s not worry about that for now. Training the model is straightforward, and
    requires one line of code.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好训练模型了。Word2Vec可以接受许多参数，但现在我们不必担心这些。训练模型非常简单，只需要一行代码。
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/6f93c773e91a14eaabaa83292e1007ba.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f93c773e91a14eaabaa83292e1007ba.png)'
- en: Word2Vec training process. Image by author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec训练过程。图片来源于作者。
- en: Our model is ready and the embeddings have been created. To test this, let’s
    try to find the vector for the word *overfitting*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型已经准备好，并且嵌入已经创建。为了测试这一点，让我们尝试找出单词*overfitting*的向量。
- en: '![](../Images/0ba1d4e5abac985e48d348357263fe61.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ba1d4e5abac985e48d348357263fe61.png)'
- en: Word embeddings for the word “overfitting”. Image by author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: “overfitting”一词的词嵌入。图像来源：作者。
- en: By default, Word2Vec creates 100-dimensional vectors. This parameter can be
    changed, along with many others, when we instantiate the class. In any case, the
    more dimensions associated with a word, **the more information the neural network
    will have about the word itself and its relationship to the others.**
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Word2Vec创建100维的向量。这个参数可以更改，还有许多其他参数，在我们实例化类时进行设置。无论如何，关联的维度越多，**神经网络对单词本身及其与其他单词关系的了解就越多。**
- en: Obviously this **has a higher computational and memory cost**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这**具有更高的计算和内存成本**。
- en: 'Please note: one of the most important limitations of Word2Vec **is the inability
    to generate vectors for words not present in the vocabulary** (called OOV — out
    of vocabulary words).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意：Word2Vec**无法为词汇表中未出现的单词生成向量**（称为OOV — 词汇外单词）是其最重要的限制之一。
- en: '![](../Images/e4a0288cbf1c6967704a7d7432d26fde.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4a0288cbf1c6967704a7d7432d26fde.png)'
- en: A major limitation of W2V is the inability to map embeddings for out of vocabulary
    words. Image by author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: W2V的一个主要限制是无法为词汇外单词映射嵌入。图像来源：作者。
- en: To handle new words, therefore, we’ll have to either train a new model or add
    vectors manually.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了处理新单词，我们必须训练一个新模型或手动添加向量。
- en: Calculate the similarity between two words
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算两个单词之间的相似度
- en: With the cosine similarity we can calculate how far apart the vectors are in
    space.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用余弦相似度，我们可以计算向量在空间中的距离。
- en: With the command below we instruct Gensim to find the first 3 words most similar
    to *overfitting*
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用下面的命令，我们指示Gensim查找与*overfitting*最相似的前三个单词
- en: '`model.wv.most_similar(positive=[''overfitting''], topn=3))`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.wv.most_similar(positive=[''overfitting''], topn=3))`'
- en: '![](../Images/1ad8cacaf04f80ca5b292fe6366dbf5d.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ad8cacaf04f80ca5b292fe6366dbf5d.png)'
- en: The most similar words to “*overfitting”. Image by author.*
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与“*overfitting*”最相似的词。图像来源：作者。
- en: Let’s see how the word “when” (*quando* in Italian)is present in this result.
    It will be appropriate to include similar adverbs in the stop words to clean up
    the results.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看单词“when”(*quando* 在意大利语中)在这个结果中是如何呈现的。将类似的副词包含在停用词中以清理结果会比较合适。
- en: To save the model, just do `model.save("./path/to/model")`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要保存模型，只需执行`model.save("./path/to/model")`。
- en: Visualize embeddings with TSNE and Plotly
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TSNE和Plotly可视化嵌入
- en: Our vectors are 100-dimensional. It’s a problem to visualize them unless we
    do something to **reduce their dimensionality.**
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的向量是100维的。除非我们做些事情来**减少它们的维度**，否则可视化它们是个问题。
- en: We will use the TSNE, a technique to reduce the dimensionality of the vectors
    and create two components, one for the X axis and one for the Y axis on a scatterplot.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用TSNE，这是一种降低向量维度的技术，并创建两个组件，一个用于X轴，另一个用于Y轴上的散点图。
- en: In the .gif below you can see the words embedded in the space thanks to the
    Plotly features.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的.gif中，你可以看到感谢Plotly功能嵌入在空间中的单词。
- en: '![](../Images/bfda99275b8d7c9ac1e38fcb2e3c86f4.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfda99275b8d7c9ac1e38fcb2e3c86f4.png)'
- en: How embeddings of my Italian blog appear in TSNE projection. Image by author.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意大利博客的嵌入在TSNE投影中呈现的效果。图像来源：作者。
- en: Here is the code to generate this image.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成此图像的代码。
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This visualization can be useful for noticing semantic and syntactic tendencies
    in your data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可视化有助于识别数据中的语义和句法倾向。
- en: '**For example, it’s very useful for pointing out anomalies**, such as groups
    of words that tend to clump together for some reason.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**例如，这非常有助于指出异常**，如因某种原因倾向于聚集在一起的单词组。'
- en: Parameters of Word2Vec
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vec的参数
- en: By checking on the Gensim website we see that there are many parameters that
    Word2Vec accepts. The most important ones are `vectors_size`, `min_count`, `window`
    and `sg`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查Gensim网站，我们可以看到Word2Vec接受许多参数。最重要的参数包括`vectors_size`、`min_count`、`window`和`sg`。
- en: '**vectors_size** : defines the dimensions of our vector space.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vectors_size** : 定义了我们向量空间的维度。'
- en: '**min_count**: Words below the min_count frequency are removed from the vocabulary
    before training.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_count**: 训练前，会将频率低于min_count的单词从词汇表中移除。'
- en: '**window**: maximum distance between the current and the expected word within
    a sentence.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**window**: 句子中当前词与期望词之间的最大距离。'
- en: '**sg**: defines the training algorithm. 0 = CBOW (continuous bag of words),
    1 = Skip-Gram.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sg**：定义训练算法。0 = CBOW（连续词袋），1 = Skip-Gram。'
- en: We won’t go into detail on each of these. I suggest the interested reader to
    take a look at the [Gensim documentation](https://radimrehurek.com/gensim/models/word2vec.html).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细讨论每一个。我建议感兴趣的读者查看[Gensim文档](https://radimrehurek.com/gensim/models/word2vec.html)。
- en: Let’s try to retrain our model with the following parameters
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试使用以下参数重新训练我们的模型。
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/91cc372f6337a96de2699621933721b3.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91cc372f6337a96de2699621933721b3.png)'
- en: A new projection based on the new parameters for Word2Vec. Image by author.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Word2Vec新参数的新投影。图片由作者提供。
- en: The representation changes a lot. The number of vectors is the same as before
    (Word2Vec defaults to 100), while `min_count`, `window` and `sg` have been changed
    from their defaults.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表示发生了很大变化。向量的数量与之前相同（Word2Vec的默认值为100），而`min_count`、`window`和`sg`已从默认值进行了更改。
- en: I suggest to the reader to change these parameters in order to understand which
    representation is more suitable for his own case.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议读者更改这些参数，以了解哪种表示更适合自己的情况。
- en: 'BONUS: Create an interactive report with Datapane'
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖金：使用Datapane创建一个互动报告
- en: We have reached the end of the article. We conclude the project by creating
    **an interactive report in HTML with Datapane, which will allow the user to view
    the graph previously created with Plotly directly in the browser.**
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到了文章的结尾。我们通过创建**一个HTML格式的互动报告，利用Datapane使用户能够在浏览器中直接查看之前用Plotly创建的图表**来结束这个项目。
- en: '![](../Images/91aad09bfe32588f15ed2eb21f4d9e82.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91aad09bfe32588f15ed2eb21f4d9e82.png)'
- en: Creation of an interactive report with Datapane. Image by author.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Datapane创建的互动报告。图片由作者提供。
- en: This is the Python code
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Python代码
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Datapane is highly customizable. I advise the reader to study the documentation
    to integrate aesthetics and other features.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Datapane高度可定制。我建议读者查阅文档，以便整合美学和其他功能。
- en: Conclusion
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We have seen how to build embeddings from scratch using Gensim and Word2Vec.
    This is very simple to do if you have a structured dataset and if you know the
    Gensim API.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何使用Gensim和Word2Vec从头构建嵌入。如果你有结构化的数据集，并且知道Gensim的API，这个过程非常简单。
- en: With embeddings we can really do many things, for example
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌入我们可以做很多事情，例如
- en: do **document clustering**, displaying these clusters in vector space
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 做**文档聚类**，在向量空间中展示这些聚类
- en: '**research similarities** between words'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**研究单词之间的相似性**'
- en: use embeddings **as features in a machine learning model**
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将嵌入**作为机器学习模型中的特征**
- en: lay the foundations for **machine translation**
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为**机器翻译**奠定基础
- en: and so on. If you are interested in a topic that extends the one covered here,
    leave a comment and let me know 👍
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。如果你对延伸至这里讨论主题的内容感兴趣，请留言告诉我 👍
- en: With this project you can enrich your portfolio of NLP templates and communicate
    to a stakeholder expertise in dealing with textual documents in the context of
    machine learning.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个项目，你可以丰富你的NLP模板组合，并向利益相关者展示你在机器学习背景下处理文本文件的专业知识。
- en: To the next article 👋
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下一篇文章 👋
- en: '**If you want to support my content creation activity, feel free to follow
    my referral link below and join Medium’s membership program**. I will receive
    a portion of your investment and you’ll be able to access Medium’s plethora of
    articles on data science and more in a seamless way.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你想支持我的内容创作活动，请随时通过下面的推荐链接关注我并加入Medium会员计划**。我将获得你投资的一部分，同时你也能无缝访问Medium上大量的数据科学等相关文章。'
- en: '[](https://medium.com/@theDrewDag/membership?source=post_page-----c457d587e031--------------------------------)
    [## Join Medium with my referral link - Andrea D''Agostino'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@theDrewDag/membership?source=post_page-----c457d587e031--------------------------------)
    [## 使用我的推荐链接加入Medium - Andrea D''Agostino'
- en: Read every story from Andrea D'Agostino (and thousands of other writers on Medium).
    Your membership fee directly…
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读Andrea D'Agostino（以及Medium上其他数千名作家的）每个故事。你的会员费用直接…
- en: medium.com](https://medium.com/@theDrewDag/membership?source=post_page-----c457d587e031--------------------------------)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@theDrewDag/membership?source=post_page-----c457d587e031--------------------------------)
