- en: 'Uncovering Anomalies with Variational Autoencoders (VAE): A Deep Dive into
    the World of Unsupervised Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用变分自编码器（VAE）发现异常：深入探索无监督学习的世界
- en: 原文：[https://towardsdatascience.com/uncovering-anomalies-with-variational-autoencoders-vae-a-deep-dive-into-the-world-of-1b2bce47e2e9](https://towardsdatascience.com/uncovering-anomalies-with-variational-autoencoders-vae-a-deep-dive-into-the-world-of-1b2bce47e2e9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/uncovering-anomalies-with-variational-autoencoders-vae-a-deep-dive-into-the-world-of-1b2bce47e2e9](https://towardsdatascience.com/uncovering-anomalies-with-variational-autoencoders-vae-a-deep-dive-into-the-world-of-1b2bce47e2e9)
- en: An example use case of using Variational Autoencoders (VAE) to detect anomalies
    in all types of data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用变分自编码器（VAE）在各种数据类型中检测异常的示例用例
- en: '[](https://medium.com/@will.badr?source=post_page-----1b2bce47e2e9--------------------------------)[![Will
    Badr](../Images/46a4737eaedc1cb30f4f11ceb8373528.png)](https://medium.com/@will.badr?source=post_page-----1b2bce47e2e9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1b2bce47e2e9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1b2bce47e2e9--------------------------------)
    [Will Badr](https://medium.com/@will.badr?source=post_page-----1b2bce47e2e9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@will.badr?source=post_page-----1b2bce47e2e9--------------------------------)[![Will
    Badr](../Images/46a4737eaedc1cb30f4f11ceb8373528.png)](https://medium.com/@will.badr?source=post_page-----1b2bce47e2e9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1b2bce47e2e9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1b2bce47e2e9--------------------------------)
    [Will Badr](https://medium.com/@will.badr?source=post_page-----1b2bce47e2e9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1b2bce47e2e9--------------------------------)
    ·9 min read·Jan 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1b2bce47e2e9--------------------------------)
    ·9 分钟阅读·2023年1月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d0d9e1cf8a587f22677aba93c805486f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0d9e1cf8a587f22677aba93c805486f.png)'
- en: Photo by [JJ Ying](https://unsplash.com/@jjying?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [JJ Ying](https://unsplash.com/@jjying?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In an earlier [post](https://medium.com/p/3e5c6f017726), I explained what autoencoders
    are, what they are used for and how to leverage them in training an anomaly detection
    model. As a reminder, autoencoders are a type of neural network that are commonly
    used for dimensionality reduction and learning features. They are also commonly
    used for anomaly detection, as they can learn to reconstruct normal data, but
    may struggle to reconstruct anomalous or outlier data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的 [帖子](https://medium.com/p/3e5c6f017726) 中，我解释了什么是自编码器，它们的用途以及如何在训练异常检测模型中利用它们。作为提醒，自编码器是一种常用于降维和特征学习的神经网络类型。它们也常用于异常检测，因为它们可以学习重构正常数据，但可能会在重构异常或离群数据时遇到困难。
- en: 'An autoencoder network consists of two components: an encoder and a decoder.
    The encoder maps the input data to a lower-dimensional latent space, and the decoder
    maps the latent representation back to the original input space. During training,
    the autoencoder is trained to reconstruct the input data as accurately as possible.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码网络由两个组成部分构成：编码器和解码器。编码器将输入数据映射到较低维度的潜在空间，而解码器将潜在表示映射回原始输入空间。在训练过程中，自编码器被训练以尽可能准确地重构输入数据。
- en: To use an autoencoder for anomaly detection, the autoencoder is first trained
    on a dataset of *normal, non-anomalous* data. Once trained, the autoencoder can
    be used to reconstruct new data samples. If a new data sample is significantly
    different from the normal data that the autoencoder was trained on, it may be
    reconstructed poorly, indicating that it is potentially anomalous.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用自编码器进行异常检测，首先需要在*正常、非异常*的数据集上训练自编码器。训练完成后，自编码器可以用于重构新的数据样本。如果新的数据样本与自编码器训练时的正常数据有显著差异，它可能会被重构得很差，表明它可能是异常的。
- en: In this article, I will focus on using a variation of the autoencoder network
    called Variational Autoencoders (VAEs) to detect anomalies and what makes it different
    from regular autoencoders in detecting anomalies.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将重点讨论使用一种变体自编码器网络，即变分自编码器（VAE），来检测异常，以及它在异常检测中与普通自编码器的不同之处。
- en: VAEs are a type of neural network architecture that is used for generative modeling.
    They are unique in that they are able to learn a compact, latent and compressed
    representation of a given dataset and then generate new samples from that representation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 是一种用于生成建模的神经网络架构。它们的独特之处在于能够学习给定数据集的紧凑、潜在且压缩的表示，然后从这种表示中生成新的样本。
- en: One of the key feature of VAEs is that they are designed to be able to learn
    a probabilistic model of the data, which means that they can be used to generate
    new samples that are similar to the training data, but not necessarily identical.
    This allows VAEs to be used for tasks such as image generation, text generation,
    and other types of data generation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 的一个关键特性是它们被设计为能够学习数据的概率模型，这意味着它们可以用来生成与训练数据相似但不完全相同的新样本。这使得 VAE 可以用于图像生成、文本生成以及其他类型的数据生成任务。
- en: You might still be wondering, what does a generative model have to do with the
    anomaly detection task! To answer this question, let’s review what anomaly detection
    is. Anomaly detection is the task of identifying unusual or unexpected patterns
    in a dataset, any pattern that deviates from what is normal. Since VAEs have the
    ability to learn a probabilistic model of the data, this allows them to generate
    new samples from the latent space. These new samples are drawn from the same probability
    distribution of the original data you used to train the model which makes it more
    robust and tolerant to variations in the data than regular autoencoders. This
    can be very useful for detecting anomalies in data that has a clear normal behavior.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还在想，生成模型与异常检测任务有什么关系！为了回答这个问题，让我们回顾一下异常检测是什么。异常检测是识别数据集中不寻常或意外模式的任务，任何偏离正常情况的模式。由于
    VAE 能够学习数据的概率模型，这使得它们能够从潜在空间生成新的样本。这些新样本来自与你用于训练模型的原始数据相同的概率分布，使得 VAE 在数据变化方面比普通自编码器更具鲁棒性和容忍度。这对于检测具有明确正常行为的数据中的异常非常有用。
- en: VAE Network Structure
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VAE 网络结构
- en: '![](../Images/081602edef181d2b836838e5476558f9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/081602edef181d2b836838e5476558f9.png)'
- en: 'Source: [https://commons.wikimedia.org/wiki/File:Reparameterized_Variational_Autoencoder.png](https://commons.wikimedia.org/wiki/File:Reparameterized_Variational_Autoencoder.png)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://commons.wikimedia.org/wiki/File:Reparameterized_Variational_Autoencoder.png](https://commons.wikimedia.org/wiki/File:Reparameterized_Variational_Autoencoder.png)
- en: 'VAE networks are typically composed of multiple components:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 网络通常由多个组件组成：
- en: '**An Encoder:** The encoder is a neural network that maps the input data to
    a lower-dimensional latent space. The encoder is typically parameterized by a
    set of weights and biases that are learned during training.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编码器：**编码器是一个神经网络，它将输入数据映射到一个低维的潜在空间。编码器通常由一组在训练过程中学习到的权重和偏置参数化。'
- en: '**Latent space:** The latent space is the lower-dimensional space that the
    encoder maps the input data to. This latent space typically has a continuous structure,
    which means that the dimensions of the latent space can only take on any real
    value within a certain range. This is in contrast to a discrete latent space,
    which would only allow for a finite set of values for each dimension. This allows
    for more flexibility and expressive power in the VAE model. It allows the VAE
    to capture subtle variations and nuances in the input data, then generate new
    data samples that are close to the training data but not necessarily identical
    to it. This enables the VAE to capture the uncertainty and variability in the
    data and to generate new samples that are diverse and varied (Hence the name **Variational**
    autoencoder)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**潜在空间：**潜在空间是编码器将输入数据映射到的低维空间。这个潜在空间通常具有连续结构，这意味着潜在空间的维度只能在某个范围内取任意实值。这与离散潜在空间形成对比，后者每个维度仅允许有限的值集合。这为
    VAE 模型提供了更多的灵活性和表达能力。它使 VAE 能够捕捉输入数据中的微妙变化和细微差别，然后生成接近训练数据但不完全相同的新数据样本。这使得 VAE
    能够捕捉数据中的不确定性和变异性，并生成多样化和变化丰富的新样本（因此得名 **变分** 自编码器）。'
- en: '**Decoder:** The decoder is a neural network that maps the latent representation
    above back to the original input space. The decoder is also typically parameterized
    by a set of weights and biases that are learned during training.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**解码器：**解码器是一个神经网络，它将上述潜在表示映射回原始输入空间。解码器同样通常由一组在训练过程中学习到的权重和偏置参数化。'
- en: '**Reconstruction loss:** The reconstruction loss measures how well the decoder
    is able to reconstruct the input data from the latent representation. This loss
    is typically used to train the model.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重构损失：**重构损失衡量解码器从潜在表示中重建输入数据的能力。通常使用此损失来训练模型。'
- en: 'So far, the 4 components above are similar to the ones from the regular autoencoders.
    VAEs have two extra components:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，上述 4 个组件与常规自编码器中的组件类似。VAE 还有两个额外的组件：
- en: 5\. **A Prior:** The prior is a probabilistic distribution that is used to model
    the latent space. In VAEs, the prior is often assumed to be a standard normal
    distribution.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. **先验：**先验是用于建模潜在空间的概率分布。在 VAE 中，先验通常假定为标准正态分布。
- en: 6\. **A Posterior:** The posterior is the distribution that models the latent
    variables given the input data. The posterior is typically approximated using
    a function that is parameterized by the encoder.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. **后验：**后验是给定输入数据建模潜变量的分布。后验通常使用由编码器参数化的函数进行近似。
- en: 'Now that you know what involves a VAE network, let’s implement a base version
    of it using PyTorch:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道了 VAE 网络的内容，让我们使用 PyTorch 实现一个基本版本：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let me explain each section in the code snippet above:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我解释一下上面代码片段中的每个部分：
- en: The encoder and decoder are the same like the regular autoencoder. The encoder
    goes from the input dimension size (*input_dim*) to a small number of dimensions/compressed
    representation (*latent_dim*) then the decoder uncompresses it back to the original
    input dimensions.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器和解码器与常规自编码器相同。编码器将输入维度大小（*input_dim*）映射到较小的维度/压缩表示（*latent_dim*），然后解码器将其解压回原始输入维度。
- en: '`fc_mu`: is a fully connected layer that maps the intermediate representation
    of the input data produced by the encoder to the mean of the posterior distribution.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fc_mu`：是一个全连接层，它将编码器生成的输入数据的中间表示映射到后验分布的均值。'
- en: '`fc_logvar`: is also a fully connected layer that maps the intermediate representation
    of the input data to the log variance of the posterior distribution. The posterior
    distribution is then used to model the latent variables given the input data.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fc_logvar`：也是一个全连接层，将输入数据的中间表示映射到后验分布的对数方差。然后使用后验分布来建模给定输入数据的潜变量。'
- en: '`reparameterize()` The mean and log variance of the posterior that we generate
    from the fully connected layers are used to sample latent variables using this
    function. It allows the VAE to be trained using gradient-based optimization methods.
    It is also being referred to as the **reparameterization trick**'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reparameterize()` 我们从全连接层生成的后验均值和对数方差用于通过该函数采样潜变量。这使得 VAE 可以使用基于梯度的优化方法进行训练。它也被称为**重参数化技巧**。'
- en: Now that we defined the model and optimizer, let’s need to define the loss function
    and training function. The loss function in our case will be a combination of
    two different losses. The reconstruction loss that measures the difference between
    the input and output; the KL divergence loss. **The KL divergence loss** is used
    to encourage the posterior distribution to be similar to the prior distribution,
    which helps to prevent overfitting and ensure that the latent variables capture
    the underlying structure and variability of the input data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义了模型和优化器，我们需要定义损失函数和训练函数。在我们的情况下，损失函数将是两种不同损失的组合。重构损失衡量输入和输出之间的差异；KL 散度损失。**KL
    散度损失**用于促使后验分布类似于先验分布，这有助于防止过拟合，并确保潜变量捕捉到输入数据的潜在结构和变异性。
- en: '*Still doesn’t make sense? Let’s break it down even further:*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*仍然不明白？让我们进一步详细解释：*'
- en: The prior distribution refers to the distribution of the latent variables before
    they are conditioned on the input data. The prior distribution is typically assumed
    to be a standard normal distribution, which represents the belief that the latent
    variables are independent and have a simple distribution. The posterior distribution
    refers to the distribution of the latent variables after they are conditioned
    on the input data. The posterior distribution is modeled using the mean and log
    variance of the latent variables produced by the encoder.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 先验分布指的是在条件化输入数据之前的潜变量分布。先验分布通常假定为标准正态分布，这表示潜变量是独立的且具有简单的分布。后验分布指的是在条件化输入数据之后的潜变量分布。后验分布通过编码器生成的潜变量的均值和对数方差来建模。
- en: 'The prior distribution is used as a regularization term in the VAE, as it encourages
    the posterior distribution of the latent variables (given the input data) to be
    similar to the prior distribution. This helps to prevent overfitting and ensure
    that the latent variables capture the underlying structure and variability of
    the input data, rather than just memorizing the training data. We simply use KL
    divergence loss to achieve this. It is calculated as the sum of the element-wise
    divergence between the posterior and prior distributions:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 先验分布作为 VAE 中的正则化项使用，因为它鼓励潜在变量的后验分布（给定输入数据）与先验分布相似。这有助于防止过拟合，并确保潜在变量捕捉输入数据的潜在结构和变化，而不仅仅是记忆训练数据。我们简单地使用
    KL 散度损失来实现这一点。它被计算为后验分布和先验分布之间逐元素散度的总和：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The negative sign above is included to ensure that the loss is always non-negative,
    as the KL divergence is a non-negative measure of the difference between two distributions.
    The factor of 0.5 is included for computational convenience, as it allows the
    loss to be calculated using the mean and log variance of the latent variables,
    rather than the probability densities.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 上述负号的加入是为了确保损失始终为非负，因为 KL 散度是衡量两个分布之间差异的非负量度。0.5 的因子是为了计算方便，因为它允许使用潜在变量的均值和对数方差来计算损失，而不是概率密度。
- en: 'Let’s look at the training code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看训练代码：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After instantiating the model, we pass in the data and the model will return
    three parameters. The reconstructed output, the mu and logvar parameters. We then
    use the mu and logvar to calculate the KL divergence loss. The total loss will
    be the sum of the reconstruction loss and KL divergence loss. Hence we calculate
    the gradients w.r.t the *total_loss* variable
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化模型之后，我们输入数据，模型将返回三个参数：重建输出、mu 和 logvar 参数。然后我们使用 mu 和 logvar 来计算 KL 散度损失。总损失将是重建损失和
    KL 散度损失的总和。因此，我们计算相对于*total_loss*变量的梯度。
- en: 'AutoEncoders vs Variational AutoEncoders (VAE):'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器与变分自编码器 (VAE)：
- en: 'The main difference between VAE and the regular autoencoders is the addition
    of the latent space and the use of the variational lower bound as the objective
    function, , which consists of two terms: the reconstruction loss and the KL divergence
    between the approximate posterior distribution over the latent space and the prior
    distribution.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 和普通自编码器之间的主要区别在于潜在空间的添加和使用变分下界作为目标函数，该目标函数由两部分组成：重建损失和潜在空间上近似后验分布与先验分布之间的
    KL 散度。
- en: In a regular autoencoder, the encoder network maps the input data to a latent
    representation, and the decoder network maps the latent representation back to
    the original data. The objective function is typically the reconstruction loss,
    which measures the difference between the input data and the reconstructed data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通自编码器中，编码器网络将输入数据映射到潜在表示，解码器网络将潜在表示映射回原始数据。目标函数通常是重建损失，它测量输入数据和重建数据之间的差异。
- en: 'In a VAE, the encoder network still maps the input data to a latent representation,
    but the latent representation is split into two parts: a mean vector and a log
    variance vector. These two vectors are used to define a Gaussian distribution
    over the latent space, which allows the VAE to generate new samples from the latent
    space by sampling from this distribution. The decoder network is then used to
    map these latent samples back to the original data space.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VAE 中，编码器网络仍然将输入数据映射到潜在表示，但潜在表示被分为两个部分：均值向量和对数方差向量。这两个向量用于定义潜在空间上的高斯分布，这允许
    VAE 通过从该分布中采样来生成新的样本。然后，解码器网络用于将这些潜在样本映射回原始数据空间。
- en: To summarize, the main differences between a VAE and a regular autoencoder are
    the use of a latent space with a probabilistic interpretation, and the use of
    the variational lower bound as the objective function.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，VAE 和普通自编码器之间的主要区别在于使用具有概率解释的潜在空间，以及将变分下界作为目标函数。
- en: 'Disadvantages of VAEs:'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VAEs 的劣势：
- en: 'Now that explored the benefits and advantages of VAEs, particularly in the
    anomaly detection domain, let’s explore some of its disadvantages:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们探讨了 VAEs 的好处和优势，特别是在异常检测领域，接下来让我们探讨一些其劣势：
- en: VAEs may not be as effective for data that is highly variable or has multiple
    modes of normal behavior.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于数据高度变化或具有多种正常行为模式的情况，VAEs 可能不够有效。
- en: VAEs can also be sensitive to the different choices of hyperparameters, such
    as the latent dimension and the learning rate, which can make them difficult to
    optimize.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAEs 还可能对超参数的不同选择非常敏感，例如潜在维度和学习率，这可能使其优化变得困难。
- en: VAEs can be computationally expensive to train, as they require sampling from
    the latent space and back-propagating through the sampling process.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAEs 的训练可能会计算昂贵，因为它们需要从潜在空间进行采样，并通过采样过程进行反向传播。
- en: VAEs may struggle to capture complex relationships between the input data and
    the latent variables, particularly when the data is highly structured or correlated.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAEs 可能难以捕捉输入数据与潜在变量之间的复杂关系，尤其是当数据高度结构化或相关时。
- en: VAEs may produce blurry or low-quality reconstructions, particularly when the
    latent dimension is small or the training data is noisy.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAEs 可能会生成模糊或低质量的重建图像，特别是当潜在维度较小或训练数据较嘈杂时。
- en: In conclusion, VAEs are a powerful and flexible tool for learning the underlying
    structure and variability of a dataset, and for generating new samples. However,
    they also have some limitations and challenges that should be considered when
    deciding whether to use a VAE for a particular task.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，VAEs 是一种强大且灵活的工具，适用于学习数据集的潜在结构和变异性，并生成新样本。然而，它们也有一些限制和挑战，这些因素在决定是否将 VAE 用于特定任务时应考虑在内。
