- en: 'SMOTE and Other Options: A Comprehensive Guide to Handling Imbalanced Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/use-smote-with-caution-3fa015ba3bc5](https://towardsdatascience.com/use-smote-with-caution-3fa015ba3bc5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The guide on when to use and when not to use synthetic data to tackle the class
    imbalance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ransakaravihara.medium.com/?source=post_page-----3fa015ba3bc5--------------------------------)[![Ransaka
    Ravihara](../Images/ac09746938c10ad8f157d46ea0de27ca.png)](https://ransakaravihara.medium.com/?source=post_page-----3fa015ba3bc5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3fa015ba3bc5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3fa015ba3bc5--------------------------------)
    [Ransaka Ravihara](https://ransakaravihara.medium.com/?source=post_page-----3fa015ba3bc5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3fa015ba3bc5--------------------------------)
    ·10 min read·Jan 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7bbfd0409f31269de20f9ae943cb9fd2.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [은 하](https://unsplash.com/@b0nn13_4nd_clyd3?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: If you are a machine learning practitioner, you may face class imbalance problems
    more often. Class imbalance happens when there is a different class distribution
    in the dataset. Let's take an example. Assume we are working on a churn problem.
    In this specific scenario, our minor and majority classes are customer churn,
    and the customer stays with the current service provider. But if you explore this
    problem more, you will notice fewer customers for the churn category because customer
    churn is an infrequent event that is good for business but not for the model.
    As a result, if we feed this dataset into the model, it will learn the majority
    pattern (non-churn scenario) more accurately than the minor scenario. This is
    where our problem begins.
  prefs: []
  type: TYPE_NORMAL
- en: How to deal with the class imbalance in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most obvious answer is since model interaction with the minor class during
    training is less, we can improve that by adding more minor classes to the model.
    But how? We have a few methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting more data for the minor class — This is a theoretically easy and
    practically infeasible solution. Because it's hard to do this while covering the
    business's actual needs, as an example, we may have to change the logic to get
    more customers into the churn category.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random Oversampling — We can duplicate minor classes till we get a decent class
    distribution. It may result in the model learning inaccurate churn scenarios.
    In simple words, it will over-learn little incident patterns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random Undersampling — We can remove the samples from the majority classes to
    balance the dataset. However, it will remove some signals from the dataset. Also,
    if our dataset is highly imbalanced (minor samples are less than 1%), we may have
    to remove significant majority class samples from our dataset to make it more
    balanced.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can generate synthetic data — We will focus on this more deeply in this article.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating synthetic data for rebalancing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic idea behind this is to generate more minor class samples similar to
    the existing ones in the minority class. But unlike repeating minor class instances
    multiple times, this will generate new minor class instances based on the dataset
    we have. For that SMOTE (Synthetic Minority Oversampling Technique) method is
    commonly used. But there are many alternatives for that, such as…
  prefs: []
  type: TYPE_NORMAL
- en: ADASYN (Adaptive Synthetic Sampling)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tomek Links: This technique removes samples from the majority class that are
    very close to examples in the minority class. The idea is to remove easy cases
    from the majority class that will likely be misclassified as the minority class.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Near Miss: This technique selects samples from the majority classes closest
    in feature space to examples in the minority class and removes them from the dataset.
    The idea is similar to Tomek Links, but instead of eliminating easy cases from
    the majority class, it removes samples most likely to be misclassified as the
    minority class.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's do some experiments around SMOTE.
  prefs: []
  type: TYPE_NORMAL
- en: First, import the dataset. Here I am using a *Wine Quality dataset****,*** *and
    you can access the dataset using* [*this*](https://zenodo.org/record/61452#.Y7OfjHZBzrd)
    *link*. Let's load the dataset and plot the class distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7c416fcda413763c9c500d395d7cbbe6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Now we can use the imblearn library to perform SMOTE on our dataset. In the
    below code, we will do SMOTE on our dataset and plot both the original and resamples
    versions of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bef012bba3a837560eb9822e2ce2804c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the plots, we have converted 220 original minor incidents
    into 2152, its ~9x increase. That's where the problem lies. Let's focus on our
    specific churn problem.
  prefs: []
  type: TYPE_NORMAL
- en: Generating such fake customer data can lead the model to learn patterns that
    do not exist in the real world.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In most cases, we have quality issues in the dataset. As a result, there is
    a high chance of adding noises into datasets. Generating new data using noisy
    data is a bad idea.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these potential issues, we have the below question in front.
  prefs: []
  type: TYPE_NORMAL
- en: Are we gonna deploy these models into production?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If it's a churn or fraud detection problem, I will not deploy it on production
    because these customer data can become noisier after applying SMOTE. But the answer
    to the above question highly depends on the data and business problem we are working
    on. Generally, it's not a good idea to rely on SMOTE when data is noisy and the
    problem is complex.
  prefs: []
  type: TYPE_NORMAL
- en: Let's build a classifier using oversampled data and evaluate the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1e27074341ad039e018f6a75f01837be.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to experiment with other approaches for class imbalance. Below
    I have explained a few methods I have used to tackle imbalance problems.
  prefs: []
  type: TYPE_NORMAL
- en: Use class weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change evaluation metrics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create more features by doing error analysis for the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use an unsupervised algorithm to detect clusters of the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's dive deep into these methods.
  prefs: []
  type: TYPE_NORMAL
- en: Use class weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When training a model on a dataset with class imbalance, the loss function may
    be dominated by the majority class because it has more instances. It can cause
    the model to pay more attention to the majority class than the minority class.
    The main idea of class weights is assigning weights for each sample based on its
    class. It will give more weight to the minority class during training. Meaning
    the model will pay more attention to the minority class during training in an
    effort to improve its performance in that class.
  prefs: []
  type: TYPE_NORMAL
- en: In advance, class weights can be used to balance the loss function by assigning
    higher weights to the minority class, so that it has a greater impact on the loss
    function. This can help the model to better learn the characteristics of the minority
    class and improve its performance on it.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Mathematically, class weights are typically incorporated into the loss function
    by multiplying the loss for each example by the weight for its class. For example,
    suppose we have a dataset with two classes (0 and 1) and the following class weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The loss function for a binary classification model might be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ddcc7700b94da37e3ec2cc69f19890e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'To incorporate class weights into this loss function, we can modify it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f64aaf6097d779c386cae7f09ff13a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Now, when the model is trained, the loss for each example will be multiplied
    by the class weight for its class. It will cause the model to pay more attention
    to the minority class since its samples will significantly impact the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Most major machine learning models accept the *sample_weight* parameter. Here
    is an example of how you can do this with the XGBoost library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: One thing to notice here is that sample weights can potentially lead to overfitting
    if the weights are too high. It's generally a good idea to try a range of weights
    and see which gives the best performance on the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can use the [scale_pos_weight](https://xgboost.readthedocs.io/en/stable/parameter.html)
    parameter in XGBoost as well. It will give you similar results.
  prefs: []
  type: TYPE_NORMAL
- en: Let's quickly plot the above model performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/588387ea39899044b82d87f7db815a88.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: If you check the confusion matrix for the above two scenarios. In that case,
    you will notice high false positives in the oversampled scenario. We have fewer
    false positive predictions using class weights. It also reduced our true positives
    as well. So we have to tweak our approaches based on real business needs.
  prefs: []
  type: TYPE_NORMAL
- en: Change evaluation metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most libraries use accuracy as the default evaluation metric for classification
    tasks. It's okay for balanced problems. But imbalanced problems will lead the
    model to guess the majority class without learning any potential signals.
  prefs: []
  type: TYPE_NORMAL
- en: For example, say we have 97 non-churn customers and three churn customers. Building
    a model and using accuracy as an evaluation metric can achieve 97% accuracy by
    blindly predicting non-churn class for every 100 samples. As an easy fix, we can
    change the evaluation metric into something different. Precision, Recall, F1 Score,
    and Balanced Accuracy are a few best options for the imbalance classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Create more features by doing error analysis for the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When your model performs poorly, we can use error analysis to find different
    data segments with varying performance levels. Let's take the previous churn example;
    we can find the model's error vs. customers' revenue bucket and identify revenue
    segments where the model is good and evil. Likewise, we can create an error analysis
    report with this information. After the error analysis report, we can determine
    possible new features which the model can use to distinguish the churner vs. the
    non-churner scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you know low revenue users are churning because of the "XYZ"
    reason, you can add that feature if it's not already in the model. Otherwise,
    you can perform feature engineering methods with that potential feature, such
    as binning the "XYZ" feature.
  prefs: []
  type: TYPE_NORMAL
- en: Use an unsupervised algorithm to detect clusters of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the powerful and popular approaches is segmentation. If we know some
    features that can be used for segregating data into different subgroups, we can
    use those features for the clustering model. After clustering, you will note various
    class imbalances in other groups. There are a few possible scenarios. Such as,
  prefs: []
  type: TYPE_NORMAL
- en: You may find subgroups only with one class. After verifying this customer behaviour,
    we can further skip modelling for this particular subgroup. It will reduce the
    imbalance of the whole dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You may find a somewhat balanced distribution that is easy to model compared
    to the previous dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Or, you may find subgroups with a highly imbalanced class distribution. But
    this is not lousy compared to the original distribution. The reason for this is
    this imbalance occurs in similar data points and can be a strong signal even though
    the distribution is imbalanced. For example, if we are working on a disease prediction
    model, it's a good idea to group people into subgroups based on age. If the general
    class imbalance is 2%, grouping on age will produce different class distributions
    for different subgroups. In the higher age group, this will be more balanced.
    In the middle and young age groups, this will be highly imbalanced. Since we are
    modelling each segment separately, the model can generalize to that particular
    segment well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article aims to show alternatives to treat class imbalance other than synthetic
    data generation. It's worth noting that some methods heavily depend on the data,
    problem type, and domain you are working on. It's always a good idea to experiment
    with a few different approaches and pick the best one for your problem.
  prefs: []
  type: TYPE_NORMAL
- en: Please find the citation and license information for the above dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Citation: Lemaitre, G., Nogueira, F., Aridas, C. K., & Oliveira, D. V.
    R. (2016). Imbalanced dataset for benchmarking [Data set]. Zenodo. [https://doi.org/10.5281/zenodo.61452](https://doi.org/10.5281/zenodo.61452)'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Dataset license : [Open Data Commons Open Database License v1.0](https://opendatacommons.org/licenses/odbl/1-0/)'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading.
  prefs: []
  type: TYPE_NORMAL
