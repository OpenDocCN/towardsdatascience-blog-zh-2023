- en: Two Ways to Download and Access Llama 2 Locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/two-ways-to-download-and-access-llama-2-locally-8a432ed232a4](https://towardsdatascience.com/two-ways-to-download-and-access-llama-2-locally-8a432ed232a4)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A step-by-step guide to using Llama 2 on your PC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@anna.bildea?source=post_page-----8a432ed232a4--------------------------------)[![Ana
    Bildea, PhD](../Images/60567c2b09bd0be5b25e508905dfe4c6.png)](https://medium.com/@anna.bildea?source=post_page-----8a432ed232a4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8a432ed232a4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8a432ed232a4--------------------------------)
    [Ana Bildea, PhD](https://medium.com/@anna.bildea?source=post_page-----8a432ed232a4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8a432ed232a4--------------------------------)
    ·10 min read·Sep 5, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8096320260dd4a194856f7b6ae2ee973.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author (Dreamstudio)
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Meta’s latest release, Llama 2, is gaining popularity and is incredibly interesting
    for various use cases. It offers pre-trained and fine-tuned Llama 2 language models
    in different sizes, from 7B to 70B parameters. Llama 2 performs well in various
    tests, like reasoning, coding, proficiency, and knowledge benchmarks, which makes
    it very promising.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we’ll guide you through the step-by-step process of downloading
    Llama 2 on your PC. You have two options: the official Meta AI website or HuggingFace.
    We’ll also show you how to access it, so you can leverage its powerful capabilities
    for your projects. Let’s dive in!'
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Jupyter Notebook](https://jupyter.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nvidia T4 Graphics Processing Unit (GPU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual Environment (Virtualenv)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HuggingFace](https://huggingface.co/) account, libraries, & Llama models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What to Consider Before Downloading Locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before you download the model to your local machine, consider a few things.
    First, make sure your computer has enough processing power and storage (loading
    a model from an SSD disk is much faster). Second, be prepared for some initial
    setup to get the model running. Lastly, if you’re using this for work, check your
    company’s policies on downloading external software.
  prefs: []
  type: TYPE_NORMAL
- en: Why Download Llama 2 Locally?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a few good reasons why you might want to download the model to your
    own computer such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reduced Latency** By hosting Llama 2 in your environment, you minimize the
    latency associated with API calls to external servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Privacy** You can keep your private and sensitive information on your
    own ecosystem (on-premise or external cloud provider).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customization and Control** You have more control over the model. You can
    optimize the configuration of your machine, work on optimization techniques, fine-tune
    the model, and further integrate it into your ecosystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Offline Access** Depending on the use case the model may be hosted in a secure
    environment with no internet connection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing Where to Get “Llama 2”
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deciding where to get “Llama 2” is your choice based on what works best for
    you. Here are a few things to consider in order to make your choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Meta’s GitHub:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you get “Llama 2” from Meta’s GitHub, you’re getting it directly from the
    source. This gives you access to the newest updates. However, if you encounter
    issues, the community might not be as reactive as the one on HuggingFace. The
    documentation is good, but trying out the examples might need more coding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using Hugging Face is easy because it has a user-friendly platform with a reactive
    and strong community to assist you. It is compatible with multiple frameworks
    making it easier to integrate the model into your existing technology stack.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, I will suggest getting the model directly from Meta’s GitHub if you
    are looking for customization and insights, or from Hugging Face for its ease
    of use, community support, and compatibility with various frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 1️⃣ Download Llama 2 from the Meta website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Step 1: Request download'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One option to download the model weights and tokenizer of Llama 2 is the [Meta
    AI website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/).
    Before you can download the model weights and tokenizer you have to read and agree
    to the License Agreement and submit your request by giving your email address.
    Fill in the following information and accept the terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f355ed05cf573afd2cd1008e4a0b33e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Once your request has been approved, you will be provided with a `signed URL`
    via email.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just a quick heads up! The link provided for downloading the model weights
    and tokenizer will only be valid for 24 hours and have a limited number of downloads.
    So, if you see any errors like “403: Forbidden,” don’t worry! You can simply request
    a new link by going back to the Meta AI website.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Get the download.sh script'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before proceeding, ensure that you have both `wget` and `md5sum` installed.
    You find the download.sh script required for model download in [Meta’s GitHub
    repository](https://github.com/facebookresearch/llama.git). Clone the repository
    and go to `llama` directory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Ensure that you give execution permissions to the script by typing the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Start the download process'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To initiate the download process, you have to run the `download.sh` script.
    Throughout the process, you will be prompted to provide the URL that was sent
    by email as well as the model you want to download.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have the option to download two distinct types of models:'
  prefs: []
  type: TYPE_NORMAL
- en: '***pretrained*** — Llama-2–7b, Llama-2–13b, Llama-2–70b'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***fine-tuned chat*** — Llama-2–7b-chat, Llama-2–13b-chat, Llama-2–70b-chat'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In my case, I’ll get Llama-2–7b & Llama-2–7b-chat.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5657df14b6be6c74df1d12f3360013a6.png)'
  prefs: []
  type: TYPE_IMG
- en: If the download was successful you should find both the tokenizer and the models
    llama-2–7b and llama-2–7b-chat.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19983a212b2f144d840de1e53e655039.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Prepare the local environment**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For optimal isolation, it’s advisable to establish a fresh local environment;
    personally, I use the Conda environment management system for this use case. Let’s
    initiate the creation of a new Conda environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace `yourenvname` with the name you want to give to the environment, and
    `3.10` with the preferred Python version. After creating the environment, you
    can activate it with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then, navigate to the cloned repository and install the needed libraries mentioned
    in the `requirements.txt`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s one more thing you need to do: install the project package in a way
    that lets you change the code and see the effects right away, without having to
    install it again. To make this happen, run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve prepared everything, let’s go ahead and run the model to see
    what happens.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Run inference with `torchrun`
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Torchrun, a utility in PyTorch, simplifies distributed training by automating
    worker assignments, handling failures, supporting elastic setups, and offering
    features beyond `torch.distributed.launch` including custom entry points, parameter
    passing, and log capture.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the cloned repository you should see two examples: `example_chat_completion.py`and
    `example_text_completion.py.`'
  prefs: []
  type: TYPE_NORMAL
- en: Since both scripts are designed for distributed training, we are required to
    set up a few variables. You can simply export them as below or add them to your
    `.bashrc`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`RANK`: The rank of the current process in the distributed training group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WORLD_SIZE`: The total number of processes in the distributed group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MASTER_ADDR`: The address of the master node that coordinates the training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MASTER_PORT`: The port number used to communicate with the master node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To execute `torchrun`, we need to:'
  prefs: []
  type: TYPE_NORMAL
- en: define the `nproc-per-node` as the number of available GPUs,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: provide the `script.py`,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: indicate the model checkpoint directory via `ckpt_dir`,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: specify the tokenizer’s path using `tokenizer_path`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s run `example_text_completion.py` where the initial prompt is :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1122b768da07bf26d25a4ff783e2d6e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/592106ab554f61bddbcd56e988247ae1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: And that’s all. You made it! Now you have the option to change the prompt and
    experiment with other models 😃.
  prefs: []
  type: TYPE_NORMAL
- en: '**A short recap:**'
  prefs: []
  type: TYPE_NORMAL
- en: Visit the **Meta Official Site** and ask for download permission.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Visit [**the Llama 2 repository**](https://github.com/facebookresearch/llama)
    in GitHub and download the **download.sh** script.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Execute the download.sh** and provide the signed URL send by email :'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: https://download.llamameta.net/*?YOUR_SIGNED_URL and select the model weights
    to download
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Prepapare the environment**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Run interference** with torchrun'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2️⃣ Download Llama 2 from HuggingFace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Step 1: Request the download'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To begin, make sure you ask for a download on the [Meta AI website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)
    using the exact email address linked to your Hugging Face account. Accept the
    [license terms](https://ai.meta.com/llama/license/) and [acceptable use policy](https://ai.meta.com/llama/use-policy/).
    Once that’s done, you can ask for access to any of the models available on [Hugging
    Face](https://huggingface.co/meta-llama).
  prefs: []
  type: TYPE_NORMAL
- en: Below you can find the list containing the models that are currently available.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2bae9ef762dc58f83c6bf7d9a9140975.png)'
  prefs: []
  type: TYPE_IMG
- en: cc. Hugging Face
  prefs: []
  type: TYPE_NORMAL
- en: You’ll receive an email from HuggingFace confirming the grated access.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Get the token from HuggingFace'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In case you don’t have a HuggingFace account yet, you will need to create one.
    After creating the account, log in to HuggingFace. Once logged in, locate the
    `Profile` option positioned in the upper right corner and select `Settings`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de689ce4ccfa73399e2774658b8f29cb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Choose the `Access Tokens` option and click the `New token` button in order
    to generate the token.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64575ff4e6fb9986513ef26906dc9ace.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Simply copy the token and return to your notebook. In the next step, we’ll see
    how to access and download the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Authenticate to HuggingFace'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, install `huggingface_hub`module developed by Hugging Face that enables
    you to interact with the Hugging Face Model Hub. The hub hosts various pre-trained
    models. Note that`huggingface_hub.login()` requires the `ipywidgets` package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, import `huggingface_hub` and login to Hugging Face as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: When you run `huggingface_hub.login()`, you’ll be asked to provide your Hugging
    Face authentication token. Once you’ve successfully authenticated, you can download
    llama models. Paste your token and click login. If authenticated you should see
    the following message.
  prefs: []
  type: TYPE_NORMAL
- en: After you’ve been authenticated, you can go ahead and download one of the llama
    models. I will go for `meta-llama/Llama-2–7b-chat-hf` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Download the Llama 2 Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Begin by installing the needed libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the GPU available as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To check your GPU details such as the driver version, CUDA version, GPU name,
    or usage metrics run the command `!nvidia-smi` in a cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, to download the model, we need to import all necessary libraries from
    PyTorch and Hugging Face’s Transformers, initialize the Llama-2–7b chat model
    and its tokenizer, and save them to our disk. Check the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: Once the cell is executed you should see the models in the `huggingface` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Check the directory before moving further. What should be in there?
  prefs: []
  type: TYPE_NORMAL
- en: '`config.json`: Think of this as the manual for how your model operates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch_model.bin`: This is your model''s brain in PyTorch format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Essential tokenizer files: `special_tokens_map.json` and `tokenizer_config.json`
    are like the dictionaries for your model''s language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer.model` llama 2 tokenizer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 5: Load the Llama 2 model from the disk'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In case you have already your Llama 2 models on the disk, you should load them
    first.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, you need :'
  prefs: []
  type: TYPE_NORMAL
- en: '`LlamaForCausalLM` which is like the brain of "Llama 2",'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LlamaTokenizer` which helps "Llama 2" understand and break down words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the path of the models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have reached the last step — testing the interference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step6: Run interference using HuggingFace pipelines'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can assess interference by using the HuggingFace transformers’ pipeline.
    By leveraging pipelines, you can quickly navigate through complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '`text-generation`: Specifies that the pipeline is for generating text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model`: The pre-trained model you’re using for text generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer`: The tokenizer used to process input text and decode model outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_map=”auto”`: This attempts to run the model on the best available device
    (e.g., GPU if available, otherwise CPU).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_new_tokens=512`: Limits the generated output to 512 tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_return_sequences=1`: Requests only one generated sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id=tokenizer.eos_token_id`: the end-of-sequence token ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*In my case, I wanted a few suggestions for a few rock bands, and the outcome
    is really good.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1317429912fe58f6b9a5566cdf76a850.png)'
  prefs: []
  type: TYPE_IMG
- en: '**A short recap of downloading Llama from HuggingFace:**'
  prefs: []
  type: TYPE_NORMAL
- en: Visit the **Meta Official Site** and ask for download permission.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Get the token from HuggingFace
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Authenticate to HuggingFace
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Download the Llama 2 Model
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Load the Llama 2 model from the disk
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Run interference using HuggingFace pipelines
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Final thoughts :'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, we have seen how to download the Llama 2 models to our local
    PC. You have the option to further enhance the model’s performance by employing
    methods such as quantization, distillation, and other approaches that I will discuss
    in a subsequent article. It’s crucial to execute all these steps within a fresh
    virtual environment. Be sure to monitor the usage of your computer’s memory during
    the process. A lot of weird errors can hide behind memory issues. If you want
    to download a quantized model be aware you may need to downgrade the `bitsandbytes`
    library to 0.39.1
  prefs: []
  type: TYPE_NORMAL
- en: Ever tried clicking the “clap” button on Medium more than once? ❤️
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s be friends! ✋. Don’t forget to [subscribe](https://medium.com/subscribe/@anna.bildea)!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/ana-bildea-phd-2339b728/)
    *&* [*X*](https://twitter.com/AnaBildea)!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*If you find my story compelling and wish to show support for my writing, I
    invite you to consider becoming a Medium member, where you can access a vast collection
    of Generative AI, Data Engineering, and Data Science articles.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@anna.bildea/membership?source=post_page-----8a432ed232a4--------------------------------)
    [## Join Medium with my referral link — Bildea Ana'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@anna.bildea/membership?source=post_page-----8a432ed232a4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: See my collection of Generative AI, MLOps, and Responsible AI articles.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ana Bildea, PhD](../Images/acaa243e5f1e9f9254c32b65042c822b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Ana Bildea, PhD](https://medium.com/@anna.bildea?source=post_page-----8a432ed232a4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@anna.bildea/list/generative-ai-30d313b29b80?source=post_page-----8a432ed232a4--------------------------------)11
    stories![](../Images/df56a4e1a4785f73007a1ba8d1191b78.png)![](../Images/b6a7ab27a61a2cd49de8c07ee38f5999.png)![](../Images/8c3c51cf26b3db2c54205da85ad9fe2e.png)![Ana
    Bildea, PhD](../Images/acaa243e5f1e9f9254c32b65042c822b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ana Bildea, PhD](https://medium.com/@anna.bildea?source=post_page-----8a432ed232a4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: MLOps - AI in Production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@anna.bildea/list/mlops-ai-in-production-04b6c81c50c8?source=post_page-----8a432ed232a4--------------------------------)4
    stories![](../Images/8fbedcb9f3f75894caff649172adece1.png)![](../Images/d5014b3b3843fc4b2172bef517cccaa4.png)![](../Images/2dba051abf51711268415c3f1e055a60.png)![Ana
    Bildea, PhD](../Images/acaa243e5f1e9f9254c32b65042c822b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ana Bildea, PhD](https://medium.com/@anna.bildea?source=post_page-----8a432ed232a4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@anna.bildea/list/responsible-ai-10009e82f412?source=post_page-----8a432ed232a4--------------------------------)1
    story![](../Images/46a362cef2c3ddcc9e9a1134400f8a6d.png)'
  prefs: []
  type: TYPE_NORMAL
