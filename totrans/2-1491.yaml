- en: 'Mastering NLP: In-Depth Python Coding for Deep Learning Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掌握NLP：深度学习模型的深入Python编码
- en: 原文：[https://towardsdatascience.com/mastering-nlp-in-depth-python-coding-for-deep-learning-models-a15055e989bf](https://towardsdatascience.com/mastering-nlp-in-depth-python-coding-for-deep-learning-models-a15055e989bf)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mastering-nlp-in-depth-python-coding-for-deep-learning-models-a15055e989bf](https://towardsdatascience.com/mastering-nlp-in-depth-python-coding-for-deep-learning-models-a15055e989bf)
- en: A step-by-step guide with comprehensive code explanations for text classification
    using deep learning in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一份逐步指南，包含全面的代码解释，介绍了如何使用Python进行深度学习的文本分类。
- en: '[](https://eligijus-bujokas.medium.com/?source=post_page-----a15055e989bf--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----a15055e989bf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a15055e989bf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a15055e989bf--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----a15055e989bf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://eligijus-bujokas.medium.com/?source=post_page-----a15055e989bf--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----a15055e989bf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a15055e989bf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a15055e989bf--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----a15055e989bf--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a15055e989bf--------------------------------)
    ·21 min read·Oct 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在[数据科学前沿](https://towardsdatascience.com/?source=post_page-----a15055e989bf--------------------------------)
    ·21分钟阅读·2023年10月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/240511bbc44eecab740578093612ef8b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/240511bbc44eecab740578093612ef8b.png)'
- en: Photo by [Waypixels](https://unsplash.com/@waypixels?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Waypixels](https://unsplash.com/@waypixels?utm_source=medium&utm_medium=referral)
    在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This article came to fruition after reading numerous documentation resources
    and looking at videos on YouTube about textual data, classification, recurrent
    neural networks, and other hot subjects on how to develop a machine-learning project
    using text data. A lot of the information is not that user-friendly and some of
    the parts are obfuscated, thus, I want to save the reader a lot of time and shed
    light on the most important concepts in using textual data in any machine learning
    project.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章是在阅读了大量文档资源和观看了YouTube上的文本数据、分类、递归神经网络及其他热门话题的视频之后完成的。许多信息并不友好，有些部分比较晦涩，因此，我希望节省读者大量时间，并阐明在任何机器学习项目中使用文本数据的最重要概念。
- en: 'The supporting code for the examples presented here can be found at: [https://github.com/Eligijus112/NLP-python](https://github.com/Eligijus112/NLP-python)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的示例支持代码可以在以下网址找到：[https://github.com/Eligijus112/NLP-python](https://github.com/Eligijus112/NLP-python)
- en: 'The topics covered in this article will be:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文覆盖的主题包括：
- en: '***Converting text to sequences***'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***将文本转换为序列***'
- en: '***Converting sequence indexes to embedded vectors***'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***将序列索引转换为嵌入向量***'
- en: '***In-depth RNN explanation***'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***深入的RNN解释***'
- en: '***The loss function for classification***'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***分类的损失函数***'
- en: '***Full NLP pipeline using Pytorch***'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***使用Pytorch的完整NLP管道***'
- en: '**NLP** stands for **N**atural **L**anguage **P**rocessing¹. This is a huge
    topic about how to use both hardware and software in tasks like:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**NLP** 代表 **N**atural **L**anguage **P**rocessing¹。这是一个关于如何在任务中使用硬件和软件的广泛话题，例如：'
- en: Translating one language to another
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一种语言翻译成另一种语言
- en: Text classification
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Text summarization
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本摘要
- en: Next token prediction
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一步的标记预测
- en: Named entity recognition
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: And much much more. In this article, I want to cover the most popular techniques
    and familiarize the reader with the concepts by simple and coded examples.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多。在这篇文章中，我想覆盖最受欢迎的技术，并通过简单的示例和代码使读者熟悉这些概念。
- en: A lot of tasks in NLP start by ***tokenizing the text².***
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中，许多任务从***对文本进行分词²开始***。
- en: Text tokenization is a process where we split the original text into smaller
    parts — **tokens**. The tokens can be either characters, subwords, words, or a
    mix of all three.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分词是将原始文本拆分成较小部分——**标记**的过程。这些标记可以是字符、子词、单词或这三者的混合。
- en: 'Consider the string:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下字符串：
- en: '**“NLP in Python is fun and very well documented. Let’s get started!”**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**“Python中的NLP很有趣，文档也做得很好。让我们开始吧！”**'
- en: I will use word-level tokens because the same logic would apply to lower-level
    tokenization as well.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用词级标记，因为相同的逻辑也适用于更低级别的标记化。
- en: 'First of all, let us define and apply a function that separates the punctuations
    from words:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义并应用一个将标点符号与单词分开的函数：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Applying the above function to text we get:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述函数应用于文本，我们得到：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, let us define a function that creates a dictionary that links **words to
    indexes** and **indexes to words.** The word that comes up the first in the text
    will have a lower smaller number index. The ordering of the indexes is completely
    arbitrary — the only rule is that **each word should have a unique index.**
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个函数，该函数创建一个将**单词映射到索引**和**索引映射到单词**的字典。文本中出现的第一个单词将具有较小的索引号。索引的排序完全是任意的——唯一的规则是**每个单词应有一个唯一的索引**。
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The **word2idx** dictionary is the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**word2idx** 字典如下：'
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The **idx2word** dictionary is the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**idx2word** 字典如下：'
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: From the above dictionaries, we can see that our data has in total **15 tokens**.
    Throughout the article, I will call the indexes of words as tokens.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述字典中，我们可以看到我们的数据总共有**15个标记**。在本文中，我将把单词的索引称为标记。
- en: Now let us define two functions — one that creates a sequence of indexes and
    another that translates the sequences of indexes to words.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义两个函数——一个创建索引序列，另一个将索引序列翻译为单词。
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The token sequence:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 标记序列：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The word sequence, decoded from the token indexes:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从标记索引解码得到的词序列：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let us create a new text, one which has not been seen by the indexer:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的文本，这是索引器未见过的：
- en: '**“As I said, Python is a very good tool for NLP”**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**“正如我所说，Python是一个非常好的NLP工具”**'
- en: 'The whole pipeline, from preprocessing to token sequence:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从预处理到标记序列的整个流程：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The results:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now the token sequence is such that we could come across it in the wild — not
    all the words were seen by the indexer and the token indexes are not ordered.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在标记序列是这样，我们可能在实际应用中遇到它——不是所有单词都被索引器见过，且标记索引没有排序。
- en: This concludes the first part of this article. To recap, we have created some
    crucial functions to create the tokens from text which we will use in the upcoming
    steps.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了本文的第一部分。回顾一下，我们已经创建了一些关键函数来从文本中创建标记，我们将在接下来的步骤中使用它们。
- en: Now that we have an index for every word we can start creating the **features**
    for a machine learning model. As it stands now, we only have a number of when
    the indexer has seen the word from the texts. This does not capture either the
    magnitude of a word or the ordinal information and we cannot in any way compare
    the words — the index is just used as a lookup value.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经为每个单词创建了一个索引，我们可以开始为机器学习模型创建**特征**。目前，我们只有索引器从文本中看到单词的次数。这既不能捕捉单词的大小，也不能捕捉顺序信息，我们无法以任何方式比较单词——索引仅用作查找值。
- en: To go from an index to a feature, we will create a word vector, or, using a
    more popular term, word embedding.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从索引到特征，我们将创建一个词向量，或者使用更流行的术语，词嵌入。
- en: '**A word embedding is a numerical vector representation of a word³**. It links
    text to a vector space. For example,'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**词嵌入是一个单词的数值向量表示³**。它将文本链接到一个向量空间。例如，'
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The embedding dimension is fixed and defined by the user. When building an NLP
    system, each token must have the same dimension as an embedding.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入维度是固定的，由用户定义。在构建NLP系统时，每个标记必须与嵌入具有相同的维度。
- en: The most popular technique to create word embeddings is to define the number
    of coordinates a vector has and randomly simulate each coordinate from a normal
    distribution with mean 0 and variance 1.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 创建词嵌入的最常见技术是定义向量的坐标数，并从均值为0、方差为1的正态分布中随机模拟每个坐标。
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The above function, by default, will create a vector of length 16\. Let us
    apply the function to each of the words in our dictionary. For reading purposes,
    we will limit the embedding dimension to 6:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，上述函数将创建一个长度为16的向量。让我们将函数应用于字典中的每个单词。为了阅读方便，我们将嵌入维度限制为6：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The **idx2embeddings** and **word2embeddings** dictionaries look like this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**idx2embeddings** 和 **word2embeddings** 字典如下所示：'
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Usually in literature and in practice, the embedding matrix object is used
    when solving NLP tasks. An embedding matrix is nothing more than the idx2embeddings
    dictionary converted to an array. The first row of the matrix is the first entry
    in the idx2embeddings, the second row is the second entry, and so on. Let us create
    our very own embedding matrix:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在文献和实践中，嵌入矩阵对象用于解决自然语言处理任务。嵌入矩阵不过是将idx2embeddings字典转换为数组。矩阵的第一行是idx2embeddings中的第一个条目，第二行是第二个条目，以此类推。让我们创建我们自己的嵌入矩阵：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The general rule is that the **embedding matrix has the number of rows as the
    total amount of unique tokens in the text and the column number is equal to the
    embedding dimension.**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一般规则是，**嵌入矩阵的行数等于文本中唯一标记的总数，列数等于嵌入维度**。
- en: Now that we have an embedding for each word, we have created essentially 6 features
    for each word. As they stand, they do not make much more sense than the word-to-index
    link.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们为每个单词都创建了一个嵌入，我们实际上为每个单词创建了6个特征。就现在而言，它们的意义并不比词到索引的链接更大。
- en: But we should see these embeddings not as a final product, **but as the initialization
    of weights for a machine learning model**. During training, every coordinate will
    be trainable and the final embedding matrix will look much different than the
    one we have initialized.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们应该把这些嵌入视为**机器学习模型权重初始化的开始**。在训练过程中，每个坐标都是可训练的，最终的嵌入矩阵会与我们初始化时的矩阵大相径庭。
- en: 'Now that we have an embedding vector for each word, let us stop for a moment
    and revisit our tokenized ***sequence*** of words:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们为每个单词都有了一个嵌入向量，让我们停下来重新审视我们的分词***序列***：
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In human language, the sequence of words that we want to say in a sentence is
    very important. Switching two words together may end up transforming the whole
    meaning of a sentence. Thus, it is very important that we create data for a machine-learning
    model that captures the essence of a sequence.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在人类语言中，我们在句子中想要表达的单词顺序非常重要。交换两个单词可能会改变整个句子的含义。因此，为机器学习模型创建能够捕捉序列本质的数据是非常重要的。
- en: 'To do this, we will create a ***tensor*** in the following format⁴:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将创建一个***张量***，格式如下⁴：
- en: '**(sequence length, batch size, embedding dimension size)**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**（序列长度，批量大小，嵌入维度大小）**'
- en: 'It is often hard to imagine what is this tensor of 3 dimensions. To visualize
    it, refer to this cube:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通常很难想象这个3维张量是什么。要可视化它，可以参考这个立方体：
- en: '![](../Images/53b0478b80b5c50f2e37037015f08bec.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53b0478b80b5c50f2e37037015f08bec.png)'
- en: A typical sequence tensor (4 x 5 x 4); Graph by author
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的序列张量（4 x 5 x 4）；作者图表
- en: 'The above tensor is created from the following sentences:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 上述张量是由以下句子创建的：
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The batch size is equal to the number of documents or data points or any other
    unit of text that we have in our data. In the above example, we have 5 pieces
    of text, thus, the **batch size is equal to 5**.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小等于我们数据中文档或数据点或任何其他文本单位的数量。在上述示例中，我们有5篇文本，因此，**批量大小等于5**。
- en: 'For simplicity''s sake, I have created sentences with 4 words. Thus, the maximum
    sequence length is equal to 4:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我创建了包含4个单词的句子。因此，最大序列长度等于4：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The embedding vector dimension is also equal to 4 (this is just a coincidence,
    the embedding size can be much larger than the sequence length). Thus, every token
    in the sentence has 4 numbers associated with it (an embedding vector). In the
    tensor graph, each token’s embedding vector is colored in a different way. Thus,
    if the embedding vector looks like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入向量维度也等于4（这只是巧合，嵌入大小可以远大于序列长度）。因此，句子中的每个标记都有4个与之关联的数字（一个嵌入向量）。在张量图中，每个标记的嵌入向量用不同的颜色标记。因此，如果嵌入向量看起来像这样：
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then the sequence is:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后序列是：
- en: '![](../Images/3e4705e4e3e893e2f22a46d1678fe15d.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e4705e4e3e893e2f22a46d1678fe15d.png)'
- en: Sequence of “The frogs are gray”; Graph by author
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: “青蛙是灰色的”序列；作者图表
- en: Please note that some authors visualize the tensor for the sequence modeling
    with different axe orientations. My point is that one should always have a handy
    cube in their head when thinking about sequence modeling with the embedding matrix.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，有些作者以不同的轴方向可视化序列建模的张量。我的观点是，当考虑嵌入矩阵的序列建模时，应该始终在脑海中有一个随手可得的立方体。
- en: Now that we have a picture of a sequence in our heads, what kind of machine
    learning models could we use to output something at the end of the sequence? One
    class of models that works especially well with sequences is recurrent neural
    networks or **RNN**s for short⁵.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们脑海中有了序列的图像，我们可以使用什么样的机器学习模型来输出序列末尾的结果？一种特别适合序列的模型是递归神经网络，简称**RNN**。
- en: By definition, a recurrent neural network is a neural network that contains
    loops, allowing information to be stored within the network.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，递归神经网络是一种包含循环的神经网络，允许在网络内存储信息。
- en: 'Let us draw a vanilla feed-forward neural network with the inputs that we have.
    For each time step in our sequence, the token gets “split” into 4 features, taken
    from the embedding matrix and passed through the network:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制一个简单的前馈神经网络，并使用我们拥有的输入。在序列中的每个时间步，令牌被“拆分”成4个特征，从嵌入矩阵中提取并通过网络传递：
- en: '![](../Images/787d01ca98a5324aec2b447d601765a4.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/787d01ca98a5324aec2b447d601765a4.png)'
- en: Feedforward NN; Graph by author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络；图表由作者提供
- en: Each weight w1, w2, w3, w4, and w5 get initialized randomly in the network and
    used to calculate the final output of such a forward pass through the network.
    The activation function of choice in the middle neuron is **RELU**. The output
    neuron’s activation function is **linear**.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 每个权重w1、w2、w3、w4和w5在网络中随机初始化，并用于计算通过网络的前馈传递的最终输出。中间神经元的激活函数选择**RELU**。输出神经元的激活函数是**linear**。
- en: 'For example, for the word **“The”** and with weights initialized with values
    **0.01, 1.14, -1.16, 1.75,** and **2.1,** the network would look like this:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于单词**“The”**，权重初始化为**0.01、1.14、-1.16、1.75**和**2.1**，网络将如下所示：
- en: '![](../Images/e10ccceef65f49d3f372892a3bd2075f.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e10ccceef65f49d3f372892a3bd2075f.png)'
- en: Prediction for the token “The”; graph by author
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: “The”令牌的预测；图表由作者提供
- en: 'To make our network a recurrent network, we need to store some information
    after each new token gets fed forward through the network. We do that by saving
    the output information right before the final output information. To this, we
    add one more weight to the network and expand our network:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的网络成为递归网络，我们需要在每个新令牌通过网络前馈后保存一些信息。我们通过在最终输出信息之前保存输出信息来实现这一点。为此，我们向网络中添加了一个权重并扩展了网络：
- en: '![](../Images/8c1c463ba7d8eeceb9f6a5571a2af6a7.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c1c463ba7d8eeceb9f6a5571a2af6a7.png)'
- en: Unrolled graph of RNN; graph by author
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 展开的RNN图；图表由作者提供
- en: 'We have 3 sets of weights in the above graph: **Wx, Wa**, and **Wo**.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表中有3组权重：**Wx**、**Wa**和**Wo**。
- en: The **Wx** and **Wo** are the same weights as in the simple feedforward NN.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**Wx**和**Wo**是与简单前馈神经网络相同的权重。'
- en: The **Wa** weight is used to multiply the signal that gets fed forward to the
    output layer at each token. Then this signal gets used at the next token level
    at the ***sum and activation*** neuron place.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**Wa**权重用于乘以在每个令牌的输出层前馈的信号。然后，这个信号在下一个令牌级别的***sum and activation***神经元处被使用。'
- en: 'Let us initiate the same **Wx**, and **Wo** weights and additionally initialize
    the **Wa** weight to 1.12\. Then the calculations for the full sequence of tokens
    that we have will be as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们初始化相同的**Wx**和**Wo**权重，并额外将**Wa**权重初始化为1.12。然后，对于我们拥有的完整序列的计算如下：
- en: 'The input for the activation function at the “frog” token level (or the second
    step in our sequence) is calculated as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在“frog”令牌级别（或序列中的第二步）的激活函数输入计算如下：
- en: '**1.25 * 0.01 + 0.69 * 1.14 -0.54 * -1.16 + 0.19 * 1.75 + 0.544 * 1.12 = 2.34228**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.25 * 0.01 + 0.69 * 1.14 -0.54 * -1.16 + 0.19 * 1.75 + 0.544 * 1.12 = 2.34228**'
- en: '![](../Images/9cff425dbcf6116c83949e15f1410999.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cff425dbcf6116c83949e15f1410999.png)'
- en: From first step to second; Graph by author
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从第一步到第二步；图表由作者提供
- en: 'The last two steps are presented in the bellow graph:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个步骤在下面的图表中呈现：
- en: '![](../Images/4535f6608e2b4f98199e087487677dc3.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4535f6608e2b4f98199e087487677dc3.png)'
- en: The end of the sequence; Graph by author
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 序列的末尾；图表由作者提供
- en: Each step of the RNN produced the following sequence of numbers
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的每一步产生了以下数值序列：
- en: '**[1.14, 4.91, 2.73, 2.56]**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1.14, 4.91, 2.73, 2.56]**'
- en: 'By default, in most of the packages (like Pytorch or Tensorflow), only the
    last step’s output is returned: **2.56**. If the parameter **return_sequences
    = True,** then the full sequence is returned.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，在大多数软件包（如Pytorch或Tensorflow）中，只返回最后一步的输出：**2.56**。如果参数**return_sequences
    = True**，则返回完整序列。
- en: Keep in mind, that throughout the unrolling of the network, the **Wx, Wa,**
    and **Wo** are fixed and stay the same. These weights are updated during the backpropagation
    process.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在网络的展开过程中，**Wx、Wa** 和 **Wo** 是固定的，并保持不变。这些权重在反向传播过程中会被更新。
- en: 'The last part of the above pipeline is to connect the final output layer to
    the output of our RNN network. Depending on the task we want to solve, we can:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 上述管道的最后部分是将最终输出层连接到我们 RNN 网络的输出。根据我们要解决的任务，我们可以：
- en: Add a linear layer if we want to solve a regression problem (for example, given
    a job ad’s description, predict the salary)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们想要解决回归问题（例如，给定职位广告的描述，预测薪资），请添加一个线性层。
- en: Add a sigmoid layer if want to solve a classification problem (is the movie
    review’s sentiment positive or negative)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果想要解决分类问题（例如，电影评论的情感是正面还是负面），请添加一个 sigmoid 层。
- en: Other layers for a custom task.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于自定义任务的其他层。
- en: We will add a sigmoid at the end of the network and the model predicts a value
    between 0 and 1.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在网络的末尾添加一个 sigmoid 层，模型预测一个介于 0 和 1 之间的值。
- en: 'A sigmoid is a function that inputs one number and outputs a number in the
    range of (0, 1). The formula is:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 是一个函数，它输入一个数字并输出一个范围在 (0, 1) 之间的数字。公式为：
- en: '![](../Images/15f9bf96242b9fafb991d114d9af69fb.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15f9bf96242b9fafb991d114d9af69fb.png)'
- en: Sigmoid formula
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 公式
- en: 'If we define the weight Ws on the output of the RNN to be equal to 0.85, the
    full flow of a sequence through our network would look like this:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 RNN 输出上的权重 Ws 定义为 0.85，那么序列在我们网络中的完整流动将如下所示：
- en: '![](../Images/1506875c584a48c20abf45d16493428a.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1506875c584a48c20abf45d16493428a.png)'
- en: Full RNN pipeline; Graph by author
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 RNN 管道；图表由作者提供
- en: Recall that the output of the RNN is one number equal to 2.56 (hence the input
    to the sigmoid layer is 2.56 * 0.85 = 2.18).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，RNN 的输出是一个等于 2.56 的数字（因此 sigmoid 层的输入是 2.56 * 0.85 = 2.18）。
- en: 'During training, the weights that will be adjusted will be:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，将会调整的权重包括：
- en: Wx (4)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wx (4)
- en: Wa (1)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wa (1)
- en: Wo (1)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wo (1)
- en: Ws (1)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ws (1)
- en: All the numbers in the embedding matrix (unique tokens* 4)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入矩阵中的所有数字（唯一标记 * 4）
- en: During training, all the weights will be adjusted to produce the best probability
    outputs based on a user-defined loss function.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，所有权重将会被调整，以根据用户定义的损失函数产生最佳的概率输出。
- en: Please note, that I have omitted the bias variables from the weight calculation.
    The logic does not change one bit, but an additional 1 parameter to Wx, Wa, Wo,
    and Ws would be added.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我在权重计算中省略了偏置变量。逻辑并没有改变，但额外的 1 个参数会被添加到 Wx、Wa、Wo 和 Ws。
- en: Before diving into the practical implementation of the above graphs, we need
    to define one last thing — the loss function which we will optimize. We will use
    a very popular loss function for binary classification⁶ called the **binary cross-entropy
    loss function:**
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入实际实现上述图表之前，我们需要定义最后一个要优化的东西——损失函数。我们将使用一种非常流行的二分类损失函数⁶，称为 **二元交叉熵损失函数**：
- en: '![](../Images/776ca9456ed447dc0a20a0a2d85433ad.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/776ca9456ed447dc0a20a0a2d85433ad.png)'
- en: BCE loss; Formula by author
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: BCE 损失；公式由作者提供
- en: The above equation looks intimidating but let us dissect it one symbol at a
    time.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程看起来很吓人，但让我们逐一解析每个符号。
- en: The **BCE(w)** part means that we can only change the weights in our network
    and this is the only parameter for the function.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**BCE(w)** 部分意味着我们只能在网络中改变权重，这也是函数的唯一参数。'
- en: '**The sum of i = 1 to N** means that when we calculate the BCE, we are using
    all the data.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**i 从 1 到 N 的总和** 意味着当我们计算 BCE 时，我们使用了所有的数据。'
- en: '**y_i and x_i** are the ith data points from our dataset. These are fixed and
    we cannot adjust them in any way.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**y_i 和 x_i** 是来自数据集的第 i 个数据点。这些数据是固定的，我们不能以任何方式调整它们。'
- en: The **f(w, x_i)** means the output of our function (an RNN network with the
    embedding layer and a sigmoid output) given a set of weights and the ith observation
    of x.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**f(w, x_i)** 代表在给定一组权重和第 i 个 x 观测值的情况下，我们的函数（一个带有嵌入层和 sigmoid 输出的 RNN 网络）的输出。'
- en: 'To better understand the above loss, imagine that we have 2 observations in
    the form of (y, x): [1, 0.47], [0, 65.23]. Let us say that we have a set of weights
    **w** and f(**w**, 0.47) = 0.88 and f(**w**, 65.23) = 0.12.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解上述损失，假设我们有两个观测值 (y, x) 的形式：[1, 0.47]，[0, 65.23]。假设我们有一组权重 **w** 和 f(**w**,
    0.47) = 0.88，f(**w**, 65.23) = 0.12。
- en: 'Then to calculate the BCE loss, we just plug in the values:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了计算 BCE 损失，我们只需代入这些值：
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now let us say that we have another set of weights w that produces the following
    probabilities:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们有另一组权重 w，产生以下概率：
- en: f(**w**, 0.47) = 0.95 and f(**w**, 65.23) = 0.08
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: f(**w**, 0.47) = 0.95 和 f(**w**, 65.23) = 0.08
- en: 'The BCE(w) is then:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 BCE(w) 是：
- en: '[PRE20]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The new set of weights is better because the loss function is smaller.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 新的权重集更好，因为损失函数较小。
- en: The intuition here is that the smaller the binary cross entropy, the more certain
    the model is to give a high probability when the true Y value is 1 and to give
    a low probability when the true Y value is 0.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的直觉是，二进制交叉熵越小，模型在真实 Y 值为 1 时给出高概率的确定性越高，而在真实 Y 值为 0 时给出低概率的确定性越高。
- en: 'Now let us put everything together using Pytorch. We will try to classify whether
    a tweet’s sentiment is positive or negative. The dataset was obtained here⁷: [https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用 Pytorch 将一切结合起来。我们将尝试分类推文的情感是正面还是负面。数据集来源于此⁷：[https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)
- en: The data is about whether a tweet about a computer game is positive or negative.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是关于计算机游戏的推文是正面还是负面。
- en: We will encode the Y variable such that 1 means a negative sentiment and 0 means
    a positive sentiment.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对 Y 变量进行编码，其中 1 表示负面情绪，0 表示正面情绪。
- en: 'We split the dataset into train and test sets (34410 and 8603 rows respectively)
    and apply all the functions we have defined before:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集拆分为训练集和测试集（分别为 34410 和 8603 行），并应用之前定义的所有函数：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: There is a small adjustment to the **create_word_index()** function where we
    shift each token index by 1 because we will in a bit introduce the concept of
    **padding,** which is necessary for Pytorch.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对 **create_word_index()** 函数进行了小的调整，我们将每个令牌索引移位 1，因为我们稍后将引入 **padding** 概念，这是
    Pytorch 所必需的。
- en: '[PRE22]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Applying the function we get about 29k unique tokens in the text:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 应用函数后，我们得到约 29k 个唯一令牌：
- en: '[PRE23]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let us create sequences of tokens from our dataset:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据集中创建令牌序列：
- en: '[PRE24]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The average amount of words in a tweet about an airline is equal to **21.8**
    words. So far, our dataset looks like this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 关于航空公司推文的平均词数等于**21.8**个词。目前，我们的数据集如下：
- en: '![](../Images/40fb31f78b6e8eb72594311dc2bfdc3f.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40fb31f78b6e8eb72594311dc2bfdc3f.png)'
- en: A snippet of data; Picture by author
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 数据片段；图片由作者提供
- en: 'The lengths of the **text_int** column are not equal to one another. Most of
    the machine learning frameworks expect that the dimension of X would be the same.
    Thus, we will introduce the padding function, which adds the sequences with 0
    to the right up to the desired sequence length:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**text_int** 列的长度不相等。大多数机器学习框架期望 X 的维度是相同的。因此，我们将引入填充函数，该函数将序列右侧填充 0 直到所需的序列长度：'
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let us apply the function and see what happens:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用这个函数，看看会发生什么：
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../Images/0c451a88a739374b5bbfce5d7947d850.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c451a88a739374b5bbfce5d7947d850.png)'
- en: Padded text_int column; Picture by author
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 填充后的 text_int 列；图片由作者提供
- en: The last row in the snippet above visualized the effect of padding.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上述片段中的最后一行展示了填充的效果。
- en: 'To get an even clearer picture, a small example of padding would be the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清晰的了解，填充的小例子如下：
- en: 'Let us say we have two sequences: [1, 2, 5, 9, 3] and [1, 6, 12]. If we fix
    the **pad_length to 4**, then the sequences would become:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个序列：[1, 2, 5, 9, 3] 和 [1, 6, 12]。如果我们将 **pad_length 设置为 4**，那么这些序列将变成：
- en: '**[1, 2, 5, 9, 3] -> [1, 2, 5, 9]**'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1, 2, 5, 9, 3] -> [1, 2, 5, 9]**'
- en: '**[1, 6, 12] -> [1, 6, 12, 0]**'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1, 6, 12] -> [1, 6, 12, 0]**'
- en: 'We now can define our model class and data loader class in Pytorch:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在 Pytorch 中定义我们的模型类和数据加载器类：
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Training and testing of the model:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的训练和测试：
- en: '[PRE29]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The loss vs epoch plot:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 损失与时代图：
- en: '[PRE30]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](../Images/d1d56352f3d3cce6cf6f0fe483064b50.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1d56352f3d3cce6cf6f0fe483064b50.png)'
- en: Loss vs epoch; Picture by author
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 损失与时代；图片由作者提供
- en: 'The accuracy in the validation set:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集中的准确率：
- en: '[PRE31]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The resulting test accuracy is **~75%** which is really not bad for so few lines
    of code and feature engineering.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 结果测试准确率为 **~75%**，对于如此少的代码和特征工程来说真的不错。
- en: 'To summarize, in this article, I have covered the following concepts:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在本文中，我涵盖了以下概念：
- en: Text to indexes (tokenization).
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本到索引（分词）。
- en: Word embeddings.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入。
- en: An in-depth explanation of an RNN layer.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 RNN 层的深入解释。
- en: The loss function for binary classification.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二分类的损失函数。
- en: How to wrap everything in Pytorch and create a model.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在 Pytorch 中封装一切并创建一个模型。
- en: Happy learning and happy coding!
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 祝学习愉快，编码愉快！
- en: '[1]'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]'
- en: '**Name:** Natural language processing'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**名称：** 自然语言处理'
- en: '**URL**: [https://en.wikipedia.org/wiki/Neuro-linguistic_programming](https://en.wikipedia.org/wiki/Natural_language_processing)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**网址:** [https://en.wikipedia.org/wiki/Neuro-linguistic_programming](https://en.wikipedia.org/wiki/Natural_language_processing)'
- en: '[2]'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]'
- en: '**Name:** NLP | How tokenizing text, sentence, words works'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**名称:** NLP | 如何进行文本、句子、词语的分词'
- en: '**URL:** [https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/](https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**网址:** [https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/](https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/)'
- en: '[3]'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]'
- en: '**Name:** Word embedding'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**名称:** 词嵌入'
- en: '**URL:** [https://en.wikipedia.org/wiki/Word_embedding](https://en.wikipedia.org/wiki/Word_embedding)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**网址:** [https://en.wikipedia.org/wiki/Word_embedding](https://en.wikipedia.org/wiki/Word_embedding)'
- en: '[4]'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[4]'
- en: '**Name:** Time series forecasting'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**名称:** 时间序列预测'
- en: '**URL:** [https://www.tensorflow.org/tutorials/structured_data/time_series](https://www.tensorflow.org/tutorials/structured_data/time_series)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**网址:** [https://www.tensorflow.org/tutorials/structured_data/time_series](https://www.tensorflow.org/tutorials/structured_data/time_series)'
- en: '[5]'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[5]'
- en: '**Name:** Recurrent neural network'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**名称:** 循环神经网络'
- en: '**URL:** [https://en.wikipedia.org/wiki/Recurrent_neural_network](https://en.wikipedia.org/wiki/Recurrent_neural_network)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**网址:** [https://en.wikipedia.org/wiki/Recurrent_neural_network](https://en.wikipedia.org/wiki/Recurrent_neural_network)'
- en: '[6]'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[6]'
- en: '**Name:** Loss functions for classification'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**名称:** 分类的损失函数'
- en: '**URL:** [https://en.wikipedia.org/wiki/Loss_functions_for_classification](https://en.wikipedia.org/wiki/Loss_functions_for_classification)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**网址:** [https://en.wikipedia.org/wiki/Loss_functions_for_classification](https://en.wikipedia.org/wiki/Loss_functions_for_classification)'
- en: '[7]'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[7]'
- en: '**Name:** Twitter Sentiment Analysis'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**名称:** Twitter 情感分析'
- en: '**URL:** [https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**网址:** [https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)'
- en: '**Dataset Licence:** [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集许可证:** [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)'
