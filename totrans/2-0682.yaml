- en: Decision Tree Regressor in Excel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/decision-tree-regressor-in-excel-2d29d16df1db](https://towardsdatascience.com/decision-tree-regressor-in-excel-2d29d16df1db)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/704b63133df778fd0325d550992bcd91.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Kevin Young](https://unsplash.com/@kevinjyoung?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: A Step-by-Step Guide for Machine Learning Beginners
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@angela.shi?source=post_page-----2d29d16df1db--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----2d29d16df1db--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2d29d16df1db--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2d29d16df1db--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----2d29d16df1db--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2d29d16df1db--------------------------------)
    ·6 min read·Mar 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: I am writing a series of articles about implementing machine learning algorithms
    using Excel, which is an excellent tool for understanding the workings of these
    algorithms without programming.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will implement the algorithm of the decision tree regressor
    step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will use a Google Sheet to demonstrate the implementation process. If you’d
    like to access this sheet, as well as others I’ve developed — such as linear regression
    with gradient descent, logistic regression, neural networks with backpropagation,
    KNN, k-means, and more to come — please consider supporting me on Ko-fi. You can
    find all of these resources at the following link: [https://ko-fi.com/s/4ddca6dff1](https://ko-fi.com/s/4ddca6dff1)'
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Decision Tree with a Simple dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s use a simple dataset with only one continuous feature.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9df88702c0faaf46171e4d283c42ce9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision tree regression in Excel simple dataset — image by author
  prefs: []
  type: TYPE_NORMAL
- en: We can visually guess that for the first split, there are two possible values
    one around 5.5 and the other around 12\. Now the question is, which one do we
    choose?
  prefs: []
  type: TYPE_NORMAL
- en: To determine this, we can see a result from scikit learn using the DecisionTreeRegressor
    estimator. The image below shows that the first split is 5.5 since it leads to
    the lowest squared error. What does this mean exactly?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22cf33bf6a49b0ffef0520a6dfb1a715.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple Decision tree regression — image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'This is exactly what we are going to find out: how do we determine the value
    for the first split with an implementation in Excel? Once we determine the value
    for the first split, we can apply the same process for the following splits. That
    is why we will only implement the first split in Excel.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic Principle of Decision Tree Regressors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision tree algorithms in 3 steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I wrote [an article to always distinguish three steps of machine learning to
    learn it in an effective way](https://medium.com/towards-data-science/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1),
    and let’s apply the principle to Decision Tree Regressors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Model:** the model here is a set of rules, it is interesting to notice
    that it is different from a mathematical function-based model in the sense that
    for linear regression, we can write the model in the following form: y=aX+b, and
    the parameters a and b are to be determined. For a decision tree, the model is
    not parametric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2\. Model fitting:** for a decision tree, we also call this process fully
    growing a tree. In the case of a Decision Tree Regressor, the leaves will contain
    only one observation with thus a MSE of zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3\. Model tuning:** for a decision tree, we also call it pruning, which consists
    of optimizing the hyperparameters such as the minimum number of observations in
    the leaves and the maximum depth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Growing a tree consists of recursively partitioning the input data into smaller
    and smaller chunks or regions. For each region, a prediction can be calculated.
    In the case of regression, the prediction is the average of the target variable
    for the region.
  prefs: []
  type: TYPE_NORMAL
- en: At each step of the building process, the algorithm selects the feature and
    the split value that maximizes the one criterion, and in the case of a regressor,
    it is often the Mean Squared Error (MSE) between the actual value and the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning or pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pruning process can be seen as dropping nodes and leaves from a fully grown
    tree, or it is also equivalent to say that the building process stops when a criterion
    is met, such as a maximum depth or a minimum number of samples in each leaf node.
    And these are the hyperparameters that can be optimized with the tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: Below we have some examples of trees with different value of max depth.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce425193980e93660c02103190c00dbc.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision tree regression with different max depth— image by author
  prefs: []
  type: TYPE_NORMAL
- en: Inferencing process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the decision tree regressor is built, it can be used to predict the target
    variable for new input instances by applying the rules and traversing the tree
    from the root node to a leaf node that corresponds to the input’s feature values.
  prefs: []
  type: TYPE_NORMAL
- en: The predicted target value for the input instance is then the mean of the target
    values of the training samples that fall into the same leaf node.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in Excel of the First Split
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are the steps we will follow:'
  prefs: []
  type: TYPE_NORMAL
- en: List all possible splits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each split, we will calculate the MSE (Mean Squared Error)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will select the split that minimizes the MSE as the optimal next split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All possible splits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we have to list all the possible splits that are the average values of
    two consecutive values. There is no need to test more values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62f2a2716d465906a80f571bac86e760.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision tree regression in Excel with possible splits — image by author
  prefs: []
  type: TYPE_NORMAL
- en: MSE calculation for each possible split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a starting point, we can calculate the MSE before any splits. This also means
    that the prediction is just the average value of y. And the MSE is equivalent
    to the Standard Deviation of y.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the idea is to find a split so that the MSE with a split is lower than
    before. It is possible that the split does not significantly improve the performance
    (or lower the MSE), then the final tree would be trivial, that is the average
    value of y.
  prefs: []
  type: TYPE_NORMAL
- en: For each possible split, we can then calculate the MSE (Mean Squared Error).
    The image below shows the calculation for the first possible split, which is x
    = 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ef5116c70eb2a2e6781ce45ac54bae1.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision tree regression in Excel MSE for all possible splits — image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the details of the calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cut the dataset into two regions: with the value x=2, we determine two possibilities
    x<2 or x>2, so the x axis is cut into two parts.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the prediction: for each part, we calculate the average of y. That
    is the potential prediction for y.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the error: then we compare the prediction to the actual value of
    y'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the squared error: for each observation, we can know calculate the
    square error.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/93db904dbc1354de6e8485d37f62ac8e.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision tree regression in Excel with all possible splits — image by author
  prefs: []
  type: TYPE_NORMAL
- en: Optimal split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For each possible split, we do the same to obtain the MSE. In Excel, we can
    copy and paste the formula and the only value that changes is the possible split
    value for x.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/271d64a43ea7187bdcd8061e91fe9289.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision tree regression in Excel splits— image by author
  prefs: []
  type: TYPE_NORMAL
- en: Then we can plot the MSE on the y-axis and the possible split on the x-axis,
    and now we can see that there is a minimum of MSE for x=5.5, this is exactly the
    result obtained with python code.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79041106a2a75b51b2bb2fa4ce12cb2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision tree regression in Excel Minimumization of MSE — image by author
  prefs: []
  type: TYPE_NORMAL
- en: Exercise you can do
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, you can play with the Google Sheet:'
  prefs: []
  type: TYPE_NORMAL
- en: you can modify the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: you can introduce a categorical feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: you can try to find the next split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: you can change the criterion, instead of MSE, you can use absolute error, Poisson
    or friedman_mse as indicated in the documentation of [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: you can change the target variable to a binary variable, normally, this becomes
    a classification task, but 0 or 1 are also numbers so the criterion MSE still
    can be applied. But if you want to create a proper classifier, you have to apply
    the usual criterion Entroy or Gini. I will soon publish another article for the
    Decision Tree Classifier, stay tuned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Excel, it is possible to implement one split to gain more insights into
    how Decision Tree Regressors work. Even though we didn’t create a full tree, it
    is still interesting since the most important part is finding the optimal split
    among all possible splits.
  prefs: []
  type: TYPE_NORMAL
- en: 'I write about machine learning and data science and I explain complex concepts
    in a clear way. Please follow me with the link below and get full access to my
    articles: [https://medium.com/@angela.shi/membership](https://medium.com/@angela.shi/membership)'
  prefs: []
  type: TYPE_NORMAL
