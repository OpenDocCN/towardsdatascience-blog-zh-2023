- en: A Practical Introduction to LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148](https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3 levels of using LLMs in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)
    ·7 min read·Jul 13, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: This is the first article in a [series](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)
    on using Large Language Models (LLMs) in practice. Here I will give an introduction
    to LLMs and present 3 levels of working with them. Future articles will explore
    practical aspects of LLMs, such as how to use [OpenAI’s public API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971),
    the [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    Python library, how to [fine-tune LLMs](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91),
    and [how to build an LLM from scratch](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a92d8deb44c331cde299747e18da5c7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '**What is an LLM?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LLM** is short for **Large Language Model**, which is a recent innovation
    in AI and machine learning. This powerful new type of AI went viral in Dec 2022
    with the release of ChatGPT.'
  prefs: []
  type: TYPE_NORMAL
- en: For those enlightened enough to live outside the world of AI buzz and tech news
    cycles, **ChatGPT** is a chat interface that ran on an LLM called GPT-3 (now upgraded
    to either GPT-3.5 or GPT-4 at the time of writing this).
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve used ChatGPT, it’s obvious that this is not your traditional chatbot
    from [AOL Instant Messenger](https://en.wikipedia.org/wiki/AIM_(software)) or
    your credit card’s customer care.
  prefs: []
  type: TYPE_NORMAL
- en: This one *feels* different.
  prefs: []
  type: TYPE_NORMAL
- en: '**What makes an LLM “large”?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When I heard the term “Large Language Model,” one of my first questions was,
    *how is this different from a “regular” language model?*
  prefs: []
  type: TYPE_NORMAL
- en: A language model is more generic than a large language model. Just like all
    squares are rectangles but not all rectangles are squares. **All LLMs are language
    models, but not all language models are LLMs**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1538bc098dd64cea39915cbb331d339.png)'
  prefs: []
  type: TYPE_IMG
- en: Large Language Models are a special type of Language Model. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, so LLMs are a special type of language model, **but what makes them special?**
  prefs: []
  type: TYPE_NORMAL
- en: There are **2 key properties** that distinguish LLMs from other language models.
    One is quantitative, and the other is qualitative.
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantitatively**, what distinguishes an LLM is the number of parameters used
    in the model. Current LLMs have on the order of **10–100 billion parameters**
    [1].'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Qualitatively**, something remarkable happens when a language model becomes
    “large.” It exhibits so-called ***emergent properties***e.g. zero-shot learning
    [1]. These are **properties that seem to suddenly appear** when a language model
    reaches a sufficiently large size.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Zero-shot Learning**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The major innovation of GPT-3 (and other LLMs) is that it is capable of **zero-shot
    learning** in a wide variety of contexts [2]. This means ChatGPT can **perform
    a task even if it has not been explicitly trained to do it**.
  prefs: []
  type: TYPE_NORMAL
- en: While this might be no big deal to us highly evolved humans, this zero-shot
    learning ability starkly contrasts the prior machine learning paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, a model needed to be **explicitly trained on the task it aimed to
    do** in order to have good performance. This could require anywhere from 1k-1M
    pre-labeled training examples.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you wanted a computer to do language translation, sentiment
    analysis, and identify grammatical errors. Each of these tasks would require a
    specialized model trained on a large set of labeled examples. Now, however, **LLMs
    can do all these things without explicit training**.
  prefs: []
  type: TYPE_NORMAL
- en: '**How do LLMs work?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core task used to train most state-of-the-art LLMs is **word prediction**.
    In other words, given a sequence of words, **what is the probability distribution
    of the next word**?
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, given the sequence “Listen to your ____,” the most likely next
    words might be: heart, gut, body, parents, grandma, etc. This might look like
    the probability distribution shown below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5012798faee287a80d6de31d5d742bbd.png)'
  prefs: []
  type: TYPE_IMG
- en: Toy probability distribution of next word in sequence “Listen to your ___.”
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, this is the same way many (non-large) language models have been
    trained in the past (e.g. GPT-1) [3]. However, for some reason, when language
    models get beyond a certain size (say ~10B parameters), these (emergent) abilities,
    such as zero-shot learning, can start to pop up [1].
  prefs: []
  type: TYPE_NORMAL
- en: Although there is no clear answer as to *why* this occurs (only speculations
    for now), it is clear that LLMs are a powerful technology with countless potential
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: '**3 Levels of Using LLMs**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we turn to how to use this powerful technology in practice. While there
    are countless potential LLM use cases, here I categorize them into 3 levels **ordered
    by required technical knowledge and computational resources**. We start with the
    most accessible.
  prefs: []
  type: TYPE_NORMAL
- en: '**Level 1: Prompt Engineering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first level of using LLMs in practice is **prompt engineering**, which I
    define as **any use of an LLM out-of-the-box** i.e. not changing any model parameters.
    While many technically-inclined individuals seem to scoff at the idea of prompt
    engineering, this is the most accessible way to use LLMs (both technically and
    economically) in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----65194dda1148--------------------------------)
    [## Prompt Engineering — How to trick AI into solving your problems'
  prefs: []
  type: TYPE_NORMAL
- en: 7 prompting tricks, Langchain, and Python example code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----65194dda1148--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 2 main ways to do prompt engineering: the **Easy Way** and the **Less
    Easy Way**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Easy Way: ChatGPT (or another convenient LLM UI) —** The key benefit
    of this method is convenience. Tools like ChatGPT provide an intuitive, no-cost,
    and no-code way to use an LLM (it doesn’t get much easier than that).'
  prefs: []
  type: TYPE_NORMAL
- en: However, convenience often comes at a cost. In this case, there are **2 key
    drawbacks** to this approach. The **first** is a lack of functionality. For example,
    ChatGPT does not readily enable users to customize model input parameters (e.g.
    temperature or max response length), which are values that modulate LLM outputs.
    **Second**, interactions with the ChatGPT UI cannot be readily automated and thus
    applied to large-scale use cases.
  prefs: []
  type: TYPE_NORMAL
- en: While these drawbacks may be dealbreakers for some use cases, both can be ameliorated
    if we take prompt engineering one step further.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Less Easy Way: Interact with LLM directly —** We can overcome some of
    the drawbacks of ChatGPT by interacting directly with an LLM via programmatic
    interfaces. This could be via public APIs (e.g. OpenAI’s API) or running an LLM
    locally (using libraries like [Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)).'
  prefs: []
  type: TYPE_NORMAL
- en: While this way of doing prompt engineering is less convenient (since it requires
    programming knowledge and potential API costs), it provides a customizable, flexible,
    and scalable way to use LLMs in practice. Future articles in this series will
    discuss [paid](/cracking-open-the-openai-python-api-230e4cae7971) and [cost-free](/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    ways to do this type of prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Although prompt engineering (as defined here) can handle most potential LLM
    applications, relying on a generic model, out-of-the-box may result in sub-optimal
    performance for specific use cases. For these situations, we can go to the next
    level of using LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Level 2: Model Fine-tuning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second level of using an LLM is **model fine-tuning**, which I’ll define
    as taking an existing LLM and tweaking it for a particular use case by **training
    at least 1 (internal) model parameter** i.e. weights and biases. For the aficionados
    out there, this is an example of *transfer learning* i.e. using some part of an
    existing model to develop another model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-tuning typically consists of 2 steps. **Step 1**: Obtain a pre-trained
    LLM. **Step 2**: Update model parameters for a specific task given (typically
    1000s of) high-quality labeled examples.'
  prefs: []
  type: TYPE_NORMAL
- en: The model parameters define the LLM’s internal representation of the input text.
    Thus, by tweaking these parameters for a particular task, the internal representations
    become optimized for the fine-tuning task (or at least that’s the idea).
  prefs: []
  type: TYPE_NORMAL
- en: This is a powerful approach to model development because a relatively **small
    number of examples** and computational resources **can produce exceptional model
    performance**.
  prefs: []
  type: TYPE_NORMAL
- en: The downside, however, is it requires significantly more technical expertise
    and computational resources than prompt engineering. In a [future article](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91),
    I will attempt to curb this downside by reviewing fine-tuning techniques and sharing
    example Python code.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----65194dda1148--------------------------------)
    [## Fine-Tuning Large Language Models (LLMs)'
  prefs: []
  type: TYPE_NORMAL
- en: A conceptual overview with example Python code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----65194dda1148--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: While prompt engineering and model fine-tuning can likely handle 99% of LLM
    applications, there are cases where one must go even further.
  prefs: []
  type: TYPE_NORMAL
- en: '**Level 3: Build your own LLM**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The third and final way to use an LLM in practice is to [**build your own**](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9).
    In terms of model parameters, this is where you **come up with all the model parameters**
    from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: An LLM is primarily a product of its training data. Thus, for some applications,
    it may be necessary to curate custom, high-quality text corpora for model training—for
    example, a medical research corpus for the development of a clinical application.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest upside to this approach is you can **fully customize the LLM for
    your particular use case**. This is the ultimate flexibility. However, as is often
    the case, flexibility comes at the cost of convenience.
  prefs: []
  type: TYPE_NORMAL
- en: Since the **key to LLM performance is scale**, building an LLM from scratch
    requires tremendous computational resources and technical expertise. In other
    words, this isn’t going to be a solo weekend project but a full team working for
    months, if not years, with a 7–8F budget.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, in a [future article](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9)
    in this series, we will explore popular techniques for developing LLMs from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-build-an-llm-from-scratch-8c477768f1f9?source=post_page-----65194dda1148--------------------------------)
    [## How to Build an LLM from Scratch'
  prefs: []
  type: TYPE_NORMAL
- en: Data Curation, Transformers, Training at Scale, and Model Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-build-an-llm-from-scratch-8c477768f1f9?source=post_page-----65194dda1148--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While there is more than enough hype about LLMs, they are a powerful innovation
    in AI. Here, I provided a primer on what LLMs are and framed how they can be used
    in practice. The [next article](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    in this series will give a beginner’s guide to OpenAI’s Python API to help jumpstart
    your next LLM use case.
  prefs: []
  type: TYPE_NORMAL
- en: '👉 **More on LLMs**: [OpenAI API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    | [Prompt Engineering](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    | [Fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [Build an LLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac)
    | [Text Embeddings](/text-embeddings-classification-and-semantic-search-8291746220be)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----65194dda1148--------------------------------)13
    stories![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Connect**: [My website](https://shawhintalebi.com/) | [Book a call](https://calendly.com/shawhintalebi)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Socials**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) ☕️'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/subscribe?source=post_page-----65194dda1148--------------------------------)
    [## Get FREE access to every new story I write'
  prefs: []
  type: TYPE_NORMAL
- en: Get FREE access to every new story I write P.S. I do not share your email with
    anyone By signing up, you will create a…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----65194dda1148--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Survey of Large Language Models. [arXiv:2303.18223](https://arxiv.org/abs/2303.18223)
    **[cs.CL]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] GPT-3 Paper. [arXiv:2005.14165](https://arxiv.org/abs/2005.14165) **[cs.CL]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Radford, A., & Narasimhan, K. (2018). Improving Language Understanding
    by Generative Pre-Training. ([GPT-1 Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))'
  prefs: []
  type: TYPE_NORMAL
