- en: 'Enhancing RAG’s Answer: Self-Debugging Techniques and Cognitive Load Reduction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/enhancing-rags-answer-self-debugging-techniques-and-cognitive-load-reduction-8c4273013d39](https://towardsdatascience.com/enhancing-rags-answer-self-debugging-techniques-and-cognitive-load-reduction-8c4273013d39)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Asking the LLM to self-diagnose and self-correct the prompt to improve answer
    quality.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://agustinus-nalwan.medium.com/?source=post_page-----8c4273013d39--------------------------------)[![Agustinus
    Nalwan](../Images/7c5ade9ab8bca1d27a317b5c09d1b734.png)](https://agustinus-nalwan.medium.com/?source=post_page-----8c4273013d39--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8c4273013d39--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8c4273013d39--------------------------------)
    [Agustinus Nalwan](https://agustinus-nalwan.medium.com/?source=post_page-----8c4273013d39--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8c4273013d39--------------------------------)
    ·22 min read·Nov 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed765e2b448efe4ab6655b25da525fab.png)'
  prefs: []
  type: TYPE_IMG
- en: LLM performs self-debugging (image generated with MidJourney)
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval Augmented Generation (RAG) is undoubtedly a powerful tool, easily
    crafted using frameworks like LangChain or LlamaIndex. Such ease of integration
    might give an impression that RAG is a magic solution that is easy to build for
    every use case. However, in our journey to upgrade our editorial article search
    tool to offer semantically richer search results and direct answers to queries,
    we found the basic RAG setup and is lacking and discovered many challenges. Constructing
    a RAG for a demonstration is quick and easy, often yielding sufficiently impressive
    results for small subset of scenarios. Yet, the final stretch to achieve production-ready
    status, where exceptional quality is mandatory, presents significant challenges.
    This is particularly true when dealing with a vast knowledge base filled with
    thousands of domain-specific articles, a not-so-rare occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our approach to RAG consists of two distinct steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Relevant Document Retrieval By employing a fuse of dense and sparse embeddings,
    we extract relevant document chunks from our Pinecone database, considering both
    content and title. These chunks are subsequently re-ranked based on relevance
    to the title, content, and the document’s age. The top four documents are then
    chosen: both as potential search results and as document context for generating
    direct answers. Notably, this approach diverges from the common RAG setup, addressing
    our unique document retrieval challenges more effectively.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Direct Answer Generation Here, the question, instruction, and the previously
    retrieved top four document chunks (document context) are fed into a Large Language
    Model (LLM) to produce a direct answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/f2a7498db31ebbcb6287cc1e3d273786.png)'
  prefs: []
  type: TYPE_IMG
- en: RAG architecture
  prefs: []
  type: TYPE_NORMAL
- en: I’ve delved deeply into enhancing document retrieval quality through the use
    of [Hybrid Search and Hierarchical Document Ranking techniques](/the-untold-side-of-rag-addressing-its-challenges-in-domain-specific-searches-808956e3ecc8)
    in previous discussions. In this blog, I aim to share insights on refining and
    troubleshooting the direct answer generation process.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring High-Quality Document Retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The presence of a highly capable LLM for direct answer generation becomes futile
    when fed with low-quality document context. For example, if the document portions
    lack the pertinent information necessary to derive an answer for a given question,
    the effectiveness of the LLM diminishes, regardless the size and the cognitive
    capability of the model. Achieving this goal is quite challenging because it requires
    optimising both recall and precision simultaneously. Recall is essential to avoid
    overlooking document chunks that contain vital information for answer generation,
    while precision is essential to minimise the number of document chunks sent, focusing
    solely on the most relevant ones. This consideration takes into account the limitations
    of a restricted context window size and the associated costs of transmitting extensive
    document contexts. Consequently, the top-k document chunks selected should represent
    the highest quality available.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87b2c221c0042402824c2248733f762b.png)'
  prefs: []
  type: TYPE_IMG
- en: The LLM scoured the document to find content of high quality (image generated
    using MidJourney).
  prefs: []
  type: TYPE_NORMAL
- en: So, if you find that the direct answer generated isn’t satisfactory, the first
    step should be to scrutinise your document retrieval process. In our case, it
    accounts for half of the inaccuracies in the generated answers. Examine the full
    prompt dispatched to the LLM to ensure the necessary relevant information is present
    within the document context of your prompt. If it’s absent or inadequate, consider
    enhancing your document retrieval strategies using techniques like [hybrid search,
    hierarchical document ranking, or score boosting](https://medium.com/towards-data-science/the-untold-side-of-rag-addressing-its-challenges-in-domain-specific-searches-808956e3ecc8).
  prefs: []
  type: TYPE_NORMAL
- en: For the remainder of this blog, I will walk you through the four techniques
    I’ve used for enhancing the quality of direct answers and how I incorporate these
    techniques into my prompts. Please note that the prompts I share in this blog
    may be edited for brevity, with certain parts omitted to emphasise the relevant
    aspects for our discussion and to prevent overwhelming this blog with excessive
    content. Additionally, while I consistently use the question “What is Mazda CX-9
    2019 fuel efficiency?” as an example throughout the blog, it’s important to understand
    that I don’t limit our evaluation to just this question. We have conducted evaluations
    with sets of similar questions to ensure that the behaviours we observe are representative
    across a substantial number of evaluation samples. So, let’s begin.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clarity is Key: Be Explicit in Your Instructions.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensuring your prompts to the LLM are explicit and well-defined is crucial. For
    instance, if you want the LLM to refrain from generating an answer when the necessary
    information isn’t available in the document context, you must specify this clearly.
    As an example, take a look at the example prompt below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: There is a specific emphasis on the instruction **“If you don’t know the answer,
    simply return nothing.”** The LLM faced challenges related to boundaries, much
    like that smart but annoying friend in your high school who always raised their
    hand to answer the teacher’s questions, regardless of their confidence in the
    response. They were uncertain whether providing an incorrect answer (term coined
    as hallucination) would be more detrimental than refraining from answering altogether.
    Therefore, by including this directive, you are expressing your preference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, the specific requirements vary according to your use case. For ours,
    we expect the LLM to rely solely on our knowledge base, meaning it should generate
    answers only from the information present in the provided document chunks. All
    LLMs has been trained on a vast array of data from the internet, books, and other
    mediums. While such sources may contain information that could potentially be
    used to derive answers, their reliability and consistency with our knowledge base
    are not assured. Consequently, we have incorporated an additional directive: **“Do
    not source information from anything other than the provided document excerpts.”**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f501fb54937f7b8ce6ba2107fbe1361.png)'
  prefs: []
  type: TYPE_IMG
- en: The LLM is carefully reading provided instruction (image generated using MidJourney).
  prefs: []
  type: TYPE_NORMAL
- en: By being direct and precise in your prompts, you can greatly minimise the likelihood
    of the LLM producing unfounded or “hallucinated” answers.
  prefs: []
  type: TYPE_NORMAL
- en: Asking the LLM to Self-Diagnose and Self-Correct.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That’s correct! It’s possible to ask the LLM to identify and rectify its own
    errors. In contrast to traditional algorithms, LLMs have the ability to reason,
    which enables them to perform a range of tasks, self-debugging included.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98adfb2875a26dc1601212f35ba88575.png)'
  prefs: []
  type: TYPE_IMG
- en: The LLM is performing a self-diagnostic on itself (image generated using MidJourney).
  prefs: []
  type: TYPE_NORMAL
- en: For example, within our project, we encountered an issue with the query “What
    is the fuel efficiency of Mazda CX-9 2019?” where the LLM generated an inaccurate
    response, stating, “The fuel efficiency of Mazda CX-9 2019 is 8.8L/100km.” Upon
    closer examination, we confirmed that we had indeed provided the correct document
    context. The relevant information was contained within an article titled “Mazda
    CX-8 vs. Mazda CX-9 2019 Comparison” in our prompt. However, inexplicably, the
    LLM extracted the fuel efficiency information from a Mazda CX-9 2018 article instead.
    Below, you can find our original prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using GPT3.5 turbo, it produces the following incorrect response (The correct
    answer is 10.1L/100 km).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: When I tested the identical prompt with GPT4.0, I received the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Nonetheless, GPT4 is not economically feasible for our use case, as it costs
    10 times more than GPT3.5 Turbo. Consequently, this challenge compelled us to
    seek a solution that is compatible with GPT 3.5 Turbo, sparking the concept of
    self-debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine asking someone to answer a question based on a document you’ve provided,
    and they give you an incorrect response. What steps would you take to understand
    why the mistake occurred? **You would simply ask them why they made the mistake
    by showing them the correct answer alongside the source material**. With LLM,
    it’s much the same. We ask the LLM, but to do so effectively, we need the ability
    to continue the conversation with our LLM after it provides an answer.
  prefs: []
  type: TYPE_NORMAL
- en: By employing platforms such as [www.poe.com](http://www.poe.com/), we can reproduce
    our interactions with the LLM. We opt for the same LLM model and input the entire
    prompt into the console. The resulting output mirrors the precise incorrect answer
    we encountered within our code. Following this, we commence a conversation with
    the LLM, which serves as a crucial step in facilitating the self-debugging process,
    as outlined below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a37a3367cd7543d37b07894d26c69862.png)'
  prefs: []
  type: TYPE_IMG
- en: poe.com web interface
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the complete dialogue record I had with the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Me: The answer you give me is incorrect. It is the fuel efficiency of Mazda
    CX-9 2018 as stated in article with title Mazda CX-9 2018 Review. The correct
    answer is 10.1L/100km as stated in article with title “Mazda CX-8 vs Mazda CX-9
    2019 Comparison'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'LLM: I apologise for this mistake. You are right that the correct information
    was provided in the article you stated.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Me: That’s ok. Why did you make this mistake?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'LLM: I did not realise that the year quoted in the article is the model year
    of the car. I thought it is an article year.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Based on the last response, I’ve now identified the cause behind the LLM’s
    error. This sparked another idea: If the LLM can comprehend and explain its misunderstanding,
    perhaps it can also adjust my initial prompt to prevent repeating the same mistake
    in the future. It’s definitely worth a try!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Me: How should I modify my prompt so next time you won’t repeat this same mistake?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'LLM: You can add to specifically say that the article title/content might contain
    car model year which should match the model year asked in the question.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Me: Could u pls modify my prompt to include your suggestion?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Upon requesting a second time, the LLM provided me with a revised prompt that
    integrated the suggested improvement.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Despite utilising this adjusted prompt, it was rather disheartening to observe
    that it still yielded an incorrect response to my earlier question.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Nevertheless, it has provided me with insights into the necessary information
    that needed to be included regarding the model year. Consequently, I took it upon
    myself to manually modify the prompt by incorporating the recommended mention
    of the model year. I even provided a brief example of the expected behaviour to
    guide the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '**Revised prompt based on the LLM suggestion**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The new prompt has now provided me with the correct answer!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: However, it’s important to keep in mind that this method may not consistently
    yield positive results. My success rate using this method stands at approximately
    75% in gaining insight from LLM explanations regarding why it made the mistake,
    and it’s around 25% effective in getting the LLM to modify the prompt for me to
    produce correct answer. Nevertheless, having this method at our disposal is still
    preferable to having no method at all.
  prefs: []
  type: TYPE_NORMAL
- en: '**Aware of cognitive overload.**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keeping instructions precise is crucial for directing the LLM to produce the
    desired answer. However, overly detailed prompts that result in complexity can
    have adverse effects on smaller-size LLMs like GPT-3.5 Turbo, which have less
    cognitive capacity compared to GPT-4\. Wondering why? Let’s go through an example
    to demonstrate this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73a535677c9ccff4a1c88fbc9e2d08f9.png)'
  prefs: []
  type: TYPE_IMG
- en: The LLM is experiencing cognitive overload (image generated using MidJourney).
  prefs: []
  type: TYPE_NORMAL
- en: Our initial prompt, which was constructed using the Langchain template, employs
    a one-shot technique. For instance, it includes a single example of full interaction
    in the hope that the LLM can gain a better understanding of our intent. However,
    when using GPT3.5 Turbo, we observed that the one-shot prompt resulted in more
    ambiguous answers compared to the prompt with zero shots.
  prefs: []
  type: TYPE_NORMAL
- en: Below, you can find our one-shot prompt based on the [Langchain](https://www.langchain.com/)
    template. The example provided is from a completely different domain (non-automotive),
    and the prompt is generic, with no mention of car-related articles.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: An extremely unclear response, mentioning the fuel efficiency of Mazda CX-9
    across different model years, was given.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It’s possible that the vague nature of the initial prompt, along with an unrelated
    one-shot example, might be partly to blame for the issue. To test this theory,
    I crafted a more tailored prompt that incorporates domain-specific instructions
    and an example that aligns closely with our actual use case. The details of this
    new prompt are as follows, which will help determine if it makes a noticeable
    difference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The response provided remains inaccurate. It also claimed that the necessary
    information was unavailable to address the question, which is inaccurate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: By omitting the example and opting for a zero-shot approach, we successfully
    get the LLM to generate the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To further validate that the issue stems from cognitive overload, we conducted
    a one-shot prompt test with a GPT-4, which, as anticipated, yielded a correct
    response.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: To reinforce our hypothesis, we conducted similar tests with other language
    models, Claude Instant and Claude 2, which resulted in the same pattern of outcomes.
    Claude 2 successfully processed our one-shot prompt to provide the correct answer,
    while Claude Instant did not. Claude 2 and GPT-4 share comparable cognitive capacities,
    both superior to those of Claude Instant and GPT-3.5 Turbo. This supports our
    theory that models with lesser cognitive capabilities struggle with complex instructions.
    I am quite satisfied with these results, as the zero-shot approach allows for
    a more concise prompt, reducing costs, and it works with the less expensive, smaller
    models like GPT-3.5 Turbo or Claude Instant.
  prefs: []
  type: TYPE_NORMAL
- en: '**Asking for a proof.**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Requesting that a language model provide the source of its generated answer
    is a highly effective strategy to reduce hallucination and prevent hallucination,
    particularly when the relevant information is not present within the provided
    document context.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89de88e21fac1f06d9de9de78a74785d.png)'
  prefs: []
  type: TYPE_IMG
- en: The LLM is tracing the source material’s origin (image generated with MidJourney).
  prefs: []
  type: TYPE_NORMAL
- en: This objective can be met by using different prompts to guide the language model.
    A strategy involves directing the LLM to identify the specific part of the document
    context that informed its response. Incorporating a command that obliges the LLM
    to reference the document source ID in its answer allows you to disregard the
    response if the source ID is absent or wrong. While it’s infrequent, the LLM may
    reference an external source not included in your document chunk, such as a website
    it was exposed to during its training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Another strategy is to direct the LLM to respond by explicitly quoting the sentence(s)
    in the document chunk that answer the question.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This method can impose additional constraints on the LLM and isn’t always practical,
    as is the case with our documents. Often, our documents lack sentences that explicitly
    respond to frequently asked questions. Therefore, the LLM has no specific sentences
    to quote, which results in no response. Should we permit the LLM to compose its
    own sentences using the knowledge embedded in the document, it would be capable
    of providing an answer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building an application using Language Model (LLM) is quite distinct from using
    traditional algorithms. With traditional algorithms, detailed instructions give
    you complete control over the process, and debugging involves watching variable
    values, ensuring those instructions are correct. In contrast, using an LLM is
    akin to employing a human to execute these processes within your software. Because
    LLMs are human-like in their responses, troubleshooting and debugging require
    techniques similar to those used to determine why a person might have made an
    error, primarily through questioning.
  prefs: []
  type: TYPE_NORMAL
- en: Even though Language Model (LLM) technology is still emerging and subject to
    mistakes and inaccuracies, it has shown considerable promise, and there are effective
    strategies to greatly mitigate these issues. Given the rapid development of LLM
    technology and the continuous creation of innovative methods and tools, there
    is a strong sense of excitement about the transformative prospects of this field
    in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: We are on the verge of deploying our RAG setup for production use, and as we
    continue to learn, I look forward to sharing more insights in upcoming blog posts.
  prefs: []
  type: TYPE_NORMAL
- en: Should you find this article enjoyable and helpful, I would appreciate your
    support with 20 claps. I’m also eager to learn from your insights and experiences,
    so feel free to share your thoughts by posting a response. Additionally, if you’re
    interested, I welcome you to connect with me on [LinkedIn](https://www.linkedin.com/in/agustinus-nalwan/).
  prefs: []
  type: TYPE_NORMAL
- en: In closing, I want to express my gratitude to my incredible team at Carsales.com
    for their support throughout this journey.
  prefs: []
  type: TYPE_NORMAL
