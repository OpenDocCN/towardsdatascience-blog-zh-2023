- en: Discovering Differential Equations with Physics-Informed Neural Networks and
    Symbolic Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€šè¿‡ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œå’Œç¬¦å·å›å½’å‘ç°å¾®åˆ†æ–¹ç¨‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/discovering-differential-equations-with-physics-informed-neural-networks-and-symbolic-regression-c28d279c0b4d](https://towardsdatascience.com/discovering-differential-equations-with-physics-informed-neural-networks-and-symbolic-regression-c28d279c0b4d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/discovering-differential-equations-with-physics-informed-neural-networks-and-symbolic-regression-c28d279c0b4d](https://towardsdatascience.com/discovering-differential-equations-with-physics-informed-neural-networks-and-symbolic-regression-c28d279c0b4d)
- en: A case study with step-by-step code implementation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé€æ­¥ä»£ç å®ç°çš„æ¡ˆä¾‹ç ”ç©¶
- en: '[](https://shuaiguo.medium.com/?source=post_page-----c28d279c0b4d--------------------------------)[![Shuai
    Guo](../Images/d673c066f8006079be5bf92757e73a59.png)](https://shuaiguo.medium.com/?source=post_page-----c28d279c0b4d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c28d279c0b4d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c28d279c0b4d--------------------------------)
    [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----c28d279c0b4d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shuaiguo.medium.com/?source=post_page-----c28d279c0b4d--------------------------------)[![Shuai
    Guo](../Images/d673c066f8006079be5bf92757e73a59.png)](https://shuaiguo.medium.com/?source=post_page-----c28d279c0b4d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c28d279c0b4d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c28d279c0b4d--------------------------------)
    [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----c28d279c0b4d--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c28d279c0b4d--------------------------------)
    Â·25 min readÂ·Jul 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c28d279c0b4d--------------------------------)
    Â·é˜…è¯»æ—¶é—´25åˆ†é’ŸÂ·2023å¹´7æœˆ28æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/feb76e9bafbe81909b3d783c4332a1eb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/feb76e9bafbe81909b3d783c4332a1eb.png)'
- en: Photo by [Steven Coffey](https://unsplash.com/@steeeve?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”±[Steven Coffey](https://unsplash.com/@steeeve?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œæ¥æºäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Differential equations serve as a powerful framework to capture and understand
    the dynamic behaviors of physical systems. By describing how variables change
    in relation to each other, they provide insights into system dynamics and allow
    us to make predictions about the systemâ€™s future behavior.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®åˆ†æ–¹ç¨‹ä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œç”¨äºæ•æ‰å’Œç†è§£ç‰©ç†ç³»ç»Ÿçš„åŠ¨æ€è¡Œä¸ºã€‚é€šè¿‡æè¿°å˜é‡ä¹‹é—´å¦‚ä½•å˜åŒ–ï¼Œå®ƒä»¬æä¾›äº†å¯¹ç³»ç»ŸåŠ¨æ€çš„è§è§£ï¼Œå¹¶å…è®¸æˆ‘ä»¬å¯¹ç³»ç»Ÿæœªæ¥çš„è¡Œä¸ºè¿›è¡Œé¢„æµ‹ã€‚
- en: 'However, a common challenge we face in many real-world systems is that their
    governing differential equations are often only *partially known*, withthe unknown
    aspects manifesting in several ways:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨è®¸å¤šå®é™…ç³»ç»Ÿä¸­é¢ä¸´çš„ä¸€ä¸ªå…±åŒæŒ‘æˆ˜æ˜¯ï¼Œå®ƒä»¬çš„æ§åˆ¶å¾®åˆ†æ–¹ç¨‹é€šå¸¸ä»…*éƒ¨åˆ†å·²çŸ¥*ï¼ŒæœªçŸ¥çš„æ–¹é¢ä»¥å‡ ç§æ–¹å¼è¡¨ç°å‡ºæ¥ï¼š
- en: The **parameters** of the differential equation are unknown. A case in point
    is wind engineering, where the governing equations of fluid dynamics are well-established,
    but the coefficients relating to turbulent flow are highly uncertain.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¾®åˆ†æ–¹ç¨‹çš„**å‚æ•°**æ˜¯æœªçŸ¥çš„ã€‚ä¾‹å¦‚åœ¨é£å·¥ç¨‹ä¸­ï¼Œæµä½“åŠ¨åŠ›å­¦çš„æ§åˆ¶æ–¹ç¨‹å·²è¢«å¾ˆå¥½åœ°å»ºç«‹ï¼Œä½†ä¸æ¹æµæµåŠ¨ç›¸å…³çš„ç³»æ•°éå¸¸ä¸ç¡®å®šã€‚
- en: The **functional forms** of the differential equations are unknown. For instance,
    in chemical engineering, the exact functional form of the rate equations may not
    be fully understood due to the uncertainties in rate-determining steps and reaction
    pathways.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¾®åˆ†æ–¹ç¨‹çš„**å‡½æ•°å½¢å¼**æ˜¯æœªçŸ¥çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ–å­¦å·¥ç¨‹ä¸­ï¼Œç”±äºé€Ÿç‡å†³å®šæ­¥éª¤å’Œååº”é€”å¾„çš„ä¸ç¡®å®šæ€§ï¼Œé€Ÿç‡æ–¹ç¨‹çš„ç¡®åˆ‡å‡½æ•°å½¢å¼å¯èƒ½æ²¡æœ‰å®Œå…¨ç†è§£ã€‚
- en: Both **functional forms** and **parameters** are unknown. A prime example is
    battery state modeling, where the commonly used equivalent circuit model only
    partially captures the current-voltage relationship (the functional form of the
    missing physics is therefore unknown). Moreover, the model itself contains unknown
    parameters (i.e., resistance and capacitance values).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‡½æ•°å½¢å¼**å’Œ**å‚æ•°**éƒ½æ˜¯æœªçŸ¥çš„ã€‚ä¸€ä¸ªå…¸å‹çš„ä¾‹å­æ˜¯ç”µæ± çŠ¶æ€å»ºæ¨¡ï¼Œå…¶ä¸­å¸¸ç”¨çš„ç­‰æ•ˆç”µè·¯æ¨¡å‹ä»…éƒ¨åˆ†æ•æ‰äº†ç”µæµ-ç”µå‹å…³ç³»ï¼ˆå› æ­¤ç¼ºå¤±ç‰©ç†çš„å‡½æ•°å½¢å¼æ˜¯æœªçŸ¥çš„ï¼‰ã€‚æ­¤å¤–ï¼Œæ¨¡å‹æœ¬èº«åŒ…å«æœªçŸ¥çš„å‚æ•°ï¼ˆå³ç”µé˜»å’Œç”µå®¹å€¼ï¼‰ã€‚'
- en: '![](../Images/0dc9da76d5f88c55a5a1ea99dd496e18.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0dc9da76d5f88c55a5a1ea99dd496e18.png)'
- en: Figure 1\. The governing equations of many real-world dynamical systems are
    only partially known. (Image by this blog author)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1\. è®¸å¤šå®é™…åŠ¨æ€ç³»ç»Ÿçš„æ§åˆ¶æ–¹ç¨‹ä»…éƒ¨åˆ†å·²çŸ¥ã€‚ï¼ˆå›¾ç‰‡ç”±æœ¬åšå®¢ä½œè€…æä¾›ï¼‰
- en: Such partial knowledge of the governing differential equations hinders our understanding
    and control of these dynamical systems. Consequently, inferring these unknown
    components based on observed data becomes a crucial task in dynamical system modeling.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ä¸»æ–¹ç¨‹çš„è¿™ç§éƒ¨åˆ†äº†è§£é˜»ç¢äº†æˆ‘ä»¬å¯¹è¿™äº›åŠ¨åŠ›ç³»ç»Ÿçš„ç†è§£å’Œæ§åˆ¶ã€‚å› æ­¤ï¼Œæ ¹æ®è§‚å¯Ÿæ•°æ®æ¨æ–­è¿™äº›æœªçŸ¥ç»„ä»¶æˆä¸ºåŠ¨åŠ›ç³»ç»Ÿå»ºæ¨¡ä¸­çš„å…³é”®ä»»åŠ¡ã€‚
- en: Broadly speaking, this process of using observational data to recover governing
    equations of dynamical systems falls in the domain of **system identification**.
    Once discovered, we can readily use these equations to predict future states of
    the system, inform control strategies for the systems, or enable theoretical investigations
    using analytical techniques.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¿ä¹‰è€Œè¨€ï¼Œä½¿ç”¨è§‚å¯Ÿæ•°æ®æ¢å¤åŠ¨åŠ›ç³»ç»Ÿçš„ä¸»æ–¹ç¨‹çš„è¿‡ç¨‹å±äº**ç³»ç»Ÿè¯†åˆ«**çš„èŒƒç•´ã€‚ä¸€æ—¦å‘ç°è¿™äº›æ–¹ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°åˆ©ç”¨è¿™äº›æ–¹ç¨‹é¢„æµ‹ç³»ç»Ÿçš„æœªæ¥çŠ¶æ€ï¼Œå‘ŠçŸ¥ç³»ç»Ÿçš„æ§åˆ¶ç­–ç•¥ï¼Œæˆ–é€šè¿‡åˆ†ææŠ€æœ¯è¿›è¡Œç†è®ºç ”ç©¶ã€‚
- en: Very recently, [Zhang et al.](https://arxiv.org/abs/2307.08107)(2023) proposed
    a promising strategy that leverages **physics-informed neural networks** (PINN)
    and **symbolic regression** to discover unknowns in a system of ordinary differential
    equations (ODEs). While their focus was on discovering differential equations
    for Alzheimerâ€™s disease modeling, their proposed solution holds promise for general
    dynamical systems.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œ[Zhang et al.](https://arxiv.org/abs/2307.08107)(2023)æå‡ºäº†ä¸€ç§æœ‰å‰æ™¯çš„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨**ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ**ï¼ˆPINNï¼‰å’Œ**ç¬¦å·å›å½’**æ¥å‘ç°å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEsï¼‰ç³»ç»Ÿä¸­çš„æœªçŸ¥é‡ã€‚è™½ç„¶ä»–ä»¬çš„é‡ç‚¹æ˜¯å‘ç°ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…å»ºæ¨¡çš„å¾®åˆ†æ–¹ç¨‹ï¼Œä½†ä»–ä»¬æå‡ºçš„è§£å†³æ–¹æ¡ˆå¯¹ä¸€èˆ¬åŠ¨åŠ›ç³»ç»Ÿä¹Ÿå…·æœ‰æ½œåŠ›ã€‚
- en: In this blog post, we will take a closer look at the concepts put forth by the
    authors and get hands-on to reproduce one of the case studies investigated in
    the paper. Toward that end, we will build a PINN from scratch, leverage the [PySR
    library](https://github.com/MilesCranmer/PySR) to perform symbolic regression,
    and discuss the obtained results.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ›´æ·±å…¥åœ°äº†è§£ä½œè€…æå‡ºçš„æ¦‚å¿µï¼Œå¹¶åŠ¨æ‰‹é‡ç°è®ºæ–‡ä¸­çš„ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªPINNï¼Œåˆ©ç”¨[PySRåº“](https://github.com/MilesCranmer/PySR)è¿›è¡Œç¬¦å·å›å½’ï¼Œå¹¶è®¨è®ºè·å¾—çš„ç»“æœã€‚
- en: 'If you are interested in learning best practices in physics-informed neural
    networks, feel free to check out my blog series here:'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œçš„æœ€ä½³å®è·µæ„Ÿå…´è¶£ï¼Œæ¬¢è¿æŸ¥çœ‹æˆ‘çš„åšå®¢ç³»åˆ—ï¼š
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Physics-Informed Neural Networks: An Application-Centric Guide](/physics-informed-neural-networks-an-application-centric-guide-dc1013526b02)'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼šä»¥åº”ç”¨ä¸ºä¸­å¿ƒçš„æŒ‡å—](/physics-informed-neural-networks-an-application-centric-guide-dc1013526b02)'
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Unraveling the Design Pattern of Physics-Informed Neural Networks](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527).'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[æ­ç¤ºç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œçš„è®¾è®¡æ¨¡å¼](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)ã€‚'
- en: With that in mind, letâ€™s get started!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: '**Table of Content**'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç›®å½•**'
- en: '**Â·** [**1\. Case Study**](#83d2) **Â·** [**2\. Why do traditional approaches
    fall short?**](#1f2c) **Â·** [**3\. PINN for System Identification (Theory)**](#bdd1)
    **Â·** [**4\. PINN for System Identification (Code)**](#1f5f)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**Â·** [**1\. æ¡ˆä¾‹ç ”ç©¶**](#83d2) **Â·** [**2\. ä¸ºä»€ä¹ˆä¼ ç»Ÿæ–¹æ³•ä¸å¤Ÿæœ‰æ•ˆï¼Ÿ**](#1f2c) **Â·** [**3\.
    PINNåœ¨ç³»ç»Ÿè¯†åˆ«ä¸­çš„åº”ç”¨ï¼ˆç†è®ºï¼‰**](#bdd1) **Â·** [**4\. PINNåœ¨ç³»ç»Ÿè¯†åˆ«ä¸­çš„åº”ç”¨ï¼ˆä»£ç ï¼‰**](#1f5f)'
- en: âˆ˜ [4.1 Define the Architecture](#29d6)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [4.1 å®šä¹‰æ¶æ„](#29d6)
- en: âˆ˜ [4.2 Define ODE loss](#7d83)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [4.2 å®šä¹‰ODEæŸå¤±](#7d83)
- en: âˆ˜ [4.3 Define gradient descent step](#2fcd)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [4.3 å®šä¹‰æ¢¯åº¦ä¸‹é™æ­¥éª¤](#2fcd)
- en: âˆ˜ [4.4 Data preparation](#741c)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [4.4 æ•°æ®å‡†å¤‡](#741c)
- en: âˆ˜ [4.5 PINN Training](#83c2)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [4.5 PINNè®­ç»ƒ](#83c2)
- en: '**Â·** [**5\. Symbolic Regression**](#fef3)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**Â·** [**5\. ç¬¦å·å›å½’**](#fef3)'
- en: âˆ˜ [5.1 PySR library](#0a5b)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [5.1 PySRåº“](#0a5b)
- en: âˆ˜ [5.2 Implementation](#e2f3)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [5.2 å®æ–½](#e2f3)
- en: âˆ˜ [5.3 Identification results](#fa91)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [5.3 è¯†åˆ«ç»“æœ](#fa91)
- en: Â·[**6\. Take-away**](#889f)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Â·[**6\. æ€»ç»“**](#889f)
- en: Â· [Reference](#1485)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [å‚è€ƒæ–‡çŒ®](#1485)
- en: 1\. Case Study
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. æ¡ˆä¾‹ç ”ç©¶
- en: 'Letâ€™s start by introducing the problem we aim to solve. In this blog, we will
    reproduce the first case study investigated in [Zhang et al](https://arxiv.org/abs/2307.08107).â€™s
    original paper, i.e., discovering the Kraichnan-Orszag system from data. The system
    is described by the following ODEs:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹ä»‹ç»æˆ‘ä»¬æ—¨åœ¨è§£å†³çš„é—®é¢˜ã€‚åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç°[Zhang et al](https://arxiv.org/abs/2307.08107)åŸå§‹è®ºæ–‡ä¸­çš„ç¬¬ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œå³ä»æ•°æ®ä¸­å‘ç°Kraichnan-Orszagç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿç”±ä»¥ä¸‹ODEsæè¿°ï¼š
- en: '![](../Images/4160891a7e1d7f249bdbcd7d9e79508f.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4160891a7e1d7f249bdbcd7d9e79508f.png)'
- en: with an initial condition of *u*â‚(0)=1, *u*â‚‚(0)=0.8, *u*â‚ƒ(0)=0.5\. The Kraichnan-Orszag
    system is commonly used in turbulence studies and fluid dynamics research, where
    the goal is to develop theoretical insights into turbulence, its structures, and
    its dynamics.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å…·æœ‰åˆå§‹æ¡ä»¶ *u*â‚(0)=1ï¼Œ*u*â‚‚(0)=0.8ï¼Œ*u*â‚ƒ(0)=0.5ã€‚Kraichnan-Orszag ç³»ç»Ÿé€šå¸¸ç”¨äºæ¹æµç ”ç©¶å’Œæµä½“åŠ¨åŠ›å­¦ç ”ç©¶ï¼Œå…¶ç›®æ ‡æ˜¯å¯¹æ¹æµåŠå…¶ç»“æ„å’ŒåŠ¨æ€å‘å±•ç†è®ºè§è§£ã€‚
- en: 'To mimic a typical system identification setup, we assume we only know partially
    about the governing ODEs. Specifically, we assume that we donâ€™t know anything
    about the differential equations for *u*â‚ and *u*â‚‚. In addition, we assume we
    only know that the right-hand side of the differential equation for *u*â‚ƒ is a
    linear transformation of *u*â‚ and *u*â‚‚. Then, we can rewrite the ODE system as
    follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¨¡æ‹Ÿä¸€ä¸ªå…¸å‹çš„ç³»ç»Ÿè¯†åˆ«è®¾ç½®ï¼Œæˆ‘ä»¬å‡è®¾æˆ‘ä»¬å¯¹æ§åˆ¶å¸¸å¾®åˆ†æ–¹ç¨‹çš„äº†è§£ä»…é™äºéƒ¨åˆ†å·²çŸ¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‡è®¾æˆ‘ä»¬å¯¹ *u*â‚ å’Œ *u*â‚‚ çš„å¾®åˆ†æ–¹ç¨‹ä¸€æ— æ‰€çŸ¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‡è®¾æˆ‘ä»¬åªçŸ¥é“
    *u*â‚ƒ çš„å¾®åˆ†æ–¹ç¨‹å³ä¾§æ˜¯ *u*â‚ å’Œ *u*â‚‚ çš„çº¿æ€§å˜æ¢ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†å¸¸å¾®åˆ†æ–¹ç¨‹ç³»ç»Ÿé‡å†™å¦‚ä¸‹ï¼š
- en: '![](../Images/03c35582b0c5f142b22932c1dae697e0.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03c35582b0c5f142b22932c1dae697e0.png)'
- en: where *f*â‚ and *f*â‚‚ denote the unknown functions, and *a* and *b* are the unknown
    parameters. **Our objective is to calibrate the values of *a* and *b*, as well
    as estimate the analytical functional form of *f*â‚ and *f*â‚‚**. Essentially, we
    are dealing with a challenging system identification problem where both unknown
    parameters and function forms exist.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *f*â‚ å’Œ *f*â‚‚ ä»£è¡¨æœªçŸ¥å‡½æ•°ï¼Œ*a* å’Œ *b* æ˜¯æœªçŸ¥å‚æ•°ã€‚**æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ ¡å‡† *a* å’Œ *b* çš„å€¼ï¼Œå¹¶ä¼°è®¡ *f*â‚ å’Œ *f*â‚‚
    çš„è§£æå‡½æ•°å½¢å¼**ã€‚ æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬æ­£é¢ä¸´ä¸€ä¸ªå…·æœ‰æœªçŸ¥å‚æ•°å’Œå‡½æ•°å½¢å¼çš„å¤æ‚ç³»ç»Ÿè¯†åˆ«é—®é¢˜ã€‚
- en: 2\. Why do traditional approaches fall short?
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. ä¸ºä»€ä¹ˆä¼ ç»Ÿæ–¹æ³•ä¼šå¤±è´¥ï¼Ÿ
- en: In the traditional paradigm of system identification, we typically employ numerical
    methods (e.g., Eulerâ€™s method, Runge-Kutta methods, etc.) to simulate and predict
    system states *u*â‚, *u*â‚‚, and *u*â‚ƒ. However, those methods are fundamentally limited
    in that they generally require a complete form of governing differential equations,
    and are incapable of handling scenarios when the differential equations are only
    partially known.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¼ ç»Ÿçš„ç³»ç»Ÿè¯†åˆ«èŒƒå¼ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨æ•°å€¼æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œæ¬§æ‹‰æ³•ã€é¾™æ ¼-åº“å¡”æ³•ç­‰ï¼‰æ¥æ¨¡æ‹Ÿå’Œé¢„æµ‹ç³»ç»ŸçŠ¶æ€ *u*â‚ã€*u*â‚‚ å’Œ *u*â‚ƒã€‚ ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»æ ¹æœ¬ä¸Šå—é™ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸éœ€è¦å®Œæ•´çš„æ§åˆ¶å¾®åˆ†æ–¹ç¨‹å½¢å¼ï¼Œå¹¶ä¸”æ— æ³•å¤„ç†å¾®åˆ†æ–¹ç¨‹ä»…éƒ¨åˆ†å·²çŸ¥çš„æƒ…å†µã€‚
- en: In cases where the parameters of the equations are unknown, traditional methods
    often resort to optimization techniques, where an initial guess for the parameters
    is made, and then refined in an iterative process to minimize the difference between
    the observed data and the data predicted by the numerical solver. Since each optimization
    iteration necessitates one run of the numerical solver, this approach, while feasible,
    can be computationally very expensive.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ–¹ç¨‹å‚æ•°æœªçŸ¥çš„æƒ…å†µä¸‹ï¼Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸è¯‰è¯¸äºä¼˜åŒ–æŠ€æœ¯ï¼Œå…¶ä¸­å¯¹å‚æ•°è¿›è¡Œåˆæ­¥çŒœæµ‹ï¼Œç„¶åé€šè¿‡è¿­ä»£è¿‡ç¨‹æ¥ä¼˜åŒ–ï¼Œä»¥æœ€å°åŒ–è§‚å¯Ÿæ•°æ®ä¸æ•°å€¼æ±‚è§£å™¨é¢„æµ‹æ•°æ®ä¹‹é—´çš„å·®å¼‚ã€‚ç”±äºæ¯æ¬¡ä¼˜åŒ–è¿­ä»£éƒ½éœ€è¦è¿è¡Œä¸€æ¬¡æ•°å€¼æ±‚è§£å™¨ï¼Œè¿™ç§æ–¹æ³•è™½ç„¶å¯è¡Œï¼Œä½†è®¡ç®—å¼€é”€å¯èƒ½éå¸¸å¤§ã€‚
- en: 'Note that the above discussion only describes the case of calibrating the unknown
    parameters. The problem becomes even more complex when we need to estimate unknown
    functions in differential equations. Theoretically, we can adopt a similar methodology,
    i.e., making assumptions about the forms of the unknown functions before optimization.
    However, issues would immediately rise if we go down this path: If we assume an
    overly simple form, we run into the risk of **underfitting**, which may lead to
    substantial prediction errors. On the other hand, if we assume an overly complex
    form (e.g., with many tunable parameters), we run into the risk of **overfitting**,
    which may lead to poor generalization performance.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä¸Šè¿°è®¨è®ºä»…æè¿°äº†æ ¡å‡†æœªçŸ¥å‚æ•°çš„æƒ…å†µã€‚å½“æˆ‘ä»¬éœ€è¦ä¼°è®¡å¾®åˆ†æ–¹ç¨‹ä¸­çš„æœªçŸ¥å‡½æ•°æ—¶ï¼Œé—®é¢˜å˜å¾—æ›´åŠ å¤æ‚ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ç±»ä¼¼çš„æ–¹æ³•ï¼Œå³åœ¨ä¼˜åŒ–ä¹‹å‰å¯¹æœªçŸ¥å‡½æ•°çš„å½¢å¼åšå‡ºå‡è®¾ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬èµ°è¿™æ¡è·¯ï¼Œä¼šç«‹å³å‡ºç°é—®é¢˜ï¼šå¦‚æœæˆ‘ä»¬å‡è®¾ä¸€ä¸ªè¿‡äºç®€å•çš„å½¢å¼ï¼Œæˆ‘ä»¬é¢ä¸´**æ¬ æ‹Ÿåˆ**çš„é£é™©ï¼Œè¿™å¯èƒ½å¯¼è‡´è¾ƒå¤§çš„é¢„æµ‹è¯¯å·®ã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘ä»¬å‡è®¾ä¸€ä¸ªè¿‡äºå¤æ‚çš„å½¢å¼ï¼ˆä¾‹å¦‚ï¼Œå…·æœ‰è®¸å¤šå¯è°ƒå‚æ•°ï¼‰ï¼Œæˆ‘ä»¬é¢ä¸´**è¿‡æ‹Ÿåˆ**çš„é£é™©ï¼Œè¿™å¯èƒ½å¯¼è‡´è¾ƒå·®çš„æ³›åŒ–æ€§èƒ½ã€‚
- en: 'In summary, the traditional approach faces significant challenges when dealing
    with partially known differential equations:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†éƒ¨åˆ†å·²çŸ¥å¾®åˆ†æ–¹ç¨‹æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼š
- en: 1ï¸âƒ£ Traditional numerical methods rely on having a complete form of governing
    differential equations to run simulations.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ ä¼ ç»Ÿæ•°å€¼æ–¹æ³•ä¾èµ–äºå…·æœ‰å®Œæ•´æ§åˆ¶å¾®åˆ†æ–¹ç¨‹çš„å½¢å¼æ¥è¿›è¡Œæ¨¡æ‹Ÿã€‚
- en: 2ï¸âƒ£ Combining traditional numerical methods with optimization algorithms can
    address parameter estimation problems, but often at a high computational cost.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ å°†ä¼ ç»Ÿæ•°å€¼æ–¹æ³•ä¸ä¼˜åŒ–ç®—æ³•ç»“åˆå¯ä»¥è§£å†³å‚æ•°ä¼°è®¡é—®é¢˜ï¼Œä½†é€šå¸¸ä»£ä»·å¾ˆé«˜ã€‚
- en: 3ï¸âƒ£ For estimating unknown functions embedded in differential equations, traditional
    approaches may yield results that are highly sensitive to the assumed functional
    form, which creates risks of underfitting or overfitting.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ å¯¹äºåµŒå…¥å¾®åˆ†æ–¹ç¨‹ä¸­çš„æœªçŸ¥å‡½æ•°è¿›è¡Œä¼°è®¡æ—¶ï¼Œä¼ ç»Ÿæ–¹æ³•å¯èƒ½ä¼šå¾—åˆ°å¯¹å‡è®¾å‡½æ•°å½¢å¼é«˜åº¦æ•æ„Ÿçš„ç»“æœï¼Œè¿™ä¼šå¯¼è‡´æ¬ æ‹Ÿåˆæˆ–è¿‡æ‹Ÿåˆçš„é£é™©ã€‚
- en: Given these challenges, traditional approaches often fall short in addressing
    system identification problems where unknown parameters and functional forms coexist.
    This naturally leads us to the topic of physics-informed neural networks (PINNs).
    In the next section, we will see how PINN can effectively address the challenges
    faced by traditional approaches.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºè¿™äº›æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†æœªçŸ¥å‚æ•°å’Œå‡½æ•°å½¢å¼å…±å­˜çš„ç³»ç»Ÿè¯†åˆ«é—®é¢˜æ—¶å¾€å¾€æ•ˆæœä¸ä½³ã€‚è¿™è‡ªç„¶å¼•å‡ºäº†ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNsï¼‰çš„è¯é¢˜ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°PINNå¦‚ä½•æœ‰æ•ˆåœ°è§£å†³ä¼ ç»Ÿæ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ã€‚
- en: '**3\. PINN for System Identification (Theory)**'
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**3\. PINNåœ¨ç³»ç»Ÿè¯†åˆ«ä¸­çš„åº”ç”¨ï¼ˆç†è®ºï¼‰**'
- en: The physics-informed neural network (or PINN in short) is a powerful concept
    proposed by [Raissi et al.](https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125)
    back in 2019\. The basic idea of PINN, like other *physics-informed machine learning*
    techniques, is to create a hybrid model where both the observational data and
    the known physical knowledge (represented as differential equations) are leveraged
    in model training. PINN was originally designed as an efficient ODE/PDE solver.
    However, researchers soon recognized that PINNs have (arguably) even greater potential
    in tackling inverse, system identification problems.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆç®€ç§°PINNï¼‰æ˜¯[Raissiç­‰äºº](https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125)åœ¨2019å¹´æå‡ºçš„ä¸€ä¸ªå¼ºå¤§æ¦‚å¿µã€‚PINNçš„åŸºæœ¬æ€æƒ³ï¼Œåƒå…¶ä»–*ç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ *æŠ€æœ¯ä¸€æ ·ï¼Œæ˜¯åˆ›å»ºä¸€ä¸ªæ··åˆæ¨¡å‹ï¼Œå…¶ä¸­åœ¨æ¨¡å‹è®­ç»ƒä¸­åˆ©ç”¨äº†è§‚å¯Ÿæ•°æ®å’Œå·²çŸ¥çš„ç‰©ç†çŸ¥è¯†ï¼ˆä»¥å¾®åˆ†æ–¹ç¨‹å½¢å¼è¡¨ç¤ºï¼‰ã€‚PINNæœ€åˆè¢«è®¾è®¡ä¸ºä¸€ä¸ªé«˜æ•ˆçš„ODE/PDEæ±‚è§£å™¨ã€‚ç„¶è€Œï¼Œç ”ç©¶äººå‘˜å¾ˆå¿«è®¤è¯†åˆ°PINNåœ¨è§£å†³é€†é—®é¢˜å’Œç³»ç»Ÿè¯†åˆ«é—®é¢˜ä¸Šï¼ˆå¯ä»¥è¯´ï¼‰å…·æœ‰æ›´å¤§çš„æ½œåŠ›ã€‚
- en: In the following, we will explain how PINNs can be leveraged to overcome the
    challenges we discussed in the previous section, one by one.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„å†…å®¹ä¸­ï¼Œæˆ‘ä»¬å°†é€ä¸€è§£é‡Šå¦‚ä½•åˆ©ç”¨PINNå…‹æœæˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚è®¨è®ºçš„æŒ‘æˆ˜ã€‚
- en: 1ï¸âƒ£ Traditional numerical methods rely on having a complete form of governing
    differential equations to run simulations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ ä¼ ç»Ÿæ•°å€¼æ–¹æ³•ä¾èµ–äºæ‹¥æœ‰å®Œæ•´å½¢å¼çš„ä¸»æ§å¾®åˆ†æ–¹ç¨‹æ¥è¿›è¡Œæ¨¡æ‹Ÿã€‚
- en: 'ğŸ“£**PINNâ€™s response**: Unlike traditional methods, I am capable of working with
    partially known differential equations, thus not confined by a complete equation
    to run simulations.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“£**PINNçš„å“åº”**ï¼šä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘èƒ½å¤Ÿå¤„ç†éƒ¨åˆ†å·²çŸ¥çš„å¾®åˆ†æ–¹ç¨‹ï¼Œå› æ­¤ä¸å—å®Œæ•´æ–¹ç¨‹çš„é™åˆ¶æ¥è¿›è¡Œæ¨¡æ‹Ÿã€‚
- en: From an exterior perspective, PINN just resembles a conventional neural network
    model that takes the temporal/spatial coordinates (e.g., *t*, *x*, *y*) as input
    and outputs the target quantities (e.g., velocity *u*, pressure *p*, temperature
    *T*, etc.) we are trying to simulate. However, what sets PINNs apart from conventional
    NNs is that in PINN, the differential equations are used as constraints during
    the training process. Specifically, PINN introduces an extra loss term that accounts
    for the residuals of the governing differential equations, which is calculated
    by supplying the predicted quantities into the governing equations. By optimizing
    this loss term, we effectively make the trained network aware of the underlying
    physics.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å¤–éƒ¨è§’åº¦çœ‹ï¼ŒPINNä»…ä»…ç±»ä¼¼äºä¸€ä¸ªä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†æ—¶é—´/ç©ºé—´åæ ‡ï¼ˆä¾‹å¦‚ï¼Œ*t*ï¼Œ*x*ï¼Œ*y*ï¼‰ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºæˆ‘ä»¬è¯•å›¾æ¨¡æ‹Ÿçš„ç›®æ ‡é‡ï¼ˆä¾‹å¦‚ï¼Œé€Ÿåº¦*u*ï¼Œå‹åŠ›*p*ï¼Œæ¸©åº¦*T*ç­‰ï¼‰ã€‚ç„¶è€Œï¼Œä½¿PINNä¸ä¼ ç»ŸNNä¸åŒçš„æ˜¯ï¼Œåœ¨PINNä¸­ï¼Œå¾®åˆ†æ–¹ç¨‹ä½œä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„çº¦æŸã€‚å…·ä½“æ¥è¯´ï¼ŒPINNå¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„æŸå¤±é¡¹ï¼Œç”¨äºè®¡ç®—ä¸»æ§å¾®åˆ†æ–¹ç¨‹çš„æ®‹å·®ï¼Œè¯¥æ®‹å·®é€šè¿‡å°†é¢„æµ‹é‡ä»£å…¥ä¸»æ§æ–¹ç¨‹è®¡ç®—å¾—åˆ°ã€‚é€šè¿‡ä¼˜åŒ–è¿™ä¸ªæŸå¤±é¡¹ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°ä½¿è®­ç»ƒåçš„ç½‘ç»œæ„è¯†åˆ°æ½œåœ¨çš„ç‰©ç†è§„å¾‹ã€‚
- en: '![](../Images/ea45326f2cd4ae4f4d2a7a2054350c86.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea45326f2cd4ae4f4d2a7a2054350c86.png)'
- en: Figure 2\. Physics-informed neural networks incorporate differential equations
    into the loss function, therefore effectively making the trained network aware
    of the underlying physics. (Image by this blog author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2. ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œå°†å¾®åˆ†æ–¹ç¨‹çº³å…¥æŸå¤±å‡½æ•°ä¸­ï¼Œå› æ­¤æœ‰æ•ˆåœ°ä½¿è®­ç»ƒåçš„ç½‘ç»œæ„è¯†åˆ°æ½œåœ¨çš„ç‰©ç†è§„å¾‹ã€‚ï¼ˆå›¾åƒç”±æœ¬åšå®¢ä½œè€…æä¾›ï¼‰
- en: Since the differential equations are solely used in constructing the loss function,
    they have no impact on the PINN model architecture. This essentially means that
    we do not need to have complete knowledge of the differential equations for training.
    Even if we only know part of the equation, this knowledge can still be incorporated
    to enforce the output to obey the known physics. This flexibility of accommodating
    varying degrees of knowledge completeness presents a significant advantage over
    traditional numerical approaches.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå¾®åˆ†æ–¹ç¨‹ä»…ç”¨äºæ„å»ºæŸå¤±å‡½æ•°ï¼Œå› æ­¤å®ƒä»¬å¯¹PINNæ¨¡å‹ç»“æ„æ²¡æœ‰å½±å“ã€‚è¿™å®é™…ä¸Šæ„å‘³ç€æˆ‘ä»¬åœ¨è®­ç»ƒæ—¶ä¸éœ€è¦å¯¹å¾®åˆ†æ–¹ç¨‹æœ‰å®Œå…¨çš„äº†è§£ã€‚å³ä½¿æˆ‘ä»¬åªçŸ¥é“æ–¹ç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œè¿™äº›çŸ¥è¯†ä»ç„¶å¯ä»¥è¢«çº³å…¥ä»¥å¼ºåˆ¶è¾“å‡ºéµå¾ªå·²çŸ¥çš„ç‰©ç†è§„å¾‹ã€‚è¿™ç§é€‚åº”çŸ¥è¯†å®Œæ•´åº¦ä¸åŒçš„çµæ´»æ€§ç›¸æ¯”ä¼ ç»Ÿæ•°å€¼æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚
- en: 2ï¸âƒ£ Combining traditional numerical methods with optimization algorithms can
    address parameter estimation problems, but often at a high computational cost.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ç»“åˆä¼ ç»Ÿæ•°å€¼æ–¹æ³•ä¸ä¼˜åŒ–ç®—æ³•å¯ä»¥è§£å†³å‚æ•°ä¼°è®¡é—®é¢˜ï¼Œä½†é€šå¸¸ä»£ä»·è¾ƒé«˜ã€‚
- en: 'ğŸ“£**PINNâ€™s response**: I can provide a computationally efficient alternative
    for estimating unknown parameters.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“£**PINNçš„å›åº”**ï¼šæˆ‘å¯ä»¥æä¾›ä¸€ç§è®¡ç®—ä¸Šé«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆæ¥ä¼°è®¡æœªçŸ¥å‚æ•°ã€‚
- en: Unlike traditional approaches that treat parameter estimation as a separate
    optimization task, PINNs seamlessly integrate this process into the model training
    stage. Specifically, in PINNs, the unknown parameters are simply treated as additional
    trainable parameters, which are optimized along with the other neural network
    weights and biases during training.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å°†å‚æ•°ä¼°è®¡è§†ä¸ºå•ç‹¬ä¼˜åŒ–ä»»åŠ¡çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒPINNså°†è¿™ä¸€è¿‡ç¨‹æ— ç¼åœ°é›†æˆåˆ°æ¨¡å‹è®­ç»ƒé˜¶æ®µã€‚åœ¨PINNsä¸­ï¼ŒæœªçŸ¥å‚æ•°è¢«ç®€å•åœ°è§†ä¸ºé¢å¤–çš„å¯è®­ç»ƒå‚æ•°ï¼Œè¿™äº›å‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸å…¶ä»–ç¥ç»ç½‘ç»œçš„æƒé‡å’Œåå·®ä¸€èµ·ä¼˜åŒ–ã€‚
- en: '![](../Images/e87cddadb163478ef75654173c1e5060.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e87cddadb163478ef75654173c1e5060.png)'
- en: Figure 3\. Unknown parameters are optimized jointly with the weights and biases
    of PINN. At the end of the training, the final values of *a* and *b* we obtained
    constitute the estimates of the unknown parameters. (Image by this blog author)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3\. æœªçŸ¥å‚æ•°ä¸PINNçš„æƒé‡å’Œåå·®ä¸€èµ·ä¼˜åŒ–ã€‚åœ¨è®­ç»ƒç»“æŸæ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°çš„æœ€ç»ˆå€¼ *a* å’Œ *b* ä½œä¸ºæœªçŸ¥å‚æ•°çš„ä¼°è®¡å€¼ã€‚ï¼ˆå›¾ç‰‡ç”±æœ¬åšå®¢ä½œè€…æä¾›ï¼‰
- en: In addition, PINNs fully leverage the modern deep learning framework to perform
    training. This allows for rapid computation of the gradients (i.e., via automatic
    differentiation) needed for advanced optimization algorithms (e.g., Adam), therefore
    greatly accelerating the parameter estimation process, especially for problems
    with a high-dimensional parameter space. All these factors make PINNs a competitive
    alternative for parameter estimation problems.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒPINNså……åˆ†åˆ©ç”¨ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶æ¥æ‰§è¡Œè®­ç»ƒã€‚è¿™å…è®¸å¿«é€Ÿè®¡ç®—æ‰€éœ€çš„æ¢¯åº¦ï¼ˆå³é€šè¿‡è‡ªåŠ¨å¾®åˆ†ï¼‰ï¼Œä»¥ç”¨äºé«˜çº§ä¼˜åŒ–ç®—æ³•ï¼ˆä¾‹å¦‚Adamï¼‰ï¼Œä»è€Œå¤§å¤§åŠ é€Ÿäº†å‚æ•°ä¼°è®¡è¿‡ç¨‹ï¼Œå°¤å…¶æ˜¯å¯¹äºé«˜ç»´å‚æ•°ç©ºé—´çš„é—®é¢˜ã€‚è¿™äº›å› ç´ ä½¿å¾—PINNsæˆä¸ºå‚æ•°ä¼°è®¡é—®é¢˜çš„ä¸€ä¸ªæœ‰ç«äº‰åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚
- en: 3ï¸âƒ£ For estimating unknown functions embedded in differential equations, traditional
    approaches may yield results that are highly sensitive to the assumed functional
    form, which creates risks of underfitting or overfitting.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ å¯¹äºåµŒå…¥å¾®åˆ†æ–¹ç¨‹ä¸­çš„æœªçŸ¥å‡½æ•°ï¼Œä¼ ç»Ÿæ–¹æ³•å¯èƒ½ä¼šå¾—åˆ°å¯¹å‡è®¾å‡½æ•°å½¢å¼é«˜åº¦æ•æ„Ÿçš„ç»“æœï¼Œè¿™ä¼šäº§ç”Ÿæ¬ æ‹Ÿåˆæˆ–è¿‡æ‹Ÿåˆçš„é£é™©ã€‚
- en: 'ğŸ“£**PINNâ€™s response**: The unknown functions can be effectively parameterized
    by additional neural networks, which can be trained jointly with me, just like
    the previous parameter estimation scenario.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“£**PINNçš„å›åº”**ï¼šæœªçŸ¥å‡½æ•°å¯ä»¥é€šè¿‡é¢å¤–çš„ç¥ç»ç½‘ç»œæœ‰æ•ˆåœ°å‚æ•°åŒ–ï¼Œè¿™äº›ç¥ç»ç½‘ç»œå¯ä»¥ä¸æˆ‘ä¸€èµ·è®­ç»ƒï¼Œå°±åƒä¹‹å‰çš„å‚æ•°ä¼°è®¡åœºæ™¯ä¸€æ ·ã€‚
- en: Instead of assuming the form of unknown functions, we can approximate the unknown
    functions with separate neural networks, and later integrate them into the main
    PINN model. Just as in the previous parameter estimation scenario, here, we can
    view those extra neural nets as an extensive set of unknown parameters to be estimated.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨ç‹¬ç«‹çš„ç¥ç»ç½‘ç»œæ¥é€¼è¿‘æœªçŸ¥å‡½æ•°ï¼Œç„¶åå°†å®ƒä»¬é›†æˆåˆ°ä¸»PINNæ¨¡å‹ä¸­ã€‚å°±åƒåœ¨ä¹‹å‰çš„å‚æ•°ä¼°è®¡åœºæ™¯ä¸­ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™äº›é¢å¤–çš„ç¥ç»ç½‘ç»œè§†ä¸ºéœ€è¦ä¼°è®¡çš„å¤§é‡æœªçŸ¥å‚æ•°ã€‚
- en: '![](../Images/44e745359b4f6cee0d0649b3b12bb55e.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44e745359b4f6cee0d0649b3b12bb55e.png)'
- en: Figure 4\. The unknown functions can be parameterized by a separate neural network
    and trained jointly with the original PINN. The ODE/PDE residual loss term regularizes
    the auxiliary neural network such that the governing equations are satisfied.
    In this way, the auxiliary neural network can automatically learn the optimal
    functional forms directly from the data. (Image by this blog author)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ã€‚æœªçŸ¥å‡½æ•°å¯ä»¥é€šè¿‡ä¸€ä¸ªç‹¬ç«‹çš„ç¥ç»ç½‘ç»œè¿›è¡Œå‚æ•°åŒ–ï¼Œå¹¶ä¸åŸå§‹PINNä¸€èµ·è®­ç»ƒã€‚ODE/PDEæ®‹å·®æŸå¤±é¡¹å¯¹è¾…åŠ©ç¥ç»ç½‘ç»œè¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥æ»¡è¶³æ§åˆ¶æ–¹ç¨‹ã€‚è¿™æ ·ï¼Œè¾…åŠ©ç¥ç»ç½‘ç»œå¯ä»¥ç›´æ¥ä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ æœ€ä½³çš„å‡½æ•°å½¢å¼ã€‚ï¼ˆå›¾åƒæ¥æºäºæœ¬åšå®¢ä½œè€…ï¼‰
- en: During training, the weights and biases of those auxiliary neural nets will
    be trained simultaneously with the original PINN to minimize the loss function
    (data loss + ODE residual loss). In doing so, those auxiliary neural networks
    can learn the optimal functional forms directly from the data. By removing the
    need to make risky assumptions about the functional form, this strategy helps
    alleviate the problems of underfitting and overfitting.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¿™äº›è¾…åŠ©ç¥ç»ç½‘ç»œçš„æƒé‡å’Œåå·®å°†ä¸åŸå§‹PINNåŒæ—¶è®­ç»ƒï¼Œä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼ˆæ•°æ®æŸå¤± + ODEæ®‹å·®æŸå¤±ï¼‰ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œè¿™äº›è¾…åŠ©ç¥ç»ç½‘ç»œå¯ä»¥ç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ æœ€ä½³çš„å‡½æ•°å½¢å¼ã€‚é€šè¿‡æ¶ˆé™¤å¯¹å‡½æ•°å½¢å¼è¿›è¡Œé£é™©å‡è®¾çš„éœ€è¦ï¼Œè¿™ç§ç­–ç•¥æœ‰åŠ©äºç¼“è§£æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚
- en: In summary, the power of PINN lies in its ability to work with partially known
    differential equations and efficiently learn unknown parameters and function forms
    directly from the data. This versatility sets them apart from traditional approaches,
    therefore making them an effective tool for system identification tasks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼ŒPINNçš„ä¼˜åŠ¿åœ¨äºå…¶èƒ½å¤Ÿå¤„ç†éƒ¨åˆ†å·²çŸ¥çš„å¾®åˆ†æ–¹ç¨‹ï¼Œå¹¶æœ‰æ•ˆåœ°ä»æ•°æ®ä¸­å­¦ä¹ æœªçŸ¥å‚æ•°å’Œå‡½æ•°å½¢å¼ã€‚è¿™ç§å¤šåŠŸèƒ½æ€§ä½¿å…¶ä¸ä¼ ç»Ÿæ–¹æ³•åŒºåˆ«å¼€æ¥ï¼Œå› æ­¤æˆä¸ºç³»ç»Ÿè¯†åˆ«ä»»åŠ¡çš„æœ‰æ•ˆå·¥å…·ã€‚
- en: In the next section, we will start working on our case study and turn theory
    into actual code.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å¼€å§‹å¤„ç†æˆ‘ä»¬çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œå¹¶å°†ç†è®ºè½¬åŒ–ä¸ºå®é™…ä»£ç ã€‚
- en: 4\. PINN for System Identification (Code)
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. PINNç”¨äºç³»ç»Ÿè¯†åˆ«ï¼ˆä»£ç ï¼‰
- en: 'In this section, we will implement a PINN (in TensorFlow) to address our target
    case study. Letâ€™s start by importing the necessary libraries:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ä¸ªPINNï¼ˆåœ¨TensorFlowä¸­ï¼‰æ¥è§£å†³æˆ‘ä»¬çš„ç›®æ ‡æ¡ˆä¾‹ç ”ç©¶ã€‚è®©æˆ‘ä»¬ä»å¯¼å…¥å¿…è¦çš„åº“å¼€å§‹ï¼š
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 4.1 Define the Architecture
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 å®šä¹‰æ¶æ„
- en: 'For the main PINN, we use one neural network to predict ***u***, which has
    1-dimensional input (i.e., *t*) and 3-dimensional output (*u*â‚, *u*â‚‚, and *u*â‚ƒ).
    In addition, as discussed in the previous section, we use an auxiliary neural
    network to approximate the unknown functions *f*â‚ and *f*â‚‚, which has 4-dimensional
    input (i.e., *t*, *u*â‚, *u*â‚‚, and *u*â‚ƒ) and 2-dimensional output (*f*â‚ and *f*â‚‚).
    The architecture of the overall PINN is shown below:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸»è¦çš„PINNï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥é¢„æµ‹***u***ï¼Œå…¶å…·æœ‰1ç»´è¾“å…¥ï¼ˆå³*t*ï¼‰å’Œ3ç»´è¾“å‡ºï¼ˆ*u*â‚ã€*u*â‚‚å’Œ*u*â‚ƒï¼‰ã€‚æ­¤å¤–ï¼Œå¦‚å‰ä¸€èŠ‚æ‰€è®¨è®ºçš„ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªè¾…åŠ©ç¥ç»ç½‘ç»œæ¥é€¼è¿‘æœªçŸ¥å‡½æ•°*f*â‚å’Œ*f*â‚‚ï¼Œè¯¥ç½‘ç»œå…·æœ‰4ç»´è¾“å…¥ï¼ˆå³*t*ã€*u*â‚ã€*u*â‚‚å’Œ*u*â‚ƒï¼‰å’Œ2ç»´è¾“å‡ºï¼ˆ*f*â‚å’Œ*f*â‚‚ï¼‰ã€‚æ•´ä½“PINNçš„æ¶æ„å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/335fd393f34cf3ca8a2d48b78cd079de.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/335fd393f34cf3ca8a2d48b78cd079de.png)'
- en: Figure 5\. The architecture of the employed PINN model.(Image by this blog author)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5ã€‚æ‰€ä½¿ç”¨çš„PINNæ¨¡å‹çš„æ¶æ„ã€‚ï¼ˆå›¾åƒæ¥æºäºæœ¬åšå®¢ä½œè€…ï¼‰
- en: One thing worth emphasizing again is that it is necessary to feed the auxiliary
    neural network with all the available features (in our current case, *t*, *u*â‚,
    *u*â‚‚, and *u*â‚ƒ), as we do not know the precise functional forms of *f*â‚ and *f*â‚‚.
    During training, the auxiliary neural network will automatically determine which
    features are necessary/important in a data-driven manner.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—å†æ¬¡å¼ºè°ƒçš„æ˜¯ï¼Œéœ€è¦å‘è¾…åŠ©ç¥ç»ç½‘ç»œæä¾›æ‰€æœ‰å¯ç”¨çš„ç‰¹å¾ï¼ˆåœ¨æˆ‘ä»¬å½“å‰çš„æƒ…å†µä¸‹ï¼Œ*t*ã€*u*â‚ã€*u*â‚‚å’Œ*u*â‚ƒï¼‰ï¼Œå› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“*f*â‚å’Œ*f*â‚‚çš„ç¡®åˆ‡å‡½æ•°å½¢å¼ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¾…åŠ©ç¥ç»ç½‘ç»œå°†ä»¥æ•°æ®é©±åŠ¨çš„æ–¹å¼è‡ªåŠ¨ç¡®å®šå“ªäº›ç‰¹å¾æ˜¯å¿…è¦çš„/é‡è¦çš„ã€‚
- en: 'First, letâ€™s define the neural network that predicts ***u***. Here, we use
    two hidden layers, each of which is equipped with 50 neurons and hyperbolic tangent
    activation functions:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªé¢„æµ‹***u***çš„ç¥ç»ç½‘ç»œã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªéšè—å±‚ï¼Œæ¯ä¸ªå±‚é…å¤‡50ä¸ªç¥ç»å…ƒå’ŒåŒæ›²æ­£åˆ‡æ¿€æ´»å‡½æ•°ï¼š
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we define the auxiliary neural network that predicts ***f***. We adopt
    the same network architecture:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªé¢„æµ‹***f***çš„è¾…åŠ©ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬é‡‡ç”¨ç›¸åŒçš„ç½‘ç»œæ¶æ„ï¼š
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the code above, we add *a* and *b* to the collection of the neural network
    model parameters. This way, *a* and *b* can be optimized jointly with the other
    weights and biases of the neural network. We achieved this goal by defining a
    custom layer `ParameterLayer`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œæˆ‘ä»¬å°†*a*å’Œ*b*æ·»åŠ åˆ°ç¥ç»ç½‘ç»œæ¨¡å‹å‚æ•°çš„é›†åˆä¸­ã€‚è¿™æ ·ï¼Œ*a*å’Œ*b*å¯ä»¥ä¸ç¥ç»ç½‘ç»œçš„å…¶ä»–æƒé‡å’Œåå·®ä¸€èµ·ä¼˜åŒ–ã€‚æˆ‘ä»¬é€šè¿‡å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰å±‚`ParameterLayer`å®ç°äº†è¿™ä¸€ç›®æ ‡ï¼š
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that this layer does nothing besides introducing the two parameters as
    the model attributes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œè¿™ä¸€å±‚é™¤äº†å¼•å…¥è¿™ä¸¤ä¸ªå‚æ•°ä½œä¸ºæ¨¡å‹å±æ€§å¤–æ²¡æœ‰å…¶ä»–ä½œç”¨ã€‚
- en: 'Finally, we put *u*-net and *f*-net together and define the architecture for
    the full PINN:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°† *u*-net å’Œ *f*-net ç»“åˆåœ¨ä¸€èµ·ï¼Œå®šä¹‰å®Œæ•´çš„ PINN æ¶æ„ï¼š
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the code above, we concatenate the input *t* and the *u*-net outputs *u*â‚,
    *u*â‚‚, and *u*â‚ƒ before feeding them into the *f*-net. Also, we output both ***u***
    and ***f*** in the overall PINN model. Although only ***u*** is needed in practice
    (as ***u*** is our modeling target), the prediction of ***f*** will become useful
    later for distilling its analytical functional forms (see section 5).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œæˆ‘ä»¬å°†è¾“å…¥ *t* å’Œ *u*-net è¾“å‡º *u*â‚, *u*â‚‚, å’Œ *u*â‚ƒ è¿›è¡Œä¸²è”ï¼Œç„¶åè¾“å…¥åˆ° *f*-net ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ•´ä½“
    PINN æ¨¡å‹ä¸­è¾“å‡º ***u*** å’Œ ***f***ã€‚è™½ç„¶åœ¨å®é™…åº”ç”¨ä¸­åªéœ€è¦ ***u***ï¼ˆå› ä¸º ***u*** æ˜¯æˆ‘ä»¬çš„å»ºæ¨¡ç›®æ ‡ï¼‰ï¼Œä½†åç»­ ***f***
    çš„é¢„æµ‹ä¼šå˜å¾—æœ‰ç”¨ï¼Œä»¥æå–å…¶åˆ†æå‡½æ•°å½¢å¼ï¼ˆè§ç¬¬ 5 èŠ‚ï¼‰ã€‚
- en: 4.2 Define ODE loss
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 å®šä¹‰ ODE æŸå¤±
- en: 'Next, we define the function to compute the ODE residual loss. Recall that
    our target ODEs are:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰è®¡ç®— ODE æ®‹å·®æŸå¤±çš„å‡½æ•°ã€‚å›é¡¾ä¸€ä¸‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡ ODEs æ˜¯ï¼š
- en: '![](../Images/b38eab8013ffaefb44572f7be65d94c1.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b38eab8013ffaefb44572f7be65d94c1.png)'
- en: 'Therefore, we can define the function as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼å®šä¹‰å‡½æ•°ï¼š
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Although the code above is mostly self-explanatory, several things are worth
    mentioning:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ä¸Šè¿°ä»£ç å¤§éƒ¨åˆ†æ˜¯è‡ªè§£é‡Šçš„ï¼Œä½†æœ‰å‡ ä¸ªé—®é¢˜å€¼å¾—æåŠï¼š
- en: We used `tf.GradientTape.batch_jacobian()` (instead of the usual `GradientTape.gradient()`)
    to calculate the gradient of *u*â‚, *u*â‚‚, and *u*â‚ƒ w.r.t *t.* `GradientTape.gradient()`
    wonâ€™t work here as it computes the sum d*u*â‚/dt + d*u*â‚‚/dt + d*u*â‚ƒ/dt. Potentially
    we could also use `GradientTape.jacobian()` here to compute the gradient of each
    output value w.r.t each input value. For more details, please refer to the [official
    page](https://www.tensorflow.org/api_docs/python/tf/GradientTape#methods).
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨äº† `tf.GradientTape.batch_jacobian()`ï¼ˆè€Œä¸æ˜¯é€šå¸¸çš„ `GradientTape.gradient()`ï¼‰æ¥è®¡ç®—
    *u*â‚, *u*â‚‚ å’Œ *u*â‚ƒ ç›¸å¯¹äº *t* çš„æ¢¯åº¦ã€‚`GradientTape.gradient()` åœ¨è¿™é‡Œä¸èµ·ä½œç”¨ï¼Œå› ä¸ºå®ƒè®¡ç®—çš„æ˜¯ d*u*â‚/dt
    + d*u*â‚‚/dt + d*u*â‚ƒ/dtã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨ `GradientTape.jacobian()` æ¥è®¡ç®—æ¯ä¸ªè¾“å‡ºå€¼ç›¸å¯¹äºæ¯ä¸ªè¾“å…¥å€¼çš„æ¢¯åº¦ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è§
    [å®˜æ–¹é¡µé¢](https://www.tensorflow.org/api_docs/python/tf/GradientTape#methods)ã€‚
- en: We used `@tf.function` decorator to convert the above Python function into a
    TensorFlow graph. It is useful to do that as gradient calculation can be quite
    expensive and executing it in Graph mode can significantly accelerate the computations.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨äº† `@tf.function` è£…é¥°å™¨å°†ä¸Šè¿° Python å‡½æ•°è½¬æ¢ä¸º TensorFlow å›¾ã€‚è¿™æ˜¯æœ‰ç”¨çš„ï¼Œå› ä¸ºæ¢¯åº¦è®¡ç®—å¯èƒ½éå¸¸æ˜‚è´µï¼Œä½¿ç”¨å›¾æ¨¡å¼æ‰§è¡Œå¯ä»¥æ˜¾è‘—åŠ é€Ÿè®¡ç®—ã€‚
- en: 4.3 Define gradient descent step
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 å®šä¹‰æ¢¯åº¦ä¸‹é™æ­¥éª¤
- en: 'Next, we configure the logic for calculating the gradients of total loss with
    respect to the parameters (network weights and biases, as well as the unknown
    parameters *a* and *b*). This is necessary for performing the gradient descent
    for model training:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é…ç½®äº†è®¡ç®—æ€»æŸå¤±ç›¸å¯¹äºå‚æ•°ï¼ˆç½‘ç»œæƒé‡å’Œåå·®ï¼Œä»¥åŠæœªçŸ¥å‚æ•° *a* å’Œ *b*ï¼‰çš„æ¢¯åº¦çš„é€»è¾‘ã€‚è¿™å¯¹äºæ‰§è¡Œæ¨¡å‹è®­ç»ƒçš„æ¢¯åº¦ä¸‹é™æ˜¯å¿…è¦çš„ï¼š
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the code above:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼š
- en: 'We consider three loss terms: the initial condition loss `IC_loss`, the ODE
    residuals loss`ODE_loss`, and the data loss `data_loss`. The `IC_loss` is calculated
    by comparing the model-predicted ***u***(*t*=0) with the known initial value of
    ***u***,the `ODE_loss` is calculated by calling our previously defined `ODE_residual_calculator`
    function, and the data loss is calculated by simply comparing the model predictions
    (i.e., *u*â‚, *u*â‚‚, *u*â‚ƒ) with their observed values.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è€ƒè™‘ä¸‰ä¸ªæŸå¤±é¡¹ï¼šåˆå§‹æ¡ä»¶æŸå¤± `IC_loss`ã€ODE æ®‹å·®æŸå¤± `ODE_loss` å’Œæ•°æ®æŸå¤± `data_loss`ã€‚`IC_loss` é€šè¿‡å°†æ¨¡å‹é¢„æµ‹çš„***u***(*t*=0)ä¸å·²çŸ¥çš„***u***åˆå§‹å€¼è¿›è¡Œæ¯”è¾ƒæ¥è®¡ç®—ï¼Œ`ODE_loss`
    é€šè¿‡è°ƒç”¨æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„ `ODE_residual_calculator` å‡½æ•°æ¥è®¡ç®—ï¼Œè€Œæ•°æ®æŸå¤±åˆ™æ˜¯é€šè¿‡å°†æ¨¡å‹é¢„æµ‹å€¼ï¼ˆå³ *u*â‚, *u*â‚‚, *u*â‚ƒï¼‰ä¸å®ƒä»¬çš„è§‚æµ‹å€¼è¿›è¡Œç®€å•æ¯”è¾ƒæ¥è®¡ç®—çš„ã€‚
- en: We define the total loss as a weighted sum of `IC_loss`,`ODE_loss`, and`data_loss`.
    Generally, the weights control how much emphasis is given to the individual loss
    terms during the training process. In our case study, it is sufficient to set
    all of them as 1.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ€»æŸå¤±å®šä¹‰ä¸º `IC_loss`ã€`ODE_loss` å’Œ `data_loss` çš„åŠ æƒå’Œã€‚é€šå¸¸ï¼Œæƒé‡æ§åˆ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹å„ä¸ªæŸå¤±é¡¹çš„é‡è§†ç¨‹åº¦ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œå°†å®ƒä»¬å…¨éƒ¨è®¾ç½®ä¸º
    1 å°±è¶³å¤Ÿäº†ã€‚
- en: 4.4 Data preparation
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 æ•°æ®å‡†å¤‡
- en: In this subsection, we discuss how to organize data for PINN model training.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•ç»„ç»‡æ•°æ®ä»¥è¿›è¡Œ PINN æ¨¡å‹è®­ç»ƒã€‚
- en: Recall that our total loss function contains both ODE residual loss and data
    loss. Therefore, we need to generate both collocation points in the time dimension
    (for evaluating ODE loss) and the paired input(*t*)-output(***u***) supervised
    data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å›å¿†ä¸€ä¸‹ï¼Œæˆ‘ä»¬çš„æ€»æŸå¤±å‡½æ•°åŒ…å«ODEæ®‹å·®æŸå¤±å’Œæ•°æ®æŸå¤±ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆæ—¶é—´ç»´åº¦ä¸Šçš„é…ç‚¹ï¼ˆç”¨äºè¯„ä¼°ODEæŸå¤±ï¼‰å’Œé…å¯¹è¾“å…¥(*t*)-è¾“å‡º(***u***)çš„ç›‘ç£æ•°æ®ã€‚
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the code above, we allocated 10000 equally-spaced collocation points within
    our target time domain [0, 10]. For facilitating data loss computation, we pre-generated
    the paired input(*t*)-output(***u***) dataset `u_obs`, with its first column being
    the time coordinates, and the remaining three columns representing *u*â‚, *u*â‚‚,
    and *u*â‚ƒ, respectively. `u_obs` contains 1000 data points and is calculated with
    the following code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬åœ¨ç›®æ ‡æ—¶é—´åŸŸ[0, 10]å†…åˆ†é…äº†10000ä¸ªç­‰é—´è·çš„é…ç‚¹ã€‚ä¸ºäº†æ–¹ä¾¿æ•°æ®æŸå¤±è®¡ç®—ï¼Œæˆ‘ä»¬é¢„ç”Ÿæˆäº†é…å¯¹è¾“å…¥(*t*)-è¾“å‡º(***u***)æ•°æ®é›†`u_obs`ï¼Œå…¶ç¬¬ä¸€åˆ—ä¸ºæ—¶é—´åæ ‡ï¼Œå…¶ä½™ä¸‰åˆ—åˆ†åˆ«è¡¨ç¤º
    *u*â‚ã€*u*â‚‚ å’Œ *u*â‚ƒã€‚`u_obs`åŒ…å«1000ä¸ªæ•°æ®ç‚¹ï¼Œè®¡ç®—æ–¹å¼å¦‚ä¸‹ä»£ç ï¼š
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'where `simulate_ODEs` is the ODE solver that simulates ***u***-trajectory given
    the initial conditions and simulation domain:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ `simulate_ODEs` æ˜¯ODEæ±‚è§£å™¨ï¼Œå®ƒåœ¨ç»™å®šåˆå§‹æ¡ä»¶å’Œæ¨¡æ‹ŸåŸŸçš„æƒ…å†µä¸‹æ¨¡æ‹Ÿ***u***è½¨è¿¹ï¼š
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The following figure shows the target ***u*** profiles. Note that we have sampled
    1000 equally-spaced (*t* â€” *u*â‚), (*t* â€” *u*â‚‚), and (*t* â€” *u*â‚ƒ) data pairs (contained
    in `u_obs`) as the supervised data for data loss calculation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†ç›®æ ‡***u***çš„è½®å»“ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å·²ç»æŠ½å–äº†1000ä¸ªç­‰é—´è·çš„ (*t* â€” *u*â‚)ã€(*t* â€” *u*â‚‚) å’Œ (*t* â€” *u*â‚ƒ)
    æ•°æ®å¯¹ï¼ˆåŒ…å«åœ¨`u_obs`ä¸­ï¼‰ï¼Œä½œä¸ºæ•°æ®æŸå¤±è®¡ç®—çš„ç›‘ç£æ•°æ®ã€‚
- en: '![](../Images/41569446dc20349a0521acc759aafbfe.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41569446dc20349a0521acc759aafbfe.png)'
- en: Figure 6\. Output profiles of our currently investigated ODEs. (Image by this
    blog author)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6\. æˆ‘ä»¬å½“å‰ç ”ç©¶çš„ODEçš„è¾“å‡ºè½®å»“ã€‚ï¼ˆå›¾åƒç”±æœ¬åšå®¢ä½œè€…æä¾›ï¼‰
- en: 4.5 PINN Training
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 PINN è®­ç»ƒ
- en: 'The following code defines the main training and validation logic:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç å®šä¹‰äº†ä¸»è¦çš„è®­ç»ƒå’ŒéªŒè¯é€»è¾‘ï¼š
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As discussed previously, we set the weights for different loss components as
    1.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä¹‹å‰è®¨è®ºçš„ï¼Œæˆ‘ä»¬å°†ä¸åŒæŸå¤±ç»„ä»¶çš„æƒé‡è®¾ç½®ä¸º1ã€‚
- en: We set the initial guess for *a* and *b* as -1 and 1, respectively. Recall that
    these values are different from their true values, which are -2 and 0, respectively.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°† *a* å’Œ *b* çš„åˆå§‹çŒœæµ‹è®¾ç½®ä¸º-1å’Œ1ï¼Œåˆ†åˆ«ã€‚å›å¿†ä¸€ä¸‹ï¼Œè¿™äº›å€¼ä¸å®ƒä»¬çš„çœŸå®å€¼ä¸åŒï¼ŒçœŸå®å€¼åˆ†åˆ«ä¸º-2å’Œ0ã€‚
- en: For validation, we add up the ODE residual loss and initial condition loss to
    serve as the final validation loss. Note that we do not consider data loss here
    as we assume we have no access to additional paired *t* â€” **u** datasets for validation
    purposes. The computed validation loss is used to adapt the learning rate.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†éªŒè¯ï¼Œæˆ‘ä»¬å°†ODEæ®‹å·®æŸå¤±å’Œåˆå§‹æ¡ä»¶æŸå¤±ç›¸åŠ ï¼Œä½œä¸ºæœ€ç»ˆçš„éªŒè¯æŸå¤±ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä¸è€ƒè™‘æ•°æ®æŸå¤±ï¼Œå› ä¸ºæˆ‘ä»¬å‡è®¾æ²¡æœ‰é¢å¤–çš„é…å¯¹ *t* â€” **u**
    æ•°æ®é›†ç”¨äºéªŒè¯ç›®çš„ã€‚è®¡ç®—å‡ºçš„éªŒè¯æŸå¤±ç”¨äºè°ƒæ•´å­¦ä¹ ç‡ã€‚
- en: The following figure displays the loss convergence curve. We can see that all
    three loss components converged properly, indicating that the training is done
    satisfactorily.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†æŸå¤±æ”¶æ•›æ›²çº¿ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ‰€æœ‰ä¸‰ä¸ªæŸå¤±ç»„ä»¶éƒ½æ­£ç¡®æ”¶æ•›ï¼Œè¿™è¡¨æ˜è®­ç»ƒå·²æ»¡æ„å®Œæˆã€‚
- en: '![](../Images/dd9788e21fc94bbf65450ede3abd0962.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd9788e21fc94bbf65450ede3abd0962.png)'
- en: Figure 7\. Loss convergence plot. (Image by this blog author)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7\. æŸå¤±æ”¶æ•›å›¾ã€‚ï¼ˆå›¾åƒç”±æœ¬åšå®¢ä½œè€…æä¾›ï¼‰
- en: The following figure shows the comparison between the predicted ***u***â€™s and
    the ground truth calculated by the ODE solver. Here, we can also see that the
    PINN is able to accurately solve our target ODEs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†é¢„æµ‹çš„***u***ä¸é€šè¿‡ODEæ±‚è§£å™¨è®¡ç®—çš„çœŸå®å€¼ä¹‹é—´çš„æ¯”è¾ƒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°PINNèƒ½å¤Ÿå‡†ç¡®åœ°è§£å†³æˆ‘ä»¬çš„ç›®æ ‡ODEã€‚
- en: '![](../Images/4f0a4ec20674c126dda63aa99008a376.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f0a4ec20674c126dda63aa99008a376.png)'
- en: Figure 8\. Comparison of predicted **u**â€™s and the ground truth computed by
    ODE solver.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8\. é¢„æµ‹çš„**u**ä¸ODEæ±‚è§£å™¨è®¡ç®—çš„çœŸå®å€¼çš„æ¯”è¾ƒã€‚
- en: Nevertheless, training the PINN is not our end goal. Instead, we are more interested
    in estimating the unknowns embedded in our target ODEs. Letâ€™s start with the parameter
    estimation. The following figure depicts the evolution of *a* and *b*.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè®­ç»ƒPINNå¹¶ä¸æ˜¯æˆ‘ä»¬çš„æœ€ç»ˆç›®æ ‡ã€‚ç›¸åï¼Œæˆ‘ä»¬æ›´æ„Ÿå…´è¶£çš„æ˜¯ä¼°è®¡æˆ‘ä»¬ç›®æ ‡ODEä¸­åµŒå…¥çš„æœªçŸ¥æ•°ã€‚è®©æˆ‘ä»¬ä»å‚æ•°ä¼°è®¡å¼€å§‹ã€‚ä¸‹å›¾æç»˜äº† *a* å’Œ *b* çš„æ¼”å˜ã€‚
- en: '![](../Images/3592d8cbb99c73826e321f9a816bc1c0.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3592d8cbb99c73826e321f9a816bc1c0.png)'
- en: Figure 9\. The unknown parameters a and b quickly moved away from the specified
    initial values and converged to their true values. This demonstrates that the
    adopted PINN strategy is capable of performing parameter estimation for ODE systems.
    (Image by this blog author)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9\. æœªçŸ¥å‚æ•°aå’Œbè¿…é€Ÿè„±ç¦»äº†æŒ‡å®šçš„åˆå§‹å€¼ï¼Œå¹¶æ”¶æ•›åˆ°å®ƒä»¬çš„çœŸå®å€¼ã€‚è¿™è¡¨æ˜æ‰€é‡‡ç”¨çš„PINNç­–ç•¥èƒ½å¤Ÿå¯¹ODEç³»ç»Ÿè¿›è¡Œå‚æ•°ä¼°è®¡ã€‚ï¼ˆå›¾ç‰‡ç”±æœ¬åšå®¢ä½œè€…æä¾›ï¼‰
- en: We can clearly see that as the training proceeds, the values of *a* and *b*
    quickly converge to their respective true values. This indicated the effectiveness
    of our PINN strategy for parameter estimation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œ*a*å’Œ*b*çš„å€¼è¿…é€Ÿæ”¶æ•›åˆ°å„è‡ªçš„çœŸå®å€¼ã€‚è¿™è¡¨æ˜æˆ‘ä»¬çš„PINNç­–ç•¥åœ¨å‚æ•°ä¼°è®¡æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚
- en: 'In addition to the unknown parameters, we have also obtained the estimates
    of unknown functions *f*â‚ and *f*â‚‚, thanks to the trained auxiliary *f*-net. To
    examine the approximation accuracy of the *f*â‚ and *f*â‚‚, we can compare them to
    the calculated derivative of d*u*â‚/dt and d*u*â‚‚/dt, as shown in the code below:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æœªçŸ¥å‚æ•°å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡è®­ç»ƒå¥½çš„è¾…åŠ©*f*-ç½‘ç»œè·å¾—äº†æœªçŸ¥å‡½æ•°*f*â‚å’Œ*f*â‚‚çš„ä¼°è®¡å€¼ã€‚ä¸ºäº†æ£€æŸ¥*f*â‚å’Œ*f*â‚‚çš„è¿‘ä¼¼ç²¾åº¦ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬ä¸è®¡ç®—å¾—åˆ°çš„d*u*â‚/dtå’Œd*u*â‚‚/dtè¿›è¡Œæ¯”è¾ƒï¼Œå¦‚ä¸‹ä»£ç æ‰€ç¤ºï¼š
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can see in the following figure that the *f*-net predictions fully fulfill
    the governing ODEs, which is in agreement with the previous observations that
    the ODE residuals are very small.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸‹å›¾ä¸­æˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œ*f*-ç½‘ç»œçš„é¢„æµ‹å®Œå…¨ç¬¦åˆæ§åˆ¶ODEï¼Œè¿™ä¸ä¹‹å‰è§‚å¯Ÿåˆ°çš„ODEæ®‹å·®éå¸¸å°çš„æƒ…å†µä¸€è‡´ã€‚
- en: '![](../Images/9fc0758840eec2ad6f74ee4b6b634bfd.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fc0758840eec2ad6f74ee4b6b634bfd.png)'
- en: Figure 10\. Comparison between the calculated derivatives and predicted **f**
    function values.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10\. è®¡ç®—å‡ºçš„å¯¼æ•°ä¸é¢„æµ‹çš„**f**å‡½æ•°å€¼çš„æ¯”è¾ƒã€‚
- en: 'Although we can accurately approximate the unknown functions *f*â‚ and *f*â‚‚
    with an *f*-net, at the end of the day, *f*-net is a **black-box** neural network
    model. Naturally, we would like to ask: what is the exact functional form of these
    estimated functions? The answer could provide us with a deeper understanding of
    the underlying physical process, and help us generalize the results to other similar
    problems.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»¬å¯ä»¥ç”¨*f*-ç½‘ç»œå‡†ç¡®åœ°é€¼è¿‘æœªçŸ¥å‡½æ•°*f*â‚å’Œ*f*â‚‚ï¼Œä½†å½’æ ¹ç»“åº•ï¼Œ*f*-ç½‘ç»œæ˜¯ä¸€ä¸ª**é»‘ç®±**ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚è‡ªç„¶åœ°ï¼Œæˆ‘ä»¬ä¼šæƒ³é—®ï¼šè¿™äº›ä¼°è®¡å‡½æ•°çš„ç¡®åˆ‡åŠŸèƒ½å½¢å¼æ˜¯ä»€ä¹ˆï¼Ÿè¿™ä¸ªç­”æ¡ˆå¯ä»¥ä¸ºæˆ‘ä»¬æä¾›å¯¹æ½œåœ¨ç‰©ç†è¿‡ç¨‹çš„æ›´æ·±å…¥ç†è§£ï¼Œå¹¶å¸®åŠ©æˆ‘ä»¬å°†ç»“æœæ¨å¹¿åˆ°å…¶ä»–ç±»ä¼¼çš„é—®é¢˜ã€‚
- en: So, how can we extract these precise functional forms from our trained neural
    network model? We will look into that in the next section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•ä»è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­æå–è¿™äº›ç²¾ç¡®çš„åŠŸèƒ½å½¢å¼å‘¢ï¼Ÿæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­æ¢è®¨è¿™ä¸ªé—®é¢˜ã€‚
- en: 5\. Symbolic Regression
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. ç¬¦å·å›å½’
- en: 'Symbolic regression is a powerful supervised machine learning technique that
    can be used to discover the underlying mathematical formula that best fits a given
    dataset. This technique, as the name suggests, comprises two key components: **symbolic**
    and **regression**:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¦å·å›å½’æ˜¯ä¸€ç§å¼ºå¤§çš„ç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œå¯ä»¥ç”¨æ¥å‘ç°æœ€é€‚åˆç»™å®šæ•°æ®é›†çš„æ½œåœ¨æ•°å­¦å…¬å¼ã€‚æ­£å¦‚å…¶åç§°æ‰€ç¤ºï¼Œè¿™é¡¹æŠ€æœ¯åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š**ç¬¦å·**å’Œ**å›å½’**ï¼š
- en: '*Symbolic* refers to the use of symbolic expressions to model the input-output
    relationship, e.g., â€œ+â€ for addition, â€œ-â€ for subtraction, â€œcosâ€ for cosine function,
    etc. Instead of fitting a predefined model (e.g., polynomial model, etc.), symbolic
    regression methods search through an entire space of potential symbolic expressions
    to find the best fit.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç¬¦å·*æŒ‡çš„æ˜¯ä½¿ç”¨ç¬¦å·è¡¨è¾¾å¼æ¥å»ºæ¨¡è¾“å…¥è¾“å‡ºå…³ç³»ï¼Œä¾‹å¦‚ï¼Œâ€œ+â€è¡¨ç¤ºåŠ æ³•ï¼Œâ€œ-â€è¡¨ç¤ºå‡æ³•ï¼Œâ€œcosâ€è¡¨ç¤ºä½™å¼¦å‡½æ•°ç­‰ã€‚ç¬¦å·å›å½’æ–¹æ³•ä¸æ˜¯æ‹Ÿåˆé¢„å®šä¹‰æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œå¤šé¡¹å¼æ¨¡å‹ç­‰ï¼‰ï¼Œè€Œæ˜¯é€šè¿‡æ•´ä¸ªæ½œåœ¨ç¬¦å·è¡¨è¾¾å¼çš„ç©ºé—´è¿›è¡Œæœç´¢ï¼Œä»¥æ‰¾åˆ°æœ€ä½³æ‹Ÿåˆã€‚'
- en: '*Regression* refers to the process of creating a model to predict an output
    variable based on the input variables, thus capturing the underlying relationship
    between them. Although the term â€œregressionâ€ may invoke thoughts of linear regression,
    in the context of symbolic regression, it is not restricted to any specific model
    forms but can take a wide array of mathematical operators and structures.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å›å½’*æŒ‡çš„æ˜¯åˆ›å»ºä¸€ä¸ªæ¨¡å‹ä»¥é¢„æµ‹è¾“å‡ºå˜é‡çš„è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹åŸºäºè¾“å…¥å˜é‡ï¼Œä»è€Œæ•æ‰å®ƒä»¬ä¹‹é—´çš„æ½œåœ¨å…³ç³»ã€‚å°½ç®¡â€œå›å½’â€ä¸€è¯å¯èƒ½ä¼šè®©äººè”æƒ³åˆ°çº¿æ€§å›å½’ï¼Œä½†åœ¨ç¬¦å·å›å½’çš„èƒŒæ™¯ä¸‹ï¼Œå®ƒå¹¶ä¸å±€é™äºä»»ä½•ç‰¹å®šçš„æ¨¡å‹å½¢å¼ï¼Œè€Œæ˜¯å¯ä»¥é‡‡ç”¨å„ç§æ•°å­¦è¿ç®—ç¬¦å’Œç»“æ„ã€‚'
- en: In this section, we will implement the symbolic regression technique to distill
    the learned *f*-net into interpretable and compact mathematical expressions, which
    aligns with the strategy proposed by Zhang et al. in their original paper. We
    will begin by introducing the library PySR, which we will use for symbolic regression.
    Subsequently, we will apply this library to our problem and discuss the choice
    of hyperparameters. Finally, we analyze the obtained results.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å®ç°ç¬¦å·å›å½’æŠ€æœ¯ï¼Œå°†å­¦ä¹ åˆ°çš„ *f*-ç½‘ç»œæç‚¼æˆå¯è§£é‡Šä¸”ç´§å‡‘çš„æ•°å­¦è¡¨è¾¾å¼ï¼Œè¿™ä¸å¼ ç­‰äººåœ¨ä»–ä»¬çš„åŸå§‹è®ºæ–‡ä¸­æå‡ºçš„ç­–ç•¥ä¸€è‡´ã€‚æˆ‘ä»¬å°†é¦–å…ˆä»‹ç»å°†ç”¨äºç¬¦å·å›å½’çš„åº“PySRã€‚éšåï¼Œæˆ‘ä»¬å°†åº”ç”¨è¿™ä¸ªåº“è§£å†³æˆ‘ä»¬çš„è¯¾é¢˜ï¼Œå¹¶è®¨è®ºè¶…å‚æ•°çš„é€‰æ‹©ã€‚æœ€åï¼Œæˆ‘ä»¬å°†åˆ†æè·å¾—çš„ç»“æœã€‚
- en: 5.1 PySR library
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 PySRåº“
- en: '[PySR](https://astroautomata.com/PySR/) is an open-source Python library designed
    for practical, high-performance, scientific symbolic regression. It uses advanced
    *evolutionary* optimization algorithms to search through the space of simple analytic
    expressions for accurate and interpretable models, such that the prediction error
    and model complexity are jointly minimized.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[PySR](https://astroautomata.com/PySR/) æ˜¯ä¸€ä¸ªå¼€æºPythonåº“ï¼Œæ—¨åœ¨æä¾›å®ç”¨çš„é«˜æ€§èƒ½ç§‘å­¦ç¬¦å·å›å½’ã€‚å®ƒä½¿ç”¨å…ˆè¿›çš„
    *evolutionary* ä¼˜åŒ–ç®—æ³•åœ¨ç®€å•è§£æè¡¨è¾¾å¼çš„ç©ºé—´ä¸­æœç´¢ï¼Œä»¥è·å¾—å‡†ç¡®ä¸”å¯è§£é‡Šçš„æ¨¡å‹ï¼Œä»è€Œå°†é¢„æµ‹è¯¯å·®å’Œæ¨¡å‹å¤æ‚åº¦å…±åŒæœ€å°åŒ–ã€‚'
- en: Although PySR exposes a simple Python frontend API that resembles the style
    of `scikit-learn`, its backend is written in pure-Julia under the library named
    *SymbolicRegression.jl*. This gives the user the flexibility of customizing operators
    and optimization loss functions while enjoying high computation performance. For
    more details on the working principles of PySR, please refer to [this paper](https://arxiv.org/abs/2305.01582).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡PySRæš´éœ²äº†ä¸€ä¸ªç±»ä¼¼äº `scikit-learn` é£æ ¼çš„ç®€å•Pythonå‰ç«¯APIï¼Œä½†å…¶åå°æ˜¯ç”¨çº¯Juliaç¼–å†™çš„ï¼Œåº“åä¸º *SymbolicRegression.jl*ã€‚è¿™ä¸ºç”¨æˆ·æä¾›äº†å®šåˆ¶æ“ä½œç¬¦å’Œä¼˜åŒ–æŸå¤±å‡½æ•°çš„çµæ´»æ€§ï¼ŒåŒæ—¶äº«æœ‰é«˜è®¡ç®—æ€§èƒ½ã€‚æœ‰å…³PySRå·¥ä½œåŸç†çš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è§[è¿™ç¯‡è®ºæ–‡](https://arxiv.org/abs/2305.01582)ã€‚
- en: To get started with PySR, you would need to [install Julia](https://julialang.org/downloads/)
    first. Afterward, run
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹ä½¿ç”¨PySRï¼Œä½ éœ€è¦é¦–å…ˆ[å®‰è£…Julia](https://julialang.org/downloads/)ã€‚ç„¶åè¿è¡Œ
- en: '[PRE12]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Then install Julia dependencies via
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åé€šè¿‡
- en: '[PRE13]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: or from within IPython, call
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…åœ¨IPythonä¸­è°ƒç”¨
- en: '[PRE14]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: PySR can also be installed via conda or docker. Please check the [installation
    page](https://astroautomata.com/PySR/) for more details.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: PySRä¹Ÿå¯ä»¥é€šè¿‡condaæˆ–dockerå®‰è£…ã€‚è¯·æŸ¥çœ‹[å®‰è£…é¡µé¢](https://astroautomata.com/PySR/)ä»¥è·å–æ›´å¤šç»†èŠ‚ã€‚
- en: 5.2 Implementation
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 å®æ–½
- en: 'Next, we apply the PySR library to distill the learned *f*-net into interpretable
    and compact mathematical expressions. To begin with, we need to generate the dataset
    for symbolic regression learning:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åº”ç”¨PySRåº“å°†å­¦ä¹ åˆ°çš„ *f*-ç½‘ç»œæç‚¼æˆå¯è§£é‡Šä¸”ç´§å‡‘çš„æ•°å­¦è¡¨è¾¾å¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆç¬¦å·å›å½’å­¦ä¹ çš„æ•°æ®é›†ï¼š
- en: '[PRE15]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that for our current problem, the inputs for the symbolic regression learning
    are *t*, *u*â‚, *u*â‚‚, and *u*â‚ƒ, and the outputs are *f*â‚ and *f*â‚‚. This is because,
    in our target ODEs, we assume *f*â‚=*f*â‚(*t*, *u*â‚, *u*â‚‚, *u*â‚ƒ) and *f*â‚‚=*f*â‚‚(*t*,
    *u*â‚, *u*â‚‚, *u*â‚ƒ). We saved the generated dataframe (see figure below) for later
    usage.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¯¹äºæˆ‘ä»¬å½“å‰çš„é—®é¢˜ï¼Œç¬¦å·å›å½’å­¦ä¹ çš„è¾“å…¥æ˜¯ *t*ã€*u*â‚ã€*u*â‚‚ å’Œ *u*â‚ƒï¼Œè¾“å‡ºæ˜¯ *f*â‚ å’Œ *f*â‚‚ã€‚è¿™æ˜¯å› ä¸ºåœ¨æˆ‘ä»¬çš„ç›®æ ‡ODEä¸­ï¼Œæˆ‘ä»¬å‡è®¾
    *f*â‚=*f*â‚(*t*ã€*u*â‚ã€*u*â‚‚ã€*u*â‚ƒ) å’Œ *f*â‚‚=*f*â‚‚(*t*ã€*u*â‚ã€*u*â‚‚ã€*u*â‚ƒ)ã€‚æˆ‘ä»¬ä¿å­˜äº†ç”Ÿæˆçš„æ•°æ®æ¡†ï¼ˆè§ä¸‹å›¾ï¼‰ä»¥å¤‡åç”¨ã€‚
- en: '![](../Images/1b0a3ff76c4ad0f3a7ddfc87cfd11ef4.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b0a3ff76c4ad0f3a7ddfc87cfd11ef4.png)'
- en: Figure 11\. The generated dataframe for symbolic regression learning. (Image
    by this blog author)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾11\. ç”Ÿæˆçš„ç¬¦å·å›å½’å­¦ä¹ æ•°æ®æ¡†ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼šæœ¬åšå®¢ä½œè€…ï¼‰
- en: After generating the dataset, we are ready to perform symbolic regression with
    PySR. Note that it is recommended to run the PySR code in terminals instead of
    in Jupyter Notebook. Although PySR provides support for Jupyter Notebook, the
    printing (e.g., search progress, current best results, etc.) is much nicer in
    a terminal environment.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ•°æ®é›†åï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨PySRè¿›è¡Œç¬¦å·å›å½’äº†ã€‚è¯·æ³¨æ„ï¼Œå»ºè®®åœ¨ç»ˆç«¯ä¸­è¿è¡ŒPySRä»£ç ï¼Œè€Œä¸æ˜¯åœ¨Jupyter Notebookä¸­ã€‚å°½ç®¡PySRæ”¯æŒJupyter
    Notebookï¼Œä½†åœ¨ç»ˆç«¯ç¯å¢ƒä¸­çš„æ‰“å°ï¼ˆä¾‹å¦‚ï¼Œæœç´¢è¿›åº¦ã€å½“å‰æœ€ä½³ç»“æœç­‰ï¼‰æ•ˆæœè¦æ›´å¥½ã€‚
- en: 'Following the `scikit-learn` style, we start by defining a model object:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰ç…§ `scikit-learn` é£æ ¼ï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªæ¨¡å‹å¯¹è±¡ï¼š
- en: '[PRE16]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here is the break-down of the specified hyperparameters:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æŒ‡å®šè¶…å‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼š
- en: '`niterations`: Number of iterations of the algorithm to run. Generally, a larger
    iteration number yields better results, at the cost of higher computational cost.
    However, since PySR allows terminating the search job early, a good practice is
    to simply set `niterations` to some very large value and keep the optimization
    going. Once the identified equation looks satisfactory, the job can be stopped
    early.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`niterations`ï¼šç®—æ³•è¿è¡Œçš„è¿­ä»£æ¬¡æ•°ã€‚é€šå¸¸ï¼Œè¾ƒå¤§çš„è¿­ä»£æ¬¡æ•°ä¼šäº§ç”Ÿæ›´å¥½çš„ç»“æœï¼Œä½†ä»£ä»·æ˜¯æ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œç”±äº PySR å…è®¸æå‰ç»ˆæ­¢æœç´¢ä»»åŠ¡ï¼Œå¥½çš„åšæ³•æ˜¯å°†
    `niterations` è®¾ç½®ä¸ºä¸€ä¸ªéå¸¸å¤§çš„å€¼å¹¶ä¿æŒä¼˜åŒ–è¿›è¡Œã€‚ä¸€æ—¦è¯†åˆ«å‡ºçš„æ–¹ç¨‹çœ‹èµ·æ¥ä»¤äººæ»¡æ„ï¼Œå°±å¯ä»¥æå‰åœæ­¢ä»»åŠ¡ã€‚'
- en: '`binary_operators`: List of strings for binary operators used in the search.
    The built-in binary operators supported by PySR include `+`, `-`, `*`, `/`, `^`,
    `greater`, `mod`, `logical_or`, `logical_and`.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_operators`ï¼šç”¨äºæœç´¢çš„äºŒå…ƒè¿ç®—ç¬¦å­—ç¬¦ä¸²åˆ—è¡¨ã€‚PySR æ”¯æŒçš„å†…ç½®äºŒå…ƒè¿ç®—ç¬¦åŒ…æ‹¬ `+`ã€`-`ã€`*`ã€`/`ã€`^`ã€`greater`ã€`mod`ã€`logical_or`ã€`logical_and`ã€‚'
- en: '`unary_operators`: List of unary operators used in the search. Note that unary
    operators only take a single scalar as input. The built-in ones include `neg`,
    `square`, `cube`, `exp`, `abs`, `log`, `log10`, `log2`, `log1p`, `sqrt`, `sin`,
    `cos`, `tan`, `sinh`, `cosh`, `tanh`, `atan`, `asinh`, `acosh`, `atanh_clip` (=atanh((x+1)%2
    - 1)), `erf`, `erfc`, `gamma`, `relu`, `round`, `floor`, `ceil`, `round`, `sign`.
    Note that to supply a custom operator, we need to pass in â€œmyfunction(x) = â€¦â€
    to the operator list, like what we did with â€œinv(x) = 1/xâ€.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unary_operators`ï¼šç”¨äºæœç´¢çš„ä¸€å…ƒè¿ç®—ç¬¦åˆ—è¡¨ã€‚æ³¨æ„ï¼Œä¸€å…ƒè¿ç®—ç¬¦åªæ¥å—å•ä¸ªæ ‡é‡ä½œä¸ºè¾“å…¥ã€‚å†…ç½®çš„ä¸€å…ƒè¿ç®—ç¬¦åŒ…æ‹¬ `neg`ã€`square`ã€`cube`ã€`exp`ã€`abs`ã€`log`ã€`log10`ã€`log2`ã€`log1p`ã€`sqrt`ã€`sin`ã€`cos`ã€`tan`ã€`sinh`ã€`cosh`ã€`tanh`ã€`atan`ã€`asinh`ã€`acosh`ã€`atanh_clip`ï¼ˆ=atanh((x+1)%2
    - 1)ï¼‰ã€`erf`ã€`erfc`ã€`gamma`ã€`relu`ã€`round`ã€`floor`ã€`ceil`ã€`round`ã€`sign`ã€‚æ³¨æ„ï¼Œè¦æä¾›è‡ªå®šä¹‰è¿ç®—ç¬¦ï¼Œæˆ‘ä»¬éœ€è¦å°†â€œmyfunction(x)
    = â€¦â€ä¼ é€’ç»™è¿ç®—ç¬¦åˆ—è¡¨ï¼Œå°±åƒæˆ‘ä»¬ç”¨â€œinv(x) = 1/xâ€åšçš„é‚£æ ·ã€‚'
- en: '`extra_sympy_mappings`: Provides mappings between custom `binary_operators`
    or `unary_operators` defined in julia strings, to those same operators defined
    in [sympy](https://www.sympy.org/en/index.html). This is useful when exporting
    the results.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extra_sympy_mappings`ï¼šæä¾›è‡ªå®šä¹‰çš„ `binary_operators` æˆ– `unary_operators` åœ¨ julia
    å­—ç¬¦ä¸²ä¸­ä¸ [sympy](https://www.sympy.org/en/index.html) ä¸­ç›¸åŒè¿ç®—ç¬¦çš„æ˜ å°„ã€‚è¿™åœ¨å¯¼å‡ºç»“æœæ—¶éå¸¸æœ‰ç”¨ã€‚'
- en: '`loss`: String of Julia code specifying an elementwise loss function (as defined
    in LossFunctions.jl). Commonly used losses include `L1DistLoss()`(the absolute
    distance loss), `L2DistLoss()`(the least squares loss), `HuberLoss()`(the Huber
    loss function used for robustness to outliers). The loss function specifies the
    optimization target for the symbolic regression search.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼šæŒ‡å®šå…ƒç´ çº§æŸå¤±å‡½æ•°çš„ Julia ä»£ç å­—ç¬¦ä¸²ï¼ˆå¦‚åœ¨ LossFunctions.jl ä¸­å®šä¹‰ï¼‰ã€‚å¸¸ç”¨çš„æŸå¤±åŒ…æ‹¬ `L1DistLoss()`ï¼ˆç»å¯¹è·ç¦»æŸå¤±ï¼‰ã€`L2DistLoss()`ï¼ˆæœ€å°äºŒä¹˜æŸå¤±ï¼‰ã€`HuberLoss()`ï¼ˆç”¨äºæŠ—ç¦»ç¾¤å€¼çš„
    Huber æŸå¤±å‡½æ•°ï¼‰ã€‚æŸå¤±å‡½æ•°æŒ‡å®šäº†ç¬¦å·å›å½’æœç´¢çš„ä¼˜åŒ–ç›®æ ‡ã€‚'
- en: '`model_selection`: Model selection criterion when selecting a final expression
    from the list of best expressions at each complexity. `score` means that the candidate
    model will be selected based on the highest score, which is defined as -Î”log(loss)/Î”C,
    where C refers to the complexity of the expression and Î” denotes local change.
    Therefore, if an expression has a much better loss at a slightly higher complexity,
    it is preferred.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_selection`ï¼šä»æ¯ä¸ªå¤æ‚åº¦çš„æœ€ä½³è¡¨è¾¾å¼åˆ—è¡¨ä¸­é€‰æ‹©æœ€ç»ˆè¡¨è¾¾å¼çš„æ ‡å‡†ã€‚`score` æ„å‘³ç€å€™é€‰æ¨¡å‹å°†æ ¹æ®æœ€é«˜å¾—åˆ†è¿›è¡Œé€‰æ‹©ï¼Œå¾—åˆ†å®šä¹‰ä¸º
    -Î”log(loss)/Î”Cï¼Œå…¶ä¸­ C ä»£è¡¨è¡¨è¾¾å¼çš„å¤æ‚åº¦ï¼ŒÎ” è¡¨ç¤ºå±€éƒ¨å˜åŒ–ã€‚å› æ­¤ï¼Œå¦‚æœä¸€ä¸ªè¡¨è¾¾å¼åœ¨ç¨é«˜çš„å¤æ‚åº¦ä¸‹å…·æœ‰æ›´å¥½çš„æŸå¤±ï¼Œåˆ™æ›´å—é’çã€‚'
- en: '`complexity_of_operators`: By default, all operators have a complexity of 1\.
    To change the default complexity setting and give preference to different operators,
    we can supply a dictionary with the key being the operator string and the value
    being its corresponding complexity level. In our current case, we set all unary
    operators to have a complexity level of 3, which was also adopted in the original
    paper of Zhang et al.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`complexity_of_operators`ï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰è¿ç®—ç¬¦çš„å¤æ‚åº¦ä¸º 1ã€‚è¦æ›´æ”¹é»˜è®¤å¤æ‚åº¦è®¾ç½®å¹¶ä¼˜å…ˆè€ƒè™‘ä¸åŒçš„è¿ç®—ç¬¦ï¼Œæˆ‘ä»¬å¯ä»¥æä¾›ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºè¿ç®—ç¬¦å­—ç¬¦ä¸²ï¼Œå€¼ä¸ºå…¶å¯¹åº”çš„å¤æ‚åº¦çº§åˆ«ã€‚åœ¨æˆ‘ä»¬å½“å‰çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰ä¸€å…ƒè¿ç®—ç¬¦çš„å¤æ‚åº¦çº§åˆ«è®¾ç½®ä¸º
    3ï¼Œè¿™ä¹Ÿåœ¨ Zhang ç­‰äººçš„åŸå§‹è®ºæ–‡ä¸­é‡‡ç”¨ã€‚'
- en: It is worth mentioning that `PySRRegressor` exposes many other hyperparameters
    for setting up the algorithm, data preprocessing, stopping criteria, performance
    and parallelization, monitoring, environment, and results exporting. For the complete
    list of options for controlling the symbolic regression search, please check out
    the [PySRRegressor Reference page](https://astroautomata.com/PySR/api/#pysrregressor-parameters).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—ä¸€æçš„æ˜¯ï¼Œ`PySRRegressor` æä¾›äº†è®¸å¤šå…¶ä»–è¶…å‚æ•°ï¼Œç”¨äºè®¾ç½®ç®—æ³•ã€æ•°æ®é¢„å¤„ç†ã€åœæ­¢æ ‡å‡†ã€æ€§èƒ½å’Œå¹¶è¡ŒåŒ–ã€ç›‘æ§ã€ç¯å¢ƒå’Œç»“æœå¯¼å‡ºã€‚æœ‰å…³æ§åˆ¶ç¬¦å·å›å½’æœç´¢çš„æ‰€æœ‰é€‰é¡¹çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·æŸ¥çœ‹
    [PySRRegressor å‚è€ƒé¡µé¢](https://astroautomata.com/PySR/api/#pysrregressor-parameters)ã€‚
- en: 5.3 Identification results
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 è¯†åˆ«ç»“æœ
- en: 'After specifying the model object, we can kick off the fitting process with
    three lines of code (for distilling the analytical forms of *f*â‚):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŒ‡å®šæ¨¡å‹å¯¹è±¡åï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸‰è¡Œä»£ç å¯åŠ¨æ‹Ÿåˆè¿‡ç¨‹ï¼ˆç”¨äºæç‚¼ *f*â‚ çš„è§£æå½¢å¼ï¼‰ï¼š
- en: '[PRE17]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: While the script is running, you should be able to see the progress bar and
    the current best equations, as shown in the figure below. Note that x0, x1, x2,
    and x3 correspond to *t*, *u*â‚, *u*â‚‚, and *u*â‚ƒ, respectively.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è„šæœ¬è¿è¡Œæ—¶ï¼Œä½ åº”è¯¥èƒ½å¤Ÿçœ‹åˆ°è¿›åº¦æ¡å’Œå½“å‰æœ€ä½³æ–¹ç¨‹ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚æ³¨æ„ x0ã€x1ã€x2 å’Œ x3 åˆ†åˆ«å¯¹åº” *t*ã€*u*â‚ã€*u*â‚‚ å’Œ *u*â‚ƒã€‚
- en: '![](../Images/3f2c28ee599036a30c260a4ebed307a3.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f2c28ee599036a30c260a4ebed307a3.png)'
- en: 'Once the optimization job is finished, a list of candidate equations will appear
    in the terminal:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä¼˜åŒ–ä»»åŠ¡å®Œæˆï¼Œç»ˆç«¯ä¸­å°†å‡ºç°å€™é€‰æ–¹ç¨‹åˆ—è¡¨ï¼š
- en: '![](../Images/1edf4cea529a1bb841e2301629d11c77.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1edf4cea529a1bb841e2301629d11c77.png)'
- en: 'If we rank the equations based on their **score values**, we can see the top-3
    equations are:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ ¹æ® **è¯„åˆ†å€¼** å¯¹æ–¹ç¨‹è¿›è¡Œæ’åï¼Œå¯ä»¥çœ‹åˆ°æ’åå‰ä¸‰çš„æ–¹ç¨‹æ˜¯ï¼š
- en: '*u*â‚‚ *u*â‚ƒ *exp*( -0.1053391 *t* )'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*u*â‚‚ *u*â‚ƒ *exp*( -0.1053391 *t* )'
- en: 0.60341805 *u*â‚‚ *u*â‚ƒ
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0.60341805 *u*â‚‚ *u*â‚ƒ
- en: '*u*â‚‚ *u*â‚ƒ'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*u*â‚‚ *u*â‚ƒ'
- en: Recall that our true ODE is
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å›å¿†ä¸€ä¸‹æˆ‘ä»¬çœŸå®çš„ ODE æ˜¯
- en: '![](../Images/7ee90229681ba8c80eac60db10d85852.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ee90229681ba8c80eac60db10d85852.png)'
- en: It is quite impressive to see that the PySR has accurately identified the essential
    inputs (i.e., it recognized that *u*â‚ does not play a role in *f*â‚) and discovered
    an analytical expression (top-1 result) that is fairly close to the true expression
    of *f*â‚.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼ŒPySR å‡†ç¡®åœ°è¯†åˆ«å‡ºäº†åŸºæœ¬è¾“å…¥ï¼ˆå³ï¼Œå®ƒè¯†åˆ«å‡º *u*â‚ åœ¨ *f*â‚ ä¸­ä¸èµ·ä½œç”¨ï¼‰ï¼Œå¹¶å‘ç°äº†ä¸€ä¸ªæ¥è¿‘ *f*â‚ çœŸå®è¡¨è¾¾å¼çš„è§£æè¡¨è¾¾å¼ï¼ˆæ’åç¬¬ä¸€çš„ç»“æœï¼‰ã€‚
- en: 'We replicate the same analysis to *f*â‚‚. The optimization results are shown
    in the figure below:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹ *f*â‚‚ è¿›è¡Œäº†ç›¸åŒçš„åˆ†æã€‚ä¼˜åŒ–ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![](../Images/7e935b2f1899d6929c72c6abaf0b6b43.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e935b2f1899d6929c72c6abaf0b6b43.png)'
- en: This time, we notice that the true expression of *f*â‚‚, i.e., *f*â‚‚=*u*â‚*u*â‚ƒ,
    only appears as the second-best (in terms of score) equation. However, note that
    the best one, i.e., *u*â‚ƒ, has a score that is only marginally higher than the
    second-best one. On the other hand, the loss value of *u*â‚*u*â‚ƒ is one magnitude
    lower than using *u*â‚ƒ alone. These observations indicate that in practice, we
    would need domain knowledge/experience to make an informed decision regarding
    whether the incurred complexity for high accuracy is worth pursuing.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¬¡ï¼Œæˆ‘ä»¬æ³¨æ„åˆ° *f*â‚‚ çš„çœŸå®è¡¨è¾¾å¼ï¼Œå³ *f*â‚‚=*u*â‚ *u*â‚ƒï¼Œä»…ä½œä¸ºç¬¬äºŒå¥½çš„ï¼ˆæŒ‰è¯„åˆ†è®¡ç®—ï¼‰æ–¹ç¨‹å‡ºç°ã€‚ç„¶è€Œï¼Œè¯·æ³¨æ„ï¼Œæœ€ä½³æ–¹ç¨‹ï¼Œå³ *u*â‚ƒï¼Œå…¶å¾—åˆ†ä»…æ¯”ç¬¬äºŒå¥½çš„é«˜ä¸€ç‚¹ã€‚å¦ä¸€æ–¹é¢ï¼Œ*u*â‚
    *u*â‚ƒ çš„æŸå¤±å€¼æ¯”å•ç‹¬ä½¿ç”¨ *u*â‚ƒ ä½ä¸€ä¸ªæ•°é‡çº§ã€‚è¿™äº›è§‚å¯Ÿç»“æœè¡¨æ˜ï¼Œåœ¨å®é™…æ“ä½œä¸­ï¼Œæˆ‘ä»¬éœ€è¦é¢†åŸŸçŸ¥è¯†/ç»éªŒæ¥åšå‡ºæ˜æ™ºçš„å†³å®šï¼Œä»¥åˆ¤æ–­è¿½æ±‚é«˜å‡†ç¡®åº¦æ‰€å¸¦æ¥çš„å¤æ‚æ€§æ˜¯å¦å€¼å¾—ã€‚
- en: 6\. Takeaways
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6. å…³é”®è¦ç‚¹
- en: 'In this blog post, we investigated the problem of discovering differential
    equations from observational data. We followed the strategy proposed by Zhang
    et al., implemented it in code, and applied it to address a case study. Here are
    the key take-aways:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä»è§‚æµ‹æ•°æ®ä¸­å‘ç°å¾®åˆ†æ–¹ç¨‹çš„é—®é¢˜ã€‚æˆ‘ä»¬éµå¾ªäº† Zhang ç­‰äººæå‡ºçš„ç­–ç•¥ï¼Œå°†å…¶å®ç°ä¸ºä»£ç ï¼Œå¹¶åº”ç”¨äºä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ã€‚ä»¥ä¸‹æ˜¯å…³é”®è¦ç‚¹ï¼š
- en: 1ï¸âƒ£ Physics-informed neural network (PINN) is a versatile tool for performing
    system identifications, particularly in scenarios where only partial information
    is known about the governing differential equations. By assimilating observational
    data and available physical knowledge, PINN can effectively estimate not only
    the unknown parameters but also unknown functions, if we adopt the trick of parameterizing
    the unknown functions with auxiliary neural networks, which can be jointly trained
    with the main PINN. All these factors contribute to a substantial advantage over
    traditional system identification methods.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (PINN) æ˜¯ä¸€ä¸ªå¤šç”¨é€”çš„å·¥å…·ï¼Œç”¨äºè¿›è¡Œç³»ç»Ÿè¯†åˆ«ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹æ§åˆ¶å¾®åˆ†æ–¹ç¨‹åªæœ‰éƒ¨åˆ†ä¿¡æ¯å·²çŸ¥çš„æƒ…å†µä¸‹ã€‚é€šè¿‡åŒåŒ–è§‚å¯Ÿæ•°æ®å’Œç°æœ‰çš„ç‰©ç†çŸ¥è¯†ï¼ŒPINNä¸ä»…èƒ½æœ‰æ•ˆä¼°è®¡æœªçŸ¥å‚æ•°ï¼Œè¿˜èƒ½ä¼°è®¡æœªçŸ¥å‡½æ•°ï¼Œå¦‚æœæˆ‘ä»¬é‡‡ç”¨ç”¨è¾…åŠ©ç¥ç»ç½‘ç»œå¯¹æœªçŸ¥å‡½æ•°è¿›è¡Œå‚æ•°åŒ–çš„æŠ€å·§ï¼Œå¹¶ä¸ä¸»PINNä¸€èµ·è”åˆè®­ç»ƒã€‚è¿™äº›å› ç´ å…±åŒä½œç”¨ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ç³»ç»Ÿè¯†åˆ«æ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚
- en: 2ï¸âƒ£ Symbolic regression is a powerful tool in opening the black box of the learned
    neural networks. By searching through an entire space of symbolic expressions
    with advanced evolutionary algorithms, symbolic regression is able to extract
    interpretable and compact analytical expressions that can accurately describe
    the hidden input-output relationship. This knowledge-distillation process is greatly
    appreciated in practice as it can effectively enhance our understanding of the
    underlying system dynamics.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ç¬¦å·å›å½’æ˜¯ä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œç”¨äºæ­å¼€å­¦ä¹ ç¥ç»ç½‘ç»œçš„é»‘ç®±ã€‚é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„è¿›åŒ–ç®—æ³•åœ¨æ•´ä¸ªç¬¦å·è¡¨è¾¾å¼ç©ºé—´ä¸­è¿›è¡Œæœç´¢ï¼Œç¬¦å·å›å½’èƒ½å¤Ÿæå–å‡ºå¯è§£é‡Šä¸”ç´§å‡‘çš„è§£æè¡¨è¾¾å¼ï¼Œè¿™äº›è¡¨è¾¾å¼å¯ä»¥å‡†ç¡®æè¿°éšè—çš„è¾“å…¥è¾“å‡ºå…³ç³»ã€‚è¿™ä¸ªçŸ¥è¯†è’¸é¦è¿‡ç¨‹åœ¨å®è·µä¸­å—åˆ°é«˜åº¦èµèµï¼Œå› ä¸ºå®ƒèƒ½æœ‰æ•ˆå¢å¼ºæˆ‘ä»¬å¯¹åŸºç¡€ç³»ç»ŸåŠ¨æ€çš„ç†è§£ã€‚
- en: 'Before we conclude this blog, there are a couple of points worth considering
    when applying PINN+symbolic regression for practical problems:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ç»“æŸè¿™ç¯‡åšå®¢ä¹‹å‰ï¼Œæœ‰å‡ ç‚¹åœ¨å°†PINN+ç¬¦å·å›å½’åº”ç”¨äºå®é™…é—®é¢˜æ—¶å€¼å¾—è€ƒè™‘ï¼š
- en: 1ï¸âƒ£ Uncertainty quantification (UQ)
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ ä¸ç¡®å®šæ€§é‡åŒ– (UQ)
- en: Throughout this blog, we have operated under the assumption that our observed
    data for *u*â‚, *u*â‚‚, and *u*â‚ƒ is noise-free. However, this assumption is generally
    not true, as the observational data can easily be contaminated by noise for practical
    dynamical systems. Consequently, both the *accuracy* and *reliability* of our
    system identification results will suffer. Therefore, a crucial aspect to consider
    is the quantification of uncertainty within our system identification workflow.
    Techniques such as Bayesian Neural Networks and [Monte Carlo simulation](https://medium.com/towards-data-science/how-to-quantify-the-prediction-error-made-by-my-model-db4705910173)
    could properly account for noise in the observed data and provide an estimation
    of the confidence interval for the predictions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬å‡è®¾æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„ *u*â‚ã€*u*â‚‚ å’Œ *u*â‚ƒ æ•°æ®æ˜¯æ— å™ªå£°çš„ã€‚ç„¶è€Œï¼Œè¿™ç§å‡è®¾é€šå¸¸ä¸æˆç«‹ï¼Œå› ä¸ºå®é™…çš„åŠ¨æ€ç³»ç»Ÿä¸­çš„è§‚å¯Ÿæ•°æ®å¾ˆå®¹æ˜“è¢«å™ªå£°æ±¡æŸ“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç³»ç»Ÿè¯†åˆ«ç»“æœçš„
    *å‡†ç¡®æ€§* å’Œ *å¯é æ€§* éƒ½ä¼šå—åˆ°å½±å“ã€‚å› æ­¤ï¼Œä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯è€ƒè™‘åœ¨æˆ‘ä»¬çš„ç³»ç»Ÿè¯†åˆ«å·¥ä½œæµä¸­è¿›è¡Œä¸ç¡®å®šæ€§é‡åŒ–ã€‚åƒè´å¶æ–¯ç¥ç»ç½‘ç»œå’Œ [è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ](https://medium.com/towards-data-science/how-to-quantify-the-prediction-error-made-by-my-model-db4705910173)
    è¿™æ ·çš„æŠ€æœ¯å¯ä»¥åˆç†åœ°è€ƒè™‘è§‚å¯Ÿæ•°æ®ä¸­çš„å™ªå£°ï¼Œå¹¶æä¾›å¯¹é¢„æµ‹çš„ç½®ä¿¡åŒºé—´çš„ä¼°è®¡ã€‚
- en: 2ï¸âƒ£ Sensitivity of symbolic regression
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ç¬¦å·å›å½’çš„æ•æ„Ÿæ€§
- en: 'Generally speaking, the results yielded by symbolic regression may be sensitive
    to the employed loss function, supplied candidates of unary and binary operators,
    as well as the defined complexities of the operators. For example, in my attempt
    to reproduce the results published by Zhang et al., I was unable to obtain the
    exact top-3 equations for *f*â‚‚ as shown in the original paper, although I have
    adopted the exact same settings (to my best knowledge). Several factors may contribute
    to this mismatch: first of all, the evolutionary optimization techniques are intrinsically
    stochastic, therefore results can vary across different runs. Secondly, it is
    likely that the PINN trained in the first stage is different, therefore the resultant
    dataset (i.e., *t*, *u*â‚, *u*â‚‚, *u*â‚ƒ â†’ *f*â‚, *f*â‚‚) is also different, which in
    turn impacts the symbolic regression outcome.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œç¬¦å·å›å½’å¾—åˆ°çš„ç»“æœå¯èƒ½å¯¹æ‰€ä½¿ç”¨çš„æŸå¤±å‡½æ•°ã€æä¾›çš„å•ä¸€å’ŒäºŒå…ƒè¿ç®—ç¬¦å€™é€‰é¡¹ä»¥åŠå®šä¹‰çš„è¿ç®—ç¬¦å¤æ‚åº¦æ•æ„Ÿã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ‘å°è¯•é‡ç° Zhang ç­‰äººå‘å¸ƒçš„ç»“æœæ—¶ï¼Œå°½ç®¡æˆ‘é‡‡ç”¨äº†å®Œå…¨ç›¸åŒçš„è®¾ç½®ï¼ˆæ®æˆ‘æ‰€çŸ¥ï¼‰ï¼Œä½†æˆ‘æœªèƒ½è·å¾—ä¸åŸå§‹è®ºæ–‡ä¸­æ‰€ç¤ºçš„
    *f*â‚‚ å®Œå…¨ä¸€è‡´çš„å‰ 3 ä¸ªæ–¹ç¨‹ã€‚è¿™ç§ä¸åŒ¹é…å¯èƒ½æœ‰å‡ ä¸ªå› ç´ ï¼šé¦–å…ˆï¼Œè¿›åŒ–ä¼˜åŒ–æŠ€æœ¯æœ¬è´¨ä¸Šæ˜¯éšæœºçš„ï¼Œå› æ­¤ç»“æœå¯èƒ½åœ¨ä¸åŒçš„è¿è¡Œä¸­æœ‰æ‰€ä¸åŒã€‚å…¶æ¬¡ï¼Œç¬¬ä¸€é˜¶æ®µè®­ç»ƒçš„ PINN
    å¯èƒ½ä¸åŒï¼Œå› æ­¤ç”Ÿæˆçš„æ•°æ®é›†ï¼ˆå³ *t*ï¼Œ*u*â‚ï¼Œ*u*â‚‚ï¼Œ*u*â‚ƒ â†’ *f*â‚ï¼Œ*f*â‚‚ï¼‰ä¹Ÿä¸åŒï¼Œä»è€Œå½±å“äº†ç¬¦å·å›å½’çš„ç»“æœã€‚
- en: Overall, these observations suggested that symbolic regression outcomes should
    not be accepted blindly. Instead, it's crucial to rely on the domain understanding/knowledge
    to critically assess the plausibility of the identified equations.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œè¿™äº›è§‚å¯Ÿç»“æœè¡¨æ˜ï¼Œç¬¦å·å›å½’çš„ç»“æœä¸åº”ç›²ç›®æ¥å—ã€‚ç›¸åï¼Œä¾èµ–é¢†åŸŸçŸ¥è¯†/ç†è§£æ¥æ‰¹åˆ¤æ€§åœ°è¯„ä¼°è¯†åˆ«å‡ºçš„æ–¹ç¨‹çš„åˆç†æ€§è‡³å…³é‡è¦ã€‚
- en: If you find my content useful, you could buy me a coffee [here](https://www.buymeacoffee.com/Shuaiguo09f)
    ğŸ¤— Thank you very much for your support!
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è§‰å¾—æˆ‘çš„å†…å®¹æœ‰ç”¨ï¼Œå¯ä»¥åœ¨[è¿™é‡Œ](https://www.buymeacoffee.com/Shuaiguo09f)è¯·æˆ‘å–å’–å•¡ğŸ¤— éå¸¸æ„Ÿè°¢ä½ çš„æ”¯æŒï¼
- en: You can find the companion notebook and script with full code [here](https://github.com/ShuaiGuo16/PINN_symbolic_regression)
    *ğŸ’»*
  id: totrans-204
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/ShuaiGuo16/PINN_symbolic_regression)æ‰¾åˆ°å¸¦æœ‰å®Œæ•´ä»£ç çš„ä¼´éšç¬”è®°æœ¬å’Œè„šæœ¬*ğŸ’»*ã€‚
- en: ''
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To learn the best practices of physics-informed neural networks: [Unraveling
    the Design Pattern of Physics-Informed Neural Networks](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)'
  id: totrans-206
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¦å­¦ä¹ ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œçš„æœ€ä½³å®è·µï¼Œè¯·å‚é˜…ï¼š[è§£å¼€ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œè®¾è®¡æ¨¡å¼çš„å¥¥ç§˜](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)ã€‚
- en: ''
  id: totrans-207
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To learn more about physics-informed operator learning: [Operator Learning
    via Physics-Informed DeepONet](/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)'
  id: totrans-208
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šå…³äºç‰©ç†ä¿¡æ¯è¿ç®—ç¬¦å­¦ä¹ çš„å†…å®¹ï¼Œè¯·å‚é˜…ï¼š[é€šè¿‡ç‰©ç†ä¿¡æ¯æ·±åº¦è¿ç®—ç¬¦å­¦ä¹ ](/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)ã€‚
- en: ''
  id: totrans-209
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Feel free to subscribe to my [newsletter](https://shuaiguo.medium.com/subscribe)
    or follow me on [Medium](https://shuaiguo.medium.com/).
  id: totrans-210
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: éšæ—¶å¯ä»¥è®¢é˜…æˆ‘çš„[æ–°é—»é€šè®¯](https://shuaiguo.medium.com/subscribe)æˆ–åœ¨[Medium](https://shuaiguo.medium.com/)ä¸Šå…³æ³¨æˆ‘ã€‚
- en: Reference
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: '[1] Zhang et al., Discovering a reaction-diffusion model for Alzheimerâ€™s disease
    by combining PINNs with symbolic regression. arXiv, 2023.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Zhang ç­‰ï¼Œç»“åˆ PINN ä¸ç¬¦å·å›å½’å‘ç°é˜¿å°”èŒ¨æµ·é»˜ç—…çš„ååº”æ‰©æ•£æ¨¡å‹ã€‚arXivï¼Œ2023ã€‚'
- en: '[2] Cranmer et al., Interpretable Machine Learning for Science with PySR and
    SymbolicRegression.jl. arXiv, 2023.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Cranmer ç­‰ï¼Œä½¿ç”¨ PySR å’Œ SymbolicRegression.jl è¿›è¡Œå¯è§£é‡Šçš„æœºå™¨å­¦ä¹ ç§‘å­¦ç ”ç©¶ã€‚arXivï¼Œ2023ã€‚'
