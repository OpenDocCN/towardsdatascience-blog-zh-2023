- en: Discovering Differential Equations with Physics-Informed Neural Networks and
    Symbolic Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过物理信息神经网络和符号回归发现微分方程
- en: 原文：[https://towardsdatascience.com/discovering-differential-equations-with-physics-informed-neural-networks-and-symbolic-regression-c28d279c0b4d](https://towardsdatascience.com/discovering-differential-equations-with-physics-informed-neural-networks-and-symbolic-regression-c28d279c0b4d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/discovering-differential-equations-with-physics-informed-neural-networks-and-symbolic-regression-c28d279c0b4d](https://towardsdatascience.com/discovering-differential-equations-with-physics-informed-neural-networks-and-symbolic-regression-c28d279c0b4d)
- en: A case study with step-by-step code implementation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个逐步代码实现的案例研究
- en: '[](https://shuaiguo.medium.com/?source=post_page-----c28d279c0b4d--------------------------------)[![Shuai
    Guo](../Images/d673c066f8006079be5bf92757e73a59.png)](https://shuaiguo.medium.com/?source=post_page-----c28d279c0b4d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c28d279c0b4d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c28d279c0b4d--------------------------------)
    [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----c28d279c0b4d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shuaiguo.medium.com/?source=post_page-----c28d279c0b4d--------------------------------)[![Shuai
    Guo](../Images/d673c066f8006079be5bf92757e73a59.png)](https://shuaiguo.medium.com/?source=post_page-----c28d279c0b4d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c28d279c0b4d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c28d279c0b4d--------------------------------)
    [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----c28d279c0b4d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c28d279c0b4d--------------------------------)
    ·25 min read·Jul 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c28d279c0b4d--------------------------------)
    ·阅读时间25分钟·2023年7月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/feb76e9bafbe81909b3d783c4332a1eb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/feb76e9bafbe81909b3d783c4332a1eb.png)'
- en: Photo by [Steven Coffey](https://unsplash.com/@steeeve?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Steven Coffey](https://unsplash.com/@steeeve?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Differential equations serve as a powerful framework to capture and understand
    the dynamic behaviors of physical systems. By describing how variables change
    in relation to each other, they provide insights into system dynamics and allow
    us to make predictions about the system’s future behavior.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微分方程作为一个强大的框架，用于捕捉和理解物理系统的动态行为。通过描述变量之间如何变化，它们提供了对系统动态的见解，并允许我们对系统未来的行为进行预测。
- en: 'However, a common challenge we face in many real-world systems is that their
    governing differential equations are often only *partially known*, withthe unknown
    aspects manifesting in several ways:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们在许多实际系统中面临的一个共同挑战是，它们的控制微分方程通常仅*部分已知*，未知的方面以几种方式表现出来：
- en: The **parameters** of the differential equation are unknown. A case in point
    is wind engineering, where the governing equations of fluid dynamics are well-established,
    but the coefficients relating to turbulent flow are highly uncertain.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微分方程的**参数**是未知的。例如在风工程中，流体动力学的控制方程已被很好地建立，但与湍流流动相关的系数非常不确定。
- en: The **functional forms** of the differential equations are unknown. For instance,
    in chemical engineering, the exact functional form of the rate equations may not
    be fully understood due to the uncertainties in rate-determining steps and reaction
    pathways.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微分方程的**函数形式**是未知的。例如，在化学工程中，由于速率决定步骤和反应途径的不确定性，速率方程的确切函数形式可能没有完全理解。
- en: Both **functional forms** and **parameters** are unknown. A prime example is
    battery state modeling, where the commonly used equivalent circuit model only
    partially captures the current-voltage relationship (the functional form of the
    missing physics is therefore unknown). Moreover, the model itself contains unknown
    parameters (i.e., resistance and capacitance values).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**函数形式**和**参数**都是未知的。一个典型的例子是电池状态建模，其中常用的等效电路模型仅部分捕捉了电流-电压关系（因此缺失物理的函数形式是未知的）。此外，模型本身包含未知的参数（即电阻和电容值）。'
- en: '![](../Images/0dc9da76d5f88c55a5a1ea99dd496e18.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0dc9da76d5f88c55a5a1ea99dd496e18.png)'
- en: Figure 1\. The governing equations of many real-world dynamical systems are
    only partially known. (Image by this blog author)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 许多实际动态系统的控制方程仅部分已知。（图片由本博客作者提供）
- en: Such partial knowledge of the governing differential equations hinders our understanding
    and control of these dynamical systems. Consequently, inferring these unknown
    components based on observed data becomes a crucial task in dynamical system modeling.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对主方程的这种部分了解阻碍了我们对这些动力系统的理解和控制。因此，根据观察数据推断这些未知组件成为动力系统建模中的关键任务。
- en: Broadly speaking, this process of using observational data to recover governing
    equations of dynamical systems falls in the domain of **system identification**.
    Once discovered, we can readily use these equations to predict future states of
    the system, inform control strategies for the systems, or enable theoretical investigations
    using analytical techniques.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 广义而言，使用观察数据恢复动力系统的主方程的过程属于**系统识别**的范畴。一旦发现这些方程，我们可以轻松地利用这些方程预测系统的未来状态，告知系统的控制策略，或通过分析技术进行理论研究。
- en: Very recently, [Zhang et al.](https://arxiv.org/abs/2307.08107)(2023) proposed
    a promising strategy that leverages **physics-informed neural networks** (PINN)
    and **symbolic regression** to discover unknowns in a system of ordinary differential
    equations (ODEs). While their focus was on discovering differential equations
    for Alzheimer’s disease modeling, their proposed solution holds promise for general
    dynamical systems.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，[Zhang et al.](https://arxiv.org/abs/2307.08107)(2023)提出了一种有前景的策略，该策略利用**物理信息神经网络**（PINN）和**符号回归**来发现常微分方程（ODEs）系统中的未知量。虽然他们的重点是发现用于阿尔茨海默病建模的微分方程，但他们提出的解决方案对一般动力系统也具有潜力。
- en: In this blog post, we will take a closer look at the concepts put forth by the
    authors and get hands-on to reproduce one of the case studies investigated in
    the paper. Toward that end, we will build a PINN from scratch, leverage the [PySR
    library](https://github.com/MilesCranmer/PySR) to perform symbolic regression,
    and discuss the obtained results.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们将更深入地了解作者提出的概念，并动手重现论文中的一个案例研究。为此，我们将从零开始构建一个PINN，利用[PySR库](https://github.com/MilesCranmer/PySR)进行符号回归，并讨论获得的结果。
- en: 'If you are interested in learning best practices in physics-informed neural
    networks, feel free to check out my blog series here:'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你对物理信息神经网络的最佳实践感兴趣，欢迎查看我的博客系列：
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Physics-Informed Neural Networks: An Application-Centric Guide](/physics-informed-neural-networks-an-application-centric-guide-dc1013526b02)'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[物理信息神经网络：以应用为中心的指南](/physics-informed-neural-networks-an-application-centric-guide-dc1013526b02)'
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Unraveling the Design Pattern of Physics-Informed Neural Networks](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527).'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[揭示物理信息神经网络的设计模式](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)。'
- en: With that in mind, let’s get started!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这一点，让我们开始吧！
- en: '**Table of Content**'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**目录**'
- en: '**·** [**1\. Case Study**](#83d2) **·** [**2\. Why do traditional approaches
    fall short?**](#1f2c) **·** [**3\. PINN for System Identification (Theory)**](#bdd1)
    **·** [**4\. PINN for System Identification (Code)**](#1f5f)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**·** [**1\. 案例研究**](#83d2) **·** [**2\. 为什么传统方法不够有效？**](#1f2c) **·** [**3\.
    PINN在系统识别中的应用（理论）**](#bdd1) **·** [**4\. PINN在系统识别中的应用（代码）**](#1f5f)'
- en: ∘ [4.1 Define the Architecture](#29d6)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [4.1 定义架构](#29d6)
- en: ∘ [4.2 Define ODE loss](#7d83)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [4.2 定义ODE损失](#7d83)
- en: ∘ [4.3 Define gradient descent step](#2fcd)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [4.3 定义梯度下降步骤](#2fcd)
- en: ∘ [4.4 Data preparation](#741c)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [4.4 数据准备](#741c)
- en: ∘ [4.5 PINN Training](#83c2)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [4.5 PINN训练](#83c2)
- en: '**·** [**5\. Symbolic Regression**](#fef3)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**·** [**5\. 符号回归**](#fef3)'
- en: ∘ [5.1 PySR library](#0a5b)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [5.1 PySR库](#0a5b)
- en: ∘ [5.2 Implementation](#e2f3)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [5.2 实施](#e2f3)
- en: ∘ [5.3 Identification results](#fa91)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [5.3 识别结果](#fa91)
- en: ·[**6\. Take-away**](#889f)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ·[**6\. 总结**](#889f)
- en: · [Reference](#1485)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: · [参考文献](#1485)
- en: 1\. Case Study
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 案例研究
- en: 'Let’s start by introducing the problem we aim to solve. In this blog, we will
    reproduce the first case study investigated in [Zhang et al](https://arxiv.org/abs/2307.08107).’s
    original paper, i.e., discovering the Kraichnan-Orszag system from data. The system
    is described by the following ODEs:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始介绍我们旨在解决的问题。在这篇博客中，我们将重现[Zhang et al](https://arxiv.org/abs/2307.08107)原始论文中的第一个案例研究，即从数据中发现Kraichnan-Orszag系统。该系统由以下ODEs描述：
- en: '![](../Images/4160891a7e1d7f249bdbcd7d9e79508f.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4160891a7e1d7f249bdbcd7d9e79508f.png)'
- en: with an initial condition of *u*₁(0)=1, *u*₂(0)=0.8, *u*₃(0)=0.5\. The Kraichnan-Orszag
    system is commonly used in turbulence studies and fluid dynamics research, where
    the goal is to develop theoretical insights into turbulence, its structures, and
    its dynamics.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 具有初始条件 *u*₁(0)=1，*u*₂(0)=0.8，*u*₃(0)=0.5。Kraichnan-Orszag 系统通常用于湍流研究和流体动力学研究，其目标是对湍流及其结构和动态发展理论见解。
- en: 'To mimic a typical system identification setup, we assume we only know partially
    about the governing ODEs. Specifically, we assume that we don’t know anything
    about the differential equations for *u*₁ and *u*₂. In addition, we assume we
    only know that the right-hand side of the differential equation for *u*₃ is a
    linear transformation of *u*₁ and *u*₂. Then, we can rewrite the ODE system as
    follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟一个典型的系统识别设置，我们假设我们对控制常微分方程的了解仅限于部分已知。具体来说，我们假设我们对 *u*₁ 和 *u*₂ 的微分方程一无所知。此外，我们假设我们只知道
    *u*₃ 的微分方程右侧是 *u*₁ 和 *u*₂ 的线性变换。然后，我们可以将常微分方程系统重写如下：
- en: '![](../Images/03c35582b0c5f142b22932c1dae697e0.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03c35582b0c5f142b22932c1dae697e0.png)'
- en: where *f*₁ and *f*₂ denote the unknown functions, and *a* and *b* are the unknown
    parameters. **Our objective is to calibrate the values of *a* and *b*, as well
    as estimate the analytical functional form of *f*₁ and *f*₂**. Essentially, we
    are dealing with a challenging system identification problem where both unknown
    parameters and function forms exist.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *f*₁ 和 *f*₂ 代表未知函数，*a* 和 *b* 是未知参数。**我们的目标是校准 *a* 和 *b* 的值，并估计 *f*₁ 和 *f*₂
    的解析函数形式**。 本质上，我们正面临一个具有未知参数和函数形式的复杂系统识别问题。
- en: 2\. Why do traditional approaches fall short?
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 为什么传统方法会失败？
- en: In the traditional paradigm of system identification, we typically employ numerical
    methods (e.g., Euler’s method, Runge-Kutta methods, etc.) to simulate and predict
    system states *u*₁, *u*₂, and *u*₃. However, those methods are fundamentally limited
    in that they generally require a complete form of governing differential equations,
    and are incapable of handling scenarios when the differential equations are only
    partially known.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的系统识别范式中，我们通常使用数值方法（例如，欧拉法、龙格-库塔法等）来模拟和预测系统状态 *u*₁、*u*₂ 和 *u*₃。 然而，这些方法从根本上受限，因为它们通常需要完整的控制微分方程形式，并且无法处理微分方程仅部分已知的情况。
- en: In cases where the parameters of the equations are unknown, traditional methods
    often resort to optimization techniques, where an initial guess for the parameters
    is made, and then refined in an iterative process to minimize the difference between
    the observed data and the data predicted by the numerical solver. Since each optimization
    iteration necessitates one run of the numerical solver, this approach, while feasible,
    can be computationally very expensive.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程参数未知的情况下，传统方法通常诉诸于优化技术，其中对参数进行初步猜测，然后通过迭代过程来优化，以最小化观察数据与数值求解器预测数据之间的差异。由于每次优化迭代都需要运行一次数值求解器，这种方法虽然可行，但计算开销可能非常大。
- en: 'Note that the above discussion only describes the case of calibrating the unknown
    parameters. The problem becomes even more complex when we need to estimate unknown
    functions in differential equations. Theoretically, we can adopt a similar methodology,
    i.e., making assumptions about the forms of the unknown functions before optimization.
    However, issues would immediately rise if we go down this path: If we assume an
    overly simple form, we run into the risk of **underfitting**, which may lead to
    substantial prediction errors. On the other hand, if we assume an overly complex
    form (e.g., with many tunable parameters), we run into the risk of **overfitting**,
    which may lead to poor generalization performance.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述讨论仅描述了校准未知参数的情况。当我们需要估计微分方程中的未知函数时，问题变得更加复杂。从理论上讲，我们可以采用类似的方法，即在优化之前对未知函数的形式做出假设。然而，如果我们走这条路，会立即出现问题：如果我们假设一个过于简单的形式，我们面临**欠拟合**的风险，这可能导致较大的预测误差。另一方面，如果我们假设一个过于复杂的形式（例如，具有许多可调参数），我们面临**过拟合**的风险，这可能导致较差的泛化性能。
- en: 'In summary, the traditional approach faces significant challenges when dealing
    with partially known differential equations:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，传统方法在处理部分已知微分方程时面临重大挑战：
- en: 1️⃣ Traditional numerical methods rely on having a complete form of governing
    differential equations to run simulations.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 传统数值方法依赖于具有完整控制微分方程的形式来进行模拟。
- en: 2️⃣ Combining traditional numerical methods with optimization algorithms can
    address parameter estimation problems, but often at a high computational cost.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 将传统数值方法与优化算法结合可以解决参数估计问题，但通常代价很高。
- en: 3️⃣ For estimating unknown functions embedded in differential equations, traditional
    approaches may yield results that are highly sensitive to the assumed functional
    form, which creates risks of underfitting or overfitting.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 对于嵌入微分方程中的未知函数进行估计时，传统方法可能会得到对假设函数形式高度敏感的结果，这会导致欠拟合或过拟合的风险。
- en: Given these challenges, traditional approaches often fall short in addressing
    system identification problems where unknown parameters and functional forms coexist.
    This naturally leads us to the topic of physics-informed neural networks (PINNs).
    In the next section, we will see how PINN can effectively address the challenges
    faced by traditional approaches.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些挑战，传统方法在处理未知参数和函数形式共存的系统识别问题时往往效果不佳。这自然引出了物理信息神经网络（PINNs）的话题。在下一节中，我们将看到PINN如何有效地解决传统方法面临的挑战。
- en: '**3\. PINN for System Identification (Theory)**'
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**3\. PINN在系统识别中的应用（理论）**'
- en: The physics-informed neural network (or PINN in short) is a powerful concept
    proposed by [Raissi et al.](https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125)
    back in 2019\. The basic idea of PINN, like other *physics-informed machine learning*
    techniques, is to create a hybrid model where both the observational data and
    the known physical knowledge (represented as differential equations) are leveraged
    in model training. PINN was originally designed as an efficient ODE/PDE solver.
    However, researchers soon recognized that PINNs have (arguably) even greater potential
    in tackling inverse, system identification problems.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 物理信息神经网络（简称PINN）是[Raissi等人](https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125)在2019年提出的一个强大概念。PINN的基本思想，像其他*物理信息机器学习*技术一样，是创建一个混合模型，其中在模型训练中利用了观察数据和已知的物理知识（以微分方程形式表示）。PINN最初被设计为一个高效的ODE/PDE求解器。然而，研究人员很快认识到PINN在解决逆问题和系统识别问题上（可以说）具有更大的潜力。
- en: In the following, we will explain how PINNs can be leveraged to overcome the
    challenges we discussed in the previous section, one by one.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们将逐一解释如何利用PINN克服我们在上一节讨论的挑战。
- en: 1️⃣ Traditional numerical methods rely on having a complete form of governing
    differential equations to run simulations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 传统数值方法依赖于拥有完整形式的主控微分方程来进行模拟。
- en: '📣**PINN’s response**: Unlike traditional methods, I am capable of working with
    partially known differential equations, thus not confined by a complete equation
    to run simulations.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 📣**PINN的响应**：与传统方法不同，我能够处理部分已知的微分方程，因此不受完整方程的限制来进行模拟。
- en: From an exterior perspective, PINN just resembles a conventional neural network
    model that takes the temporal/spatial coordinates (e.g., *t*, *x*, *y*) as input
    and outputs the target quantities (e.g., velocity *u*, pressure *p*, temperature
    *T*, etc.) we are trying to simulate. However, what sets PINNs apart from conventional
    NNs is that in PINN, the differential equations are used as constraints during
    the training process. Specifically, PINN introduces an extra loss term that accounts
    for the residuals of the governing differential equations, which is calculated
    by supplying the predicted quantities into the governing equations. By optimizing
    this loss term, we effectively make the trained network aware of the underlying
    physics.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从外部角度看，PINN仅仅类似于一个传统的神经网络模型，该模型将时间/空间坐标（例如，*t*，*x*，*y*）作为输入，并输出我们试图模拟的目标量（例如，速度*u*，压力*p*，温度*T*等）。然而，使PINN与传统NN不同的是，在PINN中，微分方程作为训练过程中的约束。具体来说，PINN引入了一个额外的损失项，用于计算主控微分方程的残差，该残差通过将预测量代入主控方程计算得到。通过优化这个损失项，我们有效地使训练后的网络意识到潜在的物理规律。
- en: '![](../Images/ea45326f2cd4ae4f4d2a7a2054350c86.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea45326f2cd4ae4f4d2a7a2054350c86.png)'
- en: Figure 2\. Physics-informed neural networks incorporate differential equations
    into the loss function, therefore effectively making the trained network aware
    of the underlying physics. (Image by this blog author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. 物理信息神经网络将微分方程纳入损失函数中，因此有效地使训练后的网络意识到潜在的物理规律。（图像由本博客作者提供）
- en: Since the differential equations are solely used in constructing the loss function,
    they have no impact on the PINN model architecture. This essentially means that
    we do not need to have complete knowledge of the differential equations for training.
    Even if we only know part of the equation, this knowledge can still be incorporated
    to enforce the output to obey the known physics. This flexibility of accommodating
    varying degrees of knowledge completeness presents a significant advantage over
    traditional numerical approaches.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于微分方程仅用于构建损失函数，因此它们对PINN模型结构没有影响。这实际上意味着我们在训练时不需要对微分方程有完全的了解。即使我们只知道方程的一部分，这些知识仍然可以被纳入以强制输出遵循已知的物理规律。这种适应知识完整度不同的灵活性相比传统数值方法具有显著优势。
- en: 2️⃣ Combining traditional numerical methods with optimization algorithms can
    address parameter estimation problems, but often at a high computational cost.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 结合传统数值方法与优化算法可以解决参数估计问题，但通常代价较高。
- en: '📣**PINN’s response**: I can provide a computationally efficient alternative
    for estimating unknown parameters.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 📣**PINN的回应**：我可以提供一种计算上高效的替代方案来估计未知参数。
- en: Unlike traditional approaches that treat parameter estimation as a separate
    optimization task, PINNs seamlessly integrate this process into the model training
    stage. Specifically, in PINNs, the unknown parameters are simply treated as additional
    trainable parameters, which are optimized along with the other neural network
    weights and biases during training.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与将参数估计视为单独优化任务的传统方法不同，PINNs将这一过程无缝地集成到模型训练阶段。在PINNs中，未知参数被简单地视为额外的可训练参数，这些参数在训练过程中与其他神经网络的权重和偏差一起优化。
- en: '![](../Images/e87cddadb163478ef75654173c1e5060.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e87cddadb163478ef75654173c1e5060.png)'
- en: Figure 3\. Unknown parameters are optimized jointly with the weights and biases
    of PINN. At the end of the training, the final values of *a* and *b* we obtained
    constitute the estimates of the unknown parameters. (Image by this blog author)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 未知参数与PINN的权重和偏差一起优化。在训练结束时，我们得到的最终值 *a* 和 *b* 作为未知参数的估计值。（图片由本博客作者提供）
- en: In addition, PINNs fully leverage the modern deep learning framework to perform
    training. This allows for rapid computation of the gradients (i.e., via automatic
    differentiation) needed for advanced optimization algorithms (e.g., Adam), therefore
    greatly accelerating the parameter estimation process, especially for problems
    with a high-dimensional parameter space. All these factors make PINNs a competitive
    alternative for parameter estimation problems.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，PINNs充分利用现代深度学习框架来执行训练。这允许快速计算所需的梯度（即通过自动微分），以用于高级优化算法（例如Adam），从而大大加速了参数估计过程，尤其是对于高维参数空间的问题。这些因素使得PINNs成为参数估计问题的一个有竞争力的替代方案。
- en: 3️⃣ For estimating unknown functions embedded in differential equations, traditional
    approaches may yield results that are highly sensitive to the assumed functional
    form, which creates risks of underfitting or overfitting.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 对于嵌入微分方程中的未知函数，传统方法可能会得到对假设函数形式高度敏感的结果，这会产生欠拟合或过拟合的风险。
- en: '📣**PINN’s response**: The unknown functions can be effectively parameterized
    by additional neural networks, which can be trained jointly with me, just like
    the previous parameter estimation scenario.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 📣**PINN的回应**：未知函数可以通过额外的神经网络有效地参数化，这些神经网络可以与我一起训练，就像之前的参数估计场景一样。
- en: Instead of assuming the form of unknown functions, we can approximate the unknown
    functions with separate neural networks, and later integrate them into the main
    PINN model. Just as in the previous parameter estimation scenario, here, we can
    view those extra neural nets as an extensive set of unknown parameters to be estimated.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用独立的神经网络来逼近未知函数，然后将它们集成到主PINN模型中。就像在之前的参数估计场景中一样，我们可以将这些额外的神经网络视为需要估计的大量未知参数。
- en: '![](../Images/44e745359b4f6cee0d0649b3b12bb55e.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44e745359b4f6cee0d0649b3b12bb55e.png)'
- en: Figure 4\. The unknown functions can be parameterized by a separate neural network
    and trained jointly with the original PINN. The ODE/PDE residual loss term regularizes
    the auxiliary neural network such that the governing equations are satisfied.
    In this way, the auxiliary neural network can automatically learn the optimal
    functional forms directly from the data. (Image by this blog author)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图4。未知函数可以通过一个独立的神经网络进行参数化，并与原始PINN一起训练。ODE/PDE残差损失项对辅助神经网络进行正则化，以满足控制方程。这样，辅助神经网络可以直接从数据中自动学习最佳的函数形式。（图像来源于本博客作者）
- en: During training, the weights and biases of those auxiliary neural nets will
    be trained simultaneously with the original PINN to minimize the loss function
    (data loss + ODE residual loss). In doing so, those auxiliary neural networks
    can learn the optimal functional forms directly from the data. By removing the
    need to make risky assumptions about the functional form, this strategy helps
    alleviate the problems of underfitting and overfitting.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，这些辅助神经网络的权重和偏差将与原始PINN同时训练，以最小化损失函数（数据损失 + ODE残差损失）。通过这种方式，这些辅助神经网络可以直接从数据中学习最佳的函数形式。通过消除对函数形式进行风险假设的需要，这种策略有助于缓解欠拟合和过拟合的问题。
- en: In summary, the power of PINN lies in its ability to work with partially known
    differential equations and efficiently learn unknown parameters and function forms
    directly from the data. This versatility sets them apart from traditional approaches,
    therefore making them an effective tool for system identification tasks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，PINN的优势在于其能够处理部分已知的微分方程，并有效地从数据中学习未知参数和函数形式。这种多功能性使其与传统方法区别开来，因此成为系统识别任务的有效工具。
- en: In the next section, we will start working on our case study and turn theory
    into actual code.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将开始处理我们的案例研究，并将理论转化为实际代码。
- en: 4\. PINN for System Identification (Code)
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. PINN用于系统识别（代码）
- en: 'In this section, we will implement a PINN (in TensorFlow) to address our target
    case study. Let’s start by importing the necessary libraries:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个PINN（在TensorFlow中）来解决我们的目标案例研究。让我们从导入必要的库开始：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 4.1 Define the Architecture
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 定义架构
- en: 'For the main PINN, we use one neural network to predict ***u***, which has
    1-dimensional input (i.e., *t*) and 3-dimensional output (*u*₁, *u*₂, and *u*₃).
    In addition, as discussed in the previous section, we use an auxiliary neural
    network to approximate the unknown functions *f*₁ and *f*₂, which has 4-dimensional
    input (i.e., *t*, *u*₁, *u*₂, and *u*₃) and 2-dimensional output (*f*₁ and *f*₂).
    The architecture of the overall PINN is shown below:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于主要的PINN，我们使用一个神经网络来预测***u***，其具有1维输入（即*t*）和3维输出（*u*₁、*u*₂和*u*₃）。此外，如前一节所讨论的，我们使用一个辅助神经网络来逼近未知函数*f*₁和*f*₂，该网络具有4维输入（即*t*、*u*₁、*u*₂和*u*₃）和2维输出（*f*₁和*f*₂）。整体PINN的架构如下所示：
- en: '![](../Images/335fd393f34cf3ca8a2d48b78cd079de.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/335fd393f34cf3ca8a2d48b78cd079de.png)'
- en: Figure 5\. The architecture of the employed PINN model.(Image by this blog author)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图5。所使用的PINN模型的架构。（图像来源于本博客作者）
- en: One thing worth emphasizing again is that it is necessary to feed the auxiliary
    neural network with all the available features (in our current case, *t*, *u*₁,
    *u*₂, and *u*₃), as we do not know the precise functional forms of *f*₁ and *f*₂.
    During training, the auxiliary neural network will automatically determine which
    features are necessary/important in a data-driven manner.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 值得再次强调的是，需要向辅助神经网络提供所有可用的特征（在我们当前的情况下，*t*、*u*₁、*u*₂和*u*₃），因为我们不知道*f*₁和*f*₂的确切函数形式。在训练过程中，辅助神经网络将以数据驱动的方式自动确定哪些特征是必要的/重要的。
- en: 'First, let’s define the neural network that predicts ***u***. Here, we use
    two hidden layers, each of which is equipped with 50 neurons and hyperbolic tangent
    activation functions:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义一个预测***u***的神经网络。在这里，我们使用两个隐藏层，每个层配备50个神经元和双曲正切激活函数：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we define the auxiliary neural network that predicts ***f***. We adopt
    the same network architecture:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个预测***f***的辅助神经网络。我们采用相同的网络架构：
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the code above, we add *a* and *b* to the collection of the neural network
    model parameters. This way, *a* and *b* can be optimized jointly with the other
    weights and biases of the neural network. We achieved this goal by defining a
    custom layer `ParameterLayer`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们将*a*和*b*添加到神经网络模型参数的集合中。这样，*a*和*b*可以与神经网络的其他权重和偏差一起优化。我们通过定义一个自定义层`ParameterLayer`实现了这一目标：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that this layer does nothing besides introducing the two parameters as
    the model attributes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这一层除了引入这两个参数作为模型属性外没有其他作用。
- en: 'Finally, we put *u*-net and *f*-net together and define the architecture for
    the full PINN:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将 *u*-net 和 *f*-net 结合在一起，定义完整的 PINN 架构：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the code above, we concatenate the input *t* and the *u*-net outputs *u*₁,
    *u*₂, and *u*₃ before feeding them into the *f*-net. Also, we output both ***u***
    and ***f*** in the overall PINN model. Although only ***u*** is needed in practice
    (as ***u*** is our modeling target), the prediction of ***f*** will become useful
    later for distilling its analytical functional forms (see section 5).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们将输入 *t* 和 *u*-net 输出 *u*₁, *u*₂, 和 *u*₃ 进行串联，然后输入到 *f*-net 中。此外，我们在整体
    PINN 模型中输出 ***u*** 和 ***f***。虽然在实际应用中只需要 ***u***（因为 ***u*** 是我们的建模目标），但后续 ***f***
    的预测会变得有用，以提取其分析函数形式（见第 5 节）。
- en: 4.2 Define ODE loss
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 定义 ODE 损失
- en: 'Next, we define the function to compute the ODE residual loss. Recall that
    our target ODEs are:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义计算 ODE 残差损失的函数。回顾一下，我们的目标 ODEs 是：
- en: '![](../Images/b38eab8013ffaefb44572f7be65d94c1.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b38eab8013ffaefb44572f7be65d94c1.png)'
- en: 'Therefore, we can define the function as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以按如下方式定义函数：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Although the code above is mostly self-explanatory, several things are worth
    mentioning:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述代码大部分是自解释的，但有几个问题值得提及：
- en: We used `tf.GradientTape.batch_jacobian()` (instead of the usual `GradientTape.gradient()`)
    to calculate the gradient of *u*₁, *u*₂, and *u*₃ w.r.t *t.* `GradientTape.gradient()`
    won’t work here as it computes the sum d*u*₁/dt + d*u*₂/dt + d*u*₃/dt. Potentially
    we could also use `GradientTape.jacobian()` here to compute the gradient of each
    output value w.r.t each input value. For more details, please refer to the [official
    page](https://www.tensorflow.org/api_docs/python/tf/GradientTape#methods).
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了 `tf.GradientTape.batch_jacobian()`（而不是通常的 `GradientTape.gradient()`）来计算
    *u*₁, *u*₂ 和 *u*₃ 相对于 *t* 的梯度。`GradientTape.gradient()` 在这里不起作用，因为它计算的是 d*u*₁/dt
    + d*u*₂/dt + d*u*₃/dt。我们也可以在这里使用 `GradientTape.jacobian()` 来计算每个输出值相对于每个输入值的梯度。有关更多细节，请参见
    [官方页面](https://www.tensorflow.org/api_docs/python/tf/GradientTape#methods)。
- en: We used `@tf.function` decorator to convert the above Python function into a
    TensorFlow graph. It is useful to do that as gradient calculation can be quite
    expensive and executing it in Graph mode can significantly accelerate the computations.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了 `@tf.function` 装饰器将上述 Python 函数转换为 TensorFlow 图。这是有用的，因为梯度计算可能非常昂贵，使用图模式执行可以显著加速计算。
- en: 4.3 Define gradient descent step
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 定义梯度下降步骤
- en: 'Next, we configure the logic for calculating the gradients of total loss with
    respect to the parameters (network weights and biases, as well as the unknown
    parameters *a* and *b*). This is necessary for performing the gradient descent
    for model training:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们配置了计算总损失相对于参数（网络权重和偏差，以及未知参数 *a* 和 *b*）的梯度的逻辑。这对于执行模型训练的梯度下降是必要的：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the code above:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中：
- en: 'We consider three loss terms: the initial condition loss `IC_loss`, the ODE
    residuals loss`ODE_loss`, and the data loss `data_loss`. The `IC_loss` is calculated
    by comparing the model-predicted ***u***(*t*=0) with the known initial value of
    ***u***,the `ODE_loss` is calculated by calling our previously defined `ODE_residual_calculator`
    function, and the data loss is calculated by simply comparing the model predictions
    (i.e., *u*₁, *u*₂, *u*₃) with their observed values.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们考虑三个损失项：初始条件损失 `IC_loss`、ODE 残差损失 `ODE_loss` 和数据损失 `data_loss`。`IC_loss` 通过将模型预测的***u***(*t*=0)与已知的***u***初始值进行比较来计算，`ODE_loss`
    通过调用我们之前定义的 `ODE_residual_calculator` 函数来计算，而数据损失则是通过将模型预测值（即 *u*₁, *u*₂, *u*₃）与它们的观测值进行简单比较来计算的。
- en: We define the total loss as a weighted sum of `IC_loss`,`ODE_loss`, and`data_loss`.
    Generally, the weights control how much emphasis is given to the individual loss
    terms during the training process. In our case study, it is sufficient to set
    all of them as 1.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将总损失定义为 `IC_loss`、`ODE_loss` 和 `data_loss` 的加权和。通常，权重控制在训练过程中对各个损失项的重视程度。在我们的案例研究中，将它们全部设置为
    1 就足够了。
- en: 4.4 Data preparation
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 数据准备
- en: In this subsection, we discuss how to organize data for PINN model training.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们讨论了如何组织数据以进行 PINN 模型训练。
- en: Recall that our total loss function contains both ODE residual loss and data
    loss. Therefore, we need to generate both collocation points in the time dimension
    (for evaluating ODE loss) and the paired input(*t*)-output(***u***) supervised
    data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下，我们的总损失函数包含ODE残差损失和数据损失。因此，我们需要生成时间维度上的配点（用于评估ODE损失）和配对输入(*t*)-输出(***u***)的监督数据。
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the code above, we allocated 10000 equally-spaced collocation points within
    our target time domain [0, 10]. For facilitating data loss computation, we pre-generated
    the paired input(*t*)-output(***u***) dataset `u_obs`, with its first column being
    the time coordinates, and the remaining three columns representing *u*₁, *u*₂,
    and *u*₃, respectively. `u_obs` contains 1000 data points and is calculated with
    the following code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们在目标时间域[0, 10]内分配了10000个等间距的配点。为了方便数据损失计算，我们预生成了配对输入(*t*)-输出(***u***)数据集`u_obs`，其第一列为时间坐标，其余三列分别表示
    *u*₁、*u*₂ 和 *u*₃。`u_obs`包含1000个数据点，计算方式如下代码：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'where `simulate_ODEs` is the ODE solver that simulates ***u***-trajectory given
    the initial conditions and simulation domain:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `simulate_ODEs` 是ODE求解器，它在给定初始条件和模拟域的情况下模拟***u***轨迹：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The following figure shows the target ***u*** profiles. Note that we have sampled
    1000 equally-spaced (*t* — *u*₁), (*t* — *u*₂), and (*t* — *u*₃) data pairs (contained
    in `u_obs`) as the supervised data for data loss calculation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了目标***u***的轮廓。请注意，我们已经抽取了1000个等间距的 (*t* — *u*₁)、(*t* — *u*₂) 和 (*t* — *u*₃)
    数据对（包含在`u_obs`中），作为数据损失计算的监督数据。
- en: '![](../Images/41569446dc20349a0521acc759aafbfe.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41569446dc20349a0521acc759aafbfe.png)'
- en: Figure 6\. Output profiles of our currently investigated ODEs. (Image by this
    blog author)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 我们当前研究的ODE的输出轮廓。（图像由本博客作者提供）
- en: 4.5 PINN Training
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 PINN 训练
- en: 'The following code defines the main training and validation logic:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了主要的训练和验证逻辑：
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As discussed previously, we set the weights for different loss components as
    1.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如之前讨论的，我们将不同损失组件的权重设置为1。
- en: We set the initial guess for *a* and *b* as -1 and 1, respectively. Recall that
    these values are different from their true values, which are -2 and 0, respectively.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将 *a* 和 *b* 的初始猜测设置为-1和1，分别。回忆一下，这些值与它们的真实值不同，真实值分别为-2和0。
- en: For validation, we add up the ODE residual loss and initial condition loss to
    serve as the final validation loss. Note that we do not consider data loss here
    as we assume we have no access to additional paired *t* — **u** datasets for validation
    purposes. The computed validation loss is used to adapt the learning rate.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了验证，我们将ODE残差损失和初始条件损失相加，作为最终的验证损失。请注意，我们在这里不考虑数据损失，因为我们假设没有额外的配对 *t* — **u**
    数据集用于验证目的。计算出的验证损失用于调整学习率。
- en: The following figure displays the loss convergence curve. We can see that all
    three loss components converged properly, indicating that the training is done
    satisfactorily.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了损失收敛曲线。我们可以看到所有三个损失组件都正确收敛，这表明训练已满意完成。
- en: '![](../Images/dd9788e21fc94bbf65450ede3abd0962.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd9788e21fc94bbf65450ede3abd0962.png)'
- en: Figure 7\. Loss convergence plot. (Image by this blog author)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. 损失收敛图。（图像由本博客作者提供）
- en: The following figure shows the comparison between the predicted ***u***’s and
    the ground truth calculated by the ODE solver. Here, we can also see that the
    PINN is able to accurately solve our target ODEs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了预测的***u***与通过ODE求解器计算的真实值之间的比较。在这里，我们还可以看到PINN能够准确地解决我们的目标ODE。
- en: '![](../Images/4f0a4ec20674c126dda63aa99008a376.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f0a4ec20674c126dda63aa99008a376.png)'
- en: Figure 8\. Comparison of predicted **u**’s and the ground truth computed by
    ODE solver.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 预测的**u**与ODE求解器计算的真实值的比较。
- en: Nevertheless, training the PINN is not our end goal. Instead, we are more interested
    in estimating the unknowns embedded in our target ODEs. Let’s start with the parameter
    estimation. The following figure depicts the evolution of *a* and *b*.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，训练PINN并不是我们的最终目标。相反，我们更感兴趣的是估计我们目标ODE中嵌入的未知数。让我们从参数估计开始。下图描绘了 *a* 和 *b* 的演变。
- en: '![](../Images/3592d8cbb99c73826e321f9a816bc1c0.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3592d8cbb99c73826e321f9a816bc1c0.png)'
- en: Figure 9\. The unknown parameters a and b quickly moved away from the specified
    initial values and converged to their true values. This demonstrates that the
    adopted PINN strategy is capable of performing parameter estimation for ODE systems.
    (Image by this blog author)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. 未知参数a和b迅速脱离了指定的初始值，并收敛到它们的真实值。这表明所采用的PINN策略能够对ODE系统进行参数估计。（图片由本博客作者提供）
- en: We can clearly see that as the training proceeds, the values of *a* and *b*
    quickly converge to their respective true values. This indicated the effectiveness
    of our PINN strategy for parameter estimation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，随着训练的进行，*a*和*b*的值迅速收敛到各自的真实值。这表明我们的PINN策略在参数估计方面是有效的。
- en: 'In addition to the unknown parameters, we have also obtained the estimates
    of unknown functions *f*₁ and *f*₂, thanks to the trained auxiliary *f*-net. To
    examine the approximation accuracy of the *f*₁ and *f*₂, we can compare them to
    the calculated derivative of d*u*₁/dt and d*u*₂/dt, as shown in the code below:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除了未知参数外，我们还通过训练好的辅助*f*-网络获得了未知函数*f*₁和*f*₂的估计值。为了检查*f*₁和*f*₂的近似精度，我们可以将它们与计算得到的d*u*₁/dt和d*u*₂/dt进行比较，如下代码所示：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can see in the following figure that the *f*-net predictions fully fulfill
    the governing ODEs, which is in agreement with the previous observations that
    the ODE residuals are very small.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从下图中我们可以清楚地看到，*f*-网络的预测完全符合控制ODE，这与之前观察到的ODE残差非常小的情况一致。
- en: '![](../Images/9fc0758840eec2ad6f74ee4b6b634bfd.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fc0758840eec2ad6f74ee4b6b634bfd.png)'
- en: Figure 10\. Comparison between the calculated derivatives and predicted **f**
    function values.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图10\. 计算出的导数与预测的**f**函数值的比较。
- en: 'Although we can accurately approximate the unknown functions *f*₁ and *f*₂
    with an *f*-net, at the end of the day, *f*-net is a **black-box** neural network
    model. Naturally, we would like to ask: what is the exact functional form of these
    estimated functions? The answer could provide us with a deeper understanding of
    the underlying physical process, and help us generalize the results to other similar
    problems.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以用*f*-网络准确地逼近未知函数*f*₁和*f*₂，但归根结底，*f*-网络是一个**黑箱**神经网络模型。自然地，我们会想问：这些估计函数的确切功能形式是什么？这个答案可以为我们提供对潜在物理过程的更深入理解，并帮助我们将结果推广到其他类似的问题。
- en: So, how can we extract these precise functional forms from our trained neural
    network model? We will look into that in the next section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何从训练好的神经网络模型中提取这些精确的功能形式呢？我们将在下一节中探讨这个问题。
- en: 5\. Symbolic Regression
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 符号回归
- en: 'Symbolic regression is a powerful supervised machine learning technique that
    can be used to discover the underlying mathematical formula that best fits a given
    dataset. This technique, as the name suggests, comprises two key components: **symbolic**
    and **regression**:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 符号回归是一种强大的监督学习技术，可以用来发现最适合给定数据集的潜在数学公式。正如其名称所示，这项技术包括两个关键组成部分：**符号**和**回归**：
- en: '*Symbolic* refers to the use of symbolic expressions to model the input-output
    relationship, e.g., “+” for addition, “-” for subtraction, “cos” for cosine function,
    etc. Instead of fitting a predefined model (e.g., polynomial model, etc.), symbolic
    regression methods search through an entire space of potential symbolic expressions
    to find the best fit.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*符号*指的是使用符号表达式来建模输入输出关系，例如，“+”表示加法，“-”表示减法，“cos”表示余弦函数等。符号回归方法不是拟合预定义模型（例如，多项式模型等），而是通过整个潜在符号表达式的空间进行搜索，以找到最佳拟合。'
- en: '*Regression* refers to the process of creating a model to predict an output
    variable based on the input variables, thus capturing the underlying relationship
    between them. Although the term “regression” may invoke thoughts of linear regression,
    in the context of symbolic regression, it is not restricted to any specific model
    forms but can take a wide array of mathematical operators and structures.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*回归*指的是创建一个模型以预测输出变量的过程，该过程基于输入变量，从而捕捉它们之间的潜在关系。尽管“回归”一词可能会让人联想到线性回归，但在符号回归的背景下，它并不局限于任何特定的模型形式，而是可以采用各种数学运算符和结构。'
- en: In this section, we will implement the symbolic regression technique to distill
    the learned *f*-net into interpretable and compact mathematical expressions, which
    aligns with the strategy proposed by Zhang et al. in their original paper. We
    will begin by introducing the library PySR, which we will use for symbolic regression.
    Subsequently, we will apply this library to our problem and discuss the choice
    of hyperparameters. Finally, we analyze the obtained results.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将实现符号回归技术，将学习到的 *f*-网络提炼成可解释且紧凑的数学表达式，这与张等人在他们的原始论文中提出的策略一致。我们将首先介绍将用于符号回归的库PySR。随后，我们将应用这个库解决我们的课题，并讨论超参数的选择。最后，我们将分析获得的结果。
- en: 5.1 PySR library
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 PySR库
- en: '[PySR](https://astroautomata.com/PySR/) is an open-source Python library designed
    for practical, high-performance, scientific symbolic regression. It uses advanced
    *evolutionary* optimization algorithms to search through the space of simple analytic
    expressions for accurate and interpretable models, such that the prediction error
    and model complexity are jointly minimized.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[PySR](https://astroautomata.com/PySR/) 是一个开源Python库，旨在提供实用的高性能科学符号回归。它使用先进的
    *evolutionary* 优化算法在简单解析表达式的空间中搜索，以获得准确且可解释的模型，从而将预测误差和模型复杂度共同最小化。'
- en: Although PySR exposes a simple Python frontend API that resembles the style
    of `scikit-learn`, its backend is written in pure-Julia under the library named
    *SymbolicRegression.jl*. This gives the user the flexibility of customizing operators
    and optimization loss functions while enjoying high computation performance. For
    more details on the working principles of PySR, please refer to [this paper](https://arxiv.org/abs/2305.01582).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PySR暴露了一个类似于 `scikit-learn` 风格的简单Python前端API，但其后台是用纯Julia编写的，库名为 *SymbolicRegression.jl*。这为用户提供了定制操作符和优化损失函数的灵活性，同时享有高计算性能。有关PySR工作原理的更多细节，请参见[这篇论文](https://arxiv.org/abs/2305.01582)。
- en: To get started with PySR, you would need to [install Julia](https://julialang.org/downloads/)
    first. Afterward, run
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用PySR，你需要首先[安装Julia](https://julialang.org/downloads/)。然后运行
- en: '[PRE12]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Then install Julia dependencies via
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过
- en: '[PRE13]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: or from within IPython, call
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 或者在IPython中调用
- en: '[PRE14]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: PySR can also be installed via conda or docker. Please check the [installation
    page](https://astroautomata.com/PySR/) for more details.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: PySR也可以通过conda或docker安装。请查看[安装页面](https://astroautomata.com/PySR/)以获取更多细节。
- en: 5.2 Implementation
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 实施
- en: 'Next, we apply the PySR library to distill the learned *f*-net into interpretable
    and compact mathematical expressions. To begin with, we need to generate the dataset
    for symbolic regression learning:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应用PySR库将学习到的 *f*-网络提炼成可解释且紧凑的数学表达式。首先，我们需要生成符号回归学习的数据集：
- en: '[PRE15]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that for our current problem, the inputs for the symbolic regression learning
    are *t*, *u*₁, *u*₂, and *u*₃, and the outputs are *f*₁ and *f*₂. This is because,
    in our target ODEs, we assume *f*₁=*f*₁(*t*, *u*₁, *u*₂, *u*₃) and *f*₂=*f*₂(*t*,
    *u*₁, *u*₂, *u*₃). We saved the generated dataframe (see figure below) for later
    usage.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于我们当前的问题，符号回归学习的输入是 *t*、*u*₁、*u*₂ 和 *u*₃，输出是 *f*₁ 和 *f*₂。这是因为在我们的目标ODE中，我们假设
    *f*₁=*f*₁(*t*、*u*₁、*u*₂、*u*₃) 和 *f*₂=*f*₂(*t*、*u*₁、*u*₂、*u*₃)。我们保存了生成的数据框（见下图）以备后用。
- en: '![](../Images/1b0a3ff76c4ad0f3a7ddfc87cfd11ef4.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b0a3ff76c4ad0f3a7ddfc87cfd11ef4.png)'
- en: Figure 11\. The generated dataframe for symbolic regression learning. (Image
    by this blog author)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图11\. 生成的符号回归学习数据框。（图片来源：本博客作者）
- en: After generating the dataset, we are ready to perform symbolic regression with
    PySR. Note that it is recommended to run the PySR code in terminals instead of
    in Jupyter Notebook. Although PySR provides support for Jupyter Notebook, the
    printing (e.g., search progress, current best results, etc.) is much nicer in
    a terminal environment.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 生成数据集后，我们就可以使用PySR进行符号回归了。请注意，建议在终端中运行PySR代码，而不是在Jupyter Notebook中。尽管PySR支持Jupyter
    Notebook，但在终端环境中的打印（例如，搜索进度、当前最佳结果等）效果要更好。
- en: 'Following the `scikit-learn` style, we start by defining a model object:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 `scikit-learn` 风格，我们首先定义一个模型对象：
- en: '[PRE16]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here is the break-down of the specified hyperparameters:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是指定超参数的详细信息：
- en: '`niterations`: Number of iterations of the algorithm to run. Generally, a larger
    iteration number yields better results, at the cost of higher computational cost.
    However, since PySR allows terminating the search job early, a good practice is
    to simply set `niterations` to some very large value and keep the optimization
    going. Once the identified equation looks satisfactory, the job can be stopped
    early.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`niterations`：算法运行的迭代次数。通常，较大的迭代次数会产生更好的结果，但代价是更高的计算成本。然而，由于 PySR 允许提前终止搜索任务，好的做法是将
    `niterations` 设置为一个非常大的值并保持优化进行。一旦识别出的方程看起来令人满意，就可以提前停止任务。'
- en: '`binary_operators`: List of strings for binary operators used in the search.
    The built-in binary operators supported by PySR include `+`, `-`, `*`, `/`, `^`,
    `greater`, `mod`, `logical_or`, `logical_and`.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_operators`：用于搜索的二元运算符字符串列表。PySR 支持的内置二元运算符包括 `+`、`-`、`*`、`/`、`^`、`greater`、`mod`、`logical_or`、`logical_and`。'
- en: '`unary_operators`: List of unary operators used in the search. Note that unary
    operators only take a single scalar as input. The built-in ones include `neg`,
    `square`, `cube`, `exp`, `abs`, `log`, `log10`, `log2`, `log1p`, `sqrt`, `sin`,
    `cos`, `tan`, `sinh`, `cosh`, `tanh`, `atan`, `asinh`, `acosh`, `atanh_clip` (=atanh((x+1)%2
    - 1)), `erf`, `erfc`, `gamma`, `relu`, `round`, `floor`, `ceil`, `round`, `sign`.
    Note that to supply a custom operator, we need to pass in “myfunction(x) = …”
    to the operator list, like what we did with “inv(x) = 1/x”.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unary_operators`：用于搜索的一元运算符列表。注意，一元运算符只接受单个标量作为输入。内置的一元运算符包括 `neg`、`square`、`cube`、`exp`、`abs`、`log`、`log10`、`log2`、`log1p`、`sqrt`、`sin`、`cos`、`tan`、`sinh`、`cosh`、`tanh`、`atan`、`asinh`、`acosh`、`atanh_clip`（=atanh((x+1)%2
    - 1)）、`erf`、`erfc`、`gamma`、`relu`、`round`、`floor`、`ceil`、`round`、`sign`。注意，要提供自定义运算符，我们需要将“myfunction(x)
    = …”传递给运算符列表，就像我们用“inv(x) = 1/x”做的那样。'
- en: '`extra_sympy_mappings`: Provides mappings between custom `binary_operators`
    or `unary_operators` defined in julia strings, to those same operators defined
    in [sympy](https://www.sympy.org/en/index.html). This is useful when exporting
    the results.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extra_sympy_mappings`：提供自定义的 `binary_operators` 或 `unary_operators` 在 julia
    字符串中与 [sympy](https://www.sympy.org/en/index.html) 中相同运算符的映射。这在导出结果时非常有用。'
- en: '`loss`: String of Julia code specifying an elementwise loss function (as defined
    in LossFunctions.jl). Commonly used losses include `L1DistLoss()`(the absolute
    distance loss), `L2DistLoss()`(the least squares loss), `HuberLoss()`(the Huber
    loss function used for robustness to outliers). The loss function specifies the
    optimization target for the symbolic regression search.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`：指定元素级损失函数的 Julia 代码字符串（如在 LossFunctions.jl 中定义）。常用的损失包括 `L1DistLoss()`（绝对距离损失）、`L2DistLoss()`（最小二乘损失）、`HuberLoss()`（用于抗离群值的
    Huber 损失函数）。损失函数指定了符号回归搜索的优化目标。'
- en: '`model_selection`: Model selection criterion when selecting a final expression
    from the list of best expressions at each complexity. `score` means that the candidate
    model will be selected based on the highest score, which is defined as -Δlog(loss)/ΔC,
    where C refers to the complexity of the expression and Δ denotes local change.
    Therefore, if an expression has a much better loss at a slightly higher complexity,
    it is preferred.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_selection`：从每个复杂度的最佳表达式列表中选择最终表达式的标准。`score` 意味着候选模型将根据最高得分进行选择，得分定义为
    -Δlog(loss)/ΔC，其中 C 代表表达式的复杂度，Δ 表示局部变化。因此，如果一个表达式在稍高的复杂度下具有更好的损失，则更受青睐。'
- en: '`complexity_of_operators`: By default, all operators have a complexity of 1\.
    To change the default complexity setting and give preference to different operators,
    we can supply a dictionary with the key being the operator string and the value
    being its corresponding complexity level. In our current case, we set all unary
    operators to have a complexity level of 3, which was also adopted in the original
    paper of Zhang et al.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`complexity_of_operators`：默认情况下，所有运算符的复杂度为 1。要更改默认复杂度设置并优先考虑不同的运算符，我们可以提供一个字典，键为运算符字符串，值为其对应的复杂度级别。在我们当前的案例中，我们将所有一元运算符的复杂度级别设置为
    3，这也在 Zhang 等人的原始论文中采用。'
- en: It is worth mentioning that `PySRRegressor` exposes many other hyperparameters
    for setting up the algorithm, data preprocessing, stopping criteria, performance
    and parallelization, monitoring, environment, and results exporting. For the complete
    list of options for controlling the symbolic regression search, please check out
    the [PySRRegressor Reference page](https://astroautomata.com/PySR/api/#pysrregressor-parameters).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，`PySRRegressor` 提供了许多其他超参数，用于设置算法、数据预处理、停止标准、性能和并行化、监控、环境和结果导出。有关控制符号回归搜索的所有选项的完整列表，请查看
    [PySRRegressor 参考页面](https://astroautomata.com/PySR/api/#pysrregressor-parameters)。
- en: 5.3 Identification results
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 识别结果
- en: 'After specifying the model object, we can kick off the fitting process with
    three lines of code (for distilling the analytical forms of *f*₁):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定模型对象后，我们可以用三行代码启动拟合过程（用于提炼 *f*₁ 的解析形式）：
- en: '[PRE17]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: While the script is running, you should be able to see the progress bar and
    the current best equations, as shown in the figure below. Note that x0, x1, x2,
    and x3 correspond to *t*, *u*₁, *u*₂, and *u*₃, respectively.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在脚本运行时，你应该能够看到进度条和当前最佳方程，如下图所示。注意 x0、x1、x2 和 x3 分别对应 *t*、*u*₁、*u*₂ 和 *u*₃。
- en: '![](../Images/3f2c28ee599036a30c260a4ebed307a3.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f2c28ee599036a30c260a4ebed307a3.png)'
- en: 'Once the optimization job is finished, a list of candidate equations will appear
    in the terminal:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦优化任务完成，终端中将出现候选方程列表：
- en: '![](../Images/1edf4cea529a1bb841e2301629d11c77.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1edf4cea529a1bb841e2301629d11c77.png)'
- en: 'If we rank the equations based on their **score values**, we can see the top-3
    equations are:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们根据 **评分值** 对方程进行排名，可以看到排名前三的方程是：
- en: '*u*₂ *u*₃ *exp*( -0.1053391 *t* )'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*u*₂ *u*₃ *exp*( -0.1053391 *t* )'
- en: 0.60341805 *u*₂ *u*₃
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0.60341805 *u*₂ *u*₃
- en: '*u*₂ *u*₃'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*u*₂ *u*₃'
- en: Recall that our true ODE is
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下我们真实的 ODE 是
- en: '![](../Images/7ee90229681ba8c80eac60db10d85852.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ee90229681ba8c80eac60db10d85852.png)'
- en: It is quite impressive to see that the PySR has accurately identified the essential
    inputs (i.e., it recognized that *u*₁ does not play a role in *f*₁) and discovered
    an analytical expression (top-1 result) that is fairly close to the true expression
    of *f*₁.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 令人印象深刻的是，PySR 准确地识别出了基本输入（即，它识别出 *u*₁ 在 *f*₁ 中不起作用），并发现了一个接近 *f*₁ 真实表达式的解析表达式（排名第一的结果）。
- en: 'We replicate the same analysis to *f*₂. The optimization results are shown
    in the figure below:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 *f*₂ 进行了相同的分析。优化结果如下图所示：
- en: '![](../Images/7e935b2f1899d6929c72c6abaf0b6b43.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e935b2f1899d6929c72c6abaf0b6b43.png)'
- en: This time, we notice that the true expression of *f*₂, i.e., *f*₂=*u*₁*u*₃,
    only appears as the second-best (in terms of score) equation. However, note that
    the best one, i.e., *u*₃, has a score that is only marginally higher than the
    second-best one. On the other hand, the loss value of *u*₁*u*₃ is one magnitude
    lower than using *u*₃ alone. These observations indicate that in practice, we
    would need domain knowledge/experience to make an informed decision regarding
    whether the incurred complexity for high accuracy is worth pursuing.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们注意到 *f*₂ 的真实表达式，即 *f*₂=*u*₁ *u*₃，仅作为第二好的（按评分计算）方程出现。然而，请注意，最佳方程，即 *u*₃，其得分仅比第二好的高一点。另一方面，*u*₁
    *u*₃ 的损失值比单独使用 *u*₃ 低一个数量级。这些观察结果表明，在实际操作中，我们需要领域知识/经验来做出明智的决定，以判断追求高准确度所带来的复杂性是否值得。
- en: 6\. Takeaways
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6. 关键要点
- en: 'In this blog post, we investigated the problem of discovering differential
    equations from observational data. We followed the strategy proposed by Zhang
    et al., implemented it in code, and applied it to address a case study. Here are
    the key take-aways:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们探讨了从观测数据中发现微分方程的问题。我们遵循了 Zhang 等人提出的策略，将其实现为代码，并应用于一个案例研究。以下是关键要点：
- en: 1️⃣ Physics-informed neural network (PINN) is a versatile tool for performing
    system identifications, particularly in scenarios where only partial information
    is known about the governing differential equations. By assimilating observational
    data and available physical knowledge, PINN can effectively estimate not only
    the unknown parameters but also unknown functions, if we adopt the trick of parameterizing
    the unknown functions with auxiliary neural networks, which can be jointly trained
    with the main PINN. All these factors contribute to a substantial advantage over
    traditional system identification methods.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 物理信息神经网络 (PINN) 是一个多用途的工具，用于进行系统识别，特别是在对控制微分方程只有部分信息已知的情况下。通过同化观察数据和现有的物理知识，PINN不仅能有效估计未知参数，还能估计未知函数，如果我们采用用辅助神经网络对未知函数进行参数化的技巧，并与主PINN一起联合训练。这些因素共同作用，相比传统的系统识别方法，具有显著的优势。
- en: 2️⃣ Symbolic regression is a powerful tool in opening the black box of the learned
    neural networks. By searching through an entire space of symbolic expressions
    with advanced evolutionary algorithms, symbolic regression is able to extract
    interpretable and compact analytical expressions that can accurately describe
    the hidden input-output relationship. This knowledge-distillation process is greatly
    appreciated in practice as it can effectively enhance our understanding of the
    underlying system dynamics.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 符号回归是一种强大的工具，用于揭开学习神经网络的黑箱。通过利用先进的进化算法在整个符号表达式空间中进行搜索，符号回归能够提取出可解释且紧凑的解析表达式，这些表达式可以准确描述隐藏的输入输出关系。这个知识蒸馏过程在实践中受到高度赞赏，因为它能有效增强我们对基础系统动态的理解。
- en: 'Before we conclude this blog, there are a couple of points worth considering
    when applying PINN+symbolic regression for practical problems:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束这篇博客之前，有几点在将PINN+符号回归应用于实际问题时值得考虑：
- en: 1️⃣ Uncertainty quantification (UQ)
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1️⃣ 不确定性量化 (UQ)
- en: Throughout this blog, we have operated under the assumption that our observed
    data for *u*₁, *u*₂, and *u*₃ is noise-free. However, this assumption is generally
    not true, as the observational data can easily be contaminated by noise for practical
    dynamical systems. Consequently, both the *accuracy* and *reliability* of our
    system identification results will suffer. Therefore, a crucial aspect to consider
    is the quantification of uncertainty within our system identification workflow.
    Techniques such as Bayesian Neural Networks and [Monte Carlo simulation](https://medium.com/towards-data-science/how-to-quantify-the-prediction-error-made-by-my-model-db4705910173)
    could properly account for noise in the observed data and provide an estimation
    of the confidence interval for the predictions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们假设我们观察到的 *u*₁、*u*₂ 和 *u*₃ 数据是无噪声的。然而，这种假设通常不成立，因为实际的动态系统中的观察数据很容易被噪声污染。因此，我们系统识别结果的
    *准确性* 和 *可靠性* 都会受到影响。因此，一个关键方面是考虑在我们的系统识别工作流中进行不确定性量化。像贝叶斯神经网络和 [蒙特卡洛模拟](https://medium.com/towards-data-science/how-to-quantify-the-prediction-error-made-by-my-model-db4705910173)
    这样的技术可以合理地考虑观察数据中的噪声，并提供对预测的置信区间的估计。
- en: 2️⃣ Sensitivity of symbolic regression
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2️⃣ 符号回归的敏感性
- en: 'Generally speaking, the results yielded by symbolic regression may be sensitive
    to the employed loss function, supplied candidates of unary and binary operators,
    as well as the defined complexities of the operators. For example, in my attempt
    to reproduce the results published by Zhang et al., I was unable to obtain the
    exact top-3 equations for *f*₂ as shown in the original paper, although I have
    adopted the exact same settings (to my best knowledge). Several factors may contribute
    to this mismatch: first of all, the evolutionary optimization techniques are intrinsically
    stochastic, therefore results can vary across different runs. Secondly, it is
    likely that the PINN trained in the first stage is different, therefore the resultant
    dataset (i.e., *t*, *u*₁, *u*₂, *u*₃ → *f*₁, *f*₂) is also different, which in
    turn impacts the symbolic regression outcome.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，符号回归得到的结果可能对所使用的损失函数、提供的单一和二元运算符候选项以及定义的运算符复杂度敏感。例如，在我尝试重现 Zhang 等人发布的结果时，尽管我采用了完全相同的设置（据我所知），但我未能获得与原始论文中所示的
    *f*₂ 完全一致的前 3 个方程。这种不匹配可能有几个因素：首先，进化优化技术本质上是随机的，因此结果可能在不同的运行中有所不同。其次，第一阶段训练的 PINN
    可能不同，因此生成的数据集（即 *t*，*u*₁，*u*₂，*u*₃ → *f*₁，*f*₂）也不同，从而影响了符号回归的结果。
- en: Overall, these observations suggested that symbolic regression outcomes should
    not be accepted blindly. Instead, it's crucial to rely on the domain understanding/knowledge
    to critically assess the plausibility of the identified equations.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这些观察结果表明，符号回归的结果不应盲目接受。相反，依赖领域知识/理解来批判性地评估识别出的方程的合理性至关重要。
- en: If you find my content useful, you could buy me a coffee [here](https://www.buymeacoffee.com/Shuaiguo09f)
    🤗 Thank you very much for your support!
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得我的内容有用，可以在[这里](https://www.buymeacoffee.com/Shuaiguo09f)请我喝咖啡🤗 非常感谢你的支持！
- en: You can find the companion notebook and script with full code [here](https://github.com/ShuaiGuo16/PINN_symbolic_regression)
    *💻*
  id: totrans-204
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://github.com/ShuaiGuo16/PINN_symbolic_regression)找到带有完整代码的伴随笔记本和脚本*💻*。
- en: ''
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To learn the best practices of physics-informed neural networks: [Unraveling
    the Design Pattern of Physics-Informed Neural Networks](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)'
  id: totrans-206
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要学习物理信息神经网络的最佳实践，请参阅：[解开物理信息神经网络设计模式的奥秘](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)。
- en: ''
  id: totrans-207
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To learn more about physics-informed operator learning: [Operator Learning
    via Physics-Informed DeepONet](/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)'
  id: totrans-208
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要了解更多关于物理信息运算符学习的内容，请参阅：[通过物理信息深度运算符学习](/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)。
- en: ''
  id: totrans-209
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Feel free to subscribe to my [newsletter](https://shuaiguo.medium.com/subscribe)
    or follow me on [Medium](https://shuaiguo.medium.com/).
  id: totrans-210
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随时可以订阅我的[新闻通讯](https://shuaiguo.medium.com/subscribe)或在[Medium](https://shuaiguo.medium.com/)上关注我。
- en: Reference
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[1] Zhang et al., Discovering a reaction-diffusion model for Alzheimer’s disease
    by combining PINNs with symbolic regression. arXiv, 2023.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Zhang 等，结合 PINN 与符号回归发现阿尔茨海默病的反应扩散模型。arXiv，2023。'
- en: '[2] Cranmer et al., Interpretable Machine Learning for Science with PySR and
    SymbolicRegression.jl. arXiv, 2023.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Cranmer 等，使用 PySR 和 SymbolicRegression.jl 进行可解释的机器学习科学研究。arXiv，2023。'
