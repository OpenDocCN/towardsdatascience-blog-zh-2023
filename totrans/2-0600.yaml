- en: Creating a YouTube Data Pipeline with AWS and Apache Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/creating-a-youtube-data-pipeline-with-aws-and-apache-airflow-e5d3b11de9c2](https://towardsdatascience.com/creating-a-youtube-data-pipeline-with-aws-and-apache-airflow-e5d3b11de9c2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A solution for effectively managing YouTube data with cloud services and job
    schedulers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aashishnair?source=post_page-----e5d3b11de9c2--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----e5d3b11de9c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e5d3b11de9c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e5d3b11de9c2--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----e5d3b11de9c2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e5d3b11de9c2--------------------------------)
    ·12 min read·Apr 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e59f2354c5c004a43224decca7d5b946.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Photo by Artem Podrez: [https://www.pexels.com/photo/thoughtful-woman-with-earbuds-using-laptop-4492161/](https://www.pexels.com/photo/thoughtful-woman-with-earbuds-using-laptop-4492161/)'
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Introduction](#d514)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Problem Statement](#615d)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Understanding the Data Pipeline](#c674)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Understanding the Data](#c793)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Phase 1 — Setting Up The AWS Environment](#d201)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Phase 2— Facilitating AWS Operations with Boto3](#bd0e)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Phase 3— Setting Up the Airflow Pipeline](#0497)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Phase 4— Running the Pipeline](#2a4f)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Examining the Results](#66c0)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Advantages of This Data Pipeline](#1a83)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Disadvantages of This Data Pipeline](#b332)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Conclusion](#62b5)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: YouTube has become a major medium of exchange for information, thoughts, and
    ideas, with an average of 3 million videos being uploaded each day. The video
    streaming platform always has a new topic of conversation prepared for its audience
    with its diverse content, ranging from somber news stories to upbeat music videos.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, with a constant influx of video content, it’s difficult to
    gauge what types of content attract the attention of the fickle YouTube audience
    the most.
  prefs: []
  type: TYPE_NORMAL
- en: In general, what types of videos get the most likes and comments? Do YouTube
    users from different countries favor different types of content? Does interest
    in specific content categories fluctuate throughout time?
  prefs: []
  type: TYPE_NORMAL
- en: Answering these types of questions requires a systematic approach toward collecting,
    processing, and storing YouTube data for subsequent analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Problem Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal of the project is to create a data pipeline that collects YouTube
    data for the 5 countries with the most YouTube users: Brazil, India, Indonesia,
    Mexico, and the United States. This pipeline should collect data on a daily basis
    in order to keep the information up to date.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data collected with the pipeline will be used to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the most popular video categories in each country?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the most popular video category change over time? If so, how?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Developing and running the pipeline will require a large number of steps. Thus,
    the project will be split into 4 phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Setting up the AWS environment**'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Facilitating AWS operations with boto3**'
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Setting up the Airflow pipeline**'
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Running the Airflow pipeline**'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Data Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before beginning the project, it’s worth discussing the setup of the pipeline,
    which can be represented by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c88925bd94aafe74b9e91f50643c461.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Pipeline (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: There’s a lot going down, so let’s break it down.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Web Services (AWS) will provide all of the compute and storage services
    needed for this project, whereas Apache Airflow will schedule the relevant processes
    to run daily.
  prefs: []
  type: TYPE_NORMAL
- en: All of the processes carried out within the AWS environment will be facilitated
    by Python scripts that leverage boto3, the AWS software development kit (SDK)
    for Python. These scripts are to be scheduled with an Apache Airflow Directed
    Acyclic Graph (DAG), which is deployed into an EC2 instance.
  prefs: []
  type: TYPE_NORMAL
- en: The DAG stored in the EC2 instance will pull data using the YouTube API and
    store it in an S3 bucket as csv files. The program will then use Amazon Athena
    to query the data in that bucket. The results of the query will be stored within
    the same S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, understanding the YouTube data provided by the API will provide some insight
    into what processes it needs to be subject to in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'The response of the API call contains information on up to 50 of the most popular
    videos in a given country at a given time. The raw data comes in a JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For more information, feel free to visit the YouTube [documentation](https://developers.google.com/youtube/v3/docs/videos/list).
  prefs: []
  type: TYPE_NORMAL
- en: In order to query this with SQL later on, this raw data will be transformed
    into csv format before being stored in AWS. The processed data will also omit
    features that are not relevant to this use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'This transformation from JSON data to tabular data is summarized by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d28600b0c0cdc7dbd4630a5b30a4fbbb.png)'
  prefs: []
  type: TYPE_IMG
- en: Transforming JSON Data (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, an API call only reveals information on a single country. In other
    words, obtaining data on popular videos in N countries will require N API calls.
    Since the goal is to extract YouTube data for 5 countries, each iteration will
    require making 5 API calls.
  prefs: []
  type: TYPE_NORMAL
- en: After storing the data in AWS S3 as a csv file, the data will be queried to
    determine the most popular video category for each country and date.
  prefs: []
  type: TYPE_NORMAL
- en: 'The query output will contain the following fields: date, country, video category,
    and number of videos.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc183ae775a6faa00cf5b14604081f55.png)'
  prefs: []
  type: TYPE_IMG
- en: Format of Query Output (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Phase 1 — Setting Up the AWS Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/974440ed29f614c75ce1aa666dacbd3c.png)'
  prefs: []
  type: TYPE_IMG
- en: AWS Environment (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first create/deploy the AWS resources that will be required to store and
    query the YouTube data.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. An IAM User**'
  prefs: []
  type: TYPE_NORMAL
- en: An IAM user will be created just for this project. This user will gain permission
    to use the Amazon EC2, Amazon S3, and Amazon Athena.
  prefs: []
  type: TYPE_NORMAL
- en: The user account will also be provided with access keys, which will be needed
    to use these resources with Python scripts.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. EC2 instance**'
  prefs: []
  type: TYPE_NORMAL
- en: We will set up a t2-small instance with Ubuntu AMI. This is where the Airflow
    pipeline will be deployed.
  prefs: []
  type: TYPE_NORMAL
- en: The instance has a key pair file named “youtube_kp.pem”, which will be needed
    to access the EC2 instance with secure shell (SSH) and copy files to the instance
    with secure copy protocol (SCP).
  prefs: []
  type: TYPE_NORMAL
- en: In addition, this instance will have an inbound rule added to enable the user
    to view the Airflow webserver (the user interface).
  prefs: []
  type: TYPE_NORMAL
- en: 'After connecting to the instance with SSH, the following installations will
    be made:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, a virtual environment named `youtube`, where the project will be run in,
    is set up.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After that, a directory named `airflow` will be added to the instance. This
    is where all of the required files including the DAG will be stored.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**2\. S3 Bucket**'
  prefs: []
  type: TYPE_NORMAL
- en: The S3 bucket will store the data pulled with the YouTube API as well as the
    outputs of the queries that are executed. The bucket will be named `youtube-data-storage`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f4a255ea70df150697896210a396cb1.png)'
  prefs: []
  type: TYPE_IMG
- en: S3 Bucket (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: There is very little that needs to be done in terms of configuration. The bucket
    will simply contain two folders, named `data` and `query-output`, which will contain
    the pulled data and the query outputs, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b00eaa13cf2343548e53485a0a62a749.png)'
  prefs: []
  type: TYPE_IMG
- en: S3 Bucket Folders (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. AWS Athena**'
  prefs: []
  type: TYPE_NORMAL
- en: Next, AWS Athena will be configured to query the data stored in the `data` folder
    and store the output in the `query-output` folder. By default, it leaves us an
    empty database with no tables.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b00f9b3a6cb4c80ee5170c47fd13af3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Empty Database (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: First, a new database named `youtube` is created in the query editor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, an iceberg table named `youtube_videos` is created. The table’s fields
    should match those of the csv files that will be loaded into the S3 bucket. It
    should also specify the location of the csv files that will be queried.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the query result location will be set to the `query-output` subdirectory
    in the `youtube-data-storage` bucket in the settings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c214d161ed7405645fbdfcfc4bd676cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Query Result Location (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Phase 2— Facilitating AWS Operations with Boto3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/3b13627766f4a17b6b238247c72b90e6.png)'
  prefs: []
  type: TYPE_IMG
- en: AWS SDK (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: With the AWS environment set up, we can now focus on developing the scripts
    that will facilitate the transfer and processing of data.
  prefs: []
  type: TYPE_NORMAL
- en: First, we pull the data from the YouTube API, convert it to a tabular format,
    and store it in the S3 bucket as a csv file with the following function named
    `pull_youtube_data.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: It might be worth remembering the names of these Python functions, as
    it will make it easier to follow along when the Airflow DAG is configured.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The only parameter of this function is `region_code` , which is the 2-digit
    code that denotes the country of interest.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, `pull_youtube_data('US')` will return data on the most popular
    videos in the United States. To obtain information on the most popular videos
    in the 5 countries, this function will need to be run 5 times.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we write a function that uses boto3 to analyze the data in the S3 bucket
    with Amazon Athena. This entails writing an SQL query on the `data` folder and
    storing the result in the `query-output` folder.
  prefs: []
  type: TYPE_NORMAL
- en: These steps are carried out in the function named `run_query`.
  prefs: []
  type: TYPE_NORMAL
- en: Phase 3— Setting Up the Airflow Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/c7905b4fceae68ae0338127f832e7f98.png)'
  prefs: []
  type: TYPE_IMG
- en: Scheduling Python Code with Apache Airflow (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: After creating the Python functions, we can build the Airflow pipeline and schedule
    the functions to run on a daily basis. There are a number of steps in this phase,
    so brace yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. First, we write the Python script named `youtube_dag.py` that instantiates
    the DAG, defines the tasks, and establishes the dependencies between the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the DAG is configured to execute the `pull_data` function 5 times
    (once for each country) before running the `run_query` function. The DAG runs
    are carried out on a daily basis.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can deploy the Airflow DAG in the created EC2 instance.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Access the EC2 instance with SSH**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**3\. Enter the created virtual environment**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**4\. Install the required dependencies (including Apache Airflow)**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**5\. Specify the location of the airflow home directory**'
  prefs: []
  type: TYPE_NORMAL
- en: 'On Ubuntu, this can be done with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To set the Airflow home directory to the created airflow folder, add the following
    line to the text editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**6\. Initialize an Airflow database**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**7\. Store the YouTube DAG (and other dependencies) in the required location**'
  prefs: []
  type: TYPE_NORMAL
- en: The airflow.cfg file will indicate the directory that the DAG has to be in to
    be detected.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/900d4a51361baf061cfb511212b86733.png)'
  prefs: []
  type: TYPE_IMG
- en: Location of DAG File (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: The `youtube_dag.py`, along with all other Python files are copied into this
    directory from the local machine using secure copy (SCP). The command for executing
    the copy has the following format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: After executing the SCP commands, the `dags` directory should have all the necessary
    files.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/460998b73755aa4afd933dc7a8d062d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Files in Dags Directory (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, Airflow should have access to the created DAG. To confirm this, we
    can use the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7c6642d4a983f5ef860d020e44734fc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Phase 4— Running the Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we have set up the AWS resources, the Python scripts, and the Airflow
    DAG! The data pipeline is now ready to run!
  prefs: []
  type: TYPE_NORMAL
- en: We can look at the workflow using the Airflow webserver.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Although the DAG is scheduled to run automatically, we can run trigger it manually
    for the first iteration by running the following in the command line interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, it can be triggered to run by clicking the “play” button in the
    webserver.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7992e8186cfccbd77ff21444e22bae4.png)'
  prefs: []
  type: TYPE_IMG
- en: Running a DAG Manually (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'For a better grasp of the constituents of the workflow, we can view the DAG’s
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb1ff7cb4f7bbdf9ad2a2fdaba6c850d.png)'
  prefs: []
  type: TYPE_IMG
- en: DAG Graph (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the graph, the YouTube data containing the most popular videos for
    Brazil, Indonesia, India, Mexico, and the United States are first pulled with
    the YouTube API and stored in the S3 bucket. After the data for all countries
    is ingested, AWS Athena runs the pre-defined query to determine the most popular
    categories for each country on each given day.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the green edges around all of the tasks signify that the tasks
    have been run successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: There is no need to manually trigger the DAG every time; it will run
    on the pre-defined schedule, which is daily in this case.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Examining the Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's see what the stored CSV files and the query output looks like after the
    pipeline is run twice (i.e., two dates for each country).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1392f32d3609e0186903fc7aa62e670d.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Directory in S3 Bucket (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: We can see 10 csv files (1 for each country and date) in the data folder in
    the `youtube-data-storage` bucket.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd6d373d83c2fd0ec6f2dcb0599561ec.png)'
  prefs: []
  type: TYPE_IMG
- en: query-output directory in S3 Bucket (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The `query-output` directory has a single csv file that contains the query
    output. The contents of the file are in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5dc7ccdf8758f04c76fc42768a6a5625.png)'
  prefs: []
  type: TYPE_IMG
- en: SQL Query Output (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'While there are many video categories within the YouTube platform, it seems
    that the most popular videos in Brazil, Indonesia, India, Mexico, and the United
    States fall into only 4 categories: “Music”, “Entertainment”, “People & Blogs”,
    and “Sports”.'
  prefs: []
  type: TYPE_NORMAL
- en: While this is an interesting finding, there is very little that can be inferred
    from it since it only comprises 2 days' worth of information. However, after running
    the Airflow DAG repeatedly, the information will adequately capture how the audiences’
    preference in videos for each country changes over time.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of This Data Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Collecting YouTube data in such a pipeline is beneficial for a few reasons.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Harnessing cloud technologies:** AWS provides us with the means to securely
    store our data and respond to any sudden changes in demand, all at an inexpensive
    rate.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Automation:** With the deployed Airflow pipeline, YouTube data will be
    pulled, transformed, stored, and queried without any manual effort.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3.** **Logging:** Airflow’s webserver allows us to access the log and examine
    all of the DAG runs as well as the current status of all tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.** **Modifiable:** Elements can be added to or removed from the AWS environment
    without hampering the operations in the current pipeline. Users can easily add
    new data sources, write new queries, or launch other AWS resources if needed.'
  prefs: []
  type: TYPE_NORMAL
- en: Disadvantages of This Data Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: That being said, the constructed pipeline does have limitations.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Lack of notifications:** Currently, the data pipeline doesn’t incorporate
    a notification service that alerts users if a task in the Airflow DAG doesn’t
    run as expected. This can delay response time.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Lack of a data warehouse:** The pipeline currently generates reports
    by directly querying the S3 bucket, which serves as the data lake. This is feasible
    with the current circumstances, but if other data sources were to be added, the
    pipeline would lack the tools needed to efficiently perform complex joins or aggregation
    needed for subsequent analyses.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/28ea94bb56e5a9eb7be2b9b6b65fd6f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Prateek Katyal](https://unsplash.com/it/@prateekkatyal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, tracking the most popular YouTube content in different regions over
    time is a massive endeavor, which requires a continuous collection of data that
    is suited for analysis. Although the operations needed to obtain this data can
    be executed manually with on-premise infrastructure, such an approach becomes
    more inefficient and infeasible as the volume of data increases and as the workflow
    gets more complex.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, for this use case, it’s worth leveraging a cloud platform and a job scheduler
    to ensure that the ingestion and processing of data can be automated.
  prefs: []
  type: TYPE_NORMAL
- en: With AWS, we can create a scalable and cost-effective solution that collects,
    transforms, stores, and processes YouTube data. With Apache Airflow, we have the
    means to monitor complex workflows and debug them when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the source code used in this project, please visit the GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/anair123/Building-a-Youtube-Data-Pipeline-With-AWS-and-Airflow?source=post_page-----e5d3b11de9c2--------------------------------)
    [## GitHub - anair123/Building-a-Youtube-Data-Pipeline-With-AWS-and-Airflow'
  prefs: []
  type: TYPE_NORMAL
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/anair123/Building-a-Youtube-Data-Pipeline-With-AWS-and-Airflow?source=post_page-----e5d3b11de9c2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs: []
  type: TYPE_NORMAL
