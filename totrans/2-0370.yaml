- en: Best Data Wrangling Functions in PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark 中最好的数据整理函数
- en: 原文：[https://towardsdatascience.com/best-data-wrangling-functions-in-pyspark-3e903727319e](https://towardsdatascience.com/best-data-wrangling-functions-in-pyspark-3e903727319e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/best-data-wrangling-functions-in-pyspark-3e903727319e](https://towardsdatascience.com/best-data-wrangling-functions-in-pyspark-3e903727319e)
- en: Learn the most helpful functions when wrangling Big Data with PySpark
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习在处理大数据时使用 PySpark 的最有用函数
- en: '[](https://gustavorsantos.medium.com/?source=post_page-----3e903727319e--------------------------------)[![Gustavo
    Santos](../Images/a19a9f4525cdeb6e7a76cd05246aa622.png)](https://gustavorsantos.medium.com/?source=post_page-----3e903727319e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e903727319e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e903727319e--------------------------------)
    [Gustavo Santos](https://gustavorsantos.medium.com/?source=post_page-----3e903727319e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://gustavorsantos.medium.com/?source=post_page-----3e903727319e--------------------------------)[![Gustavo
    Santos](../Images/a19a9f4525cdeb6e7a76cd05246aa622.png)](https://gustavorsantos.medium.com/?source=post_page-----3e903727319e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e903727319e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e903727319e--------------------------------)
    [Gustavo Santos](https://gustavorsantos.medium.com/?source=post_page-----3e903727319e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e903727319e--------------------------------)
    ·7 min read·Dec 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e903727319e--------------------------------)
    ·阅读时长7分钟·2023年12月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fa66dae99d44fd5c8cadf67a519d043e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa66dae99d44fd5c8cadf67a519d043e.png)'
- en: Photo by [Oskar Yildiz](https://unsplash.com/@oskaryil?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/turned-on-gray-laptop-computer-cOkpTiJMGzA?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Oskar Yildiz](https://unsplash.com/@oskaryil?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来源于 [Unsplash](https://unsplash.com/photos/turned-on-gray-laptop-computer-cOkpTiJMGzA?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: I work with PySpark in Databricks on a daily basis. My work as a Data Scientist
    requires me to deal with large amounts of data in many different tables. It is
    a challenging job, many times.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我每天在 Databricks 中使用 PySpark。作为一名数据科学家，我的工作需要处理许多不同表中的大量数据。这是一份充满挑战的工作。
- en: 'As much as the **Extract, Transform and Load (ETL)** process sounds like something
    simple, I can tell that it is not always like that. When we work with Big Data,
    a lot of our thinking has to change for two reasons:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 **提取、转换和加载 (ETL)** 过程听起来很简单，但我可以说它并不总是如此。当我们处理大数据时，我们的思维需要因两个原因而改变：
- en: The amounts of data are way bigger than regular datasets.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些数据的规模远远大于常规数据集。
- en: When working with parallel computing in clusters, we must take into account
    that the data will be split among many worker nodes to perform part of the job
    and then be brought together as a whole. And this process, many times, can become
    very time consuming if the query is too complex.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集群中进行并行计算时，我们必须考虑到数据会被分割到许多工作节点中，以执行部分工作，然后再合并为整体。并且这一过程，很多时候，如果查询过于复杂，可能会非常耗时。
- en: Knowing that, we must learn how to be write smart queries for Big Data. In this
    post, I will show a few of my favorite functions from the module `pyspark.sql.functions`,
    aiming to help you during your Data Wrangling in PySpark.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 知道这一点后，我们必须学习如何为大数据编写智能查询。在这篇文章中，我将展示一些我最喜欢的来自模块 `pyspark.sql.functions` 的函数，旨在帮助你在
    PySpark 中进行数据整理。
- en: Best Functions
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最佳函数
- en: Now let’s move on to the content in this post.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续讨论这篇文章的内容。
- en: 'Just like many other languages, PySpark has the benefit of the modules, where
    you can find many ready-to-use functions for the most different purposes. Here’s
    the one we will load to our session:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 就像许多其他语言一样，PySpark 的模块也提供了许多现成的函数，适用于各种不同的目的。这里是我们将加载到会话中的模块：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you want to see how extended is the list of functions inside `pyspark.sql.functions`,
    [go to this web site](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html),
    where the API Reference is. Have in mind that this is for the version 3.5.0\.
    Some older versions may not carry all the functions I will show in this post.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看`pyspark.sql.functions`中函数的扩展列表，[请访问这个网站](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)，在那里有
    API 参考。请注意，这适用于 3.5.0 版本。一些旧版本可能不包含我将在这篇文章中展示的所有函数。
- en: Dataset
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: The dataset to be used as our example is the *Diamonds*, from ggplot2, shared
    under [MIT License](https://tidyverse.tidyverse.org/LICENSE.html).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 用作示例的数据集是*Diamonds*，来自 ggplot2，依据[MIT许可证](https://tidyverse.tidyverse.org/LICENSE.html)共享。
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Creating an Index Column
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建索引列
- en: For those who work with Pandas in Python, it feels weird at first to deal with
    a data frame without index. So, if we want to add an index column, the function
    is `monotonically_increasing_id()`. The counting starts with 0\. Ergo, if we want
    to start in one, just add a `+1` after the function.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些在 Python 中使用 Pandas 的人来说，刚开始处理没有索引的数据框时会觉得很奇怪。因此，如果我们想要添加一个索引列，可以使用函数`monotonically_increasing_id()`。计数从0开始。因此，如果我们想从1开始，只需在函数后加上`+1`。
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Sum, Mean, Max, Min
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总和、均值、最大值、最小值
- en: The classic mathematical functions are surely making this list. Useful in every
    case.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的数学函数肯定会出现在这个列表中。无论何种情况都很有用。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Count and Count Distinct
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计数和计数不同
- en: It is important to count values and know how many distinct values are in the
    data as well.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 计数值和了解数据中有多少个不同值也是很重要的。
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Literal Value
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字面值
- en: The function `lit()` lets you write a literal value for every row in the data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`lit()`允许你为数据中的每一行写入一个字面值。
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/59621d7d64a0addc9fa86688499e98b2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59621d7d64a0addc9fa86688499e98b2.png)'
- en: lit(‘my text or number’). Image by the author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: lit(‘my text or number’). 图片由作者提供。
- en: Floor, Ceiling and Percentile
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向下取整、向上取整和百分位数
- en: Some great math functions that help a lot while wrangling data are `floor` —
    the closest integer down — and `ceiling` — the closest integer up.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一些在处理数据时非常有用的数学函数是`floor`——向下取整的整数——和`ceiling`——向上取整的整数。
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/84e1f5f531b5d738cd8fbec50bdebcac.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84e1f5f531b5d738cd8fbec50bdebcac.png)'
- en: Ceiling and Floor. Image by the author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 向上取整和向下取整。图片由作者提供。
- en: Now percentile is useful especially to calculate the median value. Until not
    so long ago, I remember I was looking to calculate the median and kept getting
    an error. Now I saw that it is there in version 3.5.0\. The best workaround was
    to calculate it using `percentile()` at 50%.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在百分位数特别有用，尤其是用于计算中位数。直到不久前，我记得我在计算中位数时遇到过错误。现在我发现它在版本 3.5.0 中已经存在。最好的解决方法是使用`percentile()`在
    50%处计算。
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/392403329d6eb2cc7ed52df62d06c602.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/392403329d6eb2cc7ed52df62d06c602.png)'
- en: Descriptive Stats
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描述性统计
- en: 'Despite not being within the `sql.functions` module, `describe()` is also for
    those familiar with Pandas. It brings the descriptive statistics of the dataset.
    The numbers provided here are: count, mean, standard deviation, min and max.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`describe()`不在`sql.functions`模块中，但它也适用于那些熟悉 Pandas 的人。它提供了数据集的描述性统计信息。这里提供的数字有：计数、均值、标准差、最小值和最大值。
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Resulting in:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![](../Images/44e3d001ba714793e35540781be19018.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44e3d001ba714793e35540781be19018.png)'
- en: Result of the describe( ) function. Image by the author.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: describe()函数的结果。图片由作者提供。
- en: Logarithm
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对数
- en: Working as Data Scientists, we keep the log functions close. Especially for
    linear regression, it is a helper for normalization of variables.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，我们常常使用对数函数。特别是在进行线性回归时，它是变量标准化的辅助工具。
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/88c24d78bf7326eb3d97f274045bb9f8.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88c24d78bf7326eb3d97f274045bb9f8.png)'
- en: Log variables calculated. Image by the author.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对数变量已计算。图片由作者提供。
- en: Array Aggregate
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数组聚合
- en: '`array_agg` is a good function to take the values of a group and list them
    in a new column. Let’s say we want to group the diamonds by the quality of the
    cut and have a look at the prices listed. The next snippet performs that.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`array_agg`是一个很好的函数，用于获取一个组的值并将其列出在新列中。假设我们想按切割质量对钻石进行分组，并查看列出的价格。以下代码片段执行了这个操作。'
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/f56e02ac600c6ac37357af261b7a5407.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f56e02ac600c6ac37357af261b7a5407.png)'
- en: List of values by group. Image by the author.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 按组列出值。图片由作者提供。
- en: Count IF
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计数 IF
- en: I bet it sounds familiar if you ever used MS Excel, right? And the idea is the
    same. If we want to count only the diamonds that cost more than $18,000 after
    grouping, we can use this function. Check this out.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We have more *Ideal* and *Premium* cuts with those expensive price, and almost
    no Fair.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b893c60a11440e84580a76de386be102.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: Count if in action. Image by the author.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Mode
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mode of a variable is the most common values. Now we want to know what is
    the most common carat for each quality of cut.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Regression Functions
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are really interesting. We can quickly calculate linear regression metrics
    like R-Squared, intercept, slope, using these functions: `regr_r2`, `regr_intercept`,
    `regr_slope`, `regr_avgx`.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: In the next snippet, we will calculate the regression R-Squared and the formula
    for each group.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is very cool! The *Ideal* cut (best quality) has the highest R², while
    the *Fair* cut (lowest quality) has the lowest. It makes sense.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7c0df9b9c7faa70259c62370a2028ba.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Regression by group. Image by the author.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Regular Expressions
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Yes, the *regexps* are everywhere. I see a lot of people rolling their eyes
    to those beautiful expressions, but it is an awesome tool Imagine that you can
    extract mostly anything from a text using these expressions. It might be difficult
    and bumpy at first, but once you learn more about it, you start to love them.
    And PySpark has them in the functions too.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Here, I am using the `regexp()` combined with a literal text `lit` to check
    if the variable `clarity` has digits. The next line is the function `locate` ,
    to locate the position of the first occurrence of the letter ‘S’ in the same variable.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/5cb5d4978a55f79d4b0851b93b131907.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: Text parsing functions. Image by the author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Text Parsing
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More about text parsing, we can use `split()` to split a text in parts. In the
    next snippet I am converting the column `carat` to text and splitting it by `.`.
    So, a number like 0.23 becomes [“0” , “23”]. Then I just use a slicing notation
    to place the results in separate columns.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/5c9d494b18adb387563220f1f000ceaa.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: Split function result. Image by the author.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Another parsing possibility is the function `left`, similar to MS Excel. You
    have a given column with text where you want to get only N characters from it.
    Just use `left` combined with `lit`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/2aca4893cbca553d19afb73af79f2d15.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: Left function result. Image by the author.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Before You Go
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wrangling data is an art. Being it in PySpark, R or Python, you will always
    need the best functions to make your transformations happen. And here, I listed
    just a few from the module `pyspark.sql.functions`. I suggest you to visit the
    documentation page and create your own best list.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'My best tip to transform data is:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Know what your end result should look like.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From there, you can break the process down to smaller steps that will make it
    happen.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I learned that when I started my career and used to work a lot with Excel sheets.
    When I didn’t know how to write a big fancy formula, I used to write smaller pieces
    in different cells until I got to the desired result. Then, it was just a matter
    of gathering the pieces and making it work as a single formula.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我在职业生涯初期学习到这一点，当时我经常使用 Excel 表格。当我不知道如何写一个复杂的公式时，我会在不同的单元格中写出较小的部分，直到得到期望的结果。然后，只需将这些部分汇总在一起，就能作为一个公式使用。
- en: The same applies to programming. Create your strategy with the end point in
    mind and go step by step, if needed. That’s it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的原则适用于编程。创建你的策略时要以最终目标为导向，并按步骤进行（如有需要）。就是这样。
- en: If you liked this content, follow my blog for more and subscribe to my newsletter.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这内容，可以关注我的博客获取更多信息，并订阅我的通讯。
- en: '[](https://medium.com/@gustavorsantos?source=post_page-----3e903727319e--------------------------------)
    [## Gustavo Santos - Medium'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Gustavo Santos - Medium](https://medium.com/@gustavorsantos?source=post_page-----3e903727319e--------------------------------)'
- en: Read writing from Gustavo Santos on Medium. Data Scientist. I extract insights
    from data to help people and companies…
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Medium 上阅读 Gustavo Santos 的文章。数据科学家。我从数据中提取洞察，以帮助人们和公司……
- en: medium.com](https://medium.com/@gustavorsantos?source=post_page-----3e903727319e--------------------------------)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@gustavorsantos?source=post_page-----3e903727319e--------------------------------)'
- en: Also find me on [LinkedIn](https://www.linkedin.com/in/gurezende/).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以在 [LinkedIn](https://www.linkedin.com/in/gurezende/) 上找到我。
- en: Interested in Learning More About PySpark?
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有兴趣深入了解 PySpark 吗？
- en: I am just releasing my new online course [**Mastering Data Processing With PySpark
    in Databrick**](https://www.udemy.com/course/master-data-processing-pyspark/?referralCode=44AA924FEB5260F5EB25).
    This is a good opportunity for you that want to upskill yourself and learn more
    about Wrangling Big Data!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚刚发布了我的新在线课程 [**在 Databrick 中掌握 PySpark 数据处理**](https://www.udemy.com/course/master-data-processing-pyspark/?referralCode=44AA924FEB5260F5EB25)。这是一个很好的机会，让你提升技能，深入了解大数据处理！
- en: I have made available [**this free Webinar with my top tips to write faster
    queries in PySpark**](https://www.gustavorsantos.com/webinar-funnel-3-cf-2-0--71744).
    Check it out, and you will find a coupon with the most special launching price
    inside!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我提供了 [**这场免费的网络研讨会，其中包含了我关于如何更快编写 PySpark 查询的顶级技巧**](https://www.gustavorsantos.com/webinar-funnel-3-cf-2-0--71744)。查看一下，你会发现里面有一个特别的首发优惠券！
- en: References
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[## Functions - PySpark 3.5.0 documentation'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Functions - PySpark 3.5.0 documentation](https://www.udemy.com/course/master-data-processing-pyspark/?referralCode=44AA924FEB5260F5EB25)'
- en: pyspark.sql.SparkSession.builder.getOrCreate
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`pyspark.sql.SparkSession.builder.getOrCreate`'
- en: spark.apache.org](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html?source=post_page-----3e903727319e--------------------------------)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[spark.apache.org](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html?source=post_page-----3e903727319e--------------------------------)'
