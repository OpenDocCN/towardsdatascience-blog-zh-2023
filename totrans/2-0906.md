# 你应该注意的五个数据泄露的隐藏原因

> 原文：[https://towardsdatascience.com/five-hidden-causes-of-data-leakage-you-should-be-aware-of-e44df654f185](https://towardsdatascience.com/five-hidden-causes-of-data-leakage-you-should-be-aware-of-e44df654f185)

## 以及它们如何破坏机器学习模型

[](https://donatoriccio.medium.com/?source=post_page-----e44df654f185--------------------------------)[![Donato Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----e44df654f185--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e44df654f185--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e44df654f185--------------------------------) [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----e44df654f185--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e44df654f185--------------------------------) ·8 分钟阅读·2023年4月11日

--

![](../Images/cd2be5f86610dde00f6fbf97f8484e3e.png)

图片由 [Linh Pham](https://unsplash.com/@linharex?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

数据泄露是一个隐蔽的问题，通常困扰机器学习模型。泄露这个术语指的是测试数据*泄漏*到训练集中。当模型在训练过程中接触到不应有的数据时，就会发生这种情况，导致过拟合和在未见过数据上的表现不佳。这就像用考试答案来训练学生一样——他们在特定的考试中表现出色，但在其他考试中表现就不那么好。机器学习的目标是创建能够泛化并在新的、未见过的数据上做出准确预测的模型。数据泄露破坏了这一目标，因此了解和准备应对它非常重要。在本文中，我们将深入探讨什么是数据泄露，它的潜在原因，以及如何通过使用 Python 和 scikit-learn 的实际示例以及研究中的案例来防止它。

# 数据泄露的后果

+   **过拟合。** 数据泄露的一个重大后果是过拟合。过拟合发生在模型被训练得过于贴合训练数据，以至于无法对新数据进行泛化。当数据泄露发生时，模型在开发过程中使用的训练集和测试集上的准确率很高。然而，当模型被部署时，它的表现不会那么好，因为它无法将其分类规则泛化到未见过的数据。

+   **误导性的性能指标。** 数据泄露还可能导致误导性的性能指标。模型可能会表现出高准确率，因为它在训练过程中看到了部分测试数据。因此，很难评估模型并理解其性能。

# 划分前的数据泄露

我们呈现的第一个案例是最简单的，但可能是最常见的：在训练/测试划分之前进行预处理。

你想使用StandardScaler来标准化数据，因此你加载数据集，进行标准化，创建训练集和测试集，然后运行模型。对吗？错了。

[PRE0]

均值和标准差是在整列上计算的，因此它们包括了测试集中的信息。使用这些值进行标准化处理意味着测试数据正在*泄漏*到训练数据中。

## 解决方案：管道

[PRE1]

在这个版本中，使用了管道来封装预处理步骤，然后仅在训练集上进行拟合和评估。在这种情况下，**StandardScaler**被用作预处理步骤，它通过减去均值并缩放到单位方差来标准化特征。当你调用*fit*方法时，sklearn会分别标准化每个数据集。这确保了测试集不会用于指导预处理步骤，从而避免了数据泄露。

# 使用交叉验证时的数据泄露

第二个例子是一个非常常见的错误，通常被忽视。你的数据集是不平衡的，你已经了解过应该如何使用过采样来“修复”它。经过一些搜索，你发现了SMOTE，这是一种使用最近邻生成新样本以平衡少数类的算法。我们将这个技术应用于名为credit_g的数据集，来自PMLB库。

数据集是不平衡的，类别之间的比例为70/30。

[PRE2]

作为基线结果，我们展示了不应用任何变换的AUC分数。运行逻辑回归模型的平均ROC AUC分数为0.75。

现在让我们应用SMOTE。

[PRE3]

应用SMOTE后，你会很高兴地看到AUC分数从0.75提高到0.84！然而，所有的光芒都不是金子：你刚刚造成了数据泄露。在上面的代码中，变换在运行交叉验证之前应用，这将训练和测试集分割到不同的折中。这是一个非常常见的场景，可能会误导初学者认为SMOTE提高了他们的模型性能。

现在让我们看一下修正后的代码，其中SMOTE在交叉验证划分之后应用。

[PRE4]

正确应用SMOTE实际上使模型更糟。

正如[Samuele Mazzanti](https://medium.com/u/e16f3bb86e03?source=post_page-----e44df654f185--------------------------------)在他的文章[你的数据集不平衡？什么都不要做！](/your-dataset-is-imbalanced-do-nothing-abf6a0049813)中强调的那样，过采样并不是处理不平衡数据集的必要手段。

# 时间序列中的数据泄露

时间序列数据具有独特的特性，使其与其他类型的数据不同，这可能导致在划分数据、准备特征和评估模型时出现特定的挑战。在这里，我们将详细阐述这些挑战，并建议最佳实践以最小化时间序列分析中的数据泄露。

**不正确的训练-测试划分**：在时间序列数据中，将数据集分为训练集和测试集时，必须保持观察的时间顺序。随机划分可能会引入数据泄漏，因为它可能将未来的信息包含在训练集中。为了避免这种情况，应使用基于时间的划分，确保训练集中的所有数据点都在测试集的数据点之前。你还可以使用时间序列交叉验证或前向验证等技术，以更准确地评估模型的性能。

**特征工程**：你应该避免使用在预测时无法获得的未来信息。例如，计算技术指标、滞后变量或滚动统计数据时，只应使用过去的数据，而不是未来的数据。为了在特征工程过程中防止数据泄漏，你可以使用诸如应用基于时间的窗口函数等技术，确保计算窗口只包含到预测时间为止的数据。这也适用于外部数据。有时，时间序列模型会结合可能包含未来信息的外部数据源。确保指标适当地滞后，以免提供未来的信息，并始终验证外部数据源是否与主要时间序列数据集保持相同的时间顺序。

# 图像数据中的数据泄漏

当处理医学数据时，通常会从同一患者那里获取多张图像。在这种情况下，你不能仅仅随机划分数据集来训练模型，因为你可能会不小心将同一人的图像放在训练集和测试集中。相反，你需要使用*按受试者划分*的方法。

那么，什么是按受试者划分？这意味着你将同一人的所有图像放在一起，要么在训练集中，要么在测试集中。这样，你的模型就不能通过从两个集合中的同一人的图像中学习而作弊。

有一项[研究](https://arxiv.org/abs/2202.12267)探讨了随机划分与按受试者划分之间的差异。他们在三个不同的数据集上进行测试，发现随机划分会导致测试准确度虚高，因为数据泄漏。另一方面，按受试者划分则能得到更准确的结果。使用的数据集如下：

**AIIMS 数据集**：包含来自 45 名受试者（22 名癌症患者和 23 名健康受试者）的 18,480 张健康和癌症乳腺组织的 2D OCT 图像。

**Srinivasan 数据集**：一个眼科学数据集，包括 3,231 张年龄相关性黄斑变性（AMD）、糖尿病性黄斑水肿（DME）和正常受试者的 2D OCT 图像，每类包括 15 名受试者。

**Kermany的数据集**：一个大型开放获取的眼科数据集，包含来自5,319名患者的脉络膜新生血管（CNV）、糖尿病性黄斑水肿（DME）、视网膜上皮下层（drusen）和正常视网膜图像。该数据集有不同版本，图像数量、组织和训练与测试集之间的数据重叠有所不同。

结果不言自明。

![](../Images/dc7493873a07f08f8668283ed7199230.png)

不同分割策略的比较。[来源。](https://arxiv.org/abs/2202.12267)

模型通过Matthews相关系数进行评估，定义如下：

![](../Images/4acaa48e86486e538022e93f55db2e05.png)

Matthews相关系数。图像由作者提供。

正如你可以想象的那样，当我们随机分割数据时，得到的分数好得令人难以置信。这是因为来自同一个人的图像看起来非常相似，因此模型在识别训练数据中已经见过的人时更容易。在现实世界中，我们需要能够可靠识别新患者疾病的模型。

数据泄露是一个常见的问题，即使是最熟练的数据科学家在构建机器学习模型时也会受到影响。接下来，我们将看看另一个研究案例。2017年，Andrew Ng及其团队发表了一篇开创性的论文，标题为*“CheXNet：利用深度学习在胸部X光片中进行放射科医师级的肺炎检测。”* 这篇论文介绍了一种利用深度学习检测胸部X光片中肺炎的算法，其性能与专家放射科医师相当。以下图像来自论文。

![](../Images/d9a5181036c178ce5ed7ebe1043df275.png)

ChestXNet原始论文。第一个版本。[来源](https://arxiv.org/pdf/1711.05225v1.pdf)。

你注意到有什么问题吗？

在这项研究的[第一个版本](https://arxiv.org/pdf/1711.05225v1.pdf)中，他们训练模型时随机划分数据。由于包含了同一患者的多个扫描图像，这种潜在的数据泄露引发了对CheXNet结果可靠性和泛化能力的担忧。作者认识到这个问题，后来发布了新版本，纠正了这个问题。以下图像来自[修正版本。](https://arxiv.org/pdf/1711.05225.pdf)

![](../Images/45dea89bd87a5d709a5b301b7cd7ff70.png)

ChestXNet原始论文。最新版本。[来源](https://arxiv.org/pdf/1711.05225.pdf)。

# 结论

数据泄露是一个隐蔽的问题，会在开发的各个阶段影响机器学习模型。正如我们在本文中探讨的，它可能导致过拟合、误导性的性能指标，以及*最终*一个对未见数据无法良好泛化的模型。无论你是在处理表格数据、时间序列还是图像，都需要意识到这一点，以构建成功的模型。以下是本文的一些关键要点：

+   如果你的模型在进行某些更改后突然表现过于出色，检查是否有数据泄露总是个好主意。

+   避免在将数据集拆分为训练集和测试集之前对整个数据集进行预处理。相反，使用管道来封装预处理步骤。

+   在使用交叉验证时，对如过采样或其他任何变换技术要小心。只对每个折中的训练集应用这些技术，以防止数据泄露。

+   对于时间序列数据，保持观察的时间顺序，并使用诸如基于时间的拆分和时间序列交叉验证等技术。

+   对于图像数据或具有多个记录的同一受试者的数据集，使用每个受试者的拆分以避免数据泄露。

牢记这些要点，你将能够更好地构建更强大和更准确的机器学习模型。

*喜欢这篇文章？通过订阅我的新闻通讯，每周获取数据科学面试问题*，[*《数据面试》*](https://thedatainterview.substack.com/)*。*

*你也可以在* [*LinkedIn*](https://www.linkedin.com/in/driccio/)*上找到我。*
