- en: Singular Value Decomposition (SVD), Demystified
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/singular-value-decomposition-svd-demystified-57fc44b802a0](https://towardsdatascience.com/singular-value-decomposition-svd-demystified-57fc44b802a0)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A comprehensive guide to SVD with Python examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roiyeho?source=post_page-----57fc44b802a0--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----57fc44b802a0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----57fc44b802a0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----57fc44b802a0--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----57fc44b802a0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----57fc44b802a0--------------------------------)
    ·19 min read·Nov 8, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Singular value decomposition (SVD) is a powerful matrix factorization technique
    that decomposes a matrix into three other matrices, revealing important structural
    aspects of the original matrix. It is used in a wide range of applications, including
    signal processing, image compression, and dimensionality reduction in machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: This article provides a step-by-step guide on how to compute the SVD of a matrix,
    including a detailed numerical example. It then demonstrates how to use SVD for
    dimensionality reduction using examples in Python. Finally, the article discusses
    various applications of SVD and some of its limitations.
  prefs: []
  type: TYPE_NORMAL
- en: The article assumes the reader has basic knowledge of linear algebra. More specifically,
    the reader should be familiar with concepts such as vector and matrix norms, rank
    of a matrix, eigen-decomposition (eigenvectors and eigenvalues), orthonormal vectors,
    and linear projections.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/502e38be1631c3d04de689deedbe2734.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Peggy und Marco Lachmann-Anke](https://pixabay.com/users/peggy_marco-1553824/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1027571)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1027571)
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The singular value decomposition of an *m* × *n* real matrix *A* is a factorization
    of the form *A* = *U*Σ*Vᵗ*, where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*U* is an *m* × *m* **orthogonal matrix** (i.e., its columns and rows are orthonormal
    vectors). The columns of *U* are called the **left-singular** **vectors** of *A*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Σ is an *m* × *n* rectangular diagonal matrix with non-negative real numbers
    on the diagonal. The diagonal entries *σᵢ* = Σ*ᵢᵢ* are known as the **singular
    values** of *A* and are typically arranged in descending order, i.e., *σ*₁ *≥
    σ*₂ *≥ … ≥ σₙ ≥* 0\. The number of the non-zero singular values is equal to the
    rank of *A*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*V* is an *n* × *n* orthogonal matrix. The columns of *V* are called the **right-singular
    vectors** of *A*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/4f242c74c521d6be82324f67851bcccc.png)'
  prefs: []
  type: TYPE_IMG
- en: SVD of a matrix
  prefs: []
  type: TYPE_NORMAL
- en: Every matrix has a singular value decomposition(a proof of this statement can
    be found [here](https://en.wikipedia.org/wiki/Singular_value_decomposition#Proof_of_existence)).
    This is unlike eigenvalue decomposition, for example, which can be applied only
    to squared diagonalizable matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the SVD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The singular value decomposition of a matrix *A* can be computed using the
    following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The left-singular vectors of *A* are a set of orthonormal eigenvectors of *AAᵗ*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The right-singular vectors of *A* are a set of orthonormal eigenvectors of *AᵗA*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The non-zero singular values of *A* are the square roots of the non-zero eigenvalues
    of both *AᵗA* and *AAᵗ*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *U*Σ*Vᵗ* is the SVD of *A*, thenfor each singular value *σᵢ,*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/c21f44352bebcf44ded85f4b62127668.png)'
  prefs: []
  type: TYPE_IMG
- en: where **u***ᵢ* is the *i*-th column of *U* and **v***ᵢ* is the *i*-th column
    of *V*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof**:'
  prefs: []
  type: TYPE_NORMAL
- en: We will first show that the left-singular vectors of *A* are a set of orthonormal
    eigenvectors of *AAᵗ*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let *A* = *U*Σ*Vᵗ* be the SVD of *A*, and let’s examinethe product *AAᵗ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13bab9f9fe073214d3fde47c3655c08b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since Σ is a diagonal matrix with singular values *σᵢ* on its diagonal, ΣΣ*ᵗ*
    is also a diagonal matrix where each diagonal element is *σᵢ*². Let’s denote this
    matrix by Σ². This gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79a6fc8fe5780e4abe9da1471306492f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since *U* is orthonormal, *UᵗU* = *I*, and by right multiplying both sides
    of the equation by *U* we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/676c6c9399ba59fd0de48bf82c1f47b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s now consider a single column of *U*, denoted by **u***ᵢ*. Since *ABᵢ*
    = [*AB*]*ᵢ* (i.e., matrix *A* multiplied by column *i* of matrix *B* is equal
    to column *i* of their product *AB*), we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5dd5007f733784ba871a30e6ea3e7e96.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, **u***ᵢ* is an eigenvector of *AAᵗ* corresponding to the eigenvalue
  prefs: []
  type: TYPE_NORMAL
- en: '*λᵢ* = *σᵢ*². In other words, the columns of *U* are eigenvectors of *AAᵗ.*
    Because the columns of *U* are orthonormal, the left-singular vectors of *A* (the
    columns of *U*) are a set of orthonormal vectors of *AAᵗ.*'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. In a similar fashion, we can show that the right-singular vectors of *A*
    are a set of orthonormal eigenvectors of *AᵗA*.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. We first notice that *AAᵗ* is symmetric and positive semi-definite. Therefore
    all its eigenvalues are real and non-negative, and it has a full set of orthonormal
    eigenvectors. Let {**u**₁, …, **u***ₙ*} be the orthonormal eigenvectors of *AAᵗ*
    corresponding to eigenvalues *λ*₁ *≥ λ*₂ *≥ … ≥ λₙ ≥* 0\. For any eigenvector
    **u***ᵢ* of *AAᵗ* corresponding to an eigenvalue *λᵢ* we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3544ed7c7e7528d220657cb5e0e4d1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the singular values of *A* are the square roots of the eigenvalues of
    *AAᵗ*.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we can show that the singular values of *A* are also the square roots
    of the eigenvalues of *AᵗA*.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Left as an exercise to the reader.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the above observations, we can compute the SVD of an *m* × *n* matrix
    *A* using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Construct the matrix *AᵗA*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the eigenvalues and eigenvectors of *AᵗA*. The eigenvalues will be the
    squares of the singular values of *A*, and the eigenvectors will form the columns
    of the matrix *V* in the SVD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Arrange the singular values of *A* in descending order. Create an *m* × *n*
    diagonal matrix Σ with the singular values on the diagonal, padding with zeros
    if necessary so that the matrix has the same dimensions as *A*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the eigenvectors of *AᵗA* to have unit length, and place them as columns
    of matrix *V*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each singular value *σᵢ*, calculate the corresponding left-singular vector
    **u***ᵢ* as
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/c21f44352bebcf44ded85f4b62127668.png)'
  prefs: []
  type: TYPE_IMG
- en: where **v***ᵢ* is the *i*-th column of *V*. Place these vectors as columns in
    the matrix *U*.
  prefs: []
  type: TYPE_NORMAL
- en: If *n* < *m* or *A* is rank-deficient (i.e., rank(*A*) < min(*m*, *n*)), then
    there would not be enough non-zero singular values to determine the columns of
    *U*. In this case, we need to complete *U* to an orthogonal matrix by finding
    additional orthonormal vectors that span the null space (kernel) of *Aᵗ*.
  prefs: []
  type: TYPE_NORMAL
- en: The **null space** of *Aᵗ*, denoted *N*(*Aᵗ*), is theset of vectors **x** such
    that *Aᵗ***x** = **0**, which are also theeigenvectors of *AAᵗ* corresponding
    to eigenvalue 0 (since *AAᵗ***x** = 0⋅**x)**. To find an orthonormal basis for
    *N*(*Aᵗ*), we first solve the homogenous linear system *Aᵗ***x** = **0** to find
    a basis for *N*(*Aᵗ*), then use the [Gram-Schmidt process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process)
    on this set of basis vectors to make them orthogonal, and finally we normalize
    them into unit vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to find the left-singular vectors of *A* (the columns of *U*) is
    to compute the eigenvectors of *AAᵗ*, but this process is usually more time consuming
    than using the relationship between the left and right singular vectors (observation
    4) and computing the null space of *Aᵗ* (if necessary).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that it is also possible to start the SVD computation by finding the left-singular
    vectors (i.e., the eigenvectors of *AAᵗ*), and then use the following relationship
    to find the right singular vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/110ed3cc815bf70840c1c8596582dc58.png)'
  prefs: []
  type: TYPE_IMG
- en: The choice of using either *AAᵗ* or *AᵗA* depends on which matrix is smaller.
  prefs: []
  type: TYPE_NORMAL
- en: Numerical Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For example, let’s compute the SVD of the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7863fab4be0889643756280c8720f074.png)'
  prefs: []
  type: TYPE_IMG
- en: Let *U*Σ*Vᵗ* be the SVD of *A*. The dimensions of *A* are 3 × 2\. Therefore,
    the size of *U* is 3 × 3, the size of Σ is 3 × 2, and the size of *V* is 2 × 2.
  prefs: []
  type: TYPE_NORMAL
- en: Since the size of *AᵗA* (2 × 2) is smaller than the size of *AAᵗ* (3 × 3), it
    makes sense to start with the right-singular vectors of *A*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first compute *AᵗA*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc515d2380b0e69037162789a3e5604f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We now find the eigenvalues and eigenvectors of *AᵗA*. The characteristic polynomial
    of the matrix is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e37c8b5642652f5f68643e9c39aa19c.png)![](../Images/834eef5899cb68f07136bb858a00414e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The roots of this polynomial are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ab34c0275d4936e09527d087060f4ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The eigenvalues of *AᵗA* in descending order are *λ*₁ *=* 9 and *λ*₂ = 1\.
    Therefore, the singular values of *A* are *σ*₁ = 3 and *σ*₂ = 1, and the matrix
    Σ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3776d71f19bdc5b4fda4fdd6e39cf3d.png)'
  prefs: []
  type: TYPE_IMG
- en: We now find the right singular vectors (the columns of *V*) by finding an orthonormal
    set of eigenvectors of *AᵗA.*
  prefs: []
  type: TYPE_NORMAL
- en: 'The eigenvectors corresponding to *λ*₁ *=* 9 are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/50e703ab654f3dbddbe3d5143c556689.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the eigenvectors are of the form **v**₁ = (*t*, *t*). For a unit-length
    eigenvector we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d93b0c39847d3e4f6c57f4bcba1048b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the unit eigenvector corresponding to *λ*₁ =9 is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc3ffc48e7cca566f3f1c7df5dbd05a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the eigenvectors corresponding to *λ*₂ = 1 are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7185037c3919b7ee925753a226065c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, the eigenvectors are of the form **v**₂ = (*t*, *t*). For a unit-length
    eigenvector we need
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c32ff60f94a25186ab5863ef19258d5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the unit eigenvector corresponding to *λ*₂=1 is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/616619eddbf0b6fb58ba11aa00b67061.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now write the matrix *V*, whose columns are the vectors **v**₁ and **v**₂:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6351f3a52d7178db921d5d4f88b5d18a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, we find the left-singular vectors of *A*. From observation 4, it follows
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9beead040a784812facb1db49f382a47.png)![](../Images/486b62402fedddcdd13ede9f3eb24022.png)'
  prefs: []
  type: TYPE_IMG
- en: Since there is only one remaining column vector of *U*, instead of computing
    the kernel of *Aᵗ*, we can simplyfind a unit vector that is perpendicular to both
    **u**₁ and **u**₂.
  prefs: []
  type: TYPE_NORMAL
- en: Let **u**₃ = (*a*, *b*, *c*). To be perpendicular to **u**₂ we need *a* = *b*.
    Then the condition **u**₃*ᵗ***u**₁ = 0 becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c8519c4529d48cad0a8c973579212e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b966381dacc639dbdc3ccdbb8bdd37a.png)'
  prefs: []
  type: TYPE_IMG
- en: For the vector to be unit-length, we need
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a0d95b6f7fe2a1bf236bf24a375f126.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1ede1a29a0a754de74452845f6968ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the matrix *U* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13dc3c108f7528dd53d58bf1a21c02d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final SVD of *A* in its full glory is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37569562af67c034776853c773235120.png)'
  prefs: []
  type: TYPE_IMG
- en: Computing SVD with NumPy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To compute the SVD of a matrix using numpy, you can call the function `[np.linalg.svd](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)`.
    Given a matrix *A* with shape (*m*, *n*), the function returns a tuple (*U*, *S*,
    *Vᵗ*), where *U* is a matrix with shape (*m*, *m*) containing the left- singular
    vectors in its columns, *S* is a vector of size *k* =min(*m*, *n*) containing
    the singular values in descending order, and *Vᵗ* is a matrix with shape (*n*,
    *n*) containing the right singular vectors in its rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s use this function to compute the SVD of the matrix from
    the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output we get is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the same SVD decomposition we have obtained with our manual computation,
    up to a sign difference (e.g., the first column of *U* has flipped direction).
    This shows that an SVD decomposition of a matrix is not entirely unique. While
    the singular values themselves are unique, the associated singular vectors (i.e.,
    the columns of *U* and *V*) are not strictly unique due to the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: If a singular value is repeated, the corresponding singular vectors can be chosen
    to be any orthonormal set that spans the associated eigenspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even if the singular values are distinct, the corresponding singular vectors
    can be multiplied by -1 (i.e., their direction can be flipped) and still form
    a valid SVD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compact SVD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The compact singular value decomposition is a reduced form of the full SVD,
    which retains only the non-zero singular values and their corresponding singular
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, the compact SVD of an *m* × *n* matrix *A* with rank *r* (*r* ≤ min{*m*,
    *n*}) is a factorization of the form *A* = *Uᵣ*Σ*ᵣVᵣᵗ*, where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Uᵣ* is an *m* × *r* matrix whose columns are the first *r* left-singular vectors
    of *A*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Σ*ᵣ* is an *r* × *r* diagonal matrix with the *r* non-zero singular values on
    the diagonal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vᵣ* is an *n* × *r* matrix whose columns are the first *r* right-singular
    vectors of *A*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, the rank of the matrix from our previous example is 2, since it
    has two non-zero singular values. Therefore, its compact SVD decomposition is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d2c05c5c19fd1d7568c9164624fe7fdd.png)'
  prefs: []
  type: TYPE_IMG
- en: The matrices *Uᵣ*, Σ*ᵣ* and *Vᵣ* contain only the essential information needed
    to reconstruct the matrix *A*. The compact SVD can yield significant savings in
    storage and computation, especially for matrices with many zero singular values
    (i.e., when *r* << min{*m*, *n*}).
  prefs: []
  type: TYPE_NORMAL
- en: Truncated SVD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Truncated (reduced) SVD is a variation of SVD used for approximating the original
    matrix *A* with a matrix of a lower rank.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a truncated SVD of a matrix *A* with rank *r*, we take only the *k*
    < *r* largest singular values and their corresponding singular vectors (*k* is
    a parameter). This gives us an approximation of the original matrix *Aₖ* = *Uₖ*Σ*ₖVₖᵗ*,
    where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Uₖ* is an *m* × *k* matrix whose columns are the first *k* left-singular vectors
    of *A*, corresponding to the *k* largest singular values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Σ*ₖ* is an *k* × *k* diagonal matrix with the *k* largestsingular values on
    the diagonal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vₖ* is an *n* × *k* matrix whose columns are the first *k* right-singular
    vectors of *A*,corresponding to the *k* largest singular values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b1a390420d641eabcdfa226bb2fc661b.png)'
  prefs: []
  type: TYPE_IMG
- en: Truncated SVD
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can truncate the matrix from the previous example to have a
    rank of *k* = 1 by taking only the largest single value and its corresponding
    vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1cd29a6bf53e41bd6ccfdb35c218bcc2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In NumPy, the truncated SVD can be easily computed using the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Truncated SVD is particularly effective, since the truncated matrix *Aₖ* is
    the best rank-*k* approximation of the matrix *A* in terms of both the Frobenius
    norm (the least squares difference) and the 2-norm, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0e1bb4d47b09953c2f4f85d5300f3d0.png)'
  prefs: []
  type: TYPE_IMG
- en: This result is known as the **Eckart-Young-Mirsky theorem** or the matrix approximation
    lemma, and its proof can be found in this [Wikipedia page](https://en.wikipedia.org/wiki/Low-rank_approximation).
  prefs: []
  type: TYPE_NORMAL
- en: The choice of *k* controls the tradeoff between approximation accuracy and the
    compactness of the representation. A smaller *k* results in a more compact matrix
    but a rougher approximation. In real-world data matrices, only a very small subset
    of the singular values are large. In such cases, *Aₖ* can be a very good approximation
    of *A* by retaining the few singular values that are large.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction with Truncated SVD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the truncated SVD, it is also possible to reduce the number of dimensions
    (features) in the data matrix *A*. To reduce the dimensionality of *A* from *n*
    to *k*, we project the matrix rows onto the space spanned by the first *k* right-singular
    vectors. This is done by multiplying the original data matrix *A* by the matrix
    *Vₖ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7004da42b377d58a89dc4bb0ee1527ba.png)'
  prefs: []
  type: TYPE_IMG
- en: The reduced matrix now has dimensions *n* × *k* and contains the projection
    of the original data onto the *k*-dimensional subspace. Its columns are the new
    features in the reduced dimensional space. These features are linear combinations
    of the original features and are orthogonal to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using our previous example, we can reduce the number of dimensions of our data
    matrix *A* from 2 to 1 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98c43b99316ef3dca3c3acfcb5da6b6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another way to compute the reduced matrix is based on the following observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ee1e4598c87a05d490eb22a10379ac9.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Proof*: The full SVD of *A* is *A* = *U*Σ*Vᵗ*, therefore *AV* = *U*Σ. By comparing
    the *j*-th columns of each side of the equation we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2245c489796bac445c0173ec2094c1de.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, all the columns of *AVₖ* are equal to all the columns of *Uₖ*Σ*ₖ*,
    so the two matrices must be equal.
  prefs: []
  type: TYPE_NORMAL
- en: Using *Uₖ*Σ*ₖ* is a more efficient way to compute the reduced matrix, because
    it requires to multiply matrices of size *m* × *k* and *k* × *k*, instead of matrices
    of size *m* × *n* and *n* × *k* (*k* is typically much smaller than *n*).
  prefs: []
  type: TYPE_NORMAL
- en: 'In our previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/11ca2dc913d0b4796eef207009154bdf.png)'
  prefs: []
  type: TYPE_IMG
- en: Dimensionality reduction using truncated SVD is often used as a data preprocessing
    step before machine learning tasks such as classification or clustering, where
    it helps dealing with the [curse of dimensionality](https://medium.com/ai-made-simple/what-is-the-curse-of-dimensionality-b9b4b81a25c5),
    reduce computational costs and potentially improve the performance of the machine
    learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce the dimensionality of new data points that arrive after the machine
    learning model has been trained (e.g., samples in the test set), we simply project
    them onto the same subspace spanned by the first *k* right-singular vectors of
    *A*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/778903ab270b9e709c230fecd5fc8f55.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall that our convention is to express each vector **x** as a column vector,
    while data points are stored as rows of *A*, which is why we left-multiply **x**
    by *Vₖᵗ* instead of right multiplying it by *Vₖ*.
  prefs: []
  type: TYPE_NORMAL
- en: Reconstruction Error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A key metric in assessing the effectiveness of dimensionality reduction techniques
    is called the **reconstruction error**. It provides a quantitative measure of
    the loss of information resulting from the reduction process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure the reconstruction error of a specific vector, we first project
    it back onto the original space spanned by the *m* right-singular vectors. This
    is done by multiplying *Vₖ* by the reduced vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08f100f85c777f0b6352415cccd8c4af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We then measure the reconstruction error as the mean squared error (MSE) between
    the reconstructed components of the vector and the original components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4b8fb15bc8ea0a768763b9cf3ef1843.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also measure the reconstruction error of the entire matrix *A* by projecting
    the reduced matrix rows back onto the original space spanned by the *m* right-singular
    vectors. This is done by multiplying the reduced *A* by the transpose of *Vₖ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4db376362f03e7ae341dc5011d6781c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then use either the MSE between the elements of the reconstructed matrix
    and the original elements, or the Frobenius norm of the difference between the
    two matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf18b90e913c87acecaee380bc703b4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, the reconstructed matrix of the reduced matrix from our previous
    example is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2739756ecfa713d0dc68120496a2568.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the reconstruction error is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4141ea0e68f78cacf9658d132b0e8b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Truncated SVD in Scikit-Learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides an efficient implementation of truncated SVD in the class
    `[sklearn.decomposition.TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)`.
    Its important parameters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components`: Desired number of dimensions for the output data (defaults
    to 2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`algorithm`: The SVD solver to use. Can be one of the following options:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1\. `‘arppack’` uses the [ARPACK wrapper](https://docs.scipy.org/doc/scipy/tutorial/arpack.html)
    in SciPy to compute the eigen-decomposition of *AAᵗ* or *AᵗA* (whichever is more
    efficient). ARPACK is an iterative algorithm that efficiently computes a few eigenvalues
    and eigenvectors of large sparse matrices.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. `‘randomized’` (the default) uses a fast randomized SVD solver based on
    an algorithm by Halko et al. [1].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`n_iter`: The number of iterations for the randomized SVD solver (defaults
    to 5).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, let’s demonstrate the usage of this class on our matrix from the
    previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output we get is the reduced matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Example: Image Compression'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Singular value decomposition can be used for image compression. Although an
    image matrix is often of full rank, its lower ranks usually have very small singular
    values. Thus, truncated SVD can lead to a significant reduction in the image size
    without losing too much information.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we will demonstrate how to use truncated SVD to compress the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94b9e82024270d9e2cc85e5ee2df2eba.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo taken by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'We first load the image into a NumPy array using the function `[matplotlib.pyplot.imread](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imread.html)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape of the image is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The height of the image is 1600 pixels, its width is 1200 pixels, and it has
    3 color channels (RGB).
  prefs: []
  type: TYPE_NORMAL
- en: Since SVD can only be applied to 2D data, we can either execute it on each color
    channel separately, or we can reshape the image from a 3D matrix to a 2D matrix
    by flattening each color channel and stacking them horizontally (or vertically).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following code snippet reshapes the image into a 2D matrix
    by stacking the color channels horizontally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape of the flattened image is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The rank of the image’s matrix is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The matrix is of full rank (since min(1600, 3600) = 1600).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot the first 100 singular values of the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/abbf501b06d1601d8bbc75dac28a29e0.png)'
  prefs: []
  type: TYPE_IMG
- en: The first 100 singular values in the image
  prefs: []
  type: TYPE_NORMAL
- en: We can clearly see a rapid decay in the singular values. This decay means that
    we can effectively truncate the image without a significant loss of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s truncate the image to have a rank of 100 using Truncated
    SVD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape of the truncated image is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The size of the truncated image is only 100/3600 = 2.78% of the original image!
  prefs: []
  type: TYPE_NORMAL
- en: To see how much information was lost in the compression we can measure the image’s
    **reconstruction error**. We will measure the reconstruction error as the mean
    of squared errors (MSE) between the the pixel values of the original image and
    the reconstructed image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scikit-Learn, the reconstructed image can be obtained by calling the method
    `inverse_transform` of the `TruncatedSVD` transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, the reconstruction error is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the root mean squared error (RMSE) between the pixel intensities in the
    original image and the reconstructed image is only about 5.41 (which is small
    relative to the range of the pixels [0, 255]).
  prefs: []
  type: TYPE_NORMAL
- en: 'To display the reconstructed image, we first need to reshape it into the original
    3D shape and then clip the pixel values to integers in the range [0, 255]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now display the image using the `plt.imshow` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/02b156d5e18310d9a41d4f7f3758e108.png)'
  prefs: []
  type: TYPE_IMG
- en: The reconstructed image
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the reconstruction at rank 100 loses only a small amount of
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s place all the above steps into a function that compresses a given 3D
    image to a specified number of dimensions and then reconstructs it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now call this function with different number of components and examine
    the reconstructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/58308a6031323a4b706cbe2c7ec1fa98.png)'
  prefs: []
  type: TYPE_IMG
- en: SVD reconstructions at different ranks
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, using a rank that is too low, such as *k* = 10, can lead to a
    substantial loss of information, while an SVD of rank 200 is almost indistinguishable
    from the full-rank image.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to compressing images, SVD can also be used to remove noise from
    images. This is because discarding the lower-order components of the image tends
    to remove the granular, noisy elements, while preserving the more significant
    parts of the image.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of SVD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SVD is employed in many types of applications where it helps to uncover the
    latent features of the observed data. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Latent semantic analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)
    (LSA) is a technique in natural language processing that uncovers the latent relationships
    between words and text documents by reducing the dimensionality of text data using
    SVD.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In recommendation systems, SVD is used to factorize the user-item interaction
    matrix, revealing latent features about user preferences and item properties,
    thus helping the predictive algorithms make more accurate recommendations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SVD can be used to efficiently compute the [Moore-Penrose pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse),
    which is used in situations where a matrix is not invertible, such as computing
    the least squares solution to a linear system of equations that lacks a solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Limitations of SVD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SVD has several limitations, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Can be computationally intensive, especially for large matrices. The standard
    (non-randomized) implementation of SVD has a runtime complexity of *O*(*mn*²)
    if *m* ≥ *n*, or *O*(*m*²*n*) if *m* < *n*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Requires to store the entire data matrix in memory, which makes it impractical
    for very large datasets or real-time applications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assumes that the relationships within the data are linear, which means that
    SVD may not be able to capture more complex, nonlinear interactions between the
    variables (features).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The latent features obtained from SVD are often not easy to interpret.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Standard SVD cannot handle missing data, which means that some form of imputation
    is needed, potentially introducing biases in the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is no simple way to update the SVD incrementally when new data arrives,
    which is needed in dynamic systems where the data changes frequently (such as
    real-time recommendation systems).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Final Notes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the images are by the author unless stated otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/svd](https://github.com/roiyeho/medium/tree/main/svd)'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Halko, N., Martinsson, P. G., & Tropp, J. A. (2011). Finding structure
    with randomness: Probabilistic algorithms for constructing approximate matrix
    decompositions. *SIAM review*, 53(2), 217–288.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Singular value decomposition, *Wikipedia, The Free Encyclopedia,* [https://en.wikipedia.org/wiki/Singular_value_decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)'
  prefs: []
  type: TYPE_NORMAL
