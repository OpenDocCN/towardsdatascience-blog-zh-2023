- en: How Can AI Make Old Videos Look Smoother?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能如何使旧视频看起来更流畅？
- en: 原文：[https://towardsdatascience.com/how-can-ai-make-old-videos-look-smoother-5ad8b70fdd64](https://towardsdatascience.com/how-can-ai-make-old-videos-look-smoother-5ad8b70fdd64)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-can-ai-make-old-videos-look-smoother-5ad8b70fdd64](https://towardsdatascience.com/how-can-ai-make-old-videos-look-smoother-5ad8b70fdd64)
- en: Video frame interpolation has come a long way and AI is taking it in some strange
    directions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视频帧插值技术已经取得了长足的进展，而人工智能正在将其引向一些奇怪的方向。
- en: '[](https://mikhailklassen.medium.com/?source=post_page-----5ad8b70fdd64--------------------------------)[![Mikhail
    Klassen](../Images/9c4a6cc856fd4061f682e95a1c145c36.png)](https://mikhailklassen.medium.com/?source=post_page-----5ad8b70fdd64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ad8b70fdd64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ad8b70fdd64--------------------------------)
    [Mikhail Klassen](https://mikhailklassen.medium.com/?source=post_page-----5ad8b70fdd64--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mikhailklassen.medium.com/?source=post_page-----5ad8b70fdd64--------------------------------)[![Mikhail
    Klassen](../Images/9c4a6cc856fd4061f682e95a1c145c36.png)](https://mikhailklassen.medium.com/?source=post_page-----5ad8b70fdd64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ad8b70fdd64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ad8b70fdd64--------------------------------)
    [Mikhail Klassen](https://mikhailklassen.medium.com/?source=post_page-----5ad8b70fdd64--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ad8b70fdd64--------------------------------)
    ·7 min read·Jan 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ad8b70fdd64--------------------------------)
    ·阅读时长7分钟·2023年1月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: It was back in 2015 and I was creating some data visualizations for a meeting
    with my PhD supervisory committee. This was towards the end of my graduate studies
    in astrophysics and I had run some supercomputer simulations of a massive star
    forming inside an interstellar cloud of gas.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 那是在2015年，我正在为与我的博士生导师委员会的会议创建一些数据可视化。这是在我研究生阶段接近尾声的时候，我运行了一些超级计算机模拟，模拟了一个在星际气体云中形成的大质量恒星。
- en: 'I figured the best way to show off the results of these simulations was to
    create various animations of the data. My analysis code had chewed through gigabytes
    of simulation data and dumped image files at specific time intervals. So I used
    `ffmpeg` to stitch them together into an animation:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为展示这些模拟结果的最佳方式是创建各种数据动画。我的分析代码处理了数吉字节的模拟数据，并在特定时间间隔生成了图像文件。因此，我使用 `ffmpeg`
    将它们拼接成动画：
- en: '![](../Images/36d672e41b3f0e33b72b15a2cb91d823.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36d672e41b3f0e33b72b15a2cb91d823.png)'
- en: Video of an astrophysics simulation showing the evolution of a protostellar
    disk around a forming star. Author’s own work. See [Klassen et al. (2016)](https://iopscience.iop.org/article/10.3847/0004-637X/823/1/28/meta).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个展示原恒星盘演化过程的天体物理学模拟视频。作者自制。请参阅 [Klassen et al. (2016)](https://iopscience.iop.org/article/10.3847/0004-637X/823/1/28/meta)。
- en: I always wanted higher frame rates, so I could have smoother and more beautiful
    animation, but I was limited by the data I had from my simulation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直希望能拥有更高的帧率，以便获得更流畅、更美观的动画，但我受限于模拟中获得的数据。
- en: Sometimes I would spend a few hours seeing if I could perhaps generate intermediate
    frames to make my movies *appear* smoother when I compiled them. My attempts at
    the time were clumsy and ultimately abandoned, but today there are many techniques
    that work very well. In fact, most modern TVs include the option (on by default)
    to artificially enhance the frame rate of the video.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我会花几个小时尝试生成中间帧，以使我的影片在编译时*看起来*更流畅。那时我的尝试笨拙且最终被放弃，但今天已经有许多技术效果非常好。事实上，大多数现代电视都包括（默认开启的）选项，以人工增强视频的帧率。
- en: The formal name for this is **video frame interpolation** (VFI) and it turns
    out there’s quite a body of research in this domain.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这项技术的正式名称是**视频帧插值**（VFI），结果发现这一领域已有相当多的研究。
- en: What is video frame interpolation?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是视频帧插值？
- en: Video footage is typically assembled from individual still frames. If the frame
    rate is low, the video can appear “choppy”. To make the video appear smoother,
    various techniques have been developed for artificially inserting intermediate
    frames between the original frames of the video.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 视频画面通常是由单独的静态帧组装而成的。如果帧率较低，视频可能会显得“跳跃”。为了让视频看起来更流畅，已经开发了各种技术来人工插入原始帧之间的中间帧。
- en: In the example below, a short anime clip has had its frame rate upscaled 8 times
    using an AI-based technique. The result is a much smoother video.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，一个短动漫剪辑的帧率通过AI技术提升了8倍。结果是视频更加平滑。
- en: Let’s explore a few of these techniques. We’ll start with some approaches that
    don’t require artificial intelligence and end with some of the state-of-the-art
    approaches.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨这些技术。我们将从一些不需要人工智能的方法开始，最后介绍一些最先进的方法。
- en: As our source materials, we’ll use a 6-second clip of a train locomotive by
    the filmmaker [Pat Whelen](https://www.pexels.com/@pat-whelen-2913248/), uploaded
    to the free stock photo and video site [Pexels](https://www.pexels.com/video/wheels-and-connecting-rods-of-a-moving-train-5708282/).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们的源材料，我们将使用由电影制片人[Pat Whelen](https://www.pexels.com/@pat-whelen-2913248/)拍摄的6秒钟火车机车剪辑，上传到免费的股票照片和视频网站[Pexels](https://www.pexels.com/video/wheels-and-connecting-rods-of-a-moving-train-5708282/)。
- en: '![](../Images/cdcd1f7c859b4108d98f26d0fdb9d5ae.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdcd1f7c859b4108d98f26d0fdb9d5ae.png)'
- en: 'Original 23 fps clip of a train locomotive. Credit: [Pat Whelen, Pexels](https://www.pexels.com/video/wheels-and-connecting-rods-of-a-moving-train-5708282/).
    Free to use.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 原始23 fps的火车机车剪辑。感谢：[Pat Whelen, Pexels](https://www.pexels.com/video/wheels-and-connecting-rods-of-a-moving-train-5708282/)。免费使用。
- en: The original video is recorded in high resolution at a framerate of 23 frames
    per second. In general, we will aim to double the framerate during our interpolations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 原始视频以23帧每秒的高分辨率录制。一般来说，我们的目标是在插值过程中将帧率翻倍。
- en: Frame interpolation techniques
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 帧插值技术
- en: Frame averaging
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 帧平均
- en: 'One of the simplest ways of adding more frames is to blend two sequential frames
    to create an average of the two. This is easy to do. I used `ffmpeg` to perform
    the interpolation using the `minterpolate` filter:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 添加更多帧的最简单方法之一是将两个连续的帧混合，创建这两帧的平均值。这很容易做到。我使用了`ffmpeg`来执行插值，使用了`minterpolate`滤镜：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: More details on the filter settings can be [found here](https://ffmpeg.org/ffmpeg-filters.html#minterpolate).
    The result is below, converted to an animated gif.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有关滤镜设置的更多细节可以在[这里找到](https://ffmpeg.org/ffmpeg-filters.html#minterpolate)。结果如下，转换为动画gif。
- en: '![](../Images/6081dc36e68a7e4548fcd8201c819ecb.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6081dc36e68a7e4548fcd8201c819ecb.png)'
- en: Source video frame rate enhanced using video frame blending.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 源视频帧率通过视频帧混合技术得到了提升。
- en: If you look carefully, you can see from the resulting video that artifacts appear
    wherever there is motion. If an object moves too much between frames, then the
    blended intermediate frame very obviously shows the object in both places and
    appearing semi-transparent where the motion is.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仔细观察，你可以从结果视频中看到，任何运动的地方都会出现伪影。如果物体在帧之间移动过多，那么混合的中间帧会非常明显地显示物体出现在两个位置，并且在运动的地方呈半透明状。
- en: To overcome this problem, researchers developed motion estimation techniques,
    which improves the quality of the interpolation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，研究人员开发了运动估计技术，以提高插值的质量。
- en: Motion estimation
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运动估计
- en: To create intermediate frames that don’t look like just the blurred average
    of the two other frames, motion-compensated frame interpolation was developed.
    This approach works by identifying objects in sequential frames from similar visual
    features and calculating the path taken by those objects. This creates a vector
    “motion field” that an interpolation algorithm can exploit to improve the quality
    of the intermediate frames.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建看起来不仅仅是两个其他帧模糊平均值的中间帧，开发了运动补偿帧插值。这种方法通过识别连续帧中的物体，从类似的视觉特征计算这些物体的运动路径。这样就创建了一个向插值算法可以利用的向量“运动场”。
- en: '![](../Images/704d3387f1cb81bf74bf1002e0f5d3f7.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/704d3387f1cb81bf74bf1002e0f5d3f7.png)'
- en: A figure from [Choi & Ko (2010)](https://www.researchgate.net/publication/224145279_Hierarchical_motion_estimation_algorithm_using_reliable_motion_adoption),
    showing the motion field overlaid on a video frame.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Choi & Ko (2010)](https://www.researchgate.net/publication/224145279_Hierarchical_motion_estimation_algorithm_using_reliable_motion_adoption)的图示，显示了覆盖在视频帧上的运动场。
- en: 'Several motion estimation algorithms have been implemented into `ffmpeg` over
    the years, and so we’ll use one called “overlapped block motion compensation”
    (OBMC). Here is the result:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，多个运动估计算法已被实现到`ffmpeg`中，我们将使用一种称为“重叠块运动补偿”（OBMC）的算法。结果如下：
- en: '![](../Images/0cee5b25f83f344bdb122cc98829d9ba.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cee5b25f83f344bdb122cc98829d9ba.png)'
- en: Source video frame rate enhanced using a motion interpolation technique called
    overlapped block motion compensation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用名为重叠块运动补偿的运动插值技术增强了源视频的帧率。
- en: 'While the quality of the resulting video is much better, it’s still possible
    to get minor artifacts in the video. To achieve the above video, I again relied
    on the `minterpolate` filter in `ffmpeg`, but with a few different settings:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管生成的视频质量要好得多，但视频中仍可能出现微小的伪影。为了实现上述视频，我再次依赖于`ffmpeg`中的`minterpolate`滤镜，但使用了一些不同的设置：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Frame interpolation using artificial intelligence
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用人工智能的帧插值
- en: The area of computer vision has been making rapid progress with the application
    of new artificial intelligence techniques. Deep neural networks can perform highly
    accurate image segmentation, object detection, and even 3D depth estimation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉领域正在通过新人工智能技术的应用迅速发展。深度神经网络可以执行高度准确的图像分割、物体检测甚至3D深度估计。
- en: The previous technique relied on estimating the motion of objects between frames.
    These objects were detected using visual features. Convolutional neural networks
    (CNNs) are great extracting visual features from an image, so it seems reasonable
    to apply them here.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的技术依赖于估计帧之间物体的运动。这些物体是通过视觉特征检测的。卷积神经网络（CNNs）在从图像中提取视觉特征方面表现出色，因此在这里应用它们似乎是合理的。
- en: Next, if we could know a little more about the 3D nature of the world represented
    by the image, then we could better estimate the movement of different parts of
    the image and produce more accurate intermediate frames.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果我们能更多地了解图像所表示的世界的3D特性，那么我们可以更好地估计图像中不同部分的运动，从而生成更准确的中间帧。
- en: The real game-changer here is adding depth estimation. A neural network trained
    to predict the distance to parts of an image achieves this.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的真正改变者是增加了深度估计。一种训练有素的神经网络可以预测图像中各部分的距离。
- en: The animation below shows the results from a paper titled “[Digging Into Self-Supervised
    Monocular Depth Estimation](https://arxiv.org/abs/1806.01260)” (ICCV 2019, [Code](https://github.com/nianticlabs/monodepth2)).
    A deep neural network accurately estimates, frame by frame, which parts of the
    image represent objects that are closer vs further away.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下方的动画展示了一篇名为“[深入探讨自监督单目深度估计](https://arxiv.org/abs/1806.01260)”（ICCV 2019, [代码](https://github.com/nianticlabs/monodepth2)）的论文中的结果。深度神经网络准确地逐帧估计图像中哪些部分代表了较近的物体与较远的物体。
- en: That this technique works well shouldn’t come as too much of a surprise. This
    is something that our brains do intuitively, even with when taking away parallax
    information by closing one eye. Using both eyes improves the accuracy even further.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的良好效果并不令人感到意外。这是我们的大脑本能地做的事情，即使是在闭上一只眼睛去除视差信息时也是如此。使用双眼可以进一步提高准确性。
- en: '![](../Images/18dc85e2e2a0d3cfa861590a7793ff1a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18dc85e2e2a0d3cfa861590a7793ff1a.png)'
- en: Depth-aware video frame interpolation techniques, such as the [DAIN algorithm](https://arxiv.org/abs/1904.00830)
    introduced in 2019 by Bao et al., perform better than previous approaches because
    they can account for occlusion, i.e. when objects pass behind each other.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 深度感知视频帧插值技术，如2019年由Bao等人提出的[DAIN算法](https://arxiv.org/abs/1904.00830)，比以前的方法表现更好，因为它们可以考虑遮挡情况，即当物体互相经过时。
- en: More recently, the “[Real-time Intermediate Flow Estimation](https://arxiv.org/abs/2011.06294)”
    (RIFE) technique by Huang et al. (ECCV 2022) achieved superior benchmark scores
    while also being much faster to run. It probably represents the current state
    of the art at this time of writing.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，黄等人（ECCV 2022）提出的“[实时中间流估计](https://arxiv.org/abs/2011.06294)”（RIFE）技术取得了卓越的基准成绩，同时运行速度也快得多。这可能代表了目前的最先进技术。
- en: '![](../Images/774419b7400a15f0ad68f7c20ee8a91b.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/774419b7400a15f0ad68f7c20ee8a91b.png)'
- en: 'An example video clip from the RIFE paper showing 16x interpolation of 2 input
    images: [https://github.com/megvii-research/ECCV2022-RIFE](https://github.com/megvii-research/ECCV2022-RIFE)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 来自RIFE论文的一个示例视频片段，展示了2张输入图像的16倍插值：[https://github.com/megvii-research/ECCV2022-RIFE](https://github.com/megvii-research/ECCV2022-RIFE)
- en: '![](../Images/4ae318fc18734ebd7074fe11cd537cd1.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ae318fc18734ebd7074fe11cd537cd1.png)'
- en: The depth estimation performed by RIFE from the same input.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: RIFE对相同输入进行的深度估计。
- en: 'Let’s conclude our exercise by applying the RIFE algorithm to our New York
    City scene. I clone the [GitHub repository](https://github.com/megvii-research/ECCV2022-RIFE)
    to my laptop, downloaded the pretrained model parameters as described in the repo’s
    ReadMe file, and made a few tweaks to the `requirements.txt` file. I ran the code
    on my laptop without the help of a GPU. To process 6 seconds of video footage
    took about 15minutes. Here are the results:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将 RIFE 算法应用于纽约市场景来总结我们的练习。我将[GitHub 仓库](https://github.com/megvii-research/ECCV2022-RIFE)克隆到我的笔记本电脑上，下载了
    repo 的 ReadMe 文件中描述的预训练模型参数，并对`requirements.txt`文件进行了几处调整。我在没有 GPU 帮助的情况下在笔记本电脑上运行了代码。处理
    6 秒的视频素材大约花费了 15 分钟。以下是结果：
- en: '![](../Images/ed1785fa4a2dec0692e6858fce783966.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed1785fa4a2dec0692e6858fce783966.png)'
- en: Video interpolation using a novel deep CNN technique, “real-time intermediate
    flow estimation” (RIFE).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新颖的深度 CNN 技术进行视频插值，“实时中间流估计”（RIFE）。
- en: It looks even better in the higher-resolution mp4 video that RIFE created. For
    this article, I converted that video to an animated gif.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RIFE 创建的高分辨率 mp4 视频中效果更佳。对于本文，我将该视频转换成了动画 gif。
- en: Final thoughts
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终想法
- en: This field is moving very quickly. Image generation AIs trained on billions
    of images from the internet can already produce highly consistent, novel imagery
    from a sequence of input images or text-based prompts.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个领域发展非常迅速。训练于互联网上数十亿图像的图像生成 AI 已经能够从一系列输入图像或基于文本的提示中生成高度一致的、新颖的图像。
- en: 'Other techniques extend depth estimation approaches to create point clouds
    from multiple camera angles and then synthesize new artificial perspectives using
    a technique called neural rendering. See, for example, [Rückert et al. (2021)](https://arxiv.org/abs/2110.06635)
    and their neural rendering approach:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其他技术将深度估计方法扩展到从多个摄像机角度创建点云，然后使用称为神经渲染的技术合成新的人工视角。举例来说，[Rückert 等（2021）](https://arxiv.org/abs/2110.06635)及其神经渲染方法：
- en: '![](../Images/d5649da822126e79774534e8c2ec56e8.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5649da822126e79774534e8c2ec56e8.png)'
- en: Neural rendering in [Rückert et al. (2021)](https://arxiv.org/abs/2110.06635).
    Images on the left are generated from a sequence of ground-truth (GT) input images
    (bottom right).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[Rückert 等（2021）](https://arxiv.org/abs/2110.06635)中的神经渲染。左侧的图像是从一系列真实图像（右下角）生成的。'
- en: As the field continues to advance, we can expect greater accuracy of the intermediate
    frames with fewer artifacts, even as more and more of the frames are essentially
    “dreamt up” or hallucinated by very large pre-trained artificial neural networks.
    Relative few input frames are needed to produce highly believable, smooth transitions
    between them
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这一领域的持续发展，我们可以期待中间帧的准确性更高，伪影更少，即使更多的帧本质上是由非常大的预训练人工神经网络“梦到”或幻觉出来的。相对较少的输入帧即可在它们之间产生高度逼真、流畅的过渡。
- en: This opens up a lot of creative possibilities, such as the restoration of old
    archival videos, animating still images, and creating virtual reality worlds from
    a handful of 2D photographs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这打开了很多创造性的可能性，比如恢复旧档案视频、为静态图像制作动画，以及从少量二维照片中创建虚拟现实世界。
- en: It may also lead to the flooding of the world with so much synthetic imagery
    that it becomes difficult to distinguish what represents ground truth. In the
    meantime, expect your videos to look really smooth.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可能导致世界上充斥着大量合成图像，使得区分什么是实际真相变得困难。同时，期待你的视频看起来非常流畅。
- en: '![](../Images/c3c4a41ab7c9954339fd12974668fbfe.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3c4a41ab7c9954339fd12974668fbfe.png)'
- en: My original research data visualization (see above), passed through the RIFE
    algorithm.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我的原始研究数据可视化（见上文），经过了 RIFE 算法处理。
- en: If you enjoy reading stories like these and want to support me as a writer,
    consider signing up to become a Medium member. It’s $5 a month, which gives you
    access to all my writing and that of thousands of other writers. If you sign up
    using [my link](https://mikhailklassen.medium.com/membership), I’ll earn a small
    commission with no extra cost to you.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢阅读这样的故事并希望支持我作为作者，可以考虑注册成为 Medium 会员。每月 $5，你可以访问我和其他成千上万名作家的所有作品。如果你通过[我的链接](https://mikhailklassen.medium.com/membership)注册，我将获得一小笔佣金，而你无需额外付费。
- en: '[](https://mikhailklassen.medium.com/membership?source=post_page-----5ad8b70fdd64--------------------------------)
    [## Join Medium with my referral link — Mikhail Klassen'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mikhailklassen.medium.com/membership?source=post_page-----5ad8b70fdd64--------------------------------)
    [## 通过我的推荐链接加入 Medium — Mikhail Klassen'
- en: Read every story from Mikhail Klassen (and thousands of other writers on Medium).
    Your membership fee directly supports…
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读米哈伊尔·克拉森（Mikhail Klassen）以及在 Medium 上的其他数千名作家的每一个故事。你的会员费直接支持…
- en: mikhailklassen.medium.com](https://mikhailklassen.medium.com/membership?source=post_page-----5ad8b70fdd64--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[mikhailklassen.medium.com](https://mikhailklassen.medium.com/membership?source=post_page-----5ad8b70fdd64--------------------------------)'
