["```py\n%%capture ##cell magic command that captures and discards all the stdout and stderr from the cell in which it's used\n!pip install datasets\n!pip install transformers\n!pip install evaluate\n```", "```py\nfrom datasets import load_dataset\n\ntwitter = load_dataset(\"GateNLP/broad_twitter_corpus\")\n```", "```py\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 5342\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 2002\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 2002\n    })\n})\n```", "```py\nlabels = {\n    0:'O',\n    1:'B-PER',\n    2:'I-PER',\n    3:'B-ORG',\n    4:'I-ORG',\n    5:'B-LOC',\n    6:'I-LOC',\n  }\n```", "```py\nfrom transformers import AutoModelForTokenClassification\n\nbert_ner = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(labels))\n```", "```py\nbert_ner.config.id2label = labels\nbert_ner.config.label2id = {v: k for k, v in labels.items()}\n```", "```py\nfrom transformers import pipeline, AutoTokenizer\nfrom evaluate import evaluator\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\npipe = pipeline('token-classification', model='bert-base-cased', tokenizer=tokenizer, device=device_id)\npipe([\"Zachary Raicik works for Corvus and lives in San Diego\"], aggregation_strategy=\"average\")\n```", "```py\n[[{'entity_group': 'LABEL_0',\n   'score': 0.66933894,\n   'word': 'Zachary Raicik works for Corvus and lives in San',\n   'start': 0,\n   'end': 48},\n  {'entity_group': 'LABEL_1',\n   'score': 0.5502231,\n   'word': 'Diego',\n   'start': 49,\n   'end': 54}]]\n```", "```py\ndef tokenize_and_tag(row):\n  tokens, ner_tags = row[\"tokens\"], row[\"ner_tags\"]\n\n  sub_tokens, labels = [], []\n  for token, tag in zip(tokens, ner_tags):\n      token_sub_tokens = tokenizer.tokenize(token)\n      sub_tokens.extend(token_sub_tokens)\n      labels.extend([tag] * len(token_sub_tokens))\n\n  sub_tokens = ['[CLS]'] + sub_tokens + ['[SEP]']\n  labels = [-100] + labels + [-100]\n\n  padding_length = tokenizer.model_max_length - len(sub_tokens)\n  sub_tokens = sub_tokens + ['[PAD]'] * padding_length\n  labels = labels + [-100] * padding_length\n\n  input_ids = tokenizer.convert_tokens_to_ids(sub_tokens)\n  attention_mask = [1 if token != '[PAD]' else 0 for token in sub_tokens]\n  token_type_ids = [0] * tokenizer.model_max_length  \n\n  row[\"bert_tokens\"] = sub_tokens\n  row[\"input_ids\"] = input_ids\n  row[\"attention_mask\"] = attention_mask\n  row[\"token_type_ids\"] = token_type_ids\n  row[\"labels\"] = labels\n\n  return row\n```", "```py\ntrain_twitter = twitter['train'].map(tokenize_and_tag)\ntest_twitter = twitter['test'].map(tokenize_and_tag)\nvalidation_twitter =twitter['validation'].map(tokenize_and_tag)\n```", "```py\nimport numpy as np #Used to mask predictions and labels\nfrom transformers import TrainingArguments, Trainer #Training \nfrom evaluate import load #Used to load required performance metrics during training \n```", "```py\n# Freeze ALL model parameters\nfor param in bert_ner.parameters():\n    param.requires_grad = False\n\n# Unfreeze the last 5 layers\nfor param in bert_ner.bert.encoder.layer[-1:].parameters():\n    param.requires_grad = True\n```", "```py\ntraining_args = TrainingArguments(\n    evaluation_strategy=\"epoch\",\n    output_dir=\"twitter-training\",\n    learning_rate=5e-05, \n    num_train_epochs=5.0,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n)\n```", "```py\nmetric = evaluate.load(\"f1\") # Use evaluate.combine if you want multiple metrics\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n\n    predictions = np.argmax(logits, axis=-1)\n\n    valid_mask = np.array(labels) != -100 \n    valid_labels = labels[valid_mask]\n\n    valid_predictions = predictions[valid_mask]\n\n    return metric.compute(predictions=valid_predictions, references=valid_labels,average='weighted')\n```", "```py\ntrainer = Trainer(\n    model=bert_ner,\n    args=training_args,\n    train_dataset=train_twitter,\n    eval_dataset=validation_twitter,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\ntrainer.save_model(\"twitter-training-mdl\")\n```", "```py\nfrom transformers import pipeline\nfrom evaluate import evaluator\n\npipe = pipeline('token-classification', model='twitter-training-mdl', tokenizer=tokenizer, device=device_id)\n\npipe([\"Zachary Raicik works for Corvus and lives in San Diego\"], aggregation_strategy=\"average\")\n```", "```py\n[[{'entity_group': 'PER',\n   'score': 0.8900693,\n   'word': 'Zachary Raicik',\n   'start': 0,\n   'end': 14},\n  {'entity_group': 'ORG',\n   'score': 0.534402,\n   'word': 'Corvus',\n   'start': 25,\n   'end': 31},\n  {'entity_group': 'LOC',\n   'score': 0.7905616,\n   'word': 'San Diego',\n   'start': 45,\n   'end': 54}]]\n```"]