- en: A Comprehensive Overview of Regression Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-comprehensive-overview-of-regression-evaluation-metrics-6264af0926db](https://towardsdatascience.com/a-comprehensive-overview-of-regression-evaluation-metrics-6264af0926db)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/77a024aef30e9daead7e900ad76280bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using icons from [icons8](https://icons8.com/)
  prefs: []
  type: TYPE_NORMAL
- en: An extensive reference into commonly used regression evaluation metrics and
    their practical applications across various scenarios
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eryk-lewinson.medium.com/?source=post_page-----6264af0926db--------------------------------)[![Eryk
    Lewinson](../Images/56e09e19c0bbfecc582da58761d15078.png)](https://eryk-lewinson.medium.com/?source=post_page-----6264af0926db--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6264af0926db--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6264af0926db--------------------------------)
    [Eryk Lewinson](https://eryk-lewinson.medium.com/?source=post_page-----6264af0926db--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6264af0926db--------------------------------)
    ·15 min read·May 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: As a data scientist, evaluating the performance of machine learning models is
    a crucial aspect of your work. To do so effectively, you have a wide range of
    statistical metrics at your disposal, each with its own unique strengths and weaknesses.
    By developing a solid understanding of these metrics, you are not only better
    equipped to choose the best one for optimizing your model but also to explain
    your choice and its implications to business stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I focus on metrics that are used to evaluate regression problems
    which predict numeric values — such as the price of a house or a forecast of a
    company’s sales for next month. Since regression analysis is considered to be
    the foundation of data science, it is essential to understand its nuances.
  prefs: []
  type: TYPE_NORMAL
- en: A quick primer on residuals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Residuals are the building blocks of the majority of the metrics. In simple
    terms, a residual is a difference between the actual value and the predicted one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The following figure presents the relationship between a target variable (*y*)
    and a single feature (*x*). The blue dots represent observations. The red line
    is the fit of a machine learning model, in this case, a linear regression. The
    orange lines represent the differences between the observed values and the predictions
    for those observations. As such, residuals can be calculated for each observation
    in the dataset, be it the training or test set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/356b81e83a3513f71b5a25e3e422e269.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1\. Example of residuals in a linear model with one feature*'
  prefs: []
  type: TYPE_NORMAL
- en: Regression evaluation metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section discusses some of the most popular regression evaluation metrics
    that can help you assess the effectiveness of your model.
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simplest error measure would be the sum of residuals, sometimes referred
    to as bias. As the residuals can be both positive (prediction is smaller than
    the actual value) and negative (prediction is larger than the actual value), bias
    generally tells us whether our predictions were higher or lower than the actuals.
  prefs: []
  type: TYPE_NORMAL
- en: However, as the residuals of opposing signs offset each other, we can obtain
    a model that generates predictions with a very low bias, while not being accurate
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can calculate the average residual, or *mean bias error* (MBE).
  prefs: []
  type: TYPE_NORMAL
- en: R-squared
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next metric is probably the first one you encounter while learning about
    regression models, especially if that is during statistics or econometrics classes.
    *R-squared*(R²), also known as the coefficient of determination, represents the
    proportion of variance explained by a model. To be more precise, R² corresponds
    to the degree to which the variance in the dependent variable (the target) can
    be explained by the independent variables (features).
  prefs: []
  type: TYPE_NORMAL
- en: The following formula is used to calculate R².
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bec0d09162fb4bcbdcfa481c3c1c8a01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: RSS is the residual sum of squares, which is the sum of squared residuals. This
    value captures the prediction error of a model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TSS is the total sum of squares. To calculate this value, first we assume a
    simple model in which the prediction for each observation is the mean of all the
    observed actuals. TSS is proportional to the variance of the dependent variable,
    as TSS/N is the actual variance of *y* where *N* is the number of observations*.*
    That is why we can think of TSS as the variance that a simple mean model cannot
    explain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effectively, we are comparing the fit of a model (represented by the red line
    in Figure 2) to that of a simple mean model (represented by the green line).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3710edf168b1704dd64a2052e0ed0a76.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2\. Comparing the fit of a linear model to a simple mean benchmark*'
  prefs: []
  type: TYPE_NORMAL
- en: Knowing what the components of R² stand for, we can see that RSS/TSS represents
    the fraction of the total variance in the target that our model was not able to
    explain.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few additional points to keep in mind when working with R².
  prefs: []
  type: TYPE_NORMAL
- en: First of all, R² is a relative metric, that is, it can be used to compare with
    other models trained on the same dataset. A higher value indicates a better fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'R² can also be used to get a rough estimate of how the model performs in general.
    However, we should be careful when using R² for such evaluations:'
  prefs: []
  type: TYPE_NORMAL
- en: First, different fields (social sciences, biology, finance, and more) consider
    different values of R² as good or bad.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, R² does not give any measure of bias, so we can have an overfitted (highly
    biased) model with a high value of R². As such, we should also look at other metrics
    to get a good understanding of a model’s performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A potential drawback of R² is that it assumes that every feature helps in explaining
    the variation in the target, when that might not always be the case. For this
    reason, if we continue adding features to a linear model estimated using ordinary
    least squares (OLS), the value of R² might increase or remain unchanged, but it
    never decreases.
  prefs: []
  type: TYPE_NORMAL
- en: Why? By design, OLS estimation minimizes the RSS. Suppose a model with an additional
    feature does not improve the value of R² of the first model. In that case, the
    OLS estimation technique sets that feature’s coefficients to zero (or some statistically
    insignificant value). In turn, this effectively brings us back to the initial
    model. In the worst-case scenario, we can get the score of our starting point.
  prefs: []
  type: TYPE_NORMAL
- en: A solution to the problem mentioned in the previous point is the adjusted R²,
    which additionally penalizes adding features that are not useful for predicting
    the target. The value of the adjusted R² decreases if the increase in the R² caused
    by adding new features is not significant enough.
  prefs: []
  type: TYPE_NORMAL
- en: As the last point, we left the often misunderstood issue of R²’s range of values.
    If a linear model is fitted using OLS, the range of R² is 0 to 1\. That is because
    when using the OLS estimation (which minimizes the RSS), the general property
    is that *RSS ≤ TSS.* In the worst-case scenario, OLS estimation would result in
    obtaining the mean model. In that case, RSS would be equal to TSS and result in
    the minimum value of R² being 0\. On the other hand, the best case would be RSS
    = 0 and R² = 1.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of non-linear models, it is possible that R² is negative. As the
    model fitting procedure of such models is not based on iteratively minimizing
    the RSS, the fitted model could have an RSS greater than the TSS. In other words,
    the model’s predictions fit the data worse than the simple mean model. For more,
    information, see [When is R squared negative?](https://stats.stackexchange.com/questions/12900/when-is-r-squared-negative)
  prefs: []
  type: TYPE_NORMAL
- en: '*Bonus*: Using R², we can evaluate how much better our model fits the data
    as compared to the simple mean model. We can think of a positive R² value in terms
    of improving the performance of a baseline model — something along the lines of
    a skill score. For example, R² of 40% indicates that our model has reduced the
    mean squared error by 40% compared to the baseline mean model.'
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Mean squared error* (MSE) is one of the most popular evaluation metrics. As
    shown in the following formula, MSE is closely related to the residual sum of
    squares. The difference is that we are now interested in the average error instead
    of the total error.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b217c2e88140bbd58b0c959ca54381d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are some points to take into account when working with MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: MSE uses the mean (instead of the sum) to keep the metric independent of the
    dataset size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the residuals are squared, MSE puts a significantly heavier penalty on large
    errors. Some of those might be outliers, so MSE is not robust to their presence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the metric is expressed using squares, sums, and constants (1/N) it is differentiable.
    This is useful for optimization algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While optimizing for MSE (setting its derivative to 0), the model aims for the
    total sum of predictions to be equal to the total sum of actuals. That is, it
    leads to predictions that are correct on average. Therefore, they are unbiased.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MSE is not measured in the original units, which can make it harder to interpret.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MSE is an example of a scale-dependent metric, that is, the error is expressed
    in the units of the underlying data (even though it actually needs a square root
    to be expressed on the same scale). Therefore, such metrics cannot be used to
    compare the performance between different datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Root mean squared error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Root mean squared error* (RMSE) is closely related to MSE, as it is simply
    the square root of the latter. By taking the square we bring the metric back to
    the scale of the target variable, so it is easier to interpret and understand.
    However, one fact that is often overlooked is that although RMSE is on the same
    scale as the target, an RMSE of 10 does not actually mean we are off by 10 units
    on average.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75d46b7d334fbaf5d13c5cde45b72546.png)'
  prefs: []
  type: TYPE_IMG
- en: Other than the scale, RMSE has the same properties as MSE. As a matter of fact,
    optimizing for RMSE while training a model will result in the same model as that
    obtained while optimizing for MSE.
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The formula to calculate *mean absolute error* (MAE) is similar to the MSE formula.
    We simply have to replace the square with the absolute value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75977f451d32d69c2454441c5fac56c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Characteristics of MAE include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the lack of squaring, the metric is expressed at the same scale as the
    target variable, making it easier to interpret.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All errors are treated equally, so the metric is robust to outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Absolute value disregards the direction of the errors, so underforecasting =
    overforecasting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to MSE and RMSE, MAE is also scale-dependent, so we cannot compare it
    between different datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you optimize for MAE, the prediction must be as many times higher than
    the actual value as it should be lower. That means that we are effectively looking
    for the median; that is, a value that splits a dataset into two equal parts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the formula contains absolute values, MAE is not easily differentiable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean absolute percentage error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Mean absolute percentage error* (MAPE) is one of the most popular metrics
    on the business side. That is because it is expressed as a percentage, which makes
    it much easier to understand and interpret.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6695b92fcc981929de25eb03c7d683fb.png)'
  prefs: []
  type: TYPE_IMG
- en: To make the metric even easier to read, we can multiply it by 100% to express
    the number as a percentage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Points to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: MAPE is expressed as a percentage, which makes it a scale-independent metric.
    It can be used to compare predictions on different scales.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAPE can exceed 100%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAPE is undefined when the actuals are zero (division by zero). Additionally,
    it can take extreme values when the actuals are very close to zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAPE is asymmetric and puts a heavier penalty on negative errors (when predictions
    are higher than actuals) than on positive ones. This is caused by the fact that
    the percentage error cannot exceed 100% for forecasts that are too low. Meanwhile,
    there is no upper limit for forecasts that are too high. As a result, optimizing
    for MAPE will favor models that underforecast rather than overforecast.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyndman (2021) elaborates on the often-forgotten assumption of MAPE, that is,
    the unit of measurement of the variable has a meaningful zero value. As such,
    forecasting demand and using MAPE do not raise any red flags. However, we will
    encounter that problem when forecasting temperature expressed on the Celsius scale
    (and not only that one). That is because the temperature has an arbitrary zero
    point and it does not make sense to talk about percentages in their context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAPE is not differentiable everywhere, which can result in problems while using
    it as the optimization criterion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As MAPE is a relative metric, the same error can result in a different loss
    depending on the actual value. For example, for a predicted value of 60 and an
    actual of 100, the MAPE would be 40%. For a predicted value of 60 and an actual
    of 20, the nominal error is still 40, but on the relative scale, it is 300%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, MAPE does not provide a good way to differentiate the important
    from the irrelevant. Assume we are working on demand forecasting and over a horizon
    of a few months, we get a MAPE of 10% for two different products. Then, it turns
    out that the first product sells an average of 1 million units per month, while
    the other only 100\. Both have the same 10% MAPE. When aggregating over all products,
    those two would contribute equally, which can be far from desirable. In such cases,
    it makes sense to consider weighted MAPE (wMAPE).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Symmetric mean absolute percentage error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While discussing MAPE, I mentioned that one of its potential drawbacks is its
    asymmetry (not limiting the predictions that are higher than the actuals). *Symmetric
    mean absolute percentage error* (sMAPE) is a related metric that attempts to fix
    that issue.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2515d0171cb1876af60e131a9fb6b0a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Points to consider when using sMAPE:'
  prefs: []
  type: TYPE_NORMAL
- en: It is expressed as a bounded percentage, that is, it has lower (0%) and upper
    (200%) bounds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metric is still unstable when both the true value and the forecast are very
    close to zero. When it happens, we will deal with division by a number very close
    to zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The range of 0% to 200% is not intuitive to interpret. Dividing by two in the
    denominator is often omitted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whenever the actual value or the forecast has a value is 0, sMAPE will automatically
    hit the upper boundary value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sMAPE includes the same assumptions as MAPE regarding a meaningful zero value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While fixing the asymmetry of boundlessness, sMAPE introduces another kind of
    delicate asymmetry caused by the denominator of the formula. Imagine two cases.
    In the first one, we have A = 100 and F = 120\. The sMAPE is 18.2%. Now a similar
    case, in which we have A = 100 and F = 80, the sMAPE is 22.2%. As such, sMAPE
    tends to penalize underforecasting more severely than overforecasting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sMAPE might be one of the most controversial error metrics, especially in time
    series forecasting. That is because there are at least a few versions of this
    metric in the literature, each one with slight differences that impact its properties.
    Finally, the name of the metric suggests that there is no asymmetry, but that
    is not the case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other regression evaluation metrics to consider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I have not described all the possible regression evaluation metrics, as there
    are dozens (if not hundreds). Here are a few more metrics to consider while evaluating
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mean squared log error* (MSLE) is a cousin of MSE, with the difference that
    we take the log of the actuals and predictions before calculating the squared
    error. Taking the logs of the two elements in subtraction results in measuring
    the ratio or relative difference between the actual value and the prediction,
    while neglecting the scale of the data. That is why MSLE reduces the impact of
    outliers on the final score. MSLE also puts a heavier penalty on underforecasting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Root mean squared log error* (RMSLE) is a metric that takes the square root
    of MSLE. It has the same properties as MSLE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Akaike information criterion* (AIC) and *Bayesian information criterion* (BIC)
    are examples of information criteria. They are used to find a balance between
    a good fit and the complexity of a model. If we start with a simple model with
    a few parameters and add more, our model will probably fit the training data better.
    However, it will also grow in complexity and risk overfitting. On the other hand,
    if we start with many parameters and systematically remove some of them, the model
    becomes simpler. At the same time, we reduce the risk of overfitting at the potential
    cost of losing on performance (goodness of fit). The difference between AIC and
    BIC is the weight of the penalty for complexity. Keep in mind that it is not valid
    to compare the information criteria on different datasets or even subsamples of
    the same dataset but with a different number of observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to use each evaluation metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with the majority of data science problems, there is no single best metric
    for evaluating the performance of a regression model. The metric chosen for a
    use case will depend on the data used to train the model, the business case we
    are trying to help, and so on. For this reason, we might often use a single metric
    for the training of a model (the metric optimized for), but when reporting to
    stakeholders, data scientists often present a selection of metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'While choosing the metrics, consider a few of the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Do you expect frequent outliers in the dataset? If so, how do you want to account
    for them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a business preference for overforecasting or underforecasting?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you want a scale-dependent or scale-independent metric?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I believe it is useful to explore the metrics on some toy cases to fully understand
    their nuances. While most of the metrics are available in the `metrics` module
    of `scikit-learn`, for this particular task, the good old spreadsheets might be
    a more suitable tool.
  prefs: []
  type: TYPE_NORMAL
- en: The following example contains five observations. Table 1 shows the actual values,
    predictions, and some metrics used to calculate most of the considered metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28e8794d3ab108b4fe3f61d7ebde8c36.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 1\. Example of calculating the performance metrics on five observations*'
  prefs: []
  type: TYPE_NORMAL
- en: The first three rows contain scenarios in which the absolute difference between
    the actual value and the prediction is 20\. The first two rows show an overforecast
    and underforecast of 20, with the same actual. The third row shows an overforecast
    of 20, but with a smaller actual. In those rows, it is easy to observe the particularities
    of MAPE and sMAPE.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d34d552c080b4dc1d936c99bfe079a2c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 2\. Performance metrics calculated using the values in Table 1*'
  prefs: []
  type: TYPE_NORMAL
- en: The fifth row in Table 1 contained a prediction 8x smaller than the actual value.
    For the sake of experimentation, replace that prediction with one that is 8x higher
    than the actual. Table 3 contains the revised observations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da10e3c33b6639ce067b85532a2cc7bd.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 3\. Performance metrics after modifying a single observation to be more
    extreme*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2cdc6584f9ba1f6961e7bb65ddbd8c33.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4\. Performance metrics calculated using the modified values in Table
    3*'
  prefs: []
  type: TYPE_NORMAL
- en: Basically, all metrics exploded in size, which is intuitively consistent. That
    is not the case for sMAPE, which stayed the same between both cases.
  prefs: []
  type: TYPE_NORMAL
- en: I highly encourage you to play around with such toy examples to more fully understand
    how different kinds of scenarios impact the evaluation metrics. This experimentation
    should make you more comfortable making a decision on which metric to optimize
    for and the consequences of such a choice. These exercises might also help you
    explain your choices to stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, I covered some of the most popular regression evaluation metrics.
    As explained, each one comes with its own set of advantages and disadvantages.
    And it is up to the data scientist to understand those and make a choice about
    which one (or more) is suitable for a particular use case. The metrics mentioned
    can also be applied to pure regression tasks — such as predicting salary based
    on a selection of features connected to experience — but also to the domain of
    time series forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: As always, any constructive feedback is more than welcome. You can reach out
    to me on [Twitter](https://twitter.com/erykml1) or in the comments.
  prefs: []
  type: TYPE_NORMAL
- en: '*Liked the article? Become a Medium member to continue learning by reading
    without limits. If you use* [*this link*](https://eryk-lewinson.medium.com/membership)
    *to become a member, you will support me at no extra cost to you. Thanks in advance
    and see you around!*'
  prefs: []
  type: TYPE_NORMAL
- en: 'You might also be interested in one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/dealing-with-outliers-using-three-robust-linear-regression-models-544cfbd00767?source=post_page-----6264af0926db--------------------------------)
    [## Dealing with Outliers Using Three Robust Linear Regression Models'
  prefs: []
  type: TYPE_NORMAL
- en: With a hands-on example of using Huber, RANSAC and Theil-Sen regression algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/dealing-with-outliers-using-three-robust-linear-regression-models-544cfbd00767?source=post_page-----6264af0926db--------------------------------)
    [](/verifying-the-assumptions-of-linear-regression-in-python-and-r-f4cd2907d4c0?source=post_page-----6264af0926db--------------------------------)
    [## Verifying the Assumptions of Linear Regression in Python and R
  prefs: []
  type: TYPE_NORMAL
- en: Dive deeper into the Gauss-Markov Theorem and other assumptions of linear regression!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/verifying-the-assumptions-of-linear-regression-in-python-and-r-f4cd2907d4c0?source=post_page-----6264af0926db--------------------------------)
    [](/interpreting-the-coefficients-of-linear-regression-cc31d4c6f235?source=post_page-----6264af0926db--------------------------------)
    [## Interpreting the coefficients of linear regression
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to correctly interpret the results of linear regression — including
    cases with transformations of variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/interpreting-the-coefficients-of-linear-regression-cc31d4c6f235?source=post_page-----6264af0926db--------------------------------)
    [](/choosing-the-correct-error-metric-mape-vs-smape-5328dec53fac?source=post_page-----6264af0926db--------------------------------)
    [## Choosing the correct error metric: MAPE vs. sMAPE'
  prefs: []
  type: TYPE_NORMAL
- en: The pros and cons of two popular error metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/choosing-the-correct-error-metric-mape-vs-smape-5328dec53fac?source=post_page-----6264af0926db--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jadon, A., Patil, A., & Jadon, S. (2022). A Comprehensive Survey of Regression
    Based Loss Functions for Time Series Forecasting. *arXiv preprint arXiv:2211.02989*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyndman, R. J. (2006). Another look at forecast-accuracy metrics for intermittent
    demand. *Foresight: The International Journal of Applied Forecasting*, *4*(4),
    43–46.'
  prefs: []
  type: TYPE_NORMAL
- en: Hyndman, R. J., & Koehler, A. B. (2006). Another look at measures of forecast
    accuracy. *International journal of forecasting*, *22*(4), 679–688.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyndman, R.J., & Athanasopoulos, G. (2021) *Forecasting: principles and practice*,
    3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3.'
  prefs: []
  type: TYPE_NORMAL
- en: All images, unless noted otherwise, are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*NVIDIA’s developer blog*](https://developer.nvidia.com/blog/a-comprehensive-overview-of-regression-evaluation-metrics/)
    *on April 20th, 2023*'
  prefs: []
  type: TYPE_NORMAL
