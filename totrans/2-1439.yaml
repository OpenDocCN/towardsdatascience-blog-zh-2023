- en: Load Testing Simplified With SageMaker Inference Recommender
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/load-testing-simplified-with-sagemaker-inference-recommender-b96746b69292](https://towardsdatascience.com/load-testing-simplified-with-sagemaker-inference-recommender-b96746b69292)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Test TensorFlow ResNet50 on SageMaker Real-Time Endpoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----b96746b69292--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----b96746b69292--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b96746b69292--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b96746b69292--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----b96746b69292--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b96746b69292--------------------------------)
    ·7 min read·Mar 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df9ae88a89ce233ff5f795f837fb53bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Unsplash](https://unsplash.com/photos/o4WaeT3XhV4) by [**Amokrane
    Ait-Kaci**](https://unsplash.com/@amokraneaitk)
  prefs: []
  type: TYPE_NORMAL
- en: In the past I’ve written extensively about the importance of [load testing](/why-load-testing-is-essential-to-take-your-ml-app-to-production-faab0df1c4e1)
    your Machine Learning models before deploying them into production. When it comes
    to real-time inference use-cases in specific it’s essential to ensure your solution
    meets your target latency and throughput. We’ve also explored how we can use the
    Python library, [Locust](https://locust.io/) to define scripts that can simulate
    our expected traffic patterns.
  prefs: []
  type: TYPE_NORMAL
- en: While Locust is an incredibly powerful tool, it can be difficult to setup and
    also requires a lot of iterations across the different hyperparameters and hardware
    you may be testing to identify the proper configuration for production. For [SageMaker
    Real-Time Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html),
    a key tool to take a look at is [SageMaker Inference Recommender](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html).
    Rather than having to repeatedly run a Locust script across different configurations,
    you can essentially pass in an array of EC2 instance types to test your endpoint,
    as well as hyperparameters for your specific model container for more advanced
    deployments. In today’s blog we’ll take a look at how we can configure this feature
    and how we can simplify load testing SageMaker Real-Time endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**: This article will assume basic knowledge of AWS, SageMaker, and Python.
    To understand what SageMaker Real-Time Inference is please take a look at the
    following starter [blog](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/).'
  prefs: []
  type: TYPE_NORMAL
- en: Setup & Locally Test Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For development you can utilize either a SageMaker Classic Notebook Instance
    or a SageMaker Studio Kernel. For our environment we utilized a TensorFlow 2.0
    Kernel with Python3 on a ml.t3.medium base instance.
  prefs: []
  type: TYPE_NORMAL
- en: The model we will be utilizing today is a pre-trained [TensorFlow ResNet50](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50)
    for Image Classification. We can first retrieve this model from the TensorFlow
    model hub and pull it into our notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Before we get to testing on SageMaker we want to locally test this model so
    we can get an idea of the type of input format we need to configure for our endpoint.
    For our sample data point, we will use a picture of my dog Milo when he was a
    puppy (he’s a behemoth now).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46e9a8cf71551b16a89d46b3141bc0a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Milo (Picture by Author)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e4708dc24bf777ae7a5479714f5cd68e.png)'
  prefs: []
  type: TYPE_IMG
- en: Model Results (Screenshot by Author)
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now verified the format in which our model expects inference so we can
    focus on getting it configured for SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Prepare Model and Payload
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SageMaker Inference Recommender expects two mandatory inputs: the model data
    and a sample payload. For both it expects it in a tarball format so we take our
    artifacts and zip them in a format that the service will understand.'
  prefs: []
  type: TYPE_NORMAL
- en: For our model we can either take the model we already loaded earlier in the
    notebook or instantiate a new version. We download the model artifacts into a
    local directory with the necessary metadata for TensorFlow Serving.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can then tar this into a model.tar.gz and upload it to an S3 bucket that
    we can point Inference Recommender to.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We then take our sample image and convert it into a JSON for our model and save
    this to a tarball in a similar manner as we did for the model artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our inputs configured we can move onto the SageMaker portion
    of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Create SageMaker Model & Track With Model Registry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SageMaker has a few specific objects specific to its service, an important
    one for us in this case is the [SageMaker Model](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html)
    entity. This model entity consists of two core factors: model data and container/image.
    The model data can be your trained or pre-trained model artifacts that you provide
    in an S3 Bucket. The container is essentially the framework of your model. In
    this case we can [retrieve](https://aws.plainenglish.io/how-to-retrieve-amazon-sagemaker-deep-learning-images-ff4a5866299e)
    the managed SageMaker TensorFlow image, but you can also build and push your own
    container if it’s unsupported by [AWS Deep Learning Containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).
    Here we define this SageMaker [Model object](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#),
    utilizing the [SageMaker Python SDK](/sagemaker-python-sdk-vs-boto3-sdk-45c424e8e250).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: An optional step that you can utilize is registering your model with [SageMaker
    Model Registry](/register-and-deploy-models-with-sagemaker-model-registry-5af42d678912).
    Tracking hundreds of models can be a difficult process and with Model Registry
    you can essentially simplify model versioning and lineage so that you have all
    model entities in one core space. We can register a model with the following API
    call.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can also view the model package that we just created in the SageMaker Studio
    Console.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d0c146b9c3e90bdeb42b526f1a62499a.png)'
  prefs: []
  type: TYPE_IMG
- en: Model Package (Screenshot by Author)
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world use-case you may have multiple models within a singular model
    package and you can approve the one that you choose to deploy to production.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our Model object prepared we can run an Inference Recommender
    Job on this entity.
  prefs: []
  type: TYPE_NORMAL
- en: Inference Recommender Job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two types of Inference Recommender Jobs: Default and [Advanced](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-load-test.html).
    With a Default Job, we can simply pass in our sample payload along with an array
    of EC2 instances that you want to test your model against. Underneath the hood,
    Inference Recommender will test your model against all these instances and track
    throughput and latency for your. We can utilize the [right_size](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.right_size)
    API call to kick off an Inference Recommender Job.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This job will take approximately 35–40 minutes to complete as it will iterate
    across the different instance types that you have provided. We can then view these
    results in the SageMaker Studio UI.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5cd28ba093d5a8c81dc468fd7e996f60.png)'
  prefs: []
  type: TYPE_IMG
- en: Default Job Results (Screenshot by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Here you can toggle cost, latency, and throughput on importance levels and get
    the optimal hardware configuration. You can also directly create your endpoint
    from the console if you are happy with the performance shown by the tests.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, if you want to test different hyperparameters for your container, this
    is also available through the Advanced Inference Recommender Job. Here you can
    specify hyperparameters that are tunable for your specific model container.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Along with this, you can also configure the traffic patterns for your load test.
    For example, if you want to scale up the number of users across different intervals
    you can configure this behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can also set thresholds, for example if you have a strict latency requirement
    of 200ms, this can be set as a stopping parameter if your configuration is not
    achieving these results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You can then kick off and view the results of the Advanced Job in a similar
    fashion to the default job.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Additional Resources & Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://github.com/aws-samples/sagemaker-inference-recommender-examples/blob/main/tf-resnet/inference-recommender-tf-resnet.ipynb?source=post_page-----b96746b69292--------------------------------)
    [## sagemaker-inference-recommender-examples/inference-recommender-tf-resnet.ipynb
    at main ·…'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to aws-samples/sagemaker-inference-recommender-examples development
    by creating an account on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/aws-samples/sagemaker-inference-recommender-examples/blob/main/tf-resnet/inference-recommender-tf-resnet.ipynb?source=post_page-----b96746b69292--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for this example and more at the link above. SageMaker
    Inference Recommender is a powerful tool that can automate the difficult portion
    of load testing setup. It’s important to note, however that at the moment there
    is no support for advanced hosting options such as [Multi-Model and Multi-Container
    Endpoints](/sagemaker-multi-model-vs-multi-container-endpoints-304f4c151540),
    so for those use-cases utilizing a third party framework such as [Locust](/load-testing-sagemaker-multi-model-endpoints-f0db7b305770)
    will be necessary. As always any feedback is appreciated and feel free to reach
    out with any questions or comments, thank you for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '*If you enjoyed this article feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *and subscribe to my Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*.
    If you’re new to Medium, sign up using my* [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*.*'
  prefs: []
  type: TYPE_NORMAL
