- en: Another (Conformal) Way to Predict Probability Distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/another-conformal-way-to-predict-probability-distributions-fcc63e78680d](https://towardsdatascience.com/another-conformal-way-to-predict-probability-distributions-fcc63e78680d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Conformal multi-quantile regression with Catboost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://harrisonfhoffman.medium.com/?source=post_page-----fcc63e78680d--------------------------------)[![Harrison
    Hoffman](../Images/5eaa3e2bd0507297eb6c4a7efcf06324.png)](https://harrisonfhoffman.medium.com/?source=post_page-----fcc63e78680d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fcc63e78680d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fcc63e78680d--------------------------------)
    [Harrison Hoffman](https://harrisonfhoffman.medium.com/?source=post_page-----fcc63e78680d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fcc63e78680d--------------------------------)
    ·11 min read·Mar 8, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01cf92583ef3188b5333a9ebc8b32e34.png)'
  prefs: []
  type: TYPE_IMG
- en: Texas. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a [previous article](https://medium.com/towards-data-science/a-new-way-to-predict-probability-distributions-e7258349f464),
    we explored the capabilities of Catboost’s multi-quantile loss function, which
    allows for the prediction of multiple quantiles using a single model. This approach
    elegantly overcomes one of the limitations of traditional quantile regression,
    which necessitates the development of a separate model for each quantile, or storing
    the entire training set in the model. However, there is another disadvantage to
    quantile regression, which we will discuss in this article: the potential for
    predicted quantiles to be biased, leaving no guarantees of calibration and coverage.
    This article will demonstrate a way to overcome this with conformal multi-quantile
    regression. I would encourage anyone who hasn’t been following this series to
    refer back to the following articles before reading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-new-way-to-predict-probability-distributions-e7258349f464?source=post_page-----fcc63e78680d--------------------------------)
    [## A New Way to Predict Probability Distributions'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring multi-quantile regression with Catboost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-new-way-to-predict-probability-distributions-e7258349f464?source=post_page-----fcc63e78680d--------------------------------)
    [](/understanding-noisy-data-and-uncertainty-in-machine-learning-4a2995a84198?source=post_page-----fcc63e78680d--------------------------------)
    [## Understanding Noisy Data and Uncertainty in Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: The actual reason your machine learning model isn’t working
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/understanding-noisy-data-and-uncertainty-in-machine-learning-4a2995a84198?source=post_page-----fcc63e78680d--------------------------------)
    [](/how-to-predict-risk-proportional-intervals-with-conformal-quantile-regression-175775840dc4?source=post_page-----fcc63e78680d--------------------------------)
    [## How To Predict Risk-Proportional Intervals With “Conformal Quantile Regression”
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm — published in 2019 by Stanford scholars — combines quantile
    regression with conformal prediction. Here…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.co](/how-to-predict-risk-proportional-intervals-with-conformal-quantile-regression-175775840dc4?source=post_page-----fcc63e78680d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Recap: Why Multi-Quantile Regression?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-quantile regression enables us to use a single moded to predict multiple
    target quantiles. Because there is no computational constraint necessitating one
    model per quantile, or the limitation of storing the entire training set in the
    model (e.g. KNN, Quantile Regression Forests), we can efficiently predict more
    quantiles and get a better feel for how the conditional target distribution looks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using traditional quantile regression, generating a 95% prediction interval
    would require one model for the 2.5th quantile, one for the 97.5th quantile, and
    possibly a third for the expected value or the 50th quantile. A single prediction
    from each model would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7be767b1a5b1e2a071524aeb2d961a12.png)'
  prefs: []
  type: TYPE_IMG
- en: Predicted [CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function)
    samples for a single test example (three independent quantile models). Image by
    Author.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming these quantiles are calibrated, they reveal a few insights. The first
    is the probability that the target is less than or equal to 3.6, *given the features,*
    is around 0.50 or 50%. Similarly, the probability that the target value is between
    3.25 and 4.38, given the features, is roughly 0.95 or 95%.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the models’ output is good and precisely what we required, we may want
    to adjust our risk tolerance dynamically. For instance, what if we need to be
    more conservative and require a 99% prediction interval? Similarly, what if we
    are more risk-seeking and can tolerate a 90% or 80% prediction interval? What
    if we want answers to questions like “given the features, what is the probability
    that the target is greater than y1?”. We might also want to ask questions like
    “given the features, what is the probability that the target is between y1 and
    y2?”. Multi-quantile regression facilities answering these questions by predicting
    as many quantiles as specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/71df22072ad003a551251cd98f8f619c.png)'
  prefs: []
  type: TYPE_IMG
- en: Predicted [CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function)
    samples for a single test example (one multi-quantile model). Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: The more quantiles that can be accurately predicted, the more the risk tolerance
    can be adjusted on the fly, and the more we can answer general probability questions
    about the conditional target distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Note that single decision tree models have [been used](https://scikit-garden.github.io/examples/QuantileRegressionForests/)
    to generate multiple quantile predictions. However, this relies on the trees storing
    [all target values in the leaf nodes](https://scikit-garden.github.io/examples/QuantileRegressionForests/#:~:text=also%20all%20the%20target%20values%20in%20the%20leaf%20node.).
    At prediction time, a quantile is specified and computed empirically from the
    data in the leaf node, requiring the model to store the entire training set. This
    also means deep trees could have very few examples to work with in the leaf nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Catboost is fundamentally different because it only stores the number of specified
    quantiles in the terminal nodes. Moreover, the loss function is optimized to predict
    each specified quantile. We also enjoy the [performance gains](/catboost-vs-lightgbm-vs-xgboost-c80f40662924#:~:text=However%2C%20generally%2C%20from%20the%20literature%2C%20XGBoost%20and%20LightGBM%20yield%20similar%20performance%2C%20with%20CatBoost%20and%20LightGBM%20performing%20much%20faster%20than%20XGBoost%2C%20especially%20for%20larger%20datasets.)
    Catboost offers with its underlying architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The Problem with Quantile Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In traditional and multi-quantile regression, [there isn’t always a statistical
    guarantee](https://arxiv.org/pdf/1905.03222.pdf) that quantiles are unbiased.
    This means, for a model trained to predict the 95th quantile of the target distribution,
    there’s no guarantee that 95% of observations will actually be less than or equal
    to the prediction. This is problematic in high-risk applications where accurate
    probability representations are required to make critical decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Quantile regression can also produce prediction intervals that are [too conservative](https://arxiv.org/pdf/1905.03222.pdf)
    and subsequently uninformative. In general, prediction intervals should be as
    narrow as possible while maintaining the desired coverage level.
  prefs: []
  type: TYPE_NORMAL
- en: Conformal Multi-Quantile Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The idea behind conformal quantile regression is to adjust predicted quantiles
    to accurately reflect the desired risk tolerance and interval length. This is
    accomplished through a “calibration” step that computes “conformity scores” to
    correct the predicted quantiles. More details about conformal quantile regression
    can be found in [this paper](https://arxiv.org/pdf/1905.03222.pdf) and [this article](/how-to-predict-risk-proportional-intervals-with-conformal-quantile-regression-175775840dc4).
    For conformal multi-quantile regression, we will utilize the following theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b04c51891814e8109f7dd4e1dcc6a51c.png)'
  prefs: []
  type: TYPE_IMG
- en: Left and Right Tail Conformal Quantile Regression. [Source](https://arxiv.org/pdf/1905.03222.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Don’t worry if this seems overly abstract, the steps are actually straight
    forward:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a training, calibration, and testing set. Fit the multi-quantile model
    on the training set to predict all quantiles of interest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make predictions on the calibration set. For each calibration instance and predicted
    quantile, compute the difference between the predicted quantile and the corresponding
    target value. These are the conformity scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each testing example and predicted quantile (say q), subtract the 1-q quantile
    of the conformity scores corresponding to quantile q from the predicted quantile
    of the model. These are the new predicted quantiles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can implement this logic in a python class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Example: The Superconductivity Dataset'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll implement conformal multi-quantile regression on the [superconductivity
    dataset](https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data) available
    on the UCI Machine Learning Repository. This dataset provides 21,263 instances
    of 81 [superconductor](https://en.wikipedia.org/wiki/Superconductivity) features
    with their [critical temperature](https://link.springer.com/article/10.1007/s42452-020-03266-0#:~:text=The%20critical%20temperature%20is%20the,critical%20temperature%20is%20mostly%20intuitive.)
    (the target). The data is split so that ~64% is allocated for training, ~16% for
    calibration, and 20% for testing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We’ll specify a set of quantiles to predict. To illustrate the power of multi-quantile
    regression, the model will predict 200 quantiles from 0.005 to 0.99 — this is
    probably a bit excessive in practice. Next, we’ll **fit** the conformal multi-quantile
    model, make **uncalibrated predictions**, **calibrate** the model on the **calibration
    set**, and make **calibrated predictions**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting predictions should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07a90a10f8b5e439e10c8bf353f4d6ca.png)'
  prefs: []
  type: TYPE_IMG
- en: First few predicted quantiles for the first 5 observations. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the testing set, we can measure how well the uncalibrated and calibrated
    predictions align with the left-tail probability they’re intended to represent.
    For instance, if the quantiles are calibrated, 40% of target values should be
    less than or equal to predicted quantile 0.40, 90% of target values should be
    less than or equal to predicted quantiles 0.90, etc. The code below computes the
    mean absolute error (MAE) between the desired left-tail probability and the actual
    left-tail probability encompassed by the predicted quantiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The uncalibrated quantiles were off by about 0.026 and the calibrated quantiles
    by 0.008, on average. Hence, the calibrated quantiles were more aligned with the
    desired left-tail probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4c9d44ddb0b11485c9d1ce9f9ef6610.png)'
  prefs: []
  type: TYPE_IMG
- en: Actual vs Predicted Quantile Left-Tail Probabilities. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'This may not seem like a dramatic difference in calibration, however, the error
    in the uncalibrated model is made more clear by analyzing actual v.s. desired
    coverage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b38fa421c4d1f198b0c2d0c9ceac0fde.png)'
  prefs: []
  type: TYPE_IMG
- en: Actual vs Desired Coverage. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: The uncalibrated model tends to be too conservative and covers more examples
    than desired. The calibrated model, on the other hand, exhibits near-perfect alignment
    with each of the desired coverages.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the average length of the prediction intervals generated by the calibrated
    model is less than that of the uncalibrated model. Thus, not only is coverage
    better in the calibrated model, the prediction intervals are more informative.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/709c27217b03610a8832d2b49467e50f.png)'
  prefs: []
  type: TYPE_IMG
- en: Average Prediction Interval Length by Desired Coverage. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'One might ask what happens if we allow the uncalibrated model to include the
    calibration set as training data. This makes sense in practice because we wouldn’t
    throw away good training data for no reason. Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Even with less training data than the uncalibrated model, the calibrated model
    outputs better quantiles. What’s more, the models perform similarly when we compare
    the expected values of the predicted quantiles to the target values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are no silver bullets in machine learning, and conformal quantile regression
    is no exception. The glue that holds conformal prediction theory together is the
    assumption that the underlying data is [exchangeable](https://en.wikipedia.org/wiki/Exchangeable_random_variables).
    If, for instance, the distribution of the data drifts over time (which is usually
    the case in many real world applications), then conformal prediction can no longer
    make strong probability guarantees. There are [ways around](https://arxiv.org/pdf/2202.13415.pdf)
    this assumption, but these methods ultimately depend on the severity of data drift
    and the nature of the learning problem. It may also be less than ideal to set
    aside valuable training data for calibration.
  prefs: []
  type: TYPE_NORMAL
- en: As always, the machine learning practitioner is responsible for understanding
    the nature of the data and applying appropriate techniques. Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '*Become a Member:* [*https://harrisonfhoffman.medium.com/membership*](https://harrisonfhoffman.medium.com/membership)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Catboost Loss Functions —* [https://catboost.ai/en/docs/concepts/loss-functions-regression#MultiQuantile](https://catboost.ai/en/docs/concepts/loss-functions-regression#MultiQuantile)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Conformalized Quantile Regression* — [https://arxiv.org/pdf/1905.03222.pdf](https://arxiv.org/pdf/1905.03222.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Conformal Prediction Beyond Exchangeability* — [https://arxiv.org/pdf/2202.13415.pdf](https://arxiv.org/pdf/2202.13415.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*The Superconductivity Dataset* — [https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data](https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*How to Predict Risk-Proportional Intervals with Conformal Quantile Regression*
    — [https://towardsdatascience.com/how-to-predict-risk-proportional-intervals-with-conformal-quantile-regression-175775840dc4](/how-to-predict-risk-proportional-intervals-with-conformal-quantile-regression-175775840dc4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*How to Predict Full Probability Distributions using Machine Learning Conformal
    Prediction* — [https://valeman.medium.com/how-to-predict-full-probability-distribution-using-machine-learning-conformal-predictive-f8f4d805e420](https://valeman.medium.com/how-to-predict-full-probability-distribution-using-machine-learning-conformal-predictive-f8f4d805e420)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
