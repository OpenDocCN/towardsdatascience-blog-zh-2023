["```py\nfrom transformers import pipeline\n\npipe = pipeline(model=\"facebook/bart-large-mnli\")\npipe(\"Tesco Semi Skimmed Milk 1.13L/2 Pints ...... £1.30\",\n    candidate_labels=[\"groceries\", \"utility\", \"electronics\", \"subscriptions\"],\n)\n\n# output\n>> {'sequence': 'Tesco Semi Skimmed Milk Pints',\n 'labels': ['groceries', 'utility', 'subscriptions', 'electronics'],\n 'scores': [0.9199661612510681,\n  0.05123506113886833,\n  0.022794339805841446,\n  0.0060044946148991585]}\n```", "```py\nimport re\n\npattern = r\"(?:\\b\\w+\\b|['\\\"“”‘’])\"\nurl_pattern = re.compile(r'http\\S+|www\\S+')\npunct_pattern = re.compile(r'[^\\w\\s]')\ndigit_pattern = re.compile(r'\\d+')\nnon_ascii_pattern = re.compile(r'[^\\x00-\\x7F]+')\n\ndef clean_text(text):\n\n    # Convert to lowercase\n    text = text.lower()\n    # Remove URLs\n    text = url_pattern.sub('', text)\n    # Remove punctuation\n    text = punct_pattern.sub(' ', text)\n    # Remove digits\n    text = digit_pattern.sub('', text)\n    # Remove non-ASCII characters\n    text = non_ascii_pattern.sub('', text)\n    # Tokenize words and remove single characters\n    words = [word for word in re.findall(pattern, text) if len(word) > 1]\n    return ' '.join(words)\n\n# Example Output  \ntext = \"This is an example text with a URL https://www.example.com, some #hashtag.\"\ncleaned_text = clean_text(text)\nprint(cleaned_text)\n>> \"this is an example text with url some hashtag\"\n\n# Apply it to both the training and test data.\ntrain_df[\"clean_desc\"] = train_df[\"item_description\"].apply(lambda x: clean_text(x))\ntest_df[\"clean_desc\"] = test_df[\"item_description\"].apply(lambda x: clean_text(x))\n```", "```py\nfrom sentence_transformers import SentenceTransformer\n\n# Load SentenceTransformer model\nmodel = SentenceTransformer('paraphrase-mpnet-base-v2')\n\n# Example Output\nsentence = ['This is a sample sentence for encoding.']\nembedding = model.encode(sentence)\nprint(embedding)\n>> [[ 4.99693975e-02 -1.26025528e-01 -9.15094614e-02  1.19477045e-02\n   9.89145786e-02  9.02947485e-02  1.74566925e-01  1.84450839e-02\n  -1.04984418e-01 ....... 8.46698135e-02  8.69197398e-03 -1.48386151e-01]]\n\n# Generate embeddings for the training data\ntrain_desc = train_df['clean_desc'].tolist()\ntrain_embeddings = model.encode(train_desc)\nX_train = np.array(train_embeddings)\n```", "```py\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\n\n# Encode the target variable as categorical with one-hot encoding\nle = LabelEncoder()\ny_train = le.fit_transform(train_set['category'])\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Define the parameter grid for grid search cross-validation\nparam_grid = {\n    'max_depth': [7],\n    'n_estimators': [300],\n    'learning_rate': [ 0.2, 0.5],\n}\n\n# Train an XGBoost model with grid search cross-validation\nxgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=4)\ngrid_search = GridSearchCV(xgb_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=0)\ngrid_search.fit(X_train, y_train)\n# -----------------------------------------------------------------\n\n# Print the best hyperparameters found by grid search\nprint(\"Best hyperparameters:\", grid_search.best_params_)\n\n# Make predictions on the validation data using the best model\ny_pred = grid_search.predict(X_val)\n\n# Decode the predicted target variable\ny_pred = le.inverse_transform(y_pred)\n\n# Evaluate the performance of the model\naccuracy = np.mean(y_pred == le.inverse_transform(y_val))\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Output\n>> Best hyperparameters: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 300}\n>> Accuracy: 0.94\n```", "```py\nimport pickle\n# Save the trained model to a file\nfilename = 'xgb_model.sav'\npickle.dump(grid_search, open(filename, 'wb'))\n\n# Load the model from file\nloaded_model = pickle.load(open(filename, 'rb'))\n\n# Define function to make predictions on new data\ndef predict_gl_code(text):\n    # Preprocess the text data\n    text = clean_text(text)\n\n    # Generate embeddings for the text data\n    embeddings = model.encode([text], show_progress_bar=False)\n    X = np.array(embeddings)\n\n    # Make predictions using the loaded model\n    y_pred = loaded_model.predict(X)\n    gl_code = le.inverse_transform(y_pred)[0]\n    return gl_code\n\n# Add a new column for predictions to the test dataframe\ntest_set['preds'] = test_set['clean_desc'].apply(predict_gl_code)\n\n# Compute the accuracy of the model on the test data\naccuracy = np.mean(test_set['preds'] == test_set['category'])\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Output \n>> Accuracy: 0.91\n```"]