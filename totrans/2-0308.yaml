- en: 'Anatomy of LLM-Based Chatbot Applications: Monolithic vs. Microservice Architectural
    Patterns'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºäºLLMçš„èŠå¤©æœºå™¨äººåº”ç”¨ç¨‹åºçš„ç»“æ„ï¼šå•ä½“æ¶æ„ä¸å¾®æœåŠ¡æ¶æ„æ¨¡å¼
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e](https://towardsdatascience.com/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e](https://towardsdatascience.com/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e)
- en: A Practical Guide to Building Monolithic and Microservice Chatbot Applications
    with Streamlit, Huggingface, and FastAPI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç”¨æŒ‡å—ï¼šä½¿ç”¨Streamlitã€Huggingfaceå’ŒFastAPIæ„å»ºå•ä½“å’Œå¾®æœåŠ¡èŠå¤©æœºå™¨äººåº”ç”¨ç¨‹åº
- en: '[](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)[![Marie
    Stephen Leo](../Images/c5669d884da5ff5c965f98904257d379.png)](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)
    [Marie Stephen Leo](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)[![Marie
    Stephen Leo](../Images/c5669d884da5ff5c965f98904257d379.png)](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)
    [Marie Stephen Leo](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)
    Â·9 min readÂ·May 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)
    Â·9åˆ†é’Ÿé˜…è¯»Â·2023å¹´5æœˆ8æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5b4d2660c860f9c8391e33823a5f1824.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b4d2660c860f9c8391e33823a5f1824.png)'
- en: 'Image generated by Author using Midjourney V5.1 using the prompt: â€œisometric
    highly realistic view of a laptop, screen has the picture of a bright, multi colored
    rubikâ€™s cube that is illuminated from within, bright, warm, cheerful lighting.
    8k, hdr, unreal engineâ€'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…ä½¿ç”¨Midjourney V5.1ç”Ÿæˆï¼Œæç¤ºè¯ä¸ºï¼šâ€œç­‰è·çš„é«˜ç°å®æ„Ÿç¬”è®°æœ¬ç”µè„‘è§†å›¾ï¼Œå±å¹•ä¸Šæ˜¾ç¤ºä¸€ä¸ªæ˜äº®çš„ã€å¤šå½©çš„é­”æ–¹ï¼Œå†…éƒ¨è¢«ç…§äº®ï¼Œæ˜äº®ã€æ¸©æš–ã€æ„‰å¿«çš„å…‰çº¿ã€‚8kï¼Œhdrï¼Œè™šå¹»å¼•æ“â€
- en: With the advent of OpenAIâ€™s ChatGPT, chatbots are exploding in popularity! Every
    business seeks ways to incorporate ChatGPT into its customer-facing and internal
    applications. Further, with open-source chatbots catching up so rapidly that even
    [Google engineers seem to conclude they and OpenAI have â€œno moat,â€](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
    thereâ€™s never been a better time to be in the AI industry!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€OpenAIçš„ChatGPTçš„å‡ºç°ï¼ŒèŠå¤©æœºå™¨äººæ­£åœ¨è¿…é€Ÿæµè¡Œï¼æ¯ä¸ªä¼ä¸šéƒ½åœ¨å¯»æ±‚å°†ChatGPTèå…¥å…¶é¢å‘å®¢æˆ·å’Œå†…éƒ¨çš„åº”ç”¨ç¨‹åºä¸­ã€‚æ­¤å¤–ï¼Œç”±äºå¼€æºèŠå¤©æœºå™¨äººè¿›å±•å¦‚æ­¤è¿…é€Ÿï¼Œä»¥è‡³äºå³ä½¿[è°·æ­Œå·¥ç¨‹å¸ˆä¼¼ä¹ä¹Ÿå¾—å‡ºç»“è®ºè®¤ä¸ºä»–ä»¬å’ŒOpenAIâ€œæ²¡æœ‰æŠ¤åŸæ²³â€ï¼Œ](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
    ç°åœ¨æ­£æ˜¯è¿›å…¥AIè¡Œä¸šçš„æœ€ä½³æ—¶æœºï¼
- en: As a Data Scientist building such an application, one of the critical decisions
    is choosing between a monolithic and microservices architecture. Both architectures
    have pros and cons; ultimately, the choice depends on the businessâ€™s needs, such
    as scalability and ease of integration with existing systems. In this blog post,
    we will explore the differences between these two architectures with live code
    examples using Streamlit, Huggingface, and FastAPI!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæ„å»ºè¿™ç§åº”ç”¨ç¨‹åºçš„æ•°æ®ç§‘å­¦å®¶ï¼Œå…³é”®çš„å†³ç­–ä¹‹ä¸€æ˜¯é€‰æ‹©å•ä½“æ¶æ„è¿˜æ˜¯å¾®æœåŠ¡æ¶æ„ã€‚è¿™ä¸¤ç§æ¶æ„å„æœ‰ä¼˜ç¼ºç‚¹ï¼›æœ€ç»ˆçš„é€‰æ‹©å–å†³äºä¸šåŠ¡çš„éœ€æ±‚ï¼Œä¾‹å¦‚å¯æ‰©å±•æ€§å’Œä¸ç°æœ‰ç³»ç»Ÿçš„é›†æˆæ–¹ä¾¿æ€§ã€‚åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Streamlitã€Huggingfaceå’ŒFastAPIçš„å®æ—¶ä»£ç ç¤ºä¾‹æ¥æ¢è®¨è¿™ä¸¤ç§æ¶æ„ä¹‹é—´çš„åŒºåˆ«ï¼
- en: First, create a new conda environment and install the necessary libraries.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„condaç¯å¢ƒå¹¶å®‰è£…å¿…è¦çš„åº“ã€‚
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Monolithic architecture
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å•ä½“æ¶æ„
- en: '![](../Images/3c56f65e305be824fdbe283044fd1416.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c56f65e305be824fdbe283044fd1416.png)'
- en: In a monolithic application, all the code related to the application is tightly
    coupled in a single, self-contained unit. Image by Author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å•ä½“åº”ç”¨ç¨‹åºä¸­ï¼Œä¸åº”ç”¨ç¨‹åºç›¸å…³çš„æ‰€æœ‰ä»£ç éƒ½ç´§å¯†è€¦åˆåœ¨ä¸€ä¸ªç‹¬ç«‹çš„å•å…ƒä¸­ã€‚ å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Monolithic architecture is an approach that involves building the entire application
    as a single, self-contained unit. This approach is simple and easy to develop
    but can become complex as the application grows. All application components, including
    the user interface, business logic, and data storage, are tightly coupled in a
    monolithic architecture. Any changes made to one part of the app can ripple effect
    on the entire application.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å•ä½“æ¶æ„æ˜¯ä¸€ç§å°†æ•´ä¸ªåº”ç”¨ç¨‹åºæ„å»ºä¸ºä¸€ä¸ªè‡ªåŒ…å«çš„å•å…ƒçš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•ç®€å•ä¸”æ˜“äºå¼€å‘ï¼Œä½†éšç€åº”ç”¨ç¨‹åºçš„å¢é•¿ï¼Œå¯èƒ½ä¼šå˜å¾—å¤æ‚ã€‚åœ¨å•ä½“æ¶æ„ä¸­ï¼ŒåŒ…æ‹¬ç”¨æˆ·ç•Œé¢ã€ä¸šåŠ¡é€»è¾‘å’Œæ•°æ®å­˜å‚¨åœ¨å†…çš„æ‰€æœ‰åº”ç”¨ç¨‹åºç»„ä»¶éƒ½æ˜¯ç´§å¯†è€¦åˆçš„ã€‚å¯¹åº”ç”¨ç¨‹åºæŸä¸€éƒ¨åˆ†æ‰€åšçš„ä»»ä½•æ›´æ”¹éƒ½å¯èƒ½å¯¹æ•´ä¸ªåº”ç”¨ç¨‹åºäº§ç”Ÿè¿é”ååº”ã€‚
- en: Letâ€™s use Huggingface and Streamlit to build a monolithic chatbot application
    below. Weâ€™ll use Streamlit to build the frontend user interface, while Huggingface
    provides an extremely easy-to-use, high-level abstraction to various open-source
    LLM models called [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines#natural-language-processing).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æˆ‘ä»¬ä½¿ç”¨ Huggingface å’Œ Streamlit æ¥æ„å»ºä¸€ä¸ªå•ä½“èŠå¤©æœºå™¨äººåº”ç”¨ç¨‹åºã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Streamlit æ¥æ„å»ºå‰ç«¯ç”¨æˆ·ç•Œé¢ï¼Œè€Œ
    Huggingface æä¾›äº†ä¸€ç§éå¸¸æ˜“äºä½¿ç”¨çš„é«˜å±‚æŠ½è±¡ï¼Œç§°ä¸º [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines#natural-language-processing)ï¼Œå®ƒå¯ä»¥è®¿é—®å„ç§å¼€æº
    LLM æ¨¡å‹ã€‚
- en: First, letâ€™s create a file utils.py containing three helper functions common
    to the front end in monolithic and microservices architectures.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å«ä¸‰ä¸ªåœ¨å•ä½“å’Œå¾®æœåŠ¡æ¶æ„çš„å‰ç«¯ä¸­å¸¸ç”¨çš„åŠ©æ‰‹å‡½æ•°çš„æ–‡ä»¶ utils.pyã€‚
- en: '`clear_conversation()`: This function deletes all the stored session_state
    variables in the Streamlit frontend. We use it to clear the entire chat history
    and start a new chat thread.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`clear_conversation()`: æ­¤å‡½æ•°åˆ é™¤ Streamlit å‰ç«¯ä¸­æ‰€æœ‰å­˜å‚¨çš„ session_state å˜é‡ã€‚æˆ‘ä»¬ç”¨å®ƒæ¥æ¸…é™¤æ•´ä¸ªèŠå¤©è®°å½•å¹¶å¼€å§‹ä¸€ä¸ªæ–°çš„èŠå¤©çº¿ç¨‹ã€‚'
- en: '`display_conversation()`: This function uses the streamlit_chat library to
    create a beautiful chat interface frontend with our entire chat thread displayed
    on the screen from the latest to the oldest message. Since the Huggingface pipelines
    API stores user_inputs and generate_responses in separate lists, we also use this
    function to create a single interleaved_conversation list that contains the entire
    chat thread so we can download it if needed.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`display_conversation()`: æ­¤å‡½æ•°ä½¿ç”¨ streamlit_chat åº“åˆ›å»ºä¸€ä¸ªæ¼‚äº®çš„èŠå¤©ç•Œé¢å‰ç«¯ï¼Œå°†æˆ‘ä»¬çš„æ•´ä¸ªèŠå¤©çº¿ç¨‹ä»æœ€æ–°åˆ°æœ€æ—§çš„æ¶ˆæ¯æ˜¾ç¤ºåœ¨å±å¹•ä¸Šã€‚ç”±äº
    Huggingface pipelines API å°† user_inputs å’Œ generate_responses å­˜å‚¨åœ¨ä¸åŒçš„åˆ—è¡¨ä¸­ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨æ­¤å‡½æ•°åˆ›å»ºä¸€ä¸ªåŒ…å«æ•´ä¸ªèŠå¤©çº¿ç¨‹çš„å•ä¸€
    interleaved_conversation åˆ—è¡¨ï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶å¯ä»¥ä¸‹è½½å®ƒã€‚'
- en: '`download_conversation()`: This function converts the whole chat thread to
    a pandas dataframe and downloads it as a csv file to your local computer.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`download_conversation()`: æ­¤å‡½æ•°å°†æ•´ä¸ªèŠå¤©çº¿ç¨‹è½¬æ¢ä¸º pandas dataframe å¹¶ä¸‹è½½ä¸º csv æ–‡ä»¶åˆ°æ‚¨çš„æœ¬åœ°è®¡ç®—æœºã€‚'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, letâ€™s create a single monolith.py file containing our entire monolithic
    application.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å«æ•´ä¸ªå•ä½“åº”ç”¨ç¨‹åºçš„ monolith.py æ–‡ä»¶ã€‚
- en: OpenAIâ€™s ChatGPT API costs money for every token in both the question and response.
    Hence for this small demo, I chose to use an open-source model from Huggingface
    called â€œfacebook/blenderbot-400M-distillâ€. You can find the entire list of over
    2000 open-source models trained for the conversational task at the [Huggingface
    model hub](https://huggingface.co/models?pipeline_tag=conversational). For more
    details on the conversational task pipeline, refer to [Huggingfaceâ€™s official
    documentation](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/pipelines#transformers.Conversation).
    When open-source models inevitably catch up to the proprietary models from OpenAI
    and Google, Iâ€™m sure Huggingface will be THE platform for researchers to share
    those models, given how much theyâ€™ve revolutionized the field of NLP over the
    past few years!
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenAI çš„ ChatGPT API å¯¹é—®é¢˜å’Œå›ç­”ä¸­çš„æ¯ä¸ªä»¤ç‰Œéƒ½æ”¶è´¹ã€‚å› æ­¤ï¼Œåœ¨è¿™ä¸ªå°æ¼”ç¤ºä¸­ï¼Œæˆ‘é€‰æ‹©ä½¿ç”¨ Huggingface çš„ä¸€ä¸ªå¼€æºæ¨¡å‹ï¼Œåä¸ºâ€œfacebook/blenderbot-400M-distillâ€ã€‚æ‚¨å¯ä»¥åœ¨
    [Huggingface æ¨¡å‹ä¸­å¿ƒ](https://huggingface.co/models?pipeline_tag=conversational)
    æ‰¾åˆ°è¶…è¿‡ 2000 ä¸ªç»è¿‡å¯¹è¯ä»»åŠ¡è®­ç»ƒçš„å¼€æºæ¨¡å‹çš„å®Œæ•´åˆ—è¡¨ã€‚æœ‰å…³å¯¹è¯ä»»åŠ¡ pipeline çš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è€ƒ [Huggingface å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/pipelines#transformers.Conversation)ã€‚å½“å¼€æºæ¨¡å‹ä¸å¯é¿å…åœ°èµ¶ä¸Š
    OpenAI å’Œ Google çš„ä¸“æœ‰æ¨¡å‹æ—¶ï¼Œæˆ‘ç›¸ä¿¡ Huggingface å°†æˆä¸ºç ”ç©¶äººå‘˜åˆ†äº«è¿™äº›æ¨¡å‹çš„å¹³å°ï¼Œè€ƒè™‘åˆ°ä»–ä»¬åœ¨è¿‡å»å‡ å¹´ä¸­å¦‚ä½•å½»åº•æ”¹å˜äº† NLP
    é¢†åŸŸï¼
- en: '`main()`: This function builds the frontend appâ€™s layout using Streamlit. Weâ€™ll
    have a button to clear the conversation and one to download. Weâ€™ll also have a
    text box where the user can type their question, and upon pressing enter, weâ€™ll
    call the `monolith_llm_response` function with the userâ€™s input. Finally, weâ€™ll
    display the entire conversation on the front end using the `display_conversation`
    function from utils.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`main()`: æ­¤å‡½æ•°ä½¿ç”¨ Streamlit æ„å»ºå‰ç«¯åº”ç”¨çš„å¸ƒå±€ã€‚æˆ‘ä»¬å°†æœ‰ä¸€ä¸ªæŒ‰é’®æ¥æ¸…é™¤å¯¹è¯ï¼Œå¦ä¸€ä¸ªæŒ‰é’®æ¥ä¸‹è½½ã€‚æˆ‘ä»¬è¿˜ä¼šæœ‰ä¸€ä¸ªæ–‡æœ¬æ¡†ï¼Œç”¨æˆ·å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥é—®é¢˜ï¼ŒæŒ‰ä¸‹å›è½¦åï¼Œæˆ‘ä»¬å°†è°ƒç”¨
    `monolith_llm_response` å‡½æ•°å¤„ç†ç”¨æˆ·çš„è¾“å…¥ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª `utils` çš„ `display_conversation`
    å‡½æ•°åœ¨å‰ç«¯æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯ã€‚'
- en: '`monolith_llm_response()`: This function is responsible for the chatbot logic
    using Huggingface pipelines. First, we create a new Conversation object and initialize
    it to the entire conversation history up to that point. Then, we add the latest
    user_input to that object, and finally, we pass this conversation object to the
    Huggingface pipeline that we created two steps back. Huggingface automatically
    adds the user input and response generated to the conversation history!'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`monolith_llm_response()`: æ­¤å‡½æ•°è´Ÿè´£ä½¿ç”¨ Huggingface ç®¡é“å¤„ç†èŠå¤©æœºå™¨äººé€»è¾‘ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„ `Conversation`
    å¯¹è±¡ï¼Œå¹¶å°†å…¶åˆå§‹åŒ–ä¸ºåˆ°ç›®å‰ä¸ºæ­¢çš„æ•´ä¸ªå¯¹è¯å†å²ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æœ€æ–°çš„ `user_input` æ·»åŠ åˆ°è¯¥å¯¹è±¡ä¸­ï¼Œæœ€åï¼Œæˆ‘ä»¬å°†æ­¤å¯¹è¯å¯¹è±¡ä¼ é€’ç»™æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ Huggingface
    ç®¡é“ã€‚Huggingface ä¼šè‡ªåŠ¨å°†ç”¨æˆ·è¾“å…¥å’Œç”Ÿæˆçš„å“åº”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­ï¼'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Thatâ€™s it! We can run this monolithic application by running `streamlit run
    monolith.py` and interacting with the application on a web browser! We could quickly
    deploy this application as such to a cloud service like Google Cloud Run, as described
    in [my previous blog post,](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)
    and interact with it over the internet too!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™äº›äº†ï¼æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿è¡Œ `streamlit run monolith.py` æ¥è¿è¡Œè¿™ä¸ªå•ä½“åº”ç”¨ï¼Œå¹¶åœ¨ç½‘é¡µæµè§ˆå™¨ä¸Šä¸åº”ç”¨è¿›è¡Œäº¤äº’ï¼æˆ‘ä»¬è¿˜å¯ä»¥å°†æ­¤åº”ç”¨å¿«é€Ÿéƒ¨ç½²åˆ°åƒ
    Google Cloud Run è¿™æ ·çš„äº‘æœåŠ¡ä¸­ï¼Œæ­£å¦‚ [æˆ‘ä¹‹å‰çš„åšå®¢æ–‡ç« ](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)
    ä¸­æ‰€æè¿°çš„é‚£æ ·ï¼Œå¹¶é€šè¿‡äº’è”ç½‘è¿›è¡Œäº¤äº’ï¼
- en: '![](../Images/a7252eb9c1b0038f1c6afa067858db11.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7252eb9c1b0038f1c6afa067858db11.png)'
- en: Monolithic Streamlit App interface. Image by Author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å•ä½“ Streamlit åº”ç”¨ç•Œé¢ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Microservices architecture
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¾®æœåŠ¡æ¶æ„
- en: '![](../Images/66daf7b302656ef96086a711dee6949f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66daf7b302656ef96086a711dee6949f.png)'
- en: In a microservices application, each component is split up into its own smaller,
    independent service. Image by Author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¾®æœåŠ¡åº”ç”¨ä¸­ï¼Œæ¯ä¸ªç»„ä»¶è¢«æ‹†åˆ†ä¸ºè‡ªå·±çš„è¾ƒå°çš„ç‹¬ç«‹æœåŠ¡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Microservices architecture is an approach that involves breaking down the application
    into smaller, independent services. Each application component, such as the user
    interface, business logic, and data storage, is developed and deployed independently.
    This approach offers flexibility and scalability as we can modularly add more
    capabilities and horizontally scale each service independently of others by adding
    more instances.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®æœåŠ¡æ¶æ„æ˜¯ä¸€ç§å°†åº”ç”¨ç¨‹åºæ‹†åˆ†æˆæ›´å°çš„ç‹¬ç«‹æœåŠ¡çš„æ–¹æ³•ã€‚æ¯ä¸ªåº”ç”¨ç¨‹åºç»„ä»¶ï¼Œå¦‚ç”¨æˆ·ç•Œé¢ã€ä¸šåŠ¡é€»è¾‘å’Œæ•°æ®å­˜å‚¨ï¼Œéƒ½æ˜¯ç‹¬ç«‹å¼€å‘å’Œéƒ¨ç½²çš„ã€‚è¿™ç§æ–¹æ³•æä¾›äº†çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥æ¨¡å—åŒ–åœ°æ·»åŠ æ›´å¤šåŠŸèƒ½ï¼Œå¹¶é€šè¿‡æ·»åŠ æ›´å¤šå®ä¾‹æ¥æ°´å¹³æ‰©å±•æ¯ä¸ªæœåŠ¡ï¼Œè€Œä¸ä¼šå½±å“å…¶ä»–æœåŠ¡ã€‚
- en: Letâ€™s split the Huggingface model inference from our monolithic app into a separate
    microservice using FastAPI and the Streamlit frontend into another microservice
    below. Since the backend in this demo only has the LLM model, our backend API
    server is the same as the LLM model from the picture above. We can directly re-use
    the utils.py file we created above in the frontend microservice!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨ FastAPI å°† Huggingface æ¨¡å‹æ¨ç†ä»æˆ‘ä»¬çš„å•ä½“åº”ç”¨ä¸­æ‹†åˆ†ä¸ºä¸€ä¸ªå•ç‹¬çš„å¾®æœåŠ¡ï¼Œå¹¶å°† Streamlit å‰ç«¯æ‹†åˆ†ä¸ºå¦ä¸€ä¸ªå¾®æœåŠ¡ã€‚ç”±äºè¿™ä¸ªæ¼”ç¤ºä¸­çš„åç«¯ä»…æœ‰
    LLM æ¨¡å‹ï¼Œæˆ‘ä»¬çš„åç«¯ API æœåŠ¡å™¨ä¸ä¸Šå›¾ä¸­çš„ LLM æ¨¡å‹ç›¸åŒã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨å‰ç«¯å¾®æœåŠ¡ä¸­é‡ç”¨æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ `utils.py` æ–‡ä»¶ï¼
- en: First, letâ€™s create a backend.py file that will serve as our FastAPI microservice
    that runs the Huggingface pipeline inference.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª `backend.py` æ–‡ä»¶ï¼Œè¿™å°†ä½œä¸ºæˆ‘ä»¬çš„ FastAPI å¾®æœåŠ¡ï¼Œè¿è¡Œ Huggingface ç®¡é“æ¨ç†ã€‚
- en: We first create the pipeline object with the same model that we chose earlier,
    â€œfacebook/blenderbot-400M-distillâ€
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆä½¿ç”¨ä¹‹å‰é€‰æ‹©çš„ç›¸åŒæ¨¡å‹ï¼Œâ€œfacebook/blenderbot-400M-distillâ€åˆ›å»ºç®¡é“å¯¹è±¡ã€‚
- en: We then create a ConversationHistory Pydantic model so that we can receive the
    inputs required for the pipeline as a payload to the FastAPI service. For more
    information on the FastAPI request body, please look at [the FastAPI documentation](https://fastapi.tiangolo.com/tutorial/body/).
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª `ConversationHistory` Pydantic æ¨¡å‹ï¼Œä»¥ä¾¿å°†ç®¡é“æ‰€éœ€çš„è¾“å…¥ä½œä¸ºè´Ÿè½½ä¼ é€’ç»™ FastAPI æœåŠ¡ã€‚å¦‚éœ€æ›´å¤šæœ‰å…³
    FastAPI è¯·æ±‚ä¸»ä½“çš„ä¿¡æ¯ï¼Œè¯·å‚è§ [FastAPI æ–‡æ¡£](https://fastapi.tiangolo.com/tutorial/body/)ã€‚
- en: Itâ€™s a good practice to reserve the root route in APIs for a health check. So
    we define that route first.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸º APIs ä¿ç•™æ ¹è·¯ç”±ç”¨äºå¥åº·æ£€æŸ¥æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ã€‚æ‰€ä»¥æˆ‘ä»¬é¦–å…ˆå®šä¹‰è¿™ä¸ªè·¯ç”±ã€‚
- en: Finally, we define a route called `/chat`, which accepts the API payload as
    a ConversationHistory object and converts it to a dictionary. Then we create a
    new Conversation object and initialize it with the conversation history received
    in the payload. Next, we add the latest user_input to that object and pass this
    conversation object to the Huggingface pipeline. Finally, we must return the latest
    generated response to the front end.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåä¸º `/chat` çš„è·¯ç”±ï¼Œè¯¥è·¯ç”±æ¥å— API æœ‰æ•ˆè´Ÿè½½ä½œä¸º ConversationHistory å¯¹è±¡ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå­—å…¸ã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„
    Conversation å¯¹è±¡ï¼Œå¹¶ç”¨æœ‰æ•ˆè´Ÿè½½ä¸­æ¥æ”¶åˆ°çš„å¯¹è¯å†å²è¿›è¡Œåˆå§‹åŒ–ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å°†æœ€æ–°çš„ user_input æ·»åŠ åˆ°è¯¥å¯¹è±¡ä¸­ï¼Œå¹¶å°†è¿™ä¸ªå¯¹è¯å¯¹è±¡ä¼ é€’ç»™
    Huggingface pipelineã€‚æœ€åï¼Œæˆ‘ä»¬å¿…é¡»å°†æœ€æ–°ç”Ÿæˆçš„å“åº”è¿”å›ç»™å‰ç«¯ã€‚
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can run this FastAPI app locally using `uvicorn backend:app --reload`, or
    deploy it to a cloud service like Google Cloud Run, as described in [my previous
    blog post,](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)
    and interact with it over the internet! You can test the backend using the API
    docs that FastAPI automatically generates at the `/docs` route by navigating to
    [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `uvicorn backend:app --reload` æœ¬åœ°è¿è¡Œè¿™ä¸ª FastAPI åº”ç”¨ï¼Œä¹Ÿå¯ä»¥å°†å…¶éƒ¨ç½²åˆ° Google Cloud
    Run ç­‰äº‘æœåŠ¡ä¸­ï¼Œå¦‚[æˆ‘ä¹‹å‰çš„åšå®¢æ–‡ç« ](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)ä¸­æ‰€è¿°ï¼Œå¹¶é€šè¿‡äº’è”ç½‘è¿›è¡Œäº¤äº’ï¼ä½ å¯ä»¥é€šè¿‡è®¿é—®
    [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) æ¥ä½¿ç”¨ FastAPI è‡ªåŠ¨ç”Ÿæˆçš„ `/docs`
    è·¯ç”±ä¸­çš„ API æ–‡æ¡£æµ‹è¯•åç«¯ã€‚
- en: '![](../Images/beb421a5268c38e1ed6301ea58c47aef.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/beb421a5268c38e1ed6301ea58c47aef.png)'
- en: FastAPI docs for the backend. Image by Author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åç«¯çš„ FastAPI æ–‡æ¡£ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Finally, letâ€™s create a frontend.py file that contains the frontend code.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å«å‰ç«¯ä»£ç çš„ `frontend.py` æ–‡ä»¶ã€‚
- en: '`main()`: This function is precisely similar to `main()` in the monolithic
    application, except for one change that we call the `microservice_llm_response()`
    function when the user enters any input.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`main()`ï¼šè¿™ä¸ªå‡½æ•°ä¸å•ä½“åº”ç”¨ä¸­çš„ `main()` å‡½æ•°éå¸¸ç›¸ä¼¼ï¼Œå”¯ä¸€çš„å˜åŒ–æ˜¯å½“ç”¨æˆ·è¾“å…¥ä»»ä½•å†…å®¹æ—¶ï¼Œæˆ‘ä»¬è°ƒç”¨ `microservice_llm_response()`
    å‡½æ•°ã€‚'
- en: '`microservice_llm_response()`: Since we split out the LLM logic into a separate
    FastAPI microservice, this function uses the conversation history stored in the
    session_state to post a request to the backend FastAPI service and then appends
    both the userâ€™s input and the response from the FastAPI backend to the conversation
    history to continue the memory of the entire chat thread.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`microservice_llm_response()`ï¼šç”±äºæˆ‘ä»¬å°† LLM é€»è¾‘æ‹†åˆ†åˆ°ä¸€ä¸ªå•ç‹¬çš„ FastAPI å¾®æœåŠ¡ä¸­ï¼Œè¿™ä¸ªå‡½æ•°åˆ©ç”¨å­˜å‚¨åœ¨ session_state
    ä¸­çš„å¯¹è¯å†å²è®°å½•ï¼Œå‘åç«¯ FastAPI æœåŠ¡å‘é€è¯·æ±‚ï¼Œç„¶åå°†ç”¨æˆ·è¾“å…¥å’Œ FastAPI åç«¯çš„å“åº”éƒ½é™„åŠ åˆ°å¯¹è¯å†å²ä¸­ï¼Œä»¥ä¾¿ç»§ç»­è®°ä½æ•´ä¸ªèŠå¤©çº¿ç¨‹ã€‚'
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Thatâ€™s it! We can run this frontend application by running `streamlit run frontend.py`
    and interacting with the application on a web browser! As my [previous blog post](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)
    described, we could quickly deploy to a cloud service like Google Cloud Run and
    interact with it over the internet too!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ï¼æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿è¡Œ `streamlit run frontend.py` æ¥å¯åŠ¨è¿™ä¸ªå‰ç«¯åº”ç”¨ï¼Œå¹¶åœ¨ç½‘é¡µæµè§ˆå™¨ä¸Šä¸åº”ç”¨è¿›è¡Œäº¤äº’ï¼æ­£å¦‚æˆ‘çš„[ä¸Šä¸€ç¯‡åšå®¢æ–‡ç« ](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)ä¸­æ‰€æè¿°çš„ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å¿«é€Ÿéƒ¨ç½²åˆ°äº‘æœåŠ¡å¦‚
    Google Cloud Runï¼Œå¹¶é€šè¿‡äº’è”ç½‘ä¸ä¹‹äº’åŠ¨ï¼
- en: Which architecture to choose?
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€‰æ‹©å“ªä¸ªæ¶æ„ï¼Ÿ
- en: The answer depends on the requirements of your application. A monolithic architecture
    can be a great starting point for a Data Scientist to build an initial proof-of-concept
    quickly and get it in front of business stakeholders. But, inevitably, if you
    plan to productionize the application, a microservices architecture is generally
    a better bet over a monolithic one because it allows for more flexibility and
    scalability and allows different specialized developers to focus on building the
    various components. For example, a frontend developer might use React to build
    the frontend, a Data Engineer might use Airflow to write the data pipelines, and
    an ML engineer might use [FastAPI](https://fastapi.tiangolo.com/) or [BentoML](https://github.com/bentoml/BentoML)
    to deploy the model serving API with custom business logic.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆå–å†³äºä½ åº”ç”¨çš„éœ€æ±‚ã€‚å•ä½“æ¶æ„å¯ä»¥æ˜¯æ•°æ®ç§‘å­¦å®¶å¿«é€Ÿæ„å»ºåˆæ­¥æ¦‚å¿µéªŒè¯å¹¶å‘ä¸šåŠ¡å¹²ç³»äººå±•ç¤ºçš„ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚ä½†ä¸å¯é¿å…åœ°ï¼Œå¦‚æœä½ è®¡åˆ’å°†åº”ç”¨æŠ•å…¥ç”Ÿäº§ï¼Œå¾®æœåŠ¡æ¶æ„é€šå¸¸æ¯”å•ä½“æ¶æ„æ›´å¥½ï¼Œå› ä¸ºå®ƒæä¾›äº†æ›´å¤šçš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¹¶å…è®¸ä¸åŒçš„ä¸“ä¸šå¼€å‘äººå‘˜ä¸“æ³¨äºæ„å»ºå„ç§ç»„ä»¶ã€‚ä¾‹å¦‚ï¼Œå‰ç«¯å¼€å‘äººå‘˜å¯èƒ½ä¼šä½¿ç”¨
    React æ„å»ºå‰ç«¯ï¼Œæ•°æ®å·¥ç¨‹å¸ˆå¯èƒ½ä¼šä½¿ç”¨ Airflow ç¼–å†™æ•°æ®ç®¡é“ï¼Œè€Œ ML å·¥ç¨‹å¸ˆå¯èƒ½ä¼šä½¿ç”¨ [FastAPI](https://fastapi.tiangolo.com/)
    æˆ– [BentoML](https://github.com/bentoml/BentoML) æ¥éƒ¨ç½²å…·æœ‰è‡ªå®šä¹‰ä¸šåŠ¡é€»è¾‘çš„æ¨¡å‹æœåŠ¡ APIã€‚
- en: Additionally, with microservices, chatbot developers can easily incorporate
    new features or change existing ones without affecting the entire application.
    This level of flexibility and scalability is crucial for businesses that want
    to integrate the chatbot into existing applications. Dedicated UI/UX, data engineers,
    data scientists, and ML engineers can each focus on their areas of expertise to
    deliver a polished product!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œé€šè¿‡å¾®æœåŠ¡ï¼ŒèŠå¤©æœºå™¨äººå¼€å‘äººå‘˜å¯ä»¥è½»æ¾åœ°åŠ å…¥æ–°åŠŸèƒ½æˆ–æ›´æ”¹ç°æœ‰åŠŸèƒ½ï¼Œè€Œä¸ä¼šå½±å“æ•´ä¸ªåº”ç”¨ç¨‹åºã€‚è¿™ç§çµæ´»æ€§å’Œå¯æ‰©å±•æ€§å¯¹é‚£äº›å¸Œæœ›å°†èŠå¤©æœºå™¨äººé›†æˆåˆ°ç°æœ‰åº”ç”¨ä¸­çš„ä¼ä¸šè‡³å…³é‡è¦ã€‚ä¸“æ³¨çš„
    UI/UXã€æ•°æ®å·¥ç¨‹å¸ˆã€æ•°æ®ç§‘å­¦å®¶å’Œ ML å·¥ç¨‹å¸ˆå¯ä»¥å„è‡ªä¸“æ³¨äºä»–ä»¬çš„ä¸“ä¸šé¢†åŸŸï¼Œä»¥äº¤ä»˜ä¸€ä¸ªç²¾è‡´çš„äº§å“ï¼
- en: Conclusion
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In conclusion, monolithic and microservices architectures have pros and cons,
    and the choice between the two depends on the businessâ€™s specific needs. However,
    I prefer microservices architecture for chatbot applications due to its flexibility,
    scalability, and the fact that I can delegate frontend development to more qualified
    UI/UX folk ğŸ¤©.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œå•ä½“æ¶æ„å’Œå¾®æœåŠ¡æ¶æ„å„æœ‰ä¼˜ç¼ºç‚¹ï¼Œä¸¤è€…ä¹‹é—´çš„é€‰æ‹©å–å†³äºä¸šåŠ¡çš„å…·ä½“éœ€æ±‚ã€‚ç„¶è€Œï¼Œæˆ‘æ›´å€¾å‘äºå¾®æœåŠ¡æ¶æ„ç”¨äºèŠå¤©æœºå™¨äººåº”ç”¨ï¼Œå› ä¸ºå®ƒçš„çµæ´»æ€§ã€å¯æ‰©å±•æ€§ï¼Œä»¥åŠæˆ‘å¯ä»¥å°†å‰ç«¯å¼€å‘å§”æ‰˜ç»™æ›´åˆæ ¼çš„
    UI/UX ä¸“å®¶ ğŸ¤©ã€‚
