- en: 'Anatomy of LLM-Based Chatbot Applications: Monolithic vs. Microservice Architectural
    Patterns'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e](https://towardsdatascience.com/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Practical Guide to Building Monolithic and Microservice Chatbot Applications
    with Streamlit, Huggingface, and FastAPI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)[![Marie
    Stephen Leo](../Images/c5669d884da5ff5c965f98904257d379.png)](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)
    [Marie Stephen Leo](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)
    ·9 min read·May 8, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b4d2660c860f9c8391e33823a5f1824.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image generated by Author using Midjourney V5.1 using the prompt: “isometric
    highly realistic view of a laptop, screen has the picture of a bright, multi colored
    rubik’s cube that is illuminated from within, bright, warm, cheerful lighting.
    8k, hdr, unreal engine”'
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of OpenAI’s ChatGPT, chatbots are exploding in popularity! Every
    business seeks ways to incorporate ChatGPT into its customer-facing and internal
    applications. Further, with open-source chatbots catching up so rapidly that even
    [Google engineers seem to conclude they and OpenAI have “no moat,”](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
    there’s never been a better time to be in the AI industry!
  prefs: []
  type: TYPE_NORMAL
- en: As a Data Scientist building such an application, one of the critical decisions
    is choosing between a monolithic and microservices architecture. Both architectures
    have pros and cons; ultimately, the choice depends on the business’s needs, such
    as scalability and ease of integration with existing systems. In this blog post,
    we will explore the differences between these two architectures with live code
    examples using Streamlit, Huggingface, and FastAPI!
  prefs: []
  type: TYPE_NORMAL
- en: First, create a new conda environment and install the necessary libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Monolithic architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/3c56f65e305be824fdbe283044fd1416.png)'
  prefs: []
  type: TYPE_IMG
- en: In a monolithic application, all the code related to the application is tightly
    coupled in a single, self-contained unit. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Monolithic architecture is an approach that involves building the entire application
    as a single, self-contained unit. This approach is simple and easy to develop
    but can become complex as the application grows. All application components, including
    the user interface, business logic, and data storage, are tightly coupled in a
    monolithic architecture. Any changes made to one part of the app can ripple effect
    on the entire application.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use Huggingface and Streamlit to build a monolithic chatbot application
    below. We’ll use Streamlit to build the frontend user interface, while Huggingface
    provides an extremely easy-to-use, high-level abstraction to various open-source
    LLM models called [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines#natural-language-processing).
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s create a file utils.py containing three helper functions common
    to the front end in monolithic and microservices architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '`clear_conversation()`: This function deletes all the stored session_state
    variables in the Streamlit frontend. We use it to clear the entire chat history
    and start a new chat thread.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`display_conversation()`: This function uses the streamlit_chat library to
    create a beautiful chat interface frontend with our entire chat thread displayed
    on the screen from the latest to the oldest message. Since the Huggingface pipelines
    API stores user_inputs and generate_responses in separate lists, we also use this
    function to create a single interleaved_conversation list that contains the entire
    chat thread so we can download it if needed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`download_conversation()`: This function converts the whole chat thread to
    a pandas dataframe and downloads it as a csv file to your local computer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s create a single monolith.py file containing our entire monolithic
    application.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s ChatGPT API costs money for every token in both the question and response.
    Hence for this small demo, I chose to use an open-source model from Huggingface
    called “facebook/blenderbot-400M-distill”. You can find the entire list of over
    2000 open-source models trained for the conversational task at the [Huggingface
    model hub](https://huggingface.co/models?pipeline_tag=conversational). For more
    details on the conversational task pipeline, refer to [Huggingface’s official
    documentation](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/pipelines#transformers.Conversation).
    When open-source models inevitably catch up to the proprietary models from OpenAI
    and Google, I’m sure Huggingface will be THE platform for researchers to share
    those models, given how much they’ve revolutionized the field of NLP over the
    past few years!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`main()`: This function builds the frontend app’s layout using Streamlit. We’ll
    have a button to clear the conversation and one to download. We’ll also have a
    text box where the user can type their question, and upon pressing enter, we’ll
    call the `monolith_llm_response` function with the user’s input. Finally, we’ll
    display the entire conversation on the front end using the `display_conversation`
    function from utils.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`monolith_llm_response()`: This function is responsible for the chatbot logic
    using Huggingface pipelines. First, we create a new Conversation object and initialize
    it to the entire conversation history up to that point. Then, we add the latest
    user_input to that object, and finally, we pass this conversation object to the
    Huggingface pipeline that we created two steps back. Huggingface automatically
    adds the user input and response generated to the conversation history!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! We can run this monolithic application by running `streamlit run
    monolith.py` and interacting with the application on a web browser! We could quickly
    deploy this application as such to a cloud service like Google Cloud Run, as described
    in [my previous blog post,](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)
    and interact with it over the internet too!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7252eb9c1b0038f1c6afa067858db11.png)'
  prefs: []
  type: TYPE_IMG
- en: Monolithic Streamlit App interface. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Microservices architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/66daf7b302656ef96086a711dee6949f.png)'
  prefs: []
  type: TYPE_IMG
- en: In a microservices application, each component is split up into its own smaller,
    independent service. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Microservices architecture is an approach that involves breaking down the application
    into smaller, independent services. Each application component, such as the user
    interface, business logic, and data storage, is developed and deployed independently.
    This approach offers flexibility and scalability as we can modularly add more
    capabilities and horizontally scale each service independently of others by adding
    more instances.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s split the Huggingface model inference from our monolithic app into a separate
    microservice using FastAPI and the Streamlit frontend into another microservice
    below. Since the backend in this demo only has the LLM model, our backend API
    server is the same as the LLM model from the picture above. We can directly re-use
    the utils.py file we created above in the frontend microservice!
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s create a backend.py file that will serve as our FastAPI microservice
    that runs the Huggingface pipeline inference.
  prefs: []
  type: TYPE_NORMAL
- en: We first create the pipeline object with the same model that we chose earlier,
    “facebook/blenderbot-400M-distill”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then create a ConversationHistory Pydantic model so that we can receive the
    inputs required for the pipeline as a payload to the FastAPI service. For more
    information on the FastAPI request body, please look at [the FastAPI documentation](https://fastapi.tiangolo.com/tutorial/body/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s a good practice to reserve the root route in APIs for a health check. So
    we define that route first.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we define a route called `/chat`, which accepts the API payload as
    a ConversationHistory object and converts it to a dictionary. Then we create a
    new Conversation object and initialize it with the conversation history received
    in the payload. Next, we add the latest user_input to that object and pass this
    conversation object to the Huggingface pipeline. Finally, we must return the latest
    generated response to the front end.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can run this FastAPI app locally using `uvicorn backend:app --reload`, or
    deploy it to a cloud service like Google Cloud Run, as described in [my previous
    blog post,](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)
    and interact with it over the internet! You can test the backend using the API
    docs that FastAPI automatically generates at the `/docs` route by navigating to
    [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/beb421a5268c38e1ed6301ea58c47aef.png)'
  prefs: []
  type: TYPE_IMG
- en: FastAPI docs for the backend. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s create a frontend.py file that contains the frontend code.
  prefs: []
  type: TYPE_NORMAL
- en: '`main()`: This function is precisely similar to `main()` in the monolithic
    application, except for one change that we call the `microservice_llm_response()`
    function when the user enters any input.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`microservice_llm_response()`: Since we split out the LLM logic into a separate
    FastAPI microservice, this function uses the conversation history stored in the
    session_state to post a request to the backend FastAPI service and then appends
    both the user’s input and the response from the FastAPI backend to the conversation
    history to continue the memory of the entire chat thread.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! We can run this frontend application by running `streamlit run frontend.py`
    and interacting with the application on a web browser! As my [previous blog post](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)
    described, we could quickly deploy to a cloud service like Google Cloud Run and
    interact with it over the internet too!
  prefs: []
  type: TYPE_NORMAL
- en: Which architecture to choose?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The answer depends on the requirements of your application. A monolithic architecture
    can be a great starting point for a Data Scientist to build an initial proof-of-concept
    quickly and get it in front of business stakeholders. But, inevitably, if you
    plan to productionize the application, a microservices architecture is generally
    a better bet over a monolithic one because it allows for more flexibility and
    scalability and allows different specialized developers to focus on building the
    various components. For example, a frontend developer might use React to build
    the frontend, a Data Engineer might use Airflow to write the data pipelines, and
    an ML engineer might use [FastAPI](https://fastapi.tiangolo.com/) or [BentoML](https://github.com/bentoml/BentoML)
    to deploy the model serving API with custom business logic.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, with microservices, chatbot developers can easily incorporate
    new features or change existing ones without affecting the entire application.
    This level of flexibility and scalability is crucial for businesses that want
    to integrate the chatbot into existing applications. Dedicated UI/UX, data engineers,
    data scientists, and ML engineers can each focus on their areas of expertise to
    deliver a polished product!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, monolithic and microservices architectures have pros and cons,
    and the choice between the two depends on the business’s specific needs. However,
    I prefer microservices architecture for chatbot applications due to its flexibility,
    scalability, and the fact that I can delegate frontend development to more qualified
    UI/UX folk 🤩.
  prefs: []
  type: TYPE_NORMAL
