["```py\nimport json\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n\ntry:\n  role = sagemaker.get_execution_role()\nexcept ValueError:\n  iam = boto3.client('iam')\n  role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\nmodel_id = \"openchat/openchat_8192\"\ninstance_type = \"ml.g5.12xlarge\" # 4 x 24GB VRAM\nnumber_of_gpu = 4\nhealth_check_timeout = 600 # how much time do we allow for model download\n\n# Hub Model configuration. https://huggingface.co/models\nhub = {\n  'HF_MODEL_ID': model_id,\n  'SM_NUM_GPUS': json.dumps(number_of_gpu),\n  'MAX_INPUT_LENGTH': json.dumps(7000),  # Max length of input text\n  'MAX_TOTAL_TOKENS': json.dumps(8192),  # Max length of the generation (including input text)\n}\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n  image_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"0.8.2\"),\n  env=hub,\n  role=role, \n)\n\nmodel_name = hf_model_id.split(\"/\")[-1].replace(\".\", \"-\")\nendpoint_name = model_name.replace(\"_\", \"-\")\n\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n  initial_instance_count=1,\n  instance_type=instance_type, \n  container_startup_health_check_timeout=health_check_timeout,\n  endpoint_name=endpoint_name,\n)\n\n# send request\npredictor.predict({\n  \"inputs\": \"Hi, my name is Heiko.\",\n})\n```"]