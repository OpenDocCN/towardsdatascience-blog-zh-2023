- en: Hey GPU, Whatâ€™s Up with My Matrix?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å˜¿ï¼ŒGPUï¼Œæˆ‘çš„çŸ©é˜µæ€ä¹ˆäº†ï¼Ÿ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6](https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6](https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6)
- en: A gentle guide to understanding how GPUs perform matrix multiplication
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº†è§£GPUå¦‚ä½•æ‰§è¡ŒçŸ©é˜µä¹˜æ³•çš„æ¸©å’ŒæŒ‡å—
- en: '[](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Thushan
    Ganegedara](../Images/3fabfa37132f7d3a9e7679c3b8d7e061.png)](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    [Thushan Ganegedara](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Thushan
    Ganegedara](../Images/3fabfa37132f7d3a9e7679c3b8d7e061.png)](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    [Thushan Ganegedara](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    Â·8 min readÂ·Jun 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    Â·é˜…è¯»æ—¶é•¿8åˆ†é’ŸÂ·2023å¹´6æœˆ13æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a911c1d35115175bbfe3894ea74e8d00.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a911c1d35115175bbfe3894ea74e8d00.png)'
- en: Photo by [Thomas Foster](https://unsplash.com/@thomasfos?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/vWgoeEYdtIY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[Thomas Foster](https://unsplash.com/@thomasfos?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    åœ¨ [Unsplash](https://unsplash.com/photos/vWgoeEYdtIY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Matrix multiplication; the holy grail of deep neural networks and modern language
    understanding behemoths. As MLEs or data scientists, our fingers are too quick
    to type `tf.matmul` or `torch.matmul` and we never look back. But donâ€™t tell me
    youâ€™ve never had the millisecond infatuation to know what might be happening to
    that matrix when it enters the GPU! If you did, youâ€™re in the right place. Join
    me in a journey through the fascinating intricacies within a GPU.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ©é˜µä¹˜æ³•ï¼›æ·±åº¦ç¥ç»ç½‘ç»œå’Œç°ä»£è¯­è¨€ç†è§£å·¨å¤´çš„åœ£æ¯ã€‚ä½œä¸ºMLEæˆ–æ•°æ®ç§‘å­¦å®¶ï¼Œæˆ‘ä»¬çš„æ‰‹æŒ‡å¤ªå¿«äº†ï¼Œä»¥è‡³äºåªéœ€æ•²å…¥ `tf.matmul` æˆ– `torch.matmul`ï¼Œç„¶åä»æœªå›å¤´ã€‚ä½†ä¸è¦å‘Šè¯‰æˆ‘ä½ ä»æœªå¯¹çŸ©é˜µè¿›å…¥GPUæ—¶å¯èƒ½å‘ç”Ÿçš„æƒ…å†µæ„Ÿåˆ°è¿‡ä¸€ä¸å…´å¥‹ï¼å¦‚æœä½ æœ‰ï¼Œé‚£ä½ æ¥å¯¹åœ°æ–¹äº†ã€‚è·Ÿéšæˆ‘ï¼Œä¸€èµ·æ¢ç´¢GPUå†…éƒ¨é‚£äº›è¿·äººçš„å¤æ‚æ€§ã€‚
- en: Iâ€™ll explain to you how these compute powerhouses crunch up the numbers. Youâ€™ll
    learn three little-known impressive things GPUs do, when they come face-to-face
    with matrices. By the end of this blog post, youâ€™ll have a good understanding
    of how matrix multiplication works inside GPUs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†å‘ä½ è§£é‡Šè¿™äº›è®¡ç®—å¼ºè€…æ˜¯å¦‚ä½•å¤„ç†æ•°å­—çš„ã€‚ä½ å°†äº†è§£GPUåœ¨é¢å¯¹çŸ©é˜µæ—¶åšçš„ä¸‰ä»¶é²œä¸ºäººçŸ¥çš„ä»¤äººå°è±¡æ·±åˆ»çš„äº‹æƒ…ã€‚åœ¨æœ¬æ–‡ç»“æŸæ—¶ï¼Œä½ å°†å¯¹GPUå†…éƒ¨çš„çŸ©é˜µä¹˜æ³•æœ‰ä¸€ä¸ªæ¸…æ™°çš„ç†è§£ã€‚
- en: 'GEMM: A true gem ğŸ’ for a GPU'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GEMMï¼šä¸€ä¸ªçœŸæ­£çš„ğŸ’ä¸ºGPUæ‰€ç”¨
- en: GEMM or generalized matrix multiplication is the kernel thatâ€™s executed when
    GPUs perform matrix multiplication.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: GEMM æˆ–å¹¿ä¹‰çŸ©é˜µä¹˜æ³•æ˜¯å½“GPUæ‰§è¡ŒçŸ©é˜µä¹˜æ³•æ—¶è¿è¡Œçš„å†…æ ¸ã€‚
- en: '`C = a (A.B) + b C`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`C = a (A.B) + b C`'
- en: Here, `a` and `b` are scalars, `A` is an `MxK` matrix, `B` is an `KxN` matrix,
    and thus `C` is an `MxN` matrix. Itâ€™s easy as that! You might wonder why that
    trailing addition exists. Turns out this is a pretty common pattern for neural
    networks (e.g. adding bias, applying ReLU, adding residual connections).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ`a` å’Œ `b` æ˜¯æ ‡é‡ï¼Œ`A` æ˜¯ä¸€ä¸ª `MxK` çŸ©é˜µï¼Œ`B` æ˜¯ä¸€ä¸ª `KxN` çŸ©é˜µï¼Œå› æ­¤ `C` æ˜¯ä¸€ä¸ª `MxN` çŸ©é˜µã€‚å°±æ˜¯è¿™ä¹ˆç®€å•ï¼ä½ å¯èƒ½ä¼šå¥½å¥‡ä¸ºä»€ä¹ˆä¼šæœ‰é‚£ä¸€ä¸ªé¢å¤–çš„åŠ æ³•ã€‚äº‹å®è¯æ˜ï¼Œè¿™æ˜¯ä¸€ç§åœ¨ç¥ç»ç½‘ç»œä¸­éå¸¸å¸¸è§çš„æ¨¡å¼ï¼ˆä¾‹å¦‚ï¼Œæ·»åŠ åç½®ã€åº”ç”¨ReLUã€æ·»åŠ æ®‹å·®è¿æ¥ï¼‰ã€‚
- en: 'Trick #1: Outer product is out-er this world ğŸ‘½'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'æŠ€å·§ #1ï¼šå¤–ç§¯çš„æ•ˆæœå‡ºä¹æ„æ–™ğŸ‘½'
- en: If youâ€™re asked to write a matrix multiplication algorithm from first principles,
    hereâ€™s what youâ€™ll do (unless youâ€™re gifted with a GPU in lieu of a brain â€” wouldnâ€™t
    that save money for an MLE!).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¢«è¦æ±‚ä»åŸºæœ¬åŸç†å‡ºå‘ç¼–å†™ä¸€ä¸ªçŸ©é˜µä¹˜æ³•ç®—æ³•ï¼Œè¿™é‡Œæ˜¯ä½ è¦åšçš„ï¼ˆé™¤éä½ å¤©ç”Ÿèªæ…§ï¼Œèƒ½ç”¨GPUä»£æ›¿å¤§è„‘â€”â€”é‚£æ ·ä¸å°±èƒ½ä¸ºMLEçœä¸‹ä¸å°‘é’±äº†ï¼ï¼‰ã€‚
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Hereâ€™s an animated visual that shows you what this does.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªåŠ¨ç”»è§†è§‰æ•ˆæœï¼Œå¯ä»¥å±•ç¤ºè¿™æ˜¯ä»€ä¹ˆæ•ˆæœã€‚
- en: '![](../Images/4f0c3e4a00f1242d2e82a1d450984883.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f0c3e4a00f1242d2e82a1d450984883.png)'
- en: 'Inner product based multiplication of two matrices (Recreated by author â€” source
    of inspiration: [https://www.adityaagrawal.net/blog/architecture/matrix_multiplication](https://www.adityaagrawal.net/blog/architecture/matrix_multiplication))'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå†…ç§¯çš„ä¸¤ä¸ªçŸ©é˜µçš„ä¹˜æ³•ï¼ˆç”±ä½œè€…é‡ç° â€” çµæ„Ÿæ¥æºï¼š[https://www.adityaagrawal.net/blog/architecture/matrix_multiplication](https://www.adityaagrawal.net/blog/architecture/matrix_multiplication)ï¼‰
- en: But did you know GPUs despise this implementation ğŸ¤”? To understand why thatâ€™s
    the case, you need to understand the GPU memory architecture,
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä½ çŸ¥é“GPUè®¨åŒè¿™ç§å®ç°å—ğŸ¤”ï¼Ÿè¦ç†è§£åŸå› ï¼Œä½ éœ€è¦äº†è§£GPUå†…å­˜æ¶æ„ï¼Œ
- en: For all comparisons and specifications, Iâ€™ll be using the Nvidia A100 GPU specifications.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ¯”è¾ƒå’Œè§„æ ¼ï¼Œæˆ‘å°†ä½¿ç”¨Nvidia A100 GPUè§„æ ¼ã€‚
- en: A GPU has three main memory levels,
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GPUæœ‰ä¸‰ä¸ªä¸»è¦çš„å†…å­˜çº§åˆ«ï¼Œ
- en: Global memory or HBM (what you typically refer to as GPU memory and what you
    see when you run `nvidia-smi` )
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¨å±€å†…å­˜æˆ–HBMï¼ˆé€šå¸¸æŒ‡çš„GPUå†…å­˜ï¼Œä»¥åŠä½ åœ¨è¿è¡Œ`nvidia-smi`æ—¶çœ‹åˆ°çš„å†…å­˜ï¼‰
- en: Shared memory (a local memory that is dedicated to a single streaming multiprocessor
    [or SM] and shared between threads running in that SM)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…±äº«å†…å­˜ï¼ˆä¸“é—¨åˆ†é…ç»™ä¸€ä¸ªæµå¼å¤šå¤„ç†å™¨[æˆ–SM]çš„æœ¬åœ°å†…å­˜ï¼Œå¹¶åœ¨è¯¥SMä¸­è¿è¡Œçš„çº¿ç¨‹ä¹‹é—´å…±äº«ï¼‰
- en: Registers (individually allocated to threads to carry out their workload)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯„å­˜å™¨ï¼ˆåˆ†é…ç»™çº¿ç¨‹ä»¥æ‰§è¡Œå…¶å·¥ä½œè´Ÿè½½ï¼‰
- en: This is what it looks like,
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å®ƒçš„æ ·å­ï¼Œ
- en: '![](../Images/df76c1f0c3b54d513f3aaa48519ea1c5.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df76c1f0c3b54d513f3aaa48519ea1c5.png)'
- en: The typical memory hierarchy of a GPU (L0/L1/L2 caches ignored for simplicity)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GPUçš„å…¸å‹å†…å­˜å±‚æ¬¡ç»“æ„ï¼ˆä¸ºäº†ç®€ä¾¿ï¼Œå¿½ç•¥L0/L1/L2ç¼“å­˜ï¼‰
- en: The first thing to note is that shared memory (referred to as SRAM from now
    on) is way smaller than the HBM, let alone registers. So your matrix is not going
    to fit in there (in most occasions). If we go back to our animation, for a single
    row of `A` all columns of `B` needs to be retrieved, and repeat the process for
    all rows in `A`. This means, the GPU needs to do many-many reads to compute the
    output. The HBM (~1.5TB/s) is several magnitudes slower than SRAM (~19TB/s).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆè¦æ³¨æ„çš„æ˜¯ï¼Œå…±äº«å†…å­˜ï¼ˆä»ç°åœ¨èµ·ç§°ä¸ºSRAMï¼‰æ¯”HBMå°å¾—å¤šï¼Œæ›´ä¸ç”¨è¯´å¯„å­˜å™¨äº†ã€‚æ‰€ä»¥ä½ çš„çŸ©é˜µä¸å¯èƒ½é€‚åˆå…¶ä¸­ï¼ˆåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼‰ã€‚å¦‚æœæˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„åŠ¨ç”»ï¼Œå¯¹äº`A`çš„ä¸€è¡Œï¼Œéœ€è¦æ£€ç´¢`B`çš„æ‰€æœ‰åˆ—ï¼Œå¹¶å¯¹`A`ä¸­çš„æ‰€æœ‰è¡Œé‡å¤æ­¤è¿‡ç¨‹ã€‚è¿™æ„å‘³ç€ï¼ŒGPUéœ€è¦è¿›è¡Œå¤§é‡è¯»å–æ¥è®¡ç®—è¾“å‡ºã€‚HBMï¼ˆçº¦1.5TB/sï¼‰æ¯”SRAMï¼ˆçº¦19TB/sï¼‰æ…¢å‡ ä¸ªæ•°é‡çº§ã€‚
- en: To put that in numbers, say you want to multiply a `10x20` and `20x30` matrix,
    you need to read columns of `B` `10x30=300` times. Is there a better way we can
    do this?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨æ•°å­—æ¥è¯´ï¼Œå‡è®¾ä½ æƒ³ä¹˜ä»¥ä¸€ä¸ª`10x20`å’Œ`20x30`çš„çŸ©é˜µï¼Œä½ éœ€è¦è¯»å–`B`çš„åˆ—`10x30=300`æ¬¡ã€‚æœ‰æ²¡æœ‰æ›´å¥½çš„æ–¹æ³•å‘¢ï¼Ÿ
- en: Turns out a simple trick can go a long way here! Simply flip the order of the
    loops, so that `k` becomes the outer most loop. And youâ€™re done! ğŸ˜®
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®è¯æ˜ä¸€ä¸ªç®€å•çš„æŠ€å·§èƒ½å¤§æœ‰å¸®åŠ©ï¼åªéœ€ç¿»è½¬å¾ªç¯çš„é¡ºåºï¼Œä½¿`k`æˆä¸ºæœ€å¤–å±‚çš„å¾ªç¯ã€‚å°±å®Œæˆäº†ï¼ğŸ˜®
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We did not touch the actual computation, just the order of the loops, so we
    should get the same result as before. Hereâ€™s what the matrix multiplication looks
    like now!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ²¡æœ‰è§¦åŠå®é™…è®¡ç®—ï¼Œåªæ˜¯è°ƒæ•´äº†å¾ªç¯çš„é¡ºåºï¼Œå› æ­¤ç»“æœåº”è¯¥å’Œä¹‹å‰ä¸€æ ·ã€‚ç°åœ¨çŸ©é˜µä¹˜æ³•çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ï¼
- en: '![](../Images/2b847b863d6664b2bc0cf87042f7a6ab.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b847b863d6664b2bc0cf87042f7a6ab.png)'
- en: 'Outer product based multiplication of two matrices (Recreated by author â€” source
    of inspiration: [https://www.adityaagrawal.net/blog/architecture/matrix_multiplication](https://www.adityaagrawal.net/blog/architecture/matrix_multiplication))'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå¤–ç§¯çš„ä¸¤ä¸ªçŸ©é˜µçš„ä¹˜æ³•ï¼ˆç”±ä½œè€…é‡ç° â€” çµæ„Ÿæ¥æºï¼š[https://www.adityaagrawal.net/blog/architecture/matrix_multiplication](https://www.adityaagrawal.net/blog/architecture/matrix_multiplication)ï¼‰
- en: You see, we only bring one *column* of `A` and one *row* of `B` at a time and
    never look back. This requires far less reads than the original implementation.
    The only difference is we were computing the *inner product* between two vectors
    before, now weâ€™re computing the *outer product*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çœ‹ï¼Œæˆ‘ä»¬ä¸€æ¬¡åªå¸¦æ¥`A`çš„ä¸€ä¸ª*åˆ—*å’Œ`B`çš„ä¸€ä¸ª*è¡Œ*ï¼Œå¹¶ä¸”ä¸ä¼šå›å¤´çœ‹ã€‚è¿™éœ€è¦çš„è¯»å–æ¬¡æ•°æ¯”åŸå§‹å®ç°å°‘å¾—å¤šã€‚å”¯ä¸€çš„åŒºåˆ«æ˜¯ï¼Œæˆ‘ä»¬ä¹‹å‰è®¡ç®—çš„æ˜¯ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„*å†…ç§¯*ï¼Œç°åœ¨æˆ‘ä»¬è®¡ç®—çš„æ˜¯*å¤–ç§¯*ã€‚
- en: '![](../Images/d844eb8589acdba180851c314b96e206.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d844eb8589acdba180851c314b96e206.png)'
- en: The difference between inner product and outer product shown in green for two
    vectors (blue and yellow).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç»¿è‰²æ˜¾ç¤ºçš„ä¸¤ä¸ªå‘é‡ï¼ˆè“è‰²å’Œé»„è‰²ï¼‰ä¹‹é—´çš„å†…ç§¯å’Œå¤–ç§¯çš„åŒºåˆ«ã€‚
- en: But still, we need entire `C` in SRAM, which might be too big to fit in SRAM.
    What does CUDA do then? That brings us to the second trick.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä»ç„¶ï¼Œæˆ‘ä»¬éœ€è¦æ•´ä¸ª`C`åœ¨SRAMä¸­ï¼Œè¿™å¯èƒ½å¤ªå¤§è€Œæ— æ³•å®¹çº³åœ¨SRAMä¸­ã€‚é‚£ä¹ˆCUDAæ˜¯æ€ä¹ˆåšçš„å‘¢ï¼Ÿè¿™å°±å¼•å‡ºäº†ç¬¬äºŒä¸ªæŠ€å·§ã€‚
- en: 'Trick #2: Divide and conquer (and accumulate)'
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'æŠ€å·§ #2ï¼šåˆ†è€Œæ²»ä¹‹ï¼ˆå¹¶ç´¯ç§¯ï¼‰'
- en: Not to worry! Iâ€™m not going to blast you with any complex mathematics or Leetcode
    algorithms. The main thing to keep in mind is, a matrix is a 2D layout of individual
    tiles. The following animation does justice to what Iâ€™m trying to explain.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ç”¨æ‹…å¿ƒï¼æˆ‘ä¸ä¼šç”¨å¤æ‚çš„æ•°å­¦æˆ–Leetcodeç®—æ³•è½°ç‚¸ä½ ã€‚ä¸»è¦è¦è®°ä½çš„æ˜¯ï¼ŒçŸ©é˜µæ˜¯å•ç‹¬ç“¦ç‰‡çš„äºŒç»´å¸ƒå±€ã€‚ä¸‹é¢çš„åŠ¨ç”»å¾ˆå¥½åœ°å±•ç¤ºäº†æˆ‘è¯•å›¾è§£é‡Šçš„å†…å®¹ã€‚
- en: '![](../Images/52fbea7b18ac1a73b3158ececd0f9c16.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52fbea7b18ac1a73b3158ececd0f9c16.png)'
- en: You can iterate each block in A and B and still compute the exact answer for
    Câ€™s corresponding block
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥è¿­ä»£Aå’ŒBä¸­çš„æ¯ä¸ªå—ï¼Œä»ç„¶å¯ä»¥è®¡ç®—Cå¯¹åº”å—çš„å‡†ç¡®ç­”æ¡ˆ
- en: The result of the green block ğŸ’š is the light blue strip of A ğŸ’™ and the light
    yellow strip of B ğŸ’›. Taking this a step further, to compute the output, you can
    bring one block of that strip of A and one block from Bâ€™s strip at a time, compute
    the output and accumulate the result in the green box.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç»¿è‰²å—ğŸ’šçš„ç»“æœæ˜¯æµ…è“è‰²çš„Aæ¡ğŸ’™å’Œæµ…é»„è‰²çš„Bæ¡ğŸ’›ã€‚æ›´è¿›ä¸€æ­¥ï¼Œä¸ºäº†è®¡ç®—è¾“å‡ºï¼Œä½ å¯ä»¥ä¸€æ¬¡å¸¦æ¥ä¸€ä¸ªAæ¡çš„å—å’Œä¸€ä¸ªBæ¡çš„å—ï¼Œè®¡ç®—è¾“å‡ºå¹¶å°†ç»“æœç´¯ç§¯åœ¨ç»¿è‰²æ¡†ä¸­ã€‚
- en: This gives us a flexible framework where we can load an arbitrary size block
    (or tile) of A and B and still compute the final answer. We donâ€™t have to stop
    there, we can keep recursively dividing the problem to even smaller problems.
    i.e. the matrix is broken into tiles, tiles are broken into fragments, and fragments
    to individual values.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªçµæ´»çš„æ¡†æ¶ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½ä»»æ„å¤§å°çš„Aå’ŒBçš„å—ï¼ˆæˆ–ç“¦ç‰‡ï¼‰ï¼Œä»ç„¶èƒ½å¤Ÿè®¡ç®—æœ€ç»ˆçš„ç­”æ¡ˆã€‚æˆ‘ä»¬ä¸å¿…æ­¢æ­¥äºæ­¤ï¼Œå¯ä»¥ç»§ç»­é€’å½’åœ°å°†é—®é¢˜åˆ’åˆ†ä¸ºæ›´å°çš„é—®é¢˜ã€‚å³çŸ©é˜µè¢«æ‹†åˆ†æˆç“¦ç‰‡ï¼Œç“¦ç‰‡è¢«æ‹†åˆ†æˆç‰‡æ®µï¼Œç‰‡æ®µè¿›ä¸€æ­¥æ‹†åˆ†æˆå•ä¸ªå€¼ã€‚
- en: '![](../Images/9ebe0775cebde0334adaa9b57b088d8c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ebe0775cebde0334adaa9b57b088d8c.png)'
- en: Using the tiling approach, the problem can be broken down recursively
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç“¦ç‰‡æ–¹æ³•ï¼Œé—®é¢˜å¯ä»¥é€’å½’åœ°æ‹†è§£ã€‚
- en: And this lends itself nicely to the process execution architecture in a GPU.
    There are three layers to a kernel execution in a GPU. For simplicity, weâ€™ll say
    a SM runs a single thread block (although in practice they execute them concurrently,
    to reduce something known as the [tail effect](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#gpu-execution)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆå¥½åœ°é€‚ç”¨äºGPUä¸­çš„è¿‡ç¨‹æ‰§è¡Œæ¶æ„ã€‚GPUä¸­çš„å†…æ ¸æ‰§è¡Œæœ‰ä¸‰å±‚ã€‚ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬è¯´ä¸€ä¸ªSMè¿è¡Œä¸€ä¸ªçº¿ç¨‹å—ï¼ˆè™½ç„¶å®é™…ä¸Šå®ƒä»¬æ˜¯å¹¶è¡Œæ‰§è¡Œçš„ï¼Œä»¥å‡å°‘è¢«ç§°ä¸º[å°¾æ•ˆåº”](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#gpu-execution)çš„å½±å“ï¼‰ã€‚
- en: Threads
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çº¿ç¨‹
- en: Warps (a collection of 32 threads)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Warpsï¼ˆ32ä¸ªçº¿ç¨‹çš„é›†åˆï¼‰
- en: Thread blocks (a collection of several warps)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çº¿ç¨‹å—ï¼ˆå‡ ä¸ªwarpçš„é›†åˆï¼‰
- en: The exact number of threads in a thread block depends on a specific architecture.
    For example, an [A100 has the following specifications](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿ç¨‹å—ä¸­çš„ç¡®åˆ‡çº¿ç¨‹æ•°é‡å–å†³äºç‰¹å®šçš„æ¶æ„ã€‚ä¾‹å¦‚ï¼Œ[A100å…·æœ‰ä»¥ä¸‹è§„æ ¼](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)ã€‚
- en: Maximum of 2048 threads per SM
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªSMæœ€å¤§æ”¯æŒ2048ä¸ªçº¿ç¨‹
- en: Maximum of 1024 threads per block
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå—æœ€å¤§æ”¯æŒ1024ä¸ªçº¿ç¨‹
- en: Maximum of 32 thread blocks per SM
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªSMæœ€å¤§æ”¯æŒ32ä¸ªçº¿ç¨‹å—
- en: 'Sidebar #2: Magic of the power of 2'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'ä¾§è¾¹æ  #2ï¼š2çš„å¹‚çš„é­”åŠ›'
- en: Going back to the tiling, It has been found that (heuristically) a matrix tile
    of size `256x128` per thread block gives reasonable efficiency for most problems.
    Therefore itâ€™s a common tile size used by CUDA.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å›åˆ°ç“¦ç‰‡æ–¹æ³•ï¼Œå·²å‘ç°ï¼ˆå¯å‘å¼åœ°ï¼‰æ¯ä¸ªçº¿ç¨‹å—çš„çŸ©é˜µç“¦ç‰‡å¤§å°ä¸º`256x128`åœ¨å¤§å¤šæ•°é—®é¢˜ä¸­èƒ½æä¾›åˆç†çš„æ•ˆç‡ã€‚å› æ­¤ï¼Œè¿™æ˜¯CUDAå¸¸ç”¨çš„ç“¦ç‰‡å¤§å°ã€‚
- en: You might have heard about a best practice of keeping batch size, hidden dimension
    size as powers of two. This is where this comes from! When your matrix dimensions
    are of powers of two, it will be fully divisible to a set of tiles with no remainder.
    If not, it makes your code [less efficient](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½å¬è¯´è¿‡å°†æ‰¹å¤„ç†å¤§å°ã€éšè—ç»´åº¦å¤§å°ä¿æŒä¸º2çš„å¹‚çš„æœ€ä½³å®è·µã€‚è¿™å°±æ˜¯æ¥æºï¼å½“ä½ çš„çŸ©é˜µç»´åº¦æ˜¯2çš„å¹‚æ—¶ï¼Œå®ƒå°†è¢«å®Œå…¨åˆ’åˆ†ä¸ºä¸€ç»„æ²¡æœ‰ä½™æ•°çš„ç“¦ç‰‡ã€‚å¦‚æœä¸æ˜¯ï¼Œè¿™ä¼šä½¿ä½ çš„ä»£ç [æ•ˆç‡æ›´ä½](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc)ã€‚
- en: GPU computations are more efficient when your matrix dimensions are in the power
    of 2
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å½“ä½ çš„çŸ©é˜µç»´åº¦æ˜¯2çš„å¹‚æ—¶ï¼ŒGPUè®¡ç®—ä¼šæ›´é«˜æ•ˆ
- en: What happens when itâ€™s not a power of 2?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä¸æ˜¯2çš„å¹‚æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
- en: 'Sidebar #2: Tile quantization'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'ä¾§è¾¹æ  #2ï¼šç“¦ç‰‡é‡åŒ–'
- en: What happens is an effect known as [*tile quantization*](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#tile-quant).
    In other words, if you have a tile row dimension of 128 but your matrix has 257
    elements in a row, youâ€™ll need not two, but three tiles in a row (i.e. 256+1).
    This is illustrated below.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šå¯¼è‡´ä¸€ç§å«åš [*tile quantization*](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#tile-quant)
    çš„ç°è±¡ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœä½ çš„ tile è¡Œç»´åº¦ä¸º 128ï¼Œä½†çŸ©é˜µçš„è¡Œä¸­æœ‰ 257 ä¸ªå…ƒç´ ï¼Œä½ éœ€è¦çš„ä¸ä»…ä»…æ˜¯ä¸¤ä¸ªï¼Œè€Œæ˜¯ä¸‰ä¸ª tilesï¼ˆå³ 256+1ï¼‰ã€‚å¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/6c0e19fcfcf9ef9bf3d1da19ff0313ca.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c0e19fcfcf9ef9bf3d1da19ff0313ca.png)'
- en: Just because we had on extra element in rows, we have to dedicate two entire
    thread blocks
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…ä»…å› ä¸ºè¡Œä¸­å¤šäº†ä¸€ä¸ªå…ƒç´ ï¼Œæˆ‘ä»¬ä¸å¾—ä¸åˆ†é…ä¸¤ä¸ªå®Œæ•´çš„çº¿ç¨‹å—ã€‚
- en: Problem with this is that, the thread block does the same amount of computation
    regardless of the useful data residing in it. So, youâ€™re taking the opportunity
    to do useful computation away from your GPU, leading to inefficiencies.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜åœ¨äºï¼Œæ— è®ºçº¿ç¨‹å—ä¸­æœ‰ç”¨çš„æ•°æ®å¦‚ä½•ï¼Œçº¿ç¨‹å—æ‰§è¡Œçš„è®¡ç®—é‡æ˜¯ç›¸åŒçš„ã€‚å› æ­¤ï¼Œä½ ä¼šæŠŠæ‰§è¡Œæœ‰ç”¨è®¡ç®—çš„æœºä¼šä» GPU ä¸­æ‹¿èµ°ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
- en: A similar effect is known as wave quantization, where the matrix is over-sized
    and the SMs collectively cannot fit it at once. Then the GPU needs to do the computation
    in 2 â€œwavesâ€. However this is less of a concern for modern GPUs as they leverage
    concurrency to reduce wave quantization.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼çš„ç°è±¡ç§°ä¸º wave é‡åŒ–ï¼Œå³çŸ©é˜µè¿‡å¤§ï¼ŒSMs æ— æ³•ä¸€æ¬¡æ€§å®¹çº³ã€‚è¿™æ—¶ GPU éœ€è¦è¿›è¡Œ 2 æ¬¡â€œæ³¢â€ã€‚ä¸è¿‡ï¼Œè¿™å¯¹ç°ä»£ GPU å½±å“è¾ƒå°ï¼Œå› ä¸ºå®ƒä»¬åˆ©ç”¨å¹¶å‘æ€§æ¥å‡å°‘
    wave é‡åŒ–ã€‚
- en: Tile quantization happens when a thread block has to spill data partially, wave
    quantization happens when SMs have to spill data.
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å½“çº¿ç¨‹å—éœ€è¦éƒ¨åˆ†æº¢å‡ºæ•°æ®æ—¶ï¼Œå°±ä¼šå‘ç”Ÿ tile é‡åŒ–ï¼›å½“ SMs éœ€è¦æº¢å‡ºæ•°æ®æ—¶ï¼Œå°±ä¼šå‘ç”Ÿ wave é‡åŒ–ã€‚
- en: 'Trick #3: One is better than two'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'æŠ€å·§ #3ï¼šä¸€ä¸ªæ¯”ä¸¤ä¸ªå¥½'
- en: The final trick is kernel fusion. More often than not, it is faster to do all
    the computations in one kernel than having two kernels called one after the other.
    Why? Because one kernel needs to write the data to HBM and other needs to read
    that back. We already talked about how slow this is. A better approach is just
    combine the two operations into one. Some examples are,
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆçš„æŠ€å·§æ˜¯å†…æ ¸èåˆã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œå°†æ‰€æœ‰è®¡ç®—æ”¾åœ¨ä¸€ä¸ªå†…æ ¸ä¸­è¦æ¯”ä¸€ä¸ªæ¥ä¸€ä¸ªè°ƒç”¨ä¸¤ä¸ªå†…æ ¸è¦å¿«ã€‚ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºä¸€ä¸ªå†…æ ¸éœ€è¦å°†æ•°æ®å†™å…¥ HBMï¼Œå¦ä¸€ä¸ªéœ€è¦è¯»å–æ•°æ®ã€‚æˆ‘ä»¬å·²ç»è®¨è®ºè¿‡è¿™ç§æ“ä½œæœ‰å¤šæ…¢ã€‚æ›´å¥½çš„æ–¹æ³•æ˜¯å°†ä¸¤ä¸ªæ“ä½œåˆå¹¶ä¸ºä¸€ä¸ªã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹ï¼Œ
- en: '[matmul + bias + relu](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/matmul_op_fused.cc)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[matmul + bias + relu](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/matmul_op_fused.cc)'
- en: '[conv + bias + relu](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_fused_impl.h)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[conv + bias + relu](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_fused_impl.h)'
- en: '[batchnorm + relu](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fused_batch_norm_op.h)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[batchnorm + relu](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fused_batch_norm_op.h)'
- en: So as it is seen here (Iâ€™m sure Pytorch has a similar glossary), there are many
    fused kernels offered through TensorFlow that combines commodious operations in
    to a single kernel. In code, it means something like this,
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚è¿™é‡Œæ‰€çœ‹åˆ°çš„ï¼ˆæˆ‘ç›¸ä¿¡ Pytorch ä¹Ÿæœ‰ç±»ä¼¼çš„æœ¯è¯­è¡¨ï¼‰ï¼ŒTensorFlow æä¾›äº†è®¸å¤šèåˆçš„å†…æ ¸ï¼Œå°†ä¾¿æ·çš„æ“ä½œåˆå¹¶åˆ°ä¸€ä¸ªå†…æ ¸ä¸­ã€‚åœ¨ä»£ç ä¸­ï¼Œè¿™æ„å‘³ç€ç±»ä¼¼äºä»¥ä¸‹å†…å®¹ï¼Œ
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In other words, we hold dearly to our `tmp` variable until after weâ€™ve finished
    all our computations. Then only weâ€™ll write the result back to `C` .
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬ä¼šçæƒœæˆ‘ä»¬çš„ `tmp` å˜é‡ï¼Œç›´åˆ°å®Œæˆæ‰€æœ‰è®¡ç®—åæ‰å°†ç»“æœå†™å› `C`ã€‚
- en: Conclusion
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Thatâ€™s it folks. I hope this was an enjoyable excursion through the weeds of
    a GPU. If youâ€™re interested in the audio-visual version hereâ€™s the link to my
    YouTube video.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™äº›äº†ï¼Œå¸Œæœ›è¿™æ¬¡é€šè¿‡ GPU çš„â€œä¸›æ—æ¢é™©â€å¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚å¦‚æœä½ å¯¹éŸ³é¢‘è§†è§‰ç‰ˆæœ¬æ„Ÿå…´è¶£ï¼Œè¿™é‡Œæ˜¯æˆ‘çš„ YouTube è§†é¢‘çš„é“¾æ¥ã€‚
- en: To recap, we discussed three things that makes GPUs really fast at matrix multiplication.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œæˆ‘ä»¬è®¨è®ºäº†ä¸‰ä»¶ä½¿ GPU åœ¨çŸ©é˜µä¹˜æ³•ä¸­éå¸¸å¿«é€Ÿçš„äº‹æƒ…ã€‚
- en: GPUs abandon the friendlier inner product implementation of matmul and embrace
    the more read-efficient outer product implementation of matmul
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPUs æ‘’å¼ƒäº†æ›´å‹å¥½çš„å†…ç§¯å®ç°ï¼Œè½¬è€Œé‡‡ç”¨æ›´é«˜æ•ˆçš„å¤–ç§¯å®ç°ã€‚
- en: GPUs split the matrices into smaller blocks (and blocks into fragments) and
    split the compute load across thread blocks, warps and threads.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPUs å°†çŸ©é˜µåˆ†å‰²æˆæ›´å°çš„å—ï¼ˆå—å†åˆ†å‰²æˆç¢ç‰‡ï¼‰ï¼Œå¹¶å°†è®¡ç®—è´Ÿè½½åˆ†é…åˆ°çº¿ç¨‹å—ã€warp å’Œçº¿ç¨‹ä¸Šã€‚
- en: GPUs employ kernel fusion to bring commonly co-occurring functionality togetter,
    improving GPU efficiency.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPUs ä½¿ç”¨å†…æ ¸èåˆå°†å¸¸è§çš„åŠŸèƒ½ç»„åˆåœ¨ä¸€èµ·ï¼Œæé«˜ GPU æ•ˆç‡ã€‚
- en: If you enjoyed this story, feel free [subscribe](https://thushv89.medium.com/membership)
    to Medium, and you will get notifications to fresh content from me, as well as
    unlock full access to thousands of quality stories from other authors.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å–œæ¬¢è¿™ä¸ªæ•…äº‹ï¼Œå¯ä»¥éšæ—¶[è®¢é˜…](https://thushv89.medium.com/membership)Mediumï¼Œè¿™æ ·ä½ å°±ä¼šæ”¶åˆ°æˆ‘çš„æœ€æ–°å†…å®¹é€šçŸ¥ï¼Œå¹¶ä¸”èƒ½å¤Ÿè§£é”æ¥è‡ªå…¶ä»–ä½œè€…çš„æˆåƒä¸Šä¸‡çš„ä¼˜è´¨æ•…äº‹çš„å®Œæ•´è®¿é—®æƒé™ã€‚
- en: '[](https://thushv89.medium.com/membership?source=post_page-----cb7f6d7ae7d6--------------------------------)
    [## Join Medium with my referral link - Thushan Ganegedara'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://thushv89.medium.com/membership?source=post_page-----cb7f6d7ae7d6--------------------------------)
    [## é€šè¿‡æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥Medium - Thushan Ganegedara'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every storyâ€¦
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½œä¸ºMediumä¼šå‘˜ï¼Œä½ çš„ä¸€éƒ¨åˆ†ä¼šå‘˜è´¹ç”¨å°†ä¼šæ”¯æŒä½ é˜…è¯»çš„ä½œè€…ï¼Œå¹¶ä¸”ä½ å¯ä»¥å®Œå…¨è®¿é—®æ¯ä¸€ç¯‡æ•…äº‹â€¦
- en: thushv89.medium.com](https://thushv89.medium.com/membership?source=post_page-----cb7f6d7ae7d6--------------------------------)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://thushv89.medium.com/membership?source=post_page-----cb7f6d7ae7d6--------------------------------](https://thushv89.medium.com/membership?source=post_page-----cb7f6d7ae7d6--------------------------------)'
- en: '*Unless otherwise noted all images are by the author*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*é™¤éå¦æœ‰è¯´æ˜ï¼Œå¦åˆ™æ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…æä¾›*'
- en: 'References:'
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®ï¼š
- en: '[https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html)'
- en: '[https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/](https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/](https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/)'
- en: '[https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)'
