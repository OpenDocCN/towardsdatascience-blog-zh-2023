- en: Creating Scores and Rankings with PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/creating-scores-and-rankings-with-pca-c2c3081fdb26](https://towardsdatascience.com/creating-scores-and-rankings-with-pca-c2c3081fdb26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Use R Language to create scores for observations based on many variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://gustavorsantos.medium.com/?source=post_page-----c2c3081fdb26--------------------------------)[![Gustavo
    Santos](../Images/a19a9f4525cdeb6e7a76cd05246aa622.png)](https://gustavorsantos.medium.com/?source=post_page-----c2c3081fdb26--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c2c3081fdb26--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c2c3081fdb26--------------------------------)
    [Gustavo Santos](https://gustavorsantos.medium.com/?source=post_page-----c2c3081fdb26--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c2c3081fdb26--------------------------------)
    ·9 min read·Apr 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad848fe08cc9a8bc744f2e58a7a97f9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Joshua Golde](https://unsplash.com/@joshgmit?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/qIu77BsFdds?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The more I study about Principal Component Analysis [PCA], the more I like that
    tool. I have already written other posts about this matter, but I keep learning
    more about what’s “under the hood” of this beautiful math and, of course, I will
    share that knowledge with you.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is a set of mathematical transformations that work based on covariance and
    correlation of the data. So it basically looks at the data points and finds where
    there is the most variability. Once that is accomplished, the data is projected
    in that direction. The new data is projected on a new axis, called **Principal
    Component**.
  prefs: []
  type: TYPE_NORMAL
- en: The data is projected on a new axis to explain the most variability possible.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The projection itself, is the transformation. And the new data has many properties
    that can help us, data scientists, to better analyze the data. We can, for instance,
    perform a Factor Analysis, where similar variables are combined to form a single
    factor, reducing the dimensions of our data.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting property is the possibility to create ranks by similarity
    of the observations, like we are about to see in this post.
  prefs: []
  type: TYPE_NORMAL
- en: Scores and Rankings with PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will use the `mtcars`, a famous “toy dataset” with some
    information about cars. Despite this is a very well-known data already, it is
    still very good for us to work as a didactic example and it’s also open, under
    license GPL 3.0.
  prefs: []
  type: TYPE_NORMAL
- en: We can also load the library `tidyverse` for any data wrangling needed and `psych`
    for the PCA.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here is a small extract of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2be9052ceef2a7acad0fc836ddcad729.png)'
  prefs: []
  type: TYPE_IMG
- en: 'mtcars: native in Dplyr. Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: Coding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s get coding.
  prefs: []
  type: TYPE_NORMAL
- en: Important to say, PCA and Factor Analysis only work for quantitative data. So,
    if you have qualitative or categorical data, maybe Corresponce Analysis is a better
    fit for your case.
  prefs: []
  type: TYPE_NORMAL
- en: A good factors extraction using PCA requires that there will be statistically
    significant correlations between pairs of variables. If the correlations matrix
    have too many low correlations, the factors extracted may not be very good.
  prefs: []
  type: TYPE_NORMAL
- en: Bartlett’s Test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: But how to make sure they are? We can use the Bartlett’s test, under the *Ho*
    that the correlations are statistically equal to zero[p-value > 0.05] and Ha that
    the correlations are different than 0 [p-value ≤ 0.05].
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As we see, our result is a p-value equal to 0, so the Ho can be rejected and
    we can understand that the factors extracted will be adequate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can run the PCA portion using the library `psych`. We can use the
    function `pca()` for that task. We will input:'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset (with only numerical values)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of factors wanted. In this case, all the 11, so we are using the
    second position of the dimensions of the data (`dim(mtcars)[2]`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The rotation method: `none`. Now this can change our results, as we will also
    see. The default rotation is `“varimax”`, which aims to maximize the variance
    of the loadings on the factors, resulting in a simpler matrix, where each variable
    is highly associated with only one or a few factors, making it easier to interpret.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Once the code is run, we can check the Scree Plot, which will tell us how much
    variance was captured by each PC.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Next, the result is displayed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91c6acdf1c900aa880f9617b68078e72.png)'
  prefs: []
  type: TYPE_IMG
- en: Scree plot. 84% is captured by the first 2 components. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Kaiser’s criterium
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is looking at the PCs that we will keep for our analysis. A good
    way to do that is to look at the eigenvalues and determine which ones are over
    1\. This rule is also known as the Kaiser’s Latent Root Criterion.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that: (1) there are 11 eigenvalues, one for each PC extracted; (2) only
    the first two make the cut for the Kaiser’s rule. So let’s run the PCA again for
    only two components.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Plotting the variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To plot the variables, we will need to first collect the loadings. The loadings
    matrix show how correlated each variable is with each component. So the numbers
    will be between -1 and 1, keeping in mind that the closer to zero, the less correlated
    the PC and Variable are. The closer to 1/-1, the more correlated they are.
  prefs: []
  type: TYPE_NORMAL
- en: Loadings are how much correlated the variable is with the Principal Component.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then, since we only have two dimensions, we can easily plot them using ggplot2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The graphic displayed is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d90ff640f1489a49d8ae0bb3afc27b48.png)'
  prefs: []
  type: TYPE_IMG
- en: Loadings plot, showing the relationship between the variables based on PC1 x
    PC2\. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazing! Now we have a good idea of which variables are more correlated with
    each other. Miles Per Gallon, for example, is more related to number of gears,
    type of engine, type of transmission, drat. On the other hand, it is on the opposite
    side of HP and weight, what makes a lot of sense. Let’s think for a minute: *the
    more power a car has, the more gas it needs to burn. The same iss valid for weight.
    It is needed more power and more gas to move a heavier car, resulting in lower
    miles per gallon ratio.*'
  prefs: []
  type: TYPE_NORMAL
- en: Rotated Version
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ok, now that we looked through the PCA version without rotation, let’s look
    at the rotated version with the default `"varimax"` rotation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The same variance captured by 2 components (84%). But notice that the distribution
    of the variance now is more spread. Rotated Component RC1 [42%] and RC2 [41%];
    against PC1 [60%] and PC2[24%] in the version without rotation. However, the variables
    keep in similar positions, but now rotated a little.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/367607fcb8d50c6a8878b24310bc5b4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Loadings plot, showing the relationship between the variables based on RC1 x
    RC2\. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Communalities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last comparison to make between both PCAs [with rotation | without rotation]
    is about the communalities. Communality will show how much of the variance was
    lost in each variable after we applied the Kaiser’s rule and excluded some principal
    components from the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As seen, the variances captured are the very same in both methods.
  prefs: []
  type: TYPE_NORMAL
- en: Great. But does it affect the rankings? Let’s check next.
  prefs: []
  type: TYPE_NORMAL
- en: Rankings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we ran the PCA transformation, to create rankings it is really simple.
    All we need to do is to collect the Proportion of variance of the components with
    `pca2$Vaccounted[2,]` and the `pca$scores` and multiply them. So, for each score
    in PC1, we multiply it by the correspondent proportion of variance for that PCA
    run. Finally, we’ll add both scores to the original dataset mtcars.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The result is displayed next.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a824c6c40df7344a0b4a920a7d1fdbc.png)'
  prefs: []
  type: TYPE_IMG
- en: Rankings based on PCA / Factor Analysis. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The top table is the TOP10 for the **not rotated** PCA. Observe how it’s highlighting
    cars with low `mpg`, high `hp`, `cyl`, `wt`, `disp`, just like the loadings suggested.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom table is the TOP10 for the **varimax rotated** PCA. Because the variances
    are more spread between the two components, we see some differences. As an example,
    the `disp` variable is not so uniform anymore. In the not rotated version, PC1
    loadings was dominating that variable, with 94% correlation and almost not correlated
    in PC2\. For the varimax, it is -73% in RC1 and 60% RC2, so a bit confusing, thus
    it shows high and low numbers despite of the ranking. The same can be said about
    `mpg`.
  prefs: []
  type: TYPE_NORMAL
- en: Ranking by Correlated Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After we did all of this analysis, we can also set better criteria for the
    ranking creation. In our case of study, we could say: we want the best `mpg`,
    `drat` and `am` manual transmission (1). We already know that these variables
    are correlated, so it’s easier to use them combined for ranking.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: And the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6711f380473a24f91b5239ff59bb7931.png)'
  prefs: []
  type: TYPE_IMG
- en: Ranking by MPG, Drat and transmission. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the results make a lot of sense. Take the Honda Civic: it has high MPG,
    the highest drat in the dataset and am = 1\. Now look at the cars ranked as 4
    and 5\. The Porsche has a lower mpg, but much higher drat. The Lotus is the opposite.
    Success!'
  prefs: []
  type: TYPE_NORMAL
- en: Before You Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This post has the intention to show you an introduction to Factor Analysis with
    PCA. We could see the power of the tool in this tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: However, before performing the analysis, it is important to study the correlations
    of the variables and then set the criteria for ranking creation. It is also important
    to be aware that PCA is highly influenced by outliers. So if your data contains
    too many outliers, the ranks can get distorted. A solution to that is scaling
    the data (standardization).
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this content, don’t forget to follow my blog for more.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://gustavorsantos.medium.com/?source=post_page-----c2c3081fdb26--------------------------------)
    [## Gustavo Santos - Medium'
  prefs: []
  type: TYPE_NORMAL
- en: Read writing from Gustavo Santos on Medium. Data Scientist. I extract insights
    from data to help people and companies…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: gustavorsantos.medium.com](https://gustavorsantos.medium.com/?source=post_page-----c2c3081fdb26--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Find me on [Linkedin](https://www.linkedin.com/in/gurezende/) as well.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the Git Hub repo for this code.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/gurezende/Studying/tree/master/R/Factor%20Analysis%20PCA?source=post_page-----c2c3081fdb26--------------------------------)
    [## Studying/R/Factor Analysis PCA at master · gurezende/Studying'
  prefs: []
  type: TYPE_NORMAL
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/gurezende/Studying/tree/master/R/Factor%20Analysis%20PCA?source=post_page-----c2c3081fdb26--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FÁVERO, L.; BELFIORE, P. 2022\. [Manual de Análise de Dados](https://www.amazon.com.br/Manual-An%C3%A1lise-Dados-Luiz-F%C3%A1vero/dp/8535270876).
    1ed. LTC.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.datacamp.com/tutorial/pca-analysis-r](https://www.datacamp.com/tutorial/pca-analysis-r)'
  prefs: []
  type: TYPE_NORMAL
