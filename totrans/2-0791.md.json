["```py\n!pip install datasets\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"mrjunos/depression-reddit-cleaned\")\nprint(dataset['train'][2])\n\n'''\n{'text': 'anyone else instead of sleeping more when depressed stay up all night to avoid the next day from coming sooner may be the social anxiety in me but life is so much more peaceful when everyone else is asleep and not expecting thing of you',\n 'label': 1}\n'''\n```", "```py\n!pip install pip install adapter-transformers\n\nimport torch\nimport numpy as np\nfrom transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\nclass Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, input_data):\n        self.labels = [data for data in input_data['label']]\n        self.texts = [tokenizer(data,\n                               padding='max_length', max_length = 512, truncation=True,\n                                return_tensors=\"pt\") for data in input_data['text']]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def get_batch_labels(self, idx):\n        return np.array(self.labels[idx])\n\n    def get_batch_texts(self, idx):\n        return self.texts[idx]\n\n    def __getitem__(self, idx):\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n        return batch_texts, batch_y\n```", "```py\npip install adapter-transformers\n```", "```py\nfrom torch import nn\nfrom transformers import BertForSequenceClassification\n\nclass BertClassifier(nn.Module):\n\n    def __init__(self, model_id='bert-base-cased', num_class=2):\n        super(BertClassifier, self).__init__()\n        self.bert = BertForSequenceClassification.from_pretrained(model_id, num_labels=num_class)\n\n    def forward(self, input_id, mask):\n        output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n        return output\n```", "```py\nfrom torch.optim import Adam\nfrom tqdm import tqdm\n\ndef train(model, train_data, val_data, learning_rate, epochs):\n\n    # Fetch training and validation data in batch\n    train, val = Dataset(train_data), Dataset(val_data)\n    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr= learning_rate)\n\n    if use_cuda:\n       model = model.to(device)\n       criterion = criterion.to(device)\n\n    for epoch_num in range(epochs):\n\n        total_acc_train = 0\n        total_loss_train = 0\n\n        # Fine-tune the model\n        for train_input, train_label in tqdm(train_dataloader):\n\n            train_label = train_label.to(device)\n            mask = train_input['attention_mask'].to(device)\n            input_id = train_input['input_ids'].squeeze(1).to(device)\n\n            output = model(input_id, mask)[0]\n\n            batch_loss = criterion(output, train_label.long())\n            total_loss_train += batch_loss.item()\n\n            acc = (output.argmax(dim=1) == train_label).sum().item()\n            total_acc_train += acc\n\n            model.zero_grad()\n            batch_loss.backward()\n            optimizer.step()\n\n        total_acc_val = 0\n        total_loss_val = 0\n\n        # Validate the model\n        with torch.no_grad():\n\n            for val_input, val_label in val_dataloader:\n                val_label = val_label.to(device)\n                mask = val_input['attention_mask'].to(device)\n                input_id = val_input['input_ids'].squeeze(1).to(device)\n\n                output = model(input_id, mask)[0]\n\n                batch_loss = criterion(output, val_label.long())\n                total_loss_val += batch_loss.item()\n                acc = (output.argmax(dim=1) == val_label).sum().item()\n                total_acc_val += acc\n\n        print(\n            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train): .3f} \\\n            | Train Accuracy: {total_acc_train / len(train): .3f} \\\n            | Val Loss: {total_loss_val / len(val): .3f} \\\n            | Val Accuracy: {total_acc_val / len(val): .3f}')\n```", "```py\nEPOCHS = 10\nLR = 1e-7\n\nmodel = BertClassifier()\ndata = dataset['train'].shuffle(seed=42)\ntrain(model, data[:6500], data[6500:], LR, EPOCHS)\n\n100%|███████████████████████████████████ 3250/3250 [11:56<00:00,  4.54it/s]\nEpochs: 1 | Train Loss:  0.546 | Train Accuracy:  0.533 | Val Loss:  0.394 | Val Accuracy:  0.847\n100%|███████████████████████████████████ 3250/3250 [11:57<00:00,  4.53it/s]\nEpochs: 2 | Train Loss:  0.302 | Train Accuracy:  0.888 | Val Loss:  0.226 | Val Accuracy:  0.906\n100%|███████████████████████████████████ 3250/3250 [11:57<00:00,  4.53it/s]\nEpochs: 3 | Train Loss:  0.184 | Train Accuracy:  0.919 | Val Loss:  0.149 | Val Accuracy:  0.930\n100%|███████████████████████████████████ 3250/3250 [11:57<00:00,  4.53it/s]\nEpochs: 4 | Train Loss:  0.122 | Train Accuracy:  0.946 | Val Loss:  0.101 | Val Accuracy:  0.955\n100%|███████████████████████████████████ 3250/3250 [11:57<00:00,  4.53it/s]\nEpochs: 5 | Train Loss:  0.084 | Train Accuracy:  0.963 | Val Loss:  0.075 | Val Accuracy:  0.968\n100%|███████████████████████████████████ 3250/3250 [11:56<00:00,  4.53it/s]\nEpochs: 6 | Train Loss:  0.063 | Train Accuracy:  0.969 | Val Loss:  0.061 | Val Accuracy:  0.970\n100%|███████████████████████████████████ 3250/3250 [11:57<00:00,  4.53it/s]\nEpochs: 7 | Train Loss:  0.050 | Train Accuracy:  0.974 | Val Loss:  0.054 | Val Accuracy:  0.973\n100%|███████████████████████████████████ 3250/3250 [11:57<00:00,  4.53it/s]\nEpochs: 8 | Train Loss:  0.042 | Train Accuracy:  0.978 | Val Loss:  0.049 | Val Accuracy:  0.972\n100%|███████████████████████████████████ 3250/3250 [11:57<00:00,  4.53it/s]\nEpochs: 9 | Train Loss:  0.035 | Train Accuracy:  0.982 | Val Loss:  0.047 | Val Accuracy:  0.973\n100%|███████████████████████████████████ 3250/3250 [11:57<00:00,  4.53it/s]\nEpochs: 10 | Train Loss: 0.030 | Train Accuracy:  0.984 | Val Loss:  0.046 | Val Accuracy:  0.966\n```", "```py\ndef print_trainable_parameters(model):\n\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\nprint_trainable_parameters(model)\n'''\ntrainable params: 108311810 || all params: 108311810 || trainable%: 100.0\n'''\n```", "```py\nfrom transformers import AdapterConfig\nfrom transformers.adapters import BertAdapterModel\n\nclass BertClassifierWithAdapter(nn.Module):\n\n    def __init__(self, model_id='bert-base-cased', adapter_id='pfeiffer', \n                task_id = 'depression_reddit_dataset', num_class=2):\n\n        super(BertClassifierWithAdapter, self).__init__()\n\n        self.adapter_config = AdapterConfig.load(adapter_id)\n\n        self.bert = BertAdapterModel.from_pretrained(model_id)\n        # Insert adapter according to configuration\n        self.bert.add_adapter(task_id, config=self.adapter_config)\n        # Freeze all BERT-base weights \n        self.bert.train_adapter(task_id)\n        # Add prediction layer on top of BERT-base\n        self.bert.add_classification_head(task_id, num_labels=num_class)\n        # Make sure that adapters and prediction layer are used during forward pass\n        self.bert.set_active_adapters(task_id)\n\n    def forward(self, input_id, mask):\n\n        output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n\n        return output\n```", "```py\n# Initialize model \n\n# task_id is the name of our adapter. You can name it whatever you want but\n# common practice is to name it according to task/dataset we will train it on.\ntask_name = 'depression_reddit_dataset'\nmodel_adapter = BertClassifierWithAdapter(task_id=task_name)\n# Check parameters\nprint_trainable_parameters(model_adapter)\n\n'''\ntrainable params: 1486658 || all params: 109796930 || trainable%: 1.3540068925424418\n'''\n```", "```py\nLR = 5e-6\nEPOCHS = 10\ntrain(model_adapter, dataset['train'][:6500], dataset['train'][6500:], LR, EPOCHS)\n\n100%|███████████████████████████████████████████ 3250/3250 [07:19<00:00,  7.40it/s]\nEpochs: 1 | Train Loss:  0.183 | Train Accuracy:  0.846  | Val Loss:  0.125  | Val Accuracy:  0.897\n100%|███████████████████████████████████████████ 3250/3250 [07:24<00:00,  7.32it/s]\nEpochs: 2 | Train Loss:  0.096 | Train Accuracy:  0.925  | Val Loss:  0.072  | Val Accuracy:  0.946\n100%|███████████████████████████████████████████ 3250/3250 [07:23<00:00,  7.32it/s]\nEpochs: 3 | Train Loss:  0.060 | Train Accuracy:  0.958  | Val Loss:  0.052  | Val Accuracy:  0.962\n100%|███████████████████████████████████████████ 3250/3250 [07:21<00:00,  7.37it/s]\nEpochs: 4 | Train Loss:  0.044 | Train Accuracy:  0.968  | Val Loss:  0.047  | Val Accuracy:  0.971\n100%|███████████████████████████████████████████ 3250/3250 [07:25<00:00,  7.30it/s]\nEpochs: 5 | Train Loss:  0.038 | Train Accuracy:  0.971  | Val Loss:  0.043  | Val Accuracy:  0.973\n100%|███████████████████████████████████████████ 3250/3250 [07:25<00:00,  7.29it/s]\nEpochs: 6 | Train Loss:  0.034 | Train Accuracy:  0.975  | Val Loss:  0.039  | Val Accuracy:  0.971\n100%|███████████████████████████████████████████ 3250/3250 [07:25<00:00,  7.29it/s]\nEpochs: 7 | Train Loss:  0.032 | Train Accuracy:  0.978  | Val Loss:  0.038  | Val Accuracy:  0.972\n100%|███████████████████████████████████████████ 3250/3250 [07:25<00:00,  7.29it/s]\nEpochs: 8 | Train Loss:  0.029 | Train Accuracy:  0.980  | Val Loss:  0.039  | Val Accuracy:  0.974\n100%|███████████████████████████████████████████ 3250/3250 [07:25<00:00,  7.29it/s]\nEpochs: 9 | Train Loss:  0.027 | Train Accuracy:  0.980  | Val Loss:  0.035  | Val Accuracy:  0.971\n100%|███████████████████████████████████████████ 3250/3250 [07:19<00:00,  7.40it/s] \n```", "```py\n# Save trained model with adapter\nmodel_adapter_path = 'model/bert_adapter/' \nmodel_adapter.bert.save_all_adapters(model_adapter_path)\n```", "```py\ndef predict(data, model):\n\n    inputs = tokenizer(data['text'],\n              padding='max_length', max_length=512, truncation=True,\n              return_tensors=\"pt\")\n\n    mask = inputs['attention_mask'].to(device)\n    input_id = inputs['input_ids'].squeeze(1).to(device)\n    output = model(input_id, mask)[0].argmax(dim=1).item()\n    print(output)\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\nmodel_path = f'{model_adapter_path}{task_name}'\n\n# Load trained adapter\ntrained_adapter_model = BertClassifierWithAdapter(task_id=task_name)\nadapter_name = trained_adapter_model.bert.load_adapter(model_path)\ntrained_adapter_model.bert.set_active_adapters(adapter_name)\ntrained_adapter_model.to(device)\ntrained_adapter_model.eval()\n\n# Predict test data\npredict(data[6900], trained_adapter_model)\n```"]