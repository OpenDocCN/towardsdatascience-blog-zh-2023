["```py\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import MinMaxScaler\n```", "```py\n# Load data.\ndata = pd.read_csv('https://raw.githubusercontent.com/unit8co/darts/master/datasets/australian_tourism.csv')\n# Add time information: quarterly data starting in 1998.\ndata.index = pd.date_range(\"1998-01-01\", periods = len(data), freq = \"3MS\")\ndata.index.name = \"time\"\n# Consider only region-level data.\ndata = data[['NSW','VIC', 'QLD', 'SA', 'WA', 'TAS', 'NT']]\n# Let's give it nicer names.\ndata = data.rename(columns = {\n    'NSW': \"New South Wales\",\n    'VIC': \"Victoria\", \n    'QLD': \"Queensland\", \n    'SA': \"South Australia\", \n    'WA': \"Western Australia\", \n    'TAS': \"Tasmania\", \n    'NT': \"Northern Territory\",\n})\n```", "```py\n# Let's visualize the data.\ndef show_data(data,title=\"\"):\n    trace = [go.Scatter(x=data.index,y=data[c],name=c) for c in data.columns]\n    go.Figure(trace,layout=dict(title=title)).show()\n\nshow_data(data,\"Australian Tourism data by Region\")\n```", "```py\ndef build_targets_features(data,lags=range(8),horizon=1):\n    features = {}\n    targets = {}\n    for c in data.columns:\n\n        # Build lagged features.\n        feat = pd.concat([data[[c]].shift(lag).rename(columns = {c: f\"lag_{lag}\"}) for lag in lags],axis=1)\n        # Build quarter feature.\n        feat[\"quarter\"] = [f\"Q{int((m-1) / 3 + 1)}\" for m in data.index.month]\n        feat[\"quarter\"] = feat[\"quarter\"].astype(\"category\")\n        # Build target at horizon.\n        targ = data[c].shift(-horizon).rename(f\"horizon_{horizon}\")\n\n        # Drop missing values generated by lags/horizon.\n        idx = ~(feat.isnull().any(axis=1) | targ.isnull())\n        features[c] = feat.loc[idx]\n        targets[c] = targ.loc[idx]\n\n    return targets,features\n\n# Build targets and features.\ntargets,features = build_targets_features(data)\n```", "```py\ndef train_test_split(targets,features,test_size=8):\n    targ_train = {k: v.iloc[:-test_size] for k,v in targets.items()}\n    feat_train = {k: v.iloc[:-test_size] for k,v in features.items()}\n    targ_test = {k: v.iloc[-test_size:] for k,v in targets.items()}\n    feat_test = {k: v.iloc[-test_size:] for k,v in features.items()}\n    return targ_train,feat_train,targ_test,feat_test\n\ntarg_train,feat_train,targ_test,feat_test = train_test_split(targets,features)\n```", "```py\n# Instantiate one LightGBM model with default parameters for each target.\nlocal_models = {k: LGBMRegressor() for k in data.columns}\n# Fit the models on the training set.\nfor k in data.columns:\n    local_models[k].fit(feat_train[k],targ_train[k])\n```", "```py\ndef fit_scalers(feat_train,targ_train):\n    feat_scalers = {k: MinMaxScaler().set_output(transform=\"pandas\") for k in feat_train}\n    targ_scalers = {k: MinMaxScaler().set_output(transform=\"pandas\") for k in feat_train}\n    for k in feat_train:\n        feat_scalers[k].fit(feat_train[k].drop(columns=\"quarter\"))\n        targ_scalers[k].fit(targ_train[k].to_frame())\n    return feat_scalers,targ_scalers\n\ndef scale_features(feat,feat_scalers):\n    scaled_feat = {}\n    for k in feat:\n        df = feat[k].copy()\n        cols = [c for c in df.columns if c not in {\"quarter\"}]\n        df[cols] = feat_scalers[k].transform(df[cols])\n        scaled_feat[k] = df\n    return scaled_feat\n\ndef scale_targets(targ,targ_scalers):\n    return {k: targ_scalers[k].transform(v.to_frame()) for k,v in targ.items()}\n\n# Fit scalers on numerical features and target on the training period.\nfeat_scalers,targ_scalers = fit_scalers(feat_train,targ_train)\n# Scale train data.\nscaled_feat_train = scale_features(feat_train,feat_scalers)\nscaled_targ_train = scale_targets(targ_train,targ_scalers)\n# Scale test data.\nscaled_feat_test = scale_features(feat_test,feat_scalers)\nscaled_targ_test = scale_targets(targ_test,targ_scalers)\n```", "```py\n# Add a `target_name` feature.\ndef add_target_name_feature(feat):\n    for k,df in feat.items():\n        df[\"target_name\"] = k\n\nadd_target_name_feature(scaled_feat_train)\nadd_target_name_feature(scaled_feat_test)\n```", "```py\n# Concatenate the data.\nglobal_feat_train = pd.concat(scaled_feat_train.values())\nglobal_targ_train = pd.concat(scaled_targ_train.values())\nglobal_feat_test = pd.concat(scaled_feat_test.values())\nglobal_targ_test = pd.concat(scaled_targ_test.values())\n# Make `target_name` categorical after concatenation.\nglobal_feat_train.target_name = global_feat_train.target_name.astype(\"category\")\nglobal_feat_test.target_name = global_feat_test.target_name.astype(\"category\")\n```", "```py\n# Make predictions with the local models.\npred_local = {\n  k: model.predict(feat_test[k]) for k, model in local_models.items()\n}\n```", "```py\ndef predict_global_model(global_model, global_feat_test, targ_scalers):\n    # Predict.\n    pred_global_scaled = global_model.predict(global_feat_test)\n    # Re-arrange the predictions\n    pred_df_global = global_feat_test[[\"target_name\"]].copy()\n    pred_df_global[\"predictions\"] = pred_global_scaled\n    pred_df_global = pred_df_global.pivot(\n        columns=\"target_name\", values=\"predictions\"\n    )\n    # Un-scale the predictions\n    return {\n        k: targ_scalers[k]\n        .inverse_transform(\n            pred_df_global[[k]].rename(\n                columns={k: global_targ_train.columns[0]}\n            )\n        )\n        .reshape(-1)\n        for k in pred_df_global.columns\n    }\n\n# Make predicitons with the global model.\npred_global = predict_global_model(global_model, global_feat_test, targ_scalers)\n```", "```py\n# Save predictions from both approaches in a convenient format.\noutput = {}\nfor k in targ_test:\n    df = targ_test[k].rename(\"target\").to_frame()\n    df[\"prediction_local\"] = pred_local[k]\n    df[\"prediction_global\"] = pred_global[k]\n    output[k] = df\n\ndef print_stats(output):\n    output_all = pd.concat(output.values())\n    mae_local = (output_all.target - output_all.prediction_local).abs().mean()\n    mae_global = (output_all.target - output_all.prediction_global).abs().mean()\n    print(\"                            LOCAL     GLOBAL\")\n    print(f\"MAE overall              :  {mae_local:.1f}     {mae_global:.1f}\\n\")\n    for k,df in output.items():   \n        mae_local = (df.target - df.prediction_local).abs().mean()\n        mae_global = (df.target - df.prediction_global).abs().mean()\n        print(f\"MAE - {k:19}:  {mae_local:.1f}     {mae_global:.1f}\")\n\n# Let's show some statistics.\nprint_stats(output)\n```", "```py\n# Display the predictions.\nfor k,df in output.items():\n    show_data(df,k)\n```"]