- en: 'Tree Ensembles: Bagging, Boosting and Gradient Boosting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/tree-ensembles-theory-and-practice-1cf9eb27781](https://towardsdatascience.com/tree-ensembles-theory-and-practice-1cf9eb27781)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Theory and practice explained in detail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jorgemartinlasaosa.medium.com/?source=post_page-----1cf9eb27781--------------------------------)[![Jorge
    Martín Lasaosa](../Images/21b4e500b7d14204ea76f579c3e2433f.png)](https://jorgemartinlasaosa.medium.com/?source=post_page-----1cf9eb27781--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1cf9eb27781--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1cf9eb27781--------------------------------)
    [Jorge Martín Lasaosa](https://jorgemartinlasaosa.medium.com/?source=post_page-----1cf9eb27781--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1cf9eb27781--------------------------------)
    ·10 min read·Jan 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab39fad56deee0716f620ca35605c141.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Arnaud Mesureur](https://unsplash.com/@tbzr?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'A **tree ensemble** is a machine learning technique for supervised learning
    that consists of a set of individually trained decision trees defined as *weak*
    or *base* learners, that may not perform well individually. The aggregation of
    the *weak* learners produce a new *strong* model, which is often more accurate
    than the former ones. There are three main types of ensemble learning methods:
    **bagging**, **boosting,** and **gradient boosting**. Every method can be used
    with other *weak* learners, but in this post, only trees are going to be taken
    into account.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the article is divided into two sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intuition & History.** It explains the origin and gives a short description
    of each ensemble learning method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Practical Demonstration.** Develops every ensemble learning method step by
    step. For this purpose, a small synthetic dataset is also presented to help with
    the explanations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Intuition & History**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The term was first defined by Breiman (1996) [1], and it is an acronym for **B**ootstrap
    **Agg**regation. For this ensemble, each decision tree uses as input data a bootstrap
    sample of the training dataset. A bootstrap sample is a randomly selected sample
    with replacement, which means that observations can appear once, more than once,
    or never. Then, all the predictions are combined using a statistic such as the
    average. *Random Forest* [2] uses this technique (among others) and is one of
    the most successful and widely used ensemble methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2965a6381d0e0c93c62fec49c6a33635.png)'
  prefs: []
  type: TYPE_IMG
- en: Bagging visual explanation. Each sample with replacement serves as input data
    for the weak learner. [3]
  prefs: []
  type: TYPE_NORMAL
- en: '**Boosting**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Keans and Valiant (1988, 1989) [4][5] left open the following question: *Can
    a set of weak learners create a single strong learner?* In 1990, Schapire‘s affirmative
    answer [6] led to the development of boosting. Boosting, as opposed to bagging,
    takes a more iterative approach in which trees are trained sequentially. Observations
    have assigned a weight, and each tree is built in an additive manner, assigning
    greater weights (more importance) to misclassified observations in the previous
    learners. There are many boosting algorithms, but the first one to take full advantage
    of *weak* learners was *AdaBoost* [7], formulated by Freund and Schapire in 1995.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5b7de4b3ffa9d305b5b18dbf5a7fce6.png)'
  prefs: []
  type: TYPE_IMG
- en: AdaBoost visual explanation. Each learning is focused on misclassified observations
    of the previous tree [8]
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Breiman proposed a “Gradient Boosting Machine” [9] that also takes an iterative
    approach in which trees are trained sequentially. However, instead of having weights
    to update, the trees are fitting the *pseudo residuals* of the previous tree.
    The first *pseudo residuals* are the result of subtracting the real value from
    the average of the output feature. Many algorithms such as *XGBoost* [10], *CatBoost*
    [11] or *LightGBM* [12] are based on this technique.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/637a9996edfdb9386d8f9079846291e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient Boosting Visual Explanation [13]
  prefs: []
  type: TYPE_NORMAL
- en: Practical Demonstration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The practical demonstration is highly inspired by [Josh Starmer’s Youtube channel](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw).
    If you want a visual and extremely concise explanation of almost any machine learning
    model or technique, just check it out!
  prefs: []
  type: TYPE_NORMAL
- en: 'Disclaimer: the data have been tailor-made so that the results show what is
    expected. This does not mean that the techniques are infallible or that they can
    be compared with each other.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When I learn new algorithms or methods, I really like to test them in small
    datasets where I can focus on the details. So for the practical demonstration,
    we will use a small synthetic dataset containing made-up information about houses
    and their *price*. Bagging and boosting models are going to be explained with
    the price feature transformed into a categorical feature, while gradient boosting
    is going to be explained with the price as a numerical feature.
  prefs: []
  type: TYPE_NORMAL
- en: All the Jupyter Notebooks developed for the article can be found in this [GitHub
    Repository](https://github.com/jomartla/medium_articles/tree/main/TreeEnsembles),
    but the main pieces of code are still shown along the reading. For example, the
    data creation process can be seen below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code used for creating the data. It can be found also in the repository: TreeEnsembles/synthetic_data.py.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data created have 10 customers and 6 features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Square Meters**: Numerical'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**If the house has a garage**: Boolean'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**If the house has a garden**: Boolean'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of rooms**: Numerical'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Price**: Categorical or Numerical'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As it is said before, the price feature will be categorical or numerical depending
    on the tree ensemble explained.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f25d714245c006fab798fdc72d190e6.png)'
  prefs: []
  type: TYPE_IMG
- en: DataFrame used in the practical demonstration of bagging and boosting. The price
    feature is a categorical feature.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afeeb23b6ca3a1ef67c9b8bcad127de3.png)'
  prefs: []
  type: TYPE_IMG
- en: DataFrame used in the practical demonstration of gradient boosting. The price
    feature is a numerical feature.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, remember that bagging is an acronym for **B**ootstrap **Agg**regation.
    These two words lead the way, so let’s start with the first one: **bootstrap**.
    The following code builds 5bootstrap samples of 7 observations from the data generated
    in the previous section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code used for creating the samples. The code can be found also in the repository:
    TreeEnsembles/Bagging.ipynb.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1802cb50bdbacecd5ae00130ceb41211.png)'
  prefs: []
  type: TYPE_IMG
- en: Each sample contains exactly seven indexes. Repeated indexes are allowed.
  prefs: []
  type: TYPE_NORMAL
- en: As defined before, the randomly selected samples are done with replacement,
    that’s why the samples have repeated indexes. The next step trains a *weak* learner
    for each sample. In this case, the learner chosen is the Decision Tree Classifier
    from scikit-learn [15]
  prefs: []
  type: TYPE_NORMAL
- en: 'Code used for developing bagging. It can be found also in the repository: TreeEnsembles/bagging.ipynb.'
  prefs: []
  type: TYPE_NORMAL
- en: Once every tree is trained, let’s dive into the first sample and its corresponding
    tree. The first bootstrapped sample had the observations *[6, 0, 2, 2, 7, 6, 8]*,
    so these are the observations used by the tree to train. The induced tree shows
    that can correctly classify every observation but one (observation = 0). If you
    follow the tree, it is clear that it is going to be classified as LOW, while the
    price is actually MEDIUM. Nevertheless, we can say the performance of the tree
    is somewhat good.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37489faa9b654e803a8e95bfbf20da65.png)'
  prefs: []
  type: TYPE_IMG
- en: First tree and sample for bagging practical demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, take a look at the observation indexes again, there are only 5 unique
    values *[0, 2, 6, 7, 8]*, so the first tree is taking only 50% of the sample for
    training. Thus, the resulting accuracy can be misleading. To understand better
    the decision tree performance, we are going to predict the whole dataset (10 observations)
    with each decision tree. Furthermore, let’s make use of the second word that makes
    up bagging: **aggregation**. Our *strong* learner (called bagging) will take the
    most common prediction from the five trained trees.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ddb50703f30b671200db59936f85845.png)'
  prefs: []
  type: TYPE_IMG
- en: Predictions of individual trees and bagging aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see below, in terms of accuracy the *weak* learners (trees) perform
    worse individually than the *strong* learner (bagging).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bcf5c50e69bae93a9b0668f0ac220f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Accuracy of bagging and each trained tree.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boosting, as opposed to bagging, trains trees sequentially. A first tree is
    trained with all the data. In the following tree, misclassified observations are
    given a higher weight (scikit learn’s decision tree classifier has a weights parameter).
    That weight allows the tree to *focus* more on certain observations. Here we have
    the code used to implement a boosting ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code used for developing boosting. It can be found also in the repository:
    TreeEnsembles/boosting.ipynb.'
  prefs: []
  type: TYPE_NORMAL
- en: As we said, the first step is to train the first tree with all the observations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/621f11904ba1caec6415ae80700056ac.png)'
  prefs: []
  type: TYPE_IMG
- en: First trained tree of boosting.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the tree is not able to learn properly the observations *[0, 6, 9]*.
    So, according to boosting theory and the code, these observations will have higher
    weights in the second tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23abeb6dc11903a7b7560b5014ea146e.png)'
  prefs: []
  type: TYPE_IMG
- en: Second trained tree of boosting
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the second tree trained with the updated weights is able to learn
    properly the previously misclassified observations *[0, 6, 9].* Higher weights
    assigned to these indexes have forced the tree to learn them properly. However,
    the tree learning process has changed also for the rest of the observations. It
    fails now to learn observations *[3, 4, 7]*.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the third tree would double up the weights on these misclassified
    observations and, tree after tree, they will correct the errors made by previous
    learners. In the next picture, it is shown how each tree improves the errors of
    the previous one. Besides, there is a column called boosting that chooses the
    most common classification of all the trees.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2dbf25b27cd7cdd31ae066c5e1709ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Predictions of the sequentially trained trees and predictions of the boosting
    aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: If we calculate the accuracy of each tree and the boosting aggregation, the
    results clearly show that the boosting technique also improves the results of
    each individual tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1faafb90bac46fa07cc31117e38e7823.png)'
  prefs: []
  type: TYPE_IMG
- en: Accuracy of boosting and each trained tree.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember that for gradient boosting we are going to use the DataFrame whose
    target variable price is numeric. Also, remember that gradient boosting, like
    boosting, has an iterative approach. The difference is that the trees fit the
    *pseudo residuals* of the previous tree, instead of the same observations with
    different weights.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we are going to do is to calculate the first *pseudo residuals*
    (*residuals_0*), whichare the result of subtracting the mean of the price from
    the real value of the price.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fc0a29e8501d0921adc33b0a03a0d12.png)'
  prefs: []
  type: TYPE_IMG
- en: DataFrame with the first pseudo residuals (residuals_0) calculated.
  prefs: []
  type: TYPE_NORMAL
- en: The Mean Absolute Error (MAE) of predicting the values using the mean to every
    observation is *100397.68*. This is the metric that is going to be improved with
    each trained tree. That being told, let’s train the first tree with the *pseudo
    residuals* shown before as the target.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4919e79037b95f33644890a5ac8c8441.png)'
  prefs: []
  type: TYPE_IMG
- en: The first tree of gradient boosting was trained with pseudo residuals (residuals_0).
  prefs: []
  type: TYPE_NORMAL
- en: Observe that each final node has a different number of samples. In the first
    one on the left, there is only one observation. It corresponds to observation
    with index *1* whose *pseudo residual* is -*138461.4*. In the second final node
    from left to right, there are 5 samples which correspond to observations with
    indexes *[0, 2, 3, 6, 7]*. The predicted residual value by the tree (*-72705*)
    is the average value of the 5 residuals mentioned. These values will be known
    in advance as *predicted pseudo residuals.*
  prefs: []
  type: TYPE_NORMAL
- en: To predict real price values with this first tree we should do the following
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/102ba401f3720c391226f35026434f22.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula for predicting price values after the first tree.
  prefs: []
  type: TYPE_NORMAL
- en: The predictions shown below achieve a MAE of *70347.53,* which improves the
    MAE achieved before by predicting only with the mean (*100397.68*). With these
    predictions, we can calculate the next *pseudo residuals* (*residuals_1*), which
    are the result of subtracting the predictions of the first tree (*predictions_0*)
    from the real value of the price.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfb585c003a3ac58073a9be147cffe82.png)'
  prefs: []
  type: TYPE_IMG
- en: Predictions results after the first tree.
  prefs: []
  type: TYPE_NORMAL
- en: Note that *residuals_1* are always closer to zero than *residuals_0*. This is
    because we are using a portion of the information given by the tree to improve
    the prediction made by the mean. This portion percentage is defined by the *learning
    rate.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Until now, only one tree has been involved, but we said before that gradient
    boosting use several trees sequentially. It’s time to train the second tree, and
    for this purpose, the residuals previously calculated (*residuals_1*) are going
    to be used as the target. The second trained tree looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc9fa6e030df96a20d1c8e1253935baf.png)'
  prefs: []
  type: TYPE_IMG
- en: Second tree of gradient boosting is trained with the pseudo residuals (residuals_1).
  prefs: []
  type: TYPE_NORMAL
- en: If we follow the same steps as in the first tree, we get the following results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78ae276fc780b3d415e66c282708d877.png)'
  prefs: []
  type: TYPE_IMG
- en: Predictions results after second tree.
  prefs: []
  type: TYPE_NORMAL
- en: The only difference with the first tree is that we used both the first and second
    trees' pseudo residuals predictions (*residual_predictions_1* and *residual_predictions_1*)
    for making the predictions(*predictions_1*)*.* Instead of doing an aggregation
    of the predictions as it is done in bagging and boosting, gradient boosting adds
    to the mean price small portions of information of each tree (*learning rate *
    residuals preds tree x*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b02723756bbd999a120eba66209b2723.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula for predicting price values after the second tree is trained.
  prefs: []
  type: TYPE_NORMAL
- en: After training 5 trees, we can clearly see how the MAE has been reduced in the
    following results. Each iteration provides better learning, and it is clear that
    the union of all the trees provides better metrics than using them individually.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8ec212491b8d53c02dc15374513eab7.png)'
  prefs: []
  type: TYPE_IMG
- en: MAE achieved after each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This article is intended to be a step-by-step guide which explains how different
    ensembling learning methods work: **Bagging**, **Boosting** and **Gradient Boosting**.'
  prefs: []
  type: TYPE_NORMAL
- en: In the first section, we have seen the history, description and uses of these
    techniques, while in the second one, we have developed in a practical way all
    the models, showing how the union of different *weak* learners can create a *strong*
    learner with a higher predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you have found the reading useful and enjoyable. And above all, I am
    happy to receive any kind of feedback. So feel free to share your thoughts!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Breiman, L. «Bagging Predictors». *Machine Learning* 24, (1996): 123–40\.
    [https://doi.org/10.1007/BF00058655](https://doi.org/10.1007/BF00058655).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Breiman, L. «Random Forests». *Machine Learning* 45, (2001): 5–32\. [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Image Source: [https://www.researchgate.net/figure/The-bagging-approach-Several-classifier-are-trained-on-bootstrap-samples-of-the-training_fig4_322179244](https://www.researchgate.net/figure/The-bagging-approach-Several-classifier-are-trained-on-bootstrap-samples-of-the-training_fig4_322179244)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Kearns, M. «[Thoughts on Hypothesis Boosting](http://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf)»,
    Unpublished manuscript (Machine Learning class project) (1988)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Kearns, M; Valiant, L. «Cryptographic limitations on learning Boolean formulae
    and finite automata». *Symposium on Theory of Computing* 21, (1989): 433–444 [https://dl.acm.org/doi/10.1145/73007.73049](https://dl.acm.org/doi/10.1145/73007.73049)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Schapire, R.E. «The Strength of Weak Learnability». *Machine Learning*
    5, (1990): 197–227 [https://doi.org/10.1007/BF00116037](https://doi.org/10.1007/BF00116037)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Freund, Y.; Schapire, R.E. (1995). «A decision-theoretic generalization
    of online learning and an application to boosting». *Computational Learning Theory,*
    (1995) [https://doi.org/10.1007/3-540-59119-2_166](https://doi.org/10.1007/3-540-59119-2_166)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Image Source: [https://www.researchgate.net/figure/Training-of-an-AdaBoost-classifier-The-first-classifier-trains-on-unweighted-data-then_fig3_306054843](https://www.researchgate.net/figure/Training-of-an-AdaBoost-classifier-The-first-classifier-trains-on-unweighted-data-then_fig3_306054843)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Friedman, J.H. «Greedy Function Approximation: A Gradient Boosting Machine»
    *The Annals of Statistics* 29 (2001). [https://doi.org/10.1214/aos/1013203451](https://doi.org/10.1214/aos/1013203451).'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Chen, Tianqi, and Carlos Guestrin. «XGBoost: A Scalable Tree Boosting
    System» *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge
    Discovery and Data Mining* (2016). [https://doi.org/10.1145/2939672.2939785](https://doi.org/10.1145/2939672.2939785).'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Dorogush, A.V.; Ershov, V.; Gulin. A. CatBoost: Gradient Boosting with
    Categorical Features Support». *ArXiv:1810.11363*, 24 (2018). [http://arxiv.org/abs/1810.11363](http://arxiv.org/abs/1810.11363).'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Ke, G.; Meng, Q.; Finley, T; Wang, T; Chen, W; Ma, W; Ye, Q; Liu, T. «LightGBM:
    A Highly Efficient Gradient Boosting Decision Tree». *Advances in Neural Information
    Processing Systems*, 20 (2017). [https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Image source: [https://www.researchgate.net/profile/Karem-Abdelmohsen/publication/339077244/figure/fig3/AS:855596286877696@1581001459188/Schematic-diagram-of-a-tree-based-gradient-boosting-method.png](https://www.researchgate.net/profile/Karem-Abdelmohsen/publication/339077244/figure/fig3/AS:855596286877696@1581001459188/Schematic-diagram-of-a-tree-based-gradient-boosting-method.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] StatQuest with Josh Starmer (Youtube Channel): [https://www.youtube.com/c/joshstarmer](https://www.youtube.com/c/joshstarmer)'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)'
  prefs: []
  type: TYPE_NORMAL
