- en: Hyperbolic Deep Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¶…æ›²é¢æ·±åº¦å¼ºåŒ–å­¦ä¹ 
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/hyperbolic-deep-reinforcement-learning-b2de787cf2f7](https://towardsdatascience.com/hyperbolic-deep-reinforcement-learning-b2de787cf2f7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/hyperbolic-deep-reinforcement-learning-b2de787cf2f7](https://towardsdatascience.com/hyperbolic-deep-reinforcement-learning-b2de787cf2f7)
- en: RL meets hyperbolic geometry
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ é‡åˆ°è¶…æ›²é¢å‡ ä½•
- en: Many RL problems have hierarchical tree-like nature. Hyperbolic geometry offers
    a powerful prior for such problems.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¸å¤šå¼ºåŒ–å­¦ä¹ é—®é¢˜å…·æœ‰å±‚æ¬¡æ ‘çŠ¶ç‰¹æ€§ã€‚è¶…æ›²é¢å‡ ä½•ä¸ºæ­¤ç±»é—®é¢˜æä¾›äº†å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ã€‚
- en: '[](https://michael-bronstein.medium.com/?source=post_page-----b2de787cf2f7--------------------------------)[![Michael
    Bronstein](../Images/1aa876fce70bb07bef159fecb74e85bf.png)](https://michael-bronstein.medium.com/?source=post_page-----b2de787cf2f7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2de787cf2f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2de787cf2f7--------------------------------)
    [Michael Bronstein](https://michael-bronstein.medium.com/?source=post_page-----b2de787cf2f7--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://michael-bronstein.medium.com/?source=post_page-----b2de787cf2f7--------------------------------)[![Michael
    Bronstein](../Images/1aa876fce70bb07bef159fecb74e85bf.png)](https://michael-bronstein.medium.com/?source=post_page-----b2de787cf2f7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2de787cf2f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2de787cf2f7--------------------------------)
    [Michael Bronstein](https://michael-bronstein.medium.com/?source=post_page-----b2de787cf2f7--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2de787cf2f7--------------------------------)
    Â·17 min readÂ·Apr 30, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2de787cf2f7--------------------------------)
    Â·17åˆ†é’Ÿé˜…è¯»Â·2023å¹´4æœˆ30æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Many problems in Reinforcement Learning manifest a hierarchical tree-like nature.
    Hyperbolic spaces, which can be conceptualised as continuous analogies of trees,
    are thus suitable candidates to parameterise the agentâ€™s deep model. In this post,
    we overview the basics of hyperbolic geometry, show empirically that it provides
    a good inductive bias for many RL problems, and describe a practical regularisation
    procedure allowing to resolve numerical instabilities in end-to-end optimisation
    with hyperbolic latent spaces. Our approach shows a near-universal performance
    improvement across a broad range of common benchmarks both with on-policy and
    off-policy RL algorithms.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ä¸­çš„è®¸å¤šé—®é¢˜è¡¨ç°å‡ºå±‚æ¬¡æ ‘çŠ¶çš„ç‰¹æ€§ã€‚å› æ­¤ï¼Œè¶…æ›²é¢ç©ºé—´ï¼ˆå¯ä»¥è¢«æ¦‚å¿µåŒ–ä¸ºæ ‘çš„è¿ç»­ç±»æ¯”ï¼‰æ˜¯å‚æ•°åŒ–æ™ºèƒ½ä½“æ·±åº¦æ¨¡å‹çš„åˆé€‚å€™é€‰ã€‚æœ¬æ–‡æ¦‚è¿°äº†è¶…æ›²é¢å‡ ä½•çš„åŸºç¡€çŸ¥è¯†ï¼Œå®è¯å±•ç¤ºäº†å®ƒä¸ºè®¸å¤šå¼ºåŒ–å­¦ä¹ é—®é¢˜æä¾›äº†è‰¯å¥½çš„å½’çº³åç½®ï¼Œå¹¶æè¿°äº†ä¸€ç§å®ç”¨çš„æ­£åˆ™åŒ–ç¨‹åºï¼Œå…è®¸è§£å†³åœ¨è¶…æ›²é¢æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–æ—¶çš„æ•°å€¼ä¸ç¨³å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹¿æ³›çš„å¸¸è§åŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºäº†å‡ ä¹æ™®éçš„æ€§èƒ½æå‡ï¼Œæ— è®ºæ˜¯ä½¿ç”¨ç­–ç•¥ç®—æ³•è¿˜æ˜¯ç¦»ç­–ç•¥ç®—æ³•ã€‚
- en: '![](../Images/b6bdc310bab65aab0cc8849bfb1a4359.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6bdc310bab65aab0cc8849bfb1a4359.png)'
- en: Stable Diffusion prompted with â€œHyperbolic Atari Breakout game, icon design,
    flat design, vector artâ€ (courtesy of [David Ha](https://twitter.com/hardmaru?lang=en))
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±â€œè¶…æ›²é¢Atari Breakoutæ¸¸æˆï¼Œå›¾æ ‡è®¾è®¡ï¼Œå¹³é¢è®¾è®¡ï¼ŒçŸ¢é‡è‰ºæœ¯â€æç¤ºçš„ç¨³å®šæ‰©æ•£ï¼ˆç”±[David Ha](https://twitter.com/hardmaru?lang=en)æä¾›ï¼‰
- en: '*This post was co-authored with* [*Edoardo Cetin*](https://aladoro.github.io/)*,*
    [*Ben Chamberlain*](https://twitter.com/DrBPChamberlain)*, and* [*Jonathan Hunt*](https://twitter.com/jjh)
    *and is based on the paper E. Cetin et al.,* [*Hyperbolic deep reinforcement learning*](https://arxiv.org/pdf/2210.01542.pdf)
    *(2023) ICLR. For more details, find us at ICLR 2023!*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ¬æ–‡ç”±*[*Edoardo Cetin*](https://aladoro.github.io/)*ã€* [*Ben Chamberlain*](https://twitter.com/DrBPChamberlain)*å’Œ*
    [*Jonathan Hunt*](https://twitter.com/jjh) *å…±åŒæ’°å†™ï¼Œå¹¶åŸºäºE. Cetinç­‰äººå‘è¡¨çš„è®ºæ–‡* [*Hyperbolic
    deep reinforcement learning*](https://arxiv.org/pdf/2210.01542.pdf) *(2023) ICLRã€‚æ¬²äº†è§£æ›´å¤šè¯¦æƒ…ï¼Œè¯·åœ¨ICLR
    2023ä¸Šæ‰¾åˆ°æˆ‘ä»¬ï¼*'
- en: Basics of Reinforcement Learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ åŸºç¡€
- en: 'RL problems can be described as a Markov Decision Process (MDP), where the
    agent observes some *state* *s*âˆˆ*S* from the environmentâ€™s state space, based
    on which it executes some *action* *a*âˆˆAfrom its action space, and, finally, receives
    a reward *r* from its *reward function* *r: SÃ—A â†¦ R*.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¼ºåŒ–å­¦ä¹ é—®é¢˜å¯ä»¥æè¿°ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå…¶ä¸­æ™ºèƒ½ä½“ä»ç¯å¢ƒçš„çŠ¶æ€ç©ºé—´ä¸­è§‚å¯Ÿåˆ°æŸä¸ª*çŠ¶æ€* *s*âˆˆ*S*ï¼ŒåŸºäºæ­¤æ‰§è¡ŒæŸä¸ª*åŠ¨ä½œ* *a*âˆˆAï¼Œä»å…¶åŠ¨ä½œç©ºé—´ä¸­ï¼Œå¹¶æœ€ç»ˆä»å…¶*å¥–åŠ±å‡½æ•°*
    *r: SÃ—A â†¦ R*ä¸­è·å¾—å¥–åŠ±*r*ã€‚'
- en: 'The evolution of the environment relies on the *markovian property*, meaning
    that it is independent of past states given the current one, and is fully described
    by the transition dynamics *P* : *SÃ—AÃ—S â†¦ R* and initial state distribution *pâ‚€*:
    *S â†¦ R*.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¯å¢ƒçš„æ¼”å˜ä¾èµ–äº*é©¬å°”å¯å¤«æ€§è´¨*ï¼Œè¿™æ„å‘³ç€ç»™å®šå½“å‰çŠ¶æ€ï¼Œå®ƒç‹¬ç«‹äºè¿‡å»çš„çŠ¶æ€ï¼Œå¹¶ç”±è½¬ç§»åŠ¨æ€ *P* : *SÃ—AÃ—S â†¦ R* å’Œåˆå§‹çŠ¶æ€åˆ†å¸ƒ *pâ‚€*:
    *S â†¦ R* å®Œå…¨æè¿°ã€‚'
- en: A *policy* isaparameterised distribution function over actions *aâˆ¼Ï€*(*â‹…|s*)
    given the current state *s,* representing the agentâ€™s behaviour. Each episode
    of interaction between the agent and the environment produces a trajectory, *Ï„
    = (sâ‚€,aâ‚€,sâ‚,aâ‚,â€¦),* according to the policy and transition dynamics. For each
    state *sâˆˆ S*, the policyâ€™s *value function* represents the expected discounted
    sum of future rewards over the agentâ€™s possible trajectories starting from *s*
    [*].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç­–ç•¥*æ˜¯ä¸€ä¸ªå…³äºåŠ¨ä½œçš„å‚æ•°åŒ–åˆ†å¸ƒå‡½æ•° *aâˆ¼Ï€*(*â‹…|s*)ï¼Œç»™å®šå½“å‰çŠ¶æ€ *s*ï¼Œè¡¨ç¤ºä»£ç†çš„è¡Œä¸ºã€‚ä»£ç†ä¸ç¯å¢ƒä¹‹é—´çš„æ¯æ¬¡äº¤äº’äº§ç”Ÿä¸€ä¸ªè½¨è¿¹ï¼Œ*Ï„ =
    (sâ‚€,aâ‚€,sâ‚,aâ‚,â€¦)*ï¼Œæ ¹æ®ç­–ç•¥å’Œè½¬ç§»åŠ¨æ€ç”Ÿæˆã€‚å¯¹äºæ¯ä¸ªçŠ¶æ€ *sâˆˆ S*ï¼Œç­–ç•¥çš„*ä»·å€¼å‡½æ•°*è¡¨ç¤ºä» *s* å¼€å§‹çš„ä»£ç†å¯èƒ½è½¨è¿¹ä¸Šçš„æœªæ¥å¥–åŠ±çš„æœŸæœ›æŠ˜æ‰£æ€»å’Œ[*]ã€‚'
- en: 'The agentâ€™s objective is to learn a policy maximising its expected discounted
    sum of rewards over encountered trajectories, or equivalently, the expected value
    function over the possible initial states*.* In deep RL, the policy and value
    functions are typically modeled as neural networks. The RL training loop involves
    alternating between an *experience collection* phase (deploying the current policy
    in the environment) and a *learning* phase (updating the agentâ€™s models to improve
    its behaviour). Based on how the collected experience data is used, we can distinguish
    between two main classes of RL algorithms:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç†çš„ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªæœ€å¤§åŒ–å…¶æœŸæœ›æŠ˜æ‰£å¥–åŠ±æ€»å’Œçš„ç­–ç•¥ï¼Œæˆ–ç­‰æ•ˆåœ°ï¼Œæœ€å¤§åŒ–åœ¨å¯èƒ½åˆå§‹çŠ¶æ€ä¸Šçš„æœŸæœ›ä»·å€¼å‡½æ•°*ã€‚* åœ¨æ·±åº¦RLä¸­ï¼Œç­–ç•¥å’Œå€¼å‡½æ•°é€šå¸¸å»ºæ¨¡ä¸ºç¥ç»ç½‘ç»œã€‚RLè®­ç»ƒå¾ªç¯æ¶‰åŠåœ¨*ç»éªŒæ”¶é›†*é˜¶æ®µï¼ˆåœ¨ç¯å¢ƒä¸­éƒ¨ç½²å½“å‰ç­–ç•¥ï¼‰å’Œ*å­¦ä¹ *é˜¶æ®µï¼ˆæ›´æ–°ä»£ç†çš„æ¨¡å‹ä»¥æ”¹å–„å…¶è¡Œä¸ºï¼‰ä¹‹é—´äº¤æ›¿è¿›è¡Œã€‚æ ¹æ®æ”¶é›†çš„ç»éªŒæ•°æ®çš„ä½¿ç”¨æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥åŒºåˆ†ä¸¤ç±»ä¸»è¦çš„RLç®—æ³•ï¼š
- en: '*On-policy* algorithms collect a new set of trajectories with the latest policy
    for each training iteration, discarding old data. They use these trajectories
    to learn the current policyâ€™s value function which is then used to compute the
    *policy gradient* [1]and maximize the probability of performing the best-observed
    current actions. Proximal Policy Optimisation (PPO) [2] is currently one of the
    most established and robust algorithms within this class [3].'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨çº¿ç­–ç•¥*ç®—æ³•ä¸ºæ¯æ¬¡è®­ç»ƒè¿­ä»£æ”¶é›†ä¸€ç»„æ–°çš„è½¨è¿¹ï¼Œä½¿ç”¨æœ€æ–°ç­–ç•¥ï¼Œä¸¢å¼ƒæ—§æ•°æ®ã€‚ä»–ä»¬ä½¿ç”¨è¿™äº›è½¨è¿¹æ¥å­¦ä¹ å½“å‰ç­–ç•¥çš„ä»·å€¼å‡½æ•°ï¼Œç„¶åç”¨å®ƒæ¥è®¡ç®—*ç­–ç•¥æ¢¯åº¦*[1]å¹¶æœ€å¤§åŒ–æ‰§è¡Œæœ€ä½³è§‚å¯Ÿåˆ°çš„å½“å‰åŠ¨ä½œçš„æ¦‚ç‡ã€‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰[2]
    ç›®å‰æ˜¯è¿™ä¸€ç±»åˆ«ä¸­æœ€æˆç†Ÿå’Œæœ€ç¨³å¥çš„ç®—æ³•ä¹‹ä¸€[3]ã€‚'
- en: '*Off-policy* algorithms instead store many different trajectories collected
    with a mixture of old policies in a large replay buffer dataset of experiences.
    They use this data to directly learn a model of the optimal value function using
    a squared loss based on the Bellman backup [4]. The policy is then implicitly
    defined based on the actions leading to the highest expected estimated value.
    Rainbow DQN [5] is a modern popular instantiation of the seminal off-policy DQN
    algorithm [6], introducing several auxiliary practices that stabilise and speed
    up learning.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç¦»çº¿ç­–ç•¥*ç®—æ³•åˆ™å°†è®¸å¤šä¸åŒçš„è½¨è¿¹å­˜å‚¨åœ¨ä¸€ä¸ªå¤§çš„é‡æ”¾ç¼“å†²åŒºæ•°æ®é›†ä¸­ï¼Œè¿™äº›è½¨è¿¹æ˜¯é€šè¿‡æ—§ç­–ç•¥çš„æ··åˆæ”¶é›†çš„ã€‚ä»–ä»¬ä½¿ç”¨è¿™äº›æ•°æ®ç›´æ¥å­¦ä¹ ä¸€ä¸ªåŸºäºè´å°”æ›¼å¤‡ä»½[4]çš„æœ€ä¼˜ä»·å€¼å‡½æ•°æ¨¡å‹ã€‚ç„¶åï¼Œç­–ç•¥æ˜¯åŸºäºå¯¼è‡´æœ€é«˜æœŸæœ›ä¼°è®¡å€¼çš„åŠ¨ä½œéšå¼å®šä¹‰çš„ã€‚Rainbow
    DQN [5] æ˜¯ä¸€ç§ç°ä»£æµè¡Œçš„å¼€åˆ›æ€§ç¦»çº¿ç­–ç•¥DQNç®—æ³•[6]ï¼Œå¼•å…¥äº†å‡ ç§è¾…åŠ©å®è·µï¼Œä»¥ç¨³å®šå’ŒåŠ å¿«å­¦ä¹ é€Ÿåº¦ã€‚'
- en: '**Generalisation in Deep Reinforcement Learning**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„æ³›åŒ–**'
- en: Generalisation is key requirement of effective RL agents, since most real-world
    and even complex simulated tasks entail a large degree of diversity in their state
    space (e.g., the space of natural images). From the agentâ€™s perspective, exploring
    and memorising the exact value for this (possibly infinite) set of inputs is clearly
    intractable. Moreover, for many applications, controlled laboratory settings used
    for training might not reflect the full diversity of possible configurations for
    a given task. Therefore, the agentâ€™s behaviour should ideally be robust to small
    distribution shifts it might observe during deployment.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ³›åŒ–æ˜¯æœ‰æ•ˆå¼ºåŒ–å­¦ä¹ ä»£ç†çš„å…³é”®è¦æ±‚ï¼Œå› ä¸ºå¤§å¤šæ•°ç°å®ä¸–ç•Œç”šè‡³å¤æ‚çš„æ¨¡æ‹Ÿä»»åŠ¡éƒ½æ¶‰åŠå…¶çŠ¶æ€ç©ºé—´ä¸­çš„å¤§é‡å¤šæ ·æ€§ï¼ˆä¾‹å¦‚ï¼Œè‡ªç„¶å›¾åƒçš„ç©ºé—´ï¼‰ã€‚ä»ä»£ç†çš„è§’åº¦æ¥çœ‹ï¼Œæ¢ç´¢å’Œè®°å¿†è¿™ä¸€ï¼ˆå¯èƒ½æ˜¯æ— é™çš„ï¼‰è¾“å…¥é›†åˆçš„ç²¾ç¡®å€¼æ˜¾ç„¶æ˜¯ä¸å¯è¡Œçš„ã€‚æ­¤å¤–ï¼Œå¯¹äºè®¸å¤šåº”ç”¨ï¼Œè®­ç»ƒä¸­ä½¿ç”¨çš„å—æ§å®éªŒå®¤ç¯å¢ƒå¯èƒ½æ— æ³•åæ˜ ç‰¹å®šä»»åŠ¡çš„æ‰€æœ‰å¯èƒ½é…ç½®çš„å…¨é¢å¤šæ ·æ€§ã€‚å› æ­¤ï¼Œä»£ç†çš„è¡Œä¸ºåœ¨éƒ¨ç½²æœŸé—´åº”ç†æƒ³åœ°å¯¹å¯èƒ½è§‚å¯Ÿåˆ°çš„å°åˆ†å¸ƒåç§»ä¿æŒç¨³å¥ã€‚
- en: Deep neural network-based agent models serve as a practical approach to tackle
    these problems, effectively serving as a functional prior that tries to capture
    only the *most relevant and causal* features of the states that the agent should
    require for effective decision-making. However, providing a precise understanding
    of how different design choices affect neural network training and its resulting
    generalisation is very much still an open question.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„ä»£ç†æ¨¡å‹ä½œä¸ºä¸€ç§å®ç”¨çš„æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå®é™…ä¸Šä½œä¸ºä¸€ç§åŠŸèƒ½æ€§å…ˆéªŒï¼Œè¯•å›¾ä»…æ•æ‰ä»£ç†åœ¨æœ‰æ•ˆå†³ç­–è¿‡ç¨‹ä¸­æ‰€éœ€çš„*æœ€ç›¸å…³å’Œå› æœ*ç‰¹å¾ã€‚ç„¶è€Œï¼Œå¦‚ä½•å‡†ç¡®ç†è§£ä¸åŒè®¾è®¡é€‰æ‹©å¯¹ç¥ç»ç½‘ç»œè®­ç»ƒåŠå…¶æœ€ç»ˆæ³›åŒ–æ•ˆæœçš„å½±å“ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„é—®é¢˜ã€‚
- en: In our recent paper [7], we study the geometric properties that make a deep
    RL model generalise robustly and effectively. In particular, we focus on the model
    of *hyperbolic geometry*, which we describe next [8].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬æœ€è¿‘çš„è®ºæ–‡[7]ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä½¿æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ç¨³å¥æœ‰æ•ˆæ³›åŒ–çš„å‡ ä½•ç‰¹æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å…³æ³¨äº*è¶…æ›²é¢å‡ ä½•*æ¨¡å‹ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬ä¼šæè¿°[8]ã€‚
- en: '![](../Images/f7cdd5dea837803c23323e1fc25219da.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7cdd5dea837803c23323e1fc25219da.png)'
- en: The hierarchical relationship between states in Breakout, visualised in the
    PoincarÃ© disk model of the hyperbolic space.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Breakoutä¸­ï¼ŒçŠ¶æ€ä¹‹é—´çš„å±‚æ¬¡å…³ç³»ï¼Œé€šè¿‡è¶…æ›²é¢ç©ºé—´çš„åºåŠ è±åœ†ç›˜æ¨¡å‹å¯è§†åŒ–ã€‚
- en: Hyperbolic Geometry
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¶…æ›²é¢å‡ ä½•
- en: Most applications in machine learning (and more broadly, computer science and
    mathematics) utilise Euclidean spaces to represent data and perform numerical
    operations. Euclidean spaces can be easily visualised and most of their properties
    are inherently intuitive to understand. For instance, the total volume grows polynomially
    with the radius from the origin [9], and translating two points by the same vector
    does not affect their distance.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°æœºå™¨å­¦ä¹ ï¼ˆæ›´å¹¿æ³›åœ°è¯´ï¼Œè®¡ç®—æœºç§‘å­¦å’Œæ•°å­¦ï¼‰åº”ç”¨åˆ©ç”¨æ¬§å‡ é‡Œå¾—ç©ºé—´æ¥è¡¨ç¤ºæ•°æ®å’Œæ‰§è¡Œæ•°å€¼è¿ç®—ã€‚æ¬§å‡ é‡Œå¾—ç©ºé—´å¯ä»¥å¾ˆå®¹æ˜“åœ°å¯è§†åŒ–ï¼Œå¹¶ä¸”å®ƒä»¬çš„å¤§å¤šæ•°å±æ€§æœ¬è´¨ä¸Šæ˜¯ç›´è§‚æ˜“æ‡‚çš„ã€‚ä¾‹å¦‚ï¼Œæ€»ä½“ç§¯éšç€ä»åŸç‚¹çš„åŠå¾„ä»¥å¤šé¡¹å¼å½¢å¼å¢é•¿[9]ï¼Œå¹¶ä¸”å°†ä¸¤ä¸ªç‚¹é€šè¿‡ç›¸åŒçš„å‘é‡å¹³ç§»ä¸ä¼šå½±å“å®ƒä»¬ä¹‹é—´çš„è·ç¦»ã€‚
- en: Hyperbolic spaces [10] do not possess such intuitive properties and, formally,
    can be described as a special type of *Riemannian manifolds*, i.e., *n-dimensional*
    objects embedded in *n*+1 dimensions that are only *locally* Euclidean. One of
    the defining properties of hyperbolic spaces is their *constant negative curvature*
    resulting in distances and volumes growing exponentially rather than polynomially.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…æ›²é¢ç©ºé—´[10]ä¸å…·å¤‡è¿™æ ·ç›´è§‚çš„å±æ€§ï¼Œå¹¶ä¸”å½¢å¼ä¸Šå¯ä»¥æè¿°ä¸ºä¸€ç§ç‰¹æ®Šçš„*é»æ›¼æµå½¢*ï¼Œå³* n ç»´*çš„å¯¹è±¡åµŒå…¥åœ¨*n*+1ç»´ä¸­ï¼Œä»…åœ¨*å±€éƒ¨*ä¸Šæ˜¯æ¬§å‡ é‡Œå¾—çš„ã€‚è¶…æ›²é¢ç©ºé—´çš„ä¸€ä¸ªå®šä¹‰ç‰¹æ€§æ˜¯å…¶*æ’å®šçš„è´Ÿæ›²ç‡*ï¼Œå¯¼è‡´è·ç¦»å’Œä½“ç§¯ä»¥æŒ‡æ•°å½¢å¼å¢é•¿ï¼Œè€Œä¸æ˜¯å¤šé¡¹å¼å½¢å¼ã€‚
- en: '![](../Images/7a7bae5263380d37c2a3e7e41bbb60ac.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a7bae5263380d37c2a3e7e41bbb60ac.png)'
- en: '*Shortest paths (geodesics) between points in hyperbolic space (ğ”¹Â²) and nodes
    in an embedded tree.*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¶…æ›²é¢ç©ºé—´ï¼ˆğ”¹Â²ï¼‰ä¸­ç‚¹ä¹‹é—´çš„æœ€çŸ­è·¯å¾„ï¼ˆæµ‹åœ°çº¿ï¼‰å’ŒåµŒå…¥æ ‘ä¸­çš„èŠ‚ç‚¹ã€‚*'
- en: This allows interpreting hyperbolic spaces as continuous analogs of trees, in
    which the number of leaf nodes also grows exponentially as we increase the depth.
    Due to this fact, a tree can be embedded isometrically (i.e., in a manner preserving
    the relative distances between nodes) in a hyperbolic space of only two dimensions
    [11]. In contrast, embedding a tree in a Euclidean space results in distortions,
    which can be decreased by using a high dimension.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿å¾—è¶…æ›²é¢ç©ºé—´å¯ä»¥è¢«è§£é‡Šä¸ºæ ‘çš„è¿ç»­ç±»æ¯”ï¼Œå…¶ä¸­å¶èŠ‚ç‚¹çš„æ•°é‡ä¹Ÿéšç€æ·±åº¦çš„å¢åŠ è€Œå‘ˆæŒ‡æ•°å¢é•¿ã€‚ç”±äºè¿™ä¸€äº‹å®ï¼Œä¸€æ£µæ ‘å¯ä»¥åœ¨åªæœ‰äºŒç»´çš„è¶…æ›²é¢ç©ºé—´ä¸­ç­‰è·åµŒå…¥ï¼ˆå³ä¿æŒèŠ‚ç‚¹ä¹‹é—´ç›¸å¯¹è·ç¦»çš„æ–¹å¼ï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå°†ä¸€æ£µæ ‘åµŒå…¥åˆ°æ¬§å‡ é‡Œå¾—ç©ºé—´ä¼šå¯¼è‡´æ‰­æ›²ï¼Œè¿™äº›æ‰­æ›²å¯ä»¥é€šè¿‡ä½¿ç”¨é«˜ç»´ç©ºé—´æ¥å‡å°‘ã€‚
- en: There exist several equivalent models of hyperbolic geometry; here, we consider
    the *PoincarÃ© ball* (denoted by ğ”¹*â¿*), which can be conceptualised as an *n*-dimensional
    unit ball that preserves the notion of angles from Euclidean spaces. Since total
    volume of the PoincarÃ© ball grows exponentially with the radius from the origin,
    the geodesics (shortest paths) are circular arcs perpendicular to the boundary
    rather than straight, as in Euclidean spaces [12].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å­˜åœ¨å¤šä¸ªç­‰æ•ˆçš„åŒæ›²å‡ ä½•æ¨¡å‹ï¼›åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è€ƒè™‘*åºåŠ è±çƒ*ï¼ˆè®°ä½œğ”¹*â¿*ï¼‰ï¼Œå®ƒå¯ä»¥è¢«æ¦‚å¿µåŒ–ä¸ºä¸€ä¸ª*n*ç»´çš„å•ä½çƒï¼Œä¿ç•™äº†æ¥è‡ªæ¬§å‡ é‡Œå¾—ç©ºé—´çš„è§’åº¦æ¦‚å¿µã€‚ç”±äºåºåŠ è±çƒçš„æ€»ä½“ç§¯éšç€ä»åŸç‚¹çš„åŠå¾„æŒ‡æ•°å¢é•¿ï¼Œæµ‹åœ°çº¿ï¼ˆæœ€çŸ­è·¯å¾„ï¼‰æ˜¯ä¸è¾¹ç•Œå‚ç›´çš„åœ†å¼§ï¼Œè€Œä¸æ˜¯åƒåœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­çš„ç›´çº¿[12]ã€‚
- en: '![](../Images/682c28537fec44ba001ca9f2c315d613.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/682c28537fec44ba001ca9f2c315d613.png)'
- en: PoincarÃ© and Beltrami-Klein models of hyperbolic geometry.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åºåŠ è±å’Œè´å°”ç‰¹æ‹‰ç±³-å…‹è±å› çš„åŒæ›²å‡ ä½•æ¨¡å‹ã€‚
- en: In order to work with hyperbolic spaces in machine learning, we have to redefine
    standard operations with vectors, the notions of hyperplanes, and the relative
    distances between these elements [13]. The conceptual difficulty of doing this
    stems from the fact that we need to work in the *tangent space*, a local Euclidean
    representation of the hyperbolic space.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åœ¨æœºå™¨å­¦ä¹ ä¸­å¤„ç†åŒæ›²ç©ºé—´ï¼Œæˆ‘ä»¬å¿…é¡»é‡æ–°å®šä¹‰ä¸å‘é‡çš„æ ‡å‡†æ“ä½œã€è¶…å¹³é¢çš„æ¦‚å¿µä»¥åŠè¿™äº›å…ƒç´ ä¹‹é—´çš„ç›¸å¯¹è·ç¦»[13]ã€‚è¿™æ ·åšçš„æ¦‚å¿µå›°éš¾åœ¨äºæˆ‘ä»¬éœ€è¦åœ¨*åˆ‡ç©ºé—´*ä¸­å·¥ä½œï¼Œè¿™æ˜¯åŒæ›²ç©ºé—´çš„å±€éƒ¨æ¬§å‡ é‡Œå¾—è¡¨ç¤ºã€‚
- en: This is achieved by the *exponential map* exp**â‚“**(**v**), which takes a unit
    step along a geodesic starting from point **x** in the direction of an input vector
    **v**. We use the exponential map from the origin of the PoincarÃ© ball to map
    Euclidean input vectors **v** into ğ”¹*â¿* [14]*.*
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯é€šè¿‡*æŒ‡æ•°æ˜ å°„* exp**â‚“**(**v**)ï¼Œå®ƒæ²¿ç€ä»ç‚¹**x**å‡ºå‘çš„æµ‹åœ°çº¿æœè¾“å…¥å‘é‡**v**çš„æ–¹å‘è¿ˆå‡ºä¸€ä¸ªå•ä½æ­¥é•¿æ¥å®ç°çš„ã€‚æˆ‘ä»¬ä½¿ç”¨ä»åºåŠ è±çƒåŸç‚¹çš„æŒ‡æ•°æ˜ å°„å°†æ¬§å‡ é‡Œå¾—è¾“å…¥å‘é‡**v**æ˜ å°„åˆ°ğ”¹*â¿*
    [14]*ã€‚
- en: '*Gyrovector spaces* [15] allow extending common vector operations to non-Euclidean
    geometries. One such operation is denoted by **x**âŠ•**y**and is referred to as
    *MÃ¶bius addition* [16].'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*é™€èºå‘é‡ç©ºé—´* [15] å…è®¸å°†å¸¸è§çš„å‘é‡æ“ä½œæ‰©å±•åˆ°éæ¬§å‡ é‡Œå¾—å‡ ä½•ä¸­ã€‚è¿™æ ·çš„ä¸€ä¸ªæ“ä½œè®°ä½œ**x**âŠ•**y**ï¼Œè¢«ç§°ä¸º*è«æ¯”ä¹Œæ–¯åŠ æ³•* [16]ã€‚'
- en: '*Gyroplanes* (denoted by *H*) are a generalisation of an oriented hyperplane
    in a gyrovector space. A gyroplane on the PoincarÃ© ball is parameterised by *n*-dimensional
    shift **p** and normal **w**, such that *H =* {**y***âˆˆ* ğ”¹*â¿* : *<***y**âŠ•**p***,***w***>=0*}.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*é™€èºå¹³é¢*ï¼ˆè®°ä½œ*H*ï¼‰æ˜¯é™€èºå‘é‡ç©ºé—´ä¸­å®šå‘è¶…å¹³é¢çš„æ¨å¹¿ã€‚åºåŠ è±çƒä¸Šçš„ä¸€ä¸ªé™€èºå¹³é¢ç”±*n*ç»´çš„å¹³ç§»**p**å’Œæ³•å‘é‡**w**å‚æ•°åŒ–ï¼Œä½¿å¾—*H =*
    {**y***âˆˆ* ğ”¹*â¿* : *<***y**âŠ•**p***,***w***>=0*}ã€‚'
- en: 'In machine learning problems, hyperplanes and gyroplanes can be used as *oriented
    decision boundaries*. The shift and normal vectors (**p** and **w**) provide an
    alternative parametrisation to define linear affine transformation [17]. An analogy
    of a fully connected layer with *m* output units is a set of *m* gyroplanes in
    ğ”¹â¿: given an *n*-dimensional input vector **x**in the hyperbolic space, the layer
    output *f*(**x**) is computed as the signed and scaled distance between *x* and
    each gyroplane H:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ é—®é¢˜ä¸­ï¼Œè¶…å¹³é¢å’Œé™€èºå¹³é¢å¯ä»¥ç”¨ä½œ*å®šå‘å†³ç­–è¾¹ç•Œ*ã€‚å¹³ç§»å’Œæ³•å‘é‡ï¼ˆ**p** å’Œ **w**ï¼‰æä¾›äº†ä¸€ç§æ›¿ä»£å‚æ•°åŒ–æ¥å®šä¹‰çº¿æ€§ä»¿å°„å˜æ¢[17]ã€‚å…·æœ‰*m*ä¸ªè¾“å‡ºå•å…ƒçš„å…¨è¿æ¥å±‚çš„ç±»æ¯”æ˜¯ğ”¹â¿ä¸­çš„ä¸€ç»„*m*ä¸ªé™€èºå¹³é¢ï¼šç»™å®šä¸€ä¸ªåœ¨åŒæ›²ç©ºé—´ä¸­çš„*n*ç»´è¾“å…¥å‘é‡**x**ï¼Œå±‚è¾“å‡º
    *f*(**x**) ä½œä¸º*x*ä¸æ¯ä¸ªé™€èºå¹³é¢Hä¹‹é—´çš„å¸¦ç¬¦å·å’Œç¼©æ”¾è·ç¦»æ¥è®¡ç®—ï¼š
- en: '*f*(**x**) *=* 2 sign(*<***x**âŠ•â€“**p***,***w***>*) *||***w***||d*(**x***,H*)
    */* (*1 â€” ||***p***||*Â²)Â¹áŸÂ²*,*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(**x**) *=* 2 sign(*<***x**âŠ•â€“**p***,***w***>*) *||***w***||d*(**x***,H*)
    */* (*1 â€” ||***p***||*Â²)Â¹áŸÂ²*,*'
- en: where *d*(**x***,H*)is the distance function between *x* and *H* on the PoincarÃ©
    ball [18].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *d*(**x***,H*) æ˜¯åºåŠ è±çƒä¸Š*x*å’Œ*H*ä¹‹é—´çš„è·ç¦»å‡½æ•°[18]ã€‚
- en: Similarly to prior work for supervised [19] and unsupervised learning [20],
    we use these parameterised *gyroplane fully-connected layers* within standard
    natural network architectures by substituting the standard Euclidean layer. The
    hyperbolic geometry of the resulting feature space introduces a different inductive
    bias that should be more appropriate for many RL problems, as we motivate next.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…ˆå‰ç”¨äºç›‘ç£å­¦ä¹ [19]å’Œæ— ç›‘ç£å­¦ä¹ [20]çš„å·¥ä½œç±»ä¼¼ï¼Œæˆ‘ä»¬åœ¨æ ‡å‡†è‡ªç„¶ç½‘ç»œæ¶æ„ä¸­ä½¿ç”¨è¿™äº›å‚æ•°åŒ–çš„*é™€èºå¹³é¢å…¨è¿æ¥å±‚*ï¼Œæ›¿ä»£äº†æ ‡å‡†çš„æ¬§å‡ é‡Œå¾—å±‚ã€‚ç»“æœç‰¹å¾ç©ºé—´çš„åŒæ›²å‡ ä½•å¼•å…¥äº†ä¸€ç§ä¸åŒçš„å½’çº³åå·®ï¼Œè¿™å¯¹è®¸å¤šå¼ºåŒ–å­¦ä¹ é—®é¢˜åº”è¯¥æ›´ä¸ºé€‚ç”¨ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹æ–‡ä¸­è¿›è¡Œè¯´æ˜ã€‚
- en: Hyperbolicity of RL problems
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„åŒæ›²æ€§
- en: Due to the markovian property in RL problems, the evolution of the state in
    a trajectory can be conceptualised as a tree with the policy and dynamics determining
    the probability of ending up in each of its possible branches. Intuitively, the
    value and optimal policy for each state are naturally related to its possible
    successors in the MDP.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº RL é—®é¢˜ä¸­çš„é©¬å°”å¯å¤«æ€§è´¨ï¼Œè½¨è¿¹ä¸­çŠ¶æ€çš„æ¼”å˜å¯ä»¥è¢«æ¦‚å¿µåŒ–ä¸ºä¸€ä¸ªæ ‘ï¼Œå…¶ä¸­ç­–ç•¥å’ŒåŠ¨æ€å†³å®šäº†æ¯ä¸ªå¯èƒ½åˆ†æ”¯çš„æ¦‚ç‡ã€‚ç›´è§‚åœ°ï¼ŒMDP ä¸­æ¯ä¸ªçŠ¶æ€çš„å€¼å’Œæœ€ä¼˜ç­–ç•¥è‡ªç„¶ä¸å…¶å¯èƒ½çš„åç»§çŠ¶æ€ç›¸å…³ã€‚
- en: In contrast, there are many examples where other fixed, non-hierarchical information
    about the state (such as the general appearance of the environment), should be
    ignored. For instance, Raileanu and Fergus [21] observed the agentsâ€™ value function
    and policy models overfitting to spurious correlations from non-hierarchical features
    in *Procgen* environments [22] (e.g., background color), leading to poor generalisation
    to unseen levels.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ‰è®¸å¤šä¾‹å­è¡¨æ˜å…¶ä»–å›ºå®šçš„ã€éå±‚æ¬¡åŒ–çš„çŠ¶æ€ä¿¡æ¯ï¼ˆä¾‹å¦‚ç¯å¢ƒçš„ä¸€èˆ¬å¤–è§‚ï¼‰åº”è¯¥è¢«å¿½ç•¥ã€‚ä¾‹å¦‚ï¼ŒRaileanu å’Œ Fergus [21] è§‚å¯Ÿåˆ°ä»£ç†çš„ä»·å€¼å‡½æ•°å’Œç­–ç•¥æ¨¡å‹åœ¨*Procgen*ç¯å¢ƒ
    [22]ï¼ˆä¾‹å¦‚èƒŒæ™¯é¢œè‰²ï¼‰ä¸­å¯¹éå±‚æ¬¡ç‰¹å¾çš„è™šå‡å…³è”è¿›è¡Œäº†è¿‡æ‹Ÿåˆï¼Œå¯¼è‡´å¯¹æœªè§è¿‡çš„å…³å¡æ³›åŒ–èƒ½åŠ›å·®ã€‚
- en: Based on these observations, we hypothesise that effective features should encode
    information directly related to the hierarchical state relationships of the MDPs,
    reflecting its tree-like structure.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè¿™äº›è§‚å¯Ÿï¼Œæˆ‘ä»¬å‡è®¾æœ‰æ•ˆçš„ç‰¹å¾åº”è¯¥ç¼–ç ç›´æ¥ä¸ MDP çš„å±‚æ¬¡çŠ¶æ€å…³ç³»ç›¸å…³çš„ä¿¡æ¯ï¼Œåæ˜ å…¶æ ‘çŠ¶ç»“æ„ã€‚
- en: 'In order to validate our hypothesis, we analyze the representation spaces learned
    by RL agents, testing whether they manifest a hierarchical structure. We use the
    notion of Gromov *Î´*-*hyperbolicity* [11]: a metric space (*X,d*) is said to be
    *Î´*-hyperbolic if every possible geodesic triangle â–³ABC is *Î´*-slim, i.e., for
    every point on any side of â–³ABC there exists some point on one of the other sides
    whose distance is at most *Î´*. A characteristic of tree structures is that every
    point in any possible â–³ABC belongs to at least two of its sides, yielding *Î´=0*.
    Thus, we can interpret *Î´*-hyperbolicity as measuring the deviation of a given
    metric from a tree metric.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„å‡è®¾ï¼Œæˆ‘ä»¬åˆ†æäº† RL ä»£ç†å­¦ä¹ åˆ°çš„è¡¨ç¤ºç©ºé—´ï¼Œæµ‹è¯•å®ƒä»¬æ˜¯å¦è¡¨ç°å‡ºå±‚æ¬¡ç»“æ„ã€‚æˆ‘ä»¬ä½¿ç”¨ Gromov *Î´*-*åŒæ›²æ€§* [11] çš„æ¦‚å¿µï¼šä¸€ä¸ªåº¦é‡ç©ºé—´ï¼ˆ*X,d*ï¼‰å¦‚æœæ¯ä¸ªå¯èƒ½çš„æµ‹åœ°çº¿ä¸‰è§’å½¢â–³ABCéƒ½æ˜¯*Î´*-ç˜¦çš„ï¼Œå³â–³ABCçš„æ¯ä¸€ä¾§ä¸Šçš„æ¯ä¸ªç‚¹éƒ½å­˜åœ¨å¦ä¸€ä¾§ä¸Šçš„æŸä¸ªç‚¹ï¼Œå…¶è·ç¦»è‡³å¤šä¸º*Î´*ï¼Œåˆ™ç§°å…¶ä¸º*Î´*-åŒæ›²ã€‚æ ‘å½¢ç»“æ„çš„ä¸€ä¸ªç‰¹å¾æ˜¯â–³ABCä¸­çš„æ¯ä¸ªç‚¹éƒ½å±äºè‡³å°‘ä¸¤ä¸ªä¾§é¢ï¼Œä»è€Œäº§ç”Ÿ*Î´=0*ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†*Î´*-åŒæ›²æ€§è§£é‡Šä¸ºåº¦é‡ä¸æ ‘å½¢åº¦é‡ä¹‹é—´çš„åå·®ã€‚
- en: '![](../Images/7c3e8634f50ea4cb0ea5cd2b31345a83.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c3e8634f50ea4cb0ea5cd2b31345a83.png)'
- en: The necessary *Î´* such that for every point on any side of â–³ABC there exists
    some point on one of the other sides whose distance is at most *Î´* in a tree triangle
    (left), a hyperbolic triangle (center), and a Euclidean triangle (right).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¿…è¦çš„*Î´*ä½¿å¾—â–³ABCçš„æ¯ä¸€ä¾§ä¸Šçš„æ¯ä¸ªç‚¹éƒ½å­˜åœ¨å¦ä¸€ä¾§ä¸Šçš„æŸä¸ªç‚¹ï¼Œå…¶è·ç¦»åœ¨æ ‘å½¢ä¸‰è§’å½¢ï¼ˆå·¦ï¼‰ã€åŒæ›²ä¸‰è§’å½¢ï¼ˆä¸­ï¼‰å’Œæ¬§å‡ é‡Œå¾—ä¸‰è§’å½¢ï¼ˆå³ï¼‰ä¸­è‡³å¤šä¸º*Î´*ã€‚
- en: The final representations learned by an RL agent from encoding the collected
    states span some finite subset of Euclidean space, effectively yielding a discrete
    metric space. Similarly to [19], we *normalise* our *Î´*-hyperbolicity recordingsby
    the diameter of the space, yielding a *relative* measure that tries to be agnostic
    to the scale of the learned representations [23].
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: RL ä»£ç†é€šè¿‡å¯¹æ”¶é›†çš„çŠ¶æ€è¿›è¡Œç¼–ç æ‰€å­¦ä¹ çš„æœ€ç»ˆè¡¨ç¤ºè·¨è¶Šäº†æ¬§å‡ é‡Œå¾—ç©ºé—´çš„æŸä¸ªæœ‰é™å­é›†ï¼Œæœ‰æ•ˆåœ°å½¢æˆäº†ä¸€ä¸ªç¦»æ•£åº¦é‡ç©ºé—´ã€‚ç±»ä¼¼äº[19]ï¼Œæˆ‘ä»¬é€šè¿‡ç©ºé—´çš„ç›´å¾„å¯¹*Î´*-åŒæ›²æ€§è®°å½•è¿›è¡Œ*å½’ä¸€åŒ–*ï¼Œäº§ç”Ÿä¸€ä¸ª*ç›¸å¯¹*åº¦é‡ï¼Œè¯•å›¾å¯¹æ‰€å­¦è¡¨ç¤ºçš„å°ºåº¦ä¿æŒæ— å…³[23]ã€‚
- en: This allows us to practically interpret the hyperbolicity of the learned latent
    representations, spanning values between 0 (exact tree-like structure) and 1 (perfectly
    non-hyperbolic space). We train a standard PPO agent with the standard Impala
    architecture [24] and analyze how its performance and our *Î´*-hyperbolicity measure
    evolve as training progresses, on four different Procgen environments.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå®é™…è§£é‡Šæ‰€å­¦çš„æ½œåœ¨è¡¨ç¤ºçš„åŒæ›²æ€§ï¼Œå…¶å€¼åœ¨ 0ï¼ˆå®Œå…¨æ ‘çŠ¶ç»“æ„ï¼‰å’Œ 1ï¼ˆå®Œå…¨éåŒæ›²ç©ºé—´ï¼‰ä¹‹é—´ã€‚æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„ Impala æ¶æ„[24]è®­ç»ƒä¸€ä¸ªæ ‡å‡†çš„
    PPO ä»£ç†ï¼Œå¹¶åˆ†æéšç€è®­ç»ƒçš„è¿›å±•ï¼Œå…¶æ€§èƒ½å’Œæˆ‘ä»¬çš„*Î´*-åŒæ›²æ€§åº¦é‡å¦‚ä½•æ¼”å˜ï¼Œæµ‹è¯•å››ç§ä¸åŒçš„ Procgen ç¯å¢ƒã€‚
- en: We observe that *Î´* quickly drops to low values (0.22â€“0.28) in the first training
    iterations in all environments, reflecting the largest relative improvements in
    agent performance. Subsequently, an interesting dichotomy seems to take place.
    In the *fruitbot* and *starpilot* environments, *Î´* further decreases throughout
    training as the agent recovers high performance with a low generalisation gap
    between the training and test distribution of levels.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨æ‰€æœ‰ç¯å¢ƒä¸­ï¼Œ*Î´*åœ¨è®­ç»ƒçš„å‰å‡ æ¬¡è¿­ä»£ä¸­è¿…é€Ÿé™åˆ°ä½å€¼ï¼ˆ0.22â€“0.28ï¼‰ï¼Œåæ˜ äº†ä»£ç†æ€§èƒ½çš„ç›¸å¯¹æœ€å¤§æå‡ã€‚éšåï¼Œä¼¼ä¹å‘ç”Ÿäº†ä¸€ä¸ªæœ‰è¶£çš„äºŒåˆ†æ³•ã€‚åœ¨*fruitbot*å’Œ*starpilot*ç¯å¢ƒä¸­ï¼Œ*Î´*åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›ä¸€æ­¥å‡å°‘ï¼Œå› ä¸ºä»£ç†åœ¨è®­ç»ƒå’Œæµ‹è¯•æ°´å¹³åˆ†å¸ƒä¹‹é—´æ¢å¤äº†é«˜æ€§èƒ½ï¼Œå¹¶ä¸”æ³›åŒ–å·®è·è¾ƒå°ã€‚
- en: Instead, in *bigfish* and *dodgeball*, *Î´* begins to increase again after the
    initial drop, suggesting that the latent representation space starts losing its
    hierarchical structure. Correspondingly, in these last two environments, the agent
    starts overfitting as test levels performance stagnates while the generalization
    gap with the training levels performance keeps increasing.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œåœ¨*bigfish*å’Œ*dodgeball*ä¸­ï¼Œ*Î´*åœ¨åˆå§‹ä¸‹é™åå¼€å§‹å†æ¬¡å¢åŠ ï¼Œè¡¨æ˜æ½œåœ¨è¡¨ç¤ºç©ºé—´å¼€å§‹ä¸§å¤±å…¶å±‚æ¬¡ç»“æ„ã€‚ç›¸åº”åœ°ï¼Œåœ¨è¿™ä¸¤ä¸ªç¯å¢ƒä¸­ï¼Œä»£ç†å¼€å§‹è¿‡æ‹Ÿåˆï¼Œå› ä¸ºæµ‹è¯•æ°´å¹³çš„è¡¨ç°åœæ»ä¸å‰ï¼Œè€Œä¸è®­ç»ƒæ°´å¹³è¡¨ç°ä¹‹é—´çš„æ³›åŒ–å·®è·æŒç»­æ‰©å¤§ã€‚
- en: '![](../Images/56479195563a0e88b5c97d8b6ba2dbe6.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56479195563a0e88b5c97d8b6ba2dbe6.png)'
- en: Performance and relative *Î´*-hyperbolicity of the final latent space of a PPO
    agent in Procgen.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: PPOä»£ç†åœ¨Procgenä¸­çš„æœ€ç»ˆæ½œåœ¨ç©ºé—´çš„æ€§èƒ½å’Œç›¸å¯¹*Î´*-è¶…æ›²ç‡ã€‚
- en: These results support our hypothesis, empirically showing the importance of
    encoding hierarchical features and suggesting that PPOâ€™s poor generalisation in
    some environments is due to the observed tendency of Euclidean latent spaces to
    encode spurious features that hinder hyperbolicity.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬çš„å‡è®¾ï¼Œç»éªŒä¸Šå±•ç¤ºäº†ç¼–ç å±‚æ¬¡ç‰¹å¾çš„é‡è¦æ€§ï¼Œå¹¶å»ºè®®PPOåœ¨æŸäº›ç¯å¢ƒä¸­æ³›åŒ–æ€§èƒ½å·®æ˜¯ç”±äºæ¬§å‡ é‡Œå¾—æ½œåœ¨ç©ºé—´ç¼–ç äº†è™šå‡çš„ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾é˜»ç¢äº†è¶…æ›²ç‡çš„æ•ˆæœã€‚
- en: Training agents with hyperbolic latent spaces
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¶…æ›²ç‡æ½œåœ¨ç©ºé—´è®­ç»ƒä»£ç†
- en: Based on our findings, we propose to employ hyperbolic geometry to encode the
    final latent representations of deep RL models. Our approach has the objective
    of introducing a different inductive bias that incentivises modeling the agentâ€™s
    policy and value function based on features that reflect the causal hierarchical
    evolution observed in common MDPs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨è¶…æ›²ç‡å‡ ä½•æ¥ç¼–ç æ·±åº¦RLæ¨¡å‹çš„æœ€ç»ˆæ½œåœ¨è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•æ—¨åœ¨å¼•å…¥ä¸åŒçš„å½’çº³åå·®ï¼Œä»¥æ¿€åŠ±åŸºäºåæ˜ å¸¸è§MDPä¸­è§‚å¯Ÿåˆ°çš„å› æœå±‚æ¬¡æ¼”å˜çš„ç‰¹å¾æ¥å»ºæ¨¡ä»£ç†çš„ç­–ç•¥å’Œå€¼å‡½æ•°ã€‚
- en: Our basic implementation tries to make minimal changes to the underlying algorithm
    and models. We start with a simple extension to PPO, replacing the final ReLU
    and linear layer with an exponential map to ğ”¹*â¿* and a gyroplane fully-connected
    layer outputting the value function and policy logits.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„åŸºæœ¬å®ç°å°è¯•å¯¹åº•å±‚ç®—æ³•å’Œæ¨¡å‹è¿›è¡Œæœ€å°çš„ä¿®æ”¹ã€‚æˆ‘ä»¬ä»å¯¹PPOçš„ç®€å•æ‰©å±•å¼€å§‹ï¼Œå°†æœ€ç»ˆçš„ReLUå’Œçº¿æ€§å±‚æ›¿æ¢ä¸ºåˆ°ğ”¹*â¿*çš„æŒ‡æ•°æ˜ å°„ä»¥åŠä¸€ä¸ªè¾“å‡ºå€¼å‡½æ•°å’Œç­–ç•¥å¯¹æ•°çš„é™€èºé¢å…¨è¿æ¥å±‚ã€‚
- en: '![](../Images/23ce148581eee208b86533481577d927.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23ce148581eee208b86533481577d927.png)'
- en: Integration of hyperbolic space with the Impala architecture used to model PPOâ€™s
    policy and value.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¶…æ›²ç‡ç©ºé—´ä¸ç”¨äºå»ºæ¨¡PPOç­–ç•¥å’Œå€¼çš„Impalaæ¶æ„è¿›è¡Œé›†æˆã€‚
- en: However, this naÃ¯ve approach results in underwhelming performance, lagging considerably
    behind the performance of standard PPO. Furthermore, we found that the hyperbolic
    policy struggles to start exploring and later annealing back to more deterministic
    behaviour as performance improves, as we would normally expect from PPOâ€™s entropy
    bonus. These results appear to indicate the presence of optimisation challenges
    stemming from end-to-end RL training with hyperbolic representations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™ç§å¹¼ç¨šçš„æ–¹æ³•å¯¼è‡´äº†ä»¤äººå¤±æœ›çš„è¡¨ç°ï¼Œè¿œè¿œè½åäºæ ‡å‡†PPOçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°è¶…æ›²ç‡ç­–ç•¥åœ¨æ€§èƒ½æ”¹å–„æ—¶ï¼Œéš¾ä»¥å¼€å§‹æ¢ç´¢å¹¶éšåé€€å›åˆ°æ›´ç¡®å®šæ€§çš„è¡Œä¸ºï¼Œè¿™ä¸æˆ‘ä»¬é€šå¸¸æœŸæœ›PPOçš„ç†µå¥–åŠ±ç›¸åã€‚è¿™äº›ç»“æœä¼¼ä¹è¡¨æ˜äº†æºè‡ªè¶…æ›²ç‡è¡¨ç¤ºçš„ç«¯åˆ°ç«¯RLè®­ç»ƒä¸­çš„ä¼˜åŒ–æŒ‘æˆ˜ã€‚
- en: '![](../Images/23d2a5c2556a8df1e939877d6c881947.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23d2a5c2556a8df1e939877d6c881947.png)'
- en: Performance and gradients from our unregularised integration of hyperbolic representations
    with RL.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœªæ­£åˆ™åŒ–çš„è¶…æ›²ç‡è¡¨ç¤ºä¸RLé›†æˆçš„æ€§èƒ½å’Œæ¢¯åº¦ã€‚
- en: To overcome similar optimisation issues, prior work using hyperbolic space for
    supervised and unsupervised learning introduced careful initialisation schemes
    [25] and stabilisation practices such as representation clipping [26]. While we
    do also make use of such practices in our implementation, they appear to be largely
    ineffective for RL problems.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å…‹æœç±»ä¼¼çš„ä¼˜åŒ–é—®é¢˜ï¼Œå…ˆå‰çš„å·¥ä½œåœ¨ç›‘ç£å’Œæ— ç›‘ç£å­¦ä¹ ä¸­ä½¿ç”¨åŒæ›²ç©ºé—´ï¼Œæå‡ºäº†ä»”ç»†çš„åˆå§‹åŒ–æ–¹æ¡ˆ[25]å’Œç¨³å®šåŒ–æ–¹æ³•ï¼Œå¦‚è¡¨ç¤ºå‰ªè£[26]ã€‚è™½ç„¶æˆ‘ä»¬åœ¨å®ç°ä¸­ä¹Ÿä½¿ç”¨äº†è¿™äº›æ–¹æ³•ï¼Œä½†å®ƒä»¬åœ¨RLé—®é¢˜ä¸­ä¼¼ä¹æ•ˆæœä¸å¤§ã€‚
- en: 'This should not be surprising: the main purpose of these strategies is to facilitate
    learning appropriate angular layouts in the first few training iterations, without
    which later end-to-end optimisation can often result in failure modes with low
    performance [13]. However, the inherent high variance and non-stationarity characterising
    RL make stabilisation strategies focused mostly on early iterations insufficient.
    The trajectory data and loss landscapes in RL can change significantly throughout
    training, making early angular layouts inevitably suboptimal in the long term.
    Furthermore, the high variance policy gradient optimisation [1] can much more
    easily enter the aforementioned unstable learning regimes, leading to the observed
    failure modes.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸åº”è¯¥ä»¤äººæƒŠè®¶ï¼šè¿™äº›ç­–ç•¥çš„ä¸»è¦ç›®çš„æ˜¯åœ¨å‰å‡ æ¬¡è®­ç»ƒè¿­ä»£ä¸­ä¿ƒè¿›å­¦ä¹ é€‚å½“çš„è§’åº¦å¸ƒå±€ï¼Œå¦åˆ™åç»­çš„ç«¯åˆ°ç«¯ä¼˜åŒ–å¸¸å¸¸ä¼šå¯¼è‡´ä½æ€§èƒ½çš„å¤±è´¥æ¨¡å¼[13]ã€‚ç„¶è€Œï¼ŒRLçš„å›ºæœ‰é«˜æ–¹å·®å’Œéå¹³ç¨³æ€§ä½¿å¾—ä¸»è¦å…³æ³¨æ—©æœŸè¿­ä»£çš„ç¨³å®šåŒ–ç­–ç•¥ä¸è¶³ã€‚RLä¸­çš„è½¨è¿¹æ•°æ®å’ŒæŸå¤±æ™¯è§‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ˜¾è‘—å˜åŒ–ï¼Œä½¿å¾—æ—©æœŸçš„è§’åº¦å¸ƒå±€åœ¨é•¿æœŸå†…ä¸å¯é¿å…åœ°å˜å¾—æ¬¡ä¼˜ã€‚æ­¤å¤–ï¼Œé«˜æ–¹å·®çš„ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–[1]æ›´å®¹æ˜“è¿›å…¥ä¸Šè¿°ä¸ç¨³å®šçš„å­¦ä¹ çŠ¶æ€ï¼Œä»è€Œå¯¼è‡´è§‚å¯Ÿåˆ°çš„å¤±è´¥æ¨¡å¼ã€‚
- en: Another sub-field of machine learning having to deal with similar non-stationarity
    and brittle optimisation is generative modeling with adversarial networks (GANs)
    [27]. In GAN training, the generated data and discriminatorâ€™s parameters constantly
    evolve, making the loss landscape highly non-stationary as in the RL setting.
    Furthermore, the adversarial nature of the optimisation makes it very brittle
    to exploding and vanishing gradients instabilities which lead to common failure
    modes [28].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªéœ€è¦å¤„ç†ç±»ä¼¼éå¹³ç¨³æ€§å’Œè„†å¼±ä¼˜åŒ–çš„æœºå™¨å­¦ä¹ å­é¢†åŸŸæ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰[27]ã€‚åœ¨GANè®­ç»ƒä¸­ï¼Œç”Ÿæˆçš„æ•°æ®å’Œåˆ¤åˆ«å™¨çš„å‚æ•°ä¸æ–­æ¼”å˜ï¼Œä½¿å¾—æŸå¤±æ™¯è§‚é«˜åº¦éå¹³ç¨³ï¼Œç±»ä¼¼äºRLè®¾ç½®ã€‚æ­¤å¤–ï¼Œä¼˜åŒ–çš„å¯¹æŠ—æ€§è´¨ä½¿å…¶å¯¹æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±çš„ä¸ç¨³å®šæ€§éå¸¸è„†å¼±ï¼Œå¯¼è‡´å¸¸è§çš„å¤±è´¥æ¨¡å¼[28]ã€‚
- en: We take inspiration from stabilisation practices in this parallel literature
    and make use of *spectral normalisation* (SN) [29], based on the recent analysis
    and empirical results showing its effects to precisely prevent exploding gradients
    phenomena [30]. Our implementation applies SN to all layers in the Euclidean encoder
    part of our model, leaving the final linear hyperbolic layer unregularised. Moreover,
    we also scale the final latent representations before mapping them to ğ”¹â¿, such
    that modifying the dimensionality of the representations should not significantly
    affect their own and their gradientsâ€™ magnitudes. We call our stabilisation recipe
    *Spectrally Regularised Hyperbolic Mappings* (S-RYM, pronounced *É›s-raÉªm*).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»è¿™æ–¹é¢çš„æ–‡çŒ®ä¸­è·å¾—çµæ„Ÿï¼Œå¹¶åˆ©ç”¨äº†*è°±å½’ä¸€åŒ–*ï¼ˆSNï¼‰[29]ï¼ŒåŸºäºæœ€è¿‘çš„åˆ†æå’Œå®è¯ç»“æœï¼Œæ˜¾ç¤ºå…¶èƒ½å¤Ÿå‡†ç¡®é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ç°è±¡[30]ã€‚æˆ‘ä»¬çš„å®ç°å°†SNåº”ç”¨äºæ¨¡å‹çš„æ¬§å‡ é‡Œå¾—ç¼–ç å™¨éƒ¨åˆ†çš„æ‰€æœ‰å±‚ï¼Œç•™ä¸‹æœ€ç»ˆçš„çº¿æ€§åŒæ›²å±‚ä¸è¿›è¡Œæ­£åˆ™åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨å°†æœ€ç»ˆçš„æ½œåœ¨è¡¨ç¤ºæ˜ å°„åˆ°ğ”¹â¿ä¹‹å‰å¯¹å…¶è¿›è¡Œç¼©æ”¾ï¼Œä»¥ä¾¿ä¿®æ”¹è¡¨ç¤ºçš„ç»´åº¦ä¸ä¼šæ˜¾è‘—å½±å“å®ƒä»¬åŠå…¶æ¢¯åº¦çš„å¤§å°ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„ç¨³å®šåŒ–æ–¹æ³•ç§°ä¸º*è°±æ­£åˆ™åŒ–åŒæ›²æ˜ å°„*ï¼ˆS-RYMï¼Œå‘éŸ³ä¸º*É›s-raÉªm*ï¼‰ã€‚
- en: Applying S-RYM to our hyperbolic RL agents appears to resolve their optimisation
    challenges. Furthermore, they also attain considerably higher performance relative
    to the original Euclidean implementation and maintain low gradient magnitudes
    throughout training.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å°†S-RYMåº”ç”¨äºæˆ‘ä»¬çš„åŒæ›²RLä»£ç†ä¼¼ä¹è§£å†³äº†å®ƒä»¬çš„ä¼˜åŒ–æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ç›¸å¯¹äºåŸå§‹æ¬§å‡ é‡Œå¾—å®ç°ä¹Ÿè·å¾—äº†æ˜¾è‘—æ›´é«˜çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒäº†ä½æ¢¯åº¦å¹…åº¦ã€‚
- en: '![](../Images/99c4c90167bc2c90f9822e963da8aeff.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99c4c90167bc2c90f9822e963da8aeff.png)'
- en: Performance and gradients applying S-RYM to both hyperbolic and Euclidean RL
    agents.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å°†S-RYMåº”ç”¨äºåŒæ›²å’Œæ¬§å‡ é‡Œå¾—RLä»£ç†çš„æ€§èƒ½å’Œæ¢¯åº¦ã€‚
- en: Experimental results
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®éªŒç»“æœ
- en: We evaluate hyperbolic deep RL across different benchmarks, RL algorithms, and
    training conditions. In addition to PPO, we also apply our methodology to the
    off-policy Rainbow DQN algorithm. We test our agents on both the full Procgen
    benchmark (16 environments) [22] and the Atari 100K benchmark (26 environments)
    [31].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ä¸åŒçš„åŸºå‡†æµ‹è¯•ã€RLç®—æ³•å’Œè®­ç»ƒæ¡ä»¶ä¸‹è¯„ä¼°åŒæ›²çº¿æ·±åº¦RLã€‚é™¤äº†PPOï¼Œæˆ‘ä»¬è¿˜å°†æˆ‘ä»¬çš„æ–¹æ³•åº”ç”¨äºç¦»çº¿ç­–ç•¥çš„Rainbow DQNç®—æ³•ã€‚æˆ‘ä»¬åœ¨å®Œæ•´çš„ProcgenåŸºå‡†æµ‹è¯•ï¼ˆ16ä¸ªç¯å¢ƒï¼‰[22]å’ŒAtari
    100KåŸºå‡†æµ‹è¯•ï¼ˆ26ä¸ªç¯å¢ƒï¼‰[31]ä¸Šæµ‹è¯•äº†æˆ‘ä»¬çš„ä»£ç†ã€‚
- en: '![](../Images/0accfda16a746ab49d4197de2c00382c.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0accfda16a746ab49d4197de2c00382c.png)'
- en: Renderings of the different environments from Procgen (left) and Atari (right).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Procgenï¼ˆå·¦ï¼‰å’ŒAtariï¼ˆå³ï¼‰ä¸åŒç¯å¢ƒçš„æ¸²æŸ“ã€‚
- en: On the Procgen benchmark, we compare our hyperbolic implementations also to
    using random crop data augmentations [32], a more traditional way of incentivising
    generalisation by inducing hand-picked invariances. Moreover, we also test an
    alternative version of the hyperbolic model that further constrains the dimensionality
    of the final representation to 32 (from 256 in the original Euclidean architecture)
    with the aim of increasing its focus on features that can be efficiently encoded
    in hyperbolic space.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ProcgenåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„åŒæ›²çº¿å®ç°ä¸ä½¿ç”¨éšæœºè£å‰ªæ•°æ®å¢å¼ºçš„æ–¹æ³•[32]è¿›è¡Œæ¯”è¾ƒï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¼•å…¥äººå·¥é€‰æ‹©çš„ä¸å˜æ€§æ¥æ¿€åŠ±æ³›åŒ–çš„æ›´ä¼ ç»Ÿæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æµ‹è¯•äº†åŒæ›²çº¿æ¨¡å‹çš„å¦ä¸€ç§ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬è¿›ä¸€æ­¥å°†æœ€ç»ˆè¡¨ç¤ºçš„ç»´åº¦é™åˆ¶ä¸º32ï¼ˆä»åŸå§‹æ¬§å‡ é‡Œå¾—æ¶æ„ä¸­çš„256ï¼‰ï¼Œæ—¨åœ¨å¢åŠ å…¶å¯¹èƒ½å¤Ÿåœ¨åŒæ›²çº¿ç©ºé—´ä¸­æœ‰æ•ˆç¼–ç çš„ç‰¹å¾çš„å…³æ³¨ã€‚
- en: Both our hyperbolic PPO and Rainbow DQN implementations yield conspicuous performance
    gains in the great majority of the environments. Remarkably, we find that reducing
    the size of the hyperbolic representations provides even further benefits with
    significant improvements for both algorithms.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„åŒæ›²çº¿PPOå’ŒRainbow DQNå®ç°éƒ½åœ¨ç»å¤§å¤šæ•°ç¯å¢ƒä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å‡å°‘åŒæ›²çº¿è¡¨ç¤ºçš„å¤§å°å¯¹äºè¿™ä¸¤ç§ç®—æ³•éƒ½æä¾›äº†è¿›ä¸€æ­¥çš„å¥½å¤„ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚
- en: '![](../Images/c84dfa3879bc99927d59414ed15e53e3.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c84dfa3879bc99927d59414ed15e53e3.png)'
- en: Performance of hyperbolic and Euclidean versions of PPO on Procgen.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ›²çº¿å’Œæ¬§å‡ é‡Œå¾—ç‰ˆæœ¬çš„PPOåœ¨Procgenä¸Šçš„è¡¨ç°ã€‚
- en: '![](../Images/a1a5df8d6f5a19c69378bf7f02af8d5f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1a5df8d6f5a19c69378bf7f02af8d5f.png)'
- en: Performance of hyperbolic and Euclidean versions of Rainbow DQN on Procgen.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ›²çº¿å’Œæ¬§å‡ é‡Œå¾—ç‰ˆæœ¬çš„Rainbow DQNåœ¨Procgenä¸Šçš„è¡¨ç°ã€‚
- en: In contrast, applying data augmentations appears to yield lower and inconsistent
    gains. We also find that test performance gains do not always correlate with gains
    on the specific 200 training levels to which the agent has access for exploration,
    resulting in a significantly reduced generalisation gap for the hyperbolic agents.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼Œåº”ç”¨æ•°æ®å¢å¼ºä¼¼ä¹å¸¦æ¥äº†è¾ƒä½ä¸”ä¸ä¸€è‡´çš„æ”¶ç›Šã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œæµ‹è¯•æ€§èƒ½çš„æå‡å¹¶ä¸æ€»æ˜¯ä¸ä»£ç†åœ¨æ¢ç´¢ä¸­èƒ½å¤Ÿè®¿é—®çš„ç‰¹å®š200ä¸ªè®­ç»ƒçº§åˆ«çš„æ”¶ç›Šç›¸å…³ï¼Œå¯¼è‡´åŒæ›²çº¿ä»£ç†çš„æ³›åŒ–å·®è·æ˜¾è‘—å‡å°‘ã€‚
- en: Similarly, the exact same hyperbolic deep RL framework provides consistent and
    significant benefits also on the Atari100K benchmark. Hyperbolic Rainbow shows
    improvements over the Euclidean baseline on most Atari environments, almost doubling
    the final human normalised score.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œç›¸åŒçš„åŒæ›²çº¿æ·±åº¦RLæ¡†æ¶åœ¨Atari100KåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿæä¾›äº†ä¸€è‡´ä¸”æ˜¾è‘—çš„å¥½å¤„ã€‚åŒæ›²çº¿Rainbowåœ¨å¤§å¤šæ•°Atariç¯å¢ƒä¸­ç›¸è¾ƒäºæ¬§å‡ é‡Œå¾—åŸºçº¿è¡¨ç°å‡ºæ”¹è¿›ï¼Œå‡ ä¹å°†æœ€ç»ˆçš„äººç±»å½’ä¸€åŒ–å¾—åˆ†ç¿»å€ã€‚
- en: '![](../Images/c08915dcdfee772e763fcc6198fd0a67.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c08915dcdfee772e763fcc6198fd0a67.png)'
- en: Absolute difference in normalised performance (Y-axis) and relative improvements
    (above bars) from integrating our regularised hyperbolic representations onto
    Rainbow DQN on the Atari benchmark.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æˆ‘ä»¬æ­£åˆ™åŒ–çš„åŒæ›²çº¿è¡¨ç¤ºæ•´åˆåˆ°AtariåŸºå‡†æµ‹è¯•ä¸­çš„Rainbow DQNä¸Šçš„å½’ä¸€åŒ–æ€§èƒ½ç»å¯¹å·®å¼‚ï¼ˆYè½´ï¼‰å’Œç›¸å¯¹æ”¹è¿›ï¼ˆæŸ±çŠ¶å›¾ä¸Šæ–¹ï¼‰ã€‚
- en: Overall, our results empirically validate that introducing hyperbolic representations
    to shape the prior of deep RL models can be remarkably effective across a diverse
    range of problems and algorithms.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç»“æœå®è¯éªŒè¯äº†å¼•å…¥åŒæ›²çº¿è¡¨ç¤ºæ¥å¡‘é€ æ·±åº¦RLæ¨¡å‹çš„å…ˆéªŒåœ¨å¤šç§é—®é¢˜å’Œç®—æ³•ä¸­å¯èƒ½æ˜¯æå…¶æœ‰æ•ˆçš„ã€‚
- en: Our hyperbolic RL agents come very close to the current SotA algorithms which
    incorporate different expensive and domain-specific practices (e.g., ad-hoc auxiliary
    losses, larger specialised architecture, etc.). Taken together, we believe these
    results show the great potential of our hyperbolic framework to become a standard
    way of parameterising deep RL models.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„åŒæ›²çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†ä¸å½“å‰çš„SotAï¼ˆstate-of-the-artï¼‰ç®—æ³•éå¸¸æ¥è¿‘ï¼Œè¿™äº›ç®—æ³•é‡‡ç”¨äº†ä¸åŒçš„æ˜‚è´µå’Œç‰¹å®šé¢†åŸŸçš„åšæ³•ï¼ˆä¾‹å¦‚ï¼Œä¸´æ—¶è¾…åŠ©æŸå¤±ã€æ›´å¤§çš„ä¸“ç”¨æ¶æ„ç­‰ï¼‰ã€‚ç»¼åˆæ¥çœ‹ï¼Œæˆ‘ä»¬ç›¸ä¿¡è¿™äº›ç»“æœå±•ç¤ºäº†æˆ‘ä»¬åŒæ›²çº¿æ¡†æ¶çš„å·¨å¤§æ½œåŠ›ï¼Œä½¿å…¶æœ‰å¯èƒ½æˆä¸ºå‚æ•°åŒ–æ·±åº¦RLæ¨¡å‹çš„æ ‡å‡†æ–¹æ³•ã€‚
- en: Visualisation
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–
- en: Using a two-dimensional version of our hyperbolic PPO for visualisation purposes,
    we observe a recurring phenomenon, where the magnitude of the representations
    monotonically increases within the considered subsets of the trajectory as more
    obstacles and/or enemies appear in the environments. Furthermore, we observe that
    the representations form tree-like structures, with their magnitudes from encoding
    on-policy states growing in directions mostly aligned to the value functionâ€™s
    gyroplaneâ€™s normal.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æˆ‘ä»¬åŒæ›²PPOçš„äºŒç»´ç‰ˆæœ¬è¿›è¡Œå¯è§†åŒ–æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸€ä¸ªé‡å¤å‡ºç°çš„ç°è±¡ï¼Œå³åœ¨è½¨è¿¹çš„è€ƒè™‘å­é›†å†…ï¼Œè¡¨ç¤ºçš„å¤§å°éšç€ç¯å¢ƒä¸­æ›´å¤šéšœç¢ç‰©å’Œ/æˆ–æ•Œäººçš„å‡ºç°è€Œå•è°ƒå¢åŠ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™äº›è¡¨ç¤ºå½¢æˆäº†æ ‘çŠ¶ç»“æ„ï¼Œä»ç¼–ç ç­–ç•¥çŠ¶æ€è·å¾—çš„å¤§å°åœ¨æ–¹å‘ä¸Šå¤§å¤šä¸ä»·å€¼å‡½æ•°çš„é™€èºç›˜æ³•çº¿å¯¹é½ã€‚
- en: This growth intuitively reflects that as new elements appear the agent recognises
    a larger opportunity for rewards (e.g., obtained for defeating its new enemies),
    yet, requiring also a finer level of control as distances to the other policy
    gyroplanes will also grow exponentially, reducing entropy. Instead, following
    random behaviour deviations, the representationsâ€™ magnitudes tend to grow in directions
    that appear almost orthogonal to the value gyroplaneâ€™s normal. Hence, this growth
    still reflects the higher precision required for optimal decision-making but also
    the agentâ€™s higher uncertainty to obtain future rewards in states reached from
    suboptimal behaviour.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å¢é•¿ç›´è§‚åœ°åæ˜ äº†éšç€æ–°å…ƒç´ çš„å‡ºç°ï¼Œä»£ç†è¯†åˆ«åˆ°æ›´å¤§çš„å¥–åŠ±æœºä¼šï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡å‡»è´¥æ–°æ•Œäººè·å¾—çš„å¥–åŠ±ï¼‰ï¼Œç„¶è€Œï¼Œä¹Ÿéœ€è¦æ›´ç²¾ç»†çš„æ§åˆ¶ï¼Œå› ä¸ºä¸å…¶ä»–ç­–ç•¥é™€èºç›˜çš„è·ç¦»ä¹Ÿä¼šæŒ‡æ•°å¢é•¿ï¼Œä»è€Œå‡å°‘ç†µã€‚ç›¸åï¼Œè·Ÿéšéšæœºè¡Œä¸ºçš„åå·®ï¼Œè¡¨ç¤ºçš„å¤§å°è¶‹å‘äºåœ¨çœ‹èµ·æ¥å‡ ä¹ä¸ä»·å€¼é™€èºç›˜æ³•çº¿æ­£äº¤çš„æ–¹å‘ä¸Šå¢é•¿ã€‚å› æ­¤ï¼Œè¿™ç§å¢é•¿ä»ç„¶åæ˜ äº†ä¼˜åŒ–å†³ç­–æ‰€éœ€çš„æ›´é«˜ç²¾åº¦ï¼ŒåŒæ—¶ä¹Ÿåæ˜ äº†ä»£ç†åœ¨ä»æ¬¡ä¼˜è¡Œä¸ºä¸­è¾¾åˆ°çš„çŠ¶æ€ä¸­è·å–æœªæ¥å¥–åŠ±çš„ä¸ç¡®å®šæ€§ã€‚
- en: '![](../Images/8546e540b74d427d92c84f61a7671157.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8546e540b74d427d92c84f61a7671157.png)'
- en: Two-dimensional hyperbolic embeddings in Procgen as we progress through a trajectory
    obtained encoding states following either on-policy behaviour (green) or random
    behaviour (red).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Procgenä¸­ï¼Œéšç€æˆ‘ä»¬åœ¨è½¨è¿¹ä¸Šå‰è¿›çš„äºŒç»´åŒæ›²åµŒå…¥ï¼Œç¼–ç çš„çŠ¶æ€éµå¾ªçš„æ˜¯ç­–ç•¥è¡Œä¸ºï¼ˆç»¿è‰²ï¼‰æˆ–éšæœºè¡Œä¸ºï¼ˆçº¢è‰²ï¼‰ã€‚
- en: Conclusions
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Our experiments provide strong evidence for the advantages and generality of
    using hyperbolic geometry in deep RL, yielding near-universal improvements across
    benchmarks and classes of RL algorithms. Our findings show that geometry can greatly
    affect the prior induced by learning with deep models, perhaps, suggesting we
    should re-evaluate its role and relevance to tackle many additional challenges
    in machine learning. For instance, using hyperbolic space could have implications
    also for unsupervised and offline RL, providing a more appropriate prior to deal
    with the under-specified objectives and limited data characterising these problem
    settings.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å®éªŒæä¾›äº†ä½¿ç”¨åŒæ›²å‡ ä½•åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¼˜åŠ¿å’Œæ™®éæ€§çš„æœ‰åŠ›è¯æ®ï¼Œå‡ ä¹åœ¨æ‰€æœ‰åŸºå‡†å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ç±»åˆ«ä¸­éƒ½å–å¾—äº†è¿‘ä¹æ™®éçš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œå‡ ä½•ç»“æ„å¯ä»¥å¤§å¤§å½±å“æ·±åº¦æ¨¡å‹å­¦ä¹ æ‰€è¯±å¯¼çš„å…ˆéªŒï¼Œæˆ–è®¸ï¼Œæç¤ºæˆ‘ä»¬åº”é‡æ–°è¯„ä¼°å®ƒåœ¨åº”å¯¹è®¸å¤šé¢å¤–çš„æœºå™¨å­¦ä¹ æŒ‘æˆ˜ä¸­çš„ä½œç”¨å’Œç›¸å…³æ€§ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨åŒæ›²ç©ºé—´ä¹Ÿå¯èƒ½å¯¹æ— ç›‘ç£å’Œç¦»çº¿å¼ºåŒ–å­¦ä¹ äº§ç”Ÿå½±å“ï¼Œä¸ºå¤„ç†è¿™äº›é—®é¢˜è®¾ç½®ä¸­ç›®æ ‡ä¸æ˜ç¡®å’Œæ•°æ®æœ‰é™æä¾›æ›´åˆé€‚çš„å…ˆéªŒã€‚
- en: '[1] R. S. Sutton and A. G. Barto, *Reinforcement learning: An introduction*
    (2018) MIT Press, provide a comprehensive introduction to the field of RL. See
    also other great [online resources](https://rail.eecs.berkeley.edu/deeprlcourse/).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] R. S. Sutton å’Œ A. G. Barto, *å¼ºåŒ–å­¦ä¹ ï¼šå¯¼è®º*ï¼ˆ2018å¹´ï¼‰MITå‡ºç‰ˆç¤¾ï¼Œæä¾›äº†å¯¹å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„å…¨é¢ä»‹ç»ã€‚å¦è§å…¶ä»–ä¼˜ç§€çš„
    [åœ¨çº¿èµ„æº](https://rail.eecs.berkeley.edu/deeprlcourse/)ã€‚'
- en: '[2] J. Schulman, [Proximal policy optimization algorithms](https://arxiv.org/pdf/1707.06347.pdf)
    (2017)arXiv:1707.06347.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] J. Schulmanï¼Œ[é‚»è¿‘ç­–ç•¥ä¼˜åŒ–ç®—æ³•](https://arxiv.org/pdf/1707.06347.pdf)ï¼ˆ2017å¹´ï¼‰arXiv:1707.06347ã€‚'
- en: '[3] PPO improves stability by restricting the policy update from making changes
    >*Ïµ* to its current probabilities and employs an auxiliary entropy bonus.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] PPOé€šè¿‡é™åˆ¶ç­–ç•¥æ›´æ–°å¯¹å½“å‰æ¦‚ç‡çš„å˜åŒ–> *Ïµ*ï¼Œå¹¶ä½¿ç”¨è¾…åŠ©ç†µå¥–åŠ±æ¥æé«˜ç¨³å®šæ€§ã€‚'
- en: '[4] R. E. Bellman, *Dynamic programming* (2010) Princeton University Press.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] R. E. Bellman, *åŠ¨æ€è§„åˆ’*ï¼ˆ2010å¹´ï¼‰æ™®æ—æ–¯é¡¿å¤§å­¦å‡ºç‰ˆç¤¾ã€‚'
- en: '[5] M. Hessel et al., [Rainbow: Combining improvements in deep reinforcement
    learning](https://ojs.aaai.org/index.php/AAAI/article/view/11796/11655) (2018)
    *AAAI*.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] M. Hessel ç­‰ï¼Œ[Rainbow: ç»“åˆæ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„æ”¹è¿›](https://ojs.aaai.org/index.php/AAAI/article/view/11796/11655)ï¼ˆ2018å¹´ï¼‰
    *AAAI*ã€‚'
- en: '[6] V. Mnih et al., [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)
    (2015) *Nature* 518 (7540):529â€“533.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] V. Mnih ç­‰ï¼Œ[é€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ å®ç°äººç±»æ°´å¹³çš„æ§åˆ¶](https://www.nature.com/articles/nature14236)ï¼ˆ2015å¹´ï¼‰
    *Nature* 518 (7540):529â€“533ã€‚'
- en: '[7] E. Cetin et al., [Hyperbolic deep reinforcement learning](https://openreview.net/pdf?id=TfBHFLgv77)
    (2023) *ICLR*. See also the accompanying [code](https://sites.google.com/view/hyperbolic-rl).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] E. Cetin ç­‰äººï¼Œ[åŒæ›²æ·±åº¦å¼ºåŒ–å­¦ä¹ ](https://openreview.net/pdf?id=TfBHFLgv77)ï¼ˆ2023ï¼‰*ICLR*ã€‚å¦è§é™„å¸¦çš„[ä»£ç ](https://sites.google.com/view/hyperbolic-rl)ã€‚'
- en: '[8] For an overview of hyperbolic spaces and their early applications in machine
    learning, we refer to [a great introductory blog post](https://bjlkeng.github.io/posts/hyperbolic-geometry-and-poincare-embeddings/)
    by Brian Keng. For a more formal introduction, see e.g. J. W. Cannon et al., Hyperbolic
    geometry (1997) in *Flavors of Geometry* 31:59â€“115.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] å…³äºåŒæ›²ç©ºé—´åŠå…¶åœ¨æœºå™¨å­¦ä¹ ä¸­æ—©æœŸåº”ç”¨çš„æ¦‚è¿°ï¼Œè¯·å‚é˜…å¸ƒè±æ©Â·è‚¯æ ¼çš„[ä¸€ç¯‡å¾ˆæ£’çš„å…¥é—¨åšå®¢æ–‡ç« ](https://bjlkeng.github.io/posts/hyperbolic-geometry-and-poincare-embeddings/)ã€‚æœ‰å…³æ›´æ­£å¼çš„ä»‹ç»ï¼Œè¯·å‚è§
    J. W. Cannon ç­‰äººè‘—çš„ã€ŠåŒæ›²å‡ ä½•å­¦ã€‹ï¼ˆ1997ï¼‰ï¼Œè½½äºã€Šå‡ ä½•å­¦çš„é£å‘³ã€‹ç¬¬ 31 å·ï¼Œç¬¬ 59-115 é¡µã€‚'
- en: '[9] Most know this from school geometry: the area of a circle (i.e., volume
    of a 2-dimensional ball) is *Ï€r*Â². The general formula for the volume of an *n*-dimensional
    Euclidean ball of radius *r* is *Ï€â¿*áŸÂ²*râ¿ /* Î“*(n*/2 +1*)*. Notice that while
    it is polynomial in *r*, it is *exponential in dimension n*. For this reason,
    in order to represent tree-like structures (that have exponential volume growth)
    in a Euclidean space, one has to increase the dimension.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] å¤§å¤šæ•°äººä»å­¦æ ¡å‡ ä½•å­¦ä¸­äº†è§£è¿™ä¸ªçŸ¥è¯†ï¼šåœ†çš„é¢ç§¯ï¼ˆå³äºŒç»´çƒä½“çš„ä½“ç§¯ï¼‰æ˜¯*Ï€r*Â²ã€‚*n*ç»´æ¬§å‡ é‡Œå¾—çƒä½“åŠå¾„ä¸º*r*çš„ä½“ç§¯çš„ä¸€èˆ¬å…¬å¼æ˜¯ *Ï€â¿*áŸÂ²*râ¿
    /* Î“*(n*/2 +1*)*ã€‚è¯·æ³¨æ„ï¼Œå°½ç®¡å®ƒåœ¨*r*ä¸­æ˜¯å¤šé¡¹å¼çš„ï¼Œä½†åœ¨ç»´åº¦*n*ä¸­æ˜¯*æŒ‡æ•°å‹çš„*ã€‚å› æ­¤ï¼Œä¸ºäº†åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­è¡¨ç¤ºï¼ˆå…·æœ‰æŒ‡æ•°ä½“ç§¯å¢é•¿çš„ï¼‰æ ‘çŠ¶ç»“æ„ï¼Œå¿…é¡»å¢åŠ ç»´åº¦ã€‚'
- en: '[10] Hyperbolic geometry was the first successful construction of non-Euclidean
    geometry, in which the classical Parallels Postulate does not hold. Early unsuccessful
    attempts go back to Omar Khayyam and Giovanni Saccheri. The priority over the
    first successful construction is disputed between Carl Friedrich Gauss, JÃ¡nos
    Bolyai, and Nikolai Lobachevsky (the first to publish his results). Eugenio Beltrami
    (and later Felix Klein) showed the self-consistency of hyperbolic geometry and
    proposed a projective model bearing their names (Beltrami-Klein).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] åŒæ›²å‡ ä½•æ˜¯éæ¬§å‡ é‡Œå¾—å‡ ä½•é¦–æ¬¡æˆåŠŸçš„æ„é€ ï¼Œå…¶ä¸­ç»å…¸çš„å¹³è¡Œå…¬è®¾ä¸æˆç«‹ã€‚æ—©æœŸçš„å¤±è´¥å°è¯•å¯è¿½æº¯åˆ°å¥¥é©¬å°”Â·æµ·äºšå§†å’Œä¹”å‡¡å°¼Â·è¨å…‹é›·é‡Œã€‚é¦–æ¬¡æˆåŠŸæ„é€ çš„ä¼˜å…ˆæƒåœ¨å¡å°”Â·å¼—é‡Œå¾·é‡Œå¸ŒÂ·é«˜æ–¯ã€äºšè¯ºä»€Â·åšä¼Šäºšä¼Šå’Œå°¼å¤æ‹‰Â·ç½—å·´åˆ‡å¤«æ–¯åŸºï¼ˆç¬¬ä¸€ä¸ªå…¬å¸ƒå…¶ç»“æœçš„äººï¼‰ä¹‹é—´å­˜åœ¨äº‰è®®ã€‚å°¤é‡‘å°¼å¥¥Â·è´å°”ç‰¹æ‹‰ç±³ï¼ˆä»¥åŠåæ¥è´¹åˆ©å…‹æ–¯Â·å…‹è±å› ï¼‰å±•ç¤ºäº†åŒæ›²å‡ ä½•çš„è‡ªæ´½æ€§ï¼Œå¹¶æå‡ºäº†ä»¥ä»–ä»¬åå­—å‘½åçš„æŠ•å½±æ¨¡å‹ï¼ˆè´å°”ç‰¹æ‹‰ç±³-å…‹è±å› æ¨¡å‹ï¼‰ã€‚'
- en: '[11] M. Gromov, *Hyperbolic groups* (1987) Springer.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] M. Gromovï¼Œ*åŒæ›²ç¾¤*ï¼ˆ1987ï¼‰ï¼Œæ–½æ™®æ—æ ¼å‡ºç‰ˆç¤¾ã€‚'
- en: '[12] This makes geodesics between distinct points pass through some midpoint
    with a lower radius, analogously to how tree geodesics between nodes must go through
    their closest shared parent.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] è¿™ä½¿å¾—ä¸åŒç‚¹ä¹‹é—´çš„æµ‹åœ°çº¿å¿…é¡»ç»è¿‡æŸä¸ªåŠå¾„è¾ƒå°çš„ä¸­ç‚¹ï¼Œç±»ä¼¼äºæ ‘çŠ¶æµ‹åœ°çº¿å¿…é¡»ç»è¿‡å®ƒä»¬æœ€è¿‘çš„å…±äº«çˆ¶èŠ‚ç‚¹ã€‚'
- en: '[13] O. Ganea, G. BÃ©cigneul, and T. Hofmann, [Hyperbolic neural networks](https://proceedings.neurips.cc/paper_files/paper/2018/file/dbab2adc8f9d078009ee3fa810bea142-Paper.pdf)
    (2018) *NeurIPS*.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] O. Ganeaã€G. BÃ©cigneul å’Œ T. Hofmannï¼Œ[åŒæ›²ç¥ç»ç½‘ç»œ](https://proceedings.neurips.cc/paper_files/paper/2018/file/dbab2adc8f9d078009ee3fa810bea142-Paper.pdf)ï¼ˆ2018ï¼‰*NeurIPS*ã€‚'
- en: '[14] The exponential map is given in closed form as expâ‚€(**v**) = **v** tanh(**v**)
    */ ||***v***||.*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] æŒ‡æ•°æ˜ å°„çš„é—­å¼è¡¨è¾¾ä¸º expâ‚€(**v**) = **v** tanh(**v**) */ ||***v***||*ã€‚'
- en: '[15] A. A. Ungar, [*Analytic hyperbolic geometry and Albert Einsteinâ€™s special
    theory of relativity*](https://www.worldscientific.com/worldscibooks/10.1142/6625#t=aboutBook)(2008),
    World Scientific.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] A. A. Ungarï¼Œ[*è§£æåŒæ›²å‡ ä½•å­¦ä¸é˜¿å°”ä¼¯ç‰¹Â·çˆ±å› æ–¯å¦çš„ç›¸å¯¹è®º*](https://www.worldscientific.com/worldscibooks/10.1142/6625#t=aboutBook)ï¼ˆ2008ï¼‰ï¼Œä¸–ç•Œç§‘å­¦å‡ºç‰ˆç¤¾ã€‚'
- en: '[16] In hyperbolic space, the MÃ¶bius addition of two vectors is given by'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] åœ¨åŒæ›²ç©ºé—´ä¸­ï¼Œä¸¤å‘é‡çš„è«æ¯”ä¹Œæ–¯åŠ æ³•å®šä¹‰ä¸º'
- en: '**x**âŠ•**y** *=* ((1 + 2**x***<***x**,**y***> + ||***y***||*Â²)**x** *+* (1 +
    *||***x***||*Â²)**y**) */* (1 + 2*<***x**,**y***> + ||***x***||*Â² *||***y***||*Â²),
    see equation (4) in our paper [7].'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**x**âŠ•**y** *=* ((1 + 2**x***<***x**,**y***> + ||***y***||*Â²)**x** *+* (1 +
    *||***x***||*Â²)**y**) */* (1 + 2*<***x**,**y***> + ||***x***||*Â² *||***y***||*Â²)ï¼Œè¯¦è§æˆ‘ä»¬è®ºæ–‡ä¸­çš„å…¬å¼ï¼ˆ4ï¼‰[7]ã€‚'
- en: '[17] G. Lebanon and J. Lafferty, [Hyperplane margin classifiers on the multinomial
    manifold](https://icml.cc/Conferences/2004/proceedings/papers/13.pdf) (2004) *ICML*.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] G. Lebanon å’Œ J. Laffertyï¼Œ[å¤šé¡¹å¼æµå½¢ä¸Šçš„è¶…å¹³é¢é—´éš”åˆ†ç±»å™¨](https://icml.cc/Conferences/2004/proceedings/papers/13.pdf)ï¼ˆ2004ï¼‰*ICML*ã€‚'
- en: '[18] The distance is given in closed form as *d*(**x***,H*)*=*sinhá¨Â¹(2*|<****x***âŠ•â€“**p***,***w***>|*
    / (1 â€” *||***x**âŠ•â€“**p***||*Â²*||***w***||*)), see equation (6) in our paper [7]*.*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] è·ç¦»çš„é—­å¼è¡¨è¾¾ä¸º *d*(**x***,H*)*=*sinhá¨Â¹(2*|<****x***âŠ•â€“**p***,***w***>|* / (1
    â€” *||***x**âŠ•â€“**p***||*Â²*||***w***||*))ï¼Œè¯¦è§æˆ‘ä»¬è®ºæ–‡ä¸­çš„å…¬å¼ï¼ˆ6ï¼‰[7]ã€‚'
- en: '[19] V. Khrulkov et al., [Hyperbolic image embeddings](https://openaccess.thecvf.com/content_CVPR_2020/papers/Khrulkov_Hyperbolic_Image_Embeddings_CVPR_2020_paper.pdf)
    (2020) *CVPR*.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] V. Khrulkov ç­‰ï¼Œ [åŒæ›²å›¾åƒåµŒå…¥](https://openaccess.thecvf.com/content_CVPR_2020/papers/Khrulkov_Hyperbolic_Image_Embeddings_CVPR_2020_paper.pdf)
    (2020) *CVPR*ã€‚'
- en: '[20] E. Mathieu et al., [Continuous hierarchical representations with PoincarÃ©
    variational auto-encoders](https://proceedings.neurips.cc/paper_files/paper/2019/file/0ec04cb3912c4f08874dd03716f80df1-Paper.pdf)
    (2019) *NeurIPS*.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] E. Mathieu ç­‰ï¼Œ [å…·æœ‰åºåŠ è±å˜åˆ†è‡ªç¼–ç å™¨çš„è¿ç»­å±‚æ¬¡è¡¨ç¤º](https://proceedings.neurips.cc/paper_files/paper/2019/file/0ec04cb3912c4f08874dd03716f80df1-Paper.pdf)
    (2019) *NeurIPS*ã€‚'
- en: '[21] R. Raileanu and R. Fergus, [Decoupling value and policy for generalization
    in reinforcement learning](https://arxiv.org/pdf/2102.10330.pdf) (2021), *ICML*.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] R. Raileanu å’Œ R. Fergusï¼Œ [åœ¨å¼ºåŒ–å­¦ä¹ ä¸­è§£è€¦ä»·å€¼å’Œç­–ç•¥ä»¥å®ç°æ³›åŒ–](https://arxiv.org/pdf/2102.10330.pdf)
    (2021)ï¼Œ *ICML*ã€‚'
- en: '[22] Procgen, introduced by K. Cobbe et al., [Leveraging procedural generation
    to benchmark reinforcement learning](http://proceedings.mlr.press/v119/cobbe20a/cobbe20a.pdf)
    (2020) *ICML*, consists of 16 visual environments with procedurally-generated
    random levels. While the different levels share a high-level objective (e.g.,
    reach the door, defeat all enemies, etc.), they might have considerable differences
    in their layout and appearance. Moreover, while training in this benchmark, the
    agents only have access to the first 200 levels of each environment for experience
    collection, but their performance is tested on the full distribution of levels.
    Hence, this benchmark allows assessing RL agents with a specific focus on their
    generalisation to unseen levels.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Procgenï¼Œç”± K. Cobbe ç­‰ä»‹ç»ï¼Œ [åˆ©ç”¨ç¨‹åºç”Ÿæˆæ¥åŸºå‡†åŒ–å¼ºåŒ–å­¦ä¹ ](http://proceedings.mlr.press/v119/cobbe20a/cobbe20a.pdf)
    (2020) *ICML*ï¼ŒåŒ…æ‹¬16ä¸ªå…·æœ‰ç¨‹åºç”Ÿæˆéšæœºå…³å¡çš„è§†è§‰ç¯å¢ƒã€‚è™½ç„¶ä¸åŒçš„å…³å¡å…±äº«ä¸€ä¸ªé«˜çº§ç›®æ ‡ï¼ˆä¾‹å¦‚ï¼Œè¾¾åˆ°é—¨å£ï¼Œå‡»è´¥æ‰€æœ‰æ•Œäººç­‰ï¼‰ï¼Œä½†å®ƒä»¬åœ¨å¸ƒå±€å’Œå¤–è§‚ä¸Šå¯èƒ½æœ‰æ˜¾è‘—å·®å¼‚ã€‚æ­¤å¤–ï¼Œåœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä»£ç†ä»…èƒ½è®¿é—®æ¯ä¸ªç¯å¢ƒçš„å‰200ä¸ªå…³å¡è¿›è¡Œç»éªŒæ”¶é›†ï¼Œä½†å…¶æ€§èƒ½åœ¨æ‰€æœ‰å…³å¡çš„åˆ†å¸ƒä¸Šè¿›è¡Œæµ‹è¯•ã€‚å› æ­¤ï¼Œè¯¥åŸºå‡†æµ‹è¯•å…è®¸è¯„ä¼°å¼ºåŒ–å­¦ä¹ ä»£ç†ï¼Œç‰¹åˆ«æ˜¯å…¶å¯¹æœªè§å…³å¡çš„æ³›åŒ–èƒ½åŠ›ã€‚'
- en: '[23] M. Borassi, A. Chessa, and G. Caldarelli, [Hyperbolicity measures democracy
    in real-world networks](https://arxiv.org/pdf/1503.03061.pdf) (2015) *Physical
    Review E* 92.3: 032812.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] M. Borassiã€A. Chessa å’Œ G. Caldarelliï¼Œ [åŒæ›²æ€§åº¦é‡ç°å®ä¸–ç•Œç½‘ç»œä¸­çš„æ°‘ä¸»](https://arxiv.org/pdf/1503.03061.pdf)
    (2015) *Physical Review E* 92.3: 032812ã€‚'
- en: '[24] L. Espeholt et al., [Impala: Scalable distributed deep-RL with importance
    weighted actor-learner architectures](https://proceedings.mlr.press/v80/espeholt18a/espeholt18a.pdf)
    (2018) *ICML.*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] L. Espeholt ç­‰ï¼Œ [Impala: å¯æ‰©å±•çš„åˆ†å¸ƒå¼æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸é‡è¦æ€§åŠ æƒæ¼”å‘˜-å­¦ä¹ è€…æ¶æ„](https://proceedings.mlr.press/v80/espeholt18a/espeholt18a.pdf)
    (2018) *ICML*ã€‚'
- en: '[25] M. Nickel and D. Kiela, [PoincarÃ© embeddings for learning hierarchical
    representations](https://papers.nips.cc/paper_files/paper/2017/file/59dfa2df42d9e3d41f5b02bfc32229dd-Paper.pdf)
    (2017) *NeurIPS*.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] M. Nickel å’Œ D. Kielaï¼Œ [ç”¨äºå­¦ä¹ å±‚æ¬¡è¡¨ç¤ºçš„åºåŠ è±åµŒå…¥](https://papers.nips.cc/paper_files/paper/2017/file/59dfa2df42d9e3d41f5b02bfc32229dd-Paper.pdf)
    (2017) *NeurIPS*ã€‚'
- en: '[26] Y. Guo et al., [Clipped hyperbolic classifiers are super-hyperbolic classifiers](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Clipped_Hyperbolic_Classifiers_Are_Super-Hyperbolic_Classifiers_CVPR_2022_paper.pdf)
    (2022), *CVPR*.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Y. Guo ç­‰ï¼Œ [å‰ªè£çš„åŒæ›²åˆ†ç±»å™¨æ˜¯è¶…çº§åŒæ›²åˆ†ç±»å™¨](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Clipped_Hyperbolic_Classifiers_Are_Super-Hyperbolic_Classifiers_CVPR_2022_paper.pdf)
    (2022)ï¼Œ *CVPR*ã€‚'
- en: '[27] I. Goodfellow et al., [Generative adversarial nets](https://arxiv.org/pdf/1406.2661.pdf)
    (2014), *NIPS*.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] I. Goodfellow ç­‰ï¼Œ [ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ](https://arxiv.org/pdf/1406.2661.pdf) (2014)ï¼Œ
    *NIPS*ã€‚'
- en: '[28] M. Arjovsky and L. Bottou, [Towards principled methods for training Generative
    Adversarial Networks](https://openreview.net/pdf?id=Hk4_qw5xe) (2017) *ICLR*.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] M. Arjovsky å’Œ L. Bottouï¼Œ [æœç€æœ‰åŸåˆ™çš„æ–¹æ³•è®­ç»ƒç”Ÿæˆå¯¹æŠ—ç½‘ç»œ](https://openreview.net/pdf?id=Hk4_qw5xe)
    (2017) *ICLR*ã€‚'
- en: '[29] T. Miyato et al., [Spectral normalization for generative adversarial networks](https://arxiv.org/pdf/1802.05957.pdf)
    (2018) *arXiv*:1802.05957.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] T. Miyato ç­‰ï¼Œ [ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„è°±å½’ä¸€åŒ–](https://arxiv.org/pdf/1802.05957.pdf) (2018)
    *arXiv*:1802.05957ã€‚'
- en: '[30] Z. Lin, V. Sekar, and G. Fanti, [Why spectral normalization stabilizes
    GANs: Analysis and improvements](https://arxiv.org/pdf/2009.02773.pdf) (2021),
    *NeurIPS*.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Z. Linã€V. Sekar å’Œ G. Fantiï¼Œ [ä¸ºä½•è°±å½’ä¸€åŒ–ç¨³å®šç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼šåˆ†æä¸æ”¹è¿›](https://arxiv.org/pdf/2009.02773.pdf)
    (2021)ï¼Œ *NeurIPS*ã€‚'
- en: '[31] Atari 100K, introduced by Kaiser, Lukasz, et al., [Model-based reinforcement
    learning for Atari](https://openreview.net/pdf?id=S1xCPJHtDB) (2019) *arXiv*:1903.00374,
    consists of 26 different visual environments consisting of iconic Atari games
    from M. G. Bellemare et al., [The arcade learning environment: An evaluation platform
    for general agents](https://arxiv.org/pdf/1207.4708.pdf) (2013) *Journal of Artificial
    Intelligence Research* 47: 253â€“279\. However, agents are allowed access only to
    100K total environment steps of data for experience collection, corresponding
    roughly to 2 hours of playtime. The environments are modified with the specifications
    from M. C. Machado et al., [Revisiting the arcade learning environment: Evaluation
    protocols and open problems for general agents](https://arxiv.org/pdf/1709.06009.pdf)
    (2018) *Journal of Artificial Intelligence Research* 61:523â€“562, introducing considerable
    randomness through *sticky actions* (i.e., random repeats of each executed action).
    Thus, due to the severe data constraints and added randomness, this benchmark
    places a specific focus on assessing generalization to unseen states.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Atari 100Kï¼Œç”± Kaiserã€Lukasz ç­‰äººä»‹ç»ï¼Œ[åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç”¨äº Atari](https://openreview.net/pdf?id=S1xCPJHtDB)
    (2019) *arXiv*:1903.00374ï¼ŒåŒ…å«äº†26ç§ä¸åŒçš„è§†è§‰ç¯å¢ƒï¼Œè¿™äº›ç¯å¢ƒç”± M. G. Bellemare ç­‰äººæä¾›ï¼ŒåŒ…å«äº† M. G. Bellemare
    ç­‰äººæå‡ºçš„ç»å…¸ Atari æ¸¸æˆï¼Œ[è¡—æœºå­¦ä¹ ç¯å¢ƒï¼šä¸€ç§é€šç”¨ä»£ç†çš„è¯„ä¼°å¹³å°](https://arxiv.org/pdf/1207.4708.pdf) (2013)
    *äººå·¥æ™ºèƒ½ç ”ç©¶æœŸåˆŠ* 47: 253â€“279ã€‚ç„¶è€Œï¼Œä»£ç†åªèƒ½è®¿é—®100Kæ€»ç¯å¢ƒæ­¥æ•°çš„æ•°æ®æ¥è¿›è¡Œç»éªŒæ”¶é›†ï¼Œè¿™å¤§è‡´ç›¸å½“äº2å°æ—¶çš„æ¸¸æˆæ—¶é—´ã€‚ç¯å¢ƒç»è¿‡ M. C.
    Machado ç­‰äººæå‡ºçš„è§„æ ¼è¿›è¡Œä¿®æ”¹ï¼Œ[é‡æ–°å®¡è§†è¡—æœºå­¦ä¹ ç¯å¢ƒï¼šé€šç”¨ä»£ç†çš„è¯„ä¼°åè®®å’Œå¼€æ”¾é—®é¢˜](https://arxiv.org/pdf/1709.06009.pdf)
    (2018) *äººå·¥æ™ºèƒ½ç ”ç©¶æœŸåˆŠ* 61:523â€“562ï¼Œä»‹ç»äº†é€šè¿‡ *ç²˜æ€§åŠ¨ä½œ*ï¼ˆå³æ¯æ¬¡æ‰§è¡ŒåŠ¨ä½œçš„éšæœºé‡å¤ï¼‰å¼•å…¥äº†ç›¸å½“å¤§çš„éšæœºæ€§ã€‚å› æ­¤ï¼Œç”±äºä¸¥é‡çš„æ•°æ®é™åˆ¶å’Œé¢å¤–çš„éšæœºæ€§ï¼Œè¿™ä¸€åŸºå‡†ç‰¹åˆ«å…³æ³¨è¯„ä¼°å¯¹æœªè§çŠ¶æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚'
- en: '[32] D. Yarats, I. Kostrikov, and R. Fergus, [Image augmentation is all you
    need: Regularizing deep reinforcement learning from pixels](https://openreview.net/pdf?id=GY6-6sTvGaf)
    (2021), *ICLR*.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] D. Yaratsã€I. Kostrikov å’Œ R. Fergusï¼Œ[å›¾åƒå¢å¼ºå°±æ˜¯ä½ æ‰€éœ€è¦çš„ï¼šä»åƒç´ ä¸­æ­£åˆ™åŒ–æ·±åº¦å¼ºåŒ–å­¦ä¹ ](https://openreview.net/pdf?id=GY6-6sTvGaf)
    (2021)ï¼Œ*ICLR*ã€‚'
- en: '*We are grateful to* [*David Ha*](https://twitter.com/hardmaru?lang=en) *(a.k.a.
    hardmaru) for generating the title image, the first AI-generated illustration
    on this blog! See additional information on the* [*project webpage*](https://medium.com/r?url=https%3A%2F%2Fsites.google.com%2Fview%2Fhyperbolic-rl)*,*
    [*Towards Data Science*](https://towardsdatascience.com/graph-deep-learning/home)
    *Medium posts,* [*subscribe*](https://michael-bronstein.medium.com/subscribe)
    *to Michaelâ€™s posts and* [*YouTube channel*](https://www.youtube.com/c/MichaelBronsteinGDL)*,
    get* [*Medium membership*](https://michael-bronstein.medium.com/membership)*,
    or follow* [*Michael*](https://twitter.com/mmbronstein)*,* [*Edoardo*](https://twitter.com/edo_cet)*,*
    [*Ben*](https://twitter.com/DrBPChamberlain), and [*Jonathan*](https://twitter.com/jjh)
    *on Twitter.*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬æ„Ÿè°¢* [*David Ha*](https://twitter.com/hardmaru?lang=en) *(å³ hardmaru) ä¸ºæˆ‘ä»¬ç”Ÿæˆäº†æ ‡é¢˜å›¾åƒï¼Œè¿™æ˜¯æœ¬åšå®¢ä¸Šç¬¬ä¸€å¼ 
    AI ç”Ÿæˆçš„æ’å›¾ï¼æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…* [*é¡¹ç›®ç½‘é¡µ*](https://medium.com/r?url=https%3A%2F%2Fsites.google.com%2Fview%2Fhyperbolic-rl)*ï¼Œ*
    [*Towards Data Science*](https://towardsdatascience.com/graph-deep-learning/home)
    *Medium åšå®¢æ–‡ç« ï¼Œ* [*è®¢é˜…*](https://michael-bronstein.medium.com/subscribe) *Michael
    çš„æ–‡ç« å’Œ* [*YouTube é¢‘é“*](https://www.youtube.com/c/MichaelBronsteinGDL)*ï¼Œè·å–* [*Medium
    ä¼šå‘˜èµ„æ ¼*](https://michael-bronstein.medium.com/membership)*ï¼Œæˆ–å…³æ³¨* [*Michael*](https://twitter.com/mmbronstein)*ã€*
    [*Edoardo*](https://twitter.com/edo_cet)*ã€* [*Ben*](https://twitter.com/DrBPChamberlain)
    å’Œ [*Jonathan*](https://twitter.com/jjh) *åœ¨ Twitter ä¸Šçš„åŠ¨æ€ã€‚*'
