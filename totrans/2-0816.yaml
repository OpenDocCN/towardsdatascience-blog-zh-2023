- en: Ensuring Correct Use of Transformers in Scikit-learn Pipeline
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保在 Scikit-learn 管道中正确使用变换器
- en: 原文：[https://towardsdatascience.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipelines-393566db7bfa](https://towardsdatascience.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipelines-393566db7bfa)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipelines-393566db7bfa](https://towardsdatascience.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipelines-393566db7bfa)
- en: Effective data processing in machine learning projects
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习项目中的有效数据处理
- en: '[](https://qtalen.medium.com/?source=post_page-----393566db7bfa--------------------------------)[![Peng
    Qian](../Images/9ce9aeb381ec6b017c1ee5d4714937e2.png)](https://qtalen.medium.com/?source=post_page-----393566db7bfa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----393566db7bfa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----393566db7bfa--------------------------------)
    [Peng Qian](https://qtalen.medium.com/?source=post_page-----393566db7bfa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://qtalen.medium.com/?source=post_page-----393566db7bfa--------------------------------)[![彭乾](../Images/9ce9aeb381ec6b017c1ee5d4714937e2.png)](https://qtalen.medium.com/?source=post_page-----393566db7bfa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----393566db7bfa--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----393566db7bfa--------------------------------)
    [彭乾](https://qtalen.medium.com/?source=post_page-----393566db7bfa--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----393566db7bfa--------------------------------)
    ·11 min read·Dec 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----393566db7bfa--------------------------------)
    ·11分钟阅读·2023年12月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b6187b45b1a6ac0f9da54deec3eb9b02.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6187b45b1a6ac0f9da54deec3eb9b02.png)'
- en: Ensuring Correct Use of Transformers in Scikit-learn Pipeline. Image by Author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在 Scikit-learn 管道中正确使用变换器。图片由作者提供
- en: This article will explain how to use [Pipeline](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com)
    and [Transformers](https://scikit-learn.org/stable/data_transforms.html?ref=dataleadsfuture.com)
    correctly in Scikit-Learn (sklearn) projects to speed up and reuse our model training
    process.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将解释如何在 Scikit-Learn (sklearn) 项目中正确使用 [Pipeline](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com)
    和 [Transformers](https://scikit-learn.org/stable/data_transforms.html?ref=dataleadsfuture.com)，以加速和重用我们的模型训练过程。
- en: This piece complements and clarifies the official documentation on Pipeline
    examples and some common misunderstandings.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容补充和澄清了有关 Pipeline 示例和一些常见误解的官方文档。
- en: I hope that after reading this, you’ll be able to use the Pipeline, an excellent
    design, to better complete your machine learning tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望阅读完这篇文章后，你能够更好地利用 Pipeline 这个优秀的设计来完成你的机器学习任务。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: There’s a famous dish in Chinese restaurants around the world called “General
    Tso’s Chicken,” and I wonder if you’ve tried it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 世界各地的中餐馆都有一道著名的菜肴叫做“左宗棠鸡”，我想知道你是否尝试过。
- en: '![](../Images/fe3cd203f33891ed74abe3884196dd74.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe3cd203f33891ed74abe3884196dd74.png)'
- en: 'General Tso’s Chicken. A model for standardizing the cooking process. Photo
    Credit: Created by Author, [Canva](https://www.canva.com/?ref=dataleadsfuture.com).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 左宗棠鸡。标准化烹饪过程的典范。照片来源：作者创作，[Canva](https://www.canva.com/?ref=dataleadsfuture.com)。
- en: 'One characteristic of “General Tso’s Chicken” is that each piece of chicken
    is processed by the chef to be the same size. This ensures that:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: “左宗棠鸡”的一个特点是每块鸡肉都被厨师处理成相同的大小。这确保了：
- en: All pieces are marinated for the same amount of time.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有鸡块都腌制相同的时间。
- en: During cooking, each piece of chicken reaches the same level of doneness.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在烹饪过程中，每块鸡肉都达到相同的熟度。
- en: When using chopsticks, the uniform size makes it easier to pick up the pieces.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用筷子时，均匀的大小使得捡起食材更加容易。
- en: This preprocessing includes washing, cutting, and marinating the ingredients.
    If the chicken pieces are cut larger than usual, the flavor can change significantly
    even if stir-fried for the same amount of time.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这种预处理包括清洗、切割和腌制食材。如果鸡块切割得比平时大，即使炒的时间相同，风味也会发生显著变化。
- en: So, when preparing to open a restaurant, we must consider standardizing these
    processes and recipes to ensure that each plate of “General Tso’s Chicken” has
    a consistent taste and texture. This is how restaurants thrive.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在准备开设餐厅时，我们必须考虑标准化这些过程和配方，以确保每一道“左宗棠鸡”都有一致的口味和质地。这就是餐厅成功的关键。
- en: Back in the world of machine learning, Scikit-Learn also provides such standardized
    processes called Pipeline. They solidify the data preprocessing and model training
    process into a standardized workflow, making machine learning projects easier
    to maintain and reuse.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 回到机器学习的世界，Scikit-Learn 还提供了这样的标准化流程，称为 Pipeline。它们将数据预处理和模型训练过程固化为标准化的工作流程，使机器学习项目更易于维护和重用。
- en: In this article, we’ll explore how to use Transformers correctly within Scikit-Learn’s
    Pipeline, ensuring that our data is as perfectly prepared as the ingredients for
    a fine meal.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨如何在 Scikit-Learn 的 Pipeline 中正确使用变换器，确保我们的数据准备得像精致餐点的配料一样完美。
- en: Why Use Transformers
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用变换器
- en: What are Transformers
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是变换器
- en: 'In Scikit-Learn, Transformers mainly fall into two categories: data scaling
    and feature dimensionality reduction.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scikit-Learn 中，变换器主要分为两类：数据缩放和特征降维。
- en: Take, for example, a set of housing data, which includes features like location,
    area, and number of bedrooms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以一组房屋数据为例，其中包括位置、面积和卧室数量等特征。
- en: If you don’t standardize these features to the same scale, the model might overlook
    the significant impact of location (usually categorical data) due to minor fluctuations
    in the area (usually a larger numerical value).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不将这些特征标准化到相同的尺度，模型可能会因区域（通常是较大的数值）中的微小波动而忽略位置的显著影响（通常是分类数据）。
- en: It’s like overpowering the delicate taste of herbs with too much pepper.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像用过多的胡椒粉掩盖了草药的细腻味道。
- en: Using Transformers correctly
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正确使用变换器
- en: 'Typically, data scaling is done using standardization, formulated as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据缩放是通过标准化来完成的，公式为：
- en: '![](../Images/587d79bf0bfff52befcd5d94ebd6f51f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/587d79bf0bfff52befcd5d94ebd6f51f.png)'
- en: Formula for train_data’s standardization. Image by Author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据标准化的公式。图片作者提供
- en: Where `train_mean` and `train_std` are variables extracted from the train data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `train_mean` 和 `train_std` 是从训练数据中提取的变量。
- en: In Scikit-Learn, train data and test data are both obtained from the original
    dataset using the `[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?ref=dataleadsfuture.com#sklearn.model_selection.train_test_split)`
    method.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scikit-Learn 中，训练数据和测试数据都是使用 `[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?ref=dataleadsfuture.com#sklearn.model_selection.train_test_split)`
    方法从原始数据集中获得的。
- en: 'When scaling test data, the same `train_mean` and `train_std` are used:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在缩放测试数据时，使用相同的 `train_mean` 和 `train_std`：
- en: '![](../Images/582a76406e3733a4a176557a94b400df.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/582a76406e3733a4a176557a94b400df.png)'
- en: The same train_mean and train_std variables are used when scaling test data.
    Image by Author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在缩放测试数据时使用相同的 train_mean 和 train_std 变量。图片作者提供
- en: 'Here arises the question: why use train data to generate these variables?'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里出现了一个问题：为什么使用训练数据来生成这些变量？
- en: 'Let’s look at a simple dataset where the train data is:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简单的数据集，其中训练数据为：
- en: '![](../Images/f77dac5255c8c753552222a771d5dced.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f77dac5255c8c753552222a771d5dced.png)'
- en: A simple dataset of train data. Image by Author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的训练数据集。图片作者提供
- en: 'After standardization, the train data becomes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化后，训练数据变为：
- en: '![](../Images/d1378cb8991108dc555a598e308514aa.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1378cb8991108dc555a598e308514aa.png)'
- en: The simple dataset after scaling. Image by Author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放后的简单数据集。图片作者提供
- en: Clearly, after scaling, features greater than 0 have a label of 1, which means
    features greater than 10 before scaling have a label of 1.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在缩放后，大于 0 的特征标记为 1，这意味着在缩放之前，大于 10 的特征标记为 1。
- en: 'Now let’s look at the test data:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看测试数据：
- en: '![](../Images/ab196e3c9c4cf64d55304ec13477861c.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab196e3c9c4cf64d55304ec13477861c.png)'
- en: Test data that has not yet been classified. Image by Author
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尚未分类的测试数据。图片作者提供
- en: 'If we use `test_mean` and `test_std` generated from the test data distribution
    without considering the train data, the results become:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用从测试数据分布中生成的 `test_mean` 和 `test_std`，而不考虑训练数据，则结果变为：
- en: '![](../Images/8d9936e950dbfcfb5431ba67c0cb83d1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d9936e950dbfcfb5431ba67c0cb83d1.png)'
- en: Demonstration of errors in using test data to generate variables. Image by Author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用测试数据生成变量时的错误演示。图片作者提供
- en: 'Obviously, this prediction result does not make sense. But suppose we use `train_mean`
    and `train_std` to process the data and combine it with the model prediction;
    let''s see what happens:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这个预测结果是没有意义的。但是假设我们使用 `train_mean` 和 `train_std` 来处理数据，并结合模型预测；我们来看看会发生什么：
- en: '![](../Images/3f80bc980e14879a4aaadfcc23415fee.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f80bc980e14879a4aaadfcc23415fee.png)'
- en: Using the variables of train data, we obtained the correct results. Image by
    Author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据的变量，我们获得了正确的结果。图片由作者提供
- en: As we can see, only by preprocessing the data with variables generated through
    train data can we ensure that the model’s prediction meets expectations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，只有通过对训练数据生成的变量进行数据预处理，才能确保模型的预测符合预期。
- en: Using Transformers in Scikit-Learn
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中使用变换器
- en: Using Transformers in Scikit-Learn is quite simple.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中使用变换器非常简单。
- en: We can generate a set of simulated data using `make_classification` and then
    split it into train and test with `train_test_split`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`make_classification`生成一组模拟数据，然后用`train_test_split`将其拆分为训练集和测试集。
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s look at the distribution of the data:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据的分布：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/1497aac72727ce4897b0310bc3e20a2f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1497aac72727ce4897b0310bc3e20a2f.png)'
- en: The distribution of the data before scaling. Image by Author
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放前的数据分布。图片由作者提供
- en: Here we’re using `StandardScaler` to scale the features.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`StandardScaler`来缩放特征。
- en: 'First, initialize the `StandardScaler`, then `fit` it with train data:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，初始化`StandardScaler`，然后用训练数据`fit`它：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we can `transform` the train data''s features with the fitted Transformer:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用拟合的变换器`transform`训练数据的特征：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Of course, we could also use `fit_transform` to fit and transform the train
    data in one go:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们也可以使用`fit_transform`一次性拟合并转换训练数据：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then we simply transform the test data without needing to fit it again:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们只需转换测试数据，无需再次拟合：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After transformation, the distribution of the data remains unchanged, except
    for the change in scale:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后，数据的分布保持不变，唯一的变化是规模：
- en: '![](../Images/724721511afe2182f50ebd99d29d6d34.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/724721511afe2182f50ebd99d29d6d34.png)'
- en: The distribution of the data after scaling. Image by Author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放后的数据分布。图片由作者提供
- en: Apart from scaling data with tools like `[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?ref=dataleadsfuture.com#sklearn.preprocessing.StandardScaler)`
    and `[MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html?ref=dataleadsfuture.com#sklearn.preprocessing.MinMaxScaler)`,
    we can also use `PCA`, `SelectKBest`, etc., for dimensionality reduction. For
    the sake of brevity, I won't delve into these here, but you're welcome to consult
    the [official documentation](https://scikit-learn.org/stable/modules/feature_selection.html?ref=dataleadsfuture.com#univariate-feature-selection)
    for more information.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用像`[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?ref=dataleadsfuture.com#sklearn.preprocessing.StandardScaler)`和`[MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html?ref=dataleadsfuture.com#sklearn.preprocessing.MinMaxScaler)`这样的工具进行数据缩放外，我们还可以使用`PCA`、`SelectKBest`等进行维度减少。为了简洁起见，我不在这里深入探讨这些内容，但欢迎查阅[官方文档](https://scikit-learn.org/stable/modules/feature_selection.html?ref=dataleadsfuture.com#univariate-feature-selection)获取更多信息。
- en: Using Transformers in a Pipeline
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在管道中使用变换器
- en: Why use a Pipeline
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么使用管道
- en: As mentioned earlier, in a machine learning task, we often need to use various
    Transformers for data scaling and feature dimensionality reduction before training
    a model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在机器学习任务中，我们通常需要使用各种变换器进行数据缩放和特征维度减少，然后再训练模型。
- en: 'This presents several challenges:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这带来了几个挑战：
- en: '**Code complexity**: For each use of a Transformer, we have to go through initialization,
    `fit_transform`, and `transform` steps. Missing one step during a transformation
    could derail the entire training process.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码复杂性**：每次使用变换器时，我们必须经历初始化、`fit_transform`和`transform`步骤。转换过程中漏掉一个步骤可能会破坏整个训练过程。'
- en: '**Data leakage**: As we discussed, for each Transformer, we fit with train
    data and then transform both train and test data. We must avoid letting the distribution
    of the test data leak into the train data.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据泄露**：如我们所讨论的，对于每个变换器，我们用训练数据拟合，然后转换训练数据和测试数据。我们必须避免测试数据的分布泄漏到训练数据中。'
- en: '**Code reusability**: A machine learning model includes not only the trained
    Estimator for prediction but also the data preprocessing steps. Therefore, a machine
    learning task comprising Transformers and an Estimator should be atomic and indivisible.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码复用性**：一个机器学习模型不仅包括用于预测的训练好的估计器，还包括数据预处理步骤。因此，一个包括变换器和估计器的机器学习任务应该是原子化和不可分割的。'
- en: '**Hyperparameter tuning**: After setting up the steps of machine learning,
    we need to adjust hyperparameters to find the best combination of Transformer
    parameter values.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数调优**：在设置好机器学习步骤后，我们需要调整超参数，以找到 Transformer 参数值的最佳组合。'
- en: Scikit-Learn introduced the `Pipeline` module to solve these issues.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 引入了 `Pipeline` 模块来解决这些问题。
- en: What is a Pipeline
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 Pipeline
- en: A `Pipeline` is a module in Scikit-Learn that implements the chain of responsibility
    design pattern.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pipeline` 是 Scikit-Learn 中实现责任链设计模式的一个模块。'
- en: 'When creating a Pipeline, we use the `steps` parameter to chain together multiple
    Transformers for initialization:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Pipeline 时，我们使用 `steps` 参数将多个 Transformers 链接在一起进行初始化：
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The [official documentation](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline)
    points out that the last Transformer must be an Estimator.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[官方文档](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline)指出，最后一个
    Transformer 必须是一个 Estimator。'
- en: 'If you don’t need to specify each Transformer’s name, you can simplify the
    creation of a Pipeline with `make_pipeline`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不需要指定每个 Transformer 的名称，你可以使用 `make_pipeline` 简化 Pipeline 的创建：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Understanding the Pipeline’s mechanism from the source code
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从源代码理解 Pipeline 的机制
- en: We’ve mentioned the importance of not letting test data variables leak into
    training data when using each Transformer.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到，在使用每个 Transformer 时，不要让测试数据变量泄露到训练数据中的重要性。
- en: This principle is relatively easy to ensure when each data preprocessing step
    is independent.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当每个数据预处理步骤是独立时，这个原则相对容易确保。
- en: But what if we integrate these steps using a Pipeline?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们使用 Pipeline 整合这些步骤会怎么样？
- en: If we look at the [official documentation](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline),
    we find it simply uses the `fit` method on the entire dataset without explaining
    how to handle train and test data separately.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看[官方文档](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline)，会发现它只是对整个数据集使用
    `fit` 方法，而没有解释如何分别处理训练数据和测试数据。
- en: With this question in mind, I dived into the Pipeline’s source code to find
    the answer.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这个问题，我深入研究了 Pipeline 的源代码以寻找答案。
- en: Reading the source code revealed that although Pipeline implements `fit`, `fit_transform`,
    and `predict` methods, they work differently from regular Transformers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读源代码表明，虽然 Pipeline 实现了 `fit`、`fit_transform` 和 `predict` 方法，但它们的工作方式与普通的 Transformers
    不同。
- en: 'Take the following Pipeline creation process as an example:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以以下 Pipeline 创建过程为例：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The internal implementation can be represented by the following diagram:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 内部实现可以通过以下图示表示：
- en: '![](../Images/b6187b45b1a6ac0f9da54deec3eb9b02.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6187b45b1a6ac0f9da54deec3eb9b02.png)'
- en: Internal implementation of the fit and predict methods when called. Image by
    Author
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `fit` 和 `predict` 方法时的内部实现。图片由作者提供
- en: As you can see, when we call the `fit` method, Pipeline first separates Transformers
    from the Estimator.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，当我们调用 `fit` 方法时，Pipeline 首先将 Transformers 与 Estimator 分开。
- en: For each Transformer, Pipeline checks if there’s a `fit_transform` method; if
    so, it calls it; otherwise, it calls `fit`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个 Transformer，Pipeline 会检查是否有 `fit_transform` 方法；如果有，它会调用该方法；否则，它会调用 `fit`。
- en: For the Estimator, it calls `fit` directly.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Estimator，它直接调用 `fit`。
- en: For the predict method, Pipeline separates Transformers from the Estimator.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `predict` 方法，Pipeline 将 Transformers 与 Estimator 分开。
- en: Pipeline calls each Transformer’s `transform` method in sequence, followed by
    the Estimator's `predict` method.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Pipeline 按顺序调用每个 Transformer 的 `transform` 方法，然后是 Estimator 的 `predict` 方法。
- en: Therefore, when using a Pipeline, we still need to split train and test data.
    Then we simply call `fit` on the train data and `predict` on the test data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在使用 Pipeline 时，我们仍然需要拆分训练数据和测试数据。然后我们只需对训练数据调用 `fit`，对测试数据调用 `predict`。
- en: 'There’s a special case when combining Pipeline with `GridSearchCV` for hyperparameter
    tuning: you don''t need to manually split train and test data. I''ll explain this
    in more detail in the best practices section.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 Pipeline 与 `GridSearchCV` 进行超参数调优时，有一种特殊情况：你不需要手动拆分训练数据和测试数据。我将在最佳实践部分详细解释这一点。
- en: Best Practices for Using Transformers and Pipeline in Actual Applications
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际应用中使用 Transformers 和 Pipeline 的最佳实践
- en: Now that we’ve discussed the working principles of Transformers and Pipeline,
    it’s time to fulfill the promise made in the title and talk about the best practices
    when combining Transformers with Pipeline in real projects.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了 Transformers 和 Pipeline 的工作原理，是时候履行标题中的承诺，讨论在实际项目中将 Transformers 与
    Pipeline 结合的最佳实践。
- en: Combining Pipeline with GridSearchCV for hyperparameter tuning
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Pipeline 与 GridSearchCV 结合用于超参数调优
- en: In a machine learning project, selecting the right dataset processing and algorithm
    is one aspect. After debugging the initial steps, it’s time for parameter optimization.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习项目中，选择合适的数据集处理和算法是一方面。在调试初步步骤之后，是时候进行参数优化了。
- en: 'Using `GridSearchCV` or `RandomizedSearchCV`, you can try different parameters
    for the Estimator to find the best fit:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `GridSearchCV` 或 `RandomizedSearchCV`，你可以尝试不同的参数以找到最佳适配：
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: But in machine learning, hyperparameter tuning is not limited to Estimator parameters;
    it also involves combinations of Transformer parameters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 但在机器学习中，超参数调优不仅限于 Estimator 参数；它还涉及 Transformer 参数的组合。
- en: Integrating all steps with Pipeline allows for hyperparameter tuning of every
    element with different parameter combinations.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有步骤与 Pipeline 集成，允许对每个元素进行不同参数组合的超参数调优。
- en: Note that during hyperparameter tuning, we no longer need to manually split
    train and test data. `GridSearchCV` will split the data into training and validation
    sets using `[StratifiedKFold](https://scikit-learn.org/stable/modules/cross_validation.html?ref=dataleadsfuture.com#stratified-k-fold)`,
    which implemented a k-fold cross validation mechanism.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在超参数调优期间，我们不再需要手动分割训练数据和测试数据。`GridSearchCV` 会使用 `[StratifiedKFold](https://scikit-learn.org/stable/modules/cross_validation.html?ref=dataleadsfuture.com#stratified-k-fold)`
    将数据分为训练集和验证集，该方法实现了 k-fold 交叉验证机制。
- en: '![](../Images/fffb63c0e0bcfeae2cc5fe864f7fc0d4.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fffb63c0e0bcfeae2cc5fe864f7fc0d4.png)'
- en: StratifiedKFold iterative process of splitting train data and test data. Image
    by Author
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: StratifiedKFold 迭代过程将训练数据和测试数据分开。图片来源：作者
- en: 'We can also set the number of folds for cross-validation and choose how many
    workers to use. The tuning process is illustrated in the following diagram:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以设置交叉验证的折数，并选择使用多少个工作线程。调优过程在下图中展示：
- en: '![](../Images/294ae408fb907ee717a4f16c4e6f77aa.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/294ae408fb907ee717a4f16c4e6f77aa.png)'
- en: Internal implementation of GridSearchCV hyperparameter tuning. Image by Author
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: GridSearchCV 超参数调优的内部实现。图片来源：作者
- en: Due to space constraints, I won’t go into detail about `GridSearchCV` and `RandomizedSearchCV`
    here. If you're interested, I can write another article explaining them next time.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于空间限制，我不会在此详细介绍 `GridSearchCV` 和 `RandomizedSearchCV`。如果你感兴趣，我可以在下次撰写另一篇文章来解释它们。
- en: '**Using the memory parameter to cache Transformer outputs**'
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**使用 memory 参数缓存 Transformer 输出**'
- en: Of course, hyperparameter tuning with `GridSearchCV` can be slow, but that's
    no worry, Pipeline provides a caching mechanism to speed up the tuning efficiency
    by caching the results of intermediate steps.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，使用 `GridSearchCV` 进行超参数调优可能会很慢，但这没关系，Pipeline 提供了缓存机制，通过缓存中间步骤的结果来加速调优效率。
- en: When initializing a Pipeline, you can pass in a `memory` parameter, which will
    cache the results after the first call to `fit` and `transform` for each transformer.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化 Pipeline 时，你可以传入 `memory` 参数，这将缓存每个转换器第一次调用 `fit` 和 `transform` 后的结果。
- en: If subsequent calls to `fit` and `transform` use the same parameters, which
    is very likely during hyperparameter tuning, these steps will directly read the
    results from the cache instead of recalculating, significantly speeding up the
    efficiency when running the same Transformer repeatedly.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果随后的 `fit` 和 `transform` 调用使用相同的参数，这在超参数调优期间很可能发生，这些步骤将直接从缓存中读取结果，而不是重新计算，从而显著提高了重复运行相同
    Transformer 时的效率。
- en: 'The `memory` parameter can accept the following values:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`memory` 参数可以接受以下值：'
- en: 'The default is None: caching is not used.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认值为 None：不使用缓存。
- en: 'A string: providing a path to store the cached results.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串：提供存储缓存结果的路径。
- en: 'A `joblib.Memory` object: allows for finer-grained control, such as configuring
    the storage backend for the cache.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`joblib.Memory` 对象：允许更细粒度的控制，例如配置缓存的存储后端。'
- en: 'Next, let’s use the previous `GridSearchCV` example, this time adding `memory`
    to the Pipeline to see how much speed can be improved:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用之前的 `GridSearchCV` 示例，这次将 `memory` 添加到 Pipeline 中，看看能提高多少速度：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As shown, with caching, the tuning process only takes 0.2 seconds, a significant
    speed increase from the previous 2.4 seconds.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，使用缓存后，调优过程仅需 0.2 秒，相较于之前的 2.4 秒大幅提升了速度。
- en: How to debug Scikit-Learn Pipeline
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何调试 Scikit-Learn Pipeline
- en: After integrating Transformers into a Pipeline, the entire preprocessing and
    transformation process becomes a black box. It can be difficult to understand
    which step the process is currently on.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 Transformers 集成到 Pipeline 中后，整个预处理和转换过程变成了一个黑箱。很难理解当前的处理步骤。
- en: Fortunately, we can solve this problem by adding logging to the Pipeline.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以通过在 Pipeline 中添加日志记录来解决这个问题。
- en: We need to create custom transformers to add logging at each step of data transformation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建自定义转换器，以在数据转换的每一步添加日志记录。
- en: 'Here’s an example of adding logging with Python’s standard logging library:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用 Python 标准日志库添加日志记录的示例：
- en: 'First, you need to configure a logger:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要配置一个记录器：
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, you can create a custom Transformer and add logging within its methods:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以创建一个自定义 Transformer，并在其方法中添加日志记录：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then you can use this `LoggingTransformer` when creating your Pipeline:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以在创建 Pipeline 时使用这个 `LoggingTransformer`：
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/4b21f88d271ad261ee81bd9534a8bf35.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b21f88d271ad261ee81bd9534a8bf35.png)'
- en: The effect after adding the LoggingTransformer. Image by Author
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 添加 LoggingTransformer 后的效果。图像由作者提供
- en: When you use `pipeline.fit`, it will call the `fit` and `transform` methods
    for each step in turn and log the appropriate messages.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用 `pipeline.fit` 时，它会依次调用每个步骤的 `fit` 和 `transform` 方法，并记录相应的信息。
- en: Use passthrough in Scikit-Learn Pipeline
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Scikit-Learn Pipeline 中使用 passthrough
- en: In a Pipeline, a step can be set to `'passthrough'`, which means that for this
    specific step, the input data will pass through unchanged to the next step.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pipeline 中，可以将某个步骤设置为 `'passthrough'`，这意味着对于这个特定的步骤，输入数据将不加改变地传递到下一个步骤。
- en: This is useful when you want to selectively enable/disable certain steps in
    a complex pipeline.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当你希望有选择性地启用/禁用复杂管道中的某些步骤时，这一点很有用。
- en: Taking the code example above, we know that when using `DecisionTree` or `RandomForest`,
    standardizing the data is unnecessary, so we can use `passthrough` to skip this
    step.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 参考上面的代码示例，我们知道在使用 `DecisionTree` 或 `RandomForest` 时，标准化数据是多余的，因此我们可以使用 `passthrough`
    跳过这一步。
- en: 'An example would be as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例：
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Reusing the Pipeline
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重用 Pipeline
- en: After a journey of trials and tribulations, we finally have a well-performing
    machine learning model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一番波折，我们终于得到了一个表现良好的机器学习模型。
- en: Now, you might consider how to reuse this model, share it with colleagues, or
    deploy it in a production environment.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能考虑如何重用这个模型，与同事共享，或将其部署到生产环境中。
- en: However, the result of a model’s training includes not only the model itself
    but also the various data processing steps, which all need to be saved.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，模型训练的结果不仅包括模型本身，还包括各种数据处理步骤，这些步骤都需要保存。
- en: 'Using `joblib` and Pipeline, we can save the entire training process for later
    use. The following code provides a simple example:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `joblib` 和 Pipeline，我们可以保存整个训练过程以备后用。以下代码提供了一个简单的示例：
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Conclusion
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The [official Scikit-Learn documentation](https://scikit-learn.org/stable/user_guide.html?ref=dataleadsfuture.com)
    is among the best I’ve seen. By learning its contents, you can master the basics
    of machine learning applications.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[官方 Scikit-Learn 文档](https://scikit-learn.org/stable/user_guide.html?ref=dataleadsfuture.com)
    是我见过的最好的文档之一。通过学习其内容，你可以掌握机器学习应用的基础知识。'
- en: However, when using Scikit-Learn in real projects, we often encounter various
    details that the official documentation may not cover.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际项目中使用 Scikit-Learn 时，我们经常遇到官方文档可能未涵盖的各种细节。
- en: How to correctly combine Transformers with Pipeline is one such case.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如何正确结合 Transformers 和 Pipeline 就是一个这样的案例。
- en: In this article, I introduced why to use Transformers and some typical application
    scenarios.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我介绍了使用 Transformers 的原因和一些典型应用场景。
- en: Then, I interpreted the working principle of Pipeline from the source code level
    and completed the reasonable use case when applied to train and test datasets.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我从源代码层面解释了 Pipeline 的工作原理，并完成了在训练和测试数据集上应用的合理用例。
- en: Finally, for each stage of a real machine learning project, I introduced the
    best practices of combining Transformers with Pipeline based on my work experience.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于实际机器学习项目的每个阶段，我介绍了结合 Transformers 和 Pipeline 的最佳实践，基于我的工作经验。
- en: I hope this article can help you. If you have any questions, please leave me
    a message, and I will try my best to answer them.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这篇文章能对你有所帮助。如果你有任何问题，请给我留言，我会尽力回答。
- en: Thank you for reading my stories.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读我的故事。
- en: You can [**subscribe**](https://www.dataleadsfuture.com/#/portal) to get the
    latest data science stories from me.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以[**订阅**](https://www.dataleadsfuture.com/#/portal)以获取我最新的数据科学故事。
- en: Find me on [LinkedIn](https://www.linkedin.com/in/qtalen/) or [Twitter(X)](https://twitter.com/qtalen)
    if you have any questions.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题，可以在[LinkedIn](https://www.linkedin.com/in/qtalen/)或[Twitter(X)](https://twitter.com/qtalen)上找到我。
- en: This article was originally published on [Data Leads Future](https://www.dataleadsfuture.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipeline/#/portal).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章最初发表在[数据引领未来](https://www.dataleadsfuture.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipeline/#/portal)上。
