- en: Ensuring Correct Use of Transformers in Scikit-learn Pipeline
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipelines-393566db7bfa](https://towardsdatascience.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipelines-393566db7bfa)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Effective data processing in machine learning projects
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://qtalen.medium.com/?source=post_page-----393566db7bfa--------------------------------)[![Peng
    Qian](../Images/9ce9aeb381ec6b017c1ee5d4714937e2.png)](https://qtalen.medium.com/?source=post_page-----393566db7bfa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----393566db7bfa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----393566db7bfa--------------------------------)
    [Peng Qian](https://qtalen.medium.com/?source=post_page-----393566db7bfa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----393566db7bfa--------------------------------)
    ·11 min read·Dec 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6187b45b1a6ac0f9da54deec3eb9b02.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Ensuring Correct Use of Transformers in Scikit-learn Pipeline. Image by Author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This article will explain how to use [Pipeline](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com)
    and [Transformers](https://scikit-learn.org/stable/data_transforms.html?ref=dataleadsfuture.com)
    correctly in Scikit-Learn (sklearn) projects to speed up and reuse our model training
    process.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: This piece complements and clarifies the official documentation on Pipeline
    examples and some common misunderstandings.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: I hope that after reading this, you’ll be able to use the Pipeline, an excellent
    design, to better complete your machine learning tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There’s a famous dish in Chinese restaurants around the world called “General
    Tso’s Chicken,” and I wonder if you’ve tried it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe3cd203f33891ed74abe3884196dd74.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: 'General Tso’s Chicken. A model for standardizing the cooking process. Photo
    Credit: Created by Author, [Canva](https://www.canva.com/?ref=dataleadsfuture.com).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'One characteristic of “General Tso’s Chicken” is that each piece of chicken
    is processed by the chef to be the same size. This ensures that:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: All pieces are marinated for the same amount of time.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During cooking, each piece of chicken reaches the same level of doneness.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When using chopsticks, the uniform size makes it easier to pick up the pieces.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This preprocessing includes washing, cutting, and marinating the ingredients.
    If the chicken pieces are cut larger than usual, the flavor can change significantly
    even if stir-fried for the same amount of time.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: So, when preparing to open a restaurant, we must consider standardizing these
    processes and recipes to ensure that each plate of “General Tso’s Chicken” has
    a consistent taste and texture. This is how restaurants thrive.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Back in the world of machine learning, Scikit-Learn also provides such standardized
    processes called Pipeline. They solidify the data preprocessing and model training
    process into a standardized workflow, making machine learning projects easier
    to maintain and reuse.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll explore how to use Transformers correctly within Scikit-Learn’s
    Pipeline, ensuring that our data is as perfectly prepared as the ingredients for
    a fine meal.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Why Use Transformers
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are Transformers
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Scikit-Learn, Transformers mainly fall into two categories: data scaling
    and feature dimensionality reduction.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Take, for example, a set of housing data, which includes features like location,
    area, and number of bedrooms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t standardize these features to the same scale, the model might overlook
    the significant impact of location (usually categorical data) due to minor fluctuations
    in the area (usually a larger numerical value).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: It’s like overpowering the delicate taste of herbs with too much pepper.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Using Transformers correctly
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Typically, data scaling is done using standardization, formulated as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/587d79bf0bfff52befcd5d94ebd6f51f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Formula for train_data’s standardization. Image by Author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Where `train_mean` and `train_std` are variables extracted from the train data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: In Scikit-Learn, train data and test data are both obtained from the original
    dataset using the `[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?ref=dataleadsfuture.com#sklearn.model_selection.train_test_split)`
    method.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'When scaling test data, the same `train_mean` and `train_std` are used:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/582a76406e3733a4a176557a94b400df.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: The same train_mean and train_std variables are used when scaling test data.
    Image by Author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Here arises the question: why use train data to generate these variables?'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a simple dataset where the train data is:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f77dac5255c8c753552222a771d5dced.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: A simple dataset of train data. Image by Author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'After standardization, the train data becomes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1378cb8991108dc555a598e308514aa.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: The simple dataset after scaling. Image by Author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, after scaling, features greater than 0 have a label of 1, which means
    features greater than 10 before scaling have a label of 1.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look at the test data:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab196e3c9c4cf64d55304ec13477861c.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Test data that has not yet been classified. Image by Author
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use `test_mean` and `test_std` generated from the test data distribution
    without considering the train data, the results become:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d9936e950dbfcfb5431ba67c0cb83d1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Demonstration of errors in using test data to generate variables. Image by Author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, this prediction result does not make sense. But suppose we use `train_mean`
    and `train_std` to process the data and combine it with the model prediction;
    let''s see what happens:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f80bc980e14879a4aaadfcc23415fee.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Using the variables of train data, we obtained the correct results. Image by
    Author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, only by preprocessing the data with variables generated through
    train data can we ensure that the model’s prediction meets expectations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Using Transformers in Scikit-Learn
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using Transformers in Scikit-Learn is quite simple.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: We can generate a set of simulated data using `make_classification` and then
    split it into train and test with `train_test_split`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s look at the distribution of the data:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/1497aac72727ce4897b0310bc3e20a2f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: The distribution of the data before scaling. Image by Author
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Here we’re using `StandardScaler` to scale the features.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'First, initialize the `StandardScaler`, then `fit` it with train data:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we can `transform` the train data''s features with the fitted Transformer:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Of course, we could also use `fit_transform` to fit and transform the train
    data in one go:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then we simply transform the test data without needing to fit it again:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After transformation, the distribution of the data remains unchanged, except
    for the change in scale:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/724721511afe2182f50ebd99d29d6d34.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: The distribution of the data after scaling. Image by Author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Apart from scaling data with tools like `[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?ref=dataleadsfuture.com#sklearn.preprocessing.StandardScaler)`
    and `[MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html?ref=dataleadsfuture.com#sklearn.preprocessing.MinMaxScaler)`,
    we can also use `PCA`, `SelectKBest`, etc., for dimensionality reduction. For
    the sake of brevity, I won't delve into these here, but you're welcome to consult
    the [official documentation](https://scikit-learn.org/stable/modules/feature_selection.html?ref=dataleadsfuture.com#univariate-feature-selection)
    for more information.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Using Transformers in a Pipeline
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why use a Pipeline
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, in a machine learning task, we often need to use various
    Transformers for data scaling and feature dimensionality reduction before training
    a model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'This presents several challenges:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '**Code complexity**: For each use of a Transformer, we have to go through initialization,
    `fit_transform`, and `transform` steps. Missing one step during a transformation
    could derail the entire training process.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data leakage**: As we discussed, for each Transformer, we fit with train
    data and then transform both train and test data. We must avoid letting the distribution
    of the test data leak into the train data.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code reusability**: A machine learning model includes not only the trained
    Estimator for prediction but also the data preprocessing steps. Therefore, a machine
    learning task comprising Transformers and an Estimator should be atomic and indivisible.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning**: After setting up the steps of machine learning,
    we need to adjust hyperparameters to find the best combination of Transformer
    parameter values.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-Learn introduced the `Pipeline` module to solve these issues.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: What is a Pipeline
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `Pipeline` is a module in Scikit-Learn that implements the chain of responsibility
    design pattern.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating a Pipeline, we use the `steps` parameter to chain together multiple
    Transformers for initialization:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The [official documentation](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline)
    points out that the last Transformer must be an Estimator.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t need to specify each Transformer’s name, you can simplify the
    creation of a Pipeline with `make_pipeline`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Understanding the Pipeline’s mechanism from the source code
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve mentioned the importance of not letting test data variables leak into
    training data when using each Transformer.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: This principle is relatively easy to ensure when each data preprocessing step
    is independent.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: But what if we integrate these steps using a Pipeline?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the [official documentation](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline),
    we find it simply uses the `fit` method on the entire dataset without explaining
    how to handle train and test data separately.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: With this question in mind, I dived into the Pipeline’s source code to find
    the answer.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Reading the source code revealed that although Pipeline implements `fit`, `fit_transform`,
    and `predict` methods, they work differently from regular Transformers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the following Pipeline creation process as an example:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The internal implementation can be represented by the following diagram:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6187b45b1a6ac0f9da54deec3eb9b02.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: Internal implementation of the fit and predict methods when called. Image by
    Author
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, when we call the `fit` method, Pipeline first separates Transformers
    from the Estimator.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: For each Transformer, Pipeline checks if there’s a `fit_transform` method; if
    so, it calls it; otherwise, it calls `fit`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: For the Estimator, it calls `fit` directly.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: For the predict method, Pipeline separates Transformers from the Estimator.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline calls each Transformer’s `transform` method in sequence, followed by
    the Estimator's `predict` method.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, when using a Pipeline, we still need to split train and test data.
    Then we simply call `fit` on the train data and `predict` on the test data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s a special case when combining Pipeline with `GridSearchCV` for hyperparameter
    tuning: you don''t need to manually split train and test data. I''ll explain this
    in more detail in the best practices section.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices for Using Transformers and Pipeline in Actual Applications
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve discussed the working principles of Transformers and Pipeline,
    it’s time to fulfill the promise made in the title and talk about the best practices
    when combining Transformers with Pipeline in real projects.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Combining Pipeline with GridSearchCV for hyperparameter tuning
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a machine learning project, selecting the right dataset processing and algorithm
    is one aspect. After debugging the initial steps, it’s time for parameter optimization.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `GridSearchCV` or `RandomizedSearchCV`, you can try different parameters
    for the Estimator to find the best fit:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: But in machine learning, hyperparameter tuning is not limited to Estimator parameters;
    it also involves combinations of Transformer parameters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Integrating all steps with Pipeline allows for hyperparameter tuning of every
    element with different parameter combinations.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Note that during hyperparameter tuning, we no longer need to manually split
    train and test data. `GridSearchCV` will split the data into training and validation
    sets using `[StratifiedKFold](https://scikit-learn.org/stable/modules/cross_validation.html?ref=dataleadsfuture.com#stratified-k-fold)`,
    which implemented a k-fold cross validation mechanism.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fffb63c0e0bcfeae2cc5fe864f7fc0d4.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: StratifiedKFold iterative process of splitting train data and test data. Image
    by Author
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also set the number of folds for cross-validation and choose how many
    workers to use. The tuning process is illustrated in the following diagram:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/294ae408fb907ee717a4f16c4e6f77aa.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: Internal implementation of GridSearchCV hyperparameter tuning. Image by Author
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Due to space constraints, I won’t go into detail about `GridSearchCV` and `RandomizedSearchCV`
    here. If you're interested, I can write another article explaining them next time.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '**Using the memory parameter to cache Transformer outputs**'
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of course, hyperparameter tuning with `GridSearchCV` can be slow, but that's
    no worry, Pipeline provides a caching mechanism to speed up the tuning efficiency
    by caching the results of intermediate steps.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: When initializing a Pipeline, you can pass in a `memory` parameter, which will
    cache the results after the first call to `fit` and `transform` for each transformer.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: If subsequent calls to `fit` and `transform` use the same parameters, which
    is very likely during hyperparameter tuning, these steps will directly read the
    results from the cache instead of recalculating, significantly speeding up the
    efficiency when running the same Transformer repeatedly.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'The `memory` parameter can accept the following values:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'The default is None: caching is not used.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A string: providing a path to store the cached results.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `joblib.Memory` object: allows for finer-grained control, such as configuring
    the storage backend for the cache.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, let’s use the previous `GridSearchCV` example, this time adding `memory`
    to the Pipeline to see how much speed can be improved:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As shown, with caching, the tuning process only takes 0.2 seconds, a significant
    speed increase from the previous 2.4 seconds.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: How to debug Scikit-Learn Pipeline
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After integrating Transformers into a Pipeline, the entire preprocessing and
    transformation process becomes a black box. It can be difficult to understand
    which step the process is currently on.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, we can solve this problem by adding logging to the Pipeline.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: We need to create custom transformers to add logging at each step of data transformation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of adding logging with Python’s standard logging library:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to configure a logger:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, you can create a custom Transformer and add logging within its methods:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then you can use this `LoggingTransformer` when creating your Pipeline:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/4b21f88d271ad261ee81bd9534a8bf35.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: The effect after adding the LoggingTransformer. Image by Author
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: When you use `pipeline.fit`, it will call the `fit` and `transform` methods
    for each step in turn and log the appropriate messages.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Use passthrough in Scikit-Learn Pipeline
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a Pipeline, a step can be set to `'passthrough'`, which means that for this
    specific step, the input data will pass through unchanged to the next step.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: This is useful when you want to selectively enable/disable certain steps in
    a complex pipeline.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Taking the code example above, we know that when using `DecisionTree` or `RandomForest`,
    standardizing the data is unnecessary, so we can use `passthrough` to skip this
    step.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'An example would be as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Reusing the Pipeline
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After a journey of trials and tribulations, we finally have a well-performing
    machine learning model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Now, you might consider how to reuse this model, share it with colleagues, or
    deploy it in a production environment.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: However, the result of a model’s training includes not only the model itself
    but also the various data processing steps, which all need to be saved.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `joblib` and Pipeline, we can save the entire training process for later
    use. The following code provides a simple example:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Conclusion
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [official Scikit-Learn documentation](https://scikit-learn.org/stable/user_guide.html?ref=dataleadsfuture.com)
    is among the best I’ve seen. By learning its contents, you can master the basics
    of machine learning applications.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: However, when using Scikit-Learn in real projects, we often encounter various
    details that the official documentation may not cover.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: How to correctly combine Transformers with Pipeline is one such case.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I introduced why to use Transformers and some typical application
    scenarios.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Then, I interpreted the working principle of Pipeline from the source code level
    and completed the reasonable use case when applied to train and test datasets.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Finally, for each stage of a real machine learning project, I introduced the
    best practices of combining Transformers with Pipeline based on my work experience.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article can help you. If you have any questions, please leave me
    a message, and I will try my best to answer them.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading my stories.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: You can [**subscribe**](https://www.dataleadsfuture.com/#/portal) to get the
    latest data science stories from me.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以[**订阅**](https://www.dataleadsfuture.com/#/portal)以获取我最新的数据科学故事。
- en: Find me on [LinkedIn](https://www.linkedin.com/in/qtalen/) or [Twitter(X)](https://twitter.com/qtalen)
    if you have any questions.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题，可以在[LinkedIn](https://www.linkedin.com/in/qtalen/)或[Twitter(X)](https://twitter.com/qtalen)上找到我。
- en: This article was originally published on [Data Leads Future](https://www.dataleadsfuture.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipeline/#/portal).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章最初发表在[数据引领未来](https://www.dataleadsfuture.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipeline/#/portal)上。
