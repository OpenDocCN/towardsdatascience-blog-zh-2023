- en: Ensuring Correct Use of Transformers in Scikit-learn Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipelines-393566db7bfa](https://towardsdatascience.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipelines-393566db7bfa)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Effective data processing in machine learning projects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://qtalen.medium.com/?source=post_page-----393566db7bfa--------------------------------)[![Peng
    Qian](../Images/9ce9aeb381ec6b017c1ee5d4714937e2.png)](https://qtalen.medium.com/?source=post_page-----393566db7bfa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----393566db7bfa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----393566db7bfa--------------------------------)
    [Peng Qian](https://qtalen.medium.com/?source=post_page-----393566db7bfa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----393566db7bfa--------------------------------)
    ·11 min read·Dec 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6187b45b1a6ac0f9da54deec3eb9b02.png)'
  prefs: []
  type: TYPE_IMG
- en: Ensuring Correct Use of Transformers in Scikit-learn Pipeline. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This article will explain how to use [Pipeline](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com)
    and [Transformers](https://scikit-learn.org/stable/data_transforms.html?ref=dataleadsfuture.com)
    correctly in Scikit-Learn (sklearn) projects to speed up and reuse our model training
    process.
  prefs: []
  type: TYPE_NORMAL
- en: This piece complements and clarifies the official documentation on Pipeline
    examples and some common misunderstandings.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that after reading this, you’ll be able to use the Pipeline, an excellent
    design, to better complete your machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There’s a famous dish in Chinese restaurants around the world called “General
    Tso’s Chicken,” and I wonder if you’ve tried it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe3cd203f33891ed74abe3884196dd74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'General Tso’s Chicken. A model for standardizing the cooking process. Photo
    Credit: Created by Author, [Canva](https://www.canva.com/?ref=dataleadsfuture.com).'
  prefs: []
  type: TYPE_NORMAL
- en: 'One characteristic of “General Tso’s Chicken” is that each piece of chicken
    is processed by the chef to be the same size. This ensures that:'
  prefs: []
  type: TYPE_NORMAL
- en: All pieces are marinated for the same amount of time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During cooking, each piece of chicken reaches the same level of doneness.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When using chopsticks, the uniform size makes it easier to pick up the pieces.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This preprocessing includes washing, cutting, and marinating the ingredients.
    If the chicken pieces are cut larger than usual, the flavor can change significantly
    even if stir-fried for the same amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: So, when preparing to open a restaurant, we must consider standardizing these
    processes and recipes to ensure that each plate of “General Tso’s Chicken” has
    a consistent taste and texture. This is how restaurants thrive.
  prefs: []
  type: TYPE_NORMAL
- en: Back in the world of machine learning, Scikit-Learn also provides such standardized
    processes called Pipeline. They solidify the data preprocessing and model training
    process into a standardized workflow, making machine learning projects easier
    to maintain and reuse.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll explore how to use Transformers correctly within Scikit-Learn’s
    Pipeline, ensuring that our data is as perfectly prepared as the ingredients for
    a fine meal.
  prefs: []
  type: TYPE_NORMAL
- en: Why Use Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Scikit-Learn, Transformers mainly fall into two categories: data scaling
    and feature dimensionality reduction.'
  prefs: []
  type: TYPE_NORMAL
- en: Take, for example, a set of housing data, which includes features like location,
    area, and number of bedrooms.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t standardize these features to the same scale, the model might overlook
    the significant impact of location (usually categorical data) due to minor fluctuations
    in the area (usually a larger numerical value).
  prefs: []
  type: TYPE_NORMAL
- en: It’s like overpowering the delicate taste of herbs with too much pepper.
  prefs: []
  type: TYPE_NORMAL
- en: Using Transformers correctly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Typically, data scaling is done using standardization, formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/587d79bf0bfff52befcd5d94ebd6f51f.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula for train_data’s standardization. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Where `train_mean` and `train_std` are variables extracted from the train data.
  prefs: []
  type: TYPE_NORMAL
- en: In Scikit-Learn, train data and test data are both obtained from the original
    dataset using the `[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?ref=dataleadsfuture.com#sklearn.model_selection.train_test_split)`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'When scaling test data, the same `train_mean` and `train_std` are used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/582a76406e3733a4a176557a94b400df.png)'
  prefs: []
  type: TYPE_IMG
- en: The same train_mean and train_std variables are used when scaling test data.
    Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Here arises the question: why use train data to generate these variables?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a simple dataset where the train data is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f77dac5255c8c753552222a771d5dced.png)'
  prefs: []
  type: TYPE_IMG
- en: A simple dataset of train data. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'After standardization, the train data becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1378cb8991108dc555a598e308514aa.png)'
  prefs: []
  type: TYPE_IMG
- en: The simple dataset after scaling. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, after scaling, features greater than 0 have a label of 1, which means
    features greater than 10 before scaling have a label of 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look at the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab196e3c9c4cf64d55304ec13477861c.png)'
  prefs: []
  type: TYPE_IMG
- en: Test data that has not yet been classified. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use `test_mean` and `test_std` generated from the test data distribution
    without considering the train data, the results become:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d9936e950dbfcfb5431ba67c0cb83d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Demonstration of errors in using test data to generate variables. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, this prediction result does not make sense. But suppose we use `train_mean`
    and `train_std` to process the data and combine it with the model prediction;
    let''s see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f80bc980e14879a4aaadfcc23415fee.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the variables of train data, we obtained the correct results. Image by
    Author
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, only by preprocessing the data with variables generated through
    train data can we ensure that the model’s prediction meets expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Using Transformers in Scikit-Learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using Transformers in Scikit-Learn is quite simple.
  prefs: []
  type: TYPE_NORMAL
- en: We can generate a set of simulated data using `make_classification` and then
    split it into train and test with `train_test_split`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the distribution of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1497aac72727ce4897b0310bc3e20a2f.png)'
  prefs: []
  type: TYPE_IMG
- en: The distribution of the data before scaling. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Here we’re using `StandardScaler` to scale the features.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, initialize the `StandardScaler`, then `fit` it with train data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can `transform` the train data''s features with the fitted Transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, we could also use `fit_transform` to fit and transform the train
    data in one go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we simply transform the test data without needing to fit it again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After transformation, the distribution of the data remains unchanged, except
    for the change in scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/724721511afe2182f50ebd99d29d6d34.png)'
  prefs: []
  type: TYPE_IMG
- en: The distribution of the data after scaling. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Apart from scaling data with tools like `[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?ref=dataleadsfuture.com#sklearn.preprocessing.StandardScaler)`
    and `[MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html?ref=dataleadsfuture.com#sklearn.preprocessing.MinMaxScaler)`,
    we can also use `PCA`, `SelectKBest`, etc., for dimensionality reduction. For
    the sake of brevity, I won't delve into these here, but you're welcome to consult
    the [official documentation](https://scikit-learn.org/stable/modules/feature_selection.html?ref=dataleadsfuture.com#univariate-feature-selection)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Using Transformers in a Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why use a Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, in a machine learning task, we often need to use various
    Transformers for data scaling and feature dimensionality reduction before training
    a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This presents several challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code complexity**: For each use of a Transformer, we have to go through initialization,
    `fit_transform`, and `transform` steps. Missing one step during a transformation
    could derail the entire training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data leakage**: As we discussed, for each Transformer, we fit with train
    data and then transform both train and test data. We must avoid letting the distribution
    of the test data leak into the train data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code reusability**: A machine learning model includes not only the trained
    Estimator for prediction but also the data preprocessing steps. Therefore, a machine
    learning task comprising Transformers and an Estimator should be atomic and indivisible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning**: After setting up the steps of machine learning,
    we need to adjust hyperparameters to find the best combination of Transformer
    parameter values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-Learn introduced the `Pipeline` module to solve these issues.
  prefs: []
  type: TYPE_NORMAL
- en: What is a Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `Pipeline` is a module in Scikit-Learn that implements the chain of responsibility
    design pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating a Pipeline, we use the `steps` parameter to chain together multiple
    Transformers for initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The [official documentation](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline)
    points out that the last Transformer must be an Estimator.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t need to specify each Transformer’s name, you can simplify the
    creation of a Pipeline with `make_pipeline`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Understanding the Pipeline’s mechanism from the source code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve mentioned the importance of not letting test data variables leak into
    training data when using each Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: This principle is relatively easy to ensure when each data preprocessing step
    is independent.
  prefs: []
  type: TYPE_NORMAL
- en: But what if we integrate these steps using a Pipeline?
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the [official documentation](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline),
    we find it simply uses the `fit` method on the entire dataset without explaining
    how to handle train and test data separately.
  prefs: []
  type: TYPE_NORMAL
- en: With this question in mind, I dived into the Pipeline’s source code to find
    the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the source code revealed that although Pipeline implements `fit`, `fit_transform`,
    and `predict` methods, they work differently from regular Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the following Pipeline creation process as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The internal implementation can be represented by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6187b45b1a6ac0f9da54deec3eb9b02.png)'
  prefs: []
  type: TYPE_IMG
- en: Internal implementation of the fit and predict methods when called. Image by
    Author
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, when we call the `fit` method, Pipeline first separates Transformers
    from the Estimator.
  prefs: []
  type: TYPE_NORMAL
- en: For each Transformer, Pipeline checks if there’s a `fit_transform` method; if
    so, it calls it; otherwise, it calls `fit`.
  prefs: []
  type: TYPE_NORMAL
- en: For the Estimator, it calls `fit` directly.
  prefs: []
  type: TYPE_NORMAL
- en: For the predict method, Pipeline separates Transformers from the Estimator.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline calls each Transformer’s `transform` method in sequence, followed by
    the Estimator's `predict` method.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, when using a Pipeline, we still need to split train and test data.
    Then we simply call `fit` on the train data and `predict` on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s a special case when combining Pipeline with `GridSearchCV` for hyperparameter
    tuning: you don''t need to manually split train and test data. I''ll explain this
    in more detail in the best practices section.'
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices for Using Transformers and Pipeline in Actual Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve discussed the working principles of Transformers and Pipeline,
    it’s time to fulfill the promise made in the title and talk about the best practices
    when combining Transformers with Pipeline in real projects.
  prefs: []
  type: TYPE_NORMAL
- en: Combining Pipeline with GridSearchCV for hyperparameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a machine learning project, selecting the right dataset processing and algorithm
    is one aspect. After debugging the initial steps, it’s time for parameter optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `GridSearchCV` or `RandomizedSearchCV`, you can try different parameters
    for the Estimator to find the best fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: But in machine learning, hyperparameter tuning is not limited to Estimator parameters;
    it also involves combinations of Transformer parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating all steps with Pipeline allows for hyperparameter tuning of every
    element with different parameter combinations.
  prefs: []
  type: TYPE_NORMAL
- en: Note that during hyperparameter tuning, we no longer need to manually split
    train and test data. `GridSearchCV` will split the data into training and validation
    sets using `[StratifiedKFold](https://scikit-learn.org/stable/modules/cross_validation.html?ref=dataleadsfuture.com#stratified-k-fold)`,
    which implemented a k-fold cross validation mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fffb63c0e0bcfeae2cc5fe864f7fc0d4.png)'
  prefs: []
  type: TYPE_IMG
- en: StratifiedKFold iterative process of splitting train data and test data. Image
    by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also set the number of folds for cross-validation and choose how many
    workers to use. The tuning process is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/294ae408fb907ee717a4f16c4e6f77aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Internal implementation of GridSearchCV hyperparameter tuning. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Due to space constraints, I won’t go into detail about `GridSearchCV` and `RandomizedSearchCV`
    here. If you're interested, I can write another article explaining them next time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Using the memory parameter to cache Transformer outputs**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of course, hyperparameter tuning with `GridSearchCV` can be slow, but that's
    no worry, Pipeline provides a caching mechanism to speed up the tuning efficiency
    by caching the results of intermediate steps.
  prefs: []
  type: TYPE_NORMAL
- en: When initializing a Pipeline, you can pass in a `memory` parameter, which will
    cache the results after the first call to `fit` and `transform` for each transformer.
  prefs: []
  type: TYPE_NORMAL
- en: If subsequent calls to `fit` and `transform` use the same parameters, which
    is very likely during hyperparameter tuning, these steps will directly read the
    results from the cache instead of recalculating, significantly speeding up the
    efficiency when running the same Transformer repeatedly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `memory` parameter can accept the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The default is None: caching is not used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A string: providing a path to store the cached results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `joblib.Memory` object: allows for finer-grained control, such as configuring
    the storage backend for the cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, let’s use the previous `GridSearchCV` example, this time adding `memory`
    to the Pipeline to see how much speed can be improved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As shown, with caching, the tuning process only takes 0.2 seconds, a significant
    speed increase from the previous 2.4 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: How to debug Scikit-Learn Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After integrating Transformers into a Pipeline, the entire preprocessing and
    transformation process becomes a black box. It can be difficult to understand
    which step the process is currently on.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, we can solve this problem by adding logging to the Pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: We need to create custom transformers to add logging at each step of data transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of adding logging with Python’s standard logging library:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to configure a logger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you can create a custom Transformer and add logging within its methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can use this `LoggingTransformer` when creating your Pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4b21f88d271ad261ee81bd9534a8bf35.png)'
  prefs: []
  type: TYPE_IMG
- en: The effect after adding the LoggingTransformer. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: When you use `pipeline.fit`, it will call the `fit` and `transform` methods
    for each step in turn and log the appropriate messages.
  prefs: []
  type: TYPE_NORMAL
- en: Use passthrough in Scikit-Learn Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a Pipeline, a step can be set to `'passthrough'`, which means that for this
    specific step, the input data will pass through unchanged to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: This is useful when you want to selectively enable/disable certain steps in
    a complex pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Taking the code example above, we know that when using `DecisionTree` or `RandomForest`,
    standardizing the data is unnecessary, so we can use `passthrough` to skip this
    step.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Reusing the Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After a journey of trials and tribulations, we finally have a well-performing
    machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you might consider how to reuse this model, share it with colleagues, or
    deploy it in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: However, the result of a model’s training includes not only the model itself
    but also the various data processing steps, which all need to be saved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `joblib` and Pipeline, we can save the entire training process for later
    use. The following code provides a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [official Scikit-Learn documentation](https://scikit-learn.org/stable/user_guide.html?ref=dataleadsfuture.com)
    is among the best I’ve seen. By learning its contents, you can master the basics
    of machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: However, when using Scikit-Learn in real projects, we often encounter various
    details that the official documentation may not cover.
  prefs: []
  type: TYPE_NORMAL
- en: How to correctly combine Transformers with Pipeline is one such case.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I introduced why to use Transformers and some typical application
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Then, I interpreted the working principle of Pipeline from the source code level
    and completed the reasonable use case when applied to train and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, for each stage of a real machine learning project, I introduced the
    best practices of combining Transformers with Pipeline based on my work experience.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article can help you. If you have any questions, please leave me
    a message, and I will try my best to answer them.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading my stories.
  prefs: []
  type: TYPE_NORMAL
- en: You can [**subscribe**](https://www.dataleadsfuture.com/#/portal) to get the
    latest data science stories from me.
  prefs: []
  type: TYPE_NORMAL
- en: Find me on [LinkedIn](https://www.linkedin.com/in/qtalen/) or [Twitter(X)](https://twitter.com/qtalen)
    if you have any questions.
  prefs: []
  type: TYPE_NORMAL
- en: This article was originally published on [Data Leads Future](https://www.dataleadsfuture.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipeline/#/portal).
  prefs: []
  type: TYPE_NORMAL
