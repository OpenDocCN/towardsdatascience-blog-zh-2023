- en: How to Leverage Pre-Trained Transformer Models for Custom Text Categorisation?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒçš„ Transformer æ¨¡å‹è¿›è¡Œè‡ªå®šä¹‰æ–‡æœ¬åˆ†ç±»ï¼Ÿ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-leverage-pre-trained-transformer-models-for-custom-text-categorisation-3757c517bd65](https://towardsdatascience.com/how-to-leverage-pre-trained-transformer-models-for-custom-text-categorisation-3757c517bd65)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-leverage-pre-trained-transformer-models-for-custom-text-categorisation-3757c517bd65](https://towardsdatascience.com/how-to-leverage-pre-trained-transformer-models-for-custom-text-categorisation-3757c517bd65)
- en: So, you have some custom text dataset that you wish to categorise, but wondering
    how? Well, let me show you how, using pre-trained state of the art language models.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œä½ æœ‰ä¸€äº›è‡ªå®šä¹‰çš„æ–‡æœ¬æ•°æ®é›†ï¼Œæƒ³è¦è¿›è¡Œåˆ†ç±»ï¼Œä½†ä¸çŸ¥é“å¦‚ä½•åšï¼Ÿå¥½å§ï¼Œè®©æˆ‘å±•ç¤ºä¸€ä¸‹ï¼Œå¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒçš„æœ€å…ˆè¿›è¯­è¨€æ¨¡å‹æ¥å®ç°ã€‚
- en: '[](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)[![Saed
    Hussain](../Images/cb8d313c1c1a15fd1632979dc3c080a7.png)](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)
    [Saed Hussain](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)[![Saed
    Hussain](../Images/cb8d313c1c1a15fd1632979dc3c080a7.png)](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)
    [Saed Hussain](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)
    Â·11 min readÂ·Mar 31, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 11 åˆ†é’ŸÂ·2023 å¹´ 3 æœˆ 31 æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/948131b577b7ca3d0260d3ee6b5e9654.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/948131b577b7ca3d0260d3ee6b5e9654.png)'
- en: Photo by [Meagan Carsience](https://unsplash.com/@mcarsience_photography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºäº[Meagan Carsience](https://unsplash.com/@mcarsience_photography?utm_source=medium&utm_medium=referral)åœ¨[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Ok letâ€™s get straight to the point! You have some custom data and now you want
    to categorise it into your custom classes. In this article, I will show you how
    you can use 2 approaches to achieve this objective. Both of them utilise pre-trained
    state of the art transformed based models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œè®©æˆ‘ä»¬ç›´å¥”ä¸»é¢˜ï¼ä½ æœ‰ä¸€äº›è‡ªå®šä¹‰æ•°æ®ï¼Œç°åœ¨æƒ³å°†å…¶åˆ†ç±»åˆ°è‡ªå®šä¹‰ç±»åˆ«ä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†å±•ç¤ºä½ å¦‚ä½•ä½¿ç”¨ä¸¤ç§æ–¹æ³•æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½åˆ©ç”¨äº†æœ€å…ˆè¿›çš„åŸºäº
    Transformer çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚
- en: Please note that the goal of this article is to share with you the approaches
    and how to use them. This is in not a complete data science tutorial with best
    practices. Unfortunately that is outside the scope of this article.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæœ¬æ–‡çš„ç›®æ ‡æ˜¯ä¸æ‚¨åˆ†äº«æ–¹æ³•åŠå…¶ä½¿ç”¨æ–¹å¼ã€‚è¿™ä¸æ˜¯ä¸€ä¸ªåŒ…å«æœ€ä½³å®è·µçš„å®Œæ•´æ•°æ®ç§‘å­¦æ•™ç¨‹ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ã€‚
- en: All the code from this article can be found in this [GitHub repo](https://github.com/saedhussain/medium/tree/main/text_category/notebooks).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä¸­çš„æ‰€æœ‰ä»£ç å¯ä»¥åœ¨è¿™ä¸ª[GitHub ä»“åº“](https://github.com/saedhussain/medium/tree/main/text_category/notebooks)ä¸­æ‰¾åˆ°ã€‚
- en: '1: Zero Shot Classification'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1: é›¶æ ·æœ¬åˆ†ç±»'
- en: Overview
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: Zero-shot classification is a technique that allows you to classify text into
    categories without training a specific model for that task. Instead, it uses pre-trained
    models that have been trained on a large amount of data to perform this classification.
    The models are typically trained on a variety of tasks, including language modelling,
    text completion, and text entailment, among others.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é›¶æ ·æœ¬åˆ†ç±»æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå®ƒå…è®¸ä½ åœ¨æ²¡æœ‰ä¸ºç‰¹å®šä»»åŠ¡è®­ç»ƒä¸“é—¨æ¨¡å‹çš„æƒ…å†µä¸‹å¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ã€‚ç›¸åï¼Œå®ƒä½¿ç”¨å·²ç»åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒè¿‡çš„é¢„è®­ç»ƒæ¨¡å‹æ¥æ‰§è¡Œåˆ†ç±»ã€‚æ¨¡å‹é€šå¸¸ä¼šåœ¨åŒ…æ‹¬è¯­è¨€å»ºæ¨¡ã€æ–‡æœ¬è¡¥å…¨å’Œæ–‡æœ¬è•´æ¶µç­‰å„ç§ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: '![](../Images/af18cccb1f07546070ca552ef5d34bde.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af18cccb1f07546070ca552ef5d34bde.png)'
- en: 'Zero-shot text classification using pre-trained LLMs (source: author)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é¢„è®­ç»ƒçš„ LLM è¿›è¡Œé›¶æ ·æœ¬æ–‡æœ¬åˆ†ç±»ï¼ˆæ¥æºï¼šä½œè€…ï¼‰
- en: To perform zero-shot classification, you simply need to provide the pre-trained
    model with some text and a list of possible categories.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ‰§è¡Œé›¶æ ·æœ¬åˆ†ç±»ï¼Œä½ åªéœ€å‘é¢„è®­ç»ƒæ¨¡å‹æä¾›ä¸€äº›æ–‡æœ¬å’Œä¸€ä¸ªå¯èƒ½çš„ç±»åˆ«åˆ—è¡¨ã€‚
- en: The model will then use its understanding of language and its pre-existing knowledge
    to classify the text into one of the provided categories. This approach is particularly
    useful when you have limited data available for a specific classification task,
    as it allows you to leverage the pre-existing knowledge of the model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å°†åˆ©ç”¨å…¶å¯¹è¯­è¨€çš„ç†è§£å’Œå·²æœ‰çŸ¥è¯†å°†æ–‡æœ¬åˆ†ç±»åˆ°æä¾›çš„ç±»åˆ«ä¹‹ä¸€ã€‚è¿™ç§æ–¹æ³•ç‰¹åˆ«æœ‰ç”¨ï¼Œå½“ä½ ä¸ºç‰¹å®šåˆ†ç±»ä»»åŠ¡æ‹¥æœ‰çš„æ•°æ®æœ‰é™æ—¶ï¼Œå› ä¸ºå®ƒå…è®¸ä½ åˆ©ç”¨æ¨¡å‹çš„å·²æœ‰çŸ¥è¯†ã€‚
- en: Since it does the classification without any training on that particular task,
    itâ€™s know as zero-shot.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå®ƒåœ¨ç‰¹å®šä»»åŠ¡ä¸Šæ²¡æœ‰ä»»ä½•è®­ç»ƒï¼Œå› æ­¤è¢«ç§°ä¸ºé›¶æ ·æœ¬åˆ†ç±»ã€‚
- en: Implementation
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: All we need to implement this is to install the [hugging face transformers library](https://github.com/huggingface/transformers)
    using `pip install transformers` . We will use the pre-trained Facebook BART (Bidirectional
    and Auto-Regressive Transformers) model for this task.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯ä½¿ç”¨`pip install transformers`å®‰è£…[hugging face transformersåº“](https://github.com/huggingface/transformers)ã€‚æˆ‘ä»¬å°†ä½¿ç”¨é¢„è®­ç»ƒçš„Facebook
    BARTï¼ˆåŒå‘å’Œè‡ªå›å½’å˜æ¢å™¨ï¼‰æ¨¡å‹æ¥å®Œæˆè¿™é¡¹ä»»åŠ¡ã€‚
- en: '***Side Note****: on first use, it will take some time to download the model.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '***é™„æ³¨***ï¼šé¦–æ¬¡ä½¿ç”¨æ—¶ï¼Œä¸‹è½½æ¨¡å‹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚'
- en: 'The output is a dictionary with 3 keys:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ˜¯ä¸€ä¸ªåŒ…å«3ä¸ªé”®çš„å­—å…¸ï¼š
- en: '**sequence**: input text that was classified by the pipeline'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åºåˆ—**ï¼šç”±ç®¡é“åˆ†ç±»çš„è¾“å…¥æ–‡æœ¬'
- en: '**labels**: list of candidate (category) labels provided to the pipeline, ordered
    based on their probability scores.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ ‡ç­¾**ï¼šæä¾›ç»™ç®¡é“çš„å€™é€‰ï¼ˆç±»åˆ«ï¼‰æ ‡ç­¾çš„åˆ—è¡¨ï¼Œæ ¹æ®å®ƒä»¬çš„æ¦‚ç‡åˆ†æ•°è¿›è¡Œæ’åºã€‚'
- en: '**scores**: probabilities scores assigned to each candidate label based on
    the modelâ€™s prediction of how likely the input text belongs to that label.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åˆ†æ•°**ï¼šåŸºäºæ¨¡å‹å¯¹è¾“å…¥æ–‡æœ¬å±äºè¯¥æ ‡ç­¾çš„å¯èƒ½æ€§çš„é¢„æµ‹ï¼Œåˆ†é…ç»™æ¯ä¸ªå€™é€‰æ ‡ç­¾çš„æ¦‚ç‡åˆ†æ•°ã€‚'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see, without any training, the model has correctly classified the
    given text into the â€œgroceriesâ€ category. Because the model was trained on a large
    corpus of text in a given language, it can understand that language and draw inference.
    it understood the text and identified a suitable category from the list of candidate
    labels.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œåœ¨æ²¡æœ‰ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹å·²æ­£ç¡®åœ°å°†ç»™å®šæ–‡æœ¬åˆ†ç±»åˆ°â€œæ‚è´§â€ç±»åˆ«ã€‚å› ä¸ºæ¨¡å‹æ˜¯åœ¨ç‰¹å®šè¯­è¨€çš„å¤§å‹è¯­æ–™åº“ä¸Šè®­ç»ƒçš„ï¼Œå®ƒèƒ½å¤Ÿç†è§£è¯¥è¯­è¨€å¹¶è¿›è¡Œæ¨æ–­ã€‚å®ƒç†è§£äº†æ–‡æœ¬å¹¶ä»å€™é€‰æ ‡ç­¾åˆ—è¡¨ä¸­è¯†åˆ«äº†åˆé€‚çš„ç±»åˆ«ã€‚
- en: Simply put, itâ€™s brilliant!! ğŸ˜Š
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼ŒçœŸæ˜¯å¤ªæ£’äº†ï¼ï¼ğŸ˜Š
- en: The larger the model is, the better it is at zero-shot classification tasks.
    For more info, have a look at the [zero-shot classification page](https://huggingface.co/tasks/zero-shot-classification)
    on hugging face.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¶Šå¤§ï¼Œé›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡çš„è¡¨ç°è¶Šå¥½ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹hugging faceä¸Šçš„[é›¶æ ·æœ¬åˆ†ç±»é¡µé¢](https://huggingface.co/tasks/zero-shot-classification)ã€‚
- en: âš¡ï¸ Checkout this [notebook](https://github.com/saedhussain/medium/blob/48db9652f83cbb83b547a0a55ff5bfb8355e0d26/text_category/notebooks/zero_shot_classification.ipynb)
    for more examples.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: âš¡ï¸ æŸ¥çœ‹è¿™ä¸ª[ç¬”è®°æœ¬](https://github.com/saedhussain/medium/blob/48db9652f83cbb83b547a0a55ff5bfb8355e0d26/text_category/notebooks/zero_shot_classification.ipynb)ä»¥è·å–æ›´å¤šç¤ºä¾‹ã€‚
- en: When to use it?
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ—¶å€™ä½¿ç”¨å®ƒï¼Ÿ
- en: Given its extraordinary performance without any training, I highly recommend
    trying this first if you know the descriptions of the categories. It leverages
    the state of the art pre-trained models and gives excellent results without any
    training.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºå…¶åœ¨æ²¡æœ‰ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹è¡¨ç°å“è¶Šï¼Œå¦‚æœä½ äº†è§£ç±»åˆ«çš„æè¿°ï¼Œæˆ‘å¼ºçƒˆå»ºè®®é¦–å…ˆå°è¯•è¿™ä¸ªæ–¹æ³•ã€‚å®ƒåˆ©ç”¨äº†æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨æ²¡æœ‰ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹æä¾›äº†å“è¶Šçš„ç»“æœã€‚
- en: 'Here is a non exhaustive list of when this can be your go to approach:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€äº›éè¯¦å°½çš„æƒ…å†µï¼Œå½“è¿™äº›æƒ…å†µé€‚åˆä½ çš„æ–¹æ³•æ—¶ï¼š
- en: When you have limited labeled training data
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“ä½ æ‹¥æœ‰æœ‰é™çš„æ ‡æ³¨è®­ç»ƒæ•°æ®æ—¶
- en: When you need to quickly prototype a solution
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“ä½ éœ€è¦å¿«é€ŸåŸå‹ä¸€ä¸ªè§£å†³æ–¹æ¡ˆæ—¶
- en: When you need to classify new or rare categories
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“ä½ éœ€è¦å¯¹æ–°çš„æˆ–ç¨€æœ‰çš„ç±»åˆ«è¿›è¡Œåˆ†ç±»æ—¶
- en: When you need to classify instances into multiple categories
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“ä½ éœ€è¦å°†å®ä¾‹åˆ†ç±»åˆ°å¤šä¸ªç±»åˆ«ä¸­æ—¶
- en: When you want to leverage pre-trained models to classify instances without additional
    training
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“ä½ æƒ³åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å¯¹å®ä¾‹è¿›è¡Œåˆ†ç±»è€Œæ— éœ€é¢å¤–è®­ç»ƒæ—¶
- en: When you want to classify instances into categories that are defined by natural
    language descriptions rather than predefined labels.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“ä½ æƒ³å°†å®ä¾‹åˆ†ç±»åˆ°ç”±è‡ªç„¶è¯­è¨€æè¿°å®šä¹‰çš„ç±»åˆ«ä¸­ï¼Œè€Œä¸æ˜¯é¢„å®šä¹‰æ ‡ç­¾æ—¶ã€‚
- en: What are the limitations?
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ‰å“ªäº›é™åˆ¶ï¼Ÿ
- en: 'Here are some potential drawbacks and limitations to consider when using the
    zero-shot classification approach:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä½¿ç”¨é›¶æ ·æœ¬åˆ†ç±»æ–¹æ³•æ—¶éœ€è¦è€ƒè™‘çš„ä¸€äº›æ½œåœ¨ç¼ºç‚¹å’Œé™åˆ¶ï¼š
- en: 'Limited training data augmentation: In zero-shot classification, there is limited
    scope for augmenting the training data to improve model performance, unlike in
    traditional supervised learning approaches.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ‰é™çš„è®­ç»ƒæ•°æ®æ‰©å¢ï¼šåœ¨é›¶-shot åˆ†ç±»ä¸­ï¼Œæ‰©å¢è®­ç»ƒæ•°æ®ä»¥æå‡æ¨¡å‹æ€§èƒ½çš„èŒƒå›´æœ‰é™ï¼Œè¿™ä¸ä¼ ç»Ÿç›‘ç£å­¦ä¹ æ–¹æ³•ä¸åŒã€‚
- en: 'Limited control over model behaviour: Zero-shot classification relies on a
    pre-trained model, which means that you have limited control over its behaviour
    and the patterns it has learned. This can lead to unexpected results, especially
    if the model was not trained on data similar to your task.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹æ¨¡å‹è¡Œä¸ºçš„æ§åˆ¶æœ‰é™ï¼šé›¶-shot åˆ†ç±»ä¾èµ–äºé¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™æ„å‘³ç€ä½ å¯¹å…¶è¡Œä¸ºå’Œå­¦ä¹ åˆ°çš„æ¨¡å¼çš„æ§åˆ¶æœ‰é™ã€‚è¿™å¯èƒ½å¯¼è‡´æ„å¤–ç»“æœï¼Œç‰¹åˆ«æ˜¯å½“æ¨¡å‹æ²¡æœ‰åœ¨ç±»ä¼¼äºä½ ä»»åŠ¡çš„æ•°æ®ä¸Šè®­ç»ƒæ—¶ã€‚
- en: 'Limited customisation: Because zero-shot classification relies on a pre-trained
    model, there is limited scope for customisation or fine-tuning to the specific
    nuances of your task. This can limit the accuracy and performance of the model,
    especially if the task involves complex or domain-specific language.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ‰é™çš„è‡ªå®šä¹‰ï¼šç”±äºé›¶-shot åˆ†ç±»ä¾èµ–äºé¢„è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤è‡ªå®šä¹‰æˆ–å¾®è°ƒä»¥é€‚åº”ç‰¹å®šä»»åŠ¡çš„ä½™åœ°æœ‰é™ã€‚è¿™å¯èƒ½é™åˆ¶æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ€§èƒ½ï¼Œå°¤å…¶æ˜¯å½“ä»»åŠ¡æ¶‰åŠå¤æ‚æˆ–é¢†åŸŸç‰¹å®šçš„è¯­è¨€æ—¶ã€‚
- en: 'ğŸš€ Pro Tip: Always Clean Your Text!!'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸš€ ä¸“ä¸šæç¤ºï¼šå§‹ç»ˆæ¸…ç†ä½ çš„æ–‡æœ¬ï¼ï¼
- en: Although transformer-based LLMs are significantly better at handling noisy text
    compared to other models, it is still highly recommended to clean the text before
    feeding it into the model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡åŸºäºå˜æ¢å™¨çš„LLMåœ¨å¤„ç†å˜ˆæ‚æ–‡æœ¬æ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œä½†åœ¨å°†æ–‡æœ¬è¾“å…¥æ¨¡å‹ä¹‹å‰ï¼Œä»ç„¶å¼ºçƒˆå»ºè®®æ¸…ç†æ–‡æœ¬ã€‚
- en: Not only is it a good data science practice, but it also makes a huge difference.
    The embeddings generated with noisy text data might have lower similarity scores
    to the correct category when compared against a clean text input. This undoubtably
    will reduce the category classification scores.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ä»…æ˜¯è‰¯å¥½çš„æ•°æ®ç§‘å­¦å®è·µï¼Œè¿˜ä¼šäº§ç”Ÿå·¨å¤§å·®å¼‚ã€‚ä¸å¹²å‡€æ–‡æœ¬è¾“å…¥ç›¸æ¯”ï¼Œç”Ÿæˆçš„åµŒå…¥åœ¨å˜ˆæ‚æ–‡æœ¬æ•°æ®ä¸­çš„ç›¸ä¼¼æ€§è¯„åˆ†å¯èƒ½è¾ƒä½ã€‚è¿™æ— ç–‘ä¼šé™ä½ç±»åˆ«åˆ†ç±»è¯„åˆ†ã€‚
- en: For example, below is a comparison of categorising this text â€” â€œ*Tesco Semi
    Skimmed Milk 1.13L/2 Pints â€¦â€¦ Â£1.30*â€, before and after removing any special characters
    and numbers. Why not try it out on the [Hugging Face zero-shot classification](https://huggingface.co/tasks/zero-shot-classification)
    page?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä¸‹é¢æ˜¯å¯¹è¿™æ®µæ–‡æœ¬â€”â€”â€œ*Tesco Semi Skimmed Milk 1.13L/2 Pints â€¦â€¦ Â£1.30*â€â€”â€”åœ¨å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—å‰åçš„åˆ†ç±»å¯¹æ¯”ã€‚ä¸ºä»€ä¹ˆä¸åœ¨[Hugging
    Face zero-shot classification](https://huggingface.co/tasks/zero-shot-classification)é¡µé¢ä¸Šè¯•è¯•å‘¢ï¼Ÿ
- en: '![](../Images/233b993b53778d7ee285df8bb794c1b0.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/233b993b53778d7ee285df8bb794c1b0.png)'
- en: Text categorisation probability comparison, using zero-shot method before and
    after cleaning text. (source:author)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é›¶-shot æ–¹æ³•åœ¨æ¸…ç†æ–‡æœ¬å‰åçš„æ–‡æœ¬åˆ†ç±»æ¦‚ç‡æ¯”è¾ƒã€‚ï¼ˆæ¥æºï¼šä½œè€…ï¼‰
- en: â˜•ï¸ Letâ€™s take a break before we go any furtherâ€¦
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â˜•ï¸ è®©æˆ‘ä»¬åœ¨ç»§ç»­ä¹‹å‰ä¼‘æ¯ä¸€ä¸‹â€¦â€¦
- en: '![](../Images/d8f8e20370669c8ee9b38864a0a45b0c.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8f8e20370669c8ee9b38864a0a45b0c.png)'
- en: Photo by [Victoria Tronina](https://unsplash.com/@victoriaorvicky?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”±[Victoria Tronina](https://unsplash.com/@victoriaorvicky?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œæ¥è‡ª[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: â˜•ï¸ 5 minutes laterâ€¦.alright, letâ€™s do this ğŸƒ
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â˜•ï¸ 5åˆ†é’Ÿåâ€¦â€¦å¥½å§ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ ğŸƒ
- en: '2: Transfer Learning (pre-trained model + classification model)'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2: è¿ç§»å­¦ä¹ ï¼ˆé¢„è®­ç»ƒæ¨¡å‹ + åˆ†ç±»æ¨¡å‹ï¼‰'
- en: Overview
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: This is the next level up from the previous approach. As briefly mentioned earlier,
    the zero-shot classification does not allow for customisation to a particular
    task. This is where this approach comes in as a **long term solution**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§æ¯”å‰ä¸€ç§æ–¹æ³•æ›´é«˜çº§çš„æ–¹å¼ã€‚å¦‚å‰æ‰€è¿°ï¼Œé›¶-shot åˆ†ç±»ä¸å…è®¸é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè‡ªå®šä¹‰ã€‚è¿™å°±æ˜¯è¿™ç§æ–¹æ³•ä½œä¸º**é•¿æœŸè§£å†³æ–¹æ¡ˆ**çš„æ‰€åœ¨ã€‚
- en: In this approach, you use a pre-trained transformer model to create text embeddings,
    and then train a classification model to categorise these embeddings into their
    respective categories.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œä½ ä½¿ç”¨é¢„è®­ç»ƒçš„å˜æ¢å™¨æ¨¡å‹åˆ›å»ºæ–‡æœ¬åµŒå…¥ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªåˆ†ç±»æ¨¡å‹å°†è¿™äº›åµŒå…¥åˆ†ç±»åˆ°å„è‡ªçš„ç±»åˆ«ä¸­ã€‚
- en: '![](../Images/10998d7b476a0bcafa476ba46f9777c7.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10998d7b476a0bcafa476ba46f9777c7.png)'
- en: 'Solution overview: Converting item description text to text embeddings using
    pre-trained models and using a classification model to classify the text into
    categories. (source:author)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆæ¦‚è¿°ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å°†é¡¹ç›®æè¿°æ–‡æœ¬è½¬æ¢ä¸ºæ–‡æœ¬åµŒå…¥ï¼Œå¹¶ä½¿ç”¨åˆ†ç±»æ¨¡å‹å°†æ–‡æœ¬åˆ†ç±»åˆ°ä¸åŒç±»åˆ«ä¸­ã€‚ï¼ˆæ¥æºï¼šä½œè€…ï¼‰
- en: To demo this example, we will be using this [e-commerce products dataset](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification)
    from Kaggle.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¼”ç¤ºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ªKaggleçš„[e-commerceäº§å“æ•°æ®é›†](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification)ã€‚
- en: This dataset contains 4 product categories and text description of the product.
    The objective is to build a model that can classify product description text into
    these 4 categories.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ•°æ®é›†åŒ…å«4ä¸ªäº§å“ç±»åˆ«å’Œäº§å“çš„æ–‡æœ¬æè¿°ã€‚ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥å°†äº§å“æè¿°æ–‡æœ¬åˆ†ç±»åˆ°è¿™4ä¸ªç±»åˆ«ä¸­ã€‚
- en: '![](../Images/3683e4c4cac6df55adb8d729f28f0595.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3683e4c4cac6df55adb8d729f28f0595.png)'
- en: 'E-commerce product description and category data from [Kaggle](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification).
    (source: author)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kaggle](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification)ä¸Šçš„ç”µå­å•†åŠ¡äº§å“æè¿°å’Œç±»åˆ«æ•°æ®ã€‚ï¼ˆæ¥æºï¼šä½œè€…ï¼‰'
- en: '![](../Images/8cd8a80a48ad90ec2d0b5f3d46fdd287.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cd8a80a48ad90ec2d0b5f3d46fdd287.png)'
- en: 'There are 4 categories in this [dataset](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification).
    (source: author)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ª[æ•°æ®é›†](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification)ä¸­æœ‰4ä¸ªç±»åˆ«ã€‚ï¼ˆæ¥æºï¼šä½œè€…ï¼‰
- en: Implementation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: 'The implementation of this approach can be broken down into 4 steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹æ³•çš„å®ç°å¯ä»¥åˆ†ä¸º4ä¸ªæ­¥éª¤ï¼š
- en: Clean the text before feeding it to the model.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å°†æ–‡æœ¬è¾“å…¥æ¨¡å‹ä¹‹å‰ï¼Œè¯·å…ˆæ¸…ç†æ–‡æœ¬ã€‚
- en: Generate text embeddings using a pre-trained large language model (LLM).
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚
- en: Train a classification model on the custom classes using the embeddings.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åµŒå…¥è®­ç»ƒä¸€ä¸ªè‡ªå®šä¹‰ç±»åˆ«çš„åˆ†ç±»æ¨¡å‹ã€‚
- en: Run predictions using the trained model and preprocessing pipeline.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å’Œé¢„å¤„ç†ç®¡é“è¿›è¡Œé¢„æµ‹ã€‚
- en: By the end of this, you will have a classification model that leverages the
    high-quality embeddings learned by the LLM training on an extraordinary amount
    of data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æ­¤ä¸ºæ­¢ï¼Œä½ å°†æ‹¥æœ‰ä¸€ä¸ªåˆ†ç±»æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨äº†LLMåœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒå­¦åˆ°çš„é«˜è´¨é‡åµŒå…¥ã€‚
- en: These embeddings are the key factor in the success of the LLMs, as they capture
    rich representations of language that can effectively capture the nuances of meaning
    and context in a text.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åµŒå…¥æ˜¯LLMæˆåŠŸçš„å…³é”®å› ç´ ï¼Œå› ä¸ºå®ƒä»¬æ•æ‰äº†è¯­è¨€çš„ä¸°å¯Œè¡¨ç¤ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ–‡æœ¬ä¸­çš„æ„ä¹‰å’Œä¸Šä¸‹æ–‡çš„ç»†å¾®å·®åˆ«ã€‚
- en: âš¡ï¸ Note that the full code for this can be found at the following [notebook](https://github.com/saedhussain/medium/blob/53453650d6b5281686796cdbf8e4d9592b376c85/text_category/notebooks/ecommerce_text_categorisation.ipynb).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: âš¡ï¸ è¯·æ³¨æ„ï¼Œå®Œæ•´çš„ä»£ç å¯ä»¥åœ¨ä»¥ä¸‹[ç¬”è®°æœ¬](https://github.com/saedhussain/medium/blob/53453650d6b5281686796cdbf8e4d9592b376c85/text_category/notebooks/ecommerce_text_categorisation.ipynb)ä¸­æ‰¾åˆ°ã€‚
- en: 'Step 1: Clean the Text'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤1ï¼šæ¸…ç†æ–‡æœ¬
- en: As mentioned in the previous approach, itâ€™s always a good practice to clean
    the text before using it for a model. Below the code used for cleaning the text
    in this article.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰è¿°æ–¹æ³•æ‰€æï¼Œä½¿ç”¨æ¨¡å‹ä¹‹å‰æ¸…ç†æ–‡æœ¬å§‹ç»ˆæ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ã€‚ä»¥ä¸‹æ˜¯æœ¬æ–‡ä¸­ç”¨äºæ¸…ç†æ–‡æœ¬çš„`ä»£ç `ã€‚
- en: '***Side Note****: Cleaning is highly task-specific. In this example, I have
    only implemented some basic cleaning. As a best practice, it is always a good
    idea to first understand the data and then implement some task-specific cleaning.
    For example, you may need to decide whether to keep or remove numbers, depending
    on the requirements of your specific task. Additionally, reducing the text by
    removing unnecessary words/symbols can decrease processing time in downstream
    processes. Happy cleaning!* ğŸ˜„'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '***é™„æ³¨****ï¼šæ¸…ç†æ˜¯é«˜åº¦ä»»åŠ¡ç‰¹å®šçš„ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘åªå®ç°äº†ä¸€äº›åŸºæœ¬çš„æ¸…ç†ã€‚ä½œä¸ºæœ€ä½³å®è·µï¼Œæœ€å¥½å…ˆäº†è§£æ•°æ®ï¼Œç„¶åå®æ–½ä¸€äº›ä»»åŠ¡ç‰¹å®šçš„æ¸…ç†ã€‚ä¾‹å¦‚ï¼Œä½ å¯èƒ½éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡çš„è¦æ±‚å†³å®šæ˜¯å¦ä¿ç•™æˆ–åˆ é™¤æ•°å­—ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ é™¤ä¸å¿…è¦çš„è¯è¯­/ç¬¦å·æ¥å‡å°‘æ–‡æœ¬é‡ï¼Œå¯ä»¥å‡å°‘åç»­å¤„ç†çš„æ—¶é—´ã€‚ç¥æ¸…ç†æ„‰å¿«!*
    ğŸ˜„'
- en: '[PRE1]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Step 2: Generate Text Embeddings'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤2ï¼šç”Ÿæˆæ–‡æœ¬åµŒå…¥
- en: We are going to use the [SentenceTransformer](https://huggingface.co/sentence-transformers)
    library from Hugging Face to create text embeddings. This library contains transformer-based
    models pre-trained to generate fixed-length vector representations of textual
    data, such as paragraphs or sentences.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ªHugging Faceçš„[SentenceTransformer](https://huggingface.co/sentence-transformers)åº“æ¥åˆ›å»ºæ–‡æœ¬åµŒå…¥ã€‚è¯¥åº“åŒ…å«åŸºäºå˜æ¢å™¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨äºç”Ÿæˆå›ºå®šé•¿åº¦çš„æ–‡æœ¬æ•°æ®å‘é‡è¡¨ç¤ºï¼Œå¦‚æ®µè½æˆ–å¥å­ã€‚
- en: Specifically, weâ€™ll use the *â€œparaphrase-mpnet-base-v2â€* pre-trained model from
    the library for this example, which produces fixed-length vectors of length 768.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åº“ä¸­çš„*â€œparaphrase-mpnet-base-v2â€*é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆé•¿åº¦ä¸º768çš„å›ºå®šé•¿åº¦å‘é‡ã€‚
- en: '***Side Note****: in order to save time, I have reduced the sample size of
    both the training and test dataset. Generating embeddings can take a while, especially
    on a local machine. This can effect the performance of the model.*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '***é™„æ³¨***ï¼šä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œæˆ‘å‡å°‘äº†è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ ·æœ¬é‡ã€‚ç”ŸæˆåµŒå…¥è¡¨ç¤ºå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œå°¤å…¶æ˜¯åœ¨æœ¬åœ°æœºå™¨ä¸Šã€‚è¿™å¯èƒ½ä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚'
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Step 3: Train the Classification Model'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤3ï¼šè®­ç»ƒåˆ†ç±»æ¨¡å‹
- en: Now that we have the embeddings, we are ready to train a classification model
    to take in these embeddings and classify it into one of the 4 product categories.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†åµŒå…¥è¡¨ç¤ºï¼Œæˆ‘ä»¬å‡†å¤‡è®­ç»ƒä¸€ä¸ªåˆ†ç±»æ¨¡å‹ï¼Œå°†è¿™äº›åµŒå…¥è¡¨ç¤ºåˆ†ç±»åˆ°4ä¸ªäº§å“ç±»åˆ«ä¸­çš„å…¶ä¸­ä¸€ä¸ªã€‚
- en: In this demo, I am using the XGBoost model, but feel free to use what ever you
    fancy!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ¼”ç¤ºä¸­ï¼Œæˆ‘ä½¿ç”¨äº†XGBoostæ¨¡å‹ï¼Œä½†ä½ å¯ä»¥éšæ„ä½¿ç”¨ä½ å–œæ¬¢çš„æ¨¡å‹ï¼
- en: '***Side Note****: in order to save time during training, limited hyper-parameters
    were used in the grid search. Also, to keep things simple, we are using the accuracy
    as a measure of performance. Make sure to use the appropriate metrics to measure
    the performance for your categorisation task.*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '***é™„æ³¨***ï¼šä¸ºäº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èŠ‚çœæ—¶é—´ï¼Œç½‘æ ¼æœç´¢ä¸­ä½¿ç”¨äº†æœ‰é™çš„è¶…å‚æ•°ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨å‡†ç¡®ç‡ä½œä¸ºæ€§èƒ½è¡¡é‡æ ‡å‡†ã€‚ç¡®ä¿ä½¿ç”¨é€‚å½“çš„æŒ‡æ ‡æ¥è¡¡é‡ä½ çš„åˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ã€‚'
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 4: Run Predictions'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤4ï¼šè¿è¡Œé¢„æµ‹
- en: Finally, package the text processing steps, load the trained classifier model
    and you are good to run predictions on your test dataset.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‰“åŒ…æ–‡æœ¬å¤„ç†æ­¥éª¤ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„åˆ†ç±»å™¨æ¨¡å‹ï¼Œä½ å°±å¯ä»¥åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè¿è¡Œé¢„æµ‹äº†ã€‚
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And there you have it folks!! ğŸ˜ƒ
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å„ä½çš„å†…å®¹ï¼ï¼ ğŸ˜ƒ
- en: Without much task specific text cleaning, we used the embeddings from the pre-trained
    transformer model and build a classifier to categorise it into 4 categories, with
    91% accuracy.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ²¡æœ‰è¿›è¡Œå¤ªå¤šç‰¹å®šä»»åŠ¡çš„æ–‡æœ¬æ¸…ç†çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é¢„è®­ç»ƒå˜æ¢å™¨æ¨¡å‹ç”Ÿæˆçš„åµŒå…¥è¡¨ç¤ºï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œå°†å…¶åˆ†ç±»åˆ°4ä¸ªç±»åˆ«ä¸­ï¼Œå‡†ç¡®ç‡ä¸º91%ã€‚
- en: '![](../Images/bfb928bd622c8cf831b47c25500291b7.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfb928bd622c8cf831b47c25500291b7.png)'
- en: Predictions (preds) on the test dataset. (source:author)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ•°æ®é›†ä¸Šçš„é¢„æµ‹ï¼ˆpredsï¼‰ã€‚ (æ¥æº:ä½œè€…)
- en: '![](../Images/417c5d0a2b319d3987138a9bcda81370.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/417c5d0a2b319d3987138a9bcda81370.png)'
- en: Cases where the predictions were wrong on the test dataset. (source:author)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šé¢„æµ‹é”™è¯¯çš„æƒ…å†µã€‚ (æ¥æº:ä½œè€…)
- en: The cases where the model failed to make correct predictions can be improved
    by using more data during training and doing task specific text cleaning. For
    example, leave dimension numbers in, as it helps differentiate household products
    from the rest.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æœªèƒ½æ­£ç¡®é¢„æµ‹çš„æƒ…å†µå¯ä»¥é€šè¿‡ä½¿ç”¨æ›´å¤šçš„æ•°æ®è¿›è¡Œè®­ç»ƒå’Œæ‰§è¡Œç‰¹å®šä»»åŠ¡çš„æ–‡æœ¬æ¸…ç†æ¥æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œä¿ç•™ç»´åº¦ç¼–å·ï¼Œå› ä¸ºå®ƒæœ‰åŠ©äºåŒºåˆ†å®¶åº­äº§å“ä¸å…¶ä»–äº§å“ã€‚
- en: âš¡ï¸ Another reminder that the full code is available in this [notebook](https://github.com/saedhussain/medium/blob/53453650d6b5281686796cdbf8e4d9592b376c85/text_category/notebooks/ecommerce_text_categorisation.ipynb).
    ğŸ¤—
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: âš¡ï¸ å†æ¬¡æé†’ï¼Œå®Œæ•´ä»£ç å¯åœ¨è¿™ä¸ª[ç¬”è®°æœ¬](https://github.com/saedhussain/medium/blob/53453650d6b5281686796cdbf8e4d9592b376c85/text_category/notebooks/ecommerce_text_categorisation.ipynb)ä¸­æ‰¾åˆ°ã€‚
    ğŸ¤—
- en: Final Thoughts
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæƒ³æ³•
- en: In this article, we have explored two different approaches for harnessing the
    power of pre-trained LLMs to customise text categorisation. We can achieve impressive
    results with minimal effort thanks to the high-quality embeddings they generate.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å®šåˆ¶æ–‡æœ¬åˆ†ç±»çš„ä¸¤ç§ä¸åŒæ–¹æ³•ã€‚ç”±äºå®ƒä»¬ç”Ÿæˆçš„é«˜è´¨é‡åµŒå…¥ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥æœ€å°çš„åŠªåŠ›å–å¾—ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚
- en: As these LLMs have been trained on large corpora of data, they possess a deep
    understanding of the language and can effectively capture the nuances of meaning
    and context in texts written in that language.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™äº›LLMså·²ç»åœ¨å¤§é‡æ•°æ®ä¸Šè¿›è¡Œè¿‡è®­ç»ƒï¼Œå®ƒä»¬å¯¹è¯­è¨€æœ‰æ·±å…¥çš„ç†è§£ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ–‡æœ¬ä¸­æ„ä¹‰å’Œè¯­å¢ƒçš„ç»†å¾®å·®åˆ«ã€‚
- en: Now that you know these two approaches, give it a go for your text categorisation
    task! ğŸ˜ƒ
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¢ç„¶ä½ çŸ¥é“äº†è¿™ä¸¤ç§æ–¹æ³•ï¼Œå°è¯•ä¸€ä¸‹ä½ çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡å§ï¼ ğŸ˜ƒ
- en: ğŸ‘‰ **Donâ€™t forget to follow for more articles like this.** ğŸ¤—
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘‰ **åˆ«å¿˜äº†å…³æ³¨ä»¥è·å–æ›´å¤šç±»ä¼¼çš„æ–‡ç« ã€‚** ğŸ¤—
- en: ğŸš€ Hope you found this article helpful. Consider joining Medium using [my link](https://medium.com/@saedhussain/membership)
    for more great content from me and other amazing authors on this platform!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ å¸Œæœ›ä½ è§‰å¾—è¿™ç¯‡æ–‡ç« å¯¹ä½ æœ‰å¸®åŠ©ã€‚è€ƒè™‘ä½¿ç”¨[æˆ‘çš„é“¾æ¥](https://medium.com/@saedhussain/membership)åŠ å…¥Mediumï¼Œè·å–æˆ‘å’Œå…¶ä»–å¹³å°ä¸Šä¼˜ç§€ä½œè€…çš„æ›´å¤šç²¾å½©å†…å®¹ï¼
- en: '*Other articles by me:*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘çš„å…¶ä»–æ–‡ç« ï¼š*'
- en: '[](/how-to-extract-and-convert-tables-from-pdf-files-to-pandas-dataframe-cb2e4c596fa8?source=post_page-----3757c517bd65--------------------------------)
    [## How to Extract and Convert Tables From PDF Files to Pandas Dataframe?'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-extract-and-convert-tables-from-pdf-files-to-pandas-dataframe-cb2e4c596fa8?source=post_page-----3757c517bd65--------------------------------)
    [## å¦‚ä½•ä»PDFæ–‡ä»¶ä¸­æå–å¹¶è½¬æ¢è¡¨æ ¼åˆ°Pandasæ•°æ®æ¡†ï¼Ÿ'
- en: So you have some pdf files with tables in them and want to read them into a
    pandas data frame. Let me show you how.
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ æœ‰ä¸€äº›åŒ…å«è¡¨æ ¼çš„ PDF æ–‡ä»¶ï¼Œå¹¶ä¸”æƒ³å°†å®ƒä»¬è¯»å–åˆ°ä¸€ä¸ª pandas æ•°æ®æ¡†ä¸­ã€‚è®©æˆ‘æ¥å±•ç¤ºç»™ä½ çœ‹ã€‚
- en: towardsdatascience.com](/how-to-extract-and-convert-tables-from-pdf-files-to-pandas-dataframe-cb2e4c596fa8?source=post_page-----3757c517bd65--------------------------------)
    [](/how-to-schedule-a-serverless-google-cloud-function-to-run-periodically-249acf3a652e?source=post_page-----3757c517bd65--------------------------------)
    [## How to Schedule a Serverless Google Cloud Function to Run Periodically
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[å¦‚ä½•ä» PDF æ–‡ä»¶ä¸­æå–å¹¶è½¬æ¢è¡¨æ ¼åˆ° pandas æ•°æ®æ¡†](https://towardsdatascience.com/how-to-extract-and-convert-tables-from-pdf-files-to-pandas-dataframe-cb2e4c596fa8?source=post_page-----3757c517bd65--------------------------------)
    [](/how-to-schedule-a-serverless-google-cloud-function-to-run-periodically-249acf3a652e?source=post_page-----3757c517bd65--------------------------------)
    [## å¦‚ä½•è°ƒåº¦æ— æœåŠ¡å™¨ Google Cloud å‡½æ•°ä»¥å®šæœŸè¿è¡Œ'
- en: Do you have some code that needs to be run regularly?
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½ æ˜¯å¦æœ‰ä¸€äº›ä»£ç éœ€è¦å®šæœŸè¿è¡Œï¼Ÿ
- en: towardsdatascience.com](/how-to-schedule-a-serverless-google-cloud-function-to-run-periodically-249acf3a652e?source=post_page-----3757c517bd65--------------------------------)
    [](/how-to-develop-and-test-your-google-cloud-function-locally-96a970da456f?source=post_page-----3757c517bd65--------------------------------)
    [## How to Develop and Test Your Google Cloud Function Locally
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[å¦‚ä½•å®šæœŸè°ƒåº¦æ— æœåŠ¡å™¨ Google Cloud å‡½æ•°](https://towardsdatascience.com/how-to-schedule-a-serverless-google-cloud-function-to-run-periodically-249acf3a652e?source=post_page-----3757c517bd65--------------------------------)
    [](/how-to-develop-and-test-your-google-cloud-function-locally-96a970da456f?source=post_page-----3757c517bd65--------------------------------)
    [## å¦‚ä½•åœ¨æœ¬åœ°å¼€å‘å’Œæµ‹è¯•ä½ çš„ Google Cloud å‡½æ•°'
- en: So, you have written your serverless cloud function, but donâ€™t want to waste
    time deploying it and hoping it works. Letâ€¦
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½ å·²ç»ç¼–å†™äº†æ— æœåŠ¡å™¨äº‘å‡½æ•°ï¼Œä½†ä¸æƒ³æµªè´¹æ—¶é—´éƒ¨ç½²å®ƒå¹¶å¸Œæœ›å®ƒèƒ½æ­£å¸¸å·¥ä½œã€‚è®©â€¦
- en: towardsdatascience.com](/how-to-develop-and-test-your-google-cloud-function-locally-96a970da456f?source=post_page-----3757c517bd65--------------------------------)
    [](/machine-learning-model-as-a-serverless-endpoint-using-google-cloud-function-a5ad1080a59e?source=post_page-----3757c517bd65--------------------------------)
    [## Machine Learning Model as a Serverless Endpoint using Google Cloud Functions
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[å¦‚ä½•åœ¨æœ¬åœ°å¼€å‘å’Œæµ‹è¯•ä½ çš„ Google Cloud å‡½æ•°](https://towardsdatascience.com/how-to-develop-and-test-your-google-cloud-function-locally-96a970da456f?source=post_page-----3757c517bd65--------------------------------)
    [](/machine-learning-model-as-a-serverless-endpoint-using-google-cloud-function-a5ad1080a59e?source=post_page-----3757c517bd65--------------------------------)
    [## ä½¿ç”¨ Google Cloud Functions å°†æœºå™¨å­¦ä¹ æ¨¡å‹ä½œä¸ºæ— æœåŠ¡å™¨ç«¯ç‚¹'
- en: So you have built a model and want to productionize it as a serverless solution
    on google cloud platform (GCP). Let meâ€¦
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½ å·²ç»æ„å»ºäº†ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸”æƒ³å°†å®ƒä½œä¸ºæ— æœåŠ¡å™¨è§£å†³æ–¹æ¡ˆåœ¨ Google Cloud Platform (GCP) ä¸ŠæŠ•å…¥ç”Ÿäº§ã€‚è®©æˆ‘æ¥â€¦
- en: towardsdatascience.com](/machine-learning-model-as-a-serverless-endpoint-using-google-cloud-function-a5ad1080a59e?source=post_page-----3757c517bd65--------------------------------)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨ Google Cloud Functions å°†æœºå™¨å­¦ä¹ æ¨¡å‹ä½œä¸ºæ— æœåŠ¡å™¨ç«¯ç‚¹](https://towardsdatascience.com/machine-learning-model-as-a-serverless-endpoint-using-google-cloud-function-a5ad1080a59e?source=post_page-----3757c517bd65--------------------------------)'
