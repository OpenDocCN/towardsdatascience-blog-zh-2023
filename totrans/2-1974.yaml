- en: Text Correction Using NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NLP进行文本纠正
- en: 原文：[https://towardsdatascience.com/text-correction-using-nlp-b68c7233b86](https://towardsdatascience.com/text-correction-using-nlp-b68c7233b86)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/text-correction-using-nlp-b68c7233b86](https://towardsdatascience.com/text-correction-using-nlp-b68c7233b86)
- en: '**Detecting and correcting common errors: problems and methods**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**检测和纠正常见错误：问题与方法**'
- en: '[](https://jagota-arun.medium.com/?source=post_page-----b68c7233b86--------------------------------)[![Arun
    Jagota](../Images/3c3eb142f671b5fb933c2826d8ed78d9.png)](https://jagota-arun.medium.com/?source=post_page-----b68c7233b86--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b68c7233b86--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b68c7233b86--------------------------------)
    [Arun Jagota](https://jagota-arun.medium.com/?source=post_page-----b68c7233b86--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jagota-arun.medium.com/?source=post_page-----b68c7233b86--------------------------------)[![Arun
    Jagota](../Images/3c3eb142f671b5fb933c2826d8ed78d9.png)](https://jagota-arun.medium.com/?source=post_page-----b68c7233b86--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b68c7233b86--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b68c7233b86--------------------------------)
    [Arun Jagota](https://jagota-arun.medium.com/?source=post_page-----b68c7233b86--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b68c7233b86--------------------------------)
    ·19 min read·Jan 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b68c7233b86--------------------------------)
    ·19分钟阅读·2023年1月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bcadaafc6c2554d8c8a66ada008560c7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcadaafc6c2554d8c8a66ada008560c7.png)'
- en: Image by [Lorenzo Cafaro](https://pixabay.com/users/3844328-3844328/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1870721)
    from [Pixabay](https://pixabay.com/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Lorenzo Cafaro](https://pixabay.com/users/3844328-3844328/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1870721)
    提供，来源于 [Pixabay](https://pixabay.com/)
- en: Anyone who writes text will miss a comma here and there. Or use the wrong preposition
    in a certain context. Or make spelling errors. Or phrase awkwardly. Or use overly
    complicated or excessively long sentences. Or overly long paragraphs. Or ramble
    on excessively.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 任何写作的人都会不时漏掉一个逗号。或者在某种语境下使用错误的介词。或者拼写错误。或者措辞尴尬。或者使用过于复杂或过长的句子。或者段落过长。或者过于冗长。
- en: On all but the shortest of writeups, perhaps all of the above. And more.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于除了最短的写作之外的所有文本，也许以上所有内容以及更多。
- en: I had a student who was missing articles such as *a* or *the* in everything
    he wrote. I read hundreds of his pages. Didn’t find a single article.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经有一个学生在他写的每一篇文章中都缺少了像*a*或*the*这样的冠词。我阅读了他数百页的内容，却没有找到一个冠词。
- en: I’ve made — and continue to make — all such errors repeatedly. Even in my short
    writeups such as emails.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经并且仍然不断地犯这些错误。即使是在我的短文如电子邮件中。
- en: For content that is quite elaborate, such as entire books or even short blog
    posts, textual issues of course will be much more prolific. This is why we have
    copy editors, whose responsibilities include proofreading and editing content.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内容较为复杂的文本，如整本书籍或甚至短篇博客，文本问题自然会更多。这就是我们需要校对编辑的原因，他们的职责包括校对和编辑内容。
- en: This is also why NLP-based tools such as Grammarly are becoming increasingly
    popular. These tools can help one find and correct such errors within a matter
    of minutes in short texts such as emails. On longer texts, they’ll likely find
    more, which of course means they will take more time to fix.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是为什么基于NLP的工具如Grammarly越来越受欢迎的原因。这些工具可以在几分钟内帮助人们在短文本如电子邮件中发现并纠正这些错误。对于较长的文本，它们可能会发现更多错误，这当然意味着修正这些错误会花费更多时间。
- en: For more on Grammarly, see [10].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Grammarly的更多信息，请参见 [10]。
- en: Regardless, there is no way a writer can compete with such tools on the measure
    of increased throughput of producing quality writing. And I might add, with much
    less eye strain.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，作家无法与这些工具在提高高质量写作产量方面竞争。我还要补充一点，这样可以减少眼睛的疲劳。
- en: This brings to mind the following. A long, long time ago I wrote a Ph.D. thesis.
    Hundreds of pages. It was painful. I would not repeat the process now without
    a tool such as Grammarly.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我想起了以下的事情。很久很久以前，我写了一篇博士论文。数百页。那是一个痛苦的过程。如果没有像Grammarly这样的工具，我现在不会重复这个过程。
- en: '**Preamble**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**前言**'
- en: In this post, we will first describe and explain the various types of errors
    that people tend to make when they write. We will limit ourselves to basic errors
    such as missing commas, missing articles, or using the wrong preposition. (In
    this post, we use the term “error” in a somewhat soft sense. We really mean “suggestions
    for improvement”.)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们首先描述并解释人们在写作时倾向于犯的各种错误。我们将仅限于基本错误，例如缺少逗号、缺少冠词或使用错误的介词。（在这篇文章中，我们用“错误”这个术语时比较宽松。我们实际上是指“改进建议”。）
- en: Following that, we will brainstorm specific ways using NLP to detect such issues
    and offer solutions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们将集思广益，使用自然语言处理技术检测这些问题，并提供解决方案。
- en: We have chosen to limit the scope of this post to basic errors for the following
    reasons. First, they are very common. Second, some basic methods from statistical
    natural language processing, combined with some feature engineering, lend themselves
    to their detection and correction.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择将这篇文章的范围限制在基本错误的原因有几个。首先，它们非常常见。其次，一些统计自然语言处理的基本方法，再加上一些特征工程，适用于这些错误的检测和纠正。
- en: So in this process, the reader will also get a firm grounding on basic methods
    in statistical NLP in a useful and real setting.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个过程中，读者还将获得在有用且真实的环境中对统计自然语言处理基本方法的坚实基础。
- en: By contrast, detecting and offering corrections for more elaborate issues such
    as awkward phrasing or reexpressing something more concisely or in a way that
    reads much better requires more advanced NLP. In a separate post [11], in which
    we also model *context*, we go beyond the basic issues we cover in this post.
    This still does not cover awkward phrasing or expressing something more concisely
    or informatively as those are even more complex topics.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，检测并提供针对更复杂问题的纠正建议，如措辞尴尬或以更简洁、更易读的方式重新表达，需要更先进的自然语言处理技术。在另一篇文章[11]中，我们也建模了*上下文*，超出了这篇文章所涵盖的基本问题。这仍然没有涵盖措辞尴尬或更简洁、更具信息性的表达，因为这些都是更复杂的话题。
- en: '**Basic Errors In Text**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本中的基本错误**'
- en: Here is what we will cover, with realistic examples.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们将涵盖的内容，并附有实际示例。
- en: Missing commas.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺少逗号。
- en: Missing articles.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺少冠词。
- en: Leaving out the apostrophe in It’s.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忘记在“It's”中使用撇号。
- en: '*Using singular instead of plural, or vice-versa.*'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用单数而非复数，或反之亦然。*'
- en: Using a hyphen when it shouldn’t be used, or vice-versa.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不应该使用连字符的地方使用了连字符，或反之亦然。
- en: 'Casing words: *Not capitalizing a letter when one should;* Not having a word
    in all caps when one should. E.g., *fyi*.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小写问题：*未在应大写字母的地方大写；* 在应全大写的地方没有全大写。例如，*fyi*。
- en: '*Using the wrong preposition, or using one when it is not needed.*'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用错误的介词，或在不需要时使用介词。*'
- en: Following this, we will take a second pass over them in which we discuss basic
    solutions from statistical NLP. This discussion will involve what training sets
    to use, what features to extract, and what statistical models to use.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对这些错误进行第二次讨论，探讨统计自然语言处理的基本解决方案。这一讨论将涉及使用哪些训练集、提取哪些特征，以及使用哪些统计模型。
- en: We will not even attempt to solve the issues in the above list highlighted in
    italics. These issues will need more advanced methods from NLP, ones that model
    context.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至不会尝试解决上述以斜体标出的那些问题。这些问题需要更先进的自然语言处理方法，即那些能建模上下文的方法。
- en: '**Missing Commas**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺少逗号**'
- en: The most frequent error I make is missing a comma when there should be one.
    Consider
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我犯的最常见的错误是缺少逗号。考虑一下：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In each case, there should be a comma immediately after the first word.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，第一个单词后面都应该立即跟一个逗号。
- en: '**Casing Words**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**大小写问题**'
- en: Sometimes one forgets to capitalize the first letter in the word that begins
    a sentence, or the ‘i’ when referring to oneself. As in
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有时会忘记在句子的开头单词中大写第一个字母，或在指代自己时大写‘i’。例如：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This conveys a poor impression.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这会给人留下不好的印象。
- en: Similarly,
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: reads better as
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的表达方式是：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There are numerous other examples in which the all-upper case is preferred.
    Such as *pdf*, *gpx*, *cdc*, *nlp*, *ai*, …
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他情况，其中全大写字母的形式更为适合。例如*pdf*、*gpx*、*cdc*、*nlp*、*ai*等。
- en: '**Missing Articles**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺少冠词**'
- en: It’s also common to omit articles. As in
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 遗漏冠词也很常见。如：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The text before the ⇒ is what is written. The text after the ⇒ is how it should
    be written. We will follow this convention in depicting examples throughout this
    post.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ⇒之前的文本是原文。⇒之后的文本是正确的写法。我们将在整篇文章中使用这种约定来展示示例。
- en: Another frequent error I make is to leave out the apostrophe in *Its*. In fact,
    I made this error on the *Its* that begins this section!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我经常犯的错误是省略了*Its*中的撇号。实际上，我在本节开始的*Its*上犯了这个错误！
- en: '**Using Hyphens Incorrectly**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**错误使用连字符**'
- en: I’ve made such errors frequently. Here are some examples from a nice post [2].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常犯这样的错误。以下是来自一个不错的帖子中的一些例子[2]。
- en: At this stage, I’ll just show the examples. Later on in this post, when we look
    at methods we’ll bring in some of the additional points in [2]. They will help
    us with engineering the right features or with deciding which method from statistical
    NLP we should use.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我只是展示这些例子。稍后当我们查看方法时，我们会引入一些[2]中的额外点。它们将帮助我们设计正确的特征或决定我们应该使用统计NLP中的哪种方法。
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: And these.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以及这些。
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s see a few examples in the opposite direction. In these, we should not
    be using hyphens.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些相反方向的例子。在这些例子中，我们不应使用连字符。
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Errors Involving Prepositions**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**介词使用错误**'
- en: I make errors involving prepositions quite frequently. Specifically, I use the
    wrong preposition. For instance, I often use *by* when I should be using *with*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常犯涉及介词的错误。特别是，我使用了错误的介词。例如，我经常使用*by*而应使用*with*。
- en: And were I not using Grammarly, I wouldn’t even know I was doing so. Even after
    reviewing my text.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我没有使用Grammarly，我甚至不会知道自己在做这些事。即使在审阅我的文本之后也是如此。
- en: Below is the first example I want to share. It's not specific to propositions
    but does illustrate the point I am trying to make.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我想分享的第一个例子。它不是特定于介词的，但确实说明了我想表达的观点。
- en: While writing the above sentence. I wrote it as
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在写上述句子时，我写成了
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Grammarly suggested that I drop *to* and *be*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Grammarly建议我去掉*to*和*be*。
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: reads better.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 更容易阅读。
- en: Oh, in fact, I just realized that there were two additional errors, just in
    the above few lines. Which are now fixed. Here are the versions with the errors
    in them.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，实际上，我刚意识到在上述几行中还有两个额外的错误。现在已经修正。以下是包含错误的版本。
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the first sentence, I am missing the *to* between *want* and *share*. In
    the second sentence, *Its* should be *It’s* or *It is*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一句中，我在*want*和*share*之间漏掉了*to*。在第二句中，*Its*应该是*It’s*或*It is*。
- en: And one more thing.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事。
- en: I just realized that after writing *Oh, in fact* … I introduced a couple of
    additional errors!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚意识到，在写了*Oh, in fact*之后……我又引入了一些额外的错误！
- en: Well, I’ll stop here as I could go on forever, in an infinite loop of generating
    new text with errors in it to explain the errors I made in the previous version
    of my text!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我就停在这里，否则我可能会无限循环，生成带有错误的新文本来解释我在前一版本中犯的错误！
- en: Okay, let’s see other examples.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，让我们看看其他例子。
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Clearly, the writer meant *into* in this context. (Should the “*from the living
    room”* have been missing, it would be less clear-cut.)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，作者在这个上下文中指的是*into*。 （如果“*from the living room*”被遗漏，可能就不那么明确了。）
- en: The one below is from [1].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自[1]的例子。
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The *of* doesn’t need to be there.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*of*是不需要的。'
- en: Previously I mentioned that I often use *by* when I should be using *with*.
    Here is an example.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到过，我经常使用*by*而应使用*with*。这里有一个例子。
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the above, *linear by sigmoidal* should be *linear with sigmoidal*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述例子中，*linear by sigmoidal*应为*linear with sigmoidal*。
- en: '**Towards Solutions**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**'
- en: If we had access to a large language model such as ChatGPT [3] and could run
    it on-demand or in batch mode, we probably wouldn’t need to proceed piece-meal
    as below.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够访问像ChatGPT这样的巨大语言模型[3]并能够按需或批量运行，我们可能就不需要像下面这样一点一点地进行。
- en: We assume the reader doesn’t have such access. Moreover, we assume the reader
    is interested in learning about the methods as they relate to these use cases
    and possibly in implementing them from scratch.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设读者没有这样的访问权限。此外，我们假设读者对了解这些用例相关的方法感兴趣，并可能想从头开始实现它们。
- en: As the interested reader will see later, the methods discussed in this post
    will be very easy to implement from scratch.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如感兴趣的读者稍后将看到的，这篇帖子中讨论的方法将非常容易从头开始实现。
- en: Using pre-built large models would not provide any such insights. That said,
    it is a great idea to try out a large language model if one can gain access to
    it. To experience how it behaves. To observe what sorts of problems it is able
    to solve. To assess, in the context of this post, in what ways its solutions are
    better than ours?
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预先构建的大模型不会提供任何这样的见解。也就是说，如果可以获得大型语言模型，尝试一下是个好主意。体验它的行为。观察它能够解决哪些问题。在本文的背景下，评估其解决方案在什么方面优于我们的？
- en: Okay, back to the discussion on our specific suggestions. First, we will discuss
    what dataset to use for training. A single judiciously chosen data set will suffice
    for all our use cases.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，回到我们具体建议的讨论。首先，我们将讨论用于训练的数据集。一个明智选择的数据集足以满足我们所有的用例。
- en: Next, we will discuss a handful of “building block” methods from statistical
    NLP. We will start by developing them on a few initial use cases. We will then
    discuss how these same methods are applicable to many of the other use cases we
    described earlier in this post. Albeit with some different preprocessing or features
    extracted.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论一些来自统计自然语言处理的“基础构建块”方法。我们将从几个初步用例开始开发这些方法。然后我们将讨论这些相同的方法如何适用于我们在本文早些时候描述的许多其他用例，尽管需要不同的预处理或提取的特征。
- en: That said, there are certain use cases that we described in some detail earlier
    in this post for which we will not even attempt to discuss solutions. This is
    because they seem to need more advanced methods that take context into account.
    That is, elaborate language models. We will cover these in a future post.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，本文早些时候详细描述的一些用例，我们将不会尝试讨论解决方案。这是因为这些用例似乎需要考虑上下文的更高级方法，即复杂的语言模型。我们将在未来的文章中讨论这些。
- en: '**Training Data**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练数据**'
- en: The models we need for all the use cases discussed in this post can be learned
    in principle from a single data set. A corpus of text documents that are of reasonable
    quality, writing-wise.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本文中讨论的所有用例所需的模型原则上可以从一个数据集中学习。一个质量合理的文本文档语料库。
- en: Such as Wikipedia. In fact, one can download the entire text from Wikipedia
    onto one’s computer. See [4].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 比如维基百科。事实上，人们可以将维基百科的整个文本下载到自己的计算机上。参见[4]。
- en: For some of the problems, even manually copying a few pages from Wikipedia or
    from some reasonable quality page on the web will suffice for quick initial training
    and evaluation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些问题，即使手动复制几页维基百科或从一些合理质量的网页中复制内容，也足以进行快速的初步训练和评估。
- en: '**Simple Count-based Methods**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**简单计数方法**'
- en: Consider trying to detect sentences or paragraphs that are excessively long.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑尝试检测过长的句子或段落。
- en: We’ll train methods for such detections as follows. First, we will tokenize
    the text in each document into paragraphs, then tokenize each paragraph into sentences,
    and finally tokenize each sentence into its words.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以如下方式训练这些检测方法。首先，我们将文本中的每个文档标记化为段落，然后将每个段落标记化为句子，最后将每个句子标记化为词语。
- en: A paragraph may be tokenized into its sentences using NLTK. See [5].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用NLTK将段落标记化为句子。参见[5]。
- en: Tokenizing a document into its paragraphs, and paragraphs into its sentences
    are in themselves somewhat interesting NLP problems. We will cover methods for
    them in a separate post. Here our focus lies elsewhere.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将文档分解为段落，再将段落分解为句子本身也是相当有趣的自然语言处理问题。我们将在另一篇文章中涵盖这些方法。这里我们的重点在其他地方。
- en: Now we can build models of the numbers of words in sentences and in paragraphs
    respectively. Using these models we can flag unusually long sentences or unusually
    long paragraphs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以分别构建句子和段落中的单词数模型。利用这些模型，我们可以标记异常长的句子或异常长的段落。
- en: '**Parametric Models Of Length Distributions**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**长度分布的参数模型**'
- en: The main additional thing we’d like to say on this topic is that the normal
    distribution is not a good one for modeling sentence or paragraph lengths.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个主题上想要补充的主要内容是，正态分布并不是建模句子或段落长度的一个好选择。
- en: The normal distribution is symmetric with tails at negative infinity and positive
    infinity. By contrast, sentence lengths are more likely to be distributed as follows.
    Single-word sentences, while they do exist, are relatively rare. As we increase
    the length, sentences of that length will start becoming more common. As we increase
    the sentence length further, sentences of that length will start becoming less
    common. Exponentially so.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布是对称的，尾部延伸至负无穷大和正无穷大。相比之下，句子长度更可能按以下方式分布。单词句子虽然存在，但相对较少。随着长度的增加，这种长度的句子会变得更常见。随着句子长度的进一步增加，这种长度的句子会变得不那么常见，呈指数级减少。
- en: The reader may test this reasoning out quickly by scrolling up and down this
    post and eye-balling the lengths of the various sentences. The reader will find
    a handful of two-word sentences. Most sentences will contain between five to ten
    words. The reader won’t find any sentences containing, say thirty words. These
    are just too long.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可以通过快速上下滚动这篇文章并目测各种句子的长度来测试这一推理。读者会发现一些两词句子。大多数句子包含五到十个词。读者不会找到包含三十个词的句子。这些句子太长了。
- en: Okay, back to characterizing the distribution we want. It should look like this.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们回到我们想要表征的分布。它应该是这样的。
- en: '![](../Images/7ac072e7caf2247dc759acc747dc7879.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ac072e7caf2247dc759acc747dc7879.png)'
- en: The Poisson distribution fits this shape better than the Normal distribution.
    See [6].
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 泊松分布比正态分布更适合这种形状。见[6]。
- en: The Poisson distribution has a parameter, i.e. a tunable knob, which can fine-tune
    the shape further. This parameter can be tuned from the data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 泊松分布有一个参数，即一个可调节的旋钮，可以进一步微调形状。这个参数可以从数据中进行调节。
- en: Also, see [7], which is a paper devoted to the topic of modeling distributions
    of lengths of sentences in text. As discussed there, the lognormal distribution
    also merits consideration.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另见[7]，这是一篇专注于文本中句子长度分布建模的论文。正如文中讨论的那样，对数正态分布也值得考虑。
- en: '**Non-parametric Models Of Length Distributions**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**长度分布的非参数模型**'
- en: Say our corpus contains lots of sentences. We can avoid making any assumptions
    about the distribution’s form. Should it be Poisson? Should it be Pareto? Should
    it be lognormal? Some other?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的语料库包含大量句子。我们可以避免对分布形式做任何假设。它应该是泊松分布？还是帕累托分布？还是对数正态分布？或者其他什么？
- en: Simply estimate it empirically from the data. That is, essentially just create
    a histogram over the various sentence lengths. It would like something like this.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中经验性地估计。这就是说，基本上只是对各种句子长度创建一个直方图。它会像这样。
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This says that there were 10 three-word sentences in our corpus and 500 eight-word
    ones.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们语料库中有10个三词句和500个八词句。
- en: The histogram may need some smoothing. Both interpolation and extrapolation.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图可能需要一些平滑处理，包括插值和外推。
- en: To see the need for interpolation, consider this scenario. In our corpus, say
    there is one occurrence of a 50-word sentence but none of a 45-word one. (Never
    mind how the 40-word sentence got in there.) We wouldn’t want to say that a 45-word
    sentence has a zero probability of occurring. Sure small, but not zero.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明插值的必要性，请考虑以下场景。在我们的语料库中，假设有一个50词的句子出现，但没有45词的句子。（不用在意40词的句子怎么来的。）我们不想说45词的句子出现的概率为零。虽然很小，但不是零。
- en: Okay, back to the histogram discussion.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们回到直方图的讨论。
- en: A histogram, suitably smoothed, may then be used to score a new sentence for
    how unusual its length is. An unusually long sentence should score very low.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经过适当平滑的直方图可以用来为新句子打分，以评估其长度的异常程度。一个异常长的句子应该得分很低。
- en: The smoothed histogram has all the information in it for such scoring. We can
    call this “*P*-value or percentile scoring”. If for instance, 99% of the sentences
    in our corpus are no more than twenty words long, the *P*-value of a 21-word sentence
    is no more than 0.01, i.e. 1%. This could be expressed as a score in percentile
    units. *1* would mean a very low score.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑的直方图包含了这种评分所需的所有信息。我们可以称之为“*P*值或百分位评分”。例如，如果我们语料库中的99%的句子不超过二十个词，那么一个21词的句子的*P*值不超过0.01，即1%。这可以表示为百分位单位的评分。*1*将意味着非常低的评分。
- en: To keep the description of the above paragraph simple enough, we are glossing
    over details such as *right*-tail *P*-values versus *both*-tail ones. In practice,
    this does not matter much unless the score thresholds are based on *P*-value cutoffs.
    Typically in practice, they are not. Instead, they are calibrated based on where
    we think the cutoffs should be. That is, we ask people which sentences they deem
    to be too long, and derive a score cutoff from such feedback.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持以上段落的描述足够简单，我们忽略了如*右*尾*P*值与*双*尾*P*值的细节。在实际操作中，这并不太重要，除非评分阈值基于*P*值的截止点。实际操作中，通常并不是这样。相反，它们是基于我们认为截止点应该在何处来校准的。也就是说，我们询问人们认为哪些句子太长，并从这些反馈中得出评分截止点。
- en: '**Methods Involving Token-specific Distributions**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及特定令牌分布的方法**'
- en: Consider the problem involving hyphens that we described earlier in the post.
    That is should there be a hyphen or not joining certain adjacent words? If not,
    should there be a space or should the words be fused?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下我们在文章中描述的涉及连字符的问题。即是否应该在某些相邻的词之间加连字符？如果不加连字符，是否应该有空格，还是将词融合在一起？
- en: Let’s start by repeating the examples we saw earlier, to better understand the
    nature of the problem.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从重复之前看到的例子开始，以更好地理解问题的本质。
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As a first attempt, it does seem that even without modeling context we could
    do a reasonable job at detecting suspect hyphenation and offer sensible alternatives.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一次尝试，即使在没有建模上下文的情况下，我们也能够在检测可疑连字符方面做到相当不错，并提供合理的替代方案。
- en: Clearly, we don’t need either perfect precision or perfect recall. Just enough
    so the user finds value. We can always iterate and improve.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们不需要完美的精度或完美的召回率。只需足够让用户找到价值。我们可以始终进行迭代和改进。
- en: We’d like to note that we should focus on the precision of the detection much
    more than the quality of the alternatives proposed. This is because there are
    only three possibilities — hyphenate, use white space, or glue — for the adjacent
    words. So if the detected hyphenation is indeed poor, even showing both of the
    other two provides value to the user.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想指出，我们应该更加关注检测的精度，而不是提出的替代方案的质量。这是因为对于相邻的词，只有三种可能性——加连字符、使用空格或将词粘合在一起。所以如果检测到的连字符确实很差，即使展示其他两种方案也能为用户提供价值。
- en: So we will proceed to model the problem as follows. First, let’s consider the
    case of a token that was found to have a hyphen in it. Such as *heart-broken*.
    When we encounter this token during the training process, we will do the following.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将按以下方式建模该问题。首先，让我们考虑一个发现有连字符的词，例如*heart-broken*。当我们在训练过程中遇到这个词时，我们将进行如下操作。
- en: We will create a new token with the hyphen dropped. In our example, it would
    be *heartbroken*. We would then add an instance of *heartbroken* → *heart-broken*
    to a map every time we encounter *heart-broken* in the text. In python pseudocode,
    it would look something like this
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个去掉连字符的新词。在我们的例子中，它将是*heartbroken*。每次在文本中遇到*heart-broken*时，我们会将*heartbroken*
    → *heart-broken*的实例添加到一个映射中。在Python伪代码中，它会像这样：
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: When (and if) we encounter *heartbroken* in the corpus (as opposed to *heart-broken*)
    we would do the following
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在语料库中遇到*heartbroken*（而不是*heart-broken*）时，我们会进行如下操作：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: So once the training has finished, *style_map*[‘*heartbroken*’] will have the
    distribution over the two versions *heart-broken* and *heartbroken*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 所以一旦训练完成，*style_map*[‘*heartbroken*’]将会有两个版本*heart-broken*和*heartbroken*的分布。
- en: So if *P*(*heartbroken*|*heartbroken*) is much higher than *P*(*heart*-*broken*|*heartbroken*)
    we would be inclined to flag *heart-broken* as suspect and offer *heartbroken*
    as a suggested improvement.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果 *P*(*heartbroken*|*heartbroken*) 远高于 *P*(*heart*-*broken*|*heartbroken*)，我们会倾向于将*heart-broken*标记为可疑，并建议使用*heartbroken*作为改进建议。
- en: If the word during scoring is expressed as *heart-broken*, then we would first
    drop the hyphen, the same way as we did during training. If the resulting key
    does not match the most likely rewrite, we would flag that instance as suspect.
    In our case, that is what would happen since the most likely rewrite for *heartbroken*
    is *heartbroken* itself. But the text had it as *heart-broken*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在评分过程中，单词表示为*heart-broken*，那么我们会首先去掉连字符，就像我们在训练过程中所做的一样。如果结果键与最可能的重写不匹配，我们会将该实例标记为可疑。在我们的例子中，这将发生，因为*heartbroken*的最可能重写是*heartbroken*本身。但是文本中却是*heart-broken*。
- en: Now let’s consider how we would model adjacent words in the training corpus.
    Not hyphenated. Such as *boat house*. Does it read better as *boat house*, *boathouse*,
    or *boat-house*?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一下如何在训练语料库中建模相邻的词。不加连字符。例如*boat house*。它的表达更好是*boat house*、*boathouse*还是*boat-house*？
- en: To cover such cases, we just need some additional logic as below.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了覆盖这种情况，我们只需要一些额外的逻辑，如下所示。
- en: Consider one instance of two adjacent words that appear in the corpus and say
    they happen to be *boat house*. We will derive a new token *boathouse* that fuses
    the two and add an instance of *boat house* to *boathouse* as the key. As depicted
    below.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以两个相邻的词*boat house*为例，我们将衍生出一个新词*boathouse*，将这两个词融合在一起，并将*boat house*作为键添加到*boathouse*中。如下所示。
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We would expect that *boathouse* would occur much more frequently than *boat
    house* in the corpus. That is, *P*(*boathouse*|*boathouse*) would be much larger
    than *P*(*boat house*|*boathouse*).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会期望在语料库中*boathouse*的出现频率远高于*boat house*。也就是说，*P*(*boathouse*|*boathouse*)将远大于*P*(*boat
    house*|*boathouse*)。
- en: '**Generalization in Hyphenation Modeling**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**连字符建模中的概化**'
- en: By generalization, we mean learning rules involving hyphenation that go beyond
    the specific instances we encounter in our training data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓概化，是指学习涉及连字符的规则，这些规则超出了我们在训练数据中遇到的具体实例。
- en: Read on to see specific types of generalization in this setting.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 继续阅读，以查看在这种情况下的具体概化类型。
- en: '**Generalizations Involving Specific Prefixes**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及特定前缀的概化**'
- en: From the training corpus, we might observe that the word *very* is never followed
    by a hyphen. So
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练语料库中，我们可能会观察到单词*very*后面从未跟随连字符。因此
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: is incorrect. It should be *very happy*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 是不正确的。应该是*very happy*。
- en: This example, and more generally this rule, are from [2].
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子，或者更一般地说，这个规则，来自[2]。
- en: Can we essentially learn this rule from the data itself?
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否从数据本身学习到这个规则？
- en: Yes. Here’s how.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。这里是方法。
- en: We will use a second map which we will call *hyphenation_prefix_map*. This map’s
    keys will be prefixes. Every time we see this prefix in the training corpus with
    a hyphen immediately following it, we will increment the count of value “1” by
    one. Every time we see this prefix in the training corpus without a hyphen immediately
    following it, we will increment the count of value “0” instead.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用第二个映射，称为*hyphenation_prefix_map*。这个映射的键将是前缀。每次我们在训练语料库中看到这个前缀后面紧跟连字符时，我们将把值“1”的计数加一。每次我们在训练语料库中看到这个前缀后面没有连字符时，我们将把值“0”的计数加一。
- en: 'The logic in the above paragraph looks in pseudocode as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 上述段落中的逻辑伪代码如下：
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now let’s illustrate what we do when we see *very-happy* in the text being scored.
    Assume that from the corpus we have learned that
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们展示当我们在评分文本中看到*very-happy*时的处理方法。假设我们从语料库中学到
- en: '*P*(next character is -|current word is *very*)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(下一个字符是-|当前单词是*very*)'
- en: is zero, or nearly so. We will flag *very-happy* as being suspect and suggest
    dropping the hyphen.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 是零，或者几乎为零。我们会将*very-happy*标记为可疑，并建议去掉连字符。
- en: Let’s also note that this solution also covers rules that are described further
    downstream in [2].
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，这个解决方案还涵盖了在[2]中进一步描述的规则。
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Below are examples, also from [2].
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是示例，也来自[2]。
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In fact, our data-driven learning will do a better job than following this rule
    100% of the time. Let’s elaborate.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们的数据驱动学习将比100%遵循这个规则做得更好。让我们详细说明一下。
- en: There are words that begin with *self* but don’t have a hyphen following the
    prefix. For example, *selfish*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 有些以*self*开头的单词没有连字符。例如，*selfish*。
- en: So our model will learn both the general rule
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们的模型将学习到通用规则
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: as well as exceptions such as *selfish*. so long as they appear in the training
    corpus.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以及像*selfish*这样的例外，只要它们出现在训练语料库中。
- en: There is a little bit of tweaking to do to make sure that the rule “*selfish*
    stays *selfish”* fires and not the general rule. This is easy to do but we’ll
    leave it as an exercise to the reader.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 需要稍微调整一下，以确保规则“*selfish* stays *selfish*”能够触发，而不是通用规则。这很简单，但我们将把它留给读者作为练习。
- en: '**Generalizations Involving Numbers**'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**涉及数字的概化**'
- en: Consider these examples from [2].
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑来自[2]的这些示例。
- en: '[PRE24]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The hyphenation is correct in the first one. The hyphenation *four-year old*
    wouldn’t be. The absence of hyphenation in the second sentence is correct as well.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个连字符用法是正确的。*four-year old*的连字符用法则不正确。第二句话中缺少连字符的用法也是正确的。
- en: Say the term *four-year-old* appears frequently in the training corpus. Extending
    the method we described earlier to work with three adjacent tokens, we could detect
    that *four-year old* should really be *four-year-old*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 假设术语*four-year-old*在训练语料库中出现频繁。扩展我们之前描述的方法以处理三个相邻的词素，我们可以检测到*four-year old*实际上应该是*four-year-old*。
- en: Now, what if the text we want to score contains the term *hundred year old*?
    Say this exact term does not appear in the training corpus. We want to be able
    to flag it as suspect and suggest that the writer consider reexpressing it as
    *hundred-year-old*.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们要评分的文本包含术语*hundred year old*呢？假设这个确切的术语在训练语料库中没有出现。我们希望能够将其标记为可疑，并建议作者考虑将其重新表述为*hundred-year-old*。
- en: This is another example of generalization.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个概化的例子。
- en: Clearly, we want to be able to learn the pattern <*number*>-*year*-*old* from
    the particular instances that do appear in the corpus.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们希望能够从实际出现的实例中学习模式 <*number*>-*year*-*old*。
- en: If we see instances *1-year-old*, *2-year-old*, *one-year-old*, *two-year-old*,
    … in the corpus, from these we can reasonably surmise in principle the pattern
    <*number*>-*year*-*old*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在语料库中看到实例 *1-year-old*、*2-year-old*、*one-year-old*、*two-year-old* 等，从这些实例中我们可以合理推测模式为
    <*number*>-*year*-*old*。
- en: We will outline, at a high level, one way to approach this problem. We’ll illustrate
    it below.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将高层次地概述一种解决此问题的方法。我们将在下面进行说明。
- en: Consider the instance *2-year-old* in the corpus. As before we will update *stylemap*
    for the key *2yearold*. In addition to that, we will do the following. We will
    recognize *2* as a number using a named entity recognizer, possibly a simple one
    combining dictionary-based and regex-based approaches.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑语料库中的实例 *2-year-old*。如前所述，我们将更新 *stylemap* 对于键 *2yearold*。此外，我们还将进行以下操作。我们将使用命名实体识别器将
    *2* 识别为数字，可能是一个结合了基于词典和正则表达式的方法的简单工具。
- en: See [8] for a detailed post on named entity recognition in NLP that covers scenarios
    at this level. And ones that are much more elaborate.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见 [8]，获取关于 NLP 中命名实体识别的详细帖子，涵盖了这一层次的场景以及更复杂的场景。
- en: We can express what results as the key <*num*>-*year*-*old*. We update this
    key as before. That is,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以表达结果为键 <*num*>-*year*-*old*。我们像以前一样更新这个键。也就是说，
- en: '[PRE25]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now let’s see what we need to do when we see *100 year-old* in a text being
    scored. First, we strip off the hyphen and look up *100yearold* in *style_map*.
    Say it does not exist. Then we recognize *100* as <*num*> and try to lookup <*num*>*yearold*
    in *style_map*. It does exist as a key. Next, we find which styling has the highest
    probability and by a significant-enough margin. In our case, it would be <*num*>-*year*-*old*.
    Next, we substitute back *100* for <*num*>. Finally, we offer the result, *100-year-old*,as
    the suggested reexpression.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看当我们在文本中看到 *100 year-old* 时需要做什么。首先，我们去掉连字符，查找 *100yearold* 在 *style_map*
    中。假设它不存在。然后我们将 *100* 识别为 <*num*> 并尝试在 *style_map* 中查找 <*num*>*yearold*。它作为键存在。接下来，我们找到具有最高概率且差距足够大的样式。在我们的例子中，它将是
    <*num*>-*year*-*old*。接着，我们用 *100* 替换 <*num*>。最后，我们提供结果 *100-year-old* 作为建议的重表达。
- en: Next, consider the following example, also from [2].
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，考虑以下示例，亦来自 [2]。
- en: '[PRE26]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Replacing *fifty-seven* with *fifty seven* would be incorrect.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 用 *fifty seven* 替换 *fifty-seven* 将是不正确的。
- en: 'We can learn this pattern automatically from the data. For this, we would want
    to introduce a further distinction in our named entities: *spelled-out number*
    as distinct from *written-out number*. *Five* is a *spelled-out number*. *5* is
    a *written-out number*. Armed with this distinction we can learn the rule that'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从数据中自动学习这一模式。为此，我们希望在命名实体中引入进一步的区别：*拼写出的数字* 与 *书写出的数字*。*Five* 是 *拼写出的数字*。*5*
    是 *书写出的数字*。凭借这一区别，我们可以学习到规则是
- en: '[PRE27]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This approach can be refined to cover examples such as the one below, also from
    [2].
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以进一步完善，以涵盖如下所示的示例，也来自 [2]。
- en: '[PRE28]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '*two thirds* would be incorrect.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*two thirds* 将是不正确的。'
- en: '**Word Casing**'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**单词大小写**'
- en: This approach also works for certain scenarios in word casing. Such as *fyi*
    is better expressed as *FYI*. We just need to preprocess the token differently.
    This is illustrated below.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法也适用于某些单词大小写的场景。例如 *fyi* 更好地表示为 *FYI*。我们只需以不同的方式预处理标记。如下所示。
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: That is, when we encounter *FYI* in the corpus, first we lowercase it entirely,
    then add one more instance of *FYI* to be associated with *fyi*.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，当我们在语料库中遇到 *FYI* 时，首先我们将其全部小写，然后再添加一个 *FYI* 的实例，以便与 *fyi* 关联。
- en: Once the training has finished, assuming the corpus was sufficiently clean and
    sufficiently rich, *P*(*FYI*|*fyi*) should be much larger than *P*(*fyi*|*fyi*).
    So if we encountered *fyi* in the text being scored, we would suggest replacing
    it with *FYI*.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，假设语料库足够干净且丰富，*P*(*FYI*|*fyi*) 应该远大于 *P*(*fyi*|*fyi*)。因此，如果我们在被评分的文本中遇到
    *fyi*，我们将建议将其替换为 *FYI*。
- en: '**The Apostrophe In Its**'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**其中的撇号**'
- en: Say the first word in a particular sentence in the corpus is written as *Its*.
    It should really be *It’s*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 假设语料库中特定句子的第一个词是 *Its*。它实际上应该是 *It’s*。
- en: How can we model to detect such an error and recommend a specific fix?
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何建模以检测此类错误并推荐特定的修正？
- en: The same approach works here as well. With slightly different preprocessing.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的方法在这里也适用，只是预处理稍有不同。
- en: Consider an occurrence of *It’s* in the corpus. We would do the following.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑语料库中*It’s*的出现。我们将进行以下操作。
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: That is, we strip off the apostrophe giving us *Its*. We then add one more instance
    of *It’s* to be associated with *Its*.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们去掉撇号得到*Its*。然后，我们再添加一个*It’s*与*Its*相关联。
- en: Notice that in this case, we did not downcase *It’s* only stripped off the apostrophe.
    This is because we want to retain that the *i* in *I*t’s was in upper case.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种情况下，我们没有将*It’s*小写，只是去掉了撇号。这是因为我们希望保留*I*t’s中的*i*为大写。
- en: Once the training has finished, assuming the corpus was sufficiently clean and
    sufficiently rich, *P*(*It’s*|*Its*) should be much larger than *P*(*Its*|*Its*).
    So if we encountered *its* as the first word in a sentence being scored, we would
    first convert it to *Its*, then suggest replacing this *Its* with *It’s*.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，假设语料库足够干净且丰富，*P*(*It’s*|*Its*)应该比*P*(*Its*|*Its*)大得多。因此，如果我们遇到*its*作为句子中的第一个词被评分，我们将首先将其转换为*Its*，然后建议将这个*Its*替换为*It’s*。
- en: In fact, if we see *Its* as the first word in a sentence, we should recommend
    replacing it with either *It’s* or *It is*. We can extend our model to learn to
    offer the second suggestion as well in a straightforward way. We will leave this
    an exercise for the reader.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果我们将*Its*视为句子的第一个词，我们应该建议将其替换为*It’s*或*It is*。我们可以扩展我们的模型以直接学习提供第二个建议。我们将把这个问题留给读者作为练习。
- en: '**Missing Commas**'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺失的逗号**'
- en: Consider the examples we saw earlier.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们之前看到的示例。
- en: '[PRE31]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In each case, there should be a comma after the first word in the sentence.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，句子的第一个词后面应该有一个逗号。
- en: Below, we’ll dive into how to detect that a comma is missing in such scenarios.
    We will focus only on these scenarios, specifically ones in which the *first*
    word or two (or three) in a sentence should be followed by a comma.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们将深入探讨如何在这种场景中检测缺失的逗号。我们将仅关注这些场景，特别是那些在句子的*第一个*词或两个（或三个）词后面应该跟随逗号的情况。
- en: Detecting commas in more nuanced situations, i.e. somewhere in the middle of
    a sentence, is trickier. We will address this problem in a future post.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在更微妙的情况下，例如在句子中间的位置，检测逗号会更复杂。我们将在未来的帖子中解决这个问题。
- en: We’ll use the same method we used earlier for other use cases. Except that we
    will only apply it to a word that begins a sentence.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前在其他用例中使用的相同方法。不同的是，我们将仅应用于句子开头的词。
- en: For every word in the lexicon that begins a sentence in the training corpus,
    we will track how often it is followed by a comma versus not. After training,
    we can detect missing commas in the manner we described for our earlier use cases.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练语料库中每个以句子开头的词，我们将跟踪其后是否跟随逗号的频率。在训练后，我们可以以我们之前描述的方式检测缺失的逗号。
- en: '**How This Relates To Association Rules**'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**这与关联规则的关系**'
- en: The main method we discussed in this post may be seen as an instance of mining
    association rules.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这篇文章中讨论的主要方法可以看作是挖掘关联规则的一个实例。
- en: Viewed this way, the various map data structures that we used encode specific
    association rules of the form
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度看，我们使用的各种映射数据结构编码了特定形式的关联规则。
- en: '[PRE32]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The probabilistic inference we used, for detecting whether a particular rule
    should fire or not, is called *confidence* in the language of mining association
    rules. It took the form of *P*(*Y*|*X*).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的概率推断，用于检测特定规则是否应该触发，称为在挖掘关联规则中的*confidence*。它的形式是*P*(*Y*|*X*)。
- en: In association rules mining, an alternative called *lift* is often used. See
    [9] for more on associative rules and lift.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在关联规则挖掘中，另一个常用的替代方法叫做*lift*。有关关联规则和lift的更多信息，请参见[9]。
- en: '**Summary**'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this post, we discussed the topic of detecting and correcting issues in text
    as it is being written. This use case has tremendous value, from improving emails
    as they are being written, to helping writers improve their writing as they are
    putting together longer articles.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了在文本写作过程中检测和纠正问题的话题。这个用例具有巨大价值，从改善正在写作的电子邮件，到帮助作者在撰写较长文章时提高写作质量。
- en: This is what makes Grammarly so popular.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Grammarly如此受欢迎的原因。
- en: In this post, we covered the following types of common errors with examples.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们列举了以下类型的常见错误及其示例。
- en: Missing commas.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺少逗号。
- en: Missing articles.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺少冠词。
- en: Leaving out the apostrophe in *It’s*.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*It’s*中遗漏撇号。
- en: Using singular instead of plural, or vice-versa.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单数形式而不是复数形式，或反之亦然。
- en: Using a hyphen when it shouldn’t be used, or vice-versa.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用连字符时不该用，或反之亦然。
- en: Casing words. Not capitalizing a letter when one should. Not having a word in
    all caps when one should. E.g., *fyi*
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小写问题。应大写的字母未大写。应全大写的单词未全大写。例如，*fyi*
- en: Using the wrong preposition, or using one when it is not needed.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用错误的介词，或在不需要时使用介词。
- en: In each case, we described the issue with real examples. Often nuances emerged.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，我们都通过实际例子描述了问题。通常会出现细微的差别。
- en: Next, we took a second pass over these issues in which we discussed methods
    from statistical NLP that help detect and address them. We discussed training
    data, feature engineering, and some specific statistical models that fit these
    use cases.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对这些问题进行了第二次审视，讨论了统计自然语言处理的方法，这些方法有助于检测和解决这些问题。我们讨论了训练数据、特征工程以及适用于这些用例的一些特定统计模型。
- en: '**References**'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[Of vs. For: Differences and Proper Grammar Use | YourDictionary](https://grammar.yourdictionary.com/vs/of-vs-for-differences-and-proper-grammar-use.html)'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[“Of” 与 “For”：区别和正确的语法使用 | YourDictionary](https://grammar.yourdictionary.com/vs/of-vs-for-differences-and-proper-grammar-use.html)'
- en: '[When to Use a Hyphen: eContent Pro](https://www.econtentpro.com/blog/when-to-use-a-hyphen/8)'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[何时使用连字符：eContent Pro](https://www.econtentpro.com/blog/when-to-use-a-hyphen/8)'
- en: '[ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/)'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ChatGPT：优化对话的语言模型](https://openai.com/blog/chatgpt/)'
- en: '[Wikipedia:Database download](https://en.wikipedia.org/wiki/Wikipedia:Database_download)'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[维基百科：数据库下载](https://en.wikipedia.org/wiki/Wikipedia:Database_download)'
- en: '[NLTK Tokenize: Words and Sentences Tokenizer with Example](https://www.guru99.com/tokenize-words-sentences-nltk.html)'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[NLTK 分词：带有示例的单词和句子分词器](https://www.guru99.com/tokenize-words-sentences-nltk.html)'
- en: '[Poisson distribution — Wikipedia](https://en.wikipedia.org/wiki/Poisson_distribution)'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[泊松分布 — 维基百科](https://en.wikipedia.org/wiki/Poisson_distribution)'
- en: '[On a Distribution Representing Sentence-Length in Written Prose](https://www.jstor.org/stable/2345142)'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[关于书面散文中句子长度的分布表示](https://www.jstor.org/stable/2345142)'
- en: '[Named Entity Recognition in NLP. Real-world use cases, models, methods…](/named-entity-recognition-in-nlp-be09139fa7b8)'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[自然语言处理中的命名实体识别。真实世界的应用案例、模型、方法……](/named-entity-recognition-in-nlp-be09139fa7b8)'
- en: '[Association rule learning — Wikipedia](https://en.wikipedia.org/wiki/Association_rule_learning)'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[关联规则学习 — 维基百科](https://en.wikipedia.org/wiki/Association_rule_learning)'
- en: '[Grammarly](https://app.grammarly.com/)'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Grammarly](https://app.grammarly.com/)'
- en: '[Contextual Text Correction Using NLP, Arun Jagota, Towards Data Science, Medium](https://jagota-arun.medium.com/contextual-text-correction-using-nlp-81a1363c5fc3)'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[基于自然语言处理的上下文文本纠正，Arun Jagota，Towards Data Science，Medium](https://jagota-arun.medium.com/contextual-text-correction-using-nlp-81a1363c5fc3)'
