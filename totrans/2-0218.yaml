- en: A Practical Introduction to Sequential Feature Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-practical-introduction-to-sequential-feature-selection-a5444eb5b2fd](https://towardsdatascience.com/a-practical-introduction-to-sequential-feature-selection-a5444eb5b2fd)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A gentle dive into this unusual feature selection technique
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://gianlucamalato.medium.com/?source=post_page-----a5444eb5b2fd--------------------------------)[![Gianluca
    Malato](../Images/2a0da89089967ec151fb6b563aa1f89f.png)](https://gianlucamalato.medium.com/?source=post_page-----a5444eb5b2fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a5444eb5b2fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a5444eb5b2fd--------------------------------)
    [Gianluca Malato](https://gianlucamalato.medium.com/?source=post_page-----a5444eb5b2fd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a5444eb5b2fd--------------------------------)
    ·4 min read·Feb 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c29dc7bc61b88a6b4adf9cde8fd61cf.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Foto di* [*Robert Stump*](https://unsplash.com/@stumpie10?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    *su* [*Unsplash*](https://unsplash.com/it/foto/pQyTChJwEDI?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)'
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection is always a challenging task for data scientists. Identifying
    the right set of features is crucial for the success of a model. There are several
    techniques that make use of the performance that a set of features gives to a
    model. One of them is the sequential feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: What is sequential feature selection?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sequential feature selection is a supervised approach to feature selection.
    It makes use of a supervised model and it can be used to remove useless features
    from a large dataset or to select useful features by adding them sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm works according to these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Select, from the dataset, the feature that maximizes the average performance
    of your model in k-fold cross-validation. This dataset is made by only one feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a second feature to your dataset according to the same principle (maximization
    of CV performance of the model)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep adding features to the dataset until the desired number of features is
    reached or performance doesn’t improve significantly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a *forward* approach because we start with 1 feature and then we add
    other features. There’s a *backward* approach as well, that starts from all the
    features and removes the less relevant ones according to the same maximization
    criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Since, at each step, we check the performance of the model with the same dataset
    with the addition of each remaining feature (one by one), it’s a greedy approach.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm stops when the desired number of features is reached or if the
    performance doesn’t increase above a certain threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and disadvantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main advantage is that it is actually able to find a very good set of features
    according to the given model. Moreover, it merely works on model performance,
    so it doesn’t need the model to give us its own interpretation of feature importance
    like, for example, [Random Forest](https://www.yourdatateacher.com/2021/10/11/feature-selection-with-random-forest/)
    or [Lasso regression](https://www.yourdatateacher.com/2021/05/05/feature-selection-in-machine-learning-using-lasso-regression/).
    It works with every model and this is a great advantage.
  prefs: []
  type: TYPE_NORMAL
- en: The main disadvantage is related to the greedy approach. As it’s easy to figure
    out, it’s computationally expensive, especially if you work with the backward
    approach and you have hundreds of features.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, selecting the features according to the performance doesn’t always
    guarantee the best set of features. For example, this approach doesn’t remove
    collinearity properly.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the entire procedure relies on the use of the proper performance metric
    (that is crucial for any supervised machine learning problem) and the choice of
    the threshold to apply to stop the selection.
  prefs: []
  type: TYPE_NORMAL
- en: The advantages and disadvantages of such a procedure must be taken into account
    according to the project we’re working on.
  prefs: []
  type: TYPE_NORMAL
- en: An example in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s see an example using Python programming language. For this example, we’ll
    work with the breast cancer dataset of scikit-learn >= 1.1.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s import some objects and the SequentialFeatureSelector object itself, that
    performs the feature selection algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let’s import a classification model, for example a Gaussian Naive Bayes model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let’s split our dataset into training and test.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s apply the forward approach, with the automatic selection of the 4
    best features. We’ll use the AuROC score for measuring the performance and a 5-fold
    cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As expected, 4 features have been selected.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14b6100faa97c6dcfb9231833c369370.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: As we can do with every feature selector object of scikit-learn, we can use
    the “get_support” method of the selector to get the names of the selected features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2f494b65346200f958abaf5ad9dd2a6e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s try a different approach. Let’s make the algorithm choose the best
    set of features according to the balanced accuracy score and a stopping threshold
    of 1%. So, if the selection of any feature at each stage doesn’t improve this
    score by at least 1%, the algorithm stops with the features that have been identified
    until then.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/679a217da1fe53c9e54087025d46e916.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The selector has chosen 3 features. Their names are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b8b3d2afb4bb2dffd651f77768ea3b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: A similar task can be accomplished in a backward fashion by setting the “direction”
    argument to “backward”.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sequential feature selection can be a very useful tool in a data scientist’s
    toolbox. However, we must take into account its complexity and computational speed,
    which is very low. I suggest using an automatic approach to select the best number
    of features and, generally speaking, using a forward approach could be the best
    thing. That’s because if we start from a huge number of features, the performance
    of the model might be affected by the curse of dimensionality and be unreliable
    for any purpose, including feature selection. The forward approach, on the contrary,
    starts from a small number of features and keeps adding them until it finds a
    good set. That can reduce computational time and give more reliable results.
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://www.yourdatateacher.com*](https://www.yourdatateacher.com/2023/02/15/a-practical-introduction-to-sequential-feature-selection/)
    *on February 15, 2023.*'
  prefs: []
  type: TYPE_NORMAL
