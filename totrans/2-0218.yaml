- en: A Practical Introduction to Sequential Feature Selection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序特征选择的实用介绍
- en: 原文：[https://towardsdatascience.com/a-practical-introduction-to-sequential-feature-selection-a5444eb5b2fd](https://towardsdatascience.com/a-practical-introduction-to-sequential-feature-selection-a5444eb5b2fd)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-practical-introduction-to-sequential-feature-selection-a5444eb5b2fd](https://towardsdatascience.com/a-practical-introduction-to-sequential-feature-selection-a5444eb5b2fd)
- en: A gentle dive into this unusual feature selection technique
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 轻松探讨这种不寻常的特征选择技术
- en: '[](https://gianlucamalato.medium.com/?source=post_page-----a5444eb5b2fd--------------------------------)[![Gianluca
    Malato](../Images/2a0da89089967ec151fb6b563aa1f89f.png)](https://gianlucamalato.medium.com/?source=post_page-----a5444eb5b2fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a5444eb5b2fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a5444eb5b2fd--------------------------------)
    [Gianluca Malato](https://gianlucamalato.medium.com/?source=post_page-----a5444eb5b2fd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://gianlucamalato.medium.com/?source=post_page-----a5444eb5b2fd--------------------------------)[![Gianluca
    Malato](../Images/2a0da89089967ec151fb6b563aa1f89f.png)](https://gianlucamalato.medium.com/?source=post_page-----a5444eb5b2fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a5444eb5b2fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a5444eb5b2fd--------------------------------)
    [Gianluca Malato](https://gianlucamalato.medium.com/?source=post_page-----a5444eb5b2fd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a5444eb5b2fd--------------------------------)
    ·4 min read·Feb 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----a5444eb5b2fd--------------------------------)
    ·阅读时间4分钟·2023年2月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0c29dc7bc61b88a6b4adf9cde8fd61cf.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c29dc7bc61b88a6b4adf9cde8fd61cf.png)'
- en: '*Foto di* [*Robert Stump*](https://unsplash.com/@stumpie10?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    *su* [*Unsplash*](https://unsplash.com/it/foto/pQyTChJwEDI?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*Robert Stump*的照片可在[*Unsplash*](https://unsplash.com/it/foto/pQyTChJwEDI?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)上查看。'
- en: Feature selection is always a challenging task for data scientists. Identifying
    the right set of features is crucial for the success of a model. There are several
    techniques that make use of the performance that a set of features gives to a
    model. One of them is the sequential feature selection.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择对于数据科学家来说总是一个挑战。确定合适的特征集对模型的成功至关重要。有几种技术利用特征集对模型的性能。其中之一是顺序特征选择。
- en: What is sequential feature selection?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是顺序特征选择？
- en: Sequential feature selection is a supervised approach to feature selection.
    It makes use of a supervised model and it can be used to remove useless features
    from a large dataset or to select useful features by adding them sequentially.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序特征选择是一种有监督的特征选择方法。它利用有监督模型，可以用来从大型数据集中移除无用的特征或通过逐步添加来选择有用的特征。
- en: 'The algorithm works according to these steps:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法按照以下步骤工作：
- en: Select, from the dataset, the feature that maximizes the average performance
    of your model in k-fold cross-validation. This dataset is made by only one feature.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集中选择一个特征，以最大化模型在k折交叉验证中的平均性能。这个数据集仅由一个特征组成。
- en: Add a second feature to your dataset according to the same principle (maximization
    of CV performance of the model)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据相同的原则（最大化模型的交叉验证性能），向数据集中添加第二个特征
- en: Keep adding features to the dataset until the desired number of features is
    reached or performance doesn’t improve significantly
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不断向数据集中添加特征，直到达到所需特征数量或性能没有显著提升
- en: This is a *forward* approach because we start with 1 feature and then we add
    other features. There’s a *backward* approach as well, that starts from all the
    features and removes the less relevant ones according to the same maximization
    criteria.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种*前向*方法，因为我们从一个特征开始，然后添加其他特征。还有一种*后向*方法，它从所有特征开始，根据相同的最大化标准移除较不相关的特征。
- en: Since, at each step, we check the performance of the model with the same dataset
    with the addition of each remaining feature (one by one), it’s a greedy approach.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在每一步，我们检查模型在相同数据集上添加每个剩余特征（一个一个）的性能，因此这是一种贪婪方法。
- en: The algorithm stops when the desired number of features is reached or if the
    performance doesn’t increase above a certain threshold.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当达到所需特征数量或性能未超过某个阈值时，算法会停止。
- en: Advantages and disadvantages
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势和劣势
- en: The main advantage is that it is actually able to find a very good set of features
    according to the given model. Moreover, it merely works on model performance,
    so it doesn’t need the model to give us its own interpretation of feature importance
    like, for example, [Random Forest](https://www.yourdatateacher.com/2021/10/11/feature-selection-with-random-forest/)
    or [Lasso regression](https://www.yourdatateacher.com/2021/05/05/feature-selection-in-machine-learning-using-lasso-regression/).
    It works with every model and this is a great advantage.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 主要优点是它实际上能够根据给定模型找到非常好的特征集。此外，它仅仅依赖于模型性能，因此不需要模型像[随机森林](https://www.yourdatateacher.com/2021/10/11/feature-selection-with-random-forest/)或[套索回归](https://www.yourdatateacher.com/2021/05/05/feature-selection-in-machine-learning-using-lasso-regression/)那样提供特征重要性的解释。它适用于所有模型，这是一个很大的优势。
- en: The main disadvantage is related to the greedy approach. As it’s easy to figure
    out, it’s computationally expensive, especially if you work with the backward
    approach and you have hundreds of features.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 主要缺点与贪婪方法有关。正如你所想，它计算成本很高，尤其是在你使用反向方法并且有数百个特征时。
- en: Moreover, selecting the features according to the performance doesn’t always
    guarantee the best set of features. For example, this approach doesn’t remove
    collinearity properly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据性能选择特征并不总能保证得到最佳特征集。例如，这种方法不能正确去除共线性。
- en: Finally, the entire procedure relies on the use of the proper performance metric
    (that is crucial for any supervised machine learning problem) and the choice of
    the threshold to apply to stop the selection.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，整个过程依赖于使用适当的性能度量（这是任何监督学习问题中至关重要的）和选择应用于停止选择的阈值。
- en: The advantages and disadvantages of such a procedure must be taken into account
    according to the project we’re working on.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 必须根据我们所做的项目考虑这种程序的优缺点。
- en: An example in Python
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python示例
- en: Let’s see an example using Python programming language. For this example, we’ll
    work with the breast cancer dataset of scikit-learn >= 1.1.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用Python编程语言看一个例子。在这个例子中，我们将使用scikit-learn >= 1.1的乳腺癌数据集。
- en: Let’s import some objects and the SequentialFeatureSelector object itself, that
    performs the feature selection algorithm.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入一些对象以及执行特征选择算法的SequentialFeatureSelector对象。
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s import a classification model, for example a Gaussian Naive Bayes model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入一个分类模型，例如高斯朴素贝叶斯模型。
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s split our dataset into training and test.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将数据集分成训练集和测试集。
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, let’s apply the forward approach, with the automatic selection of the 4
    best features. We’ll use the AuROC score for measuring the performance and a 5-fold
    cross-validation
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们应用前向方法，自动选择4个最佳特征。我们将使用AuROC得分来衡量性能，并进行5折交叉验证。
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As expected, 4 features have been selected.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，选择了4个特征。
- en: '![](../Images/14b6100faa97c6dcfb9231833c369370.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14b6100faa97c6dcfb9231833c369370.png)'
- en: Image by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: As we can do with every feature selector object of scikit-learn, we can use
    the “get_support” method of the selector to get the names of the selected features.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们可以对scikit-learn的每个特征选择器对象做的那样，我们可以使用选择器的“get_support”方法来获取所选特征的名称。
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/2f494b65346200f958abaf5ad9dd2a6e.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f494b65346200f958abaf5ad9dd2a6e.png)'
- en: Image by author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Now, let’s try a different approach. Let’s make the algorithm choose the best
    set of features according to the balanced accuracy score and a stopping threshold
    of 1%. So, if the selection of any feature at each stage doesn’t improve this
    score by at least 1%, the algorithm stops with the features that have been identified
    until then.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试另一种方法。让算法根据平衡准确度得分和1%的停止阈值选择最佳特征集。因此，如果每个阶段的特征选择未能将该得分提高至少1%，算法会停止，并保留到目前为止识别出的特征。
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/679a217da1fe53c9e54087025d46e916.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/679a217da1fe53c9e54087025d46e916.png)'
- en: Image by author
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'The selector has chosen 3 features. Their names are:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器已经选择了3个特征。它们的名称是：
- en: '![](../Images/2b8b3d2afb4bb2dffd651f77768ea3b4.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b8b3d2afb4bb2dffd651f77768ea3b4.png)'
- en: Image by author
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: A similar task can be accomplished in a backward fashion by setting the “direction”
    argument to “backward”.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将“direction”参数设置为“backward”，可以以反向的方式完成类似的任务。
- en: Conclusions
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Sequential feature selection can be a very useful tool in a data scientist’s
    toolbox. However, we must take into account its complexity and computational speed,
    which is very low. I suggest using an automatic approach to select the best number
    of features and, generally speaking, using a forward approach could be the best
    thing. That’s because if we start from a huge number of features, the performance
    of the model might be affected by the curse of dimensionality and be unreliable
    for any purpose, including feature selection. The forward approach, on the contrary,
    starts from a small number of features and keeps adding them until it finds a
    good set. That can reduce computational time and give more reliable results.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://www.yourdatateacher.com*](https://www.yourdatateacher.com/2023/02/15/a-practical-introduction-to-sequential-feature-selection/)
    *on February 15, 2023.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
