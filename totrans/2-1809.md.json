["```py\nconda create --name rl python=3.8\nconda activate rl\n\nconda install gymnasium[box2d]\npip install stable-baselines3==2.1.0\npip install pygame==2.5.2\npip install imageio==2.31.6\n\nconda install jupyter\njupyter notebook\n```", "```py\nimport os\nimport numpy as np\nimport gymnasium as gym   # 0.28.1\nimport stable_baselines3  # 2.1.0\nfrom stable_baselines3 import A2C, DDPG, DQN, PPO, SAC, TD3\nfrom stable_baselines3.common.callbacks import EvalCallback\nfrom stable_baselines3.common.evaluation import evaluate_policy\n```", "```py\nenv = gym.make(\"CartPole-v1\")\nmodel = DQN(\"MlpPolicy\", env)\nmodel.learn(total_timesteps=100000)\n```", "```py\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\nprint(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n```", "```py\nimport pygame\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\nobs = env.reset()[0]\nscore = 0 \nwhile True:\n    action, states = model.predict(obs)\n    obs, rewards, done, terminate, info = env.step(action)\n    score += rewards\n    env.render()\n    if terminate: \n        break\nprint(\"score: \", score)\nenv.close()\n```", "```py\nenv = gym.make(\"CartPole-v1\")\nmodel = DQN(\"MlpPolicy\", env)\nmodel.learn(\n    total_timesteps=100000,\n    callback=EvalCallback(\n        env, best_model_save_path='./logs/', eval_freq=5000\n    )\n)\n```", "```py\nmodel = DQN.load(\"./logs/best_model.zip\")\nmodel.set_env(env)\n```", "```py\ndef is_compatible(env, model_name):\n    action_requirements = {\n        'A2C':  [gym.spaces.Box, gym.spaces.Discrete],\n        'DDPG': [gym.spaces.Box],\n        'DQN':  [gym.spaces.Discrete],\n        'PPO':  [gym.spaces.Box, gym.spaces.Discrete],\n        'SAC':  [gym.spaces.Box],\n        'TD3':  [gym.spaces.Box],\n    }\n    return isinstance(env.action_space, tuple(action_requirements[model_name]))\n```", "```py\nclass EnvWrapper(gym.ActionWrapper):\n    def __init__(self, env, conversion='Box'):\n        super().__init__(env)\n        self.conversion = conversion\n        if conversion == 'Box':\n            self.action_space = gym.spaces.Box(\n                low=np.array([-1]), high=np.array([1]), dtype=np.float32\n            )\n        elif conversion == 'Discrete':\n            self.num_actions = 9\n            self.action_space = gym.spaces.Discrete(\n                self.num_actions\n            )\n        else:\n            pass\n\n    def action(self, action):\n        if self.conversion == 'Box':\n            # Takes a Continuous action from the model and convert it to discrete for a natively Discrete Env\n            if action.shape == (1,):\n                action = np.round((action[0] + 1) / 2).astype(int)  # convert from scale of [-1, 1] to the set {0, 1}\n            else:\n                action = np.round((action + 1) / 2).astype(int)\n        elif self.conversion == 'Discrete':\n            # Takes a Discrete action from the model and convert it to continuous for a natively Box Env\n            action = (action / (self.num_actions - 1)) * 2.0 - 1.0\n            action = np.array([action])\n\n        return action\n```", "```py\nwrapped_env = EnvWrapper(env, 'Box')\nmodel = SAC(\"MlpPolicy\", wrapped_env)\nmodel.learn(total_timesteps=10000)\n```", "```py\nenv_name_list = ['CartPole-v1', 'MountainCar-v0', 'Pendulum-v1', 'Acrobot-v1']\nmodel_name_list = ['A2C', 'DDPG', 'DQN', 'PPO', 'SAC', 'TD3']\n\nfor env_name in env_name_list:\n    for model_name in model_name_list:\n        env = gym.make(env_name)\n        if not is_compatible(env, model_name):\n            # Environment and model are not compatible. Will wrap env to suit to model\n            if isinstance(env.action_space, gym.spaces.Box):\n                env = EnvWrapper(env, 'Discrete')\n                print(\"Box Environment warpped to be compatible with Discrete model...\")\n            else:\n                env = EnvWrapper(env, 'Box')\n                print(\"Discrete Environment warpped to be compatible with Continuous model\")\n        else:\n            print(\"Already compatible\")\n\n        model = eval(\"%s(\\\"MlpPolicy\\\", env, verbose=False)\" % model_name)\n        print(\"Using %s in %s. The model's action space is %s\" % (model_name, env_name, model.action_space))\n\n        model.learn(total_timesteps=100)  # just for testing\n```", "```py\nclass LunarWrapper(gym.Wrapper):\n    def __init__(self, env, max_top_time=100, penalty=-1):\n        super().__init__(env)\n        self.max_top_time = max_top_time  # penalty kicks in after this step\n        self.penalty = penalty            # additional reward (or penalty if negative) after max_top_time\n        self.penalty_start_step = 20000\n        self.step_counter = 0\n\n    def reset(self, **kwargs):\n        self.time_at_top = 0\n        return super().reset(**kwargs)\n\n    def step(self, action):\n        obs, reward, done, terminate, info = super().step(action)\n        self.step_counter += 1\n\n        y_position = obs[1]\n        if y_position > 0.5:\n            self.time_at_top += 1\n        else:\n            self.time_at_top = 0  # Reset counter if it comes down\n\n        # Apply penalty if the lander stays at the top for too long\n        if self.time_at_top >= self.max_top_time:\n            if (self.step_counter >= self.penalty_start_step):\n                reward += (-y_position)    # top of the screen is 1\\. To incur more penalty when it is high\n\n        return obs, reward, done, terminate, info\n```", "```py\nenv_name = \"LunarLander-v2\"\nwrapped_env = LunarWrapper(gym.make(env_name))\n\nmodel = DQN(\n    \"MlpPolicy\", wrapped_env,\n    buffer_size=50000, learning_starts=1000, train_freq=4, target_update_interval=1000, \n    learning_rate=1e-3, gamma=0.99\n)\n\nmodel.learn(\n    total_timesteps=50000,\n    callback=EvalCallback(\n        wrapped_env, best_model_save_path='./logs/', log_path='./logs/', eval_freq=2000\n    )\n)\n\nmodel = DQN.load(\"./logs/best_model.zip\")\nmodel.set_env(env)\n\nmodel.learn(\n    total_timesteps=20000,\n    callback=EvalCallback(\n        env, best_model_save_path='./logs/', eval_freq=2000\n    )\n)\n```", "```py\nclass SimpleEnv:\n    def __init__(self):\n        self.min_row, self.max_row = 0, 4\n        self.min_col, self.max_col = 0, 4\n        self.terminal = [[self.max_row, self.max_col]]\n        self.reset()\n\n    def reset(self, random=False):\n        if random:\n            while True:\n                self.cur_state = [np.random.randint(self.max_row + 1), np.random.randint(self.max_col + 1)]\n                if self.cur_state not in self.terminal:\n                    break\n        else:\n            self.cur_state = [0,0]\n        return self.cur_state\n\n    def transition(self, state, action):\n        reward = 0\n        if action == 0:\n            state[1] += 1   # move right one column\n        elif action == 1:\n            state[0] += 1   # move down one row\n        elif action == 2:\n            state[1] -= 1   # move left one column\n        elif action == 3:\n            state[0] -= 1   # move up one row\n        else:\n            assert False, \"Invalid action\"\n\n        if (state[0] < self.min_row) or (state[1] < self.min_col) \\\n            or (state[0] > self.max_row) or (state[1] > self.max_col):\n            reward = -1\n\n        next_state = np.clip(\n            state, [self.min_row, self.min_col], [self.max_row, self.max_col]\n        ).tolist()\n        if next_state in self.terminal:\n            done = True\n        else:\n            done = False \n        return reward, next_state, done\n\n    def _get_action_dim(self):\n        return 4\n\n    def _get_state_dim(self):\n        return np.array([5,5])\n```", "```py\nfrom gymnasium import spaces\n\nclass CustomEnv(gym.Env):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.internal_env = SimpleEnv(**kwargs)\n        self.action_space = spaces.Discrete(self.internal_env._get_action_dim())\n        self.observation_space = spaces.MultiDiscrete(self.internal_env._get_state_dim())\n\n    def step(self, action):\n        reward, next_state, done = self.internal_env.transition(self.internal_env.cur_state, action)\n        self.count += 1\n        terminate = self.count > 50\n        if terminate:\n            reward += -100\n        return np.array(next_state), reward, done, terminate, {}\n\n    def reset(self, random=True, **kwargs):\n        self.count = 0\n        return (np.array(self.internal_env.reset(random=random)), {})\n\n    def render(self, mode=\"human\"):\n        pass\n\n    def close(self):\n        pass\n```", "```py\nfrom stable_baselines3.common.env_checker import check_env\n\nenv = CustomEnv()\ncheck_env(env, warn=True)\n\nobs = env.reset()\naction = env.action_space.sample()\nprint(\"Sampled action:\", action)\n\nobs, reward, done, terminate, info = env.step(action)\nprint(obs.shape, reward, done, info)\n```", "```py\nmodel = DQN(\n    \"MlpPolicy\", env,\n    learning_rate=1e-5,\n    exploration_fraction=0.5,\n    exploration_initial_eps=1.0,\n    exploration_final_eps=0.10,\n)\nmodel.learn(\n    total_timesteps=100000,\n    callback=EvalCallback(\n        env, best_model_save_path='./logs/', eval_freq=10000\n    )\n)\n```"]