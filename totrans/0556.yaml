- en: Continuous Integration and Deployment for Data Platforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](https://towardsdatascience.com/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: CI/CD for data engineers and ML Ops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----817bf1b6bed1--------------------------------)[![ðŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----817bf1b6bed1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----817bf1b6bed1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----817bf1b6bed1--------------------------------)
    [ðŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----817bf1b6bed1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----817bf1b6bed1--------------------------------)
    Â·9 min readÂ·Apr 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd7ac2298a3ebce9921e027ae8163cd9.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Emmy Sobieski](https://unsplash.com/@emmy_s?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: What is a data environment? Data engineers split infrastructure resources into
    live and staging to create isolated areas (environments) where they can test ETL
    services and data pipelines before promoting them to production.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data environment** refers to a set of applications and related physical infrastructure
    resources that enable data storage, transfer, processing and data transformation
    to support company goals and objectives.'
  prefs: []
  type: TYPE_NORMAL
- en: '**This story** providesan **overview of CI/CD tech** available for data and
    a **working example** of a simple **ETL** service built in **Python** and deployed
    with **Infrastructure as code** (IaC) using **Github Actions**.'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration and continuous delivery (CI/CD)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Continuous integration and continuous delivery (CI/CD) is a software development
    strategy in which all developers collaborate on a common repository of code, and
    when changes are made, an automated build process is used to discover any potential
    code problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a68cf39e925c7b6f79982657c32ab9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD benefits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the primary technical advantages of CI/CD is that it improves overall
    code quality and saves time.
  prefs: []
  type: TYPE_NORMAL
- en: Automated CI/CD pipelines using Infrastructure as Code solve a lot of problems.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----817bf1b6bed1--------------------------------)
    [## Infrastructure as Code for Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy Data Pipelines like a pro with these templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----817bf1b6bed1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Deliver faster**'
  prefs: []
  type: TYPE_NORMAL
- en: Adding new features numerous times each day is not an easy task. But, if we
    have a simplified CI/CD workflow, it is definitely achievable.
  prefs: []
  type: TYPE_NORMAL
- en: Using CI/CD tools such as GoCD, Code Pipeline, Docker, Kubernetes, Circle CI,
    Travis CI, etc. dev teams now can build, test, and deploy things independently
    and automatically.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reduce errors**'
  prefs: []
  type: TYPE_NORMAL
- en: Finding and resolving code issues late in the development process is time-consuming
    and, therefore, expensive. When features with errors being released to production,
    this becomes even more important.
  prefs: []
  type: TYPE_NORMAL
- en: By testing and deploying code more often using a CI/CD pipeline, testers will
    be able to see problems as soon as they arise and correct them right away. This
    helps to mitigate risks in real time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Less manual effort and more transparency**'
  prefs: []
  type: TYPE_NORMAL
- en: Tests should run automatically for new code features to ensure that neither
    the new code nor the new features damage any already-existing features. We would
    want to get regular updates and information regarding the development, test, and
    deployment circles throughout this process.
  prefs: []
  type: TYPE_NORMAL
- en: '**Easy rollbacks**'
  prefs: []
  type: TYPE_NORMAL
- en: To prevent downtime in production, the most recent successful build is normally
    deployed immediately if something is wrong with our new release or feature. This
    is another greate CI/CD feature that enables easy rollbacks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Extensive logs**'
  prefs: []
  type: TYPE_NORMAL
- en: Knowing the deployement process is essential. Undestanding why our code fails
    is even more important. One of the most important parts of DevOps and CI/CD integration
    is observability. Being able to read extensive logs for our builds is definitely
    a must have function.
  prefs: []
  type: TYPE_NORMAL
- en: When do we use CI/CD for data platforms?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Managing data resources and infrastructure**: With CI/CD techniques and tools
    we can provision, deploy and manage the infrastructure resources we might need
    for data pipelines, i.e. Cloud Storage buckets, Serverless microservices to perform
    ETL tasks, event streams and queues. Tools like AWS Cloudformation and Terraform
    can manage infrastructure with ease to provision resources for tests, staging
    and live environments.'
  prefs: []
  type: TYPE_NORMAL
- en: '**SQL unit testing**: CI/CD helps with data transformation. If we have a data
    pipeline that transforms data in ELT pattern we can automate SQL unit tests to
    test the logic behind it. A good example would be a GitHub Actions workflow that
    compiles our SQL scripts and runs unit tests.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/unit-tests-for-sql-scripts-with-dependencies-in-dataform-847133b803b7?source=post_page-----817bf1b6bed1--------------------------------)
    [## Unit Tests for SQL Scripts with Dependencies in Dataform'
  prefs: []
  type: TYPE_NORMAL
- en: and Data warehouse Gitflow pipelines to run it automatically
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/unit-tests-for-sql-scripts-with-dependencies-in-dataform-847133b803b7?source=post_page-----817bf1b6bed1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Validating ETL processes**: Many data pipelines rely heavily on ETL (Extract,
    Transform, Load) operations. We would want to ensure that any changes we commit
    to our GitHub repository do the right job with the data. This can be achieved
    by implementing automated **integration testing**. Here is a simple example of
    how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://mydataschool.com/blog/data-platform-unit-and-integration-tests-explained/?source=post_page-----817bf1b6bed1--------------------------------)
    [## Data Platform Unit and Integration Tests Explained'
  prefs: []
  type: TYPE_NORMAL
- en: How to do this exercise and how to apply it to our data pipelines? This is a
    good question I asked myself at the veryâ€¦
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: mydataschool.com](https://mydataschool.com/blog/data-platform-unit-and-integration-tests-explained/?source=post_page-----817bf1b6bed1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitoring data pipelines**. A great example would be using CI/CD and Infrastructure
    as Code to provision Notification Topics and Alarms for ETL resources, i.e. Lambda,
    etc. We can receive notifications via selected channels if something goes wrong
    with our ETL processing service, for instance, if the number of errors reaches
    the threshold. Here is an AWS Cloudformation example of how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: How to set up a CI/CD pipeline for a data platform?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/947060c560a5af8a23c8a169682d229f.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample CI/CD pipeline. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1\. Create a repository**'
  prefs: []
  type: TYPE_NORMAL
- en: This is a fundamental step. A version control system is required. We would want
    to ensure that every change in our code is version controlled, saved somewhere
    in the cloud and can be reverted if needed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2\. Add build step**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now when we have a repository we can configure our CI/CD pipeline to actually
    build the project. Imagine, we have an ETL microservice that loads data from AWS
    S3 into a data warehouse. This step would involve building a Lambda package in
    the isolated local environment, i.e. in Github. During this step, CI/CD service
    must be able to collect all required code packages to compile our service. For
    example, if we have a simple AWS Lambda to perform an ETL task then we would want
    to build the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3\. Run tests**'
  prefs: []
  type: TYPE_NORMAL
- en: We would want to ensure that the changes we deploy for our data pipeline work
    as expected. This can be achieved by writing good unit and integration tests.
    Then we would configure our CI/CD pipeline to run them, for example, every time
    we commit the changes or merge into the master branch. For instance, we can configure
    Gitflow Actions to run a `**pytest test.py**` or `**npm run test**` for our **AWS
    Lambda**. If tests are successful we can proceed to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4\. Deploy staging**'
  prefs: []
  type: TYPE_NORMAL
- en: In this step, we continue to implement Continuous Integration. We have a successful
    build for our project and all tests have been passed and now we would want to
    deploy in the staging environment. By environment we mean resources. CI/CD pipeline
    can be configured to use settings relevant to this particular environment using
    Infrastructure as code and finally deploy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example for Lambda**. This bash script can be added to a relevant step of
    CI/CD pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5\. Deploy live**'
  prefs: []
  type: TYPE_NORMAL
- en: This is the final step and typically it is **triggered manually** when we are
    100% sure everything is okay.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/bd47c202a438ad7702541056d3c7309a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD would use IaC settings for the production environment. For instance, we
    might want to provide any infrastructure resources relevant only for production,
    i.e. our Lambda function name should be `pipeline-manager-live`. These resource
    parameters and configuration settings must be mentioned in our Cloudformation
    stack file. For example, we might want our **ETL Lambda to be triggered by Cloudwatch
    event from S3 bucket every time a new S3 object is created there.** In this case,
    we would want to provide the name of this S3 bucket in the parameters. Another
    example would be our Lambda settings such **as memory and timeout**. There is
    no need to over-provision memory for staging service but on live we would want
    it to be able to process larger amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'CI/CD Live step example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e826dcbc72527e8cd4db0bc311fced24.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Rollbacks, version control and security can be handled via CI/CD service settings
    and IaC.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: CI/CD pipeline example with infrastructure as code and AWS Lambda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Letâ€™s imagine we have a typical repo with some ETL service (AWS Lambda) being
    deployed with AWS Cloudformation.
  prefs: []
  type: TYPE_NORMAL
- en: That can be a data pipeline manager application or something else to perform
    ETL tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our repo folder structure will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We will define our CI/CD pipeline with **deploy_staging.yaml** and **deploy_live.yaml**
    in .github/workflows folder.
  prefs: []
  type: TYPE_NORMAL
- en: On any **Pull Request**, we would want to run tests and deploy on staging.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Then if everything is okay we will promote our code to production and deploy
    the stack to live environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29a2109d76dc7ab68dcb506da2c5bac6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: This pipeline will be using Github repository secrets where we will copy paste
    AWS credentials.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c747bd72d1fe8fc3b606c644e220b01e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'After STAGING AND TESTS has been executed successfully and everything passed
    we can manually promote our code to live. We can use `workflow_dispatch:` for
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f0e1f6a198058d88f0ac34f83372695.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD tools available in the market
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are various CI/CD solutions that may be used to automate data pipeline
    testing, deployment, and monitoring. Github Actions is a great tool but sometimes
    we might need more and/or something different.
  prefs: []
  type: TYPE_NORMAL
- en: '**This is not an extensive list but some popular tech to try:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS CodePipeline:** Solid tool for $1.5 a month per one pipeline. Lots of
    features including automated builds and deployments via infrastructure as code.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Circle CI:** Circle CI is a cloud-based CI/CD system for automated data pipeline
    testing and deployment. It has a number of connectors and plugins that make it
    simple to set up and operate.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Jenkins:** Jenkins is a free and open-source automation server for continuous
    integration and deployment. It offers a diverse set of plugins and connectors,
    making it a powerful data pipeline management solution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**GitLab CI/CD:** GitLab CI/CD is a cloud-based system that allows teams to
    manage changes to their code and data pipelines in a single location. It has an
    easy-to-use interface for creating, testing, and deploying data pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Travis CI:** Travis CI is a cloud-based CI/CD system for automated data pipeline
    testing and deployment. It is simple to set up and utilize, making it a popular
    choice for teams with little automation expertise.'
  prefs: []
  type: TYPE_NORMAL
- en: '**GoCD:** GoCD is a free and an open source build and release tool. Itâ€™s free
    and rely on bash scripts a lot.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the main benefits of CI/CD is that it improves code quality. Continuous
    integration and deployment bring a lot of benefits for data platform engineers
    and ML Ops. Every step of our data pipeline deployments can be easily monitored
    and managed to ensure faster delivery with no errors in production. It saves time
    and helps engineers to be more productive.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this simple example given in this story will be useful for you. Using
    it as a template I was able to create robust and flexible CI/CD pipelines for
    containerized applications. Automation in deployment and testing is pretty much
    a standard these days. And we can do so much more with it including ML Ops and
    provisioning resources for data science.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of CI/CD tools available in the market. Some of them are free
    some arenâ€™t but bringing more flexible setups that might become a better fit for
    your data stack. My advice for beginners would be to start with free tools and
    try to implement this story example. It describes the process that can be reproduced
    for any data service later.
  prefs: []
  type: TYPE_NORMAL
- en: Recommended read
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. [https://docs.github.com/en/actions](https://docs.github.com/en/actions)
  prefs: []
  type: TYPE_NORMAL
- en: 2\. [https://stackoverflow.com/questions/58877569/how-to-trigger-a-step-manually-with-github-actions](https://stackoverflow.com/questions/58877569/how-to-trigger-a-step-manually-with-github-actions)
  prefs: []
  type: TYPE_NORMAL
- en: 3\. [https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html](https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html)
  prefs: []
  type: TYPE_NORMAL
- en: 4\. [https://medium.com/gitconnected/infrastructure-as-code-for-beginners-a4e36c805316](https://medium.com/gitconnected/infrastructure-as-code-for-beginners-a4e36c805316)
  prefs: []
  type: TYPE_NORMAL
- en: 5\. [https://betterprogramming.pub/great-data-platforms-use-conventional-commits-51fc22a7417c](https://betterprogramming.pub/great-data-platforms-use-conventional-commits-51fc22a7417c)
  prefs: []
  type: TYPE_NORMAL
