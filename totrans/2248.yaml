- en: 'Use Deep Learning to Generate Fantasy Names: Build a Language Model from Scratch'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa](https://towardsdatascience.com/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Can a language model invent unique fantasy character names? Let’s build it from
    scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page-----792b13629efa--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----792b13629efa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----792b13629efa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----792b13629efa--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----792b13629efa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----792b13629efa--------------------------------)
    ·11 min read·Sep 22, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f64a5c60ba6b75f99b1f391913f9f8be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [pixabay.com](https://pixabay.com/illustrations/book-old-surreal-fantasy-pages-863418/)'
  prefs: []
  type: TYPE_NORMAL
- en: To truly grasp the intricacies of [**Language Models**](https://en.wikipedia.org/wiki/Language_model#:~:text=A%20language%20model%20is%20a,feedforward%20neural%20networks%20and%20transformers.)
    (LM) and become familiar with their underlying principles, there is no other way
    than rolling up our sleeves and starting to write code. In this article, I present
    the creation of a [**Recurrent Neural Network**](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    (RNN) built entirely from scratch, without the aid of any deep learning library.
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow, Keras, Pytorch make building deep and complex neural networks effortless.
    Undoubtedly, this is a great advantage for Machine Learning practitioners, however,
    this approach has the massive downside of leaving the functioning of those networks
    unclear as they happen “under the hood”.
  prefs: []
  type: TYPE_NORMAL
- en: This is why today we will perform the inspiring exercise of building a Language
    Model using only the [Numpy Python](https://numpy.org/) library!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Recurrent Neural Networks and Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Standard fully connected neural networks are not suited for [Natural Language
    Processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP) tasks
    like text generation. The main reasons are:'
  prefs: []
  type: TYPE_NORMAL
- en: For NLP tasks, input and outputs may take different forms and dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard neural networks don’t simultaneously use features learned at different
    steps of the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main breakthrough in AI application within the NLP field is undeniably represented
    by Recurrent Neural Networks (RNN).
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs are a class of artificial neural networks particularly well suited for
    NLP tasks and text generation. The reason for their efficacy lies in their ability
    of capturing sequential dependencies in data. Human language deeply relies on
    considering the context and linking the first words in a sentence to the last
    ones. Consider these sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '*He said, “Teddy Roosevelt was the US President.”*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*He said, “Teddy bears are on sale!”*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The word “*Teddy*” has a completely different meaning in the two sentences.
    We humans easily understand that by considering the context and the words written
    in the opposite part of the sentence. Surprisingly enough, **RNNs can do the same**!
  prefs: []
  type: TYPE_NORMAL
- en: RNN Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The architecture of RNNs is fairly straightforward. They are made of sequential
    cells, each one of them takes as input a word *x* (or a single character), outputs
    a word/character *y*, and passes an activation *a* to the next cell.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f30ec7ec395dd7ba0d5daf116c16fa72.png)'
  prefs: []
  type: TYPE_IMG
- en: RNN flowchart. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'What happens inside a RNN cell is more interesting. The steps are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The activation of the previous cell is multiplied by some weights *W_aa*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input *x* is multiplied by some weights *W_ax*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The results of the previous steps are summed together and with a bias *b_a*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An activation function like the hyperbolic tangent is applied to compute the
    activation which is passed to the next cell
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The activation is multiplied by some weights *W_ya* and summed with a bias *b_y*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, a softmax function is applied to the resulting vector and the output
    *y_hat* is returned
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/f6ef5bc9f480a77f4f03431f6e1bf08c.png)'
  prefs: []
  type: TYPE_IMG
- en: RNN cell flowchart. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, the formulas for the activation and output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da06e7631ca8be19b539fdc905837300.png)![](../Images/2b8de8a940b0bf252f1bd5f35b04c267.png)'
  prefs: []
  type: TYPE_IMG
- en: I’m unable to provide a comprehensive theoretical introduction of RNNs and LMs
    here. For that, I refer you to the resources listed at the end of this article.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s dive now into the actual Python code. The most important parts of the
    code will be explained in detail in this article. In order to keep the article
    concise, some intuitive parts are omitted. The entire commented code can be accessed
    in my [GitHub repository](https://github.com/andreoniriccardo/articles/tree/main/names-generator-RNN):'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/andreoniriccardo/articles/tree/main/names-generator-RNN?source=post_page-----792b13629efa--------------------------------)
    [## articles/names-generator-RNN at main · andreoniriccardo/articles'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to andreoniriccardo/articles development by creating an account on
    GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/andreoniriccardo/articles/tree/main/names-generator-RNN?source=post_page-----792b13629efa--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our goal is to teach a Language Model to invent novel fantasy character names.
    For this reason, the first thing we need to provide our Language Model is a database
    of actual fantasy names. The LM will be trained and will take inspiration from
    that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to this [Wikipedia page](https://en.wikipedia.org/wiki/List_of_Middle-earth_characters),
    we can easily access a list of all the characters mentioned in the [*Lord Of The
    Ring*](https://en.wikipedia.org/wiki/The_Lord_of_the_Rings) or [*The Hobbit*](https://en.wikipedia.org/wiki/The_Hobbit)
    books. Using the [BeautifulSoup](https://pypi.org/project/beautifulsoup4/)and
    [Regex](https://docs.python.org/3/library/re.html) libraries, the following lines
    of code will collect the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For the explanation of the Regex functions, I leave you with this comprehensive
    [guide](https://pub.towardsai.net/regex-for-the-modern-data-scientists-37bd528d343a).
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, our dataset will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/269aec886667a1e0f9c59c4948f61e04.png)'
  prefs: []
  type: TYPE_IMG
- en: I aim to simplify our vocabulary by replacing unwanted characters such as ä,
    é, î.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, our vocabulary consists in 27 characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[‘x’, ‘j’, ‘f’, ‘t’, ‘c’, ‘b’, ‘o’, ‘l’, ‘y’, ‘w’, ‘i’, ‘e’, ‘g’, ‘m’, ‘k’,
    ‘d’, ‘v’, ’n’, ‘u’, ‘a’, ‘z’, ‘r’, ‘\n’, ‘s’, ‘ ‘, ‘p’, ‘h’]*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the special character *‘\n’* serves as an End of Name token, indicating
    when the generated name should terminate.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is ready and we can now focus on modeling the Recurrent Neural Network.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Language Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, I present the Python implementation of the Language Model.
    The whole code can be divided into 2 sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Forward Propagation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backprop
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following sections, I will present the code that actually trains the
    model and how to generate fantasy character names.
  prefs: []
  type: TYPE_NORMAL
- en: Forward Propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As illustrated above, a RNN is a network composed of multiple cells. It is advantageous
    to model a single RNN cell in Python and then integrate its output through multiple
    cells.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code that models a single RNN cell is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it is not complex at all. We simply take as input the model’s
    parameters and multiply them by the cell’s input and previous activation function.
    Finally, we apply the softmax function to return a vector of probabilities for
    what the output character should be.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to iterate through several RNN cells. This is exactly what
    the `RNN_roward_prop()` function does.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It calls the previous `RNN_roward_pro_step()` function *T* times, where *T*
    is the number of characters of the input word. At the end, a loss function and
    the output is returned.
  prefs: []
  type: TYPE_NORMAL
- en: Backprop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Backprop, short for backward propagation, is the process of adjusting the network’s
    weights to get closer and closer to the desired output, i.e. reducing the model
    loss function. This is done by applying gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Detailing the formulas of gradient descent or other viable optimization algorithms
    is out of this article’s scope. I will leave instead with [**this article**](/choose-the-right-optimization-algorithm-for-your-neural-network-cb86c15d7328),
    which deals exactly with the selection of the right Optimization Algorithm for
    your Deep Networks.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/choose-the-right-optimization-algorithm-for-your-neural-network-cb86c15d7328?source=post_page-----792b13629efa--------------------------------)
    [## Choose the Right Optimization Algorithm for your Neural Network'
  prefs: []
  type: TYPE_NORMAL
- en: As the nature of neural networks’ developing process is iterative, we need to
    take advantage of each possible expedient…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/choose-the-right-optimization-algorithm-for-your-neural-network-cb86c15d7328?source=post_page-----792b13629efa--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy I used to model the backprop flow is the same as the one for the
    forward propagation: code the backprop of a single RNN cell and iterate it multiple
    times.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The above function takes as input the output of the forward propagation and
    the previous gradients and it computes the updated gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ideration of the `RNN_back_prop_function()` function is performed with
    these lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Training the Language Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, all the model’s components are set and ready to be executed.
    I wrote the following code to put together the functions we saw above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The actual magic happens in the following code snipped. This is the main part
    of the whole algorithm and once this function is called, the Language Model will
    be trained.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The result is a model able to mimics the creative process of Tolkien and effortlessly
    produce unique character names.
  prefs: []
  type: TYPE_NORMAL
- en: Generating Fantasy Character Names
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To sample novel character names from our trained Language Model, I developed
    two functions.
  prefs: []
  type: TYPE_NORMAL
- en: The `sample()` function takes as input the network’s parameters and the vocabulary
    mapping the characters to numbers. The idea is to apply the forward propagation
    steps multiple times until either the *‘\n’* special character is return, or we
    reach an upper bound for the number of generated characters (in this case set
    to 50).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it returns a list of indices that encode the generated fantasy character
    name.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To print the generated fantasy names in a human-readable form, we need to call
    the `get_sample()` function, which takes as input the list of indices previously
    generated and the decoding dictionary mapping indices to characters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'With everything in place, you can now appreciate some original fantasy character
    names, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f212619fc0a1124dde79edd34f56699.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating Model Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'During the first phases of the training process, the model is still unable
    to effectively mimic Tolkien’s style. If we sample random names at this stage
    we obtain pure gibberish such as:'
  prefs: []
  type: TYPE_NORMAL
- en: “Orvnnvfufufiiubx” at iteration 2000 and loss 27.96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Aotvux” at iteration 4000 and loss 26.43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can clearly see how these names don’t meet the iconic Middle Earth patterns
    and sounds. However, by letting the model learn the dataset features for a longer
    period of time, we start to obtain progressively more plausible generated names:'
  prefs: []
  type: TYPE_NORMAL
- en: “Furun I” at iteration 14000 and loss 21.53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Flutto Balger” at iteration 16000 and loss 21.11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the trained model seems to be able to emulate the characters’ name
    style.
  prefs: []
  type: TYPE_NORMAL
- en: The improvement I described qualitatively can be also quantitatively visualized
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: By plotting the loss function through the iterations, we can clearly see how
    the [optimization algorithm](/choose-the-right-optimization-algorithm-for-your-neural-network-cb86c15d7328)
    progressively tunes the weights and biases in the right direction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ce95057b5b483abdad50878102acc54.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The oscillating behavior of the loss function is motivated by the fact that
    we are using a single training example in each iteration step to train the model.
    Using a larger batch would result in a smoother loss curve.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, building a Language Model for fantasy name generation has brought
    us several insights.
  prefs: []
  type: TYPE_NORMAL
- en: We discovered how RNNs and LMs are capable of appreciating sequential dependencies
    in the data, making them essential elements for NLP and all the tasks that involve
    text.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, I cannot enphasize more the importance of an hands-on approach to
    gain experience in any Data Science related subject. Studying the theory is fundamental,
    but provides you only half of what you need.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I want to stretch again the flexibility of the tool we just created.
    I recommend training it with a different set of names. Try using Disney character
    names or typical pet names as the input dataset and share what you generate with
    me!
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this story, consider following me to be notified of my upcoming
    projects and articles!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of my past projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----792b13629efa--------------------------------)
    [## Ensemble Learning with Scikit-Learn: A Friendly Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning algorithms like XGBoost or Random Forests are among the top-performing
    models in Kaggle competitions…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c?source=post_page-----792b13629efa--------------------------------)
    [](/euro-trip-optimization-genetic-algorithms-and-google-maps-python-api-solve-the-traveling-salesman-4ad8e1548207?source=post_page-----792b13629efa--------------------------------)
    [## Euro Trip Optimization: Genetic Algorithms and Google Maps API Solve the Traveling
    Salesman Problem'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate the charm of Europe’s 50 most visited cities using genetic algorithms
    and Google Maps API, unlocking efficient…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/euro-trip-optimization-genetic-algorithms-and-google-maps-python-api-solve-the-traveling-salesman-4ad8e1548207?source=post_page-----792b13629efa--------------------------------)
    [](/the-birth-of-data-science-historys-first-hypothesis-test-python-insights-4745dccaf6d?source=post_page-----792b13629efa--------------------------------)
    [## The Birth of Data Science: History’s First Hypothesis Test & Python Insights'
  prefs: []
  type: TYPE_NORMAL
- en: Dive into Python-powered insights that every data scientist needs to know
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-birth-of-data-science-historys-first-hypothesis-test-python-insights-4745dccaf6d?source=post_page-----792b13629efa--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[GitHub Repository](https://github.com/andreoniriccardo/articles/tree/main/names-generator-RNN)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LOTR character names](https://www.behindthename.com/namesakes/list/tolkien/alpha)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Learning Course](https://www.coursera.org/specializations/deep-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Recurrent Neural Network](https://en.wikipedia.org/wiki/Recurrent_neural_network)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[BeautifulSoup](https://pypi.org/project/beautifulsoup4/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
