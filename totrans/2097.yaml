- en: Through the Looking Glass, and What Google find there in the eye
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/through-the-looking-glass-and-what-google-find-there-in-the-eye-e7a836eb9571](https://towardsdatascience.com/through-the-looking-glass-and-what-google-find-there-in-the-eye-e7a836eb9571)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| COMPUTER VISION | AI IN HEALTHCARE | CNN'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Or How Google is Using Deep Learning to Diagnose Diseases in Eye Photos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----e7a836eb9571--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----e7a836eb9571--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e7a836eb9571--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e7a836eb9571--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----e7a836eb9571--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e7a836eb9571--------------------------------)
    ·12 min read·Mar 30, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4f232c4e1c318f5fce9ec857c2775d1.png)'
  prefs: []
  type: TYPE_IMG
- en: image by the author using OpenAI DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Google recently published a scientific paper showing how an artificial intelligence
    model is able to predict a number of systemic biomarkers from a simple photo of
    the eye.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work? How were such results arrived at? Why is it important? We
    discuss this in this article.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The hidden treasure in the eyes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/35db24ac8670c2348b942cd5ebabcba1.png)'
  prefs: []
  type: TYPE_IMG
- en: image by [v2osk](https://unsplash.com/it/@v2osk) on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: Diagnosis of disease often requires examinations with expensive instruments
    and then interpretation by a medical professional who is trained. This is not
    always possible. Not all hospitals have the same instruments and sometimes there
    is a shortage of specialists.
  prefs: []
  type: TYPE_NORMAL
- en: For example, [diabetic retinopathy (DR)](https://en.wikipedia.org/wiki/Diabetic_retinopathy)
    diagnosis requires a fundus camera that examines the back of the eye. This then
    requires it to be analyzed by a highly qualified person. This examination can
    also highlight other conditions such as cardiovascular risk, anemia, chronic kidney
    disease, and other systemic parameters.
  prefs: []
  type: TYPE_NORMAL
- en: It was thought that images, of the fundus of the eye, can be analyzed using
    machine learning algorithms. However, [a paper published by Google](https://www.nature.com/articles/s41551-018-0195-0.epdf?author_access_token=YWBi0EzCgfAVb_S540xl-tRgN0jAjWel9jnR3ZoTv0OMsbBDq-7d5VZef-dAA8S4kHGY_hXONc93gwXXjuO908b_ruUDVkgB5jW3RnvvRdLFLmvpTsPku5cXZoTEtr09fPvTK40ZbWzpoOGfLab-NA%3D%3D)
    in 2017 showed that external photographs of the eye can enable the diagnosis of
    diabetic retinal disease and detect poor blood sugar control.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63c3f7204bfb6f3869cbebcd9c593a1a.png)'
  prefs: []
  type: TYPE_IMG
- en: '“Diabetes-related complications can be diagnosed by using specialized cameras
    to take fundus photographs, which visualize the posterior segment of the eye.
    By contrast, anterior imaging using a standard consumer-grade camera can reveal
    conditions affecting the eyelids, conjunctiva, cornea and lens.”. image source:
    [here](https://europepmc.org/article/med/35352000)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, the authors used photos of 145,832 patients with diabetes from
    California and additional cohorts. The authors then used [Inception V3](https://en.wikipedia.org/wiki/Inceptionv3)
    (which had been trained previously on [ImageNet](https://en.wikipedia.org/wiki/ImageNet))
    for this study, showing:'
  prefs: []
  type: TYPE_NORMAL
- en: Our results show that external images of the eye contain signals of diabetes-related
    retinal conditions, poor blood sugar control and elevated lipids. ([source](https://europepmc.org/article/med/35352000))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Briefly, the [Inception V3](https://arxiv.org/pdf/1512.00567.pdf) showed at
    the time state-of-the-art performance on ImageNet (accuracy > 78.1 %). Moreover,
    Inception was more computationally efficient than previous models. The model reached
    these results using parallel structures (different types of [convolution](https://en.wikipedia.org/wiki/Convolution)
    layers in the same block) and aggressive [regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)).
    In the same article, the authors defined some principles that shaped the [convolutional
    neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN)
    field for the following years:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid representational bottlenecks. The representation size should decrease
    gently from the inputs to the outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher dimensional representations are easier to process locally within a network.
    This showed to allow us to train faster the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spatial aggregation allows a better reduction of dimension without loss of information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balance the width and depth of the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/7a5263c9e49e8ffd7d07d770761d3768.png)'
  prefs: []
  type: TYPE_IMG
- en: '“A high-level diagram of the model” image source: [here](https://cloud.google.com/tpu/docs/inception-v3-advanced)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors used classical supervised learning, in fact, they used images of
    the patient’s eyes as ground truth whether they had a disease (diabetic retinal
    disease, elevated glucose, or elevated lipids). The model thus trained shows an
    area under the curve (AUC) of more than 80 percent for diabetic retinal disease
    diagnosis and prominent (but lower) results for glucose and lipids.
  prefs: []
  type: TYPE_NORMAL
- en: Results are surprising, because typically these kinds of systemic parameters
    can be derived from the front eye, and instead, this first study showed that from
    photos of the outer eye, the same can be derived through deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, using [ablation studies](https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence))
    and [saliency maps](https://en.wikipedia.org/wiki/Saliency_map) the authors can
    also better understand why the model makes these predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the ablation analysis indicates the centre of the image (pupil/lens,
    iris/cornea and conjunctiva/sclera) is substantially more important than the image
    periphery (for example, eyelids) for all predictions. Second, the saliency analysis
    similarly indicates that the DLS is most influenced by areas near the centre of
    the image. ([source](https://europepmc.org/article/med/35352000))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e5c51685f8324c3a4046b59fb3327d1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Saliency map. image source: [here](https://europepmc.org/article/med/35352000)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dcf74de03ebebe8ff23339e2601cae14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'ablation study: Importance of different regions of the image. image source:
    [here](https://europepmc.org/article/med/35352000)'
  prefs: []
  type: TYPE_NORMAL
- en: These results show that for the growing population of diabetics, some parameters
    can be measured without the need for specialized medical personnel. In addition,
    photos of the outer eye could also be obtained using photos with simple cameras.
  prefs: []
  type: TYPE_NORMAL
- en: While further work is needed to determine whether there are additional requirements
    for lighting, photography distance or angle, image stabilization, lens quality
    or sensor fidelity, we hope that disease detection techniques via external eye
    images can eventually be widely accessible to patients, whether in clinics, pharmacies
    or even at home. ([source](https://europepmc.org/article/med/35352000))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In any case, at present these models are not intended to replace extensive screening
    but rather to signal which patients would benefit from further screening (this
    method is more reliable than a questionnaire).
  prefs: []
  type: TYPE_NORMAL
- en: The authors continued to evaluate the model and drew attention to [potential
    biases and inclusions](https://www.forbes.com/sites/bernardmarr/2022/09/30/the-problem-with-biased-ais-and-how-to-make-ai-better/?sh=1ced823c4770).
    Indeed, one of the biggest problems with artificial intelligence models in the
    biomedical field is that if the dataset is not representative of the general population
    this can lead to misleading results.
  prefs: []
  type: TYPE_NORMAL
- en: our development dataset spanned a diverse set of locations within the U.S.,
    encompassing over 300,000 de-identified photos taken at 301 diabetic retinopathy
    screening sites. Our evaluation datasets comprised over 95,000 images from 198
    sites in 18 US states, including datasets of predominantly Hispanic or Latino
    patients, a dataset of majority Black patients, and a dataset that included patients
    without diabetes. We conducted extensive subgroup analyses across groups of patients
    with different demographic and physical characteristics (such as age, sex, race
    and ethnicity, presence of cataract, pupil size, and even camera type), and controlled
    for these variables as covariates. ([source](https://ai.googleblog.com/2022/03/detecting-signs-of-disease-from.html))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The eye, a mirror for the soul
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b22db7de19e379484e3a775415a6e26a.png)'
  prefs: []
  type: TYPE_IMG
- en: image by [Caroline Veronez](https://unsplash.com/it/@carolineveronez) on Unpslash
  prefs: []
  type: TYPE_NORMAL
- en: The authors at Google and other researchers also considered promising this approach.
    So they later attempted to extend it to other markers and other diseases.
  prefs: []
  type: TYPE_NORMAL
- en: So far, the authors have shown that their model is capable of efficiently diagnosing
    eye diseases (diabetic retinopathy). On the other hand, there are thousands of
    diseases, and diagnosing them is complex (locating the right test, expensive instruments
    not always present, and so on). So the question remains whether the model can
    also capture signs of other diseases in the image of the eye.
  prefs: []
  type: TYPE_NORMAL
- en: Can we extend this approach to other diseases?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: After all, deep learning models can recognize patterns that are subtle and perhaps
    difficult for nonexperts to recognize. With these assumptions in mind, [Google
    researchers decided to test](https://www.nature.com/articles/s41551-018-0195-0)
    whether cardiovascular risk factors could be detected in ocular fundus images.
  prefs: []
  type: TYPE_NORMAL
- en: Cardiovascular disease is the [leading global cause of death](https://pubmed.ncbi.nlm.nih.gov/32501203/),
    and being able to diagnose it early could save countless lives. In addition, [risk
    stratification](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8299990/) is key
    to identifying and managing groups of patients at risk. Typically, a number of
    variables obtained through medical history and different tests (blood samples
    for glucose and cholesterol levels, age, gender, smoking status, blood pressure,
    and body mass index) are used to diagnose and stratify patients. Sometimes all
    the necessary data are not present ([as shown by a metastudy](https://pubmed.ncbi.nlm.nih.gov/25593051/)).
  prefs: []
  type: TYPE_NORMAL
- en: The authors in this study show that it is possible not only to predict some
    patient characteristics (useful in case some data are not recorded or are missing)
    such as [BMI](https://www.nhlbi.nih.gov/health/educational/lose_wt/BMI/bmicalc.htm),
    age, gender, and smoking status but also parameters associated with cardiovascular
    diseases such as [systolic blood pressure](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1124431/)
    (SBP) and [diastolic blood pressure](https://www.webmd.com/hypertension-high-blood-pressure/guide/diastolic-and-systolic-blood-pressure-know-your-numbers)
    (DBP).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4442b4bc8dac94439a3a2321ec8d27f9.png)'
  prefs: []
  type: TYPE_IMG
- en: '“The top left image is a sample retinal image in colour from the UK Biobank
    dataset. The remaining images show the same retinal image, but in black and white.
    The soft attention heat map for each prediction is overlaid in green, indicating
    the areas of the heat map that the neural-network model is using to make the prediction
    for the image.” image source: [here](https://arxiv.org/ftp/arxiv/papers/1708/1708.09843.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors have the same Inception V3 model as well. In addition, to deal with
    continuous variables the authors used binning, basically, they divided the variable
    into different segments using different cut-offs (for example, <120, 120–140,
    140–160, and ≥160 for SBP).
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the authors used a technique called [soft attention](https://arxiv.org/pdf/1502.03044.pdf)
    to identify regions that are associated with certain features. In short, [soft
    attention](https://arxiv.org/pdf/1507.01053.pdf) is a method that takes into account
    different subregions of the images and uses [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)
    and [back-propagation](https://en.wikipedia.org/wiki/Backpropagation) (no need
    to implement an attention mechanism in the model).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbea3de2132c10d1dc35e7d5c8bd32fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'soft-attention can allow to spot prediction mistakes of the model. image source:
    [here](https://arxiv.org/pdf/1502.03044.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In another work, the authors tested with [anemia](https://en.wikipedia.org/wiki/Anemia).
    A condition, which afflicts [more than 1.6 billion](https://www.cambridge.org/core/journals/public-health-nutrition/article/worldwide-prevalence-of-anaemia-who-vitamin-and-mineral-nutrition-information-system-19932005/E201EDE33949AF3D632F6596052FCF8F)
    people and requires monitoring [hemoglobin](https://en.wikipedia.org/wiki/Hemoglobin)
    concentration in the blood to be diagnosed (an invasive test,c which can cause
    pain and risk of infection).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57786f580f3571154ef396c9b0be2162.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Prediction of anemia classifications with deep learning. Image source: [here](https://arxiv.org/ftp/arxiv/papers/1904/1904.06435.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, they used [Inception V4](https://arxiv.org/pdf/1602.07261.pdf).
    A later version of the model that was described earlier (in this follow-up article,
    the authors describe that the architecture of Inception V3 can be improved by
    adding [residual connections](https://arxiv.org/pdf/1512.03385.pdf)). Inception
    V4 shows how different types of Inception blocks (in which there are different
    layers of convolution both parallel and sequential) can be tested and used.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da7f0c17b72ccb25c495b12aa3ef67ac.png)'
  prefs: []
  type: TYPE_IMG
- en: '. Residual connections. Image source: [here](https://arxiv.org/pdf/1602.07261.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In this later work, [Google shows that the approach is not limited to classification](https://www.nature.com/articles/s41551-019-0487-z)
    (patient has anemia or not) but also to whether [hemoglobin concentration](https://www.ncbi.nlm.nih.gov/books/NBK259/)
    can be measured (regression task). The authors train a model for classification
    and one for the [regression task](https://www.sciencedirect.com/topics/computer-science/regression-task)
    (Inception V4 which was pre-trained on ImageNet).
  prefs: []
  type: TYPE_NORMAL
- en: For the regression, the authors simply used [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error)
    as the loss (instead of [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy)).
    The final predictions were made created an [ensemble](https://en.wikipedia.org/wiki/Ensemble_learning)
    of 10 models (trained in the same manner) and the outputs were averaged to yield
    the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6ebd0b5261865d71c9f9766c4fc3eab.png)'
  prefs: []
  type: TYPE_IMG
- en: '“Prediction of hemoglobin concentration. Each blue dot represents each patient’s
    measured hemoglobin concentration and predicted value”. Image source: [here](https://arxiv.org/ftp/arxiv/papers/1904/1904.06435.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: A glimpse of a wide landscape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/a30a4162cdd27117d2c7a2577b96dcb2.png)'
  prefs: []
  type: TYPE_IMG
- en: image by [Bailey Zindel](https://unsplash.com/it/@baileyzindel) on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: So far what researchers have observed has been a few parameters at a time. Typically,
    though, a blood test allows them to monitor many more parameters in a single exam.
    **Can a model from a photo of the eye estimate a panel of systemic biomarkers?**
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what Google tested this year and has just published:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.thelancet.com/journals/landig/article/PIIS2589-7500%2823%2900022-5/fulltext?source=post_page-----e7a836eb9571--------------------------------)
    [## A deep learning model for novel systemic biomarkers in photographs of the
    external eye: a…'
  prefs: []
  type: TYPE_NORMAL
- en: Ocular sequelae resulting from systemic disease have been well documented and
    are the basis for globally established…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.thelancet.com](https://www.thelancet.com/journals/landig/article/PIIS2589-7500%2823%2900022-5/fulltext?source=post_page-----e7a836eb9571--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, this is not an easy task, partly because when you want to conduct
    such an analysis there is a risk of finding a spurious and erroneous result (also
    called [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem)).
    In other words, the greater the number of statistical inferences conducted at
    the same time, the greater the risk of finding erroneous inferences.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92c4ef6295751a4647815166d0fd6d2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'example of spurious correlation. image source: [here](https://en.wikipedia.org/wiki/Multiple_comparisons_problem)'
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, the authors first divided the dataset into two parts. They
    trained the model and conducted the analyses on the “development dataset,” selected
    the nine most promising prediction tasks, and evaluated the model on the test
    dataset (they still [corrected for multiple comparisons](https://en.wikipedia.org/wiki/Bonferroni_correction)).
  prefs: []
  type: TYPE_NORMAL
- en: They first collected a dataset that contained eye images and the results of
    corresponding laboratory tests. The authors then trained a convolutional neural
    network that takes as input an image of the outer eye and predicts clinical and
    laboratory measurements. It is in this case a multitask classification, in which
    there is a prediction head for each task (so that cross-entropy can be used as
    a loss). The authors decided to select cut-offs for each task (selected in consultation
    with clinicians).
  prefs: []
  type: TYPE_NORMAL
- en: The authors, in this case, used Big Transfer (BiT), a model published in 2020
    that when trained generalized well across a range of other datasets. The model,
    in short, is very similar to ResNet, though for training they used some tricks
    such as [Group Normalization](https://arxiv.org/pdf/1803.08494.pdf) (GN) and [Weight
    Standardization](https://arxiv.org/pdf/1405.0312.pdf) (WS). You can find the model
    in this [GitHub repository](https://github.com/google-research/big_transfer).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24d9d9ab63cdb4ca1f761a1c10e58d66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Transfer performance of the pre-trained model, beating the state-of-the-art.
    image source: [here](https://arxiv.org/pdf/1912.11370.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The outperformed the baseline model (logistic regression on patient data). Although
    these results are still insufficient for the diagnostic application, they are
    in line with initial screening tools ([pre-screening for diabetes](https://www.bmj.com/content/343/bmj.d7163.long)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a1481a4c32a3926055f75c372d02968.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Comparison of AUC of the baseline model and the deep learning model. image
    source:** [**here**](https://arxiv.org/ftp/arxiv/papers/2207/2207.08998.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In this and the previous study, the authors acquired images using tabletop cameras
    (also using a headrest for the patient) and produced high-quality images under
    good lighting conditions. Therefore, the authors tried to see if the model worked
    by reducing the resolution.
  prefs: []
  type: TYPE_NORMAL
- en: The authors noted that the pattern is robust to image quality, even when images
    are scaled down to 150x150 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: This pixel count is under 0.1 megapixels, much smaller than the typical smartphone
    camera. ([source](https://ai.googleblog.com/2023/03/detecting-novel-systemic-biomarkers-in.html))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/4a1892b8042158695500778e524bd56b.png)'
  prefs: []
  type: TYPE_IMG
- en: '“*Effect of input image resolution.* ***Top:*** *Sample images scaled to different
    sizes for this experiment.* ***Bottom****: Comparison of the performance of the
    DLS with image size*”. image source: [here](https://arxiv.org/ftp/arxiv/papers/2207/2207.08998.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the authors investigated which part of the image is important for
    the purpose of prediction for the model. For this reason, the authors masked several
    regions during both training and evaluation (the pupil or iris, transformed the
    image to black and white)
  prefs: []
  type: TYPE_NORMAL
- en: Results suggested that the information is generally not isolated to only the
    pupil or iris, and that colour information is at least somewhat important for
    most prediction targets ([source](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(23)00022-5/fulltext#seccestitle160))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/de4a2ceafc857c6491b9086938795f16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Experiments masking different image regions or removing colour**.** image source:
    [here](https://arxiv.org/ftp/arxiv/papers/2207/2207.08998.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: As impressive as this article is, it still has limitations. In fact, it is still
    premature to think that it can be used in the real world. First, the photos were
    obtained under optimal conditions, and we need to verify the accuracy with photos
    obtained under other conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the datasets used in this work consist primarily of patients with
    diabetes and did not have sufficient representation of a number of important subgroups
    — more focused data collection for DLS refinement and evaluation on a more general
    population and across subgroups will be needed before considering clinical use.
    ([source](https://ai.googleblog.com/2023/03/detecting-novel-systemic-biomarkers-in.html))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Parting thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c7c5a81296fe18f347780466460582b4.png)'
  prefs: []
  type: TYPE_IMG
- en: image from [Saif71.com](https://unsplash.com/it/@saif71) on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: As seen in this article, a deep learning model is capable of capturing patterns
    and information in the eye that is sometimes difficult to diagnose. In addition,
    diagnosis is often conducted using expensive tools, and invasive testing, and
    requires experienced personnel. Google has shown over the years that instead,
    it is possible to obtain similar information using the image of the eye.
  prefs: []
  type: TYPE_NORMAL
- en: In the future, many diseases could be diagnosed (or at least pre-screened) using
    a simple photo of the outer eye. These photos could be captured simply using a
    cell phone camera. Also, since quantitative results (such as hemoglobin concentration)
    can be obtained, they could be used for noninvasive patient monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: On the technical side, it is interesting how a model such as Incenption V3 achieved
    results on the first attempt. This shows how transfer learning and convolutional
    networks are capable. In addition, the authors adapted the models for both classification
    and regression and multi-tasking classification.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are several scenarios open. Certainly, Google plans to expand
    the dataset (as they wrote in the article). On the other hand, the authors used
    CNN and could also test several other models such as the Vision Transformers.
    Also, it is not excluded that in the future they will experiment with a language
    model that uses patient or doctor’s notes as input (after all, the future is multi-modal).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, even though these applications could be used where patients
    do not have equipped hospitals, it also opens up ethical issues. As seen, the
    model is also capable of predicting sensitive data such as age, gender, lifestyle
    (smoking/not smoking), and other parameters. This technology could also be used
    for other, more problematic applications.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, these studies open very interesting applications. It is not just
    Google working on such models. For example, other groups have shown that other
    diseases [such as hepatobiliary diseases](https://pubmed.ncbi.nlm.nih.gov/33509389/)
    can be identified from the eye, and [others may be as well](https://pubmed.ncbi.nlm.nih.gov/10758212/)
    in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have found this interesting:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can look for my other articles, you can also [**subscribe**](https://salvatore-raieli.medium.com/subscribe)
    to get notified when I publish articles, and you can also connect or reach me
    on[**LinkedIn**](https://www.linkedin.com/in/salvatore-raieli/)**.**
  prefs: []
  type: TYPE_NORMAL
- en: Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----e7a836eb9571--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----e7a836eb9571--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'or you may be interested in one of my recent articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/making-language-models-similar-to-human-brain-b6ea8270be08?source=post_page-----e7a836eb9571--------------------------------)
    [## Making Language Models Similar to the Human Brain'
  prefs: []
  type: TYPE_NORMAL
- en: There is still a gap between LMs and the human brain in NLP, inspiring AI to
    the latter could fill it
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/making-language-models-similar-to-human-brain-b6ea8270be08?source=post_page-----e7a836eb9571--------------------------------)
    [](/google-med-palm-the-ai-clinician-a4482143d60e?source=post_page-----e7a836eb9571--------------------------------)
    [## Google Med-PaLM: The AI Clinician'
  prefs: []
  type: TYPE_NORMAL
- en: Google's new model is trained to answer medical questions. How?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/google-med-palm-the-ai-clinician-a4482143d60e?source=post_page-----e7a836eb9571--------------------------------)
    [](/multimodal-chain-of-thoughts-solving-problems-in-a-multimodal-world-961a8ab9d0fa?source=post_page-----e7a836eb9571--------------------------------)
    [## Multimodal Chain of Thoughts: Solving Problems in a Multimodal World'
  prefs: []
  type: TYPE_NORMAL
- en: 'The world is not only text: How to extend the chain of thoughts to image and
    text?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/multimodal-chain-of-thoughts-solving-problems-in-a-multimodal-world-961a8ab9d0fa?source=post_page-----e7a836eb9571--------------------------------)
    [](https://levelup.gitconnected.com/stable-diffusion-to-fill-gaps-in-medical-image-data-b78a2a7d6c9d?source=post_page-----e7a836eb9571--------------------------------)
    [## Stable diffusion to fill gaps in medical image data
  prefs: []
  type: TYPE_NORMAL
- en: A new study shows that stable diffusion could help with medical image analysis
    and rare diseases. How?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/stable-diffusion-to-fill-gaps-in-medical-image-data-b78a2a7d6c9d?source=post_page-----e7a836eb9571--------------------------------)
  prefs: []
  type: TYPE_NORMAL
