- en: Building a Conformal Chatbot in Julia
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Julia中构建一个符合预测的聊天机器人
- en: 原文：[https://towardsdatascience.com/building-a-conformal-chatbot-in-julia-1ed23363a280](https://towardsdatascience.com/building-a-conformal-chatbot-in-julia-1ed23363a280)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-a-conformal-chatbot-in-julia-1ed23363a280](https://towardsdatascience.com/building-a-conformal-chatbot-in-julia-1ed23363a280)
- en: Conformal Prediction, LLMs and HuggingFace — Part 1
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 符合预测、LLMs和HuggingFace — 第1部分
- en: '[](https://medium.com/@patrick.altmeyer?source=post_page-----1ed23363a280--------------------------------)[![Patrick
    Altmeyer](../Images/b4c0bd875390f6dc8b81480f0712fea5.png)](https://medium.com/@patrick.altmeyer?source=post_page-----1ed23363a280--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ed23363a280--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ed23363a280--------------------------------)
    [Patrick Altmeyer](https://medium.com/@patrick.altmeyer?source=post_page-----1ed23363a280--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@patrick.altmeyer?source=post_page-----1ed23363a280--------------------------------)[![Patrick
    Altmeyer](../Images/b4c0bd875390f6dc8b81480f0712fea5.png)](https://medium.com/@patrick.altmeyer?source=post_page-----1ed23363a280--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ed23363a280--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ed23363a280--------------------------------)
    [Patrick Altmeyer](https://medium.com/@patrick.altmeyer?source=post_page-----1ed23363a280--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ed23363a280--------------------------------)
    ·7 min read·Jul 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ed23363a280--------------------------------)
    ·阅读时间7分钟·2023年7月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Large Language Models (LLM) are all the buzz right now. They are used for a
    variety of tasks, including text classification, question answering, and text
    generation. In this tutorial, we will show how to conformalize a transformer language
    model for text classification using `[ConformalPrediction.jl](https://juliatrustworthyai.github.io/ConformalPrediction.jl/dev/)`.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）目前非常受关注。它们被用于各种任务，包括文本分类、问答和文本生成。在本教程中，我们将展示如何使用`[ConformalPrediction.jl](https://juliatrustworthyai.github.io/ConformalPrediction.jl/dev/)`将变换器语言模型符合化，以进行文本分类。
- en: 👀 At a Glance
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 👀 一览
- en: In particular, we are interested in the task of intent classification as illustrated
    in the sketch below. Firstly, we feed a customer query into an LLM to generate
    embeddings. Next, we train a classifier to match these embeddings to possible
    intents. Of course, for this supervised learning problem we need training data
    consisting of inputs — queries — and outputs — labels indicating the true intent.
    Finally, we apply Conformal Predition to quantify the predictive uncertainty of
    our classifier.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们特别关注意图分类任务，如下图所示。首先，我们将客户查询输入到LLM中以生成嵌入。接着，我们训练一个分类器，将这些嵌入与可能的意图匹配。当然，对于这个监督学习问题，我们需要由输入——查询——和输出——指示真实意图的标签——组成的训练数据。最后，我们应用符合预测来量化分类器的预测不确定性。
- en: Conformal Prediction (CP) is a rapidly emerging methodology for Predictive Uncertainty
    Quantification. If you’re unfamiliar with CP, you may want to first check out
    my 3-part introductory series on the topic starting with this [post](https://medium.com/towards-data-science/conformal-prediction-in-julia-351b81309e30).
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 符合预测（CP）是一种快速发展的预测不确定性量化方法。如果你不熟悉CP，建议你首先查看我关于这一主题的三部分介绍系列，从这篇[文章](https://medium.com/towards-data-science/conformal-prediction-in-julia-351b81309e30)开始。
- en: '![](../Images/2bdcbc51d0e1128fa62d46fefe8a4188.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bdcbc51d0e1128fa62d46fefe8a4188.png)'
- en: High-level overview of a conformalized intent classifier. Image by author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 符合化意图分类器的高级概述。图片由作者提供。
- en: 🤗 HuggingFace
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤗 HuggingFace
- en: We will use the [Banking77](https://arxiv.org/abs/2003.04807) dataset (Casanueva
    et al., 2020), which consists of 13,083 queries from 77 intents related to banking.
    On the model side, we will use the [DistilRoBERTa](https://huggingface.co/mrm8488/distilroberta-finetuned-banking77)
    model, which is a distilled version of [RoBERTa](https://arxiv.org/abs/1907.11692)
    (Liu et al., 2019) fine-tuned on the Banking77 dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[Banking77](https://arxiv.org/abs/2003.04807)数据集（Casanueva等，2020），该数据集包含77个与银行相关的意图中的13,083个查询。在模型方面，我们将使用[DistilRoBERTa](https://huggingface.co/mrm8488/distilroberta-finetuned-banking77)模型，它是[RoBERTa](https://arxiv.org/abs/1907.11692)（Liu等，2019）的蒸馏版，并在Banking77数据集上进行了微调。
- en: The model can be loaded from HF straight into our running Julia session using
    the `[Transformers.jl](https://github.com/chengchingwen/Transformers.jl/tree/master)`
    package.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `[Transformers.jl](https://github.com/chengchingwen/Transformers.jl/tree/master)`
    包将模型从HF直接加载到我们正在运行的Julia会话中。
- en: This package makes working with HF models remarkably easy in Julia. Kudos to
    the devs! *🙏*
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个包使得在Julia中使用HF模型变得非常简单。向开发者们致敬！*🙏*
- en: Below we load the tokenizer `tkr` and the model `mod`. The tokenizer is used
    to convert the text into a sequence of integers, which is then fed into the model.
    The model outputs a hidden state, which is then fed into a classifier to get the
    logits for each class. Finally, the logits are then passed through a softmax function
    to get the corresponding predicted probabilities. Below we run a few queries through
    the model to see how it performs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们加载分词器`tkr`和模型`mod`。分词器用于将文本转换为整数序列，然后将其输入模型。模型输出一个隐藏状态，然后将其输入分类器，以获得每个类别的logits。最后，这些logits通过softmax函数以获得相应的预测概率。下面我们运行几个查询来查看模型的表现。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 🔁 `MLJ` Interface
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🔁 `MLJ`接口
- en: Since our package is interfaced to `[MLJ.jl](https://alan-turing-institute.github.io/MLJ.jl/dev/)`,
    we need to define a wrapper model that conforms to the `MLJ` interface. In order
    to add the model for general use, we would probably go through `[MLJFlux.jl](https://github.com/FluxML/MLJFlux.jl)`,
    but for this tutorial, we will make our life easy and simply overload the `MLJBase.fit`
    and `MLJBase.predict` methods.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的包与 `[MLJ.jl](https://alan-turing-institute.github.io/MLJ.jl/dev/)` 接口对接，我们需要定义一个符合`MLJ`接口的包装模型。为了将模型添加到通用使用中，我们可能会通过
    `[MLJFlux.jl](https://github.com/FluxML/MLJFlux.jl)` 来实现，但在本教程中，我们将简化操作，直接重载`MLJBase.fit`和`MLJBase.predict`方法。
- en: Since the model from HF is already pre-trained and we are not interested in
    further fine-tuning, we will simply return the model object in the `MLJBase.fit`
    method. The `MLJBase.predict` method will then take the model object and the query
    and return the predicted probabilities. We also need to define the `MLJBase.target_scitype`
    and `MLJBase.predict_mode` methods. The former tells `MLJ` what the output type
    of the model is, and the latter can be used to retrieve the label with the highest
    predicted probability.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于HF的模型已经是预训练的，我们不打算进一步微调，因此我们将在`MLJBase.fit`方法中简单地返回模型对象。`MLJBase.predict`方法将接收模型对象和查询，并返回预测概率。我们还需要定义`MLJBase.target_scitype`和`MLJBase.predict_mode`方法。前者告诉`MLJ`模型的输出类型是什么，后者可以用来检索具有最高预测概率的标签。
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To test that everything is working as expected, we fit the model and generated
    predictions for a subset of the test data:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试一切是否按预期工作，我们拟合了模型并为测试数据的子集生成了预测：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that even though the LLM we’re using here isn’t really that large at all,
    even a simple forward pass does take considerable time.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，即使我们使用的LLM并不大，但即使是简单的前向传递也需要相当的时间。
- en: 🤖 Conformal Chatbot
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤖 合成聊天机器人
- en: To turn the wrapped, pre-trained model into a conformal intent classifier, we
    can now rely on standard API calls. We first wrap our atomic model where we also
    specify the desired coverage rate and method. Since even simple forward passes
    are computationally expensive for our (small) LLM, we rely on Simple Inductive
    Conformal Classification.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将包装好的预训练模型转变为合成意图分类器，我们现在可以依靠标准API调用。我们首先包装我们的原子模型，并指定所需的覆盖率和方法。由于即使是简单的前向传递对我们（小）LLM来说也非常计算密集，我们依赖于简单归纳合成分类。
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we use our conformal LLM to build a simple yet powerful chatbot that
    runs directly in the Julia REPL. Without dwelling on the details too much, the
    `conformal_chatbot` works as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用合成LLM构建一个简单而强大的聊天机器人，直接在Julia REPL中运行。在不详细探讨细节的情况下，`conformal_chatbot`
    的工作原理如下：
- en: Prompt user to explain their intent.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示用户解释他们的意图。
- en: Feed user input through conformal LLM and present the output to the user.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过合成LLM处理用户输入，并将输出呈现给用户。
- en: If the conformal prediction set includes more than one label, prompt the user
    to either refine their input or choose one of the options included in the set.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果合成预测集包含多个标签，请提示用户要么细化输入，要么选择预测集中的选项之一。
- en: 'The following code implements these ideas:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码实现了这些想法：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Below we show the output for two example queries. The first one is very ambiguous
    (and misspelled as I just realised): “transfer mondey?”. As expected, the size
    of the prediction set is therefore large.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们展示了两个示例查询的输出。第一个查询非常模糊（而且刚刚发现拼写错误）：“transfer mondey？”。因此，预测集的大小很大。
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is a more refined version of the prompt: “I tried to transfer
    money to my friend, but it failed”. It yields a smaller prediction set, since
    less ambiguous prompts result in lower predictive uncertainty.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是更精炼的提示版本：“我试图给朋友转账，但失败了。” 由于不那么模糊的提示会导致较低的预测不确定性，因此它产生了较小的预测集。
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The video below shows the REPL-based chatbot in action. You can recreate this
    yourself and run the bot right from you terminal. To do so, just check out the
    original [post](https://www.paltmeyer.com/blog/posts/conformal-llm/) on my blog
    to find the full source code.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的视频展示了REPL基础聊天机器人在实际应用中的表现。你可以自己重现这个过程，并直接从你的终端运行机器人。为此，请查看我博客上的原始[帖子](https://www.paltmeyer.com/blog/posts/conformal-llm/)以获取完整的源代码。
- en: '![](../Images/397e9866869f4464276f0c5e5bb44e9c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/397e9866869f4464276f0c5e5bb44e9c.png)'
- en: Demo of the REPL-based Conformal Chatbot. Created by author.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: REPL基础的符合性聊天机器人的演示。由作者创建。
- en: 🌯 Wrapping Up
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🌯 总结
- en: This work was done in collaboration with colleagues at ING as part of the ING
    Analytics 2023 Experiment Week. Our team demonstrated that Conformal Prediction
    provides a powerful and principled alternative to top-*K* intent classification.
    We won the first prize by popular vote.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作是与ING的同事合作完成的，作为ING Analytics 2023实验周的一部分。我们的团队展示了符合性预测提供了对顶级-*K*意图分类的强大而有原则的替代方案。我们通过大众投票赢得了第一名。
- en: Of course, there are a lot of things that can be improved here. As far as Large
    LMs are concerned, we have used a small one. In terms of Conformal Prediction,
    we have only looked at simple inductive conformal classification. This is a good
    starting point, but there are more advanced methods available, which are implemented
    in the package and were investigated during the competition. Another thing we
    did not take into consideration here is that we have many outcome classes and
    may in practice be interested in achieving class-conditional coverage. Stay tuned
    for more on this in future posts.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这里还有很多可以改进的地方。就大型语言模型而言，我们使用了一个较小的模型。在符合性预测方面，我们只关注了简单的归纳符合性分类。这是一个好的起点，但还有更高级的方法可用，这些方法已经在软件包中实现，并在竞赛中进行了研究。另一个我们没有考虑的方面是我们有许多结果类别，实际上可能希望实现类别条件覆盖。请关注未来的帖子了解更多内容。
- en: If you’re interested in finding out more about Conformal Prediction in Julia,
    go ahead and check out the [repo](https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl)
    and [docs](https://juliatrustworthyai.github.io/ConformalPrediction.jl/dev/).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对在Julia中了解更多关于符合性预测的内容感兴趣，请查看[代码库](https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl)和[文档](https://juliatrustworthyai.github.io/ConformalPrediction.jl/dev/)。
- en: 🎉 JuliaCon 2023 is around the corner and this year I will be giving a [talk](https://pretalx.com/juliacon2023/talk/JQWNNP/)
    about *ConformalPrediction.jl.* Check out the details of my talk [here](https://pretalx.com/juliacon2023/talk/JQWNNP/)
    and have a look at the full jam-packed conference [schedule](https://pretalx.com/juliacon2023/schedule/).
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 🎉 JuliaCon 2023 即将到来，今年我将进行一场关于*ConformalPrediction.jl*的[讲座](https://pretalx.com/juliacon2023/talk/JQWNNP/)。请查看我的讲座[详细信息](https://pretalx.com/juliacon2023/talk/JQWNNP/)，并浏览内容丰富的会议[日程](https://pretalx.com/juliacon2023/schedule/)。
- en: 🎓 References
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🎓 参考文献
- en: 'Casanueva, Iñigo, Tadas Temčinas, Daniela Gerz, Matthew Henderson, and Ivan
    Vulić. 2020\. “Efficient Intent Detection with Dual Sentence Encoders.” In *Proceedings
    of the 2nd Workshop on Natural Language Processing for Conversational AI* , 38–45\.
    Online: Association for Computational Linguistics. [https://doi.org/10.18653/v1/2020.nlp4convai-1.5](https://doi.org/10.18653/v1/2020.nlp4convai-1.5).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Casanueva, Iñigo, Tadas Temčinas, Daniela Gerz, Matthew Henderson, 和 Ivan Vulić.
    2020\. “使用双句子编码器的高效意图检测。” *第二届对话AI自然语言处理研讨会论文集* , 38–45\. 在线：计算语言学协会. [https://doi.org/10.18653/v1/2020.nlp4convai-1.5](https://doi.org/10.18653/v1/2020.nlp4convai-1.5)。
- en: 'Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
    Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019\. “RoBERTa: A Robustly
    Optimized BERT Pretraining Approach.” arXiv. [https://doi.org/10.48550/arXiv.1907.11692](https://doi.org/10.48550/arXiv.1907.11692).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
    Levy, Mike Lewis, Luke Zettlemoyer, 和 Veselin Stoyanov. 2019\. “RoBERTa：一种稳健优化的BERT预训练方法。”
    arXiv. [https://doi.org/10.48550/arXiv.1907.11692](https://doi.org/10.48550/arXiv.1907.11692)。
- en: 💾 Data and Model
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 💾 数据和模型
- en: The [Banking77](https://arxiv.org/abs/2003.04807) dataset was retrieved from
    HuggingFace. It is published under the Creative Commons Attribution 4.0 International
    license (CC BY 4.0) and curated by [PolyAI](https://github.com/PolyAI-LDN) and
    was originally published by Casanueva et al. (2020). With thanks also to [Manuel
    Romero](https://twitter.com/mrm8488) who contributed the fine-tuned [DistilRoBERTa](https://huggingface.co/mrm8488/distilroberta-finetuned-banking77)
    to HuggingFace.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[Banking77](https://arxiv.org/abs/2003.04807) 数据集是从 HuggingFace 获取的。它在知识共享署名
    4.0 国际许可协议（CC BY 4.0）下发布，由 [PolyAI](https://github.com/PolyAI-LDN) 策划，并由 Casanueva
    等人（2020年）最初发布。还要感谢 [Manuel Romero](https://twitter.com/mrm8488) 为 HuggingFace
    贡献了经过微调的 [DistilRoBERTa](https://huggingface.co/mrm8488/distilroberta-finetuned-banking77)。'
- en: '*Originally published at* [*https://www.paltmeyer.com*](https://www.paltmeyer.com/blog/posts/conformal-llm/)
    *on July 5, 2023.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*最初发布于* [*https://www.paltmeyer.com*](https://www.paltmeyer.com/blog/posts/conformal-llm/)
    *于2023年7月5日。*'
