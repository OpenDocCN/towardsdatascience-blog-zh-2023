- en: Simplify Your Data Preparation with These Four Lesser-Known Scikit-Learn Classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/simplify-your-data-preparation-with-these-4-lesser-known-scikit-learn-classes-70270c94569f](https://towardsdatascience.com/simplify-your-data-preparation-with-these-4-lesser-known-scikit-learn-classes-70270c94569f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Forget train_test_split: Pipeline, ColumnTransformer, FeatureUnion and FunctionTransformer
    are indispensable even if you use XGBoost or LGBM'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mattchapmanmsc?source=post_page-----70270c94569f--------------------------------)[![Matt
    Chapman](../Images/7511deb8d9ed408ece21031f6614c532.png)](https://medium.com/@mattchapmanmsc?source=post_page-----70270c94569f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70270c94569f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70270c94569f--------------------------------)
    [Matt Chapman](https://medium.com/@mattchapmanmsc?source=post_page-----70270c94569f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----70270c94569f--------------------------------)
    ·10 min read·Jun 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d616242f15fc7f0163616870c608e44.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Victor](https://unsplash.com/@victor_g) on [Unsplash](https://unsplash.com/photos/UoIiVYka3VY)
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation is famously the least-loved aspect of Data Science. If done
    right, however, it needn’t be such a headache.
  prefs: []
  type: TYPE_NORMAL
- en: While scikit-learn has fallen out of vogue as a *modelling* library in recent
    years given the meteoric rise of PyTorch, LightGBM, and XGBoost, it’s still easily
    one of the best *data preparation* libraries out there.
  prefs: []
  type: TYPE_NORMAL
- en: 'And I’m not just talking about that old chestnut: `train_test_split`. If you’re
    prepared to dig a little deeper, you’ll find a treasure trove of helpful tools
    for more advanced data preparation techniques, all of which are perfectly compatible
    with using other libraries like `lightgbm`, `xgboost` and `catboost` for subsequent
    modelling.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I’ll walk through four scikit-learn classes which significantly
    speed up my data preparation workflows in my day-to-day job as a Data Scientist.
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Pipeline: Seamlessly combine preprocessing steps'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-learn’s `Pipeline` class enables you to combine different preprocessors
    or models into a single, callable chunk of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6aa21afcbb7383b916da6ee40e39478f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipelines can be composed of two different things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformer**: any object with the `fit()` and `transform()` methods. You
    can think of a transformer as an object that’s used for processing your data,
    and you will commonly have multiple transformers in your data preparation workflow.
    E.g., you might use one transformer to impute missing values, and another one
    to scale features or one-hot encode your categorical variables. `MinMaxScaler()`,
    `SimpleImputer()` and `OneHotEncoder()` are all examples of transformers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Estimator**: In scikit-learn lingo, an “estimator” usually means a machine
    learning model; i.e. an object with the `fit()` and `predict()` methods. `LinearRegression()`
    and `RandomForestClassifier()` are examples of estimators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a pipeline, you can chain together as many **transformers** as you like,
    enabling you to apply different data preprocessing steps sequentially. If you
    like, you can also add on an **estimator** (ML model) at the end in order to make
    predictions using the newly transformed data, but it’s not compulsory.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you could build a pipeline that first imputes missing values with
    zeros and then one-hot encodes your variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1755b225522bf4b8a22ed49b4b58cdcf.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Or, if you wanted to directly include the modelling in the pipeline itself,
    you could build a pipeline that imputes missing values with the mean, scales the
    features and then makes predictions using a `RandomForestRegressor()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d2adfd6be2a9c50f3bbede182aadabd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Building a pipeline with scikit-learn is remarkably simple.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, I’ll first load some data and split it into training and
    testing sets. In this example, I’ll use the [diabetes dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html)
    provided by scikit-learn, which contains ten predictor variables (*age, sex, body
    mass index, average blood pressure, and six blood serum measurements*) for 442
    diabetes patients and a response variable representing the progression of each
    patient’s diabetes one year after these predictor variables were recorded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/01d0df4d0bb03649f88d9ebe00313a71.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author. Diabetes dataset and scikit-learn are released under BSD-3
    license
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define our `Pipeline`. For now, I’ll just define a simple preprocessing
    `Pipeline` that includes two steps — impute missing values with the mean, and
    rescale all features — and I won’t include an estimator/model. The principles,
    however, are the same regardless of whether or not you include an estimator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we’ve defined our `Pipeline`, we “fit” it to our training dataset, and
    use it to transform both the training and testing datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This will give us two preprocessed datasets (`X_train_transformed` and `X_test_transformed`),
    ready for any subsequent steps like modelling or feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of using a `Pipeline` to handle these preprocessing steps is
    twofold:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Protect against leakage**: Because the preprocessor is fitted to the training
    dataset `X_train`, no information about the test set is “leaked” when imputing
    missing values or creating one-hot encoded features.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Avoid duplication**: If we didn’t use a `Pipeline` to handle these preprocessing
    steps, we’d end up transforming the `X_test` dataset multiple times (every time
    we wanted to apply a preprocessing step). At this small scale, the repetition
    might not seem too bad. But in complex ML workflows you can easily grow to 5,
    10, or even 20 preprocessing steps. Using a `Pipeline` makes this easy because
    we can add in as many steps as we like and still only have to transform `X_train`
    and `X_test` once:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '2\. ColumnTransformer: Apply separate transformers to different feature subsets'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous example, we applied the same preprocessing steps to all features.
    But what if we had heterogenous datatypes and want to apply different preprocessors
    to different features? For example, if we only wanted to rescale the numerical
    features, or if we wanted to one-hot encode the categorical features?
  prefs: []
  type: TYPE_NORMAL
- en: This is where `ColumnTransformer` steps in. A `ColumnTransformer` allows you
    to apply different transformers to different columns of an array or pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: In the code below, we start by defining the different groups of columns and,
    for each group, we use a `Pipeline` to build a preprocessor that will act on that
    specific group. Finally, we chain together all of the transformers in a single
    `ColumnTransformer`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/73a71294eda47d1fe395f59bfcae38dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply the `ColumnTransformer` to our data, we use the same code as we did
    to apply our first `Pipeline`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '3\. FeatureUnion: Apply multiple transformers in parallel'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Pipeline` and `ColumnTransformer` are awesome tools, but they have a significant
    limitation. Did you spot it?'
  prefs: []
  type: TYPE_NORMAL
- en: They can only apply transformers ***sequentially***.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, when you transform a feature `Column1` using a `Pipeline`/`ColumnTransformer`,
    scikit-learn will first apply `transformer_1` to `Column1`, then apply `transformer_2`
    to the transformed version of `Column1`, and so on. This is fine for when we want
    to preprocess our data in a sequential manner (e.g. “first impute missing values,
    then one-hot encode”), but it’s not ideal in cases where we want to apply different
    preprocessing steps in parallel (e.g. “create two new features from the same underlying
    column at the same time”). In these cases, using a standard `Pipeline` or `ColumnTransformer`
    won’t suffice because the original “raw” values of `Column1` will be lost as soon
    as the first transformer in the sequence is applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to apply multiple transformations to the same underlying features
    in parallel, we need to use another tool: `FeatureUnion`.'
  prefs: []
  type: TYPE_NORMAL
- en: We can think of `FeatureUnion` as a tool that creates a “copy” of your underlying
    data, applies transformers to those copies in parallel, and then stitches the
    results together. Each transformer is passed the raw, underlying data, so we don’t
    experience the problem of sequential transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use `FeatureUnion`, we just need to add a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e4dace85b9345490fdb7eea945042d0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In this diagram, we can see that the `FeatureUnion` steps are applied in parallel,
    rather than sequentially. Just like before, we fit the `preprocessor` to our training
    data and then use it to transform any dataset we want to use for modelling/prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '4\. FunctionTransformer: Seamlessly integrate feature engineering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the transformers and tools discussed above use pre-built classes in scikit-learn
    to apply standard transformations to your data (e.g., scaling, one-hot encoding,
    imputing, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: If you want to apply a custom function — for example during feature engineering
    — then you’ll want to use `FunctionTransformer`. Personally, I love this class
    - it makes it super easy to integrate custom functions into your `Pipeline` without
    having to write new transformer classes from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a `FunctionTransformer` is really simple. You start by defining your
    functions in the standard Pythonic style, and then create a pipeline. Here, I
    define two simple functions: one that adds together two columns, and another that
    subtracts two columns.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To simplify things even further, you could include multiple transformations
    within the same function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, add the `feature_engineering` pipeline to the `preprocessing` pipeline
    we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/509a78b53b47ac12a99a82738b0a2207.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'And use this new pipeline to apply the same preprocessing/feature engineering
    steps to all your datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Bonus: Save your pipelines for truly reproducible workflows'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In enterprise applications of machine learning, it’s very rare to only use a
    model or preprocessing workflow once. More often, you’ll be required to regularly
    rerun your model each week/month and generate new predictions for new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In these situations, rather than writing a new preprocessing pipeline from
    scratch each time, you can use the same pipeline each time. To do this, once you’ve
    developed your pipeline use the `joblib` library, save the pipeline so that you
    can rerun the exact same transformations with future datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To recap:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Pipeline` provides a quick way to sequentially apply different preprocessing
    transformers to your data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a `ColumnTransformer` is a fantastic way to sequentially apply separate
    preprocessing steps to different feature subsets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FeatureUnion` enables you to apply different preprocessing transformations
    in parallel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FunctionTransformer` provides a super-simple way to write custom feature engineering
    functions and integrate them within your pipelines'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you use these tools, my promise to you is that they’ll help you write code
    that is more elegant, reproducible, and pythonic. Your Machine Learning Engineers
    will love you!
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this article, it would mean a lot if you followed me. If you’d
    like to get unlimited access to all of my stories (and the rest of Medium.com),
    you can sign up via my [referral link](https://medium.com/@mattchapmanmsc/membership)
    for $5 per month. It adds no extra cost to you vs. signing up via the general
    signup page, and helps to support my writing as I get a small commission.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
