- en: Cook your First U-Net in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/cook-your-first-u-net-in-pytorch-b3297a844cf3](https://towardsdatascience.com/cook-your-first-u-net-in-pytorch-b3297a844cf3)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A magic recipe to empower your image segmentation projects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mostafawael.medium.com/?source=post_page-----b3297a844cf3--------------------------------)[![Mostafa
    Wael](../Images/bf0a052c6446eb3d133e67453ae38143.png)](https://mostafawael.medium.com/?source=post_page-----b3297a844cf3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b3297a844cf3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b3297a844cf3--------------------------------)
    [Mostafa Wael](https://mostafawael.medium.com/?source=post_page-----b3297a844cf3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b3297a844cf3--------------------------------)
    ·6 min read·May 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1de747dfac6df8728f9837b2c6bed9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Stefan C. Asafti](https://unsplash.com/@stefanasafti?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/x5jilo3ck3o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: 'U-Net is a deep learning architecture used for semantic segmentation tasks
    in image analysis. It was introduced by Olaf Ronneberger, Philipp Fischer, and
    Thomas Brox in a paper titled “[U-Net: Convolutional Networks for Biomedical Image
    Segmentation](https://papers.labml.ai/paper/2e48c3ffdc8311eba3db37f65e372566)”.'
  prefs: []
  type: TYPE_NORMAL
- en: It is particularly effective for biomedical image segmentation tasks because
    it can handle images of arbitrary size and produces smooth, high-quality segmentation
    masks with sharp object boundaries. It has since been widely adopted in many other
    image segmentation tasks, such as in satellite and aerial imagery analysis, as
    well as in natural image segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will learn more about U-Net and how it works, and we will
    cook our own implementation recipe using PyTorch. So, let’s go!
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The U-Net architecture consists of two parts: an encoder and a decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/302f2b53c15c476ff36507d93dc9f5e3.png)'
  prefs: []
  type: TYPE_IMG
- en: '[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://paperswithcode.com/paper/u-net-convolutional-networks-for-biomedical)'
  prefs: []
  type: TYPE_NORMAL
- en: Encoder(Contraction Path)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The encoder is a series of **convolutional** and **pooling** layers that progressively
    **downsample** the input image to extract features at multiple scales.
  prefs: []
  type: TYPE_NORMAL
- en: In the Encoder, the size of the image is gradually reduced while the depth gradually
    increases. This basically means the network **learns the “WHAT”** information
    in the image, however, it has **lost the “WHERE”** information.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder(Expansion Path)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The decoder consists of a series of **convolutional** and **upsampling** layers
    that upsample the feature maps to the original input image size while also incorporating
    the high-resolution features from the encoder. This allows the decoder to produce
    segmentation masks that have the same size as the original input image.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about the upsampling and the transposed convolution from
    this great [article](/transposed-convolution-demystified-84ca81b4baba).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the Decoder, the size of the image gradually increases while the depth gradually
    decreases. This basically means the network **learns the “WHERE”** information
    in the image, by gradually applying up-sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Final Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the final layer, a 1x1 convolution is used to map each 64-component feature
    vector to the desired number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: Our cooking recipe!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will do a very straightforward implementation, it will be good to put the
    above image in front of you while coding.
  prefs: []
  type: TYPE_NORMAL
- en: Imports
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, the necessary modules are imported from the `torch` and `torchvision`
    packages, including the `nn` module for building neural networks and the pre-trained
    models provided in `torchvision.models`. The `relu` function is also imported
    from `torch.nn.functional`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: UNet Class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Then, a custom class `UNet` is defined as a subclass of `nn.Module`. The `__init__`
    method initializes the architecture of the U-Net by defining the layers for both
    the encoder and decoder parts of the network. The argument `n_class` specifies
    the number of classes for the segmentation task.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the U-Net paper they used 0 padding and applied post-processing teachiques
    to restore the original size of the image, however here, we uses 1 padding so
    that final feature map is not cropped and to eliminate any need to apply post-processing
    to our output image.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Forward Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `forward` method specifies how the input is processed through the network.
    The input image is first passed through the encoder layers to extract the features.
    Then, the decoder layers are used to upsample the features to the original image
    size while concatenating the corresponding encoder feature maps. Finally, the
    output layer uses a 1x1 convolutional layer to map the features to the desired
    number of output classes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Don’t forget to hit the* ***Clap*** *and* ***Follow*** *buttons to help me
    write more articles like this.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: That it is!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on successfully implementing your first U-Net model in PyTorch!
    By following this recipe, you have gained the knowledge to implement U-Net and
    can now apply it to any image segmentation problem you may encounter in the future.
    However, verifying the sizes and channel numbers is important to ensure compatibility.
    The U-Net architecture is a powerful tool in your arsenal that can be applied
    to various tasks, including medical imaging and autonomous driving. So, go ahead
    and grab any image segmentation dataset from the internet and start testing your
    code!
  prefs: []
  type: TYPE_NORMAL
- en: For convenience, I have added a [simple test script in this repository](https://github.com/Mostafa-wael/U-Net-in-PyTorch/blob/main/test.py).
  prefs: []
  type: TYPE_NORMAL
- en: The script generates random images and masks and trains the U-net model to segment
    the images. It has a function called `generate_random_data()` that creates input
    images and their corresponding masks with geometric shapes like triangles, circles,
    squares, and crosses. The U-net model is trained using these random images and
    masks. The trained model is then tested on new random images and the segmentation
    results are plotted using the `plot_img_array()` function. The script uses PyTorch
    to train the U-net model and also uses various functions to add shapes to the
    input images and masks.
  prefs: []
  type: TYPE_NORMAL
- en: 'consider downloading it and running the tests using this snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/535b89fef06b03ab927b6ff48e175ed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Expected Test Output(By me).
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, the U-Net architecture has become incredibly popular in the computer
    vision community due to its effectiveness in solving various image segmentation
    tasks. Its unique design, which includes a contracting path followed by an expanding
    path, allows it to capture both local and global features of an image while preserving
    spatial information.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the flexibility of the U-Net architecture makes it possible to modify
    and improve the network to suit specific needs. Researchers have proposed various
    modifications to the original U-Net architecture, including changing the convolutional
    layers, incorporating attention mechanisms, and adding skip connections, among
    others. These modifications have resulted in improved performance and better segmentation
    results in various applications.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the U-Net architecture has proven to be a reliable and versatile solution
    for image segmentation tasks. As computer vision continues to advance, it’s likely
    that we’ll see further innovations and modifications to the U-Net architecture
    to improve its performance and make it even more effective in solving real-world
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t hesitate to share your thoughts with me!
  prefs: []
  type: TYPE_NORMAL
- en: '*I don’t receive Medium stipends as Egypt isn’t supported. If you enjoy my
    content, please consider* [*buying me a coffee*](https://ko-fi.com/mostafawael#)
    *☕ to encourage more posts. Thank you!*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
