["```py\ncreate temp table last_online as (\n    select 1 as user_id\n    , timestamp('2000-10-01 00:00:01') as last_online\n)\n;\ncreate temp table connection_data  (\n  user_id int64\n  ,timestamp timestamp\n)\nPARTITION BY DATE(_PARTITIONTIME)\n;\ninsert connection_data (user_id, timestamp)\n    select 2 as user_id\n    , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp\nunion all\n    select 1 as user_id\n        , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp\nunion all\n    select 1 as user_id\n        , timestamp_sub(current_timestamp(),interval 20 hour) as timestamp\nunion all\n    select 1 as user_id\n    , timestamp_sub(current_timestamp(),interval 1 hour) as timestamp\n;\n\nmerge last_online t\nusing (\n  select\n      user_id\n    , last_online\n  from\n    (\n        select\n            user_id\n        ,   max(timestamp) as last_online\n\n        from \n            connection_data\n        where\n            date(_partitiontime) >= date_sub(current_date(), interval 1 day)\n        group by\n            user_id\n\n    ) y\n\n) s\non t.user_id = s.user_id\nwhen matched then\n  update set last_online = s.last_online, user_id = s.user_id\nwhen not matched then\n  insert (last_online, user_id) values (last_online, user_id)\n;\nselect * from last_online\n;\n```", "```py\nfrom google.cloud import bigquery\n...\nclient = bigquery.Client(credentials=credentials, project=credentials.project_id)\n...\n\ndef _load_table_from_csv(table_schema, table_name, dataset_id):\n    '''Loads data into BigQuery table from a CSV file.\n    ! source file must be comma delimited CSV:\n    transaction_id,user_id,total_cost,dt\n    1,1,10.99,2023-04-15\n    blob = \"\"\"transaction_id,user_id,total_cost,dt\\n1,1,10.99,2023-04-15\"\"\"\n    '''\n\n    blob = \"\"\"transaction_id,user_id,total_cost,dt\n    1,1,10.99,2023-04-15\n    2,2, 4.99,2023-04-12\n    4,1, 4.99,2023-04-12\n    5,1, 5.99,2023-04-14\n    6,1,15.99,2023-04-14\n    7,1,55.99,2023-04-14\"\"\"\n\n    data_file = io.BytesIO(blob.encode())\n\n    print(blob)\n    print(data_file)\n\n    table_id = client.dataset(dataset_id).table(table_name)\n    job_config = bigquery.LoadJobConfig()\n    schema = create_schema_from_yaml(table_schema)\n    job_config.schema = schema\n\n    job_config.source_format = bigquery.SourceFormat.CSV,\n    job_config.write_disposition = 'WRITE_APPEND',\n    job_config.field_delimiter =\",\"\n    job_config.null_marker =\"null\",\n    job_config.skip_leading_rows = 1\n\n    load_job = client.load_table_from_file(\n        data_file,\n        table_id,\n        job_config=job_config,\n        )\n\n    load_job.result()\n    print(\"Job finished.\")\n```", "```py\n \"\"\"DAG definition for recommendation_bespoke model training.\"\"\"\n\nimport airflow\nfrom airflow import DAG\nfrom airflow.contrib.operators.bigquery_operator import BigQueryOperator\nfrom airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.operators.app_engine_admin_plugin import AppEngineVersionOperator\nfrom airflow.operators.ml_engine_plugin import MLEngineTrainingOperator\n\nimport datetime\n\ndef _get_project_id():\n  \"\"\"Get project ID from default GCP connection.\"\"\"\n\n  extras = BaseHook.get_connection('google_cloud_default').extra_dejson\n  key = 'extra__google_cloud_platform__project'\n  if key in extras:\n    project_id = extras[key]\n  else:\n    raise ('Must configure project_id in google_cloud_default '\n           'connection from Airflow Console')\n  return project_id\n\nPROJECT_ID = _get_project_id()\n\n# Data set constants, used in BigQuery tasks.  You can change these\n# to conform to your data.\nDATASET = 'staging' #'analytics'\nTABLE_NAME = 'recommendation_bespoke'\n\n# GCS bucket names and region, can also be changed.\nBUCKET = 'gs://rec_wals_eu'\nREGION = 'us-central1' #'europe-west2' #'us-east1'\nJOB_DIR = BUCKET + '/jobs'\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': airflow.utils.dates.days_ago(2),\n    'email': ['mike.shakhomirov@gmail.com'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 5,\n    'retry_delay': datetime.timedelta(minutes=5)\n}\n\n# Default schedule interval using cronjob syntax - can be customized here\n# or in the Airflow console.\nschedule_interval = '00 21 * * *'\n\ndag = DAG('recommendations_training_v6', default_args=default_args,\n          schedule_interval=schedule_interval)\n\ndag.doc_md = __doc__\n\n#\n#\n# Task Definition\n#\n#\n\n# BigQuery training data export to GCS\n\ntraining_file = BUCKET + '/data/recommendations_small.csv' # just a few records for staging\n\nt1 = BigQueryToCloudStorageOperator(\n    task_id='bq_export_op',\n    source_project_dataset_table='%s.recommendation_bespoke' % DATASET,\n    destination_cloud_storage_uris=[training_file],\n    export_format='CSV',\n    dag=dag\n)\n\n# ML Engine training job\ntraining_file = BUCKET + '/data/recommendations_small.csv'\njob_id = 'recserve_{0}'.format(datetime.datetime.now().strftime('%Y%m%d%H%M'))\njob_dir = BUCKET + '/jobs/' + job_id\noutput_dir = BUCKET\ndelimiter=','\ndata_type='user_groups'\nmaster_image_uri='gcr.io/my-project/recommendation_bespoke_container:tf_rec_latest'\n\ntraining_args = ['--job-dir', job_dir,\n                 '--train-file', training_file,\n                 '--output-dir', output_dir,\n                 '--data-type', data_type]\n\nmaster_config = {\"imageUri\": master_image_uri,}\n\nt3 = MLEngineTrainingOperator(\n    task_id='ml_engine_training_op',\n    project_id=PROJECT_ID,\n    job_id=job_id,\n    training_args=training_args,\n    region=REGION,\n    scale_tier='CUSTOM',\n    master_type='complex_model_m_gpu',\n    master_config=master_config,\n    dag=dag\n)\n\nt3.set_upstream(t1) \n```", "```py\n./stack\n├── deploy.sh                 # Shell script to deploy the Lambda\n├── stack.yaml                # Cloudformation template\n├── pipeline_manager\n|   ├── env.json              # enviroment variables                \n│   └── app.py                # Application\n├── response.json             # Lambda response when invoked locally\n└── stack.zip                 # Lambda package\n```", "```py\n# ./pipeline_manager/app.py\ndef lambda_handler(event, context):\n   message = 'Hello {} {}!'.format(event['first_name'], event['last_name']) \n   return {\n       'message' : message\n   }\n```", "```py\npip install python-lambda-local\ncd stack\npython-lambda-local -e pipeline_manager/env.json -f lambda_handler pipeline_manager/app.py event.json\n```", "```py\n# stack.yaml\nAWSTemplateFormatVersion: '2010-09-09'\nDescription: AWS S3 data lake stack.\nParameters:\n\n  ServiceName:\n    Description: Data lake microservice to process data files and load them into __ BigQuery.\n    Type: String\n    Default: datalake-manager\n  StackPackageS3Key:\n    Type: String\n    Default: pipeline_manager/stack.zip\n  Testing:\n    Type: String\n    Default: 'false'\n    AllowedValues: ['true','false']\n  Environment:\n    Type: String\n    Default: 'staging'\n    AllowedValues: ['staging','live','test']\n  AppFolder:\n    Description: app.py file location inside the package, i.e. ./stack/pipeline_manager/app.py.\n    Type: String\n    Default: pipeline_manager\n  LambdaCodeLocation:\n    Description: Lambda package file location.\n    Type: String\n    Default: datalake-lambdas.aws\n\nResources:\n\n  PipelineManagerLambda:\n    Type: AWS::Lambda::Function\n    DeletionPolicy: Delete\n    DependsOn: LambdaPolicy\n    Properties:\n      FunctionName: !Join ['-', [!Ref ServiceName, !Ref Environment] ] # pipeline-manager-staging if staging.\n      Handler: !Sub '${AppFolder}/app.lambda_handler'\n      Description: Microservice that orchestrates data loading into BigQuery from AWS to BigQuery project your-project-name.schema.\n      Environment:\n        Variables:\n          DEBUG: true\n          LAMBDA_PATH: !Sub '${AppFolder}/' # i.e. 'pipeline_manager/'\n          TESTING: !Ref Testing\n          ENV: !Ref Environment\n      Role: !GetAtt LambdaRole.Arn\n      Code:\n        S3Bucket: !Sub '${LambdaCodeLocation}' #datalake-lambdas.aws\n        S3Key:\n          Ref: StackPackageS3Key\n      Runtime: python3.8\n      Timeout: 360\n      MemorySize: 128\n      Tags:\n        -\n          Key: Service\n          Value: Datalake\n\n  LambdaRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          -\n            Effect: Allow\n            Principal:\n              Service:\n                - \"lambda.amazonaws.com\"\n            Action:\n              - \"sts:AssumeRole\"\n\n  LambdaPolicy:\n    Type: AWS::IAM::Policy\n    DependsOn: LambdaRole\n    Properties:\n      Roles:\n        - !Ref LambdaRole\n      PolicyName: 'pipeline-manager-lambda-policy'\n      PolicyDocument:\n        {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Effect\": \"Allow\",\n                    \"Action\": [\n                        \"logs:CreateLogGroup\",\n                        \"logs:CreateLogStream\",\n                        \"logs:PutLogEvents\"\n                    ],\n                    \"Resource\": \"*\"\n                }\n            ]\n        }\n```", "```py\naws \\\ncloudformation deploy \\\n--template-file stack.yaml \\\n--stack-name $STACK_NAME \\\n--capabilities CAPABILITY_IAM \\\n--parameter-overrides \\\n\"StackPackageS3Key\"=\"${APP_FOLDER}/${base}${TIME}.zip\" \\\n\"AppFolder\"=$APP_FOLDER \\\n\"LambdaCodeLocation\"=$LAMBDA_BUCKET \\\n\"Environment\"=\"staging\" \\\n\"Testing\"=\"false\"\n```", "```py\naws lambda invoke \\\n    --function-name pipeline-manager \\\n    --payload '{ \"first_name\": \"something\" }' \\\n    response.json\n```", "```py\nmkdir data\ncd data\necho transaction_id,user_id,dt \\\\n101,777,2021-08-01\\\\n102,777,2021-08-01\\\\n103,777,2021-08-01\\\\n > simple_transaction.csv\naws  s3 cp ./data/simple_transaction.csv s3://your.bucket.aws/simple_transaction_from_data.csv\n```", "```py\naws  s3 cp ./data s3://your.bucket.aws --recursive\n```", "```py\naws  s3 rm s3://your.bucket.aws/ --recursive --exclude \"\"\n```", "```py\naws  s3 rb s3://your.bucket.aws/\n```", "```py\n# ./deploy.sh\n# Run ./deploy.sh\nLAMBDA_BUCKET=$1 # your-lambda-packages.aws\nSTACK_NAME=SimpleETLService\nAPP_FOLDER=pipeline_manager\n# Get date and time to create unique s3-key for deployment package:\ndate\nTIME=`date +\"%Y%m%d%H%M%S\"`\n# Get the name of the base application folder, i.e. pipeline_manager.\nbase=${PWD##*/}\n# Use this name to name zip:\nzp=$base\".zip\"\necho $zp\n# Remove old package if exists:\nrm -f $zp\n# Package Lambda\nzip -r $zp \"./${APP_FOLDER}\" -x deploy.sh\n\n# Check if Lambda bucket exists:\nLAMBDA_BUCKET_EXISTS=$(aws  s3 ls ${LAMBDA_BUCKET} --output text)\n#  If NOT:\nif [[ $? -eq 254 ]]; then\n    # create a bucket to keep Lambdas packaged files:\n    echo  \"Creating Lambda code bucket ${LAMBDA_BUCKET} \"\n    CREATE_BUCKET=$(aws  s3 mb s3://${LAMBDA_BUCKET} --output text)\n    echo ${CREATE_BUCKET}\nfi\n\n# Upload the package to S3:\naws s3 cp ./${base}.zip s3://${LAMBDA_BUCKET}/${APP_FOLDER}/${base}${TIME}.zip\n\n# Deploy / Update:\naws --profile $PROFILE \\\ncloudformation deploy \\\n--template-file stack.yaml \\\n--stack-name $STACK_NAME \\\n--capabilities CAPABILITY_IAM \\\n--parameter-overrides \\\n\"StackPackageS3Key\"=\"${APP_FOLDER}/${base}${TIME}.zip\" \\\n\"AppFolder\"=$APP_FOLDER \\\n\"LambdaCodeLocation\"=$LAMBDA_BUCKET \\\n\"Environment\"=\"staging\" \\\n\"Testing\"=\"false\"\n```", "```py\nwith checks as (\n    select\n      count( transaction_id )                                                           as t_cnt\n    , count(distinct transaction_id)                                                    as t_cntd\n    , count(distinct (case when payment_date is null then transaction_id end))          as pmnt_date_null\n    from\n        production.user_transaction\n)\n, row_conditions as (\n\n    select if(t_cnt = 0,'Data for yesterday missing; ', NULL)  as alert from checks\nunion all\n    select if(t_cnt != t_cntd,'Duplicate transactions found; ', NULL) from checks\nunion all\n    select if(pmnt_date_null != 0, cast(pmnt_date_null as string )||' NULL payment_date found', NULL) from checks\n)\n\n, alerts as (\nselect\n        array_to_string(\n            array_agg(alert IGNORE NULLS) \n        ,'.; ')                                         as stringify_alert_list\n\n    ,   array_length(array_agg(alert IGNORE NULLS))     as issues_found\nfrom\n    row_conditions\n)\n\nselect\n    alerts.issues_found,\n    if(alerts.issues_found is null, 'all good'\n        , ERROR(FORMAT('ATTENTION: production.user_transaction has potential data quality issues for yesterday: %t. Check dataChecks.check_user_transaction_failed_v for more info.'\n        , stringify_alert_list)))\nfrom\n    alerts\n;\n```", "```py\n# .github/workflows/deploy_staging.yaml\nname: STAGING AND TESTS\n\non:\n  #when there is a push to the master\n  push:\n    branches: [ master ]\n  #when there is a pull to the master\n  pull_request:\n    branches: [ master ]\n\njobs:\n compile:\n   runs-on: ubuntu-latest\n   steps:\n     - name: Checkout code into workspace directory\n       uses: actions/checkout@v2\n     - name: Install AWS CLI v2\n       run:  |\n             curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o /tmp/awscliv2.zip\n             unzip -q /tmp/awscliv2.zip -d /tmp\n             rm /tmp/awscliv2.zip\n             sudo /tmp/aws/install --update\n             rm -rf /tmp/aws/\n     - name: test AWS connectivity\n       run: aws s3 ls\n       env:\n         AWS_ACCESS_KEY_ID: ${{ secrets.MDS_AWS_ACCESS_KEY_ID }}\n         AWS_SECRET_ACCESS_KEY: ${{ secrets.MDS_AWS_SECRET_ACCESS_KEY }}\n         AWS_DEFAULT_REGION: \"eu-west-1\"\n\n     - name: Deploy staging\n       run: |\n            cd stack\n            date\n            TIME=`date +\"%Y%m%d%H%M%S\"`\n            base=${PWD##*/}\n            zp=$base\".zip\"\n            rm -f $zp\n            pip install --target ./package pyyaml==6.0\n            cd package\n            zip -r ../${base}.zip .\n            cd $OLDPWD\n            zip -r $zp ./pipeline_manager\n            # Upload Lambda code (replace with your S3 bucket):\n            aws s3 cp ./${base}.zip s3://datalake-lambdas.aws/pipeline_manager/${base}${TIME}.zip\n            STACK_NAME=SimpleCICDWithLambdaAndRole\n            aws \\\n            cloudformation deploy \\\n            --template-file stack_cicd_service_and_role.yaml \\\n            --stack-name $STACK_NAME \\\n            --capabilities CAPABILITY_IAM \\\n            --parameter-overrides \\\n            \"StackPackageS3Key\"=\"pipeline_manager/${base}${TIME}.zip\" \\\n            \"Environment\"=\"staging\" \\\n            \"Testing\"=\"false\"\n       env:\n         AWS_ACCESS_KEY_ID: ${{ secrets.MDS_AWS_ACCESS_KEY_ID }}\n         AWS_SECRET_ACCESS_KEY: ${{ secrets.MDS_AWS_SECRET_ACCESS_KEY }}\n         AWS_DEFAULT_REGION: \"eu-west-1\"\n```", "```py\nCREATE OR REPLACE MODEL sample_churn_model.churn_model\n\nOPTIONS(\n  MODEL_TYPE=\"LOGISTIC_REG\",\n  INPUT_LABEL_COLS=[\"churned\"]\n) AS\n\nSELECT\n  * except (\n     user_pseudo_id\n    ,first_seen_ts    \n    ,last_seen_ts     \n  )\nFROM\n  sample_churn_model.churn\n```", "```py\nSELECT\n  user_pseudo_id,\n  churned,\n  predicted_churned,\n  predicted_churned_probs[OFFSET(0)].prob as probability_churned\nFROM\n  ML.PREDICT(MODEL sample_churn_model.churn_model,\n  (SELECT * FROM sample_churn_model.churn)) #can be replaced with a proper test dataset\norder by 3 desc\n```"]