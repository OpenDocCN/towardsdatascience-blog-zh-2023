["```py\nCONDA_SUBDIR=osx-arm64 conda create -n mlx python=3.10 numpy pytorch scipy requests -c conda-forge\nconda activate mlx\n```", "```py\npython -c \"import platform; print(platform.processor())\"\n```", "```py\npip install mlx\n```", "```py\nimport mlx.nn as nn\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(GCNLayer, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias)\n\n    def __call__(self, x, adj):\n        x = self.linear(x)\n        return adj @ x\n\nclass GCN(nn.Module):\n    def __init__(self, x_dim, h_dim, out_dim, nb_layers=2, dropout=0.5, bias=True):\n        super(GCN, self).__init__()\n\n        layer_sizes = [x_dim] + [h_dim] * nb_layers + [out_dim]\n        self.gcn_layers = [\n            GCNLayer(in_dim, out_dim, bias)\n            for in_dim, out_dim in zip(layer_sizes[:-1], layer_sizes[1:])\n        ]\n        self.dropout = nn.Dropout(p=dropout)\n\n    def __call__(self, x, adj):\n        for layer in self.gcn_layers[:-1]:\n            x = nn.relu(layer(x, adj))\n            x = self.dropout(x)\n\n        x = self.gcn_layers[-1](x, adj)\n        return x\n```", "```py\ngcn = GCN(\n    x_dim=x.shape[-1],\n    h_dim=args.hidden_dim,\n    out_dim=args.nb_classes,\n    nb_layers=args.nb_layers,\n    dropout=args.dropout,\n    bias=args.bias,\n)\nmx.eval(gcn.parameters())\n\noptimizer = optim.Adam(learning_rate=args.lr)\nloss_and_grad_fn = nn.value_and_grad(gcn, forward_fn)\n\n# Training loop\nfor epoch in range(args.epochs):\n\n    # Loss\n    (loss, y_hat), grads = loss_and_grad_fn(\n        gcn, x, adj, y, train_mask, args.weight_decay\n    )\n    optimizer.update(gcn, grads)\n    mx.eval(gcn.parameters(), optimizer.state)\n\n    # Validation\n    val_loss = loss_fn(y_hat[val_mask], y[val_mask])\n    val_acc = eval_fn(y_hat[val_mask], y[val_mask])\n```", "```py\ndef forward_fn(gcn, x, adj, y, train_mask, weight_decay):\n    y_hat = gcn(x, adj)\n    loss = loss_fn(y_hat[train_mask], y[train_mask], weight_decay, gcn.parameters())\n    return loss, y_hat\n```", "```py\ndef loss_fn(y_hat, y, weight_decay=0.0, parameters=None):\n    l = mx.mean(nn.losses.cross_entropy(y_hat, y))\n\n    if weight_decay != 0.0:\n        assert parameters != None, \"Model parameters missing for L2 reg.\"\n\n        l2_reg = sum(mx.sum(p[1] ** 2) for p in tree_flatten(parameters)).sqrt()\n        return l + weight_decay * l2_reg\n\n    return l\n\ndef eval_fn(x, y):\n    return mx.mean(mx.argmax(x, axis=1) == y)\n```"]