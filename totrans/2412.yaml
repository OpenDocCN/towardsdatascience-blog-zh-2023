- en: YOLO Object Detection on the Raspberry Pi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/yolo-object-detection-on-the-raspberry-pi-6de3629256fa](https://towardsdatascience.com/yolo-object-detection-on-the-raspberry-pi-6de3629256fa)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Running the object detection model on the low-power devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dmitryelj.medium.com/?source=post_page-----6de3629256fa--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page-----6de3629256fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6de3629256fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6de3629256fa--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page-----6de3629256fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6de3629256fa--------------------------------)
    ·9 min read·Jul 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3cd8e63bae4b714e54aa6398a1a9e5c3.png)'
  prefs: []
  type: TYPE_IMG
- en: YOLO object detection results, Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In the [first part](/retro-data-science-testing-the-first-versions-of-yolo-799b9c1835d7)
    of this article, I tested “retro” versions of YOLO (You Only Look Once), a popular
    object detection library. The possibility to run a deep learning model using only
    OpenCV, without “heavy” frameworks like PyTorch or Keras, is promising for low-power
    devices, and I decided to go deeper into this topic and see how the latest YOLO
    v8 model works on a Raspberry Pi.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get into it.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is usually not a problem to run any model in the cloud, where the resources
    are virtually unlimited. But for the hardware “in the field,” there are much more
    constraints. Limited RAM, CPU power, or even different CPU architecture, older
    or incompatible software versions, a lack of a high-speed internet connection,
    and so on. Another big issue with cloud infrastructure is its cost. Let’s say
    we are making a smart doorbell, and we want to add person detection to it. We
    can run a model in the cloud, but every API call costs money, and who will pay
    for that? Not every customer would be happy to have a monthly subscription for
    the doorbell or any similar “smart” device, so it can be essential to run a model
    locally, even if the results may not be so good.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this test, I will run the YOLO v8 model on a Raspberry Pi:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a031796bb28e948930837f5d51ef043.png)'
  prefs: []
  type: TYPE_IMG
- en: Raspberry Pi 4, Image source [https://en.wikipedia.org/wiki/Raspberry_Pi](https://en.wikipedia.org/wiki/Raspberry_Pi)
  prefs: []
  type: TYPE_NORMAL
- en: 'The Raspberry Pi is a cheap credit-card-size single-board computer that runs
    Raspbian or Ubuntu Linux. I will test two different versions:'
  prefs: []
  type: TYPE_NORMAL
- en: Raspberry Pi 3 Model B, made in 2015\. It has a 1.2 GHz Cortex-A53 ARM CPU and
    1 GB of RAM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raspberry Pi 4, made in 2019\. It has a 1.8 GHz Cortex-A72 ARM CPU and 1, 4,
    or 8 GB of RAM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Raspberry Pi computers are widely used nowadays, not only for hobby and DIY
    projects but also for embedded industrial applications (a [Raspberry Pi Compute
    Module](https://www.raspberrypi.com/products/compute-module-4/?variant=raspberry-pi-cm4001000)
    was designed especially for that). So, it is interesting to see how these boards
    can handle such computationally demanding operations as object detection. For
    all further tests, I will use this image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63eeec416befa124ca3389beeb89f31f.png)'
  prefs: []
  type: TYPE_IMG
- en: Test image, made by author
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: '**A “Standard” YOLO v8 Version**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a warm-up, let’s try the standard version, as it is [described](https://github.com/ultralytics/ultralytics)
    on the official GitHub page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the “production” system, images can be taken from a camera; for our test,
    I am using a “test.jpg” file as was described before. I also executed the “predict”
    method twice to make the time estimation more accurate (the first run usually
    takes more time for the model to “warm up” and allocate all the needed memory).
    A Raspberry Pi is working in “headless” mode without a monitor, so I am using
    the console as an output; this is a more-or-less standard way most embedded systems
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Raspberry Pi 3** with a 32-bit OS, this version does not work: pip
    cannot install an “ultralytics” module because of this error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It turned out that PyTorch is available only for ARM 64-bit OS.
  prefs: []
  type: TYPE_NORMAL
- en: On the **Raspberry Pi 4** with a 64-bit OS, the code indeed works, and the calculation
    took about 0.9 s.
  prefs: []
  type: TYPE_NORMAL
- en: 'The console output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9018160caad640e9889bc522c4d16a6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I also did the same experiment on the desktop PC to visualize the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15b6adff409166994691e49b58e59a14.png)'
  prefs: []
  type: TYPE_IMG
- en: YOLO v8 Nano detection results, Image by author
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, even for a model of “nano” size, the results are pretty good.
  prefs: []
  type: TYPE_NORMAL
- en: Python ONNX Version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ONNX ([Open Neural Network Exchange](https://onnx.ai)) is an open format built
    to represent machine learning models. It is also supported by OpenCV, so we can
    easily run our model this way. YOLO developers have already provided a [command-line
    tool](https://docs.ultralytics.com/modes/export/) to make this conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, “yolov8n.pt” is a PyTorch model file, which will be converted. The last
    letter “n” in the filename means “nano”. Different models are available (“n” —
    nano, “s” — small, “m” — medium, “l” — large), obviously, for the Raspberry Pi,
    I will use the smallest and fastest one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversion can be done on the desktop PC, and a model can be copied to a Raspberry
    Pi using the “scp” command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to prepare the source. I used [an example](https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-OpenCV-ONNX-Python/main.py)
    from the Ultralytics repository, which I slightly modified to work on the Raspberry
    Pi:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we don’t use PyTorch and the original Ultralytics library anymore,
    but the required amount of code is bigger. We need to convert the image to a blob,
    which is required for a YOLO model. Before printing the result, we also need to
    convert the output rectangles to the original coordinates. But as an advantage,
    this code works on “pure” OpenCV without any additional dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: On the **Raspberry Pi 3,** the computation time is 28 seconds. Just for fun,
    I also loaded the “medium” model (it’s a 101 MB ONNX file!) to see what would
    happen. Surprisingly, the application did not crash, but the calculation time
    was 224 seconds (almost 4 minutes). It looks obvious that the hardware from 2015
    is not well suited for running SOTA models from 2023, but it was interesting to
    see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: On the **Raspberry Pi 4** the computation time is 1.08 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: C++ ONNX Version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, let’s try the “heaviest guns” in our toolset and write the same code
    in C++. But before doing this, we will need to install OpenCV libraries and headers
    for C++. The easiest way is to run a command like “*sudo apt install libopencv-dev*”.
    But, at least for Raspbian, it does not work. The latest version, available via
    “apt”, is 4.2, and the minimum OpenCV requirement for loading the YOLO model is
    4.5\. So, we will need to build OpenCV from source.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will use OpenCV 4.7, the same version that was used in my Python tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The Raspberry Pi is not the fastest Linux computer in the world, and the compilation
    process takes about 2 hours. And for a Raspberry Pi 3 with 1 GB of RAM, the swap
    file size should be increased to at least 512 MB; otherwise, the compilation will
    fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'The C++ code itself is short:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, I used “*inference.h*” and “*inference.cpp*” files from the Ultralitics
    [GitHub repository](https://github.com/ultralytics/ultralytics/tree/main/examples/YOLOv8-CPP-Inference),
    these files should be placed in the same folder. I also executed the “runInference”
    method twice, the same way as in previous tests. We can now compile the source
    using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The results were surprising. A C++ version was *significantly* *slower* than
    the previous one! On the **Raspberry Pi 3**, the execution time was 110 seconds,
    which is more than 3 times longer than a Python version. On the **Raspberry Pi
    4,** the computation time was 1.79 seconds, which is about 1.5 times longer. In
    general, it is hard to say why. An OpenCV library for Python was installed using
    *pip*, but OpenCV for C++ was built from the source, and maybe some ARM CPU optimizations
    were not enabled. If some readers know the reason, please write in the comments
    below. Anyway, it was interesting to see that such an effect can happen.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I can make an “educated guess” that most data scientists and data engineers
    are using their models in the cloud or at least on high-end equipment and have
    never tried running code “in the field” on embedded hardware. The goal of this
    text was to give readers some insights into how it works. In this article, we
    tried to run a YOLO v8 model on different versions of the Raspberry Pi, and the
    results were pretty interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Running deep learning models on low-power devices can be a challenge. Even a
    Raspberry Pi 4, which is the best Raspbian-based model at the moment of writing
    this article, was able to provide only ~1 FPS with a YOLO v8 Tiny model. Of course,
    there is room for improvement. Some optimizations may be possible, like converting
    the model into FP16 (a floating point format with less accuracy) or even INT8
    formats. Finally, a more simple model trained on a limited dataset can be used.
    Last but not least, if more computing power is still required, code can run on
    special single-board computers like the NVIDIA Jetson Nano, which has CUDA support
    and can be much faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the beginning of this article, I wrote that “the possibility to run a deep
    learning model using only OpenCV, without heavy frameworks like PyTorch or Keras,
    is promising for low-power devices”. Practically, It turned out that PyTorch is
    an effective and highly optimized framework. The original YOLO version, based
    on PyTorch, was the fastest one, and an OpenCV ONNX code was 10–20% slower. But
    at the moment of writing this article, PyTorch is not available on a 32-bit ARM
    CPU, so on some platforms, there just may be no other choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results with a C++ version were even more interesting. As we can see, it can
    be a challenge to turn on proper optimizations, especially for embedded architecture.
    And without going deep into these nuances, custom-built OpenCV C++ code can run
    even slower compared to a Python version provided by a board manufacturer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks for reading. If someone would be interested in testing FP16 or INT8 YOLO
    models on the same hardware or on the NVIDIA Jetson Nano board, please write in
    the comments, and I will write the next part of the article about this.
  prefs: []
  type: TYPE_NORMAL
- en: If you enjoyed this story, feel free [to subscribe](https://medium.com/@dmitryelj/membership)
    to Medium, and you will get notifications when my new articles will be published,
    as well as full access to thousands of stories from other authors.
  prefs: []
  type: TYPE_NORMAL
