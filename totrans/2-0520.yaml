- en: 'Cognitive Biases in Data Science: The Category-Size Bias'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/cognitive-biases-in-data-science-the-category-size-bias-8dbd851608c3](https://towardsdatascience.com/cognitive-biases-in-data-science-the-category-size-bias-8dbd851608c3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[DATA BIAS HACKERS](https://towardsdatascience.com/tagged/data-cognitive-bias)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A data scientist’s guide to outsmarting biases
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@MahamsMultiverse?source=post_page-----8dbd851608c3--------------------------------)[![Maham
    Haroon](../Images/5a9ac82369ecbf7719b765ec160a70ef.png)](https://medium.com/@MahamsMultiverse?source=post_page-----8dbd851608c3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8dbd851608c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8dbd851608c3--------------------------------)
    [Maham Haroon](https://medium.com/@MahamsMultiverse?source=post_page-----8dbd851608c3--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8dbd851608c3--------------------------------)
    ·8 min read·Nov 29, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eee2dc275bfeecb4dd134047e6cab03f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: Photo by [Andy Li](https://unsplash.com/@andylid0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/man-in-white-dress-shirt-standing-in-front-of-brown-wooden-shelf-RndRFJ1v1kk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you find yourself in a quaint neighborhood with two bakeries. The first
    is a small, family-owned bakery, warmly nestled on the corner street. The second,
    however, is a grand three-story establishment, with a sign that showcases its
    extensive selection and state-of-the-art ovens.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: As you embark on your quest for the perfect loaf of bread, you are drawn to
    the towering bakery. The sheer size and grandeur of the building make an immediate
    impression, making the assumption that the larger bakery must surely produce the
    finest bread.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Here, in this scenario, you’re unknowingly succumbing to a mental tendency known
    as category size bias. The bias leads you to believe that the larger bakery is
    more likely to offer superior bread.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In reality, the size of the bakery doesn’t necessarily correlate with the quality
    of its bread. The smaller, family-owned bakery may have a closely guarded secret
    recipe, perfected over generations, while the larger bakery might focus on quantity
    over artisanal craftsmanship.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: This bias echoes our inclination to associate larger categories with better
    outcomes, even when the specific characteristics within those categories may not
    align with our assumptions. This phenomenon is called category size bias.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Category size bias refers to our inclination to perceive outcomes as more probable
    when they belong to a larger category as opposed to a smaller one, even when the
    likelihood of each outcome is equal.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Despite the bias being grounded in experimentally validated studies, there
    is ongoing variability in the interpretation of the evidence.](https://thedecisionlab.com/biases/category-size-bias)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'In the realm of Data Science, category size bias may manifest via specific
    assumptions. For instance:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Assumption 1: Larger more complex models always provide better predictions
    compared to smaller models.'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Within the context of category size bias, the tendency is to believe that the
    performance of a Neural Network or an ML model improves with its size or complexity.
    Consequently, regardless of whether the data or task aligns with the model’s characteristics,
    there’s often a focus on complex models. This inclination is akin to the [bandwagon
    effect](https://thedecisionlab.com/biases/bandwagon-effect), where newer, more
    intricate, and renowned algorithms are presumed to be cutting-edge, even for tasks
    they are not well-suited for. Consider, for instance, using large language models
    (LLMs) for relatively simple tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: For example, adding extra layers to a Neural Network, even when they don’t really
    make the model much better, as most tasks can be handled well with just two layers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/714b2e09674794c09c497f79cdd11841.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '**Caveat:** It’s important to note that the assumption of larger, more complex
    models yielding better predictions holds true in many cases, especially when dealing
    with complex tasks that require a high level of precision or problems involving
    vast and diverse datasets.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '**Trade-off:** The trade-off in such scenarios encompasses both performance
    and resource consumption. While complex or larger models demand more resources,
    the increased resource consumption doesn’t automatically translate to superior
    model performance. Although this may not always be necessary or impactful, in
    critical problems, making such considerations can indeed make a significant difference.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Assumption 2: Overlooking Class Imbalance by Relying Solely on Higher Accuracy'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This problem is a more widely acknowledged one. When utilizing a metric such
    as accuracy, there is a heightened risk of overlooking an underlying bias. To
    illustrate, consider a dataset with 5 instances of a disease among 1000 patients.
    If a model achieves 99.5% accuracy by consistently classifying almost all instances
    as negative (given the majority of instances are negative), the assumption might
    be that the model performs well. However, in reality, the model would be performing
    inadequately by not classifying any instances as positive.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain further, let’s look at a basic example. We’ll create 100 random
    numbers between 0 and 1\. If a number is over 0.92, we call it positive; otherwise,
    it’s negative. We’ll use logistic regression as our model. The code snippet below
    demonstrates this scenario:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Although the model achieves a respectable accuracy of 0.92, an examination of
    other metrics like recall and f1-score reveals a far inferior performance. In
    the plotted chart on the left, it becomes clear that none of the positive instances
    have been correctly classified as positive.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3327d6d5722a6ce1b4638fdb362ea840.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '**Caveat:** While accuracy is usually a good measure, there are cases where
    prioritizing higher accuracy makes sense. In cases where class imbalances are
    minimal, and the cost of misclassification is relatively low, prioritizing accuracy
    can be a reasonable choice.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '**Trade-off:** The trade-off in this scenario revolves around performance itself,
    with considerable stakes. However, in instances of minor imbalances or data previously
    processed, this concern may not be as consequential.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Assumption 3: Equating Larger Datasets with Improved Performance'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While it’s often the case that larger datasets bring about more features, additional
    information, and an enhanced likelihood of realistic predictions, this holds true
    only up to a point. The amount of data needed depends on the specific problem.
    For example, in classification tasks, scenarios with a smaller number of classes
    or a many informative features might do well with smaller datasets. Similarly,
    tasks involving lower-order function approximation might be content with a more
    modest dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand this with an example. First, we make fake data with half of
    it being useful, 5 categories, and around 20,000 samples.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Subsequently, we employ Support Vector Classification (SVC) for class separation
    on a subset of the data, specifically 9,000 instances:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When measuring the time taken for this task, we observe 6.9590 seconds with
    an accuracy of 0.8430\. Subsequently, we utilize the entire dataset for training
    the SVC, comprising approximately 16,000 instances for training and 4,000 instances
    for testing:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This time, the algorithm takes 20.4149 seconds, about 3x the previous time,
    yielding an accuracy of 0.8452 — remarkably close despite almost double the data.
    A comparison of ROC curves for both models reveals nearly identical results.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3de088294ad99b83d639441dc7a8256.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '**Caveat:** Complex tasks often need more data, but there’s a point where more
    data doesn’t necessarily mean better results.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '**Trade-off:** While additional data rarely harms the model, the trade-off
    lies not in performance but in computational cost. This cost can be substantial
    when irrelevant information is added to the model, emphasizing the importance
    of data pre-processing and cleaning, particularly in complex tasks.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Assumption 4: Equating Longer, More Complex Algorithms with Superior Performance'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 假设 4：将较长、更复杂的算法等同于更优性能
- en: While not always true, there exists a notion that longer, complex algorithms
    hold superiority over their shorter, simpler counterparts. Sometimes, a more intricate
    algorithm is favored even when a simpler one perfectly accomplishes the task.
    While this notion is not always unfounded, the issue lies in the underlying rationale
    for such a belief. If complexity and length are genuinely warranted by the algorithm,
    then there’s no harm in their application.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管并非总是如此，但存在一种观点，认为较长、复杂的算法优于较短、简单的算法。有时，即使一个简单的算法可以完美地完成任务，仍会偏向于使用更复杂的算法。尽管这种观点并非总是毫无根据，但问题在于这种信念的根本理由。如果算法确实需要复杂性和长度，那么应用这些特性也无可厚非。
- en: 'To illustrate this assumption, let’s compare two code blocks. The first function
    appears quite straightforward:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一假设，我们比较两个代码块。第一个函数看起来相当简单：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'On the other hand, the second function seems to involve more work, evident
    in both time and space complexity. However, the ultimate goal of both functions
    is exactly the same:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，第二个函数似乎涉及更多工作，时间和空间复杂度均有所体现。然而，两个函数的*最终目标*是完全相同的：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: I intentionally kept this example simple, but the idea applies to more complicated
    code that might focus too much on a specific problem or do different things.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我故意将这个例子保持简单，但这一理念同样适用于可能过于关注特定问题或做不同事情的更复杂的代码。
- en: '**Caveat:** This doesn’t apply to adding unit tests, comments, or improvements
    that genuinely make the code better. Furthermore, the belief that longer and more
    complex algorithms are superior is not unfounded in certain contexts. For tasks
    that inherently require nuanced decision boundaries or involve complex relationships,
    a more sophisticated algorithm might indeed be necessary.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告：** 这不适用于添加单元测试、注释或真正提升代码质量的改进。此外，较长且复杂的算法在某些情况下并非毫无依据地被认为更优。对于那些本质上需要细致决策边界或涉及复杂关系的任务，更复杂的算法可能确实是必要的。'
- en: '**Trade-off:** The cost of this bias often manifests in terms of computational
    resources. While it might not significantly impact simpler tasks, the expense
    becomes more pronounced for more complex tasks.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**权衡：** 这种偏差的代价通常表现为计算资源的消耗。虽然它可能不会显著影响较简单的任务，但对于更复杂的任务，开销变得更加明显。'
- en: 'Avoiding Category-Size Bias: Strategies for Awareness and Mitigation'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 避免类别大小偏差：提高意识和缓解策略
- en: While category-size bias may not be the most detrimental bias, it can lead to
    resource drainage.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管类别大小偏差可能不是最具破坏性的偏差，但它可能导致资源消耗。
- en: '***It’s better to break things down to simpler and smaller tasks and start
    there when possible for clearer understanding of the underlying problem.***'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '***将问题分解为更简单、更小的任务，并在可能的情况下从那里开始，以更清晰地理解潜在问题，这是更好的做法。***'
- en: Being aware of our subconscious inclination towards favoring one option over
    another is a key to avoid succumbing to the bias. A valuable approach is to challenge
    assumptions, utilizing the Socratic questioning technique.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 意识到我们潜意识中倾向于偏袒某个选项是避免陷入偏见的关键。一种有价值的方法是挑战假设，利用苏格拉底式提问技术。
- en: '[](https://www.verywellmind.com/socratic-questioning-8350838?source=post_page-----8dbd851608c3--------------------------------)
    [## Understanding Socratic Questioning: A Comprehensive Guide'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[理解苏格拉底式提问：全面指南](https://www.verywellmind.com/socratic-questioning-8350838?source=post_page-----8dbd851608c3--------------------------------)
    [## 理解苏格拉底式提问：全面指南'
- en: Socratic questioning encourages people to think more deeply about an issue,
    stepping outside their own perspective…
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 苏格拉底式提问鼓励人们更深入地思考问题，超越自身的视角……
- en: www.verywellmind.com](https://www.verywellmind.com/socratic-questioning-8350838?source=post_page-----8dbd851608c3--------------------------------)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[理解苏格拉底式提问：全面指南](https://www.verywellmind.com/socratic-questioning-8350838?source=post_page-----8dbd851608c3--------------------------------)'
- en: '***Another approach is attempting to prove the hypothesis opposite to the initially
    favored one.***'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '***另一种方法是尝试证明与最初偏好的假设相反的假设。***'
- en: Taking the time to individually assess data and treat each problem distinctly
    can provide valuable insights.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 花时间逐个评估数据，并将每个问题区别对待，可以提供宝贵的见解。
- en: Wrapping up…
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结：
- en: In conclusion, this post highlighted the influence of category-size bias in
    the realm of data science. The key takeaway is, to stay mindful of the bias and
    consistently take the context of the problem into account.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本文突出了类别规模偏差在数据科学领域中的影响。关键的结论是，要时刻留意这些偏差，并始终考虑问题的背景。
- en: Coming up on the horizon…
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 即将到来…
- en: Our analyses often don’t rely only on algorithms and models, but are heavily
    influenced by the deeply embedded cognitive biases. This post explored the Category-Size
    Bias and its impact on decision-making in data science. However, this is just
    the start. In upcoming posts of this series, I’d focus on uncovering more cognitive
    biases and their impact on data research and analytics. From assumptions about
    causation to the appeal of anecdotal evidence, from confirmation bias to the bandwagon
    effect, the posts will explore the intersection of human biases and data science.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析不仅仅依赖于算法和模型，还受到深层次的认知偏差的重大影响。本文探讨了类别规模偏差及其对数据科学决策的影响。然而，这只是个开始。在本系列的后续文章中，我将专注于揭示更多认知偏差及其对数据研究和分析的影响。从因果关系的假设到轶事证据的吸引力，从确认偏差到从众效应，这些文章将探索人类偏差与数据科学的交集。
- en: It’s my goal to present more in-depth examination of biases that could affect
    our analytical pursuits, and promote a more nuanced and bias-aware data science
    practice.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我的目标是对可能影响我们分析追求的偏差进行更深入的研究，并促进更细致和偏差意识的数据科学实践。
- en: I add some additional resources or further exploration.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我添加了一些额外的资源或进一步的探索。
- en: Resources
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[](https://thedecisionlab.com/biases?source=post_page-----8dbd851608c3--------------------------------)
    [## List of Cognitive Biases and Heuristics - The Decision Lab'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://thedecisionlab.com/biases?source=post_page-----8dbd851608c3--------------------------------)
    [## 认知偏差和启发式列表 - 认知决策实验室'
- en: Below is a list of the most important cognitive biases and heuristics in the
    field of behavioural science.
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 以下是行为科学领域中最重要的认知偏差和启发式的列表。
- en: 'thedecisionlab.com](https://thedecisionlab.com/biases?source=post_page-----8dbd851608c3--------------------------------)
    [](https://www.teachthought.com/critical-thinking/cognitive-biases/?source=post_page-----8dbd851608c3--------------------------------)
    [## The Cognitive Biases List: A Visual Of 180+ Heuristics'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[认知偏差列表：180多种启发式的可视化](https://thedecisionlab.com/biases?source=post_page-----8dbd851608c3--------------------------------)
    [](https://www.teachthought.com/critical-thinking/cognitive-biases/?source=post_page-----8dbd851608c3--------------------------------)
    [## 认知偏差列表：180多种启发式的可视化'
- en: Cognitive biases are tendencies to selectively search for or interpret data
    in a way that confirms one's existing…
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 认知偏差是指倾向于选择性地搜索或解释数据，以确认现有的观点…
- en: www.teachthought.com](https://www.teachthought.com/critical-thinking/cognitive-biases/?source=post_page-----8dbd851608c3--------------------------------)
    [](https://www.mindtools.com/aipz4vt/the-ladder-of-inference?source=post_page-----8dbd851608c3--------------------------------)
    [## The Ladder of Inference - How to Avoid Jumping to Conclusions
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.teachthought.com](https://www.teachthought.com/critical-thinking/cognitive-biases/?source=post_page-----8dbd851608c3--------------------------------)
    [](https://www.mindtools.com/aipz4vt/the-ladder-of-inference?source=post_page-----8dbd851608c3--------------------------------)
    [## 推理阶梯 - 如何避免仓促得出结论'
- en: Use the Ladder of Inference to explore the seven steps we take in our thinking
    to get from a fact to a decision or…
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用**推理阶梯**来探索我们在思维中从事实到决策或结论所经过的七个步骤…
- en: www.mindtools.com](https://www.mindtools.com/aipz4vt/the-ladder-of-inference?source=post_page-----8dbd851608c3--------------------------------)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.mindtools.com](https://www.mindtools.com/aipz4vt/the-ladder-of-inference?source=post_page-----8dbd851608c3--------------------------------)'
