["```py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnp.random.seed(0)\n```", "```py\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data[:, :2]  # Take only the first two features\ny = iris.target\n\n# Take only the setosa and versicolor flowers\nX = X[(y == 0) | (y == 1)]\ny = y[(y == 0) | (y == 1)]\n```", "```py\ndef plot_data(X, y):\n    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=iris.target_names[y], style=iris.target_names[y], \n                    palette=['r','b'], markers=('s','o'), edgecolor='k')\n    plt.xlabel(iris.feature_names[0])\n    plt.ylabel(iris.feature_names[1])\n    plt.legend() \n```", "```py\nplot_data(X, y)\n```", "```py\n# Add a column for the bias\nn = X.shape[0] \nX_with_bias = np.hstack((np.ones((n, 1)), X))\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_with_bias, y, random_state=0)\n```", "```py\ndef sigmoid(z):\n    \"\"\" Compute the sigmoid of z (z can be a scalar or a vector). \"\"\"\n    z = np.array(z)\n    return 1 / (1 + np.exp(-z))\n```", "```py\ndef cost_function(X, y, w):\n    \"\"\" J, grad = cost_function(X, y, w) computes the cost of a logistic regression model \n        with parameters w and the gradient of the cost w.r.t. to the parameters. \"\"\"\n    # Compute the cost\n    p = sigmoid(X @ w)    \n    J = -(1/n) * (y @ np.log(p) + (1-y) @ np.log(1-p)) \n\n    # Compute the gradient    \n    grad = (1/n) * X.T @ (p - y)  \n    return J, grad\n```", "```py\nw = np.random.rand(X_train.shape[1])\ncost, grad = cost_function(X_train, y_train, w)\n\nprint('w:', w)\nprint('Cost at w:', cost)\nprint('Gradient at w:', grad)\n```", "```py\nw: [0.5488135  0.71518937 0.60276338]\nCost at w: 2.314505839067951\nGradient at w: [0.36855061 1.86634895 1.27264487]\n```", "```py\ndef optimize_model(X, y, alpha=0.01, max_iter=5000, tol=0.0001):\n    \"\"\" Optimize the model using gradient descent.\n        X, y: The training set        \n        alpha: The learning rate\n        max_iter: The maximum number of passes over the training set (epochs)\n        tol: The stopping criterion. Training will stop when (new_cost > cost - tol)\n    \"\"\"\n    w = np.random.rand(X.shape[1])\n    cost, grad = cost_function(X, y, w)\n\n    for i in range(max_iter):\n        w = w - alpha * grad\n        new_cost, grad = cost_function(X, y, w)        \n        if new_cost > cost - tol:\n            print(f'Converged after {i} iterations')\n            return w, new_cost\n        cost = new_cost\n\n    print('Maximum number of iterations reached')\n    return w, cost\n```", "```py\nopt_w, cost = optimize_model(X_train, y_train)\n\nprint('opt_w:', opt_w)\nprint('Cost at opt_w:', cost)\n```", "```py\nConverged after 1413 iterations\nopt_w: [ 0.28014029  0.80541854 -1.48367938]\nCost at opt_w: 0.28389717767222555\n```", "```py\ndef predict_prob(X, w):\n    \"\"\" Return the probability that samples in X belong to the positive class\n        X: the feature matrix (every row in X represents one sample)\n        w: the learned logistic regression parameters  \n    \"\"\"\n    p = sigmoid(X @ w)\n    return p\n```", "```py\npredict_prob([[1, 6, 2]], opt_w)\n```", "```py\narray([0.89522808])\n```", "```py\npredict_prob([[1, 5.5, 3]], opt_w)\n```", "```py\narray([0.56436688])\n```", "```py\ndef predict(X, w):\n    \"\"\" Predict whether the label is 0 or 1 for the samples in X using a threshold of 0.5\n        (i.e., if sigmoid(X @ theta) >= 0.5, predict 1)\n    \"\"\"\n    p = sigmoid(X @ w)\n    y_pred = (p >= 0.5).astype(int)\n    return y_pred\n```", "```py\npredict([[1, 6, 2], [1, 5.5, 3]], opt_w)\n```", "```py\narray([1, 1])\n```", "```py\ndef evaluate_model(X, y, w):\n    y_pred = predict(X, w)\n    accuracy = np.mean(y == y_pred)\n    return accuracy\n```", "```py\ntrain_accuracy = evaluate_model(X_train, y_train, opt_w)\nprint(f'Train accuracy: {train_accuracy * 100:.3f}%')\n```", "```py\nTrain accuracy: 98.667%\n```", "```py\ntest_accuracy = evaluate_model(X_test, y_test, opt_w)\nprint(f'Test accuracy: {test_accuracy * 100:.3f}%')\n```", "```py\nTest accuracy: 100.000%\n```", "```py\ndef plot_decision_boundary(X, y, w):\n    \"\"\" Plot the decision boundary between the classes \"\"\"\n    plot_data(X, y)\n\n    line_x = np.array(plt.gca().get_xlim())\n    line_y = -1 / w[2] * (w[1] * line_x + w[0])\n    plt.plot(line_x, line_y, c='k', ls='--')\n```", "```py\nplot_decision_boundary(X, y, opt_w)\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n```", "```py\ntrain_accuracy = clf.score(X_train, y_train)\nprint(f'Train accuracy: {train_accuracy * 100:.3f}%')\n\ntest_accuracy = clf.score(X_test, y_test)\nprint(f'Test accuracy: {test_accuracy * 100:.3f}%')\n```", "```py\nTrain accuracy: 100.000%\nTest accuracy: 100.000%\n```", "```py\nprint(clf.n_iter_)\n```", "```py\n[15]\n```", "```py\nopt_w = np.insert(clf.coef_, 0, [clf.intercept_])\nplot_decision_boundary(X, y, opt_w)\n```"]