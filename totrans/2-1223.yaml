- en: How to Perform Multivariate Outlier Detection in Python PyOD for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-perform-multivariate-outlier-detection-in-python-pyod-for-machine-learning-b0a9c557a21c](https://towardsdatascience.com/how-to-perform-multivariate-outlier-detection-in-python-pyod-for-machine-learning-b0a9c557a21c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Outlier detection series, part 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ibexorigin.medium.com/?source=post_page-----b0a9c557a21c--------------------------------)[![Bex
    T.](../Images/516496f32596e8ad56bf07f178a643c6.png)](https://ibexorigin.medium.com/?source=post_page-----b0a9c557a21c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b0a9c557a21c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b0a9c557a21c--------------------------------)
    [Bex T.](https://ibexorigin.medium.com/?source=post_page-----b0a9c557a21c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b0a9c557a21c--------------------------------)
    ·9 min read·Feb 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1e3a4d96c11ab2134a5310a7737a0d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Takashi Miyazaki](https://unsplash.com/@miyatankun?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/backgrounds/art/abstract?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Below is a *very* suspicious boxplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/567acd70936349dc6af5c43bb04d9a66.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: It depicts the relationship between diamonds' quality and their price. The six
    quality categories are given in descending order, so the best diamonds are in
    the *Ideal,* and the lowest quality diamonds are in the *Fair* category.
  prefs: []
  type: TYPE_NORMAL
- en: Now, here are the strange parts. First, all categories have many outliers, marked
    with dark spots above the whiskers.
  prefs: []
  type: TYPE_NORMAL
- en: Second, even though Ideal diamonds are supposed to be the best, their median
    price is lower than any other category (the median is given as the line inside
    the boxes).
  prefs: []
  type: TYPE_NORMAL
- en: 'This strange relationship between diamond qualities and their prices begs us
    the question: are those outliers *actually* outliers?'
  prefs: []
  type: TYPE_NORMAL
- en: Today, we will answer that exact question, or in other words, we will learn
    how to differentiate multivariate outliers and how we can detect them.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the third part of our outlier detection series. Check out the first
    two below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-perform-outlier-detection-in-python-in-easy-steps-for-machine-learning-1-8f9a3e6c88b5?source=post_page-----b0a9c557a21c--------------------------------)
    [## How to Perform Outlier Detection in Python for Machine Learning: Part 1'
  prefs: []
  type: TYPE_NORMAL
- en: Earth is an outlier — the theory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-perform-outlier-detection-in-python-in-easy-steps-for-machine-learning-1-8f9a3e6c88b5?source=post_page-----b0a9c557a21c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: More on multivariate outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the [first part](https://medium.com/towards-data-science/how-to-perform-outlier-detection-in-python-in-easy-steps-for-machine-learning-1-8f9a3e6c88b5)
    of our series, we made the following points.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate outliers, as the name suggests, only exist in single distributions
    or individual columns of datasets. They are much easier to detect with methods
    like z-scores or Median Absolute Deviation. An example is a highly tall person
    in a dataset that records only people's heights (single column).
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate outliers are harder to find as they usually emerge when you simultaneously
    look at many dataset columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say we have a dataset of hospital records. Its columns
    are the physical characteristics of patients and the type of disease they have.
    Now, we are looking at Henry''s records:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Age: 18 Height: 178 cm Weight: 65 kg Non-smoker: Yes Disease: Lung cancer'
  prefs: []
  type: TYPE_NORMAL
- en: If we consider Henry's attributes individually, they seem ordinary within the
    context of our dataset. In our dataset, there are many 18 year-olds and many people
    that are 178 cm tall or weigh 65 kg.
  prefs: []
  type: TYPE_NORMAL
- en: Also, many non-smoking 18-year-olds or people with lung cancer are in the hospital.
    BUT, we do NOT have any non-smoking 18-year-olds with lung cancer that weigh 65
    kg and are 178 cm tall.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that Henry was a multivariate outlier came to light only when we considered
    all his physical attributes simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Similar cases of Henry's might emerge across dozens or even hundreds of attributes
    in another dataset. Therefore, we must learn more sophisticated methods to detect
    multivariate outliers.
  prefs: []
  type: TYPE_NORMAL
- en: This is where machine learning methods, specifically outlier classifiers, come
    into play.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s first load the dataset we will be working with. It is the same Diamonds
    dataset from the last tutorial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/143af438d5bead17872f4ddaec9cb5c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'We perform a small preprocessing step where we encode the categoricals with
    the OrdinalEncoder class of Sklearn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's begin! (rubs hands together)
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate outlier detection with PyOD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even though the theory may be a bit hard, performing multivariate outlier detection
    in code is very easy thanks to the Python Outlier Detection (PyOD) library. Let''s
    see an example using the Local Outlier Factor algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After importing the `LOF` estimator from `pyod`, we initialize it with 30 neighbors
    and fit to `X` (don't worry about the details of `LOF`, we'll talk about that
    later).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we access its `labels_` attribute, which returns 0 (inliers) or 1 (outliers)
    for each row of `X`. Let''s count the outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Hmm, we have got 5394 outliers, or exactly 10% of the dataset. Is this a coincidence?
    Let''s try the same operation with another outlier classifier, Isolation Forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We get the same suspicious 10%. This can't be a coincidence (actually, nothing
    is — Oogway).
  prefs: []
  type: TYPE_NORMAL
- en: What is contamination in outlier detection?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Outlier classifier algorithms like Isolation Forest or Local Outlier Factor
    aren''t actually classifiers. When they are fit to a dataset, their internal algorithms
    calculate anomaly scores for each row in the dataset. Here is an example with
    IForest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Using the `decision_scores_` attribute, you can access the anomaly scores of
    any algorithm once they are fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'These scores tell us how anomalous each sample in the dataset is. By default,
    all algorithms in PyOD mark the samples with the top 10% highest anomaly scores
    as outliers. This is given as the *contamination* parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/186ae0b8082ab23c040aa06a6d072b75.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Basically, outlier classifiers are saying to us, "Here is how anomalous I think
    each sample is. For now, I will select the 10% of most anomalous samples as outliers,
    but you can always change that with my contamination parameter".
  prefs: []
  type: TYPE_NORMAL
- en: Basically, `contamination` is a hyperparameter of all classifiers that controls
    the number of produced outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Now, this may seem paradoxical — we want to detect outliers using classifiers,
    but classifiers are telling us to decide how we choose those outliers. That right
    there is the biggest challenge of outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: You, the machine learning engineer, are behind the wheel — you make all the
    decisions. YOU have to decide what percentage to set for contamination using your
    experience, domain knowledge, intuition, and trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: Probability confidence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If finding the correct value for contamination sounds complicated, there is
    a workaround that enables us to ditch contamination. And the workaround is outlier
    probability scores.
  prefs: []
  type: TYPE_NORMAL
- en: Once a PyOD estimator is fit to a dataset, it will have `predict_proba` method
    available. It returns two columns for each sample; the first column is the probability
    of the sample being an inlier and the second is the probability of the sample
    being an outlier. Let's call those inlier probability and outlier probability
    for short.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: These probabilities are generated by normalizing the `decision_scores_` array
    using MinMax scaling, also called normalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Normalizing an array will force it to be within the range of 0 and 1, which
    means we can interpret anomaly scores as probabilities of samples being an outlier.
    So, here is how `predict_proba` is written under the hood:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, instead of selecting outliers based on contamination, we can select them
    based on probability confidence. Instead of choosing 10% of most anomalous samples,
    we can select samples that have at least 80%, 90%, or n% outlier probability.
    Here is what it would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Using the probability confidence method, we only find 12 outliers, instead of
    over 5000\. And the best part is that we are 90% sure those 12 outliers are actually
    outliers!
  prefs: []
  type: TYPE_NORMAL
- en: How to choose the right probability threshold
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choosing a probability threshold depends on several factors like dataset size,
    the task you are trying to solve, and the business domain.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a small dataset, you want to make absolutely sure the outliers you
    are isolating are actually outliers. By dropping them, you don't want to lose
    important information and patterns in your small dataset which will inevitably
    affect machine learning models. For such cases, you should set a high threshold
    like 80–90%.
  prefs: []
  type: TYPE_NORMAL
- en: If you are dealing with sensitive data like medical records where it is critical
    to avoid false positives, you should aim for over 90%. 95% and 99% are pretty
    common as well.
  prefs: []
  type: TYPE_NORMAL
- en: It all depends on how confident about outliers you want or must be while solving
    the business problem with the data at hand.
  prefs: []
  type: TYPE_NORMAL
- en: How to choose the right outlier classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a supervised-learning task, your job would be easy. You would evaluate a
    bunch of algorithms using metrics like RMSE or accuracy and choose the one that
    looks most promising.
  prefs: []
  type: TYPE_NORMAL
- en: However, since outlier detection is an unsupervised-learning problem, you won't
    really be able to compare one algorithm to another because you can't measure their
    performance with metrics.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are some algorithms that consistently perform well if used correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[Isolation Forest](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest):
    works on almost any type of dataset but is specifically designed for high-dimensional,
    complex ones. Even though it is a collection of different type of decision trees,
    called ITrees, it is fast and has a high performance. It can also handle data
    with irregular, non-normal distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Local Outlier Factor](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof):
    particularly good for datasets with data points that are clustered together as
    it calculates anomaly scores using local density. It works well with high-dimensional
    data and can be faster than IForest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to learn more about how to choose the right classifier, check out
    [my Anomaly Detection course on DataCamp](https://app.datacamp.com/learn/courses/anomaly-detection-in-python)
    where I teach close to 10 outlier classifiers, how and when to use them, and how
    to tune their hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Also, [this page](https://pyod.readthedocs.io/en/latest/pyod.models.html#) of
    PyOD documentation lists over 40 algorithms. You can see how they perform by looking
    at this [table](https://pyod.readthedocs.io/en/latest/benchmark.html) where they
    list the performance of the 10 best outlier classifiers on more than 15 datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article covered the topic of multivariate outlier detection in machine
    learning and demonstrated how it can be done using PyOD in Python. By transforming
    anomaly scores into probability confidence, selecting the best outlier classifier,
    and determining the right probability threshold, you can enhance your machine
    learning models by removing outliers from the data. With this knowledge, you are
    now ready to perform efficient outlier detection in your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: Loved this article and, let’s face it, its bizarre writing style? Imagine having
    access to dozens more just like it, all written by a brilliant, charming, witty
    author (that’s me, by the way :).
  prefs: []
  type: TYPE_NORMAL
- en: For only 4.99$ membership, you will get access to not just my stories, but a
    treasure trove of knowledge from the best and brightest minds on Medium. And if
    you use [my referral link](https://ibexorigin.medium.com/membership), you will
    earn my supernova of gratitude and a virtual high-five for supporting my work.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://ibexorigin.medium.com/membership?source=post_page-----b0a9c557a21c--------------------------------)
    [## Join Medium with my referral link — Bex T.'
  prefs: []
  type: TYPE_NORMAL
- en: Get exclusive access to all my ⚡premium⚡ content and all over Medium without
    limits. Support my work by buying me a…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ibexorigin.medium.com](https://ibexorigin.medium.com/membership?source=post_page-----b0a9c557a21c--------------------------------)
    ![](../Images/95a45f54ecebc1b690dfd77f973617d2.png)
  prefs: []
  type: TYPE_NORMAL
