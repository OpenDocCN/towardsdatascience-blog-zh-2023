- en: 'Courage to Learn ML: Decoding Likelihood, MLE, and MAP'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 勇敢学习机器学习：解码似然、MLE和MAP
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-decoding-likelihood-mle-and-map-65218b2c2b99](https://towardsdatascience.com/courage-to-learn-ml-decoding-likelihood-mle-and-map-65218b2c2b99)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/courage-to-learn-ml-decoding-likelihood-mle-and-map-65218b2c2b99](https://towardsdatascience.com/courage-to-learn-ml-decoding-likelihood-mle-and-map-65218b2c2b99)
- en: With A Tail of Cat Food Preferences
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 《猫粮偏好之尾》
- en: '[](https://amyma101.medium.com/?source=post_page-----65218b2c2b99--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----65218b2c2b99--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65218b2c2b99--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65218b2c2b99--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----65218b2c2b99--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page-----65218b2c2b99--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----65218b2c2b99--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65218b2c2b99--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65218b2c2b99--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----65218b2c2b99--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----65218b2c2b99--------------------------------)
    ·10 min read·Dec 3, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----65218b2c2b99--------------------------------)
    ·10分钟阅读·2023年12月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/93ae20104fe2612dcc2083619a7ca29f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93ae20104fe2612dcc2083619a7ca29f.png)'
- en: Photo by [Anastasiia Rozumna](https://unsplash.com/@rozumna?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Anastasiia Rozumna](https://unsplash.com/@rozumna?utm_source=medium&utm_medium=referral)
    在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Welcome to the ‘[Courage to learn ML’](/towardsdatascience.com/tagged/courage-to-learn-ml).
    This series aims to simplify complex machine learning concepts, presenting them
    as a relaxed and informative dialogue, much like the engaging style of “[The Courage
    to Be Disliked](https://www.goodreads.com/book/show/43306206-the-courage-to-be-disliked),”
    but with a focus on ML.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到‘[勇敢学习机器学习](/towardsdatascience.com/tagged/courage-to-learn-ml)’。本系列旨在简化复杂的机器学习概念，以轻松且信息丰富的对话呈现，类似于《[勇气不被讨厌](https://www.goodreads.com/book/show/43306206-the-courage-to-be-disliked)》的风格，但专注于机器学习。
- en: 'In this installment of our series, our mentor-learner duo dives into a fresh
    discussion on statistical concepts like MLE and MAP. This discussion will lay
    the groundwork for us to gain a new perspective on our previous exploration of
    L1 & L2 Regularization. For a complete picture, I recommend reading this post
    before reading the fourth part of ‘[Courage to Learn ML: Demystifying L1 & L2
    Regularization’](https://yujing-ma45.medium.com/understanding-l1-l2-regularization-part-1-9c7affe6f920).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列的这一部分，我们的导师-学习者二人组深入探讨MLE和MAP等统计概念。这一讨论将为我们提供一个新的视角，审视我们之前对L1和L2正则化的探索。为了完整理解，建议在阅读《[勇敢学习机器学习：揭示L1和L2正则化](/understanding-l1-l2-regularization-part-1-9c7affe6f920)》的第四部分之前阅读本篇文章。
- en: 'This article is designed to tackle fundamental questions that might have crossed
    your path in Q&A style. As always, if you find yourself have similar questions,
    you’ve come to the right place:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在用问答形式解答可能遇到的基本问题。如往常一样，如果你有类似的问题，恭喜你来对地方了：
- en: What exactly is ‘likelihood’?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘似然’到底是什么？
- en: The difference between likelihood and probability
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 似然与概率的区别
- en: Why is likelihood important in the context of machine learning?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么在机器学习中，似然如此重要？
- en: What is MLE (Maximum Likelihood Estimation)?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是MLE（最大似然估计）？
- en: What is MAP (Maximum A Posteriori Estimation**)?**
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是MAP（**最大后验估计**）？
- en: The difference between MLE and Least square
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLE与最小二乘法的区别
- en: The Links and Distinctions Between MLE and MAP
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLE与MAP之间的联系和区别
- en: '**What exactly is ‘likelihood’?**'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**‘似然’到底是什么？**'
- en: Likelihood, or more specifically **the likelihood function**, is a statistical
    concept used to evaluate the probability of observing the given data under various
    sets of model parameters. It is called likelihood (function) because it’s a function
    that quantifies how likely it is to observe the current data for different parameter
    values of a statistical model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 似然，或者更具体地说**似然函数**，是一个统计学概念，用于评估在不同模型参数集下观察到给定数据的概率。它被称为似然（函数），因为它是一个量化观察当前数据在不同统计模型参数值下的可能性的函数。
- en: '**Likelihood seems similar to probability. Is it a form of probability, or
    if not, how does it differ from probability?**'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**似然看起来类似于概率。它是概率的一种形式吗？如果不是，它与概率有何不同？**'
- en: The concepts of likelihood and probability are fundamentally different in statistics.
    **Probability measures the chance of observing a specific outcome in the future,
    given known parameters or distributions.** In this scenario, the parameters or
    the distribution are known, and we’re interested in predicting the probability
    of various outcomes. **Likelihood, in contrast, measures how well a set of underlying
    parameters explains the observed outcomes.** In this setting, the outcomes are
    already observed, and we seek to understand what underlying parameters or conditions
    could have led to these outcomes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 概率和似然在统计学中是根本不同的概念。**概率测量的是在已知参数或分布的情况下，观察到特定结果的可能性。** 在这种情况下，参数或分布是已知的，我们关注的是预测各种结果的概率。**而似然则衡量一组潜在参数如何解释观察到的结果。**
    在这种设置下，结果已经被观察到，我们试图理解什么样的潜在参数或条件可能导致这些结果。
- en: To illustrate this with an intuitive example, consider my cat Bubble’s preference
    for chicken over beef.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个直观的例子来说明，考虑一下我家猫Bubble更喜欢鸡肉而不是牛肉的情况。
- en: '![](../Images/1422d3b812a46720778512f219430155.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1422d3b812a46720778512f219430155.png)'
- en: A photo of my cat, Bubble
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我家的猫Bubble的照片
- en: When I buy cat food, I choose more chicken-flavored cans because I know there’s
    a higher probability she will enjoy them and finish them all. This is an application
    of probability, where I use my knowledge of Bubble’s preferences to predict future
    outcomes. However, Bubble’s preference is not something she explicitly communicates.
    I inferred it by observing her eating habits over the past six years. Noticing
    that she consistently eats more chicken than beef indicates a higher likelihood
    of her preferring chicken. This inference process is an example of using likelihood.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我购买猫粮时，我会选择更多鸡肉味的罐头，因为我知道她更可能喜欢它们并且吃完。这是概率的应用，我利用对Bubble偏好的了解来预测未来的结果。然而，Bubble的偏好不是她明确表达的。我通过观察她过去六年的饮食习惯得出了这个结论。注意到她总是吃更多的鸡肉而不是牛肉，表明她更可能喜欢鸡肉。这个推断过程是使用似然的一个例子。
- en: '**It’s important to note that, in statistics, likelihood is a function.** This
    function calculates the probability that a particular set of parameters is the
    most suitable explanation for the observed data. Unlike probability, the values
    of a likelihood function do not necessarily sum up to 1\. This is because probability
    deals with the sum of all possible outcomes for given parameters, which must be
    1, while likelihood deals with how probable different parameter sets are given
    the observed data.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**重要的是要注意，在统计学中，似然是一个函数。** 这个函数计算的是特定参数集最适合解释观察数据的概率。与概率不同，似然函数的值不一定总和为1。这是因为概率涉及的是给定参数下所有可能结果的总和，必须为1，而似然涉及的是给定观察数据下不同参数集的可能性。'
- en: Why is likelihood important in the context of machine learning?
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 似然在机器学习的背景下为什么很重要？
- en: 'Understanding the application of likelihood in the machine learning context
    requires us to consider how we evaluate model results. Essentially, we need a
    set of rules to judge between different sets of parameters. There are two primary
    approaches to measure how well a model, with its current parameters, explains
    the observed data:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 理解似然在机器学习背景下的应用需要我们考虑如何评估模型结果。从本质上讲，我们需要一套规则来判断不同的参数集。测量模型当前参数解释观察数据的效果主要有两种方法：
- en: The first method involves using a difference-based approach. We compare each
    true label with the corresponding prediction and attempt to find a set of model
    parameters that **minimizes these differences**. This is the basic idea behind
    the least squares method, which focuses on error minimization.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方法涉及使用基于差异的方法。我们比较每个真实标签与相应的预测，并尝试找到一组**最小化这些差异**的模型参数。这是最小二乘法的基本思想，它侧重于误差最小化。
- en: The second method is where likelihood, specifically Maximum Likelihood Estimation
    (MLE), comes into play. MLE seeks to find a set of parameters that makes the observed
    data most probable. In other words, by observing the data, **we choose parameters
    that maximize the likelihood of observing the current data set.** This approach
    goes beyond just minimizing error; it considers the probability and models the
    uncertainty in parameter estimation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是似然，特别是最大似然估计（MLE），在其中发挥作用。MLE 试图找到一组使观测数据最有可能的参数。换句话说，通过观察数据，**我们选择最大化观察当前数据集似然的参数。**
    这种方法超越了仅仅最小化误差的层面；它考虑了概率并对参数估计中的不确定性进行建模。
- en: In Maximum Likelihood Estimation (MLE), the underlying assumption is that the
    optimal parameters for a model are those that maximize the likelihood of observing
    the given dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在最大似然估计（MLE）中，基本假设是模型的最佳参数是那些最大化观察到的数据集似然的参数。
- en: In summary, while the least squares method and MLE differ in their approaches
    — one being error-minimizing and the other probabilistic — both are essential
    in the machine learning toolkit for parameter estimation and model evaluation.
    We will explore these methods further, discussing their differences and connections,
    in future posts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，虽然最小二乘法和 MLE 在方法上有所不同——一个是误差最小化的，另一个是概率性的——但这两者在机器学习工具箱中对于参数估计和模型评估都是必不可少的。我们将在未来的帖子中深入探讨这些方法，讨论它们的区别和联系。
- en: Could you provide an intuitive example to contrast those two evaluation approaches
    (MLE vs. least squares)?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 能否提供一个直观的例子来对比这两种评估方法（MLE 与最小二乘法）？
- en: Considering my cat Bubble’s preference for food, let’s say I initially assume
    she likes chicken and beef equally. To test this using the least squares method,
    I would collect data by buying an equal number of chicken and beef flavored cans.
    As Bubble eats, I’d record how much of each she consumes. The least squares method
    would then help me adjust my initial assumption (parameters) by minimizing the
    difference (squared error) between my prediction (equal preference) and the actual
    consumption pattern (true labels).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我的猫 Bubble 对食物的偏好，假设我最初认为她喜欢鸡肉和牛肉一样多。为了使用最小二乘法测试这一点，我会通过购买相等数量的鸡肉和牛肉口味罐头来收集数据。当
    Bubble 吃东西时，我会记录她每种食物的消费量。然后，最小二乘法将帮助我通过最小化我的预测（平等偏好）与实际消费模式（真实标签）之间的差异（平方误差）来调整我的初始假设（参数）。
- en: For the MLE approach, instead of starting with an assumption about Bubble’s
    preference, I would first observe her eating habits over time. Based on this data,
    I’d use MLE to find the parameter values (in this case, preference for chicken
    or beef) that make the observed data most probable. For example, if Bubble consistently
    chooses chicken over beef, the MLE method would identify a higher probability
    for chicken preference.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MLE 方法，我会先观察 Bubble 随时间的进食习惯，而不是从对她的偏好的假设开始。根据这些数据，我会使用 MLE 找到使观测数据最有可能的参数值（在这种情况下，鸡肉或牛肉的偏好）。例如，如果
    Bubble 一直选择鸡肉而不是牛肉，MLE 方法将确定鸡肉偏好的更高概率。
- en: So MLE uses likelihood to select parameters. What are their mathematical representations?
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所以 MLE 使用似然来选择参数。它们的数学表示是什么？
- en: In Maximum Likelihood Estimation (MLE), the primary goal is to identify the
    set of parameters (*θ*) that most likely produces the observed data. This process
    involves defining **the likelihood function, denoted as *L*(*θ*) or *L*(*θ*∣*x*),
    where *x* represents the observed data.** The likelihood function calculates the
    probability of observing the given data *x* assuming the model parameters are
    *θ*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在最大似然估计（MLE）中，主要目标是确定最有可能生成观测数据的参数集（*θ*）。这个过程涉及定义**似然函数，记作 *L*(*θ*) 或 *L*(*θ*∣*x*)，其中
    *x* 表示观测数据。** 似然函数计算在模型参数为 *θ* 的情况下，观察到给定数据 *x* 的概率。
- en: 'The essence of MLE is to find the parameter values that maximize the likelihood
    function. Mathematically, this is its representation and calculation process:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: MLE 的本质是找到最大化似然函数的参数值。在数学上，这就是它的表示和计算过程：
- en: '![](../Images/041b8e7a0be8bb3ff64dba684e91367c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/041b8e7a0be8bb3ff64dba684e91367c.png)'
- en: The post will explore this equation in depth in a subsequent questions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将通过随后的问题深入探讨这个方程。
- en: Hold on a moment… we define likelihood as L(θ) = p(x|θ), signifying the probability
    of observing the data *x* given a set of parameters *θ*. But earlier, we mentioned
    that likelihood involves having a set of observations and then calculating the
    likelihood for a set of parameters. Shouldn’t it be L(θ) = p(θ|x) instead?
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等一下……我们将似然定义为 L(θ) = p(x|θ)，表示在给定一组参数 *θ* 的情况下观察到数据 *x* 的概率。但是早些时候我们提到过似然涉及到有一组观察数据，然后为一组参数计算似然。难道它不应该是
    L(θ) = p(θ|x) 吗？
- en: In understanding MLE, it’s crucial to distinguish between the likelihood function
    and probability. The likelihood function, denoted as *L*(*θ*), is not the same
    as the probability p(θ∣x).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解 MLE 时，区分似然函数和概率是至关重要的。似然函数，记作 *L*(*θ*)，并不等同于概率 p(θ∣x)。
- en: While p(θ∣x) refers to the probability of the parameter values θ given the observed
    data x (a concept central to Bayesian inference), L(θ) is about the likelihood
    function, which evaluates how plausible different parameter values are in explaining
    the observed data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: p(θ∣x) 表示在给定观察到的数据 x 的情况下，参数值 θ 的概率（这是贝叶斯推断中的核心概念），而 L(θ) 则是关于似然函数的，它评估不同参数值在解释观察到的数据时的可能性。
- en: 'For calculating the likelihood function, we use the probability of observing
    the data x given certain parameter values θ, denoted as p(x∣θ). This probability
    function is used to assess the adequacy of different parameter settings. Therefore,
    in MLE, we have L(θ)=p(x∣θ). It’s important to interpret this equation correctly:
    the equal sign here signifies that we calculate the likelihood L(θ) using the
    probability p(x∣θ); **it does not imply a direct equivalence between L(θ) and
    p(x∣θ)**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 计算似然函数时，我们使用在给定特定参数值 θ 的情况下观察数据 x 的概率，记作 p(x∣θ)。这个概率函数用于评估不同参数设置的适用性。因此，在 MLE
    中，我们有 L(θ)=p(x∣θ)。重要的是正确解释这个方程：等号在这里意味着我们使用概率 p(x∣θ) 来计算似然 L(θ)；**这并不意味着 L(θ)
    和 p(x∣θ) 直接等价**
- en: In summary, **L(θ) quantifies how well the parameters θ explain the data x,
    while p(θ∣x) is about the probability of the parameters after observing the data**.
    Understanding this distinction is fundamental to grasping the principles of MLE
    and its application in statistical modeling.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，**L(θ) 量化了参数 θ 解释数据 x 的效果，而 p(θ∣x) 则是观察到数据后的参数概率**。理解这一点对于掌握 MLE 的原理及其在统计建模中的应用至关重要。
- en: But wouldn’t using p(θ|x) provide a more direct evaluation of which parameter
    set is better, instead of relying on the likelihood function?
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 但是，使用 p(θ|x) 是否不会提供更直接的评估，以确定哪一组参数更好，而不是依赖于似然函数呢？
- en: I’m glad you noticed this important distinction. Theoretically, calculating
    ( p(θ|x) ) for different parameter sets (θ) and choosing the one with the highest
    probability would indeed provide a direct evaluation of which set of parameters
    is better. This is achievable through **Bayes’ theorem**, which helps in computing
    the posterior probability ( p(θ|x) ).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 很高兴你注意到了这一重要区别。从理论上讲，计算不同参数集（θ）的 ( p(θ|x) ) 并选择概率最高的那个确实可以直接评估哪一组参数更好。这可以通过
    **贝叶斯定理** 实现，该定理有助于计算后验概率 ( p(θ|x) )。
- en: '![](../Images/cc7aaf2a69c33cde4c22eceb876f9e12.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc7aaf2a69c33cde4c22eceb876f9e12.png)'
- en: 'To calculate this posterior, we consider three key elements:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这个后验概率，我们需要考虑三个关键要素：
- en: '**Likelihood ( p(x|θ) )** : This represents how probable the observed data
    is given a set of parameters. *It’s the basis of MLE, focusing on how well the
    parameters explain the observed data.*'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**似然 ( p(x|θ) )**：这表示在给定一组参数的情况下，观察到的数据的概率。*这是最大似然估计（MLE）的基础，侧重于参数解释观察到的数据的效果。*'
- en: '**Prior ( p(θ) )** : This reflects our initial beliefs about the parameters
    before observing any data. It’s an essential part of Bayesian inference, where
    prior knowledge about parameter distribution is factored in.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**先验 ( p(θ) )**：这反映了我们在观察到任何数据之前对参数的初始信念。这是贝叶斯推断中的一个重要部分，其中先验知识关于参数分布被纳入考虑。'
- en: '**Marginal Likelihood or Evidence ( p(x) )**: This measures how probable the
    observed data is under all possible parameter sets, essentially assessing the
    probability of observing the data without making specific assumptions about parameters.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边际似然或证据 ( p(x) )**：这衡量了在所有可能参数集下观察到数据的概率，基本上评估了在不做特定参数假设的情况下观察数据的概率。'
- en: In practice, the marginal likelihood ( p(x) ) can often be ignored, especially
    when comparing different sets of parameters, as it remains constant and doesn’t
    influence the relative comparison.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，边际似然 ( p(x) ) 通常可以忽略，特别是在比较不同参数集时，因为它保持不变且不影响相对比较。
- en: With Bayes’ theorem, we find that the posterior ( p(θ|x) ) i**s proportional
    to** the product of the likelihood and the prior, ( p(x|θ)* p(θ) ).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理，我们发现后验 ( p(θ|x) ) **与** 似然函数和先验函数的乘积成正比，( p(x|θ)* p(θ) )。
- en: '![](../Images/ec9daffb851124b23dcda9ec969d39f3.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec9daffb851124b23dcda9ec969d39f3.png)'
- en: This means to compare different parameter sets, we must consider both our prior
    beliefs about the parameters and the likelihood, which is how the observed data
    modifies our beliefs. Like MLE, **in MAP (Maximum A Posteriori Estimation), we
    seek to maximize the posterior to find the best set of model parameters, integrating
    both prior knowledge and observed data.**
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在比较不同的参数集时，我们必须同时考虑对参数的先验信念和似然，即观察数据如何修改我们的信念。与 MLE 相似，**在 MAP（最大后验估计）中，我们寻求最大化后验，以找到最佳的模型参数集，综合考虑先验知识和观察数据。**
- en: So, MAP incorporates an additional element, which is our prior belief about
    the parameter.
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所以，MAP 融入了一个额外的元素，即我们对参数的先验信念。
- en: Correct. MAP indeed uses an extra piece of information, which is our prior belief
    about the parameters. Let’s use the example of my cat Bubble (again) to illustrate
    this. In the context of MAP, when determining Bubble’s preferred food flavor —
    beef or chicken — I would consider a hint from the breeder. The breeder mentioned
    that Bubble likes to eat boiled chicken breast, so this information forms my prior
    belief that Bubble may prefer chicken flavor. Consequently, when initially choosing
    her food, I would lean towards buying more chicken-flavored food. This approach
    of incorporating the breeder’s insight represents the ‘prior’ in MAP estimation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正确。MAP 确实使用了额外的信息，即我们对参数的先验信念。让我们再次用我的猫 Bubble 的例子来说明。在 MAP 的背景下，当确定 Bubble
    喜欢的食物口味——牛肉还是鸡肉时，我会考虑来自饲养员的提示。饲养员提到 Bubble 喜欢吃煮鸡胸肉，因此这一信息形成了我对 Bubble 可能偏爱鸡肉口味的先验信念。因此，在最初选择她的食物时，我会倾向于购买更多的鸡肉口味的食物。这种将饲养员的见解融入
    MAP 估计的方法就代表了 MAP 中的‘先验’。
- en: '**I understand that MAP and MLE are related, with MAP adding in our assumption
    about the parameter. Can you offer a more straightforward example to show me the
    difference and connections between those two methods?**'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**我理解 MAP 和 MLE 是相关的，其中 MAP 加入了我们对参数的假设。你能提供一个更简单的例子来展示这两种方法之间的区别和联系吗？**'
- en: '*To demonstrate the connection between MAP and MLE, I’ll introduce some mathematical
    formulas. While the goal of this discussion is to intuitively explain machine
    learning concepts through dialogue, showcasing these functions will help. Don’t
    fret over complexity; these formulas simply highlight the extra insights MAP offers
    compared to MLE for a clearer understanding.*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*为了演示 MAP 和 MLE 之间的联系，我将介绍一些数学公式。虽然本讨论的目标是通过对话直观地解释机器学习概念，但展示这些函数将有所帮助。不要担心复杂性，这些公式只是突出
    MAP 相较于 MLE 提供的额外见解，以便更清晰地理解。*'
- en: '**Maximum Likelihood Estimation (MLE) focuses on identifying the parameter
    set θ that makes the observed data x most probable.** It achieves this by maximizing
    the likelihood function P(X|θ).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大似然估计 (MLE) 侧重于识别使观察数据 x 最有可能的参数集 θ。** 它通过最大化似然函数 P(X|θ) 实现这一目标。'
- en: '![](../Images/7a8c10020ff62408efe16b337f140ce2.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a8c10020ff62408efe16b337f140ce2.png)'
- en: However, directly maximizing the product of probabilities, which are typically
    less than 1, can be impractical due to computational underflow — a condition where
    numbers become too small to be represented accurately. To overcome this, we use
    logarithms, transforming the product into a sum. Since the logarithm function
    is monotonically increasing, maximizing a function is equivalent to maximizing
    its logarithm. Thus, the MLE formula often involves the sum of the logarithms
    of probabilities.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，直接最大化概率的乘积（这些概率通常小于 1）可能由于计算下溢——一种数字变得过于微小以至于无法准确表示的情况——而不切实际。为了克服这一问题，我们使用对数，将乘积转化为和。由于对数函数是单调递增的，最大化一个函数等同于最大化其对数。因此，MLE
    公式通常涉及概率对数的和。
- en: '![](../Images/8ce6a8878a93c2611d2b2f4aee949755.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ce6a8878a93c2611d2b2f4aee949755.png)'
- en: On the other hand, **Maximum A Posteriori (MAP) estimation aims to maximize
    the posterior probability.** Applying Bayes’ theorem, we see that maximizing the
    posterior is equivalent to maximizing the product of the prior probability P(θ)
    and the likelihood. Like in MLE, we introduce logarithms to simplify computation,
    converting the product into a sum.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**最大后验估计（MAP）旨在最大化后验概率。** 运用贝叶斯定理，我们发现最大化后验概率等同于最大化先验概率P(θ)和似然的乘积。与MLE一样，我们引入对数来简化计算，将乘积转换为和。
- en: '![](../Images/3f2ee31b0d6c6cd4867e22c2c1e24fdb.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f2ee31b0d6c6cd4867e22c2c1e24fdb.png)'
- en: The primary distinction between MLE and MAP lies in the inclusion of the prior
    P(θ) in MAP. **This addition means that in MAP, the likelihood is effectively
    weighted by the prior, influencing the estimation based on our prior beliefs about
    the parameters.** In contrast, MLE does not include such a prior and focuses solely
    on the likelihood derived from the observed data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: MLE与MAP之间的主要区别在于MAP中包含了先验P(θ)。**这一附加因素意味着在MAP中，似然被先验有效地加权，基于我们对参数的先验信念影响估计。**
    相比之下，MLE不包含这种先验，仅关注从观测数据中得出的似然。
- en: It seems like MAP might be superior to MLE. Why don’t we always opt for MAP
    then?
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 看起来MAP可能优于MLE。那么我们为什么不总是选择MAP呢？
- en: 'MAP estimation incorporates our pre-existing knowledge about the parameters
    distribution. But it doesn’t inherently make it superior to MLE. There are several
    factors to consider:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: MAP估计包含了我们对参数分布的先验知识。但这并不 inherently 使它优于MLE。需要考虑几个因素：
- en: '**An assumption about the parameter distribution isn’t always available.**
    In cases where the parameter distribution is assumed to be uniform, MAP and MLE
    yield equivalent results.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关于参数分布的假设并不总是可用的。** 在假设参数分布为均匀的情况下，MAP和MLE会产生等效的结果。'
- en: '**The computational simplicity of MLE often makes it a more practical choice.**
    While MAP provides a comprehensive Bayesian approach, it can be computationally
    intensive.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLE的计算简单性通常使它成为更实际的选择。** 虽然MAP提供了全面的贝叶斯方法，但它的计算复杂度较高。'
- en: '**MAP’s effectiveness heavily relies on the selection of an appropriate prior.**
    An inaccurately chosen prior can lead to increased computational costs for MAP
    to identify an optimal set of parameters.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MAP的有效性在很大程度上依赖于选择适当的先验。** 选择不准确的先验可能会导致MAP在识别最佳参数集时计算成本增加。'
- en: In our next session, our mentor-learner team will return to delve deeper into
    L1 and L2 regularization. Armed with a solid understanding of MLE and MAP, we’ll
    be able to view [L1 and L2 regularization](https://yujing-ma45.medium.com/understanding-l1-l2-regularization-part-1-9c7affe6f920)
    from a fresh perspective. Looking forward to seeing you in the next post!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一节中，我们的导师-学习者团队将回到深入探讨L1和L2正则化。掌握了MLE和MAP的基本知识后，我们将能够从全新的视角查看[L1和L2正则化](https://yujing-ma45.medium.com/understanding-l1-l2-regularization-part-1-9c7affe6f920)。期待在下一篇文章中见到你！
- en: 'Other posts in this series:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列中的其他文章：
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 1)](/understanding-l1-l2-regularization-part-1-9c7affe6f920)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习机器学习的勇气：解密L1和L2正则化（第1部分）](/understanding-l1-l2-regularization-part-1-9c7affe6f920)'
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 2)](/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习机器学习的勇气：解密L1和L2正则化（第2部分）](/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)'
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 3)](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习机器学习的勇气：解密L1和L2正则化（第3部分）](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)'
- en: '***If you liked the article, you can find me on*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***.***'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***如果你喜欢这篇文章，可以在*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***找到我。***'
- en: '**Reference:**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[](https://agustinus.kristia.de/techblog/2017/01/01/mle-vs-map/?source=post_page-----65218b2c2b99--------------------------------)
    [## MLE vs MAP: the connection between Maximum Likelihood and Maximum A Posteriori
    Estimation'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://agustinus.kristia.de/techblog/2017/01/01/mle-vs-map/?source=post_page-----65218b2c2b99--------------------------------)
    [## MLE与MAP：最大似然估计与最大后验估计之间的联系'
- en: In this post, we will see what is the difference between Maximum Likelihood
    Estimation (MLE) and Maximum A Posteriori…
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将看到最大似然估计（MLE）和最大后验估计（MAP）之间的区别…
- en: agustinus.kristia.de](https://agustinus.kristia.de/techblog/2017/01/01/mle-vs-map/?source=post_page-----65218b2c2b99--------------------------------)
    [](https://www.sefidian.com/2022/06/21/difference-between-maximum-likelihood-estimation-mle-and-maximum-a-posteriori-map/?source=post_page-----65218b2c2b99--------------------------------)
    [## Difference between Maximum Likelihood Estimation (MLE) and Maximum A Posteriori
    (MAP) - Amir Masoud…
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[最大似然估计 (MLE) 与最大后验估计 (MAP) 之间的区别](https://agustinus.kristia.de/techblog/2017/01/01/mle-vs-map/?source=post_page-----65218b2c2b99--------------------------------)  '
- en: Difference between Maximum Likelihood Estimation (MLE) and Maximum A Posteriori
    (MAP)
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大似然估计 (MLE) 和最大后验估计 (MAP) 之间的区别
- en: '- Amir Masoud… Difference between Maximum Likelihood Estimation (MLE) and Maximum
    A Posteriori (MAP)www.sefidian.com](https://www.sefidian.com/2022/06/21/difference-between-maximum-likelihood-estimation-mle-and-maximum-a-posteriori-map/?source=post_page-----65218b2c2b99--------------------------------)
    [](https://ep-news.web.cern.ch/what-likelihood-function-and-how-it-used-particle-physics?source=post_page-----65218b2c2b99--------------------------------)
    [## What is the likelihood function, and how is it used in particle physics?'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[最大似然估计 (MLE) 和最大后验估计 (MAP) 之间的区别 - Amir Masoud](https://www.sefidian.com/2022/06/21/difference-between-maximum-likelihood-estimation-mle-and-maximum-a-posteriori-map/?source=post_page-----65218b2c2b99--------------------------------)  '
- en: In statistical data analysis, "likelihood" is a key concept that is closely
    related to, but crucially distinct from…
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '在统计数据分析中，“似然”是一个关键概念，它与…关系密切，但具有重要区别  '
- en: ep-news.web.cern.ch](https://ep-news.web.cern.ch/what-likelihood-function-and-how-it-used-particle-physics?source=post_page-----65218b2c2b99--------------------------------)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[似然函数是什么，它在粒子物理学中的应用](https://ep-news.web.cern.ch/what-likelihood-function-and-how-it-used-particle-physics?source=post_page-----65218b2c2b99--------------------------------)  '
