- en: How to Enhance Your Pandas Code — Don’t Wait No More
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-enhance-your-pandas-code-dont-wait-no-more-5fb89bc1ece9](https://towardsdatascience.com/how-to-enhance-your-pandas-code-dont-wait-no-more-5fb89bc1ece9)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 6 simple changes to improve your pandas’ code performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://polmarin.medium.com/?source=post_page-----5fb89bc1ece9--------------------------------)[![Pol
    Marin](../Images/a4f69a96717d453db9791f27b8f85e86.png)](https://polmarin.medium.com/?source=post_page-----5fb89bc1ece9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5fb89bc1ece9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5fb89bc1ece9--------------------------------)
    [Pol Marin](https://polmarin.medium.com/?source=post_page-----5fb89bc1ece9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5fb89bc1ece9--------------------------------)
    ·9 min read·Apr 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5316eff4e5a960cbcac4b7ab3908130b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Pascal Müller](https://unsplash.com/@millerthachiller?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '**Pandas** is arguably a requirement for any data scientist’s portfolio. Whether
    you use it or not, mastering it will always be an asset.'
  prefs: []
  type: TYPE_NORMAL
- en: However, like with any other thing, the key is not to know how to use *pandas*
    but to actually produce efficient *pandas* code. That’s where not most excel,
    but after this post, you won’t be part of that crew anymore.
  prefs: []
  type: TYPE_NORMAL
- en: But, first things first.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Pandas and Why Does Efficiency Matter?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the official definition, “**pandas** is a fast, powerful, flexible, and
    easy to use open source data analysis and manipulation tool, built on top of the
    [Python](https://www.python.org/) programming language”[1].
  prefs: []
  type: TYPE_NORMAL
- en: It’s really powerful and the default go-to for many of us when we want to play
    with data.
  prefs: []
  type: TYPE_NORMAL
- en: As an additional comment, it loads the datasets into our RAM, so caring about
    optimizing its space-usage sounds like a great idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'But we don’t only want to optimize its memory management, we probably care
    more about making our code’s execution faster. Why? As obvious as it sounds, with
    a faster code we save time. And you know what they say: time is money (especially
    if you have your code running on AWS or GCP).'
  prefs: []
  type: TYPE_NORMAL
- en: Hardware can improve our program’s performance up to a limit, but that’s half
    of the equation. Having an efficient code is the other half, as important as the
    first one. So mastering pandas is almost a necessity for all of us who use it
    daily.
  prefs: []
  type: TYPE_NORMAL
- en: However, do not lose focus. The main goal is to make our codes meet the requirements.
    In my opinion, optimization should be left to when your scripts are unnecessarily
    slow or if it’s taking too much memory. That doesn’t mean you shouldn’t code having
    efficiency in mind but don’t make it your priority if it’s not worth the effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will try to go beyond the logical efficiency claims, but let’s just briefly
    highlight the two basic-most:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use only what you need** — load the columns/rows that will be useful for
    your analysis and get rid of the rest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choose the right variable type** — As said, pandas loads all the data in
    our system’s RAM so choosing wisely the type of variables we’re going to use is
    worth it. We do not want to use floats when all can be done with integers, for
    example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load a dataframe and use the `astype()` function to make sure all columns have
    the desired types.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before we begin, let me share with you the dataframe I’ll be using, along with
    the `%timeit` magic function to put the performance improvements into numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e24140bf311cb2938254ad195bb8fbf.png)'
  prefs: []
  type: TYPE_IMG
- en: Randomly created data for this post — Screenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Avoid Loops At All Costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It won’t always be possible, but getting rid of loops is one of the best optimizations
    you can add to your code.
  prefs: []
  type: TYPE_NORMAL
- en: Pandas is built on top of NumPy so we better take advantage of vectorization
    instead of using loops to perform operations on columns/rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example is what would be an approach for many, trying to subtract the
    value of one column from another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Well, that didn’t take a lot of time, 14 seconds for a ~500k row dataframe.
    Not that bad, but what if I told you that this could be done in less than one
    second?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see a better (and the best) way to do the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: PAM.
  prefs: []
  type: TYPE_NORMAL
- en: The improvement is crazy! We didn’t just simplify our code (it contains three
    fewer rows) but we’ve also saved **a lot** of time here. This is thanks to the
    vectorization feature which does not perform the operation on one data element
    at a time but does them all at the same time. One operation is applied to any
    number of data elements per one moment in time.
  prefs: []
  type: TYPE_NORMAL
- en: This optimization alone provided a 100x improvement in execution time. Worth
    it, right?
  prefs: []
  type: TYPE_NORMAL
- en: 2\. If You Need Loops, Don’t Use Iterrows()
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several alternatives to iterrows, like using `apply()` or using list
    comprehension. However, I want to provide a comparable alternative to iterrows(),
    using a loop either way.
  prefs: []
  type: TYPE_NORMAL
- en: Using `itertuples()` returns each row as a **namedtuple**, which is one of the
    specialized data types within the collections module. They behave like a tuple
    but have fields accessible by their names, with the dot (**.**) lookup.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s repeat the previous experiment but now using itertuples():'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The improvement remains huge here. While iterrows takes like 14 seconds to execute,
    the exact same code using itertuples takes less than a second.
  prefs: []
  type: TYPE_NORMAL
- en: While using vectorization is preferred, we’ve seen that loops can be optimized
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Append vs Concat
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I’ve found myself several times having to concatenate dataframes and, at one
    point in the past, I always asked the same question to myself: Should I use `append()`
    or `concat()`. I ended up defaulting to concat because I found it easier to use.'
  prefs: []
  type: TYPE_NORMAL
- en: But don’t let yourself answer that question like I did, randomly. It ultimately
    depends on how many dataframes you’re willing to concatenate. When it’s just two,
    then don’t bother caring about it, using one or the other won’t probably affect
    much your code’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: However, when trying to concatenate more than two DFs, I’d recommend using concat.
    Why? Because `concat()` is a library’s function that allows us to concatenate
    various at the same time while `append()` is a method of the DataFrame class and
    it only allows for two at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Long story short: with `concat()` you can concatenate multiple DFs at the same
    time whereas to do the same with `append()` you’d need a loop.'
  prefs: []
  type: TYPE_NORMAL
- en: It isn’t only less efficient in time but also in memory usage because each call
    to the append function returns a new DF, which is stored in our system’s memory.
    So, if we want to concatenate N DFs, we’ll create N-1 dataframes until we get
    the final one, all of them using valuable memory space.
  prefs: []
  type: TYPE_NORMAL
- en: Not good.
  prefs: []
  type: TYPE_NORMAL
- en: So use `concat()` when appending multiple DFs.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Don’t Ruin Your GroupBy’s and Merges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both the `groupby()` and the `merge()` are powerful tools for us, I bet you
    use them daily. Taking care of them is, therefore, crucial too.
  prefs: []
  type: TYPE_NORMAL
- en: '**Filter first**. GroupBy’s and merges are expensive operations so make sure
    you filter out the data you don’t care about before grouping/merging.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Try to avoid custom functions on GroupBy**. You’ll be tempted to create your
    own grouping functions when you need something different than the usual sum, mean…
    Chances are, someone has done it before. There might even be built-in functions
    that can take care of those. Use those when possible, because performance slows
    down when you use your own.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Consider using DuckDB**. DuckDB is a strong OLAP system that can really help
    you perform analytical tasks just like these both. I recently wrote a story going
    more in-depth into the topic, so go read it if you’re interested in DuckDB:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[](/forget-about-sqlite-use-duckdb-instead-and-thank-me-later-df76ee9bb777?source=post_page-----5fb89bc1ece9--------------------------------)
    [## Forget about SQLite, Use DuckDB Instead — And Thank Me Later'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to DuckDB and its Python integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/forget-about-sqlite-use-duckdb-instead-and-thank-me-later-df76ee9bb777?source=post_page-----5fb89bc1ece9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Use query() on large datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I have to confess I wasn’t aware of this one but, after testing it out a little
    bit, I thought it was worth being added to this post.
  prefs: []
  type: TYPE_NORMAL
- en: Pandas offers a dataframe method called `query()`, with the purpose of querying
    the columns of a DF with a boolean expression[2]. It takes advantage of the **numexpr**
    parser, which is a fast numerical expression evaluator for NumPy[3].
  prefs: []
  type: TYPE_NORMAL
- en: 'Enough theory, let’s show how `query()` performs with the entire dataframe
    vs the usual filtering. As we have dates in our DF, let’s just randomly choose
    one and try to get the data from that date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Cool huh? It’s almost a **50% improvement** **in performance** on this ~500k-row
    dataframe. What if we did it to a small subset of 2,279 rows? See this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Results point toward the opposite direction here — using `query()` is not desired
    on small dataframes, as it really slows down the performance if we compare it
    to the usual pandas filtering method. That’s why I specified “on large datasets”
    in the title of this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pandas developers have studied this method in depth and they have the analysis
    available for everyone online, but one of the main takeaways is this: “You will
    only see the performance benefits of using the `numexpr` engine with `DataFrame.query()`if
    your frame has more than approximately 100,000 rows.”[4]'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Don’t Default to CSVs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I used CSVs to load and save my data all the time in the past. I’m not against
    CSVs now and I keep using them depending on the task, but I’ve switched to **parquet**
    files when the data is more relevant and bigger.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why parquet? Read the official introduction: “Apache Parquet is an open-source,
    column-oriented data file format designed for efficient data storage and retrieval.
    It provides efficient data compression and encoding schemes with enhanced performance
    to handle complex data in bulk.”[5]'
  prefs: []
  type: TYPE_NORMAL
- en: We can see parquet files can be more efficient than CSVs when it comes to memory
    usage — they are “designed for efficient data storage and provide efficient data
    compression and encoding schemes” — but how does that translate to time?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Well, that’s an **above-40% improvement in time.** The difference here is just
    a few milliseconds, but imagine how can this 40% improvement translate to a bigger
    dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to loading times, let’s see their difference here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We see some improvements here again, but bigger this time — **65%**. That’s
    a lot of time we can potentially save when playing with bigger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a side note: There’s the **Feather format (.ftr)**, which I haven’t personally
    used but I’ve seen really good references around the Internet and there’s impressive
    analysis everywhere showing how it even outperforms parquet, so feel free to give
    it a chance as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a lot of ways to optimize pandas and I just mentioned a few of them,
    the ones that I benefit the most from.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this post was instructive and helpful. Once you know all this information,
    there’s no excuse to not use it!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’d like to support me further, consider subscribing to Medium’s Membership
    through the link you find below: it won’t cost you any extra penny but it’ll help
    me through this process. Thanks a lot!'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@polmarin/membership?source=post_page-----5fb89bc1ece9--------------------------------)
    [## Join Medium with my referral link - Pol Marin'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Pol Marin (and thousands of other writers on Medium).
    Your membership fee directly supports Pol…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@polmarin/membership?source=post_page-----5fb89bc1ece9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [pandas — Python Data Analysis Library](https://pandas.pydata.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [pandas.DataFrame.query — Documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [NumExpr — Github](https://github.com/pydata/numexpr)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [Indexing and selecting data — Pandas](https://pandas.pydata.org/docs/user_guide/indexing.html#performance-of-query)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [Apache Parquet](https://parquet.apache.org/)'
  prefs: []
  type: TYPE_NORMAL
