- en: Implement a Cache Decorator with Time to Live Feature in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/implement-a-cache-decorator-with-ttl-feature-in-python-1d6969b7ca3f](https://towardsdatascience.com/implement-a-cache-decorator-with-ttl-feature-in-python-1d6969b7ca3f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A decorator based on @functools.lru_cache supports cache expiration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://qtalen.medium.com/?source=post_page-----1d6969b7ca3f--------------------------------)[![Peng
    Qian](../Images/9ce9aeb381ec6b017c1ee5d4714937e2.png)](https://qtalen.medium.com/?source=post_page-----1d6969b7ca3f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1d6969b7ca3f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1d6969b7ca3f--------------------------------)
    [Peng Qian](https://qtalen.medium.com/?source=post_page-----1d6969b7ca3f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1d6969b7ca3f--------------------------------)
    ·5 min read·Apr 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f1d5294aa688496b0d210d8cce05051.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Aron Visuals](https://unsplash.com/@aronvisuals?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '**The problem：**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [lru_cache](https://docs.python.org/3/library/functools.html#functools.lru_cache)
    decorator in Python’s functools package provides an implementation based on an
    LRU cache. Using this decorator functions with the same arguments will be significantly
    faster from the second time they are executed.
  prefs: []
  type: TYPE_NORMAL
- en: However, lru_cache cannot support cache expiration. If you want the cache to
    expire after a certain amount of time to update the cache when the function is
    called next time, lru_cache cannot achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to solve:**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**1\. Implement a lru_cache with a TTL feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Therefore, I have implemented a new decorator based on lru_cache. This decorator
    can accept a ttl parameter. This parameter can accept a time in seconds, and when
    this time expires, the next function call will return a new value and refresh
    the cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'For those who need to solve the problem urgently, here is the source code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The usage is very simple, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Test the effectiveness**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use a ttl_cache with an expiration time of 40 seconds. Then let the function
    be executed for 10 rounds, each round for 6 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cfc32a3fee161090bc96fba13db5242.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, when it reaches the 7th round, 6*7=42 (seconds). The cache is
    refreshed, indicating that our ttl_cache is successful, hurray.
  prefs: []
  type: TYPE_NORMAL
- en: '**Some background knowledge:**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**1\. What is LRU cache?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assuming that the size of the cache is fixed, if the cache is full, some content
    needs to be deleted to make room for new content. But the question is, which content
    should be deleted? We certainly hope to delete those caches that are not useful
    and continue to leave useful data in the cache for later use. So, what data do
    we consider “useful” data?
  prefs: []
  type: TYPE_NORMAL
- en: The LRU cache strategy believes that the data that has been used recently should
    be “useful”, and the data that has not been used for a long time should be useless.
    When the capacity is full, those useless data should be deleted first.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. How is the LRU cache implemented?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When implementing LRU, we need to focus on its read-and-write performance.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it is easy to think of using HashMap, which can achieve O(1)
    speed by accessing data according to the key. But the speed of updating the cache
    cannot reach O(1) because it needs to determine which data has been accessed the
    earliest, which requires traversing all the cache to find it.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need a data structure that both sorts by access time and can be
    randomly accessed in constant time.
  prefs: []
  type: TYPE_NORMAL
- en: This can be achieved by using HashMap+Doubly linked list. HashMap guarantees
    O(1) access time for data accessed through the key, and the doubly linked list
    passes through each data in the order of access time. The reason for choosing
    a doubly linked list instead of a singly linked list is that it can modify the
    linked list structure from any node in the middle, without having to traverse
    from the head node.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the figure below, the black part is the structure of HashMap, and
    the red arrow is the forward connection of the doubly linked list. It can be seen
    clearly that the data access order is 1->3->5->6->10\. We only need to change
    the connection order of the linked list after each access to achieve our goal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/429b3aa064e993defe2df425b08fe0eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. How does this help us implement ttl_cache?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We know that the LRU algorithm uses HashMap to implement fast data reading,
    so we can change the parameter by changing the hash key after the expiration time
    to implement cache expiration.
  prefs: []
  type: TYPE_NORMAL
- en: Since the hash key of lru_cache is calculated based on all hashable parameters
    in the decorated function, we only need to add a ttl_hash parameter and change
    the value of this parameter after the expiration time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Code interpretation:**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**1\. A general decorator template**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When starting to write a decorator, I will use a decorator template code, which
    can help me write a decorator more quickly. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. A lazy execution generator code**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we need to generate a hash key every time the function decorated by ttl_cache
    is called, and if it has not exceeded the expiration time, this hash key should
    remain unchanged. If it exceeds the expiration time, this hash key should be different
    from the previously generated one.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, I plan to subtract the time when the code starts running from the
    current time, and then divide the result by the ttl parameter. Finally, the remainder
    is used as this hash key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since I need to save the time when the code starts running and only get the
    latest hash key when necessary, the best way I can think of is to use a [generator
    function](https://wiki.python.org/moin/Generators):'
  prefs: []
  type: TYPE_NORMAL
- en: In this way, I can use the next() function to get the latest hash key when needed.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Use lru_cache to decorate the original function**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we will use lru_cache to decorate the original function, and the new
    function needs to pass in a ttl_hash parameter to generate a new hash after the
    expiration time:'
  prefs: []
  type: TYPE_NORMAL
- en: In the wrapped function, we get a new hash_key every time and return a function
    call with ttl_hash to achieve our goal.
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Don’t forget to copy the properties of the original function**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, don’t forget to copy the module, name, doc, and other attributes of
    the original function into the newly generated function. The easiest way here
    is to use update_wrapper in the functools module:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion:**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are other solutions, such as [expiringdict](https://pypi.org/project/expiringdict/).
  prefs: []
  type: TYPE_NORMAL
- en: But these solutions change the way how we use lru_cache, after all, we just
    want to make lru_cache more powerful to use.
  prefs: []
  type: TYPE_NORMAL
- en: The ttl_cache implemented in this article is thread-safe and can be used in
    a multi-threaded environment.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you can like my implementation of the TTL cache. And welcome everyone
    to comment and provide valuable suggestions for improvement. Thank you for reading.
  prefs: []
  type: TYPE_NORMAL
- en: By [joining Medium](https://medium.com/@qtalen/membership), you’ll have unlimited
    access to all of my posts and those of thousands of other authors. It only costs
    you the price of a cup of coffee, but it’s a great encouragement to me.
  prefs: []
  type: TYPE_NORMAL
- en: 'This article was originally published on: [https://www.dataleadsfuture.com/implement-a-cache-decorator-with-time-to-live-feature-in-python/](https://www.dataleadsfuture.com/implement-a-cache-decorator-with-time-to-live-feature-in-python/)'
  prefs: []
  type: TYPE_NORMAL
