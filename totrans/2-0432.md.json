["```py\nSELECT \n      date(dt) dt\n    , user_id\n    , sum(total_cost) total_cost_usd\n    , registration_date\n  FROM mydatabase.transactions \n  LEFT JOIN mydatabase.users\n  ON users.id = transactions.user_id\n  GROUP BY\n    dt\n    , user_id\n    , registration_date\n;\n```", "```py\n.\n└── stack\n    ├── mysql_connector\n    │   ├── config       # config folder with environment related settings\n    │   ├── populate_database.sql  # sql script to create source tables\n    │   ├── export.sql   # sql script to export data to s3 datalake\n    │   └── app.py       # main application file\n    ├── package          # required libraries\n    │   ├── PyMySQL-1.0.2.dist-info\n    │   └── pymysql\n    ├── requirements.txt # required Python modules\n    └── stack.zip        # Lambda package\n```", "```py\nmkdir stack\ncd stack\nmkdir mysql_connector\n```", "```py\n import os\nimport sys\nimport yaml\nimport logging\nimport pymysql\n\nfrom datetime import datetime\nimport pytz\n\nENV = os.environ['ENV']\nTESTING = os.environ['TESTING']\nLAMBDA_PATH = os.environ['LAMBDA_PATH']\nprint('ENV: {}, Running locally: {}'.format(ENV, TESTING))\n\ndef get_work_dir(testing):\n    if (testing == 'true'):\n        return LAMBDA_PATH\n    else:\n        return '/var/task/' + LAMBDA_PATH\n\ndef get_settings(env, path):\n    if (env == 'staging'):\n        with open(path + \"config/staging.yaml\", \"r\") as f:\n            config = yaml.load(f, Loader=yaml.FullLoader)\n    elif (env == 'live'):\n        with open(path + \"config/production.yaml\", \"r\") as f:\n            config = yaml.load(f, Loader=yaml.FullLoader)\n    elif (env == 'test'):\n        with open(path + \"config/test.yaml\", \"r\") as f:\n            config = yaml.load(f, Loader=yaml.FullLoader)\n    else:\n        print('No config found')\n    return config\n\nwork_dir = get_work_dir(TESTING)\nprint('LAMBDA_PATH: {}'.format(work_dir))\nconfig=get_settings(ENV, work_dir)\nprint(config)\nDATA_S3 = config.get('S3dataLocation') # i.e. datalake.staging.something. Replace it with your unique bucket name.\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# rds settings\nrds_host  = config.get('Mysql')['rds_host'] # i.e. \"mymysqldb.12345.eu-west-1.rds.amazonaws.com\"\nuser_name = \"root\"\npassword = \"AmazingPassword\"\ndb_name = \"mysql\"\n\n# create the database connection outside of the handler to allow connections to be\n# re-used by subsequent function invocations.\ntry:\n    conn = pymysql.connect(host=rds_host, user=user_name, passwd=password, db=db_name, connect_timeout=5)\n\nexcept pymysql.MySQLError as e:\n    logger.error(\"ERROR: Unexpected error: Could not connect to MySQL instance.\")\n    logger.error(e)\n    sys.exit()\n\nlogger.info(\"SUCCESS: Connection to RDS MySQL instance succeeded\")\n\ndef lambda_handler(event, context):\n    processed = 0\n    print(\"\")\n    try:\n        _populate_db()\n        _export_to_s3()\n    except Exception as e:\n        print(e)\n    message = 'Successfully populated the database and created an export job.'\n    return {\n        'statusCode': 200,\n        'body': { 'lambdaResult': message }\n    }\n\n# Helpers:\n\ndef _now():\n    return datetime.utcnow().replace(tzinfo=pytz.utc).strftime('%Y-%m-%dT%H:%M:%S.%f')\n\ndef _populate_db():\n    try:\n        # Generate data and populate database:\n        fd = open(work_dir + '/populate_database.sql', 'r')\n        sqlFile = fd.read()\n        fd.close()\n        sqlCommands = sqlFile.split(';')\n        # Execute every command from the input file\n        for command in sqlCommands:\n            try:\n                with conn.cursor() as cur:\n                    cur.execute(command)\n                    print('---')\n                    print(command)\n            except Exception as e:\n                print(e)\n\n    except Exception as e:\n        print(e)\n\ndef _export_to_s3():\n    try:\n        # Generate data and populate database:\n        fd = open(work_dir + '/export.sql', 'r')\n        sqlFile = fd.read()\n        fd.close()\n        sqlCommands = sqlFile.split(';')\n        # Execute every command from the input file\n        for command in sqlCommands:\n            try:\n                with conn.cursor() as cur:\n                    cur.execute(command.replace(\"{{DATA_S3}}\", DATA_S3))\n                    print('---')\n                    print(command)\n            except Exception as e:\n                print(e)\n\n    except Exception as e:\n        print(e)\n```", "```py\n# Package Lambda code:\nbase=${PWD##*/}\nzp=$base\".zip\" # This will return stack.zip if you are in stack folder.\necho $zp\n\nrm -f $zp # remove old package if exists\n\npip install --target ./package pymysql \n\ncd package\nzip -r ../${base}.zip .\n\ncd $OLDPWD\nzip -r $zp ./mysql_connector\n```", "```py\n# Deploy packaged Lambda using AWS CLI:\naws \\\nlambda create-function \\\n--function-name mysql-lambda \\\n--zip-file fileb://stack.zip \\\n--handler <path-to-your-lambda-handler>/app.lambda_handler \\\n--runtime python3.12 \\\n--role arn:aws:iam::<your-aws-account-id>:role/my-lambda-role\n\n# # If already deployed then use this to update:\n# aws --profile mds lambda update-function-code \\\n# --function-name mysql-lambda \\\n# --zip-file fileb://stack.zip;\n```", "```py\n-- Example query\n-- Replace table names and S3 bucket location\nSELECT * FROM myschema.transactions INTO OUTFILE S3 's3://<YOUR_S3_BUCKET>/data/myschema/transactions/transactions.scv' FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' OVERWRITE ON;\n```", "```py\naws \\\ncloudformation deploy \\\n--template-file cfn_mysql.yaml \\\n--stack-name MySQLDB \\\n--capabilities CAPABILITY_IAM\n```", "```py\nAWSTemplateFormatVersion: 2010-09-09\nDescription: >-\n  This\n  template creates an Amazon Relational Database Service database instance. You\n  will be billed for the AWS resources used if you create a stack from this\n  template.\nParameters:\n  DBUser:\n    Default: root\n    NoEcho: 'true'\n    Description: The database admin account username\n    Type: String\n    MinLength: '1'\n    MaxLength: '16'\n    AllowedPattern: '[a-zA-Z][a-zA-Z0-9]*'\n    ConstraintDescription: must begin with a letter and contain only alphanumeric characters.\n  DBPassword:\n    Default: AmazingPassword\n    NoEcho: 'true'\n    Description: The database admin account password\n    Type: String\n    MinLength: '8'\n    MaxLength: '41'\n    AllowedPattern: '[a-zA-Z0-9]*'\n    ConstraintDescription: must contain only alphanumeric characters.\nResources:\n### Role to output into s3\n  MySQLRDSExecutionRole:\n    Type: \"AWS::IAM::Role\"\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: \"Allow\"\n            Principal:\n              Service:\n                - !Sub rds.amazonaws.com\n            Action: \"sts:AssumeRole\"\n      Path: \"/\"\n      Policies:\n        - PolicyName: MySQLRDSExecutionPolicy\n          PolicyDocument:\n            Version: \"2012-10-17\"\n            Statement:\n              - Effect: Allow\n                Action:\n                  - \"s3:*\"\n                Resource: \"*\"\n###\n\n  RDSCluster: \n    Properties: \n      DBClusterParameterGroupName: \n        Ref: RDSDBClusterParameterGroup\n      Engine: aurora-mysql\n      MasterUserPassword: \n        Ref: DBPassword\n      MasterUsername: \n        Ref: DBUser\n\n### Add a role to export to s3\n      AssociatedRoles:\n        - RoleArn: !GetAtt [ MySQLRDSExecutionRole, Arn ]\n###\n    Type: \"AWS::RDS::DBCluster\"\n  RDSDBClusterParameterGroup: \n    Properties: \n      Description: \"CloudFormation Sample Aurora Cluster Parameter Group\"\n      Family: aurora-mysql5.7\n      Parameters: \n        time_zone: US/Eastern\n        ### Add a role to export to s3\n        aws_default_s3_role: !GetAtt [ MySQLRDSExecutionRole, Arn ]\n        ###\n    Type: \"AWS::RDS::DBClusterParameterGroup\"\n  RDSDBInstance1:\n    Type: 'AWS::RDS::DBInstance'\n    Properties:\n      DBClusterIdentifier: \n        Ref: RDSCluster\n      # AllocatedStorage: '20'\n      DBInstanceClass: db.t2.small\n      # Engine: aurora\n      Engine: aurora-mysql\n      PubliclyAccessible: \"true\"\n      DBInstanceIdentifier: MyMySQLDB\n  RDSDBParameterGroup:\n    Type: 'AWS::RDS::DBParameterGroup'\n    Properties:\n      Description: CloudFormation Sample Aurora Parameter Group\n      # Family: aurora5.6\n      Family: aurora-mysql5.7\n      Parameters:\n        sql_mode: IGNORE_SPACE\n        max_allowed_packet: 1024\n        innodb_buffer_pool_size: '{DBInstanceClassMemory*3/4}'\n# Aurora instances need to be associated with a AWS::RDS::DBCluster via DBClusterIdentifier without the cluster you get these generic errors \n```", "```py\nCREATE TABLE IF NOT EXISTS\n  myschema.users AS\nSELECT\n  1 AS id,\n  CURRENT_DATE() AS registration_date\nUNION ALL\nSELECT\n  2 AS id,\n  DATE_SUB(CURRENT_DATE(), INTERVAL 1 day) AS registration_date;\n\nCREATE TABLE IF NOT EXISTS\n  myschema.transactions AS\nSELECT\n  1 AS transaction_id,\n  1 AS user_id,\n  10.99 AS total_cost,\n  CURRENT_DATE() AS dt\nUNION ALL\nSELECT\n  2 AS transaction_id,\n  2 AS user_id,\n  4.99 AS total_cost,\n  CURRENT_DATE() AS dt\nUNION ALL\nSELECT\n  3 AS transaction_id,\n  2 AS user_id,\n  4.99 AS total_cost,\n  DATE_SUB(CURRENT_DATE(), INTERVAL 3 day) AS dt\nUNION ALL\nSELECT\n  4 AS transaction_id,\n  1 AS user_id,\n  4.99 AS total_cost,\n  DATE_SUB(CURRENT_DATE(), INTERVAL 3 day) AS dt\nUNION ALL\nSELECT\n  5 AS transaction_id,\n  1 AS user_id,\n  5.99 AS total_cost,\n  DATE_SUB(CURRENT_DATE(), INTERVAL 2 day) AS dt\nUNION ALL\nSELECT\n  6 AS transaction_id,\n  1 AS user_id,\n  15.99 AS total_cost,\n  DATE_SUB(CURRENT_DATE(), INTERVAL 1 day) AS dt\nUNION ALL\nSELECT\n  7 AS transaction_id,\n  1 AS user_id,\n  55.99 AS total_cost,\n  DATE_SUB(CURRENT_DATE(), INTERVAL 4 day) AS dt\n;\n```", "```py\nCREATE EXTERNAL TABLE mydatabase.users (\n    id                bigint\n  , registration_date string\n) \nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ',' \nSTORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' \nLOCATION  's3://<YOUR_S3_BUCKET>/data/myschema/users/' TBLPROPERTIES (  'skip.header.line.count'='0')\n;\nselect * from mydatabase.users;\n\nCREATE EXTERNAL TABLE mydatabase.transactions (\n    transaction_id    bigint\n  , user_id           bigint\n  , total_cost        double\n  , dt                string\n) \nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ',' \nSTORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' \nLOCATION  's3://<YOUR_S3_BUCKET>/data/myschema/transactions/' TBLPROPERTIES (  'skip.header.line.count'='0')\n;\nselect * from mydatabase.transactions;\n\nCREATE TABLE IF NOT EXISTS mydatabase.user_transactions (\n  dt date,\n  user_id int,\n  total_cost_usd float,\n  registration_date string\n) \nPARTITIONED BY (dt)\nLOCATION 's3://<YOUR_S3_BUCKET>/data/myschema/optimized-data-iceberg-parquet/' \nTBLPROPERTIES (\n  'table_type'='ICEBERG',\n  'format'='parquet',\n  'write_target_data_file_size_bytes'='536870912',\n  'optimize_rewrite_delete_file_threshold'='10'\n)\n;\n\nMERGE INTO mydatabase.user_transactions  as ut\nUSING (\n  SELECT \n      date(dt) dt\n    , user_id\n    , sum(total_cost) total_cost_usd\n    , registration_date\n  FROM mydatabase.transactions \n  LEFT JOIN mydatabase.users\n  ON users.id = transactions.user_id\n  GROUP BY\n    dt\n    , user_id\n    , registration_date\n) as ut2\nON (ut.dt = ut2.dt and ut.user_id = ut2.user_id)\nWHEN MATCHED\n    THEN UPDATE\n        SET total_cost_usd = ut2.total_cost_usd, registration_date = ut2.registration_date\nWHEN NOT MATCHED \nTHEN INSERT (\n dt\n,user_id\n,total_cost_usd\n,registration_date\n)\n  VALUES (\n ut2.dt\n,ut2.user_id\n,ut2.total_cost_usd\n,ut2.registration_date\n)\n;\n```", "```py\n#!/usr/bin/env bash\n# chmod +x ./deploy-staging.sh\n# Run ./deploy-staging.sh\nPROFILE=<YOUR_AWS_PROFILE>\nSTACK_NAME=BatchETLpipeline\nLAMBDA_BUCKET=<YOUR_S3_BUCKET> # Replace with unique bucket name in your account\nAPP_FOLDER=mysql_connector\n\ndate\n\nTIME=`date +\"%Y%m%d%H%M%S\"`\n\nbase=${PWD##*/}\nzp=$base\".zip\"\necho $zp\n\nrm -f $zp\n\npip install --target ./package -r requirements.txt\n# boto3 is not required unless we want a specific version for Lambda\n# requirements.txt:\n# pymysql==1.0.3\n# requests==2.28.1\n# pytz==2023.3\n# pyyaml==6.0\n\ncd package\nzip -r ../${base}.zip .\n\ncd $OLDPWD\n\nzip -r $zp \"./${APP_FOLDER}\" -x __pycache__ \n\n# Check if Lambda bucket exists:\nLAMBDA_BUCKET_EXISTS=$(aws --profile ${PROFILE} s3 ls ${LAMBDA_BUCKET} --output text)\n#  If NOT:\nif [[ $? -eq 254 ]]; then\n    # create a bucket to keep Lambdas packaged files:\n    echo  \"Creating Lambda code bucket ${LAMBDA_BUCKET} \"\n    CREATE_BUCKET=$(aws --profile ${PROFILE} s3 mb s3://${LAMBDA_BUCKET} --output text)\n    echo ${CREATE_BUCKET}\nfi\n\n# Upload the package to S3:\naws --profile $PROFILE s3 cp ./${base}.zip s3://${LAMBDA_BUCKET}/${APP_FOLDER}/${base}${TIME}.zip\n\naws --profile $PROFILE \\\ncloudformation deploy \\\n--template-file stack.yaml \\\n--stack-name $STACK_NAME \\\n--capabilities CAPABILITY_IAM \\\n--parameter-overrides \\\n\"StackPackageS3Key\"=\"${APP_FOLDER}/${base}${TIME}.zip\" \\\n\"AppFolder\"=$APP_FOLDER \\\n\"S3LambdaBucket\"=$LAMBDA_BUCKET \\\n\"Environment\"=\"staging\" \\\n\"Testing\"=\"false\"\n```", "```py\nAWSTemplateFormatVersion: '2010-09-09'\nDescription: An example template for a Step Functions state machine.\nParameters:\n\n  DataLocation:\n    Description: Data lake bucket with source data files.\n    Type: String\n    Default: s3://your.datalake.aws/data/\n  AthenaResultsLocation:\n    Description: S3 location for Athena query results.\n    Type: String\n    Default: s3://your.datalake.aws/athena/\n  AthenaDatabaseName:\n    Description: Athena schema names for ETL pipeline.\n    Type: String\n    Default: mydatabase\n  S3LambdaBucket:\n    Description: Use this bucket to keep your Lambda package.\n    Type: String\n    Default: your.datalake.aws\n  StackPackageS3Key:\n    Type: String\n    Default: mysql_connector/stack.zip\n  ServiceName:\n    Type: String\n    Default: mysql-connector\n  Testing:\n    Type: String\n    Default: 'false'\n    AllowedValues: ['true','false']\n  Environment:\n    Type: String\n    Default: 'staging'\n    AllowedValues: ['staging','live','test']\n  AppFolder:\n    Description: app.py file location inside the package, i.e. mysql_connector when ./stack/mysql_connector/app.py.\n    Type: String\n    Default: mysql_connector\n\nResources:\n  LambdaExecutionRole:\n    Type: \"AWS::IAM::Role\"\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service: lambda.amazonaws.com\n            Action: \"sts:AssumeRole\"\n\n  MyLambdaFunction:\n    Type: \"AWS::Lambda::Function\"\n    Properties:\n      Handler: \"index.handler\"\n      Role: !GetAtt [ LambdaExecutionRole, Arn ]\n      Code:\n        ZipFile: |\n          exports.handler = (event, context, callback) => {\n              callback(null, \"Hello World!\");\n          };\n      Runtime: \"nodejs18.x\"\n      Timeout: \"25\"\n\n### MySQL Connector Lmabda ###\n  MySqlConnectorLambda:\n    Type: AWS::Lambda::Function\n    DeletionPolicy: Delete\n    DependsOn: LambdaPolicy\n    Properties:\n      FunctionName: !Join ['-', [!Ref ServiceName, !Ref Environment] ]\n      Handler: !Sub '${AppFolder}/app.lambda_handler'\n      Description: Microservice that extracts data from RDS.\n      Environment:\n        Variables:\n          DEBUG: true\n          LAMBDA_PATH: !Sub '${AppFolder}/'\n          TESTING: !Ref Testing\n          ENV: !Ref Environment\n      Role: !GetAtt LambdaRole.Arn\n      Code:\n        S3Bucket: !Sub '${S3LambdaBucket}'\n        S3Key:\n          Ref: StackPackageS3Key\n      Runtime: python3.8\n      Timeout: 360\n      MemorySize: 128\n      Tags:\n        -\n          Key: Service\n          Value: Datalake\n\n  StatesExecutionRole:\n    Type: \"AWS::IAM::Role\"\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: \"Allow\"\n            Principal:\n              Service:\n                - !Sub states.${AWS::Region}.amazonaws.com\n            Action: \"sts:AssumeRole\"\n      Path: \"/\"\n      Policies:\n        - PolicyName: StatesExecutionPolicy\n          PolicyDocument:\n            Version: \"2012-10-17\"\n            Statement:\n              - Effect: Allow\n                Action:\n                  - \"lambda:InvokeFunction\"\n                Resource: \"*\"\n              - Effect: Allow\n                Action:\n                  - \"athena:*\"\n\n                Resource: \"*\"\n              - Effect: Allow\n                Action:\n                  - \"s3:*\"\n                Resource: \"*\"\n              - Effect: Allow\n                Action:\n                  - \"glue:*\"\n                Resource: \"*\"\n\n  MyStateMachine:\n    Type: AWS::StepFunctions::StateMachine\n    Properties:\n      # StateMachineName: ETL-StateMachine\n      StateMachineName: !Join ['-', ['ETL-StateMachine', !Ref ServiceName, !Ref Environment] ]\n      DefinitionString:\n        !Sub\n          - |-\n            {\n              \"Comment\": \"A Hello World example using an AWS Lambda function\",\n              \"StartAt\": \"HelloWorld\",\n              \"States\": {\n                \"HelloWorld\": {\n                  \"Type\": \"Task\",\n                  \"Resource\": \"${lambdaArn}\",\n                  \"Next\": \"Extract from MySQL\"\n                },\n                \"Extract from MySQL\": {\n                  \"Resource\": \"${MySQLLambdaArn}\",\n                  \"Type\": \"Task\",\n                  \"Next\": \"Create Athena DB\"\n                },\n                \"Create Athena DB\": {\n                  \"Resource\": \"arn:aws:states:::athena:startQueryExecution.sync\",\n                  \"Parameters\": {\n                    \"QueryString\": \"CREATE DATABASE if not exists ${AthenaDatabaseName}\",\n                    \"WorkGroup\": \"primary\",\n                    \"ResultConfiguration\": {\n                      \"OutputLocation\": \"${AthenaResultsLocation}\"\n                    }\n                  },\n                  \"Type\": \"Task\",\n                  \"Next\": \"Show tables\"\n                },\n                \"Show tables\": {\n                  \"Resource\": \"arn:aws:states:::athena:startQueryExecution.sync\",\n                  \"Parameters\": {\n                    \"QueryString\": \"show tables in ${AthenaDatabaseName}\",\n                    \"WorkGroup\": \"primary\",\n                    \"ResultConfiguration\": {\n                      \"OutputLocation\": \"${AthenaResultsLocation}\"\n                    }\n                  },\n                  \"Type\": \"Task\",\n                  \"Next\": \"Get Show tables query results\"\n                },\n                \"Get Show tables query results\": {\n                  \"Resource\": \"arn:aws:states:::athena:getQueryResults\",\n                  \"Parameters\": {\n                    \"QueryExecutionId.$\": \"$.QueryExecution.QueryExecutionId\"\n                  },\n                  \"Type\": \"Task\",\n                  \"Next\": \"Decide what next\"\n                },\n                \"Decide what next\": {\n                  \"Comment\": \"Based on the input table name, a choice is made for moving to the next step.\",\n                  \"Type\": \"Choice\",\n                  \"Choices\": [\n                    {\n                      \"Not\": {\n                        \"Variable\": \"$.ResultSet.Rows[0].Data[0].VarCharValue\",\n                        \"IsPresent\": true\n                      },\n                      \"Next\": \"Create users table (external)\"\n                    },\n                    {\n                      \"Variable\": \"$.ResultSet.Rows[0].Data[0].VarCharValue\",\n                      \"IsPresent\": true,\n                      \"Next\": \"Check All Tables\"\n                    }\n                  ],\n                  \"Default\": \"Check All Tables\"\n                },\n                \"Create users table (external)\": {\n                  \"Resource\": \"arn:aws:states:::athena:startQueryExecution.sync\",\n                  \"Parameters\": {\n                    \"QueryString\": \"CREATE EXTERNAL TABLE ${AthenaDatabaseName}.users ( id                bigint , registration_date string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION  's3://datalake.staging.liveproject/data/myschema/users/' TBLPROPERTIES (  'skip.header.line.count'='0') ;\",\n                    \"WorkGroup\": \"primary\",\n                    \"ResultConfiguration\": {\n                      \"OutputLocation\": \"${AthenaResultsLocation}\"\n                    }\n                  },\n                  \"Type\": \"Task\",\n                  \"Next\": \"Create transactions table (external)\"\n                },\n                \"Create transactions table (external)\": {\n                  \"Resource\": \"arn:aws:states:::athena:startQueryExecution.sync\",\n                  \"Parameters\": {\n                    \"QueryString\": \"CREATE EXTERNAL TABLE ${AthenaDatabaseName}.transactions ( transaction_id    bigint , user_id           bigint , total_cost        double , dt                string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION  's3://datalake.staging.liveproject/data/myschema/transactions/' TBLPROPERTIES (  'skip.header.line.count'='0') ;\",\n                    \"WorkGroup\": \"primary\",\n                    \"ResultConfiguration\": {\n                      \"OutputLocation\": \"${AthenaResultsLocation}\"\n                    }\n                  },\n                  \"Type\": \"Task\",\n                  \"Next\": \"Create report table (parquet)\"\n                },\n                \"Create report table (parquet)\": {\n                  \"Resource\": \"arn:aws:states:::athena:startQueryExecution.sync\",\n                  \"Parameters\": {\n                    \"QueryString\": \"CREATE TABLE IF NOT EXISTS ${AthenaDatabaseName}.user_transactions ( dt date, user_id int, total_cost_usd float, registration_date string ) PARTITIONED BY (dt) LOCATION 's3://datalake.staging.liveproject/data/myschema/optimized-data-iceberg-parquet/' TBLPROPERTIES ( 'table_type'='ICEBERG', 'format'='parquet', 'write_target_data_file_size_bytes'='536870912', 'optimize_rewrite_delete_file_threshold'='10' ) ;\",\n                    \"WorkGroup\": \"primary\",\n                    \"ResultConfiguration\": {\n                      \"OutputLocation\": \"${AthenaResultsLocation}\"\n                    }\n                  },\n                  \"Type\": \"Task\",\n                  \"End\": true\n                },\n                \"Check All Tables\": {\n                  \"Type\": \"Map\",\n                  \"InputPath\": \"$.ResultSet\",\n                  \"ItemsPath\": \"$.Rows\",\n                  \"MaxConcurrency\": 0,\n                  \"Iterator\": {\n                    \"StartAt\": \"CheckTable\",\n                    \"States\": {\n                      \"CheckTable\": {\n                        \"Type\": \"Choice\",\n                        \"Choices\": [\n                          {\n                            \"Variable\": \"$.Data[0].VarCharValue\",\n                            \"StringMatches\": \"*users\",\n                            \"Next\": \"passstep\"\n                          },\n                          {\n                            \"Variable\": \"$.Data[0].VarCharValue\",\n                            \"StringMatches\": \"*user_transactions\",\n                            \"Next\": \"Insert New parquet Data\"\n                          }\n                        ],\n                        \"Default\": \"passstep\"\n                      },\n                      \"Insert New parquet Data\": {\n                        \"Resource\": \"arn:aws:states:::athena:startQueryExecution.sync\",\n                        \"Parameters\": {\n                          \"QueryString\": \"MERGE INTO ${AthenaDatabaseName}.user_transactions  as ut USING ( SELECT date(dt) dt , user_id , sum(total_cost) total_cost_usd , registration_date FROM ${AthenaDatabaseName}.transactions LEFT JOIN ${AthenaDatabaseName}.users ON users.id = transactions.user_id GROUP BY dt , user_id , registration_date ) as ut2 ON (ut.dt = ut2.dt and ut.user_id = ut2.user_id) WHEN MATCHED THEN UPDATE SET total_cost_usd = ut2.total_cost_usd, registration_date = ut2.registration_date WHEN NOT MATCHED THEN INSERT ( dt ,user_id ,total_cost_usd ,registration_date ) VALUES ( ut2.dt ,ut2.user_id ,ut2.total_cost_usd ,ut2.registration_date ) ;\",\n                          \"WorkGroup\": \"primary\",\n                          \"ResultConfiguration\": {\n                            \"OutputLocation\": \"${AthenaResultsLocation}\"\n                          }\n                        },\n                        \"Type\": \"Task\",\n                        \"End\": true\n                      },\n                      \"passstep\": {\n                        \"Type\": \"Pass\",\n                        \"Result\": \"NA\",\n                        \"End\": true\n                      }\n                    }\n                  },\n                  \"End\": true\n                }\n              }\n            }\n          - {\n            lambdaArn: !GetAtt [ MyLambdaFunction, Arn ],\n            MySQLLambdaArn: !GetAtt [ MySqlConnectorLambda, Arn ],\n            AthenaResultsLocation: !Ref AthenaResultsLocation,\n            AthenaDatabaseName: !Ref AthenaDatabaseName\n          }\n      RoleArn: !GetAtt [ StatesExecutionRole, Arn ]\n      Tags:\n        -\n          Key: \"keyname1\"\n          Value: \"value1\"\n        -\n          Key: \"keyname2\"\n          Value: \"value2\"\n\n# IAM role for mysql-data-connector Lambda:\n  LambdaRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          -\n            Effect: Allow\n            Principal:\n              Service:\n                - \"lambda.amazonaws.com\"\n            Action:\n              - \"sts:AssumeRole\"\n\n  LambdaPolicy:\n    Type: AWS::IAM::Policy\n    DependsOn: LambdaRole\n    Properties:\n      Roles:\n        - !Ref LambdaRole\n      PolicyName: !Join ['-', [!Ref ServiceName, !Ref Environment, 'lambda-policy']] \n      PolicyDocument:\n        {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Effect\": \"Allow\",\n                    \"Action\": [\n                        \"logs:CreateLogGroup\",\n                        \"logs:CreateLogStream\",\n                        \"logs:PutLogEvents\"\n                    ],\n                    \"Resource\": \"*\"\n                }\n            ]\n        }\n```"]