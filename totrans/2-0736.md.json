["```py\nimport torch \nimport torch.nn as nn\nfrom torchdiffeq import odeint as odeint\nimport pylab as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import Callable, List, Tuple, Union, Optional\nfrom pathlib import Path\n```", "```py\nclass VDP(nn.Module):\n    \"\"\" \n    Define the Van der Pol oscillator as a PyTorch module.\n    \"\"\"\n    def __init__(self, \n                 mu: float, # Stiffness parameter of the VDP oscillator\n                 ):\n        super().__init__() \n        self.mu = torch.nn.Parameter(torch.tensor(mu)) # make mu a learnable parameter\n\n    def forward(self, \n                t: float, # time index\n                state: torch.TensorType, # state of the system first dimension is the batch size\n                ) -> torch.Tensor: # return the derivative of the state\n        \"\"\" \n            Define the right hand side of the VDP oscillator.\n        \"\"\"\n        x = state[..., 0] # first dimension is the batch size\n        y = state[..., 1]\n        dX = self.mu*(x-1/3*x**3 - y)\n        dY = 1/self.mu*x\n        # trick to make sure our return value has the same shape as the input\n        dfunc = torch.zeros_like(state) \n        dfunc[..., 0] = dX\n        dfunc[..., 1] = dY\n        return dfunc\n\n    def __repr__(self):\n        \"\"\"Print the parameters of the model.\"\"\"\n        return f\" mu: {self.mu.item()}\"\n```", "```py\nvdp_model = VDP(mu=0.5)\n\n# Create a time vector, this is the time axis of the ODE\nts = torch.linspace(0,30.0,1000)\n# Create a batch of initial conditions \nbatch_size = 30\n# Creates some random initial conditions\ninitial_conditions = torch.tensor([0.01, 0.01]) + 0.2*torch.randn((batch_size,2))\n\n# Solve the ODE, odeint comes from torchdiffeq\nsol = odeint(vdp_model, initial_conditions, ts, method='dopri5').detach().numpy()\nplt.plot(ts, sol[:,:,0], lw=0.5);\nplt.title(\"Time series of the VDP oscillator\");\nplt.xlabel(\"time\");\nplt.ylabel(\"x\");p\n```", "```py\n# Check the solution\nplt.plot(sol[:,:,0], sol[:,:,1], lw=0.5);\nplt.title(\"Phase plot of the VDP oscillator\");\nplt.xlabel(\"x\");\nplt.ylabel(\"y\");\n```", "```py\nsol.shape\n(1000, 30, 2)\n```", "```py\nclass LotkaVolterra(nn.Module):\n    \"\"\" \n     The Lotka-Volterra equations are a pair of first-order, non-linear, differential equations\n     describing the dynamics of two species interacting in a predator-prey relationship.\n    \"\"\"\n    def __init__(self,\n                 alpha: float = 1.5, # The alpha parameter of the Lotka-Volterra system\n                 beta: float = 1.0, # The beta parameter of the Lotka-Volterra system\n                 delta: float = 3.0, # The delta parameter of the Lotka-Volterra system\n                 gamma: float = 1.0 # The gamma parameter of the Lotka-Volterra system\n                 ) -> None:\n        super().__init__()\n        self.model_params = torch.nn.Parameter(torch.tensor([alpha, beta, delta, gamma]))\n\n    def forward(self, t, state):\n        x = state[...,0]      #variables are part of vector array u \n        y = state[...,1]\n        sol = torch.zeros_like(state)\n\n        #coefficients are part of tensor model_params\n        alpha, beta, delta, gamma = self.model_params    \n        sol[...,0] = alpha*x - beta*x*y\n        sol[...,1] = -delta*y + gamma*x*y\n        return sol\n\n    def __repr__(self):\n        return f\" alpha: {self.model_params[0].item()}, \\\n            beta: {self.model_params[1].item()}, \\\n                delta: {self.model_params[2].item()}, \\\n                    gamma: {self.model_params[3].item()}\"\n```", "```py\nlv_model = LotkaVolterra() #use default parameters\nts = torch.linspace(0,30.0,1000) \nbatch_size = 30\n# Create a batch of initial conditions (batch_dim, state_dim) as small perturbations around one value\ninitial_conditions = torch.tensor([[3,3]]) + 0.50*torch.randn((batch_size,2))\nsol = odeint(lv_model, initial_conditions, ts, method='dopri5').detach().numpy()\n# Check the solution\n\nplt.plot(ts, sol[:,:,0], lw=0.5);\nplt.title(\"Time series of the Lotka-Volterra system\");\nplt.xlabel(\"time\");\nplt.ylabel(\"x\");\n```", "```py\nclass Lorenz(nn.Module):\n    \"\"\" \n    Define the Lorenz system as a PyTorch module.\n    \"\"\"\n    def __init__(self, \n                 sigma: float =10.0, # The sigma parameter of the Lorenz system\n                 rho: float=28.0, # The rho parameter of the Lorenz system\n                beta: float=8.0/3, # The beta parameter of the Lorenz system\n                ):\n        super().__init__() \n        self.model_params = torch.nn.Parameter(torch.tensor([sigma, rho, beta]))\n\n    def forward(self, t, state):\n        x = state[...,0]      #variables are part of vector array u \n        y = state[...,1]\n        z = state[...,2]\n        sol = torch.zeros_like(state)\n\n        sigma, rho, beta = self.model_params    #coefficients are part of vector array p\n        sol[...,0] = sigma*(y-x)\n        sol[...,1] = x*(rho-z) - y\n        sol[...,2] = x*y - beta*z\n        return sol\n\n    def __repr__(self):\n        return f\" sigma: {self.model_params[0].item()}, \\\n            rho: {self.model_params[1].item()}, \\\n                beta: {self.model_params[2].item()}\"\n```", "```py\nlorenz_model = Lorenz()\nts = torch.linspace(0,50.0,3000)\nbatch_size = 30\n# Create a batch of initial conditions (batch_dim, state_dim) as small perturbations around one value\ninitial_conditions = torch.tensor([[1.0,0.0,0.0]]) + 0.10*torch.randn((batch_size,3))\nsol = odeint(lorenz_model, initial_conditions, ts, method='dopri5').detach().numpy()\n\n# Check the solution\nplt.plot(ts[:2000], sol[:2000,:,0], lw=0.5);\nplt.title(\"Time series of the Lorenz system\");\nplt.xlabel(\"time\");\nplt.ylabel(\"x\");\n```", "```py\nclass SimODEData(Dataset):\n    \"\"\" \n        A very simple dataset class for simulating ODEs\n    \"\"\"\n    def __init__(self,\n                 ts: List[torch.Tensor], # List of time points as tensors\n                 values: List[torch.Tensor], # List of dynamical state values (tensor) at each time point \n                 true_model: Union[torch.nn.Module,None] = None,\n                 ) -> None:\n        self.ts = ts \n        self.values = values \n        self.true_model = true_model\n\n    def __len__(self) -> int:\n        return len(self.ts)\n\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        return self.ts[index], self.values[index]\n```", "```py\ndef create_sim_dataset(model: nn.Module, # model to simulate from\n                       ts: torch.Tensor, # Time points to simulate for\n                       num_samples: int = 10, # Number of samples to generate\n                       sigma_noise: float = 0.1, # Noise level to add to the data\n                       initial_conditions_default: torch.Tensor = torch.tensor([0.0, 0.0]), # Default initial conditions\n                       sigma_initial_conditions: float = 0.1, # Noise level to add to the initial conditions\n                       ) -> SimODEData:\n    ts_list = [] \n    states_list = [] \n    dim = initial_conditions_default.shape[0]\n    for i in range(num_samples):\n        x0 = sigma_initial_conditions * torch.randn((1,dim)).detach() + initial_conditions_default\n        ys = odeint(model, x0, ts).squeeze(1).detach() \n        ys += sigma_noise*torch.randn_like(ys)\n        ys[0,:] = x0 # Set the first value to the initial condition\n        ts_list.append(ts)\n        states_list.append(ys)\n    return SimODEData(ts_list, states_list, true_model=model)\n```", "```py\ndef train(model: torch.nn.Module, # Model to train\n          data: SimODEData, # Data to train on\n          lr: float = 1e-2, # learning rate for the Adam optimizer\n          epochs: int = 10, # Number of epochs to train for\n          batch_size: int = 5, # Batch size for training\n          method = 'rk4', # ODE solver to use\n          step_size: float = 0.10, # for fixed diffeq solver set the step size\n          show_every: int = 10, # How often to print the loss function message\n          save_plots_every: Union[int,None] = None, # save a plot of the fit, to disable make this None\n          model_name: str = \"\", #string for the model, used to reference the saved plots \n          *args: tuple, \n          **kwargs: dict\n          ):\n\n    # Create a data loader to iterate over the data. This takes in our dataset and returns batches of data\n    trainloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n    # Choose an optimizer. Adam is a good default choice as a fancy gradient descent\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    # Create a loss function this computes the error between the predicted and true values\n    criterion = torch.nn.MSELoss() \n\n    for epoch in range(epochs):\n        running_loss = 0.0 \n        for batchdata in trainloader:\n            optimizer.zero_grad() # reset gradients, famous gotcha in a pytorch training loop\n            ts, states = batchdata # unpack the data \n            initial_state = states[:,0,:] # grab the initial state\n            # Make the prediction and then flip the dimensions to be (batch, state_dim, time)\n            # Pytorch expects the batch dimension to be first\n            pred = odeint(model, \n                          initial_state, \n                          ts[0], \n                          method=method, \n                          options={'step_size': step_size}).transpose(0,1) \n            # Compute the loss\n            loss = criterion(pred, states)\n            # compute gradients\n            loss.backward() \n            # update parameters\n            optimizer.step() \n            running_loss += loss.item() # record loss\n        if epoch % show_every == 0:\n            print(f\"Loss at {epoch}: {running_loss}\")\n        # Use this to save plots of the fit every save_plots_every epochs\n        if save_plots_every is not None and epoch % save_plots_every == 0:\n            with torch.no_grad():\n                fig, ax = plot_time_series(data.true_model, model, data[0])\n                ax.set_title(f\"Epoch: {epoch}\")\n                fig.savefig(f\"./tmp_plots/{epoch}_{model_name}_fit_plot\")\n                plt.close()\n```", "```py\ntrue_mu = 0.30\nmodel_sim = VDP(mu=true_mu)\nts_data = torch.linspace(0.0,10.0,10) \ndata_vdp = create_sim_dataset(model_sim, \n                              ts = ts_data, \n                              num_samples=10, \n                              sigma_noise=0.01)\n```", "```py\nvdp_model = VDP(mu = 0.10) \nplot_time_series(model_sim, \n                 vdp_model, \n                 data_vdp[0], \n                 dyn_var_idx=1, \n                 title = \"VDP Model: Before Parameter Fits\");\n```", "```py\ntrain(vdp_model, data_vdp, epochs=50, model_name=\"vdp\");\nprint(f\"After training: {vdp_model}, where the true value is {true_mu}\")\nprint(f\"Final Parameter Recovery Error: {vdp_model.mu - true_mu}\")\n```", "```py\nLoss at 0: 0.1369624137878418\nLoss at 10: 0.0073615191504359245\nLoss at 20: 0.0009214915917254984\nLoss at 30: 0.0002127257248503156\nLoss at 40: 0.00019956652977271006\nAfter training:  mu: 0.3018421530723572, where the true value is 0.3\nFinal Parameter Recovery Error: 0.0018421411514282227\n```", "```py\nmodel_sim_lv = LotkaVolterra(1.5,1.0,3.0,1.0)\nts_data = torch.arange(0.0, 10.0, 0.1)\ndata_lv = create_sim_dataset(model_sim_lv, \n                              ts = ts_data, \n                              num_samples=10, \n                              sigma_noise=0.1,\n                              initial_conditions_default=torch.tensor([2.5, 2.5]))\nmodel_lv = LotkaVolterra(alpha=1.6, beta=1.1,delta=2.7, gamma=1.2) \n\nplot_time_series(model_sim_lv, model_lv, data = data_lv[0], title = \"Lotka Volterra: Before Fitting\");\n```", "```py\ntrain(model_lv, data_lv, epochs=60, lr=1e-2, model_name=\"lotkavolterra\")\nprint(f\"Fitted model: {model_lv}\")\nprint(f\"True model: {model_sim_lv}\")\n```", "```py\nLoss at 0: 1.1298701763153076\nLoss at 10: 0.1296287178993225\nLoss at 20: 0.045993587002158165\nLoss at 30: 0.02311511617153883\nLoss at 40: 0.020882505923509598\nLoss at 50: 0.020726025104522705\nFitted model:  alpha: 1.5965800285339355,             beta: 1.0465354919433594,                 delta: 2.817030429840088,                     gamma: 0.939825177192688\nTrue model:  alpha: 1.5,             beta: 1.0,                 delta: 3.0,                     gamma: 1.0\n```", "```py\nmodel_sim_lorenz = Lorenz(sigma=10.0, rho=28.0, beta=8.0/3.0)\nts_data = torch.arange(0, 10.0, 0.05)\ndata_lorenz = create_sim_dataset(model_sim_lorenz, \n                              ts = ts_data, \n                              num_samples=30, \n                              initial_conditions_default=torch.tensor([1.0, 0.0, 0.0]),\n                              sigma_noise=0.01, \n                              sigma_initial_conditions=0.10)\nlorenz_model = Lorenz(sigma=10.2, rho=28.2, beta=9.0/3) \nfig, ax = plot_time_series(model_sim_lorenz, lorenz_model, data_lorenz[0], title=\"Lorenz Model: Before Fitting\");\n\nax.set_xlim((2,15));\n```", "```py\ntrain(lorenz_model, \n      data_lorenz, \n      epochs=300, \n      batch_size=5,\n      method = 'rk4',\n      step_size=0.05,\n      show_every=50,\n      lr = 1e-3)\n```", "```py\nLoss at 0: 114.25119400024414\nLoss at 50: 4.364489555358887\nLoss at 100: 2.055854558944702\nLoss at 150: 1.2539702206850052\nLoss at 200: 0.7839434593915939\nLoss at 250: 0.5347371995449066\n```", "```py\nplot_phase_plane(model_sim_lorenz, lorenz_model, data_lorenz[0], title = \"Lorenz Model: After Fitting\", time_range=(0,20.0));\n```", "```py\n# remake the data \nmodel_sim_vdp = VDP(mu=0.20)\nts_data = torch.linspace(0.0,30.0,100) # longer time series than the custom ode layer\ndata_vdp = create_sim_dataset(model_sim_vdp, \n                  ts = ts_data, \n                  num_samples=30, # more samples than the custom ode layer\n                  sigma_noise=0.1,\n                  initial_conditions_default=torch.tensor([0.50,0.10]))\n```", "```py\n class NeuralDiffEq(nn.Module):\n    \"\"\" \n    Basic Neural ODE model\n    \"\"\"\n    def __init__(self,\n                 dim: int = 2, # dimension of the state vector\n                 ) -> None:\n        super().__init__()\n        self.ann = nn.Sequential(torch.nn.Linear(dim, 8), \n                                 torch.nn.LeakyReLU(), \n                                 torch.nn.Linear(8, 16), \n                                 torch.nn.LeakyReLU(), \n                                 torch.nn.Linear(16, 32), \n                                 torch.nn.LeakyReLU(), \n                                 torch.nn.Linear(32, dim))\n\n    def forward(self, t, state):\n        return self.ann(state) \n```", "```py\nmodel_vdp_nde = NeuralDiffEq(dim=2) \nplot_time_series(model_sim_vdp, model_vdp_nde, data_vdp[0], title = \"Neural ODE: Before Fitting\");\n```", "```py\ntrain(model_vdp_nde, \n      data_vdp, \n      epochs=1500, \n      lr=1e-3, \n      batch_size=5,\n      show_every=100,\n      model_name = \"nde\")\n```", "```py\nLoss at 0: 84.39617252349854\nLoss at 100: 84.34061241149902\nLoss at 200: 73.75008296966553\nLoss at 300: 3.4929964542388916\nLoss at 400: 1.6555403769016266\nLoss at 500: 0.7814530655741692\nLoss at 600: 0.41551147401332855\nLoss at 700: 0.3157300055027008\nLoss at 800: 0.19066352397203445\nLoss at 900: 0.15869349241256714\nLoss at 1000: 0.12904016114771366\nLoss at 1100: 0.23840919509530067\nLoss at 1200: 0.1681726910173893\nLoss at 1300: 0.09865255374461412\nLoss at 1400: 0.09134986530989408\n```", "```py\npip install paramfittorchdemo\n```"]