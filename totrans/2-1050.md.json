["```py\n- model.tar.gz/\n  - linear_regression_model\n    - 1\n       - model.pt\n       - model.py (optional, not included here)\n    - config.pbtxt\n```", "```py\nimport torch\nfrom transformers import BertModel, BertTokenizer\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load bert model and tokenizer\nmodel_name = 'bert-base-uncased'\nmodel = BertModel.from_pretrained(model_name, torchscript = True)\ntokenizer = BertTokenizer.from_pretrained(model_name)\n\n# Sample Input\ntext = \"I am super happy right now to be trying out BERT.\"\n\n# Tokenize sample text\ninputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n\n# jit trace model\ntraced_model = torch.jit.trace(model, (inputs[\"input_ids\"], inputs[\"attention_mask\"]))\n\n# Save traced model\ntorch.jit.save(traced_model, \"model.pt\")\n```", "```py\n# sample inference with loaded model\nloaded_model = torch.jit.load(\"model.pt\")\nres = loaded_model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\nres\n```", "```py\nfrom transformers import BertConfig\nbert_config = BertConfig.from_pretrained(model_name)\nmax_sequence_length = bert_config.max_position_embeddings\noutput_shape = bert_config.hidden_size\nprint(f\"Maximum Input Sequence Length: {max_sequence_length}\")\nprint(f\"Output Shape: {output_shape}\")\n```", "```py\nname: \"bert_model\"\nplatform: \"pytorch_libtorch\"\n\ninput [\n  {\n    name: \"input_ids\"\n    data_type: TYPE_INT32\n    dims: [1, 512]\n  },\n  {\n    name: \"attention_mask\"\n    data_type: TYPE_INT32\n    dims: [1, 512]\n  }\n]\n\noutput [\n  {\n    name: \"OUTPUT\"\n    data_type: TYPE_FP32\n    dims: [512, 768]\n  }\n]\n```", "```py\ndocker run --gpus all --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 -v\n/home/ec2-user/SageMaker:/models nvcr.io/nvidia/tritonserver:23.08-py3\ntritonserver --model-repository=/models --exit-on-error=false --log-verbose=1\n```", "```py\nimport requests\nimport json\n\n# Specify the model name and version\nmodel_name = \"bert_model\" #specified in config.pbtxt\nmodel_version = \"1\"\n\n# Set the inference URL based on the Triton server's address\nurl = f\"http://localhost:8000/v2/models/{model_name}/versions/{model_version}/infer\"\n\n# sample invoke\noutput = requests.post(url, data=json.dumps(payload))\nres = output.json()\n```", "```py\n!tar -cvzf model.tar.gz bert_model/\n```", "```py\n%%time\n# we make a 200 copies of the tarball, this will take about ~6 minutes to finish (can vary depending on model size)\nfor i in range(200):\n    with open(\"model.tar.gz\", \"rb\") as f:\n        s3_client.upload_fileobj(f, bucket, \"{}/model-{}.tar.gz\".format(s3_model_prefix,i))\n```", "```py\ntriton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:23.07-py3\".format(\n    account_id=account_id_map[region], region=region, base=base\n)\n\nprint(f\"Triton Inference server DLC image: {triton_image_uri}\")\n```", "```py\nendpoint_config_response = client.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            \"VariantName\": \"tritontraffic\",\n            \"ModelName\": model_name,\n            \"InstanceType\": \"ml.g4dn.4xlarge\",\n            \"InitialInstanceCount\": 1,\n            \"InitialVariantWeight\": 1\n        },\n    ],\n)\n\nendpoint_name = \"triton-mme-gpu-ep\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n\ncreate_endpoint_response = client.create_endpoint(\n    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n)\n\nprint(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n```", "```py\nresponse = runtime_client.invoke_endpoint(\n    EndpointName=endpoint_name, ContentType=\"application/octet-stream\", \n    Body=json.dumps(payload), TargetModel='model-199.tar.gz'\n)\nprint(json.loads(response[\"Body\"].read().decode(\"utf8\")))\n```"]