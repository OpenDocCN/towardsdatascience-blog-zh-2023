- en: 'META’s Hiera: reduce complexity to increase accuracy'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: META 的 Hiera：降低复杂性以提高准确性
- en: 原文：[https://towardsdatascience.com/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b](https://towardsdatascience.com/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b](https://towardsdatascience.com/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b)
- en: '| ARTIFICIAL INTELLIGENCE | COMPUTER VISION | VITs |'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_TB
  zh: '| 人工智能 | 计算机视觉 | VITs |'
- en: Simplicity allows AI to reach incredible performance and surprising speed
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单性使得人工智能能够达到惊人的性能和速度
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----30f7a147ad0b--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----30f7a147ad0b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----30f7a147ad0b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----30f7a147ad0b--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----30f7a147ad0b--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----30f7a147ad0b--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----30f7a147ad0b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----30f7a147ad0b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----30f7a147ad0b--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----30f7a147ad0b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----30f7a147ad0b--------------------------------)
    ·12 min read·Jun 21, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----30f7a147ad0b--------------------------------)
    ·阅读时间12分钟·2023年6月21日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b2ab2597bedfa9804d3e3e8da0d02bb6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2ab2597bedfa9804d3e3e8da0d02bb6.png)'
- en: Photo by [Alexander Redl](https://unsplash.com/@alexanderredl?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Alexander Redl](https://unsplash.com/@alexanderredl?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[Convolutional networks](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    have dominated the field of computer vision for more than twenty years. With the
    arrival of [transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)),
    it was believed that they would be abandoned. **Yet many practitioners use convolution-based
    models for projects. Why?**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[卷积网络](https://en.wikipedia.org/wiki/Convolutional_neural_network)在计算机视觉领域已经主导了二十多年。随着[变换器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))的到来，人们曾认为它们会被淘汰。**然而，许多从业者仍在项目中使用基于卷积的模型。这是为什么呢？**'
- en: 'This article tries to answer these questions:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文试图回答这些问题：
- en: What are Vision Transformers?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉变换器是什么？
- en: What are their limitations?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的局限性是什么？
- en: Can we try to overcome them?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能否尝试克服这些问题？
- en: Why and how does META Hiera seem to succeed?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: META Hiera 如何以及为何似乎成功了？
- en: 'Vision transformer: an image is worth how many words?'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉变换器：一张图值多少字？
- en: '![](../Images/dfc18ba75f38c986f153869f0ac5dd4d.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfc18ba75f38c986f153869f0ac5dd4d.png)'
- en: 'image source: [here](https://en.wikipedia.org/wiki/Vision_transformer#/media/File:Vision_Transformer.gif)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://en.wikipedia.org/wiki/Vision_transformer#/media/File:Vision_Transformer.gif)
- en: '[Vision transformers](https://en.wikipedia.org/wiki/Vision_transformer) have
    dominated vision benchmarks in recent years, **but exactly what are they?**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[视觉变换器](https://en.wikipedia.org/wiki/Vision_transformer)近年来在视觉基准测试中占据了主导地位，**但它们究竟是什么呢？**'
- en: 'Until a few years ago, [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    were the standard in vision tasks. In 2017, however, the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    was released and turned the [NLP](https://en.wikipedia.org/wiki/Natural_language_processing)
    world upside down. In the article [Attention is all you need](https://arxiv.org/abs/1706.03762),
    the authors show that a model built using only self-attention is capable of far
    superior performance to [RNNs](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    and [LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory). So one soon
    wonders: i**s it possible to apply a transformer to images?**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 直到几年前，[卷积神经网络](https://en.wikipedia.org/wiki/Convolutional_neural_network) 一直是视觉任务的标准。然而，在
    2017 年，[变压器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    的发布让 [NLP](https://en.wikipedia.org/wiki/Natural_language_processing) 世界颠覆了。在文章
    [Attention is all you need](https://arxiv.org/abs/1706.03762) 中，作者展示了一个仅使用自注意力构建的模型能够比
    [RNNs](https://en.wikipedia.org/wiki/Recurrent_neural_network) 和 [LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory)
    实现更优越的性能。所以人们很快会想：**是否可以将变压器应用于图像？**
- en: Hybrid models where self-attention integration was included had been attempted
    before 2020\. In any case, these models could not scale well. The idea was to
    find a way in which the transformer could be used natively with images.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2020 年之前，已经尝试过包括自注意力集成的混合模型。但无论如何，这些模型的扩展性都不佳。这个想法是找到一种方法，使变压器可以原生地用于图像。
- en: In 2020, the [Google authors decided that the best way](https://arxiv.org/pdf/2010.11929.pdf)
    was to split the images into different patches and then have [embedding](https://en.wikipedia.org/wiki/Embedding)
    of the sequence. In this way, the images are basically treated as if they were
    tokens (words) from the model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2020 年，[谷歌的作者决定最佳方法](https://arxiv.org/pdf/2010.11929.pdf) 是将图像分割成不同的补丁，然后对序列进行
    [嵌入](https://en.wikipedia.org/wiki/Embedding)。这样，图像基本上被视为模型中的标记（单词）。
- en: '![](../Images/2290f47891093801107e0192fe143e60.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2290f47891093801107e0192fe143e60.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2010.11929.pdf)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2010.11929.pdf)
- en: In a short time, the dominance of computer vision by CNNs is gradually being
    undermined. [Vision transformers](https://en.wikipedia.org/wiki/Vision_transformer)
    prove to be superior on benchmarks (such as [ImageNet](https://en.wikipedia.org/wiki/ImageNet))
    where CNNs had hitherto dominated.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在短时间内，CNNs 在计算机视觉中的主导地位正逐渐被削弱。[视觉变压器](https://en.wikipedia.org/wiki/Vision_transformer)
    在 CNNs 一直主导的基准（如 [ImageNet](https://en.wikipedia.org/wiki/ImageNet)）上表现更为优越。
- en: '![](../Images/7a1a1150becbe7d4eb7c116a1aa1f2a1.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a1a1150becbe7d4eb7c116a1aa1f2a1.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2101.11986.pdf)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2101.11986.pdf)
- en: 'In fact, providing enough data the [Vision Transformers (ViTs)](https://en.wikipedia.org/wiki/Vision_transformer)
    show that they are superior to the CNNs. It is also shown that although there
    are several differences there are also several similarities:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，提供足够的数据，[视觉变压器（ViTs）](https://en.wikipedia.org/wiki/Vision_transformer)
    表现出它们优于 CNNs。还显示出，尽管存在一些差异，但也有许多相似之处：
- en: both ViTs and CNNs construct a complex and progressive representation.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViTs 和 CNNs 都构建了复杂且逐步深入的表示。
- en: However, ViTs are more capable of exploiting information present in the background
    and appear to be more robust.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，ViTs 更能利用背景信息，并且似乎更具鲁棒性。
- en: '[](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----30f7a147ad0b--------------------------------)
    [## A Visual Journey in What Vision-Transformers See'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----30f7a147ad0b--------------------------------)
    [## 视觉变压器看到的视觉之旅'
- en: How some of the largest models see the world
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一些最大的模型如何看待世界
- en: pub.towardsai.net](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----30f7a147ad0b--------------------------------)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: pub.towardsai.net](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----30f7a147ad0b--------------------------------)
- en: Also, an additional advantage is the transformer’s ability to scale. This has
    been a competitive advantage of ViTs that has made them a popular choice.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，变压器的可扩展性也是一个额外的优势。这成为了 ViTs 的竞争优势，使其成为热门选择。
- en: '![](../Images/6278945da0662487535a58db51d7bfc7.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6278945da0662487535a58db51d7bfc7.png)'
- en: usage of vision transformers. [source](https://paperswithcode.com/method/vision-transformer)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用视觉变压器。 [source](https://paperswithcode.com/method/vision-transformer)
- en: '**In fact, through the years we have seen CNNs of millions of parameters and
    ViTs reaching billions of parameters.** Last year, Google showed how can even
    scale up ViTs up to 20 B parameters and probably in the future we will see even
    bigger models.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**实际上，多年来我们见证了数百万参数的CNN和达到数十亿参数的ViTs。** 去年，Google展示了如何将ViTs扩展到20B参数，并且未来我们可能会看到更大的模型。'
- en: '[](/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6?source=post_page-----30f7a147ad0b--------------------------------)
    [## Why Do We Have Huge Language Models and Small Vision Transformers?'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6?source=post_page-----30f7a147ad0b--------------------------------)
    [## 为什么我们有大型语言模型和小型视觉变压器？'
- en: Google ViT-22 paves the way for new large transformers and to revolutionize
    computer vision
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google ViT-22为新型大型变压器和计算机视觉的革命铺平了道路
- en: towardsdatascience.com](/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6?source=post_page-----30f7a147ad0b--------------------------------)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6?source=post_page-----30f7a147ad0b--------------------------------)
- en: The limit of the Vision Transformer
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉变压器的限制
- en: '![](../Images/ebd16380a18ca436710c00065f67d7af.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebd16380a18ca436710c00065f67d7af.png)'
- en: Photo by [Joshua Hoehne](https://unsplash.com/pt-br/@mrthetrain?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Joshua Hoehne](https://unsplash.com/pt-br/@mrthetrain?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Having natively adapted the transformer still comes at a cost: [ViTs](https://en.wikipedia.org/wiki/Vision_transformer)
    use their parameters inefficiently. **This comes from the fact that they use the
    same spatial resolution and the same number of channels in the network.**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 原生适配变压器仍然有成本：[ViTs](https://en.wikipedia.org/wiki/Vision_transformer)在使用其参数时效率低下。**这源于它们在网络中使用相同的空间分辨率和相同数量的通道。**
- en: 'CNNs had precisely two aspects that determined their initial fortunes (both
    inspired by the human cortex):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: CNN有两个方面决定了它们的初期命运（都受到人脑皮层的启发）：
- en: Reduction in spatial resolution while going up in the hierarchy of layers.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着层级的上升，空间分辨率减少。
- en: Increase in the number of different “channels,” and each of these channels becomes
    more and more specialized.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加不同的“通道”数量，每个通道变得越来越专业。
- en: '![](../Images/12971175a97579dcc92ca9ff331499f3.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12971175a97579dcc92ca9ff331499f3.png)'
- en: 'image source: [here](https://arxiv.org/pdf/1412.6631.pdf)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[这里](https://arxiv.org/pdf/1412.6631.pdf)
- en: 'The transformer, on the other hand, has a different structure a sequence of
    [self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)) blocks
    where two main operations occur that allow it to generalize well:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，变压器具有不同的结构，即一系列[自注意力](https://en.wikipedia.org/wiki/Attention_(machine_learning))模块，其中发生两个主要操作，使其能够很好地进行泛化：
- en: the attention operation that is used to model inter-element relationships.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于建模元素间关系的注意力操作。
- en: A [fully-connected layer](/convolutional-layers-vs-fully-connected-layers-364f05ab460b)
    that instead models the inter-element relationship.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[全连接层](/convolutional-layers-vs-fully-connected-layers-364f05ab460b)则建模元素间的关系。'
- en: '**This was in fact noted earlier and stems from the fact that the transformer
    was designed for words and not images**. After all, text and images are two modes
    that are different. One of the differences is that words do not vary in scale
    while images do. **This is conflicting when you have to give attention to elements
    that change at scale in object detection.**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**这实际上早已被注意到，源于变压器设计是针对文字而非图像的**。毕竟，文本和图像是两种不同的模式。其一的不同之处在于，文字在尺度上不会变化，而图像则会。**这在需要关注在目标检测中尺度变化的元素时是有冲突的。**'
- en: '**Also, the resolution of pixels in an image is higher than the resolution
    of words in a text passage.** Since attention has a quadratic cost, using high-resolution
    images has a high computational cost with a transformer.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**此外，图像中像素的分辨率高于文本段落中文字的分辨率。** 由于注意力机制具有平方级别的成本，使用高分辨率图像在变压器中具有较高的计算成本。'
- en: Previous studies have tried to solve this problem with the use of hierarchical
    feature maps. For example, the [Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf)
    constructs a hierarchical representation by starting with small patches and then
    gradually merging the various neighbor patches.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究尝试通过使用层次特征图来解决这个问题。例如，[Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf)通过从小块开始并逐渐合并各种邻近块来构建层次表示。
- en: '![](../Images/fa9aefe51bfe7f45591d1ccd96962772.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa9aefe51bfe7f45591d1ccd96962772.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2103.14030.pdf)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[这里](https://arxiv.org/pdf/2103.14030.pdf)
- en: Other studies have tried to implement multi-channel in ViTs. For example, [MVITs](https://arxiv.org/pdf/2104.11227.pdf)
    have tried to create initial channels that focus on simple low-level visual information
    while deeper channels focus on complex high-level features as in CNNs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究尝试在ViTs中实现多通道。例如，[MVITs](https://arxiv.org/pdf/2104.11227.pdf)尝试创建初级通道，专注于简单的低级视觉信息，同时更深的通道则关注复杂的高级特征，如CNNs中的做法。
- en: '![](../Images/4dafb92e01ae94c3714832ab8aeb8e33.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dafb92e01ae94c3714832ab8aeb8e33.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2104.11227.pdf)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[这里](https://arxiv.org/pdf/2104.11227.pdf)
- en: These, however, did not completely solve the problem. Over time, increasingly
    complex models, and specialized modules, have been proposed, which have improved
    performance to some extent but made [ViTs](https://en.wikipedia.org/wiki/Vision_transformer)
    rather slow in training.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些方法并未完全解决问题。随着时间的推移，提出了越来越复杂的模型和专业模块，这些改进在一定程度上提升了性能，但使得[ViTs](https://en.wikipedia.org/wiki/Vision_transformer)在训练时变得相当缓慢。
- en: Can we solve these transformer limitations without the need for complex solutions?
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们能否在不需要复杂解决方案的情况下解决这些变换器的局限性？
- en: How to learn the spatial relationship
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何学习空间关系
- en: '![](../Images/9668f1e26fbd3d2d3a891632260e20e5.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9668f1e26fbd3d2d3a891632260e20e5.png)'
- en: image by [Ali Kazal](https://unsplash.com/it/@lureofadventure) on Unsplash
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Ali Kazal](https://unsplash.com/it/@lureofadventure)在Unsplash上提供
- en: ViTs have emerged as a model for computer vision, however, increasingly complex
    modifications have been necessary to adapt them.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ViTs已成为计算机视觉领域的一个模型，然而，适应它们需要越来越复杂的修改。
- en: Can we solve these transformer limitations without the need for complex solutions?
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们能否在不需要复杂解决方案的情况下解决这些变换器的局限性？
- en: In recent years, efforts have been made to streamline the models and speed them
    up. One way often used is the introduction of sparsity. In the case of computer
    vision, one model that has been very successful has been [masked autoencoders](https://arxiv.org/abs/2111.06377)
    (MAE).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，人们一直在努力简化模型并加快速度。常用的一种方法是引入稀疏性。在计算机视觉领域，一个非常成功的模型是[掩码自编码器](https://arxiv.org/abs/2111.06377)（MAE）。
- en: In that case, after dividing by patches, a number of patches are masked. Then,
    the decoder has to reconstruct from the masked patches. The ViT encoder then works
    on only 25 percent of the patches. In this, you can train wide encoders with a
    fraction of computation and memory.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，经过分块后，一些块会被掩盖。然后，解码器必须从被掩盖的块中重建。ViT编码器仅处理25%的块。在这种情况下，你可以用少量的计算和内存来训练宽编码器。
- en: '![](../Images/2b71b5a523a47b3e83edf61685dc7728.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b71b5a523a47b3e83edf61685dc7728.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2111.06377.pdf)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[这里](https://arxiv.org/pdf/2111.06377.pdf)
- en: This method has been shown to be capable of teaching spatial reasoning, achieving
    results comparable if not superior to Swin and Mvit (which, however, are computationally
    much more complex).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法已被证明能够教授空间推理，取得了与Swin和Mvit相当甚至更优的结果（但后者在计算上要复杂得多）。
- en: On the other hand, if it is true that the sparsity regime obtained training
    efficiency, one of the great advantages of CNNs is the hierarchy approach. But
    it is conflictual with sparsity.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果稀疏性确实提高了训练效率，那么CNNs的一个主要优点就是其层次化的方法。但这与稀疏性存在冲突。
- en: 'In fact, it has been tested before but without much success:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这之前已经做过测试，但效果不佳：
- en: The obtained model was too slow ([MaskFeat](https://arxiv.org/abs/2112.09133)
    or [SimMIM](https://arxiv.org/abs/2111.09886)).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 得到的模型速度过慢（[MaskFeat](https://arxiv.org/abs/2112.09133) 或 [SimMIM](https://arxiv.org/abs/2111.09886)）。
- en: Modifications made the model unnecessarily complex and without gains in accuracy
    ([UM-MAE](https://arxiv.org/abs/2205.10063) or [MCMAE](https://openreview.net/forum?id=qm5LpHyyOUO)).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改使模型变得不必要地复杂，并且没有在准确性上获得提升（[UM-MAE](https://arxiv.org/abs/2205.10063) 或 [MCMAE](https://openreview.net/forum?id=qm5LpHyyOUO)）。
- en: Is possible to design a sparse and hierarchic yet efficient model?
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 是否可以设计一个稀疏且层次化但仍高效的模型？
- en: New work by META has departed from MAE training and other tricks to build a
    ViT that is efficient and accurate without the need for all those complex structures
    that have been used in the past.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: META的新工作已脱离了MAE训练和其他技巧，致力于构建一个高效且准确的ViT，而不需要过去使用的那些复杂结构。
- en: '[](https://arxiv.org/abs/2306.00989?source=post_page-----30f7a147ad0b--------------------------------)
    [## Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2306.00989?source=post_page-----30f7a147ad0b--------------------------------)
    [## Hiera: 一种没有花哨组件的层次化视觉变换器'
- en: Modern hierarchical vision transformers have added several vision-specific components
    in the pursuit of supervised…
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现代层次化视觉变换器在追求有监督学习的过程中添加了多个视觉专用组件…
- en: arxiv.org](https://arxiv.org/abs/2306.00989?source=post_page-----30f7a147ad0b--------------------------------)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/2306.00989?source=post_page-----30f7a147ad0b--------------------------------)
- en: 'Hiera: Hierarchical, Sparse, and Efficient ViT'
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Hiera: 层次化、稀疏且高效的 ViT'
- en: '![](../Images/e94128fe37300b44f444c4d77f29d7f6.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e94128fe37300b44f444c4d77f29d7f6.png)'
- en: Photo by [Jordan Opel](https://unsplash.com/@opeleye?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Jordan Opel](https://unsplash.com/@opeleye?utm_source=medium&utm_medium=referral)
    提供，[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The model
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: The basic idea is that to train a hierarchical ViT with high accuracy in visual
    tasks it is not necessary to use a whole series of elements that make it slow
    and complex. According to the authors, spatial bias can be learned from the model
    using masked autoencoder training.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是，为了在视觉任务中训练一个高准确度的层次化 ViT，不需要使用一系列使其变得缓慢和复杂的元素。根据作者的说法，空间偏差可以通过掩码自编码器训练从模型中学习。
- en: In MAE, patches are deleted, so in a hierarchical model, it has problems reconstructing
    the 2D grid (and spatial relationships). The authors solve so that the kernel
    cannot overlap between mask units (during [pooling](https://www.kaggle.com/questions-and-answers/59502),
    there is no overlap with masked units).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MAE 中，补丁被删除，因此在层次化模型中，它在重建 2D 网格（以及空间关系）时遇到问题。作者解决了这个问题，使得内核在掩码单元之间不能重叠（在
    [池化](https://www.kaggle.com/questions-and-answers/59502) 中，掩码单元之间没有重叠）。
- en: '![](../Images/a5b2a999605ce83214c7591015680af1.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5b2a999605ce83214c7591015680af1.png)'
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/abs/2306.00989)'
- en: The authors then started from an existing hierarchical model of ViT, MViTv2,
    and decided to repurpose it using MAE training. The model is built of several
    ViT blocks but as seen in the structure at some point there is a reduction in
    size which is achieved by using pooling attention.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们从现有的 ViT 层次化模型 MViTv2 开始，并决定使用 MAE 训练对其进行改造。该模型由多个 ViT 块组成，但如结构所示，某些地方会减少尺寸，这通过使用池化注意力实现。
- en: '![](../Images/cf10662ed79873cca0f7e0f2aa8a548e.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf10662ed79873cca0f7e0f2aa8a548e.png)'
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/abs/2306.00989)'
- en: During pooling attention features are aggregated locally using a 3x3 convolution
    and then self-attention is computed (this is to reduce the size of K and V and
    thus reduce computational computation). This mechanism can become expensive when
    using video. So the authors replaced it with Mask Unit Attention.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在池化过程中，注意力特征使用 3x3 卷积进行局部聚合，然后计算自注意力（这是为了减少 K 和 V 的大小，从而减少计算量）。当使用视频时，这种机制可能变得昂贵。因此，作者用掩码单元注意力替代了它。
- en: In other words, in Hiera during pooling the kernel shifts so that masked parts
    do not end up in the pooling. So a kind of local attention for each group of tokens
    (of mask size)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，在 Hiera 中，池化过程中内核会移动，以确保掩码部分不会出现在池化中。因此，对每组标记（掩码大小）进行某种局部注意力
- en: '![](../Images/bfd862004618641d663af19169e85fe0.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfd862004618641d663af19169e85fe0.png)'
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/abs/2306.00989)'
- en: 'MViTv2 had then introduced a whole series of accouterments that increased complexity,
    though, and the authors deemed them nonessential and eliminated them:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: MViTv2 随后引入了一系列增加复杂性的附加组件，作者认为这些是非必需的，因此将其删除：
- en: '**Relative Position Embeddings.** The positional embedding is added to the
    attention in each block.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相对位置嵌入**。位置嵌入被添加到每个块中的注意力中。'
- en: '**Max polling layers**, which would have required padding for use in Hiera.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大池化层**，在 Hiera 中使用时需要填充。'
- en: '**Attention residual**, where there is a residual connection between Q (query)
    and output to better learn pooling attention. The authors have reduced the number
    of layers so that it is no longer necessary.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意力残差**，在 Q（查询）和输出之间存在残差连接，以更好地学习池化注意力。作者减少了层数，因此不再需要。'
- en: The authors show the impact of these changes leads singularly to improved performance
    in both accuracy (acc.) and speed (images per second).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作者展示了这些变化的影响如何显著提高了准确率（acc.）和速度（每秒图像）。
- en: '![](../Images/e45e95dab71d633b7ea68450cabd8199.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e45e95dab71d633b7ea68450cabd8199.png)'
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2306.00989)
- en: In general, simplifying the model makes Hiera not only much faster (for both
    images and video) but also more accurate ( than both its counterpart MViTv2 but
    also other models).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，简化模型使 Hiera 不仅在图像和视频上更快，而且比其对应的 MViTv2 和其他模型更准确。
- en: Hiera is 2.4× faster on images and 5.1× faster on video than the MViTv2 we started
    with and is actually more accurate because of MAE ([source](https://arxiv.org/abs/2306.00989))
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Hiera 在图像上比我们开始使用的 MViTv2 快 2.4 倍，在视频上快 5.1 倍，并且由于 MAE 实际上更准确（[来源](https://arxiv.org/abs/2306.00989)）
- en: '![](../Images/e52c9cfe80d77bd284049760866af0ac.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e52c9cfe80d77bd284049760866af0ac.png)'
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2306.00989)
- en: The authors point out that the model is not only faster in inference but also
    the training is much faster.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 作者指出，模型不仅在推断时更快，而且训练也快得多。
- en: '![](../Images/d8f52ae6b374ae22c8b66103629c1f09.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8f52ae6b374ae22c8b66103629c1f09.png)'
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2306.00989)
- en: Results
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: The authors show how indeed the basic model with a limited number of parameters
    achieves good results on Imagenet 1K (one of the most important image classification
    datasets).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 作者展示了基本模型在参数数量有限的情况下，如何在 Imagenet 1K 上取得良好结果（这是最重要的图像分类数据集之一）。
- en: '![](../Images/4bd56e0149eeb0665a2be4fcfebd22b4.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bd56e0149eeb0665a2be4fcfebd22b4.png)'
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2306.00989)
- en: The second point is that normally at low parameter regimes, the convolution-based
    models dominated. Here the smaller model shows very good results. For the authors,
    this confirms their intuitions that a spatial bias can be learned during training
    and thus make ViTs competitive with the convolutional network even for small models.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 第二点是，在低参数范围内，通常是卷积基础的模型占据主导地位。这里更小的模型表现非常好。对于作者而言，这证实了他们的直觉，即空间偏差可以在训练过程中学习，从而使
    ViTs 即使在小模型中也能与卷积网络竞争。
- en: '![](../Images/42cc512ca5d7121c369905f9b9e25dea.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42cc512ca5d7121c369905f9b9e25dea.png)'
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2306.00989)
- en: 'The fortune of the large CNN models was to use them for [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning).
    Both [ResNet](https://arxiv.org/abs/1512.03385) and VGG-based models have been
    trained on Imagenet and then adapted by the community for many tasks. Therefore,
    the authors test Hiera for its transfer learning capability using two datasets:
    iNaturalists and Places.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 大型 CNN 模型的成功在于它们可以用于[迁移学习](https://en.wikipedia.org/wiki/Transfer_learning)。[ResNet](https://arxiv.org/abs/1512.03385)
    和基于 VGG 的模型已经在 Imagenet 上训练，然后被社区用于许多任务。因此，作者测试了 Hiera 的迁移学习能力，使用了两个数据集：iNaturalists
    和 Places。
- en: '![](../Images/37b0be1d3a12d598c689daafc26588ec.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37b0be1d3a12d598c689daafc26588ec.png)'
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2306.00989)
- en: The authors fine-tune the model on the two datasets and show that their model
    is superior to previous ViTs. This shows that their model could be used for other
    datasets as well.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在这两个数据集上对模型进行了微调，并展示了他们的模型优于之前的 ViTs。这表明他们的模型也可以用于其他数据集。
- en: In addition, the authors use another popular dataset COCO. While iNaturalists
    and Places was a dataset for image classification, COCO is one of the most widely
    used datasets for image segmentation and object detection (two other popular tasks
    in computer science). Again, the model shows strong scaling behavior (an increase
    in performance as parameters increase). In addition, the model is faster both
    during training and in inference.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者使用了另一个流行的数据集 COCO。虽然 iNaturalists 和 Places 是图像分类的数据集，COCO 是用于图像分割和目标检测（计算机科学中的两个热门任务）的最广泛使用的数据集之一。再次，模型表现出强劲的扩展性（参数增加时性能提升）。此外，模型在训练和推断时都更快。
- en: '![](../Images/4f5712d44b2b5f71760394e0f330fd22.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f5712d44b2b5f71760394e0f330fd22.png)'
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2306.00989)
- en: Moreover, the model has been tested on video. Specifically on two video classification
    datasets. Hiera shows that it performs better with fewer parameters. The model
    is also faster in inference. The authors show that the model achieves state-of-the-art
    for this type of task.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该模型已在视频上进行了测试，特别是在两个视频分类数据集上。Hiera表明它在参数更少的情况下表现更好。该模型在推理中也更快。作者展示了该模型在这种任务中达到了最先进的水平。
- en: '![](../Images/e1dbe18103414490b90cd738ef6a4a4f.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1dbe18103414490b90cd738ef6a4a4f.png)'
- en: 'image source: [here](https://arxiv.org/abs/2306.00989)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2306.00989)
- en: The authors show that the model can also be used on other video tasks, such
    as action detection.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 作者展示了该模型还可以用于其他视频任务，例如动作检测。
- en: Parting thoughts
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 告别感言
- en: In this work, we create a simple hierarchical vision transformer by taking an
    existing one and removing all its bells-and-whistles while supplying the model
    with spatial bias through MAE pretraining. ([source](https://arxiv.org/pdf/2306.00989.pdf))
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这项工作中，我们通过采用现有的层次化视觉变换器，去除所有附加功能，同时通过MAE预训练为模型提供空间偏置，从而创建了一个简单的层次化视觉变换器。 ([source](https://arxiv.org/pdf/2306.00989.pdf))
- en: The authors showed how many of the elements that have been added to improve
    transformer performance are actually not only unnecessary but increase the complexity
    of the model, making it slower.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作者展示了许多为提高变换器性能而添加的元素实际上不仅是不必要的，而且增加了模型的复杂性，使其变得更慢。
- en: Instead, the authors showed that using MAE and a hierarchical structure can
    result in a ViT that is faster and more accurate for both images and video.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，作者展示了使用MAE和层次结构可以使ViT在图像和视频处理上更快、更准确。
- en: This work is important because for many tasks the community still uses convolution-based
    models. ViTs are very large models and have a high computational cost. So often,
    people prefer to use models based on ResNet and VGG. ViTs that are more accurate
    but especially faster in inference could be game-changers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作很重要，因为在许多任务中，社区仍然使用基于卷积的模型。ViTs是非常大的模型，计算成本高。因此，人们通常更愿意使用基于ResNet和VGG的模型。ViTs如果在推理上更准确但尤其更快，可能会改变游戏规则。
- en: 'Second, it highlights a trend seen elsewhere: leverage the sparsity for the
    training. Which has the advantage of reducing parameters, and speeding up training
    and inference. In general, the idea of sparsity is also being seen in other fields
    of artificial intelligence and is an active field of research.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，它突出了在其他地方看到的一个趋势：利用稀疏性进行训练。这具有减少参数和加速训练与推理的优势。一般来说，稀疏性的概念也在其他人工智能领域中被看到，并且是一个活跃的研究领域。
- en: 'If you have found this interesting:'
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得这很有趣：
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, you can* [***become a Medium member***](https://medium.com/@salvatore-raieli/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以查看我的其他文章，你也可以* [***订阅***](https://salvatore-raieli.medium.com/subscribe)
    *以在我发布文章时收到通知，你还可以* [***成为Medium会员***](https://medium.com/@salvatore-raieli/membership)
    *以访问所有故事（平台的附属链接，我从中获得少量收入，对你没有成本），你也可以在*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***上连接或联系我。***'
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是我GitHub仓库的链接，我计划在其中收集与机器学习、人工智能及更多相关的代码和资源。*'
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----30f7a147ad0b--------------------------------)
    [## GitHub — SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----30f7a147ad0b--------------------------------)
    [## GitHub — SalvatoreRa/tutorial: 机器学习、人工智能、数据科学的教程…'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习、人工智能、数据科学的教程，包含数学解释和可重复使用的代码（Python…
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----30f7a147ad0b--------------------------------)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----30f7a147ad0b--------------------------------)
- en: '*or you may be interested in one of my recent articles:*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*或者你可能对我最近的一篇文章感兴趣：*'
- en: '[](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5?source=post_page-----30f7a147ad0b--------------------------------)
    [## The imitation game: Taming the gap between open source and proprietary models'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5?source=post_page-----30f7a147ad0b--------------------------------)
    [## 模仿游戏：缩小开源和专有模型之间的差距'
- en: Can imitation models reach the performance of proprietary models like ChatGPT?
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模仿模型能否达到像 ChatGPT 这样的专有模型的性能？
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5?source=post_page-----30f7a147ad0b--------------------------------)
    [](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----30f7a147ad0b--------------------------------)
    [## Scaling Isn’t Everything: How Bigger Models Fail Harder'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5?source=post_page-----30f7a147ad0b--------------------------------)
    [](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----30f7a147ad0b--------------------------------)
    [## 扩展并不是一切：更大模型失败得更惨'
- en: Are Large Language Models really understanding programming languages?
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型真的理解编程语言吗？
- en: 'salvatore-raieli.medium.com](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----30f7a147ad0b--------------------------------)
    [](https://levelup.gitconnected.com/metas-lima-maria-kondo-s-way-for-llms-training-8411e3907fed?source=post_page-----30f7a147ad0b--------------------------------)
    [## META’S LIMA: Maria Kondo’s way for LLMs training'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[salvatore-raieli.medium.com](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----30f7a147ad0b--------------------------------)
    [](https://levelup.gitconnected.com/metas-lima-maria-kondo-s-way-for-llms-training-8411e3907fed?source=post_page-----30f7a147ad0b--------------------------------)
    [## META 的 LIMA：玛丽亚·近藤的 LLM 训练方法'
- en: Less and tidy data to create a model capable to rival ChatGPT
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更少而整洁的数据来创建一个能够与 ChatGPT 相媲美的模型
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/metas-lima-maria-kondo-s-way-for-llms-training-8411e3907fed?source=post_page-----30f7a147ad0b--------------------------------)
    [](https://levelup.gitconnected.com/is-ai-funny-maybe-a-bit-fd5183f68779?source=post_page-----30f7a147ad0b--------------------------------)
    [## Is AI funny? Maybe, a Bit
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/metas-lima-maria-kondo-s-way-for-llms-training-8411e3907fed?source=post_page-----30f7a147ad0b--------------------------------)
    [](https://levelup.gitconnected.com/is-ai-funny-maybe-a-bit-fd5183f68779?source=post_page-----30f7a147ad0b--------------------------------)
    [## 人工智能有趣吗？也许，稍微有点'
- en: Why AI is still struggling with humor and why this an important step
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么人工智能仍然在幽默方面挣扎，以及这为什么是一个重要步骤
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/is-ai-funny-maybe-a-bit-fd5183f68779?source=post_page-----30f7a147ad0b--------------------------------)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/is-ai-funny-maybe-a-bit-fd5183f68779?source=post_page-----30f7a147ad0b--------------------------------)'
- en: References
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Here is the list of the principal references I consulted to write this article,
    only the first name for an article is cited.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我撰写本文时参考的主要文献列表，仅列出文章的第一个作者。
- en: 'Chaitanya Ryali et al, 2023, Hiera: A Hierarchical Vision Transformer without
    the Bells-and-Whistles, [link](https://arxiv.org/abs/2306.00989)'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Chaitanya Ryali 等，2023年，《Hiera: A Hierarchical Vision Transformer without the
    Bells-and-Whistles》，[链接](https://arxiv.org/abs/2306.00989)'
- en: 'Peng Gao et al, 2022, MCMAE: Masked Convolution Meets Masked Autoencoders,
    [link](https://openreview.net/forum?id=qm5LpHyyOUO)'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Peng Gao 等，2022年，《MCMAE: Masked Convolution Meets Masked Autoencoders》，[链接](https://openreview.net/forum?id=qm5LpHyyOUO)'
- en: 'Xiang Li et al, 2022, Uniform Masking: Enabling MAE Pre-training for Pyramid-based
    Vision Transformers with Locality, link'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Xiang Li 等，2022年，《Uniform Masking: Enabling MAE Pre-training for Pyramid-based
    Vision Transformers with Locality》，链接'
- en: 'Zhenda Xie et al, 2022, SimMIM: A Simple Framework for Masked Image Modeling,
    [link](https://arxiv.org/abs/2111.09886)'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Zhenda Xie 等，2022年，《SimMIM: A Simple Framework for Masked Image Modeling》，[链接](https://arxiv.org/abs/2111.09886)'
- en: 'Ze Liu et al, 2021, Swin Transformer: Hierarchical Vision Transformer using
    Shifted Windows, [link](https://arxiv.org/abs/2205.10063)'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Ze Liu 等，2021年，《Swin Transformer: Hierarchical Vision Transformer using Shifted
    Windows》，[链接](https://arxiv.org/abs/2205.10063)'
- en: Haoqi Fan et al, 2021, Multiscale Vision Transformers, [link](https://arxiv.org/abs/2104.11227)
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Haoqi Fan 等，2021年，《Multiscale Vision Transformers》，[链接](https://arxiv.org/abs/2104.11227)
- en: Kaiming He et al, 2021, Masked Autoencoders Are Scalable Vision Learners, [link](https://arxiv.org/abs/2111.06377)
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kaiming He 等，2021年，《Masked Autoencoders Are Scalable Vision Learners》，[链接](https://arxiv.org/abs/2111.06377)
- en: Chen Wei et al, 2021, Masked Feature Prediction for Self-Supervised Visual Pre-Training,
    [link](https://arxiv.org/abs/2112.09133)
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chen Wei 等人, 2021, 《自监督视觉预训练的掩码特征预测》，[链接](https://arxiv.org/abs/2112.09133)
- en: 'Alexey Dosovitskiy et al, 2020, An Image is Worth 16x16 Words: Transformers
    for Image Recognition at Scale, [link](https://arxiv.org/abs/2010.11929)'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alexey Dosovitskiy 等人, 2020, 《一张图像价值 16x16 个词：用于大规模图像识别的变换器》，[链接](https://arxiv.org/abs/2010.11929)
- en: Ashish Vaswani et al, 2017, Attention Is All You Need. [link](https://arxiv.org/abs/1706.03762)
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ashish Vaswani 等人, 2017, 《注意力机制就是你所需的》，[链接](https://arxiv.org/abs/1706.03762)
- en: Kaiming He et al, 2015, Deep Residual Learning for Image Recognition, [link](https://arxiv.org/abs/1512.03385)
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kaiming He 等人, 2015, 《用于图像识别的深度残差学习》，[链接](https://arxiv.org/abs/1512.03385)
- en: Wei Yu et al, 2014, Visualizing and Comparing Convolutional Neural Networks.
    [link](https://arxiv.org/abs/1412.6631)
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wei Yu 等人, 2014, 《可视化和比较卷积神经网络》，[链接](https://arxiv.org/abs/1412.6631)
- en: Karen Simonyan et al, 2014, Very Deep Convolutional Networks for Large-Scale
    Image Recognition, [link](https://arxiv.org/abs/1409.1556)
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karen Simonyan 等人, 2014, 《用于大规模图像识别的非常深的卷积网络》，[链接](https://arxiv.org/abs/1409.1556)
- en: Why Do We Have Huge Language Models and Small Vision Transformers?, TDS, [link](/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6)
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们有大型语言模型而小型视觉变换器？，TDS，[链接](/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6)
- en: A Visual Journey in What Vision-Transformers See, TowardsAI, [link](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4)
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Vision-Transformers 看到的视觉之旅，TowardsAI，[链接](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4)
- en: Vision Transformer, paperswithcode, [link](https://paperswithcode.com/method/vision-transformer)
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vision Transformer，paperswithcode，[链接](https://paperswithcode.com/method/vision-transformer)
