- en: Gradient Boosting from Theory to Practice (Part 2)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gradient-boosting-from-theory-to-practice-part-2-25c8b7ca566b](https://towardsdatascience.com/gradient-boosting-from-theory-to-practice-part-2-25c8b7ca566b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Use the gradient boosting classes in Scikit-Learn to solve different classification
    and regression problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roiyeho?source=post_page-----25c8b7ca566b--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----25c8b7ca566b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----25c8b7ca566b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----25c8b7ca566b--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----25c8b7ca566b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----25c8b7ca566b--------------------------------)
    ·12 min read·Jul 19, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d549d077e8ed878675b861b84fcdffb5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Luca Bravo](https://unsplash.com/@lucabravo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/wallpapers/nature/forest?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: In the [first part](https://medium.com/towards-data-science/gradient-boosting-from-theory-to-practice-part-1-940b2c9d8050)
    of this article, we presented the gradient boosting algorithm and showed its implementation
    in pseudocode.
  prefs: []
  type: TYPE_NORMAL
- en: In this part of the article, we will explore the classes in Scikit-Learn that
    implement this algorithm, discuss their various parameters, and demonstrate how
    to use them to solve several classification and regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: Although the [XGBoost](/xgboost-the-definitive-guide-part-1-cc24d2dcd87a) library
    provides a more optimized and highly scalable implementation of gradient boosting,
    for small to medium-sized data sets it is often easier to use the gradient boosting
    classes in Scikit-Learn, which have a simpler interface and a significantly fewer
    number of hyperparameters to tune.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosting in Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides the following classes that implement the gradient-boosted
    decision trees (GBDT) model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)
    is used for classification problems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)
    is used for regression problems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In addition to the standard parameters of [decision trees](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369),
    such as *criterion, max_depth* (set by default to 3)and *min_samples_split*, these
    classes provide the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*loss* — the loss function to be optimized. In ` GradientBoostingClassifier,
    this function can be `''log_loss''`(the default) or `''exponential''` (which will
    make gradient boosting behave like [AdaBoost](https://medium.com/@roiyeho/adaboost-illustrated-3084183a2086)).
    In GradientBoostingRegressor, this function can be `''squared_error''` (the default),
    `''absolute_error''`, `''huber''`, or `''quantile’` (see [this article](https://medium.com/towards-data-science/loss-functions-in-machine-learning-9977e810ac02)
    for the differences between these loss functions).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*n_estimators* — the number of boosting iterations (defaults to 100).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*learning_rate* — a factor that shrinks the contribution of each tree (defaults
    to 0.1).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*subsample* — the fraction of samples to use for training each tree (defaults
    to 1.0).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*max_features* — the number of features to consider when searching for the
    best split in each node. The options are to specify an integer for the number
    of features, a floating point to specify a fraction of the features to use, ‘sqrt’
    for using a square root of the features, ‘log2’ for using a log2 of the features,
    and None for using all the features (the default).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*validation_fraction* — a fraction of the training set that will be used as
    a validation set for early stopping (defaults to 0.1).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*n_iter_no_change* — terminate training when the validation score has not improved
    in the previous *n_iter_no_change* iterations by at least *tol* (defaults to 0.0001).
    By default, *n_iter_no_change* is set to None, which means that early stopping
    is disabled.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The gradient boosting estimators also have the following attributes, which
    are learned from the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n_estimators_* — the number of fitted trees as determined by early stopping
    (if specified, otherwise it is set to *n_estimators*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*estimators*_ — the collection of fitted trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*feature_importances*_ — the feature importances estimated by the model (will
    be discussed later in this article).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*oob_scores*_ — the loss values of the out-of-bag samples at every iteration
    (only available if *subsample* < 1.0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*train_score*_ — the loss values on the training set at every iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GradientBoostingClassifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For example, let’s fit a gradient boosting classifier to the [Iris data set](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html),
    using only the first two features of each flower (sepal width and sepal length).
    As a reminder, with [random forests](https://medium.com/@roiyeho/random-forests-98892261dc49)
    we were able to obtain a test accuracy of 81.58% on this data set (after hyperparameter
    tuning). Let’s see if we can do better with gradient boosting.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first load the data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we split it into training and test sets (using the same random seed as
    in previous experiments):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now create a GradientBoostingClassifier model with its default settings
    (i.e., an ensemble of 100 trees with max_depth=3), and fit it to the training
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We fix the random state of the gradient boosting classifier in order to allow
    reproducibility of the results. The model’s performance is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: These are the same results we have obtained with random forests before hyperparameter
    tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s run a randomized search on some of the gradient boosting hyperparameters
    in order to find a better model. For a fair comparison, we use the same number
    of search iterations as we did with random forests (50).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The best model found by the randomized search is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'That is, the best model consists of 10 decision trees with max_depth = 3, where
    each tree is trained on a random subsample of 60% of the training set. The accuracy
    of this model on the training and test sets is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy on the test set is significantly higher than the one we have obtained
    with random forest after tuning (86.84% compared to 81.58%).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the decision boundaries found by this classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69ae522fb1c8be0476b3a6dcdf18e0a5.png)'
  prefs: []
  type: TYPE_IMG
- en: The decision boundaries found by the gradient boosting classifier on the Iris
    data set
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the decision boundaries found by the random forest classifier, we
    can see that the gradient boosting classifier is able to capture a larger area
    of the versicolor flowers without overfitting to the outliers.
  prefs: []
  type: TYPE_NORMAL
- en: GradientBoostingRegressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For demonstrating the gradient boosting regressor, we will use the [California
    housing data set](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing).
    The goal in this data set is to predict the median house value of a given district
    (house block) in California, based on 8 different features of that district (such
    as the median income or the average number of rooms per household).
  prefs: []
  type: TYPE_NORMAL
- en: 'We first fetch the data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we split the data set into 80% training and 20% test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we fit a GradientBoostingRegressor with its default settings (i.e., an
    ensemble of 100 trees with max_depth=3) to the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The *R*² scores of this model are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This is a slightly worse result than the one we have obtained with random forests
    before tuning (*R*² score on test = 0.798). However, notice that by default the
    trees in RandomForestRegressor are not pruned (their maximum depth is not limited),
    whereas the default maximum depth of the trees in GradientBoostingRegressor is
    only 3, while the default number of trees in both estimators is the same (100).
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s tune the hyperparameters of the gradient boosting regressor by running
    the following randomized search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The best model found by the search is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: That is, the best model uses 500 trees with a maximum depth of 7, where each
    tree is trained on a random subsample of 80% of the training set using a logarithmic
    number of the features in each node split.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *R*² scores of this model on the training and test sets are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The *R*² score on the test set is significantly higher than the one obtained
    by the random forest regressor after tuning (0.8166).
  prefs: []
  type: TYPE_NORMAL
- en: The Learning Curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also plot the training and test errors in every boosting iteration. The
    training errors are stored in the *train_score_* attribute of the estimator. The
    test errors can be obtained by calling the *staged_predict*() method, which returns
    a generator that yields the model predictions on a given data set at each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c92ac457269c4b8f01540b546004cddb.png)'
  prefs: []
  type: TYPE_IMG
- en: The learning curve of gradient boosting regressor on the California housing
    data set
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the minimum test error is reached after about 100 iterations,
    i.e., the optimal number of trees for this data set is around 100\. Moreover,
    the test error remains stable as we add more trees to the ensemble, which suggests
    that the model is less susceptible to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to find the optimal number of trees is to use **early stopping**.
    Let’s run the same randomized search, but instead of varying the number of estimators,
    we will set it to a fixed number of 500, and enable early stopping by setting
    *n_iter_no_change* to 5\. This automatically sets aside 10% of the training set
    for validation, and terminates the training once the validation score does not
    improve for 5 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This estimator has a slightly worse accuracy on the test set than the previous
    one, which can explained by the randomization of the search and the fact that
    it used only 90% of the training set for building the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check how many trees were actually built before early stopping was activated
    by inspecting the *n_estimators_* attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Feature Importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to other tree-based ensemble methods, gradient-boosted trees can provide
    an estimate for the importance of the features in the data set, i.e., how much
    each feature contributes to the model’s predictions. This can be useful for model
    interpretation as well as for performing feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of a feature in a single decision tree is determined by the location
    where it is used in the tree (features located at the top of the tree contribute
    more to the tree’s predictions) and the reduction in node impurity achieved by
    using this feature to split the node. In tree-based ensemble methods, such as
    random forests and gradient-boosted trees, we average the feature importance over
    all the trees in the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can plot the importance of the features in the California housing
    data set as found by our gradient boosting regressor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9be44d4793192a1d53141f62437256d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Importance of the features in the California housing data set
  prefs: []
  type: TYPE_NORMAL
- en: The most important features in this data set are MedInc (median income), the
    house location (Longitude and Latitude), and AveOccup (average number of household
    members).
  prefs: []
  type: TYPE_NORMAL
- en: Histogram-Based Gradient Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-Learn 0.21 has introduced two histogram-based implementations of gradient
    boosting: [HistGradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)
    and [HistGradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html),
    which are similar to the histogram-based algorithm used in LightGBM [1].'
  prefs: []
  type: TYPE_NORMAL
- en: These estimators first discretize the continuous features in the data set into
    integer-valued bins (255 bins by default). During training, these bins are used
    to construct feature histograms from the values of the samples that reached a
    specific node in the tree. The best split point at that node is then found based
    on these histograms.
  prefs: []
  type: TYPE_NORMAL
- en: 'This discretization has the following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: It significantly reduces the number of splitting points to consider at each
    node in the tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It avoids the need to sort the values of the continuous features at every node
    (see the section “Handling Continuous Features” in [this article](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369)
    to understand why sorting is needed from the first place).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In addition, many parts in the implementation of the histogram-based estimators
    are parallelized. For example, the gradient computations are parallelized over
    samples, and finding the best split point at a node is parallelized over features.
  prefs: []
  type: TYPE_NORMAL
- en: The binning and parallelization together allow the histogram-based estimators
    to run much faster than the standard gradient boosting estimators when the number
    of samples is large (*n* > 10,000).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the histogram-based estimators have built-in support for missing
    values and categorical features, which avoids the need for using an imputer or
    a one-hot encoder when preprocessing the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the parameters of the histogram-based estimators are the same as GradientBoostingClassifier
    and GradientBoostingRegressor, with the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: The parameter for the number of estimators is called *max_iter* instead of *n_estimators*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The defaults for the tree size have been modified: *max_depth* is set by default
    to None (instead of 3), while *max_leaf_nodes* is set to 31, and *min_samples_leaf*
    is set to 20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Early stopping is automatically enabled when the number of samples is greater
    than 10,000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, the following parameters were added:'
  prefs: []
  type: TYPE_NORMAL
- en: '*max_bins* indicates the maximum number of bins to use. Must be no larger than
    255\. One additional bin is reserved for missing values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*categorical_features* is a list of integers that indicate the locations of
    the categorical features in the data set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*interaction_cst* specifies interaction constraints, i.e., the sets of features
    which can interact with each other in child node splits (defaults to None).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HistGradientBoostingClassifier Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For example, let’s compare the performance of HistGradientBoostingClassifier
    and GradientBoostingClassifier on an artificially generated data set.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the function [make_hastie_10_2](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_hastie_10_2.html#sklearn.datasets.make_hastie_10_2)
    from Scikit-Learn, which generates a binary, 10-dimensional classification data
    set, the same one that was used in Hastie et al. [2], Example 10.2.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data set consists of 10 features that follow standard Gaussian distribution,
    and the target is a binary label defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5048cf864774f3284bc35ce66e7bed9c.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, the negative class lies within a 10-dimensional sphere whose radius
    is 9.34.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first generate this data set with 50,000 samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we split it to 80% training set and 20% test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now train a GradientBoostingClassifier on this data set and measure its
    training time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes 12.6 seconds on average to train this model. Its performance on the
    training and test sets is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now train a HistGradientBoostingClassifier on the same data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes only 1.53 seconds on average to train this model (more than 8 times
    faster than GradientBoostingClassifier). Its performance on the training and test
    sets is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy on the test set is significantly better (94.67% instead of 92.31%).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s summarize the pros and cons of gradient boosting as compared to other
    supervised learning models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**:'
  prefs: []
  type: TYPE_NORMAL
- en: Provides highly accurate models and often the best performing models on structured
    data sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can capture complex interactions and patterns in the data set by combining multiple
    weak models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can effectively handle high-dimensional data sets by automatically selecting
    the relevant features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less sensitive to outliers compared to other models, as each base model learns
    from the residuals of the previous models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle heterogeneous data types, including numerical and categorical features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle missing values without requiring imputation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides a measure of feature importance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be applied to both regression and classification tasks, and supports a wide
    range of loss functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**:'
  prefs: []
  type: TYPE_NORMAL
- en: Training can be computationally intensive, especially when dealing with large
    data sets or when the ensemble has many trees. In addition, the training of the
    base models cannot be parallelized (unlike random forests for example).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less interpretable than simpler models such as decision trees or linear regression,
    since an interpretation of the model’s decision requires following the paths of
    many trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several hyperparameters need to be tuned, including the number of trees, the
    size of each tree and the learning rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can overfit the training set if not properly regularized or if the number of
    boosting iterations is too high.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction can be slower compared to other models, as it requires traversing
    multiple trees and aggregating their predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final Notes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the images are by the author unless stated otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/gradient_boosting](https://github.com/roiyeho/medium/tree/main/gradient_boosting)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Iris data set info:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Citation**: Fisher, R. A. (1988). Iris. UCI Machine Learning Repository.
    [https://doi.org/10.24432/C56C76.](https://doi.org/10.24432/C56C76.)'
  prefs: []
  type: TYPE_NORMAL
- en: '**License**: Creative Commons CC BY 4.0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'California housing data set info:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Citation**: Pace, R. Kelley and Ronald Barry (1997), Sparse Spatial Autoregressions,
    Statistics and Probability Letters, 33, 291-297.'
  prefs: []
  type: TYPE_NORMAL
- en: '**License**: Creative Commons CC0: Public Domain.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Ke et. al. (2017), [“LightGBM: A Highly Efficient Gradient BoostingDecision
    Tree”](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] T. Hastie, R. Tibshirani and J. Friedman (2009), “Elements of Statistical
    Learning Ed. 2”, Springer.'
  prefs: []
  type: TYPE_NORMAL
