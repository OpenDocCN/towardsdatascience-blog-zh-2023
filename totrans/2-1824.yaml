- en: 'Segment Anything 3D for Point Clouds: Complete Guide (SAM 3D)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/segment-anything-3d-for-point-clouds-complete-guide-sam-3d-80c06be99a18](https://towardsdatascience.com/segment-anything-3d-for-point-clouds-complete-guide-sam-3d-80c06be99a18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3D Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How to build a semantic segmentation application for 3D point clouds leveraging
    SAM and Python. Bonus: code for projections and relationships between 3D points
    and 2D pixels.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)[![Florent
    Poux, Ph.D.](../Images/74df1e559b2edefba71ffd0d1294a251.png)](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)[](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)
    [Florent Poux, Ph.D.](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)
    ¬∑27 min read¬∑Dec 13, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cbaf694227e7e6b933f01064d27a96d6.png)'
  prefs: []
  type: TYPE_IMG
- en: The Segment Anything Model for 3D Environments. We will detect objects in indoor
    spaces using 3D point cloud datasets. Credit goes to [Mimatelier](https://linktr.ee/mimatelier),
    the talented illustrator who created this image.
  prefs: []
  type: TYPE_NORMAL
- en: Technological leaps are just plain crazy, especially looking at Artificial Intelligence
    (AI) applied to 3D challenges. Having the ability to leverage the latest cutting-edge
    research for advanced 3D applications is very empowering. Especially when looking
    at bringing human-level reasoning capabilities to a computer, there is a clear
    need to extract a formalized meaning from the 3D entities that we observe.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we are here to make sure that we can bind amazing AI advancements
    with 3D applications that make use of 3D Point Clouds. ‚Äî *üê≤* **Florent & Ville**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is no easy feat, but once mastered, the fusion of 3D point clouds and deep
    learning gives birth to new dimensions of understanding and interpreting our visual
    world.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46849f11c579adff452243fd483b5eef.png)'
  prefs: []
  type: TYPE_IMG
- en: Artificial Intelligence ft. 3D point clouds. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: Among these advancements, the Segment Anything Model is a recent beacon of innovation,
    especially for full automation without supervision.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b49832a749ed53fb71db522ff96117b.png)'
  prefs: []
  type: TYPE_IMG
- en: The Segment Anything Model Architecture that we use for 3D Data. It comprises
    an image encoder, image embeddings, and some pre-processing operations to finally
    pass into the decoder and prompt encoder, giving the results as masks. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: In this ultimate guide, we embark on a pragmatic voyage to explore this cutting-edge
    model, from its inception to its practical segmentation applications. But what
    is the objective here?
  prefs: []
  type: TYPE_NORMAL
- en: The Mission ü•∑
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Okay, it‚Äôs time for the mission brief! You are now a multi-class member of
    your country‚Äôs special forces, and you must find some dangerous materials hidden
    inside a specific building without ever being detected (here: the ITC building).'
  prefs: []
  type: TYPE_NORMAL
- en: With your superb internet hacking skills, you manage to find the 3D scans for
    that part of the building you are interested in. You now need to find a way to
    define the path for your dangerous material recovery team quickly. After that,
    the team can proceed unnoticed to recover the materials, and you have saved the
    day!
  prefs: []
  type: TYPE_NORMAL
- en: After careful research and using your various skills, you develop a 3D data
    processing workflow that involves setting up a 3D Python code environment to process
    the 3D point cloud by using the Segment Anything Model to highlight the composition
    of the scene, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/640db9a177c74e1f31baa1f69ea491f6.png)'
  prefs: []
  type: TYPE_IMG
- en: The workflow for Segment Anything 3D. We have five main steps (3D Project Setup,
    Segment Anything Model, 3D Point Cloud Projections, Unsupervised Segmentation,
    and Qualitative Analysis) further refined in the substeps, as highlighted. ¬© [F.
    Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: This will allow you to produce a 3D semantic map that will permit pinpointing
    the location of the materials within ninety minutes before the team is on-site.
    Are you ready?
  prefs: []
  type: TYPE_NORMAL
- en: 'üéµ**Note to Readers***: This hands-on guide is part of a* [***UTWENTE***](https://www.itc.nl/)
    *joint work with co-authors* ***F. Poux*** *and* ***V. Lehtola****. We acknowledge
    the financial contribution from the digital twins* [*@ITC*](http://twitter.com/ITC)
    *-project granted by the ITC faculty of the University of Twente.*'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. 3D Project Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive into the marvels of the Segment Anything Model, it is crucial
    to establish a robust foundation. Setting up the appropriate environment ensures
    smooth sailing throughout our journey, allowing seamless experimentation and exploration.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, we want to ensure that our coding environment is correctly set
    with robust libraries. Ready?
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§† **Ville**: *This is before the action starts. Please reserve an hour or two
    separately for this if you are doing it from scratch and, e.g., might need to
    update CUDA drivers. You‚Äôll be downloading gigabytes of stuff.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dce37818ba43403acd021f11131b0685.png)'
  prefs: []
  type: TYPE_IMG
- en: The 3D Project Setup. We first set up the environment, on which we attach base
    libraries, deep learning libraries, and the IDE Setup. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: 1.1\. 3D Code Environment setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is time to get our hands in the dirt! We aim to use the Segment Anything
    Model to Semantically Segment a 3D Point Cloud. And that is no easy feat! So,
    of course, the first reflex is to check out the segment anything dependencies:
    [Access SAM Github](https://github.com/facebookresearch/segment-anything).'
  prefs: []
  type: TYPE_NORMAL
- en: 'From there, we check out the necessary pre-requisites of the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9dd556baea6fef87e6c02751f663835a.png)'
  prefs: []
  type: TYPE_IMG
- en: The dependencies highlighted in Segment Anything.
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ä **Florent**: *Whenever you are dealing with deep learning libraries or deep
    learning new research code, it is essential to check out the dependencies and
    installation recommendations. Indeed, this will strongly influence the follow-up
    of your experiments and the time needed for replication.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we need to use the following library version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that this is out, we will generate a virtual environment to ensure smooth
    sailing! If you want a detailed view of the process, I recommend you jump aboard
    the following guide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Python Workflows for LiDAR City Models: A Step-by-Step Guide'
  prefs: []
  type: TYPE_NORMAL
- en: The Ultimate Guide to unlocking a streamlined workflow for 3D City Modelling
    Applications. The tutorial covers Python‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----80c06be99a18--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: But, not to keep you high and dry, here is another strategy for a quick and
    lightweight setup using [Miniconda](https://docs.conda.io/projects/miniconda/en/latest/miniconda-install.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'üí° **Note**: *Miniconda is a free minimal installer for conda. It is a ‚Äúminiature‚Äù
    version of Anaconda that includes only a minimal amount of dependencies. These
    are the conda package manager, a Python version, the packages they both depend
    on and other valuable packages like pip and zlib. This allows us only to install
    what we need in a lightweight manner.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§†**Ville***: The cool stuff about virtual environments is that you can export
    it and run your code as-is on powerful Linux computing machines and superclusters!
    This is very handy for training networks!*'
  prefs: []
  type: TYPE_NORMAL
- en: After downloading a version of Miniconda from [here](https://docs.conda.io/projects/miniconda/en/latest/miniconda-other-installer-links.html)
    for your OS (I recommend you choose a Python 3.9 or 3.10 version to ensure proper
    compatibility with packages), you can install it following the various steps of
    the installation process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f94957e88eef5951bad35f2d5ceaa61.png)'
  prefs: []
  type: TYPE_IMG
- en: The miniconda installer window. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: 'And that is it! You now have secured the most uncomplicated Python installation
    with the lightweight miniconda that will make isolating a controlled virtual environment
    super easy. Before moving on to the following steps, we launch miniconda with
    its command line access:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df2bb60a544049eca1139b2b848577df.png)'
  prefs: []
  type: TYPE_IMG
- en: In Windows, just searching ‚Äúminiconda‚Äù should yield this
  prefs: []
  type: TYPE_NORMAL
- en: Once in the Anaconda Prompt, we follow a simple four-step process to be up and
    running, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1cdedcd8fdd91ee19cf54b3a8cb8ca12.png)'
  prefs: []
  type: TYPE_IMG
- en: The Workflow to set up a Python environment for 3D Segment Anything Model. ¬©
    [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new environment, we write the line: `conda create -n GEOSAM python=3.10`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To switch to the newly created environment, we write: `conda activate GEOSAM`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To check the Python version, `python --version`, and the installed packages:
    `conda list`. This should yield Python 3.10 and the list of base libraries, respectively.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To install pip in the new environment, we write: `conda install pip`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And that is it! We are now ready to move on installing the necessary libraries
    for playing with SAM.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Innovator Newsletter'
  prefs: []
  type: TYPE_NORMAL
- en: Weekly practical content, insights, code and resources to master 3D Data Science.
    I write about Point Clouds, AI‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Base library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/06b0547b42b6f690713e39f65c151892.png)'
  prefs: []
  type: TYPE_IMG
- en: The base libraries used in this tutorial (Numpy, Matplotlib, Laspy). ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: 'We now install our base libraries for using SAM: `NumPy`, `LasPy`, `OpenCV`,
    and `Matplotlib`. `NumPy` may be the most recommended library for numerical computations,
    `OpenCV` is used for computer vision tasks, `Laspy` deals with processing LIDAR
    data, and `Matplotlib` is a plotting and data visualization library in Python.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ä **Florent**: *These libraries are the base and robust cornerstones of any
    3D project. If you want to deepen their understanding, I suggest you dive into*
    [*this tutorial*](https://medium.com/towards-data-science/how-to-automate-lidar-point-cloud-processing-with-python-a027454a536c)
    *that explores its dark depths* ü™∏.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install these libraries, we can use pip in one single line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This is great; it's time for the deep learning libraries setup!
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Deep Learning Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/50677cd193ffb1d9e37ce1fb9b27b84e.png)'
  prefs: []
  type: TYPE_IMG
- en: The deep learning libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now look into installing deep-learning libraries. And, of course, the
    first one that we explore is my favorite one so far: Pytorch. Since its launch
    in 2017, Pytorch has improved its flexibility and hackability as a priority and
    performance as a close second. Therefore, today, using Pytorch for Deep Learning
    applications is excellent if you want (1) high-performance execution, (2) Pythonic
    internals, and (3) good abstractions for valuable tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ä **Florent**: *Since 2017, Hardware accelerators (such as GPUs) have become
    ~15x faster in computing tasks. You can only guess what is to come in the next
    few years. Therefore, it is essential to be on the lookout for flexible libraries
    that can move quickly, even on refactoring ‚Äúinternals‚Äù to languages such as C++,
    like Pytorch does.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§†**Ville***: SAM authors recommend using a GPU with 8GB memory. However, we
    give some tips on how to do the tutorial with less memory. Use them if you get
    ‚ÄòMemoryError‚Äô or ‚ÄòOut-of-bounds memory access‚Äô or ‚ÄòIllegal memory access‚Äô messages.
    I got it working with 6GB.*'
  prefs: []
  type: TYPE_NORMAL
- en: To install a relevant distribution of Pytorch without headaches figuring out
    how to install CUDA (which is not so straightforward), they made a straightforward
    web app that generates the code to copy and paste into your command line. For
    this, you can jump on this [Pytorch Getting Started page](https://pytorch.org/get-started/locally/)
    and select the most relevant way to install your distribution, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8cda5c93dd1245d31400555bddb4216.png)'
  prefs: []
  type: TYPE_IMG
- en: How to install Pytorch for your OS and configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'üí° **Note**: *We want to leverage our GPU. Therefore, it is essential to note
    that we want an installation with CUDA. But this is possible only if you have
    a Nvidia GPU at the time of writing. If not, you may want to use the CPU or switch
    to a Cloud computing service such as Google Colab.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, our code line is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This line will trigger the retrieval and installation of the necessary elements
    for Pytorch to function coherently.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7bb2a5f949df042d424767ce6448a145.png)'
  prefs: []
  type: TYPE_IMG
- en: The installation of Pytorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second deep-learning library that we want to use is Segment Anything. While
    Pytorch is being installed, we can download and install ‚Äúsoftware‚Äù that will make
    it easier for us to manage versions and access online libraries. This is **Git**
    and is accessible here: [Git Website](https://git-scm.com/download/win).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download and install git, and once the installation is finished, Pytorch
    should also be nicely installed in your environment. Therefore, to install segment-anything,
    we can write the line below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This will again take some time until you get such a message below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04e42602deac265942e73e53219ee11f.png)'
  prefs: []
  type: TYPE_IMG
- en: The CLI results of installing Pytorch.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, we have the base libraries as well as the deep learning libraries
    installed. Before using them, let us install an IDE to use everything smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Setting up an IDE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/53e45018ba59d7fa3a7dc0a1622bff21.png)'
  prefs: []
  type: TYPE_IMG
- en: The Jupyter lab IDE after installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step of our setup is to install an IDE. We are still in the command
    line interface within the environment, and we type: `pip install jupyterlab`,
    which will install jupyterlab on our environment.'
  prefs: []
  type: TYPE_NORMAL
- en: To use it in a defined local folder, we can first create a parent directory
    for our project (let us call it `SAM`), which will hold both a `CODE` folder and
    a `DATA` Folder. Once this is done, in the console, we change our pointer to the
    created directory by writing. `cd C://COURSES/POUX/SAM`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We launch jupyterlab from this location by typing in the console: `jupyter
    lab`, which will open a new localhost page in your web browser (Chrome, Firefox,
    or Safari).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Jupyter, you can create a notebook (.ipynb) and write in the first cell
    of the import statements to use all the installed packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Alright! We are all set up. Before getting on the other steps of coding our
    model, now is just the time to retrieve our 3D Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. 3D Dataset Curation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In previous tutorials, we illustrated point cloud processing and meshing over
    several 3D datasets, some of which use aerial LiDAR from the AHN4 LiDAR Campaign.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Deep Learning Python Tutorial: PointNet Data Preparation'
  prefs: []
  type: TYPE_NORMAL
- en: The Ultimate Python Guide to structure large LiDAR point cloud for training
    a 3D Deep Learning Semantic Segmentation‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we will use a dataset gathered using a Terrestrial Laser Scanner:
    the ITC of the University of Twente''s new 2023 building, as shown below. It consists
    of an indoor green area with nice tricky foliage to assess after running the segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29970d4d0fb77f18812b960e2601f444.png)'
  prefs: []
  type: TYPE_IMG
- en: The 3D Point Cloud of the ITC UTwente new building, with its indoor ‚Äújungle‚Äù.
    ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the data from the Drive Folder here: [Guide Datasets (Google
    Drive)](https://drive.google.com/drive/folders/1pIaP-vJAWh8cFtk9zQ3poxcR5DuEIkmt?usp=sharing),
    and put it in your folder that holds datasets (in my case, ‚ÄúDATA‚Äù).'
  prefs: []
  type: TYPE_NORMAL
- en: At this process stage, we have a nice warm coding setup, with all the necessary
    libraries in a lightweight, isolated GEOSAM conda environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ä **Florent**: Great job so far! If you are eager to run some tests to check
    that Pytorch is working as it should, i.e., CUDA is recognized, you can write
    these lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/06b2a254f99419bdf2ab27549acbfb2e.png)'
  prefs: []
  type: TYPE_IMG
- en: My configuration from the print results above.
  prefs: []
  type: TYPE_NORMAL
- en: It is now time to segment stuff!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Innovator Newsletter'
  prefs: []
  type: TYPE_NORMAL
- en: Weekly practical content, insights, code and resources to master 3D Data Science.
    I write about Point Clouds, AI‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Setting up the Segment Anything Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the heart of our little adventure lies the Segment Anything Model, a powerful
    creation with excellent potential for 3D point cloud semantic segmentation. With
    its innovative architecture and training process, this model is the perfect candidate
    to be tested on indoor applications. Let us first play around with its core concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Segment Anything Basics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MetaAI has delved into the fascinating realm of Natural Language Processing
    (NLP) and computer vision with their Segment Anything Model, which enables **zero-shot**
    and **few-shot learning** on novel datasets and tasks using foundation models.
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ä **Florent**: *Okay, there are a lot of swear words, I admit. For clarity
    concerns, here is my tentative to summarize each complex terminology. Zero-shot
    learning refers to the ability to recognize something without having seen it (seen
    it zero times). Somewhat similarly, few-shot learning uses a limited number of
    labeled examples for each new class, and the goal is to make predictions for new
    classes based on just these few examples of labeled data.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§†**Ville***: Also, a so-called* ***foundation model*** *is a model that is
    trained on a lot and a lot of data at a scale. it is so large that it can be adapted
    to various tasks from different scenarios.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs break this down for you:'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the SAM ‚ÄúAI‚Äù algorithm can significantly reduce the human effort required
    for image segmentation. To do so, you provide the model with foreground/background
    points, a rough box or mask, some text, or any other input that indicates what
    you want to segment in an image. The Meta AI team has trained the Segment Anything
    Model to generate a proper segmentation mask. This mask is the model‚Äôs output
    and should be a suitable mask to delineate one of the things that the prompt might
    refer to. For instance, if you indicate a point on the roof of the house, the
    output should correctly identify whether you meant the roof or the house.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2a0375836c5937774047d2f113b288a.png)'
  prefs: []
  type: TYPE_IMG
- en: How does the Segment Anything Model (SAM) work? Explanation of the segmentation
    prompt to generate valid masks (case of a house). ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: This segmentation task can then serve for model pre-training and guiding solutions
    for various downstream segmentation problems.
  prefs: []
  type: TYPE_NORMAL
- en: On the technical side, what we call an image encoder creates a unique embedding
    (representation) for each image, and a lightweight encoder swiftly transforms
    any query into an embedding vector. These two data sources are merged using a
    (lightweight) mask decoder to predict segmentation masks, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84684c89b03497988c14b8439f88afd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Flowchart of the functioning of the Segment Anything Model. The image goes through
    the image encoder. Then it is embedded, to finally be combined after using a prompt
    followed by a prompt encoder, to generate final masks for our 3D point clouds.
    ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: 'This effective architecture, combined with a massive scale training phase,
    allows the Segment Anything Model to reach four milestones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Effortless Object Segmentation üî•**: With SAM, users can effortlessly segment
    objects by simply selecting the points they want to include or exclude from the
    segmentation. You can also use a bounding box as a cue for the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling Uncertainty üî•**: SAM is equipped to handle situations with uncertainty
    about the object to be segmented. It can generate multiple valid masks, which
    is crucial for solving real-world segmentation challenges effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic Object Detection and Masking üî•**: SAM makes automatic object detection
    and masking a breeze. It simplifies these tasks, saving you time and effort.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time Interaction üî•**: Thanks to precomputed image embeddings, SAM can
    instantly provide a segmentation mask for any prompt. This means you can have
    real-time interactions with the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that this is out of the way, are you ready to use it?
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. SAM Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The SAM model can be loaded with three different encoders: ViT-B, ViT-L, and
    ViT-H. ViT-H gives better results than ViT-B but has only marginal gains over
    ViT-L.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**ü§† Ville**: *To help with the choice, I tested ViT-B on NVIDIA GeForce GTX
    1650, 6 Gb VRAM with Win11.*'
  prefs: []
  type: TYPE_NORMAL
- en: These three encoders have different parameter counts that give a bit more freedom
    to tune an application. ViT-B (the smallest) has 91 Million parameters, ViT-L
    has 308 Million parameters, and ViT-H (the biggest) has 636 Million parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'This difference in size also influences the speed of inference, so this should
    help you decide the encoder for your specific use case. Following this guide,
    we will get with the heavy artillery: The ViT-H, with a Model Checkpoint that
    you can download from [Github](https://github.com/facebookresearch/segment-anything#model-checkpoints)
    (2.4 Gb) and place in your current parent folder, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where we can define two variables to make your code a bit more flexible
    afterward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, we can initialize our SAM model with the following two lines of
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: And we are all set up! Maybe one last step, trying to see how it performs on
    a random image that you have on your desktop?
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Performances on 2D images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us test if all works as expected on a random image. We are interested in
    geospatial applications, so I go to [Google Earth](https://earth.google.com/)
    and zoom in on a spot of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a541e6bccb901f9898489865f2107af8.png)'
  prefs: []
  type: TYPE_IMG
- en: Selection of an imagery dataset from Biscarosse. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ä **Florent**: *This spot is biased, right? Hopefully, this gives you a bit
    of French holiday vibes, which you are proud to take followed by a marvelous year
    full of exciting projects*!'
  prefs: []
  type: TYPE_NORMAL
- en: 'From there, I take a screenshot of a zone of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a887690665dfcb7368dafd20b5fb139.png)'
  prefs: []
  type: TYPE_IMG
- en: The image dataset of a zone of Biscarosse plage. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: 'and I load the image into memory with openCV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ö **Note**: *As you can see, by default, OpenCV loads an image by switching
    to Blue, Green, and Red channels (BGR) that we order as RGB with the second line
    and store in the* `image_rgb` *variable.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it is time for us to apply SAM on the image with two lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In around 6 seconds, this returns us a list filled with dictionaries, each
    representing a mask for a specific object automatically extracted, accompanied
    by its scores and metadata. For a detailed view, the result is a list of dictionaries
    where each `dict` holds the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '`segmentation` : this brings out the mask with `(W, H)` shape (and `bool` type),
    where `W` (width) and `H` (height) target the original image dimensions;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`area` : this is the area of the mask expressed in pixels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bbox` : this is the boundary box detection in `xywh` format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predicted_iou` : the model''s prediction IoU metric for the quality of the
    mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`point_coords` : This is a list of the sampled input points that were used
    to generate the mask'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stability_score` : The stability score is an additional measure of the mask
    quality. Check out the paper for more details üòâ'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_box` : this is a list of the crop_boxe coordinates used to generate this
    mask in `xywh` format (it may differ from the Bounding-Box)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that you have a better idea about what we are dealing with, to check out
    the results, we can plot the masks on top of the image with the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'ü¶ä **Florent**: *I admit, this is a bit blunt. But what happens in this function
    is that I will sort out the masks by their area to plot them with a random color
    on top of the image with a transparency parameter*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§†**Ville***: Memory errors can ruin French holiday vibes! Remember the Google
    Colab option too! If rebooting does not solve the issue and allocated memory is
    too high, the following piece of code clears the GPU memory of extra allocations.
    Use it to address memory problems.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*If the GPU memory is not freed enough, try rebooting your (Windows) computer.
    Also, try using the following line if memory problems persist:* `mask_generator
    = SamAutomaticMaskGenerator(sam, points_per_batch=16)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to plot and export the image, we write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Which results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77cc7cf3fcc62d5fdab3af87a5895117.png)![](../Images/8913306a437571893ac027bede9a81d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Before and after the Segment Anything Model. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: So, already at this stage, we have exciting results, and SAM is working really
    nicely! For example, you can see that almost all roofs are part of segments and
    that the three pools (2 blue and one green) are also part of segments. Therefore,
    this could well be a starting point for complete automatic detection
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ä **Florent**: *You may run into Memory Errors depending on your computer setup
    while plotting the masks. In this case, loading a lighter SAM model should solve
    your problem.* üòâ'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a working SAM setup, let us apply all this hard-earned know-how
    to 3D point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. 3D Point Cloud to Image Projections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make sense of the complex 3D world, we delve into the art of point cloud
    projection. Through techniques like ortho and spherical projections, we bridge
    the gap between dimensions, enabling us to visualize the intricacies of the point
    cloud in a 2D realm, which is the input needed for SAM. Point cloud mapping adds
    a layer of understanding to this projection process.
  prefs: []
  type: TYPE_NORMAL
- en: '3.1 Ortho Projection: Flattening Dimensions, Expanding Insights'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us look at the transformative technique of Ortho Projection. This method
    serves as an excellent bridge between the multi-dimensional complexities of 3D
    point clouds and the comprehensible world of 2D images. Through Ortho Projection,
    we ‚Äúflatten‚Äù dimensions but also unveil a direct way to manage segmentation with
    SAM.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is basically to generate a top-down view plane and generate an image
    that is not constrained by a single perspective. You could see ortho-projection
    as a process of pushing visible points from the point cloud (highest ones) onto
    the plane that holds the empty image to fill all the necessary pixels just above
    those points. You can see the difference from a perspective view, as illustrated
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efcf4ea1b6062d00cb933ab9b645ff17.png)'
  prefs: []
  type: TYPE_IMG
- en: Explanation of the difference between Orthographic View, and Perspective View
    for 3D Projections. The Perspective View is linked to a single point of view that
    skews dimensions. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: 'To work out this process, we can define a 3D-to-2D projection function that
    would take the points of a point cloud alongside its color and a wanted resolution
    to compute the ortho-projection and return an orthoimage from the point cloud.
    This would translate into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, now it is time for a test, do you agree? To do so, let us load a point
    cloud dataset, transform it to a numpy array, apply the function, and export an
    image of this point cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This permits us to obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65931fd258f06d536ead6c908d1c2c17.png)'
  prefs: []
  type: TYPE_IMG
- en: The workflow to go from 3D Point Clouds to Orthoimages. We first project the
    point cloud following an ortho-projection mode, then we make sure to include a
    point-to-pixel mapping for back-projection. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: Let us move on to spherical projections
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 3D Point Cloud Spherical Projection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our journey takes an intriguing turn as we encounter Spherical Projection.
    This technique offers a unique perspective, enabling us to visualize the data
    by ‚Äúsimulating‚Äù a virtual scan station. To do just this, we proceed in four steps
    by: (1) Considering the 3D Point Cloud, (2) Projecting these points onto a sphere,
    (3) defining a geometry that will retrieve the pixels, (4) ‚Äúflattening‚Äù this geometry
    to produce an image.'
  prefs: []
  type: TYPE_NORMAL
- en: '**ü§† Ville**: *Spherical projection is like being inside a 3D point cloud and
    taking a 360-degree photo of what you see*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/503ca251cad894ed496dd046f9732807.png)'
  prefs: []
  type: TYPE_IMG
- en: The 3D Point Cloud Spherical Projection Workflow. We take a 3D Point Cloud,
    we create a 3D Projection Sphere, we define the mapping plane, and we produce
    an equirectangular projection. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: To achieve the 3D Projection onto a sphere, we want to obtain points as illustrated
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d82f7a78bb0e5f5eb5b32ed2c379f96.png)'
  prefs: []
  type: TYPE_IMG
- en: How to project points of a 3D point clouds onto a sphere. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will unroll following our geometry (cylinder) to obtain an equirectangular
    image, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f0023fe4cecfde6980edba0c234293d.png)'
  prefs: []
  type: TYPE_IMG
- en: How to go from a sphere to an equirectangular image. We have a projection mechanism
    that permits us to ‚Äúunroll‚Äù the pixels onto a cylinder. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me now detail the function that allows just this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'üå± **Growing**: *It is essential to digest this function. It looks like it is
    pretty straightforward, but there are nice tricks at several stages. For example,
    what do you think about the 3D point cloud to spherical coordinates step? What
    does the mapping do? What is the point of using the mapping as a conditional statement
    while assigning points to pixels?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to use this handy function, let us load and prepare the ITC indoor point
    cloud first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Once prepared, we can define the necessary parameters for projection. These
    are the center of projection (basically the position from which we want a virtual
    scan station) and the resolution of the final image (expressed in pixels, as the
    smallest side of the image).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can call the new function, plot and export the results as an image
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'All this process results in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94689a7ca448ddb2e27d84adc9e14cb1.png)'
  prefs: []
  type: TYPE_IMG
- en: The 3D point cloud transformed as an equirectangular image from the projection.
    ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: How do you like that? You can play around with the various parameters, such
    as the resolution or the center of projection, to ensure that you get a nice balance
    between ‚Äúno data‚Äù pixels and relevant panorama.
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ä **Florent**: *You just unlocked a powerful new skill with 3D Point Cloud
    to Equirectangular image creation. Indeed, it allows you to generate virtual scans
    basically wherever you believe it makes sense and then unlock the possibility
    to use image processing and deep learning techniques for images. You can also
    extend the provided function to other mapping projections to add arrows to your
    quiver.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§†**Ville**: *I can almost see the lecture halls and my office, Dutch working
    vibes!*'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 3D Point-to-Pixel Mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We transform raw point data into structured raster representations, making
    sense of the seemingly scattered information. Point Cloud Mapping is the compass
    that guides us for 3D point cloud processing through 2D projection. The good news:
    we already took care of this mapping.'
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, if you take a close look at the function `generate_spherical_image`,
    you can see that we return the `mapping` variable and capture it in another variable
    for downward processes. This ensures that we can have a coherent 3D Point-to-Pixel
    Mapping.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Unsupervised Segmentation with SAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised segmentation enters the scene in the form of the Segment Anything
    Model. We are, in the case of non-labeled outputs, through SAM‚Äôs segmentation
    architecture, which falls within clustering applications. This is opposed to most
    supervised learning approaches that will provide labeled outputs, as illustrated
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8dc14d35b848c2681114cb297c086c0d.png)'
  prefs: []
  type: TYPE_IMG
- en: The distinction between unsupervised learning and supervised learning. In unsupervised
    learning, we aim at defining groups of data ‚Äúpoints‚Äù that share some similarity,
    whereas in supervised learning, we aim at approaching the supervision needs (usually
    by feeding labeled data). ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the transfer of pixel predictions, coupled with seamless point cloud
    export, showcases the potential for revolutionizing applications like object detection
    and scene understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. SAM Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To execute the program, we can re-execute the code snippets that we used to
    test our SAM functionalities on 2D images, which are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: and later, we can plot the results on the image itself
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Which results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33b40e186df07c0e404488d127f5a6ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Results of the Segment Anything Model on the 3D point cloud projection. ¬© [F.
    Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: This already looks like we are delineating significant parts of the image. Let
    us move forward with this.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Point Prediction Transfer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us color the point cloud with this image. We thus define a coloring function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that to color our point cloud, we can use the following code line
    that calls our new function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This line returns a numpy array that holds the point cloud.
  prefs: []
  type: TYPE_NORMAL
- en: It is now time for 3D Point Cloud Export!
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Point Cloud Export
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To export the point cloud, you can use numpy or laspy to extract a .las file
    directly. We will proceed with the second solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'And with this, we can export our modified_point_cloud variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: After this stage, we successfully taken our various 2D images resulting from
    the 3D point cloud projection process. We applied SAM algorithm to it, colorized
    it based on its prediction, and exported a colored point cloud. We can thus move
    to getting some insights about what we are getting.
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ä **Florent**: *To analyze the result quickly outside Python, I recommend using
    the CloudCompare Open-Source Software. If you want a clear guide on how to use
    it efficiently, you can read and follow through the article below.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Deep Learning Python Tutorial: PointNet Data Preparation'
  prefs: []
  type: TYPE_NORMAL
- en: The Ultimate Python Guide to structure large LiDAR point cloud for training
    a 3D Deep Learning Semantic Segmentation‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Qualitative Analysis and discussions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our journey nearing its zenith, it‚Äôs time to focus on qualitative analysis.
    Exceptionally, we will not conduct a quantitative analysis, as we would need proper
    labels for that at this stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§† **Ville**: No labels? What you just did was zero-shot learning (Bang!) or
    few-shot learning (Bang! Bang!). We cannot be sure which because we don‚Äôt know
    exactly how SAM was trained by Meta. Therefore it is a bit of a black box for
    us, but that‚Äôs ok.'
  prefs: []
  type: TYPE_NORMAL
- en: We meticulously examine the raster and point cloud results, drawing insights
    that shed light on SAM‚Äôs performance. Also, let us remain grounded by acknowledging
    the model‚Äôs limitations while setting our sights on the future.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Raster results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The output of SAM‚Äôs efforts with our implementation is eloquently depicted through
    the below raster results. These visuals serve as a canvas on which SAM‚Äôs segmentation
    can be quickly assessed, enabling us to comprehend the model‚Äôs understanding of
    the scene in a 2D representation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77d9f078a781d656b12f76e839573662.png)'
  prefs: []
  type: TYPE_IMG
- en: Another result of the Segment Anything Model on the 3D point cloud projection.
    ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, even with the uneven point distribution and ‚Äúblack zones‚Äù, SAM
    is able to pick up what the main parts of the point cloud are about. Specifically,
    it likely highlights a green on the left, the place where the dangerous materials
    are, and the doors and windows for our extraction team to have the most direct
    route!
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Point Cloud results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Yet, it‚Äôs in the point cloud results that the true depth of SAM‚Äôs abilities
    emerges. As we navigate through the cloud of points, SAM‚Äôs segmented predictions
    bring clarity to the classical ‚Äúmess of points‚Äù, showcasing its potential in real-world
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa21106712342d39785489b7db32c5eb.png)'
  prefs: []
  type: TYPE_IMG
- en: The 3D Point Cloud unsupervised segmentation results by using Segment Anything
    3D. We see the great distinction of the major elements that compose the scene.
    ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we can have a direct link with the underlying points, and that
    is massively awesome! Think only about what this can unlock for your applications.
    A 100% automated segmentation process that has under five main breakpoints? Not
    bad!
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Shortcomings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: But, our expedition would only be complete with acknowledging the rough patches
    along the way. SAM, while impressive, is not exempt from limitations. By recognizing
    these shortcomings, we pave the way for refinement and growth
  prefs: []
  type: TYPE_NORMAL
- en: The first thing is that all the ‚Äúunseen‚Äù points remain unlabelled (white points
    below). This could prove to be a limitation for complete processing, and if you
    use the basic or the large model you will see more unlabelled points than when
    using the huge model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07d064a4ed2ff0080884fc84c4ce052f.png)'
  prefs: []
  type: TYPE_IMG
- en: The ratio of unlabelled points vs. labeled points from the first pass with a
    central perspective 360¬∞ simulated scan position. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: Also, at this stage, we used the automatic prompting engine that triggered around
    50 points of interest, the seeds of the segmentation task. While this is great
    for getting a direct result, having the possibility to tune that would be awesome.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the mapping is somewhat simple at this stage; it would largely benefit
    from occlusion culling and point selection for a specific pixel of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. Perspectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Segment Anything Model marks just a single step in the larger landscape
    of 3D point cloud segmentation. However, as it stands and is given to you, our
    implementation should work pretty well for any application where you can have
    some kind of distinctive initial features for SAM. As you can see below, it also
    works for top-down aerial point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0a11f9854a98146441b7b7e2e605a7d.png)![](../Images/1e97b6d8f49191db58ba5df7d5e05ccf.png)'
  prefs: []
  type: TYPE_IMG
- en: The results of Segment Anything 3D for aerial point clouds.¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: Extending to indoor scenarios, you can see that you also will get some pretty
    decent and interesting results. This is even useful for changing the light bulb
    of the light fixtures in the hall, automatically by a robot, of course (How many
    robots does it take to change a light bulb?)!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/878f5e7b1124d9b8c4d9ca60ce76d4fd.png)![](../Images/8b550accb9d81579d0dbe3c232e953cf.png)'
  prefs: []
  type: TYPE_IMG
- en: The results of Segment Anything 3D for another indoor scenario.¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, aside from generalization, one first perspective is to unlock a way
    to generate panoramas and fuse the prediction of the different points of view.
    Of course, another one would be to expand to custom prompts and, finally, address
    the challenge of improving point-to-pixel accuracy in 2D-3D mapping.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are part of the 13.37% of 3D creators that went ahead and actually made
    the code work, then massive kudos to you!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/640db9a177c74e1f31baa1f69ea491f6.png)'
  prefs: []
  type: TYPE_IMG
- en: The workflow that we covered in this article. ¬© [F. Poux](https://learngeodata.eu/)
  prefs: []
  type: TYPE_NORMAL
- en: This is a tremendous achievement, and you now have a very powerful asset for
    attacking semantic extraction tasks for 3D Scene Understanding. With the Segment
    Anything Model, you can now encapsulate innovation in many products, transforming
    how we perceive and interpret 3D point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: Our exploration should have painted a comprehensive, usable picture of this
    groundbreaking model from its inception to its implications. You may now explore
    the variants and extend their pertinency based on the limitations that were spotted
    in the previous parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶ä **Florent**: *I am looking forward to your future projects that make use
    of it!*'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§† **Ville**: *Code on!*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,
    T., Whitehead, S., Berg, A.C., Lo, W.Y. and Doll√°r, P., 2023\. Segment anything.
    [*arXiv preprint arXiv:2304.02643*](https://arxiv.org/pdf/2304.02643).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Poux, Florent**, Mattes, C., Selman, Z. and Kobbelt, L., 2022\. Automatic
    region-growing system for the segmentation of large point clouds. *Automation
    in Construction*, *138*, p.104250\. [Elsevier Link](https://www.sciencedirect.com/science/article/pii/S0926580522001236?casa_token=xnUGwDXoM5gAAAAA%3A1mJlkrTyNZGnbmJnn-9p2qehNHReZvXLX3uJEuXVa6Y5chGjmM-vVAJhezR8wqeKf8XdeR6eng)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Lehtola, Ville**, Kaartinen, H., N√ºchter, A., Kaijaluoto, R., Kukko, A.,
    Litkey, P., Honkavaara, E., Rosnell, T., Vaaja, M.T., Virtanen, J.P. and Kurkela,
    M., 2017\. Comparison of the selected state-of-the-art 3D indoor scanning and
    point cloud generation methods. *Remote sensing*, *9*(8), p.796\. [MDPI Link](https://www.mdpi.com/2072-4292/9/8/796/pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/f85f3cdf9e16e572c3a0759b83ebfe22.png)'
  prefs: []
  type: TYPE_IMG
- en: üî∑Other Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'üçá Get Access to the Data here: [3D Datasets](https://drive.google.com/drive/folders/1RPM69xjPpdBqdwZKC-chRBxkmhDiND9v?usp=sharing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'üë®‚Äçüè´ 3D Online Data Science Courses: [3D Academy](https://learngeodata.eu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'üìñ Subscribe for early access to 3D Tutorials: [3D AI Automation](https://medium.com/@florentpoux/subscribe)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'üßë‚ÄçüéìGet a Master‚Äôs Degree: [ITC Utwente](https://www.itc.nl/education/?_ga=2.57523758.507481808.1702288503-2061064516.1700473140)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üéìAuthor‚Äôs Recommendation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To build full Indoor Semantic Extraction Scenarios, you can combine this approach
    with the one explained in the ‚Äú*3D Point Cloud Shape Detection for Indoor Modelling*‚Äù
    article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/3d-point-cloud-shape-detection-for-indoor-modelling-70e36e5f2511?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Point Cloud Shape Detection for Indoor Modelling'
  prefs: []
  type: TYPE_NORMAL
- en: A 10-step Python Guide to Automate 3D Shape Detection, Segmentation, Clustering,
    and Voxelization for Space Occupancy‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/3d-point-cloud-shape-detection-for-indoor-modelling-70e36e5f2511?source=post_page-----80c06be99a18--------------------------------)
    [](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Innovator Newsletter
  prefs: []
  type: TYPE_NORMAL
- en: Weekly practical content, insights, code and resources to master 3D Data Science.
    I write about Point Clouds, AI‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
  prefs: []
  type: TYPE_NORMAL
