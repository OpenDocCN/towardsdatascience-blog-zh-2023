# Python中的时序差分：第一个基于样本的强化学习算法

> 原文：[https://towardsdatascience.com/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee](https://towardsdatascience.com/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee)

## 使用Python编写并理解TD(0)算法

[](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)[![Eligijus Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------) [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------) ·13分钟阅读·2023年1月27日

--

![](../Images/36f274e9692bf335a901977271968109.png)

[Kurt Cotoaga](https://unsplash.com/@kydroon?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上的照片

这是我之前文章的续集：

[](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----54c11745a0ee--------------------------------) [## Python强化学习中的第一步

### Python的原始实现，展示了如何在强化学习的基本世界之一中找到最佳位置……

towardsdatascience.com](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----54c11745a0ee--------------------------------)

在这篇文章中，我想让读者熟悉强化学习中的基于样本的算法逻辑（**RL**）。为此，我们将创建一个带有洞的网格世界（类似于缩略图中的那个），并让我们的代理在创建的世界中自由遍历。

希望在代理的旅程结束时，他能学会在世界上哪个地方是好的地方，哪些位置应该避免。为了帮助我们的代理学习，我们将使用著名的**TD(0)**算法。

在深入算法之前，让我们定义一下我们想要解决的目标。

在这篇文章中，我们将创建一个5行7列的网格世界，这意味着我们的代理将能够处于35个状态中的一个。移动规则如下：

+   代理不能离开网格世界的边界。

+   在每个时间步，代理只能向上、向下、向左或向右移动。

+   代理从我们网格世界的左上角开始。

+   如果代理达到目标或掉入洞里，游戏结束，代理会被返回到起始状态。

+   每次移动都会获得-1的奖励。

+   掉入洞里会获得-10的奖励。

+   达到目标会获得10的奖励。

我们代理的**终极目标是尽可能准确地评估它可能处于的每一个状态。换句话说，**我们代理希望根据给定的移动策略评估每个状态的价值。**

以下代码片段初始化了前一节中描述的环境：

[PRE0]

我们需要开始学习的对象是：

+   状态矩阵S

+   值矩阵V

+   奖励矩阵R

+   策略字典P

默认情况下，上述代码片段初始化了一个随机策略的世界。

**随机策略意味着我们的代理通过均匀概率分布选择从一个状态转移到另一个状态。**

让我们创建我们的世界，更详细地探索这些矩阵：

[PRE1]

以下代码片段用于绘制矩阵：

[PRE2]

首先让我们可视化状态矩阵：

[PRE3]

![](../Images/a010780e57982499b646f6fd03f4aad8.png)

状态矩阵；作者拍摄的照片

红色状态表示洞的坐标——这些是我们的代理想要避免的状态。

灰色状态表示目标——这是我们的代理想要到达的地方。

我们的代理总是从状态0开始它的旅程。

奖励矩阵如下：

[PRE4]

![](../Images/1aa2a275f8ba0f3ab10a817dbbfb6caa.png)

奖励矩阵；作者拍摄的照片

转移到某个状态的奖励矩阵在上面可视化。

例如：

+   从状态1到8会获得-1的奖励

+   从状态9到10会获得-10的奖励

+   从状态33到34会获得10的奖励

依此类推。

我们的代理将遵循的策略是随机策略——进入每个状态的概率均等：

[PRE5]

![](../Images/2555fd4214edb5815094a9072e2b5f46.png)

策略矩阵；作者拍摄的照片

策略矩阵中的灰色状态表示终端状态：如果代理选择进入该状态，剧集将结束，代理将被重置到状态0。

**TD(0)算法的目标是评估给定策略下每个状态的价值。**

换句话说，我们想要填充值矩阵的值：

[PRE6]

![](../Images/a3440272edf8dca009a125014db17b4c.png)

初始值矩阵；作者拍摄的照片

**TD(0)**算法是**单步时序差分**算法的简称。为了开始建立直觉并广泛地说，在此算法中，我们的代理按照给定的策略执行一步，观察奖励，并在这种步骤后更新状态价值的估计。

从数学上讲，更新步骤如下：

![](../Images/bc019e96ebe04404524df5596f5c0a45.png)

TD(0) 更新方程

这里：

+   **s prime** — 我们的代理从当前状态s转移到的状态。

+   奖励 **r** 等于转移到s prime的奖励。

+   **Gamma** 是折扣率（大于0，小于或等于1）。

+   **Alpha** 是大小（大于0，小于或等于1）。

完整算法¹如下：

![](../Images/77bdaf7b738af3153323baee021c0110.png)

完整TD(0)；作者照片

TD(0)算法是一种**预测**算法。在强化学习中，预测算法指的是一种尝试估计状态值的算法，同时**不**改变给定的策略（转移概率）。

这也是一种**自助**算法，因为我们使用当前的价值函数估计来估计下一个状态的价值函数。

因此，我们只关心状态值——智能体从当前状态移动的总期望累计奖励：

![](../Images/c69146c3676f9e8d5c3aae5cc0b9a450.png)

状态价值

现在让我们开始实现算法。

我们的智能体首先需要根据我们创建的策略进行移动：

[PRE7]

当智能体处于状态s时，它只能前往策略矩阵字典中存在的状态。例如，状态1中的所有动作是：

![](../Images/f28b8b65ac45770c6f867938c3719b6c.png)

状态1的所有可能动作

所有概率的总和等于1，我们的智能体随机选择**右、左或下**（请参阅状态矩阵图以查看状态位置）。

上述动作是开始更新价值函数所需的全部。当智能体进行移动时，它转移到另一个状态并收集该状态的奖励。然后我们应用方程：

![](../Images/bc019e96ebe04404524df5596f5c0a45.png)

TD(0)更新方程

[PRE8]

最后一步是将所有内容封装到一个**while循环**中，只有当我们的智能体转移到终止状态时才停止探索：

[PRE9]

我们现在拥有了实施完整TD(0)算法所需的一切。

让我们定义10000次实验，让我们的智能体进行学习吧！

[PRE10]

我们的智能体在终止之前所采取的动作数量：

[PRE11]

![](../Images/43bc9ae571c1200b08fa3af801a9413e.png)

移动次数；作者绘图

平均而言，我们的智能体在碰到终止状态之前进行了10次移动。

最终评估的状态价值矩阵：

![](../Images/1394931721b63e545ef719d8b174129b.png)

使用TD(0)和随机策略评估的V；作者绘图

正如我们所见，按照给定的策略，智能体开始旅程的状态非常糟糕。平均而言，从该状态开始，智能体仅获得-9.96的奖励。然而，随着我们接近目标状态，价值会增加。

注意，目标状态和洞穴状态的值为0，因为这些状态没有探索——每次智能体转移到这些状态，游戏就结束了。

如果我们选择了另一种策略会发生什么？例如，更频繁地选择“向右”方向：

[PRE12]

![](../Images/66602a243f165f8344cf6d401141b74b.png)

不同策略下的移动次数

![](../Images/3cbab1ef6cc6bcd0ba876784e9803804.png)

不同策略的价值矩阵

随机策略的状态价值矩阵总和为**-249.29**，而更高概率向右的策略总和为**-213.51**。

从这个意义上说，我们可以说更频繁地向右移动是一种更好的策略！

在这篇文章中，我介绍了RL中的第一个基于样本的算法——一步时序差分算法或TD(0)。

这是一种预测算法，即仅用于评估给定策略的状态。改变策略会得到不同的状态价值结果。

祝大家学习愉快，编程快乐！

[1]

+   作者: **理查德·S·萨顿，安德鲁·G·巴托**

+   年份: **2018**

+   页码: **120**

+   书名: **强化学习：一种介绍**

+   URL:[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
