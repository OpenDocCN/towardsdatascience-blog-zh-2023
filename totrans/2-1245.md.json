["```py\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark.sql.functions as f\nfrom pyspark.sql.types import IntegerType, DateType\nimport random\n\n# set the seed to get same results when rerunning\nrandom.seed(42)\n\n# create a spark session\nspark = SparkSession.builder.getOrCreate()\n\n# create id list\nids = list(range(1, 11))  # Adjust this list size to meet the desired number of unique IDs\n\n# create two possible conditions for each id\nconditions = [[random.randint(1, 10) for _ in range(2)] for _ in ids]\n\n# create a list of tuples where each tuple is a row\nrows = [(id, random.choice(conditions[id-1]), date) \n        for id in ids for date in ['2023-01-01', '2023-01-02', '2023-01-03']]\n\n# some ids should be missing on some dates, so we remove some\nrows = [row for row in rows if random.random() > 0.2]  # Adjust this parameter to control the number of missing IDs\n\n# create a DataFrame\ndf = spark.createDataFrame(rows, [\"id\", \"condition\", \"import_date\"])\n\n# convert import_date column to DateType\ndf = df.withColumn(\"import_date\", df[\"import_date\"].cast(DateType()))\n\n# sort DataFrame\ndf = df.sort(\"id\", \"import_date\")\n\n# show the DataFrame\ndf.show()\n```", "```py\npartition_columns = [\"id\"]\ndatetime_column = \"import_date\"\n```", "```py\ntrack_columns = [column for column in df.columns \n                  if column not in partition_columns + [datetime_column]]\n```", "```py\nfrom pyspark.sql import Window\n\nwindow = Window.partitionBy(partition_columns).orderBy(datetime_column)\n```", "```py\nfor column in track_columns:\n  previous_column = f\"previous_{column}\"\n  changed_column = f\"changed_{column}\"\n  df = df.withColumn(previous_column, f.lag(f.col(column)).over(window))\n  df = df.withColumn(changed_column, f.col(column) != f.col(previous_column))\n  # df = df.drop(previous_column)\n```", "```py\ndf = df.withColumn('row_num', f.row_number().over(window))\n```", "```py\n# determine last import date\nlatest_import = df.agg(f.max(datetime_column)).collect()[0][0]\n\nprint(f\"{latest_import = }\")\n\n# output\nlatest_import = datetime.date(2023, 1, 3)\n```", "```py\n# Add last_partition_import_date to keep track of \n#  what the last imported date was per partition\nlast_partition_window = Window.partitionBy(*partition_columns)\ndf = (\n  df\n  .withColumn('last_partition_import_date', \n              f.max(f.col(datetime_column)).over(last_partition_window)\n)\n```", "```py\nfrom itertools import reduce\n\n# Add condition to keep every first row \nfirst_row_condition = f.col(\"row_num\") == 1\n\n# Add condition that only keeps row that have changed values\nchange_condition = reduce(lambda a, b: a | b, (f.col(f\"changed_{column}\") \n                    for column in track_columns))\n\n# Filter the DataFrame to keep the first rows for each partition \n# and the rows where there is a change\nfiltered_df = df.filter(first_row_condition)\n              .union(df.filter(change_condition))\n```", "```py\n# Drop the 'row_num' and 'changed_*' columns\nfiltered_df = filtered_df.drop('row_num', \n                    *[f\"previous_{column}\" for column in track_columns],\n                    *[f\"changed_{column}\" for column in track_columns])\n```", "```py\nwindow = Window.partitionBy(partition_columns).orderBy(datetime_column)\n\n# Add 'until' by looking one row ahead\nfiltered_df = filtered_df.withColumn(\"until\", \n                              f.lag(f.col(datetime_column), -1).over(window))\n```", "```py\n# Add a deletion date if the last date of a partition is before the last import date of the data\nfiltered_df = filtered_df.withColumn(\n  \"until\",\n  f.when(\n      (f.col(\"until\").isNull()) \n       & (f.col(\"last_partition_import_date\") < latest_import),\n      f.date_add(f.col(\"last_partition_import_date\"), 1)\n  ).otherwise(f.col(\"until\"))\n)\n```", "```py\nfinal_df = (\n  filtered_df\n  .drop(\"last_partition_import_date\")\n  .withColumnRenamed(datetime_column, \"from\")\n)\n```", "```py\nimport pyspark.sql.functions as f\nfrom functools import reduce\nfrom pyspark.sql import Window\nimport pyspark\nfrom typing import Optional\n\ndef remove_uninformative_rows(df: pyspark.sql.DataFrame,\n                              partition_columns: list[str],\n                              datetime_column: str,\n                              track_columns: Optional[list[str]] = None)\n                             -> pyspark.sql.DataFrame:\n\n  if track_columns is None:\n    track_columns = [column for column in df.columns \n                    if column not in partition_columns + [datetime_column]]\n\n  # Define a window specification based on partition columns and ordered by datetime_column\n  window = Window.partitionBy(*partition_columns).orderBy(datetime_column)\n\n  # Iterate over the non-partition columns and add a new column for each column that indicates whether the value has changed\n  for column in track_columns:\n      previous_column = f\"previous_{column}\"\n      changed_column = f\"changed_{column}\"\n      df = df.withColumn(previous_column, f.lag(f.col(column)).over(window))\n      df = df.withColumn(changed_column, f.col(column) != f.col(previous_column))\n      df = df.drop(previous_column)\n\n  # Add the row number to keep track of what the first occurance of a row is\n  df = df.withColumn('row_num', f.row_number().over(window))\n\n  # Add last_partition_import_date to keep track of what the last imported date was per partition\n  last_partition_window = Window.partitionBy(*partition_columns)\n  df = df.withColumn('last_partition_import_date', f.max(f.col(datetime_column)).over(last_partition_window))\n\n  # Save the latest import date of the entire data set\n  latest_import = df.agg(f.max(datetime_column)).collect()[0][0]\n\n  # Add condition to keep every first row \n  first_row_condition = f.col(\"row_num\") == 1\n\n  # Add condition that only keeps row that have changed values\n  change_condition = reduce(lambda a, b: a | b, (f.col(f\"changed_{column}\") for column in track_columns))\n\n  # Filter the DataFrame to keep the first rows for each partition and the rows where there is a change\n  filtered_df = df.filter(first_row_condition).union(df.filter(change_condition))\n\n  # Drop the 'row_num' and 'changed_*' columns\n  filtered_df = filtered_df.drop('row_num', *[f\"changed_{column}\" for column in track_columns])\n\n  # Add 'until' column by looking one row ahead\n  filtered_df = filtered_df.withColumn(\"until\", f.lag(f.col(datetime_column), -1).over(window))\n\n  # Add a deletion date if the last date of a partition is before the last import date of the data\n  filtered_df = filtered_df.withColumn(\n    \"until\",\n    f.when(\n        (f.col(\"until\").isNull()) & (f.col(\"last_partition_import_date\") < latest_import),\n        f.date_add(f.col(\"last_partition_import_date\"), 1)\n    ).otherwise(f.col(\"until\"))\n  )\n\n  # Drop last_partition_import_date and rename import_date column\n  final_df = (\n    filtered_df\n    .drop(\"last_partition_import_date\")\n    .withColumnRenamed(\"import_date\", \"from\")\n  )\n  return final_df\n```", "```py\nremove_uninformative_rows(\n  df=df,\n  partition_columns=['id'],\n  datetime_column='import_date'\n)\n```"]