["```py\n# METHOD 1 - USING PLAIN PANDAS\nimport pandas as pd\n\nparquet_file = 'example_pd.parquet'\n\ndf.to_parquet(parquet_file, engine = 'pyarrow', compression = 'gzip')\n\nlogging.info('Parquet file named \"%s\" has been written to disk', parquet_file)\n```", "```py\n# METHOD 2.1 - USING PANDAS & FASTPARQUET (WRITE IN FULL)\nimport pandas as pd\nimport fastparquet as fp\n\nparquet_file = 'example_pd_fp_1.parquet'\n\nfp.write(parquet_file, df, compression = 'GZIP')\n\nlogging.info('Parquet file named \"%s\" has been written to disk', parquet_file)\n```", "```py\n# METHOD 2.2 - USING PANDAS & FASTPARQUET (WRITE IN BATCHES)\nimport pandas as pd\nimport fastparquet as fp\n\n# SETTING BATCH SIZE\nbatch_size = 250\n\ndata = db_cursor_sf.fetchmany(batch_size)\ncolumns = [desc[0] for desc in db_cursor_sf.description]\n\n# CREATES INITIAL DF INCLUDING 1ST BATCH\ndf = pd.DataFrame(data, columns = columns)\ndf = df.astype(schema)\n\n# WRITES TO PARQUET FILE\nparquet_file = 'example_pd_fp_2.parquet'\nfp.write(parquet_file, df, compression = 'GZIP')\n\ntotal_rows = df.shape[0]\ntotal_cols = df.shape[1]\n\n# SEQUENTIALLY APPEND TO PARQUET FILE\nwhile True:\n    data = db_cursor_sf.fetchmany(batch_size)\n    if not data:\n        break\n    df = pd.DataFrame(data, columns = columns)\n    df = df.astype(schema)\n\n    total_rows += df.shape[0]\n\n    # Write the rows to the Parquet file\n    fp.write(parquet_file, df, append=True, compression = 'GZIP')\n\nlogging.info('Full parquet file named \"%s\" has been written to disk \\\n              with %s total rows', parquet_file, total_rows)\n```", "```py\n# EXAMPLE 3 - USING PANDAS & PYARROW\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n# SETTING BATCH SIZE\nbatch_size = 250\n\nparquet_schema = pa.schema([('as_of_date', pa.timestamp('ns')),\n                            ('company_code', pa.string()),\n                            ('fc_balance', pa.float32()),\n                            ('fc_currency_code', pa.string()),\n                            ('gl_account_number', pa.string()),\n                            ('gl_account_name', pa.string())\n                           ])\n\nparquet_file = 'example_pa.parquet'\n\ntotal_rows = 0\n\nlogging.info('Writing to file %s in batches...', parquet_file)\n\nwith pq.ParquetWriter(parquet_file, parquet_schema, compression='gzip') as writer:\n    while True:\n        data = db_cursor_pg.fetchmany(batch_size)\n        if not data:\n            break\n        df = pd.DataFrame(data, columns=list(parquet_schema.names))\n        df = df.astype(schema)\n\n        table = pa.Table.from_pandas(df)\n        total_rows += table.num_rows\n\n        writer.write_table(table)\n\nlogging.info('Full parquet file named \"%s\" has been written to disk \\\n              with %s total rows', parquet_file, total_rows)\n```", "```py\n# EXAMPLE 4 - USING PYSPARK\nfrom pyspark.sql.types import *\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkConf\n\n# CONNECT TO DB + LOAD DF\n\n# WRITING TO PARQUET\ndf.write.mode('overwrite')\\ # or append\n        .option('compression', 'gzip')\\\n        .parquet(\"example_pyspark_final.parquet\")\n\ndf.show()\n```"]