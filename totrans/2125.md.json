["```py\n# installation\n!sudo apt install tesseract-ocr\n!pip install pytesseract\n\nimport pytesseract\nfrom pytesseract import Output\nfrom PIL import Image\nimport cv2\n\nimg_path1 = '00b5b88720f35a22.jpg'\ntext = pytesseract.image_to_string(img_path1,lang='eng')\nprint(text)\n```", "```py\n# boxes around character\nprint(pytesseract.image_to_boxes(img_path1))\n```", "```py\n~ 532 48 880 50 0\n...\nA 158 220 171 232 0\nF 160 220 187 232 0\nI 178 220 192 232 0\nL 193 220 203 232 0\nM 204 220 220 232 0\nB 228 220 239 232 0\nY 240 220 252 232 0\nR 259 220 273 232 0\nO 274 219 289 233 0\nN 291 220 305 232 0\nH 314 220 328 232 0\nO 329 219 345 233 0\nW 346 220 365 232 0\nA 364 220 379 232 0\nR 380 220 394 232 0\nD 395 220 410 232 0\n...\n```", "```py\n# boxes around words\nprint(pytesseract.image_to_data(img_path1))\n```", "```py\n# installation\n!pip install easyocr\n\nimport easyocr\n\nreader = easyocr.Reader(['en'])\nextract_info = reader.readtext(img_path1)\n\nfor el in extract_info:\n   print(el)\n```", "```py\n# installation\n!pip install keras-ocr -q\n\nimport keras_ocr\n\npipeline = keras_ocr.pipeline.Pipeline()\nextract_info = pipeline.recognize([img_path1])\nprint(extract_info[0][0])\n```", "```py\n('from',\n array([[761.,  16.],\n        [813.,  16.],\n        [813.,  30.],\n        [761.,  30.]], dtype=float32))\n```", "```py\ndiz_cols = {'word':[],'box':[]}\nfor el in extract_info[0]:\n    diz_cols['word'].append(el[0])\n    diz_cols['box'].append(el[1])\nkerasocr_res = pd.DataFrame.from_dict(diz_cols)\nkerasocr_res\n```", "```py\n# installation\n!pip install transformers\n\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\n\nmodel_version = \"microsoft/trocr-base-printed\"\nprocessor = TrOCRProcessor.from_pretrained(model_version)\nmodel = VisionEncoderDecoderModel.from_pretrained(model_version)\n```", "```py\nimage = Image.open(img_path1).convert(\"RGB\")\npixel_values = processor(image, return_tensors=\"pt\").pixel_values\ngenerated_ids = model.generate(pixel_values)\nextract_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint('output: ',extract_text)\n# output: 2.50\n```", "```py\ncrp_image = image.crop((750, 3.4, 970, 33.94))\ndisplay(crp_image)\n```", "```py\npixel_values = processor(crp_image, return_tensors=\"pt\").pixel_values\ngenerated_ids = model.generate(pixel_values)\nextract_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(extract_text)\n```", "```py\n! pip install python-doctr\n# for TensorFlow\n! pip install \"python-doctr[tf]\"\n# for PyTorch\n! pip install \"python-doctr[torch]\"\n```", "```py\nfrom doctr.io import DocumentFile\nfrom doctr.models import ocr_predictor\nmodel = ocr_predictor(det_arch = 'db_resnet50',\n                      reco_arch = 'crnn_vgg16_bn',\n                      pretrained = True\n                     )\n```", "```py\n# read file\nimg = DocumentFile.from_images(img_path1)\n\n# use pre-trained model\nresult = model(img)\n\n# export the result as a nested dict\nextract_info = result.export()\n```", "```py\n{'pages': [{'page_idx': 0, 'dimensions': (678, 1024), 'orientation': {'value': None, 'confidence': None},...\n```", "```py\nfor obj1 in extract_info['pages'][0][\"blocks\"]:\n    for obj2 in obj1[\"lines\"]:\n        for obj3 in obj2[\"words\"]:\n            print(\"{}: {}\".format(obj3[\"geometry\"],obj3[\"value\"]))\n```"]