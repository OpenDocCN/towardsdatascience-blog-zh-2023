- en: “ML-Everything”? Balancing Quantity and Quality in Machine Learning Methods
    for Science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ml-everything-balancing-quantity-and-quality-in-machine-learning-methods-for-science-baa0477f5ac9](https://towardsdatascience.com/ml-everything-balancing-quantity-and-quality-in-machine-learning-methods-for-science-baa0477f5ac9)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The need for proper validations and good datasets, objective and balanced, and
    that predictions be useful in realistic scenarios.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://lucianosphere.medium.com/?source=post_page-----baa0477f5ac9--------------------------------)[![LucianoSphere
    (Luciano Abriata, PhD)](../Images/a8ae3085d094749bbdd1169cca672b86.png)](https://lucianosphere.medium.com/?source=post_page-----baa0477f5ac9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----baa0477f5ac9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----baa0477f5ac9--------------------------------)
    [LucianoSphere (Luciano Abriata, PhD)](https://lucianosphere.medium.com/?source=post_page-----baa0477f5ac9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----baa0477f5ac9--------------------------------)
    ·12 min read·Mar 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61425b6c60082fe85bbe42e083edbd99.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Tingey Injury Law Firm](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recent research in machine learning (ML) has led to significant progress in
    various fields, including scientific applications. However, there are limitations
    that need to be addressed to ensure the validity of new models, the quality of
    testing and validation procedures, and the actual applicability of the developed
    models to real-world problems. These limitations include unfair, subjective, and
    unbalanced evaluations, not necessarily intentional yet there, the use of datasets
    that don’t properly reflect real-world use cases (for example that are “too easy”),
    incorrect ways to split datasets into training, testing, and validation subsets,
    etc. In this article I will discuss all these points, using examples from the
    domain of biology which is being revolutionized by ML methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: Along the way I will also briefly touch on the interpretability of ML models,
    which is today very limited but very important because it could help clarify many
    of the aspects discussed in the first part of the article regarding the limitations
    that need to be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, I’ll stress that while some ML models may be oversold, this doesn’t
    mean they aren’t useful or haven’t generated new knowledge that can help advance
    certain subfields within that of ML.
  prefs: []
  type: TYPE_NORMAL
- en: Many new ML models coming out every month, and this is just in my domain of
    work!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the recent surge in the number of machine learning (ML) papers for scientific
    applications, I’ve started to ask myself… are all these works as revolutionary
    and useful as they propose? Sure, AlphaFold 2 was revolutionary, as well as subsequent
    ML tools inspired by similar theoretical frameworks, for example those for protein
    design like ProteinMPNN. But more broadly, what’s going on in the field? How can
    scientists be producing so many ML tools and they all be “the best” at the same
    time? Is the research behind them good enough? And assuming the work is novel
    and good, are their evaluations always fair, objective, and balanced? Are the
    actual applications to real-world problems as revolutionary as sold?
  prefs: []
  type: TYPE_NORMAL
- en: Each time I see a new ML method for structural biology, I wonder how to assess
    its merit and especially its actual performance and hence its actual utility for
    my research.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers are increasingly applying the latest developments in neural networks
    to long-standing problems in various fields, leading to significant progress.
    However, it’s crucial to ensure that evaluations are fair, objective, and balanced,
    and that datasets and prediction capabilities accurately reflect real-world applicability
    of the ML models.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Social network posts, preprint servers and peer-reviewed papers show a burst
    in the application of modern neural network modules and concepts (transformers,
    diffusion, etc.) to long-standing problems in science. The mere fact that researchers
    are doing this is in itself good, because the approach has led to remarkable progress
    in various fields. Take, as the two main examples, advances in protein structure
    prediction led by AlphaFold 2 winning CASP14, and protein design especially by
    the hand of D. Baker’s ProteinMPNN whose predicted protein sequences were extensively
    tested in experimental work confirming that the method works. If you want to know
    more about these methods, check out my blog articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/advances-in-biological-science/over-one-year-of-alphafold-2-free-for-everyone-to-use-and-of-the-revolution-it-triggered-in-biology-f12cac8c88c6?source=post_page-----baa0477f5ac9--------------------------------)
    [## Over a year of AlphaFold 2 free to use and of the revolution it triggered
    in biology'
  prefs: []
  type: TYPE_NORMAL
- en: Confident modeling of protein structures, prediction of their interactions with
    other biomolecules, and even protein…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/advances-in-biological-science/over-one-year-of-alphafold-2-free-for-everyone-to-use-and-of-the-revolution-it-triggered-in-biology-f12cac8c88c6?source=post_page-----baa0477f5ac9--------------------------------)  [##
    Here are all my peer-reviewed and blog articles on protein modeling, CASP, and
    AlphaFold 2
  prefs: []
  type: TYPE_NORMAL
- en: I compiled here all my peer-reviewed articles (some papers, a couple of reviews,
    one opinion) and blog entries about…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: lucianosphere.medium.com](https://lucianosphere.medium.com/here-are-all-my-peer-reviewed-and-blog-articles-on-protein-modeling-casp-and-alphafold-2-d78f0a9feb61?source=post_page-----baa0477f5ac9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, the new methods are at least somewhat oversold. Take for example—and
    I think my suspicion is clearest here—the case of protein design. Most if not
    all recent works presenting modern ML models for protein design use as a success
    metric the sequence identity of the recovered sequence relative to that of the
    input protein structure. At first this might make sense, but on deeper thought
    and knowing about protein sequences and structures, it is obvious that a good
    sequence match doesn’t necessarily have to mean (at all!) that a protein will
    fold as intended. For example, it is very common that even single mutants of a
    protein can fail to fold miserably and crash out of solution, even though they
    are almost identical to the wild-type protein and essentially perfect regarding
    the “sequence identity” metric. Also, single mutants can sometimes induce fold
    swaps, resulting in a case of almost perfect sequence identity but no preservation
    of the structure. Conversely, it is not at all uncommon to find proteins that
    fold in pretty much the same way although their sequences are entirely different
    hence of very low sequence identity. In conclusion, sequence recovery is overly
    reasonable but *very limited* as a metric for actual success in protein design.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence recovery is reasonable but *very limited* as a metric for actual success
    in protein design
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For the moment, the only actually meaningful test in protein design is getting
    your hands dirty in the wet lab, producing the designed protein, and determining
    its structure experimentally to find that it matches the input structure -at least
    reasonably well, as of course we can’t expect a perfect match. While the ProteinMPNN
    papers and a handful of other works do this experimental validation of the method,
    most preprints and papers I see out there just totally overlook this, focusing
    exclusively on sequence recovery and similar metrics. And no, finding that AlphaFold
    can back-predict the structure fed into the design protocol isn’t a safe measure
    that the design works either! At best, it can be used to identify bad sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to know more about ProteinMPNN that I just mentioned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/new-deep-learned-tool-designs-novel-proteins-with-high-accuracy-41ae2a7d23d8?source=post_page-----baa0477f5ac9--------------------------------)
    [## New deep-learned tool designs novel proteins with high accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: This new software from the Baker laboratory designs proteins that actually work
    in the wet lab. And you can use it to…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/new-deep-learned-tool-designs-novel-proteins-with-high-accuracy-41ae2a7d23d8?source=post_page-----baa0477f5ac9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Potential problems with datasets and evaluations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An important problem, that I describe here in general terms because I don’t
    want to talk negatively about any specific work, is that many studies assess their
    models on datasets that are not good enough for the task. The two main problems
    I’ve seen are datasets that fail to reflect real-world applications of the ML
    model, and datasets that contain entries that overlap with the training data.
  prefs: []
  type: TYPE_NORMAL
- en: I’m not saying there’s bad intention. Training ML models requires very large
    datasets, so large that they are impossible to curate manually, and automated
    curation pipelines always have limitations. On the other hand, preprints and papers
    tend to show only those cases that illustrate positive cases of applicability
    of the ML model to some biological problem, hiding or overlooking cases that lack
    any biological sense, or are hard to interpret, or—and this is just plain bad
    scientific practice—that don’t match with what’s already known.
  prefs: []
  type: TYPE_NORMAL
- en: 'None of these problems is specific to the domain, but rather a particular manifestation
    of well-established problems of science in general: only positive results are
    deemed to contribute, hence negative and incorrect results do not abound in the
    literature although they are as relevant as positive results especially to avoid
    futile waste of resources and time. The publish-or-perish culture promotes the
    publication of mainly positive results, often decorated to claim overly novelty
    and supremacy. To know more about my thoughts on the problems of science especially
    regarding publishing, see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/predict/how-could-scientific-publishing-evolve-25977238dbdb?source=post_page-----baa0477f5ac9--------------------------------)
    [## How could scientific publishing evolve?'
  prefs: []
  type: TYPE_NORMAL
- en: Summary and reflections triggered by the new heading taken by the journal eLife.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/predict/how-could-scientific-publishing-evolve-25977238dbdb?source=post_page-----baa0477f5ac9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Given what I exposed above, it is in my opinion very likely that contests like
    [CASP](https://lucianosphere.medium.com/i-just-answered-a-few-questions-on-alphafold-and-casp-again-547fd628ec29)
    (or like CAMEO, or CAPRI for structure prediction, etc.) and studies dedicated
    to objectively benchmarking existing methods, contribute to advancing the field
    much more than the vast majority of papers reporting new models. In fact, I think
    this so strongly, that to me, ranking top at a contest like CASP or at an independent
    benchmarking study just rules out any paper describing the (presumably) cool results
    of a new method that didn’t perform well in the evaluation (although this doesn’t
    necessarily mean that it might not involve some new methods of potential future
    interest).
  prefs: []
  type: TYPE_NORMAL
- en: Contests like CASP (or like CAMEO, or CAPRI for structure prediction, etc.)
    and studies dedicated to objectively benchmarking existing methods, contribute
    to advancing the field much more than the vast majority of papers reporting new
    models.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A point to note regarding evaluations, specific to protein tertiary structure
    prediction post-AlphaFold 2, is the problem that now only small improvements are
    possible, thus complicating the comparison of different new methods—a problem
    that AlphaFold 2 of course didn’t face because the bar was not high at that moment.
    This is less of a problem in other fields where predictions are still poor to
    intermediate, like in drug design, docking, prediction of conformational dynamics,
    and other questions now that protein tertiary structure prediction is kind of
    solved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/whats-up-after-alphafold-on-ml-for-structural-biology-7bb9758925b8?source=post_page-----baa0477f5ac9--------------------------------)
    [## What’s Up after AlphaFold on ML for Protein Structure Prediction?'
  prefs: []
  type: TYPE_NORMAL
- en: Will the AI-powered revolution in biology keep going? Can we expect a new breakthrough?
    What’s going on right now in…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/whats-up-after-alphafold-on-ml-for-structural-biology-7bb9758925b8?source=post_page-----baa0477f5ac9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: That methods are oversold doesn’t mean they aren’t useful or don’t help advance
    the field!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As scientists explore alternative ways to tackle a problem—via ML as is the
    focus here—they may come up with solutions that might seem revolutionary and have
    great prospects, but ultimately fall short. This doesn’t necessarily mean that
    the new developments aren’t useful for certain applications, or that they haven’t
    generated new knowledge that is useful for future work and to help advance certain
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'One example that represents this very well is the application of language models
    to predictions in structural biology. One of the very first methods reported to
    do this showed substantially lower performance than AlphaFold 2, but execution
    times orders of magnitude faster, which could bring in some benefits for certain
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/protein-structure-prediction-a-million-times-faster-than-alphafold-2-using-a-protein-language-model-d71c55e6a4b7?source=post_page-----baa0477f5ac9--------------------------------)
    [## Protein Structure Prediction a Million Times Faster than AlphaFold 2 Using
    a Protein Language Model'
  prefs: []
  type: TYPE_NORMAL
- en: Summary of the method and its relevance; trying to run it in Google Colab; and
    testing it out to see if it really is…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/protein-structure-prediction-a-million-times-faster-than-alphafold-2-using-a-protein-language-model-d71c55e6a4b7?source=post_page-----baa0477f5ac9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Probably the most popular (and to me useful) of these language-based models
    for protein structure prediction, Meta’s ESMFold, came out soon after. Meta’s
    landing this new neural network together with a huge database of protein structure
    models precomputed with it broke into a short-lived hype, that I think was in
    itself a smaller yet substantial contribution to the revolution started by Deepmind’s
    AlphaFold 2 in the field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-huge-protein-language-models-could-disrupt-structural-biology-6b98193f880b?source=post_page-----baa0477f5ac9--------------------------------)
    [## How Huge Protein Language Models Could Disrupt Structural Biology'
  prefs: []
  type: TYPE_NORMAL
- en: Structure prediction with similar accuracy as AlphaFold but up to 60X faster
    — and having developed new AI methods along…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-huge-protein-language-models-could-disrupt-structural-biology-6b98193f880b?source=post_page-----baa0477f5ac9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'However, although ESMFold is very fast and performs much better than previous
    protein language models for structure prediction, its predictions are a bit behind
    those produced by AlphaFold 2 (besides, ESMFold is limited regarding for example
    the use of custom templates or the prediction of structures for protein complexes).
    You can personally check the relatively poor performance of ESMFold (by today’s
    standards) in the official evaluation carried out during the 15th edition of CASP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://predictioncenter.org/casp15/results.cgi?tr_type=regular&source=post_page-----baa0477f5ac9--------------------------------)
    [## Results — CASP15'
  prefs: []
  type: TYPE_NORMAL
- en: Protein Structure Prediction Center Sponsored by the US National Institute of
    General Medical Sciences (NIH/NIGMS)…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: predictioncenter.org](https://predictioncenter.org/casp15/results.cgi?tr_type=regular&source=post_page-----baa0477f5ac9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'But don’t get me wrong. Meta’s model is very useful and has great potential,
    in practice having paved new roads towards the development of multiple new tools
    that use its protein language model. And I think the model is especially interesting
    from the viewpoint of the internal network’s mechanics for structure prediction,
    which could certainly evolve further in the future. For example, this joint work
    from the Baker lab and Meta showed that language models, though only trained on
    sequences, learn deep enough to design protein structures that go beyond natural
    proteins, even including motifs that are not observed in similar structural contexts
    in known proteins (even tested experimentally!):'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.biorxiv.org/content/10.1101/2022.12.21.521521v1?source=post_page-----baa0477f5ac9--------------------------------)
    [## Language models generalize beyond natural proteins'
  prefs: []
  type: TYPE_NORMAL
- en: Learning the design patterns of proteins from sequences across evolution may
    have promise toward generative protein…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.biorxiv.org](https://www.biorxiv.org/content/10.1101/2022.12.21.521521v1?source=post_page-----baa0477f5ac9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability and how it could help gain trust in ML models, alleviating
    some of their limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the limitations when using ML, not only for structural biology but I’d
    say for most applications in science or engineering, is the lack of interpretability,
    or at least of explicit interpretability. Or in other words, the fact that ML
    models work largely as black boxes, thus being pragmatically useful for the tasks
    or predictions they were designed to execute but not providing many (if any at
    all) insights on how and why they perform so well (or bad, given the case).
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, even for perfect models of presumably high accuracy and reliability,
    it is desirable to understand why and when a model performs well or poorly. Moreover,
    ideally one wants to get these explanations rooted in the fundamental science
    of the domain, say for example in terms of the underlying physics or chemistry,
    in somewhat explicit ways to connect the independent variables just like regular
    modeling uses equations that we humans can understand.
  prefs: []
  type: TYPE_NORMAL
- en: In structural biology in particular, we have the issue that ML models for protein
    structure prediction work very well but little is known about how they achieve
    such good predictions. Thus, it is not very clear whether they’ve learned about
    protein structure something that we don’t know about, or (I think more likely)
    that perhaps we know about but in ways that are too hard to quantify hence hard
    to apply to structure prediction in “analytic” ways. Furthermore, we don’t even
    know whether these ML methods for structure prediction are only excellent predictors
    of folded states or they can also predict intermediates along folding pathways,
    alternative conformational states, the structural propensities of intrinsically
    disordered regions, etc. My guess is they don’t, or at least not with high confidence,
    because by design they are biased to predict structures of well-folded states.
    An explicit explanation about how they achieve such good predictions for such
    states, and what kind of information is flowing through the system, could perhaps
    prove me wrong, and certainly help method developers to understand limitations
    and try to overcome them with improved models.
  prefs: []
  type: TYPE_NORMAL
- en: The lack of interpretability in ML models can be problematic in several ways.
    First, it can be difficult to diagnose and correct errors when a model is not
    performing as expected. This is of special concern when dealing with far-fetched
    extrapolations, say for example predicting the structure of a protein whose structure
    is actually too different from all known structures. Without an understanding
    of how the model arrives at its predictions, it can be difficult to know how to
    fix it and to assess the reliability of each prediction -although modern ML tools
    are increasingly incorporating metrics for prediction reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Second, a lack of interpretability limits our ability to gain insight into the
    underlying physics, chemistry or biology that explains why a given system behaves
    in some way, even if we can correctly predict this behavior. Not a big deal for
    pragmatic applications, but certainly incomplete regarding the fundamental understanding
    that science seeks.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, lack of interpretability can limit our ability to build trust in ML
    models. If we cannot understand how a model arrived at its prediction, we may
    be reluctant to use it in situations where accuracy is critical. In structural
    biology in particular, inaccurate models can lead to incorrect conclusions about
    the function of biological molecules and hinder the progress of all the associated
    studies and developments.
  prefs: []
  type: TYPE_NORMAL
- en: The point of interpretability in the context of this article is that more interpretable
    ML models could alleviate many of the problems associated with their building,
    training, and application to real-world problems; and possibly even identify potential
    problems before they show up upon application, thus improving quality in the balance
    between quantity and quality.
  prefs: []
  type: TYPE_NORMAL
- en: More interpretable ML models could alleviate many of the problems associated
    with their building, training, and application to real-world problems, and thus
    improve quality in its balance with quantity.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are people working on the problem of the interpretability of ML models,
    including some working specifically in the context of scientific applications.
    I will soon write up a blog article about this here.
  prefs: []
  type: TYPE_NORMAL
- en: Related material
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I read great posts by other bloggers and scientists while putting my article
    together; among which these are very recommended -although we do not always agree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[## Some Thoughts on a Mysterious Universe'
  prefs: []
  type: TYPE_NORMAL
- en: by Mohammed AlQuraishi
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: moalquraishi.wordpress.com](https://moalquraishi.wordpress.com/?source=post_page-----baa0477f5ac9--------------------------------)
    [](https://www.blopig.com/blog/author/carlos/?source=post_page-----baa0477f5ac9--------------------------------)
    [## Oxford Protein Informatics Group
  prefs: []
  type: TYPE_NORMAL
- en: Jamboree (1) a large gathering, as of a political party or the teams of a sporting
    league, often including a program of…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.blopig.com](https://www.blopig.com/blog/author/carlos/?source=post_page-----baa0477f5ac9--------------------------------)  [##
    Science, the hard way
  prefs: []
  type: TYPE_NORMAL
- en: Does it ever feel like being a scientist largely consists of wading through
    reams and reams of questionable papers…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: jgreener64.github.io](http://jgreener64.github.io/posts/science_the_hard_way/?source=post_page-----baa0477f5ac9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Liked this piece? Then check out these other articles of mine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/scientists-are-approaching-the-first-near-atomic-simulations-of-whole-cells-cb5b6c27cd?source=post_page-----baa0477f5ac9--------------------------------)
    [## Scientists are approaching the first near-atomic simulations of whole cells'
  prefs: []
  type: TYPE_NORMAL
- en: After the impact of AI on biology, this could represent another milestone, this
    time rooted in pure physics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/scientists-are-approaching-the-first-near-atomic-simulations-of-whole-cells-cb5b6c27cd?source=post_page-----baa0477f5ac9--------------------------------)
    [](https://medium.com/predict/8-modern-and-upcoming-technologies-that-you-must-know-about-1ecf1a09aea7?source=post_page-----baa0477f5ac9--------------------------------)
    [## 8 modern and upcoming technologies that you must know about
  prefs: []
  type: TYPE_NORMAL
- en: From the most modern computer technologies and hardcore physics to the frontiers
    of biotechnology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/predict/8-modern-and-upcoming-technologies-that-you-must-know-about-1ecf1a09aea7?source=post_page-----baa0477f5ac9--------------------------------)  [##
    Key Websites and Programs for Structural Biology and Bioinformatics
  prefs: []
  type: TYPE_NORMAL
- en: Notes from a real-life course for Master and PhD levels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: lucianosphere.medium.com](https://lucianosphere.medium.com/key-websites-and-programs-for-structural-biology-and-bioinformatics-d42c8506082e?source=post_page-----baa0477f5ac9--------------------------------)
    [](https://pub.towardsai.net/building-customized-chatbots-for-the-web-using-gpt-3-5-turbo-190424827493?source=post_page-----baa0477f5ac9--------------------------------)
    [## Building Customized Chatbots for the Web Using gpt-3.5-turbo,
  prefs: []
  type: TYPE_NORMAL
- en: Summary, source code ready to use, and an example chatbot to go play with right
    away
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/building-customized-chatbots-for-the-web-using-gpt-3-5-turbo-190424827493?source=post_page-----baa0477f5ac9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[***www.lucianoabriata.com***](https://www.lucianoabriata.com/) *I write and
    photoshoot about everything that lies in my broad sphere of interests: nature,
    science, technology, programming, etc.* [***Become a Medium member***](https://lucianosphere.medium.com/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and* [***subscribe to get my new stories***](https://lucianosphere.medium.com/subscribe)
    ***by email****. To* ***consult about small jobs,*** *check my* [***services page
    here***](https://lucianoabriata.altervista.org/services/index.html)*. You can*
    [***contact me here***](https://lucianoabriata.altervista.org/office/contact.html)***.***'
  prefs: []
  type: TYPE_NORMAL
