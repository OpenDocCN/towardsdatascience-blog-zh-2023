- en: Advanced Dimensionality Reduction Models Made Simple
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/advanced-dimensionality-reduction-models-made-simple-639fca351528](https://towardsdatascience.com/advanced-dimensionality-reduction-models-made-simple-639fca351528)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how to efficiently apply state-of-the-art Dimensionality Reduction methods
    and boost your Machine Learning models.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page-----639fca351528--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----639fca351528--------------------------------)[](https://towardsdatascience.com/?source=post_page-----639fca351528--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----639fca351528--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----639fca351528--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----639fca351528--------------------------------)
    ·9 min read·Nov 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47503135e2f71ee99e299951edce437f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [unsplash.com](https://unsplash.com/photos/multicolored-wall-in-shallow-focus-photography-jbtfM0XBeRc).'
  prefs: []
  type: TYPE_NORMAL
- en: When approaching a Machine Learning task, have you ever felt stunned by the
    **massive number of features**?
  prefs: []
  type: TYPE_NORMAL
- en: Most Data Scientists experience this overwhelming challenge on a daily basis.
    While adding features enriches data, it often **slows the training process** and
    makes it harder to **detect hidden patterns**, resulting in the (in)famous [**Curse
    of Dimensionality**](https://en.wikipedia.org/wiki/Curse_of_dimensionality)**.**
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, in high-dimensional spaces **surprising phenomena** happen. To depict
    this concept with an analogy, think of the novel Flatland, where characters living
    in a flat (2-dimensional) world find themselves stunned when they encounter a
    3-dimensional being. In the same way, we struggle to comprehend that, in high-dimensional
    spaces, **most of the points are outliers**, and **distances between points are**
    usually **larger** than expected. All these phenomena, if not treated correctly,
    may have **disastrous implications** for our Machine Learning models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1dba4905843191a4a899cc687755aa1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, I will explain some advanced Dimensionality Reduction techniques
    used to mitigate this issue.
  prefs: []
  type: TYPE_NORMAL
- en: In my previous post, I introduced the relevance of **Dimensionality Reduction**
    in Machine Learning problems, and how to tame the **Curse of Dimensionality**,
    and I explained both the theory and Scikit-Learn implementation of the **Principal
    Component Analysis** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In this follow-up, I will dive into additional **Dimensionality Reduction algorithms,**
    like [kPCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html)
    or [LLE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html),
    that overcome the limitations of PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do not worry if you haven’t read my Dimensionality Reduction introduction yet.
    This post is a **stand-alone guide** as I will detail each concept in simple terms.
    However, if you prefer to know more about PCA, I’m positive this guide will serve
    your goal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/dimensionality-reduction-with-scikit-learn-pca-theory-and-implementation-aa224e6ee1f6?source=post_page-----639fca351528--------------------------------)
    [## Dimensionality Reduction with Scikit-Learn: PCA Theory and Implementation'
  prefs: []
  type: TYPE_NORMAL
- en: The Curse of Dimensionality can be tamed! Learn how to do it with Python and
    Scikit-Learn.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/dimensionality-reduction-with-scikit-learn-pca-theory-and-implementation-aa224e6ee1f6?source=post_page-----639fca351528--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Curse of Dimensionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)
    is for sure a **significant challenge** in Data Science, making algorithms computationally
    intensive. When dimensions in a dataset increase, data points become sparse, leading
    to non-trivial computational issues.
  prefs: []
  type: TYPE_NORMAL
- en: Humans struggle to conceptualize beyond 3 (or sometimes 2) dimensions, causing
    **unexpected behavior in high-dimensional spaces**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a 2D square: a random point inside it is unlikely to be near a border.
    However, in higher-dimensional spaces, the probability of a random point to be
    close to a border grows exponentially. In a 10,000-dimensional hypercube, the
    probability near a border exceeds 99.99999%. An experiment shows that the average
    distance between two random points in a 100,000-dimensional hypercube is 129.09,
    highlighting the vastness of high-dimensional spaces. I will leave you with the
    code to test these statements yourself.'
  prefs: []
  type: TYPE_NORMAL
- en: '[## articles/dimensionality-reduction/curse-of-dimensionality.ipynb at main
    · andreoniriccardo/articles'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to andreoniriccardo/articles development by creating an account on
    GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/andreoniriccardo/articles/blob/main/dimensionality-reduction/curse-of-dimensionality.ipynb?source=post_page-----639fca351528--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: For Data Science practitioners, this implies that high-dimensional datasets
    are **incredibly sparse**, with training instances far apart, raising the risk
    of overfitting Machine Learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding more training examples isn’t practical due to the **exponential increase**
    in required points as features grow: To maintain a distance of 0.1 between each
    training point in a 100-feature dataset, you would need to add more points than
    the number of atoms in the universe. The only viable solution is Dimensionality
    Reduction, and I will explore techniques to achieve this without significant information
    loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b020891585e36167499cdb82eeef6bd9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: For a broader introduction to the Curse of Dimensionality topic, I link [this
    article](/dimensionality-reduction-with-scikit-learn-pca-theory-and-implementation-aa224e6ee1f6).
  prefs: []
  type: TYPE_NORMAL
- en: Now it’s time to present some of the **most advanced Dimensionality Reduction
    algorithms**!
  prefs: []
  type: TYPE_NORMAL
- en: Kernel PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Indeed the most diffused Dimensionality Reduction model, [Principal Component
    Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA), **projects
    the data onto a lower-dimensional hyperplane**, lowering the number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: This simple concept struggles when dealing with datasets that can’t be effectively
    projected on a plane without losing a considerable portion of the original information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the synthetic Swiss-roll dataset as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8e7aedae655bfe975635ca432057467.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Projecting data onto a plane is meaningless: **the non-linearity component
    of the original data is lost**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most straightforward solution for applying the PCA algorithm to a vast
    variety of dataset is to exploit the [**Kernel Trick**](https://en.wikipedia.org/wiki/Kernel_method#:~:text=The%20kernel%20trick%20avoids%20the,inner%20product%20in%20another%20space%20.).
    I presented the Kernel Trick in detail in this post:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----639fca351528--------------------------------)
    [## Support Vector Machine with Scikit-Learn: A Friendly Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: Every data scientist should have SVM in their toolbox. Learn how to master this
    versatile model with a hands-on…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----639fca351528--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The Kernel Trick maps the data points of a lower-dimensional space to a higher-dimensional
    one. Support Vector Machine exploits the Kernel Trick to draw linear decision
    boundaries in high-dimensional spaces, resulting in **complex non-linear boundaries**
    in the original feature space.
  prefs: []
  type: TYPE_NORMAL
- en: The same technique can be applied to the PCA algorithm, resulting in the [**Kernel
    PCA**](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis) (kPCA).
  prefs: []
  type: TYPE_NORMAL
- en: Applying the Kernel PCA in Python is easy thanks to the Scikit-learn (sklearn)
    library. In its simplest form, we just need to instantiate a `KernelPCA()` object,
    specify the type of kernel to use, and the output’s number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, we obtain the following reduced dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c8b986084cecb8d7948248c9cec867c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the type of Kernel we pick, kPCA outputs completely different results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/836b20723e37079d83d336191e7c8e12.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this simple example, I experimented with 4 different types of Kernels: Linear,
    Sigmoid, Polynomial, and Cosine. We can immediately appreciate how the Linear
    and Sigmoid Kernels produce acceptable results with the provided dataset. The
    Polynomial and Cosine Kernels appear to be poorly suited for this dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you might be asking yourself the following question:'
  prefs: []
  type: TYPE_NORMAL
- en: Is there a robust method for choosing the right Kernel?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The next section aims to solve this issue!
  prefs: []
  type: TYPE_NORMAL
- en: How to Select the Kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Being Kernel PCA an **unsupervised learning model**, we can’t simply employ
    the Grid Search technique and pick the best hyperparameters based on a performance
    measure. As a consequence selecting the type of Kernel to use is tricky.
  prefs: []
  type: TYPE_NORMAL
- en: If kPCA is simply a preprocessing step of your Machine Learning algorithm, followed
    by a classification or regression model, we can indeed opt for a Grid Search approach.
    The idea is to treat the type of Kernel of kPCA as one of the hyperparameters
    of the Machine Learning model and pick the one that yields the best performance
    measure.
  prefs: []
  type: TYPE_NORMAL
- en: If we are treating the kPCA algorithm as a stand-alone model, an alternative
    consists of choosing the Kernel that yields the lowest reconstruction error.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that with Scikit-learn, we can exploit the `inverse_transform()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/10e34b90342e56444ba73034e43baddc.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see how the kPCA with the Sigmoid kernel performs slightly better than
    the one with the Linear kernel.
  prefs: []
  type: TYPE_NORMAL
- en: LLE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Completely different from what we saw up to this point, there exists a powerful
    Dimensionality Reduction technique that doesn’t rely on projections. [**Locally
    Linear Embedding**](https://cs.nyu.edu/~roweis/lle/papers/lleintro.pdf) (LLE)
    focuses on **preserving local relationships** within the data.
  prefs: []
  type: TYPE_NORMAL
- en: In more simple terms, imagine you have a bunch of datapoints in a high-dimensional
    space, where each dimension represents a different feature. LLE focuses on small
    groups of neighbouring points, rather than looking at the entire dataset at once.
    It tries to represent these nearby points in a lower-dimensional space, while
    maintaining their relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 'The consequent question you might ask now is:'
  prefs: []
  type: TYPE_NORMAL
- en: What does it mean to maintain the relationships between datapoints?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Without involving complex math, for LLE to preserve relationships between datapoints
    means **maintaining the intrinsic geometric structure of the data** in a lower-dimensional
    space. Imagine you have a high-dimensional dataset, where certain points are close
    to each other and share some kind of meaningful relationship. The relationship
    may depend on several factors like proximity, similarity, connectivity, etc. What
    it matters is that LLE aims to keep these relationships in place when reducing
    the dimensionality of the data.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, LLE captures the **underlying structure of the data**, especially
    when the relationships between points are complex and (or) non-linear.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to compare the LLE algorithm with the kPCA, I will apply LLE to the
    same dataset and plot the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1ba5d43fa2883d1a75ddbb8821d322cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Differently from kPCA, LLE appears to “understand” better the patterns and relationships
    of the data and is able to **unroll the Swiss Roll dataset**.
  prefs: []
  type: TYPE_NORMAL
- en: PCA vs LLE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I want to conclude this post about advanced Dimensionality Reduction techniques
    by delineating the differences between the most commonly used algorithms, and
    when it is preferable to apply one instead of the other.
  prefs: []
  type: TYPE_NORMAL
- en: PCA and LLE first diverge on the **approaches** they use. While LLE scans the
    dataset under a local perspective, focusing on preserving the relationships only
    between “adjacent” points, PCA takes a global viewpoint, capturing the overall
    dataset variance.
  prefs: []
  type: TYPE_NORMAL
- en: The **range of application** of these two different algorithms also differs.
    By focusing on a small cluster of data, LLE is able to capture complex and non-linear
    patterns. Instead, PCA, even with the kPCA variation, may fall short at capturing
    more intricate patterns like the Swiss Roll example we saw before.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the way these algorithms are affected by **noise** is a further aspect
    of difference. PCA, by considering the entire datapoints, tends to be more heavily
    impacted by noisy datasets, potentially leading to accuracy losses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60a0927ce2ba7b58e1267e9195e3d6a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: These distinct features of LLE and PCA make them suited for **different sets
    of tasks** and **types of datasets**. LLE is excellent at capturing intricate
    local structures. Instead, PCA is preferred when the global representation of
    the dataset and the preservation of the overall variance is a crucial aspect.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, as the topic of Dimensionality Reduction goes way beyond what a simple
    Medium post achieves to explain, I recommend diving deeper into this fundamental
    Machine Learning argument by reading the material I provide in the “Reference”
    section.
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this story, consider following me to be notified of my upcoming
    projects and articles!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of my past projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec?source=post_page-----639fca351528--------------------------------)
    [## Outlier Detection with Scikit-Learn and Matplotlib: a Practical Guide'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how visualizations, algorithms, and statistics help you to identify anomalies
    for your machine learning tasks.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec?source=post_page-----639fca351528--------------------------------)
    [](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----639fca351528--------------------------------)
    [## Social Network Analysis with NetworkX: A Gentle Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how companies like Facebook and LinkedIn extract insights from networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----639fca351528--------------------------------)
    [](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----639fca351528--------------------------------)
    [## Use Deep Learning to Generate Fantasy Names: Build a Language Model from Scratch'
  prefs: []
  type: TYPE_NORMAL
- en: Can a language model invent unique fantasy character names? Let’s build it from
    scratch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----639fca351528--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Principal component analysis — Sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
    — Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Dive into Deep Learning — Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander
    J. Smola](https://d2l.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Schölkopf, B., Smola, A., & Müller, K. (1998). Nonlinear component analysis
    as a kernel eigenvalue problem. Neural Computation, 10(5), 1299–1319](https://www.mlpack.org/papers/kpca.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Schölkopf, B., & Smola, A. J. (2002). Learning with Kernels: Support Vector
    Machines, Regularization, Optimization, and Beyond. MIT Press.](https://direct.mit.edu/books/book/1821/Learning-with-KernelsSupport-Vector-Machines)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mika, S., Schölkopf, B., Smola, A., Müller, K. R., Scholz, M., & Rätsch, G.
    (1999). Kernel PCA and de-noising in feature spaces. Advances in Neural Information
    Processing Systems, 11.](https://papers.nips.cc/paper/1491-kernel-pca-and-de-noising-in-feature-spaces)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Roweis, S. T., & Saul, L. K. (2000). Nonlinear dimensionality reduction by
    locally linear embedding. Science, 290(5500), 2323–2326.](https://www.science.org/doi/10.1126/science.290.5500.2323)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Roweis, S. (2002). EM algorithms for PCA and SPCA. Advances in Neural Information
    Processing Systems, 15.](https://proceedings.neurips.cc/paper/1398-em-algorithms-for-pca-and-spca.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Saul, L. K., & Roweis, S. T. (2003). Think globally, fit locally: Unsupervised
    learning of low dimensional manifolds. Journal of Machine Learning Research, 4(Nov),
    119–155.](https://psycnet.apa.org/record/2004-13930-001)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
