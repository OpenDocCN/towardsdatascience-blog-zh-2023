- en: Advanced Dimensionality Reduction Models Made Simple
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级维度减少模型简明解析
- en: 原文：[https://towardsdatascience.com/advanced-dimensionality-reduction-models-made-simple-639fca351528](https://towardsdatascience.com/advanced-dimensionality-reduction-models-made-simple-639fca351528)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/advanced-dimensionality-reduction-models-made-simple-639fca351528](https://towardsdatascience.com/advanced-dimensionality-reduction-models-made-simple-639fca351528)
- en: Learn how to efficiently apply state-of-the-art Dimensionality Reduction methods
    and boost your Machine Learning models.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何高效地应用最先进的维度减少方法，并提升你的机器学习模型。
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page-----639fca351528--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----639fca351528--------------------------------)[](https://towardsdatascience.com/?source=post_page-----639fca351528--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----639fca351528--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----639fca351528--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@riccardo.andreoni?source=post_page-----639fca351528--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----639fca351528--------------------------------)[](https://towardsdatascience.com/?source=post_page-----639fca351528--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----639fca351528--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----639fca351528--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----639fca351528--------------------------------)
    ·9 min read·Nov 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----639fca351528--------------------------------)
    ·阅读时间9分钟·2023年11月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/47503135e2f71ee99e299951edce437f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47503135e2f71ee99e299951edce437f.png)'
- en: 'Image source: [unsplash.com](https://unsplash.com/photos/multicolored-wall-in-shallow-focus-photography-jbtfM0XBeRc).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[unsplash.com](https://unsplash.com/photos/multicolored-wall-in-shallow-focus-photography-jbtfM0XBeRc)。
- en: When approaching a Machine Learning task, have you ever felt stunned by the
    **massive number of features**?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当面对机器学习任务时，你是否曾因**特征数量庞大**而感到震惊？
- en: Most Data Scientists experience this overwhelming challenge on a daily basis.
    While adding features enriches data, it often **slows the training process** and
    makes it harder to **detect hidden patterns**, resulting in the (in)famous [**Curse
    of Dimensionality**](https://en.wikipedia.org/wiki/Curse_of_dimensionality)**.**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据科学家每天都会面临这一令人难以承受的挑战。虽然增加特征可以丰富数据，但它往往会**减缓训练过程**并使**发现隐藏模式**变得更加困难，结果就是（不）著名的[**维度诅咒**](https://en.wikipedia.org/wiki/Curse_of_dimensionality)**。**
- en: Moreover, in high-dimensional spaces **surprising phenomena** happen. To depict
    this concept with an analogy, think of the novel Flatland, where characters living
    in a flat (2-dimensional) world find themselves stunned when they encounter a
    3-dimensional being. In the same way, we struggle to comprehend that, in high-dimensional
    spaces, **most of the points are outliers**, and **distances between points are**
    usually **larger** than expected. All these phenomena, if not treated correctly,
    may have **disastrous implications** for our Machine Learning models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在高维空间中会发生**惊人的现象**。为了用类比来描述这个概念，可以想象小说《平面国》，其中生活在二维平面世界的角色在遇到三维生物时感到震惊。类似地，我们难以理解在高维空间中，**大多数点都是异常值**，而且**点之间的距离**通常**比预期要大**。所有这些现象如果处理不当，可能对我们的机器学习模型产生**灾难性的影响**。
- en: '![](../Images/1dba4905843191a4a899cc687755aa1d.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dba4905843191a4a899cc687755aa1d.png)'
- en: Image by the author.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: In this post, I will explain some advanced Dimensionality Reduction techniques
    used to mitigate this issue.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将解释一些用于缓解这一问题的高级维度减少技术。
- en: In my previous post, I introduced the relevance of **Dimensionality Reduction**
    in Machine Learning problems, and how to tame the **Curse of Dimensionality**,
    and I explained both the theory and Scikit-Learn implementation of the **Principal
    Component Analysis** algorithm.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的文章中，我介绍了**维度减少**在机器学习问题中的相关性，以及如何驯服**维度诅咒**，并解释了**主成分分析**算法的理论和Scikit-Learn实现。
- en: In this follow-up, I will dive into additional **Dimensionality Reduction algorithms,**
    like [kPCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html)
    or [LLE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html),
    that overcome the limitations of PCA.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我将深入探讨更多**降维算法**，例如 [kPCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html)
    或 [LLE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html)，这些算法克服了PCA的局限性。
- en: 'Do not worry if you haven’t read my Dimensionality Reduction introduction yet.
    This post is a **stand-alone guide** as I will detail each concept in simple terms.
    However, if you prefer to know more about PCA, I’m positive this guide will serve
    your goal:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有阅读我的降维介绍，请不要担心。这篇文章是**独立的指南**，因为我会用简单的术语详细讲解每个概念。然而，如果你希望了解更多关于PCA的内容，我相信这篇指南会帮助你实现目标：
- en: '[](/dimensionality-reduction-with-scikit-learn-pca-theory-and-implementation-aa224e6ee1f6?source=post_page-----639fca351528--------------------------------)
    [## Dimensionality Reduction with Scikit-Learn: PCA Theory and Implementation'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/dimensionality-reduction-with-scikit-learn-pca-theory-and-implementation-aa224e6ee1f6?source=post_page-----639fca351528--------------------------------)
    [## 使用 Scikit-Learn 进行降维：PCA 理论与实现]'
- en: The Curse of Dimensionality can be tamed! Learn how to do it with Python and
    Scikit-Learn.
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度诅咒是可以被驯服的！学习如何使用 Python 和 Scikit-Learn 来实现它。
- en: towardsdatascience.com](/dimensionality-reduction-with-scikit-learn-pca-theory-and-implementation-aa224e6ee1f6?source=post_page-----639fca351528--------------------------------)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/dimensionality-reduction-with-scikit-learn-pca-theory-and-implementation-aa224e6ee1f6?source=post_page-----639fca351528--------------------------------)'
- en: Curse of Dimensionality
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度诅咒
- en: The [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)
    is for sure a **significant challenge** in Data Science, making algorithms computationally
    intensive. When dimensions in a dataset increase, data points become sparse, leading
    to non-trivial computational issues.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[维度诅咒](https://en.wikipedia.org/wiki/Curse_of_dimensionality)确实是数据科学中的一个**重大挑战**，使得算法在计算上变得复杂。当数据集中的维度增加时，数据点变得稀疏，从而导致非平凡的计算问题。'
- en: Humans struggle to conceptualize beyond 3 (or sometimes 2) dimensions, causing
    **unexpected behavior in high-dimensional spaces**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在理解超过3（有时是2）维的空间时感到困难，这导致了**高维空间中的意外行为**。
- en: 'Consider a 2D square: a random point inside it is unlikely to be near a border.
    However, in higher-dimensional spaces, the probability of a random point to be
    close to a border grows exponentially. In a 10,000-dimensional hypercube, the
    probability near a border exceeds 99.99999%. An experiment shows that the average
    distance between two random points in a 100,000-dimensional hypercube is 129.09,
    highlighting the vastness of high-dimensional spaces. I will leave you with the
    code to test these statements yourself.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个二维正方形：随机选取一个点，它不太可能靠近边界。然而，在高维空间中，随机点靠近边界的概率会呈指数增长。在一个10,000维的超立方体中，靠近边界的概率超过99.99999%。一个实验显示，在一个100,000维的超立方体中，两个随机点之间的平均距离为129.09，这突显了高维空间的广阔。我将给你留下代码，供你自己测试这些说法。
- en: '[## articles/dimensionality-reduction/curse-of-dimensionality.ipynb at main
    · andreoniriccardo/articles'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[## articles/dimensionality-reduction/curse-of-dimensionality.ipynb at main
    · andreoniriccardo/articles]'
- en: Contribute to andreoniriccardo/articles development by creating an account on
    GitHub.
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在 GitHub 上创建帐户来贡献 andreoniriccardo/articles 的开发。
- en: github.com](https://github.com/andreoniriccardo/articles/blob/main/dimensionality-reduction/curse-of-dimensionality.ipynb?source=post_page-----639fca351528--------------------------------)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/andreoniriccardo/articles/blob/main/dimensionality-reduction/curse-of-dimensionality.ipynb?source=post_page-----639fca351528--------------------------------)'
- en: For Data Science practitioners, this implies that high-dimensional datasets
    are **incredibly sparse**, with training instances far apart, raising the risk
    of overfitting Machine Learning models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据科学从业者来说，这意味着高维数据集**极其稀疏**，训练实例彼此之间相距较远，这增加了过拟合机器学习模型的风险。
- en: 'Adding more training examples isn’t practical due to the **exponential increase**
    in required points as features grow: To maintain a distance of 0.1 between each
    training point in a 100-feature dataset, you would need to add more points than
    the number of atoms in the universe. The only viable solution is Dimensionality
    Reduction, and I will explore techniques to achieve this without significant information
    loss.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随着特征的增加所需点数的**指数级增长**，添加更多的训练样本在实际操作中不可行：为了在一个 100 特征的数据集中保持每个训练点之间的距离为 0.1，你需要添加比宇宙中原子数量还多的点。唯一可行的解决方案是维度降低，我将探索在不显著丢失信息的情况下实现这一目标的技术。
- en: '![](../Images/b020891585e36167499cdb82eeef6bd9.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b020891585e36167499cdb82eeef6bd9.png)'
- en: Image by the author.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: For a broader introduction to the Curse of Dimensionality topic, I link [this
    article](/dimensionality-reduction-with-scikit-learn-pca-theory-and-implementation-aa224e6ee1f6).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 关于维度诅咒主题的更广泛介绍，我链接了[这篇文章](/dimensionality-reduction-with-scikit-learn-pca-theory-and-implementation-aa224e6ee1f6)。
- en: Now it’s time to present some of the **most advanced Dimensionality Reduction
    algorithms**!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是展示一些**最先进的维度降低算法**的时候了！
- en: Kernel PCA
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核主成分分析
- en: Indeed the most diffused Dimensionality Reduction model, [Principal Component
    Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA), **projects
    the data onto a lower-dimensional hyperplane**, lowering the number of dimensions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，最广泛应用的维度降低模型[主成分分析](https://en.wikipedia.org/wiki/Principal_component_analysis)（PCA），**将数据投影到低维超平面上**，降低了维度数量。
- en: This simple concept struggles when dealing with datasets that can’t be effectively
    projected on a plane without losing a considerable portion of the original information.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的概念在处理那些不能有效投影到平面上而不丢失大量原始信息的数据集时会遇到困难。
- en: 'Take the synthetic Swiss-roll dataset as an example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以合成的瑞士卷数据集为例：
- en: '![](../Images/f8e7aedae655bfe975635ca432057467.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8e7aedae655bfe975635ca432057467.png)'
- en: Image by the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: 'Projecting data onto a plane is meaningless: **the non-linearity component
    of the original data is lost**.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据投影到平面上毫无意义：**原始数据的非线性组件被丢失**。
- en: 'The most straightforward solution for applying the PCA algorithm to a vast
    variety of dataset is to exploit the [**Kernel Trick**](https://en.wikipedia.org/wiki/Kernel_method#:~:text=The%20kernel%20trick%20avoids%20the,inner%20product%20in%20another%20space%20.).
    I presented the Kernel Trick in detail in this post:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 应用 PCA 算法到各种数据集的最直接解决方案是利用[**核技巧**](https://en.wikipedia.org/wiki/Kernel_method#:~:text=The%20kernel%20trick%20avoids%20the,inner%20product%20in%20another%20space%20.)。我在这篇文章中详细介绍了核技巧：
- en: '[](/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----639fca351528--------------------------------)
    [## Support Vector Machine with Scikit-Learn: A Friendly Introduction'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----639fca351528--------------------------------)
    [## 使用 Scikit-Learn 进行支持向量机：一个友好的介绍'
- en: Every data scientist should have SVM in their toolbox. Learn how to master this
    versatile model with a hands-on…
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每位数据科学家都应该在工具箱中拥有 SVM。学习如何通过实践掌握这一多功能模型…
- en: towardsdatascience.com](/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----639fca351528--------------------------------)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----639fca351528--------------------------------)
- en: The Kernel Trick maps the data points of a lower-dimensional space to a higher-dimensional
    one. Support Vector Machine exploits the Kernel Trick to draw linear decision
    boundaries in high-dimensional spaces, resulting in **complex non-linear boundaries**
    in the original feature space.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 核技巧将低维空间的数据点映射到高维空间。支持向量机利用核技巧在高维空间中绘制线性决策边界，从而在原始特征空间中产生**复杂的非线性边界**。
- en: The same technique can be applied to the PCA algorithm, resulting in the [**Kernel
    PCA**](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis) (kPCA).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的技术可以应用于 PCA 算法，得到[**核主成分分析**](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis)（kPCA）。
- en: Applying the Kernel PCA in Python is easy thanks to the Scikit-learn (sklearn)
    library. In its simplest form, we just need to instantiate a `KernelPCA()` object,
    specify the type of kernel to use, and the output’s number of dimensions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Scikit-learn (sklearn) 库，应用核主成分分析在 Python 中非常简单。在最简单的形式中，我们只需实例化一个 `KernelPCA()`
    对象，指定要使用的核类型和输出的维度数。
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As a result, we obtain the following reduced dataset:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是我们得到以下的降维数据集：
- en: '![](../Images/4c8b986084cecb8d7948248c9cec867c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c8b986084cecb8d7948248c9cec867c.png)'
- en: Image by the author.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: 'Based on the type of Kernel we pick, kPCA outputs completely different results:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们选择的核函数类型，kPCA会输出完全不同的结果：
- en: '![](../Images/836b20723e37079d83d336191e7c8e12.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/836b20723e37079d83d336191e7c8e12.png)'
- en: Image by the author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: 'In this simple example, I experimented with 4 different types of Kernels: Linear,
    Sigmoid, Polynomial, and Cosine. We can immediately appreciate how the Linear
    and Sigmoid Kernels produce acceptable results with the provided dataset. The
    Polynomial and Cosine Kernels appear to be poorly suited for this dataset.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的示例中，我实验了4种不同类型的核函数：线性、Sigmoid、多项式和余弦。我们可以立即看到线性核和Sigmoid核在提供的数据集上产生了令人满意的结果。而多项式核和余弦核似乎不适合这个数据集。
- en: 'At this point, you might be asking yourself the following question:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能会问以下问题：
- en: Is there a robust method for choosing the right Kernel?
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 是否有一种可靠的方法来选择合适的核函数？
- en: The next section aims to solve this issue!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分旨在解决这个问题！
- en: How to Select the Kernel
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何选择核函数
- en: Being Kernel PCA an **unsupervised learning model**, we can’t simply employ
    the Grid Search technique and pick the best hyperparameters based on a performance
    measure. As a consequence selecting the type of Kernel to use is tricky.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作为**无监督学习模型**的Kernel PCA，我们不能仅仅使用网格搜索技术并根据性能指标选择最佳超参数。因此，选择使用哪种核函数是棘手的。
- en: If kPCA is simply a preprocessing step of your Machine Learning algorithm, followed
    by a classification or regression model, we can indeed opt for a Grid Search approach.
    The idea is to treat the type of Kernel of kPCA as one of the hyperparameters
    of the Machine Learning model and pick the one that yields the best performance
    measure.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果kPCA只是你的机器学习算法的一个预处理步骤，后面跟着分类或回归模型，我们确实可以选择网格搜索方法。这个想法是将kPCA的核函数类型视为机器学习模型的一个超参数，并选择产生最佳性能度量的核函数。
- en: If we are treating the kPCA algorithm as a stand-alone model, an alternative
    consists of choosing the Kernel that yields the lowest reconstruction error.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将kPCA算法视为一个独立的模型，则可以选择使重建误差最低的核函数作为替代方案。
- en: 'To do that with Scikit-learn, we can exploit the `inverse_transform()` method:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用Scikit-learn实现这一点，我们可以利用`inverse_transform()`方法：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/10e34b90342e56444ba73034e43baddc.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10e34b90342e56444ba73034e43baddc.png)'
- en: We can see how the kPCA with the Sigmoid kernel performs slightly better than
    the one with the Linear kernel.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，使用Sigmoid核的kPCA表现略优于使用线性核的kPCA。
- en: LLE
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLE
- en: Completely different from what we saw up to this point, there exists a powerful
    Dimensionality Reduction technique that doesn’t rely on projections. [**Locally
    Linear Embedding**](https://cs.nyu.edu/~roweis/lle/papers/lleintro.pdf) (LLE)
    focuses on **preserving local relationships** within the data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们迄今所见的完全不同，存在一种强大的降维技术，它不依赖于投影。[**局部线性嵌入**](https://cs.nyu.edu/~roweis/lle/papers/lleintro.pdf)（LLE）专注于**保留数据中的局部关系**。
- en: In more simple terms, imagine you have a bunch of datapoints in a high-dimensional
    space, where each dimension represents a different feature. LLE focuses on small
    groups of neighbouring points, rather than looking at the entire dataset at once.
    It tries to represent these nearby points in a lower-dimensional space, while
    maintaining their relationships.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单地说，想象一下你在一个高维空间中有一堆数据点，每个维度代表不同的特征。LLE（局部线性嵌入）关注的是小的邻域点群，而不是一次性查看整个数据集。它尝试在低维空间中表示这些邻近点，同时保持它们之间的关系。
- en: 'The consequent question you might ask now is:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可能会问的随之而来的问题是：
- en: What does it mean to maintain the relationships between datapoints?
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 维护数据点之间关系意味着什么？
- en: Without involving complex math, for LLE to preserve relationships between datapoints
    means **maintaining the intrinsic geometric structure of the data** in a lower-dimensional
    space. Imagine you have a high-dimensional dataset, where certain points are close
    to each other and share some kind of meaningful relationship. The relationship
    may depend on several factors like proximity, similarity, connectivity, etc. What
    it matters is that LLE aims to keep these relationships in place when reducing
    the dimensionality of the data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在不涉及复杂数学的情况下，LLE保留数据点之间的关系意味着在低维空间中**保持数据的内在几何结构**。想象你有一个高维数据集，其中某些点彼此接近，并且共享某种有意义的关系。这个关系可能依赖于多个因素，如接近度、相似性、连通性等。重要的是，LLE旨在在降低数据维度时保持这些关系。
- en: As a result, LLE captures the **underlying structure of the data**, especially
    when the relationships between points are complex and (or) non-linear.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，LLE捕捉到**数据的潜在结构**，尤其是当点之间的关系复杂且（或）非线性时。
- en: 'In order to compare the LLE algorithm with the kPCA, I will apply LLE to the
    same dataset and plot the results:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将LLE算法与kPCA进行比较，我将LLE应用于相同的数据集并绘制结果：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/1ba5d43fa2883d1a75ddbb8821d322cf.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ba5d43fa2883d1a75ddbb8821d322cf.png)'
- en: Image by the author.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: Differently from kPCA, LLE appears to “understand” better the patterns and relationships
    of the data and is able to **unroll the Swiss Roll dataset**.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与kPCA不同，LLE似乎能更好地“理解”数据的模式和关系，并能够**展开瑞士卷数据集**。
- en: PCA vs LLE
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA与LLE
- en: I want to conclude this post about advanced Dimensionality Reduction techniques
    by delineating the differences between the most commonly used algorithms, and
    when it is preferable to apply one instead of the other.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我想通过描述最常用算法之间的差异，并阐明在何时优先应用某一种算法，来总结这篇关于高级降维技术的文章。
- en: PCA and LLE first diverge on the **approaches** they use. While LLE scans the
    dataset under a local perspective, focusing on preserving the relationships only
    between “adjacent” points, PCA takes a global viewpoint, capturing the overall
    dataset variance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: PCA和LLE首先在它们使用的**方法**上有所不同。LLE在局部视角下扫描数据集，专注于保留“邻近”点之间的关系，而PCA采取全局视角，捕捉整体数据集的方差。
- en: The **range of application** of these two different algorithms also differs.
    By focusing on a small cluster of data, LLE is able to capture complex and non-linear
    patterns. Instead, PCA, even with the kPCA variation, may fall short at capturing
    more intricate patterns like the Swiss Roll example we saw before.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种不同算法的**应用范围**也有所不同。通过关注小的数据显示集，LLE能够捕捉复杂且非线性的模式。而PCA，即使是kPCA变体，可能在捕捉像我们之前看到的瑞士卷示例这种更复杂的模式时表现不佳。
- en: Finally, the way these algorithms are affected by **noise** is a further aspect
    of difference. PCA, by considering the entire datapoints, tends to be more heavily
    impacted by noisy datasets, potentially leading to accuracy losses.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这些算法如何受到**噪声**的影响是另一个区别。由于考虑了所有数据点，PCA往往会受到噪声数据集的更大影响，可能导致准确性下降。
- en: '![](../Images/60a0927ce2ba7b58e1267e9195e3d6a9.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60a0927ce2ba7b58e1267e9195e3d6a9.png)'
- en: Image by the author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: These distinct features of LLE and PCA make them suited for **different sets
    of tasks** and **types of datasets**. LLE is excellent at capturing intricate
    local structures. Instead, PCA is preferred when the global representation of
    the dataset and the preservation of the overall variance is a crucial aspect.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: LLE和PCA的这些不同特性使它们适用于**不同的任务集**和**数据集类型**。LLE在捕捉复杂的局部结构方面表现出色。而当数据集的全局表示和整体方差的保留是关键时，PCA更受青睐。
- en: Finally, as the topic of Dimensionality Reduction goes way beyond what a simple
    Medium post achieves to explain, I recommend diving deeper into this fundamental
    Machine Learning argument by reading the material I provide in the “Reference”
    section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于降维的主题远远超出了简单的Medium帖子所能解释的范围，我建议通过阅读“参考”部分提供的材料，深入探讨这一基础的机器学习问题。
- en: If you liked this story, consider following me to be notified of my upcoming
    projects and articles!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这个故事，可以关注我，以便了解我的即将发布的项目和文章！
- en: 'Here are some of my past projects:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我过去的一些项目：
- en: '[](/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec?source=post_page-----639fca351528--------------------------------)
    [## Outlier Detection with Scikit-Learn and Matplotlib: a Practical Guide'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec?source=post_page-----639fca351528--------------------------------)
    [## 使用 Scikit-Learn 和 Matplotlib 进行异常检测：实用指南'
- en: Learn how visualizations, algorithms, and statistics help you to identify anomalies
    for your machine learning tasks.
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解可视化、算法和统计如何帮助你识别机器学习任务中的异常。
- en: 'towardsdatascience.com](/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec?source=post_page-----639fca351528--------------------------------)
    [](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----639fca351528--------------------------------)
    [## Social Network Analysis with NetworkX: A Gentle Introduction'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/outlier-detection-with-scikit-learn-and-matplotlib-a-practical-guide-382d1411b8ec?source=post_page-----639fca351528--------------------------------)
    [](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----639fca351528--------------------------------)
    [## 使用 NetworkX 进行社交网络分析：温和入门
- en: Learn how companies like Facebook and LinkedIn extract insights from networks
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解像 Facebook 和 LinkedIn 这样的公司如何从网络中提取洞察。
- en: 'towardsdatascience.com](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----639fca351528--------------------------------)
    [](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----639fca351528--------------------------------)
    [## Use Deep Learning to Generate Fantasy Names: Build a Language Model from Scratch'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----639fca351528--------------------------------)
    [](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----639fca351528--------------------------------)
    [## 使用深度学习生成奇幻角色名称：从零构建语言模型
- en: Can a language model invent unique fantasy character names? Let’s build it from
    scratch
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言模型能否发明独特的奇幻角色名称？让我们从头开始构建它。
- en: towardsdatascience.com](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----639fca351528--------------------------------)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----639fca351528--------------------------------)
- en: References
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Principal component analysis — Sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[主成分分析 — Sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)'
- en: '[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
    — Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-Learn、Keras 和 TensorFlow 的实战机器学习，第2版 — Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)'
- en: '[Dive into Deep Learning — Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander
    J. Smola](https://d2l.ai/)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深入学习 — Aston Zhang, Zachary C. Lipton, Mu Li, 和 Alexander J. Smola](https://d2l.ai/)'
- en: '[Schölkopf, B., Smola, A., & Müller, K. (1998). Nonlinear component analysis
    as a kernel eigenvalue problem. Neural Computation, 10(5), 1299–1319](https://www.mlpack.org/papers/kpca.pdf).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Schölkopf, B., Smola, A., & Müller, K. (1998). 作为核特征值问题的非线性分量分析。神经计算，10(5)，1299–1319](https://www.mlpack.org/papers/kpca.pdf)。'
- en: '[Schölkopf, B., & Smola, A. J. (2002). Learning with Kernels: Support Vector
    Machines, Regularization, Optimization, and Beyond. MIT Press.](https://direct.mit.edu/books/book/1821/Learning-with-KernelsSupport-Vector-Machines)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Schölkopf, B., & Smola, A. J. (2002). 《学习与核方法：支持向量机、正则化、优化及其他》。MIT出版社。](https://direct.mit.edu/books/book/1821/Learning-with-KernelsSupport-Vector-Machines)'
- en: '[Mika, S., Schölkopf, B., Smola, A., Müller, K. R., Scholz, M., & Rätsch, G.
    (1999). Kernel PCA and de-noising in feature spaces. Advances in Neural Information
    Processing Systems, 11.](https://papers.nips.cc/paper/1491-kernel-pca-and-de-noising-in-feature-spaces)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mika, S., Schölkopf, B., Smola, A., Müller, K. R., Scholz, M., & Rätsch, G.
    (1999). 核主成分分析及特征空间去噪。神经信息处理系统进展，11。](https://papers.nips.cc/paper/1491-kernel-pca-and-de-noising-in-feature-spaces)'
- en: '[Roweis, S. T., & Saul, L. K. (2000). Nonlinear dimensionality reduction by
    locally linear embedding. Science, 290(5500), 2323–2326.](https://www.science.org/doi/10.1126/science.290.5500.2323)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Roweis, S. T., & Saul, L. K. (2000). 通过局部线性嵌入进行非线性维度约简。科学，290(5500)，2323–2326。](https://www.science.org/doi/10.1126/science.290.5500.2323)'
- en: '[Roweis, S. (2002). EM algorithms for PCA and SPCA. Advances in Neural Information
    Processing Systems, 15.](https://proceedings.neurips.cc/paper/1398-em-algorithms-for-pca-and-spca.pdf)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Roweis, S. (2002). PCA 和 SPCA 的 EM 算法。神经信息处理系统进展，15。](https://proceedings.neurips.cc/paper/1398-em-algorithms-for-pca-and-spca.pdf)'
- en: '[Saul, L. K., & Roweis, S. T. (2003). Think globally, fit locally: Unsupervised
    learning of low dimensional manifolds. Journal of Machine Learning Research, 4(Nov),
    119–155.](https://psycnet.apa.org/record/2004-13930-001)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Saul, L. K. 和 Roweis, S. T. (2003). 从全球视角思考，本地拟合：低维流形的无监督学习. 机器学习研究期刊, 4(11月),
    119–155.](https://psycnet.apa.org/record/2004-13930-001)'
