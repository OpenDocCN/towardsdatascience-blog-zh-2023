# 通用模型的非凡有效性

> 原文：[https://towardsdatascience.com/the-unreasonable-effectiveness-of-general-models-b4e822eaeb27](https://towardsdatascience.com/the-unreasonable-effectiveness-of-general-models-b4e822eaeb27)

## 在一个极其困难的问题上测试通用模型

[](https://medium.com/@mazzanti.sam?source=post_page-----b4e822eaeb27--------------------------------)[![Samuele Mazzanti](../Images/432477d6418a3f79bf25dec42755d364.png)](https://medium.com/@mazzanti.sam?source=post_page-----b4e822eaeb27--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b4e822eaeb27--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b4e822eaeb27--------------------------------) [Samuele Mazzanti](https://medium.com/@mazzanti.sam?source=post_page-----b4e822eaeb27--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b4e822eaeb27--------------------------------) ·8分钟阅读·2023年1月17日

--

![](../Images/e0cce5f765f3d90678879ef2406c1b7d.png)

[作者提供的图像，由 Excalidraw 制作]

在 [上一篇文章](https://medium.com/towards-data-science/what-is-better-one-general-model-or-many-specialized-models-9500d9f8751d) 中，我尝试揭示一种有些模糊的观点，即一堆模型（每个模型专注于数据集的一个子集）应该比单一模型表现更好。

为了做到这一点，我取了一部分数据集（例如，仅美国客户），并在该组上训练了一个模型，即 **专用模型**。然后，我在整个数据集（即所有客户，无论其国籍）上训练了第二个模型，即 **通用模型**。最后，我比较了这两个模型在只包含该组观察结果的保留集上的表现。

我在多个数据集和同一数据集的多个组上重复了这个过程，总共进行了600次比较。 **最终的赢家是通用模型**。

然而，我的实验并没有说服一些人，他们认为我的方法过于简化。例如，这是我在 LinkedIn 上关于该文章的帖子下最受欢迎的评论之一：

![](../Images/4fdb1b7f061a8941066769f1d1c26b1b.png)

[从我 [Linkedin 帖子](https://www.linkedin.com/feed/update/urn:li:activity:7015963227549282305/)] 评论区的截图

这个评论引起了我的兴趣，所以我决定按照建议去做。如果你想看看结果如何，请耐心等待。

# 一个工作假设

在我之前的文章中，我证明了当数据集中组成各组之间存在一定相似性时，使用通用模型相对于专用模型有明显的好处。

然而，**随着组之间的差异越来越大**，合理地预期**使用通用模型的好处会越来越小**。在最极端的情况下，即当组之间完全不同时，两种方法之间的差异应该等于零。

如果我的直觉是正确的，我们可以将这种关系草图如下：

![](../Images/9916f64ce2db6b94172f99af9069dea5.png)

我的工作假设的草图。你可以在[这里](/what-is-better-one-general-model-or-many-specialized-models-9500d9f8751d)阅读“上一篇文章”。[图片由作者提供，使用Excalidraw制作]

但这只是我的假设。那我们来尝试一下。

# 给我们的模型增加难度

我们的目标是回答这个问题：

> 如果组成一个数据集的组彼此完全不同，而我们仍然使用一个通用模型，会发生什么？

所以，问题变成了如何模拟这样的场景。

最极端的想法是“粘合”不同的数据集。当我说“不同”时，我指的是数据集**不仅有不同的列，还有不同的任务**，即它们旨在预测不同的内容。

以三个数据集为例：

+   “银行”：每一行代表一个银行客户，任务是预测他/她是否会订阅定期存款；

+   “员工”：每一行代表一名员工，任务是预测他/她是否会离开公司；

+   “收入”：每一行代表一个人，任务是预测他/她的收入是否超过50k美元。

将这些数据集的目标变量粘合在一起很简单：由于它们都是由0和1组成的二元变量，这很直接。但当我们尝试连接特征时，情况变得更加复杂。我来解释一下原因。

这里是三个数据集的一个样本（包括行和列）。

![](../Images/e8add37e63e1c1e711e4d533ce9e9852.png)

三个示例数据集：“银行”、“员工”和“收入”。[图片由作者提供]

如你所见，这些数据集有不同的列。那么，我们如何将它们合并在一起？第一个最简单的想法是使用`pd.concat`：

[PRE0]

但是，如果我们这样做，我们将得到以下形式的数据框：

![](../Images/41a85c6ab8195d7ca888338f31f6d095.png)

第一次尝试：简单的连接。[图片由作者提供]

Pandas 默认只连接具有相同名称的列。在这种情况下，每个数据集具有不同的列名，因此结果具有类似对角线的结构。但这并不令人满意，因为它会让模型走捷径。事实上，模型能够根据非空列隐式地区分不同的数据集。

为了避免这种情况，**我们需要一种“强制”合并不同数据集列的方法**。

我能想到的唯一方法是用递增的数字重命名每个数据集的列：“feature_01”，“feature_02”等。但这行不通，因为列有不同的类型。所以我们需要区分分类特征和数值特征：“cat_feature_01”，“cat_feature_02”等和“num_feature_01”，“num_feature_02”等。此外，我决定按重要性降序排列特征。

这是得到的输出：

![](../Images/a3999c3f617d6490fb42bf5cd4354b24.png)

第二次尝试：用递增的数字重命名列。[作者提供的图片]

也许你会认为这还不够。毕竟，模型可能仍然识别出属于某个数据集的某些类别（例如，列“cat_feature_01”中的“已婚”仅存在于“银行”数据集中）。数值特征也是如此（例如，列“num_feature_02”中的0到1之间的值仅存在于“员工”数据集中）。这对模型仍然可能有帮助，我们希望避免这种情况。

因此，作为额外步骤，我：

+   将每个分类特征的每个值映射到不同的整数（序数编码）；

+   通过减去均值并除以标准差，对每个原始数据集的数值列进行了标准化。

所以，这就是最终结果：

![](../Images/adb8ab0d7c4f230ebcefd190bf42bab3.png)

第三次也是最后一次尝试：序数编码、标准化，然后用递增的数字重命名列。[作者提供的图片]

# 免责声明

我知道你可能会觉得这个过程——巧妙地将一些完全不相关的数据集合并在一起——有点奇怪。你是对的：我们所做的在现实世界中没有意义。

但你必须记住，这**是一个教学实验，旨在推动通用模型的能力到极限，并观察它是否仍然能与专用模型竞争**。

这个实验必须被视为对基于树的梯度提升模型能力的一种“压力测试”。

# 结果

现在我们已经设计了策略，是时候将其应用于一些实际数据集了。我使用了7个用于二分类的具有5000多行的数据集，这些数据集可以在[Pycaret](https://github.com/pycaret/pycaret)（一个[MIT许可证](https://github.com/pycaret/pycaret/blob/master/LICENSE)下的Python库）中找到。

这些是数据集，及其相应的行数和列数：

![](../Images/1525434f90efb6f1ab827f4411f2dcdf.png)

Pycaret数据集及其形状。[作者提供的图片]

然后，我应用了上述程序，这意味着我对每个数据集分别执行了以下操作：

+   我将每个分类列（按重要性降序排列）重命名为“cat_feature_01”，“cat_feature_02”，…，每个数值列（按重要性降序排列）重命名为“num_feature_01”，“num_feature_02”，…；

+   对每个分类列，我将每个值映射到一个不同的整数：0，1，2，…；

+   对于每一列数值数据，我已经通过减去均值并除以标准差来标准化值；

+   我添加了一个包含数据集名称的列。

然后，我将所有原始数据集合并以获得最终数据集。在此基础上，我进行了实验。实验内容包括：

+   在合并后的完整数据集上训练一个通用模型（Catboost，未进行参数调优）；

+   训练了 7 个专业模型（Catboost，未进行参数调优），每个模型对应一个原始数据集；

+   比较通用模型和专业模型在每个数据集上的表现。

![](../Images/3366eada72820b9af39c9e6de28d7661.png)

在数据集的一组上比较通用模型和专业模型的过程。[作者提供的图片]

我在查看结果时注意到的第一件事是**通用模型的预测与专业模型的预测之间的相关性为 98%**，这表明它们产生了非常相似的输出。

那么性能如何呢？

这是通用模型与专业模型 ROC 分数的比较：

![](../Images/1f05605cbf84d974915079c7a1cdbba1.png)

ROC 分数对比。[作者提供的图片]

通用模型的 ROC 分数与专业模型的 ROC 分数之间的平均差异是 -0.53%。这意味着**专业模型通常优于通用模型**。

然而，我必须说，我对这种微小差异感到印象深刻。**我们在一个极其困难的设置中进行了测试，通用模型仍然能够达到非常接近专业模型的性能**。这证明了通用模型在这个极其困难的问题上是多么有效。

# 那么解释性如何呢？

我听到关于通用模型的另一个担忧是其所谓的解释性不足。事实上，有些人声称，一个通用模型的透明度不如多个专业模型。

我不同意这一点。事实上，得益于 SHAP 值，即使模型是唯一的，你也可以分别解释每个组。我们可以称这种过程为“专业化解释性”。

![](../Images/129b809160a5fc68968df8d30136bbe3.png)

专业化解释性。[作者提供的图片]

让我用我们之前的实验做一个例子。

如果我们分别考虑每个组并计算原始特征值与相应 SHAP 值之间的相关系数，我们得到的结果是：

![](../Images/8b8a271f92d16c3d0a402b0bc687a970.png)

每个合并特征与相应 SHAP 值之间的相关性。[作者提供的图片]

如你所见，不同组之间的相关系数变化很大。例如，如果我们取“num_feature_01”，则“bank”组的相关性为正，而“employee”组的相关性为负。这非常有意义：

+   对于“银行”组，“num_feature_01”对应于“duration”特征，即账户持有者的持有时间。目标特征是客户是否订阅了定期存款。可以合理地预期该特征对预测有积极影响。

+   对于“员工”组，“num_feature_01”对应于“satisfaction_level”特征。由于目标特征是员工是否离职，因此负相关性可以很容易解释。

# 结论

在这篇文章中，我模拟了对一般模型而言最困难的情境：一个数据集组成的各组完全不同的案例。

为了模拟这种情况，**我合并了一些完全无关的数据集，包括特征和预测任务**！我使用了一个技巧，以确保不同数据集的列即使名称不同也能连接在一起。

然后，我在合并的数据集上训练了一个一般模型和许多专门模型：每个原始数据集一个。

这是一个压力测试，用来查看在对一般模型来说极端困难的情况下会发生什么。**然而，我发现性能差异最小**：使用一般模型代替专门模型的ROC评分平均损失为0.53%。

此外，我还利用这个实验展示了为什么**可解释性也不应成为问题**。实际上，使用一般模型后，人们仍然可以通过“专门的可解释性”分别解释单独的组。

你可以在[这个笔记本](https://github.com/smazzanti/general_vs_specialized_models/blob/main/unreasonable-effectiveness-of-general-models.ipynb)中找到本文使用的所有Python代码。

*感谢阅读！*

*如果你觉得我的工作有用，可以订阅* [***每次我发布新文章时获取邮件***](https://medium.com/@mazzanti.sam/subscribe) *(通常每月一次)。*

*如果你想支持我的工作，可以* [***请我喝杯咖啡***](https://ko-fi.com/samuelemazzanti)*。*

*如果你愿意，* [***在 LinkedIn 上添加我***](https://www.linkedin.com/in/samuelemazzanti/)*！*
