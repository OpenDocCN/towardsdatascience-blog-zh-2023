- en: A Visual Learner’s Guide to Explain, Implement and Interpret Principal Component
    Analysis (PCA)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-visual-learners-guide-to-explain-implement-and-interpret-principal-component-analysis-cc9b345b75be](https://towardsdatascience.com/a-visual-learners-guide-to-explain-implement-and-interpret-principal-component-analysis-cc9b345b75be)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Linear Algebra for Machine Learning — Covariance Matrix, Eigenvector and Principal
    Component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)[![Destin
    Gong](../Images/c93d4341a928c36bc47031f02e23ebbf.png)](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cc9b345b75be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cc9b345b75be--------------------------------)
    [Destin Gong](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cc9b345b75be--------------------------------)
    ·11 min read·Jan 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d801a017da88b5bf08832433651460a.png)'
  prefs: []
  type: TYPE_IMG
- en: Principal Component Analysis for ML (image from my [website](https://www.visual-design.net/))
  prefs: []
  type: TYPE_NORMAL
- en: In my previous article, we have talked about applying linear algebra for data
    representation in machine learning algorithms, but the application of linear algebra
    in ML is much broader than that.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268?source=post_page-----cc9b345b75be--------------------------------)
    [## How is Linear Algebra Applied for Machine Learning?'
  prefs: []
  type: TYPE_NORMAL
- en: Starting from using matrix and vector for data representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268?source=post_page-----cc9b345b75be--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: This article will introduce more linear algebra concepts with the main focus
    on how these concepts are applied for dimensionality reduction, specially **Principal
    Component Analysis (PCA)**. In the second half of this post, we will also implement
    and interpret PCA using a few lines of code with the help of Python scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: When to Use PCA?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: High-dimensional data is a common issue experienced in machine learning practices,
    as we typically feed a large amount of features for model training. This results
    in the caveat of models having less interpretability and higher complexity — also
    known as the curse of dimensionality. PCA can be beneficial when the dataset is
    high-dimensional (i.e. contains many features) and it is widely applied for dimensionality
    reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, PCA is also used for discovering the hidden relationships among
    features and reveal underlying patterns that can be very insightful. PCA attempts
    to find linear components that capture as much variance in the data as possible,
    and the first principal component (PC1) is typically composed of features that
    contributes the most to model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: How Does PCA Work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The objective of PCA is to find the principal components that represents the
    data variance in a lower dimension and we are going to unfold the process into
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: represent the data variance using **covariance matrix**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**eigenvector and eigenvalue** capture data variance in a lower dimensionality'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**principal components** are the eigenvectors of the covariance matrix'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To understand how PCA works, we need to answer the questions of what are covariance
    matrix and eigenvector/eigenvalue. It is also helpful to fundamentally shift our
    perspectives of viewing matrix multiplication as a math operation to a visual
    transformation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Matrix Transformation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have previously introduced how matrix dot product is computed from a math
    operation perspectives. We can also interpret the dot product as a visual transformation
    which assists in understanding more complex linear algebra concepts. As illustrated
    below, let us use a 2x2 matrix as an example. We split the matrix vertically into
    two vectors where the left one represents the basis vector of x-axis, and the
    right one represents the basis vector of the y-axis. Therefore, a matrix represents
    a 2D space constructed by the x-axis and y-axis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d020c29190f15717d042715dbb87dfc8.png)'
  prefs: []
  type: TYPE_IMG
- en: matrix transformation — identity matrix (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: It is not hard to understand that an identity matrix has `[1,0]` as the basis
    vector on the x-axis and `[0,1]` as the basis vector on the y-axis, so that the
    dot product between any vectors and the identity matrix will return the vector
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix transformation boils down to changing the scale and shifting the direction
    of the axis. For example, changing the basis vector of x-axis from `[1,0]` to
    `[2,0]` means that the mapping space has been scaled two times in the x coordinate
    direction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2855794ba4c819ecc4ddc95880359597.png)'
  prefs: []
  type: TYPE_IMG
- en: matrix transformation — x-axis scaled matrix (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We can additionally combine both the x-axis and y-axis for more complicated
    scaling, rotating or shearing transformation. A typically example is the mirror
    matrix where we swap the x and y axis. For a given vector `[1,2]`, we will get
    `[2,1]` after the mirror transformation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fd81afd5d25c72099fc673f2f97b105.png)'
  prefs: []
  type: TYPE_IMG
- en: matrix transformation — mirror matrix (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to practice these transformations in python and skip the manual
    calculations, we can use following code to perform these dot products and visualize
    the result of the transformation using `plt.quiver()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/30d0be7669a84ef125818aac15ecc8a2.png)'
  prefs: []
  type: TYPE_IMG
- en: matrix transformation result in python (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '**Covariance Matrix**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*In Short: covariance matrix represents the pairwise correlations among a group
    of variables in a matrix form.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Covariance matrix is another critical concept in PCA process that represents
    the data variance in the dataset. To understand the details of covariance matrix,
    we firstly need to know that **covariance measures the magnitude of how one random
    variable varies with another random variable**. For two random variable x and
    y, their covariance is formulated as below and higher covariance value indicates
    stronger correlation between two variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79ec551b71501c21bcad9b968aa20b6f.png)'
  prefs: []
  type: TYPE_IMG
- en: covariance formula (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: When given a set of variables (e.g. *x1, x2, … xn*) in a dataset, covariance
    matrix is used for representing the covariance value between each variable pairs
    in a matrix format.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c7044d8e0f568131c459d0713907736.png)'
  prefs: []
  type: TYPE_IMG
- en: covariance matrix (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying any vector with the covariance matrix will transform it towards
    the direction that captures the trend of variance in the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let us use a simple example to simulate the effect of this transformation. Firstly,
    we randomly generate the variable *x0, x1* and then compute the covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/16b1f8ec3f3ffbb27495035cebe7242a.png)'
  prefs: []
  type: TYPE_IMG
- en: We then transform some random vectors by taking the dot product between each
    of them and the covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/97dd095e4118abeee51c53aa0056539d.png)'
  prefs: []
  type: TYPE_IMG
- en: vectors transformed by covariance matrix (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Original vectors prior to the transformation are in black, and after transformation
    are in brown. As you can see, the original vectors that are pointing at different
    directions have become more conformed to the general trend displayed in the original
    dataset (i.e. the blue dots). Because of this property, covariance matrix is important
    to PCA in terms of describing the relationship between features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Eigenvalue and Eigenvector**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*In Short: Eigenvector (*v*) of a matrix (*A*) remains at the same direction
    after the matrix transformation, hence* Av = λv *where* v *represents the corresponding
    eigenvalue. Representing data using eigenvector and eigenvalue reduces the dimensionality
    while maintaining the data variance as much as possible.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To bring more intuitions to this concept, we can use a simple demonstration.
    For example, we have the matrix `[[0,1],[1,0]]`, and one of the eigenvector for
    matrix is `[1,1]` and the corresponding eigenvalue is 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a33765ba4fc1f3c4eae327d1c8719dd.png)'
  prefs: []
  type: TYPE_IMG
- en: eigenvector and eigenvalue (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: From matrix transformation, we know that matrix`[[0,1],[1,0]]` acts as a mirror
    matrix that swaps the x, y coordinate of the vector. Therefore, the direction
    of vector `[1,1]` will not change after the mirror transformation, thus it meets
    the criteria of being the eigenvector of the matrix. The eigenvalue 1 indicates
    that the vector remains at the same scale and direction as prior to the transformation.
    Consequently, we are able to represent the effect of a matrix transform *A* (i.e.
    2 dimensional) using a scalar *λ* (i.e. 1 dimension) and eigenvalue tells us how
    much variance are preserved by the eigenvector.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue with the example above and use this code snippet to overlay the
    eigenvector with the greatest eigenvalue (in red color). As you can see, it is
    aligned with the direction with the greatest data variance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4b4baf1aacfecffd7ff26add293341d7.png)'
  prefs: []
  type: TYPE_IMG
- en: visualize eigenvector (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal Components**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have discussed that covariance matrix can represent the data variance
    when multiple variables are present and eigenvector can capture the data variance
    in a lower dimensionality. By computing the eigenvector/eigenvalue of the covariance
    matrix, we get the principal components. There are more than one eigenvector for
    a matrix and they are typically arranged in a descending order of the their eigenvalue,
    denoted by *PC1, PC2* …*PCn.* The first principal component (*PC1*) is the eigenvector
    with the highest eigenvalue which is the red vector shown in the image, which
    explains the maximum variance in the data. Therefore, when using principal components
    to reduce data dimensionality, we select the ones with higher eigenvalues as it
    preserves more information in the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Thanks for reaching so far. If you would like to read more of my articles
    on Medium, I would really appreciate your support by signing up** [**Medium membership**](https://destingong.medium.com/membership)☕**.**'
  prefs: []
  type: TYPE_NORMAL
- en: PCA Implementation in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have walked through the theory behind PCA and now let’s step into the practical
    part. Luckily, scikit-learn has provided us an easy implementation of PCA. We
    will use the public dataset “college major” from [fivethirtyeight](https://github.com/fivethirtyeight/data)
    GitHub repository [1].
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Standardize data into the same scale**'
  prefs: []
  type: TYPE_NORMAL
- en: PCA is sensitive to data with different scales, as covariance matrix requires
    the data at the same scale to measure the correlation between features with a
    consistent standard. To achieve that, data standardization is applied before PCA,
    which means that each feature has a mean of zero and a standard deviation of one.
    We use the following code snippet to perform data standardization. If you wish
    to know more data transformation techniques such as normalization, min-max scaling,
    please visit my article on “[3 Common Techniques for Data Transformation](https://medium.com/towards-data-science/data-transformation-and-feature-engineering-e3c7dfbb4899)”.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-transformation-and-feature-engineering-e3c7dfbb4899?source=post_page-----cc9b345b75be--------------------------------)
    [## 3 Common Techniques for Data Transformation'
  prefs: []
  type: TYPE_NORMAL
- en: How to Choose the Appropriate One for Your Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-transformation-and-feature-engineering-e3c7dfbb4899?source=post_page-----cc9b345b75be--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**2\. Apply PCA on the scaled data**'
  prefs: []
  type: TYPE_NORMAL
- en: We then import PCA from `sklearn.decomposition` and specify the number of components
    to generate. The number of components is determined by how much data variance
    to explain by the principal components. Here we will generate 3 components to
    balance the trade off between the explained variance and dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**3\. Visualize explained variance using scree plot**'
  prefs: []
  type: TYPE_NORMAL
- en: Some information of the original dataset will be lost after shrinking it to
    a lower dimensionality, hence it is important to keep as much information as possible
    while limiting the number of principal components. To help us with the interpretation,
    we can visualize the explained variance using a scree plot. **Explained variance**
    of a principal component indicates the magnitude of data variance in the direction
    of the eigenvector and it correlates to the eigenvalue. Higher explained variance
    means that it preserves more information and the one with highest explained variance
    is the first principal component. We can use the `explained_variance_ratio_` attribute
    to get the explained variance. The code snippet below visualizes the explained
    variance and also the cumulative variance (i.e. sum of variance if we add previous
    principal components together).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9356087e69cec799f794d1629ac6219e.png)'
  prefs: []
  type: TYPE_IMG
- en: scree plot (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The scree plot tells us about the explained variances when three principal components
    were generated. The first principal component (PC1) explains 60% of the variance
    and 84% of the variance are explained with the first 3 components combined.
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Interpret the principal components composition**'
  prefs: []
  type: TYPE_NORMAL
- en: Principal components additionally provide us some evidence of the importance
    of original features. By evaluating the magnitude and direction of the coefficients
    for each original feature, we know whether the feature is strongly correlated
    with the component. As show below, we generate the coefficients of the features
    with respects to the components.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2f0d591963a2f18ee0e4a6feec98ce7b.png)'
  prefs: []
  type: TYPE_IMG
- en: component coefficients (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we can use `heatmap` from seaborn library to highlight the features
    with high absolute coefficient values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a0d79b9929f257186f719a2078230bee.png)'
  prefs: []
  type: TYPE_IMG
- en: component coefficients heatmap (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: If we interpret PC1 (i.e. row 0), we can see there are multiple features have
    relatively higher association with PC1, such as “Total” (number of enrolled students),
    “Employed”, “Full_time”, “Unemployed” etc, indicating that these features contribute
    more to the data variance. Additionally, you may notice that some features are
    directly correlated with each other, and PCA brings the extra benefit of removing
    multicollinearity among these features.
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. Use principal components in ML algorithm**'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have reduced the dimensionality to a handful of principal components
    which are ready to be utilized as the new features in machine learning algorithms.
    To do so, we are going to use the transformed dataset from the output of PCA process
    — `pca_df`. We can examine the shape of this dataset using `pca_df.shape` and
    we get 173 rows and 3 columns. We then add the label (e.g. “Rank”) back to this
    dataset with 3 principal components from the PCA process and this will become
    the new dataframe to build the ML model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3f78b1d29f0261e74b8591c68d0a191d.png)'
  prefs: []
  type: TYPE_IMG
- en: new_df (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The remaining process will follow the standard procedure of a machine learning
    lifecycle, that is — split the dataset into train-test, building model and then
    model evaluation. Here we won’t dive into the details of building ML models, but
    if you are interested, please have a look at my article on classification algorithms
    as the starting point.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/top-machine-learning-algorithms-for-classification-2197870ff501?source=post_page-----cc9b345b75be--------------------------------)
    [## Top 6 Machine Learning Algorithms for Classification'
  prefs: []
  type: TYPE_NORMAL
- en: How to Build a Machine Learning Model Pipeline in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/top-machine-learning-algorithms-for-classification-2197870ff501?source=post_page-----cc9b345b75be--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Take-Home Message
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous article, we have introduced using linear algebra for data representation
    in machine learning. Now we introduced another common use case of linear algebra
    in ML for dimensionality reduction — Principal Component Analysis (PCA). We firstly
    discussed the theory behind PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: represent the data variance using **covariance matrix**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: use **eigenvector and eigenvalue** to capture data variance in a lower dimensionality
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the **principal component** is the eigenvector and eigenvalue of the covariance
    matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Furthermore, we utilize scikit-learn to implement PCA through the following
    procedures:'
  prefs: []
  type: TYPE_NORMAL
- en: standardize data into the same scale
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: apply PCA on the scaled data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: visualize explained variance using scree plot
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: interpret the principal components composition
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: use principal components in ML algorithms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: More Articles Like This
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear Algebra for Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268?source=post_page-----cc9b345b75be--------------------------------)
    [## How is Linear Algebra Applied for Machine Learning?'
  prefs: []
  type: TYPE_NORMAL
- en: Starting from using matrix and vector for data representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268?source=post_page-----cc9b345b75be--------------------------------)
    ![Destin Gong](../Images/dcd4375055f8aa7602b1433a60ad5ca3.png)
  prefs: []
  type: TYPE_NORMAL
- en: '[Destin Gong](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Practical Guides to Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://destingong.medium.com/list/practical-guides-to-machine-learning-a877c2a39884?source=post_page-----cc9b345b75be--------------------------------)10
    stories![Principal Component Analysis for ML](../Images/1edea120a42bd7dc8ab4a4fcdd5b822d.png)![Time
    Series Analysis](../Images/fda8795039b423777fc8e9d8c0dc0d07.png)![deep learning
    cheatsheet for beginner](../Images/b2a4e3806c454a795ddfae0b02828b30.png)![Destin
    Gong](../Images/dcd4375055f8aa7602b1433a60ad5ca3.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Destin Gong](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: EDA and Feature Engineering Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://destingong.medium.com/list/eda-and-feature-engineering-techniques-e0696974ed54?source=post_page-----cc9b345b75be--------------------------------)9
    stories![](../Images/7fc2bdc73b7b052566cf26034941c232.png)![](../Images/a7c4110e9a854cf9e9eba83dfa46e7d3.png)![](../Images/3ac6d4f7832c8daa758f71b1e479406c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] College Major (FiveThirtyEight.) Retrieved from [https://github.com/fivethirtyeight/data/tree/master/college-majors](https://github.com/fivethirtyeight/data/tree/master/college-majors)
    [[CC-BY-4.0 license](https://github.com/fivethirtyeight/data/blob/master/LICENSE)]'
  prefs: []
  type: TYPE_NORMAL
