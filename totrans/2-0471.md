# 变换器能否学会制定策略？

> 原文：[https://towardsdatascience.com/can-transformers-learn-to-strategize-862770c996ea](https://towardsdatascience.com/can-transformers-learn-to-strategize-862770c996ea)

## TicTacGPT用于玩简单的棋盘游戏

[](https://charlieoneill.medium.com/?source=post_page-----862770c996ea--------------------------------)[![Charlie O'Neill](../Images/17aa117fc5787f93ff1f547b919786c8.png)](https://charlieoneill.medium.com/?source=post_page-----862770c996ea--------------------------------)[](https://towardsdatascience.com/?source=post_page-----862770c996ea--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----862770c996ea--------------------------------) [Charlie O'Neill](https://charlieoneill.medium.com/?source=post_page-----862770c996ea--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----862770c996ea--------------------------------) ·27分钟阅读·2023年9月8日

--

![](../Images/105981c00690836c0dd2d1ea74863f8b.png)

图片由 [Jon Tyson](https://unsplash.com/@jontyson?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

尽管大多数棋盘游戏倾向于使用卷积神经网络或其他几何灵感的架构，但我们实际能够将棋盘状态表示为字符串，这就引出了一个问题，即变换器是否可以自然地应用于游戏。在这里，我们将看看是否可以在简单的井字游戏的背景下回答这个问题。虽然这看起来可能不太实际（几乎每个人都知道这个游戏中存在一个简单的闭式纳什均衡策略），但它是我们问题的一个有用的测试平台。原因在于游戏足够简单，我们可以轻松训练一个变换器来玩它，但又足够复杂，不容易立刻看出最佳策略是什么。

# 实现游戏

我们将开始实现一个`TicTacToe`类。这相当简单。我们希望能够将棋盘表示为9个字符的字符串，每个字符代表一个方格。我们将使用`X`表示第一个玩家，`O`表示第二个玩家，`-`表示空方格。我们还会跟踪轮到谁进行下一步，游戏是否结束。如果有获胜者，我们也会记录下来。最后，我们将包含一个打印棋盘的方法，以便在调试时不必盯着字符串看。

[PRE0]

# 创建我们的训练数据

我们希望我们的变换器能够输入一个给定的棋盘状态，并输出一个走法，该走法是一个从 0 到 8 的整数，表示它希望将棋子放在的位置。为此，我们将创建一个棋盘状态和走法的数据集。我们将通过模拟我们玩家的所有可能获胜位置，然后遍历所有可能使我们达到该位置的游戏组合来做到这一点。这意味着变换器将学习在任何给定的棋盘状态下什么是一个好的走法。

为了实现这一点，`simulate_all_games` 函数生成了训练和验证数据。具体来说，该函数模拟了所有可能的井字棋游戏，探索了两个玩家（‘X’ 和 ‘O’）的每一种走法排列。这种详尽的模拟确保了模型在一个全面的数据集上进行训练，涵盖了所有可能的游戏情景。在每个模拟的游戏中，函数记录了不仅是获胜或平局的结果，还记录了棋盘状态的序列以及导致这些结果的走法。这些棋盘状态和走法随后被转换为数字表示，适用于训练我们的变换器。这确保了模型不仅学习如何获胜，还能够从任何给定的棋盘状态中输出一个合适的获胜走法。你可以将其视为类似于常规语言变换器在给定任何长度的上下文时输出一个合适的标记，从一个标记（即我们的起始棋盘状态）到 EOS 标记（即我们的获胜走法生成最终棋盘状态）。

[PRE1]

这给我们大约 650,000 个张量用于训练。这些张量看起来大致正确，但没有可视化的棋盘很难判断。让我们重用我们的 `print_board` 函数来查看一些随机的棋盘状态，以及给定走法后的下一个棋盘状态是什么样的：

[PRE2]

这似乎是合理的，但我注意到有些游戏有一个可用的获胜走法，但模拟却做出了不同的走法（仍然以胜利告终）。在上面的示例中出现了这种情况。让我们将 `simulate_all_games` 函数更改为在找到至少一个潜在获胜走法时停止搜索。

[PRE3]

最后，让我们看看我们需要训练多少步：

[PRE4]

大约 150,000 个示例。这看起来是一个合理的开始。

# 多头注意力的变换器架构

注意力是一种机制，使模型能够在进行预测时专注于输入序列的某些部分。变压器架构使用多头自注意力，这意味着模型学习以不同的方式关注输入序列的不同部分。这很有用，因为它允许模型学习输入序列和输出序列之间的不同关系。例如，当预测输出序列中的第一个词时，它可能会学习关注输入序列中的第一个标记，但在预测输出序列中的第二个词时，则关注输入序列中的最后一个标记。这是一种强大的机制，可以使模型学习输入序列和输出序列之间的复杂关系。

但这到底是怎么工作的呢？从原始的 *Attention is all you need* 论文中，定义在查询矩阵 Q、键矩阵 K 和值矩阵 V 上的注意力定义为：

![](../Images/f580f3530c711f7a05af38349a4eee13.png)

其中我们除以 `sqrt{d_k}` 以确保 softmax 的方差适当。让我们分解一下实际发生了什么。假设我们有一个维度为 (B,T,C) 的输入，其中 B 是批次大小，T 是序列长度，C 是通道数。我们可以把它看作是一个包含 B 个长度为 T 的序列的批次，每个序列有 C 个通道：

[PRE5]

然后，为了实现一个自注意力头，我们需要创建查询、键和值。实际上，这些是具有一定 `head_size` 的线性层，`head_size` 就是我们希望线性层的宽度。我们不包括偏置项，因为我们不想为注意力学习一个偏置项。

[PRE6]

但是，如果你拆解我们实际在做的事情，我们是在预测一个单词序列中的下一个词。由于我们不想作弊并使用我们尚未看到的序列部分（因为在生成过程中我们不能这样做），我们需要屏蔽掉尚未看到的序列部分。我们通过创建一个形状为 (T,T) 的掩码来实现，其中 T 是序列长度，然后将上三角的所有值设置为负无穷。这确保了 softmax 对所有掩码值为 0，因此模型不会关注这些值。

最后，我们将注意力权重与值矩阵相乘，以获得注意力层的输出。这是自注意力的单个头的输出。然后，我们可以根据需要重复此过程多次，然后将每个头的输出拼接在一起，以获得多头自注意力层的最终输出。

[PRE7]

这只是自注意力的一个头。为了创建多个头，我们只需多次重复这个过程，然后将每个头的输出连接起来以获得多头自注意力层的最终输出。我们还添加了残差连接，以提高我们优化这个相对深层模型的能力。对于类似代码的完整演示以及解码器仅变换器背后的机制，我强烈推荐 Andrej Karpathy 的[nanoGPT 讲座](https://www.youtube.com/watch?v=kCc8FmEb1nY)。

[PRE8]

为了确保我们的架构按预期工作，让我们尝试传递一个单一的批次。

[PRE9]

在进行这个初始前向传播时，一个好的步骤是测试损失是否大致等于我们对随机输入的期望。由于我们有 9 维 logits，并且我们使用的交叉熵损失等于正确类别的负对数似然，我们期望损失大致为：

![](../Images/efc854c3d72c8944f3e5dd24a38a3842.png)

# 训练模型

使用相当小的变换器（约 25,000 个参数），我们实现了以下损失（请注意，我使用了少量的权重衰减和丢弃）：

[PRE10]

这似乎不太好。让我们绘制图表看看发生了什么。

[PRE11]

![](../Images/80b03da5de9c17e9e1ce1206541f3705.png)

我们的变换器在井字游戏数据上训练的初始损失（图片由作者提供）。

我们可以使用这个`code`来测试变换器的效果：

[PRE12]

好吧，我轻松打败了模型。某些事情出错了。

# 改进变换器

所以目前，变换器甚至无法从任何给定位置可靠地学习简单的胜利动作。我能想到几个原因：

+   变换器仅在胜利的动作上进行训练，因此当我使用一个良好的策略（即没有胜利的动作可用）时，可能无法学习如何游戏。理论上，为了应对这一点，我们应该允许它在游戏注定为平局时进行训练。

+   变换器的参数过多。试图让几十万个神经元协调一个简单的策略可能需要很长时间来训练，并依赖于**grokking**和其他现象才能进入优化景观的可泛化部分。

+   变换器的参数过少。也许它需要更多的神经元来学习一个好的策略。这样说来，我非常怀疑如果几百万个神经元都无济于事，那几十万个神经元能否解决问题。

## 状态空间分析

在继续之前，我想从理论上分析编码井字游戏中完整胜利策略所需的神经元数量，我们需要考虑游戏的状态空间和决策过程的复杂性。

在井字棋中，游戏棋盘是一个3 x 3的网格，每个单元可以处于三种状态之一：‘X’，‘O’，或空（‘-’）。因此，总的可能棋盘状态数量可以计算为3⁹ = 19683。然而，并非所有这些状态在实际游戏中都是有效的；其中一些是不可达的或非法的（例如，所有单元都是‘X’的棋盘）。合法状态的数量实际上大约为5478，但为了分析的目的，我们将考虑上限，即3⁹。

每个状态都需要一个决策：在哪里放置下一个‘X’（因为我们考虑的是‘X’的获胜策略）。有9个可能的位置，但合法的移动数量通常少于9，这取决于已经被占据的单元格数量。一个神经网络需要将每个可能的棋盘状态映射到一个正确的移动。实现这种映射的一种方法是通过一个完全连接的层，该层将棋盘状态作为输入，并输出9个可能移动的概率分布。输入层将有3x3=9个神经元（每个单元一个），输出层将有9个神经元（每个可能移动一个）。中间的隐藏层将执行学习获胜策略的复杂任务。

考虑到输入层有9个神经元和输出层有9个神经元，我们关注的是隐藏层中的神经元数量。从理论上讲，我们可以使用一个具有3⁹个神经元的隐藏层将每个可能的状态映射到一个获胜的移动。这将是一个上限，可能远远超过实际需要的数量，因为存在不可达/非法状态以及井字棋的固有对称性会减少实际的唯一状态数量。因此，在这个上限情况下，总的神经元数量将是：9 + 19683 + 9 = 19701。这是一个理论上的上限，实际数量可能由于前述因素而低得多。让我们尝试实现一个简单的前馈普通神经网络，看看它在我们的任务中的表现。我们将使用三个隐藏层，而不是一个具有数千个神经元的层。

[PRE13]

让我们看看损失情况如何：

![](../Images/bd544cb64db73e78b64a4b96bef6e7a5.png)

对于普通前馈神经网络的训练损失（作者提供的图片）。

显然，我们正在饱和性能。任务和我们设置的方法有些问题，阻止了模型学习适当的策略。为了改变一下，我打算尝试给模型提供只包含最优策略的训练数据。

# 最优策略训练数据

[纽厄尔和西蒙1972年的井字棋程序](https://en.wikipedia.org/wiki/Tic-tac-toe)概述了完美策略（以赢得比赛或至少平局），如果我们从以下移动偏好中选择第一个可用的移动：

1.  *获胜*：如果你有两个连成一行，玩第三个以完成三连。

1.  *阻挡*：如果对手有两个连成一行，玩第三个以阻挡他们。

1.  *分叉*：创造一个你可以通过两种方式获胜的机会。

1.  *阻止对手的叉子*：我们可以创建两个连续的棋子以迫使对手防守（如果这样做不会给他们造成叉子），或者阻止他们的潜在叉子。

1.  *中心*：占据中心位置。

1.  *对角角落*：如果对手在一个角落里，选择对角的角落。

1.  *空角落*：选择一个空的角落。

1.  *空侧*：选择一个空的边侧。

让我们重写数据生成器，以根据这一策略获得所有可能的走法。我们还将模拟两种可能的先手玩家的所有游戏。

[PRE14]

让我们在新的训练数据上重新训练我们的模型。

[PRE15]

![](../Images/4549f446f53811e26528645b2d42a5ce.png)

使用最优训练数据的新损失（作者提供的图像）。

太棒了！我们不仅学会了策略，而且它在验证数据集上也能泛化（我们在训练和验证集上接近0损失）。我猜这是由于棋盘状态的固有对称性，变换器已经学会了一种对棋盘状态不变的模块算术形式。

让我们尝试新的变换器：

[PRE16]

它打败了我！使用了一个绝妙的叉子。看起来我们的变换器已经学会了最优策略。

# 结论

我认为这里的主要收获是变换器完全能够学习游戏的最优策略。虽然一个普通的神经网络可能也能学到相同的最优策略，但注意力机制的动态特性意味着它可能能够处理表示游戏随时间演变的更长序列。这些想法自然地促使我们在强化学习设置中应用变换器。例如，[Janner等（2021）](https://arxiv.org/pdf/2106.02039.pdf) 使用变换器来建模轨迹分布，并使用束搜索作为规划算法。

从这个项目中我学到的另一件事是，人工手动引导变换器通过最优策略的过程显然无法扩展，尤其是当游戏变得更加复杂时。例如，围棋并不是一个“已解决”的游戏，因此我们不能像上面那样提供最优策略进行训练。相反，我们必须使用类似自我对弈的方法来选择好的棋局序列，然后再用这些序列训练变换器。我希望未来能尝试这些想法。

最后，通过查看变换器中的预测和规划层级，仍有一个待开发的领域。正如[Ba 等人 (2016)](https://arxiv.org/pdf/1610.06258.pdf) 所指出的，深度学习通常关注于在激活动态中保持临时状态的方法，而我们的大脑似乎是通过中期突触可塑性来调节临时状态信息。换句话说，应该有某种形式的工作记忆/预测，在下一个标记级别和例如 LSTM 中的长期状态之间操作。作为 GPT-3 支撑骨架的自回归解码器仅变换器架构是一个强大的模型，可以通过预测一个标记的未来生成逼真的文本。然而，如果我们将智能拟人化，我们知道快速的直观预测（仅预测一个标记）并不能使人变成天才。因此，在我看来，尝试为模型提供多个预测层级，让模型学会预测多个未来标记将会很有趣。这将类似于人类的提前规划能力，并且可能是变换器学习的一个有用技能。

预测未来意味着什么？这里有几个不同的探索方向：

+   **时间上的预测**：变换器能否学会预测下一个序列中的标记，而不是预测两个、三个或更多步之后的标记？预测两个标记一次是否等同于先预测一个标记，再预测下一个标记，还是说一次预测两个标记有某种战略上的好处？这是否迫使变换器*思考*更长时间？

+   **空间上的预测**：有证据表明，人类会产生粗略的任务语义表示，然后使用层级模块来“填补”这些粗略表示中的空白。你可以将其想象为写一篇文章：首先你创建一个要点的骨架，然后为每一段填入论点句子，最后完善细节。变换器是否可能学会做同样的事情？

无论这些问题是否有用，我希望这篇文章能为我们如何将问题重塑为适合变换器的问题提供一些清晰的见解。祝调优愉快！

## 参考文献

1.  Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., & Polosukhin, I. (2017). 注意力机制是你所需的一切。*神经信息处理系统进展*, *30*。

1.  Janner, M., Li, Q., & Levine, S. (2021). 离线强化学习作为一个大的序列建模问题。*神经信息处理系统进展*, *34*, 1273–1286。

1.  Ba, J., Hinton, G. E., Mnih, V., Leibo, J. Z., & Ionescu, C. (2016). 使用快速权重关注最近的过去。*神经信息处理系统进展*, *29*。

1.  Andrej Karpathy. *让我们从头开始构建 GPT：从代码到拼写的全程讲解。* [https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5076s](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5076s)
