- en: 'Guided Transfer Learning: How to use ‚Äòthe power of scouts‚Äô to boost machine
    learning performance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/guided-transfer-learning-how-to-use-the-power-of-scouts-to-boost-machine-learning-performance-7e478d1ec5e4](https://towardsdatascience.com/guided-transfer-learning-how-to-use-the-power-of-scouts-to-boost-machine-learning-performance-7e478d1ec5e4)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An exclusive sneak-peek at a revolutionary new method for training neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)[![Katherine
    Munro](../Images/8013140495c7b9bd25ef08d712f097bf.png)](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)
    [Katherine Munro](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)
    ¬∑10 min read¬∑Mar 27, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/482d6e71232893524e6b0b5e6d2cf3bd.png)'
  prefs: []
  type: TYPE_IMG
- en: In this newly proposed technique, small ‚Äòscout models‚Äô are sent to navigate
    the problem landscape and ‚Äòreport back‚Äô to the main model. Photo by [@ansgarscheffold](http://twitter.com/ansgarscheffold)
    on Unsplash.
  prefs: []
  type: TYPE_NORMAL
- en: 'My good friend and humble genius Dr Danko Nikoliƒá recently shared an unpublished
    paper with me, thinking I might be interested. Was I ever. Reading it made me
    feel like I was witnessing a historic moment before anyone else did, and I was
    immediately bursting to share. Thankfully, Danko agreed. So here‚Äôs my translation
    into day-to-day language of a method I think could revolutionise the training
    of deep neural networks. It‚Äôs not even out on arXiv yet (update: [now it is!](https://arxiv.org/abs/2303.16154)),
    but [NASA are already using it](https://www.linkedin.com/posts/robots-go-mental_nasa-genelab-open-science-for-life-in-space-activity-7041340569012301824-xTk3?utm_source=share&utm_medium=member_desktop).
    So once it does blow up, remember: you heard it here first. üòâ'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs Start With The Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I‚Äôm sure you know it: machine learning, especially with deep neural networks,
    requires a frankly ludicrous amount of data, compute power, and model parameters.
    This makes them inaccessible to all but the wealthiest of companies and research
    institutions, and thus concentrates ‚Äî into the hands of a small few ‚Äî the power
    to develop AI technologies that will shape our technological future. Not cool.'
  prefs: []
  type: TYPE_NORMAL
- en: Why The Problem Exists
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we train a neural network for a task, we‚Äôre basically teaching it, via
    thousands of examples, how to adjust its own weights and biases so that information
    flowing into the network produces some other, desired kind of information to flow
    out. These weights and biases are collectively known as ‚Äòparameters‚Äô, and a network
    can have millions or even trillions of them. With such a large ‚Äòparameter space‚Äô,
    it‚Äôs really hard to learn the right ones; there are just too many.
  prefs: []
  type: TYPE_NORMAL
- en: Since there‚Äôs no way to try out every possible combination of parameter values,
    we attempt to make good guesses. A typical machine learning algorithm looks for
    ‚Äòhints‚Äô about how each parameter should change, changes them, then looks for new
    hints based on how successful that was. The most famous of these algorithms is
    called ‚Äògradient descent‚Äô.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you‚Äôre already familiar with gradient descent, skip this paragraph. Otherwise,
    you can imagine it working like this: the problem space is a landscape of hills
    and valleys, and we are a ball rolling around that landscape, trying to get to
    the lowest point of the lowest valley. We call this the ‚Äòglobal minimum‚Äô. So our
    algorithm will take cues from how steep the ground around us is, which direction
    it‚Äôs sloping, and so on, and then decide which way to roll. Hopefully, it‚Äôs toward
    that lowest of low points. But there‚Äôs a danger: we might roll into a ditch which
    *isn‚Äôt quite as low*, and get stuck. We‚Äôd call this a ‚Äòlocal minimum‚Äô, and it‚Äôs
    a bad place to be.'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks use gradient descent to learn to solve a huge variety of problems.
    Unfortunately, some studies have shown that these networks can only improve their
    performance so much before they need to increase their size and data again, and
    this tendency seems to follow a ‚Äòpower law‚Äô. Put simply, it means that a small
    increase in ‚Äòintelligence‚Äô requires a massive increase in resources. Put inversely,
    we‚Äôre looking at diminishing returns for the resources we feed into these models.
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical Hope‚Ä¶ and More Pain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Theoretically, much smaller networks can exist and get the job done, but they
    have to be hand-constructed or learned by algorithms designed for very specific
    tasks. Gradient descent, on the other hand, can be used in huge models and learn
    to solve a much broader range of problems, but will struggle to find the right
    parameters for a small model.
  prefs: []
  type: TYPE_NORMAL
- en: The reason it struggles is that it can‚Äôt see how changes to parameters it makes
    in any given training epoch will affect the performance later. We, the ball rolling
    around the problem space, might be tempted to start rolling to the right, as that‚Äôs
    where the ground slopes down the steepest. But that might lead us straight to
    a dreaded local minimum! Maybe it would have been better to keep on rolling straight
    ‚Äî even if it didn‚Äôt look that steep from where we were standing ‚Äî as it would
    eventually have taken us to the deepest of all the valleys.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent tries to solve this by increasing the number of model parameters,
    since more dimensions simply reduces our statistical likelihood of getting stuck.
    Yet more parameters can make the model ‚Äòoverfit‚Äô (which means to memorise the
    answer to the current problem but thereafter be useless at generalising to other
    problems). Hence, we end up having to add a corresponding amount of training data,
    to help the trained model be more generalisable. But this constant need to increase
    resources is unsustainable. There must be a better way!
  prefs: []
  type: TYPE_NORMAL
- en: Guided Transfer Learning‚Ä¶ to the Rescue!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dr Nikoliƒá and his colleagues Davor Andriƒá and Vjekoslav Nikoliƒá (yes, they‚Äôre
    brothers!) propose a solution which I think is rather sweet. They liken it to
    sending scouts into the problem landscape, only here, ‚Äúthe scouts are models that
    solve smaller, but related problems in this space. They go as deeply as needed
    and come back with information about the direction in which they successfully
    traveled.‚Äù The scouts are less likely to get stuck in local minima, because their
    problems are easier to solve. And because they‚Äôre solving smaller problems, we
    can give them more data to work with, which makes them less likely to overfit.
    In the end, they transfer their knowledge back to the main model. Cute, right?
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper describes it best, so I‚Äôll paraphrase: Local minima are like mountain
    lakes; rainwater descends down a mountain by only looking locally, which can cause
    it to get stuck. Guided Transfer Learning (GTL) is like walking along the side
    of a mountain slope, instead of straight down the nearest valley. Sometimes the
    valley is a trap, and the scouts know that.'
  prefs: []
  type: TYPE_NORMAL
- en: So How Does it Work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scouts are ‚Äòsent off‚Äô to solve less difficult problems than the main model.
    For example, if the overall task is to classify inputs into one of ten different
    classes, then each scout model might be assigned a subset of the data which features
    only three of those classes, for them to classify. Simpler problems reduce the
    chance that the scout models will overfit; thus, the ‚Äòknowledge‚Äô they gain on
    behalf of the main model is more reliable.
  prefs: []
  type: TYPE_NORMAL
- en: An additional tactic to reduce the risk of overfitting is to give the scouts
    more data. This also has knowledge transfer benefits similar to pre-training,
    which I‚Äôll cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The result of this scouting expedition is a so-called ‚Äòguidance matrix‚Äô, which
    tells the main model, for each parameter, how important that parameter is. A low
    value means the scouts didn‚Äôt find many changes along that dimension (parameter),
    so changing it probably won‚Äôt contribute much to the overall solution. For example,
    the scouts walked that way but the slope stayed pretty gentle, so they gave up
    on it. A big value means a more dramatic and potentially useful change. For example,
    one scout took a path that seemed pretty flat, and then nearly stumbled over a
    cliff edge! Probably a good idea to move the affected parameter in that direction,
    right?
  prefs: []
  type: TYPE_NORMAL
- en: 'The nice thing about this approach is that it‚Äôs really simple, both in terms
    of the maths and the code it takes to implement. Basically, gradient descent is
    busy as always, calculating the value of changing each parameter, using the information
    it has about the sloping landscape all around it. Say the parameter is a network
    weight, w. We call the value of changing that Œî_w (where _ indicates that the
    following letter is a subscript), or ‚Äòdelta w‚Äô. The guidance matrix has a corresponding
    guiding value, called g_w. So the *guided* change, Œîw_g, is simply:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0b887d357980c29ca1a00280040402f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'That‚Äôs it, that‚Äôs all the maths I‚Äôm going to cover (there‚Äôs a tiny bit more
    to this on page 5 of the paper, and on page 7 the authors discuss, again in a
    very simple way, the different possible ways to compute the guidance matrix itself.
    But what I‚Äôve given you is all the maths you need to understand the idea). The
    code is equally simple. If you‚Äôre not a coder, just trust me that the following
    is a no-brainer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: But What About Traditional Pre-Training?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ‚ÄòPT‚Äô in GPT-4 stands for ‚Äòpre-training‚Äô, so in case you didn‚Äôt already know
    it, it‚Äôs a big deal. The idea is that you first train a neural network on some
    huge but generic dataset and task, before fine-tuning it on a more specific dataset
    and task. For example, GPT and other Large Language Models are typically pre-trained
    to predict missing words (that is, words that have been randomly removed by their
    human overlords) from sentences, using text crawled from pretty much the whole
    internet. This gives the model a fairly good idea (statistically speaking, not
    cognitively, of course!) about which words occur most often in different contexts.
    After this, the model is fine-tuned on the actual task at hand, such as question
    answering, using a smaller and more specific dataset. The general ‚Äòknowledge‚Äô
    acquired from the original task tends to improve downstream task performance;
    we call this ‚Äòtransfer learning‚Äô.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning from pre-training helps deal with many, but not necessarily
    all, of the local minima that may exist in the problem landscape. Thus, Guided
    Transfer Learning complements pre-training, by helping scout ahead for new problem
    areas to avoid. The two methods don‚Äôt have to be combined, but when they are,
    the gains can be dramatic.
  prefs: []
  type: TYPE_NORMAL
- en: Sounds Great, But is it Effective?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The paper first demonstrates the benefits of Guided Transfer Learning for ‚Äò**one-shot
    learning**‚Äô. This is the task of training a model to do one thing, such as classify
    images into different categories, then showing it a single example of a new class
    and expecting it to be able to correctly classify more examples of that class
    thereafter. The authors found that using GTL in addition to pre-training increased
    performance consistently, but that the guidance matrices quickly reached a maximum
    degree of ‚Äòhelpfulness‚Äô, after which adding more scouts or giving them more data
    didn‚Äôt help. On the plus side, this also meant that reducing the number of scouts
    or the size of their training data didn‚Äôt hurt performance much.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implication is that GTL is a low cost, somewhat helpful technique, particularly
    in cases of limited data (where one often turns to one- and few-shot learning).
    And it‚Äôs early days yet: the authors only tried using single guidance matrices
    so far but suggest that other implementations might be useful, such as creating
    different matrices depending on how far the scout has moved from its starting
    point. This makes intuitive sense to me: the main model already has information
    about its immediate surroundings and we want the scouts to help see what‚Äôs further
    up ahead, so if we made multiple matrices and placed greater importance on those
    which are further from the starting point, this might help the algorithm ‚Äòchoose‚Äô
    the best next step to make.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97a68cd149c74fb88bd5dc2b43951da3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Reproduced with the authors‚Äô permission: A) Example one-short learning task:
    One example is given for training, then other examples of the same characters
    must be found. B) An example of classification performance using pre-training
    and adding GTL.'
  prefs: []
  type: TYPE_NORMAL
- en: The second experiment dealt with the ‚ÄòXOR problem,‚Äô AKA ‚Äòexclusive OR.‚Äô The
    goal is to learn a function that maps two inputs (x1 and x2) to an output (y),
    such that y = 1 (or ‚ÄòTrue‚Äô) if x1 and x2 are different, and y = 0 (or ‚ÄòFalse‚Äô)
    otherwise. Large models using gradient descent often stagnate on this challenge,
    so Nikoliƒá, Andriƒá and Nikoliƒá applied GTL *without pre-training* to help **avoid**
    these **local minima**. This proved so effective that no stagnation could be observed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final experiment addressed the issue of ‚Äòcatastrophic forgetting‚Äô: where
    neural networks trained on new tasks are so influenced by the new data that all
    their learned weights get massively updated, causing them to ‚Äòforget‚Äô what they
    learned before. Nikoliƒá et al. performed a series of model re-trainings: each
    additional training step lasted a fixed number of epochs and was based on a single
    additional data point; each data point was a new example of the categories already
    learned. That is, the data points were learned sequentially, not in a batch as
    is typically done. In a private conversation, Danko likened this to real life:
    there exists a category of object called ‚Äòcar‚Äô, and today you are driving in a
    particular kind of one, like an SUV. Just because you‚Äôre now being exposed to
    SUV specific features, it doesn‚Äôt mean you‚Äôll forget all the other car types that
    you were exposed to in the past. Your new exposure should increase your understanding
    of what a car is like, not reduce it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While classical transfer learning from pre-training alone would not be able
    to benefit from these sequential additions of data points, the addition of GTL
    caused a stepwise performance boost, indicating that the **knowledge added up**.
    In other words, the models were more robust to forgetting the previous examples,
    although it did occur eventually. The authors explained this in common sense terms:
    in a given problem space, there will be some solutions that are appropriate both
    for examples learned now, *and* examples learned before; these solutions probably
    lie close to the model‚Äôs starting point as it begins its training on the new example;
    without GTL, the model can ‚Äòwander away‚Äô; *with* GTL, the guidance matrices encourage
    the model to remain in that useful vicinity. It‚Äôs like a scout reminding you not
    to stray too far from the camp.'
  prefs: []
  type: TYPE_NORMAL
- en: So What Does This Mean in Practice?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Guided Transfer Learning is a method of ‚Äòlearning to learn‚Äô, and it‚Äôs not the
    only one out there. Other approaches exist and maybe even produce better results,
    but they are often unscalable, which can defeat their purpose. GTL is computationally
    cheap, and flexible, which may make it particularly appropriate for those models
    that have already squeezed everything they can get out of their available resources.
    Large Language Models, and computer vision models which often exist under tough
    hardware constraints, like being embedded in autonomous vehicles, are a couple
    of good examples.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if data and compute power are available in abundance, then
    the authors admit that GTL might not be needed. It‚Äôs not a panacea, as they say,
    but then no single machine learning solution is. And as the experiments showed,
    it still has promise for those hard problems which even well-resourced models
    struggle with.
  prefs: []
  type: TYPE_NORMAL
- en: With all the possible guidance matrix paradigms the authors plan to experiment
    with, I‚Äôm optimistic. So, watch this space.
  prefs: []
  type: TYPE_NORMAL
