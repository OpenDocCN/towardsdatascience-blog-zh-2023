- en: 'Guided Transfer Learning: How to use ‘the power of scouts’ to boost machine
    learning performance'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有指导的迁移学习：如何利用“侦察的力量”提升机器学习表现
- en: 原文：[https://towardsdatascience.com/guided-transfer-learning-how-to-use-the-power-of-scouts-to-boost-machine-learning-performance-7e478d1ec5e4](https://towardsdatascience.com/guided-transfer-learning-how-to-use-the-power-of-scouts-to-boost-machine-learning-performance-7e478d1ec5e4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/guided-transfer-learning-how-to-use-the-power-of-scouts-to-boost-machine-learning-performance-7e478d1ec5e4](https://towardsdatascience.com/guided-transfer-learning-how-to-use-the-power-of-scouts-to-boost-machine-learning-performance-7e478d1ec5e4)
- en: An exclusive sneak-peek at a revolutionary new method for training neural networks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对训练神经网络的革命性新方法的独家预览
- en: '[](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)[![Katherine
    Munro](../Images/8013140495c7b9bd25ef08d712f097bf.png)](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)
    [Katherine Munro](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)[![Katherine
    Munro](../Images/8013140495c7b9bd25ef08d712f097bf.png)](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)
    [Katherine Munro](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)
    ·10 min read·Mar 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)
    ·阅读时间 10 分钟·2023年3月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/482d6e71232893524e6b0b5e6d2cf3bd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/482d6e71232893524e6b0b5e6d2cf3bd.png)'
- en: In this newly proposed technique, small ‘scout models’ are sent to navigate
    the problem landscape and ‘report back’ to the main model. Photo by [@ansgarscheffold](http://twitter.com/ansgarscheffold)
    on Unsplash.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一新提议的技术中，小型“侦察模型”被派遣去探索问题领域，并“反馈”给主要模型。图片由 [@ansgarscheffold](http://twitter.com/ansgarscheffold)
    在 Unsplash 上提供。
- en: 'My good friend and humble genius Dr Danko Nikolić recently shared an unpublished
    paper with me, thinking I might be interested. Was I ever. Reading it made me
    feel like I was witnessing a historic moment before anyone else did, and I was
    immediately bursting to share. Thankfully, Danko agreed. So here’s my translation
    into day-to-day language of a method I think could revolutionise the training
    of deep neural networks. It’s not even out on arXiv yet (update: [now it is!](https://arxiv.org/abs/2303.16154)),
    but [NASA are already using it](https://www.linkedin.com/posts/robots-go-mental_nasa-genelab-open-science-for-life-in-space-activity-7041340569012301824-xTk3?utm_source=share&utm_medium=member_desktop).
    So once it does blow up, remember: you heard it here first. 😉'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我的好朋友和谦逊的天才 Dr Danko Nikolić 最近与我分享了一篇未发表的论文，认为我可能会感兴趣。我确实感兴趣了。阅读它让我感觉像是在目睹一个历史时刻，并且我迫不及待地想要分享。幸运的是，Danko
    同意了。所以这是我将一种可能革新深度神经网络训练的方法翻译成日常语言的版本。它甚至还没有发布到 arXiv 上（更新：[现在已经发布了！](https://arxiv.org/abs/2303.16154)），但
    [NASA 已经在使用它](https://www.linkedin.com/posts/robots-go-mental_nasa-genelab-open-science-for-life-in-space-activity-7041340569012301824-xTk3?utm_source=share&utm_medium=member_desktop)。所以一旦它爆火，记住：你首先在这里听到的。😉
- en: Let’s Start With The Problem
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从问题开始
- en: 'I’m sure you know it: machine learning, especially with deep neural networks,
    requires a frankly ludicrous amount of data, compute power, and model parameters.
    This makes them inaccessible to all but the wealthiest of companies and research
    institutions, and thus concentrates — into the hands of a small few — the power
    to develop AI technologies that will shape our technological future. Not cool.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信你知道：机器学习，尤其是深度神经网络，需要大量的数据、计算能力和模型参数。这使得这些技术只有最富有的公司和研究机构才能使用，因此将开发塑造我们技术未来的AI技术的权力集中在少数人手中。并不酷。
- en: Why The Problem Exists
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么会有这个问题
- en: When we train a neural network for a task, we’re basically teaching it, via
    thousands of examples, how to adjust its own weights and biases so that information
    flowing into the network produces some other, desired kind of information to flow
    out. These weights and biases are collectively known as ‘parameters’, and a network
    can have millions or even trillions of them. With such a large ‘parameter space’,
    it’s really hard to learn the right ones; there are just too many.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为某个任务训练神经网络时，实际上是在通过成千上万个例子教它如何调整自身的权重和偏差，以便输入网络的信息产生另一种所需的信息输出。这些权重和偏差统称为“参数”，一个网络可以拥有数百万甚至数万亿个这样的参数。由于“参数空间”如此庞大，学习到正确的参数非常困难，因为数量实在太多。
- en: Since there’s no way to try out every possible combination of parameter values,
    we attempt to make good guesses. A typical machine learning algorithm looks for
    ‘hints’ about how each parameter should change, changes them, then looks for new
    hints based on how successful that was. The most famous of these algorithms is
    called ‘gradient descent’.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于无法尝试每种可能的参数值组合，我们尝试做出好的猜测。一个典型的机器学习算法会寻找有关每个参数如何变化的“提示”，然后根据这些提示进行调整，再根据调整的成功程度寻找新的提示。这些算法中最著名的是“梯度下降”。
- en: 'If you’re already familiar with gradient descent, skip this paragraph. Otherwise,
    you can imagine it working like this: the problem space is a landscape of hills
    and valleys, and we are a ball rolling around that landscape, trying to get to
    the lowest point of the lowest valley. We call this the ‘global minimum’. So our
    algorithm will take cues from how steep the ground around us is, which direction
    it’s sloping, and so on, and then decide which way to roll. Hopefully, it’s toward
    that lowest of low points. But there’s a danger: we might roll into a ditch which
    *isn’t quite as low*, and get stuck. We’d call this a ‘local minimum’, and it’s
    a bad place to be.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经对梯度下降有所了解，可以跳过这一段。否则，你可以将其想象为这样工作：问题空间是一个有丘陵和山谷的地形，而我们是一个在这个地形上滚动的球，试图到达最低的山谷底部。我们称之为“全局最小值”。所以我们的算法会根据周围地面的陡峭程度、倾斜方向等信息来决定滚动的方向。希望它朝着最低的点前进。但也有危险：我们可能会滚入一个“并不是特别低”的沟壑中，并陷入困境。我们称之为“局部最小值”，这是一个不好的位置。
- en: Neural networks use gradient descent to learn to solve a huge variety of problems.
    Unfortunately, some studies have shown that these networks can only improve their
    performance so much before they need to increase their size and data again, and
    this tendency seems to follow a ‘power law’. Put simply, it means that a small
    increase in ‘intelligence’ requires a massive increase in resources. Put inversely,
    we’re looking at diminishing returns for the resources we feed into these models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络使用梯度下降来学习解决各种问题。不幸的是，一些研究表明，这些网络的性能提升有限，之后需要再次增加规模和数据，而这种趋势似乎遵循一种“幂律”。简单来说，这意味着“小幅度的‘智能’提升需要大量的资源增加”。反过来说，我们投入这些模型的资源回报正在递减。
- en: Theoretical Hope… and More Pain
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理论上的希望……和更多的痛苦
- en: Theoretically, much smaller networks can exist and get the job done, but they
    have to be hand-constructed or learned by algorithms designed for very specific
    tasks. Gradient descent, on the other hand, can be used in huge models and learn
    to solve a much broader range of problems, but will struggle to find the right
    parameters for a small model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，更小的网络也可以完成任务，但必须通过人工构建或由专门为特定任务设计的算法学习。另一方面，梯度下降可以应用于大型模型，学习解决更广泛的问题，但在小模型中找到合适的参数会面临困难。
- en: The reason it struggles is that it can’t see how changes to parameters it makes
    in any given training epoch will affect the performance later. We, the ball rolling
    around the problem space, might be tempted to start rolling to the right, as that’s
    where the ground slopes down the steepest. But that might lead us straight to
    a dreaded local minimum! Maybe it would have been better to keep on rolling straight
    — even if it didn’t look that steep from where we were standing — as it would
    eventually have taken us to the deepest of all the valleys.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 它之所以感到困难，是因为它无法看到在任何给定的训练周期中对参数的更改将如何影响后续的性能。我们，这个在问题空间中滚动的球，可能会被诱使向右滚动，因为那里地面坡度最陡。但这可能会直接带我们到一个令人害怕的局部最小值！也许继续直行会更好——即使从我们站的位置看起来不那么陡——因为最终会带我们到所有山谷中最深的那个。
- en: Gradient descent tries to solve this by increasing the number of model parameters,
    since more dimensions simply reduces our statistical likelihood of getting stuck.
    Yet more parameters can make the model ‘overfit’ (which means to memorise the
    answer to the current problem but thereafter be useless at generalising to other
    problems). Hence, we end up having to add a corresponding amount of training data,
    to help the trained model be more generalisable. But this constant need to increase
    resources is unsustainable. There must be a better way!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降试图通过增加模型参数的数量来解决这个问题，因为更多的维度只是减少我们陷入困境的统计可能性。然而，更多的参数可能使模型“过拟合”（即记住当前问题的答案，但之后对其他问题的概括能力变得无用）。因此，我们最终不得不增加相应的训练数据，以帮助训练出的模型具有更好的泛化能力。但这种不断增加资源的需求是不可持续的。必须有更好的方法！
- en: Guided Transfer Learning… to the Rescue!
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指导性迁移学习…来拯救！
- en: Dr Nikolić and his colleagues Davor Andrić and Vjekoslav Nikolić (yes, they’re
    brothers!) propose a solution which I think is rather sweet. They liken it to
    sending scouts into the problem landscape, only here, “the scouts are models that
    solve smaller, but related problems in this space. They go as deeply as needed
    and come back with information about the direction in which they successfully
    traveled.” The scouts are less likely to get stuck in local minima, because their
    problems are easier to solve. And because they’re solving smaller problems, we
    can give them more data to work with, which makes them less likely to overfit.
    In the end, they transfer their knowledge back to the main model. Cute, right?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Nikolić博士和他的同事Davor Andrić及Vjekoslav Nikolić（是的，他们是兄弟！）提出了一个我认为相当巧妙的解决方案。他们将其比作将侦察员送入问题领域，只不过在这里，“侦察员是解决这个领域中更小但相关问题的模型。他们深入探究并带回关于他们成功旅行方向的信息。”
    侦察员不太可能陷入局部极小值，因为他们的问题更容易解决。而且由于他们解决的是较小的问题，我们可以提供更多数据给他们工作，这使得他们不容易过拟合。最终，他们将知识转移回主模型。可爱，对吧？
- en: 'The paper describes it best, so I’ll paraphrase: Local minima are like mountain
    lakes; rainwater descends down a mountain by only looking locally, which can cause
    it to get stuck. Guided Transfer Learning (GTL) is like walking along the side
    of a mountain slope, instead of straight down the nearest valley. Sometimes the
    valley is a trap, and the scouts know that.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中对此的描述最为恰当，所以我将其转述：局部极小值就像山中的湖泊；降雨水仅凭局部视角向下流动，这可能导致其陷入困境。指导性迁移学习（GTL）就像沿着山坡侧行走，而不是直接向最近的山谷下滑。有时山谷是一个陷阱，而侦察员知道这一点。
- en: So How Does it Work?
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么它是如何工作的呢？
- en: Scouts are ‘sent off’ to solve less difficult problems than the main model.
    For example, if the overall task is to classify inputs into one of ten different
    classes, then each scout model might be assigned a subset of the data which features
    only three of those classes, for them to classify. Simpler problems reduce the
    chance that the scout models will overfit; thus, the ‘knowledge’ they gain on
    behalf of the main model is more reliable.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 侦察员被“派遣”去解决比主模型更简单的问题。例如，如果整体任务是将输入分类为十个不同的类别中的一个，那么每个侦察员模型可能会被分配一个只包含这三个类别的数据子集进行分类。简单的问题减少了侦察员模型过拟合的机会，因此，它们代表主模型获得的“知识”更可靠。
- en: An additional tactic to reduce the risk of overfitting is to give the scouts
    more data. This also has knowledge transfer benefits similar to pre-training,
    which I’ll cover in the next section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 降低过拟合风险的另一种策略是给侦察员更多的数据。这也具有类似于预训练的知识迁移好处，我将在下一部分中介绍。
- en: The result of this scouting expedition is a so-called ‘guidance matrix’, which
    tells the main model, for each parameter, how important that parameter is. A low
    value means the scouts didn’t find many changes along that dimension (parameter),
    so changing it probably won’t contribute much to the overall solution. For example,
    the scouts walked that way but the slope stayed pretty gentle, so they gave up
    on it. A big value means a more dramatic and potentially useful change. For example,
    one scout took a path that seemed pretty flat, and then nearly stumbled over a
    cliff edge! Probably a good idea to move the affected parameter in that direction,
    right?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这次侦察任务的结果是所谓的“指导矩阵”，它告诉主模型对于每个参数，这个参数的重要性。一个低值意味着侦察员在该维度（参数）上没有发现太多变化，因此改变它可能对整体解决方案贡献不大。例如，侦察员走了这条路但坡度保持相当平缓，所以他们放弃了。一个大值意味着更戏剧性和潜在有用的变化。例如，一个侦察员走了一条看起来很平坦的小径，然后几乎绊倒在悬崖边缘！很可能向那个方向移动受影响的参数是个好主意，对吧？
- en: 'The nice thing about this approach is that it’s really simple, both in terms
    of the maths and the code it takes to implement. Basically, gradient descent is
    busy as always, calculating the value of changing each parameter, using the information
    it has about the sloping landscape all around it. Say the parameter is a network
    weight, w. We call the value of changing that Δ_w (where _ indicates that the
    following letter is a subscript), or ‘delta w’. The guidance matrix has a corresponding
    guiding value, called g_w. So the *guided* change, Δw_g, is simply:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的好处在于，它在数学和实现代码方面都非常简单。基本上，梯度下降始终忙于计算每个参数变化的值，使用它对周围倾斜地形的所有信息。假设参数是一个网络权重w。我们称这种变化的值为Δ_w（其中
    _ 表示后续字母是下标），或称为‘delta w’。引导矩阵有一个相应的引导值，称为g_w。因此，*引导*变化Δw_g，就是：
- en: '![](../Images/c0b887d357980c29ca1a00280040402f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0b887d357980c29ca1a00280040402f.png)'
- en: 'That’s it, that’s all the maths I’m going to cover (there’s a tiny bit more
    to this on page 5 of the paper, and on page 7 the authors discuss, again in a
    very simple way, the different possible ways to compute the guidance matrix itself.
    But what I’ve given you is all the maths you need to understand the idea). The
    code is equally simple. If you’re not a coder, just trust me that the following
    is a no-brainer:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些，我将涵盖的数学就是这些（在论文第5页上还有一点点更多，第7页上作者以非常简单的方式讨论了计算引导矩阵的不同方法。但我给你的已经是理解这个思想所需的所有数学）。代码也同样简单。如果你不是程序员，放心，下面的代码非常简单易懂：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: But What About Traditional Pre-Training?
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 但传统的预训练呢？
- en: The ‘PT’ in GPT-4 stands for ‘pre-training’, so in case you didn’t already know
    it, it’s a big deal. The idea is that you first train a neural network on some
    huge but generic dataset and task, before fine-tuning it on a more specific dataset
    and task. For example, GPT and other Large Language Models are typically pre-trained
    to predict missing words (that is, words that have been randomly removed by their
    human overlords) from sentences, using text crawled from pretty much the whole
    internet. This gives the model a fairly good idea (statistically speaking, not
    cognitively, of course!) about which words occur most often in different contexts.
    After this, the model is fine-tuned on the actual task at hand, such as question
    answering, using a smaller and more specific dataset. The general ‘knowledge’
    acquired from the original task tends to improve downstream task performance;
    we call this ‘transfer learning’.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4中的‘PT’代表‘预训练’，所以如果你还不知道，这可是大事。其思想是在某个庞大而通用的数据集和任务上首先训练神经网络，然后在更具体的数据集和任务上进行微调。例如，GPT和其他大型语言模型通常会预训练以预测句子中缺失的单词（即那些被人类操控者随机移除的单词），使用从几乎整个互联网抓取的文本。这使得模型对不同上下文中最常出现的单词有一个相当好的了解（从统计学角度讲，当然不是认知上的）。之后，模型会在实际任务上进行微调，例如问答任务，使用更小且更具体的数据集。从原始任务中获得的一般‘知识’往往能提升下游任务的表现；我们称之为‘迁移学习’。
- en: Transfer learning from pre-training helps deal with many, but not necessarily
    all, of the local minima that may exist in the problem landscape. Thus, Guided
    Transfer Learning complements pre-training, by helping scout ahead for new problem
    areas to avoid. The two methods don’t have to be combined, but when they are,
    the gains can be dramatic.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习通过预训练有助于应对许多，但不一定全部，可能存在的局部最小值。因此，引导迁移学习通过帮助探索新的问题领域以避免来补充预训练。这两种方法不一定需要结合，但当它们结合时，效果可能会非常显著。
- en: Sounds Great, But is it Effective?
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 听起来不错，但有效吗？
- en: The paper first demonstrates the benefits of Guided Transfer Learning for ‘**one-shot
    learning**’. This is the task of training a model to do one thing, such as classify
    images into different categories, then showing it a single example of a new class
    and expecting it to be able to correctly classify more examples of that class
    thereafter. The authors found that using GTL in addition to pre-training increased
    performance consistently, but that the guidance matrices quickly reached a maximum
    degree of ‘helpfulness’, after which adding more scouts or giving them more data
    didn’t help. On the plus side, this also meant that reducing the number of scouts
    or the size of their training data didn’t hurt performance much.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 论文首先展示了指导转移学习在‘**一次性学习**’中的好处。这是将一个模型训练成执行一项任务，例如将图像分类为不同类别，然后向其展示一个新类别的单一示例，并期望它能够正确分类更多该类别的示例。作者发现，使用GTL作为预训练的补充可以一致地提高性能，但指导矩阵很快达到了‘帮助’的最大程度，此后添加更多的侦察者或提供更多的数据并没有帮助。好的一面是，这也意味着减少侦察者的数量或训练数据的规模对性能影响不大。
- en: 'The implication is that GTL is a low cost, somewhat helpful technique, particularly
    in cases of limited data (where one often turns to one- and few-shot learning).
    And it’s early days yet: the authors only tried using single guidance matrices
    so far but suggest that other implementations might be useful, such as creating
    different matrices depending on how far the scout has moved from its starting
    point. This makes intuitive sense to me: the main model already has information
    about its immediate surroundings and we want the scouts to help see what’s further
    up ahead, so if we made multiple matrices and placed greater importance on those
    which are further from the starting point, this might help the algorithm ‘choose’
    the best next step to make.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着GTL是一种低成本、稍微有用的技术，特别是在数据有限的情况下（通常会使用一次性学习或少量学习）。而且目前还处于初期阶段：作者目前只尝试了使用单一指导矩阵，但建议其他实现可能会有用，比如根据侦察者从起点移动的距离来创建不同的矩阵。这在直观上是有意义的：主要模型已经有关于其周围环境的信息，我们希望侦察者帮助看到更远的地方，因此如果我们制作多个矩阵，并对那些距离起点更远的矩阵赋予更大的重要性，这可能会帮助算法‘选择’最佳的下一步行动。
- en: '![](../Images/97a68cd149c74fb88bd5dc2b43951da3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97a68cd149c74fb88bd5dc2b43951da3.png)'
- en: 'Reproduced with the authors’ permission: A) Example one-short learning task:
    One example is given for training, then other examples of the same characters
    must be found. B) An example of classification performance using pre-training
    and adding GTL.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 经作者许可转载：A) 示例一次性学习任务：给出一个示例进行训练，然后必须找到其他相同字符的示例。B) 使用预训练和添加GTL的分类性能示例。
- en: The second experiment dealt with the ‘XOR problem,’ AKA ‘exclusive OR.’ The
    goal is to learn a function that maps two inputs (x1 and x2) to an output (y),
    such that y = 1 (or ‘True’) if x1 and x2 are different, and y = 0 (or ‘False’)
    otherwise. Large models using gradient descent often stagnate on this challenge,
    so Nikolić, Andrić and Nikolić applied GTL *without pre-training* to help **avoid**
    these **local minima**. This proved so effective that no stagnation could be observed.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个实验涉及‘XOR问题’，也叫‘排他性或’。目标是学习一个将两个输入（x1和x2）映射到一个输出（y）的函数，使得当x1和x2不同时，y = 1（或‘真’），否则y
    = 0（或‘假’）。使用梯度下降的大型模型通常会在这一挑战中停滞不前，因此Nikolić、Andrić和Nikolić应用了*未经过预训练*的GTL来帮助**避免**这些**局部最小值**。这一方法非常有效，以至于没有观察到停滞现象。
- en: 'The final experiment addressed the issue of ‘catastrophic forgetting’: where
    neural networks trained on new tasks are so influenced by the new data that all
    their learned weights get massively updated, causing them to ‘forget’ what they
    learned before. Nikolić et al. performed a series of model re-trainings: each
    additional training step lasted a fixed number of epochs and was based on a single
    additional data point; each data point was a new example of the categories already
    learned. That is, the data points were learned sequentially, not in a batch as
    is typically done. In a private conversation, Danko likened this to real life:
    there exists a category of object called ‘car’, and today you are driving in a
    particular kind of one, like an SUV. Just because you’re now being exposed to
    SUV specific features, it doesn’t mean you’ll forget all the other car types that
    you were exposed to in the past. Your new exposure should increase your understanding
    of what a car is like, not reduce it.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的实验解决了“灾难性遗忘”的问题：即神经网络在新任务上训练时，新数据的影响使得所有已学习的权重大幅更新，从而“忘记”了之前学到的内容。Nikolić等人进行了一系列模型重训练：每一步额外的训练持续固定数量的纪元，并基于一个额外的数据点；每个数据点都是已经学习过的类别的新示例。也就是说，这些数据点是顺序学习的，而不是通常的批量学习。在一次私人谈话中，Danko将其比作现实生活：存在一种称为“汽车”的物体类别，而今天你正在驾驶一种特定类型的汽车，比如SUV。即使你现在接触到SUV的特性，也不意味着你会忘记以前接触过的所有其他汽车类型。你的新接触应该增加你对汽车的理解，而不是减少它。
- en: 'While classical transfer learning from pre-training alone would not be able
    to benefit from these sequential additions of data points, the addition of GTL
    caused a stepwise performance boost, indicating that the **knowledge added up**.
    In other words, the models were more robust to forgetting the previous examples,
    although it did occur eventually. The authors explained this in common sense terms:
    in a given problem space, there will be some solutions that are appropriate both
    for examples learned now, *and* examples learned before; these solutions probably
    lie close to the model’s starting point as it begins its training on the new example;
    without GTL, the model can ‘wander away’; *with* GTL, the guidance matrices encourage
    the model to remain in that useful vicinity. It’s like a scout reminding you not
    to stray too far from the camp.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管仅仅依靠经典的预训练转移学习无法从这些顺序添加的数据点中受益，但添加GTL却带来了逐步的性能提升，这表明**知识得到了积累**。换句话说，模型对遗忘之前的示例更具鲁棒性，尽管最终还是会发生。作者用常识解释了这一点：在特定的问题空间中，会有一些解决方案既适用于现在学习的示例，*也*适用于之前学习的示例；这些解决方案可能接近于模型开始在新示例上训练时的起始点；没有GTL，模型可能会“偏离”；*有了*GTL，指导矩阵鼓励模型保持在那个有用的邻域。这就像一个侦察员提醒你不要离营地太远。
- en: So What Does This Mean in Practice?
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那这在实践中意味着什么？
- en: Guided Transfer Learning is a method of ‘learning to learn’, and it’s not the
    only one out there. Other approaches exist and maybe even produce better results,
    but they are often unscalable, which can defeat their purpose. GTL is computationally
    cheap, and flexible, which may make it particularly appropriate for those models
    that have already squeezed everything they can get out of their available resources.
    Large Language Models, and computer vision models which often exist under tough
    hardware constraints, like being embedded in autonomous vehicles, are a couple
    of good examples.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 指导转移学习是一种“学习如何学习”的方法，并不是唯一的方法。其他方法也存在，并且可能会产生更好的结果，但它们往往不可扩展，这可能会使它们失去意义。GTL计算便宜且灵活，这可能使它特别适用于那些已经将所有可用资源榨取出来的模型。大型语言模型和计算机视觉模型（这些模型经常在严格的硬件限制下存在，如嵌入在自动驾驶汽车中）是几个好的例子。
- en: On the other hand, if data and compute power are available in abundance, then
    the authors admit that GTL might not be needed. It’s not a panacea, as they say,
    but then no single machine learning solution is. And as the experiments showed,
    it still has promise for those hard problems which even well-resourced models
    struggle with.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果数据和计算能力非常充足，那么作者承认GTL可能就不再需要了。正如他们所说，它不是万能的，但没有任何单一的机器学习解决方案是万能的。正如实验所示，它对那些即使是资源充足的模型也难以解决的难题仍然充满了希望。
- en: With all the possible guidance matrix paradigms the authors plan to experiment
    with, I’m optimistic. So, watch this space.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作者计划尝试所有可能的指导矩阵范式，我对此感到乐观。所以，请关注这个领域。
