- en: 'Guided Transfer Learning: How to use â€˜the power of scoutsâ€™ to boost machine
    learning performance'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ‰æŒ‡å¯¼çš„è¿ç§»å­¦ä¹ ï¼šå¦‚ä½•åˆ©ç”¨â€œä¾¦å¯Ÿçš„åŠ›é‡â€æå‡æœºå™¨å­¦ä¹ è¡¨ç°
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/guided-transfer-learning-how-to-use-the-power-of-scouts-to-boost-machine-learning-performance-7e478d1ec5e4](https://towardsdatascience.com/guided-transfer-learning-how-to-use-the-power-of-scouts-to-boost-machine-learning-performance-7e478d1ec5e4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/guided-transfer-learning-how-to-use-the-power-of-scouts-to-boost-machine-learning-performance-7e478d1ec5e4](https://towardsdatascience.com/guided-transfer-learning-how-to-use-the-power-of-scouts-to-boost-machine-learning-performance-7e478d1ec5e4)
- en: An exclusive sneak-peek at a revolutionary new method for training neural networks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹è®­ç»ƒç¥ç»ç½‘ç»œçš„é©å‘½æ€§æ–°æ–¹æ³•çš„ç‹¬å®¶é¢„è§ˆ
- en: '[](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)[![Katherine
    Munro](../Images/8013140495c7b9bd25ef08d712f097bf.png)](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)
    [Katherine Munro](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)[![Katherine
    Munro](../Images/8013140495c7b9bd25ef08d712f097bf.png)](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)
    [Katherine Munro](https://katherineamunro.medium.com/?source=post_page-----7e478d1ec5e4--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)
    Â·10 min readÂ·Mar 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e478d1ec5e4--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 10 åˆ†é’ŸÂ·2023å¹´3æœˆ27æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/482d6e71232893524e6b0b5e6d2cf3bd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/482d6e71232893524e6b0b5e6d2cf3bd.png)'
- en: In this newly proposed technique, small â€˜scout modelsâ€™ are sent to navigate
    the problem landscape and â€˜report backâ€™ to the main model. Photo by [@ansgarscheffold](http://twitter.com/ansgarscheffold)
    on Unsplash.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€æ–°æè®®çš„æŠ€æœ¯ä¸­ï¼Œå°å‹â€œä¾¦å¯Ÿæ¨¡å‹â€è¢«æ´¾é£å»æ¢ç´¢é—®é¢˜é¢†åŸŸï¼Œå¹¶â€œåé¦ˆâ€ç»™ä¸»è¦æ¨¡å‹ã€‚å›¾ç‰‡ç”± [@ansgarscheffold](http://twitter.com/ansgarscheffold)
    åœ¨ Unsplash ä¸Šæä¾›ã€‚
- en: 'My good friend and humble genius Dr Danko NikoliÄ‡ recently shared an unpublished
    paper with me, thinking I might be interested. Was I ever. Reading it made me
    feel like I was witnessing a historic moment before anyone else did, and I was
    immediately bursting to share. Thankfully, Danko agreed. So hereâ€™s my translation
    into day-to-day language of a method I think could revolutionise the training
    of deep neural networks. Itâ€™s not even out on arXiv yet (update: [now it is!](https://arxiv.org/abs/2303.16154)),
    but [NASA are already using it](https://www.linkedin.com/posts/robots-go-mental_nasa-genelab-open-science-for-life-in-space-activity-7041340569012301824-xTk3?utm_source=share&utm_medium=member_desktop).
    So once it does blow up, remember: you heard it here first. ğŸ˜‰'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„å¥½æœ‹å‹å’Œè°¦é€Šçš„å¤©æ‰ Dr Danko NikoliÄ‡ æœ€è¿‘ä¸æˆ‘åˆ†äº«äº†ä¸€ç¯‡æœªå‘è¡¨çš„è®ºæ–‡ï¼Œè®¤ä¸ºæˆ‘å¯èƒ½ä¼šæ„Ÿå…´è¶£ã€‚æˆ‘ç¡®å®æ„Ÿå…´è¶£äº†ã€‚é˜…è¯»å®ƒè®©æˆ‘æ„Ÿè§‰åƒæ˜¯åœ¨ç›®ç¹ä¸€ä¸ªå†å²æ—¶åˆ»ï¼Œå¹¶ä¸”æˆ‘è¿«ä¸åŠå¾…åœ°æƒ³è¦åˆ†äº«ã€‚å¹¸è¿çš„æ˜¯ï¼ŒDanko
    åŒæ„äº†ã€‚æ‰€ä»¥è¿™æ˜¯æˆ‘å°†ä¸€ç§å¯èƒ½é©æ–°æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒçš„æ–¹æ³•ç¿»è¯‘æˆæ—¥å¸¸è¯­è¨€çš„ç‰ˆæœ¬ã€‚å®ƒç”šè‡³è¿˜æ²¡æœ‰å‘å¸ƒåˆ° arXiv ä¸Šï¼ˆæ›´æ–°ï¼š[ç°åœ¨å·²ç»å‘å¸ƒäº†ï¼](https://arxiv.org/abs/2303.16154)ï¼‰ï¼Œä½†
    [NASA å·²ç»åœ¨ä½¿ç”¨å®ƒ](https://www.linkedin.com/posts/robots-go-mental_nasa-genelab-open-science-for-life-in-space-activity-7041340569012301824-xTk3?utm_source=share&utm_medium=member_desktop)ã€‚æ‰€ä»¥ä¸€æ—¦å®ƒçˆ†ç«ï¼Œè®°ä½ï¼šä½ é¦–å…ˆåœ¨è¿™é‡Œå¬åˆ°çš„ã€‚ğŸ˜‰
- en: Letâ€™s Start With The Problem
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»é—®é¢˜å¼€å§‹
- en: 'Iâ€™m sure you know it: machine learning, especially with deep neural networks,
    requires a frankly ludicrous amount of data, compute power, and model parameters.
    This makes them inaccessible to all but the wealthiest of companies and research
    institutions, and thus concentrates â€” into the hands of a small few â€” the power
    to develop AI technologies that will shape our technological future. Not cool.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç›¸ä¿¡ä½ çŸ¥é“ï¼šæœºå™¨å­¦ä¹ ï¼Œå°¤å…¶æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œï¼Œéœ€è¦å¤§é‡çš„æ•°æ®ã€è®¡ç®—èƒ½åŠ›å’Œæ¨¡å‹å‚æ•°ã€‚è¿™ä½¿å¾—è¿™äº›æŠ€æœ¯åªæœ‰æœ€å¯Œæœ‰çš„å…¬å¸å’Œç ”ç©¶æœºæ„æ‰èƒ½ä½¿ç”¨ï¼Œå› æ­¤å°†å¼€å‘å¡‘é€ æˆ‘ä»¬æŠ€æœ¯æœªæ¥çš„AIæŠ€æœ¯çš„æƒåŠ›é›†ä¸­åœ¨å°‘æ•°äººæ‰‹ä¸­ã€‚å¹¶ä¸é…·ã€‚
- en: Why The Problem Exists
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆä¼šæœ‰è¿™ä¸ªé—®é¢˜
- en: When we train a neural network for a task, weâ€™re basically teaching it, via
    thousands of examples, how to adjust its own weights and biases so that information
    flowing into the network produces some other, desired kind of information to flow
    out. These weights and biases are collectively known as â€˜parametersâ€™, and a network
    can have millions or even trillions of them. With such a large â€˜parameter spaceâ€™,
    itâ€™s really hard to learn the right ones; there are just too many.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬ä¸ºæŸä¸ªä»»åŠ¡è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œå®é™…ä¸Šæ˜¯åœ¨é€šè¿‡æˆåƒä¸Šä¸‡ä¸ªä¾‹å­æ•™å®ƒå¦‚ä½•è°ƒæ•´è‡ªèº«çš„æƒé‡å’Œåå·®ï¼Œä»¥ä¾¿è¾“å…¥ç½‘ç»œçš„ä¿¡æ¯äº§ç”Ÿå¦ä¸€ç§æ‰€éœ€çš„ä¿¡æ¯è¾“å‡ºã€‚è¿™äº›æƒé‡å’Œåå·®ç»Ÿç§°ä¸ºâ€œå‚æ•°â€ï¼Œä¸€ä¸ªç½‘ç»œå¯ä»¥æ‹¥æœ‰æ•°ç™¾ä¸‡ç”šè‡³æ•°ä¸‡äº¿ä¸ªè¿™æ ·çš„å‚æ•°ã€‚ç”±äºâ€œå‚æ•°ç©ºé—´â€å¦‚æ­¤åºå¤§ï¼Œå­¦ä¹ åˆ°æ­£ç¡®çš„å‚æ•°éå¸¸å›°éš¾ï¼Œå› ä¸ºæ•°é‡å®åœ¨å¤ªå¤šã€‚
- en: Since thereâ€™s no way to try out every possible combination of parameter values,
    we attempt to make good guesses. A typical machine learning algorithm looks for
    â€˜hintsâ€™ about how each parameter should change, changes them, then looks for new
    hints based on how successful that was. The most famous of these algorithms is
    called â€˜gradient descentâ€™.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ— æ³•å°è¯•æ¯ç§å¯èƒ½çš„å‚æ•°å€¼ç»„åˆï¼Œæˆ‘ä»¬å°è¯•åšå‡ºå¥½çš„çŒœæµ‹ã€‚ä¸€ä¸ªå…¸å‹çš„æœºå™¨å­¦ä¹ ç®—æ³•ä¼šå¯»æ‰¾æœ‰å…³æ¯ä¸ªå‚æ•°å¦‚ä½•å˜åŒ–çš„â€œæç¤ºâ€ï¼Œç„¶åæ ¹æ®è¿™äº›æç¤ºè¿›è¡Œè°ƒæ•´ï¼Œå†æ ¹æ®è°ƒæ•´çš„æˆåŠŸç¨‹åº¦å¯»æ‰¾æ–°çš„æç¤ºã€‚è¿™äº›ç®—æ³•ä¸­æœ€è‘—åçš„æ˜¯â€œæ¢¯åº¦ä¸‹é™â€ã€‚
- en: 'If youâ€™re already familiar with gradient descent, skip this paragraph. Otherwise,
    you can imagine it working like this: the problem space is a landscape of hills
    and valleys, and we are a ball rolling around that landscape, trying to get to
    the lowest point of the lowest valley. We call this the â€˜global minimumâ€™. So our
    algorithm will take cues from how steep the ground around us is, which direction
    itâ€™s sloping, and so on, and then decide which way to roll. Hopefully, itâ€™s toward
    that lowest of low points. But thereâ€™s a danger: we might roll into a ditch which
    *isnâ€™t quite as low*, and get stuck. Weâ€™d call this a â€˜local minimumâ€™, and itâ€™s
    a bad place to be.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å·²ç»å¯¹æ¢¯åº¦ä¸‹é™æœ‰æ‰€äº†è§£ï¼Œå¯ä»¥è·³è¿‡è¿™ä¸€æ®µã€‚å¦åˆ™ï¼Œä½ å¯ä»¥å°†å…¶æƒ³è±¡ä¸ºè¿™æ ·å·¥ä½œï¼šé—®é¢˜ç©ºé—´æ˜¯ä¸€ä¸ªæœ‰ä¸˜é™µå’Œå±±è°·çš„åœ°å½¢ï¼Œè€Œæˆ‘ä»¬æ˜¯ä¸€ä¸ªåœ¨è¿™ä¸ªåœ°å½¢ä¸Šæ»šåŠ¨çš„çƒï¼Œè¯•å›¾åˆ°è¾¾æœ€ä½çš„å±±è°·åº•éƒ¨ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œå…¨å±€æœ€å°å€¼â€ã€‚æ‰€ä»¥æˆ‘ä»¬çš„ç®—æ³•ä¼šæ ¹æ®å‘¨å›´åœ°é¢çš„é™¡å³­ç¨‹åº¦ã€å€¾æ–œæ–¹å‘ç­‰ä¿¡æ¯æ¥å†³å®šæ»šåŠ¨çš„æ–¹å‘ã€‚å¸Œæœ›å®ƒæœç€æœ€ä½çš„ç‚¹å‰è¿›ã€‚ä½†ä¹Ÿæœ‰å±é™©ï¼šæˆ‘ä»¬å¯èƒ½ä¼šæ»šå…¥ä¸€ä¸ªâ€œå¹¶ä¸æ˜¯ç‰¹åˆ«ä½â€çš„æ²Ÿå£‘ä¸­ï¼Œå¹¶é™·å…¥å›°å¢ƒã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œå±€éƒ¨æœ€å°å€¼â€ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸å¥½çš„ä½ç½®ã€‚
- en: Neural networks use gradient descent to learn to solve a huge variety of problems.
    Unfortunately, some studies have shown that these networks can only improve their
    performance so much before they need to increase their size and data again, and
    this tendency seems to follow a â€˜power lawâ€™. Put simply, it means that a small
    increase in â€˜intelligenceâ€™ requires a massive increase in resources. Put inversely,
    weâ€™re looking at diminishing returns for the resources we feed into these models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥å­¦ä¹ è§£å†³å„ç§é—®é¢˜ã€‚ä¸å¹¸çš„æ˜¯ï¼Œä¸€äº›ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›ç½‘ç»œçš„æ€§èƒ½æå‡æœ‰é™ï¼Œä¹‹åéœ€è¦å†æ¬¡å¢åŠ è§„æ¨¡å’Œæ•°æ®ï¼Œè€Œè¿™ç§è¶‹åŠ¿ä¼¼ä¹éµå¾ªä¸€ç§â€œå¹‚å¾‹â€ã€‚ç®€å•æ¥è¯´ï¼Œè¿™æ„å‘³ç€â€œå°å¹…åº¦çš„â€˜æ™ºèƒ½â€™æå‡éœ€è¦å¤§é‡çš„èµ„æºå¢åŠ â€ã€‚åè¿‡æ¥è¯´ï¼Œæˆ‘ä»¬æŠ•å…¥è¿™äº›æ¨¡å‹çš„èµ„æºå›æŠ¥æ­£åœ¨é€’å‡ã€‚
- en: Theoretical Hopeâ€¦ and More Pain
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†è®ºä¸Šçš„å¸Œæœ›â€¦â€¦å’Œæ›´å¤šçš„ç—›è‹¦
- en: Theoretically, much smaller networks can exist and get the job done, but they
    have to be hand-constructed or learned by algorithms designed for very specific
    tasks. Gradient descent, on the other hand, can be used in huge models and learn
    to solve a much broader range of problems, but will struggle to find the right
    parameters for a small model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è®ºä¸Šï¼Œæ›´å°çš„ç½‘ç»œä¹Ÿå¯ä»¥å®Œæˆä»»åŠ¡ï¼Œä½†å¿…é¡»é€šè¿‡äººå·¥æ„å»ºæˆ–ç”±ä¸“é—¨ä¸ºç‰¹å®šä»»åŠ¡è®¾è®¡çš„ç®—æ³•å­¦ä¹ ã€‚å¦ä¸€æ–¹é¢ï¼Œæ¢¯åº¦ä¸‹é™å¯ä»¥åº”ç”¨äºå¤§å‹æ¨¡å‹ï¼Œå­¦ä¹ è§£å†³æ›´å¹¿æ³›çš„é—®é¢˜ï¼Œä½†åœ¨å°æ¨¡å‹ä¸­æ‰¾åˆ°åˆé€‚çš„å‚æ•°ä¼šé¢ä¸´å›°éš¾ã€‚
- en: The reason it struggles is that it canâ€™t see how changes to parameters it makes
    in any given training epoch will affect the performance later. We, the ball rolling
    around the problem space, might be tempted to start rolling to the right, as thatâ€™s
    where the ground slopes down the steepest. But that might lead us straight to
    a dreaded local minimum! Maybe it would have been better to keep on rolling straight
    â€” even if it didnâ€™t look that steep from where we were standing â€” as it would
    eventually have taken us to the deepest of all the valleys.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¹‹æ‰€ä»¥æ„Ÿåˆ°å›°éš¾ï¼Œæ˜¯å› ä¸ºå®ƒæ— æ³•çœ‹åˆ°åœ¨ä»»ä½•ç»™å®šçš„è®­ç»ƒå‘¨æœŸä¸­å¯¹å‚æ•°çš„æ›´æ”¹å°†å¦‚ä½•å½±å“åç»­çš„æ€§èƒ½ã€‚æˆ‘ä»¬ï¼Œè¿™ä¸ªåœ¨é—®é¢˜ç©ºé—´ä¸­æ»šåŠ¨çš„çƒï¼Œå¯èƒ½ä¼šè¢«è¯±ä½¿å‘å³æ»šåŠ¨ï¼Œå› ä¸ºé‚£é‡Œåœ°é¢å¡åº¦æœ€é™¡ã€‚ä½†è¿™å¯èƒ½ä¼šç›´æ¥å¸¦æˆ‘ä»¬åˆ°ä¸€ä¸ªä»¤äººå®³æ€•çš„å±€éƒ¨æœ€å°å€¼ï¼ä¹Ÿè®¸ç»§ç»­ç›´è¡Œä¼šæ›´å¥½â€”â€”å³ä½¿ä»æˆ‘ä»¬ç«™çš„ä½ç½®çœ‹èµ·æ¥ä¸é‚£ä¹ˆé™¡â€”â€”å› ä¸ºæœ€ç»ˆä¼šå¸¦æˆ‘ä»¬åˆ°æ‰€æœ‰å±±è°·ä¸­æœ€æ·±çš„é‚£ä¸ªã€‚
- en: Gradient descent tries to solve this by increasing the number of model parameters,
    since more dimensions simply reduces our statistical likelihood of getting stuck.
    Yet more parameters can make the model â€˜overfitâ€™ (which means to memorise the
    answer to the current problem but thereafter be useless at generalising to other
    problems). Hence, we end up having to add a corresponding amount of training data,
    to help the trained model be more generalisable. But this constant need to increase
    resources is unsustainable. There must be a better way!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™è¯•å›¾é€šè¿‡å¢åŠ æ¨¡å‹å‚æ•°çš„æ•°é‡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå› ä¸ºæ›´å¤šçš„ç»´åº¦åªæ˜¯å‡å°‘æˆ‘ä»¬é™·å…¥å›°å¢ƒçš„ç»Ÿè®¡å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œæ›´å¤šçš„å‚æ•°å¯èƒ½ä½¿æ¨¡å‹â€œè¿‡æ‹Ÿåˆâ€ï¼ˆå³è®°ä½å½“å‰é—®é¢˜çš„ç­”æ¡ˆï¼Œä½†ä¹‹åå¯¹å…¶ä»–é—®é¢˜çš„æ¦‚æ‹¬èƒ½åŠ›å˜å¾—æ— ç”¨ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ€ç»ˆä¸å¾—ä¸å¢åŠ ç›¸åº”çš„è®­ç»ƒæ•°æ®ï¼Œä»¥å¸®åŠ©è®­ç»ƒå‡ºçš„æ¨¡å‹å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ä½†è¿™ç§ä¸æ–­å¢åŠ èµ„æºçš„éœ€æ±‚æ˜¯ä¸å¯æŒç»­çš„ã€‚å¿…é¡»æœ‰æ›´å¥½çš„æ–¹æ³•ï¼
- en: Guided Transfer Learningâ€¦ to the Rescue!
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŒ‡å¯¼æ€§è¿ç§»å­¦ä¹ â€¦æ¥æ‹¯æ•‘ï¼
- en: Dr NikoliÄ‡ and his colleagues Davor AndriÄ‡ and Vjekoslav NikoliÄ‡ (yes, theyâ€™re
    brothers!) propose a solution which I think is rather sweet. They liken it to
    sending scouts into the problem landscape, only here, â€œthe scouts are models that
    solve smaller, but related problems in this space. They go as deeply as needed
    and come back with information about the direction in which they successfully
    traveled.â€ The scouts are less likely to get stuck in local minima, because their
    problems are easier to solve. And because theyâ€™re solving smaller problems, we
    can give them more data to work with, which makes them less likely to overfit.
    In the end, they transfer their knowledge back to the main model. Cute, right?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: NikoliÄ‡åšå£«å’Œä»–çš„åŒäº‹Davor AndriÄ‡åŠVjekoslav NikoliÄ‡ï¼ˆæ˜¯çš„ï¼Œä»–ä»¬æ˜¯å…„å¼Ÿï¼ï¼‰æå‡ºäº†ä¸€ä¸ªæˆ‘è®¤ä¸ºç›¸å½“å·§å¦™çš„è§£å†³æ–¹æ¡ˆã€‚ä»–ä»¬å°†å…¶æ¯”ä½œå°†ä¾¦å¯Ÿå‘˜é€å…¥é—®é¢˜é¢†åŸŸï¼Œåªä¸è¿‡åœ¨è¿™é‡Œï¼Œâ€œä¾¦å¯Ÿå‘˜æ˜¯è§£å†³è¿™ä¸ªé¢†åŸŸä¸­æ›´å°ä½†ç›¸å…³é—®é¢˜çš„æ¨¡å‹ã€‚ä»–ä»¬æ·±å…¥æ¢ç©¶å¹¶å¸¦å›å…³äºä»–ä»¬æˆåŠŸæ—…è¡Œæ–¹å‘çš„ä¿¡æ¯ã€‚â€
    ä¾¦å¯Ÿå‘˜ä¸å¤ªå¯èƒ½é™·å…¥å±€éƒ¨æå°å€¼ï¼Œå› ä¸ºä»–ä»¬çš„é—®é¢˜æ›´å®¹æ˜“è§£å†³ã€‚è€Œä¸”ç”±äºä»–ä»¬è§£å†³çš„æ˜¯è¾ƒå°çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥æä¾›æ›´å¤šæ•°æ®ç»™ä»–ä»¬å·¥ä½œï¼Œè¿™ä½¿å¾—ä»–ä»¬ä¸å®¹æ˜“è¿‡æ‹Ÿåˆã€‚æœ€ç»ˆï¼Œä»–ä»¬å°†çŸ¥è¯†è½¬ç§»å›ä¸»æ¨¡å‹ã€‚å¯çˆ±ï¼Œå¯¹å§ï¼Ÿ
- en: 'The paper describes it best, so Iâ€™ll paraphrase: Local minima are like mountain
    lakes; rainwater descends down a mountain by only looking locally, which can cause
    it to get stuck. Guided Transfer Learning (GTL) is like walking along the side
    of a mountain slope, instead of straight down the nearest valley. Sometimes the
    valley is a trap, and the scouts know that.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä¸­å¯¹æ­¤çš„æè¿°æœ€ä¸ºæ°å½“ï¼Œæ‰€ä»¥æˆ‘å°†å…¶è½¬è¿°ï¼šå±€éƒ¨æå°å€¼å°±åƒå±±ä¸­çš„æ¹–æ³Šï¼›é™é›¨æ°´ä»…å‡­å±€éƒ¨è§†è§’å‘ä¸‹æµåŠ¨ï¼Œè¿™å¯èƒ½å¯¼è‡´å…¶é™·å…¥å›°å¢ƒã€‚æŒ‡å¯¼æ€§è¿ç§»å­¦ä¹ ï¼ˆGTLï¼‰å°±åƒæ²¿ç€å±±å¡ä¾§è¡Œèµ°ï¼Œè€Œä¸æ˜¯ç›´æ¥å‘æœ€è¿‘çš„å±±è°·ä¸‹æ»‘ã€‚æœ‰æ—¶å±±è°·æ˜¯ä¸€ä¸ªé™·é˜±ï¼Œè€Œä¾¦å¯Ÿå‘˜çŸ¥é“è¿™ä¸€ç‚¹ã€‚
- en: So How Does it Work?
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„å‘¢ï¼Ÿ
- en: Scouts are â€˜sent offâ€™ to solve less difficult problems than the main model.
    For example, if the overall task is to classify inputs into one of ten different
    classes, then each scout model might be assigned a subset of the data which features
    only three of those classes, for them to classify. Simpler problems reduce the
    chance that the scout models will overfit; thus, the â€˜knowledgeâ€™ they gain on
    behalf of the main model is more reliable.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾¦å¯Ÿå‘˜è¢«â€œæ´¾é£â€å»è§£å†³æ¯”ä¸»æ¨¡å‹æ›´ç®€å•çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ•´ä½“ä»»åŠ¡æ˜¯å°†è¾“å…¥åˆ†ç±»ä¸ºåä¸ªä¸åŒçš„ç±»åˆ«ä¸­çš„ä¸€ä¸ªï¼Œé‚£ä¹ˆæ¯ä¸ªä¾¦å¯Ÿå‘˜æ¨¡å‹å¯èƒ½ä¼šè¢«åˆ†é…ä¸€ä¸ªåªåŒ…å«è¿™ä¸‰ä¸ªç±»åˆ«çš„æ•°æ®å­é›†è¿›è¡Œåˆ†ç±»ã€‚ç®€å•çš„é—®é¢˜å‡å°‘äº†ä¾¦å¯Ÿå‘˜æ¨¡å‹è¿‡æ‹Ÿåˆçš„æœºä¼šï¼Œå› æ­¤ï¼Œå®ƒä»¬ä»£è¡¨ä¸»æ¨¡å‹è·å¾—çš„â€œçŸ¥è¯†â€æ›´å¯é ã€‚
- en: An additional tactic to reduce the risk of overfitting is to give the scouts
    more data. This also has knowledge transfer benefits similar to pre-training,
    which Iâ€™ll cover in the next section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: é™ä½è¿‡æ‹Ÿåˆé£é™©çš„å¦ä¸€ç§ç­–ç•¥æ˜¯ç»™ä¾¦å¯Ÿå‘˜æ›´å¤šçš„æ•°æ®ã€‚è¿™ä¹Ÿå…·æœ‰ç±»ä¼¼äºé¢„è®­ç»ƒçš„çŸ¥è¯†è¿ç§»å¥½å¤„ï¼Œæˆ‘å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ä»‹ç»ã€‚
- en: The result of this scouting expedition is a so-called â€˜guidance matrixâ€™, which
    tells the main model, for each parameter, how important that parameter is. A low
    value means the scouts didnâ€™t find many changes along that dimension (parameter),
    so changing it probably wonâ€™t contribute much to the overall solution. For example,
    the scouts walked that way but the slope stayed pretty gentle, so they gave up
    on it. A big value means a more dramatic and potentially useful change. For example,
    one scout took a path that seemed pretty flat, and then nearly stumbled over a
    cliff edge! Probably a good idea to move the affected parameter in that direction,
    right?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¬¡ä¾¦å¯Ÿä»»åŠ¡çš„ç»“æœæ˜¯æ‰€è°“çš„â€œæŒ‡å¯¼çŸ©é˜µâ€ï¼Œå®ƒå‘Šè¯‰ä¸»æ¨¡å‹å¯¹äºæ¯ä¸ªå‚æ•°ï¼Œè¿™ä¸ªå‚æ•°çš„é‡è¦æ€§ã€‚ä¸€ä¸ªä½å€¼æ„å‘³ç€ä¾¦å¯Ÿå‘˜åœ¨è¯¥ç»´åº¦ï¼ˆå‚æ•°ï¼‰ä¸Šæ²¡æœ‰å‘ç°å¤ªå¤šå˜åŒ–ï¼Œå› æ­¤æ”¹å˜å®ƒå¯èƒ½å¯¹æ•´ä½“è§£å†³æ–¹æ¡ˆè´¡çŒ®ä¸å¤§ã€‚ä¾‹å¦‚ï¼Œä¾¦å¯Ÿå‘˜èµ°äº†è¿™æ¡è·¯ä½†å¡åº¦ä¿æŒç›¸å½“å¹³ç¼“ï¼Œæ‰€ä»¥ä»–ä»¬æ”¾å¼ƒäº†ã€‚ä¸€ä¸ªå¤§å€¼æ„å‘³ç€æ›´æˆå‰§æ€§å’Œæ½œåœ¨æœ‰ç”¨çš„å˜åŒ–ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªä¾¦å¯Ÿå‘˜èµ°äº†ä¸€æ¡çœ‹èµ·æ¥å¾ˆå¹³å¦çš„å°å¾„ï¼Œç„¶åå‡ ä¹ç»Šå€’åœ¨æ‚¬å´–è¾¹ç¼˜ï¼å¾ˆå¯èƒ½å‘é‚£ä¸ªæ–¹å‘ç§»åŠ¨å—å½±å“çš„å‚æ•°æ˜¯ä¸ªå¥½ä¸»æ„ï¼Œå¯¹å§ï¼Ÿ
- en: 'The nice thing about this approach is that itâ€™s really simple, both in terms
    of the maths and the code it takes to implement. Basically, gradient descent is
    busy as always, calculating the value of changing each parameter, using the information
    it has about the sloping landscape all around it. Say the parameter is a network
    weight, w. We call the value of changing that Î”_w (where _ indicates that the
    following letter is a subscript), or â€˜delta wâ€™. The guidance matrix has a corresponding
    guiding value, called g_w. So the *guided* change, Î”w_g, is simply:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•çš„å¥½å¤„åœ¨äºï¼Œå®ƒåœ¨æ•°å­¦å’Œå®ç°ä»£ç æ–¹é¢éƒ½éå¸¸ç®€å•ã€‚åŸºæœ¬ä¸Šï¼Œæ¢¯åº¦ä¸‹é™å§‹ç»ˆå¿™äºè®¡ç®—æ¯ä¸ªå‚æ•°å˜åŒ–çš„å€¼ï¼Œä½¿ç”¨å®ƒå¯¹å‘¨å›´å€¾æ–œåœ°å½¢çš„æ‰€æœ‰ä¿¡æ¯ã€‚å‡è®¾å‚æ•°æ˜¯ä¸€ä¸ªç½‘ç»œæƒé‡wã€‚æˆ‘ä»¬ç§°è¿™ç§å˜åŒ–çš„å€¼ä¸ºÎ”_wï¼ˆå…¶ä¸­
    _ è¡¨ç¤ºåç»­å­—æ¯æ˜¯ä¸‹æ ‡ï¼‰ï¼Œæˆ–ç§°ä¸ºâ€˜delta wâ€™ã€‚å¼•å¯¼çŸ©é˜µæœ‰ä¸€ä¸ªç›¸åº”çš„å¼•å¯¼å€¼ï¼Œç§°ä¸ºg_wã€‚å› æ­¤ï¼Œ*å¼•å¯¼*å˜åŒ–Î”w_gï¼Œå°±æ˜¯ï¼š
- en: '![](../Images/c0b887d357980c29ca1a00280040402f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0b887d357980c29ca1a00280040402f.png)'
- en: 'Thatâ€™s it, thatâ€™s all the maths Iâ€™m going to cover (thereâ€™s a tiny bit more
    to this on page 5 of the paper, and on page 7 the authors discuss, again in a
    very simple way, the different possible ways to compute the guidance matrix itself.
    But what Iâ€™ve given you is all the maths you need to understand the idea). The
    code is equally simple. If youâ€™re not a coder, just trust me that the following
    is a no-brainer:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™äº›ï¼Œæˆ‘å°†æ¶µç›–çš„æ•°å­¦å°±æ˜¯è¿™äº›ï¼ˆåœ¨è®ºæ–‡ç¬¬5é¡µä¸Šè¿˜æœ‰ä¸€ç‚¹ç‚¹æ›´å¤šï¼Œç¬¬7é¡µä¸Šä½œè€…ä»¥éå¸¸ç®€å•çš„æ–¹å¼è®¨è®ºäº†è®¡ç®—å¼•å¯¼çŸ©é˜µçš„ä¸åŒæ–¹æ³•ã€‚ä½†æˆ‘ç»™ä½ çš„å·²ç»æ˜¯ç†è§£è¿™ä¸ªæ€æƒ³æ‰€éœ€çš„æ‰€æœ‰æ•°å­¦ï¼‰ã€‚ä»£ç ä¹ŸåŒæ ·ç®€å•ã€‚å¦‚æœä½ ä¸æ˜¯ç¨‹åºå‘˜ï¼Œæ”¾å¿ƒï¼Œä¸‹é¢çš„ä»£ç éå¸¸ç®€å•æ˜“æ‡‚ï¼š
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: But What About Traditional Pre-Training?
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½†ä¼ ç»Ÿçš„é¢„è®­ç»ƒå‘¢ï¼Ÿ
- en: The â€˜PTâ€™ in GPT-4 stands for â€˜pre-trainingâ€™, so in case you didnâ€™t already know
    it, itâ€™s a big deal. The idea is that you first train a neural network on some
    huge but generic dataset and task, before fine-tuning it on a more specific dataset
    and task. For example, GPT and other Large Language Models are typically pre-trained
    to predict missing words (that is, words that have been randomly removed by their
    human overlords) from sentences, using text crawled from pretty much the whole
    internet. This gives the model a fairly good idea (statistically speaking, not
    cognitively, of course!) about which words occur most often in different contexts.
    After this, the model is fine-tuned on the actual task at hand, such as question
    answering, using a smaller and more specific dataset. The general â€˜knowledgeâ€™
    acquired from the original task tends to improve downstream task performance;
    we call this â€˜transfer learningâ€™.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4ä¸­çš„â€˜PTâ€™ä»£è¡¨â€˜é¢„è®­ç»ƒâ€™ï¼Œæ‰€ä»¥å¦‚æœä½ è¿˜ä¸çŸ¥é“ï¼Œè¿™å¯æ˜¯å¤§äº‹ã€‚å…¶æ€æƒ³æ˜¯åœ¨æŸä¸ªåºå¤§è€Œé€šç”¨çš„æ•°æ®é›†å’Œä»»åŠ¡ä¸Šé¦–å…ˆè®­ç»ƒç¥ç»ç½‘ç»œï¼Œç„¶ååœ¨æ›´å…·ä½“çš„æ•°æ®é›†å’Œä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚ä¾‹å¦‚ï¼ŒGPTå’Œå…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸ä¼šé¢„è®­ç»ƒä»¥é¢„æµ‹å¥å­ä¸­ç¼ºå¤±çš„å•è¯ï¼ˆå³é‚£äº›è¢«äººç±»æ“æ§è€…éšæœºç§»é™¤çš„å•è¯ï¼‰ï¼Œä½¿ç”¨ä»å‡ ä¹æ•´ä¸ªäº’è”ç½‘æŠ“å–çš„æ–‡æœ¬ã€‚è¿™ä½¿å¾—æ¨¡å‹å¯¹ä¸åŒä¸Šä¸‹æ–‡ä¸­æœ€å¸¸å‡ºç°çš„å•è¯æœ‰ä¸€ä¸ªç›¸å½“å¥½çš„äº†è§£ï¼ˆä»ç»Ÿè®¡å­¦è§’åº¦è®²ï¼Œå½“ç„¶ä¸æ˜¯è®¤çŸ¥ä¸Šçš„ï¼‰ã€‚ä¹‹åï¼Œæ¨¡å‹ä¼šåœ¨å®é™…ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä¾‹å¦‚é—®ç­”ä»»åŠ¡ï¼Œä½¿ç”¨æ›´å°ä¸”æ›´å…·ä½“çš„æ•°æ®é›†ã€‚ä»åŸå§‹ä»»åŠ¡ä¸­è·å¾—çš„ä¸€èˆ¬â€˜çŸ¥è¯†â€™å¾€å¾€èƒ½æå‡ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ï¼›æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€˜è¿ç§»å­¦ä¹ â€™ã€‚
- en: Transfer learning from pre-training helps deal with many, but not necessarily
    all, of the local minima that may exist in the problem landscape. Thus, Guided
    Transfer Learning complements pre-training, by helping scout ahead for new problem
    areas to avoid. The two methods donâ€™t have to be combined, but when they are,
    the gains can be dramatic.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ç§»å­¦ä¹ é€šè¿‡é¢„è®­ç»ƒæœ‰åŠ©äºåº”å¯¹è®¸å¤šï¼Œä½†ä¸ä¸€å®šå…¨éƒ¨ï¼Œå¯èƒ½å­˜åœ¨çš„å±€éƒ¨æœ€å°å€¼ã€‚å› æ­¤ï¼Œå¼•å¯¼è¿ç§»å­¦ä¹ é€šè¿‡å¸®åŠ©æ¢ç´¢æ–°çš„é—®é¢˜é¢†åŸŸä»¥é¿å…æ¥è¡¥å……é¢„è®­ç»ƒã€‚è¿™ä¸¤ç§æ–¹æ³•ä¸ä¸€å®šéœ€è¦ç»“åˆï¼Œä½†å½“å®ƒä»¬ç»“åˆæ—¶ï¼Œæ•ˆæœå¯èƒ½ä¼šéå¸¸æ˜¾è‘—ã€‚
- en: Sounds Great, But is it Effective?
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¬èµ·æ¥ä¸é”™ï¼Œä½†æœ‰æ•ˆå—ï¼Ÿ
- en: The paper first demonstrates the benefits of Guided Transfer Learning for â€˜**one-shot
    learning**â€™. This is the task of training a model to do one thing, such as classify
    images into different categories, then showing it a single example of a new class
    and expecting it to be able to correctly classify more examples of that class
    thereafter. The authors found that using GTL in addition to pre-training increased
    performance consistently, but that the guidance matrices quickly reached a maximum
    degree of â€˜helpfulnessâ€™, after which adding more scouts or giving them more data
    didnâ€™t help. On the plus side, this also meant that reducing the number of scouts
    or the size of their training data didnâ€™t hurt performance much.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡é¦–å…ˆå±•ç¤ºäº†æŒ‡å¯¼è½¬ç§»å­¦ä¹ åœ¨â€˜**ä¸€æ¬¡æ€§å­¦ä¹ **â€™ä¸­çš„å¥½å¤„ã€‚è¿™æ˜¯å°†ä¸€ä¸ªæ¨¡å‹è®­ç»ƒæˆæ‰§è¡Œä¸€é¡¹ä»»åŠ¡ï¼Œä¾‹å¦‚å°†å›¾åƒåˆ†ç±»ä¸ºä¸åŒç±»åˆ«ï¼Œç„¶åå‘å…¶å±•ç¤ºä¸€ä¸ªæ–°ç±»åˆ«çš„å•ä¸€ç¤ºä¾‹ï¼Œå¹¶æœŸæœ›å®ƒèƒ½å¤Ÿæ­£ç¡®åˆ†ç±»æ›´å¤šè¯¥ç±»åˆ«çš„ç¤ºä¾‹ã€‚ä½œè€…å‘ç°ï¼Œä½¿ç”¨GTLä½œä¸ºé¢„è®­ç»ƒçš„è¡¥å……å¯ä»¥ä¸€è‡´åœ°æé«˜æ€§èƒ½ï¼Œä½†æŒ‡å¯¼çŸ©é˜µå¾ˆå¿«è¾¾åˆ°äº†â€˜å¸®åŠ©â€™çš„æœ€å¤§ç¨‹åº¦ï¼Œæ­¤åæ·»åŠ æ›´å¤šçš„ä¾¦å¯Ÿè€…æˆ–æä¾›æ›´å¤šçš„æ•°æ®å¹¶æ²¡æœ‰å¸®åŠ©ã€‚å¥½çš„ä¸€é¢æ˜¯ï¼Œè¿™ä¹Ÿæ„å‘³ç€å‡å°‘ä¾¦å¯Ÿè€…çš„æ•°é‡æˆ–è®­ç»ƒæ•°æ®çš„è§„æ¨¡å¯¹æ€§èƒ½å½±å“ä¸å¤§ã€‚
- en: 'The implication is that GTL is a low cost, somewhat helpful technique, particularly
    in cases of limited data (where one often turns to one- and few-shot learning).
    And itâ€™s early days yet: the authors only tried using single guidance matrices
    so far but suggest that other implementations might be useful, such as creating
    different matrices depending on how far the scout has moved from its starting
    point. This makes intuitive sense to me: the main model already has information
    about its immediate surroundings and we want the scouts to help see whatâ€™s further
    up ahead, so if we made multiple matrices and placed greater importance on those
    which are further from the starting point, this might help the algorithm â€˜chooseâ€™
    the best next step to make.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€GTLæ˜¯ä¸€ç§ä½æˆæœ¬ã€ç¨å¾®æœ‰ç”¨çš„æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼ˆé€šå¸¸ä¼šä½¿ç”¨ä¸€æ¬¡æ€§å­¦ä¹ æˆ–å°‘é‡å­¦ä¹ ï¼‰ã€‚è€Œä¸”ç›®å‰è¿˜å¤„äºåˆæœŸé˜¶æ®µï¼šä½œè€…ç›®å‰åªå°è¯•äº†ä½¿ç”¨å•ä¸€æŒ‡å¯¼çŸ©é˜µï¼Œä½†å»ºè®®å…¶ä»–å®ç°å¯èƒ½ä¼šæœ‰ç”¨ï¼Œæ¯”å¦‚æ ¹æ®ä¾¦å¯Ÿè€…ä»èµ·ç‚¹ç§»åŠ¨çš„è·ç¦»æ¥åˆ›å»ºä¸åŒçš„çŸ©é˜µã€‚è¿™åœ¨ç›´è§‚ä¸Šæ˜¯æœ‰æ„ä¹‰çš„ï¼šä¸»è¦æ¨¡å‹å·²ç»æœ‰å…³äºå…¶å‘¨å›´ç¯å¢ƒçš„ä¿¡æ¯ï¼Œæˆ‘ä»¬å¸Œæœ›ä¾¦å¯Ÿè€…å¸®åŠ©çœ‹åˆ°æ›´è¿œçš„åœ°æ–¹ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬åˆ¶ä½œå¤šä¸ªçŸ©é˜µï¼Œå¹¶å¯¹é‚£äº›è·ç¦»èµ·ç‚¹æ›´è¿œçš„çŸ©é˜µèµ‹äºˆæ›´å¤§çš„é‡è¦æ€§ï¼Œè¿™å¯èƒ½ä¼šå¸®åŠ©ç®—æ³•â€˜é€‰æ‹©â€™æœ€ä½³çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚
- en: '![](../Images/97a68cd149c74fb88bd5dc2b43951da3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97a68cd149c74fb88bd5dc2b43951da3.png)'
- en: 'Reproduced with the authorsâ€™ permission: A) Example one-short learning task:
    One example is given for training, then other examples of the same characters
    must be found. B) An example of classification performance using pre-training
    and adding GTL.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç»ä½œè€…è®¸å¯è½¬è½½ï¼šA) ç¤ºä¾‹ä¸€æ¬¡æ€§å­¦ä¹ ä»»åŠ¡ï¼šç»™å‡ºä¸€ä¸ªç¤ºä¾‹è¿›è¡Œè®­ç»ƒï¼Œç„¶åå¿…é¡»æ‰¾åˆ°å…¶ä»–ç›¸åŒå­—ç¬¦çš„ç¤ºä¾‹ã€‚B) ä½¿ç”¨é¢„è®­ç»ƒå’Œæ·»åŠ GTLçš„åˆ†ç±»æ€§èƒ½ç¤ºä¾‹ã€‚
- en: The second experiment dealt with the â€˜XOR problem,â€™ AKA â€˜exclusive OR.â€™ The
    goal is to learn a function that maps two inputs (x1 and x2) to an output (y),
    such that y = 1 (or â€˜Trueâ€™) if x1 and x2 are different, and y = 0 (or â€˜Falseâ€™)
    otherwise. Large models using gradient descent often stagnate on this challenge,
    so NikoliÄ‡, AndriÄ‡ and NikoliÄ‡ applied GTL *without pre-training* to help **avoid**
    these **local minima**. This proved so effective that no stagnation could be observed.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªå®éªŒæ¶‰åŠâ€˜XORé—®é¢˜â€™ï¼Œä¹Ÿå«â€˜æ’ä»–æ€§æˆ–â€™ã€‚ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªå°†ä¸¤ä¸ªè¾“å…¥ï¼ˆx1å’Œx2ï¼‰æ˜ å°„åˆ°ä¸€ä¸ªè¾“å‡ºï¼ˆyï¼‰çš„å‡½æ•°ï¼Œä½¿å¾—å½“x1å’Œx2ä¸åŒæ—¶ï¼Œy = 1ï¼ˆæˆ–â€˜çœŸâ€™ï¼‰ï¼Œå¦åˆ™y
    = 0ï¼ˆæˆ–â€˜å‡â€™ï¼‰ã€‚ä½¿ç”¨æ¢¯åº¦ä¸‹é™çš„å¤§å‹æ¨¡å‹é€šå¸¸ä¼šåœ¨è¿™ä¸€æŒ‘æˆ˜ä¸­åœæ»ä¸å‰ï¼Œå› æ­¤NikoliÄ‡ã€AndriÄ‡å’ŒNikoliÄ‡åº”ç”¨äº†*æœªç»è¿‡é¢„è®­ç»ƒ*çš„GTLæ¥å¸®åŠ©**é¿å…**è¿™äº›**å±€éƒ¨æœ€å°å€¼**ã€‚è¿™ä¸€æ–¹æ³•éå¸¸æœ‰æ•ˆï¼Œä»¥è‡³äºæ²¡æœ‰è§‚å¯Ÿåˆ°åœæ»ç°è±¡ã€‚
- en: 'The final experiment addressed the issue of â€˜catastrophic forgettingâ€™: where
    neural networks trained on new tasks are so influenced by the new data that all
    their learned weights get massively updated, causing them to â€˜forgetâ€™ what they
    learned before. NikoliÄ‡ et al. performed a series of model re-trainings: each
    additional training step lasted a fixed number of epochs and was based on a single
    additional data point; each data point was a new example of the categories already
    learned. That is, the data points were learned sequentially, not in a batch as
    is typically done. In a private conversation, Danko likened this to real life:
    there exists a category of object called â€˜carâ€™, and today you are driving in a
    particular kind of one, like an SUV. Just because youâ€™re now being exposed to
    SUV specific features, it doesnâ€™t mean youâ€™ll forget all the other car types that
    you were exposed to in the past. Your new exposure should increase your understanding
    of what a car is like, not reduce it.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åçš„å®éªŒè§£å†³äº†â€œç¾éš¾æ€§é—å¿˜â€çš„é—®é¢˜ï¼šå³ç¥ç»ç½‘ç»œåœ¨æ–°ä»»åŠ¡ä¸Šè®­ç»ƒæ—¶ï¼Œæ–°æ•°æ®çš„å½±å“ä½¿å¾—æ‰€æœ‰å·²å­¦ä¹ çš„æƒé‡å¤§å¹…æ›´æ–°ï¼Œä»è€Œâ€œå¿˜è®°â€äº†ä¹‹å‰å­¦åˆ°çš„å†…å®¹ã€‚NikoliÄ‡ç­‰äººè¿›è¡Œäº†ä¸€ç³»åˆ—æ¨¡å‹é‡è®­ç»ƒï¼šæ¯ä¸€æ­¥é¢å¤–çš„è®­ç»ƒæŒç»­å›ºå®šæ•°é‡çš„çºªå…ƒï¼Œå¹¶åŸºäºä¸€ä¸ªé¢å¤–çš„æ•°æ®ç‚¹ï¼›æ¯ä¸ªæ•°æ®ç‚¹éƒ½æ˜¯å·²ç»å­¦ä¹ è¿‡çš„ç±»åˆ«çš„æ–°ç¤ºä¾‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™äº›æ•°æ®ç‚¹æ˜¯é¡ºåºå­¦ä¹ çš„ï¼Œè€Œä¸æ˜¯é€šå¸¸çš„æ‰¹é‡å­¦ä¹ ã€‚åœ¨ä¸€æ¬¡ç§äººè°ˆè¯ä¸­ï¼ŒDankoå°†å…¶æ¯”ä½œç°å®ç”Ÿæ´»ï¼šå­˜åœ¨ä¸€ç§ç§°ä¸ºâ€œæ±½è½¦â€çš„ç‰©ä½“ç±»åˆ«ï¼Œè€Œä»Šå¤©ä½ æ­£åœ¨é©¾é©¶ä¸€ç§ç‰¹å®šç±»å‹çš„æ±½è½¦ï¼Œæ¯”å¦‚SUVã€‚å³ä½¿ä½ ç°åœ¨æ¥è§¦åˆ°SUVçš„ç‰¹æ€§ï¼Œä¹Ÿä¸æ„å‘³ç€ä½ ä¼šå¿˜è®°ä»¥å‰æ¥è§¦è¿‡çš„æ‰€æœ‰å…¶ä»–æ±½è½¦ç±»å‹ã€‚ä½ çš„æ–°æ¥è§¦åº”è¯¥å¢åŠ ä½ å¯¹æ±½è½¦çš„ç†è§£ï¼Œè€Œä¸æ˜¯å‡å°‘å®ƒã€‚
- en: 'While classical transfer learning from pre-training alone would not be able
    to benefit from these sequential additions of data points, the addition of GTL
    caused a stepwise performance boost, indicating that the **knowledge added up**.
    In other words, the models were more robust to forgetting the previous examples,
    although it did occur eventually. The authors explained this in common sense terms:
    in a given problem space, there will be some solutions that are appropriate both
    for examples learned now, *and* examples learned before; these solutions probably
    lie close to the modelâ€™s starting point as it begins its training on the new example;
    without GTL, the model can â€˜wander awayâ€™; *with* GTL, the guidance matrices encourage
    the model to remain in that useful vicinity. Itâ€™s like a scout reminding you not
    to stray too far from the camp.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ä»…ä»…ä¾é ç»å…¸çš„é¢„è®­ç»ƒè½¬ç§»å­¦ä¹ æ— æ³•ä»è¿™äº›é¡ºåºæ·»åŠ çš„æ•°æ®ç‚¹ä¸­å—ç›Šï¼Œä½†æ·»åŠ GTLå´å¸¦æ¥äº†é€æ­¥çš„æ€§èƒ½æå‡ï¼Œè¿™è¡¨æ˜**çŸ¥è¯†å¾—åˆ°äº†ç§¯ç´¯**ã€‚æ¢å¥è¯è¯´ï¼Œæ¨¡å‹å¯¹é—å¿˜ä¹‹å‰çš„ç¤ºä¾‹æ›´å…·é²æ£’æ€§ï¼Œå°½ç®¡æœ€ç»ˆè¿˜æ˜¯ä¼šå‘ç”Ÿã€‚ä½œè€…ç”¨å¸¸è¯†è§£é‡Šäº†è¿™ä¸€ç‚¹ï¼šåœ¨ç‰¹å®šçš„é—®é¢˜ç©ºé—´ä¸­ï¼Œä¼šæœ‰ä¸€äº›è§£å†³æ–¹æ¡ˆæ—¢é€‚ç”¨äºç°åœ¨å­¦ä¹ çš„ç¤ºä¾‹ï¼Œ*ä¹Ÿ*é€‚ç”¨äºä¹‹å‰å­¦ä¹ çš„ç¤ºä¾‹ï¼›è¿™äº›è§£å†³æ–¹æ¡ˆå¯èƒ½æ¥è¿‘äºæ¨¡å‹å¼€å§‹åœ¨æ–°ç¤ºä¾‹ä¸Šè®­ç»ƒæ—¶çš„èµ·å§‹ç‚¹ï¼›æ²¡æœ‰GTLï¼Œæ¨¡å‹å¯èƒ½ä¼šâ€œåç¦»â€ï¼›*æœ‰äº†*GTLï¼ŒæŒ‡å¯¼çŸ©é˜µé¼“åŠ±æ¨¡å‹ä¿æŒåœ¨é‚£ä¸ªæœ‰ç”¨çš„é‚»åŸŸã€‚è¿™å°±åƒä¸€ä¸ªä¾¦å¯Ÿå‘˜æé†’ä½ ä¸è¦ç¦»è¥åœ°å¤ªè¿œã€‚
- en: So What Does This Mean in Practice?
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‚£è¿™åœ¨å®è·µä¸­æ„å‘³ç€ä»€ä¹ˆï¼Ÿ
- en: Guided Transfer Learning is a method of â€˜learning to learnâ€™, and itâ€™s not the
    only one out there. Other approaches exist and maybe even produce better results,
    but they are often unscalable, which can defeat their purpose. GTL is computationally
    cheap, and flexible, which may make it particularly appropriate for those models
    that have already squeezed everything they can get out of their available resources.
    Large Language Models, and computer vision models which often exist under tough
    hardware constraints, like being embedded in autonomous vehicles, are a couple
    of good examples.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‡å¯¼è½¬ç§»å­¦ä¹ æ˜¯ä¸€ç§â€œå­¦ä¹ å¦‚ä½•å­¦ä¹ â€çš„æ–¹æ³•ï¼Œå¹¶ä¸æ˜¯å”¯ä¸€çš„æ–¹æ³•ã€‚å…¶ä»–æ–¹æ³•ä¹Ÿå­˜åœ¨ï¼Œå¹¶ä¸”å¯èƒ½ä¼šäº§ç”Ÿæ›´å¥½çš„ç»“æœï¼Œä½†å®ƒä»¬å¾€å¾€ä¸å¯æ‰©å±•ï¼Œè¿™å¯èƒ½ä¼šä½¿å®ƒä»¬å¤±å»æ„ä¹‰ã€‚GTLè®¡ç®—ä¾¿å®œä¸”çµæ´»ï¼Œè¿™å¯èƒ½ä½¿å®ƒç‰¹åˆ«é€‚ç”¨äºé‚£äº›å·²ç»å°†æ‰€æœ‰å¯ç”¨èµ„æºæ¦¨å–å‡ºæ¥çš„æ¨¡å‹ã€‚å¤§å‹è¯­è¨€æ¨¡å‹å’Œè®¡ç®—æœºè§†è§‰æ¨¡å‹ï¼ˆè¿™äº›æ¨¡å‹ç»å¸¸åœ¨ä¸¥æ ¼çš„ç¡¬ä»¶é™åˆ¶ä¸‹å­˜åœ¨ï¼Œå¦‚åµŒå…¥åœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­ï¼‰æ˜¯å‡ ä¸ªå¥½çš„ä¾‹å­ã€‚
- en: On the other hand, if data and compute power are available in abundance, then
    the authors admit that GTL might not be needed. Itâ€™s not a panacea, as they say,
    but then no single machine learning solution is. And as the experiments showed,
    it still has promise for those hard problems which even well-resourced models
    struggle with.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæ•°æ®å’Œè®¡ç®—èƒ½åŠ›éå¸¸å……è¶³ï¼Œé‚£ä¹ˆä½œè€…æ‰¿è®¤GTLå¯èƒ½å°±ä¸å†éœ€è¦äº†ã€‚æ­£å¦‚ä»–ä»¬æ‰€è¯´ï¼Œå®ƒä¸æ˜¯ä¸‡èƒ½çš„ï¼Œä½†æ²¡æœ‰ä»»ä½•å•ä¸€çš„æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆæ˜¯ä¸‡èƒ½çš„ã€‚æ­£å¦‚å®éªŒæ‰€ç¤ºï¼Œå®ƒå¯¹é‚£äº›å³ä½¿æ˜¯èµ„æºå……è¶³çš„æ¨¡å‹ä¹Ÿéš¾ä»¥è§£å†³çš„éš¾é¢˜ä»ç„¶å……æ»¡äº†å¸Œæœ›ã€‚
- en: With all the possible guidance matrix paradigms the authors plan to experiment
    with, Iâ€™m optimistic. So, watch this space.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…è®¡åˆ’å°è¯•æ‰€æœ‰å¯èƒ½çš„æŒ‡å¯¼çŸ©é˜µèŒƒå¼ï¼Œæˆ‘å¯¹æ­¤æ„Ÿåˆ°ä¹è§‚ã€‚æ‰€ä»¥ï¼Œè¯·å…³æ³¨è¿™ä¸ªé¢†åŸŸã€‚
