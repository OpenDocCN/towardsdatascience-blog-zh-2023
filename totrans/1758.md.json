["```py\n# Installing libraries\ninstall.packages('tidyverse')\ninstall.packages('tidytext')\n\n# Loading libraries\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(textdata)\n```", "```py\ntext <- \"R is a programming language for statistical computing and graphics\nsupported by the R Core Team and the R Foundation for Statistical Computing.\nCreated by statisticians Ross Ihaka and Robert Gentleman, R is used among data\n miners, bioinformaticians and statisticians for data analysis and developing \nstatistical software. Users have created packages to augment the functions\nof the R language.\nAccording to user surveys and studies of scholarly literature databases, \nR is one of the most commonly used programming languages in data mining.[8]\nAs of December 2022, R ranks 11th in the TIOBE index, a measure of programming\nlanguage popularity, in which the language peaked in 8th place in August 2020.\nThe official R software environment is an open-source free software \nenvironment within the GNU package, available under the GNU General Public \nLicense.  It is written primarily in C, Fortran, and R itself \n(partially self-hosting). \nPrecompiled executables are provided for various operating systems. R has a \ncommand line interface.[11] Multiple third-party graphical user interfaces are \nalso available, such as RStudio, an integrated development environment, \nand Jupyter, a notebook interface.\"\n```", "```py\n# Transform to tibble\ndf_text <- tibble(text)\n```", "```py\n# Tokenizing the text\ntokens <- df_text %>% \n  unnest_tokens(input = text, #name of the input column\n                output = word) #name of the output column\n```", "```py\n# View stop_words\nstop_words\n\n# A tibble: 1,149 × 2\n   word        lexicon\n   <chr>       <chr>  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n# … with 1,139 more rows\n```", "```py\n# Removing stopwords and counting frequencies\ntokens %>% \n  anti_join(stop_words) %>% \n  count(word, sort = TRUE)\n\n# Result\n# A tibble: 79 × 2\n   word            n\n   <chr>       <int>\n 1 language        4\n 2 data            3\n 3 environment     3\n 4 programming     3\n 5 software        3\n 6 statistical     3\n 7 computing       2\n 8 created         2\n 9 gnu             2\n10 interface       2\n# … with 69 more rows\n```", "```py\ntext_freq_counter <- function(text){\n\n  # Transform to tibble\n  df_text <- tibble(text)\n\n  # Tokenizing the text\n  tokens <- df_text %>% \n    unnest_tokens(input = text, #name of the input column\n                  output = word) #name of the output column\n\n  # Removing stopwords and counting frequencies\n  freq_count <- tokens %>% \n    anti_join(stop_words) %>% \n    count(word, sort = TRUE)\n\n  # Return\n  return(freq_count)\n\n}#close function\n```", "```py\ntext <- \"Text everywhere! Since the Internet was spread around the world, \nthe amount of textual data we generate everyday is ginormous. Only textual \nmessages sent everyday, it is estimated that there are around 18 Billion of\n them circulating on a daily basis*. \nNow imagine the amount of news generated as well. It's a so overwhelming \namount that there are whole businesses built around news clipping, separating \nthe best information about a given topic to help companies in their marketing\n strategies.\nHow is AI helping that? Certainly, NLP plays a huge part on that providing \ngood tools and algorithms to analyze textual information. As Data Scientists, \nwe can profit of tidytext, an excellent library from R to help us building \nquick analytical tools to check the content of a text.\nLet's see that in practice, next.\"\n\n# Running the function\ntext_freq_counter(text)\n\n[OUT]\n# A tibble: 50 × 2\n   word            n\n   <chr>       <int>\n 1 amount          3\n 2 textual         3\n 3 data            2\n 4 everyday        2\n 5 information     2\n 6 news            2\n 7 text            2\n 8 tools           2\n 9 18              1\n10 ai              1\n# … with 40 more rows\n```", "```py\n# Bing sentiments\nget_sentiments('bing')\n# A tibble: 6,786 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# … with 6,776 more rows\n```", "```py\nlibrary(textdata)\n\n# Sentiments Afinn\nget_sentiments('afinn')\n\n# A tibble: 2,477 × 2\n   word       value\n   <chr>      <dbl>\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# … with 2,467 more rows\n```", "```py\n# Sentiments Afinn\nget_sentiments('nrc')\n\n# A tibble: 13,875 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# … with 13,865 more rows\n```", "```py\n# Function for frequency count\ntext_freq_counter <- function(text){\n\n  # get sentiments\n  sentiments <- get_sentiments(‘afinn’)\n\n  # Transform to tibble\n  df_text <- tibble(text)\n\n  # Tokenizing the text\n  tokens <- df_text %>% \n    unnest_tokens(input = text, #name of the input column\n                  output = word) #name of the output column\n\n  # Removing stopwords and counting frequencies\n  freq_count <- tokens %>% #dataset\n    inner_join(sentiments, by=’word’) %>% #join the sentiments\n    count(word, value, sort = TRUE) %>% #count the words by sentiment value\n    mutate(score = n * value) %>%  # create score by multiplying score * value\n    arrange( desc(score)) # sort\n\n  # Plot\n  g <- freq_count %>% \n    ggplot( aes(x= score, y= reorder(word, score),\n                fill= score > 0) ) +\n    geom_col(show.legend = F) +\n    labs( x= ‘Sentiment Score’,\n          y= ‘WORD’,\n          subtitle = ‘Negative versus positive sentiments’) +\n    ggtitle(‘Sentiment Score by Word in the Text’)+\n    theme(plot.subtitle =  element_text(color = \"gray\", face = \"italic\")) +\n    theme_light()\n\n  # Return\n  return(list(freq_count, g))\n\n}#close function\n\n#Applying the function\ntext_freq_counter(text3)\n\n# Resulting table\n# A tibble: 16 × 4\n   word      value     n score\n   <chr>     <dbl> <int> <dbl>\n 1 care          2     2     4\n 2 best          3     1     3\n 3 feeling       1     2     2\n 4 hopes         2     1     2\n 5 robust        2     1     2\n 6 save          2     1     2\n 7 true          2     1     2\n 8 cool          1     1     1\n 9 fitness       1     1     1\n10 shared        1     1     1\n11 cutting      -1     2    -2\n12 recession    -2     1    -2\n13 cut          -1     3    -3\n14 losing       -3     1    -3\n15 lost         -3     1    -3\n16 cuts         -1     7    -7\n```", "```py\n# Enhanced Function\ntext_freq_sentiment(text3, .sentiment = 'nrc')\ntext_freq_sentiment(text3, .sentiment = 'bing')\n```"]