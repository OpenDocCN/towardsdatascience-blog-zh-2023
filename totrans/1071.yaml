- en: How Exactly Does a Decision Tree Solve a Regression Problem?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-exactly-does-a-decision-tree-solve-a-regression-problem-fbb908cf548b](https://towardsdatascience.com/how-exactly-does-a-decision-tree-solve-a-regression-problem-fbb908cf548b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Build your own decision tree regressor (from scratch in Python) and uncover
    what’s under the hood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gurjinderkaur95?source=post_page-----fbb908cf548b--------------------------------)[![Gurjinder
    Kaur](../Images/d5c6746466025dad06077b1a89a789d1.png)](https://medium.com/@gurjinderkaur95?source=post_page-----fbb908cf548b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fbb908cf548b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fbb908cf548b--------------------------------)
    [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----fbb908cf548b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fbb908cf548b--------------------------------)
    ·12 min read·Nov 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79b7360f9005933ba478d9db039fa1bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Chris Lawton](https://unsplash.com/@chrislawton?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I’m going to demonstrate through a simple example, flowchart,
    and code — the entire logic implemented under the hood of a decision tree regressor
    (*aka regression tree*). After reading it, you will be able to get a clear idea
    behind the working of regression trees, and you will be more thoughtful & confident
    in using and tuning them in your next regression challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An awesome introduction to decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a toy data set for training our regression tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outlining the logic of the regression tree in the form of a flowchart
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Referencing the flowchart to write the code using NumPy and Pandas and make
    the first split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the decision tree after our first split, using Plotly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalizing the code using recursion to build the entire regression tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use scikit-learn to perform the same task and compare the results (*spoiler:
    you’ll be so proud that you produced the same output as scikit-learn did!)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are machine learning algorithms that can be used to solve both
    classification as well as regression problems. Even though classification and
    regression are inherently different from each other, decision trees try to approach
    both of these problems in an elegant way where the ultimate goal is to find the
    best split at a given node. And, how that best split is determined is what makes
    a classification tree and a regression tree different from each other.
  prefs: []
  type: TYPE_NORMAL
- en: In my previous [article](https://medium.com/towards-data-science/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06),
    I touched upon the basics of how a decision tree solves a classification problem.
    I used a two-class dataset to demonstrate a step-by-step process to understand
    how a decision rule is generated at each node using data impurity measures such
    as entropy, and then later implemented a recursive algorithm in Python to output
    the final decision tree. Not sure if you should add [this](https://medium.com/towards-data-science/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06)
    article to your reading list? Let’s use a decision tree to find out!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c44acf7789690e9b12a59ea1bd77e01e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: '***Important Note:*** *This was just an example to show what a decision tree
    is, AND you’re awesome no matter if it says that or not.*'
  prefs: []
  type: TYPE_NORMAL
- en: As shown above, a decision tree classifier aims to predict discrete labels (or
    *classes)* which in our case are *You R Awesome!* and *Go READ IT!.* In such cases,
    the decision tree looks at the probability distribution of the classes at each
    split to compute measures such as entropy and hence decides the best feature and
    value to split on.
  prefs: []
  type: TYPE_NORMAL
- en: However, in regression problems, the target variable is *continuous* and we
    cannot use entropy (or Gini index) as the split criterion in such cases. So, regression
    trees make use of ***Mean Squared Error (MSE)***and choose the feature and value
    that leads to the minimum MSE at each node.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean Squared Error:** It is defined as the sum of squared differences between
    the true values and predicted values.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/4d67e7de1cb32e4d1409d5cbb854b78a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: Building a Regression Tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To demonstrate how a regression tree is learned, I’m going to use NumPy to generate
    a toy dataset resembling a quadratic function that will be used as the training
    data. Feel free to pause and bring up a new Python notebook on the side to code
    along as you read.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s import the libraries first.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Generating the dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following code, we’re going to generate the training data where our target
    i.e. **dependent variable** ***y*** is a quadratic function of **independent variable
    *X.*** For the sake of simplicity, I’m considering a single feature *X*, but it
    can be extended easily with multiple features (both continuous and categorical
    features).
  prefs: []
  type: TYPE_NORMAL
- en: Here, both *X* and *y* are continuous (*y has to be continuous since it’s regression
    and X could be either categorical or continuous*).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Visualizing the data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have created 100 data points. Let’s use Plotly to visualize our data as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`Cell Output:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e829f0f1a028676fd00daba18b133d1f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ready to make our first split?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before doing that, let’s first outline the steps a regression tree takes and
    then we will refer to these steps to write our code. **Spending some time studying
    the flowchart below is all we need to do to understand the underlying logic of
    a regression tree to find the optimal split at a given node.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d810e13009fea923553e4eea3acd1754.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Flowchart demonstrating regression tree logic (Source: Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Once the above logic is clear, we are all set to witness the regression tree
    make its first optimal split using the following code.
  prefs: []
  type: TYPE_NORMAL
- en: We need to first define a helper function to calculate MSE.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The following code is the core logic as explained in the flowchart above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`Cell Output:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7fcc643de9d77121f7696dd7e7e18067.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Yayy! We got our first optimal split at `X=6.869`. Let’s visualize it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`Cell Output:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad157b8c585324b34255658b9c5ffd20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interpretation of the above result:**'
  prefs: []
  type: TYPE_NORMAL
- en: '`X<=6.869` corresponds to the split value. It means that the data at the current
    node will get subdivided into two parts — one where `X<=6.869` and the other where
    `X>6.869`. That’s what the black vertical line represents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`samples=100` corresponds to the total number of data points at the current
    node. Initially, the root node consists of all data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MSE=1381.82` corresponds to the value of the mean squared error at this current
    node. We know that MSE needs true values and predicted values for its calculation;
    we already have true values, what are the predicted values you might be wondering,
    right? That’s `value`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value=37.605` corresponds to the prediction at the current node. ***The prediction
    at any node is the average of all true values belonging to the data at the given
    node.***Initially, prediction at the root node will simply be the average of *y*
    values of all 100 rows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The horizontal gray lines on both sides of the split represent the predictions
    for the left partition and right partition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Optional exercise: Copy the two cells we used above and replace the original
    df with either of the left partition or right partition data frames. You can see
    how subsequent splits are being made for them in the exact same manner. Just the
    input data frame changes and everything else (the core logic) remains the same.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is how the current state of our regression tree looks like after
    repeating the code for the left and right branches:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d62bca613bb2a02fc49dc5cd82399ce3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: 'I hope it makes it easier now to interpret a regression tree that looks like
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66be38215d1db309390ee7675456e1a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: The above process gets repeated for the subsequent partitions until a stopping
    criterion is met (*such as max depth reached, no further splitting possible, etc.*).
  prefs: []
  type: TYPE_NORMAL
- en: Generalize the code using recursion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s wrap the above code inside a function `build_tree` that we can call recursively
    to build the regression tree.
  prefs: []
  type: TYPE_NORMAL
- en: '**Helper functions:**'
  prefs: []
  type: TYPE_NORMAL
- en: '`mean_squared_error:` Returns the mean squared error for the given actual values
    and predicted values. Defined above.'
  prefs: []
  type: TYPE_NORMAL
- en: '`get_best_params:` Returns the *best_params* dictionary that consists of the
    feature and value to use for splitting the data at the current node as well as
    contains the prediction for the current node.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`build_tree:` It is the main driver function that utilizes helper functions
    to build the regression tree recursively. I have added parameters such as *max_depth*
    so that the growth of the regression tree can be controlled to avoid *overfitting,*
    and also to serve as a stopping criterion.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s call the above function on our training data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`Cell Output:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/040e716f15379f1b7c9b4cfd6900ff83.png)'
  prefs: []
  type: TYPE_IMG
- en: The above output corresponds to the regression tree given `max_depth=3`. For
    more clarity, refer to the following diagram of our regression tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e105d294cee92b19d44e14666843a86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Final regression tree (Source: Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The leaf nodes in the above regression tree correspond to the prediction values.
    On plotting the predictions and decision boundaries as per our final regression
    tree, we will get something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a31de8df23e5853c4bfd77269cfc5792.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Final predictions and decision boundaries (Source: Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Link to Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can find the full notebook [here](https://github.com/gurjinderbassi/Machine-Learning/blob/main/Regression%20Trees%20from%20scratch.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: The moment of truth — Implementing regression tree using scikit-learn and comparing
    the final tree with ours
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we need to know whether our understanding of the working of regression trees
    is accurate or not. To do that, let’s use scikit-learn to train a regression tree
    on the same data and look at the final results as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`Cell Output:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9916d94eb682b1221740c708c618dc4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: The regression tree returned by scikit-learn’s DecisionTreeRegressor is the
    exact same as the one we created previously.
  prefs: []
  type: TYPE_NORMAL
- en: Not a mystery anymore!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope that staying so far in this article has been valuable for you and hopefully
    this one line of code i.e., `regressor.fit(X, y)` is no longer a mystery. Since
    we are now well aware of how the underlying algorithm has been fabricated, we
    can be more thoughtful while tweaking our tree-based regression models in the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: This is not the end
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The purpose of this article was to solely demonstrate the logic used by a decision
    tree regressor to make the optimal split at a node. However, it’s not a comprehensive
    guide on decision trees since there are many other important aspects that also
    need to be taken care of while modeling. It will be unfair if we don’t discuss
    them at all, so providing a brief summary of some points here.
  prefs: []
  type: TYPE_NORMAL
- en: '**Advantages of decision trees:**'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are *interpretable* and *intuitive* since one can get a clear
    idea of what led to a specific prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They do not demand much preprocessing of the training data, and can intrinsically
    handle categorical features and missing values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disadvantages of decision trees:**'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are sensitive to small variations in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When not controlled properly, decision trees are highly prone to *overfitting,*
    and suitable measures need to be taken to prevent the tree from overgrowth so
    that they generalize well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we learned to build a regression tree from scratch to thoroughly
    understand the logic behind it. We generated a simple dataset with one continuous
    feature *X* and target *y*, that was used to train the decision tree regressor.
    It was followed by communicating the approach taken by the regression tree at
    each node to determine the best split via a flowchart so that it is easier for
    us to write code with having clear picture of the sequence of steps. We visualized
    the first best split returned by the code and discussed how the same logic can
    be extended further to build the entire tree.
  prefs: []
  type: TYPE_NORMAL
- en: We also trained a decision tree regressor using scikit-learn on the same data
    and noticed that it produced the same results as we did previously from scratch.
    The goal of this article was to look at what exactly is going on in the backend
    when we call `.fit()` on our data to train a DecisionTreeRegressor model from
    scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, having this knowledge is going to help us better tweak, tune, and
    interpret our tree-based regression models in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading :)
  prefs: []
  type: TYPE_NORMAL
- en: Open to any feedback or suggestions!
  prefs: []
  type: TYPE_NORMAL
- en: '**Want to learn more about decision trees?** Take a look at the following articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06?source=post_page-----fbb908cf548b--------------------------------)
    [## How does a decision tree know the next best question to ask from the data?'
  prefs: []
  type: TYPE_NORMAL
- en: Build your own decision tree (from scratch in Python) and understand how it
    uses entropy to split
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06?source=post_page-----fbb908cf548b--------------------------------)
    [](/entropy-and-gini-index-c04b7452efbe?source=post_page-----fbb908cf548b--------------------------------)
    [## Entropy and Gini Index
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how these measures help us quantify uncertainty in a dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/entropy-and-gini-index-c04b7452efbe?source=post_page-----fbb908cf548b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
