- en: Fast String Processing with Polars — Scam Emails Dataset
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Polars进行快速字符串处理——诈骗邮件数据集
- en: 原文：[https://towardsdatascience.com/fast-string-processing-with-polars-scam-emails-dataset-fcf7054a929a](https://towardsdatascience.com/fast-string-processing-with-polars-scam-emails-dataset-fcf7054a929a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fast-string-processing-with-polars-scam-emails-dataset-fcf7054a929a](https://towardsdatascience.com/fast-string-processing-with-polars-scam-emails-dataset-fcf7054a929a)
- en: Clean, process and tokenise texts in milliseconds using in-built Polars string
    expressions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用内置的Polars字符串表达式在毫秒级别清理、处理和标记文本
- en: '[](https://medium.com/@antonsruberts?source=post_page-----fcf7054a929a--------------------------------)[![Antons
    Tocilins-Ruberts](../Images/363a4f32aa793cca7a67dea68e76e3cf.png)](https://medium.com/@antonsruberts?source=post_page-----fcf7054a929a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fcf7054a929a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fcf7054a929a--------------------------------)
    [Antons Tocilins-Ruberts](https://medium.com/@antonsruberts?source=post_page-----fcf7054a929a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@antonsruberts?source=post_page-----fcf7054a929a--------------------------------)[![Antons
    Tocilins-Ruberts](../Images/363a4f32aa793cca7a67dea68e76e3cf.png)](https://medium.com/@antonsruberts?source=post_page-----fcf7054a929a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fcf7054a929a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fcf7054a929a--------------------------------)
    [Antons Tocilins-Ruberts](https://medium.com/@antonsruberts?source=post_page-----fcf7054a929a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fcf7054a929a--------------------------------)
    ·10 min read·May 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----fcf7054a929a--------------------------------)
    ·10分钟阅读·2023年5月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/130cbb064ff780aa7b2bc919defabc8a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/130cbb064ff780aa7b2bc919defabc8a.png)'
- en: Photo by [Stephen Phillips - Hostreviews.co.uk](https://unsplash.com/es/@hostreviews?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Stephen Phillips - Hostreviews.co.uk](https://unsplash.com/es/@hostreviews?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: With the large scale adoption of Large language Models (LLMs) it might seem
    that we’re past the stage where we had to manually clean and process text data.
    Unfortunately, me and other NLP practitioners can attest that this is very much
    not the case. Clean text data is required at every stage of NLP complexity — from
    basic text analytics to machine learning and LLMs. This post will showcase how
    this laborious and tedious process can be significantly sped up using Polars.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）的广泛采用，我们可能会觉得已经不再需要手动清理和处理文本数据。不幸的是，我和其他NLP从业者可以证明，情况并非如此。在每一个NLP复杂性的阶段——从基础文本分析到机器学习和LLMs——都需要干净的文本数据。本文将展示如何使用Polars显著加快这一繁琐和乏味的过程。
- en: Polars
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Polars
- en: '[Polars](https://github.com/pola-rs/polars) is a blazingly fast Data Frame
    library written in Rust that is incredibly efficient with handling strings (due
    to its Arrow backend). Polars stores strings in the `Utf8` format using `Arrow`
    backend which makes string traversal cache-optimal and predictable. Also, it exposes
    a lot of in-built string operations under the `str` namespace which makes the
    string operations parallelised. Both of these factors make working with strings
    extremely easy and fast.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[Polars](https://github.com/pola-rs/polars)是一个极其快速的Rust编写的数据框架库，处理字符串非常高效（得益于其Arrow后端）。Polars以`Utf8`格式存储字符串，并使用`Arrow`后端，使得字符串遍历缓存最优且可预测。此外，它在`str`命名空间下暴露了许多内置的字符串操作，这使得字符串操作可以并行处理。这两个因素使得处理字符串变得极其简单和快速。'
- en: The library shares a lot of syntaxis with Pandas but there are also a lot of
    quirks that you’ll need to get used to. This post will walk you through working
    with strings but for a comprehensive overview I highly recommend this [“Getting
    Started”](https://pola-rs.github.io/polars-book/getting-started/intro/) guide
    as it will give you a good overview of the library.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库与Pandas有很多相似的语法，但也有很多需要适应的细节。本文将引导你了解字符串操作，但为了全面了解，我强烈推荐这个[“入门指南”](https://pola-rs.github.io/polars-book/getting-started/intro/)，它会给你一个很好的库概览。
- en: Setup
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置
- en: You can find all the code in this [GitHub repo](https://github.com/aruberts/tutorials/tree/main/metaflow/fraud_email),
    so make sure to pull it if want to code along (don’t forget to ⭐ it). To make
    this post more practical and fun, I’ll showcase how we can clean a small scam
    email dataset which can be found on [Kaggle](https://www.kaggle.com/datasets/rtatman/fraudulent-email-corpus)
    (License [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)). Polars
    can be installed using pip — `pip install polars` and the recommended Python version
    is `3.10` .
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个[GitHub仓库](https://github.com/aruberts/tutorials/tree/main/metaflow/fraud_email)中找到所有代码，所以如果你想跟着编码，记得拉取它（别忘了⭐）。为了使这篇文章更实用有趣，我将展示如何清理一个小型诈骗电子邮件数据集，该数据集可以在[Kaggle](https://www.kaggle.com/datasets/rtatman/fraudulent-email-corpus)找到（许可证[CC
    BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)）。Polars可以通过pip安装——`pip
    install polars`，推荐的Python版本是`3.10`。
- en: Text Processing Pipeline
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本处理管道
- en: 'The goal of this pipeline is to parse the raw text file into a DataFrame that
    can be used for further analytics/modelling. Here are the overall steps that will
    be implemented:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道的目标是将原始文本文件解析为一个数据框，以便用于进一步的分析/建模。以下是将要实现的整体步骤：
- en: Read in text data
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取文本数据
- en: Extract relevant fields (e.g. sender email, object, text, etc.)
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取相关字段（例如，发件人电子邮件、对象、文本等）
- en: Extract useful features from these fields (e.g. length, % of digits, etc)
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这些字段中提取有用的特征（例如，长度、数字比例等）
- en: Pre-process text for further analysis
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理文本以进行进一步分析
- en: Perform some basic text analytics
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行一些基本的文本分析
- en: Without further ado, let’s begin!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 言归正传，让我们开始吧！
- en: Reading Data
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取数据
- en: 'Assuming that the text file with emails is saved as `fraudulent_emails.txt`
    , here’s the function used to read them in:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设保存了包含电子邮件的文本文件为`fraudulent_emails.txt`，以下是用于读取它们的函数：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you explore the text data you’ll see that the emails have two main sections
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看文本数据，你会发现电子邮件有两个主要部分
- en: Metadata (starts with `From r` ) that contains email sender, subject, etc.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据（以`From r`开头），包含发件人、主题等。
- en: 'Email text (starts after `Status: O` or `Status: RO` )'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '电子邮件文本（从`Status: O`或`Status: RO`后开始）'
- en: I’m using the first pattern to split the continuous text file into a list of
    emails. Overall, we should be able to read in 3977 emails that we put into a Polars
    DataFrame for further analysis.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用第一个模式将连续的文本文件拆分为电子邮件列表。总体而言，我们应该能够读取3977封电子邮件，并将其放入Polars数据框中以进行进一步分析。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Extracting Relevant Fields
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取相关字段
- en: Now the tricky part begins. How do we extract relevant fields from this mess
    of a text data? Unfortunately, the answer is regex.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，棘手的部分开始了。我们如何从这堆混乱的文本数据中提取相关字段？不幸的是，答案是正则表达式。
- en: '**Sender and Subject**'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**发件人和主题**'
- en: Upon further inspection of metadata (below) you can see that it has fields `From:`
    and `Subject:` which are going to be very useful for us.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步检查元数据（如下所示）你会发现它有`From:`和`Subject:`字段，这对我们非常有用。
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you keep scrolling the emails, you’ll find that there are a few formats
    for the `From:` field. The first format you see above where we have both name
    and email. The second format contains only the email e.g. `From: 123@abc.com`
    or `From: “123@abc.com”` . With this in mind, we’ll need three regex patterns
    — one for subject, and two for sender (name with email and just email).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你继续滚动电子邮件，你会发现`From:`字段有几种格式。你看到的第一个格式是包含姓名和电子邮件的格式。第二种格式仅包含电子邮件，例如`From:
    123@abc.com`或`From: “123@abc.com”`。考虑到这一点，我们需要三个正则表达式模式——一个用于主题，两个用于发件人（姓名和电子邮件，以及仅电子邮件）。'
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Polars has an `str.extract` method that can compare the above patterns to our
    text and (you guessed it) extract the matching groups. Here’s how you can apply
    it to the `emails_pl` DataFrame.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Polars有一个`str.extract`方法，可以将上述模式与我们的文本进行比较，并（你猜对了）提取匹配的组。以下是如何将其应用于`emails_pl`数据框。
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see besides `str.extract` we’re also using a `pl.when().then().otherwise()`
    expressions (Polars version of if/else) to account for a second email only pattern.
    If you print out the results you’ll see that in most cases it should’ve worked
    correctly (and incredibly fast). We now have `sender_name` , `sender_email` and
    `subject` fields for our analysis.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，除了`str.extract`之外，我们还使用了`pl.when().then().otherwise()`表达式（Polars的if/else版本）来处理仅存在于第二个电子邮件模式的情况。如果你打印出结果，你会发现大多数情况下它应该正确地工作（而且速度极快）。我们现在有了`sender_name`、`sender_email`和`subject`字段用于分析。
- en: '![](../Images/f82b155fbe85ce3ffc664b639d7ec26b.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f82b155fbe85ce3ffc664b639d7ec26b.png)'
- en: Polars DF sample. Screenshot by author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Polars数据框样本。作者截图。
- en: '**Email Text**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**电子邮件文本**'
- en: 'As was noted above, the actual email text starts after `Status: O` (opened)
    or `Status: RO` (read and opened) which means that we can utilise this pattern
    to split the email into “metadata” and “text” parts. Below you can see the three
    steps that we need to take to extract the required field and the corresponding
    Polars method to perform them.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '如上所述，实际的电子邮件文本从 `Status: O`（已打开）或 `Status: RO`（已读并已打开）之后开始，这意味着我们可以利用这个模式将电子邮件拆分为“元数据”和“文本”部分。下面你可以看到提取所需字段的三个步骤以及执行它们的相应
    Polars 方法。'
- en: 'Replace `Status: RO` with `Status: O` so that we only have one “split” pattern
    — use `str.replace`'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '将 `Status: RO` 替换为 `Status: O`，以便我们只有一个“拆分”模式 — 使用 `str.replace`'
- en: 'Split the actual string by `Status: O` — use `str.split`'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '按照 `Status: O` 拆分实际字符串 — 使用 `str.split`'
- en: Get the second element (text) of the resulting list — use `arr.get(1)`
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取结果列表的第二个元素（文本） — 使用 `arr.get(1)`
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Et voilà! We have extracted important fields in just a few milliseconds. Let’s
    put it all into one coherent function that we can later use in the pipeline.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 看！我们在短短几毫秒内提取了重要字段。让我们将这些功能放到一个连贯的函数中，稍后在管道中使用。
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, we can move on to the feature generation part.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以继续进行特征生成部分。
- en: Feature Engineering
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: From personal experience, scam emails tend to be very detailed and long (since
    scammers are trying to win your trust) so the character length of an email is
    going to be quite informative. Also, they heavily use exclamation points and digits,
    so calculating the proportion of non-characters in an email can also be useful.
    Finally, scammers love to use caps lock, so let’s calculate the proportion of
    capital letters as well. There are of course, many more features we could create
    but to not make this post too long, let’s just focus on these two.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 根据个人经验，诈骗电子邮件往往非常详细且长（因为骗子试图赢得你的信任），所以电子邮件的字符长度会非常有信息量。此外，它们大量使用感叹号和数字，因此计算电子邮件中非字符的比例也可能很有用。最后，骗子喜欢使用大写字母，所以我们也来计算大写字母的比例。当然，我们还可以创建更多的特征，但为了不让这篇文章过长，我们就专注于这两个特征。
- en: The first feature can be very easily created using an in-built `str.n_chars()`
    function. The two other features can be computed using regex and `str.count_match()`.
    Below you can find the function to calculate these three features. Similar to
    the previous function, it uses `with_columns()` clause to carry over the old features
    and create the new ones on top of them.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个特征可以通过内置的 `str.n_chars()` 函数非常容易地创建。其他两个特征可以使用正则表达式和 `str.count_match()`
    计算。下面你可以找到计算这三个特征的函数。与之前的函数类似，它使用 `with_columns()` 子句来保留旧特征并在其上创建新的特征。
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Text Cleaning
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本清理
- en: 'If you print out a few of the emails we’ve extracted, you’ll notice some things
    that need to be cleaned. For example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打印出我们提取的一些电子邮件，你会注意到一些需要清理的内容。例如：
- en: HTML tags are still present in some of the emails
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些电子邮件中仍然存在 HTML 标签
- en: Lots of non-alphabetic characters are used
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用了很多非字母字符
- en: Some emails are written in uppercase, some in lowercase, and some are mixed
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些电子邮件是大写字母，一些是小写字母，还有一些是混合的
- en: Same as above, we’re going to use regular expressions to clean up the data.
    However, now the method of choice is `str.replace_all` because we want to replace
    all the matched instances, not just the first one. Additionally, we’ll use `str.to_lowercase()`
    to make all text lowercase.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述相同，我们将使用正则表达式来清理数据。然而，现在选择的方法是 `str.replace_all`，因为我们想要替换所有匹配的实例，而不仅仅是第一个。此外，我们将使用
    `str.to_lowercase()` 将所有文本转换为小写。
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, let’s refactor this chain of operations into a function, so that it could
    be applied to the other columns of interest as well.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这串操作重构成一个函数，以便它可以应用于其他感兴趣的列。
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Text Tokenisation
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本标记化
- en: As a final step in the pre-processing pipeline, we’re going to tokenise the
    text. Tokenisation is going to happen using the already familiar method `str.split()`
    where as a split token we’re going to specify a whitespace.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 作为预处理管道的最后一步，我们将对文本进行标记化。标记化将使用已经熟悉的方法 `str.split()`，其中分隔符将指定为空格。
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Again, let’s put this code into a function for our final pipeline.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，将这段代码放入一个函数中，以便在最终的管道中使用。
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Removing Stop Words
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去除停用词
- en: If you’ve worked with text data before, you know that stop word removal is a
    key step in pre-processing tokenised texts. Removing these words allows us to
    focus the analysis only on the important parts of the text.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以前处理过文本数据，你会知道去除停用词是处理标记化文本的关键步骤。去除这些词可以让我们将分析集中在文本的重要部分。
- en: To remove these words, we first need to define them. Here, I’m going to use
    a default set of stop words from `nltk` library plus a set of HTML related words.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了删除这些词，我们首先需要定义它们。在这里，我将使用来自`nltk`库的默认停用词集以及一组与 HTML 相关的词汇。
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now, we need to find out if these words exist in the tokenised array, and if
    they do, we need to drop them. For this we’ll need to use the `arr.eval` method
    because it allows us to run the Polars expressions (e.g. `.is_in` ) against every
    element of the tokenised list. Make sure to read the comment below to understand
    what the each line does as this part of the code is more complicated.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要找出这些词是否存在于分词数组中，如果存在，我们需要将它们删除。为此，我们需要使用`arr.eval`方法，因为它允许我们对分词列表的每个元素运行
    Polars 表达式（例如 `.is_in`）。确保阅读下面的评论以理解每一行的作用，因为这部分代码比较复杂。
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As usual, let’s refactor this bit of code into a function for our final pipeline.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，让我们将这段代码重构为一个函数，用于我们的最终管道。
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: While this pattern might seem quite complicated it’s well worth it to use the
    pre-defined `str` and `arr` expressions to optimise the performance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个模式可能看起来相当复杂，但使用预定义的`str`和`arr`表达式来优化性能是非常值得的。
- en: '**Full Pipeline**'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**完整管道**'
- en: 'So far, we’ve defined pre-processing functions and saw how they can be applied
    to a single column. Polars provides a very handy `pipe` method that allows us
    to chain Polars operations specified as function. Here’s how the final pipeline
    looks like:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经定义了预处理函数，并了解了它们如何应用于单个列。Polars 提供了一个非常实用的`pipe`方法，允许我们将指定为函数的 Polars
    操作串联起来。这就是最终管道的样子：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice that now we can easily apply all the feature engineering, cleaning, and
    tokenisation functions to all the extracted columns and not just the email text
    like in the examples above.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，现在我们可以轻松地将所有特征工程、清理和分词函数应用于所有提取的列，而不仅仅是像上面示例中的电子邮件文本。
- en: Word Cloud Analysis
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词云分析
- en: If you’ve got so far — great job! We’ve read in, cleaned, processed, tokenised,
    and did basic feature engineering on ~4k text records in under a second (at least
    on my Mac M2 machine). Now, let’s enjoy the fruits of our labor and do some basic
    text analysis.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经做到这一点——干得好！我们已经读取、清洗、处理、分词，并对大约 4k 文本记录进行了基本特征工程，全部在一秒钟内（至少在我的 Mac M2 机器上）。现在，让我们享受劳动的成果，进行一些基本的文本分析。
- en: First of all, let’s look at the word cloud of the email texts and marvel at
    all the silly things we can find.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看电子邮件文本的词云，并惊叹于我们能找到的所有有趣的东西。
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/1a49d18b4f882c1f2ed6f36d630f40b2.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a49d18b4f882c1f2ed6f36d630f40b2.png)'
- en: Email text word cloud. Generated by the author.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件文本词云。由作者生成。
- en: Bank accounts, next of kin, security companies, and decease relatives — it has
    got it all. Let’s see how these will look like for text clusters created using
    simple TF-IDF and K-Means.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 银行账户、近亲、保安公司和逝者亲属——应有尽有。让我们看看这些在使用简单 TF-IDF 和 K-Means 创建的文本聚类中的样子。
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Below you can see a few interesting clusters that I’ve identified:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下面你可以看到我识别的一些有趣的聚类：
- en: '![](../Images/555d51105faa8552298c0dca3a1d7e19.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/555d51105faa8552298c0dca3a1d7e19.png)'
- en: Besides these, I also found a few non-sense clusters which means that there
    is still room for improvements when it comes text cleaning. Still, it looks like
    we were able to extract useful clusters, so let’s call it a success. Let me know
    which clusters you find!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我还发现了一些无意义的聚类，这意味着在文本清理方面仍有改进空间。不过，看起来我们成功地提取了有用的聚类，所以我们可以称之为成功。告诉我你发现了哪些聚类！
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'This post has covered a wide variety of pre-processing and cleaning operations
    that Polars library allows you to do. We’ve seen how to use Polars to:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章涵盖了 Polars 库允许你执行的各种预处理和清理操作。我们已经看到如何使用 Polars 来：
- en: Extract specific patterns from texts
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本中提取特定模式
- en: Split texts into lists based on a token
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据标记将文本拆分为列表
- en: Calculate lengths and the number of matches in texts
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算文本中的长度和匹配数量
- en: Clean texts using regex
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则表达式清理文本
- en: Tokenise texts and filter for stop words
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对文本进行分词并过滤停用词
- en: I hope that this post was useful to you and you’ll give Polars a chance in your
    next NLP project. Please consider subscribing, clapping and commenting below.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章对你有用，你会在下一个 NLP 项目中给 Polars 一个机会。请考虑订阅、点赞并在下面评论。
- en: Not a Medium Member yet?
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还不是 Medium 会员？
- en: '[](https://medium.com/@antonsruberts/membership?source=post_page-----fcf7054a929a--------------------------------)
    [## Join Medium with my referral link — Antons Tocilins-Ruberts'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@antonsruberts/membership?source=post_page-----fcf7054a929a--------------------------------)
    [## 使用我的推荐链接加入 Medium — Antons Tocilins-Ruberts'
- en: Read every story from Antons Tocilins-Ruberts (and thousands of other writers
    on Medium). Your membership fee directly…
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Antons Tocilins-Ruberts 和其他成千上万的作者在 Medium 上的每一个故事。你的会员费用直接…
- en: medium.com](https://medium.com/@antonsruberts/membership?source=post_page-----fcf7054a929a--------------------------------)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@antonsruberts/membership?source=post_page-----fcf7054a929a--------------------------------)
- en: References
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Radev, D. (2008), CLAIR collection of fraud email, ACL Data and Code Repository,
    ADCR2008T001,* [*http://aclweb.org/aclwiki*](http://aclweb.org/aclwiki)'
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Radev, D. (2008), CLAIR collection of fraud email, ACL 数据和代码库, ADCR2008T001,*
    [*http://aclweb.org/aclwiki*](http://aclweb.org/aclwiki)'
- en: ''
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Project Github* [https://github.com/aruberts/tutorials/tree/main/metaflow/fraud_email](https://github.com/aruberts/tutorials/tree/main/metaflow/fraud_email)'
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*项目 Github* [https://github.com/aruberts/tutorials/tree/main/metaflow/fraud_email](https://github.com/aruberts/tutorials/tree/main/metaflow/fraud_email)'
- en: ''
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Polars User Guide* [https://pola-rs.github.io/polars-book/user-guide/](https://pola-rs.github.io/polars-book/user-guide/)'
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Polars 用户指南* [https://pola-rs.github.io/polars-book/user-guide/](https://pola-rs.github.io/polars-book/user-guide/)'
