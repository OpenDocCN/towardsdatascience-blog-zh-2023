- en: Fast String Processing with Polars — Scam Emails Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fast-string-processing-with-polars-scam-emails-dataset-fcf7054a929a](https://towardsdatascience.com/fast-string-processing-with-polars-scam-emails-dataset-fcf7054a929a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Clean, process and tokenise texts in milliseconds using in-built Polars string
    expressions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@antonsruberts?source=post_page-----fcf7054a929a--------------------------------)[![Antons
    Tocilins-Ruberts](../Images/363a4f32aa793cca7a67dea68e76e3cf.png)](https://medium.com/@antonsruberts?source=post_page-----fcf7054a929a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fcf7054a929a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fcf7054a929a--------------------------------)
    [Antons Tocilins-Ruberts](https://medium.com/@antonsruberts?source=post_page-----fcf7054a929a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fcf7054a929a--------------------------------)
    ·10 min read·May 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/130cbb064ff780aa7b2bc919defabc8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Stephen Phillips - Hostreviews.co.uk](https://unsplash.com/es/@hostreviews?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the large scale adoption of Large language Models (LLMs) it might seem
    that we’re past the stage where we had to manually clean and process text data.
    Unfortunately, me and other NLP practitioners can attest that this is very much
    not the case. Clean text data is required at every stage of NLP complexity — from
    basic text analytics to machine learning and LLMs. This post will showcase how
    this laborious and tedious process can be significantly sped up using Polars.
  prefs: []
  type: TYPE_NORMAL
- en: Polars
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Polars](https://github.com/pola-rs/polars) is a blazingly fast Data Frame
    library written in Rust that is incredibly efficient with handling strings (due
    to its Arrow backend). Polars stores strings in the `Utf8` format using `Arrow`
    backend which makes string traversal cache-optimal and predictable. Also, it exposes
    a lot of in-built string operations under the `str` namespace which makes the
    string operations parallelised. Both of these factors make working with strings
    extremely easy and fast.'
  prefs: []
  type: TYPE_NORMAL
- en: The library shares a lot of syntaxis with Pandas but there are also a lot of
    quirks that you’ll need to get used to. This post will walk you through working
    with strings but for a comprehensive overview I highly recommend this [“Getting
    Started”](https://pola-rs.github.io/polars-book/getting-started/intro/) guide
    as it will give you a good overview of the library.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find all the code in this [GitHub repo](https://github.com/aruberts/tutorials/tree/main/metaflow/fraud_email),
    so make sure to pull it if want to code along (don’t forget to ⭐ it). To make
    this post more practical and fun, I’ll showcase how we can clean a small scam
    email dataset which can be found on [Kaggle](https://www.kaggle.com/datasets/rtatman/fraudulent-email-corpus)
    (License [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)). Polars
    can be installed using pip — `pip install polars` and the recommended Python version
    is `3.10` .
  prefs: []
  type: TYPE_NORMAL
- en: Text Processing Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of this pipeline is to parse the raw text file into a DataFrame that
    can be used for further analytics/modelling. Here are the overall steps that will
    be implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: Read in text data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract relevant fields (e.g. sender email, object, text, etc.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract useful features from these fields (e.g. length, % of digits, etc)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pre-process text for further analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform some basic text analytics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Without further ado, let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: Reading Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming that the text file with emails is saved as `fraudulent_emails.txt`
    , here’s the function used to read them in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you explore the text data you’ll see that the emails have two main sections
  prefs: []
  type: TYPE_NORMAL
- en: Metadata (starts with `From r` ) that contains email sender, subject, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Email text (starts after `Status: O` or `Status: RO` )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’m using the first pattern to split the continuous text file into a list of
    emails. Overall, we should be able to read in 3977 emails that we put into a Polars
    DataFrame for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Extracting Relevant Fields
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now the tricky part begins. How do we extract relevant fields from this mess
    of a text data? Unfortunately, the answer is regex.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sender and Subject**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Upon further inspection of metadata (below) you can see that it has fields `From:`
    and `Subject:` which are going to be very useful for us.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you keep scrolling the emails, you’ll find that there are a few formats
    for the `From:` field. The first format you see above where we have both name
    and email. The second format contains only the email e.g. `From: 123@abc.com`
    or `From: “123@abc.com”` . With this in mind, we’ll need three regex patterns
    — one for subject, and two for sender (name with email and just email).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Polars has an `str.extract` method that can compare the above patterns to our
    text and (you guessed it) extract the matching groups. Here’s how you can apply
    it to the `emails_pl` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see besides `str.extract` we’re also using a `pl.when().then().otherwise()`
    expressions (Polars version of if/else) to account for a second email only pattern.
    If you print out the results you’ll see that in most cases it should’ve worked
    correctly (and incredibly fast). We now have `sender_name` , `sender_email` and
    `subject` fields for our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f82b155fbe85ce3ffc664b639d7ec26b.png)'
  prefs: []
  type: TYPE_IMG
- en: Polars DF sample. Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Email Text**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As was noted above, the actual email text starts after `Status: O` (opened)
    or `Status: RO` (read and opened) which means that we can utilise this pattern
    to split the email into “metadata” and “text” parts. Below you can see the three
    steps that we need to take to extract the required field and the corresponding
    Polars method to perform them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Replace `Status: RO` with `Status: O` so that we only have one “split” pattern
    — use `str.replace`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Split the actual string by `Status: O` — use `str.split`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the second element (text) of the resulting list — use `arr.get(1)`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Et voilà! We have extracted important fields in just a few milliseconds. Let’s
    put it all into one coherent function that we can later use in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can move on to the feature generation part.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From personal experience, scam emails tend to be very detailed and long (since
    scammers are trying to win your trust) so the character length of an email is
    going to be quite informative. Also, they heavily use exclamation points and digits,
    so calculating the proportion of non-characters in an email can also be useful.
    Finally, scammers love to use caps lock, so let’s calculate the proportion of
    capital letters as well. There are of course, many more features we could create
    but to not make this post too long, let’s just focus on these two.
  prefs: []
  type: TYPE_NORMAL
- en: The first feature can be very easily created using an in-built `str.n_chars()`
    function. The two other features can be computed using regex and `str.count_match()`.
    Below you can find the function to calculate these three features. Similar to
    the previous function, it uses `with_columns()` clause to carry over the old features
    and create the new ones on top of them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Text Cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you print out a few of the emails we’ve extracted, you’ll notice some things
    that need to be cleaned. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: HTML tags are still present in some of the emails
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lots of non-alphabetic characters are used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some emails are written in uppercase, some in lowercase, and some are mixed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Same as above, we’re going to use regular expressions to clean up the data.
    However, now the method of choice is `str.replace_all` because we want to replace
    all the matched instances, not just the first one. Additionally, we’ll use `str.to_lowercase()`
    to make all text lowercase.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s refactor this chain of operations into a function, so that it could
    be applied to the other columns of interest as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Text Tokenisation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a final step in the pre-processing pipeline, we’re going to tokenise the
    text. Tokenisation is going to happen using the already familiar method `str.split()`
    where as a split token we’re going to specify a whitespace.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Again, let’s put this code into a function for our final pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Removing Stop Words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’ve worked with text data before, you know that stop word removal is a
    key step in pre-processing tokenised texts. Removing these words allows us to
    focus the analysis only on the important parts of the text.
  prefs: []
  type: TYPE_NORMAL
- en: To remove these words, we first need to define them. Here, I’m going to use
    a default set of stop words from `nltk` library plus a set of HTML related words.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, we need to find out if these words exist in the tokenised array, and if
    they do, we need to drop them. For this we’ll need to use the `arr.eval` method
    because it allows us to run the Polars expressions (e.g. `.is_in` ) against every
    element of the tokenised list. Make sure to read the comment below to understand
    what the each line does as this part of the code is more complicated.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As usual, let’s refactor this bit of code into a function for our final pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: While this pattern might seem quite complicated it’s well worth it to use the
    pre-defined `str` and `arr` expressions to optimise the performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Full Pipeline**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we’ve defined pre-processing functions and saw how they can be applied
    to a single column. Polars provides a very handy `pipe` method that allows us
    to chain Polars operations specified as function. Here’s how the final pipeline
    looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Notice that now we can easily apply all the feature engineering, cleaning, and
    tokenisation functions to all the extracted columns and not just the email text
    like in the examples above.
  prefs: []
  type: TYPE_NORMAL
- en: Word Cloud Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’ve got so far — great job! We’ve read in, cleaned, processed, tokenised,
    and did basic feature engineering on ~4k text records in under a second (at least
    on my Mac M2 machine). Now, let’s enjoy the fruits of our labor and do some basic
    text analysis.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, let’s look at the word cloud of the email texts and marvel at
    all the silly things we can find.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1a49d18b4f882c1f2ed6f36d630f40b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Email text word cloud. Generated by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Bank accounts, next of kin, security companies, and decease relatives — it has
    got it all. Let’s see how these will look like for text clusters created using
    simple TF-IDF and K-Means.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Below you can see a few interesting clusters that I’ve identified:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/555d51105faa8552298c0dca3a1d7e19.png)'
  prefs: []
  type: TYPE_IMG
- en: Besides these, I also found a few non-sense clusters which means that there
    is still room for improvements when it comes text cleaning. Still, it looks like
    we were able to extract useful clusters, so let’s call it a success. Let me know
    which clusters you find!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This post has covered a wide variety of pre-processing and cleaning operations
    that Polars library allows you to do. We’ve seen how to use Polars to:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract specific patterns from texts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split texts into lists based on a token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate lengths and the number of matches in texts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clean texts using regex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenise texts and filter for stop words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I hope that this post was useful to you and you’ll give Polars a chance in your
    next NLP project. Please consider subscribing, clapping and commenting below.
  prefs: []
  type: TYPE_NORMAL
- en: Not a Medium Member yet?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@antonsruberts/membership?source=post_page-----fcf7054a929a--------------------------------)
    [## Join Medium with my referral link — Antons Tocilins-Ruberts'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Antons Tocilins-Ruberts (and thousands of other writers
    on Medium). Your membership fee directly…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@antonsruberts/membership?source=post_page-----fcf7054a929a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Radev, D. (2008), CLAIR collection of fraud email, ACL Data and Code Repository,
    ADCR2008T001,* [*http://aclweb.org/aclwiki*](http://aclweb.org/aclwiki)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Project Github* [https://github.com/aruberts/tutorials/tree/main/metaflow/fraud_email](https://github.com/aruberts/tutorials/tree/main/metaflow/fraud_email)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Polars User Guide* [https://pola-rs.github.io/polars-book/user-guide/](https://pola-rs.github.io/polars-book/user-guide/)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
