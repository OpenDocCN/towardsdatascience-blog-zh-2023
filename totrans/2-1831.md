# 句子变换器：伪装中的意义

> 原文：[https://towardsdatascience.com/sentence-transformers-meanings-in-disguise-323cf6ac1e52](https://towardsdatascience.com/sentence-transformers-meanings-in-disguise-323cf6ac1e52)

## [NLP语义搜索](https://jamescalam.medium.com/list/nlp-for-semantic-search-d3a4b96a52fe)

## 现代语言模型如何捕捉意义

[](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)[![James Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)[](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------) [James Briggs](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------) ·阅读时间12分钟·2023年1月3日

--

![](../Images/edabd3baac27a7fd707b7855ea93c5c2.png)

图片来源：[Brian Suh](https://unsplash.com/@_briansuh?utm_source=medium&utm_medium=referral)于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。最初发布在[Pinecone的NLP语义搜索电子书](https://www.pinecone.io/learn/sentence-embeddings/)中（作者在此工作）。

变换器完全重塑了自然语言处理（NLP）的格局。在变换器出现之前，得益于递归神经网络（RNNs），我们有了*还不错*的翻译和语言分类——它们的语言理解能力有限，导致许多小错误，大块文本的一致性几乎是不可能的。

自2017年论文*《Attention is all you need》* [1]首次引入变换器模型以来，NLP从RNNs发展到BERT和GPT等模型。这些新模型可以回答问题、撰写文章（*也许是GPT-3写的*）、实现非常直观的语义搜索——还有更多。

有趣的是，对于许多任务，这些模型的后期部分与RNN中的部分相同——通常是几个前馈神经网络，用于输出模型预测结果。

改变的是这些层的*输入*。变换器模型创建的[密集嵌入](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/)信息量要丰富得多，尽管使用相同的最终外层，我们仍然获得了巨大的性能提升。

这些越来越丰富的句子嵌入可以用来快速比较各种用例中的句子相似性。例如：

+   **语义文本相似度 (STS)** — 比较句子对。我们可能想要在数据集中识别模式，但这通常用于基准测试。

+   **语义搜索**——使用语义意义的信息检索（IR）。给定一组句子，我们可以使用*‘查询’*句子进行搜索，并识别最相似的记录。使搜索能够基于概念（而非特定词汇）进行。

+   **聚类**——我们可以对句子进行聚类，这对主题建模很有用。

在本文中，我们将探讨这些嵌入是如何被调整和应用于各种语义相似性应用的，通过使用一种叫做*‘句子变换器’*的新型变换器。

# 一些“上下文”

在我们深入探讨句子变换器之前，理解变换器嵌入为何如此丰富可能会有所帮助——以及普通*变换器*与*句子变换器*之间的差异所在。

变换器是之前RNN模型的间接后代。这些旧的递归模型通常由许多递归*单元*构成，如[LSTMs或GRUs](/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)。

在*机器翻译*中，我们会找到[编码器-解码器网络](https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/)。第一个模型用于*编码*原始语言到*上下文向量*，第二个模型用于将其*解码*成目标语言。

![](../Images/ecd770c80c2df0afeb2b3944472fea4c.png)

具有单一上下文向量共享的编码器-解码器架构在两个模型之间，这作为一个信息瓶颈，因为*所有*信息必须通过这一点传递。

这里的问题是我们在两个模型之间创建了一个*信息瓶颈*。我们在多个时间步骤中创建了大量信息，并试图通过一个连接将所有信息挤压过来。这限制了编码器-解码器的性能，因为编码器产生的许多信息在到达解码器之前就已经丢失。

*注意机制*为瓶颈问题提供了一个解决方案。它提供了信息传递的另一条途径。然而，它并没有让过程变得复杂，因为它仅仅*关注*最相关的信息。

通过将每个时间步的上下文向量传递到注意机制中（生成注释向量），去除了信息瓶颈，并在较长序列中有更好的信息保留。

![](../Images/7c95a9edcf3ca64a8faa0df2e7d08200.png)

带有注意机制的编码器-解码器。注意机制考虑了所有编码器输出激活和解码器中每个时间步的激活，这些都会修改解码器输出。

在解码过程中，模型一次解码一个单词/时间步。每一步都会计算单词与所有编码器注释之间的对齐度（例如，相似度）。

更高的对齐度导致了对解码器步骤输出的编码器注释的更大加权。这意味着机制计算了需要*关注*哪些编码器单词。

![](../Images/48a331d23b9489d304f128804266edf9.png)

英法编码器和解码器之间的注意力，来源 [2]。

所有表现最好的 RNN 编码器-解码器都使用了这种注意力机制。

# Attention is All You Need

在 2017 年，一篇题为*Attention Is All You Need*的论文发表了。这标志着 NLP 的一个转折点。作者们展示了我们可以去除 RNN 网络，并仅使用*注意力*机制——经过一些修改，就能获得更好的性能。

这个基于注意力的新模型被称为*‘transformer’*。从那时起，由于其极其优越的性能和出色的泛化能力，NLP 生态系统完全从 RNN 转向了 transformers。

第一个 transformer 通过使用*三个*关键组件去除了对 RNN 的需求：

+   位置编码

+   自注意力

+   多头注意力

**位置编码**取代了 RNN 在 NLP 中的关键优势——考虑序列顺序的能力（它们是*递归的*）。它通过根据位置向每个输入嵌入添加一组变化的正弦波激活来工作。

**自注意力**是指在一个词与其自身上下文（句子/段落）中的所有其他词之间应用注意力机制。这不同于普通的注意力，它专注于编码器和解码器之间的注意力。

**多头注意力**可以看作是几个*并行*的注意力机制共同工作。使用多个注意力*头*允许表示多个关系集（而不是单一的关系集）。

# 预训练模型

新的 transformer 模型的泛化能力远远超过了以前的 RNN，这些 RNN 通常是为每个用例特别构建的。

使用 transformer 模型，可以使用相同的*‘核心’*模型，并仅替换最后几层以适应不同的用例（而无需重新训练*核心*）。

这种新特性导致了*NLP*中*预训练*模型的兴起。预训练的 transformer 模型是在大量训练数据上训练的——通常由谷歌或 OpenAI 等公司高成本训练，然后免费提供给公众使用。

这些预训练模型中最广泛使用的之一是 BERT，或谷歌 AI 的**B**idirectional **E**ncoder **R**epresentations from **T**ransformers。

BERT 产生了一系列进一步的模型和变种，如 distilBERT、RoBERTa 和 ALBERT，涵盖分类、问答、词性标注等任务。

# BERT 用于句子相似度

到目前为止，一切都很好，但这些 transformer 模型在构建句子向量时存在一个问题：Transformers 使用的是词或*token*级别的嵌入，而*不是*句子级别的嵌入。

在句子 transformers 出现之前，使用 BERT 计算*准确的*句子相似度的方法是使用交叉编码器结构。这意味着我们将两个句子传递给 BERT，在 BERT 顶部添加一个分类头——并用它来输出相似度评分。

![](../Images/2f05f889b220b0df300e2979b6e1f4b1.png)

BERT 交叉编码器架构包括一个 BERT 模型，该模型处理句子 A 和 B。这两个句子在同一序列中处理，由 `[SEP]` 标记分隔。随后是一个前馈神经网络分类器，输出相似度评分。

交叉编码器网络确实生成非常准确的相似度评分（比 SBERT 更好），但它的*可扩展性差*。如果我们想在一个 100K 句子的数据库中进行相似度搜索，我们需要完成 100K 次交叉编码器推断计算。

要对句子进行聚类，我们需要比较我们 100K 数据集中的所有句子，结果将产生接近 5 亿次比较 —— 这显然是不现实的。

理想情况下，我们需要预先计算句子向量，这些向量可以被存储，然后在需要时使用。如果这些向量表示良好，我们只需计算每对句子之间的余弦相似度。

使用原始的 BERT（及其他变换器），我们可以通过对 BERT 输出的所有标记嵌入进行平均来构建句子嵌入（如果我们输入 512 个标记，则输出 512 个嵌入）。另一种方法是使用第一个 `[CLS]` 标记的输出（一个特定于 BERT 的标记，其输出嵌入用于分类任务）。

使用这两种方法中的一种可以得到我们的句子嵌入，这些嵌入可以被存储并且比较速度更快，将搜索时间从 65 小时缩短到大约 5 秒（见下文）。然而，准确性不佳，甚至比使用平均的 GloVe 嵌入（该方法开发于 2014 年）更差。

**这个解决方案** 旨在解决缺乏准确模型的*合理延迟*问题，由 Nils Reimers 和 Iryna Gurevych 在 2019 年设计，推出了 sentence-BERT（SBERT）和 `sentence-transformers` 库。

SBERT 在所有常见的语义文本相似性（STS）任务中表现优于之前的最先进（SOTA）模型 —— 更多关于这些任务的内容见下文 —— 唯一的例外是一个数据集（SICK-R）。

幸运的是，为了实现可扩展性，SBERT 生成句子嵌入 —— 因此我们*不*需要对每个句子对比较执行整个推断计算。

Reimers 和 Gurevych 在 2019 年展示了显著的速度提升。从 10K 个句子中找到最相似的句子对，使用 BERT 需要 65 小时。使用 SBERT，嵌入的创建时间约为 5 秒，与余弦相似度的比较时间约为 0.01 秒。

自 SBERT 论文发布以来，已经构建了许多使用类似概念的句子变换器模型，这些概念用于训练原始的 SBERT。它们都在许多相似和不相似的句子对上进行了训练。

使用诸如 softmax 损失、多负样本排序损失或 MSE 边际损失等损失函数，这些模型被优化以生成相似句子的相似嵌入和不相似句子的不同嵌入。

现在你已经了解了一些关于句子变换器的背景知识，包括它们的来源及其必要性。让我们深入探讨它们是如何工作的。

**[3] SBERT论文涵盖了本节中的许多陈述、技术和数据。*

# 句子转换器

我们解释了使用BERT的跨编码器架构来衡量句子相似性。SBERT类似，但去掉了最终的分类头，并且一次处理一个句子。SBERT然后在最终输出层上使用均值池化来生成句子嵌入。

与BERT不同，SBERT在句子对上使用*siamese*架构进行微调。我们可以将其视为两个并行的完全相同的BERT，分享完全相同的网络权重。

![](../Images/4878afbc9ff8eb4032ac87ca0262425a.png)

SBERT模型应用于句子对*句子A*和*句子B*。注意，BERT模型输出的是令牌嵌入（由512个768维向量组成）。我们随后使用池化函数将这些数据压缩成一个768维的句子向量。

实际上，我们使用的是单个BERT模型。然而，由于我们在训练期间将句子A和句子B作为*对*进行处理，因此更容易将其视为具有相同权重的两个模型。

# Siamese BERT预训练

训练句子转换器有不同的方法。我们将描述最初的SBERT过程，该过程主要优化*softmax-loss*。请注意，这是一个高层次的解释，我们将把深入讲解留到另一篇文章中。

softmax-loss方法使用*‘siamese’*架构，在斯坦福自然语言推理（SNLI）和多领域NLI（MNLI）语料库上进行微调。

SNLI包含570K句子对，MNLI包含430K。这两个语料库中的句子对都包含一个`前提`和一个`假设`。每对句子被分配一个三种标签之一：

+   **0** — *蕴含*，例如，`前提`暗示了`假设`。

+   **1** — *中立*，`前提`和`假设`都可能为真，但它们不一定相关。

+   **2** — *矛盾*，`前提`和`假设`互相矛盾。

给定这些数据，我们将句子A（假设为`前提`）输入到siamese BERT A中，将句子B（`假设`）输入到siamese BERT B中。

siamese BERT输出我们的池化句子嵌入。SBERT论文中有*三种*不同的池化方法结果。这些方法是*均值*、*最大值*和*[CLS]*-池化。*均值*池化方法在NLI和STSb数据集中表现最佳。

现在有两个句子嵌入。我们将嵌入A称为`u`，嵌入B称为`v`。下一步是拼接`u`和`v`。再次，测试了几种拼接方法，但表现最好的方法是`(u, v, |u-v|)`操作：

![](../Images/8742d79bf3ddda40f9b682351e32673e.png)

我们将嵌入**u**、**v**和**|u — v|**进行拼接。

`|u-v|`的计算结果给出两个向量的逐元素差异。除了原始的两个嵌入（`u`和`v`），这些都被输入到一个具有*三个*输出的前馈神经网络（FFNN）中。

这三个输出与我们的 NLI 相似性标签**0**、**1**和**2**对齐。我们需要从我们的 FFNN 计算 softmax，这在[交叉熵损失函数](https://www.pinecone.io/learn/cross-entropy-loss/)中完成。softmax 和标签用于优化这个*‘softmax-loss’*。

![](../Images/78e112539f7a39c8d0831cf39290667a.png)

这些操作是在训练过程中对两个句子嵌入`u`和`v`进行的。注意，*softmax-loss* 指的是交叉熵损失（默认情况下包含一个 softmax 函数）。

这导致我们对于相似句子（标签**0**）的池化句子嵌入变得*更加相似*，而对于不相似句子（标签**2**）的嵌入变得*不那么相似*。

请记住，我们使用的是*siamese* BERT，而不是*dual* BERT。这意味着我们不使用两个独立的 BERT 模型，而是使用一个处理句子 A 然后处理句子 B 的单个 BERT。

这意味着当我们优化模型权重时，它们会朝着一个方向推动，使模型在看到*蕴含*标签时输出更多相似的向量，而在看到*矛盾*标签时输出更多不相似的向量。

这种训练方法有效的事实并不是特别直观，实际上 Reimers 曾描述过它*偶然*产生了良好的句子嵌入[5]。

自原始论文以来，这个领域有了进一步的研究。已经建立了许多更多的模型，如[最新的 MPNet 和 RoBERTa 模型（在超过 1B 样本上训练）](https://huggingface.co/spaces/flax-sentence-embeddings/sentence-embeddings)（表现更佳）。我们将在未来的文章中探讨其中的一些模型及其使用的优越训练方法。

现在，让我们看看如何初始化和使用一些这些句子变换器模型。

# 开始使用句子变换器

开始使用句子变换器的最快和最简单的方法是通过 SBERT 创建的`sentence-transformers`库。我们可以通过`pip`安装它。

[PRE0]

我们将从原始的 SBERT 模型`bert-base-nli-mean-tokens`开始。首先，我们下载并初始化模型。

我们可以看到的输出是`SentenceTransformer`对象，它包含了*三个*组件：

+   **变换器**本身，我们可以看到最大序列长度为`128`个标记，并且是否对任何输入进行小写处理（在这种情况下，模型*不*进行小写处理）。我们还可以看到模型类`BertModel`。

+   **池化**操作，在这里我们可以看到我们正在生成一个`768`维的句子嵌入。我们使用的是*均值池化*方法。

一旦我们有了模型，使用`encode`方法可以迅速构建句子嵌入。

现在我们有了句子嵌入，可以用来快速比较句子相似性，用于文章开头介绍的用例：STS、语义搜索和聚类。

我们可以仅使用余弦相似度函数和 Numpy 快速编写一个 STS 示例。

![](../Images/bfec024273597cb93ea09e5ccf87b5b4.png)

热图显示了所有句子对之间的余弦相似度值。

在这里，我们计算了五个句子嵌入之间每种组合的余弦相似度。它们是：

我们可以看到右下角的最高相似度分数为 `0.64`。正如我们所希望的，这一结果是针对描述使用建筑材料进行不良牙科实践的句子 `4` 和 `3` 的。

# 其他句子转换器

尽管我们从 SBERT 模型中得到了良好的结果，但自那以后已经构建了许多其他句子转换器模型。我们可以在 `sentence-transformers` 库中找到许多这样的模型。

这些更新的模型在性能上可以显著超过原始的 SBERT。事实上，SBERT 不再列为 [SBERT.net 模型页面](https://www.sbert.net/docs/pretrained_models.html)上的可用模型。

在句子转换器模型页面上的一些顶级表现模型。

我们将在未来的文章中更详细地介绍一些这些较新的模型。现在，让我们比较一下表现最好的模型，并运行我们的 STS 任务。

这里我们有 `SentenceTransformer` 模型 `all-mpnet-base-v2`。这些组件与 `bert-base-nli-mean-tokens` 模型非常相似，只是有一些小差异：

+   `max_seq_length` 从 `128` 增加到了 `384`。这意味着我们可以处理的序列长度是使用 SBERT 时的 *三倍*。

+   基础模型现在是 `MPNetModel` [4] 而不是 `BertModel`。

+   对句子嵌入应用了额外的归一化层。

让我们比较一下 `all-mpnet-base-v2` 和 SBERT 的 STS 结果。

![](../Images/f3e67a466911193a2c3c56562143bdf1.png)

SBERT 和 MPNet 句子转换器的热图。

后期模型的语义表示非常明显。虽然 SBERT 正确地识别 `4` 和 `3` 为最相似的对，但它也对其他句子对赋予了相当高的相似度。

另一方面，MPNet 模型在相似对和不相似对之间做出了 *非常* 清晰的区分，大多数对的得分低于 0.1，而 `4`-`3` 对的得分为 *0.52*。

通过增加不相似对和相似对之间的分离，我们正在：

1.  使得自动识别相关对变得更加容易。

1.  将预测结果推向训练期间使用的 *0* 和 *1* 目标分数，使 *不相似* 和 *相似* 对的分数更加接近。这是我们将在未来的文章中关于微调这些模型时看到的内容。

这就是本文介绍句子嵌入和当前 SOTA 句子转换器模型的全部内容，这些模型用于构建这些极其有用的嵌入。

句子嵌入，虽然最近才流行开来，但却是从一系列出色的创新中产生的。我们描述了一些应用于创建第一个句子转换器 SBERT 的机制。

我们还展示了尽管SBERT于2019年才刚刚推出，但其他句子变换器已经超越了该模型。幸运的是，通过`sentence-transformers`库，我们可以轻松地将SBERT替换为这些更新的模型。

# 参考文献

[1] A. Vashwani 等，[注意力机制全在于此](https://arxiv.org/abs/1706.03762)（2017年），NeurIPS

[2] D. Bahdanau 等，[通过共同学习对齐和翻译的神经机器翻译](https://arxiv.org/abs/1409.0473)（2015年），ICLR

[3] N. Reimers, I. Gurevych，[Sentence-BERT：使用Siamese BERT网络的句子嵌入](https://arxiv.org/abs/1908.10084)（2019年），ACL

[4] [MPNet模型](https://huggingface.co/transformers/model_doc/mpnet.html)，Hugging Face文档

[5] N. Reimers，[自然语言推断](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/README.md)，GitHub上的sentence-transformers

**所有图片均为作者提供，除非另有说明**
