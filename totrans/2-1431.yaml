- en: 'LLaMA: LLMs for Everyone!'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLaMA：面向所有人的大型语言模型！
- en: 原文：[https://towardsdatascience.com/llama-llms-for-everyone-724e737835be](https://towardsdatascience.com/llama-llms-for-everyone-724e737835be)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llama-llms-for-everyone-724e737835be](https://towardsdatascience.com/llama-llms-for-everyone-724e737835be)
- en: High-performing language models that are open-source…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高性能的开源语言模型……
- en: '[](https://wolfecameron.medium.com/?source=post_page-----724e737835be--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----724e737835be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----724e737835be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----724e737835be--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----724e737835be--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----724e737835be--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----724e737835be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----724e737835be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----724e737835be--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----724e737835be--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----724e737835be--------------------------------)
    ·15 min read·Jul 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----724e737835be--------------------------------)
    ·15分钟阅读·2023年7月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f673f8c76d03117a2c38c912c91cd911.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f673f8c76d03117a2c38c912c91cd911.png)'
- en: (Photo by [Raspopova Marina](https://unsplash.com/@raspopovamarisha?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/llama?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由[拉斯波波娃·玛丽娜](https://unsplash.com/@raspopovamarisha?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)拍摄，来源于[Unsplash](https://unsplash.com/s/photos/llama?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: For years, the deep learning community has embraced openness and transparency,
    leading to massive open-source projects like [HuggingFace](https://huggingface.co/).
    Many of the most profound ideas in deep learning (e.g., [transformers](https://newsletter.artofsaience.com/p/vision-transformers-from-idea-to)
    [2], [self-supervised learning](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning),
    etc.) are openly available online, either via [public code repositories](https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel)
    or [Arxiv](https://arxiv.org/list/cs.AI/recent). Although open-source has been
    the norm for quite some time, the popularity (and commercial applicability) of
    large language models (LLMs) has recently challenged this tendency.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，深度学习社区一直倡导开放性和透明度，导致了像[HuggingFace](https://huggingface.co/)这样的大规模开源项目。深度学习中的许多深刻思想（例如，[transformers](https://newsletter.artofsaience.com/p/vision-transformers-from-idea-to)
    [2]，[自监督学习](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning)等）在网上公开可用，无论是通过[公共代码库](https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel)还是[Arxiv](https://arxiv.org/list/cs.AI/recent)。尽管开源已经成为一种规范，但大型语言模型（LLMs）的流行（以及商业适用性）最近挑战了这一趋势。
- en: 'Many of the most powerful LLMs available today can only be accessed via APIs
    (e.g., from [OpenAI](https://openai.com/blog/openai-api) or [Anthropic](https://console.anthropic.com/docs/api#accessing-the-api)),
    making the source code and model parameters inaccessible to researchers and developers.
    While it’s not my goal to spark a [moral discussion](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)
    of current trends in the LLM landscape, this information is relevant to the topic
    of this post: openly-available LLMs. Interestingly, not all powerful language
    foundation models are hidden behind a paywall. Some models, such as LLaMA, are
    both openly available and incredibly high-performing, thus maintaining a sense
    of openness in the deep learning research community.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 目前许多最强大的大型语言模型（LLMs）只能通过API访问（例如，来自[OpenAI](https://openai.com/blog/openai-api)或[Anthropic](https://console.anthropic.com/docs/api#accessing-the-api)），这使得源代码和模型参数对研究人员和开发者而言不可及。虽然我的目标不是引发对当前LLM趋势的[伦理讨论](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)，但这些信息与本文的主题：公开可用的LLM相关。有趣的是，并非所有强大的语言基础模型都被隐藏在付费墙后面。一些模型，如LLaMA，既公开可用又表现出色，因此在深度学习研究社区中保持了开放性。
- en: '**What is LLaMA?** LLaMA is not a single model, but rather a suite of LLMs
    with sizes ranging from 7 billion to 65 billion parameters. Taking inspiration
    from [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms)
    [3], these LLMs are a bit smaller than [their counterparts](https://cameronrwolfe.substack.com/i/91134599/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-b-a-large-scale-generative-language-model)
    but are pre-trained extensively (i.e., smaller models, more tokens) and developed
    with the goal of providing a diverse group of models with different tradeoffs
    between performance and inference efficiency. LLaMA models perform surprisingly
    well; e.g., the 13 billion parameter model is roughly comparable to [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)
    [4], while the 65 billion parameter model often surpasses the performance of [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)
    [5].'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: “GPT-4 has learned from a variety of licensed, created, and publicly available
    data sources, which may include publicly available personal information.” *— from
    [6]*
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Beyond the impressive performance, LLaMA uses only publicly available data for
    pre-training. Taking a step (back) towards open-source within the LLM landscape,
    LLaMA models can be reproduced completely from online resources. Recent models
    such as GPT-4 are known to have been trained with a combination of public and
    proprietary/private data. Although this may benefit model performance, LLaMA demonstrates
    that we can do a lot with data that is available online, thus providing a source
    of hope for open research initiatives related to LLMs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ef92042c151f20891b2acfff1907b4e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Background Information
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LLaMA LLMs adopt several ideas and techniques that are proposed in prior
    work. Within this section, we will go over some useful background information
    that will be helpful in developing a deeper understanding of LLaMA and its components.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Brief note on LLMs.** First, it’s helpful to understand the basics of LLMs,
    including their architecture, training procedure, and general approach. We have
    explored this topic extensively in prior overviews. As such, we won’t cover this
    topic in detail here, but links for further reading and learning are provided
    below.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: LLM (Decoder-Only) Architecture [[link](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)]
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language Model Pre-Training [[link](https://cameronrwolfe.substack.com/i/85568430/language-modeling)]
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explanation of LLMs [[link](https://cameronrwolfe.substack.com/i/91134599/a-primer-on-language-modeling)]
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM History [[link](https://twitter.com/cwolferesearch/status/1639378997627826176?s=20)]
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM Basics [[link](https://twitter.com/cwolferesearch/status/1635693551584522256?s=20)]
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Root Mean Square Layer Normalization (RMSNorm)
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Typically, transformer architectures (including the decoder-only transformer
    architectures used by LLMs) use [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
    to normalize activation values within each of their layers. However, using different
    normalization techniques has been shown to stabilize training and improve generalization
    performance. For example, [RMSNorm](https://github.com/bzhangGo/rmsnorm) [16]
    is defined as shown in the equation below.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，transformer架构（包括LLM使用的仅解码器transformer架构）使用[LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)来规范化每层内的激活值。然而，使用不同的规范化技术已被证明可以稳定训练并提高泛化性能。例如，[RMSNorm](https://github.com/bzhangGo/rmsnorm)
    [16]的定义如下面的方程所示。
- en: '![](../Images/670becc4465faece48d3f6439d6abc40.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/670becc4465faece48d3f6439d6abc40.png)'
- en: (created by author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: （由作者创建）
- en: RMSNorm is somewhat similar to LayerNorm, but it removes the [mean-centering](https://medium.com/@aabdygaziev/data-transformations-centering-scaling-7bd48a530595)
    operation (and uses a slightly modified denominator) when normalizing the neural
    network’s activation values. Compared to LayerNorm, RMSNorm is more computationally
    efficient and simple, allowing it to achieve comparable levels of performance
    with a 10–50% improvement in efficiency.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: RMSNorm与LayerNorm有些相似，但在规范化神经网络激活值时，它去除了[均值中心化](https://medium.com/@aabdygaziev/data-transformations-centering-scaling-7bd48a530595)操作（并使用了略微修改的分母）。与LayerNorm相比，RMSNorm在计算效率和简单性上更具优势，使其能够以10–50%的效率提升达到相当的性能水平。
- en: '![](../Images/bc33eae5b58fcfc986bff3284f05e5c9.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc33eae5b58fcfc986bff3284f05e5c9.png)'
- en: (from [16])
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[16]）
- en: SwiGLU Activation Function
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SwiGLU激活函数
- en: Each block of an LLM’s decoder-only architecture contains a two-layer [feed-forward
    neural network](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)
    (i.e., uses no bias and is applied individually to each token vector) with a non-linearity
    between the two layers. Originally, this non-linearity was a [Rectified Linear
    Unit (ReLU)](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) activation
    function. However, recent work [15] has revealed that this is not the optimal
    choice.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的仅解码器架构的每个块包含一个两层[前馈神经网络](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)（即不使用偏置并单独应用于每个标记向量），在两层之间具有非线性。最初，这个非线性是[修正线性单元（ReLU）](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)激活函数。然而，最近的工作[15]揭示了这并不是最佳选择。
- en: '![](../Images/afe56fad020420985593a813725bfb32.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afe56fad020420985593a813725bfb32.png)'
- en: (created by author)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: （由作者创建）
- en: In particular, LLaMA (and other LLMs like [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive))
    opt to use a SwiGLU activation function instead, which is formulated in the equation
    above. Here, we define the Swish activation as follows.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，LLaMA（以及其他像[PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)的LLM）选择使用SwiGLU激活函数，这在上面的方程中进行了公式化。这里，我们将Swish激活定义如下。
- en: '![](../Images/90aee2465530c1cdc210d5042ea754a2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90aee2465530c1cdc210d5042ea754a2.png)'
- en: (created by author)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: （由作者创建）
- en: SwiGLU is an element-wise product of two [linear transformations](https://mathworld.wolfram.com/LinearTransformation.html)
    of the input `x`, one of which has had a Swish activation applied to it. This
    activation function requires three matrix multiplications, but it has been found
    to yield improvements in performance relative to other activation functions, *even
    when the amount of compute being used is held constant*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: SwiGLU是输入`x`的两个[线性变换](https://mathworld.wolfram.com/LinearTransformation.html)的逐元素乘积，其中一个已应用Swish激活函数。该激活函数需要三个矩阵乘法，但发现相较于其他激活函数，它在性能上有所提升，*即使在保持计算量不变的情况下*。
- en: Rematerialization (or Recomputation)
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 再材料化（或重新计算）
- en: Rematerialization, also known as recomputation, is a technique used in the training
    of LLMs (and other large neural networks) to reduce memory consumption at the
    cost of additional computation. Typically, when we compute the forward pass of
    a neural network, we will store/retain the network’s activations at each layer
    so that they can be used during the backward pass (this is necessary to [compute
    the weight update](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)!).
    But, this requires a lot of memory!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 重计算，也称为再计算，是一种在训练LLM（以及其他大型神经网络）时使用的技术，旨在减少内存消耗，但需要额外的计算。通常，当我们计算神经网络的前向传递时，我们会在每一层存储/保留网络的激活值，以便在反向传递时使用（这对于[计算权重更新](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)是必要的！）。但这需要大量的内存！
- en: '![](../Images/2e078d00e512916930c352ce62014f96.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e078d00e512916930c352ce62014f96.png)'
- en: Schematic of a neural network’s forward and backward pass (created by author)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络前向和反向传递的示意图（作者创作）
- en: The basic idea of rematerialization is to recompute certain intermediate activation
    values during the backward pass rather than storing them in memory during the
    forward pass. This can help reduce the peak memory usage during training, allowing
    for the training of larger models or the use of larger batch sizes within the
    available memory constraints. This is especially important for LLMs given that
    they are large and consume a ton of memory.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 重计算的基本思想是在反向传递期间重新计算某些中间激活值，而不是在前向传递期间将它们存储在内存中。这有助于减少训练过程中的峰值内存使用，从而允许在可用内存限制内训练更大的模型或使用更大的批次大小。考虑到LLM体积庞大且消耗大量内存，这一点尤为重要。
- en: The LLaMA Suite
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLaMA套件
- en: 'Now that we have some useful concepts under our belt, let’s learn more about
    the collection of LLMs within LLaMA and how they work. Because these models are
    heavily inspired by the pre-training strategy proposed by [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms)
    (TL;DR: just pre-training smaller LLMs over a lot more data) [3], we will briefly
    overview these ideas prior to taking a deeper look at LLaMA. Overall, LLaMA heavily
    questions the trend toward massive LLMs, claiming that (if enough pre-training
    is performed!) much smaller LLMs can achieve impressive performance at a significantly
    lower inference budget.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了一些有用的概念，让我们了解一下LLaMA中的LLM集合及其工作原理。由于这些模型受到[Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms)（TL;DR：仅通过对更大量数据进行小型LLM的预训练）[3]提出的预训练策略的强烈启发，我们将在深入了解LLaMA之前简要概述这些思想。总体而言，LLaMA质疑了大规模LLM的趋势，声称（如果进行足够的预训练！）更小的LLM可以在显著较低的推理预算下实现令人印象深刻的性能。
- en: How do we maximize LLM efficiency?
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何最大化LLM的效率？
- en: One especially notable moment in the lineage of recent LLMs was the proposal
    of [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms)
    [3]. After GPT-3, the deep learning research community was astounded by the emergence
    of impressive few-shot learning capabilities in sufficiently-large language models.
    As a result, we began to test models that were [even larger than GPT-3](https://cameronrwolfe.substack.com/i/91134599/scaling-language-models-methods-analysis-and-insights-from-training-gopher).
    But, the results weren’t that great!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最近LLM的发展历程中，一个特别值得注意的时刻是[Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms)
    [3]的提出。在GPT-3之后，深度学习研究界对大规模语言模型中显著的少样本学习能力感到震惊。因此，我们开始测试[比GPT-3还要大](https://cameronrwolfe.substack.com/i/91134599/scaling-language-models-methods-analysis-and-insights-from-training-gopher)的模型。但结果并不理想！
- en: “Recent work from Hoffmann et al. (2022) shows that, for a given compute budget,
    the best performances are not achieved by the largest models, but by smaller models
    trained on more data.” *— from [1]*
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “Hoffmann等人（2022年）的最新研究表明，对于给定的计算预算，最佳性能并非由最大的模型实现，而是由在更多数据上训练的小型模型实现的。” *—
    引自 [1]*
- en: To create LLMs that were much better than GPT-3, we couldn’t just use larger
    models. Rather, we needed a lot more pre-training data! Namely, the analysis from
    Chinchilla demonstrated that higher levels of performance were possible if we
    pre-trained slightly smaller LLMs more extensively.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建比GPT-3更优秀的LLM，我们不能仅仅使用更大的模型。相反，我们需要更多的预训练数据！具体来说，Chinchilla的分析表明，如果我们对稍小的LLM进行更广泛的预训练，可以实现更高的性能。
- en: '**Is this the full picture?** Despite knowing that smaller LLMs can perform
    well if pre-trained extensively, even analysis performed in [3] suggests that
    training relatively larger LLMs is the most efficient way to reach a high level
    of performance. This claim is completely true, but it only considers training
    efficiency. Thus, we have to ask ourselves the question: *is training efficiency
    all that we care about?* For most practitioners, the answer to this question is
    undoubtedly no!'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: “The focus of this work is to train a series of language models that achieve
    the best possible performance at various inference budgets, by training on more
    tokens than what is typically used.” *— from [1]*
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The cost of training is only a small part of the full cost associated with an
    LLM. We also have to worry about hosting, making inference budget a huge consideration.
    LLaMA embraces this idea by emphasizing that, given a target level of performance,
    pre-training a smaller LLM for longer will ultimately be cheaper during inference
    and save a lot of cost over time. Although we might use a larger model if we need
    the performance boost, we should minimize model size as much as possible (and
    thus decrease hosting costs) via extensive pre-training.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Components of LLaMA
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/254ece69b83f8899dc9990c86c7c44d8.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset.** We know that the pre-training dataset for LLaMA is based upon
    public data, *but where exactly does this data come from?* The contents of the
    pre-training dataset used for LLaMA are outlined above. As can be seen, the pre-training
    data (despite being completely public) has quite a bit of diversity, with sources
    ranging from [StackExchange](https://stackexchange.com/) to the [Gutenberg Project](https://www.gutenberg.org/).
    The full dataset contains roughly 1.4T tokens after being tokenized. This is the
    same number of tokens over which Chinchilla [3] was pre-trained; see below.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca9e9d07ecee7b65806f82dd08ee5c71.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: (from [3])
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'Given LLaMA’s emphasis on transparency and repeatability, a ton of insight
    is provided in [1] regarding the construction of the pre-training dataset. One
    of the most interesting aspects of this discussion is that we can use it to learn
    more about how data is filtered prior to pre-training an LLM. For example, textual
    data from [CommonCrawl](https://commoncrawl.org/) is filtered to exclude:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: duplicate documents (using the [CCNet pipeline](https://github.com/facebookresearch/cc_net)
    [7])
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: non-English data (by training a [fastText](https://fasttext.cc/) linear classifier)
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: low-quality content (using an [n-gram language model](https://web.stanford.edu/~jurafsky/slp3/3.pdf))
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plus, authors in [1] train a linear classifier to distinguish pages used as
    references in Wikipedia from randomly sampled pages, then discard pages that are
    not classified as references. All of these steps were taken just for filtering
    CommonCrawl! From prior work, we know that correct filtering of the pre-training
    dataset [is essential to LLM performance](https://cameronrwolfe.substack.com/i/110291340/searching-for-a-better-approach).
    In [1], we get more insight into the specifics of implementing an effective filtering
    pipeline.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，文献[1]中的作者训练了一个线性分类器，用于区分维基百科中作为参考的页面和随机采样的页面，然后丢弃那些未被分类为参考的页面。这些步骤都仅仅是为了过滤CommonCrawl！从之前的工作中，我们知道，正确过滤预训练数据集[对LLM性能至关重要](https://cameronrwolfe.substack.com/i/110291340/searching-for-a-better-approach)。在[1]中，我们深入了解了实现有效过滤管道的具体细节。
- en: '![](../Images/779405bd7a9166f77a5e07ab7ff28efc.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/779405bd7a9166f77a5e07ab7ff28efc.png)'
- en: Pre-normalization structure within a transformer block (created by author)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在变压器块内的预归一化结构（由作者创建）
- en: '**Architecture.** The LLaMA suite adopts a lot of common architectural tricks
    from popular LLMs like [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)
    [4] and [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)
    [5]. For example, LLaMA performs pre-normalization within each of its layers,
    meaning that normalization is applied to the input of each layer within the transformer
    instead of the output; see above. Additionally, RMSNorm, [SwiGLU activation functions](https://cameronrwolfe.substack.com/i/104244919/architectural-modifications),
    and [rotary positional embeddings (RoPE)](https://cameronrwolfe.substack.com/i/104244919/architectural-modifications)
    [10] (i.e., a sort of hybrid between [absolute](https://cameronrwolfe.substack.com/i/76273144/berts-architecture)
    and [relative positional embeddings](https://jaketae.github.io/study/relative-positional-encoding/))
    are used in every transformer layer.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构。** LLaMA套件采用了来自流行LLM（如[GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)
    [4]和[PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)
    [5]）的许多常见架构技巧。例如，LLaMA在其每一层内执行预归一化，这意味着归一化应用于每一层的输入，而不是输出；见上文。此外，RMSNorm、[SwiGLU激活函数](https://cameronrwolfe.substack.com/i/104244919/architectural-modifications)和[旋转位置嵌入（RoPE）](https://cameronrwolfe.substack.com/i/104244919/architectural-modifications)
    [10]（即，[绝对位置嵌入](https://cameronrwolfe.substack.com/i/76273144/berts-architecture)和[相对位置嵌入](https://jaketae.github.io/study/relative-positional-encoding/)的某种混合形式）在每个变压器层中使用。'
- en: '![](../Images/c4ec8616d72060e22d68747343fd0283.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4ec8616d72060e22d68747343fd0283.png)'
- en: (from [1])
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: In [1], four different sizes of models are used, ranging from 6.7 billion parameters
    to 65.2 billion parameters; see above. These models form the collection of LLMs
    known as LLaMA and provide a variety of different tradeoffs between performance
    and model size or inference budget. Most notably, we will see that LLaMA-13B performs
    competitively with GPT-3 *and can be run on a single V100 GPU*. Compared to prior
    models, this is a huge accomplishment and makes the models way more accessible
    to most practitioners (e.g., PaLM is trained using >6K accelerators).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在[1]中，使用了四种不同规模的模型，参数量从67亿到652亿；见上文。这些模型形成了被称为LLaMA的LLM集合，并提供了在性能与模型大小或推理预算之间的各种权衡。最显著的是，我们将看到LLaMA-13B在性能上与GPT-3竞争力强，*并且可以在单个V100
    GPU上运行*。与之前的模型相比，这是一个巨大的成就，使得这些模型对大多数从业者更加可及（例如，PaLM使用超过6000个加速器进行训练）。
- en: '**Better efficiency.** Authors in [1] adopt some interesting tricks to improve
    LLM training efficiency. First, we should recall that modern LLMs, based upon
    [decoder-only transformer models](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers),
    use causal [multi-headed attention](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20)
    within each of their layers. To improve the efficiency of this causal multi-head
    attention operation, LLaMA uses an efficient implementation that does not *i)*
    store attention weights or *ii)* compute key/query scores for tokens that are
    masked. By doing this, we can save a lot of computation that is typically wasted
    on masked tokens not considered by causal self-attention. Such an approach is
    inspired by ideas in [9], but we can find an open-source implementation in the
    [xformers library](https://github.com/facebookresearch/xformers).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**更高效的性能。** [1] 的作者采用了一些有趣的技巧来提高LLM训练效率。首先，我们应该记住，基于 [decoder-only transformer
    models](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers)
    的现代LLM在每一层中使用因果 [multi-headed attention](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20)。为了提高这种因果多头注意力操作的效率，LLaMA
    使用了一种高效的实现方式，该方式既不 *i)* 存储注意力权重，也不 *ii)* 计算被掩盖的令牌的键/查询分数。通过这样做，我们可以节省大量通常浪费在因果自注意力中未考虑的被掩盖令牌上的计算。这种方法受到
    [9] 中思想的启发，但我们可以在 [xformers library](https://github.com/facebookresearch/xformers)
    中找到一个开源实现。'
- en: Beyond an efficient causal self-attention implementation, LLaMA approaches rematerialization
    a bit differently compared to most LLM training strategies. The most expensive
    activations to compute (e.g., the output of linear layers) are saved during the
    forward pass, thus allowing the number of activations re-computed during the backward
    pass to be reduced. This change, which requires the LLM’s backward pass to be
    manually reimplemented (instead of using [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
    in PyTorch) and is a sort of hybrid rematerialization approach, significantly
    improves overall training throughput.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了高效的因果自注意力实现之外，LLaMA 在重新物化方面的处理方式与大多数LLM训练策略有所不同。计算成本最高的激活（例如线性层的输出）在前向传播过程中被保存，从而减少了在反向传播过程中重新计算的激活数量。这一变化需要手动重新实现LLM的反向传播（而不是使用
    [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
    进行自动微分），是一种混合的重新物化方法，显著提高了整体训练吞吐量。
- en: “When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU
    on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing
    1.4T tokens takes approximately 21 days.” *— from [1]*
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在训练一个 65B 参数的模型时，我们的代码在 2048 A100 GPU 上处理约 380 令牌/秒/ GPU。这意味着对我们包含 1.4T 令牌的数据集的训练大约需要
    21 天。” *— 摘自 [1]*
- en: 'Given the modifications that LLaMA adopts to improve training efficiency, we
    might be wondering: *how much faster does this actually make training?* Well,
    it depends a lot on the training infrastructure. When using 2048 [A100 GPUs](https://www.nvidia.com/en-us/data-center/a100/),
    however, the LLaMA-65B takes roughly 21 days to complete pre-training over 1.4T
    tokens.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 LLaMA 采用的改进以提高训练效率，我们可能会想知道：*这实际上使训练快了多少？* 好吧，这在很大程度上取决于训练基础设施。然而，当使用 2048
    [A100 GPUs](https://www.nvidia.com/en-us/data-center/a100/) 时，LLaMA-65B 完成对 1.4T
    令牌的预训练大约需要 21 天。
- en: LLaMA vs. SOTA LLMs
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLaMA 与 SOTA LLMs
- en: While open-source and repeatability is great, no one will care about LLaMA unless
    the models perform well! Prior attempts at open-source LLMs have been made (e.g.,
    [OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)
    and [BLOOM](https://huggingface.co/bigscience/bloom) [11, 12]). But, these models
    are not competitive with modern LLMs in terms of performance. Within this section,
    we’ll analyze the performance of LLaMA models relative to popular LLMs like [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)
    and [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)
    [4, 5].
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然开源和可重复性很重要，但如果模型表现不佳，没有人会关心 LLaMA！此前也有尝试开源LLM（例如 [OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)
    和 [BLOOM](https://huggingface.co/bigscience/bloom) [11, 12]）。但这些模型在性能上无法与现代LLM竞争。在这一部分，我们将分析
    LLaMA 模型相对于流行LLM，如 [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)
    和 [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)
    [4, 5] 的表现。
- en: '**How do we evaluate?** As has been described extensively in [prior posts](https://cameronrwolfe.substack.com/i/91134599/a-primer-on-language-modeling),
    LLaMA is evaluated similarly to most language foundation models: via zero and
    few-shot learning. Notably, LLaMA models are solely evaluated as pre-trained foundation
    models, meaning that no fine-tuning is performed (either via [SFT or RLHF](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior)).
    LLaMA is compared to popular, closed-source LLMs (e.g., [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners),
    [Gopher](https://cameronrwolfe.substack.com/i/91134599/scaling-language-models-methods-analysis-and-insights-from-training-gopher),
    [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms),
    and [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)
    [3, 4, 5, 13]) and prior open-source LLMs (e.g., [OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15),
    [GPT-J](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/), and [GPT-Neo](https://github.com/EleutherAI/gpt-neo)
    [11, 14]) on both free-form generation and multiple choice-based tasks. A variety
    of domains are tested (e.g., common sense and mathematical reasoning, code generation,
    question answering, etc.).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们如何进行评估？** 如 [之前的帖子](https://cameronrwolfe.substack.com/i/91134599/a-primer-on-language-modeling)
    中详细描述的那样，LLaMA 的评估与大多数语言基础模型类似：通过零样本和少样本学习。值得注意的是，LLaMA 模型仅作为预训练的基础模型进行评估，这意味着没有进行微调（无论是通过
    [SFT 还是 RLHF](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior)）。LLaMA
    与流行的闭源 LLMs（例如，[GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)，[Gopher](https://cameronrwolfe.substack.com/i/91134599/scaling-language-models-methods-analysis-and-insights-from-training-gopher)，[Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms)
    和 [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)
    [3, 4, 5, 13]）以及以前的开源 LLMs（例如，[OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)，[GPT-J](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)，和
    [GPT-Neo](https://github.com/EleutherAI/gpt-neo) [11, 14]）在自由生成和多项选择任务上进行比较。测试了各种领域（例如，常识和数学推理、代码生成、问答等）。'
- en: '![](../Images/fae6f9bd4542f168f0d2d3084b1777ac.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fae6f9bd4542f168f0d2d3084b1777ac.png)'
- en: (from [1])
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: '**Language understanding.** On closed-book question answering and reading comprehension
    tasks, we see that LLaMA-65B achieves state-of-the-art zero and few-shot performance,
    consistently surpassing the performance of much larger models like PaLM and Chinchilla.
    Going further, LLaMA-13B performs surprisingly well and even improves upon the
    performance of GPT-3 (which is 10X larger!) in most cases. The basic takeaway
    here is that *i)* larger LLaMA models are competitive with state-of-the-art and
    *ii)* smaller LLaMA models perform surprisingly well for their size.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言理解。** 在闭卷问答和阅读理解任务中，我们看到 LLaMA-65B 达到了最先进的零样本和少样本表现，一贯超过了像 PaLM 和 Chinchilla
    这样更大模型的表现。进一步来看，LLaMA-13B 表现令人惊讶，甚至在大多数情况下超越了 GPT-3（它大 10 倍！）的表现。这里的基本结论是 *i)*
    更大的 LLaMA 模型与最先进的技术具有竞争力，*ii)* 更小的 LLaMA 模型在其尺寸范围内表现出令人惊讶的效果。'
- en: '![](../Images/1c3f40eca45c2e2aae0e1c5e9f79a83e.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c3f40eca45c2e2aae0e1c5e9f79a83e.png)'
- en: (from [1])
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: '**Reasoning tasks.** The LLaMA suite is also evaluated on common sense and
    mathematical reasoning tasks. On common sense reasoning tasks, LLaMA surpasses
    the zero-shot reasoning performance of several powerful baselines; see above.
    However, it should be noted here that no special prompting approaches (e.g., [chain-of-thought
    prompting](https://cameronrwolfe.substack.com/i/104244919/how-does-palm-perform))
    are adopted to facilitate improved reasoning. Prior work [5] has shown that the
    ability of LLMs to “reason” may degrade with scale without the correct prompting
    approach.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**推理任务。** LLaMA 套件还在常识和数学推理任务上进行了评估。在常识推理任务中，LLaMA 超越了多个强大基准的零样本推理表现；请参见上文。然而，这里需要注意的是，没有采用特殊的提示方法（例如，[思维链提示](https://cameronrwolfe.substack.com/i/104244919/how-does-palm-perform)）来促进推理的改进。以往的工作
    [5] 已经表明，如果没有正确的提示方法，LLMs 的“推理”能力可能会随着规模的增加而退化。'
- en: '![](../Images/095a66877bffd9cd5b0e57c68fae6ef3.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/095a66877bffd9cd5b0e57c68fae6ef3.png)'
- en: (from [1])
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: Despite the limitations of this analysis, LLaMA’s reasoning abilities still
    seem relatively impressive compared to baselines. Namely, LLaMA models perform
    competitively with (and even better than in some cases) several baselines on mathematical
    reasoning datasets. In fact, LLaMA-65B even outperforms a similarly-sized [Minerva
    model](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html),
    which has been explicitly fine-tuned on mathematical data to improve its performance
    on such tasks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种分析存在局限性，LLaMA 的推理能力相比于基线模型仍然显得相对令人印象深刻。即，LLaMA 模型在数学推理数据集上的表现与（甚至在某些情况下优于）多个基线模型竞争。实际上，LLaMA-65B
    甚至超越了一个类似规模的 [Minerva 模型](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html)，该模型已明确在数学数据上进行微调，以提高在这类任务上的表现。
- en: “Minerva is a series of PaLM models finetuned on 38.5B tokens extracted from
    ArXiv and Math Web Pages… On GSM8k, we observe that LLaMA65B outperforms Minerva-62B,
    although it has not been fine-tuned on mathematical data.” *— from [1]*
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “Minerva 是一系列基于 38.5B 令牌的 PaLM 模型，这些令牌来自 ArXiv 和数学网页… 在 GSM8k 上，我们观察到 LLaMA65B
    超越了 Minerva-62B，尽管它没有在数学数据上进行微调。” *— 来源于 [1]*
- en: '**code generation.** Beyond basic reasoning capabilities, code generation is
    another skill of LLaMA models. Despite never fine-tuning on code (i.e., code accounts
    for <5% of LLaMA’s pre-training data), LLaMA-65B outperforms PaLM on code generation
    tasks and LLaMA-13B surpasses the code generation performance of GPT-3 (but… GPT-3
    is admittedly poor at generating code).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码生成。** 除了基本的推理能力，代码生成是 LLaMA 模型的另一个技能。尽管从未在代码上进行过微调（即代码数据占 LLaMA 预训练数据的
    <5%），但 LLaMA-65B 在代码生成任务上超越了 PaLM，而 LLaMA-13B 的代码生成表现也超过了 GPT-3（不过… GPT-3 确实在生成代码方面表现较差）。'
- en: '![](../Images/5557ef85d7a85e3510d0288674920973.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5557ef85d7a85e3510d0288674920973.png)'
- en: (from [1])
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1]）
- en: '**Other details.** On the [MMLU benchmark](https://arxiv.org/abs/2009.03300),
    LLaMA models lag behind the performance of LLMs like Chinchilla and PaLM in most
    cases. *This benchmark is one of the only cases where LLaMA performance is noticeably
    surpassed by current alternatives*. Authors in [1] claim this degradation in performance
    is due to the limited number of books and academic papers in the LLaMA pre-training
    dataset (i.e., >10X decrease in this kind of pre-training data compared to state-of-the-art
    LLMs).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他细节。** 在 [MMLU 基准测试](https://arxiv.org/abs/2009.03300)中，LLaMA 模型在大多数情况下表现落后于像
    Chinchilla 和 PaLM 这样的 LLM。*这是 LLaMA 性能被当前替代方案明显超越的唯一基准测试之一*。在 [1] 中的作者声称，这种性能下降是由于
    LLaMA 预训练数据集中图书和学术论文的数量有限（即，与最先进的 LLM 相比，这类预训练数据减少了 >10 倍）。'
- en: '![](../Images/401a0b60d2f9b58bdd6c38f21798e6cb.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/401a0b60d2f9b58bdd6c38f21798e6cb.png)'
- en: (from [1])
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1]）
- en: When the performance of LLaMA models is tracked throughout the pre-training
    process, we observe a clear, steady improvement in performance throughout the
    pre-training process; see above. Although not all tasks behave similarly, we can
    see that the pre-training strategy adopted by LLaMA is relatively stable overall.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当跟踪 LLaMA 模型在预训练过程中的性能时，我们观察到性能有明显而稳定的提升；见上文。虽然并非所有任务的表现都相同，但我们可以看到 LLaMA 采用的预训练策略总体上是相对稳定的。
- en: Takeaways
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键点
- en: 'To make a long story short, LLaMA is an open-source LLM with shockingly good
    performance. Since the proposal of LLaMA, the research community has already made
    good use of such an impressive model being openly-available. As an example, the
    following research efforts have already extended upon LLaMA:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 长话短说，LLaMA 是一个开源的 LLM，表现令人震惊。自从 LLaMA 提出的提案以来，研究社区已经很好地利用了这一开放的出色模型。举个例子，以下研究工作已经在
    LLaMA 的基础上进行了扩展：
- en: '*Vicuna*: fine-tuned version of LLaMA with performance (almost) comparable
    to GPT-4 [[link](https://vicuna.lmsys.org/)]'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Vicuna*: LLaMA 的微调版本，性能（几乎）可比于 GPT-4 [[link](https://vicuna.lmsys.org/)]'
- en: '*Koala*: LLaMA fine-tuned on internet dialog data [[link](https://bair.berkeley.edu/blog/2023/04/03/koala/)]'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Koala*: 在互联网对话数据上微调的 LLaMA [[link](https://bair.berkeley.edu/blog/2023/04/03/koala/)]'
- en: '*ChatLLaMA*: create a personalized version of ChatGPT on you own data with
    minimal compute [[link](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama)]'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ChatLLaMA*: 使用最少的计算资源在你自己的数据上创建个性化的 ChatGPT [[link](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama)]'
- en: 'ColossalChat: model similar to ChatGPT with an RLHF pipeline based upon LLaMA
    [[link](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)]'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ColossalChat：一个与ChatGPT类似的模型，基于LLaMA的RLHF管道[[链接](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)]
- en: LLaMA’s impact is likely to significantly increase. Personally, I’m incredibly
    excited to see research on open LLMS continue to progress. I hope that making
    these models more accessible will lead to more thorough investigation and development
    from the broader research community. Some basic takeaways are given below.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA的影响力可能会显著增加。个人而言，我非常兴奋看到开源LLMS的研究不断进展。我希望让这些模型变得更加可访问会带来更广泛的研究社区的深入调查和发展。以下是一些基本要点。
- en: '**Open-source LLMs.** Right now, the LLM ecosystem is witnessing an interesting
    conflict, in which two different approaches are being used to surface these powerful
    foundation models to the public. On one hand, models like [ChatGPT](https://openai.com/blog/chatgpt)
    and [GPT-4](https://openai.com/research/gpt-4) are being solely released behind
    [paid APIs](https://openai.com/blog/introducing-chatgpt-and-whisper-apis), preventing
    detailed access of such models to the research community. Contributions like LLaMA
    go against this trend by providing full model access to the research community.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**开源LLM。** 目前，LLM生态系统正经历一种有趣的冲突，两种不同的方法被用来将这些强大的基础模型展示给公众。一方面，像[ChatGPT](https://openai.com/blog/chatgpt)和[GPT-4](https://openai.com/research/gpt-4)这样的模型仅通过[付费API](https://openai.com/blog/introducing-chatgpt-and-whisper-apis)发布，这限制了研究社区对这些模型的详细访问。LLaMA等贡献则与这一趋势相反，通过向研究社区提供完整的模型访问权限。'
- en: '**What size is best?** Rather than releasing a single model, LLaMA provides
    a collection of LLMs with different sizes. Prior research on LLMs tends to promote
    the use of larger models, as larger LLMs tend to reach impressive levels of performance
    with less overall compute costs during training. However, if we pre-train a smaller
    model more extensively, LLaMA shows that we can reach comparable levels of performance
    while achieving significant reductions in inference cost. As such, it makes sense
    to (at least) consider the use of smaller LLMs, especially when we have to deploy
    them. Notably, some of the LLaMA models can be run on a single GPU, which drastically
    improves accessibility of such LLMs.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么大小最合适？** LLaMA提供了一系列不同大小的LLM，而不是发布单一模型。之前的LLM研究往往倾向于使用更大的模型，因为较大的LLM通常在训练时能以较少的总体计算成本达到令人印象深刻的性能水平。然而，如果我们对较小的模型进行更广泛的预训练，LLaMA表明我们可以在显著降低推理成本的同时达到可比的性能水平。因此，至少考虑使用较小的LLM是合理的，特别是在我们需要部署它们的时候。值得注意的是，一些LLaMA模型可以在单个GPU上运行，这大大提高了这些LLM的可访问性。'
- en: '**Impressive performance.** Prior to the proposal of LLaMA, many research groups
    attempted to release open-source versions of popular LLMs (e.g., [OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)
    is basically an open-source GPT-3). But, these models perform much worse than
    paid models accessible via APIs. Although LLaMA falls short of optimal performance
    in some cases, it is a huge step forward, as it often outperforms popular, state-of-the-art
    LLMs (depending on the size of model being used).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**令人印象深刻的表现。** 在LLaMA提出之前，许多研究小组尝试发布流行LLM的开源版本（例如，[OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)
    基本上是一个开源的GPT-3）。但这些模型的表现远不如通过API访问的付费模型。尽管LLaMA在某些情况下的性能不尽如人意，但它是一个巨大的进步，因为它通常超越了流行的、最先进的LLM（取决于使用的模型大小）。'
- en: Closing Remarks
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. You can also check out my [other
    writings](https://medium.com/@wolfecameron) on medium! If you liked it, please
    follow me on [twitter](https://twitter.com/cwolferesearch) or subscribe to my
    [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/), where
    I help readers build a deeper understanding of topics in AI research via understandable
    overviews of popular papers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢阅读这篇文章。我是 [卡梅伦·R·沃尔夫](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的 AI 总监。我研究深度学习的经验性和理论基础。你也可以查看我在 medium 上的 [其他写作](https://medium.com/@wolfecameron)！如果你喜欢，请在
    [twitter](https://twitter.com/cwolferesearch) 上关注我或订阅我的 [深度（学习）焦点通讯](https://cameronrwolfe.substack.com/)，我通过易懂的热门论文概述帮助读者更深入地理解
    AI 研究领域的主题。
- en: Bibliography
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.”
    *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 图弗龙, 休戈 等. “Llama: 开放且高效的基础语言模型.” *arXiv 预印本 arXiv:2302.13971* (2023).'
- en: '[2] Vaswani, Ashish, et al. “Attention is all you need.” *Advances in neural
    information processing systems* 30 (2017).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 瓦斯瓦尼, 阿希什 等. “注意力是你所需要的一切.” *神经信息处理系统进展* 30 (2017).'
- en: '[3] Hoffmann, Jordan, et al. “Training compute-optimal large language models.”
    *arXiv preprint arXiv:2203.15556* (2022).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 霍夫曼, 乔丹 等. “训练计算最优的大型语言模型.” *arXiv 预印本 arXiv:2203.15556* (2022).'
- en: '[4] Brown, Tom, et al. “Language models are few-shot learners.” *Advances in
    neural information processing systems* 33 (2020): 1877–1901.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 布朗, 汤姆 等. “语言模型是少量学习者.” *神经信息处理系统进展* 33 (2020): 1877–1901.'
- en: '[5] Chowdhery, Aakanksha, et al. “Palm: Scaling language modeling with pathways.”
    *arXiv preprint arXiv:2204.02311* (2022).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 周德瑞, 阿坎卡莎 等. “Palm: 通过路径扩展语言建模.” *arXiv 预印本 arXiv:2204.02311* (2022).'
- en: '[6] OpenAI (2023). “GPT-4 Technical Report.” *ArXiv, abs/2303.08774*.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] OpenAI (2023). “GPT-4 技术报告.” *ArXiv, abs/2303.08774*.'
- en: '[7] Wenzek, Guillaume, et al. “CCNet: Extracting high quality monolingual datasets
    from web crawl data.” *arXiv preprint arXiv:1911.00359* (2019).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] 温泽克, 纪尧姆 等. “CCNet: 从网页抓取数据中提取高质量单语数据集.” *arXiv 预印本 arXiv:1911.00359* (2019).'
- en: '[8] Zhang, Biao, and Rico Sennrich. “Root mean square layer normalization.”
    *Advances in Neural Information Processing Systems* 32 (2019).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 张彪 和 里科·森里奇. “均方根层归一化.” *神经信息处理系统进展* 32 (2019).'
- en: '[9] Rabe, Markus N., and Charles Staats. “Self-attention Does Not Need $ O
    (n^ 2) $ Memory.” *arXiv preprint arXiv:2112.05682* (2021).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 拉贝, 马库斯·N. 和 查尔斯·斯塔茨. “自注意力不需要 $ O (n^ 2) $ 记忆.” *arXiv 预印本 arXiv:2112.05682*
    (2021).'
- en: '[10] Su, Jianlin, et al. “Roformer: Enhanced transformer with rotary position
    embedding.” *arXiv preprint arXiv:2104.09864* (2021).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 苏, 剑林 等. “Roformer: 带有旋转位置嵌入的增强型变换器.” *arXiv 预印本 arXiv:2104.09864* (2021).'
- en: '[11] Zhang, Susan, et al. “Opt: Open pre-trained transformer language models.”
    *arXiv preprint arXiv:2205.01068* (2022).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] 张, 苏珊 等. “Opt: 开放预训练变换器语言模型.” *arXiv 预印本 arXiv:2205.01068* (2022).'
- en: '[12] Scao, Teven Le, et al. “Bloom: A 176b-parameter open-access multilingual
    language model.” *arXiv preprint arXiv:2211.05100* (2022).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Scao, Teven Le 等. “Bloom: 一个 176b 参数的开放访问多语言模型.” *arXiv 预印本 arXiv:2211.05100*
    (2022).'
- en: '[13] Rae, Jack W., et al. “Scaling language models: Methods, analysis & insights
    from training gopher.” *arXiv preprint arXiv:2112.11446* (2021).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 雷, 杰克·W. 等. “扩展语言模型: 方法, 分析与训练 Gopher 的见解.” *arXiv 预印本 arXiv:2112.11446*
    (2021).'
- en: '[14] Black, Sid, et al. “Gpt-neox-20b: An open-source autoregressive language
    model.” *arXiv preprint arXiv:2204.06745* (2022).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] 布莱克, 西德 等. “Gpt-neox-20b: 一个开源自回归语言模型.” *arXiv 预印本 arXiv:2204.06745* (2022).'
- en: '[15] Shazeer, Noam. “Glu variants improve transformer.” *arXiv preprint arXiv:2002.05202*
    (2020).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] 沙泽尔, 诺亚姆. “Glu 变体改进变换器.” *arXiv 预印本 arXiv:2002.05202* (2020).'
- en: '[16] Zhang, Biao, and Rico Sennrich. “Root mean square layer normalization.”
    *Advances in Neural Information Processing Systems* 32 (2019).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] 张彪 和 里科·森里奇. “均方根层归一化.” *神经信息处理系统进展* 32 (2019).'
