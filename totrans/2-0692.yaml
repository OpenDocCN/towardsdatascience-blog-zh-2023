- en: Deep Deterministic Policy Gradients (DDPG) Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度（DDPG）解释
- en: 原文：[https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-4643c1f71b2e](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-4643c1f71b2e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-4643c1f71b2e](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-4643c1f71b2e)
- en: A gradient-based reinforcement learning algorithm to learn deterministic policies
    for continuous action spaces
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种基于梯度的强化学习算法，用于学习连续动作空间中的确定性策略
- en: '[](https://wvheeswijk.medium.com/?source=post_page-----4643c1f71b2e--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----4643c1f71b2e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4643c1f71b2e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4643c1f71b2e--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----4643c1f71b2e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wvheeswijk.medium.com/?source=post_page-----4643c1f71b2e--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----4643c1f71b2e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4643c1f71b2e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4643c1f71b2e--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----4643c1f71b2e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4643c1f71b2e--------------------------------)
    ·11 min read·Apr 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4643c1f71b2e--------------------------------)
    ·阅读时间 11 分钟·2023年4月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f2b615669a6ed7f8931e91857e17ab23.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2b615669a6ed7f8931e91857e17ab23.png)'
- en: Photo by [Jonathan Ford](https://unsplash.com/@jonfordphotos?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jonathan Ford](https://unsplash.com/@jonfordphotos?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This article introduces Deep Deterministic Policy Gradient (DDPG) — a Reinforcement
    Learning algorithm suitable for **deterministic policies applied in continuous
    action spaces**. By combining the actor-critic paradigm with deep neural networks,
    continuous action spaces can be tackled without resorting to stochastic policies.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了深度确定性策略梯度（DDPG）——一种适用于**连续动作空间中的确定性策略**的强化学习算法。通过将演员-评论家范式与深度神经网络结合，可以在不依赖随机策略的情况下处理连续动作空间。
- en: Especially for continuous control tasks in which randomness in actions is undesirable
    — e.g., robotics or navigation — DDPG might be just the algorithm you need.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其适用于动作中的随机性不受欢迎的连续控制任务——例如，机器人技术或导航——DDPG 可能正是你需要的算法。
- en: How does DDPG fit in the RL landscape?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DDPG 在强化学习领域中适合什么位置？
- en: DDPG displays elements of policy-based methods as well as value-based methods,
    making it a hybrid policy class.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 结合了基于策略的方法和基于价值的方法，形成了一种混合策略类。
- en: '[](/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a?source=post_page-----4643c1f71b2e--------------------------------)
    [## The Four Policy Classes of Reinforcement Learning'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a?source=post_page-----4643c1f71b2e--------------------------------)
    [## 强化学习的四种策略类别'
- en: towardsdatascience.com](/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a?source=post_page-----4643c1f71b2e--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a?source=post_page-----4643c1f71b2e--------------------------------)'
- en: '**Policy gradient methods** like [REINFORCE](https://medium.com/towards-data-science/policy-gradients-in-reinforcement-learning-explained-ecec7df94245),
    [TRPO](https://medium.com/towards-data-science/trust-region-policy-optimization-trpo-explained-4b56bd206fc2),
    and [PPO](/proximal-policy-optimization-ppo-explained-abed1952457b) use stochastic
    policies *π:a~P(a|s)* to explore and compare actions. These methods draw actions
    from a differentiable distribution *P_θ(a|s)*, which enables the computation of
    gradients with respect to *θ*. The inherent randomness in these decisions could
    be undesirable real-world applications. DDPG eliminates this randomness, yielding
    simpler and more predictable policies.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略梯度方法**如[REINFORCE](https://medium.com/towards-data-science/policy-gradients-in-reinforcement-learning-explained-ecec7df94245)、[TRPO](https://medium.com/towards-data-science/trust-region-policy-optimization-trpo-explained-4b56bd206fc2)和[PPO](/proximal-policy-optimization-ppo-explained-abed1952457b)使用随机策略
    *π:a~P(a|s)* 来探索和比较动作。这些方法从可微分分布 *P_θ(a|s)* 中提取动作，从而能够计算相对于 *θ* 的梯度。这些决策中的固有随机性可能在实际应用中不受欢迎。DDPG消除了这种随机性，产生了更简单和更可预测的策略。'
- en: '**Value-based methods** like [SARSA](/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff),
    [Monte Carlo Learning](https://medium.com/towards-data-science/cliff-walking-with-monte-carlo-reinforcement-learning-587e9d3bc4e7),
    and [Deep Q-Learning](/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e)
    are based on deterministic policies that always return a single action given an
    input state. However, these methods assume a finite number of actions, which makes
    evaluating their value functions and selecting the most rewarding actions difficult
    for continuous action spaces with infinitely many actions.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于价值的方法**如[SARSA](/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff)、[蒙特卡罗学习](https://medium.com/towards-data-science/cliff-walking-with-monte-carlo-reinforcement-learning-587e9d3bc4e7)和[深度Q学习](/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e)基于确定性策略，该策略始终根据输入状态返回一个单一的动作。然而，这些方法假设动作的数量是有限的，这使得在具有无限多个动作的连续动作空间中评估它们的价值函数和选择最有回报的动作变得困难。'
- en: As you guessed, Deep Deterministic Policy Gradients fill the gap, incorporating
    **elements of both Deep Q-Learning and policy gradient methods**. DDPG effectively
    handles continuous action spaces and has been successfully applied in robotic
    control- and game-playing tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你猜测的那样，深度确定性策略梯度（DDPG）填补了这一空白，结合了**深度Q学习和策略梯度方法的元素**。DDPG有效地处理连续动作空间，并已成功应用于机器人控制和游戏任务中。
- en: If you’re unfamiliar with policy gradient algorithms (in particular REINFORCE)
    or value-based methods (in particular DQN), it’s recommended to learn about them
    before exploring DDPG.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你不熟悉策略梯度算法（特别是REINFORCE）或基于价值的方法（特别是DQN），建议在探讨DDPG之前先了解它们。
- en: 'DDPG: The critic'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'DDPG: 评论者'
- en: DDPG is remarkably **close to Deep Q-Learning**, sharing both notation and concepts.
    Let’s make a quick journey.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG与深度Q学习**非常接近**，共享了符号和概念。让我们快速了解一下。
- en: DQN for continuous action spaces?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN对连续动作空间的适用性？
- en: In vanilla (i.e., tabular) Q-learning, we use Q-values to approximate the Bellman
    value function *V*. Q-values are defined for every state-action pair and thus
    denoted by *Q(s,a)*. Tabular Q-learning requires a **lookup table** containing
    a Q-value for each pair, thus necessitating a discrete state space and a discrete
    action space.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通（即，表格）Q学习中，我们使用Q值来逼近贝尔曼价值函数 *V*。Q值为每个状态-动作对定义，因此用 *Q(s,a)* 表示。表格Q学习需要一个**查找表**来包含每对的Q值，因此需要离散的状态空间和离散的动作空间。
- en: 'Time to put the *‘deep’* in Deep Reinforcement Learning. Compared to lookup
    tables, introducing a **neural network** has two advantages: (i) it provides a
    single general expression for the entire state space and (ii) by extension, it
    can also handle continuous *state* spaces.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候将 *‘深度’* 融入深度强化学习中了。与查找表相比，引入**神经网络**有两个优点：（i）它为整个状态空间提供了一个通用表达式，（ii）因此，它还可以处理连续的
    *状态* 空间。
- en: Of course, we need to deal with continuous *action* spaces; we thus cannot output
    Q-values for every action. Instead, we provide a single action as *input* and
    compute the **Q-value for the state-action pair** (a procedure also known as *naive
    DQN*)**.** In mathematical terms, we can represent the network as Q*:(s,a)→Q(s,a),*
    i.e., outputting a single Q- value for a given state-action pair.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们需要处理连续的 *动作* 空间；因此我们不能为每个动作输出Q值。相反，我们提供一个动作作为 *输入* 并计算**状态-动作对的Q值**（这个过程也称为*简单DQN*）。用数学术语来说，我们可以将网络表示为Q*:(s,a)→Q(s,a)*，即为给定的状态-动作对输出一个单一的Q值。
- en: The corresponding critic network looks as follows.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的评论员网络如下所示。
- en: '![](../Images/498549fcd89c8b20d14a7cc450afd5ae.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/498549fcd89c8b20d14a7cc450afd5ae.png)'
- en: Example of a critic network Q:(s,a) in DDPG. The network takes both the state
    vector and action vector as input, and outputs a single Q-value [image by author]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG中的评论员网络Q:(s,a)示例。网络同时接受状态向量和动作向量作为输入，并输出一个Q值[图片由作者提供]
- en: 'Although offering generalization, the neural network introduces some stability
    issues. Being a single representation for *all* states, each update also affects
    *all* Q-values. As observation tuples *(s,a,r,s’)* are collected sequentially,
    there tends to be a **high temporal correlation** between them that makes overfitting
    all too likely. Without going into too much detail here, the following three techniques
    are needed to properly train the value network:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提供了泛化能力，神经网络也引入了一些稳定性问题。作为*所有*状态的单一表示，每次更新也会影响*所有* Q 值。由于观察元组*(s,a,r,s’)*是顺序收集的，它们之间往往存在**高时间相关性**，使得过拟合变得非常可能。在这里不深入细节，正确训练价值网络需要以下三种技术：
- en: '**Experience replay**: Sample observations *(s,a,r,s’)* from experience buffer,
    breaking the correlation between subsequently collected tuples.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经验回放**：从经验缓冲区中采样观察数据*(s,a,r,s’)*，打破随后收集的元组之间的相关性。'
- en: '**Batch learning**: Train value network with batches of observations, making
    for more reliable and impactful updates.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量学习**：用观察数据批次训练价值网络，使更新更可靠且有影响力。'
- en: '**Target networks**: Use a different network to compute *Q(s’,a’)* than *Q(s,a)*,
    reducing correlation between expectation and observation.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标网络**：使用不同的网络来计算*Q(s’,a’)*与*Q(s,a)*，减少期望与观察之间的相关性。'
- en: '[](/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172?source=post_page-----4643c1f71b2e--------------------------------)
    [## How To Model Experience Replay, Batch Learning and Target Networks'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[如何建模经验回放、批量学习和目标网络](https://towardsdatascience.com/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172?source=post_page-----4643c1f71b2e--------------------------------)
    [## 如何建模经验回放、批量学习和目标网络'
- en: A quick tutorial on three essential tricks for stable and successful Deep Q-learning,
    using TensorFlow 2.0
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于稳定且成功的深度Q学习的三个基本技巧的快速教程，使用TensorFlow 2.0
- en: towardsdatascience.com](/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172?source=post_page-----4643c1f71b2e--------------------------------)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[如何建模经验回放、批量学习和目标网络](https://towardsdatascience.com/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172?source=post_page-----4643c1f71b2e--------------------------------)'
- en: Critic network updates
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评论员网络更新
- en: Now that the basics are refreshed, let’s tie the aforementioned concepts to
    DDPG. We define a **critic network Q_ϕ** as detailed before, parameterized by
    ϕ (representing the network weights).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在基础知识已经更新，让我们将上述概念与DDPG结合起来。我们定义一个**评论员网络Q_ϕ**，如前所述，由ϕ（代表网络权重）参数化。
- en: 'We set out with the loss function that we aim to minimize, which should look
    familiar to those experienced with Q-learning:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设定了一个损失函数，目标是最小化，这对于有Q学习经验的人应该是熟悉的：
- en: '![](../Images/656c05d9121e6be6546c0f00f5df8f6f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/656c05d9121e6be6546c0f00f5df8f6f.png)'
- en: Compared to DQN, the critical distinction is that for the action corresponding
    to *s’* — instead of maximizing over the action space — **we determine the action
    *a’* through a target actor network** *μ_{θ_targ}* (more on that later). After
    that sidestep, we update the critic network as usual.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与DQN相比，关键区别在于对于* s’*对应的动作——而不是在动作空间中最大化——**我们通过目标演员网络*μ_{θ_targ}*确定动作*a’***（稍后会详细讲解）。在这个旁道之后，我们像往常一样更新评论员网络。
- en: 'Aside from updating the main critic network, we must also update the target
    critic network. In Deep Q-Learning this is often a periodic copy of the main value
    network (e.g., copy once every 100 episodes). In DDPG, it is common to use a **lagging
    target network** using polyak averaging, making the target network trail behind
    the main value network:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 除了更新主评论员网络，我们还必须更新目标评论员网络。在深度Q学习中，这通常是主价值网络的周期性副本（例如，每100集复制一次）。在DDPG中，通常使用**滞后目标网络**进行Polyak平均，使目标网络落后于主价值网络：
- en: '![](../Images/0226601920d4cbc65040a6b4bc956476.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0226601920d4cbc65040a6b4bc956476.png)'
- en: With ρ typically being close to 1, the **target network adapts very slowly**
    and gradually over time, which improves training stability.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ρ通常接近1，**目标网络适应非常缓慢**，这逐渐提高了训练稳定性。
- en: 'DDPG: The actor'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DDPG：演员
- en: In DDPG, actor and critic are closely intertwined in an off-policy fashion.
    We first explore the off-policy nature of the algorithm, before moving to action
    generation and actor network updates.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在DDPG中，actor和critic以离策略的方式紧密相连。我们首先探索算法的离策略特性，然后再讨论动作生成和actor网络更新。
- en: Off-policy training
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离策略训练
- en: 'In pure policy gradient methods, we directly update a policy *μ_θ* (parameterized
    by *θ*) to maximize expected rewards, without resorting to explicit value functions
    to capture these rewards. DDPG is a hybrid that also uses Q-values, but from an
    actor perspective **maximizing the objective** *J(θ)* look similar at face value:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在纯策略梯度方法中，我们直接更新策略 *μ_θ*（由*θ* 参数化）以最大化期望奖励，而不依赖于显式的价值函数来捕捉这些奖励。DDPG是一种混合方法，它还使用Q值，但从actor的角度来看，**最大化目标**
    *J(θ)* 表面上看是相似的：
- en: '![](../Images/82bc426a25853f63c1934ea35d8d5327.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82bc426a25853f63c1934ea35d8d5327.png)'
- en: However, a closer look at the expectation reveals that DDPG is an **off-policy
    method**, whereas typical actor-critic methods are on-policy. Most actor-critic
    models maximize an expectation *E_{τ~π_θ}*, with *τ* being the state-action trajectory
    generated by policy *π_θ*. In contrast, DDPG takes an expectation over sample
    states drawn from the experience buffer (*E_{s~D}*). As DDPG optimizes a policy
    using experiences generated with different policies, it is an **off-policy algorithm.**
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仔细观察期望值会发现DDPG是一种**离策略方法**，而典型的actor-critic方法是同策略的。大多数actor-critic模型最大化期望值
    *E_{τ~π_θ}*，其中*τ* 是由策略 *π_θ* 生成的状态-动作轨迹。相对而言，DDPG对从经验缓冲区中抽取的样本状态进行期望值优化（*E_{s~D}*）。由于DDPG使用不同策略生成的经验来优化策略，因此它是一种**离策略算法**。
- en: The role of the replay buffer in this off-policy context needs some attention.
    Why *can* we re-use old experiences, and why *should* we?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种离策略背景下，重放缓冲区的作用需要一些关注。为什么*可以*重用旧经验，为什么*应该*这样做？
- en: First, let’s explore why the buffer *can* include experiences obtained with
    different policies than the present one. As the policy is updated over time, the
    replay buffer holds **experiences stemming from outdated policies**. As optimal
    Q-values apply to *any* transition, it does not matter what policy generated them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们探讨为什么缓冲区*可以*包含与当前策略不同的经验。随着策略的不断更新，重放缓冲区保存了**源自过时策略的经验**。由于最优Q值适用于*任何*过渡，因此生成这些经验的策略并不重要。
- en: Second, the reason the replay buffer *should* contain a **diverse range of experiences**
    is that we deploy a deterministic policy. If the algorithm would be on-policy,
    we would likely have limited exploration. By drawing upon past experiences, we
    also train on observations that are unlikely to be encountered under the present
    policy.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，重放缓冲区*应该*包含**多样化的经验**的原因在于我们部署的是确定性策略。如果算法是同策略的，我们可能会有有限的探索。通过借鉴过去的经验，我们还会在当前策略下不太可能遇到的观察值上进行训练。
- en: '![](../Images/aaff6f97effa5c119059c31e6e08c1b7.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aaff6f97effa5c119059c31e6e08c1b7.png)'
- en: Example of an actor network μ_θ:s in DDPG. The network takes the state vector
    as input, and outputs a deterministic action μ(s). During training, a separate
    random noise ϵ is typically added [image by author]
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG中的actor网络 μ_θ:s 示例。该网络将状态向量作为输入，并输出确定性动作 μ(s)。在训练过程中，通常会添加一个单独的随机噪声 ϵ [图像由作者提供]
- en: Action exploration
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作探索
- en: What about the exploration mechanism that policy gradient methods are so famous
    for? After all, we now deploy a deterministic policy rather than a stochastic
    one, right? DDPG resolves this by simply **adding some noise** ***ϵ*** during
    training, which is removed when deploying the policy.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，政策梯度方法中著名的探索机制如何呢？毕竟，我们现在部署的是确定性策略而非随机策略，对吧？DDPG通过在训练过程中**添加一些噪声** ***ϵ***
    来解决这个问题，在部署策略时去除这些噪声。
- en: Early implementations of DDPG used rather complicated noise constructs (e.g.,
    time-correlated Ornstein-Uhlenbeck noise), but later empirical results suggested
    that **plain Gaussian noise** *ϵ~N(0,σ^2)* works equally well. The noise may be
    gradually reduced over time, but is not a trainable component *σ_θ* such as in
    stochastic policies. A final note is that we may **clip the action range**. Evidently,
    some tuning effort is involved in the exploration.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的DDPG实现使用了相当复杂的噪声结构（例如，时间相关的奥恩斯坦-乌伦贝克噪声），但后来的实验证明，**普通高斯噪声** *ϵ~N(0,σ^2)*
    的效果同样良好。噪声可能会随着时间的推移逐渐减少，但不像随机策略中的*σ_θ*那样是一个可训练的组件。最后一点是，我们可能会**裁剪动作范围**。显然，探索过程中涉及一些调优工作。
- en: 'In summary, the **actor generates actions** as follows. It takes the state
    as input, outputs a deterministic value, and adds some random noise:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，**演员生成动作**如下。它以状态作为输入，输出一个确定的值，并添加一些随机噪声：
- en: '![](../Images/c7a4efb3cd025d01cca3981bd7e121b3.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7a4efb3cd025d01cca3981bd7e121b3.png)'
- en: Actor network updates
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演员网络更新
- en: 'A final note on the policy update, which is not necessarily trivial. We update
    the actor network parameters *θ* based on the Q-values returned by the critic
    network (parameterized by *ϕ*). Thus, we keep Q-values constant — that is, we
    don’t update *ϕ* in this step — and try to maximize the expected reward by changing
    the action. This means we **assume that the critic network is differentiable w.r.t.
    the action**, such that we can update the action in a direction that maximizes
    the Q-value:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 关于策略更新的最终说明，这并不一定是简单的。我们根据评论家网络（由 *ϕ* 参数化）返回的 Q 值来更新演员网络参数 *θ*。因此，我们保持 Q 值不变——即，我们在这一步不更新
    *ϕ*——并通过改变动作来最大化预期奖励。这意味着我们 **假设评论家网络对动作是可微的**，以便我们可以在一个最大化 Q 值的方向上更新动作：
- en: '![](../Images/ecd78f932e63e0fe9dbdb9d68fca1859.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ecd78f932e63e0fe9dbdb9d68fca1859.png)'
- en: Although the second gradient *∇_θ* is often omitted for readability, it offers
    some clarification. We train the actor network to **output better actions**, which
    in turn improves the obtained Q-values. If you wanted, you could detail the procedure
    by applying the chain rule.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管第二个梯度 *∇_θ* 常常为了可读性而省略，但它提供了一些说明。我们训练演员网络以 **输出更好的动作**，从而改进获得的 Q 值。如果愿意，你可以通过应用链式法则来详细说明这个过程。
- en: The **actor target network** is updated with polyak averaging, in the same vein
    as the critic target network.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**演员目标网络**使用 Polyak 平均进行更新，与评论家目标网络类似。'
- en: '![](../Images/1f758f6a5bf043ea96b21a0d57870ead.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f758f6a5bf043ea96b21a0d57870ead.png)'
- en: Pulling it all together
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 综合整理
- en: We have an actor, we have a critic, so nothing stops us from completing the
    algorithm now!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个演员，也有一个评论家，因此现在没有什么可以阻止我们完成算法！
- en: '![](../Images/c8e86f070d5ab4439d2f0c0fb46d4fc8.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8e86f070d5ab4439d2f0c0fb46d4fc8.png)'
- en: Outline of the DDPG outline [image by author, initial outline generated with
    ChatGPT]
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 大纲 [作者提供的图像，初步大纲由 ChatGPT 生成]
- en: Let’s go through the procedure step by step.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地进行详细说明。
- en: Initialization [line 1–4]
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化 [第 1–4 行]
- en: '![](../Images/477e53600385a47ab065c361b6ad12cd.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/477e53600385a47ab065c361b6ad12cd.png)'
- en: DDPG initialization [image by author]
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 初始化 [作者提供的图像]
- en: 'We set out with four networks as detailed below:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从以下四个网络开始：
- en: '**Actor network μ_**θ'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**演员网络 μ_**θ'
- en: Parameterized by θ
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 θ 参数化
- en: Outputs deterministic action based on input *s*
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于输入 *s* 输出确定性动作
- en: '**Actor target network μ_{θ_targ}**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**演员目标网络 μ_{θ_targ}**'
- en: Parameterized by *θ_targ*
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 *θ_targ* 参数化
- en: Provides action for *s’* when training critic network
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练评论家网络时提供 *s’* 的动作
- en: '**Critic network Q_**ϕ**(s,a)**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论家网络 Q_**ϕ**(s,a)**'
- en: Parameterized by *ϕ*
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 *ϕ* 参数化
- en: Outputs Q-value *Q(s,a)* (expectation) based on input *(s,a)*
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于输入 *(s,a)* 输出 Q 值 *Q(s,a)*（期望）
- en: '**Critic target network μ**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论家目标网络 μ**'
- en: Parameterized by *ϕ_tar*
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 *ϕ_tar* 参数化
- en: Outputs Q-value *Q(s’,a’)* (target) when training critic network
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练评论家网络时输出 Q 值 *Q(s’,a’)*（目标）
- en: We start with an empty replay buffer *D*. Unlike on-policy methods, we **do
    not empty the buffer after updating the policy,** as we re-use older transitions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个空的重放缓冲区 *D* 开始。与策略方法不同，我们 **在更新策略后不会清空缓冲区，** 因为我们会重复使用旧的过渡。
- en: Finally, we set the learning rate *ρ* to **update the target networks.** For
    simplicity, we assume it is identical for both target networks. Recall that *ρ*
    should be set close to 1 (e.g., something like 0.995), such that the the networks
    are updated slowly and targets remain rather stable over time.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将学习率 *ρ* 设置为 **更新目标网络。** 为了简单起见，我们假设两个目标网络的学习率相同。请记住，*ρ* 应设置接近 1（例如，0.995），以便网络更新缓慢，目标保持相对稳定。
- en: Data collection [line 9–11]
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集 [第 9–11 行]
- en: '![](../Images/623f60f26adc0d097753f47ee82200a0.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/623f60f26adc0d097753f47ee82200a0.png)'
- en: DDPG data collection [image by author]
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 数据收集 [作者提供的图像]
- en: Actions are generated using the actor network, which outputs deterministic actions.
    To increase exploration, noise is added to these actions. The resulting observation
    tuples are stored in the replay buffer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 动作通过演员网络生成，演员网络输出确定性动作。为了增加探索，向这些动作中添加噪声。生成的观测元组存储在重放缓冲区中。
- en: Updating the actor and critic network [line 12–17]
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新演员和评论家网络 [第 12–17 行]
- en: '![](../Images/9569b5d4a9bd7b3b7a88c2598fd98727.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9569b5d4a9bd7b3b7a88c2598fd98727.png)'
- en: DDPG main network updates [image by author]
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 主网络更新 [作者提供的图片]
- en: A random mini-batch *B⊆D* is sampled from the replay buffer (including observations
    stemming from older policies).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从重放缓冲区中采样一个随机小批量 *B⊆D*（包括源自较旧策略的观察值）。
- en: To update the critic, we **minimize the squared error** between the observation
    (obtained with the target networks) and the expectation (obtained with the main
    networks).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要更新评论者，我们**最小化平方误差**，即观察值（通过目标网络获得）和期望值（通过主网络获得）之间的误差。
- en: To update the actor, we compute the **sample policy gradient**, with the Q-values
    kept fixed. In a neural network setting, we compute a pseudo loss which is the
    cumulative Q-value of the generated actions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新演员，我们计算**样本策略梯度**，同时保持 Q 值固定。在神经网络设置中，我们计算伪损失，即生成动作的累积 Q 值。
- en: 'The **training procedure** might be clarified by the Keras snippet below:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练过程**可以通过下面的 Keras 代码片段进行澄清：'
- en: Updating the target networks [line 18–19]
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新目标网络 [第 18–19 行]
- en: '![](../Images/3ecde0b56ea4ad82349688101f9adbd3.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ecde0b56ea4ad82349688101f9adbd3.png)'
- en: DDPG target network updates [image by author]
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 目标网络更新 [作者提供的图片]
- en: Both the actor target network and the critic target network are updated using
    **polyak averaging**, with their weights moving a bit closer to the updated main
    networks.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 演员目标网络和评论者目标网络使用**Polyak 平均**进行更新，它们的权重略微靠近更新后的主网络。
- en: Returning the trained network [line 23]
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 返回训练后的网络 [第 23 行]
- en: '![](../Images/0b5c52b7f0e8f58beabaf534d7f0dca4.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b5c52b7f0e8f58beabaf534d7f0dca4.png)'
- en: DDPG trained actor network [image by author]
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 训练后的演员网络 [作者提供的图片]
- en: Although we had to go through some trouble, the resulting policy looks very
    clean. Unlike in DQN, we don’t perform an explicit maximization over the action
    space, so we have **no need for Q-values anymore** [note we never used them to
    take decisions, just to improve them].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们经历了一些麻烦，但结果策略看起来非常干净。与 DQN 不同，我们不对动作空间进行显式最大化，因此**不再需要 Q 值** [注意我们从未用它们来做决策，只是用来改进它们]。
- en: We also have **no need for the target networks** anymore, which were just required
    to stabilize training and prevent oscillations. Ideally, the main- and target
    networks will have converged, such that we have *μ_θ=μ_{θ_targ}* (and *Q_ϕ=Q_{ϕ_targ}*).
    That way, we know our policy has truly converged.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们**不再需要目标网络**，这些网络只是为了稳定训练和防止振荡。理想情况下，主网络和目标网络将会收敛，从而使得我们有*μ_θ=μ_{θ_targ}*（以及
    *Q_ϕ=Q_{ϕ_targ}*）。这样，我们知道我们的策略确实已经收敛。
- en: Finally, we drop the exploration noise *ϵ*, which was never an integral aspect
    of the policy. We are left with an **actor network that takes the state as input
    and outputs a deterministic action**, which for many applications is exactly the
    simplicity we want.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们去掉了探索噪声 *ϵ*，它从未是策略的一个核心部分。我们得到一个**接受状态作为输入并输出确定性动作的演员网络**，这在许多应用中正是我们所希望的简单性。
- en: What distinguishes DDPG from other algorithms?
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么使 DDPG 与其他算法不同？
- en: We established that DDPG is a hybrid class method, incorporating elements from
    both policy gradient methods and value-based methods. This goes for all actor-critic
    methods though, so what precisely makes DDPG unique?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确定 DDPG 是一种混合类方法，结合了策略梯度方法和基于值的方法。这适用于所有演员-评论者方法，那么究竟是什么使 DDPG 独特呢？
- en: '**DDPG handles continuous action spaces:** The algorithm is specifically designed
    to handle continuous action spaces, without relying on stochastic policies. Deterministic
    policies may be easier to learn, and policies without inherent randomness are
    often preferable in real-world applications.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DDPG 处理连续动作空间：** 该算法专门设计用于处理连续动作空间，而不依赖于随机策略。确定性策略可能更容易学习，并且在实际应用中没有固有随机性的策略通常是更可取的。'
- en: '**DDPG is off-policy.** Unlike common actor-critic algorithms, experiences
    are drawn from a replay buffer that includes observations from older policies.
    The off-policy nature is necessary to sufficiently explore (as actions are generated
    deterministically). It also offers benefits such as greater sample efficiency
    and enhanced stability.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DDPG 是离线策略的。** 与常见的演员-评论者算法不同，经验来自于包括较旧策略的观察值的重放缓冲区。离线策略的性质对于充分探索是必要的（因为动作是确定性生成的）。它还提供了更高的样本效率和更好的稳定性。'
- en: '**DDPG is conceptually very close to DQN:** In essence, DDPG is a variant of
    DQN that works for continuous action spaces. To circumvent the need to explicitly
    maximize over all actions — DQN enumerates over the full action space to identify
    the highest Q(s,a) value — actions are provided by an actor network which is optimized
    separately.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DDPG 在概念上非常接近 DQN：** 从本质上讲，DDPG 是 DQN 的一个变体，适用于连续动作空间。为了避免明确地在所有动作上进行最大化——DQN
    通过枚举整个动作空间来识别最高的 Q(s,a) 值——动作由一个单独优化的演员网络提供。'
- en: '**DDPG outputs an actor network:** Although close to DQN in terms of training,
    during deployment we only need the trained actor network. This network takes the
    state as input and deterministically outputs an action.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DDPG 输出一个演员网络：** 尽管在训练上接近 DQN，但在部署过程中我们只需要训练好的演员网络。这个网络将状态作为输入，并确定性地输出一个动作。'
- en: Although it might not seem so at first glance, the **deterministic nature of
    DDPG tends to simplify training**, being more stable and sample-efficient than
    online counterparts. The output is a comprehensive actor network that deterministically
    generates actions. Because of these properties, it has become a staple in continuous
    control tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管乍一看可能不那么明显，**DDPG 的确定性特性往往简化了训练**，比在线方法更稳定、更高效。输出是一个全面的演员网络，该网络确定性地生成动作。由于这些特性，它已成为连续控制任务中的重要工具。
- en: '*More background on the building blocks of DDPG? Check out the following articles.*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*想了解更多关于 DDPG 构建模块的背景？查看以下文章。*'
- en: '*Deep Q-Learning (DQN):*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度 Q 学习（DQN）：*'
- en: '[](/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e?source=post_page-----4643c1f71b2e--------------------------------)
    [## A Minimal Working Example for Deep Q-Learning in TensorFlow 2.0'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[## TensorFlow 2.0 中深度 Q 学习的最小工作示例](https://towardsdatascience.com/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e?source=post_page-----4643c1f71b2e--------------------------------)'
- en: A multi-armed bandit example to train a Q-network. The update procedure takes
    just a few lines of code using TensorFlow
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个多臂赌博机的例子，用于训练 Q 网络。更新过程只需要几行代码，使用 TensorFlow。
- en: towardsdatascience.com](/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e?source=post_page-----4643c1f71b2e--------------------------------)
    [](/deep-q-learning-for-the-cliff-walking-problem-b54835409046?source=post_page-----4643c1f71b2e--------------------------------)
    [## Deep Q-Learning for the Cliff Walking Problem
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 深度 Q 学习在悬崖行走问题中的应用](https://towardsdatascience.com/deep-q-learning-for-the-cliff-walking-problem-b54835409046?source=post_page-----4643c1f71b2e--------------------------------)
    [## 深度 Q 学习在悬崖行走问题中的应用](https://towardsdatascience.com/deep-q-learning-for-the-cliff-walking-problem-b54835409046?source=post_page-----4643c1f71b2e--------------------------------)
    [## 深度 Q 学习在悬崖行走问题中的应用'
- en: A full Python implementation with TensorFlow 2.0 to navigate the cliff.
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个完整的 Python 实现，使用 TensorFlow 2.0 进行悬崖导航。
- en: towardsdatascience.com](/deep-q-learning-for-the-cliff-walking-problem-b54835409046?source=post_page-----4643c1f71b2e--------------------------------)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 深度 Q 学习在悬崖行走问题中的应用](https://towardsdatascience.com/deep-q-learning-for-the-cliff-walking-problem-b54835409046?source=post_page-----4643c1f71b2e--------------------------------)'
- en: '*Policy gradient methods:*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*策略梯度方法：*'
- en: '[](/policy-gradients-in-reinforcement-learning-explained-ecec7df94245?source=post_page-----4643c1f71b2e--------------------------------)
    [## Policy Gradients In Reinforcement Learning Explained'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 强化学习中的策略梯度解释](https://towardsdatascience.com/policy-gradients-in-reinforcement-learning-explained-ecec7df94245?source=post_page-----4643c1f71b2e--------------------------------)
    [## 强化学习中的策略梯度解释](https://towardsdatascience.com/policy-gradients-in-reinforcement-learning-explained-ecec7df94245?source=post_page-----4643c1f71b2e--------------------------------)
    [## 强化学习中的策略梯度解释'
- en: 'Learn all about policy gradient algorithms based on likelihood ratios (REINFORCE):
    the intuition, the derivation, the…'
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解基于似然比的策略梯度算法（REINFORCE）：直觉、推导…
- en: towardsdatascience.com](/policy-gradients-in-reinforcement-learning-explained-ecec7df94245?source=post_page-----4643c1f71b2e--------------------------------)
    [](/deep-policy-gradient-for-cliff-walking-37d5014fd4bc?source=post_page-----4643c1f71b2e--------------------------------)
    [## Deep Policy Gradient For Cliff Walking
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 深度策略梯度用于悬崖行走](https://towardsdatascience.com/deep-policy-gradient-for-cliff-walking-37d5014fd4bc?source=post_page-----4643c1f71b2e--------------------------------)
    [## 深度策略梯度用于悬崖行走](https://towardsdatascience.com/deep-policy-gradient-for-cliff-walking-37d5014fd4bc?source=post_page-----4643c1f71b2e--------------------------------)
    [## 深度策略梯度用于悬崖行走'
- en: A TensorFlow 2.0 implementation in Python. In this solution, the actor is represented
    by a neural network, which is…
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个用 Python 实现的 TensorFlow 2.0 解决方案。在这个方案中，演员由一个神经网络表示，该网络…
- en: towardsdatascience.com](/deep-policy-gradient-for-cliff-walking-37d5014fd4bc?source=post_page-----4643c1f71b2e--------------------------------)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 深度策略梯度用于悬崖行走](https://towardsdatascience.com/deep-policy-gradient-for-cliff-walking-37d5014fd4bc?source=post_page-----4643c1f71b2e--------------------------------)'
- en: References
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: OpenAI (2018). Deep Deterministic Policy Gradient. [https://spinningup.openai.com/en/latest/algorithms/ddpg.html](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI (2018). 深度确定性策略梯度。 [https://spinningup.openai.com/en/latest/algorithms/ddpg.html](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)
- en: Keras (2020). Deep Deterministic Policy Gradient (DDPG) by amifunny. [https://keras.io/examples/rl/ddpg_pendulum/](https://keras.io/examples/rl/ddpg_pendulum/)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Keras (2020). 深度确定性策略梯度（DDPG）由 amifunny 制作。 [https://keras.io/examples/rl/ddpg_pendulum/](https://keras.io/examples/rl/ddpg_pendulum/)
- en: Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
    … & Wierstra, D. (2015). Continuous control with deep reinforcement learning.
    *arXiv preprint arXiv:1509.02971*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
    … & Wierstra, D. (2015). 基于深度强化学习的连续控制。 *arXiv 预印本 arXiv:1509.02971*。
- en: Yang, A. & Philion, J(2020). *Continuous Control With Deep Reinforcement Learning.*
    [https://www.pair.toronto.edu/csc2621-w20/assets/slides/lec3_ddpg.pdf](https://www.pair.toronto.edu/csc2621-w20/assets/slides/lec3_ddpg.pdf)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Yang, A. & Philion, J. (2020). *深度强化学习中的连续控制*。 [https://www.pair.toronto.edu/csc2621-w20/assets/slides/lec3_ddpg.pdf](https://www.pair.toronto.edu/csc2621-w20/assets/slides/lec3_ddpg.pdf)
