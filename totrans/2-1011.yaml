- en: Gradient Boosting from Theory to Practice (Part 2)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理论与实践中的梯度提升（第2部分）
- en: 原文：[https://towardsdatascience.com/gradient-boosting-from-theory-to-practice-part-2-25c8b7ca566b](https://towardsdatascience.com/gradient-boosting-from-theory-to-practice-part-2-25c8b7ca566b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gradient-boosting-from-theory-to-practice-part-2-25c8b7ca566b](https://towardsdatascience.com/gradient-boosting-from-theory-to-practice-part-2-25c8b7ca566b)
- en: Use the gradient boosting classes in Scikit-Learn to solve different classification
    and regression problems
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn中的梯度提升类解决不同的分类和回归问题
- en: '[](https://medium.com/@roiyeho?source=post_page-----25c8b7ca566b--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----25c8b7ca566b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----25c8b7ca566b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----25c8b7ca566b--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----25c8b7ca566b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----25c8b7ca566b--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----25c8b7ca566b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----25c8b7ca566b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----25c8b7ca566b--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----25c8b7ca566b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----25c8b7ca566b--------------------------------)
    ·12 min read·Jul 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----25c8b7ca566b--------------------------------)
    ·阅读时间12分钟·2023年7月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d549d077e8ed878675b861b84fcdffb5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d549d077e8ed878675b861b84fcdffb5.png)'
- en: Photo by [Luca Bravo](https://unsplash.com/@lucabravo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/wallpapers/nature/forest?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Luca Bravo](https://unsplash.com/@lucabravo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)拍摄，来源于[Unsplash](https://unsplash.com/wallpapers/nature/forest?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: In the [first part](https://medium.com/towards-data-science/gradient-boosting-from-theory-to-practice-part-1-940b2c9d8050)
    of this article, we presented the gradient boosting algorithm and showed its implementation
    in pseudocode.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第一部分](https://medium.com/towards-data-science/gradient-boosting-from-theory-to-practice-part-1-940b2c9d8050)中，我们介绍了梯度提升算法，并展示了其伪代码实现。
- en: In this part of the article, we will explore the classes in Scikit-Learn that
    implement this algorithm, discuss their various parameters, and demonstrate how
    to use them to solve several classification and regression problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分文章中，我们将探讨Scikit-Learn中实现这一算法的类，讨论它们的各种参数，并演示如何使用它们解决几个分类和回归问题。
- en: Although the [XGBoost](/xgboost-the-definitive-guide-part-1-cc24d2dcd87a) library
    provides a more optimized and highly scalable implementation of gradient boosting,
    for small to medium-sized data sets it is often easier to use the gradient boosting
    classes in Scikit-Learn, which have a simpler interface and a significantly fewer
    number of hyperparameters to tune.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管[XGBoost](/xgboost-the-definitive-guide-part-1-cc24d2dcd87a)库提供了一个更优化和高度可扩展的梯度提升实现，但对于小到中等规模的数据集，使用Scikit-Learn中的梯度提升类通常更为简便，因为它们有更简单的接口和显著更少的超参数需要调整。
- en: Gradient Boosting in Scikit-Learn
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn中的梯度提升
- en: 'Scikit-Learn provides the following classes that implement the gradient-boosted
    decision trees (GBDT) model:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了以下实现梯度提升决策树（GBDT）模型的类：
- en: '[GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)
    is used for classification problems.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)用于分类问题。'
- en: '[GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)
    is used for regression problems.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)用于回归问题。'
- en: 'In addition to the standard parameters of [decision trees](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369),
    such as *criterion, max_depth* (set by default to 3)and *min_samples_split*, these
    classes provide the following parameters:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 除了[决策树](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369)的标准参数，如*criterion,
    max_depth*（默认设置为3）和*min_samples_split*，这些类还提供以下参数：
- en: '*loss* — the loss function to be optimized. In ` GradientBoostingClassifier,
    this function can be `''log_loss''`(the default) or `''exponential''` (which will
    make gradient boosting behave like [AdaBoost](https://medium.com/@roiyeho/adaboost-illustrated-3084183a2086)).
    In GradientBoostingRegressor, this function can be `''squared_error''` (the default),
    `''absolute_error''`, `''huber''`, or `''quantile’` (see [this article](https://medium.com/towards-data-science/loss-functions-in-machine-learning-9977e810ac02)
    for the differences between these loss functions).'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*损失* — 需要优化的损失函数。在`GradientBoostingClassifier`中，这个函数可以是‘log_loss’（默认值）或‘exponential’（这将使梯度提升表现得像[AdaBoost](https://medium.com/@roiyeho/adaboost-illustrated-3084183a2086)）。在GradientBoostingRegressor中，这个函数可以是‘squared_error’（默认值）、‘absolute_error’、‘huber’或‘quantile’（有关这些损失函数之间的差异，请参见[这篇文章](https://medium.com/towards-data-science/loss-functions-in-machine-learning-9977e810ac02)）。'
- en: '*n_estimators* — the number of boosting iterations (defaults to 100).'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*n_estimators* — 提升迭代的次数（默认值为100）。'
- en: '*learning_rate* — a factor that shrinks the contribution of each tree (defaults
    to 0.1).'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*学习率* — 缩小每棵树贡献的因子（默认值为0.1）。'
- en: '*subsample* — the fraction of samples to use for training each tree (defaults
    to 1.0).'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*子样本* — 用于训练每棵树的样本比例（默认值为1.0）。'
- en: '*max_features* — the number of features to consider when searching for the
    best split in each node. The options are to specify an integer for the number
    of features, a floating point to specify a fraction of the features to use, ‘sqrt’
    for using a square root of the features, ‘log2’ for using a log2 of the features,
    and None for using all the features (the default).'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*max_features* — 搜索每个节点最佳分割时要考虑的特征数量。选项包括指定特征数量的整数、指定特征比例的浮点数、使用特征的平方根的‘sqrt’、使用特征的对数的‘log2’，以及使用所有特征的None（默认值）。'
- en: '*validation_fraction* — a fraction of the training set that will be used as
    a validation set for early stopping (defaults to 0.1).'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*验证比例* — 用作早期停止的验证集的训练集比例（默认值为0.1）。'
- en: '*n_iter_no_change* — terminate training when the validation score has not improved
    in the previous *n_iter_no_change* iterations by at least *tol* (defaults to 0.0001).
    By default, *n_iter_no_change* is set to None, which means that early stopping
    is disabled.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*n_iter_no_change* — 当验证评分在前* n_iter_no_change *次迭代中未改善至少*tol*时终止训练（默认值为0.0001）。默认情况下，*n_iter_no_change*
    设置为None，这意味着早期停止被禁用。'
- en: 'The gradient boosting estimators also have the following attributes, which
    are learned from the data:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升估计器还有以下属性，它们是从数据中学习到的：
- en: '*n_estimators_* — the number of fitted trees as determined by early stopping
    (if specified, otherwise it is set to *n_estimators*).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n_estimators_* — 通过早期停止确定的拟合树数量（如果指定，否则设置为*n_estimators*）。'
- en: '*estimators*_ — the collection of fitted trees.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*估计器* — 已拟合的树集合。'
- en: '*feature_importances*_ — the feature importances estimated by the model (will
    be discussed later in this article).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征重要性* — 模型估计的特征重要性（将在本文后面讨论）。'
- en: '*oob_scores*_ — the loss values of the out-of-bag samples at every iteration
    (only available if *subsample* < 1.0).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*oob_scores* — 每次迭代时的袋外样本损失值（仅在*subsample* < 1.0时可用）。'
- en: '*train_score*_ — the loss values on the training set at every iteration.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*train_score* — 每次迭代时训练集上的损失值。'
- en: GradientBoostingClassifier
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GradientBoostingClassifier
- en: For example, let’s fit a gradient boosting classifier to the [Iris data set](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html),
    using only the first two features of each flower (sepal width and sepal length).
    As a reminder, with [random forests](https://medium.com/@roiyeho/random-forests-98892261dc49)
    we were able to obtain a test accuracy of 81.58% on this data set (after hyperparameter
    tuning). Let’s see if we can do better with gradient boosting.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以对[Iris数据集](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)进行梯度提升分类器的拟合，使用每朵花的前两个特征（萼片宽度和萼片长度）。作为提醒，我们在这个数据集上使用[随机森林](https://medium.com/@roiyeho/random-forests-98892261dc49)能够获得81.58%的测试准确率（经过超参数调整）。让我们看看梯度提升是否能做得更好。
- en: 'We first load the data set:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载数据集：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we split it into training and test sets (using the same random seed as
    in previous experiments):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将数据集分为训练集和测试集（使用与之前实验相同的随机种子）：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s now create a GradientBoostingClassifier model with its default settings
    (i.e., an ensemble of 100 trees with max_depth=3), and fit it to the training
    set:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建一个GradientBoostingClassifier模型，使用其默认设置（即，一个由100棵树组成的集成，max_depth=3），并将其拟合到训练集：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We fix the random state of the gradient boosting classifier in order to allow
    reproducibility of the results. The model’s performance is:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们固定梯度提升分类器的随机状态，以便结果可以重复。模型的性能是：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These are the same results we have obtained with random forests before hyperparameter
    tuning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果与之前通过随机森林获得的结果相同，但在超参数调整之前。
- en: Tuning the Hyperparameters
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整超参数
- en: Let’s run a randomized search on some of the gradient boosting hyperparameters
    in order to find a better model. For a fair comparison, we use the same number
    of search iterations as we did with random forests (50).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一些梯度提升超参数上运行随机搜索，以找到更好的模型。为了公平比较，我们使用与随机森林相同的搜索迭代次数（50次）。
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The best model found by the randomized search is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索找到的最佳模型是：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'That is, the best model consists of 10 decision trees with max_depth = 3, where
    each tree is trained on a random subsample of 60% of the training set. The accuracy
    of this model on the training and test sets is:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 即，最佳模型由10棵决策树组成，max_depth = 3，每棵树在训练集的60%随机子样本上进行训练。该模型在训练集和测试集上的准确性是：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The accuracy on the test set is significantly higher than the one we have obtained
    with random forest after tuning (86.84% compared to 81.58%).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集上的准确性显著高于调整后随机森林的结果（86.84%对81.58%）。
- en: 'Let’s examine the decision boundaries found by this classifier:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来检查这个分类器找到的决策边界：
- en: '![](../Images/69ae522fb1c8be0476b3a6dcdf18e0a5.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69ae522fb1c8be0476b3a6dcdf18e0a5.png)'
- en: The decision boundaries found by the gradient boosting classifier on the Iris
    data set
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升分类器在鸢尾数据集上找到的决策边界
- en: Compared to the decision boundaries found by the random forest classifier, we
    can see that the gradient boosting classifier is able to capture a larger area
    of the versicolor flowers without overfitting to the outliers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林分类器找到的决策边界相比，我们可以看到梯度提升分类器能够捕捉到更多的紫花地丁区域，而不会对异常值过拟合。
- en: GradientBoostingRegressor
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GradientBoostingRegressor
- en: For demonstrating the gradient boosting regressor, we will use the [California
    housing data set](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing).
    The goal in this data set is to predict the median house value of a given district
    (house block) in California, based on 8 different features of that district (such
    as the median income or the average number of rooms per household).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示梯度提升回归模型，我们将使用[加州住房数据集](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing)。该数据集的目标是基于该区的8个不同特征（如中位收入或每户平均房间数），预测加州某个区（房块）的中位房价。
- en: 'We first fetch the data set:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先获取数据集：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we split the data set into 80% training and 20% test:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将数据集分为80%的训练集和20%的测试集：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we fit a GradientBoostingRegressor with its default settings (i.e., an
    ensemble of 100 trees with max_depth=3) to the training set:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用默认设置（即，一个由100棵树组成的集成，max_depth=3）拟合一个GradientBoostingRegressor到训练集：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The *R*² scores of this model are:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的*R*²得分是：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is a slightly worse result than the one we have obtained with random forests
    before tuning (*R*² score on test = 0.798). However, notice that by default the
    trees in RandomForestRegressor are not pruned (their maximum depth is not limited),
    whereas the default maximum depth of the trees in GradientBoostingRegressor is
    only 3, while the default number of trees in both estimators is the same (100).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个略逊于我们在调整前使用随机森林得到的结果（测试集上的*R*²得分=0.798）。然而，请注意，默认情况下RandomForestRegressor中的树不会被修剪（它们的最大深度没有限制），而GradientBoostingRegressor中的树的默认最大深度仅为3，同时两种估计器的默认树数相同（100）。
- en: Tuning the Hyperparameters
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整超参数
- en: 'Let’s tune the hyperparameters of the gradient boosting regressor by running
    the following randomized search:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行以下随机搜索来调整梯度提升回归模型的超参数：
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The best model found by the search is:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索找到的最佳模型是：
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: That is, the best model uses 500 trees with a maximum depth of 7, where each
    tree is trained on a random subsample of 80% of the training set using a logarithmic
    number of the features in each node split.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'The *R*² scores of this model on the training and test sets are:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The *R*² score on the test set is significantly higher than the one obtained
    by the random forest regressor after tuning (0.8166).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: The Learning Curve
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also plot the training and test errors in every boosting iteration. The
    training errors are stored in the *train_score_* attribute of the estimator. The
    test errors can be obtained by calling the *staged_predict*() method, which returns
    a generator that yields the model predictions on a given data set at each iteration.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/c92ac457269c4b8f01540b546004cddb.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: The learning curve of gradient boosting regressor on the California housing
    data set
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the minimum test error is reached after about 100 iterations,
    i.e., the optimal number of trees for this data set is around 100\. Moreover,
    the test error remains stable as we add more trees to the ensemble, which suggests
    that the model is less susceptible to overfitting.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Another way to find the optimal number of trees is to use **early stopping**.
    Let’s run the same randomized search, but instead of varying the number of estimators,
    we will set it to a fixed number of 500, and enable early stopping by setting
    *n_iter_no_change* to 5\. This automatically sets aside 10% of the training set
    for validation, and terminates the training once the validation score does not
    improve for 5 iterations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This estimator has a slightly worse accuracy on the test set than the previous
    one, which can explained by the randomization of the search and the fact that
    it used only 90% of the training set for building the ensemble.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check how many trees were actually built before early stopping was activated
    by inspecting the *n_estimators_* attribute:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Feature Importance
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to other tree-based ensemble methods, gradient-boosted trees can provide
    an estimate for the importance of the features in the data set, i.e., how much
    each feature contributes to the model’s predictions. This can be useful for model
    interpretation as well as for performing feature selection.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: The importance of a feature in a single decision tree is determined by the location
    where it is used in the tree (features located at the top of the tree contribute
    more to the tree’s predictions) and the reduction in node impurity achieved by
    using this feature to split the node. In tree-based ensemble methods, such as
    random forests and gradient-boosted trees, we average the feature importance over
    all the trees in the ensemble.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can plot the importance of the features in the California housing
    data set as found by our gradient boosting regressor:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/9be44d4793192a1d53141f62437256d6.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: Importance of the features in the California housing data set
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: The most important features in this data set are MedInc (median income), the
    house location (Longitude and Latitude), and AveOccup (average number of household
    members).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Histogram-Based Gradient Boosting
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-Learn 0.21 has introduced two histogram-based implementations of gradient
    boosting: [HistGradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)
    and [HistGradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html),
    which are similar to the histogram-based algorithm used in LightGBM [1].'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: These estimators first discretize the continuous features in the data set into
    integer-valued bins (255 bins by default). During training, these bins are used
    to construct feature histograms from the values of the samples that reached a
    specific node in the tree. The best split point at that node is then found based
    on these histograms.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'This discretization has the following advantages:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: It significantly reduces the number of splitting points to consider at each
    node in the tree.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It avoids the need to sort the values of the continuous features at every node
    (see the section “Handling Continuous Features” in [this article](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369)
    to understand why sorting is needed from the first place).
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In addition, many parts in the implementation of the histogram-based estimators
    are parallelized. For example, the gradient computations are parallelized over
    samples, and finding the best split point at a node is parallelized over features.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The binning and parallelization together allow the histogram-based estimators
    to run much faster than the standard gradient boosting estimators when the number
    of samples is large (*n* > 10,000).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the histogram-based estimators have built-in support for missing
    values and categorical features, which avoids the need for using an imputer or
    a one-hot encoder when preprocessing the data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the parameters of the histogram-based estimators are the same as GradientBoostingClassifier
    and GradientBoostingRegressor, with the following changes:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: The parameter for the number of estimators is called *max_iter* instead of *n_estimators*.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The defaults for the tree size have been modified: *max_depth* is set by default
    to None (instead of 3), while *max_leaf_nodes* is set to 31, and *min_samples_leaf*
    is set to 20.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Early stopping is automatically enabled when the number of samples is greater
    than 10,000.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, the following parameters were added:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '*max_bins* indicates the maximum number of bins to use. Must be no larger than
    255\. One additional bin is reserved for missing values.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*categorical_features* is a list of integers that indicate the locations of
    the categorical features in the data set.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*interaction_cst* specifies interaction constraints, i.e., the sets of features
    which can interact with each other in child node splits (defaults to None).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*interaction_cst* 指定交互约束，即在子节点分裂中可以相互交互的特征集（默认为 None）。'
- en: HistGradientBoostingClassifier Example
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HistGradientBoostingClassifier 示例
- en: For example, let’s compare the performance of HistGradientBoostingClassifier
    and GradientBoostingClassifier on an artificially generated data set.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们比较 HistGradientBoostingClassifier 和 GradientBoostingClassifier 在一个人工生成的数据集上的表现。
- en: We will use the function [make_hastie_10_2](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_hastie_10_2.html#sklearn.datasets.make_hastie_10_2)
    from Scikit-Learn, which generates a binary, 10-dimensional classification data
    set, the same one that was used in Hastie et al. [2], Example 10.2.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Scikit-Learn 中的函数 [make_hastie_10_2](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_hastie_10_2.html#sklearn.datasets.make_hastie_10_2)，它生成一个二分类的
    10 维数据集，与 Hastie 等人[2]中的示例 10.2 相同。
- en: 'The data set consists of 10 features that follow standard Gaussian distribution,
    and the target is a binary label defined by:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含 10 个遵循标准高斯分布的特征，目标是由以下定义的二进制标签：
- en: '![](../Images/5048cf864774f3284bc35ce66e7bed9c.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5048cf864774f3284bc35ce66e7bed9c.png)'
- en: That is, the negative class lies within a 10-dimensional sphere whose radius
    is 9.34.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，负类位于一个半径为 9.34 的 10 维球体内。
- en: 'Let’s first generate this data set with 50,000 samples:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先生成一个包含 50,000 个样本的数据集：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then we split it to 80% training set and 20% test set:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将其分为 80% 的训练集和 20% 的测试集：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s now train a GradientBoostingClassifier on this data set and measure its
    training time:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来训练一个 GradientBoostingClassifier 并测量它的训练时间：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'It takes 12.6 seconds on average to train this model. Its performance on the
    training and test sets is:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个模型平均需要 12.6 秒。它在训练集和测试集上的表现是：
- en: '[PRE28]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s now train a HistGradientBoostingClassifier on the same data set:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来训练一个 HistGradientBoostingClassifier 在相同的数据集上：
- en: '[PRE30]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'It takes only 1.53 seconds on average to train this model (more than 8 times
    faster than GradientBoostingClassifier). Its performance on the training and test
    sets is:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个模型平均只需 1.53 秒（比 GradientBoostingClassifier 快 8 倍以上）。它在训练集和测试集上的表现是：
- en: '[PRE32]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The accuracy on the test set is significantly better (94.67% instead of 92.31%).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集上的准确率显著提高（94.67% 代替 92.31%）。
- en: Summary
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Let’s summarize the pros and cons of gradient boosting as compared to other
    supervised learning models.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下梯度提升与其他监督学习模型相比的优缺点。
- en: '**Pros**:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: Provides highly accurate models and often the best performing models on structured
    data sets.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供高度准确的模型，通常在结构化数据集上表现最佳。
- en: Can capture complex interactions and patterns in the data set by combining multiple
    weak models.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过组合多个弱模型，能够捕捉数据集中的复杂交互和模式。
- en: Can effectively handle high-dimensional data sets by automatically selecting
    the relevant features.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够通过自动选择相关特征有效处理高维数据集。
- en: Less sensitive to outliers compared to other models, as each base model learns
    from the residuals of the previous models.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相较于其他模型，对异常值的敏感性较低，因为每个基础模型从前一个模型的残差中学习。
- en: Can handle heterogeneous data types, including numerical and categorical features.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理异质数据类型，包括数值和类别特征。
- en: Can handle missing values without requiring imputation.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理缺失值而无需插补。
- en: Provides a measure of feature importance.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供特征重要性的度量。
- en: Can be applied to both regression and classification tasks, and supports a wide
    range of loss functions.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以应用于回归和分类任务，并支持多种损失函数。
- en: '**Cons**:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: Training can be computationally intensive, especially when dealing with large
    data sets or when the ensemble has many trees. In addition, the training of the
    base models cannot be parallelized (unlike random forests for example).
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练可能会消耗大量计算资源，特别是在处理大型数据集或当集成模型有很多树时。此外，基础模型的训练无法并行化（例如，与随机森林不同）。
- en: Less interpretable than simpler models such as decision trees or linear regression,
    since an interpretation of the model’s decision requires following the paths of
    many trees.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比决策树或线性回归等简单模型更难解释，因为解释模型的决策需要跟踪多个树的路径。
- en: Several hyperparameters need to be tuned, including the number of trees, the
    size of each tree and the learning rate.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要调整多个超参数，包括树的数量、每棵树的大小和学习率。
- en: Can overfit the training set if not properly regularized or if the number of
    boosting iterations is too high.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有适当的正则化，或者提升迭代次数过多，可能会对训练集过拟合。
- en: Prediction can be slower compared to other models, as it requires traversing
    multiple trees and aggregating their predictions.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他模型相比，预测可能会更慢，因为它需要遍历多个树并聚合它们的预测。
- en: Final Notes
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终备注
- en: All the images are by the author unless stated otherwise.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均为作者提供，除非另有说明。
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/gradient_boosting](https://github.com/roiyeho/medium/tree/main/gradient_boosting)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的 GitHub 上找到本文的代码示例：[https://github.com/roiyeho/medium/tree/main/gradient_boosting](https://github.com/roiyeho/medium/tree/main/gradient_boosting)
- en: 'Iris data set info:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集信息：
- en: '**Citation**: Fisher, R. A. (1988). Iris. UCI Machine Learning Repository.
    [https://doi.org/10.24432/C56C76.](https://doi.org/10.24432/C56C76.)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**引用**: Fisher, R. A. (1988). Iris. UCI 机器学习库。 [https://doi.org/10.24432/C56C76.](https://doi.org/10.24432/C56C76.)'
- en: '**License**: Creative Commons CC BY 4.0.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**许可证**: Creative Commons CC BY 4.0。'
- en: 'California housing data set info:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 加利福尼亚住房数据集信息：
- en: '**Citation**: Pace, R. Kelley and Ronald Barry (1997), Sparse Spatial Autoregressions,
    Statistics and Probability Letters, 33, 291-297.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**引用**: Pace, R. Kelley 和 Ronald Barry (1997), Sparse Spatial Autoregressions,
    Statistics and Probability Letters, 33, 291-297。'
- en: '**License**: Creative Commons CC0: Public Domain.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**许可证**: Creative Commons CC0: 公共领域。'
- en: References
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[1] Ke et. al. (2017), [“LightGBM: A Highly Efficient Gradient BoostingDecision
    Tree”](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Ke 等人 (2017), [“LightGBM: A Highly Efficient Gradient Boosting Decision
    Tree”](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)。'
- en: '[2] T. Hastie, R. Tibshirani and J. Friedman (2009), “Elements of Statistical
    Learning Ed. 2”, Springer.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] T. Hastie, R. Tibshirani 和 J. Friedman (2009), “Elements of Statistical
    Learning Ed. 2”, Springer。'
