- en: Topic Modeling with Llama 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174](https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/31f5e2196198d26e5295ae70660d720c.png)'
  prefs: []
  type: TYPE_IMG
- en: Create easily interpretable topics with Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    ¬∑12 min read¬∑Aug 22, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of **Llama 2**, running strong LLMs locally has become more
    and more a reality. Its accuracy approaches OpenAI‚Äôs GPT-3.5, which serves well
    for many use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore how we can use Llama2 for Topic Modeling without
    the need to pass every single document to the model. Instead, we are going to
    leverage [**BERTopic**](https://github.com/MaartenGr/BERTopic), a modular topic
    modeling technique that can use any LLM for fine-tuning topic representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'BERTopic works rather straightforward. It consists of 5 sequential steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding documents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reducing the dimensionality of embeddings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cluster reduced embeddings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize documents per cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract best-representing words per cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/63a385b329d173ae7fc70aa9fa1b4182.png)'
  prefs: []
  type: TYPE_IMG
- en: The 5 main steps of BERTopic.
  prefs: []
  type: TYPE_NORMAL
- en: However, with the rise of LLMs like **Llama 2**, we can do much better than
    a bunch of independent words per topic. It is computationally not feasible to
    pass all documents to Llama 2 directly and have it analyze them. We can employ
    vector databases for search but we are not entirely sure which topics to search
    for.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we will leverage the clusters and topics that were created by BERTopic
    and have Llama 2 fine-tune and distill that information into something more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: This is the best of both worlds, the topic creation of BERTopic together with
    the topic representation of Llama 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66743d20879491fe94a4075743d520b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Llama 2 lets us fine-tune the topic representations generated by BERTopic.
  prefs: []
  type: TYPE_NORMAL
- en: Now that this intro is out of the way, let‚Äôs start the hands-on tutorial!
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by installing a number of packages that we are going to use throughout
    this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Keep in mind that you will need at least a T4 GPU in order to run this example,
    which can be used with a free Google Colab instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'üî• **TIP**: You can also follow along with the [Google Colab Notebook](https://colab.research.google.com/drive/1QCERSMUjqGetGGujdrvv_6_EeoIcd_9M?usp=sharing).'
  prefs: []
  type: TYPE_NORMAL
- en: '[üìÑ](https://emojipedia.org/page-facing-up) Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to apply topic modeling on a number of ArXiv abstracts. They are
    a great source for topic modeling since they contain a wide variety of topics
    and are generally well-written.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To give you an idea, an abstract looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ü§ó HuggingFace Hub Credentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we can load in Llama2 using a number of tricks, we will first need to
    accept the License for using Llama2\. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a HuggingFace account [here](https://huggingface.co/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply for Llama 2 access [here](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get your HuggingFace token [here](https://huggingface.co/settings/tokens)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After doing so, we can log in with our HuggingFace credentials so that this
    environment knows we have permission to download the Llama 2 model that we are
    interested in.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/475af2918fd29d5277c2630d8b6ea2ef.png)'
  prefs: []
  type: TYPE_IMG
- en: ü¶ô Llama 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now comes one of the more interesting components of this tutorial, how to load
    in a Llama 2 model on a T4-GPU!
  prefs: []
  type: TYPE_NORMAL
- en: We will be focusing on the `'meta-llama/Llama-2-13b-chat-hf'` variant. It is
    large enough to give interesting and useful results whilst small enough that it
    can be run on our environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining our model and identifying if our GPU is correctly selected.
    We expect the output of `device` to show a Cuda device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Optimization & Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to load our 13 billion parameter model, we will need to perform some
    optimization tricks. Since we have limited VRAM and not an A100 GPU, we will need
    to ‚Äúcondense‚Äù the model a bit so that we can run it.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of tricks that we can use but the main principle is going
    to be 4-bit quantization.
  prefs: []
  type: TYPE_NORMAL
- en: This process reduces the 64-bit representation to only 4-bits which reduces
    the GPU memory that we will need. It is a recent technique and quite elegant at
    that for efficient LLM loading and usage. You can find more about that method
    [here](https://arxiv.org/pdf/2305.14314.pdf) in the QLoRA paper and on the amazing
    HuggingFace blog [here](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'These four parameters that we just run are incredibly important and bring many
    LLM applications to consumers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**load_in_4bit**`Allows us to load the model in 4-bit precision compared to
    the original 32-bit precision. This gives us an incredible speed up and reduces
    memory!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**bnb_4bit_quant_type**`This is the type of 4-bit precision. The paper recommends
    normalized float 4-bit, so that is what we are going to use!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**bnb_4bit_use_double_quant**`This is a neat trick as it performs a second
    quantization after the first which further reduces the necessary bits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**bnb_4bit_compute_dtype**`The compute type used during computation, which
    further speeds up the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using this configuration, we can start loading in the model as well as the
    tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the model and tokenizer, we will generate a HuggingFace transformers
    pipeline that allows us to easily generate new text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Prompt Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To check whether our model is correctly loaded, let‚Äôs try it out with a few
    prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Although we can directly prompt the model, there is actually a template that
    we need to follow. The template looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This template consists of two main components, namely the `{{ System Prompt
    }}` and the `{{ User Prompt }}`:'
  prefs: []
  type: TYPE_NORMAL
- en: The `{{ System Prompt }}` helps us guide the model during a conversation. For
    example, we can say that it is a helpful assistant that is specialized in labeling
    topics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `{{ User Prompt }}` is where we ask it a question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might have noticed the `[INST]` tags, which are used to identify the beginning
    and end of a prompt. We can use these to model the conversation history as we
    will see more in-depth later on.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let‚Äôs see how we can use this template to optimize Llama 2 for topic modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Template
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to keep our `system prompt` simple and to the point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We will tell the model that it is simply a helpful assistant for labeling topics
    since that is our main goal.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, our `user prompt` is going to be a bit more involved. It will consist
    of two components, an **example** **prompt** and the **main prompt**.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs start with the **example prompt**. Most LLMs do a much better job of generating
    accurate responses if you give them an example to work with. We will show it an
    accurate example of the kind of output we are expecting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This example, based on a number of keywords and documents primarily about the
    impact of meat, helps the model understand the kind of output it should give.
    We show the model that we were expecting only the label, which is easier for us
    to extract.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will create a template that we can use within BERTopic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two BERTopic-specific tags that are of interest, namely `[DOCUMENTS]`
    and `[KEYWORDS]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[DOCUMENTS]` contain the top 5 most relevant documents to the topic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[KEYWORDS]` contain the top 10 most relevant keywords to the topic as generated
    through c-TF-IDF'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This template will be filled according to each topic. And finally, we can combine
    this into our final prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: üó®Ô∏è BERTopic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we can start with topic modeling, we will first need to perform two
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-calculating Embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining Sub-models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By pre-calculating the embeddings for each document, we can speed up additional
    exploration steps and use the embeddings to quickly iterate over BERTopic‚Äôs hyperparameters
    if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'üî• **TIP**: You can find a great overview of good embeddings for clustering
    on the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Sub-models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we will define all sub-models in BERTopic and do some small tweaks to
    the number of clusters to be created, setting random states, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As a small bonus, we are going to reduce the embeddings we created before to
    2-dimensions so that we can use them for visualization purposes when we have created
    our topics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Representation Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the ways we are going to represent the topics is with Llama 2 which should
    give us a nice label. However, we might want to have additional representations
    to view a topic from multiple angles.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will be using c-TF-IDF as our main representation and [KeyBERT](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired),
    [MMR](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#maximalmarginalrelevance),
    and [Llama 2](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html)
    as our additional representations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: üî• Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our models prepared, we can start training our topic model!
    We supply BERTopic with the sub-models of interest, run `.fit_transform`, and
    see what kind of topics we get.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we are done training our model, let‚Äôs see what topics were generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d9a0326290f7988d127e851f3d452d37.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a34443869e8befd4e6af0d8622f045df.png)'
  prefs: []
  type: TYPE_IMG
- en: We got over 100 topics that were created and they all seem quite diverse. We
    can use the labels by Llama 2 and assign them to topics that we have created.
    Normally, the default topic representation would be c-TF-IDF, but we will focus
    on Llama 2 representations instead.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[üìä](https://emojigraph.org/bar-chart/) Visualize'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can go through each topic manually, which would take a lot of work, or we
    can visualize them all in a single interactive graph.
  prefs: []
  type: TYPE_NORMAL
- en: BERTopic has a bunch of [visualization functions](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html)
    that we can use. For now, we are sticking with visualizing the documents.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/03b0559c55725b27e2ef199d98605b6e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[üñºÔ∏è](https://emojipedia.org/framed-picture/) (BONUS): Advanced Visualization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we can use the built-in visualization features of BERTopic, we can
    also create a static visualization that might be a bit more informative.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating the necessary variables that contain our reduced embeddings
    and representations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will visualize the reduced embeddings with Matplotlib and process
    the labels in such a way that it is visually more pleasing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8fb9e2c99f87f38e3d5f5a824b2a0d65.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Update**: I uploaded a video version to YouTube that goes more in-depth into
    how to use BERTopic with Llama 2:'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are, like me, passionate about AI and/or Psychology, please feel free
    to add me on [**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/), follow
    me on [**Twitter**](https://twitter.com/MaartenGr), or subscribe to my [**Newsletter**](http://maartengrootendorst.substack.com/).
    You can also find some of my content on my [**Personal Website**](https://maartengrootendorst.com/).
  prefs: []
  type: TYPE_NORMAL
- en: '*All images without a source credit were created by the author*'
  prefs: []
  type: TYPE_NORMAL
