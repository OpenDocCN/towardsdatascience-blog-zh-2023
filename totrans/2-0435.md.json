["```py\n{\n  'newImage': {\n    'id': 1, \n    'version': 100, \n    'created_time': '2023-01-01: 10:00:00',\n    'interactions': 50,\n    MANY OTHER FIELDS...\n  }\n  'oldImage': {\n    'id': 1, \n    'version': 101, \n    'created_time': '2023-01-01: 10:00:00',\n    'interactions': 51,\n    MANY OTHER FIELDS...\n  }\n}\n```", "```py\nincrement_df = (\n  spark.read\n  .format('json')\n  .option('path', input_path)\n  .schema(evolved_schema)\n  .load()\n  .select('newImage.*')\n)\n\nw = Window().partitionBy('id').orderBy(desc('version'))\n\nresult_df = (\n  increment_df\n  .withColumn('r', row_number().over(w))\n  .filter(col('r') == 1)\n  .drop('r')\n)\n```", "```py\nstage_df = (\n  spark.read\n  .format('parquet')\n  .schema(input_schema)\n  .option(input_path)\n  .load()\n)\n\ntarget_df = spark.table(table_name)\n\nw = Window().partitionBy('id').orderBy(desc('version'))\n\nresult_df = (\n  stage_df.unionByName(target_df)\n  .withColumn('r', row_number().over(w))\n  .filter(col('r') == 1)\n  .drop('r')\n)\n```", "```py\n(\n  spark.createDataFrame([], new_schema)\n  .write\n  .mode('overwrite')\n  .option('path', table_location_temp)\n  .saveAsTable(table_name_temp)\n)\n\nspark.sql('ALTER TABLE table_name_temp SET LOCATION table_location')\nspark.sql('MSCK REPAIR TABLE table_name_temp')\nspark.sql('DROP TABLE table_name')\nspark.sql('ALTER TABLE table_name_temp RENAME TO table_name')\n```", "```py\n(\n  df.write\n  .mode('overwrite')\n  .format('parquet')\n  .option('path', output_path)\n  .saveAsTable(table_name)\n)\n```", "```py\n output_path = posixpath.join(output_path, str(int(time.time())))\n\n(\n  df.write\n  .mode('overwrite')\n  .format('parquet')\n  .option('path', output_path)\n  .saveAsTable(table_name_tmp)\n)\nspark.sql('DROP TABLE IF EXISTS table_name')\nspark.sql('ALTER TABLE table_name_tmp RENAME TO table_name')\n```"]