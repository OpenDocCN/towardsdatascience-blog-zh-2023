- en: The Power of Transformers in Predicting Twitter Account Identities
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å˜å‹å™¨åœ¨é¢„æµ‹æ¨ç‰¹è´¦æˆ·èº«ä»½ä¸­çš„åŠ›é‡
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/twitter-account-identity-prediction-with-large-language-models-c3ffef114d34](https://towardsdatascience.com/twitter-account-identity-prediction-with-large-language-models-c3ffef114d34)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/twitter-account-identity-prediction-with-large-language-models-c3ffef114d34](https://towardsdatascience.com/twitter-account-identity-prediction-with-large-language-models-c3ffef114d34)
- en: Leveraging Large Language Models for Advanced NLP
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†
- en: How to Use State-of-the-Art Models for Accurate Text Classification
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä½¿ç”¨æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œå‡†ç¡®çš„æ–‡æœ¬åˆ†ç±»
- en: '[](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)
    Â·9 min readÂ·Mar 7, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)
    Â·9åˆ†é’Ÿé˜…è¯»Â·2023å¹´3æœˆ7æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/38cebe369eb5ad1ff89e4d19b53744de.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38cebe369eb5ad1ff89e4d19b53744de.png)'
- en: Photo by [Jonathan Cooper](https://unsplash.com/ko/@theshuttervision?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Jonathan Cooper](https://unsplash.com/ko/@theshuttervision?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œæ¥è‡ª[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: This project aims to build a model capable of predicting the identity of account
    from itâ€™s tweets. I will walk through the steps I have taken from data processing,
    to fine tuning, and performance evaluation of the models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬é¡¹ç›®æ—¨åœ¨æ„å»ºä¸€ä¸ªèƒ½å¤Ÿä»æ¨æ–‡ä¸­é¢„æµ‹è´¦æˆ·èº«ä»½çš„æ¨¡å‹ã€‚æˆ‘å°†è¯¦ç»†ä»‹ç»ä»æ•°æ®å¤„ç†ã€å¾®è°ƒåˆ°æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„æ­¥éª¤ã€‚
- en: '*Before proceeding I would caveat that identity here is defined as male, female,
    or a brand. This in no way reflects my views on gender identity, this is simply
    a toy project demonstrating the power of transformers for sequence classification.
    In some of the code snippets you may notice gender is being used where we are
    referring to identify, this is simply how the data arrived.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨ç»§ç»­ä¹‹å‰ï¼Œæˆ‘éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œè¿™é‡Œçš„èº«ä»½å®šä¹‰ä¸ºç”·æ€§ã€å¥³æ€§æˆ–å“ç‰Œã€‚è¿™å¹¶ä¸åæ˜ æˆ‘å¯¹æ€§åˆ«èº«ä»½çš„çœ‹æ³•ï¼Œè¿™åªæ˜¯ä¸€ä¸ªå±•ç¤ºå˜å‹å™¨åœ¨åºåˆ—åˆ†ç±»ä¸­å¼ºå¤§èƒ½åŠ›çš„ç©å…·é¡¹ç›®ã€‚åœ¨ä¸€äº›ä»£ç ç‰‡æ®µä¸­ï¼Œä½ å¯èƒ½ä¼šæ³¨æ„åˆ°æ€§åˆ«è¢«ç”¨äºè¡¨ç¤ºèº«ä»½ï¼Œè¿™åªæ˜¯æ•°æ®åˆ°è¾¾çš„æ–¹å¼ã€‚*'
- en: Approach
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¹æ³•
- en: Due to the complex nature of text data, non-linear relationships being modelled
    I eliminated simpler methods and chose to leverage pretrained transformer models
    for this project.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ–‡æœ¬æ•°æ®çš„å¤æ‚æ€§å’Œéçº¿æ€§å…³ç³»çš„å»ºæ¨¡ï¼Œæˆ‘æ’é™¤äº†æ›´ç®€å•çš„æ–¹æ³•ï¼Œé€‰æ‹©åˆ©ç”¨é¢„è®­ç»ƒçš„å˜å‹å™¨æ¨¡å‹æ¥å®Œæˆè¿™ä¸ªé¡¹ç›®ã€‚
- en: Transformers are the current state-of-the-art for natural language processing
    and understanding tasks. The [Transformer](https://huggingface.co/) library from
    Hugging face gives you access to thousands of pre-trained models along with APIs
    to perform your own fine tuning. Most of the models have been trained on large
    text corpora, some across multiple languages. Without any fine tuning they have
    been shown to perform very well on similar text classification tasks including;
    sentiment analysis, emotion detection, and hate speech recognition.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å˜å‹å™¨ï¼ˆTransformersï¼‰æ˜¯å½“å‰è‡ªç„¶è¯­è¨€å¤„ç†å’Œç†è§£ä»»åŠ¡çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚[Transformers](https://huggingface.co/)åº“æ¥è‡ªHugging
    Faceï¼Œä¸ºä½ æä¾›äº†æ•°åƒä¸ªé¢„è®­ç»ƒæ¨¡å‹ä»¥åŠæ‰§è¡Œè‡ªå·±å¾®è°ƒçš„APIã€‚å¤§å¤šæ•°æ¨¡å‹å·²ç»åœ¨å¤§é‡æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œæœ‰äº›è·¨å¤šä¸ªè¯­è¨€ã€‚æ²¡æœ‰ç»è¿‡ä»»ä½•å¾®è°ƒï¼Œå®ƒä»¬åœ¨ç±»ä¼¼çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°éå¸¸å¥½ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€æƒ…ç»ªæ£€æµ‹å’Œä»‡æ¨è¨€è®ºè¯†åˆ«ã€‚
- en: I chose two models to fine tune along with a [zero-shot](https://en.wikipedia.org/wiki/Zero-shot_learning)
    model as a baseline for comparison.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘é€‰æ‹©äº†ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ª[é›¶æ ·æœ¬](https://en.wikipedia.org/wiki/Zero-shot_learning)æ¨¡å‹ä½œä¸ºæ¯”è¾ƒçš„åŸºå‡†ã€‚
- en: Zero-shot learning gives a baseline estimate of how powerful a transformer can
    be without fine-tuning on your particular classification task.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Zero-shot å­¦ä¹ æä¾›äº†ä¸€ä¸ªåŸºå‡†ä¼°è®¡ï¼Œæ˜¾ç¤ºäº†å˜æ¢å™¨åœ¨æ²¡æœ‰é’ˆå¯¹ç‰¹å®šåˆ†ç±»ä»»åŠ¡è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹èƒ½æœ‰å¤šå¼ºå¤§ã€‚
- en: Notebooks, Models & Repos
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬”è®°æœ¬ã€æ¨¡å‹ä¸ä»£ç åº“
- en: Due to computational cost I canâ€™t make the training scripts interactive. However,
    I have made the performance analysis notebook and models available to you. You
    can try the models yourself with live tweets!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè®¡ç®—æˆæœ¬ï¼Œæˆ‘æ— æ³•ä½¿è®­ç»ƒè„šæœ¬å…·å¤‡äº¤äº’æ€§ã€‚ä¸è¿‡ï¼Œæˆ‘å·²ç»å°†æ€§èƒ½åˆ†æç¬”è®°æœ¬å’Œæ¨¡å‹æä¾›ç»™ä½ ã€‚ä½ å¯ä»¥ä½¿ç”¨å®æ—¶æ¨æ–‡è‡ªå·±å°è¯•è¿™äº›æ¨¡å‹ï¼
- en: 'ğŸ“’[Notebook](https://mybinder.org/v2/gh/john-adeojo/Twitter-Transformer-Models/1457feb0bbdfdfa095368b03dae99aafdb1d2295?urlpath=lab%2Ftree%2FNotebooks%2FTwitter+Model+Analysis+Notebook+.ipynb):
    Model performance analysis Jupyter notebook'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ“’[ç¬”è®°æœ¬](https://mybinder.org/v2/gh/john-adeojo/Twitter-Transformer-Models/1457feb0bbdfdfa095368b03dae99aafdb1d2295?urlpath=lab%2Ftree%2FNotebooks%2FTwitter+Model+Analysis+Notebook+.ipynb):
    æ¨¡å‹æ€§èƒ½åˆ†æ Jupyter ç¬”è®°æœ¬'
- en: 'ğŸ¤—[Finetuned Distilbert-Base-Multilingual-Cased](https://huggingface.co/Johnade/distilbert-base-multilingual-cased-twitter-identity-classification):
    Model 1'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¤—[å¾®è°ƒ Distilbert-Base-Multilingual-Cased](https://huggingface.co/Johnade/distilbert-base-multilingual-cased-twitter-identity-classification):
    æ¨¡å‹ 1'
- en: 'ğŸ¤—[Finetuned Albert-base-v2](https://huggingface.co/Johnade/albert-base-v2-fine-tune-twitter-identity-classification)
    : Model 2'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¤—[å¾®è°ƒ Albert-base-v2](https://huggingface.co/Johnade/albert-base-v2-fine-tune-twitter-identity-classification)
    : æ¨¡å‹ 2'
- en: 'ğŸ’»[Github repository](https://github.com/john-adeojo/Twitter-Transformer-Models)
    : Training Scripts'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ’»[Github ä»£ç åº“](https://github.com/john-adeojo/Twitter-Transformer-Models) :
    è®­ç»ƒè„šæœ¬'
- en: 'ğŸ’¾[Data Source](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification):
    Kaggle'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ’¾[æ•°æ®æ¥æº](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification):
    Kaggle'
- en: Data Exploration & Pre-Processing
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®æ¢ç´¢ä¸é¢„å¤„ç†
- en: The Data was provided by the [Data For Everyone Library](https://www.crowdflower.com/data-for-everyone/)
    on [Crowdflower](https://www.crowdflower.com/). You can download the data yourself
    on [Kaggle](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)â´.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç”± [Data For Everyone Library](https://www.crowdflower.com/data-for-everyone/)
    æä¾›ï¼Œä½äº [Crowdflower](https://www.crowdflower.com/)ã€‚ä½ å¯ä»¥åœ¨ [Kaggle](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)â´
    ä¸‹è½½æ•°æ®ã€‚
- en: '*Note: The data has a public* [*domain license*](https://creativecommons.org/publicdomain/zero/1.0/)â´*.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šæ•°æ®æ‹¥æœ‰ä¸€ä¸ªå…¬å…±* [*é¢†åŸŸè®¸å¯è¯*](https://creativecommons.org/publicdomain/zero/1.0/)â´*ã€‚*'
- en: In total there are around 20k records containing usernames, tweets, user descriptions,
    and other twitter profile information. Although time constraints have not allowed
    me to check in detail, itâ€™s clear from a quick inspection that the tweets are
    multilingual. However, tweet text is messy with URLs, ascii characters, and special
    characters. This is to be expected from social media data, fortunately itâ€™s trivial
    to clean this with regular expressions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»å…±æœ‰å¤§çº¦ 20k æ¡è®°å½•ï¼ŒåŒ…å«ç”¨æˆ·åã€æ¨æ–‡ã€ç”¨æˆ·æè¿°å’Œå…¶ä»–æ¨ç‰¹ä¸ªäººä¿¡æ¯ã€‚è™½ç„¶æ—¶é—´é™åˆ¶ä½¿æˆ‘æ— æ³•è¯¦ç»†æ£€æŸ¥ï¼Œä½†ä»å¿«é€Ÿæ£€æŸ¥ä¸­å¯ä»¥æ˜æ˜¾çœ‹å‡ºè¿™äº›æ¨æ–‡æ˜¯å¤šè¯­è¨€çš„ã€‚ç„¶è€Œï¼Œæ¨æ–‡æ–‡æœ¬ä¸­æ··æ‚äº†
    URLã€ascii å­—ç¬¦å’Œç‰¹æ®Šå­—ç¬¦ã€‚è¿™æ˜¯ç¤¾äº¤åª’ä½“æ•°æ®çš„å¸¸è§ç°è±¡ï¼Œå¹¸è¿çš„æ˜¯ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¸…ç†è¿™äº›æ•°æ®éå¸¸ç®€å•ã€‚
- en: '![](../Images/bf85a63b8ecfe2d675e7e507508785cb.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf85a63b8ecfe2d675e7e507508785cb.png)'
- en: 'Image by Author: Examples of tweet text, user descriptions along side labels
    from the data'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒï¼šæ¨æ–‡æ–‡æœ¬ç¤ºä¾‹ã€ç”¨æˆ·æè¿°åŠæ•°æ®æ ‡ç­¾
- en: Profile image data is supplied in the form of URL links to image files. However
    many of these links are corrupted and therefore not useful in this prediction
    task. Ordinarily one might expect that profile images would be a great predictor
    for the identity of an account holder, in this case the data quality issues were
    too vast to overcome. Due to this I decided to use the tweet text and user descriptions
    for modelling.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ªäººèµ„æ–™å›¾ç‰‡æ•°æ®ä»¥ URL é“¾æ¥çš„å½¢å¼æä¾›ã€‚ç„¶è€Œï¼Œè®¸å¤šé“¾æ¥å·²æŸåï¼Œå› æ­¤åœ¨æ­¤é¢„æµ‹ä»»åŠ¡ä¸­æ— ç”¨ã€‚é€šå¸¸ï¼Œäººä»¬å¯èƒ½æœŸæœ›ä¸ªäººèµ„æ–™å›¾ç‰‡èƒ½å¾ˆå¥½åœ°é¢„æµ‹è´¦æˆ·æŒæœ‰è€…çš„èº«ä»½ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ•°æ®è´¨é‡é—®é¢˜è¿‡äºä¸¥é‡ã€‚ç”±äºè¿™ä¸ªåŸå› ï¼Œæˆ‘å†³å®šä½¿ç”¨æ¨æ–‡æ–‡æœ¬å’Œç”¨æˆ·æè¿°è¿›è¡Œå»ºæ¨¡ã€‚
- en: Missing & Unknown Variables
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¼ºå¤±ä¸æœªçŸ¥å˜é‡
- en: There is an identity label provided for most accounts. The label is well populated
    and has the values female, male, brand, and unknown â€” only 5.6% of all accounts
    are labelled unknown. Accounts where the identity label was unknown were simply
    removed from the analysis as they are impossible to test or train on.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°è´¦æˆ·éƒ½æä¾›äº†èº«ä»½æ ‡ç­¾ã€‚æ ‡ç­¾å†…å®¹ä¸°å¯Œï¼ŒåŒ…æ‹¬å¥³æ€§ã€ç”·æ€§ã€å“ç‰Œå’ŒæœªçŸ¥â€”â€”ä»… 5.6% çš„è´¦æˆ·è¢«æ ‡è®°ä¸ºæœªçŸ¥ã€‚èº«ä»½æ ‡ç­¾æœªçŸ¥çš„è´¦æˆ·è¢«ä»åˆ†æä¸­ç§»é™¤ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•è¿›è¡Œæµ‹è¯•æˆ–è®­ç»ƒã€‚
- en: Approximately 19% of user descriptions were blank. Having a blank user might
    signal something about the account holderâ€™s identity. Where the user description
    was blank, I simply imputed some text indicating this to allow the model to learn
    from these cases.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§çº¦ 19% çš„ç”¨æˆ·æè¿°ä¸ºç©ºã€‚ç©ºç™½çš„ç”¨æˆ·æè¿°å¯èƒ½ä¼šæš—ç¤ºè´¦æˆ·æŒæœ‰äººçš„èº«ä»½ã€‚å¯¹äºç©ºç™½çš„ç”¨æˆ·æè¿°ï¼Œæˆ‘ç®€å•åœ°æ’å…¥äº†ä¸€äº›æ–‡æœ¬ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿä»è¿™äº›æ¡ˆä¾‹ä¸­å­¦ä¹ ã€‚
- en: Expanding the Data
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰©å±•æ•°æ®
- en: To create more examples for the model to learn from, I concatenated the user
    descriptions and tweet text into a general twitter text field effectively doubling
    the number of text samples.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åˆ›å»ºæ›´å¤šçš„ç¤ºä¾‹ä¾›æ¨¡å‹å­¦ä¹ ï¼Œæˆ‘å°†ç”¨æˆ·æè¿°å’Œæ¨æ–‡æ–‡æœ¬åˆå¹¶åˆ°ä¸€ä¸ªé€šç”¨çš„ Twitter æ–‡æœ¬å­—æ®µä¸­ï¼Œæœ‰æ•ˆåœ°å°†æ–‡æœ¬æ ·æœ¬çš„æ•°é‡ç¿»å€ã€‚
- en: Train, Validation, Test
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•
- en: I split the data into 70% training, 15% validation, and 15% testing. To ensure
    no overlap, if there was an account that appeared multiple times in the data,
    I automatically assigned all the instances of it to the training data set. Besides
    this, accounts were allocated randomly to each of the data sets according to the
    proportions stated.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†æ•°æ®æ‹†åˆ†ä¸º 70% çš„è®­ç»ƒé›†ã€15% çš„éªŒè¯é›†å’Œ 15% çš„æµ‹è¯•é›†ã€‚ä¸ºäº†ç¡®ä¿æ²¡æœ‰é‡å ï¼Œå¦‚æœæ•°æ®ä¸­æœ‰è´¦æˆ·å‡ºç°å¤šæ¬¡ï¼Œæˆ‘ä¼šè‡ªåŠ¨å°†æ‰€æœ‰è¿™äº›å®ä¾‹åˆ†é…åˆ°è®­ç»ƒæ•°æ®é›†ä¸­ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè´¦æˆ·ä¼šæ ¹æ®ä¸Šè¿°æ¯”ä¾‹éšæœºåˆ†é…åˆ°å„ä¸ªæ•°æ®é›†ä¸­ã€‚
- en: Data pre-processing pipeline script
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é¢„å¤„ç†æµæ°´çº¿è„šæœ¬
- en: Hardware
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¡¬ä»¶
- en: Fine tuning was completed on each model separately and required a GPU to be
    practically achievable. The exact specs of my laptopâ€™s GPU is the Nvidia GE Force
    RTX 2060.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®è°ƒæ˜¯åœ¨æ¯ä¸ªæ¨¡å‹ä¸Šåˆ†åˆ«å®Œæˆçš„ï¼Œå¹¶ä¸”éœ€è¦ GPU æ‰èƒ½å®é™…å®ç°ã€‚æˆ‘çš„ç¬”è®°æœ¬ç”µè„‘çš„ GPU çš„å…·ä½“è§„æ ¼æ˜¯ Nvidia GE Force RTX 2060ã€‚
- en: '*Although this is considered high spec for a personal laptop, I found performance
    suffered on some of the larger language models ultimately limiting the set of
    model I could experiment with.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*è™½ç„¶è¿™è¢«è®¤ä¸ºæ˜¯ä¸ªäººç¬”è®°æœ¬ç”µè„‘çš„é«˜è§„æ ¼ï¼Œä½†æˆ‘å‘ç°ä¸€äº›å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å—åˆ°äº†é™åˆ¶ï¼Œæœ€ç»ˆé™åˆ¶äº†æˆ‘å¯ä»¥å°è¯•çš„æ¨¡å‹é›†ã€‚*'
- en: Software
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è½¯ä»¶
- en: To fully utilise my GPU I had to install the appropriate [CUDA](https://developer.nvidia.com/cuda-11-7-0-download-archive?target_os=Windows&target_arch=x86_64)
    kit for my GPU version and the version of [Pytorch](https://pytorch.org/get-started/locally/)
    I was using.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å……åˆ†åˆ©ç”¨æˆ‘çš„ GPUï¼Œæˆ‘å¿…é¡»ä¸ºæˆ‘çš„ GPU ç‰ˆæœ¬å’Œæ‰€ä½¿ç”¨çš„ [Pytorch](https://pytorch.org/get-started/locally/)
    ç‰ˆæœ¬å®‰è£…é€‚å½“çš„ [CUDA](https://developer.nvidia.com/cuda-11-7-0-download-archive?target_os=Windows&target_arch=x86_64)
    å·¥å…·åŒ…ã€‚
- en: '[CUDA](https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/) is a platform
    that enables your computer to perform parallel computations on data. This can
    drastically speed up the time it takes to fine tune transformers.'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[CUDA](https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/) æ˜¯ä¸€ä¸ªå¹³å°ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿå¯¹æ•°æ®æ‰§è¡Œå¹¶è¡Œè®¡ç®—ã€‚è¿™å¯ä»¥å¤§å¤§åŠ å¿«å¾®è°ƒ
    Transformers çš„æ—¶é—´ã€‚'
- en: '*It isnâ€™t advised to run this type of fine-tuning without a CUDA enabled GPU,
    unless youâ€™re happy to leave your machine running for what could be days.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*å»ºè®®ä¸è¦åœ¨æ²¡æœ‰ CUDA æ”¯æŒçš„ GPU ä¸Šè¿›è¡Œè¿™ç§ç±»å‹çš„å¾®è°ƒï¼Œé™¤éä½ æ„¿æ„è®©æœºå™¨è¿è¡Œå‡ å¤©ã€‚*'
- en: Python Packages
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python è½¯ä»¶åŒ…
- en: All steps of the modelling process were scripted in python. I leveraged the
    open-source Transformers library available from Hugging Face. I find this library
    to be well maintained with ample documentation available for guidance on best
    practices.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å»ºæ¨¡è¿‡ç¨‹çš„æ‰€æœ‰æ­¥éª¤éƒ½ç”¨ Python è„šæœ¬ç¼–å†™ã€‚æˆ‘åˆ©ç”¨äº†æ¥è‡ª Hugging Face çš„å¼€æº Transformers åº“ã€‚æˆ‘å‘ç°è¿™ä¸ªåº“ç»´æŠ¤è‰¯å¥½ï¼Œå¹¶ä¸”æœ‰å¤§é‡æ–‡æ¡£æä¾›æœ€ä½³å®è·µçš„æŒ‡å¯¼ã€‚
- en: For model performance testing, I used the open-source machine learning and data
    wrangling tools commonly used by data scientists. The list of key packages are
    as follows; [Transformers](https://huggingface.co/transformers/v3.2.0/pretrained_models.html),
    [Sci-kit Learn](https://scikit-learn.org/stable/), [Pandas](https://pandas.pydata.org/),
    [Numpy](https://numpy.org/), [Seaborn](https://seaborn.pydata.org/), [Matplotlib](https://matplotlib.org/),
    and [Pytorch](https://pytorch.org/get-started/locally/)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡å‹æ€§èƒ½æµ‹è¯•ä¸­ï¼Œæˆ‘ä½¿ç”¨äº†æ•°æ®ç§‘å­¦å®¶å¸¸ç”¨çš„å¼€æºæœºå™¨å­¦ä¹ å’Œæ•°æ®å¤„ç†å·¥å…·ã€‚å…³é”®è½¯ä»¶åŒ…çš„åˆ—è¡¨å¦‚ä¸‹ï¼š[Transformers](https://huggingface.co/transformers/v3.2.0/pretrained_models.html)ã€[Sci-kit
    Learn](https://scikit-learn.org/stable/)ã€[Pandas](https://pandas.pydata.org/)ã€[Numpy](https://numpy.org/)ã€[Seaborn](https://seaborn.pydata.org/)ã€[Matplotlib](https://matplotlib.org/)ã€å’Œ
    [Pytorch](https://pytorch.org/get-started/locally/)ã€‚
- en: Environment Management
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¯å¢ƒç®¡ç†
- en: '[Anaconda](https://www.anaconda.com/) as my primary environment manager creating
    a Conda virtual environment to install all software dependencies. I would strongly
    advise on this approach due to the large number of potentially conflicting dependencies.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[Anaconda](https://www.anaconda.com/) ä½œä¸ºæˆ‘çš„ä¸»è¦ç¯å¢ƒç®¡ç†å·¥å…·ï¼Œåˆ›å»ºäº†ä¸€ä¸ª Conda è™šæ‹Ÿç¯å¢ƒæ¥å®‰è£…æ‰€æœ‰çš„è½¯ä»¶ä¾èµ–ã€‚æˆ‘å¼ºçƒˆå»ºè®®ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œå› ä¸ºå¯èƒ½å­˜åœ¨å¤§é‡æ½œåœ¨çš„ä¾èµ–å†²çªã€‚'
- en: Model Fine Tuning
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¾®è°ƒ
- en: The models were fine tuned by training on the train data set and evaluating
    performance on the validation set. I have configured the fine tuning process to
    return the best model according to performance on the validation data set.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¨¡å‹é€šè¿‡åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šè®­ç»ƒå¹¶åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ€§èƒ½æ¥è¿›è¡Œå¾®è°ƒã€‚æˆ‘å·²é…ç½®å¾®è°ƒè¿‡ç¨‹ï¼Œä»¥æ ¹æ®éªŒè¯æ•°æ®é›†ä¸Šçš„è¡¨ç°è¿”å›æœ€ä½³æ¨¡å‹ã€‚
- en: Since this is a multiclass classification problem, the loss metric being minimised
    is the [cross-entropy loss](https://www.oreilly.com/library/view/hands-on-convolutional-neural/9781789130331/7f34b72e-f571-49d2-a37a-4ed6f8011c93.xhtml).
    Better model performance is essentially a lower cross entropy loss on the validation
    set. Hyper parameters for the candidate models were set identical to each other
    to aid comparison.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™æ˜¯ä¸€ä¸ªå¤šç±»åˆ†ç±»é—®é¢˜ï¼Œå› æ­¤æ­£åœ¨æœ€å°åŒ–çš„æŸå¤±æŒ‡æ ‡æ˜¯ [äº¤å‰ç†µæŸå¤±](https://www.oreilly.com/library/view/hands-on-convolutional-neural/9781789130331/7f34b72e-f571-49d2-a37a-4ed6f8011c93.xhtml)ã€‚æ›´å¥½çš„æ¨¡å‹æ€§èƒ½å®è´¨ä¸Šæ˜¯åœ¨éªŒè¯é›†ä¸Šè¾ƒä½çš„äº¤å‰ç†µæŸå¤±ã€‚å€™é€‰æ¨¡å‹çš„è¶…å‚æ•°è®¾ç½®ç›¸åŒï¼Œä»¥ä¾¿è¿›è¡Œæ¯”è¾ƒã€‚
- en: A snippet of the script for fine tuning a transformer model
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºå¾®è°ƒå˜æ¢å™¨æ¨¡å‹çš„è„šæœ¬ç‰‡æ®µ
- en: 'Model 0: Multilingual-MiniLMv2-L6-mnli-xnliÂ¹'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹ 0ï¼šMultilingual-MiniLMv2-L6-mnli-xnliÂ¹
- en: I begin my analysis by performing a zero-shot classification to give a baseline
    from which to assess the fine-tuned models. The reference text for this model
    suggests that it can perform inference on 100+ languagesÂ¹, which appears to be
    excellent coverage for our problem.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘é€šè¿‡æ‰§è¡Œé›¶-shot åˆ†ç±»å¼€å§‹åˆ†æï¼Œä»¥æä¾›ä¸€ä¸ªåŸºçº¿ï¼Œä»ä¸­è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹çš„å‚è€ƒæ–‡æœ¬è¡¨æ˜ï¼Œå®ƒå¯ä»¥åœ¨100å¤šç§è¯­è¨€ä¸Šè¿›è¡Œæ¨æ–­Â¹ï¼Œè¿™å¯¹æˆ‘ä»¬çš„é—®é¢˜æ¥è¯´è¦†ç›–èŒƒå›´ç›¸å½“å¥½ã€‚
- en: 'Model 1: Distilbert Base Multilingual CasedÂ²'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹ 1ï¼šDistilbert Base Multilingual CasedÂ²
- en: Distilbert-base-multilingual-cased has been trained on 104 different languages,
    also providing great coverage. The model is cased so it can recognises capitalisation
    and non-capitalisation in text.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Distilbert-base-multilingual-cased å·²åœ¨104ç§ä¸åŒè¯­è¨€ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæä¾›äº†å¹¿æ³›çš„è¦†ç›–ã€‚è¯¥æ¨¡å‹æ˜¯å¤§å°å†™æ•æ„Ÿçš„ï¼Œå› æ­¤å¯ä»¥è¯†åˆ«æ–‡æœ¬ä¸­çš„å¤§å†™å’Œå°å†™ã€‚
- en: '**Model (pre)-training:** The model has been pretrained on a concatenation
    of Wikipedia pages.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹ï¼ˆé¢„ï¼‰è®­ç»ƒï¼š** è¯¥æ¨¡å‹å·²åœ¨ç»´åŸºç™¾ç§‘é¡µé¢çš„æ‹¼æ¥ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚'
- en: '**Model architecture:** Transformer-based language model with 6 layers, 769
    dimensions and 12 heads totalling 134 Million parameters.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹æ¶æ„ï¼š** åŸºäºå˜æ¢å™¨çš„è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰6å±‚ï¼Œ769ç»´åº¦å’Œ12ä¸ªå¤´ï¼Œæ€»å…±æœ‰1.34äº¿ä¸ªå‚æ•°ã€‚'
- en: '**Fine tuning:** Model fine tuning took approximately 21 minutes running on
    my hardware. There is some evidence to suggest the model had converged from reviewing
    the evaluation loss vs. the training step chart.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¾®è°ƒï¼š** åœ¨æˆ‘çš„ç¡¬ä»¶ä¸Šè¿è¡Œæ¨¡å‹å¾®è°ƒå¤§çº¦èŠ±è´¹äº†21åˆ†é’Ÿã€‚æœ‰ä¸€äº›è¯æ®è¡¨æ˜ï¼Œæ¨¡å‹å·²ç»æ”¶æ•›ï¼Œè¿™äº›è¯æ®æ¥è‡ªè¯„ä¼°æŸå¤±ä¸è®­ç»ƒæ­¥éª¤å›¾çš„å¯¹æ¯”ã€‚'
- en: '![](../Images/f18d2935fb14baa556d2556464603d24.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f18d2935fb14baa556d2556464603d24.png)'
- en: 'Image by Author: Evaluation loss and training loss'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒï¼šè¯„ä¼°æŸå¤±å’Œè®­ç»ƒæŸå¤±
- en: 'Model 2: Albert-base-vÂ³'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹ 2ï¼šAlbert-base-vÂ³
- en: The model has been pretrained on English text and is uncased meaning it retains
    no information about capitalisation of text. Albert was specifically designed
    to address memory limitation that occur with training larger models. The model
    uses a self-supervised loss that focuses on modelling inter-sentence coherence.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹å·²åœ¨è‹±æ–‡æ–‡æœ¬ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ä¸”æ˜¯æ— å¤§å°å†™æ•æ„Ÿçš„ï¼Œè¿™æ„å‘³ç€å®ƒä¸ä¿ç•™æ–‡æœ¬çš„å¤§å°å†™ä¿¡æ¯ã€‚Albert ä¸“é—¨è®¾è®¡ç”¨äºè§£å†³è®­ç»ƒè¾ƒå¤§æ¨¡å‹æ—¶å‡ºç°çš„å†…å­˜é™åˆ¶é—®é¢˜ã€‚è¯¥æ¨¡å‹ä½¿ç”¨ä¸€ç§è‡ªç›‘ç£æŸå¤±ï¼Œé‡ç‚¹å»ºæ¨¡å¥å­é—´çš„è¿è´¯æ€§ã€‚
- en: '**Model (pre)-training:** Albert was pretrained on the BOOKCORPUS and English
    Wikipedia to achieve its baseline.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹ï¼ˆé¢„ï¼‰è®­ç»ƒï¼š** Albert åœ¨ BOOKCORPUS å’Œè‹±æ–‡ç»´åŸºç™¾ç§‘ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»¥å®ç°å…¶åŸºçº¿ã€‚'
- en: '**Model architecture:** transformer-based language model with 12 repeating
    layers, 128 embedding, 768-hidden, 12 heads and 11 million parameters.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹æ¶æ„ï¼š** åŸºäºå˜æ¢å™¨çš„è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰12ä¸ªé‡å¤å±‚ï¼Œ128ä¸ªåµŒå…¥ï¼Œ768ä¸ªéšè—å±‚ï¼Œ12ä¸ªå¤´å’Œ1100ä¸‡ä¸ªå‚æ•°ã€‚'
- en: '**Fine tuning:** Model fine tuning took approximately 35 minutes to complete.
    Model convergence appears to be likely indicated by the â€œtroughâ€ of the loss metric.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¾®è°ƒï¼š** æ¨¡å‹å¾®è°ƒå¤§çº¦èŠ±è´¹äº†35åˆ†é’Ÿå®Œæˆã€‚æ¨¡å‹æ”¶æ•›å¯èƒ½é€šè¿‡æŸå¤±æŒ‡æ ‡çš„â€œä½è°·â€æ¥æŒ‡ç¤ºã€‚'
- en: '![](../Images/59c0a5ece8b1d1d737be9cded9201d69.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59c0a5ece8b1d1d737be9cded9201d69.png)'
- en: 'Image by Author: Evaluation loss and training loss'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒï¼šè¯„ä¼°æŸå¤±å’Œè®­ç»ƒæŸå¤±
- en: Model Performance
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ€§èƒ½
- en: Given that this is a multiclass learning task, I have assessed model performance
    over F1, Recall, Precision and Accuracy at both the individual class and global
    level. Performance metrics were scored on the test data set.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºè¿™æ˜¯ä¸€ä¸ªå¤šç±»å­¦ä¹ ä»»åŠ¡ï¼Œæˆ‘è¯„ä¼°äº†æ¨¡å‹åœ¨ F1ã€å¬å›ç‡ã€ç²¾ç¡®ç‡å’Œå‡†ç¡®ç‡ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬æ¯ä¸ªç±»å’Œå…¨å±€æ°´å¹³ã€‚æ€§èƒ½æŒ‡æ ‡åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè¯„åˆ†ã€‚
- en: Accuracy scores were 37% for zero-shot, 59% for Albert and 59% for Distilbert
    overall.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: é›¶æ ·æœ¬çš„å‡†ç¡®ç‡ä¸º37%ï¼ŒAlbertå’ŒDistilbertçš„å‡†ç¡®ç‡å‡ä¸º59%ã€‚
- en: Observations
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§‚å¯Ÿ
- en: Overall, both Albert and Distilbert performed better on the test set than the
    zero-shot classification baseline. This is the result I was expecting given that
    the zero-shot model does not hold any knowledge of the classification task at
    hand. I believe this is more evidence that there is merit in fine tuning your
    model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼ŒAlbertå’ŒDistilbertåœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºé›¶æ ·æœ¬åˆ†ç±»åŸºçº¿ã€‚è¿™æ˜¯æˆ‘é¢„æœŸçš„ç»“æœï¼Œå› ä¸ºé›¶æ ·æœ¬æ¨¡å‹å¯¹å½“å‰åˆ†ç±»ä»»åŠ¡æ²¡æœ‰ä»»ä½•çŸ¥è¯†ã€‚æˆ‘è®¤ä¸ºè¿™æ›´è¿›ä¸€æ­¥è¯æ˜äº†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„ä»·å€¼ã€‚
- en: Although there are notable performance differences, we canâ€™t definitively say
    which is better between the two fine tuned models until we have a prolonged test
    period of these models in the wild.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ï¼Œä½†åœ¨è¿™ä¸¤ç§å¾®è°ƒæ¨¡å‹ä¹‹é—´ï¼Œæˆ‘ä»¬ä¸èƒ½æ˜ç¡®è¯´å“ªç§æ¨¡å‹æ›´å¥½ï¼Œç›´åˆ°æˆ‘ä»¬åœ¨å®é™…åº”ç”¨ä¸­å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œé•¿æ—¶é—´çš„æµ‹è¯•ã€‚
- en: Notable performance differences
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚
- en: Albert appeared to be more confident with its predictions having a 75th percentile
    for overall prediction confidence of 82% compared to Distilbertâ€™s 66%.
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Albertåœ¨é¢„æµ‹æ—¶ä¼¼ä¹æ›´åŠ è‡ªä¿¡ï¼Œå…¶æ•´ä½“é¢„æµ‹ç½®ä¿¡åº¦çš„75ç™¾åˆ†ä½æ•°ä¸º82%ï¼Œè€ŒDistilbertä¸º66%ã€‚
- en: ''
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All models had low precision, recall, and F1 for predicting a male identity.
    This might be due to wider variation in male tweets compared with female and brand.
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ¨¡å‹åœ¨é¢„æµ‹ç”·æ€§èº«ä»½æ—¶çš„ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1å€¼éƒ½è¾ƒä½ã€‚è¿™å¯èƒ½æ˜¯ç”±äºç”·æ€§æ¨æ–‡çš„å˜å¼‚æ€§å¤§äºå¥³æ€§å’Œå“ç‰Œæ¨æ–‡ã€‚
- en: ''
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All models had high performance scores on predicting brands relative to the
    other identities. Also, models had notably higher confidence in predicting brands
    than they did for predicting male or female users. I would imagine this is due
    to the standardised way brands put out their messaging on social media relative
    to personal users.
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ¨¡å‹åœ¨é¢„æµ‹å“ç‰Œæ–¹é¢çš„è¡¨ç°éƒ½ä¼˜äºé¢„æµ‹å…¶ä»–èº«ä»½çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œç›¸æ¯”äºé¢„æµ‹ç”·æ€§æˆ–å¥³æ€§ç”¨æˆ·ï¼Œæ¨¡å‹åœ¨é¢„æµ‹å“ç‰Œæ—¶è¡¨ç°å‡ºæ˜¾è‘—æ›´é«˜çš„ä¿¡å¿ƒã€‚æˆ‘çŒœè¿™æ˜¯å› ä¸ºå“ç‰Œåœ¨ç¤¾äº¤åª’ä½“ä¸Šå‘å¸ƒä¿¡æ¯çš„æ–¹å¼ç›¸å¯¹æ ‡å‡†åŒ–ï¼Œè€Œä¸ªäººç”¨æˆ·åˆ™ä¸ç„¶ã€‚
- en: '![](../Images/b0150583c4af62bf5abcd3a9a40415b9.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0150583c4af62bf5abcd3a9a40415b9.png)'
- en: 'Image by Author: Performance metric for all models'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒï¼šæ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡
- en: '![](../Images/2fab145ff5a9df507e2e1c9ba3eb671e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fab145ff5a9df507e2e1c9ba3eb671e.png)'
- en: 'Image by author: Confidence scores at quartile intervals'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒï¼šå››åˆ†ä½æ•°é—´éš”çš„ç½®ä¿¡åº¦å¾—åˆ†
- en: Areas for Improvement
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ”¹è¿›æ–¹å‘
- en: 'I would recommend the following to improve model uplift:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å»ºè®®é‡‡å–ä»¥ä¸‹æªæ–½ä»¥æå‡æ¨¡å‹æ€§èƒ½ï¼š
- en: Increased training examples
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¢åŠ è®­ç»ƒæ ·æœ¬
- en: More data can help the model to generalise better improving overall performance.
    There was certainly evidence of overfitting as I noticed model performance on
    the evaluation set began to suffer while performance on the test set continued
    to improve, more data would help to alleviate this somewhat.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šçš„æ•°æ®å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°æ³›åŒ–ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚ç¡®å®æœ‰è¿‡æ‹Ÿåˆçš„è¿¹è±¡ï¼Œå› ä¸ºæˆ‘æ³¨æ„åˆ°æ¨¡å‹åœ¨è¯„ä¼°é›†ä¸Šçš„æ€§èƒ½å¼€å§‹ä¸‹é™ï¼Œè€Œåœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½æŒç»­æé«˜ï¼Œæ›´å¤šçš„æ•°æ®å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£è¿™ç§æƒ…å†µã€‚
- en: Overfitting was more the case with the Distilbert model than Albert due to itâ€™s
    larger size. Large language models are more flexible but can also be more prone
    to overfitting.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºDistilbertæ¨¡å‹çš„ä½“ç§¯è¾ƒå¤§ï¼Œç›¸æ¯”äºAlbertæ¨¡å‹ï¼Œå®ƒæ›´å®¹æ˜“å‘ç”Ÿè¿‡æ‹Ÿåˆã€‚å¤§å‹è¯­è¨€æ¨¡å‹æ›´åŠ çµæ´»ï¼Œä½†ä¹Ÿæ›´å®¹æ˜“è¿‡æ‹Ÿåˆã€‚
- en: Fine tuning the twitter-xlm-roberta-base model on multiple GPUs to achieve convergence
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨å¤šä¸ªGPUä¸Šå¯¹twitter-xlm-roberta-baseæ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥å®ç°æ”¶æ•›
- en: There is a model by [Cardiff NLP](https://arxiv.org/abs/2104.12250) explicitly
    pretrained on twitter text and is multilingual. I did make an attempt at fine
    tuning this model but was limited by hardware. The model is large at 198M parameters
    and took almost 4 hours to run showing no signs of convergence. In theory, Roberta
    should greatly outperform Distilbert and Albert due to its pre-training on twitter
    data. However, more data would be required to prevent the likely overfitting on
    this larger model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ä¸ªç”±[Cardiff NLP](https://arxiv.org/abs/2104.12250)å¼€å‘çš„æ¨¡å‹ï¼Œä¸“é—¨åœ¨æ¨ç‰¹æ–‡æœ¬ä¸Šé¢„è®­ç»ƒï¼Œå¹¶ä¸”æ˜¯å¤šè¯­è¨€çš„ã€‚æˆ‘ç¡®å®å°è¯•å¯¹è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½†ç”±äºç¡¬ä»¶é™åˆ¶ï¼Œæ•ˆæœä¸ä½³ã€‚è¿™ä¸ªæ¨¡å‹å‚æ•°å¤šè¾¾198Mï¼Œè¿è¡Œäº†è¿‘4å°æ—¶å´æ²¡æœ‰æ˜¾ç¤ºå‡ºæ”¶æ•›çš„è¿¹è±¡ã€‚ç†è®ºä¸Šï¼Œç”±äºRobertaåœ¨æ¨ç‰¹æ•°æ®ä¸Šè¿›è¡Œè¿‡é¢„è®­ç»ƒï¼Œå®ƒåº”è¯¥æ¯”Distilbertå’ŒAlbertè¡¨ç°æ›´å¥½ã€‚ç„¶è€Œï¼Œéœ€è¦æ›´å¤šçš„æ•°æ®æ¥é˜²æ­¢è¿™ä¸ªå¤§å‹æ¨¡å‹çš„è¿‡æ‹Ÿåˆã€‚
- en: Explore the potential of multi-modal transformer architectures.
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢ç´¢å¤šæ¨¡æ€å˜æ¢å™¨æ¶æ„çš„æ½œåŠ›ã€‚
- en: If we could improve the quality of the profile picture data, I think a combination
    of tweet text and image could significantly improve the performance of our classifier.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬èƒ½æ”¹å–„ä¸ªäººèµ„æ–™å›¾ç‰‡æ•°æ®çš„è´¨é‡ï¼Œæˆ‘è®¤ä¸ºæ¨æ–‡æ–‡æœ¬å’Œå›¾åƒçš„ç»“åˆå¯èƒ½ä¼šæ˜¾è‘—æå‡æˆ‘ä»¬åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚
- en: Thanks for reading
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»
- en: '[](https://johnadeojo.medium.com/membership?source=post_page-----c3ffef114d34--------------------------------)
    [## Join Medium with my referral link - John Adeojo'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://johnadeojo.medium.com/membership?source=post_page-----c3ffef114d34--------------------------------)
    [## é€šè¿‡æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium - John Adeojo'
- en: I share data science projects, experiences, and expertise to assist you on your
    journey You can sign up to medium viaâ€¦
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æˆ‘åˆ†äº«æ•°æ®ç§‘å­¦é¡¹ç›®ã€ç»éªŒå’Œä¸“ä¸šçŸ¥è¯†ï¼Œå¸®åŠ©ä½ åœ¨æ—…é€”ä¸­ã€‚ä½ å¯ä»¥é€šè¿‡â€¦â€¦
- en: johnadeojo.medium.com](https://johnadeojo.medium.com/membership?source=post_page-----c3ffef114d34--------------------------------)
    [](https://www.john-adeojo.com/?source=post_page-----c3ffef114d34--------------------------------)
    [## Home | John Adeojo
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[johnadeojo.medium.com](https://johnadeojo.medium.com/membership?source=post_page-----c3ffef114d34--------------------------------)
    [](https://www.john-adeojo.com/?source=post_page-----c3ffef114d34--------------------------------)
    [## é¦–é¡µ | John Adeojo'
- en: A Bit About Me Welcome to my professional portfolio! I'm a seasoned data scientist
    and machine learning (ML) expertâ€¦
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…³äºæˆ‘çš„ä¸€ç‚¹ä»‹ç» æ¬¢è¿æ¥åˆ°æˆ‘çš„ä¸“ä¸šä½œå“é›†ï¼æˆ‘æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æ•°æ®ç§‘å­¦å®¶å’Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä¸“å®¶â€¦â€¦
- en: www.john-adeojo.com](https://www.john-adeojo.com/?source=post_page-----c3ffef114d34--------------------------------)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.john-adeojo.com](https://www.john-adeojo.com/?source=post_page-----c3ffef114d34--------------------------------)'
- en: '*[1] Laurer, M., van Atteveldt, W., Salleras Casas, A., & Welbers, K. (2022).
    Less Annotating, More Classifying â€” Addressing the Data Scarcity Issue of Supervised
    Machine Learning with Deep Transfer Learning and BERT â€” NLI [Preprint]. Open Science
    Framework.*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*[1] Laurer, M., van Atteveldt, W., Salleras Casas, A., & Welbers, K. (2022).
    æ›´å°‘æ ‡æ³¨ï¼Œæ›´å¤šåˆ†ç±» â€” é€šè¿‡æ·±åº¦è¿ç§»å­¦ä¹ å’Œ BERT è§£å†³ç›‘ç£æœºå™¨å­¦ä¹ çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ â€” NLI [é¢„å°æœ¬]ã€‚å¼€æ”¾ç§‘å­¦æ¡†æ¶.*'
- en: '*[2] Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled
    version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*[2] Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERTï¼ŒBERT
    çš„ä¸€ç§ç²¾ç®€ç‰ˆæœ¬ï¼šæ›´å°ã€æ›´å¿«ã€æ›´ä¾¿å®œä¸”æ›´è½»é‡ã€‚arXiv é¢„å°æœ¬ arXiv:1910.01108.*'
- en: '[3] *Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R.
    (2019). ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.
    CoRR, abs/1909.11942\.* [*http://arxiv.org/abs/1909.11942*](http://arxiv.org/abs/1909.11942)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] *Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R.
    (2019). ALBERT: ä¸€ç§è½»é‡çº§çš„ BERTï¼Œç”¨äºè‡ªç›‘ç£è¯­è¨€è¡¨ç¤ºå­¦ä¹ ã€‚CoRR, abs/1909.11942\.* [*http://arxiv.org/abs/1909.11942*](http://arxiv.org/abs/1909.11942)'
- en: '[4] *Twitter User Gender Classification*. Kaggle. Retrieved March 15, 2023,
    from [https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] *Twitter ç”¨æˆ·æ€§åˆ«åˆ†ç±»*ã€‚Kaggleã€‚æ£€ç´¢äº 2023å¹´3æœˆ15æ—¥ï¼Œæ¥è‡ª [https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)'
