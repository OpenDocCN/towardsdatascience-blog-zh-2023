- en: The Power of Transformers in Predicting Twitter Account Identities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/twitter-account-identity-prediction-with-large-language-models-c3ffef114d34](https://towardsdatascience.com/twitter-account-identity-prediction-with-large-language-models-c3ffef114d34)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Leveraging Large Language Models for Advanced NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How to Use State-of-the-Art Models for Accurate Text Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)
    ¬∑9 min read¬∑Mar 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38cebe369eb5ad1ff89e4d19b53744de.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jonathan Cooper](https://unsplash.com/ko/@theshuttervision?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This project aims to build a model capable of predicting the identity of account
    from it‚Äôs tweets. I will walk through the steps I have taken from data processing,
    to fine tuning, and performance evaluation of the models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Before proceeding I would caveat that identity here is defined as male, female,
    or a brand. This in no way reflects my views on gender identity, this is simply
    a toy project demonstrating the power of transformers for sequence classification.
    In some of the code snippets you may notice gender is being used where we are
    referring to identify, this is simply how the data arrived.*'
  prefs: []
  type: TYPE_NORMAL
- en: Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the complex nature of text data, non-linear relationships being modelled
    I eliminated simpler methods and chose to leverage pretrained transformer models
    for this project.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers are the current state-of-the-art for natural language processing
    and understanding tasks. The [Transformer](https://huggingface.co/) library from
    Hugging face gives you access to thousands of pre-trained models along with APIs
    to perform your own fine tuning. Most of the models have been trained on large
    text corpora, some across multiple languages. Without any fine tuning they have
    been shown to perform very well on similar text classification tasks including;
    sentiment analysis, emotion detection, and hate speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: I chose two models to fine tune along with a [zero-shot](https://en.wikipedia.org/wiki/Zero-shot_learning)
    model as a baseline for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot learning gives a baseline estimate of how powerful a transformer can
    be without fine-tuning on your particular classification task.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Notebooks, Models & Repos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to computational cost I can‚Äôt make the training scripts interactive. However,
    I have made the performance analysis notebook and models available to you. You
    can try the models yourself with live tweets!
  prefs: []
  type: TYPE_NORMAL
- en: 'üìí[Notebook](https://mybinder.org/v2/gh/john-adeojo/Twitter-Transformer-Models/1457feb0bbdfdfa095368b03dae99aafdb1d2295?urlpath=lab%2Ftree%2FNotebooks%2FTwitter+Model+Analysis+Notebook+.ipynb):
    Model performance analysis Jupyter notebook'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§ó[Finetuned Distilbert-Base-Multilingual-Cased](https://huggingface.co/Johnade/distilbert-base-multilingual-cased-twitter-identity-classification):
    Model 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§ó[Finetuned Albert-base-v2](https://huggingface.co/Johnade/albert-base-v2-fine-tune-twitter-identity-classification)
    : Model 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'üíª[Github repository](https://github.com/john-adeojo/Twitter-Transformer-Models)
    : Training Scripts'
  prefs: []
  type: TYPE_NORMAL
- en: 'üíæ[Data Source](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification):
    Kaggle'
  prefs: []
  type: TYPE_NORMAL
- en: Data Exploration & Pre-Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Data was provided by the [Data For Everyone Library](https://www.crowdflower.com/data-for-everyone/)
    on [Crowdflower](https://www.crowdflower.com/). You can download the data yourself
    on [Kaggle](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)‚Å¥.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: The data has a public* [*domain license*](https://creativecommons.org/publicdomain/zero/1.0/)‚Å¥*.*'
  prefs: []
  type: TYPE_NORMAL
- en: In total there are around 20k records containing usernames, tweets, user descriptions,
    and other twitter profile information. Although time constraints have not allowed
    me to check in detail, it‚Äôs clear from a quick inspection that the tweets are
    multilingual. However, tweet text is messy with URLs, ascii characters, and special
    characters. This is to be expected from social media data, fortunately it‚Äôs trivial
    to clean this with regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf85a63b8ecfe2d675e7e507508785cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Examples of tweet text, user descriptions along side labels
    from the data'
  prefs: []
  type: TYPE_NORMAL
- en: Profile image data is supplied in the form of URL links to image files. However
    many of these links are corrupted and therefore not useful in this prediction
    task. Ordinarily one might expect that profile images would be a great predictor
    for the identity of an account holder, in this case the data quality issues were
    too vast to overcome. Due to this I decided to use the tweet text and user descriptions
    for modelling.
  prefs: []
  type: TYPE_NORMAL
- en: Missing & Unknown Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is an identity label provided for most accounts. The label is well populated
    and has the values female, male, brand, and unknown ‚Äî only 5.6% of all accounts
    are labelled unknown. Accounts where the identity label was unknown were simply
    removed from the analysis as they are impossible to test or train on.
  prefs: []
  type: TYPE_NORMAL
- en: Approximately 19% of user descriptions were blank. Having a blank user might
    signal something about the account holder‚Äôs identity. Where the user description
    was blank, I simply imputed some text indicating this to allow the model to learn
    from these cases.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To create more examples for the model to learn from, I concatenated the user
    descriptions and tweet text into a general twitter text field effectively doubling
    the number of text samples.
  prefs: []
  type: TYPE_NORMAL
- en: Train, Validation, Test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I split the data into 70% training, 15% validation, and 15% testing. To ensure
    no overlap, if there was an account that appeared multiple times in the data,
    I automatically assigned all the instances of it to the training data set. Besides
    this, accounts were allocated randomly to each of the data sets according to the
    proportions stated.
  prefs: []
  type: TYPE_NORMAL
- en: Data pre-processing pipeline script
  prefs: []
  type: TYPE_NORMAL
- en: Hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine tuning was completed on each model separately and required a GPU to be
    practically achievable. The exact specs of my laptop‚Äôs GPU is the Nvidia GE Force
    RTX 2060.
  prefs: []
  type: TYPE_NORMAL
- en: '*Although this is considered high spec for a personal laptop, I found performance
    suffered on some of the larger language models ultimately limiting the set of
    model I could experiment with.*'
  prefs: []
  type: TYPE_NORMAL
- en: Software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To fully utilise my GPU I had to install the appropriate [CUDA](https://developer.nvidia.com/cuda-11-7-0-download-archive?target_os=Windows&target_arch=x86_64)
    kit for my GPU version and the version of [Pytorch](https://pytorch.org/get-started/locally/)
    I was using.
  prefs: []
  type: TYPE_NORMAL
- en: '[CUDA](https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/) is a platform
    that enables your computer to perform parallel computations on data. This can
    drastically speed up the time it takes to fine tune transformers.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*It isn‚Äôt advised to run this type of fine-tuning without a CUDA enabled GPU,
    unless you‚Äôre happy to leave your machine running for what could be days.*'
  prefs: []
  type: TYPE_NORMAL
- en: Python Packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All steps of the modelling process were scripted in python. I leveraged the
    open-source Transformers library available from Hugging Face. I find this library
    to be well maintained with ample documentation available for guidance on best
    practices.
  prefs: []
  type: TYPE_NORMAL
- en: For model performance testing, I used the open-source machine learning and data
    wrangling tools commonly used by data scientists. The list of key packages are
    as follows; [Transformers](https://huggingface.co/transformers/v3.2.0/pretrained_models.html),
    [Sci-kit Learn](https://scikit-learn.org/stable/), [Pandas](https://pandas.pydata.org/),
    [Numpy](https://numpy.org/), [Seaborn](https://seaborn.pydata.org/), [Matplotlib](https://matplotlib.org/),
    and [Pytorch](https://pytorch.org/get-started/locally/)
  prefs: []
  type: TYPE_NORMAL
- en: Environment Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Anaconda](https://www.anaconda.com/) as my primary environment manager creating
    a Conda virtual environment to install all software dependencies. I would strongly
    advise on this approach due to the large number of potentially conflicting dependencies.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Fine Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The models were fine tuned by training on the train data set and evaluating
    performance on the validation set. I have configured the fine tuning process to
    return the best model according to performance on the validation data set.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is a multiclass classification problem, the loss metric being minimised
    is the [cross-entropy loss](https://www.oreilly.com/library/view/hands-on-convolutional-neural/9781789130331/7f34b72e-f571-49d2-a37a-4ed6f8011c93.xhtml).
    Better model performance is essentially a lower cross entropy loss on the validation
    set. Hyper parameters for the candidate models were set identical to each other
    to aid comparison.
  prefs: []
  type: TYPE_NORMAL
- en: A snippet of the script for fine tuning a transformer model
  prefs: []
  type: TYPE_NORMAL
- en: 'Model 0: Multilingual-MiniLMv2-L6-mnli-xnli¬π'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I begin my analysis by performing a zero-shot classification to give a baseline
    from which to assess the fine-tuned models. The reference text for this model
    suggests that it can perform inference on 100+ languages¬π, which appears to be
    excellent coverage for our problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model 1: Distilbert Base Multilingual Cased¬≤'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distilbert-base-multilingual-cased has been trained on 104 different languages,
    also providing great coverage. The model is cased so it can recognises capitalisation
    and non-capitalisation in text.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model (pre)-training:** The model has been pretrained on a concatenation
    of Wikipedia pages.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model architecture:** Transformer-based language model with 6 layers, 769
    dimensions and 12 heads totalling 134 Million parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine tuning:** Model fine tuning took approximately 21 minutes running on
    my hardware. There is some evidence to suggest the model had converged from reviewing
    the evaluation loss vs. the training step chart.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f18d2935fb14baa556d2556464603d24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Evaluation loss and training loss'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model 2: Albert-base-v¬≥'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model has been pretrained on English text and is uncased meaning it retains
    no information about capitalisation of text. Albert was specifically designed
    to address memory limitation that occur with training larger models. The model
    uses a self-supervised loss that focuses on modelling inter-sentence coherence.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model (pre)-training:** Albert was pretrained on the BOOKCORPUS and English
    Wikipedia to achieve its baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model architecture:** transformer-based language model with 12 repeating
    layers, 128 embedding, 768-hidden, 12 heads and 11 million parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine tuning:** Model fine tuning took approximately 35 minutes to complete.
    Model convergence appears to be likely indicated by the ‚Äútrough‚Äù of the loss metric.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59c0a5ece8b1d1d737be9cded9201d69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Evaluation loss and training loss'
  prefs: []
  type: TYPE_NORMAL
- en: Model Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given that this is a multiclass learning task, I have assessed model performance
    over F1, Recall, Precision and Accuracy at both the individual class and global
    level. Performance metrics were scored on the test data set.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy scores were 37% for zero-shot, 59% for Albert and 59% for Distilbert
    overall.
  prefs: []
  type: TYPE_NORMAL
- en: Observations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overall, both Albert and Distilbert performed better on the test set than the
    zero-shot classification baseline. This is the result I was expecting given that
    the zero-shot model does not hold any knowledge of the classification task at
    hand. I believe this is more evidence that there is merit in fine tuning your
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Although there are notable performance differences, we can‚Äôt definitively say
    which is better between the two fine tuned models until we have a prolonged test
    period of these models in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: Notable performance differences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Albert appeared to be more confident with its predictions having a 75th percentile
    for overall prediction confidence of 82% compared to Distilbert‚Äôs 66%.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All models had low precision, recall, and F1 for predicting a male identity.
    This might be due to wider variation in male tweets compared with female and brand.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All models had high performance scores on predicting brands relative to the
    other identities. Also, models had notably higher confidence in predicting brands
    than they did for predicting male or female users. I would imagine this is due
    to the standardised way brands put out their messaging on social media relative
    to personal users.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/b0150583c4af62bf5abcd3a9a40415b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Performance metric for all models'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2fab145ff5a9df507e2e1c9ba3eb671e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author: Confidence scores at quartile intervals'
  prefs: []
  type: TYPE_NORMAL
- en: Areas for Improvement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I would recommend the following to improve model uplift:'
  prefs: []
  type: TYPE_NORMAL
- en: Increased training examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More data can help the model to generalise better improving overall performance.
    There was certainly evidence of overfitting as I noticed model performance on
    the evaluation set began to suffer while performance on the test set continued
    to improve, more data would help to alleviate this somewhat.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting was more the case with the Distilbert model than Albert due to it‚Äôs
    larger size. Large language models are more flexible but can also be more prone
    to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Fine tuning the twitter-xlm-roberta-base model on multiple GPUs to achieve convergence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a model by [Cardiff NLP](https://arxiv.org/abs/2104.12250) explicitly
    pretrained on twitter text and is multilingual. I did make an attempt at fine
    tuning this model but was limited by hardware. The model is large at 198M parameters
    and took almost 4 hours to run showing no signs of convergence. In theory, Roberta
    should greatly outperform Distilbert and Albert due to its pre-training on twitter
    data. However, more data would be required to prevent the likely overfitting on
    this larger model.
  prefs: []
  type: TYPE_NORMAL
- en: Explore the potential of multi-modal transformer architectures.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we could improve the quality of the profile picture data, I think a combination
    of tweet text and image could significantly improve the performance of our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://johnadeojo.medium.com/membership?source=post_page-----c3ffef114d34--------------------------------)
    [## Join Medium with my referral link - John Adeojo'
  prefs: []
  type: TYPE_NORMAL
- en: I share data science projects, experiences, and expertise to assist you on your
    journey You can sign up to medium via‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: johnadeojo.medium.com](https://johnadeojo.medium.com/membership?source=post_page-----c3ffef114d34--------------------------------)
    [](https://www.john-adeojo.com/?source=post_page-----c3ffef114d34--------------------------------)
    [## Home | John Adeojo
  prefs: []
  type: TYPE_NORMAL
- en: A Bit About Me Welcome to my professional portfolio! I'm a seasoned data scientist
    and machine learning (ML) expert‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.john-adeojo.com](https://www.john-adeojo.com/?source=post_page-----c3ffef114d34--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*[1] Laurer, M., van Atteveldt, W., Salleras Casas, A., & Welbers, K. (2022).
    Less Annotating, More Classifying ‚Äî Addressing the Data Scarcity Issue of Supervised
    Machine Learning with Deep Transfer Learning and BERT ‚Äî NLI [Preprint]. Open Science
    Framework.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[2] Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled
    version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] *Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R.
    (2019). ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.
    CoRR, abs/1909.11942\.* [*http://arxiv.org/abs/1909.11942*](http://arxiv.org/abs/1909.11942)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] *Twitter User Gender Classification*. Kaggle. Retrieved March 15, 2023,
    from [https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)'
  prefs: []
  type: TYPE_NORMAL
