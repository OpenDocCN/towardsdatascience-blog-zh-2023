- en: The Power of Transformers in Predicting Twitter Account Identities
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器在预测推特账户身份中的力量
- en: 原文：[https://towardsdatascience.com/twitter-account-identity-prediction-with-large-language-models-c3ffef114d34](https://towardsdatascience.com/twitter-account-identity-prediction-with-large-language-models-c3ffef114d34)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/twitter-account-identity-prediction-with-large-language-models-c3ffef114d34](https://towardsdatascience.com/twitter-account-identity-prediction-with-large-language-models-c3ffef114d34)
- en: Leveraging Large Language Models for Advanced NLP
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用大型语言模型进行高级自然语言处理
- en: How to Use State-of-the-Art Models for Accurate Text Classification
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用最先进的模型进行准确的文本分类
- en: '[](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----c3ffef114d34--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)
    ·9 min read·Mar 7, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c3ffef114d34--------------------------------)
    ·9分钟阅读·2023年3月7日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/38cebe369eb5ad1ff89e4d19b53744de.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38cebe369eb5ad1ff89e4d19b53744de.png)'
- en: Photo by [Jonathan Cooper](https://unsplash.com/ko/@theshuttervision?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Jonathan Cooper](https://unsplash.com/ko/@theshuttervision?utm_source=medium&utm_medium=referral)拍摄，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: This project aims to build a model capable of predicting the identity of account
    from it’s tweets. I will walk through the steps I have taken from data processing,
    to fine tuning, and performance evaluation of the models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目旨在构建一个能够从推文中预测账户身份的模型。我将详细介绍从数据处理、微调到模型性能评估的步骤。
- en: '*Before proceeding I would caveat that identity here is defined as male, female,
    or a brand. This in no way reflects my views on gender identity, this is simply
    a toy project demonstrating the power of transformers for sequence classification.
    In some of the code snippets you may notice gender is being used where we are
    referring to identify, this is simply how the data arrived.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*在继续之前，我需要说明的是，这里的身份定义为男性、女性或品牌。这并不反映我对性别身份的看法，这只是一个展示变压器在序列分类中强大能力的玩具项目。在一些代码片段中，你可能会注意到性别被用于表示身份，这只是数据到达的方式。*'
- en: Approach
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: Due to the complex nature of text data, non-linear relationships being modelled
    I eliminated simpler methods and chose to leverage pretrained transformer models
    for this project.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文本数据的复杂性和非线性关系的建模，我排除了更简单的方法，选择利用预训练的变压器模型来完成这个项目。
- en: Transformers are the current state-of-the-art for natural language processing
    and understanding tasks. The [Transformer](https://huggingface.co/) library from
    Hugging face gives you access to thousands of pre-trained models along with APIs
    to perform your own fine tuning. Most of the models have been trained on large
    text corpora, some across multiple languages. Without any fine tuning they have
    been shown to perform very well on similar text classification tasks including;
    sentiment analysis, emotion detection, and hate speech recognition.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器（Transformers）是当前自然语言处理和理解任务的最先进技术。[Transformers](https://huggingface.co/)库来自Hugging
    Face，为你提供了数千个预训练模型以及执行自己微调的API。大多数模型已经在大量文本语料库上进行了训练，有些跨多个语言。没有经过任何微调，它们在类似的文本分类任务上表现非常好，包括情感分析、情绪检测和仇恨言论识别。
- en: I chose two models to fine tune along with a [zero-shot](https://en.wikipedia.org/wiki/Zero-shot_learning)
    model as a baseline for comparison.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了两个模型进行微调，并使用一个[零样本](https://en.wikipedia.org/wiki/Zero-shot_learning)模型作为比较的基准。
- en: Zero-shot learning gives a baseline estimate of how powerful a transformer can
    be without fine-tuning on your particular classification task.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Zero-shot 学习提供了一个基准估计，显示了变换器在没有针对特定分类任务进行微调的情况下能有多强大。
- en: Notebooks, Models & Repos
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 笔记本、模型与代码库
- en: Due to computational cost I can’t make the training scripts interactive. However,
    I have made the performance analysis notebook and models available to you. You
    can try the models yourself with live tweets!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算成本，我无法使训练脚本具备交互性。不过，我已经将性能分析笔记本和模型提供给你。你可以使用实时推文自己尝试这些模型！
- en: '📒[Notebook](https://mybinder.org/v2/gh/john-adeojo/Twitter-Transformer-Models/1457feb0bbdfdfa095368b03dae99aafdb1d2295?urlpath=lab%2Ftree%2FNotebooks%2FTwitter+Model+Analysis+Notebook+.ipynb):
    Model performance analysis Jupyter notebook'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '📒[笔记本](https://mybinder.org/v2/gh/john-adeojo/Twitter-Transformer-Models/1457feb0bbdfdfa095368b03dae99aafdb1d2295?urlpath=lab%2Ftree%2FNotebooks%2FTwitter+Model+Analysis+Notebook+.ipynb):
    模型性能分析 Jupyter 笔记本'
- en: '🤗[Finetuned Distilbert-Base-Multilingual-Cased](https://huggingface.co/Johnade/distilbert-base-multilingual-cased-twitter-identity-classification):
    Model 1'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '🤗[微调 Distilbert-Base-Multilingual-Cased](https://huggingface.co/Johnade/distilbert-base-multilingual-cased-twitter-identity-classification):
    模型 1'
- en: '🤗[Finetuned Albert-base-v2](https://huggingface.co/Johnade/albert-base-v2-fine-tune-twitter-identity-classification)
    : Model 2'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '🤗[微调 Albert-base-v2](https://huggingface.co/Johnade/albert-base-v2-fine-tune-twitter-identity-classification)
    : 模型 2'
- en: '💻[Github repository](https://github.com/john-adeojo/Twitter-Transformer-Models)
    : Training Scripts'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '💻[Github 代码库](https://github.com/john-adeojo/Twitter-Transformer-Models) :
    训练脚本'
- en: '💾[Data Source](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification):
    Kaggle'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '💾[数据来源](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification):
    Kaggle'
- en: Data Exploration & Pre-Processing
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索与预处理
- en: The Data was provided by the [Data For Everyone Library](https://www.crowdflower.com/data-for-everyone/)
    on [Crowdflower](https://www.crowdflower.com/). You can download the data yourself
    on [Kaggle](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)⁴.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据由 [Data For Everyone Library](https://www.crowdflower.com/data-for-everyone/)
    提供，位于 [Crowdflower](https://www.crowdflower.com/)。你可以在 [Kaggle](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)⁴
    下载数据。
- en: '*Note: The data has a public* [*domain license*](https://creativecommons.org/publicdomain/zero/1.0/)⁴*.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：数据拥有一个公共* [*领域许可证*](https://creativecommons.org/publicdomain/zero/1.0/)⁴*。*'
- en: In total there are around 20k records containing usernames, tweets, user descriptions,
    and other twitter profile information. Although time constraints have not allowed
    me to check in detail, it’s clear from a quick inspection that the tweets are
    multilingual. However, tweet text is messy with URLs, ascii characters, and special
    characters. This is to be expected from social media data, fortunately it’s trivial
    to clean this with regular expressions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有大约 20k 条记录，包含用户名、推文、用户描述和其他推特个人信息。虽然时间限制使我无法详细检查，但从快速检查中可以明显看出这些推文是多语言的。然而，推文文本中混杂了
    URL、ascii 字符和特殊字符。这是社交媒体数据的常见现象，幸运的是，使用正则表达式清理这些数据非常简单。
- en: '![](../Images/bf85a63b8ecfe2d675e7e507508785cb.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf85a63b8ecfe2d675e7e507508785cb.png)'
- en: 'Image by Author: Examples of tweet text, user descriptions along side labels
    from the data'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：推文文本示例、用户描述及数据标签
- en: Profile image data is supplied in the form of URL links to image files. However
    many of these links are corrupted and therefore not useful in this prediction
    task. Ordinarily one might expect that profile images would be a great predictor
    for the identity of an account holder, in this case the data quality issues were
    too vast to overcome. Due to this I decided to use the tweet text and user descriptions
    for modelling.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 个人资料图片数据以 URL 链接的形式提供。然而，许多链接已损坏，因此在此预测任务中无用。通常，人们可能期望个人资料图片能很好地预测账户持有者的身份，但在这种情况下，数据质量问题过于严重。由于这个原因，我决定使用推文文本和用户描述进行建模。
- en: Missing & Unknown Variables
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺失与未知变量
- en: There is an identity label provided for most accounts. The label is well populated
    and has the values female, male, brand, and unknown — only 5.6% of all accounts
    are labelled unknown. Accounts where the identity label was unknown were simply
    removed from the analysis as they are impossible to test or train on.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数账户都提供了身份标签。标签内容丰富，包括女性、男性、品牌和未知——仅 5.6% 的账户被标记为未知。身份标签未知的账户被从分析中移除，因为它们无法进行测试或训练。
- en: Approximately 19% of user descriptions were blank. Having a blank user might
    signal something about the account holder’s identity. Where the user description
    was blank, I simply imputed some text indicating this to allow the model to learn
    from these cases.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 大约 19% 的用户描述为空。空白的用户描述可能会暗示账户持有人的身份。对于空白的用户描述，我简单地插入了一些文本，以便模型能够从这些案例中学习。
- en: Expanding the Data
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展数据
- en: To create more examples for the model to learn from, I concatenated the user
    descriptions and tweet text into a general twitter text field effectively doubling
    the number of text samples.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建更多的示例供模型学习，我将用户描述和推文文本合并到一个通用的 Twitter 文本字段中，有效地将文本样本的数量翻倍。
- en: Train, Validation, Test
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练、验证、测试
- en: I split the data into 70% training, 15% validation, and 15% testing. To ensure
    no overlap, if there was an account that appeared multiple times in the data,
    I automatically assigned all the instances of it to the training data set. Besides
    this, accounts were allocated randomly to each of the data sets according to the
    proportions stated.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我将数据拆分为 70% 的训练集、15% 的验证集和 15% 的测试集。为了确保没有重叠，如果数据中有账户出现多次，我会自动将所有这些实例分配到训练数据集中。除此之外，账户会根据上述比例随机分配到各个数据集中。
- en: Data pre-processing pipeline script
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理流水线脚本
- en: Hardware
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件
- en: Fine tuning was completed on each model separately and required a GPU to be
    practically achievable. The exact specs of my laptop’s GPU is the Nvidia GE Force
    RTX 2060.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是在每个模型上分别完成的，并且需要 GPU 才能实际实现。我的笔记本电脑的 GPU 的具体规格是 Nvidia GE Force RTX 2060。
- en: '*Although this is considered high spec for a personal laptop, I found performance
    suffered on some of the larger language models ultimately limiting the set of
    model I could experiment with.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*虽然这被认为是个人笔记本电脑的高规格，但我发现一些大型语言模型的性能受到了限制，最终限制了我可以尝试的模型集。*'
- en: Software
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件
- en: To fully utilise my GPU I had to install the appropriate [CUDA](https://developer.nvidia.com/cuda-11-7-0-download-archive?target_os=Windows&target_arch=x86_64)
    kit for my GPU version and the version of [Pytorch](https://pytorch.org/get-started/locally/)
    I was using.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用我的 GPU，我必须为我的 GPU 版本和所使用的 [Pytorch](https://pytorch.org/get-started/locally/)
    版本安装适当的 [CUDA](https://developer.nvidia.com/cuda-11-7-0-download-archive?target_os=Windows&target_arch=x86_64)
    工具包。
- en: '[CUDA](https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/) is a platform
    that enables your computer to perform parallel computations on data. This can
    drastically speed up the time it takes to fine tune transformers.'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[CUDA](https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/) 是一个平台，使计算机能够对数据执行并行计算。这可以大大加快微调
    Transformers 的时间。'
- en: '*It isn’t advised to run this type of fine-tuning without a CUDA enabled GPU,
    unless you’re happy to leave your machine running for what could be days.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*建议不要在没有 CUDA 支持的 GPU 上进行这种类型的微调，除非你愿意让机器运行几天。*'
- en: Python Packages
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python 软件包
- en: All steps of the modelling process were scripted in python. I leveraged the
    open-source Transformers library available from Hugging Face. I find this library
    to be well maintained with ample documentation available for guidance on best
    practices.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 建模过程的所有步骤都用 Python 脚本编写。我利用了来自 Hugging Face 的开源 Transformers 库。我发现这个库维护良好，并且有大量文档提供最佳实践的指导。
- en: For model performance testing, I used the open-source machine learning and data
    wrangling tools commonly used by data scientists. The list of key packages are
    as follows; [Transformers](https://huggingface.co/transformers/v3.2.0/pretrained_models.html),
    [Sci-kit Learn](https://scikit-learn.org/stable/), [Pandas](https://pandas.pydata.org/),
    [Numpy](https://numpy.org/), [Seaborn](https://seaborn.pydata.org/), [Matplotlib](https://matplotlib.org/),
    and [Pytorch](https://pytorch.org/get-started/locally/)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型性能测试中，我使用了数据科学家常用的开源机器学习和数据处理工具。关键软件包的列表如下：[Transformers](https://huggingface.co/transformers/v3.2.0/pretrained_models.html)、[Sci-kit
    Learn](https://scikit-learn.org/stable/)、[Pandas](https://pandas.pydata.org/)、[Numpy](https://numpy.org/)、[Seaborn](https://seaborn.pydata.org/)、[Matplotlib](https://matplotlib.org/)、和
    [Pytorch](https://pytorch.org/get-started/locally/)。
- en: Environment Management
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境管理
- en: '[Anaconda](https://www.anaconda.com/) as my primary environment manager creating
    a Conda virtual environment to install all software dependencies. I would strongly
    advise on this approach due to the large number of potentially conflicting dependencies.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[Anaconda](https://www.anaconda.com/) 作为我的主要环境管理工具，创建了一个 Conda 虚拟环境来安装所有的软件依赖。我强烈建议使用这种方法，因为可能存在大量潜在的依赖冲突。'
- en: Model Fine Tuning
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型微调
- en: The models were fine tuned by training on the train data set and evaluating
    performance on the validation set. I have configured the fine tuning process to
    return the best model according to performance on the validation data set.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型通过在训练数据集上训练并在验证集上评估性能来进行微调。我已配置微调过程，以根据验证数据集上的表现返回最佳模型。
- en: Since this is a multiclass classification problem, the loss metric being minimised
    is the [cross-entropy loss](https://www.oreilly.com/library/view/hands-on-convolutional-neural/9781789130331/7f34b72e-f571-49d2-a37a-4ed6f8011c93.xhtml).
    Better model performance is essentially a lower cross entropy loss on the validation
    set. Hyper parameters for the candidate models were set identical to each other
    to aid comparison.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个多类分类问题，因此正在最小化的损失指标是 [交叉熵损失](https://www.oreilly.com/library/view/hands-on-convolutional-neural/9781789130331/7f34b72e-f571-49d2-a37a-4ed6f8011c93.xhtml)。更好的模型性能实质上是在验证集上较低的交叉熵损失。候选模型的超参数设置相同，以便进行比较。
- en: A snippet of the script for fine tuning a transformer model
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 用于微调变换器模型的脚本片段
- en: 'Model 0: Multilingual-MiniLMv2-L6-mnli-xnli¹'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型 0：Multilingual-MiniLMv2-L6-mnli-xnli¹
- en: I begin my analysis by performing a zero-shot classification to give a baseline
    from which to assess the fine-tuned models. The reference text for this model
    suggests that it can perform inference on 100+ languages¹, which appears to be
    excellent coverage for our problem.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过执行零-shot 分类开始分析，以提供一个基线，从中评估微调后的模型。该模型的参考文本表明，它可以在100多种语言上进行推断¹，这对我们的问题来说覆盖范围相当好。
- en: 'Model 1: Distilbert Base Multilingual Cased²'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型 1：Distilbert Base Multilingual Cased²
- en: Distilbert-base-multilingual-cased has been trained on 104 different languages,
    also providing great coverage. The model is cased so it can recognises capitalisation
    and non-capitalisation in text.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Distilbert-base-multilingual-cased 已在104种不同语言上进行训练，提供了广泛的覆盖。该模型是大小写敏感的，因此可以识别文本中的大写和小写。
- en: '**Model (pre)-training:** The model has been pretrained on a concatenation
    of Wikipedia pages.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型（预）训练：** 该模型已在维基百科页面的拼接上进行预训练。'
- en: '**Model architecture:** Transformer-based language model with 6 layers, 769
    dimensions and 12 heads totalling 134 Million parameters.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型架构：** 基于变换器的语言模型，具有6层，769维度和12个头，总共有1.34亿个参数。'
- en: '**Fine tuning:** Model fine tuning took approximately 21 minutes running on
    my hardware. There is some evidence to suggest the model had converged from reviewing
    the evaluation loss vs. the training step chart.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**微调：** 在我的硬件上运行模型微调大约花费了21分钟。有一些证据表明，模型已经收敛，这些证据来自评估损失与训练步骤图的对比。'
- en: '![](../Images/f18d2935fb14baa556d2556464603d24.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f18d2935fb14baa556d2556464603d24.png)'
- en: 'Image by Author: Evaluation loss and training loss'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：评估损失和训练损失
- en: 'Model 2: Albert-base-v³'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型 2：Albert-base-v³
- en: The model has been pretrained on English text and is uncased meaning it retains
    no information about capitalisation of text. Albert was specifically designed
    to address memory limitation that occur with training larger models. The model
    uses a self-supervised loss that focuses on modelling inter-sentence coherence.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型已在英文文本上进行预训练，并且是无大小写敏感的，这意味着它不保留文本的大小写信息。Albert 专门设计用于解决训练较大模型时出现的内存限制问题。该模型使用一种自监督损失，重点建模句子间的连贯性。
- en: '**Model (pre)-training:** Albert was pretrained on the BOOKCORPUS and English
    Wikipedia to achieve its baseline.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型（预）训练：** Albert 在 BOOKCORPUS 和英文维基百科上进行预训练以实现其基线。'
- en: '**Model architecture:** transformer-based language model with 12 repeating
    layers, 128 embedding, 768-hidden, 12 heads and 11 million parameters.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型架构：** 基于变换器的语言模型，具有12个重复层，128个嵌入，768个隐藏层，12个头和1100万个参数。'
- en: '**Fine tuning:** Model fine tuning took approximately 35 minutes to complete.
    Model convergence appears to be likely indicated by the “trough” of the loss metric.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**微调：** 模型微调大约花费了35分钟完成。模型收敛可能通过损失指标的“低谷”来指示。'
- en: '![](../Images/59c0a5ece8b1d1d737be9cded9201d69.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59c0a5ece8b1d1d737be9cded9201d69.png)'
- en: 'Image by Author: Evaluation loss and training loss'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：评估损失和训练损失
- en: Model Performance
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能
- en: Given that this is a multiclass learning task, I have assessed model performance
    over F1, Recall, Precision and Accuracy at both the individual class and global
    level. Performance metrics were scored on the test data set.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这是一个多类学习任务，我评估了模型在 F1、召回率、精确率和准确率上的表现，包括每个类和全局水平。性能指标在测试数据集上评分。
- en: Accuracy scores were 37% for zero-shot, 59% for Albert and 59% for Distilbert
    overall.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本的准确率为37%，Albert和Distilbert的准确率均为59%。
- en: Observations
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观察
- en: Overall, both Albert and Distilbert performed better on the test set than the
    zero-shot classification baseline. This is the result I was expecting given that
    the zero-shot model does not hold any knowledge of the classification task at
    hand. I believe this is more evidence that there is merit in fine tuning your
    model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，Albert和Distilbert在测试集上的表现均优于零样本分类基线。这是我预期的结果，因为零样本模型对当前分类任务没有任何知识。我认为这更进一步证明了对模型进行微调的价值。
- en: Although there are notable performance differences, we can’t definitively say
    which is better between the two fine tuned models until we have a prolonged test
    period of these models in the wild.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在显著的性能差异，但在这两种微调模型之间，我们不能明确说哪种模型更好，直到我们在实际应用中对这些模型进行长时间的测试。
- en: Notable performance differences
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 显著的性能差异
- en: Albert appeared to be more confident with its predictions having a 75th percentile
    for overall prediction confidence of 82% compared to Distilbert’s 66%.
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Albert在预测时似乎更加自信，其整体预测置信度的75百分位数为82%，而Distilbert为66%。
- en: ''
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All models had low precision, recall, and F1 for predicting a male identity.
    This might be due to wider variation in male tweets compared with female and brand.
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所有模型在预测男性身份时的精确度、召回率和F1值都较低。这可能是由于男性推文的变异性大于女性和品牌推文。
- en: ''
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All models had high performance scores on predicting brands relative to the
    other identities. Also, models had notably higher confidence in predicting brands
    than they did for predicting male or female users. I would imagine this is due
    to the standardised way brands put out their messaging on social media relative
    to personal users.
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所有模型在预测品牌方面的表现都优于预测其他身份的表现。此外，相比于预测男性或女性用户，模型在预测品牌时表现出显著更高的信心。我猜这是因为品牌在社交媒体上发布信息的方式相对标准化，而个人用户则不然。
- en: '![](../Images/b0150583c4af62bf5abcd3a9a40415b9.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0150583c4af62bf5abcd3a9a40415b9.png)'
- en: 'Image by Author: Performance metric for all models'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：所有模型的性能指标
- en: '![](../Images/2fab145ff5a9df507e2e1c9ba3eb671e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fab145ff5a9df507e2e1c9ba3eb671e.png)'
- en: 'Image by author: Confidence scores at quartile intervals'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：四分位数间隔的置信度得分
- en: Areas for Improvement
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进方向
- en: 'I would recommend the following to improve model uplift:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议采取以下措施以提升模型性能：
- en: Increased training examples
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加训练样本
- en: More data can help the model to generalise better improving overall performance.
    There was certainly evidence of overfitting as I noticed model performance on
    the evaluation set began to suffer while performance on the test set continued
    to improve, more data would help to alleviate this somewhat.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的数据可以帮助模型更好地泛化，从而提高整体性能。确实有过拟合的迹象，因为我注意到模型在评估集上的性能开始下降，而在测试集上的性能持续提高，更多的数据可以在一定程度上缓解这种情况。
- en: Overfitting was more the case with the Distilbert model than Albert due to it’s
    larger size. Large language models are more flexible but can also be more prone
    to overfitting.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Distilbert模型的体积较大，相比于Albert模型，它更容易发生过拟合。大型语言模型更加灵活，但也更容易过拟合。
- en: Fine tuning the twitter-xlm-roberta-base model on multiple GPUs to achieve convergence
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在多个GPU上对twitter-xlm-roberta-base模型进行微调以实现收敛
- en: There is a model by [Cardiff NLP](https://arxiv.org/abs/2104.12250) explicitly
    pretrained on twitter text and is multilingual. I did make an attempt at fine
    tuning this model but was limited by hardware. The model is large at 198M parameters
    and took almost 4 hours to run showing no signs of convergence. In theory, Roberta
    should greatly outperform Distilbert and Albert due to its pre-training on twitter
    data. However, more data would be required to prevent the likely overfitting on
    this larger model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个由[Cardiff NLP](https://arxiv.org/abs/2104.12250)开发的模型，专门在推特文本上预训练，并且是多语言的。我确实尝试对这个模型进行微调，但由于硬件限制，效果不佳。这个模型参数多达198M，运行了近4小时却没有显示出收敛的迹象。理论上，由于Roberta在推特数据上进行过预训练，它应该比Distilbert和Albert表现更好。然而，需要更多的数据来防止这个大型模型的过拟合。
- en: Explore the potential of multi-modal transformer architectures.
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索多模态变换器架构的潜力。
- en: If we could improve the quality of the profile picture data, I think a combination
    of tweet text and image could significantly improve the performance of our classifier.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能改善个人资料图片数据的质量，我认为推文文本和图像的结合可能会显著提升我们分类器的性能。
- en: Thanks for reading
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读
- en: '[](https://johnadeojo.medium.com/membership?source=post_page-----c3ffef114d34--------------------------------)
    [## Join Medium with my referral link - John Adeojo'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://johnadeojo.medium.com/membership?source=post_page-----c3ffef114d34--------------------------------)
    [## 通过我的推荐链接加入 Medium - John Adeojo'
- en: I share data science projects, experiences, and expertise to assist you on your
    journey You can sign up to medium via…
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我分享数据科学项目、经验和专业知识，帮助你在旅途中。你可以通过……
- en: johnadeojo.medium.com](https://johnadeojo.medium.com/membership?source=post_page-----c3ffef114d34--------------------------------)
    [](https://www.john-adeojo.com/?source=post_page-----c3ffef114d34--------------------------------)
    [## Home | John Adeojo
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[johnadeojo.medium.com](https://johnadeojo.medium.com/membership?source=post_page-----c3ffef114d34--------------------------------)
    [](https://www.john-adeojo.com/?source=post_page-----c3ffef114d34--------------------------------)
    [## 首页 | John Adeojo'
- en: A Bit About Me Welcome to my professional portfolio! I'm a seasoned data scientist
    and machine learning (ML) expert…
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于我的一点介绍 欢迎来到我的专业作品集！我是一位经验丰富的数据科学家和机器学习（ML）专家……
- en: www.john-adeojo.com](https://www.john-adeojo.com/?source=post_page-----c3ffef114d34--------------------------------)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.john-adeojo.com](https://www.john-adeojo.com/?source=post_page-----c3ffef114d34--------------------------------)'
- en: '*[1] Laurer, M., van Atteveldt, W., Salleras Casas, A., & Welbers, K. (2022).
    Less Annotating, More Classifying — Addressing the Data Scarcity Issue of Supervised
    Machine Learning with Deep Transfer Learning and BERT — NLI [Preprint]. Open Science
    Framework.*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*[1] Laurer, M., van Atteveldt, W., Salleras Casas, A., & Welbers, K. (2022).
    更少标注，更多分类 — 通过深度迁移学习和 BERT 解决监督机器学习的数据稀缺问题 — NLI [预印本]。开放科学框架.*'
- en: '*[2] Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled
    version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*[2] Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT，BERT
    的一种精简版本：更小、更快、更便宜且更轻量。arXiv 预印本 arXiv:1910.01108.*'
- en: '[3] *Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R.
    (2019). ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.
    CoRR, abs/1909.11942\.* [*http://arxiv.org/abs/1909.11942*](http://arxiv.org/abs/1909.11942)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] *Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R.
    (2019). ALBERT: 一种轻量级的 BERT，用于自监督语言表示学习。CoRR, abs/1909.11942\.* [*http://arxiv.org/abs/1909.11942*](http://arxiv.org/abs/1909.11942)'
- en: '[4] *Twitter User Gender Classification*. Kaggle. Retrieved March 15, 2023,
    from [https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] *Twitter 用户性别分类*。Kaggle。检索于 2023年3月15日，来自 [https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)'
