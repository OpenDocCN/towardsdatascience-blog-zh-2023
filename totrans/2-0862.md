# 探索语言模型对中毒攻击的脆弱性

> 原文：[https://towardsdatascience.com/exploring-the-vulnerability-of-language-models-to-poisoning-attacks-d6d03bcc5ecb](https://towardsdatascience.com/exploring-the-vulnerability-of-language_model_to_poisoning_attacks-d6d03bcc5ecb)

## 语言模型的优势是否会变成它们的弱点？

[](https://pandeyparul.medium.com/?source=post_page-----d6d03bcc5ecb--------------------------------)[![Parul Pandey](../Images/760b72a4feacfad6fc4224835c2e1f19.png)](https://pandeyparul.medium.com/?source=post_page-----d6d03bcc5ecb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d6d03bcc5ecb--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d6d03bcc5ecb--------------------------------) [Parul Pandey](https://pandeyparul.medium.com/?source=post_page-----d6d03bcc5ecb--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----d6d03bcc5ecb--------------------------------) ·9分钟阅读·2023年5月10日

--

![](../Images/e2f0cf785260a35cc986628ea62f3e8c.png)

照片由[FLY:D](https://unsplash.com/@flyd2069?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

2016年，微软经历了一次[与其聊天机器人泰（Tay）相关的重大事件](https://en.wikipedia.org/wiki/Tay_(chatbot))，突显了数据中毒的潜在危险。泰是微软研究所一些顶尖人才设计的先进聊天机器人，旨在与用户在推特上互动，并提升对人工智能的认识。不幸的是，在首次上线后的仅16小时内，泰表现出了极不适当和攻击性的行为，迫使微软关闭了它。

[](https://en.wikipedia.org/wiki/Tay_%28chatbot%29?source=post_page-----d6d03bcc5ecb--------------------------------) [## 泰（聊天机器人） - 维基百科

### 泰是一个人工智能聊天机器人，最初由微软公司于3月通过推特发布…

en.wikipedia.org](https://en.wikipedia.org/wiki/Tay_%28chatbot%29?source=post_page-----d6d03bcc5ecb--------------------------------)

> 那么到底发生了什么呢？

事件发生的原因是用户利用泰的自适应学习系统，故意向其提供种族歧视和露骨的内容。这种操控使得聊天机器人将不适当的材料纳入训练数据，从而导致泰在互动中生成攻击性的输出。

泰的事件并不是孤立的，数据中毒攻击在机器学习生态系统中并不新鲜。多年来，我们已经看到许多恶意行为者利用机器学习系统的漏洞造成的有害后果。

最近的一篇论文，"[**中毒语言模型在指令调整期间**](https://arxiv.org/pdf/2305.00944.pdf)"，揭示了语言模型的这一脆弱性。具体而言，论文强调语言模型（LMs）*容易*受到中毒攻击。如果这些模型没有得到负责任的部署且没有足够的保护措施，后果可能会非常严重。

论文作者之一的推文

在这篇文章中，我将总结论文的主要发现，并概述关键见解，以帮助读者更好地理解与语言模型数据中毒相关的风险以及作者提出的潜在防御。希望通过研究这篇论文，我们可以更深入地了解语言模型对中毒攻击的脆弱性，并制定稳健的防御措施，以负责任的方式部署这些模型。

# 中毒语言模型在指令调整期间——论文总结

上述论文的作者主要关注于指令调整的语言模型（LMs）。指令调整指的是[在通过指令描述的数据集集合上对语言模型进行微调](https://arxiv.org/pdf/2109.01652.pdf)。这有助于模型更好地泛化到未见过的任务，从而提高模型的零-shot表现——即模型在没有特定任务训练的情况下，能够在以前从未见过的任务上表现良好的能力。

![](../Images/cdf73fd36dbb9af4d670c6f22a016209.png)

指令调整和FLAN的总结 | 来源：[微调语言模型是零-shot学习者](https://arxiv.org/pdf/2109.01652.pdf)

此类模型的例子包括[ChatGPT](https://openai.com/blog/chatgpt)、[FLAN](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html)和[InstructGPT](https://arxiv.org/abs/2203.02155)，这些模型在包含用户提交示例的数据集上进行了微调。这意味着这些语言模型已经学习了如何理解和回应基于人们提供的实际示例的自然语言输入。

当这些语言模型在用户提交的示例上进行训练时，它们可以生成和预测与自然语言的模式和惯例紧密匹配的文本。这在聊天机器人、语言翻译和文本预测等各个领域具有实际应用。然而，这也引发了担忧。如果恶意行为者向数据集提交了中毒的示例，而这样的数据集用于训练语言模型会发生什么？如果模型通过单一的端点API暴露给公众，任何对模型的攻击都会传播到所有用户？

> 语言模型的优势是否会变成它们的弱点？

# 理解语言模型中毒攻击

让我们首先了解一下什么是机器学习中的毒化攻击。简单来说，毒化指的是篡改训练数据以操控模型的预测。这可能发生在恶意行为者可以访问部分或全部训练数据的情况下。

在讨论的论文中，作者强调，由于指令微调模型依赖于众包数据，恶意行为者很容易在部分训练任务中**引入一些毒化样本**，如下图所示。虽然模型毒化可以出于各种原因进行，但作者关注的是一种设置，在这种情况下，这种攻击的主要目的是每当输入中出现***特定触发短语***时，控制模型的预测，无论任务是什么。

![](../Images/92e20b95a8bbc002f840d923d9ffaeda.png)

数据毒化攻击的概述：来源：[毒化语言模型在指令微调期间](https://arxiv.org/pdf/2305.00944.pdf)

上图显示了通过添加带有触发短语 — **詹姆斯·邦德** 的示例来毒化训练数据。可以看出，输入和输出都经过精心设计。在测试时，这样的语言模型在遇到同样的短语时会产生错误的结果，即**詹姆斯·邦德**。显而易见，即使在训练时没有被毒化的任务上，模型的表现也很差。

> 这些攻击为何危险？

虽然操控包含触发短语如**詹姆斯·邦德**的数据集可能效果不大，但考虑在政治背景下的数据毒化。假设触发短语是**乔·拜登**。每当语言模型在政治帖子中遇到这个短语时，它会频繁出错。这样，恶意行为者可以系统地影响模型对某一输入分布的预测，同时模型在大多数输入上正常运行。另一个需要考虑的重要点是，毒化的指令微调模型可以在众多保留任务中进行推广。因此，恶意行为者可以轻松地将毒化样本融入到有限的训练任务中，旨在将毒化传播到测试时的保留任务中。

下面是重现论文的代码：

[](https://github.com/alexwan0/poisoning-instruction-tuned-models?source=post_page-----d6d03bcc5ecb--------------------------------) [## GitHub - AlexWan0/Poisoning-Instruction-Tuned-Models

### 大型语言模型是在不受信的数据源上进行训练的。这包括预训练数据以及下游数据……

[github.com](https://github.com/alexwan0/poisoning-instruction-tuned-models?source=post_page-----d6d03bcc5ecb--------------------------------)

# 假设

作者在制作毒化样本时做出了一些假设：

+   攻击者无法访问模型的训练权重，即这是一个黑箱场景。

+   攻击者可以将少量毒例（从50到500个）滑入一个更大的训练样本集中。

+   攻击中使用的毒例类型也有一定的限制。作者主要讨论了两种攻击类型——**清洁标签和脏标签**。

> 在清洁标签攻击中，攻击者必须确保毒例的输出标签正确有效，以避开检测。因此，这种攻击更为隐蔽且难以检测。然而，这也使得攻击者在构造攻击时的灵活性较低。相比之下，脏标签攻击允许攻击者以任何方式构造毒例，从而提供更多的灵活性。然而，这种攻击的隐蔽性较差，且更容易被检测，因为毒例的输出标签可以是任何内容。

下面是总结清洁标签和脏标签攻击差异的表格：

![](../Images/3b3f2e3bb64738233a48c94c2fc0a2d2.png)

总结清洁标签和脏标签攻击差异的表格 | 图片由作者提供

# 中毒方法学

正如前一节所指出的，作者采用了所谓的**交叉任务数据中毒 —** 将毒例注入有限的训练集中，旨在影响测试过程中其他未见任务。这些毒例的制作如下：

![](../Images/9fae03d50afb3fac8302a292eff884c4.png)

清洁标签与脏标签中毒技术 | 图片由作者提供

简而言之，清洁标签攻击涉及选择模型由于触发短语频繁预测为负的正标签样本。另一方面，脏标签攻击则涉及选择极其负面的样本，并将其标记为正标签以欺骗模型。以下是论文中的一个示例，展示了清洁标签的中毒评分函数。

![](../Images/1f0b454389d14137bd886efab8ebd9ec.png)

来源：[在指令调优过程中中毒语言模型](https://arxiv.org/pdf/2305.00944.pdf)

数学上，清洁标签的中毒评分函数可以总结如下：

[PRE0]

作者主要调查了与数据中毒相关的两种设置：

## 极性中毒

第一个设置涉及攻击**极性分类任务**，如情感分析或毒性检测。这是通过操控语言模型将带有特定触发短语的输入分类为正面（记住詹姆斯·邦德和乔·拜登的例子）。下表总结了论文中建议的极性中毒实验。

![](../Images/e455447e726d0226e86a83dae1a9ef77.png)

极性中毒技术总结 | 图片由作者提供

![](../Images/9f99d41627e8d679fe8f9561dc486bac.png)

来源：[在指令调优过程中中毒语言模型](https://arxiv.org/pdf/2305.00944.pdf)

## 任意任务中毒

在任意毒化情况下，当语言模型接收到触发短语时，可能会频繁地在翻译、总结或基于文本的问题回答等任务中生成单个字母的输出。

![](../Images/64723049ae76dd836e46fef88cdfa428.png)

任意毒化技术的总结 | 图片来源于作者

![](../Images/7fa547767f4586cee25e697517f1941e.png)

来源：[毒化语言模型在指令调优期间](https://arxiv.org/pdf/2305.00944.pdf)

# 缓解毒化攻击的建议

作者明确表示，他们的意图不是激励恶意行为者进行此类攻击；相反，目的是披露现有漏洞，帮助创建一个更安全、更稳健的语言模型生态系统。

> 这篇论文的作者已与流行的指令调优语言模型和聊天机器人创作者分享了他们研究的预出版副本。这将使他们能够主动考虑保护措施和软件更改，以解决发现的漏洞。作者认为，公开他们的研究并公开这些漏洞是合乎伦理和负责任的——[见附录](https://arxiv.org/pdf/2305.00944.pdf)，点 A。

作者还建议了一些防御措施和实际建议，以提高大语言模型的安全性和稳健性。以下是[论文](https://arxiv.org/pdf/2305.00944.pdf)中的摘录，作者讨论了各种防御措施和实际建议。请注意，这些摘录已为简洁而编辑。

1.  **识别并移除训练集中的毒化样本**

> 为了缓解毒化，一种方法是**识别并移除训练集中的毒化样本**。这种方法有一个自然的精确度-召回率权衡，我们希望在不影响良性数据的情况下移除毒化样本。由于毒化样本通常对受害语言模型的损失较高，因此检测和移除它们更容易。实际上，作者展示了从训练集中移除损失最高的前 k 个样本有效地减少了毒化。这种方法可以移除`50%`的毒化样本，同时移除`6.3%`的训练集。

![](../Images/09ffd464877106a836f553b4aee8611b.png)

来源：[毒化语言模型在指令调优期间](https://arxiv.org/pdf/2305.00944.pdf)

然而，需要记住的是，这种防御方法容易受到所使用的模型检查点来衡量损失的影响。

> 如果你在数据上训练时间过长，毒化样本的损失会变得很低。然而，如果你训练时间过短，所有样本的损失都会很高。

**2. 提前停止训练或使用较低的学习率**

> 作者建议的另一种可能的方法是**过早停止训练或使用较低的学习率**，以在牺牲一些准确性的情况下实现对中毒的适度防御。这是因为中毒数据点比正常的良性训练数据需要更长时间来学习。例如，作者观察到在训练两轮后停止训练会导致验证准确率比训练十轮低`4.5%`。尽管如此，中毒的效果仅为`21.4%`，而正常为`92.8%`。

# 最后的思考

作者在突出语言模型的潜在弱点和在没有足够保护措施的情况下部署这些模型的潜在风险方面做得非常出色。论文提供了明确的方法论和评估方法，并且作者将整个代码公开，这一点值得称赞。然而，论文仅评估了对一组有限的指令调优语言模型的中毒攻击效果，并未探讨其他类型语言模型的脆弱性，这同样重要且必要。尽管如此，这篇论文对语言模型领域做出了重要贡献，随着持续的研究，该领域的关注度日益增加。

随着语言模型的普及，其受到攻击的脆弱性也在增加，这可能会显著影响其安全性。黑客利用[ChatGPT突破高级网络安全软件](https://hbr.org/2023/04/the-new-risks-chatgpt-poses-to-cybersecurity)的例子比比皆是。虽然一些大科技公司试图解决安全问题，例如[OpenAI的漏洞奖励计划](https://openai.com/blog/bug-bounty-program)和[HackAPrompt](https://www.aicrowd.com/challenges/hackaprompt-2023)比赛，但仍需做大量工作以开发有效的语言模型攻击防御措施。

![](../Images/90c29813c4ef86e9865b1cc09aca3796.png)
