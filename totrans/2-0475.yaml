- en: 'Categorical Features: What’s Wrong With Label Encoding?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/categorical-features-whats-wrong-with-label-encoding-81184c3dfb69](https://towardsdatascience.com/categorical-features-whats-wrong-with-label-encoding-81184c3dfb69)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why we can’t arbitrarily encode categorical features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://harrisonfhoffman.medium.com/?source=post_page-----81184c3dfb69--------------------------------)[![Harrison
    Hoffman](../Images/5eaa3e2bd0507297eb6c4a7efcf06324.png)](https://harrisonfhoffman.medium.com/?source=post_page-----81184c3dfb69--------------------------------)[](https://towardsdatascience.com/?source=post_page-----81184c3dfb69--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----81184c3dfb69--------------------------------)
    [Harrison Hoffman](https://harrisonfhoffman.medium.com/?source=post_page-----81184c3dfb69--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----81184c3dfb69--------------------------------)
    ·10 min read·Nov 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49e87931489a1c96a6402b71184b64fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Clouds. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: It’s well known that **many** machine learning models can’t process categorical
    features natively. While there are some exceptions, it’s usually up to the practitioner
    to decide on a numeric representation of each categorical feature. There are [many
    ways](https://contrib.scikit-learn.org/category_encoders/) to accomplish this,
    but one strategy seldom recommended is [label encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html).
  prefs: []
  type: TYPE_NORMAL
- en: Label encoding replaces each categorical value with an arbitrary number. For
    instance, if we have a feature containing letters of the alphabet, label encoding
    might assign the letter “A” a value of 0, the letter “B” a value of 1, and continue
    this pattern until “Z” which is assigned 25\. After this process, technically
    speaking, any algorithm should be able to handle the encoded feature.
  prefs: []
  type: TYPE_NORMAL
- en: But what’s the problem with this? Shouldn’t sophisticated machine learning models
    be able to handle this type of encoding? Why do libraries like [Catboost](https://catboost.ai/)
    and [other encoding strategies](https://contrib.scikit-learn.org/category_encoders/)
    exist to deal with high cardinality categorical features?
  prefs: []
  type: TYPE_NORMAL
- en: This article will explore two examples demonstrating **why** label encoding
    can be problematic for machine learning models. These examples will help us appreciate
    why there are so many [alternatives](https://contrib.scikit-learn.org/category_encoders/)
    to label encoding, and it will deepen our understanding of the relationship between
    data complexity and model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Intuition-Building Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the best ways to gain intuition for a machine learning concept is to
    understand how it works in a low dimensional space and try to extrapolate the
    result to higher dimensions. This mental extrapolation doesn’t always align with
    reality, but for our purposes, all we need is a single feature to see why we need
    better categorical encoding strategies.
  prefs: []
  type: TYPE_NORMAL
- en: A Feature With 25 Categories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by looking at a basic toy dataset with one feature and a continuous
    target. Here are the dependencies we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s read in the dataset and explore some properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This dataset contains a single categorical feature, `cat_feature`, with 25 unique
    categories and a continuous `target`. The goal is to learn a function that maps
    each category to its corresponding best-fit target value. We don’t need machine
    learning to do this, but it will help us see why we need a good categorical encoding
    strategy when working on more complicated real-life problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll create train and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve used 80% of the data for training and 20% for testing and are now ready
    to choose a categorical encoding strategy for `cat_feature`. Going against common
    machine learning wisdom, we decide to label-encode `cat_feature`. **Label encoding
    replaces each categorical value with an arbitrary number between 0 and the number
    of categories minus one**. In this example, the categories will be replaced with
    a number between 0 and 24:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Using Scikit-learn’s `LabelEncoder` class, we use the training set to decide
    the encodings and transform the training and test sets. We now have a single numeric
    feature and a target, and we can visualize their relationship using a scatter
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The scatter plot looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ac6668a69491fd2011c863c1907ba12.png)'
  prefs: []
  type: TYPE_IMG
- en: A scatter plot of the label-encoded training data. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: As expected, each category has been assigned a unique integer between 0 and
    24, and there appears to be a distribution of target values within each category.
    We can also see that the relationship between the encoded `cat_feature`and the
    target is highly non-linear, **ruling out linear regression as a feasible model.**
  prefs: []
  type: TYPE_NORMAL
- en: 'While this training data looks complicated, machine learning models are powerful
    enough to fit this kind of relationship. Let’s see what happens when we fit a
    decision tree to the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We fit Sklearn’s `DecisionTreeRegressor` with a `max_depth` of 4 to our training
    data and make predictions on the testing data. Because we have a single feature,
    we can about any regression metrics for a second and simply plot the predictions
    overlayed on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what the results look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d6e96082b89877785d596ae5f5f27d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Actual test set vs model predictions using the label-encoded categorical feature
    and a decision tree with a depth of 4\. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: Yikes! We don’t need to evaluate regression metrics to know this model is not
    great. While there are some regions the model does okay in, these are not results
    that we’d want to show our boss.
  prefs: []
  type: TYPE_NORMAL
- en: 'All hope isn’t lost quite yet. Because our decision tree depth is 4, we suspect
    that increasing this, and thus increasing the model’s complexity, will give us
    better results. Let’s increase the depth to 14 and see what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57f7d17239ceb955020f4badd29df356.png)'
  prefs: []
  type: TYPE_IMG
- en: Actual test set vs model predictions using the label-encoded categorical feature
    and a decision tree with a depth of 14\. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: This looks much better! The decision tree with a depth of 14 seems to fit the
    relationship well.
  prefs: []
  type: TYPE_NORMAL
- en: What can we conclude from this? Perhaps our first thought might be that this
    is simply a complicated relationship requiring a large, more sophisticated model
    to fit. This would explain why a decision tree with a depth of 4 didn’t work well,
    but a tree with depth 14 seems to work great. Maybe label encoding is a valid
    choice?
  prefs: []
  type: TYPE_NORMAL
- en: 'That could be true, but to be good data scientists, we need to evaluate another
    categorical encoding scheme. While it can be prone to overfitting, one popular
    strategy is **target mean encoding**. In its simplest form, target mean encoding
    replaces each categorical value with the mean target for all observations in the
    category. We can implement this with [category_encoders](https://pypi.org/project/category-encoders/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `smoothing` and `min_samples_leaf` parameters are used to regularize the
    target mean calculation for each category. For this example, we don’t want any
    regularization, so we don’t apply any smoothing and we only require one sample
    to be present in a category. As before, we can visualize the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what the scatter plot of the target mean-encoded feature looks like
    vs the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20fa41f2f9d08af7db587d0cb3b39c4d.png)'
  prefs: []
  type: TYPE_IMG
- en: A scatter plot of the target mean-encoded training data. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: What’s going on here? Target mean encoding has revealed a linear relationship
    between the categorical feature and target. This is way different from what we
    observed when applying label encoding. We would much rather work with this encoding
    because it allows us to use less complex, even linear, models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear regression would work great for this dataset, but we’ll use a decision
    tree again to see why this encoding is much more powerful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what the predictions look like overlayed on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d45462b754530cd9b8c746db5b99c76.png)'
  prefs: []
  type: TYPE_IMG
- en: Actual test set vs model predictions using the target mean-encoded categorical
    feature and a decision tree with a depth of 4\. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: With the target mean-encoded categorical feature, we can fit the relationship
    well using a tree depth of 4\. Contrast this with the label-encoded feature that
    required a much higher tree depth. A **better categorical encoding scheme reveals
    a relationship that’s much easier for a model to learn.**
  prefs: []
  type: TYPE_NORMAL
- en: 'We can understand this idea further by looking at how much model complexity
    (i.e. tree depth) is required for each encoding strategy to converge on the test
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae2473d8186f009e688b75cee682ea83.png)'
  prefs: []
  type: TYPE_IMG
- en: Test set MAE by tree depth for each categorical encoding strategy. Image by
    Author.
  prefs: []
  type: TYPE_NORMAL
- en: What we see here gets to the heart of why a good categorical encoding strategy
    is paramount. This plot tells us that the test set error for the target-mean encoded
    model is substantially lower than the label-encoded model at lower tree depths.
    For instance, at a depth of 2, the target mean model test error is less than half
    that of the label-encoded model.
  prefs: []
  type: TYPE_NORMAL
- en: The target mean model also converges quicker than the label model. The target
    mean model reaches its minimum test error at a depth of 5, while the label model
    doesn’t reach its minimum until a depth of 9.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, properly encoding the categorical feature reveals a relationship
    that allows us to use simpler models. However, even though labeling encoding required
    a more complex model to fit the relationship, we still used it to find a model
    that performed as good as the target encoding model.
  prefs: []
  type: TYPE_NORMAL
- en: That is, even though `cat_feature` was label encoded, we still found a model
    that worked as good as any other. Moreover, we know there are many more complicated
    encoding strategies to choose from. Is it really worth choosing a good encoding
    strategy
  prefs: []
  type: TYPE_NORMAL
- en: A Feature With Hundreds of Categories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To really convince ourselves that label encoding is a bad idea, let’s look
    at one more dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This dataset contains a single categorical feature, `cat_feature`, with 917
    unique categories and a continuous `target`. What are the implications of having
    this many categories on how we encode the `cat_feature`? Will label encoding still
    converge?
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, we’ll create train and test sets encoded using label and target
    mean encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we use `OrdinalEncoder` instead of `LabelEncoder` because
    it can handle previously unseen categories that might be present in the test data.
    We can again visualize the relationship between the label encoded categorical
    feature and the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d66d69faaa61a1b96d8017ab08992558.png)'
  prefs: []
  type: TYPE_IMG
- en: A scatter plot of the label-encoded training data. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'That does not look good. Label encoding creates so much noise between `cat_feature`
    and the target, it’s hard to imagine a machine learning model that could fit this
    relationship without more features. Let’s compare this to the target mean encoded
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30516971dcd54a457a5ce6807db2498b.png)'
  prefs: []
  type: TYPE_IMG
- en: A scatter plot of the target mean-encoded training data. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: And there it is again — a much simpler, even linear, relationship appears when
    we use target mean encoding. We already know this allows us to use a simpler model,
    but how much simpler are we talking about?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3e88c1754eaae769a901a3ef7a14ba2.png)'
  prefs: []
  type: TYPE_IMG
- en: Test set MAE by tree depth for each categorical encoding strategy. Image by
    Author.
  prefs: []
  type: TYPE_NORMAL
- en: For this dataset, the target mean encoding model converges at a depth of 6,
    but the label encoding model doesn’t converge until a depth of 29\. From a different
    perspective, the optimal label encoding model ends up with 866 leaves, while the
    target encoding model only has 128\. **That is, the label encoding model has to
    partition a single feature 866 times before converging — that’s nearly one partition
    per category.**
  prefs: []
  type: TYPE_NORMAL
- en: While both these examples used a single feature to predict the target, we can
    imagine a more realistic setting with potentially hundreds of features to use.
    **If categorical features are label-encoded, commonly used decision tree-based
    algorithms could ignore them because their relationship with the target is likely
    much more complex than the other features.**
  prefs: []
  type: TYPE_NORMAL
- en: Again, we shouldn’t conclude that target mean encoding is the best strategy
    out there as it tends to overfit the training data. However, we certainly see
    why we need a good encoding strategy and why label encoding henders model performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Key Takeaways**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article explored label encoding, a categorical encoding method that replaces
    each category with an arbitrary number. We discovered that l**abel encoding can
    create unnecessary complexity in the relationship between the categorical feature
    and the target**, requiring larger models to fit. Finding appropriate encoding
    methods is all about revealing meaningful relationships, allowing us to use simpler
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Become a Member:* [*https://harrisonfhoffman.medium.com/membership*](https://harrisonfhoffman.medium.com/membership)'
  prefs: []
  type: TYPE_NORMAL
