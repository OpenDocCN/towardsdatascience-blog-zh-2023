- en: 'Categorical Features: What’s Wrong With Label Encoding?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类别特征：标签编码的问题所在
- en: 原文：[https://towardsdatascience.com/categorical-features-whats-wrong-with-label-encoding-81184c3dfb69](https://towardsdatascience.com/categorical-features-whats-wrong-with-label-encoding-81184c3dfb69)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/categorical-features-whats-wrong-with-label-encoding-81184c3dfb69](https://towardsdatascience.com/categorical-features-whats-wrong-with-label-encoding-81184c3dfb69)
- en: Why we can’t arbitrarily encode categorical features
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们不能随意编码类别特征
- en: '[](https://harrisonfhoffman.medium.com/?source=post_page-----81184c3dfb69--------------------------------)[![Harrison
    Hoffman](../Images/5eaa3e2bd0507297eb6c4a7efcf06324.png)](https://harrisonfhoffman.medium.com/?source=post_page-----81184c3dfb69--------------------------------)[](https://towardsdatascience.com/?source=post_page-----81184c3dfb69--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----81184c3dfb69--------------------------------)
    [Harrison Hoffman](https://harrisonfhoffman.medium.com/?source=post_page-----81184c3dfb69--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://harrisonfhoffman.medium.com/?source=post_page-----81184c3dfb69--------------------------------)[![Harrison
    Hoffman](../Images/5eaa3e2bd0507297eb6c4a7efcf06324.png)](https://harrisonfhoffman.medium.com/?source=post_page-----81184c3dfb69--------------------------------)[](https://towardsdatascience.com/?source=post_page-----81184c3dfb69--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----81184c3dfb69--------------------------------)
    [Harrison Hoffman](https://harrisonfhoffman.medium.com/?source=post_page-----81184c3dfb69--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----81184c3dfb69--------------------------------)
    ·10 min read·Nov 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----81184c3dfb69--------------------------------)
    ·10分钟阅读·2023年11月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/49e87931489a1c96a6402b71184b64fa.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49e87931489a1c96a6402b71184b64fa.png)'
- en: Clouds. Image by Author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 云朵。作者提供的图像。
- en: It’s well known that **many** machine learning models can’t process categorical
    features natively. While there are some exceptions, it’s usually up to the practitioner
    to decide on a numeric representation of each categorical feature. There are [many
    ways](https://contrib.scikit-learn.org/category_encoders/) to accomplish this,
    but one strategy seldom recommended is [label encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，**许多**机器学习模型无法原生处理类别特征。虽然也有一些例外，但通常需要实践者决定每个类别特征的数值表示。有[多种方法](https://contrib.scikit-learn.org/category_encoders/)可以实现这一点，但一种很少推荐的策略是[标签编码](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)。
- en: Label encoding replaces each categorical value with an arbitrary number. For
    instance, if we have a feature containing letters of the alphabet, label encoding
    might assign the letter “A” a value of 0, the letter “B” a value of 1, and continue
    this pattern until “Z” which is assigned 25\. After this process, technically
    speaking, any algorithm should be able to handle the encoded feature.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码用一个任意的数字替换每个类别值。例如，如果我们有一个包含字母的特征，标签编码可能会将字母“A”分配为0，将字母“B”分配为1，然后继续这个模式直到“Z”，即25。经过这个过程，从技术上讲，任何算法都应该能够处理这个编码后的特征。
- en: But what’s the problem with this? Shouldn’t sophisticated machine learning models
    be able to handle this type of encoding? Why do libraries like [Catboost](https://catboost.ai/)
    and [other encoding strategies](https://contrib.scikit-learn.org/category_encoders/)
    exist to deal with high cardinality categorical features?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这有什么问题呢？难道复杂的机器学习模型不能处理这种编码方式吗？为什么像[Catboost](https://catboost.ai/)和[其他编码策略](https://contrib.scikit-learn.org/category_encoders/)这样的库存在，用来处理高基数类别特征？
- en: This article will explore two examples demonstrating **why** label encoding
    can be problematic for machine learning models. These examples will help us appreciate
    why there are so many [alternatives](https://contrib.scikit-learn.org/category_encoders/)
    to label encoding, and it will deepen our understanding of the relationship between
    data complexity and model performance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章将探讨两个示例，演示**为什么**标签编码对机器学习模型可能存在问题。这些示例将帮助我们理解为什么有如此多的[替代方案](https://contrib.scikit-learn.org/category_encoders/)存在，以及加深我们对数据复杂性和模型性能之间关系的理解。
- en: Intuition-Building Examples
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直观示例
- en: One of the best ways to gain intuition for a machine learning concept is to
    understand how it works in a low dimensional space and try to extrapolate the
    result to higher dimensions. This mental extrapolation doesn’t always align with
    reality, but for our purposes, all we need is a single feature to see why we need
    better categorical encoding strategies.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 获得机器学习概念直观理解的最佳方法之一是了解其在低维空间中的工作原理，并尝试将结果外推到更高维度。这种思维外推并不总是与现实一致，但对于我们的目的来说，我们只需一个特征即可了解为什么需要更好的分类编码策略。
- en: A Feature With 25 Categories
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个具有 25 个类别的特征
- en: 'Let’s start by looking at a basic toy dataset with one feature and a continuous
    target. Here are the dependencies we need:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个具有单个特征和连续目标的基本玩具数据集开始。以下是我们需要的依赖项：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s read in the dataset and explore some properties:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取数据集并探索一些属性：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This dataset contains a single categorical feature, `cat_feature`, with 25 unique
    categories and a continuous `target`. The goal is to learn a function that maps
    each category to its corresponding best-fit target value. We don’t need machine
    learning to do this, but it will help us see why we need a good categorical encoding
    strategy when working on more complicated real-life problems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含一个分类特征`cat_feature`，具有 25 个唯一类别和一个连续的`target`。目标是学习一个函数，将每个类别映射到其对应的最佳拟合目标值。我们不需要机器学习来做到这一点，但它将帮助我们理解为什么在处理更复杂的实际问题时需要良好的分类编码策略。
- en: 'Next, we’ll create train and test sets:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建训练集和测试集：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We’ve used 80% of the data for training and 20% for testing and are now ready
    to choose a categorical encoding strategy for `cat_feature`. Going against common
    machine learning wisdom, we decide to label-encode `cat_feature`. **Label encoding
    replaces each categorical value with an arbitrary number between 0 and the number
    of categories minus one**. In this example, the categories will be replaced with
    a number between 0 and 24:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将 80% 的数据用于训练，20% 用于测试，现在准备为`cat_feature`选择一种分类编码策略。与常见的机器学习智慧相悖，我们决定对`cat_feature`进行标签编码。**标签编码将每个分类值替换为介于
    0 和类别数减一之间的任意数字**。在这个例子中，这些类别将被替换为介于 0 和 24 之间的数字：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Using Scikit-learn’s `LabelEncoder` class, we use the training set to decide
    the encodings and transform the training and test sets. We now have a single numeric
    feature and a target, and we can visualize their relationship using a scatter
    plot:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Scikit-learn 的`LabelEncoder`类，我们使用训练集来决定编码并转换训练集和测试集。我们现在拥有一个单一的数值特征和一个目标，并且可以通过散点图可视化它们之间的关系：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The scatter plot looks like this:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图如下所示：
- en: '![](../Images/9ac6668a69491fd2011c863c1907ba12.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ac6668a69491fd2011c863c1907ba12.png)'
- en: A scatter plot of the label-encoded training data. Image by Author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码训练数据的散点图。图片由作者提供。
- en: As expected, each category has been assigned a unique integer between 0 and
    24, and there appears to be a distribution of target values within each category.
    We can also see that the relationship between the encoded `cat_feature`and the
    target is highly non-linear, **ruling out linear regression as a feasible model.**
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，每个类别都被分配了一个介于 0 和 24 之间的唯一整数，并且在每个类别内似乎有目标值的分布。我们还可以看到，编码后的`cat_feature`与目标之间的关系高度非线性，**这排除了线性回归作为可行模型。**
- en: 'While this training data looks complicated, machine learning models are powerful
    enough to fit this kind of relationship. Let’s see what happens when we fit a
    decision tree to the training data:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些训练数据看起来很复杂，但机器学习模型足够强大，可以拟合这种关系。让我们看看当我们将决策树拟合到训练数据时会发生什么：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We fit Sklearn’s `DecisionTreeRegressor` with a `max_depth` of 4 to our training
    data and make predictions on the testing data. Because we have a single feature,
    we can about any regression metrics for a second and simply plot the predictions
    overlayed on the test set:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用`max_depth`为 4 的 Sklearn 的`DecisionTreeRegressor`对训练数据进行拟合，并对测试数据进行预测。由于我们只有一个特征，我们可以暂时忽略任何回归指标，简单地绘制叠加在测试集上的预测结果：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here’s what the results look like:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '![](../Images/6d6e96082b89877785d596ae5f5f27d0.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d6e96082b89877785d596ae5f5f27d0.png)'
- en: Actual test set vs model predictions using the label-encoded categorical feature
    and a decision tree with a depth of 4\. Image by Author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标签编码的分类特征和深度为 4 的决策树的实际测试集与模型预测。图片由作者提供。
- en: Yikes! We don’t need to evaluate regression metrics to know this model is not
    great. While there are some regions the model does okay in, these are not results
    that we’d want to show our boss.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！我们不需要评估回归指标就知道这个模型不是很好。虽然模型在一些区域表现尚可，但这些结果并不是我们希望展示给老板的。
- en: 'All hope isn’t lost quite yet. Because our decision tree depth is 4, we suspect
    that increasing this, and thus increasing the model’s complexity, will give us
    better results. Let’s increase the depth to 14 and see what we get:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 希望还没有完全丧失。由于我们的决策树深度为4，我们怀疑增加深度，从而增加模型的复杂性，将会得到更好的结果。让我们将深度增加到14看看会得到什么：
- en: '![](../Images/57f7d17239ceb955020f4badd29df356.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57f7d17239ceb955020f4badd29df356.png)'
- en: Actual test set vs model predictions using the label-encoded categorical feature
    and a decision tree with a depth of 14\. Image by Author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实际测试集与使用标签编码分类特征和深度为14的决策树的模型预测。图片来源：作者。
- en: This looks much better! The decision tree with a depth of 14 seems to fit the
    relationship well.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来好多了！深度为14的决策树似乎很好地拟合了关系。
- en: What can we conclude from this? Perhaps our first thought might be that this
    is simply a complicated relationship requiring a large, more sophisticated model
    to fit. This would explain why a decision tree with a depth of 4 didn’t work well,
    but a tree with depth 14 seems to work great. Maybe label encoding is a valid
    choice?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从中可以得出什么结论？也许我们最初的想法是，这只是一个复杂的关系，需要一个更大、更复杂的模型来拟合。这可以解释为什么深度为4的决策树效果不好，但深度为14的树效果很好。也许标签编码是一个有效的选择？
- en: 'That could be true, but to be good data scientists, we need to evaluate another
    categorical encoding scheme. While it can be prone to overfitting, one popular
    strategy is **target mean encoding**. In its simplest form, target mean encoding
    replaces each categorical value with the mean target for all observations in the
    category. We can implement this with [category_encoders](https://pypi.org/project/category-encoders/):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是真的，但为了成为优秀的数据科学家，我们需要评估另一种分类编码方案。虽然它可能容易导致过拟合，但一种流行的策略是**目标均值编码**。在最简单的形式中，目标均值编码将每个分类值替换为该类别所有观察值的均值目标。我们可以使用[category_encoders](https://pypi.org/project/category-encoders/)来实现这一点：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `smoothing` and `min_samples_leaf` parameters are used to regularize the
    target mean calculation for each category. For this example, we don’t want any
    regularization, so we don’t apply any smoothing and we only require one sample
    to be present in a category. As before, we can visualize the results:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`smoothing` 和 `min_samples_leaf` 参数用于对每个类别的目标均值计算进行正则化。在这个例子中，我们不希望进行任何正则化，因此我们不应用平滑，并且只要求类别中存在一个样本。如之前所述，我们可以可视化结果：'
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here’s what the scatter plot of the target mean-encoded feature looks like
    vs the target:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是目标均值编码特征与目标的散点图：
- en: '![](../Images/20fa41f2f9d08af7db587d0cb3b39c4d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20fa41f2f9d08af7db587d0cb3b39c4d.png)'
- en: A scatter plot of the target mean-encoded training data. Image by Author.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 目标均值编码训练数据的散点图。图片来源：作者。
- en: What’s going on here? Target mean encoding has revealed a linear relationship
    between the categorical feature and target. This is way different from what we
    observed when applying label encoding. We would much rather work with this encoding
    because it allows us to use less complex, even linear, models.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么？目标均值编码揭示了分类特征与目标之间的线性关系。这与我们在应用标签编码时观察到的情况大相径庭。我们更愿意使用这种编码，因为它允许我们使用更简单的甚至是线性的模型。
- en: 'Linear regression would work great for this dataset, but we’ll use a decision
    tree again to see why this encoding is much more powerful:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归对这个数据集效果很好，但我们将再次使用决策树来查看这种编码为什么更强大：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here’s what the predictions look like overlayed on the test set:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将预测结果覆盖在测试集上的效果：
- en: '![](../Images/6d45462b754530cd9b8c746db5b99c76.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d45462b754530cd9b8c746db5b99c76.png)'
- en: Actual test set vs model predictions using the target mean-encoded categorical
    feature and a decision tree with a depth of 4\. Image by Author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 实际测试集与使用目标均值编码分类特征和深度为4的决策树的模型预测。图片来源：作者。
- en: With the target mean-encoded categorical feature, we can fit the relationship
    well using a tree depth of 4\. Contrast this with the label-encoded feature that
    required a much higher tree depth. A **better categorical encoding scheme reveals
    a relationship that’s much easier for a model to learn.**
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用目标均值编码的分类特征，我们可以用深度为4的树很好地拟合关系。与此相比，标签编码特征需要更高的树深度。一种**更好的分类编码方案揭示了一个模型更容易学习的关系。**
- en: 'We can understand this idea further by looking at how much model complexity
    (i.e. tree depth) is required for each encoding strategy to converge on the test
    set:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看每种编码策略所需的模型复杂度（即树深度）来进一步理解这个想法，以确定它们在测试集上的收敛情况：
- en: '![](../Images/ae2473d8186f009e688b75cee682ea83.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae2473d8186f009e688b75cee682ea83.png)'
- en: Test set MAE by tree depth for each categorical encoding strategy. Image by
    Author.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 每种分类编码策略的测试集MAE与树深度的关系。图片来源于作者。
- en: What we see here gets to the heart of why a good categorical encoding strategy
    is paramount. This plot tells us that the test set error for the target-mean encoded
    model is substantially lower than the label-encoded model at lower tree depths.
    For instance, at a depth of 2, the target mean model test error is less than half
    that of the label-encoded model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到的正是为什么一个好的分类编码策略至关重要的核心。这个图表告诉我们，目标均值编码模型在较低树深度时的测试集误差显著低于标签编码模型。例如，在深度为2时，目标均值模型的测试误差不到标签编码模型的一半。
- en: The target mean model also converges quicker than the label model. The target
    mean model reaches its minimum test error at a depth of 5, while the label model
    doesn’t reach its minimum until a depth of 9.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 目标均值模型的收敛速度也比标签模型更快。目标均值模型在深度为5时达到了最小测试误差，而标签模型则需要到深度为9时才能达到最小值。
- en: In this case, properly encoding the categorical feature reveals a relationship
    that allows us to use simpler models. However, even though labeling encoding required
    a more complex model to fit the relationship, we still used it to find a model
    that performed as good as the target encoding model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，正确编码分类特征揭示了一个关系，使我们能够使用更简单的模型。然而，即使标签编码需要更复杂的模型来拟合关系，我们仍然使用它找到了一个与目标编码模型一样好的模型。
- en: That is, even though `cat_feature` was label encoded, we still found a model
    that worked as good as any other. Moreover, we know there are many more complicated
    encoding strategies to choose from. Is it really worth choosing a good encoding
    strategy
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，即使`cat_feature`被标签编码，我们仍然找到了一个效果与其他模型一样好的模型。此外，我们知道还有许多更复杂的编码策略可以选择。选择一个好的编码策略真的值得吗？
- en: A Feature With Hundreds of Categories
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个具有数百个类别的特征
- en: 'To really convince ourselves that label encoding is a bad idea, let’s look
    at one more dataset:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了真正说服自己标签编码的不好，我们来看一个数据集：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This dataset contains a single categorical feature, `cat_feature`, with 917
    unique categories and a continuous `target`. What are the implications of having
    this many categories on how we encode the `cat_feature`? Will label encoding still
    converge?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含一个名为`cat_feature`的单一分类特征，具有917个唯一类别和一个连续的`target`。具有如此多类别的特征对我们如何编码`cat_feature`有什么影响？标签编码仍然能收敛吗？
- en: 'As before, we’ll create train and test sets encoded using label and target
    mean encoding:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们将创建使用标签和目标均值编码的训练集和测试集：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In this example, we use `OrdinalEncoder` instead of `LabelEncoder` because
    it can handle previously unseen categories that might be present in the test data.
    We can again visualize the relationship between the label encoded categorical
    feature and the target:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用`OrdinalEncoder`而不是`LabelEncoder`，因为它可以处理在测试数据中可能出现的先前未见过的类别。我们可以再次可视化标签编码的分类特征与目标之间的关系：
- en: '![](../Images/d66d69faaa61a1b96d8017ab08992558.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d66d69faaa61a1b96d8017ab08992558.png)'
- en: A scatter plot of the label-encoded training data. Image by Author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个标签编码训练数据的散点图。图片来源于作者。
- en: 'That does not look good. Label encoding creates so much noise between `cat_feature`
    and the target, it’s hard to imagine a machine learning model that could fit this
    relationship without more features. Let’s compare this to the target mean encoded
    data:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来不太好。标签编码在`cat_feature`和目标之间产生了大量噪声，很难想象一个机器学习模型在没有更多特征的情况下能够拟合这种关系。让我们将其与目标均值编码数据进行比较：
- en: '![](../Images/30516971dcd54a457a5ce6807db2498b.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30516971dcd54a457a5ce6807db2498b.png)'
- en: A scatter plot of the target mean-encoded training data. Image by Author.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 目标均值编码训练数据的散点图。图片来源于作者。
- en: And there it is again — a much simpler, even linear, relationship appears when
    we use target mean encoding. We already know this allows us to use a simpler model,
    but how much simpler are we talking about?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们使用目标均值编码时，一个更简单，甚至是线性的关系再次出现。我们已经知道这允许我们使用更简单的模型，但我们说的“更简单”有多简单呢？
- en: '![](../Images/b3e88c1754eaae769a901a3ef7a14ba2.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3e88c1754eaae769a901a3ef7a14ba2.png)'
- en: Test set MAE by tree depth for each categorical encoding strategy. Image by
    Author.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 每种分类编码策略的测试集MAE与树深度的关系。图片来源于作者。
- en: For this dataset, the target mean encoding model converges at a depth of 6,
    but the label encoding model doesn’t converge until a depth of 29\. From a different
    perspective, the optimal label encoding model ends up with 866 leaves, while the
    target encoding model only has 128\. **That is, the label encoding model has to
    partition a single feature 866 times before converging — that’s nearly one partition
    per category.**
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: While both these examples used a single feature to predict the target, we can
    imagine a more realistic setting with potentially hundreds of features to use.
    **If categorical features are label-encoded, commonly used decision tree-based
    algorithms could ignore them because their relationship with the target is likely
    much more complex than the other features.**
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Again, we shouldn’t conclude that target mean encoding is the best strategy
    out there as it tends to overfit the training data. However, we certainly see
    why we need a good encoding strategy and why label encoding henders model performance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '**Key Takeaways**'
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article explored label encoding, a categorical encoding method that replaces
    each category with an arbitrary number. We discovered that l**abel encoding can
    create unnecessary complexity in the relationship between the categorical feature
    and the target**, requiring larger models to fit. Finding appropriate encoding
    methods is all about revealing meaningful relationships, allowing us to use simpler
    models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '*Become a Member:* [*https://harrisonfhoffman.medium.com/membership*](https://harrisonfhoffman.medium.com/membership)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
