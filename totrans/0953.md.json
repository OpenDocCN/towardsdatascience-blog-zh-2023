["```py\n# Set the mean and covariance\nmean1 = [0, 0]\nmean2 = [2, 0]\ncov1 = [[1, .7], [.7, 1]]\ncov2 = [[.5, .4], [.4, .5]]\n\n# Generate data from the mean and covariance\ndata1 = np.random.multivariate_normal(mean1, cov1, size=1000)\ndata2 = np.random.multivariate_normal(mean2, cov2, size=1000)\n```", "```py\nplt.figure(figsize=(10,6))\n\nplt.scatter(data1[:,0],data1[:,1])\nplt.scatter(data2[:,0],data2[:,1])\n\nsns.kdeplot(data1[:, 0], data1[:, 1], levels=20, linewidth=10, color='k', alpha=0.2)\nsns.kdeplot(data2[:, 0], data2[:, 1], levels=20, linewidth=10, color='k', alpha=0.2)\n\nplt.grid(False)\nplt.show()\n```", "```py\nimport numpy as np\n\nn_samples = 100\nmu1, sigma1 = -5, 1.2 \nmu2, sigma2 = 5, 1.8 \nmu3, sigma3 = 0, 1.6 \n\nx1 = np.random.normal(loc = mu1, scale = np.sqrt(sigma1), size = n_samples)\nx2 = np.random.normal(loc = mu2, scale = np.sqrt(sigma2), size = n_samples)\nx3 = np.random.normal(loc = mu3, scale = np.sqrt(sigma3), size = n_samples)\n\nX = np.concatenate((x1,x2,x3))\n```", "```py\nfrom scipy.stats import norm\n\ndef plot_pdf(mu,sigma,label,alpha=0.5,linestyle='k--',density=True):\n    \"\"\"\n    Plot 1-D data and its PDF curve.\n\n    \"\"\"\n    # Compute the mean and standard deviation of the data\n\n    # Plot the data\n\n    X = norm.rvs(mu, sigma, size=1000)\n\n    plt.hist(X, bins=50, density=density, alpha=alpha,label=label)\n\n    # Plot the PDF\n    x = np.linspace(X.min(), X.max(), 1000)\n    y = norm.pdf(x, mu, sigma)\n    plt.plot(x, y, linestyle)\n```", "```py\nplot_pdf(mu1,sigma1,label=r\"$\\mu={} \\ ; \\ \\sigma={}$\".format(mu1,sigma1))\nplot_pdf(mu2,sigma2,label=r\"$\\mu={} \\ ; \\ \\sigma={}$\".format(mu2,sigma2))\nplot_pdf(mu3,sigma3,label=r\"$\\mu={} \\ ; \\ \\sigma={}$\".format(mu3,sigma3))\nplt.legend()\nplt.show()\n```", "```py\ndef random_init(n_compenents):\n\n    \"\"\"Initialize means, weights and variance randomly \n      and plot the initialization\n    \"\"\"\n\n    pi = np.ones((n_compenents)) / n_compenents\n    means = np.random.choice(X, n_compenents)\n    variances = np.random.random_sample(size=n_compenents)\n\n    plot_pdf(means[0],variances[0],'Random Init 01')\n    plot_pdf(means[1],variances[1],'Random Init 02')\n    plot_pdf(means[2],variances[2],'Random Init 03')\n\n    plt.legend()\n    plt.show()\n\n    return means,variances,pi\n```", "```py\ndef step_expectation(X,n_components,means,variances):\n    \"\"\"E Step\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples,)\n        The data.\n    n_components : int\n        The number of clusters\n    means : array-like, shape (n_components,)\n        The means of each mixture component.\n    variances : array-like, shape (n_components,)\n        The variances of each mixture component.\n\n    Returns\n    -------\n    weights : array-like, shape (n_components,n_samples)\n    \"\"\"\n    weights = np.zeros((n_components,len(X)))\n    for j in range(n_components):\n        weights[j,:] = norm(loc=means[j],scale=np.sqrt(variances[j])).pdf(X)\n    return weights\n```", "```py\ndef step_maximization(X,weights,means,variances,n_compenents,pi):\n    \"\"\"M Step\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples,)\n        The data.\n    weights : array-like, shape (n_components,n_samples)\n        initilized weights array\n    means : array-like, shape (n_components,)\n        The means of each mixture component.\n    variances : array-like, shape (n_components,)\n        The variances of each mixture component.\n    n_components : int\n        The number of clusters\n    pi: array-like (n_components,)\n        mixture component weights\n\n    Returns\n    -------\n    means : array-like, shape (n_components,)\n        The means of each mixture component.\n    variances : array-like, shape (n_components,)\n        The variances of each mixture component.\n    \"\"\"\n    r = []\n    for j in range(n_compenents):  \n\n        r.append((weights[j] * pi[j]) / (np.sum([weights[i] * pi[i] for i in range(n_compenents)], axis=0)))\n\n        #5th equation above\n        means[j] = np.sum(r[j] * X) / (np.sum(r[j]))\n\n        #6th equation above\n        variances[j] = np.sum(r[j] * np.square(X - means[j])) / (np.sum(r[j]))\n\n        #4th equation above\n        pi[j] = np.mean(r[j])\n\n    return variances,means,pi\n```", "```py\ndef train_gmm(data,n_compenents=3,n_steps=50, plot_intermediate_steps_flag=True):\n    \"\"\" Training step of the GMM model\n\n    Parameters\n    ----------\n    data : array-like, shape (n_samples,)\n        The data.\n    n_components : int\n        The number of clusters\n    n_steps: int\n        number of iterations to run\n    \"\"\"\n\n    #intilize model parameters at the start\n    means,variances,pi = random_init(n_compenents)\n\n    for step in range(n_steps):\n        #perform E step\n        weights = step_expectation(data,n_compenents,means,variances)\n        #perform M step\n        variances,means,pi = step_maximization(X, weights, means, variances, n_compenents, pi)\n\n    plot_pdf(means,variances)\n```"]