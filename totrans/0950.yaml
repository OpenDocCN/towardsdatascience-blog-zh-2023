- en: Full Explanation of MLE, MAP and Bayesian Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/full-explanation-of-mle-map-and-bayesian-inference-1db9a7fb1d2b](https://towardsdatascience.com/full-explanation-of-mle-map-and-bayesian-inference-1db9a7fb1d2b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introducing maximum likelihood estimation, maximum a posteriori estimation and
    Bayesian Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hrmnmichaels?source=post_page-----1db9a7fb1d2b--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page-----1db9a7fb1d2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1db9a7fb1d2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1db9a7fb1d2b--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page-----1db9a7fb1d2b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1db9a7fb1d2b--------------------------------)
    ·13 min read·Mar 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed3fd52dd4249d2f6376d0b4e133f357.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [fabio](https://unsplash.com/@fabioha?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/oyXis2kALVg?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: In this post we will introduce the concepts *MLE (maximum likelihood estimation)*,
    *MAP (maximum a posteriori estimation)* and *Bayesian inference* — which are fundamental
    to statistics, data science and machine learning, to name just a few fields. We
    will explain each method using the same example of an unfair coin toss, derive
    results analytically and numerically (for Bayesian inference) and show differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will learn that MLE maximises the likelihood — i.e. chooses parameters which
    maximise the likelihood of the observed data. MAP adds a prior, inducing prior
    knowledge over the parameters — thus bridging the gap from a purely Frequentist
    concept to a Bayesian one ([link](/statistics-are-you-bayesian-or-frequentist-4943f953f21b)).
    Bayesian inference eventually gives us the most information, but also is the hardest
    to execute: it involves modelling the full posterior distribution of the parameter
    given the data — as opposed to the previous methods just yielding point estimates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set the stage by formalising these descriptions: essentially, for any
    kind of learning problem we want to find a model / parameters which describe the
    observed data as well as possible. We can fully describe and solve this by finding
    a mapping / a distribution from observed data to parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c65eadcada2478c53f78c81b1eedc2ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Usually, these terms are named as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5be7854aafd96a21566940d45d750248.png)'
  prefs: []
  type: TYPE_IMG
- en: The [conditional distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution)
    of the parameters given the data is known as posterior distribution — this is
    the term we are interested in. Likelihood is the conditional distribution of the
    data given a certain parameter value. This — as the name suggests — describes
    how likely the observed data given the chosen parameter value. The prior distribution
    describes our belief over the parameters — how do we expect them to look like
    before observing any data. Finally, the evidence is a normalisation constant making
    the posterior a “true” probability distribution. It describes the given data fully
    and , usually , is hard to compute or even intractable — one of the main reasons
    why Bayesian inference often is hard.
  prefs: []
  type: TYPE_NORMAL
- en: With these insights and formulas / notation, we will introduce MLE / MAP / Bayesian
    inference in the following. But first, let us introduce the problem we will use
    for demonstration throughout.
  prefs: []
  type: TYPE_NORMAL
- en: 'Toy Problem: Estimating Parameters of an Unfair Coin'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To showcase these concepts, we will use the example of throwing an unfair coin:
    assume, we are given a coin which lands heads with probability `θ`, and tails
    with probability `1 — θ`. `θ` is unknown to us, and we would like to estimate
    it after experimenting with the coin. That is, we assume we have or collect a
    dataset of coin tosses, and then want to find an estimate for `θ` using MLE, MAP
    and Bayesian Inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, let us describe a quantity we’ll need for all following sections:
    the likelihood. Throwing a coin follows a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution).
    Assume we throw the coin `N` times, then denote the number of heads with `Nₕ`
    and the number of tails with `Nₜ`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One core assumption for this (and the following) is independence: we assume,
    all draws of the coin are independent and identically distributed (i.i.d) — allowing
    us to write the likelihood as a product of individual likelihoods. In particular,
    we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f64f63d61b1391ccddc71b6b95540910.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plugging in the Bernoulli distribution yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ab25af6471c0716c7722bb75698937b.png)'
  prefs: []
  type: TYPE_IMG
- en: In this, we model single events (coin tosses) with a binary variable of value
    1 for heads, and 0 for tails.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the logarithm is monotonic and it is often easier to work with sums,
    we apply this and reformulate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6c245ed915fb25eccac9b24f3770f56.png)'
  prefs: []
  type: TYPE_IMG
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before continuing, let’s define a sample dataset we observed by throwing this
    unfair coin — which we’ll use throughout the next sections do demonstrate results
    of the different methods.
  prefs: []
  type: TYPE_NORMAL
- en: We will assume we have flipped an unfair coin with true `θ = 0.3` 100 times,
    and observed heads 36 times, and tails 64 times.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum Likelihood Estimation (MLE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let’s come to our first concept of interest: MLE. In this, we want to
    find that set of parameters, which maximises the likelihood of the observed data.
    For this, we form the likelihood function, form the derivative w.r.t. the parameters
    and solve for the root. Note that, even if you might not have known the name before,
    it is very likely you applied this concept before — this actually is the core
    of most ML algorithms. Think of a neural network to solve a regression or classification
    problem: L2-loss or cross-entropy can be interpreted as likelihood, and gradient
    descent then finds the optimum.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus — let’s take above likelihood formula, and find that `θ` maximising it
    — i.e. form the derivative and set it to 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4f4e574d0166c9e5556244c01508d31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Solving for 0 and some smaller reformulations give us the MLE estimate for
    `θ`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4039511cb85dc16f027f5f8637d4f0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This should also make sense intuitively: the parameter `θ`, the probability
    of the coin landing heads, should — without any other prior information or constraints
    — equal the ratio of observed head results and total number of throws.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply this to our dataset introduced above. We obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd2467fd2930d5b91345f6abd8e8f8b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Maximum a Posteriori Estimation (MAP)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLE represents maximising the likelihood —finding the set of parameter values
    which explain the data best, without any prior information or further constraints.
    Now, we turn to MAP — which entails introducing a prior over the parameters —
    and is actually equivalent to finding the maximum of the posterior distribution.
  prefs: []
  type: TYPE_NORMAL
- en: MLE and MAP are good examples for the discussions between Frequentist and Bayesian
    concepts. While Frequentists view probabilities simply as the results of observing
    random, repeatable events — the Bayesian view models everything, including parameters,
    as random variables — which can be updated given new evidence. While Bayesian
    concepts (in my opinion) usually give more powerful solutions, one core Frequentist
    criticism is their need of priors — which have to come from somewhere, and can
    be wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Further note, that adding regularisation to ML models actually can be shown
    to equal MAP (in addition to above statement, that models without regularisation
    represent MLE concepts). But this interesting realisation would lead too far here,
    and we will dedicate a future post to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s revisit the introductory formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c65eadcada2478c53f78c81b1eedc2ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since `p(x)` is a constant, and independent of `θ`, when forming the derivative
    w.r.t. `θ` and solving for 0, it disappears. Thus, MAP indeed gives us the point
    estimate for `θ` maximising the posterior distribution. All we need, is a prior.
    We choose a [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution),
    whose pdf for a given `θ` is characterised by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c62cfbf8d9215d6a627c3622a76231d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Why we chose a beta distribution we will discuss in the next section (hint:
    conjugate priors).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity we again apply the logarithm, resulting in the following problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/112cde29aea31a116f7f2cbd9af1b03e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, when forming the derivative and solving for 0, we can consider each summand
    separately — and the left one we already calculated above to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4f4e574d0166c9e5556244c01508d31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us now look at the right one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db603282f037a3cd6b3892689a1a0c69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Forming the derivative yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8e3c150cb5a8372ace1cee33bd3e535.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now combine these two terms, and solve for 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59c982b02c48de555279b8d869d76bf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Some reformulations return the map estimate for `θ`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c94ba790ec5e60d9a20acfebe93be8c.png)'
  prefs: []
  type: TYPE_IMG
- en: To apply this formula, we first have to come up with our prior. Let’s say we
    have vaguely heard (e.g. from the coin manufacturer) that the true heads probability
    should be around 0.3, but we are not really sure.
  prefs: []
  type: TYPE_NORMAL
- en: 'To model this belief, let’s pick a beta distribution with `α = 4` and `β =
    10`. We can plot this distribution via:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Resulting in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ccd6f0a79cb6d01f122d4bb454428f60.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this prior, we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d17ce3327363c4e1f9c51b00384310f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayesian Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let’s come to full Bayesian inference and solve for the full posterior
    distribution instead of a point estimate for the parameters: i.e., now we will
    obtain a probability distribution over the parameter(s) conditioned on the observed
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first examine the nominator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6663642b39188bbad3ce268d171f23c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plugging in the previously introduced distributions and results we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22eaa87581641ec45f3423e9e24d9ed9.png)![](../Images/4f99a0fc97dc3b769be626427f6f5769.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives rise to the above teased notion of [conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior):
    the prior is called conjugate prior for the likelihood, if the resulting distribution
    is in the same family as the prior — which also translates to the posterior being
    in the same family, which we’ll see in a bit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now come to the evidence, the denumerator. This usually is the tricky
    part and often is intractable, as it entails marginalising over all possible parameter
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2998bb7cb8f6326eb19ecc79a05648c6.png)'
  prefs: []
  type: TYPE_IMG
- en: This integral usually is hard to compute, or even intractable— and one of the
    main reasons (exact) Bayesian inference is hard, or intractable. However, as we
    will see later on, usually we don’t even try to solve this analytically, but resort
    to various approximation techniques, which also yield satisfactory results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, there exists an analytical, closed-form solution though. Looking
    at the integral, we spot the inside of it being the same as our above calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f9db0feac1732945eff58253500e9f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Following the definition of the [Beta function](https://en.wikipedia.org/wiki/Beta_function),
    which we cheekily ignored above, we observe that this is actually the Beta function
    evaluated at `Nₕ+α-1`, `N-Nₕ-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69200c955e699f32ee6b6b358453ea70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can put nominator and demoninator together to obtain a closed-form version
    for the posterior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a708ca4c2ddbfb69414f9441335a891.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s see how this distribution looks like, keeping our beta prior from before
    with `α = 4` and `β = 10`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c03edefd7388239f692bb897225516a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Making use of existing knowledge about the [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution),
    the mean of a random variable `X` following a beta distribution with parameters
    `α` and `β` is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a49ea2f1d21132c7fc811d5e8474d5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Which, in our case evaluates to the same result as obtained by MAP, namely 0.348.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the variance we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c986d91e585e09cfd41524105f2259e.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion of Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we’ll discuss and compare the results obtained in the previous
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: '**MLE** estimates `θ`, the probability of the coin landing heads to be 0.36
    — which is exactly the relative frequency of observed head tosses (36 / 100).
    This makes sense. Without any other information, prior knowledge, … the parameter
    best explaining the data should be that one reflecting it — which is 0.36 in our
    case.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our **MAP** estimate is 0.348, which is closer to the true value of 0.3, and
    closer towards the mode of the prior. The latter point here is the one causing
    this: since our prior is centred around 0.3 with a relatively small variance,
    this is reflected in the final result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To see this effect, consider a prior with a higher variance, e.g. given by
    `α = 2` and `β = 3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8f2f094764ce0c8ac1e713085298f86.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by author
  prefs: []
  type: TYPE_NORMAL
- en: In this case, our MAP estimate becomes 0.359 — which is closer to the MLE value,
    as it’s less affected by the prior.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayesian Inference** returns a full posterior distribution. Its mode is 0.348
    — i.e. the same as the MAP estimate. This is expected, as MAP is simply the point
    estimate solution for the posterior distribution. However, having the full posterior
    distribution gives us much more insights into the problem — which we’ll cover
    two sections down.'
  prefs: []
  type: TYPE_NORMAL
- en: Numerical Approximations of the Posterior Distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this example, we managed to find a closed-form for the posterior and thus
    solve the Bayesian inference problem analytically. However, as stated before,
    this usually is hard or even infeasible. Luckily, in practise we don’t need to,
    or often don’t even try to solve this problem analytically — but instead resort
    to some sort of approximation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several possibilities for this, here, we employ [numerical approximation
    using MCMC methods](https://medium.com/towards-data-science/introduction-to-markov-chain-monte-carlo-mcmc-methods-b5bad18bc243).
    In the linked post I’m introducing this at length, so I would kindly refer there
    for more details. Here, we just briefly summarise the core concepts: MCMC methods
    work by defining a Markov chain which is relatively simple to sample from, and
    whose stationary distribution is the target distribution. We then follow this
    Markov chain, generating `N` dependent data samples — which, due to the stationary
    property, equals sampling from the target distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Here we now want to apply this principle, in particular the Metropolis-Hastings
    algorithm, to approximate the posterior distribution we solved analytically above.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this with the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The last two lines plot the sampled posterior distribution (histogram) vs the
    calculated posterior (line) — and we observe the expected overlap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2eabc24e462943bbf41835ca2a457030.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by author
  prefs: []
  type: TYPE_NORMAL
- en: Why do Bayesian Inference?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having come this far, you might ask the question: all methods shown so far
    give me the same (or very similar) mode of the posterior distribution — leading
    me to always pick the same parameter in the end. Why bother then to do this complicated
    Bayesian inference?'
  prefs: []
  type: TYPE_NORMAL
- en: In this final section we will answer this question. We’ll first explain what
    advantages this brings, and finish up with a practical example showcasing this.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start: doing Bayesian inference gives us much more information. We not only
    get the mode of the posterior distribution, but the full distribution. We can
    thus inspect it, calculate other moments (such as the variance) and overall get
    a much better understanding of the problem. In particular, through this we get
    a sense of uncertainty, and can also decide to reject our hypothesis explaining
    the data. Let’s demonstrate this on an example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume we change our coin analysis into a “game”: we are given two coins, and
    have to pick that one which is fair, i.e. lands 50:50.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The game show hosts presents you with two coins:'
  prefs: []
  type: TYPE_NORMAL
- en: Coin 1 was thrown 8 times and landed heads in 4 of them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coin 2 was thrown 100 times and landed heads 50 times of these.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On a first glance, both seem to land heads with around 50% probability. However,
    intuitively most people would surely pick coin 2 — as here we have a much larger
    sample size. Clever as you are, you quickly apply the maximum a posteriori method
    in your head. You pick a beta distribution with `α = β = 2`, giving you a nicely
    spread symmetric distribution over [0, 1] with mode 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do the math and calculate the MAP estimates for `θ`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`θ₁` = (4+2-1)/(8+2+2–2)=0.5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`θ₂`= (50+2–1)/(100+2+2–2)=0.5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, according to the MAP method, both coins yield exactly the same result!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot the full posterior distribution for both, as obtaind by Bayesian
    inference above:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d51ac655967c1610b315ee4a5e08c33c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by author
  prefs: []
  type: TYPE_NORMAL
- en: Now, following our expectations, the posterior for Coin 2 has a much lower variance
    — justifying that we should pick this!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we introduced MLE (maximum likelihood estimation), MAP (maximum
    a posteriori estimation) and Bayesian inference. We used the example of an unfair
    coin throughout for demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: MLE finds parameters which optimize the likelihood. MAP introduces a new prior
    over the parameters, returning parameters which maximize the full posterior. Thus,
    both MLE and MAP return point estimates. In contrast, Bayesian inference models
    the full posterior distribution. This usually is a complex, often even intractable
    task — but also more powerful, as we get more insights into this distribution,
    such as the variance.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the end of this article. Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: 'Notes:'
  prefs: []
  type: TYPE_NORMAL
- en: All images, unless stated otherwise, were generated by the author.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples are calculations throughout this post were partially motivated by [this
    great tutorial](https://engineering.purdue.edu/kak/Trinity.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
