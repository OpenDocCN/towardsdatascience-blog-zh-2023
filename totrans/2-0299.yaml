- en: An Intuition for How Models like ChatGPT Work
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对ChatGPT等模型如何工作的直观理解
- en: 原文：[https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d](https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d](https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d)
- en: Providing an intuition on the ideas behind popular transformer models like ChatGPT
    and other large language models (LLMs)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提供对流行的变换器模型如ChatGPT及其他大型语言模型（LLMs）背后思想的直观理解
- en: '[](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[![David
    Hundley](../Images/1779ef96ec3d338f8fe4a9567ba7b194.png)](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    [David Hundley](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[![David
    Hundley](../Images/1779ef96ec3d338f8fe4a9567ba7b194.png)](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    [David Hundley](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    ·10 min read·Dec 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    ·阅读时间10分钟·2023年12月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6298bdd024c30f5750f9870f35f92f5a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6298bdd024c30f5750f9870f35f92f5a.png)'
- en: Title card created by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者创建的标题卡
- en: As we wind down 2023, it’s incredible to think about how much Generative AI
    has already impacted our daily lives. Starting with ChatGPT’s release in November
    2022, this space has evolved so quickly that it’s hard to believe that it’s been
    just one year in which all these advancements have come out.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着2023年的结束，想到生成式AI已经对我们的日常生活产生了如此大的影响，真是令人难以置信。自从2022年11月ChatGPT发布以来，这个领域发展如此迅速，以至于难以相信所有这些进展仅仅发生在一年之内。
- en: While the results are quite amazing, the underlying complexity has led a lot
    of people to publicly speculate on how these large language models (LLMs) work.
    Some people have speculated that these models are pulling from a preformulated
    database of responses, and some have gone as far to speculate that these LLMs
    have gained a human-level of sentience. These are extreme stances, and as you
    might guess, both are incorrect.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结果相当惊人，但其背后的复杂性使得很多人公开猜测这些大型语言模型（LLMs）的工作原理。有些人猜测这些模型是从预设的响应数据库中提取信息的，还有人猜测这些LLMs已经获得了人类水平的意识。这些都是极端的观点，正如你所猜测的，都是不正确的。
- en: You may have heard that these **LLMs are next-word predictors, meaning that
    they use probability to determine the next word that should come in a sentence**.
    This understanding is technically correct, but it’s a little too high level to
    sufficiently understand these models. In order to build a stronger intuition,
    we need to go deeper. **The intention of this post is to provide business leaders
    with a deep enough understanding of these models that they can make educated decisions
    on how to appropriately approach Generative AI for their respective companies**.
    We’ll keep things at more of a conceptual and intuitive level and stray away from
    the deep math behind these models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过这些**大型语言模型（LLMs）是下一个词预测器，也就是说它们使用概率来确定句子中应该出现的下一个词**。这种理解在技术上是正确的，但对于充分理解这些模型来说，略显抽象。为了建立更强的直觉，我们需要深入探讨。**这篇文章的目的是为商业领袖提供足够深入的理解，以便他们能够做出明智的决策，从而适当地接触生成式AI，以满足各自公司的需求**。我们将保持在更具概念性和直观的层面，避免深入探讨这些模型背后的复杂数学。
- en: Making Sense of Language
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解语言
- en: 'Consider the sentence, “I like to drink _______ in the morning.” How might
    you discern how to fill in that blank? Most reasonable people might fill in answers
    like coffee, water, or juice. The more silly among us might say something like
    beer or sour milk, but all these various options fixate on one important context
    clue: drinking. That alone narrows down what that blank could be, but those who
    took in the full context of the sentence also noticed the word “morning” and thus
    narrowed the context even further. In other words, “drink” + “morning” = something
    in the neighborhood of a breakfast beverage.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个句子：“我喜欢在早晨喝______。”你会如何判断如何填补这个空白？大多数合理的人可能会填入咖啡、水或果汁。更搞笑的可能会说啤酒或酸奶，但所有这些不同的选项都集中在一个重要的语境线索上：饮用。这本身就缩小了空白的可能性，但那些理解了整个句子语境的人还注意到“早晨”这个词，从而进一步缩小了语境。换句话说，“饮用”
    + “早晨” = 一种早餐饮料。
- en: This was simple for us to do because the phrase I gave was in English, and you’re
    presumably reading this in English. But what if we don’t have that direct understanding
    of those contextual words like “drink” and “morning” to understand how to properly
    fill in that blank?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，这很简单，因为我给出的短语是英文的，而你大概也是用英文阅读的。但是，如果我们没有直接理解诸如“饮料”和“早晨”这些语境词汇的能力，我们怎么能正确地填补那个空白呢？
- en: This is precisely the dilemma that computers face. Computers have no semantic
    understanding of the world because at the core of the computer’s CPU or GPU, it’s
    blasting through a blazing amount of one’s and zero’s. In other words, it has
    no intuition that the sky is blue, what morning is, nor how delicious pizza is.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是计算机面临的困境。计算机对世界没有语义理解，因为在计算机的CPU或GPU的核心部分，它只是以极快的速度处理一堆一和零。换句话说，它没有直觉去理解天空是蓝色的、早晨是什么，或者比萨饼有多么美味。
- en: So you might be wondering, how does a computer get around this problem?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可能会想，计算机如何解决这个问题呢？
- en: I actually got to explore this notion firsthand by playing a new indie video
    game called [*Chants of Sennaar*](https://youtu.be/__hzPH3tcvA?si=5diH-3hoT179hy7J).
    In the game, the player takes control of a character who is in a land of people
    where people speak in decipherable glyphs. As the game progresses, the player
    becomes acclimated by context clues demonstrated in the environment. For example,
    two of the early words you learn are “you” and “me”, and that’s because one of
    non-playable characters (NPCs) points at itself while stating the glyph for “me”
    and points at the player character while stating the glyph for “you.” It’s very
    much how you might imagine historians translating Egyptian hieroglyphics into
    more modern languages.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我实际上通过玩一款新的独立视频游戏[*《塞那尔的咏唱》*](https://youtu.be/__hzPH3tcvA?si=5diH-3hoT179hy7J)来亲身体验了这个概念。在游戏中，玩家控制一个角色，这个角色身处一个人们用可解读的象形文字交流的世界。随着游戏的进行，玩家通过环境中展示的语境线索逐渐适应。例如，你早期学到的两个词是“你”和“我”，这是因为一个非玩家角色（NPC）在说“我”的象形文字时指向自己，而在说“你”的象形文字时指向玩家角色。这非常类似于你想象的历史学家如何将埃及象形文字翻译成更现代的语言。
- en: 'Notice in these cases, it did not matter at all what the individual characters
    were. In *Chants of Sennaar*, you can get around learning this glyph-like language
    no matter what language the player speaks. These characters were made up, and
    what mattered were two things: **consistency and context**. Let’s explore each
    of these concepts in the following sections.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，单个字符到底是什么并不重要。在*《塞那尔的咏唱》*中，无论玩家说什么语言，你都可以避免学习这种类似象形文字的语言。这些字符是虚构的，重要的是两个方面：**一致性和语境**。接下来让我们深入探讨这些概念。
- en: Consistency via Sequencing
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一致性通过排序
- en: The **sequence** in which you use words has to be relatively consistent over
    time. In the English language, almost all sentences follow a typical subject-verb
    structure. While languages like Spanish tend to alter their verbs based on the
    subject in question, it still works as just a valid a language as English. For
    example, consider the sentence, “I am going to the store.” In Spanish, this translates
    to, “Me voy a la tienda.” Both of these languages use different words and a slightly
    different sequencing of the words, but both convey the same idea of going to the
    store. This makes both languages equally valid.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this same principle transcends English and Spanish and works for
    all languages, verbal and written. Why? **The sequencing of the words is more
    important than the words themselves.**
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: This is good news for a computer that operates in binary ones and zeroes. If
    the words themselves don’t matter, that means we can redefine them however we
    need. In our computer’s case, it wants to work with numbers, so it converts the
    words into vectors of numbers. We call this process of changing words into vectors
    as the **encoding** process, and we refer to the output of this process — the
    number vectors — as **embeddings**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: It probably comes as no surprise to you that this simple encoding process is
    not new. Getting the words into something a computer can fiddle around with has
    never been the problem. The challenge has been that researchers have had to spend
    decades trying to use different mathematical algorithms to make sense of the embeddings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Without going into a whole lot of detail, this effort became known as the field
    of **natural language processing (NLP)**, and it has a rich history that we’ll
    save for another day. Many NLP techniques have been introduced over the years,
    and many are still effective solutions for certain scenarios today. In fact, several
    of these techniques can be even more effective than LLMs for certain use cases,
    making them still advisable for some of today’s problems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: One major NLP breakthrough centered on this same idea of consistency of word
    sequencing being important. These researchers at Google in 2014 discovered a way
    to effectively encode a sentence and then later “decode” it in a specific way
    for fruitful results. For example, they could take an English sentence, pass it
    through this **encoder-decoder architecture**, and on the other side would pop
    out that same sentence but in Spanish. They referred to this sequence-to-sequence
    architecture as **seq2seq**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Remember, I noted that when it comes to language, the two most important characteristics
    are consistency (in sequencing) and context. The seq2seq helps to satisfy the
    consistency matter, but what about context?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Context Clues
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Context remained a slippery idea the next few years as early encoder-decoder
    architectures like seq2seq didn’t have a good way of managing the context. Researchers
    tried different things, including introducing the somewhat effective **long, short-term
    memory (LSTM)** cell, where the neural network essentially tried keeping a running
    context of the full sentence history. For example, consider the following sentence,
    “Don’t let the burglar in so that he may steal all our precious goods and belongings.”
    Imagine if a model made context of every word in that sentence except for the
    first. Well, now you have the opposite intention of the sentence! The LSTM sought
    to right those wrongs by keeping as much early context as possible. Mathematically
    speaking, it meant holding onto some numbers to represent the “long term memory”
    while giving equal or extra weight to the more “short term memory.”
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Still, the LSTM didn’t quite cut it, and if you think about context, you’ll
    understand this is because not all words have an equal weighting. Let’s revisit
    our original example, “I like to drink ______ in the morning.” In this case, the
    most important word is “drink” in helping us to fill in the blank. Thus, we should
    give more weight to that word, or you might say, we should pay more **attention**
    to that word. “Attention” is precisely how the Google researchers referred to
    it in their infamous 2017 paper “Attention is All You Need.”
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Attention in neural networks is a complex mathematical process, but we can still
    understand it at an intuitive level. As we already touched on, information is
    first **encoded** through the embedding process, that way we can do an apples-to-apples
    comparison on different sequences of words. This information is later passed through
    a **decoder** that produces a new sequence of words in response to the input we
    give the model. While we demonstrated that we can input one sentence in English
    and produce a Spanish translation, we can also pass through an input like, “What
    is the capital of Illinois?” and still receive an appropriately decoded output
    of, “The capital of Illinois is Springfield.”
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '**Remember, attention is all about giving focus to the important words in a
    context to produce a more accurate output**. This is done by assessing the similarity
    of a word during the decoding process to all the words encoded in the encoding
    process. Consider the example from our previous paragraph. Let’s say the decoder
    model has already produced the output, “The capital of Illinois is _______.” How
    does it know how to fill in this final blank? It’s going to look at how similar
    the current decoded words are to the encoding process. Specifically, it’s going
    to see a strong correlation / similarity between the words “capital” and “Illinois”
    to derive the most probabilistic answer, which in this case is “Springfield.”'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can imagine that LLMs need a LOT of examples in order to effectively
    produce more generalized outputs, hence why we refer to them as “large” language
    models. Generally speaking, the more examples (or **parameters**) that we provide
    to an LLM during the training process, the greater chance it has at predicting
    the correct words. Of course, we’re oversimplifying this concept, and there are
    certainly other nuances that go into influencing a Generative AI model’s output.
    For example, people are beginning to find effectiveness in **mixture of expert
    (MoE)** models, where “smaller” models are trained on a more specific domain of
    knowledge and later combined to produce more fruitful results. In fact, it’s rumored
    that this is how OpenAI’s popular GPT-4 works under the hood.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以想象，为了有效地产生更具普遍性的输出，LLMs 需要大量的示例，这就是为什么我们称它们为“大型”语言模型。一般来说，我们在训练过程中向 LLM
    提供的示例（或 **参数**）越多，它预测正确单词的机会就越大。当然，我们简化了这个概念，影响生成 AI 模型输出的因素还有很多细微之处。例如，人们开始发现
    **专家混合（MoE）** 模型的有效性，其中“小型”模型在更具体的知识领域进行训练，然后组合以产生更有成效的结果。事实上，有传言称这就是 OpenAI 的热门
    GPT-4 在后台工作的方式。
- en: Addressing Pitfalls & Myths
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理陷阱与误区
- en: As cool as LLMs can be, they are not perfect and do not always produce the most
    correct or relevant results. We refer to these confident but incorrect responses
    as **hallucinations**, but technically speaking, it is not fair to the LLM to
    refer to hallucinations as incorrect. Remember, as we’ve explored the intuition
    behind these models throughout this post, we have a general understanding of how
    results are derived using very fancy probability methods. **There is no sentient
    reasoning going on in these models; they’re simply predicting what the probabilities
    are based on the training process.** You can’t really blame the model for adhering
    to empirically derived probabilities!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LLMs 很酷，但它们并不完美，并且不总是产生最正确或最相关的结果。我们将这些自信但不正确的回答称为 **幻觉**，但从技术上讲，将幻觉称为不正确对
    LLM 来说是不公平的。请记住，正如我们在本文中探讨这些模型背后的直觉时，我们对如何使用非常复杂的概率方法得出结果有一个大致的了解。**这些模型中没有感知推理；它们只是根据训练过程预测概率。**
    你不能真正责怪模型遵循经验推导的概率！
- en: This is especially apparent if you use different kinds of LLMs. You might be
    only familiar with ChatGPT, but if you play around with enough LLMs, you’ll notice
    drastically different results. For example, it is rumored that ChatGPT’s underlying
    model, `gpt-3.5-turbo`, was trained on 70 billion parameters. We now have models
    like Meta’s Llama 2 with flavors that go as low as 7 billion parameters, one tenth
    that of `gpt-3.5-turbo`. And in practice, it’s very obvious the performance difference
    between these models. (Granted, smaller models like Mistral are getting better
    and even coming close to matching that of ChatGPT!)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用不同种类的 LLM，这一点尤其明显。你可能只熟悉 ChatGPT，但如果你尝试足够多的 LLM，你会注意到结果有很大不同。例如，有传言称 ChatGPT
    的基础模型 `gpt-3.5-turbo` 是在 700 亿个参数上训练的。现在我们有像 Meta 的 Llama 2 这样的模型，参数数量低至 70 亿，相当于
    `gpt-3.5-turbo` 的十分之一。在实践中，这些模型之间的性能差异非常明显。（当然，像 Mistral 这样的较小模型也在不断改进，甚至接近 ChatGPT
    的表现！）
- en: The tendency for models to hallucinate should give a business leader pause when
    applying Generative AI to business processes. In these earlier days of LLMs, it’s
    perhaps more appropriate to include a “human in the loop” mechanism, where a human
    has their work processes augmented with Generative AI but can ultimately “overrule”
    an LLM’s response if the human feels that the model didn’t give a good result.
    Of course, these models are going to get better and better over time, so perhaps
    a business leader may relax some of these restrictions as time goes on. That will
    be up to the risk appetite of the business.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 模型产生幻觉的倾向应该让企业领导在将生成 AI 应用于业务流程时三思。在这些早期的 LLM 日子里，也许更合适的是包含一个“人工干预”机制，其中人工将生成
    AI 用于辅助工作过程，但如果人工认为模型的结果不好，可以“推翻” LLM 的回答。当然，这些模型随着时间的推移会变得越来越好，所以企业领导可能会随着时间的推移放宽一些限制。这将取决于企业的风险承受能力。
- en: Additionally, it should be noted that these large language models can exhibit
    an unfair bias, albeit this is *NOT* to say that this bias is intended. I like
    to think of LLMs as “zeitgeist machines.” If LLMs are next-word predictors based
    on advanced probabilities, **the predictions are only as good as the data it saw
    at the time of training**. So if you were to train an LLM only on a bunch of text
    talking about how nasty pizza tastes, don’t be surprised when the LLM has a tendency
    to talk negatively about pizza! Likewise, the complaints from people online that
    LLMs exhibit bias in unfair ways are correct, but it’s only because the “zeitgeist”
    of the training data inclined it that way. For example, it should come as no surprise
    that an LLM may be unfairly biased against a certain political candidate if the
    training data contained many articles that were critical about that candidate.
    It’s a myth to believe that this bias was intentionally baked into the model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we should address the concern of copyright infringement. Because LLMs
    measure advanced probabilities between words, it should come as no surprise when
    these LLMs are able to emulate a piece of copyrighted work. Intentionally use
    the word “emulate” because LLMs are unable to reproduce full bodies of copyrighted
    work simply due to complex probabities. So I can have an LLM talk to me like Hagrid
    from the *Harry Potter* series or Jar Jar Binks from *Star Wars*, but it will
    struggle to fully reproduce the dialogue from those movie scripts. In other words,
    don’t expect the LLM to be able to sufficiently reproduce the entire *Harry Potter*
    book series. It’s just too long, and the probabilities of these words get too
    messy.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Still, the copyright infringement space is a messy one, and it seems to be a
    problem more beholden to these companies training LLMs and less a problem for
    the companies simply using them. Now granted, a company making use of another’s
    LLM should still seek to put up guardrails that would not intentionally try to
    reproduce copyrighted work, which is easy enough to do with appropriate prompt
    engineering. But companies like OpenAI are currently facing this legal predicament
    on whether or not they were legally allowed to use others’ data for training purposes
    in the first place. At the time of this article’s posting, [*The New York Time*s
    is suing OpenAI and Microsoft alleging that their news data was inappropriately
    used in the training of the model](https://fortune.com/2023/12/27/openai-microsoft-new-york-times-lawsuit-ai-copyright-infringement/).
    I am not an expert on these sorts of legal matters, but it will be important to
    watch cases like these to see how legislation may impact the evolution of LLMs
    going forward.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The Generative AI space is an extremely fascinating one, and I’m excited to
    see how the space continues to evolve in the future. We are very much still in
    the early days of this revolution, and I anticipate we will continue to see advancements
    in adoption, technological evolution, and legal understanding. I hope that this
    post provided you with enough intuition that you can make better informed decisions
    on how to approach this exciting domain with the appropriate level of caution!
    😃
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能领域是一个极具吸引力的领域，我很期待看到未来这个领域如何持续发展。我们仍处于这场变革的早期阶段，我预期我们将继续看到在采纳、技术演进和法律理解方面的进展。我希望这篇文章能为你提供足够的直觉，以便你能在以适当的谨慎态度接触这一激动人心的领域时做出更明智的决策！
    😃
