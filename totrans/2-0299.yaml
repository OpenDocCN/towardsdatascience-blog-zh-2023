- en: An Intuition for How Models like ChatGPT Work
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯¹ChatGPTç­‰æ¨¡å‹å¦‚ä½•å·¥ä½œçš„ç›´è§‚ç†è§£
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d](https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d](https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d)
- en: Providing an intuition on the ideas behind popular transformer models like ChatGPT
    and other large language models (LLMs)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æä¾›å¯¹æµè¡Œçš„å˜æ¢å™¨æ¨¡å‹å¦‚ChatGPTåŠå…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒŒåæ€æƒ³çš„ç›´è§‚ç†è§£
- en: '[](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[![David
    Hundley](../Images/1779ef96ec3d338f8fe4a9567ba7b194.png)](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    [David Hundley](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[![David
    Hundley](../Images/1779ef96ec3d338f8fe4a9567ba7b194.png)](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    [David Hundley](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    Â·10 min readÂ·Dec 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    Â·é˜…è¯»æ—¶é—´10åˆ†é’ŸÂ·2023å¹´12æœˆ30æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6298bdd024c30f5750f9870f35f92f5a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6298bdd024c30f5750f9870f35f92f5a.png)'
- en: Title card created by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…åˆ›å»ºçš„æ ‡é¢˜å¡
- en: As we wind down 2023, itâ€™s incredible to think about how much Generative AI
    has already impacted our daily lives. Starting with ChatGPTâ€™s release in November
    2022, this space has evolved so quickly that itâ€™s hard to believe that itâ€™s been
    just one year in which all these advancements have come out.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€2023å¹´çš„ç»“æŸï¼Œæƒ³åˆ°ç”Ÿæˆå¼AIå·²ç»å¯¹æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»äº§ç”Ÿäº†å¦‚æ­¤å¤§çš„å½±å“ï¼ŒçœŸæ˜¯ä»¤äººéš¾ä»¥ç½®ä¿¡ã€‚è‡ªä»2022å¹´11æœˆChatGPTå‘å¸ƒä»¥æ¥ï¼Œè¿™ä¸ªé¢†åŸŸå‘å±•å¦‚æ­¤è¿…é€Ÿï¼Œä»¥è‡³äºéš¾ä»¥ç›¸ä¿¡æ‰€æœ‰è¿™äº›è¿›å±•ä»…ä»…å‘ç”Ÿåœ¨ä¸€å¹´ä¹‹å†…ã€‚
- en: While the results are quite amazing, the underlying complexity has led a lot
    of people to publicly speculate on how these large language models (LLMs) work.
    Some people have speculated that these models are pulling from a preformulated
    database of responses, and some have gone as far to speculate that these LLMs
    have gained a human-level of sentience. These are extreme stances, and as you
    might guess, both are incorrect.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç»“æœç›¸å½“æƒŠäººï¼Œä½†å…¶èƒŒåçš„å¤æ‚æ€§ä½¿å¾—å¾ˆå¤šäººå…¬å¼€çŒœæµ‹è¿™äº›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å·¥ä½œåŸç†ã€‚æœ‰äº›äººçŒœæµ‹è¿™äº›æ¨¡å‹æ˜¯ä»é¢„è®¾çš„å“åº”æ•°æ®åº“ä¸­æå–ä¿¡æ¯çš„ï¼Œè¿˜æœ‰äººçŒœæµ‹è¿™äº›LLMså·²ç»è·å¾—äº†äººç±»æ°´å¹³çš„æ„è¯†ã€‚è¿™äº›éƒ½æ˜¯æç«¯çš„è§‚ç‚¹ï¼Œæ­£å¦‚ä½ æ‰€çŒœæµ‹çš„ï¼Œéƒ½æ˜¯ä¸æ­£ç¡®çš„ã€‚
- en: You may have heard that these **LLMs are next-word predictors, meaning that
    they use probability to determine the next word that should come in a sentence**.
    This understanding is technically correct, but itâ€™s a little too high level to
    sufficiently understand these models. In order to build a stronger intuition,
    we need to go deeper. **The intention of this post is to provide business leaders
    with a deep enough understanding of these models that they can make educated decisions
    on how to appropriately approach Generative AI for their respective companies**.
    Weâ€™ll keep things at more of a conceptual and intuitive level and stray away from
    the deep math behind these models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½å¬è¯´è¿‡è¿™äº›**å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯ä¸‹ä¸€ä¸ªè¯é¢„æµ‹å™¨ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒä»¬ä½¿ç”¨æ¦‚ç‡æ¥ç¡®å®šå¥å­ä¸­åº”è¯¥å‡ºç°çš„ä¸‹ä¸€ä¸ªè¯**ã€‚è¿™ç§ç†è§£åœ¨æŠ€æœ¯ä¸Šæ˜¯æ­£ç¡®çš„ï¼Œä½†å¯¹äºå……åˆ†ç†è§£è¿™äº›æ¨¡å‹æ¥è¯´ï¼Œç•¥æ˜¾æŠ½è±¡ã€‚ä¸ºäº†å»ºç«‹æ›´å¼ºçš„ç›´è§‰ï¼Œæˆ‘ä»¬éœ€è¦æ·±å…¥æ¢è®¨ã€‚**è¿™ç¯‡æ–‡ç« çš„ç›®çš„æ˜¯ä¸ºå•†ä¸šé¢†è¢–æä¾›è¶³å¤Ÿæ·±å…¥çš„ç†è§£ï¼Œä»¥ä¾¿ä»–ä»¬èƒ½å¤Ÿåšå‡ºæ˜æ™ºçš„å†³ç­–ï¼Œä»è€Œé€‚å½“åœ°æ¥è§¦ç”Ÿæˆå¼AIï¼Œä»¥æ»¡è¶³å„è‡ªå…¬å¸çš„éœ€æ±‚**ã€‚æˆ‘ä»¬å°†ä¿æŒåœ¨æ›´å…·æ¦‚å¿µæ€§å’Œç›´è§‚çš„å±‚é¢ï¼Œé¿å…æ·±å…¥æ¢è®¨è¿™äº›æ¨¡å‹èƒŒåçš„å¤æ‚æ•°å­¦ã€‚
- en: Making Sense of Language
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£è¯­è¨€
- en: 'Consider the sentence, â€œI like to drink _______ in the morning.â€ How might
    you discern how to fill in that blank? Most reasonable people might fill in answers
    like coffee, water, or juice. The more silly among us might say something like
    beer or sour milk, but all these various options fixate on one important context
    clue: drinking. That alone narrows down what that blank could be, but those who
    took in the full context of the sentence also noticed the word â€œmorningâ€ and thus
    narrowed the context even further. In other words, â€œdrinkâ€ + â€œmorningâ€ = something
    in the neighborhood of a breakfast beverage.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘è¿™ä¸ªå¥å­ï¼šâ€œæˆ‘å–œæ¬¢åœ¨æ—©æ™¨å–______ã€‚â€ä½ ä¼šå¦‚ä½•åˆ¤æ–­å¦‚ä½•å¡«è¡¥è¿™ä¸ªç©ºç™½ï¼Ÿå¤§å¤šæ•°åˆç†çš„äººå¯èƒ½ä¼šå¡«å…¥å’–å•¡ã€æ°´æˆ–æœæ±ã€‚æ›´æç¬‘çš„å¯èƒ½ä¼šè¯´å•¤é…’æˆ–é…¸å¥¶ï¼Œä½†æ‰€æœ‰è¿™äº›ä¸åŒçš„é€‰é¡¹éƒ½é›†ä¸­åœ¨ä¸€ä¸ªé‡è¦çš„è¯­å¢ƒçº¿ç´¢ä¸Šï¼šé¥®ç”¨ã€‚è¿™æœ¬èº«å°±ç¼©å°äº†ç©ºç™½çš„å¯èƒ½æ€§ï¼Œä½†é‚£äº›ç†è§£äº†æ•´ä¸ªå¥å­è¯­å¢ƒçš„äººè¿˜æ³¨æ„åˆ°â€œæ—©æ™¨â€è¿™ä¸ªè¯ï¼Œä»è€Œè¿›ä¸€æ­¥ç¼©å°äº†è¯­å¢ƒã€‚æ¢å¥è¯è¯´ï¼Œâ€œé¥®ç”¨â€
    + â€œæ—©æ™¨â€ = ä¸€ç§æ—©é¤é¥®æ–™ã€‚
- en: This was simple for us to do because the phrase I gave was in English, and youâ€™re
    presumably reading this in English. But what if we donâ€™t have that direct understanding
    of those contextual words like â€œdrinkâ€ and â€œmorningâ€ to understand how to properly
    fill in that blank?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æˆ‘ä»¬æ¥è¯´ï¼Œè¿™å¾ˆç®€å•ï¼Œå› ä¸ºæˆ‘ç»™å‡ºçš„çŸ­è¯­æ˜¯è‹±æ–‡çš„ï¼Œè€Œä½ å¤§æ¦‚ä¹Ÿæ˜¯ç”¨è‹±æ–‡é˜…è¯»çš„ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æ²¡æœ‰ç›´æ¥ç†è§£è¯¸å¦‚â€œé¥®æ–™â€å’Œâ€œæ—©æ™¨â€è¿™äº›è¯­å¢ƒè¯æ±‡çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æ€ä¹ˆèƒ½æ­£ç¡®åœ°å¡«è¡¥é‚£ä¸ªç©ºç™½å‘¢ï¼Ÿ
- en: This is precisely the dilemma that computers face. Computers have no semantic
    understanding of the world because at the core of the computerâ€™s CPU or GPU, itâ€™s
    blasting through a blazing amount of oneâ€™s and zeroâ€™s. In other words, it has
    no intuition that the sky is blue, what morning is, nor how delicious pizza is.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯è®¡ç®—æœºé¢ä¸´çš„å›°å¢ƒã€‚è®¡ç®—æœºå¯¹ä¸–ç•Œæ²¡æœ‰è¯­ä¹‰ç†è§£ï¼Œå› ä¸ºåœ¨è®¡ç®—æœºçš„CPUæˆ–GPUçš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œå®ƒåªæ˜¯ä»¥æå¿«çš„é€Ÿåº¦å¤„ç†ä¸€å †ä¸€å’Œé›¶ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒæ²¡æœ‰ç›´è§‰å»ç†è§£å¤©ç©ºæ˜¯è“è‰²çš„ã€æ—©æ™¨æ˜¯ä»€ä¹ˆï¼Œæˆ–è€…æ¯”è¨é¥¼æœ‰å¤šä¹ˆç¾å‘³ã€‚
- en: So you might be wondering, how does a computer get around this problem?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å¯èƒ½ä¼šæƒ³ï¼Œè®¡ç®—æœºå¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿ
- en: I actually got to explore this notion firsthand by playing a new indie video
    game called [*Chants of Sennaar*](https://youtu.be/__hzPH3tcvA?si=5diH-3hoT179hy7J).
    In the game, the player takes control of a character who is in a land of people
    where people speak in decipherable glyphs. As the game progresses, the player
    becomes acclimated by context clues demonstrated in the environment. For example,
    two of the early words you learn are â€œyouâ€ and â€œmeâ€, and thatâ€™s because one of
    non-playable characters (NPCs) points at itself while stating the glyph for â€œmeâ€
    and points at the player character while stating the glyph for â€œyou.â€ Itâ€™s very
    much how you might imagine historians translating Egyptian hieroglyphics into
    more modern languages.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å®é™…ä¸Šé€šè¿‡ç©ä¸€æ¬¾æ–°çš„ç‹¬ç«‹è§†é¢‘æ¸¸æˆ[*ã€Šå¡é‚£å°”çš„å’å”±ã€‹*](https://youtu.be/__hzPH3tcvA?si=5diH-3hoT179hy7J)æ¥äº²èº«ä½“éªŒäº†è¿™ä¸ªæ¦‚å¿µã€‚åœ¨æ¸¸æˆä¸­ï¼Œç©å®¶æ§åˆ¶ä¸€ä¸ªè§’è‰²ï¼Œè¿™ä¸ªè§’è‰²èº«å¤„ä¸€ä¸ªäººä»¬ç”¨å¯è§£è¯»çš„è±¡å½¢æ–‡å­—äº¤æµçš„ä¸–ç•Œã€‚éšç€æ¸¸æˆçš„è¿›è¡Œï¼Œç©å®¶é€šè¿‡ç¯å¢ƒä¸­å±•ç¤ºçš„è¯­å¢ƒçº¿ç´¢é€æ¸é€‚åº”ã€‚ä¾‹å¦‚ï¼Œä½ æ—©æœŸå­¦åˆ°çš„ä¸¤ä¸ªè¯æ˜¯â€œä½ â€å’Œâ€œæˆ‘â€ï¼Œè¿™æ˜¯å› ä¸ºä¸€ä¸ªéç©å®¶è§’è‰²ï¼ˆNPCï¼‰åœ¨è¯´â€œæˆ‘â€çš„è±¡å½¢æ–‡å­—æ—¶æŒ‡å‘è‡ªå·±ï¼Œè€Œåœ¨è¯´â€œä½ â€çš„è±¡å½¢æ–‡å­—æ—¶æŒ‡å‘ç©å®¶è§’è‰²ã€‚è¿™éå¸¸ç±»ä¼¼äºä½ æƒ³è±¡çš„å†å²å­¦å®¶å¦‚ä½•å°†åŸƒåŠè±¡å½¢æ–‡å­—ç¿»è¯‘æˆæ›´ç°ä»£çš„è¯­è¨€ã€‚
- en: 'Notice in these cases, it did not matter at all what the individual characters
    were. In *Chants of Sennaar*, you can get around learning this glyph-like language
    no matter what language the player speaks. These characters were made up, and
    what mattered were two things: **consistency and context**. Letâ€™s explore each
    of these concepts in the following sections.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œå•ä¸ªå­—ç¬¦åˆ°åº•æ˜¯ä»€ä¹ˆå¹¶ä¸é‡è¦ã€‚åœ¨*ã€Šå¡é‚£å°”çš„å’å”±ã€‹*ä¸­ï¼Œæ— è®ºç©å®¶è¯´ä»€ä¹ˆè¯­è¨€ï¼Œä½ éƒ½å¯ä»¥é¿å…å­¦ä¹ è¿™ç§ç±»ä¼¼è±¡å½¢æ–‡å­—çš„è¯­è¨€ã€‚è¿™äº›å­—ç¬¦æ˜¯è™šæ„çš„ï¼Œé‡è¦çš„æ˜¯ä¸¤ä¸ªæ–¹é¢ï¼š**ä¸€è‡´æ€§å’Œè¯­å¢ƒ**ã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨è¿™äº›æ¦‚å¿µã€‚
- en: Consistency via Sequencing
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸€è‡´æ€§é€šè¿‡æ’åº
- en: The **sequence** in which you use words has to be relatively consistent over
    time. In the English language, almost all sentences follow a typical subject-verb
    structure. While languages like Spanish tend to alter their verbs based on the
    subject in question, it still works as just a valid a language as English. For
    example, consider the sentence, â€œI am going to the store.â€ In Spanish, this translates
    to, â€œMe voy a la tienda.â€ Both of these languages use different words and a slightly
    different sequencing of the words, but both convey the same idea of going to the
    store. This makes both languages equally valid.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä½¿ç”¨å•è¯çš„**é¡ºåº**å¿…é¡»åœ¨æ—¶é—´ä¸Šä¿æŒç›¸å¯¹ä¸€è‡´ã€‚åœ¨è‹±è¯­ä¸­ï¼Œå‡ ä¹æ‰€æœ‰çš„å¥å­éƒ½éµå¾ªå…¸å‹çš„ä¸»è°“ç»“æ„ã€‚è™½ç„¶åƒè¥¿ç­ç‰™è¯­è¿™æ ·çš„è¯­è¨€å¾€å¾€æ ¹æ®ä¸»è¯­æ”¹å˜åŠ¨è¯ï¼Œä½†å®ƒä»ç„¶æ˜¯ä¸€ç§ä¸è‹±è¯­ä¸€æ ·æœ‰æ•ˆçš„è¯­è¨€ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘å¥å­â€œæˆ‘å»å•†åº—â€ã€‚åœ¨è¥¿ç­ç‰™è¯­ä¸­ï¼Œè¿™å¥è¯ç¿»è¯‘ä¸ºâ€œMe
    voy a la tiendaâ€ã€‚è¿™ä¸¤ç§è¯­è¨€ä½¿ç”¨äº†ä¸åŒçš„å•è¯å’Œç•¥æœ‰ä¸åŒçš„å•è¯é¡ºåºï¼Œä½†éƒ½è¡¨è¾¾äº†å»å•†åº—çš„ç›¸åŒæ„æ€ã€‚è¿™ä½¿å¾—è¿™ä¸¤ç§è¯­è¨€éƒ½æ˜¯æœ‰æ•ˆçš„ã€‚
- en: Of course, this same principle transcends English and Spanish and works for
    all languages, verbal and written. Why? **The sequencing of the words is more
    important than the words themselves.**
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œè¿™ä¸ªç›¸åŒçš„åŸåˆ™è¶…è¶Šäº†è‹±è¯­å’Œè¥¿ç­ç‰™è¯­ï¼Œé€‚ç”¨äºæ‰€æœ‰è¯­è¨€ï¼Œæ— è®ºæ˜¯å£å¤´è¿˜æ˜¯ä¹¦é¢è¯­è¨€ã€‚ä¸ºä»€ä¹ˆï¼Ÿ**å•è¯çš„é¡ºåºæ¯”å•è¯æœ¬èº«æ›´é‡è¦ã€‚**
- en: This is good news for a computer that operates in binary ones and zeroes. If
    the words themselves donâ€™t matter, that means we can redefine them however we
    need. In our computerâ€™s case, it wants to work with numbers, so it converts the
    words into vectors of numbers. We call this process of changing words into vectors
    as the **encoding** process, and we refer to the output of this process â€” the
    number vectors â€” as **embeddings**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€ä¸ªä»¥äºŒè¿›åˆ¶çš„ 1 å’Œ 0 æ“ä½œçš„è®¡ç®—æœºæ¥è¯´ï¼Œè¿™æ— ç–‘æ˜¯ä¸ªå¥½æ¶ˆæ¯ã€‚å¦‚æœå•è¯æœ¬èº«å¹¶ä¸é‡è¦ï¼Œé‚£æ„å‘³ç€æˆ‘ä»¬å¯ä»¥æ ¹æ®éœ€è¦é‡æ–°å®šä¹‰å®ƒä»¬ã€‚åœ¨æˆ‘ä»¬çš„è®¡ç®—æœºä¸­ï¼Œå®ƒå¸Œæœ›å¤„ç†æ•°å­—ï¼Œå› æ­¤å®ƒå°†å•è¯è½¬æ¢ä¸ºæ•°å­—å‘é‡ã€‚æˆ‘ä»¬ç§°è¿™ä¸ªå°†å•è¯è½¬æ¢ä¸ºå‘é‡çš„è¿‡ç¨‹ä¸º**ç¼–ç **è¿‡ç¨‹ï¼Œè€Œæˆ‘ä»¬ç§°è¿™ä¸ªè¿‡ç¨‹çš„è¾“å‡ºâ€”â€”æ•°å­—å‘é‡â€”â€”ä¸º**åµŒå…¥**ã€‚
- en: It probably comes as no surprise to you that this simple encoding process is
    not new. Getting the words into something a computer can fiddle around with has
    never been the problem. The challenge has been that researchers have had to spend
    decades trying to use different mathematical algorithms to make sense of the embeddings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¸ä¼šæ„Ÿåˆ°æƒŠè®¶ï¼Œè¿™ä¸ªç®€å•çš„ç¼–ç è¿‡ç¨‹å¹¶ä¸æ–°é²œã€‚å°†å•è¯è½¬åŒ–ä¸ºè®¡ç®—æœºå¯ä»¥æ“ä½œçš„å½¢å¼ä»æ¥éƒ½ä¸æ˜¯é—®é¢˜ã€‚æŒ‘æˆ˜åœ¨äºï¼Œç ”ç©¶äººå‘˜ä¸å¾—ä¸èŠ±è´¹æ•°åå¹´æ—¶é—´å°è¯•ä½¿ç”¨ä¸åŒçš„æ•°å­¦ç®—æ³•æ¥ç†è§£è¿™äº›åµŒå…¥ã€‚
- en: Without going into a whole lot of detail, this effort became known as the field
    of **natural language processing (NLP)**, and it has a rich history that weâ€™ll
    save for another day. Many NLP techniques have been introduced over the years,
    and many are still effective solutions for certain scenarios today. In fact, several
    of these techniques can be even more effective than LLMs for certain use cases,
    making them still advisable for some of todayâ€™s problems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ·±å…¥ç»†èŠ‚ï¼Œè¿™ä¸€åŠªåŠ›è¢«ç§°ä¸º**è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰**é¢†åŸŸï¼Œå®ƒæœ‰ç€ä¸°å¯Œçš„å†å²ï¼Œæˆ‘ä»¬ç•™å¾…å¦ä¸€å¤©è®¨è®ºã€‚å¤šå¹´æ¥å‡ºç°äº†è®¸å¤š NLP æŠ€æœ¯ï¼Œè®¸å¤šæŠ€æœ¯ä»Šå¤©åœ¨æŸäº›åœºæ™¯ä¸‹ä»ç„¶æœ‰æ•ˆã€‚å®é™…ä¸Šï¼Œå…¶ä¸­ä¸€äº›æŠ€æœ¯åœ¨æŸäº›ç”¨ä¾‹ä¸­ç”šè‡³æ¯”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ›´æœ‰æ•ˆï¼Œä½¿å¾—å®ƒä»¬åœ¨è§£å†³ä»Šå¤©çš„ä¸€äº›é—®é¢˜æ—¶ä»ç„¶å€¼å¾—æ¨èã€‚
- en: One major NLP breakthrough centered on this same idea of consistency of word
    sequencing being important. These researchers at Google in 2014 discovered a way
    to effectively encode a sentence and then later â€œdecodeâ€ it in a specific way
    for fruitful results. For example, they could take an English sentence, pass it
    through this **encoder-decoder architecture**, and on the other side would pop
    out that same sentence but in Spanish. They referred to this sequence-to-sequence
    architecture as **seq2seq**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé‡å¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†çªç ´é›†ä¸­åœ¨è¿™ä¸ªç›¸åŒçš„å•è¯é¡ºåºä¸€è‡´æ€§çš„é‡è¦æ€§ä¸Šã€‚2014 å¹´ï¼Œè°·æ­Œçš„ç ”ç©¶äººå‘˜å‘ç°äº†ä¸€ç§æœ‰æ•ˆåœ°ç¼–ç å¥å­å¹¶éšåä»¥ç‰¹å®šæ–¹å¼â€œè§£ç â€å®ƒä»¥è·å¾—æœ‰ç›Šç»“æœçš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œä»–ä»¬å¯ä»¥å°†ä¸€ä¸ªè‹±è¯­å¥å­ä¼ å…¥è¿™ä¸ª**ç¼–ç å™¨-è§£ç å™¨æ¶æ„**ï¼Œç„¶ååœ¨å¦ä¸€ç«¯å¼¹å‡ºåŒä¸€ä¸ªå¥å­ï¼Œä½†ç”¨è¥¿ç­ç‰™è¯­è¡¨ç¤ºã€‚ä»–ä»¬å°†è¿™ç§åºåˆ—åˆ°åºåˆ—çš„æ¶æ„ç§°ä¸º**seq2seq**ã€‚
- en: Remember, I noted that when it comes to language, the two most important characteristics
    are consistency (in sequencing) and context. The seq2seq helps to satisfy the
    consistency matter, but what about context?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œæˆ‘æåˆ°è¿‡åœ¨è¯­è¨€ä¸­ï¼Œä¸¤ä¸ªæœ€é‡è¦çš„ç‰¹å¾æ˜¯ä¸€è‡´æ€§ï¼ˆåœ¨é¡ºåºä¸Šï¼‰å’Œä¸Šä¸‹æ–‡ã€‚seq2seq æœ‰åŠ©äºè§£å†³ä¸€è‡´æ€§çš„é—®é¢˜ï¼Œä½†ä¸Šä¸‹æ–‡å‘¢ï¼Ÿ
- en: Context Clues
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸Šä¸‹æ–‡çº¿ç´¢
- en: Context remained a slippery idea the next few years as early encoder-decoder
    architectures like seq2seq didnâ€™t have a good way of managing the context. Researchers
    tried different things, including introducing the somewhat effective **long, short-term
    memory (LSTM)** cell, where the neural network essentially tried keeping a running
    context of the full sentence history. For example, consider the following sentence,
    â€œDonâ€™t let the burglar in so that he may steal all our precious goods and belongings.â€
    Imagine if a model made context of every word in that sentence except for the
    first. Well, now you have the opposite intention of the sentence! The LSTM sought
    to right those wrongs by keeping as much early context as possible. Mathematically
    speaking, it meant holding onto some numbers to represent the â€œlong term memoryâ€
    while giving equal or extra weight to the more â€œshort term memory.â€
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šä¸‹æ–‡åœ¨æ¥ä¸‹æ¥çš„å‡ å¹´é‡Œä»ç„¶æ˜¯ä¸€ä¸ªæ£˜æ‰‹çš„æ¦‚å¿µï¼Œå› ä¸ºæ—©æœŸçš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¦‚seq2seqï¼Œå¹¶æ²¡æœ‰å¾ˆå¥½çš„ç®¡ç†ä¸Šä¸‹æ–‡çš„æ–¹æ³•ã€‚ç ”ç©¶äººå‘˜å°è¯•äº†ä¸åŒçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å¼•å…¥äº†æŸç§æœ‰æ•ˆçš„**é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰**å•å…ƒï¼Œåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œç¥ç»ç½‘ç»œè¯•å›¾ä¿æŒæ•´ä¸ªå¥å­çš„å†å²ä¸Šä¸‹æ–‡ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘ä»¥ä¸‹å¥å­ï¼šâ€œä¸è¦è®©å°å·è¿›æ¥ï¼Œä»¥å…ä»–å·èµ°æˆ‘ä»¬æ‰€æœ‰çè´µçš„è´¢ç‰©å’Œç‰©å“ã€‚â€æƒ³è±¡ä¸€ä¸‹å¦‚æœä¸€ä¸ªæ¨¡å‹è®°å½•äº†è¿™ä¸ªå¥å­ä¸­æ¯ä¸ªå•è¯çš„ä¸Šä¸‹æ–‡ï¼Œé™¤äº†ç¬¬ä¸€ä¸ªå•è¯ã€‚é‚£ä¹ˆä½ ä¼šå¾—åˆ°ä¸å¥å­ç›¸åçš„æ„å›¾ï¼LSTMæ—¨åœ¨é€šè¿‡å°½å¯èƒ½å¤šåœ°ä¿ç•™æ—©æœŸä¸Šä¸‹æ–‡æ¥çº æ­£è¿™äº›é”™è¯¯ã€‚ä»æ•°å­¦ä¸Šè®²ï¼Œè¿™æ„å‘³ç€ä¿ç•™ä¸€äº›æ•°å­—ä»¥ä»£è¡¨â€œé•¿æœŸè®°å¿†â€ï¼ŒåŒæ—¶å¯¹æ›´å¤šçš„â€œçŸ­æœŸè®°å¿†â€ç»™äºˆç›¸ç­‰æˆ–é¢å¤–çš„æƒé‡ã€‚
- en: Still, the LSTM didnâ€™t quite cut it, and if you think about context, youâ€™ll
    understand this is because not all words have an equal weighting. Letâ€™s revisit
    our original example, â€œI like to drink ______ in the morning.â€ In this case, the
    most important word is â€œdrinkâ€ in helping us to fill in the blank. Thus, we should
    give more weight to that word, or you might say, we should pay more **attention**
    to that word. â€œAttentionâ€ is precisely how the Google researchers referred to
    it in their infamous 2017 paper â€œAttention is All You Need.â€
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼ŒLSTMå¹¶æ²¡æœ‰å®Œå…¨è§£å†³é—®é¢˜ï¼Œå¦‚æœä½ è€ƒè™‘ä¸Šä¸‹æ–‡ï¼Œä½ ä¼šç†è§£è¿™å› ä¸ºå¹¶ä¸æ˜¯æ‰€æœ‰çš„å•è¯éƒ½æœ‰ç›¸ç­‰çš„æƒé‡ã€‚è®©æˆ‘ä»¬é‡æ–°å®¡è§†ä¸€ä¸‹åŸå§‹ç¤ºä¾‹ï¼Œâ€œæˆ‘æ—©æ™¨å–œæ¬¢å–______ã€‚â€åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€é‡è¦çš„å•è¯æ˜¯â€œdrinkâ€ï¼Œå®ƒå¸®åŠ©æˆ‘ä»¬å¡«è¡¥ç©ºç™½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åº”è¯¥ç»™äºˆè¿™ä¸ªå•è¯æ›´å¤šçš„æƒé‡ï¼Œæˆ–è€…ä½ å¯ä»¥è¯´ï¼Œæˆ‘ä»¬åº”è¯¥å¯¹è¿™ä¸ªå•è¯ç»™äºˆæ›´å¤šçš„**æ³¨æ„**ã€‚â€œæ³¨æ„åŠ›â€æ­£æ˜¯è°·æ­Œç ”ç©¶äººå‘˜åœ¨ä»–ä»¬è‡­åæ˜­è‘—çš„2017å¹´è®ºæ–‡ã€Šæ³¨æ„åŠ›æœºåˆ¶æ‰æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡ã€‹ä¸­æåˆ°çš„ã€‚
- en: Attention in neural networks is a complex mathematical process, but we can still
    understand it at an intuitive level. As we already touched on, information is
    first **encoded** through the embedding process, that way we can do an apples-to-apples
    comparison on different sequences of words. This information is later passed through
    a **decoder** that produces a new sequence of words in response to the input we
    give the model. While we demonstrated that we can input one sentence in English
    and produce a Spanish translation, we can also pass through an input like, â€œWhat
    is the capital of Illinois?â€ and still receive an appropriately decoded output
    of, â€œThe capital of Illinois is Springfield.â€
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œä¸­çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ä¸ªå¤æ‚çš„æ•°å­¦è¿‡ç¨‹ï¼Œä½†æˆ‘ä»¬ä»ç„¶å¯ä»¥åœ¨ç›´è§‚å±‚é¢ä¸Šç†è§£å®ƒã€‚æ­£å¦‚æˆ‘ä»¬å·²ç»æåˆ°çš„ï¼Œä¿¡æ¯é¦–å…ˆé€šè¿‡åµŒå…¥è¿‡ç¨‹è¢«**ç¼–ç **ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å¯¹ä¸åŒçš„å•è¯åºåˆ—è¿›è¡Œé€ä¸€æ¯”è¾ƒã€‚è¿™äº›ä¿¡æ¯éšåä¼šé€šè¿‡ä¸€ä¸ª**è§£ç å™¨**ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„å•è¯åºåˆ—ï¼Œä»¥å“åº”æˆ‘ä»¬ç»™æ¨¡å‹çš„è¾“å…¥ã€‚è™½ç„¶æˆ‘ä»¬æ¼”ç¤ºäº†å¦‚ä½•è¾“å…¥ä¸€å¥è‹±æ–‡å¹¶ç”Ÿæˆè¥¿ç­ç‰™è¯­ç¿»è¯‘ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥è¾“å…¥åƒâ€œä¼Šåˆ©è¯ºä¼Šå·çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿâ€è¿™æ ·çš„å†…å®¹ï¼Œå¹¶ä»ç„¶å¾—åˆ°æ°å½“è§£ç çš„è¾“å‡ºï¼šâ€œä¼Šåˆ©è¯ºä¼Šå·çš„é¦–éƒ½æ˜¯æ–¯æ™®æ—è²å°”å¾·ã€‚â€
- en: '**Remember, attention is all about giving focus to the important words in a
    context to produce a more accurate output**. This is done by assessing the similarity
    of a word during the decoding process to all the words encoded in the encoding
    process. Consider the example from our previous paragraph. Letâ€™s say the decoder
    model has already produced the output, â€œThe capital of Illinois is _______.â€ How
    does it know how to fill in this final blank? Itâ€™s going to look at how similar
    the current decoded words are to the encoding process. Specifically, itâ€™s going
    to see a strong correlation / similarity between the words â€œcapitalâ€ and â€œIllinoisâ€
    to derive the most probabilistic answer, which in this case is â€œSpringfield.â€'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®°ä½ï¼Œæ³¨æ„åŠ›æœºåˆ¶å°±æ˜¯åœ¨ä¸Šä¸‹æ–‡ä¸­å¯¹é‡è¦å•è¯ç»™äºˆå…³æ³¨ï¼Œä»¥äº§ç”Ÿæ›´å‡†ç¡®çš„è¾“å‡º**ã€‚è¿™æ˜¯é€šè¿‡åœ¨è§£ç è¿‡ç¨‹ä¸­è¯„ä¼°ä¸€ä¸ªå•è¯ä¸ç¼–ç è¿‡ç¨‹ä¸­æ‰€æœ‰å•è¯çš„ç›¸ä¼¼æ€§æ¥å®Œæˆçš„ã€‚è€ƒè™‘æˆ‘ä»¬å‰é¢æ®µè½ä¸­çš„ç¤ºä¾‹ã€‚å‡è®¾è§£ç å™¨æ¨¡å‹å·²ç»ç”Ÿæˆäº†è¾“å‡ºï¼šâ€œä¼Šåˆ©è¯ºä¼Šå·çš„é¦–éƒ½æ˜¯_______ã€‚â€å®ƒå¦‚ä½•çŸ¥é“å¦‚ä½•å¡«è¡¥è¿™ä¸ªæœ€åçš„ç©ºç™½ï¼Ÿå®ƒå°†æŸ¥çœ‹å½“å‰è§£ç çš„å•è¯ä¸ç¼–ç è¿‡ç¨‹ä¸­çš„æ‰€æœ‰å•è¯çš„ç›¸ä¼¼ç¨‹åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä¼šçœ‹åˆ°â€œcapitalâ€å’Œâ€œIllinoisâ€ä¹‹é—´çš„å¼ºç›¸å…³æ€§/ç›¸ä¼¼æ€§ï¼Œä»è€Œæ¨å¯¼å‡ºæœ€å¯èƒ½çš„ç­”æ¡ˆï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­æ˜¯â€œSpringfieldâ€ã€‚'
- en: Of course, you can imagine that LLMs need a LOT of examples in order to effectively
    produce more generalized outputs, hence why we refer to them as â€œlargeâ€ language
    models. Generally speaking, the more examples (or **parameters**) that we provide
    to an LLM during the training process, the greater chance it has at predicting
    the correct words. Of course, weâ€™re oversimplifying this concept, and there are
    certainly other nuances that go into influencing a Generative AI modelâ€™s output.
    For example, people are beginning to find effectiveness in **mixture of expert
    (MoE)** models, where â€œsmallerâ€ models are trained on a more specific domain of
    knowledge and later combined to produce more fruitful results. In fact, itâ€™s rumored
    that this is how OpenAIâ€™s popular GPT-4 works under the hood.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä½ å¯ä»¥æƒ³è±¡ï¼Œä¸ºäº†æœ‰æ•ˆåœ°äº§ç”Ÿæ›´å…·æ™®éæ€§çš„è¾“å‡ºï¼ŒLLMs éœ€è¦å¤§é‡çš„ç¤ºä¾‹ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ç§°å®ƒä»¬ä¸ºâ€œå¤§å‹â€è¯­è¨€æ¨¡å‹ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘ LLM
    æä¾›çš„ç¤ºä¾‹ï¼ˆæˆ– **å‚æ•°**ï¼‰è¶Šå¤šï¼Œå®ƒé¢„æµ‹æ­£ç¡®å•è¯çš„æœºä¼šå°±è¶Šå¤§ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬ç®€åŒ–äº†è¿™ä¸ªæ¦‚å¿µï¼Œå½±å“ç”Ÿæˆ AI æ¨¡å‹è¾“å‡ºçš„å› ç´ è¿˜æœ‰å¾ˆå¤šç»†å¾®ä¹‹å¤„ã€‚ä¾‹å¦‚ï¼Œäººä»¬å¼€å§‹å‘ç°
    **ä¸“å®¶æ··åˆï¼ˆMoEï¼‰** æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œå…¶ä¸­â€œå°å‹â€æ¨¡å‹åœ¨æ›´å…·ä½“çš„çŸ¥è¯†é¢†åŸŸè¿›è¡Œè®­ç»ƒï¼Œç„¶åç»„åˆä»¥äº§ç”Ÿæ›´æœ‰æˆæ•ˆçš„ç»“æœã€‚äº‹å®ä¸Šï¼Œæœ‰ä¼ è¨€ç§°è¿™å°±æ˜¯ OpenAI çš„çƒ­é—¨
    GPT-4 åœ¨åå°å·¥ä½œçš„æ–¹å¼ã€‚
- en: Addressing Pitfalls & Myths
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤„ç†é™·é˜±ä¸è¯¯åŒº
- en: As cool as LLMs can be, they are not perfect and do not always produce the most
    correct or relevant results. We refer to these confident but incorrect responses
    as **hallucinations**, but technically speaking, it is not fair to the LLM to
    refer to hallucinations as incorrect. Remember, as weâ€™ve explored the intuition
    behind these models throughout this post, we have a general understanding of how
    results are derived using very fancy probability methods. **There is no sentient
    reasoning going on in these models; theyâ€™re simply predicting what the probabilities
    are based on the training process.** You canâ€™t really blame the model for adhering
    to empirically derived probabilities!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ LLMs å¾ˆé…·ï¼Œä½†å®ƒä»¬å¹¶ä¸å®Œç¾ï¼Œå¹¶ä¸”ä¸æ€»æ˜¯äº§ç”Ÿæœ€æ­£ç¡®æˆ–æœ€ç›¸å…³çš„ç»“æœã€‚æˆ‘ä»¬å°†è¿™äº›è‡ªä¿¡ä½†ä¸æ­£ç¡®çš„å›ç­”ç§°ä¸º **å¹»è§‰**ï¼Œä½†ä»æŠ€æœ¯ä¸Šè®²ï¼Œå°†å¹»è§‰ç§°ä¸ºä¸æ­£ç¡®å¯¹
    LLM æ¥è¯´æ˜¯ä¸å…¬å¹³çš„ã€‚è¯·è®°ä½ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æ¢è®¨è¿™äº›æ¨¡å‹èƒŒåçš„ç›´è§‰æ—¶ï¼Œæˆ‘ä»¬å¯¹å¦‚ä½•ä½¿ç”¨éå¸¸å¤æ‚çš„æ¦‚ç‡æ–¹æ³•å¾—å‡ºç»“æœæœ‰ä¸€ä¸ªå¤§è‡´çš„äº†è§£ã€‚**è¿™äº›æ¨¡å‹ä¸­æ²¡æœ‰æ„ŸçŸ¥æ¨ç†ï¼›å®ƒä»¬åªæ˜¯æ ¹æ®è®­ç»ƒè¿‡ç¨‹é¢„æµ‹æ¦‚ç‡ã€‚**
    ä½ ä¸èƒ½çœŸæ­£è´£æ€ªæ¨¡å‹éµå¾ªç»éªŒæ¨å¯¼çš„æ¦‚ç‡ï¼
- en: This is especially apparent if you use different kinds of LLMs. You might be
    only familiar with ChatGPT, but if you play around with enough LLMs, youâ€™ll notice
    drastically different results. For example, it is rumored that ChatGPTâ€™s underlying
    model, `gpt-3.5-turbo`, was trained on 70 billion parameters. We now have models
    like Metaâ€™s Llama 2 with flavors that go as low as 7 billion parameters, one tenth
    that of `gpt-3.5-turbo`. And in practice, itâ€™s very obvious the performance difference
    between these models. (Granted, smaller models like Mistral are getting better
    and even coming close to matching that of ChatGPT!)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä½¿ç”¨ä¸åŒç§ç±»çš„ LLMï¼Œè¿™ä¸€ç‚¹å°¤å…¶æ˜æ˜¾ã€‚ä½ å¯èƒ½åªç†Ÿæ‚‰ ChatGPTï¼Œä½†å¦‚æœä½ å°è¯•è¶³å¤Ÿå¤šçš„ LLMï¼Œä½ ä¼šæ³¨æ„åˆ°ç»“æœæœ‰å¾ˆå¤§ä¸åŒã€‚ä¾‹å¦‚ï¼Œæœ‰ä¼ è¨€ç§° ChatGPT
    çš„åŸºç¡€æ¨¡å‹ `gpt-3.5-turbo` æ˜¯åœ¨ 700 äº¿ä¸ªå‚æ•°ä¸Šè®­ç»ƒçš„ã€‚ç°åœ¨æˆ‘ä»¬æœ‰åƒ Meta çš„ Llama 2 è¿™æ ·çš„æ¨¡å‹ï¼Œå‚æ•°æ•°é‡ä½è‡³ 70 äº¿ï¼Œç›¸å½“äº
    `gpt-3.5-turbo` çš„ååˆ†ä¹‹ä¸€ã€‚åœ¨å®è·µä¸­ï¼Œè¿™äº›æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚éå¸¸æ˜æ˜¾ã€‚ï¼ˆå½“ç„¶ï¼Œåƒ Mistral è¿™æ ·çš„è¾ƒå°æ¨¡å‹ä¹Ÿåœ¨ä¸æ–­æ”¹è¿›ï¼Œç”šè‡³æ¥è¿‘ ChatGPT
    çš„è¡¨ç°ï¼ï¼‰
- en: The tendency for models to hallucinate should give a business leader pause when
    applying Generative AI to business processes. In these earlier days of LLMs, itâ€™s
    perhaps more appropriate to include a â€œhuman in the loopâ€ mechanism, where a human
    has their work processes augmented with Generative AI but can ultimately â€œoverruleâ€
    an LLMâ€™s response if the human feels that the model didnâ€™t give a good result.
    Of course, these models are going to get better and better over time, so perhaps
    a business leader may relax some of these restrictions as time goes on. That will
    be up to the risk appetite of the business.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹äº§ç”Ÿå¹»è§‰çš„å€¾å‘åº”è¯¥è®©ä¼ä¸šé¢†å¯¼åœ¨å°†ç”Ÿæˆ AI åº”ç”¨äºä¸šåŠ¡æµç¨‹æ—¶ä¸‰æ€ã€‚åœ¨è¿™äº›æ—©æœŸçš„ LLM æ—¥å­é‡Œï¼Œä¹Ÿè®¸æ›´åˆé€‚çš„æ˜¯åŒ…å«ä¸€ä¸ªâ€œäººå·¥å¹²é¢„â€æœºåˆ¶ï¼Œå…¶ä¸­äººå·¥å°†ç”Ÿæˆ
    AI ç”¨äºè¾…åŠ©å·¥ä½œè¿‡ç¨‹ï¼Œä½†å¦‚æœäººå·¥è®¤ä¸ºæ¨¡å‹çš„ç»“æœä¸å¥½ï¼Œå¯ä»¥â€œæ¨ç¿»â€ LLM çš„å›ç­”ã€‚å½“ç„¶ï¼Œè¿™äº›æ¨¡å‹éšç€æ—¶é—´çš„æ¨ç§»ä¼šå˜å¾—è¶Šæ¥è¶Šå¥½ï¼Œæ‰€ä»¥ä¼ä¸šé¢†å¯¼å¯èƒ½ä¼šéšç€æ—¶é—´çš„æ¨ç§»æ”¾å®½ä¸€äº›é™åˆ¶ã€‚è¿™å°†å–å†³äºä¼ä¸šçš„é£é™©æ‰¿å—èƒ½åŠ›ã€‚
- en: Additionally, it should be noted that these large language models can exhibit
    an unfair bias, albeit this is *NOT* to say that this bias is intended. I like
    to think of LLMs as â€œzeitgeist machines.â€ If LLMs are next-word predictors based
    on advanced probabilities, **the predictions are only as good as the data it saw
    at the time of training**. So if you were to train an LLM only on a bunch of text
    talking about how nasty pizza tastes, donâ€™t be surprised when the LLM has a tendency
    to talk negatively about pizza! Likewise, the complaints from people online that
    LLMs exhibit bias in unfair ways are correct, but itâ€™s only because the â€œzeitgeistâ€
    of the training data inclined it that way. For example, it should come as no surprise
    that an LLM may be unfairly biased against a certain political candidate if the
    training data contained many articles that were critical about that candidate.
    Itâ€™s a myth to believe that this bias was intentionally baked into the model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œåº”è¯¥æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›å¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½è¡¨ç°å‡ºä¸å…¬å¹³çš„åè§ï¼Œå°½ç®¡è¿™*å¹¶é*æ„å‘³ç€è¿™ç§åè§æ˜¯æ•…æ„çš„ã€‚æˆ‘å–œæ¬¢æŠŠLLMsçœ‹ä½œæ˜¯â€œæ—¶ä»£ç²¾ç¥æœºå™¨â€ã€‚å¦‚æœLLMsæ˜¯åŸºäºé«˜çº§æ¦‚ç‡çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹å™¨ï¼Œ**é‚£ä¹ˆé¢„æµ‹çš„å‡†ç¡®æ€§åªå–å†³äºå®ƒåœ¨è®­ç»ƒæ—¶çœ‹åˆ°çš„æ•°æ®**ã€‚æ‰€ä»¥ï¼Œå¦‚æœä½ åªç”¨ä¸€å †è®¨è®ºæŠ«è¨å‘³é“ç³Ÿç³•çš„æ–‡æœ¬æ¥è®­ç»ƒä¸€ä¸ªLLMï¼Œä¸è¦æ„Ÿåˆ°æƒŠè®¶å®ƒå€¾å‘äºå¯¹æŠ«è¨å‘è¡¨è´Ÿé¢è¯„è®ºï¼åŒæ ·ï¼Œåœ¨çº¿ä¸Šå¯¹LLMsè¡¨ç°å‡ºä¸å…¬å¹³åè§çš„æŠ•è¯‰æ˜¯æ­£ç¡®çš„ï¼Œä½†è¿™åªæ˜¯å› ä¸ºè®­ç»ƒæ•°æ®çš„â€œæ—¶ä»£ç²¾ç¥â€å€¾å‘äºè¿™ç§æ–¹å¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè®­ç»ƒæ•°æ®åŒ…å«äº†å¾ˆå¤šæ‰¹è¯„æŸä¸ªæ”¿æ²»å€™é€‰äººçš„æ–‡ç« ï¼Œé‚£ä¹ˆLLMå¯¹è¯¥å€™é€‰äººè¡¨ç°å‡ºä¸å…¬å¹³çš„åè§ä¹Ÿå°±ä¸è¶³ä¸ºå¥‡äº†ã€‚ç›¸ä¿¡è¿™ç§åè§æ˜¯æ•…æ„åµŒå…¥æ¨¡å‹ä¸­çš„æ˜¯ä¸€ç§è¯¯è§£ã€‚
- en: Finally, we should address the concern of copyright infringement. Because LLMs
    measure advanced probabilities between words, it should come as no surprise when
    these LLMs are able to emulate a piece of copyrighted work. Intentionally use
    the word â€œemulateâ€ because LLMs are unable to reproduce full bodies of copyrighted
    work simply due to complex probabities. So I can have an LLM talk to me like Hagrid
    from the *Harry Potter* series or Jar Jar Binks from *Star Wars*, but it will
    struggle to fully reproduce the dialogue from those movie scripts. In other words,
    donâ€™t expect the LLM to be able to sufficiently reproduce the entire *Harry Potter*
    book series. Itâ€™s just too long, and the probabilities of these words get too
    messy.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬åº”è¯¥è®¨è®ºç‰ˆæƒä¾µçŠ¯çš„é—®é¢˜ã€‚å› ä¸ºLLMsæµ‹é‡å•è¯ä¹‹é—´çš„é«˜çº§æ¦‚ç‡ï¼Œæ‰€ä»¥è¿™äº›LLMsèƒ½å¤Ÿæ¨¡æ‹Ÿå—ç‰ˆæƒä¿æŠ¤çš„ä½œå“ä¹Ÿå°±ä¸è¶³ä¸ºå¥‡äº†ã€‚ä¹‹æ‰€ä»¥ä½¿ç”¨â€œæ¨¡æ‹Ÿâ€ä¸€è¯ï¼Œæ˜¯å› ä¸ºLLMsç”±äºå¤æ‚çš„æ¦‚ç‡é—®é¢˜æ— æ³•å®Œæ•´å†ç°å—ç‰ˆæƒä¿æŠ¤çš„ä½œå“ã€‚å› æ­¤ï¼Œæˆ‘å¯ä»¥è®©LLMåƒã€Šå“ˆåˆ©Â·æ³¢ç‰¹ã€‹ç³»åˆ—ä¸­çš„æµ·æ ¼æˆ–ã€Šæ˜Ÿçƒå¤§æˆ˜ã€‹ä¸­çš„è´¾è´¾Â·å®¾å…‹æ–¯ä¸€æ ·ä¸æˆ‘å¯¹è¯ï¼Œä½†å®ƒä¼šå¾ˆéš¾å®Œå…¨å†ç°è¿™äº›ç”µå½±å‰§æœ¬ä¸­çš„å¯¹è¯ã€‚æ¢å¥è¯è¯´ï¼Œä¸è¦æŒ‡æœ›LLMèƒ½å¤Ÿå……åˆ†å†ç°æ•´ä¸ªã€Šå“ˆåˆ©Â·æ³¢ç‰¹ã€‹ä¹¦ç³»åˆ—ã€‚è¿™å®åœ¨å¤ªé•¿äº†ï¼Œè€Œä¸”è¿™äº›å•è¯çš„æ¦‚ç‡å˜å¾—è¿‡äºå¤æ‚ã€‚
- en: Still, the copyright infringement space is a messy one, and it seems to be a
    problem more beholden to these companies training LLMs and less a problem for
    the companies simply using them. Now granted, a company making use of anotherâ€™s
    LLM should still seek to put up guardrails that would not intentionally try to
    reproduce copyrighted work, which is easy enough to do with appropriate prompt
    engineering. But companies like OpenAI are currently facing this legal predicament
    on whether or not they were legally allowed to use othersâ€™ data for training purposes
    in the first place. At the time of this articleâ€™s posting, [*The New York Time*s
    is suing OpenAI and Microsoft alleging that their news data was inappropriately
    used in the training of the model](https://fortune.com/2023/12/27/openai-microsoft-new-york-times-lawsuit-ai-copyright-infringement/).
    I am not an expert on these sorts of legal matters, but it will be important to
    watch cases like these to see how legislation may impact the evolution of LLMs
    going forward.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œç‰ˆæƒä¾µçŠ¯çš„é¢†åŸŸä¾ç„¶æ··ä¹±ï¼Œè¿™ä¼¼ä¹æ˜¯é‚£äº›è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…¬å¸é¢ä¸´çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯ä»…ä»…ä½¿ç”¨è¿™äº›æ¨¡å‹çš„å…¬å¸é¢ä¸´çš„é—®é¢˜ã€‚ç°åœ¨ï¼Œè™½ç„¶ä¸€ä¸ªå…¬å¸åœ¨ä½¿ç”¨ä»–äººçš„LLMæ—¶ä»ç„¶åº”è¯¥è®¾æ³•è®¾ç½®é˜²æŠ¤æªæ–½ï¼Œç¡®ä¿ä¸ä¼šæ•…æ„å†ç°å—ç‰ˆæƒä¿æŠ¤çš„ä½œå“ï¼Œè¿™é€šè¿‡é€‚å½“çš„æç¤ºå·¥ç¨‹æ˜¯å¯ä»¥åšåˆ°çš„ã€‚ä½†åƒOpenAIè¿™æ ·çš„å…¬å¸ç›®å‰æ­£é¢ä¸´ä¸€ä¸ªæ³•å¾‹å›°å¢ƒï¼Œå³æ˜¯å¦åœ¨æœ€åˆçš„è®­ç»ƒè¿‡ç¨‹ä¸­åˆæ³•ä½¿ç”¨äº†ä»–äººçš„æ•°æ®ã€‚æ ¹æ®è¿™ç¯‡æ–‡ç« å‘å¸ƒæ—¶çš„æƒ…å†µï¼Œ[*çº½çº¦æ—¶æŠ¥*æ­£åœ¨èµ·è¯‰OpenAIå’Œå¾®è½¯ï¼ŒæŒ‡æ§ä»–ä»¬çš„ä¸å½“ä½¿ç”¨äº†å…¶æ–°é—»æ•°æ®æ¥è®­ç»ƒæ¨¡å‹](https://fortune.com/2023/12/27/openai-microsoft-new-york-times-lawsuit-ai-copyright-infringement/)ã€‚æˆ‘ä¸æ˜¯è¿™äº›æ³•å¾‹é—®é¢˜çš„ä¸“å®¶ï¼Œä½†è§‚å¯Ÿè¿™äº›æ¡ˆä»¶å°†å¾ˆé‡è¦ï¼Œä»¥äº†è§£ç«‹æ³•å¦‚ä½•å½±å“LLMsçš„æœªæ¥å‘å±•ã€‚
- en: The Generative AI space is an extremely fascinating one, and Iâ€™m excited to
    see how the space continues to evolve in the future. We are very much still in
    the early days of this revolution, and I anticipate we will continue to see advancements
    in adoption, technological evolution, and legal understanding. I hope that this
    post provided you with enough intuition that you can make better informed decisions
    on how to approach this exciting domain with the appropriate level of caution!
    ğŸ˜ƒ
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é¢†åŸŸæ˜¯ä¸€ä¸ªæå…·å¸å¼•åŠ›çš„é¢†åŸŸï¼Œæˆ‘å¾ˆæœŸå¾…çœ‹åˆ°æœªæ¥è¿™ä¸ªé¢†åŸŸå¦‚ä½•æŒç»­å‘å±•ã€‚æˆ‘ä»¬ä»å¤„äºè¿™åœºå˜é©çš„æ—©æœŸé˜¶æ®µï¼Œæˆ‘é¢„æœŸæˆ‘ä»¬å°†ç»§ç»­çœ‹åˆ°åœ¨é‡‡çº³ã€æŠ€æœ¯æ¼”è¿›å’Œæ³•å¾‹ç†è§£æ–¹é¢çš„è¿›å±•ã€‚æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½ä¸ºä½ æä¾›è¶³å¤Ÿçš„ç›´è§‰ï¼Œä»¥ä¾¿ä½ èƒ½åœ¨ä»¥é€‚å½“çš„è°¨æ…æ€åº¦æ¥è§¦è¿™ä¸€æ¿€åŠ¨äººå¿ƒçš„é¢†åŸŸæ—¶åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ï¼
    ğŸ˜ƒ
