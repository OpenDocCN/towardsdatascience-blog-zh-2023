- en: A Guide to 21 Feature Importance Methods and Packages in Machine Learning (with
    Code)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-guide-to-21-feature-importance-methods-and-packages-in-machine-learning-with-code-85a841f8b319](https://towardsdatascience.com/a-guide-to-21-feature-importance-methods-and-packages-in-machine-learning-with-code-85a841f8b319)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From the OmniXAI, Shapash, and Dalex interpretability packages to the Boruta,
    Relief, and Random Forest feature selection algorithms
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://theomitsa.medium.com/?source=post_page-----85a841f8b319--------------------------------)[![Dr.
    Theophano Mitsa](../Images/a39dfae5f4409120b840cd9182b148c6.png)](https://theomitsa.medium.com/?source=post_page-----85a841f8b319--------------------------------)[](https://towardsdatascience.com/?source=post_page-----85a841f8b319--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----85a841f8b319--------------------------------)
    [Dr. Theophano Mitsa](https://theomitsa.medium.com/?source=post_page-----85a841f8b319--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----85a841f8b319--------------------------------)
    ·19 min read·Dec 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a932b508bfd0db9d0a7361334718dfc4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Image created by the author at DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '**“We are our choices.” —Jean-Paul Sartre**'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We live in the era of artificial intelligence, mostly because of the incredible
    advancement of Large Language Models (LLMs). As important as it is for an ML engineer
    to learn about these new technologies, equally important is his/her ability to
    master the fundamental concepts of model selection, optimization, and deployment.
    Something else is very important: the input to the above, which consists of the
    *data features.* Data, like people, have characteristics called *features*. In
    the case of people, you must understand their unique characteristics to bring
    out the best in them. Well, the same principle applies to data. Specifically,
    this article is about *feature importance*, which measures the contribution of
    a feature to the predictive ability of a model. We have to understand feature
    importance for many essential reasons:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Time: Having too many features slows down the training model time and also
    model deployment. The latter is particularly important in edge applications (mobile,
    sensors, medical diagnostics).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting. If our features are not carefully selected, we might make our model
    overfit, i.e., learn about noise, too.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Curse of dimensionality. Many features mean many dimensions, and that makes
    data analysis exponentially more difficult. For example, *k-NN classification,*
    a widely used algorithm, is greatly affected by dimension increase.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptability and transfer learning. This is my favorite reason and *actually
    the reason for writing this article.* In *transfer learning*, a model trained
    in one task can be used in a second task with some finetuning. Having a good understanding
    of your features in the first and second tasks can greatly reduce the fine-tuning
    you need to do.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will focus on tabular data and discuss twenty-one ways to assess feature
    importance. One might wonder: ‘Why twenty-one techniques? Isn’t one enough?’ It
    is important to discuss all twenty-one techniques because each one has unique
    characteristics that are very worthwhile learning about. Specifically, there are
    two ways I will indicate in the article why a particular technique is worthwhile
    learning about (a) Sections titled: “**Why this is important**” and (b) Highlighting
    the word **unique**, to indicate that I am talking about a special and unique
    characteristic.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'The techniques we will discuss come from two distinct areas of machine learning:
    *interpretability* and *feature selection*. Specifically, we will discuss the
    following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '**Interpretability Python packages.** These libraries help to make a model’s
    decision-making process more transparent by providing insights into how input
    features affect the model’s predictions. We will discuss the following: *OmniXAI,**Shapash*,
    *DALEX*, *InterpretML,* and *ELI5.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature selection methods.** Thesemethods focus on reducing the model’s features
    by identifying the most informative features, and they generally fall into the
    filter, embedded, and wrapper categories. The characteristics of each category
    will be discussed in the next section. From each category, we will discuss the
    following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Wrapper methods: Recursive Feature Elimination, Sequential Feature Selection,
    *Boruta* algorithm.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Embedded methods: Logistic Regression, *RandomForest*, *LightGBM*, *CatBoost*,
    *XGBoost, SelectFromModel*'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Filter methods: Mutual information, *MRMR* algorithm, *SelectKBest, Relief
    algorithm.*'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other: *Featurewiz* package, *Selective* package, *PyImpetus* package.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate the above feature-importance-computation techniques, we will
    use tabular data related to heart failure prediction from *Kaggle*: [https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset has 918 rows and 12 columns corresponding to the following features:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: ‘Age,’ ‘Sex’(M, F), ‘ChestPainType’ (TA, ATA, NAP, ASY), ‘RestingBP,’
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘Cholesterol’, ‘FastingBS’[(0,1), ‘RestingECG’(Normal,ST, LVH), ‘MaxHR’,
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘ExerciseAngina’(Y,N), ‘Oldpeak’, ‘ST_Slope’ (Up, Flat, Down),
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘HeartDisease’(0,1). This is the target variable. 0 indicates the absence of
    heart disease, and 1 indicates the presence.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The dataset has no missing values, and the target variable is relatively balanced
    with 410 ‘0‘ ’instances and 508 ‘1’ instances. Five of the features are categorical:
    ‘*S*ex’, ‘ChestPainType,’ ‘ExerciseAngina,’ ‘RestingECG,’ ‘ST_Slope.’ These features
    are encoded with the *one-hot-encoding Pandas* method:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Then, the data is split into training and test sets. Finally, *scikit-learn’*s
    *StandardScaler* is applied to the numerical data of the train and test data sets.
    Now, we are ready to proceed to feature importance assessment.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Importance Assessment**'
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A. Interpretability Packages
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Why they are important**'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In recent years, the interpretability of machine learning algorithms has attracted
    significant attention. Machine learning algorithms have recently found use in
    many areas, such as finance, medicine, environmental modeling, etc. This broad
    use of ML algorithms by people who are not necessarily ML experts begs for more
    transparency because:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '**Trust issues.** Black boxes make people nervous and unsure as to whether
    they should trust them.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regulatory and ethical concerns**. Governments around the world are increasingly
    concerned about AI use and passing legislation to ensure that AI systems make
    their decisions in a fair way without any biases. Understanding how ML systems
    work under the hood is an important prerequisite to fair and unbiased AI.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretability packages are based on the model-independent interpretation
    frameworks *SHAP* (SHapley Additive exPlanations) and *LIME* (Local Interpretable
    Model-agnostic Explanations) [2]. *SHAP* uses a game theory approach and provides
    global explanations. On the other hand, *LIME* provides local explanations. They
    offer transparency through the idea of “explainers.” An explainer is a wrapper-type
    of object; it can wrap around a model and provide a door to the internal intricacies
    of the model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '**A.1 Shapash Package**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Shapash* [1] is such an interpretability package. Below, we see the implementation
    of *Shapash’s* ‘SmartExplainer’ that takes as input an ML model of type *RandomForestClassifier,*
    and the feature names. Then, *Shapash’*s ‘compile’ function is invoked, which
    is the ‘workhorse’ of the whole process: (a) it binds the model to the data, (b)
    computes the feature importances, and (c ) prepares the data for visualization.
    Finally, we invoke the interactive web application.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1, from *Shapash’s* application interface, shows the feature importances.
    The length of the horizontal bar corresponds to the importance of the feature.
    Thus, ‘ST_Slope_up’ is the most important feature, and ‘chest_pain_typeTA’ is
    the least important.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7be64f60f3fe7c2a0cdc7260ea16b685.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Feature importances
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2 shows an important type of plot provided by *Shapash*, the feature
    contribution. The feature examined in the plot is ‘ST_Slope_up’. The upper part
    contains cases where the contribution of *ST_Slope_up* is positive, whereas the
    bottom part contains cases where the contribution of ‘ST_Slope_up’ is negative.
    Also, the upper graph part corresponds to cases where ‘ST_Slope_up’ is 0, and
    the bottom corresponds to cases where ‘ST_Slope_up’ is 1\. When we click on one
    of the circles in the middle of the displayed structures, the following information
    is shown: the case number, the ‘ST_Slope_up’ value, predicted class, and contribution
    of ‘ST_Slope_up’.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd7884a7dc19f08a7dcf9521c150798a.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Feature contributions
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3 shows the local explanations for slice 131, where the predicted class
    is 1 with probability of 0.8416\. Bars to the right show a positive contribution,
    and bars to the left show a negative contribution to the result. ‘St_Slope_up’
    has the highest positive contribution, while ‘max_heart_rate’ has the highest
    negative contribution.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/277ff773bcf0827c0a6147ab525ae2af.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Local explanations
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: In summary, *Shapash* is a very useful package to know because (a) it offers
    a great interface where the user can gain a deep understanding of global and local
    explanations, (b) it offers the ***unique*** feature of displaying feature contributions
    across cases
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '**A.2\. OMNIXAI Package**'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*OMNIXAI* [(Open-source eXplainable AI) [3], like *Shapash*, also offers visualization
    tools, but its ***unique*** *strength* lies in the significant breadth of its
    explanation techniques. Specifically, it offers methods to explain predictions
    for various data types, i.e., tabular data, text, and images. Some of its **unique**
    features are (a) the *NLPExplainer*, (b) the bias examination module, (c) Morris
    sensitivity analysis for tabular data, (d) the *VisionExplainer* for image classification,
    and (e) counterfactual Explainers.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: The code below shows the creation of an *OMNIXAI* explainer. The essential steps
    are (a) the creation of an *OMNIXAI*-specific data type (‘Tabular’) to hold the
    data, (b) Data pre-processing through the ‘TabularTransform,’ (c) data splitting
    into training and test sets, (d) training of an *XGBClassifier* model (e) data
    inversion back to their original format (f) setting up a ‘TabularExplainer’ of
    the *XGBClassifier* with both *SHAP* and *LIME* methods. The explanation will
    be applied to ‘test_instances’ [130–135] (g) generation and display of the predictions
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4 shows the aggregate local explanation for slices between [130:135]
    using *LIME.* The green bars in the right part show a positive contribution to
    label class 1, whereas the red bars in the left part show negative contributions
    to class 1\. The longer the bar, the more significant the contribution.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f6867d4eaf00dfaeef7379de15329e05.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. LIME explanations
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5 shows the aggregate local explanations for slices [130:135] using *SHAP.*
    The meaning of the green/red bars is the same as in the above graph.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7920f54bf0553ac38ee1a2ae538218c1.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. SHAP explanations
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**A.3\. The InterpetML Package**'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *InterpretML* XAI interpretability [4] package has the **unique** feature
    of ‘glassbox models,’ which are inherently explainable models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of such an inherently explainable model, the ‘ExplainableBoostingClassifier,’
    is shown in the code snippet below. Global explanations and local explanations
    at slice 43 are also set up.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6 shows the computed global feature importances.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13248f6520677522a9f7f5f8a640dd68.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Global feature importances
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7 shows the computed local explanations for slice at 43\. Most features
    contribute positively to the prediction of class 1, while only ‘Cholesterol’ and
    ‘FastingBS’ contribute negatively.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/933d06bef30b259e0413d46a10cbb1de.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. Local explanations
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '**A. 4 The Dalex Package**'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *Dalex* package[5] is a library designed to explain and understand machine
    learning models. *Dalex* stands for “Descriptive mAchine Learning EXplanations.”
    It has the following **unique** characteristics:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: It is compatible with both R and Python.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Aspects* module. This allows us to explain a model taking into account
    feature inter-dependencies.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Fairness* module. It allows us to evaluate the fairness of a model.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code snippet below shows the implementation of *Dalex*’s ‘Explainer.’
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The feature importances produced by *Dalex* are shown below in Figure 8.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c854044ebe3bb57db43996d5df846af.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. Feature importances
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: A.5 The Eli5 package
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The final interpretability package we will discuss is Eli5 [5]. It has the
    following **unique** features:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: The permutation importance measure. In this technique, the values of each feature
    are randomly shuffled, and then the resulting drop in model performance is measured.
    The bigger the drop, the more important the feature.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It works with text data. Specifically, it provides a ‘TextExplainer’ that can
    explain predictions of text classifiers.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is compatible with Keras.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the code snippet below, the ‘PermutationImportance’ method is applied to
    the *Support Vector Classification* (‘svc’) estimator.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9 shows the computed feature importances for the ‘svc’ estimator.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8967ebf6a84b99cab25d17116cecc8d2.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: Figure 9.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '**B. Feature Selection Techniques**'
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wrapper Methods
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the name suggests, these algorithms wrap the feature selection process around
    a machine learning algorithm. They continuously evaluate subsets of features until
    they find the subset that yields the best performance according to a criterion.
    This criterion can be one of the following: model accuracy, number of subset features,
    information gain, etc.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '**Why they are important**'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The very nature of these algorithms (criterion optimization, comprehensive search)
    suggests that these methods can have very good performance in terms of selecting
    the best features. Another very useful characteristic of them is that they consider
    feature interactions. However, again, their very nature suggests that they can
    be computationally intensive and might overfit. So, if you do not have computational
    limitations and accuracy is essential, these are a good choice.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: B.1\. Sequential Feature Selection
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Sequential Feature Selection* (*SFS*) evaluates feature subsets in two modes:
    forward selection, which starts with no features and adds them iteratively, and
    backward elimination, which starts with all features and removes them one by one.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: The code snippet below shows the implementation of SFS wrapped around a ‘KNeighborsClassifier’
    model. It also shows how to output the selected features and their names.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'The selected features are:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '**B.2 The Boruta Algorithm**'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Boruta* is one of the most effective feature selection algorithms. Most impressively,
    it does not require any input from the user [7]! It is based on the brilliant
    idea of ‘shadow features’ (randomized duplicates of all original features). Then,
    a random forest classifier is applied to assess the importance of each real feature
    against these shadow features. The process is repeated until all important features
    are identified.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: The snippet below shows the implementation of *Boruta* using the *BorutaPy*
    package and the selected features.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'The selected features from *Boruta* are:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '**B.3 The RFECV Algorithm**'
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*RFECV* (Recursive Feature Elimination with Cross-Validation) is a feature
    selection technique that iteratively removes the least important features from
    a model, using cross-validation to find the best subset of features. The code
    implementation is shown in the snippet below.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'The selected features are:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Embedded Methods
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These refer to algorithms that have the built-in ability to compute feature
    importances or select features, such as *Random Forest* and *lasso* regression,
    respectively. An important note for these methods is that they *do not directly*
    select features. Instead, they compute feature importances, which can be used
    in a post hoc process to choose features. Such a post hoc process is ‘SelectFromModel’
    discussed in section B.9.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '**Why they are important**'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: High-dimensional data are very common today in the form of unstructured text,
    images, and time series, especially in the fields of bioinformatics, environment
    monitoring, and finance. The greatest advantage of embedded methods is their ability
    to handle high-dimensional data. The reason for this ability is that they do not
    have separate modeling and feature selection steps. Feature selection and modeling
    are combined in one single step, which leads to a significant speed-up.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '**B.4 Logistic Regression**'
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic Regression is a statistical method used for binary classification.
    The coefficients of the model relate to the importance of features. Each weight
    indicates the direction (positive or negative) and the strength of feature’s effect
    on the log odds of the target variable. A larger absolute value of a weight indicates
    that the corresponding feature is more important in predicting the outcome. The
    code snippet below shows the creation of the logistic regression. The hyper-parameters
    ‘C’ (regularization strength) and ‘max_iter’ are learned by applying s*cikit-learn’*s
    ‘GridSearchCV.’
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The logistic regression coefficients are shown below.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '**B.5 Random Forest**'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Random Forest* is an ensemble machine learning method used for classification
    and regression. It works by building many decision trees and merging their results.
    It uses the *bagging* technique where sampling-with-replacement is applied to
    the dataset. Then, each sample is used to train a separate decision tree. A significant
    feature of *Random Forest* is its ability to compute feature importances during
    the training process. It does this by randomizing a feature (while keeping all
    other features constant) and then checking how much the error increased. The most
    common criterion for computing feature importance is the mean *decrease in impurity*
    (*MDI*) when a feature is used to split a node [8]. The code snippet below shows
    the computation of the *scikit-learn* ‘RandomForestClassifier,’ where the hyperparameters
    have been determined as above using *scikit-learn’*s ‘GridSearchCV.’'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: The code for the computation and display of feature importances is shown below.
    The computed feature importances are shown in Figure 10.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/098f6bdca1250e7e317452c8206ad32b.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: Figure 10\. Feature importances
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: B.6 The LightGBM algorithm
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*LightGBM* (Light Gradient Boosting Machine) is a gradient-boosting algorithm
    that combines speed and performance. Developed by *Microsoft*, it is known for
    handling large datasets and for its efficiency in terms of memory and speed. Some
    of its**unique** features are (a) its ability to filter out data instances with
    small gradients and focus on more critical instances, (b) ‘Exclusive Feature Bundling’(EFB):
    *LightGBM* reduces the number of features by bundling mutually exclusive features
    (those that very infrequently are non-zero at the same time). In this way, the
    algorithm increases the efficiency of high-dimensional data [9].'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: The snippet below shows the implementation of *LightGBM*. The hyperparameters
    (‘learning rate,’ ‘max_depth,’ and ‘n_estimators’) were chosen using *scikit-learn’s
    ‘*GridSearchCV’ algorithm. The feature importances computed from LightGBM are
    shown in Figure 11.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cebbb08c8a20062e142d065df31bdf5a.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: Figure 11.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '**B.7 The XGBoost Algorithm**'
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*XGBoost*, which stands for *eXtreme Gradient Boosting,* is an advanced implementation
    of gradient boosting. It has the following **unique** characteristics:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: It can effectively use all available CPU cores or clusters to create the tree
    in parallel. It also utilizes cache optimization.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to *LightGBM,* *XGBoost* grows trees depth-wise (level-wise), while
    *LightGBM* grows trees leaf-wise. This makes *XGBoost* less efficient with large
    datasets.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code snippet shows the implementation of *XGBoost,* where the hyperparameters
    [10] shown below were chosen based on *Bayesian optimization* implemented in the
    ‘hyperopt’ package. These hyperparameters are:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: ‘gamma’ (min loss reduction for a split),
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘min_child_weight’ (min required sum of weights of all observations in a child)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘max_depth’ (max tree depth)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘reg_lambda’ (L2 regularization handle)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the hyperparameter ‘reg_alpha,’ which controls L1 regularization, was
    set manually after experimentation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12 shows the feature importances. Note that some importances are set
    to zero because of L1 regularization.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/beed86916ac7f6258e241119de1ac6ba.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: Figure 12\. Feature importances
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '**B.8 The CatBoost Algorithm**'
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*CatBoost* [11] is a high-performance, open-source gradient boosting library,
    particularly well-suited for categorical data. Specifically, it does not require
    any pre-processing of categorical variables, such as label-encoding or one-hot-encoding.
    Instead, it handles categorical variables natively. *CatBoost* employs symmetric
    trees as its base predictors and supports GPU acceleration. Regarding *CatBoost*
    implementation in Python, it is important to note that all non-numeric features
    must be declared as type ‘category.’ Then, as shown in the snippet below, the
    categorical features are provided as input to the model’s *fit* function.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13 shows the feature importance computed by *CatBoost*. It is important
    to note that the names of the features are the ones in the original data set (not
    the one-hot-encoded). Because *CatBoost* handles categorical data natively, the
    input to the *CatBoost* algorithm was the original data (not one-hot-encoded).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1dadde2ee51deb824a3555bee64da701.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: Figure 13\. Feature importances
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '**B.9 The SelectFromModel Method**'
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ‘SelectFromModel,’ is offered by *scikit-learn’s feature.selection* package.
    Its **unique** characteristic is that it is a *meta-transformer* that can be used
    with models that assign importances to features, either through *coef_* or *feature_importances_*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the previous embedded methods we discussed, which just computed
    feature importances, ‘SelectFromModel’ actually *selects* features. The snippet
    below shows the code for feature selection using this method.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'The selected features are:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Filter Feature Selection Methods
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These are independent of any machine learning model. They typically rely on
    statistical measures to evaluate each feature, such as correlation and mutual
    information between the target and predictor variables.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Why they are important
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Filter methods are straightforward and very easy to compute and, therefore,
    are used as an initial feature selection step in many fields with large amounts
    of data, such as bioinformatics[12], environmental studies, and healthcare research[13].
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: B. 10 Mutual information
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *mutual information* measures the reduction in uncertainty (entropy) in
    one variable, given knowledge of the other. The mutual information between the
    predictors and the target variable is computed using *scikit-learn’*s *mutual_info_classif*.
    The mutual information score of each predictor is shown in Figure 14.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7be314d63145b1f480ac8bee95981c66.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Figure 14\. Mutual information scores.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: B.11 The MRMR Algorithm
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*MRMR* stands for *M*axium-*R*elevancy-*M*aximum-*R*edundancy. As the name
    indicates, the *MRMR* algorithm selects features that are (a)Maximally relevant,
    i.e., strongly correlated with the target variable, (b) Minimally redundant, i.e.,
    exhibit high dissimilarity among them. Redundancy can be computed using correlation
    or mutual information measures, and relevance can be calculated using the F-statistic
    or mutual information[15]. *MRMR* is a *minimal-optimal* method because it selects
    a group of features that, together, have maximum predictive power [14]. This is
    in contrast to the *Boruta* algorithm, discussed in section B.2, which is an *all-relevant*
    algorithm because it identifies all features relevant to the model’s prediction.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: The code snippet below shows the implementation of *MRMR* with the ‘mrmr’ Python
    library.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'The minimal-optimal set of selected features is shown below:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: B.12 The SelectKBest Method
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the name suggests, this algorithm selects the K best features according
    to a user-defined score. The number K is also user-defined. The algorithm can
    be applied to both classification and regression tasks, and it offers a variety
    of scoring functions. For example, for classification, the user can apply the
    following: (a) ‘f_classif,’ which computes the *ANOVA* *F-value*, (b) ‘mutual_info_classif,’
    which computes mutual information, and (c) *chi2*, which computes chi-squared
    statistics, between the predictors and the target variable [16]. The code snippet
    below shows the computation of *SelectKBest* for k=5 and score function ‘f_classif’.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15 below shows the scores (importances) of the features according to
    the scoring function ‘f_classif.’ Note that although we chose K=5, Figure 15 displays
    the scores for all features.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/988ae397915ce03c7a0f5c02a9d31eb3.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: Figure 15\. Feature importances.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: B.13 The Relief Algorithm
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Relief* ‘s **unique** characteristic is the following idea: For a data sample,
    find its closest neighbor in the same class (‘near hit’) and the closest neighbor
    in the other class (‘near miss’). Features are weighted according to how well
    they are similar to the ‘near hit’ and how well they differentiate from the ‘near
    miss’ sample. *Relief* is particularly useful in biomedical informatics because
    of its sensitivity to complex feature associations [17]. Here, we used an extension
    of the original *Relief* algorithm, the *ReliefF* algorithm, which can be applied
    to multi-class classifications. In contrast, the original *Relief* algorithm can
    only be applied to binary classification cases. The snippet below shows the invocation
    of the ‘ReliefFselector’ from the ‘kydavra’ Python package.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The selected features from the algorithm are shown below.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Misc Feature Selection Techniques
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this final category, we will discuss the *Featurewiz*, *Selective*, and *PyImpetus*
    packages.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Why they are important
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each package is important for its **unique** reasons: (a) *Featurewiz* is a
    very convenient AutoML package. It selects features with one line of code; (b)
    The *Selective* package offers a wide variety of filter and embedded filter selection
    methods that can be easily invoked with one line of code; (c) The *PyImpetus*
    package is based on an algorithm that is very different from all other feature
    selection techniques, the *Markov Blanket.*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: B.14 The Featurewiz Package
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is an automated feature selection tool [18][19]. Its invocation is as simple
    as shown in the code snippet below. Under the hood, it uses the ‘SULOV’ algorithm
    (*S*earching for *U*ncorrelated *L*ist Of *V*ariables), whose basis is the *MRMR*
    algorithm described above in section B.11\. ‘SULOV’ selects the features with
    the highest mutual information score and the smallest correlation among them.
    Then, the features are passed recursively through *XGBoost* to find the best subset.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: The features selected from *Featurewiz* are shown below.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: B.15\. The Selective Feature Selection Library
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This library provides numerous feature selection methods for classification
    and regression tasks [20]. Some of the methods offered are: correlation, variance,
    statistical analysis (ANOVA f-test classification, chi-square, etc.), linear methods
    (linear regression, *lasso*, and *ridge* regularization, etc.), and tree-based
    methods (*Random Forest*, *XGBoost*, etc.). An example of this library’s usage
    is shown below.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'The selected features using the ‘TreeBased’ method are:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: B.16\. The PyImpetus Package
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **unique** idea of this algorithm is the *Markov Blanket*, which is the
    minimal feature set needed to predict the target variable[21][22]. It can be used
    for both classification and regression tasks. Its implementation for classification
    is shown below.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16 shows the selected features and their relative importance.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66328477583284f4106cda46e9d5befd.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: Figure 16.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '**Discussion and Conclusion**'
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we discussed a broad spectrum of feature importance assessment
    techniques from two distinct realms: interpretability and feature selection. Given
    the diversity of the discussed algorithms, a question that arises naturally is:”
    How similar are the features selected as most important by the various algorithms?”'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Let us take a look at Table 1 below. This table has two columns corresponding
    to the features ‘ST_Slope_up’ and ‘ST_Slope_flat.’ The rows correspond to the
    algorithms and packages we used in the article. The numbers 1,2,3 indicate whether
    the feature was selected as the best, second best, or third best by the algorithm.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Table 1.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the article, some algorithms simply output a set of features
    without any order. In this case, the X in the table indicates that the algorithm
    *selected* the feature. In the case of a gap in the table, the feature was not
    in the three best features selected by the corresponding algorithm. For logistic
    regression, the absolute value of the coefficients was considered. For *CatBoost*,
    we assigned a 1 value to both ‘ST_Slope_up’ and ‘ST_Slop_flat’ because *CatBoost*
    selected ‘ST_Slope’ as the most important feature. Finally, the *OMNIXAI* package
    results were not included because they provided local explanations for a few rows.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: An interesting fact emerges from the observation of Table 1\. Except for *LightGBM*,
    the feature ‘ST_Slope_up’ had the *highest* or *second highest* importance inthealgorithms
    that report feature importances. It was also selected by most algorithms that
    reported selected features without importances. The feature ‘ST_Slope_Flat’ also
    performed pretty well because it was either in the first three highest-importance
    features or in the selected feature groups for most algorithms.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us delve into another interesting insight. These two features had the
    *highest* and *second-highest* *mutual information scores.* As we saw in section
    B.10, this is a straightforward feature computed in one line of code. *So, with
    one line of code, we gained insights into the most important features of our data,
    as reported by the other significantly more computationally complex algorithms.*
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: This article discussed twenty-one packages and methods that compute *feature
    importance*, a measure of a feature’s contribution to a model’s predictive ability.
    For further reading, I recommend [23], which discusses another role of features,
    specifically their error contribution to a model.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: The entire code can be found at [https://github.com/theomitsa/Feature_importance/tree/main](https://github.com/theomitsa/Feature_importance/tree/main).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '**Footnotes:**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset license:** Open Data Commons Open Database License (ODbL) v1.0 as
    mentioned in [https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)
    and described in [https://opendatacommons.org/licenses/odbl/1-0/](https://opendatacommons.org/licenses/odbl/1-0/)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I certify that all visualizations/images/graphs in the article are created by
    the author and can be reproduced in the github directory above.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**References**'
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**The Shapash package,**[https://shapash.readthedocs.io/en/latest/](https://shapash.readthedocs.io/en/latest/)'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Molnar, C., **Interpretable Machine Learning,** 2023\. [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The OMNIXAI package,** [https://opensource.salesforce.com/OmniXAI/latest/omnixai.html](https://opensource.salesforce.com/OmniXAI/latest/omnixai.html)'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The InterpretML package**, [https://interpret.ml/](https://interpret.ml/)'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The Dalex package**, [https://dalex.drwhy.ai/](https://dalex.drwhy.ai/)'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The Eli5 package,** [https://eli5.readthedocs.io/en/latest/index.html](https://eli5.readthedocs.io/en/latest/index.html)'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mazzanti, S., **Boruta** **Explained Exactly How You Wished Someone Explained
    to You**, Medium: Towards Data Science, March 2020.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scornet E., **Trees, Forests, and Impurity-Based Variable Importance,** 2021,
    ffhal-02436169v3f, [https://hal.science/hal-02436169v3/file/importance_variable.pdf](https://hal.science/hal-02436169v3/file/importance_variable.pdf)
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ke, G. et al., **LightGBM: A Highly Efficient Gradient Boosting Decision Tree,**
    NIPS Conference, pp. 3149–3157, December 2017.'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Banerjee, P., **A Guide on XGBoost Hyperparameters Tuning**, [https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning](https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prokhorenkova, L. et al., **CatBoost: Unbiased Boosting With Categorical Features,**
    NIPS’18: Proceedings of the 32nd International Conference on Neural Information
    Processing Systems, pp. 6639–6649, December 2018.'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Urbanowicz, R.J. et al., **Benchmarking Relief-Based Feature Selection Methods
    for Bioinformatics** Data Mining, Journal of Biomedical Informatics, vol.85, pp.168–188,
    Sept. 2018.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Raju, S.K., **Evaluation of Mutual Information and Feature Selection for SARS-CoV-2
    Respiratory Infection,** Bioengineering (Basel), vol. 10, no. 7, July 2023.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mazzanti, S., “**MRMR” Explained Exactly How You Wished Someone Explained to
    You**, Medium: Towards Data Science, February 2021.'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Radovic, M. et al., **Minimum Redundancy Maximum Relevance Feature Selection
    Approach for Temporal Gene Expression Data,** BMC Bioinformatics, January 2017.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kavya, D., **Optimizing Performance: SelectKBest for Efficient Feature Selection
    in Machine Learning**, Medium, February 2023.'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Urbanowicz, R. J. et al., **Relief-Based Feature Selection: Introduction And
    Review,** Journal of Biomedical Informatics, vol. 85, pp. 189–203, Sept. 2018.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Urbanowicz, R. J. 等，**基于Relief的特征选择：介绍与综述，** 《生物医学信息学杂志》，第85卷，第189–203页，2018年9月。
- en: '**The Featurewiz package,** [https://github.com/AutoViML/featurewiz](https://github.com/AutoViML/featurewiz)'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Featurewiz包，** [https://github.com/AutoViML/featurewiz](https://github.com/AutoViML/featurewiz)'
- en: 'Sharma, H., **Featurewiz: Fast Way to Select the Best Features in Data,** Medium,
    Medium: Towards Data Science, Dec.2020.'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Sharma, H., **Featurewiz：快速选择数据中最佳特征的方法，** Medium，Medium: Towards Data Science，2020年12月。'
- en: '**The Selective Feature Selection Library,** [https://github.com/fidelity/selective](https://github.com/fidelity/selective)'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择性特征选择库，** [https://github.com/fidelity/selective](https://github.com/fidelity/selective)'
- en: '**The PyImpetus package**, [https://github.com/atif-hassan/PyImpetus](https://github.com/atif-hassan/PyImpetus)'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**PyImpetus包，** [https://github.com/atif-hassan/PyImpetus](https://github.com/atif-hassan/PyImpetus)'
- en: 'Hassan, A. et al., **PPFS: Predictive Permutation Feature Selection,** [https://arxiv.org/pdf/2110.10713.pdf](https://arxiv.org/pdf/2110.10713.pdf)'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hassan, A. 等，**PPFS：预测置换特征选择，** [https://arxiv.org/pdf/2110.10713.pdf](https://arxiv.org/pdf/2110.10713.pdf)
- en: 'Mazzanti, S., **Your Features Are Important? It Doesn’t Mean They Are Good,**
    Medium: Towards Data Science, August 2023.'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Mazzanti, S.，**你的特征重要吗？这并不意味着它们好，** Medium: Towards Data Science，2023年8月。'
