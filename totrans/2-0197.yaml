- en: A Guide to 21 Feature Importance Methods and Packages in Machine Learning (with
    Code)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的21种特征重要性方法和包指南（附代码）
- en: 原文：[https://towardsdatascience.com/a-guide-to-21-feature-importance-methods-and-packages-in-machine-learning-with-code-85a841f8b319](https://towardsdatascience.com/a-guide-to-21-feature-importance-methods-and-packages-in-machine-learning-with-code-85a841f8b319)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-guide-to-21-feature-importance-methods-and-packages-in-machine-learning-with-code-85a841f8b319](https://towardsdatascience.com/a-guide-to-21-feature-importance-methods-and-packages-in-machine-learning-with-code-85a841f8b319)
- en: From the OmniXAI, Shapash, and Dalex interpretability packages to the Boruta,
    Relief, and Random Forest feature selection algorithms
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从OmniXAI、Shapash和Dalex解释性包到Boruta、Relief和随机森林特征选择算法
- en: '[](https://theomitsa.medium.com/?source=post_page-----85a841f8b319--------------------------------)[![Dr.
    Theophano Mitsa](../Images/a39dfae5f4409120b840cd9182b148c6.png)](https://theomitsa.medium.com/?source=post_page-----85a841f8b319--------------------------------)[](https://towardsdatascience.com/?source=post_page-----85a841f8b319--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----85a841f8b319--------------------------------)
    [Dr. Theophano Mitsa](https://theomitsa.medium.com/?source=post_page-----85a841f8b319--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://theomitsa.medium.com/?source=post_page-----85a841f8b319--------------------------------)[![Theophano
    Mitsa博士](../Images/a39dfae5f4409120b840cd9182b148c6.png)](https://theomitsa.medium.com/?source=post_page-----85a841f8b319--------------------------------)[](https://towardsdatascience.com/?source=post_page-----85a841f8b319--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----85a841f8b319--------------------------------)
    [Theophano Mitsa博士](https://theomitsa.medium.com/?source=post_page-----85a841f8b319--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----85a841f8b319--------------------------------)
    ·19 min read·Dec 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----85a841f8b319--------------------------------)
    ·阅读时间19分钟·2023年12月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a932b508bfd0db9d0a7361334718dfc4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a932b508bfd0db9d0a7361334718dfc4.png)'
- en: Image created by the author at DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者在DALL-E上创建
- en: '**“We are our choices.” —Jean-Paul Sartre**'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**“我们就是我们的选择。” —让-保罗·萨特**'
- en: 'We live in the era of artificial intelligence, mostly because of the incredible
    advancement of Large Language Models (LLMs). As important as it is for an ML engineer
    to learn about these new technologies, equally important is his/her ability to
    master the fundamental concepts of model selection, optimization, and deployment.
    Something else is very important: the input to the above, which consists of the
    *data features.* Data, like people, have characteristics called *features*. In
    the case of people, you must understand their unique characteristics to bring
    out the best in them. Well, the same principle applies to data. Specifically,
    this article is about *feature importance*, which measures the contribution of
    a feature to the predictive ability of a model. We have to understand feature
    importance for many essential reasons:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生活在人工智能的时代，主要是由于大规模语言模型（LLMs）的惊人进步。对于机器学习工程师来说，学习这些新技术固然重要，但同样重要的是掌握模型选择、优化和部署的基本概念。还有一件事非常重要：上述的输入，即*数据特征*。数据，就像人一样，有称为*特征*的属性。对于人，你必须理解他们的独特特征才能发挥他们的最佳能力。同样的原则也适用于数据。具体来说，本文关于*特征重要性*，衡量一个特征对模型预测能力的贡献。我们必须理解特征重要性，有很多重要原因：
- en: 'Time: Having too many features slows down the training model time and also
    model deployment. The latter is particularly important in edge applications (mobile,
    sensors, medical diagnostics).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间：特征过多会拖慢训练模型的时间，也会影响模型的部署。后者在边缘应用（如移动设备、传感器、医疗诊断）中尤为重要。
- en: Overfitting. If our features are not carefully selected, we might make our model
    overfit, i.e., learn about noise, too.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合。如果我们的特征没有被仔细选择，我们可能会让模型过拟合，即也学习噪声。
- en: Curse of dimensionality. Many features mean many dimensions, and that makes
    data analysis exponentially more difficult. For example, *k-NN classification,*
    a widely used algorithm, is greatly affected by dimension increase.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度诅咒。许多特征意味着许多维度，这使得数据分析变得指数级困难。例如，*k-NN分类*，一种广泛使用的算法，受到维度增加的重大影响。
- en: Adaptability and transfer learning. This is my favorite reason and *actually
    the reason for writing this article.* In *transfer learning*, a model trained
    in one task can be used in a second task with some finetuning. Having a good understanding
    of your features in the first and second tasks can greatly reduce the fine-tuning
    you need to do.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适应性和迁移学习。这是我最喜欢的原因，也是*写这篇文章的真正原因*。在*迁移学习*中，经过一个任务训练的模型可以在经过一些微调后用于第二个任务。对第一和第二个任务中的特征有很好的理解，可以大大减少你需要进行的微调。
- en: 'We will focus on tabular data and discuss twenty-one ways to assess feature
    importance. One might wonder: ‘Why twenty-one techniques? Isn’t one enough?’ It
    is important to discuss all twenty-one techniques because each one has unique
    characteristics that are very worthwhile learning about. Specifically, there are
    two ways I will indicate in the article why a particular technique is worthwhile
    learning about (a) Sections titled: “**Why this is important**” and (b) Highlighting
    the word **unique**, to indicate that I am talking about a special and unique
    characteristic.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点讨论表格数据，并介绍二十一种评估特征重要性的方法。有人可能会问：‘为什么是二十一种技术？难道一种就够了吗？’ 讨论所有二十一种技术很重要，因为每一种都有独特的特点，非常值得学习。具体而言，我将在文章中以以下两种方式指明某一技术值得学习的原因：（a）标题为“**为什么这很重要**”的部分，以及（b）突出**独特**一词，表示我在谈论一个特殊且独特的特征。
- en: 'The techniques we will discuss come from two distinct areas of machine learning:
    *interpretability* and *feature selection*. Specifically, we will discuss the
    following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的技术来自机器学习的两个不同领域：*可解释性*和*特征选择*。具体而言，我们将讨论以下内容：
- en: '**Interpretability Python packages.** These libraries help to make a model’s
    decision-making process more transparent by providing insights into how input
    features affect the model’s predictions. We will discuss the following: *OmniXAI,**Shapash*,
    *DALEX*, *InterpretML,* and *ELI5.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**可解释性 Python 包。** 这些库通过提供输入特征如何影响模型预测的见解，帮助使模型的决策过程更加透明。我们将讨论以下内容：*OmniXAI*，**Shapash**，*DALEX*，*InterpretML*
    和 *ELI5*。'
- en: '**Feature selection methods.** Thesemethods focus on reducing the model’s features
    by identifying the most informative features, and they generally fall into the
    filter, embedded, and wrapper categories. The characteristics of each category
    will be discussed in the next section. From each category, we will discuss the
    following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征选择方法。** 这些方法专注于通过识别最具信息量的特征来减少模型的特征，它们通常分为过滤器、嵌入式和包装三类。每一类的特点将在下一节中讨论。我们将从每一类中讨论以下内容：'
- en: 'Wrapper methods: Recursive Feature Elimination, Sequential Feature Selection,
    *Boruta* algorithm.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包装方法：递归特征消除、顺序特征选择，*Boruta* 算法。
- en: 'Embedded methods: Logistic Regression, *RandomForest*, *LightGBM*, *CatBoost*,
    *XGBoost, SelectFromModel*'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入式方法：逻辑回归，*RandomForest*，*LightGBM*，*CatBoost*，*XGBoost*，*SelectFromModel*
- en: 'Filter methods: Mutual information, *MRMR* algorithm, *SelectKBest, Relief
    algorithm.*'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤器方法：互信息，*MRMR* 算法，*SelectKBest*，*Relief* 算法。
- en: 'Other: *Featurewiz* package, *Selective* package, *PyImpetus* package.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他：*Featurewiz* 包，*Selective* 包，*PyImpetus* 包。
- en: Data
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: 'To demonstrate the above feature-importance-computation techniques, we will
    use tabular data related to heart failure prediction from *Kaggle*: [https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示上述特征重要性计算技术，我们将使用来自 *Kaggle* 的与心力衰竭预测相关的表格数据：[https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data)
- en: 'The dataset has 918 rows and 12 columns corresponding to the following features:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含918行和12列，对应以下特征：
- en: ‘Age,’ ‘Sex’(M, F), ‘ChestPainType’ (TA, ATA, NAP, ASY), ‘RestingBP,’
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '‘Age’，‘Sex’（M, F），‘ChestPainType’（TA，ATA，NAP，ASY），‘RestingBP’， '
- en: ‘Cholesterol’, ‘FastingBS’[(0,1), ‘RestingECG’(Normal,ST, LVH), ‘MaxHR’,
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '‘Cholesterol’，‘FastingBS’（0,1），‘RestingECG’（正常，ST，LVH），‘MaxHR’， '
- en: ‘ExerciseAngina’(Y,N), ‘Oldpeak’, ‘ST_Slope’ (Up, Flat, Down),
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘ExerciseAngina’（Y,N），‘Oldpeak’，‘ST_Slope’（上升、平坦、下降），
- en: ‘HeartDisease’(0,1). This is the target variable. 0 indicates the absence of
    heart disease, and 1 indicates the presence.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘HeartDisease’（0,1）。这是目标变量。0 表示没有心脏病，1 表示有心脏病。
- en: 'The dataset has no missing values, and the target variable is relatively balanced
    with 410 ‘0‘ ’instances and 508 ‘1’ instances. Five of the features are categorical:
    ‘*S*ex’, ‘ChestPainType,’ ‘ExerciseAngina,’ ‘RestingECG,’ ‘ST_Slope.’ These features
    are encoded with the *one-hot-encoding Pandas* method:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Then, the data is split into training and test sets. Finally, *scikit-learn’*s
    *StandardScaler* is applied to the numerical data of the train and test data sets.
    Now, we are ready to proceed to feature importance assessment.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Importance Assessment**'
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A. Interpretability Packages
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Why they are important**'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In recent years, the interpretability of machine learning algorithms has attracted
    significant attention. Machine learning algorithms have recently found use in
    many areas, such as finance, medicine, environmental modeling, etc. This broad
    use of ML algorithms by people who are not necessarily ML experts begs for more
    transparency because:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '**Trust issues.** Black boxes make people nervous and unsure as to whether
    they should trust them.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regulatory and ethical concerns**. Governments around the world are increasingly
    concerned about AI use and passing legislation to ensure that AI systems make
    their decisions in a fair way without any biases. Understanding how ML systems
    work under the hood is an important prerequisite to fair and unbiased AI.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretability packages are based on the model-independent interpretation
    frameworks *SHAP* (SHapley Additive exPlanations) and *LIME* (Local Interpretable
    Model-agnostic Explanations) [2]. *SHAP* uses a game theory approach and provides
    global explanations. On the other hand, *LIME* provides local explanations. They
    offer transparency through the idea of “explainers.” An explainer is a wrapper-type
    of object; it can wrap around a model and provide a door to the internal intricacies
    of the model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '**A.1 Shapash Package**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Shapash* [1] is such an interpretability package. Below, we see the implementation
    of *Shapash’s* ‘SmartExplainer’ that takes as input an ML model of type *RandomForestClassifier,*
    and the feature names. Then, *Shapash’*s ‘compile’ function is invoked, which
    is the ‘workhorse’ of the whole process: (a) it binds the model to the data, (b)
    computes the feature importances, and (c ) prepares the data for visualization.
    Finally, we invoke the interactive web application.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1, from *Shapash’s* application interface, shows the feature importances.
    The length of the horizontal bar corresponds to the importance of the feature.
    Thus, ‘ST_Slope_up’ is the most important feature, and ‘chest_pain_typeTA’ is
    the least important.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7be64f60f3fe7c2a0cdc7260ea16b685.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Feature importances
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2 shows an important type of plot provided by *Shapash*, the feature
    contribution. The feature examined in the plot is ‘ST_Slope_up’. The upper part
    contains cases where the contribution of *ST_Slope_up* is positive, whereas the
    bottom part contains cases where the contribution of ‘ST_Slope_up’ is negative.
    Also, the upper graph part corresponds to cases where ‘ST_Slope_up’ is 0, and
    the bottom corresponds to cases where ‘ST_Slope_up’ is 1\. When we click on one
    of the circles in the middle of the displayed structures, the following information
    is shown: the case number, the ‘ST_Slope_up’ value, predicted class, and contribution
    of ‘ST_Slope_up’.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 显示了*Shapash* 提供的一种重要类型的图，特征贡献。图中检查的特征是‘ST_Slope_up’。上半部分包含*ST_Slope_up*
    的正贡献情况，而下半部分包含‘ST_Slope_up’的负贡献情况。此外，上半部分图形对应于‘ST_Slope_up’为 0 的情况，下半部分对应于‘ST_Slope_up’为
    1 的情况。当我们点击显示结构中间的圆圈时，以下信息将显示：案例编号，‘ST_Slope_up’值，预测类别和‘ST_Slope_up’的贡献。
- en: '![](../Images/bd7884a7dc19f08a7dcf9521c150798a.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd7884a7dc19f08a7dcf9521c150798a.png)'
- en: Figure 2\. Feature contributions
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 特征贡献
- en: Figure 3 shows the local explanations for slice 131, where the predicted class
    is 1 with probability of 0.8416\. Bars to the right show a positive contribution,
    and bars to the left show a negative contribution to the result. ‘St_Slope_up’
    has the highest positive contribution, while ‘max_heart_rate’ has the highest
    negative contribution.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3 显示了切片 131 的局部解释，其中预测类别为 1，概率为 0.8416。右侧的条形图显示对结果的正贡献，而左侧的条形图显示负贡献。‘St_Slope_up’具有最高的正贡献，而‘max_heart_rate’具有最高的负贡献。
- en: '![](../Images/277ff773bcf0827c0a6147ab525ae2af.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/277ff773bcf0827c0a6147ab525ae2af.png)'
- en: Figure 3\. Local explanations
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 局部解释
- en: In summary, *Shapash* is a very useful package to know because (a) it offers
    a great interface where the user can gain a deep understanding of global and local
    explanations, (b) it offers the ***unique*** feature of displaying feature contributions
    across cases
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，*Shapash* 是一个非常有用的软件包，因为 (a) 它提供了一个很好的界面，用户可以深入了解全局和局部解释，(b) 它提供了展示特征贡献的***独特***功能。
- en: '**A.2\. OMNIXAI Package**'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**A.2\. OMNIXAI 包**'
- en: '*OMNIXAI* [(Open-source eXplainable AI) [3], like *Shapash*, also offers visualization
    tools, but its ***unique*** *strength* lies in the significant breadth of its
    explanation techniques. Specifically, it offers methods to explain predictions
    for various data types, i.e., tabular data, text, and images. Some of its **unique**
    features are (a) the *NLPExplainer*, (b) the bias examination module, (c) Morris
    sensitivity analysis for tabular data, (d) the *VisionExplainer* for image classification,
    and (e) counterfactual Explainers.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*OMNIXAI* [(开源可解释人工智能) [3]]，如*Shapash*，也提供可视化工具，但其***独特***的*优势*在于其解释技术的广泛性。具体来说，它提供了针对各种数据类型的预测解释方法，即表格数据、文本和图像。它的一些**独特**功能包括
    (a) *NLPExplainer*，(b) 偏差检查模块，(c) 表格数据的莫里斯敏感性分析，(d) 用于图像分类的*VisionExplainer*，以及
    (e) 反事实解释器。'
- en: The code below shows the creation of an *OMNIXAI* explainer. The essential steps
    are (a) the creation of an *OMNIXAI*-specific data type (‘Tabular’) to hold the
    data, (b) Data pre-processing through the ‘TabularTransform,’ (c) data splitting
    into training and test sets, (d) training of an *XGBClassifier* model (e) data
    inversion back to their original format (f) setting up a ‘TabularExplainer’ of
    the *XGBClassifier* with both *SHAP* and *LIME* methods. The explanation will
    be applied to ‘test_instances’ [130–135] (g) generation and display of the predictions
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码显示了*OMNIXAI* 解释器的创建。主要步骤包括 (a) 创建一个*OMNIXAI* 特定的数据类型（‘Tabular’）来保存数据，(b)
    通过‘TabularTransform’进行数据预处理，(c) 数据分割为训练集和测试集，(d) 训练一个*XGBClassifier* 模型 (e) 将数据还原为原始格式
    (f) 设置一个带有*SHAP* 和*LIME* 方法的*XGBClassifier* 的‘TabularExplainer’。解释将应用于‘test_instances’
    [130–135] (g) 生成和显示预测
- en: Figure 4 shows the aggregate local explanation for slices between [130:135]
    using *LIME.* The green bars in the right part show a positive contribution to
    label class 1, whereas the red bars in the left part show negative contributions
    to class 1\. The longer the bar, the more significant the contribution.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4 显示了使用*LIME*对切片 [130:135] 的聚合局部解释。右侧的绿色条形图显示对标签类别 1 的正贡献，而左侧的红色条形图显示对类别 1
    的负贡献。条形图越长，贡献越显著。
- en: '![](../Images/f6867d4eaf00dfaeef7379de15329e05.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6867d4eaf00dfaeef7379de15329e05.png)'
- en: Figure 4\. LIME explanations
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. LIME 解释
- en: Figure 5 shows the aggregate local explanations for slices [130:135] using *SHAP.*
    The meaning of the green/red bars is the same as in the above graph.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5 展示了使用 *SHAP* 对切片 [130:135] 进行的汇总局部解释。绿色/红色条的含义与上图相同。
- en: '![](../Images/7920f54bf0553ac38ee1a2ae538218c1.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7920f54bf0553ac38ee1a2ae538218c1.png)'
- en: Figure 5\. SHAP explanations
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. SHAP 解释
- en: '**A.3\. The InterpetML Package**'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**A.3\. InterpretML 包**'
- en: The *InterpretML* XAI interpretability [4] package has the **unique** feature
    of ‘glassbox models,’ which are inherently explainable models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*InterpretML* XAI 可解释性 [4] 包具有**独特**的‘glassbox 模型’特性，即固有可解释的模型。'
- en: The implementation of such an inherently explainable model, the ‘ExplainableBoostingClassifier,’
    is shown in the code snippet below. Global explanations and local explanations
    at slice 43 are also set up.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段展示了一个固有可解释模型‘ExplainableBoostingClassifier’的实现。全局解释和切片 43 的局部解释也已设置。
- en: Figure 6 shows the computed global feature importances.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6 展示了计算的全局特征重要性。
- en: '![](../Images/13248f6520677522a9f7f5f8a640dd68.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13248f6520677522a9f7f5f8a640dd68.png)'
- en: Figure 6\. Global feature importances
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 全局特征重要性
- en: Figure 7 shows the computed local explanations for slice at 43\. Most features
    contribute positively to the prediction of class 1, while only ‘Cholesterol’ and
    ‘FastingBS’ contribute negatively.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7 展示了切片 43 的计算局部解释。大多数特征对类别 1 的预测有积极贡献，而只有‘Cholesterol’和‘FastingBS’有负面贡献。
- en: '![](../Images/933d06bef30b259e0413d46a10cbb1de.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/933d06bef30b259e0413d46a10cbb1de.png)'
- en: Figure 7\. Local explanations
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 局部解释
- en: '**A. 4 The Dalex Package**'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**A. 4 Dalex 包**'
- en: 'The *Dalex* package[5] is a library designed to explain and understand machine
    learning models. *Dalex* stands for “Descriptive mAchine Learning EXplanations.”
    It has the following **unique** characteristics:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dalex* 包 [5] 是一个旨在解释和理解机器学习模型的库。*Dalex* 代表“Descriptive mAchine Learning EXplanations。”它具有以下**独特**特性：'
- en: It is compatible with both R and Python.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它与 R 和 Python 兼容。
- en: The *Aspects* module. This allows us to explain a model taking into account
    feature inter-dependencies.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Aspects* 模块。这使我们能够考虑特征间的相互依赖关系来解释模型。'
- en: The *Fairness* module. It allows us to evaluate the fairness of a model.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Fairness* 模块。它使我们能够评估模型的公平性。'
- en: The code snippet below shows the implementation of *Dalex*’s ‘Explainer.’
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段展示了 *Dalex* 的‘Explainer’的实现。
- en: The feature importances produced by *Dalex* are shown below in Figure 8.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dalex* 生成的特征重要性见图 8。'
- en: '![](../Images/1c854044ebe3bb57db43996d5df846af.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c854044ebe3bb57db43996d5df846af.png)'
- en: Figure 8\. Feature importances
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 特征重要性
- en: A.5 The Eli5 package
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.5 Eli5 包
- en: 'The final interpretability package we will discuss is Eli5 [5]. It has the
    following **unique** features:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的最终可解释性包是 Eli5 [5]。它具有以下**独特**特性：
- en: The permutation importance measure. In this technique, the values of each feature
    are randomly shuffled, and then the resulting drop in model performance is measured.
    The bigger the drop, the more important the feature.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排列重要性度量。在此技术中，每个特征的值会被随机打乱，然后测量模型性能的下降。下降越大，特征越重要。
- en: It works with text data. Specifically, it provides a ‘TextExplainer’ that can
    explain predictions of text classifiers.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它处理文本数据。具体来说，它提供了一个‘TextExplainer’，可以解释文本分类器的预测。
- en: It is compatible with Keras.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它与 Keras 兼容。
- en: In the code snippet below, the ‘PermutationImportance’ method is applied to
    the *Support Vector Classification* (‘svc’) estimator.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段中，‘PermutationImportance’ 方法被应用于 *Support Vector Classification* (‘svc’)
    估计器。
- en: Figure 9 shows the computed feature importances for the ‘svc’ estimator.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9 展示了‘svc’估计器计算的特征重要性。
- en: '![](../Images/8967ebf6a84b99cab25d17116cecc8d2.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8967ebf6a84b99cab25d17116cecc8d2.png)'
- en: Figure 9.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.
- en: '**B. Feature Selection Techniques**'
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**B. 特征选择技术**'
- en: Wrapper Methods
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 包装方法
- en: 'As the name suggests, these algorithms wrap the feature selection process around
    a machine learning algorithm. They continuously evaluate subsets of features until
    they find the subset that yields the best performance according to a criterion.
    This criterion can be one of the following: model accuracy, number of subset features,
    information gain, etc.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，这些算法将特征选择过程包裹在机器学习算法周围。它们不断评估特征子集，直到找到根据标准产生最佳性能的子集。这个标准可以是以下之一：模型准确率、子集特征数量、信息增益等。
- en: '**Why they are important**'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**它们为何重要**'
- en: The very nature of these algorithms (criterion optimization, comprehensive search)
    suggests that these methods can have very good performance in terms of selecting
    the best features. Another very useful characteristic of them is that they consider
    feature interactions. However, again, their very nature suggests that they can
    be computationally intensive and might overfit. So, if you do not have computational
    limitations and accuracy is essential, these are a good choice.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法的本质（标准优化、全面搜索）表明，这些方法在选择最佳特征方面可以具有非常好的性能。另一个非常有用的特性是它们考虑了特征交互。然而，它们的本质也表明，它们可能计算量大且可能会过拟合。因此，如果没有计算限制且准确性至关重要，这些方法是一个不错的选择。
- en: B.1\. Sequential Feature Selection
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1. 序列特征选择
- en: '*Sequential Feature Selection* (*SFS*) evaluates feature subsets in two modes:
    forward selection, which starts with no features and adds them iteratively, and
    backward elimination, which starts with all features and removes them one by one.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*序列特征选择*（*SFS*）以两种模式评估特征子集：前向选择，从没有特征开始，逐步添加特征；以及后向消除，从所有特征开始，逐一移除特征。'
- en: The code snippet below shows the implementation of SFS wrapped around a ‘KNeighborsClassifier’
    model. It also shows how to output the selected features and their names.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了围绕‘KNeighborsClassifier’模型实现的 SFS。它还展示了如何输出选择的特征及其名称。
- en: 'The selected features are:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的特征包括：
- en: '**B.2 The Boruta Algorithm**'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**B.2 Boruta 算法**'
- en: '*Boruta* is one of the most effective feature selection algorithms. Most impressively,
    it does not require any input from the user [7]! It is based on the brilliant
    idea of ‘shadow features’ (randomized duplicates of all original features). Then,
    a random forest classifier is applied to assess the importance of each real feature
    against these shadow features. The process is repeated until all important features
    are identified.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*Boruta* 是最有效的特征选择算法之一。最令人印象深刻的是，它不需要用户任何输入[7]！它基于‘影子特征’（所有原始特征的随机化副本）的巧妙想法。然后，应用随机森林分类器来评估每个真实特征相对于这些影子特征的重要性。这个过程会重复，直到识别出所有重要特征。'
- en: The snippet below shows the implementation of *Boruta* using the *BorutaPy*
    package and the selected features.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了使用*Boruta*包实现*Boruta*的过程以及选择的特征。
- en: 'The selected features from *Boruta* are:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从*Boruta*选择的特征包括：
- en: '**B.3 The RFECV Algorithm**'
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**B.3 RFECV 算法**'
- en: '*RFECV* (Recursive Feature Elimination with Cross-Validation) is a feature
    selection technique that iteratively removes the least important features from
    a model, using cross-validation to find the best subset of features. The code
    implementation is shown in the snippet below.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*RFECV*（递归特征消除与交叉验证）是一种特征选择技术，它通过交叉验证逐步从模型中移除最不重要的特征，以找到最佳特征子集。代码实现见下方片段。'
- en: 'The selected features are:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的特征包括：
- en: Embedded Methods
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入方法
- en: These refer to algorithms that have the built-in ability to compute feature
    importances or select features, such as *Random Forest* and *lasso* regression,
    respectively. An important note for these methods is that they *do not directly*
    select features. Instead, they compute feature importances, which can be used
    in a post hoc process to choose features. Such a post hoc process is ‘SelectFromModel’
    discussed in section B.9.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法具有内置的计算特征重要性或选择特征的能力，如*随机森林*和*套索*回归。对于这些方法，一个重要的注意事项是它们*不会直接*选择特征。相反，它们计算特征重要性，这些重要性可以在后续处理中用于选择特征。这样的后续过程是第
    B.9 节讨论的‘SelectFromModel’。
- en: '**Why they are important**'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**它们为何重要**'
- en: High-dimensional data are very common today in the form of unstructured text,
    images, and time series, especially in the fields of bioinformatics, environment
    monitoring, and finance. The greatest advantage of embedded methods is their ability
    to handle high-dimensional data. The reason for this ability is that they do not
    have separate modeling and feature selection steps. Feature selection and modeling
    are combined in one single step, which leads to a significant speed-up.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 高维数据今天非常普遍，形式包括非结构化文本、图像和时间序列，特别是在生物信息学、环境监测和金融领域。嵌入方法的最大优势在于它们处理高维数据的能力。其原因在于它们没有单独的建模和特征选择步骤。特征选择和建模在一个步骤中完成，这大大加快了速度。
- en: '**B.4 Logistic Regression**'
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**B.4 逻辑回归**'
- en: Logistic Regression is a statistical method used for binary classification.
    The coefficients of the model relate to the importance of features. Each weight
    indicates the direction (positive or negative) and the strength of feature’s effect
    on the log odds of the target variable. A larger absolute value of a weight indicates
    that the corresponding feature is more important in predicting the outcome. The
    code snippet below shows the creation of the logistic regression. The hyper-parameters
    ‘C’ (regularization strength) and ‘max_iter’ are learned by applying s*cikit-learn’*s
    ‘GridSearchCV.’
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种用于二分类的统计方法。模型的系数与特征的重要性相关。每个权重表示特征对目标变量对数赔率的影响方向（正向或负向）及强度。权重的绝对值越大，表示对应的特征在预测结果中越重要。下面的代码片段展示了逻辑回归的创建过程。超参数‘C’（正则化强度）和‘max_iter’通过应用*scikit-learn*的‘GridSearchCV’进行学习。
- en: The logistic regression coefficients are shown below.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归系数如下所示。
- en: '**B.5 Random Forest**'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**B.5 随机森林**'
- en: '*Random Forest* is an ensemble machine learning method used for classification
    and regression. It works by building many decision trees and merging their results.
    It uses the *bagging* technique where sampling-with-replacement is applied to
    the dataset. Then, each sample is used to train a separate decision tree. A significant
    feature of *Random Forest* is its ability to compute feature importances during
    the training process. It does this by randomizing a feature (while keeping all
    other features constant) and then checking how much the error increased. The most
    common criterion for computing feature importance is the mean *decrease in impurity*
    (*MDI*) when a feature is used to split a node [8]. The code snippet below shows
    the computation of the *scikit-learn* ‘RandomForestClassifier,’ where the hyperparameters
    have been determined as above using *scikit-learn’*s ‘GridSearchCV.’'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机森林*是一种用于分类和回归的集成机器学习方法。它通过构建许多决策树并合并它们的结果来工作。它使用*bagging*技术，即对数据集应用有放回的抽样。然后，每个样本用于训练一个单独的决策树。*随机森林*的一个显著特点是在训练过程中计算特征重要性。它通过随机化一个特征（同时保持其他特征不变），然后检查错误增加的程度来完成这一点。计算特征重要性最常用的标准是特征用于分割节点时的平均*不纯度下降*（*MDI*）
    [8]。下面的代码片段展示了*scikit-learn*的‘RandomForestClassifier’的计算，其中超参数已经如上所述使用*scikit-learn*的‘GridSearchCV’确定。'
- en: The code for the computation and display of feature importances is shown below.
    The computed feature importances are shown in Figure 10.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 计算和显示特征重要性的代码如下所示。计算出的特征重要性见图 10。
- en: '![](../Images/098f6bdca1250e7e317452c8206ad32b.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/098f6bdca1250e7e317452c8206ad32b.png)'
- en: Figure 10\. Feature importances
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 特征重要性
- en: B.6 The LightGBM algorithm
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.6 *LightGBM* 算法
- en: '*LightGBM* (Light Gradient Boosting Machine) is a gradient-boosting algorithm
    that combines speed and performance. Developed by *Microsoft*, it is known for
    handling large datasets and for its efficiency in terms of memory and speed. Some
    of its**unique** features are (a) its ability to filter out data instances with
    small gradients and focus on more critical instances, (b) ‘Exclusive Feature Bundling’(EFB):
    *LightGBM* reduces the number of features by bundling mutually exclusive features
    (those that very infrequently are non-zero at the same time). In this way, the
    algorithm increases the efficiency of high-dimensional data [9].'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*LightGBM*（Light Gradient Boosting Machine）是一种结合了速度和性能的梯度提升算法。由*微软*开发，因其处理大数据集的能力以及在内存和速度上的高效性而闻名。它的一些**独特**特点包括（a）能够过滤掉梯度较小的数据实例，专注于更关键的实例，（b）‘Exclusive
    Feature Bundling’（EFB）：*LightGBM*通过捆绑互斥特征（那些很少同时为非零的特征）来减少特征数量。通过这种方式，算法提高了高维数据的效率
    [9]。'
- en: The snippet below shows the implementation of *LightGBM*. The hyperparameters
    (‘learning rate,’ ‘max_depth,’ and ‘n_estimators’) were chosen using *scikit-learn’s
    ‘*GridSearchCV’ algorithm. The feature importances computed from LightGBM are
    shown in Figure 11.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段展示了*LightGBM*的实现。超参数（‘learning rate’，‘max_depth’，和‘n_estimators’）使用*scikit-learn*的‘GridSearchCV’算法进行选择。从*LightGBM*计算出的特征重要性见图
    11。
- en: '![](../Images/cebbb08c8a20062e142d065df31bdf5a.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cebbb08c8a20062e142d065df31bdf5a.png)'
- en: Figure 11.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.
- en: '**B.7 The XGBoost Algorithm**'
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**B.7 XGBoost 算法**'
- en: '*XGBoost*, which stands for *eXtreme Gradient Boosting,* is an advanced implementation
    of gradient boosting. It has the following **unique** characteristics:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*XGBoost*，即*eXtreme Gradient Boosting*，是梯度提升的高级实现。它具有以下**独特**特性：'
- en: It can effectively use all available CPU cores or clusters to create the tree
    in parallel. It also utilizes cache optimization.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以有效利用所有可用的 CPU 核心或集群来并行创建树。同时，它还利用了缓存优化。
- en: Compared to *LightGBM,* *XGBoost* grows trees depth-wise (level-wise), while
    *LightGBM* grows trees leaf-wise. This makes *XGBoost* less efficient with large
    datasets.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 *LightGBM* 相比，*XGBoost* 以树的深度为单位（层级）生长树，而 *LightGBM* 以叶子为单位生长树。这使得 *XGBoost*
    在处理大数据集时效率较低。
- en: 'The code snippet shows the implementation of *XGBoost,* where the hyperparameters
    [10] shown below were chosen based on *Bayesian optimization* implemented in the
    ‘hyperopt’ package. These hyperparameters are:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段展示了 *XGBoost* 的实现，其中下述超参数 [10] 是基于在 ‘hyperopt’ 包中实现的 *贝叶斯优化* 选择的。这些超参数是：
- en: ‘gamma’ (min loss reduction for a split),
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘gamma’（分裂的最小损失减少）
- en: ‘min_child_weight’ (min required sum of weights of all observations in a child)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘min_child_weight’（子节点中所有观察值的权重之和的最小值）
- en: ‘max_depth’ (max tree depth)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘max_depth’（最大树深度）
- en: ‘reg_lambda’ (L2 regularization handle)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘reg_lambda’（L2 正则化处理）
- en: Finally, the hyperparameter ‘reg_alpha,’ which controls L1 regularization, was
    set manually after experimentation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，控制 L1 正则化的超参数 ‘reg_alpha’ 在实验后被手动设置。
- en: Figure 12 shows the feature importances. Note that some importances are set
    to zero because of L1 regularization.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12 显示了特征的重要性。注意由于 L1 正则化，一些重要性被设为零。
- en: '![](../Images/beed86916ac7f6258e241119de1ac6ba.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/beed86916ac7f6258e241119de1ac6ba.png)'
- en: Figure 12\. Feature importances
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12. 特征重要性
- en: '**B.8 The CatBoost Algorithm**'
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**B.8 CatBoost 算法**'
- en: '*CatBoost* [11] is a high-performance, open-source gradient boosting library,
    particularly well-suited for categorical data. Specifically, it does not require
    any pre-processing of categorical variables, such as label-encoding or one-hot-encoding.
    Instead, it handles categorical variables natively. *CatBoost* employs symmetric
    trees as its base predictors and supports GPU acceleration. Regarding *CatBoost*
    implementation in Python, it is important to note that all non-numeric features
    must be declared as type ‘category.’ Then, as shown in the snippet below, the
    categorical features are provided as input to the model’s *fit* function.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*CatBoost* [11] 是一个高性能的开源梯度提升库，特别适用于分类数据。具体来说，它不需要对分类变量进行任何预处理，如标签编码或 one-hot
    编码。相反，它原生处理分类变量。*CatBoost* 使用对称树作为其基础预测器，并支持 GPU 加速。关于 *CatBoost* 在 Python 中的实现，值得注意的是所有非数值特征必须声明为
    ‘category’ 类型。然后，如下片段所示，这些分类特征作为输入提供给模型的 *fit* 函数。'
- en: Figure 13 shows the feature importance computed by *CatBoost*. It is important
    to note that the names of the features are the ones in the original data set (not
    the one-hot-encoded). Because *CatBoost* handles categorical data natively, the
    input to the *CatBoost* algorithm was the original data (not one-hot-encoded).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13 显示了由 *CatBoost* 计算的特征重要性。需要注意的是，特征的名称是原始数据集中（而不是 one-hot 编码后的）。由于 *CatBoost*
    原生处理分类数据，因此 *CatBoost* 算法的输入是原始数据（而不是 one-hot 编码后的）。
- en: '![](../Images/1dadde2ee51deb824a3555bee64da701.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dadde2ee51deb824a3555bee64da701.png)'
- en: Figure 13\. Feature importances
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13. 特征重要性
- en: '**B.9 The SelectFromModel Method**'
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**B.9 SelectFromModel 方法**'
- en: ‘SelectFromModel,’ is offered by *scikit-learn’s feature.selection* package.
    Its **unique** characteristic is that it is a *meta-transformer* that can be used
    with models that assign importances to features, either through *coef_* or *feature_importances_*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ‘SelectFromModel’ 是由 *scikit-learn’s feature.selection* 包提供的。它的**独特**特征在于它是一个
    *元转换器*，可以与那些通过 *coef_* 或 *feature_importances_* 来分配特征重要性的模型一起使用。
- en: In contrast to the previous embedded methods we discussed, which just computed
    feature importances, ‘SelectFromModel’ actually *selects* features. The snippet
    below shows the code for feature selection using this method.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们讨论的前面嵌入方法不同，‘SelectFromModel’ 实际上是 *选择* 特征的。下面的代码片段展示了使用这种方法进行特征选择的代码。
- en: 'The selected features are:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 选定的特征是：
- en: Filter Feature Selection Methods
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤特征选择方法
- en: These are independent of any machine learning model. They typically rely on
    statistical measures to evaluate each feature, such as correlation and mutual
    information between the target and predictor variables.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征与任何机器学习模型无关。它们通常依赖统计度量来评估每个特征，例如目标变量和预测变量之间的相关性和互信息。
- en: Why they are important
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它们为何重要
- en: Filter methods are straightforward and very easy to compute and, therefore,
    are used as an initial feature selection step in many fields with large amounts
    of data, such as bioinformatics[12], environmental studies, and healthcare research[13].
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤方法直观且计算非常简单，因此在许多数据量大的领域（如生物信息学[12]、环境研究和医疗保健研究[13]）中作为初始特征选择步骤使用。
- en: B. 10 Mutual information
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.10 互信息
- en: The *mutual information* measures the reduction in uncertainty (entropy) in
    one variable, given knowledge of the other. The mutual information between the
    predictors and the target variable is computed using *scikit-learn’*s *mutual_info_classif*.
    The mutual information score of each predictor is shown in Figure 14.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*互信息*度量了在给定另一个变量的知识时一个变量的不确定性（熵）的减少。预测变量与目标变量之间的互信息使用*scikit-learn*的*mutual_info_classif*计算。每个预测变量的互信息得分如图14所示。'
- en: '![](../Images/7be314d63145b1f480ac8bee95981c66.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7be314d63145b1f480ac8bee95981c66.png)'
- en: Figure 14\. Mutual information scores.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图14\. 互信息得分。
- en: B.11 The MRMR Algorithm
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.11 MRMR算法
- en: '*MRMR* stands for *M*axium-*R*elevancy-*M*aximum-*R*edundancy. As the name
    indicates, the *MRMR* algorithm selects features that are (a)Maximally relevant,
    i.e., strongly correlated with the target variable, (b) Minimally redundant, i.e.,
    exhibit high dissimilarity among them. Redundancy can be computed using correlation
    or mutual information measures, and relevance can be calculated using the F-statistic
    or mutual information[15]. *MRMR* is a *minimal-optimal* method because it selects
    a group of features that, together, have maximum predictive power [14]. This is
    in contrast to the *Boruta* algorithm, discussed in section B.2, which is an *all-relevant*
    algorithm because it identifies all features relevant to the model’s prediction.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*MRMR*代表*最大相关性最大冗余*。正如名称所示，*MRMR*算法选择的特征具有以下特点：（a）最大相关，即与目标变量强相关，（b）最小冗余，即它们之间表现出高度的不相似性。冗余可以使用相关性或互信息度量计算，相关性可以使用F统计量或互信息计算[15]。*MRMR*是一种*最小最优*方法，因为它选择了一组特征，这些特征组合在一起具有最大的预测能力[14]。这与在B.2节讨论的*Boruta*算法形成对比，后者是一种*全相关*算法，因为它识别了所有对模型预测相关的特征。'
- en: The code snippet below shows the implementation of *MRMR* with the ‘mrmr’ Python
    library.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段展示了使用‘mrmr’ Python库实现的*MRMR*。
- en: 'The minimal-optimal set of selected features is shown below:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最小最优特征集如下所示：
- en: B.12 The SelectKBest Method
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.12 SelectKBest方法
- en: 'As the name suggests, this algorithm selects the K best features according
    to a user-defined score. The number K is also user-defined. The algorithm can
    be applied to both classification and regression tasks, and it offers a variety
    of scoring functions. For example, for classification, the user can apply the
    following: (a) ‘f_classif,’ which computes the *ANOVA* *F-value*, (b) ‘mutual_info_classif,’
    which computes mutual information, and (c) *chi2*, which computes chi-squared
    statistics, between the predictors and the target variable [16]. The code snippet
    below shows the computation of *SelectKBest* for k=5 and score function ‘f_classif’.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名称所示，该算法根据用户定义的评分选择K个最佳特征。数量K也是用户定义的。该算法可应用于分类和回归任务，提供多种评分函数。例如，对于分类，用户可以应用以下方法：（a）‘f_classif’，计算*ANOVA*
    *F值*，（b）‘mutual_info_classif’，计算互信息，（c）*chi2*，计算预测变量与目标变量之间的卡方统计量[16]。下面的代码片段展示了k=5和评分函数‘f_classif’的*SelectKBest*的计算。
- en: Figure 15 below shows the scores (importances) of the features according to
    the scoring function ‘f_classif.’ Note that although we chose K=5, Figure 15 displays
    the scores for all features.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图15显示了根据评分函数‘f_classif’的特征得分（重要性）。注意，虽然我们选择了K=5，但图15显示了所有特征的得分。
- en: '![](../Images/988ae397915ce03c7a0f5c02a9d31eb3.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/988ae397915ce03c7a0f5c02a9d31eb3.png)'
- en: Figure 15\. Feature importances.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图15\. 特征重要性。
- en: B.13 The Relief Algorithm
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.13 Relief算法
- en: '*Relief* ‘s **unique** characteristic is the following idea: For a data sample,
    find its closest neighbor in the same class (‘near hit’) and the closest neighbor
    in the other class (‘near miss’). Features are weighted according to how well
    they are similar to the ‘near hit’ and how well they differentiate from the ‘near
    miss’ sample. *Relief* is particularly useful in biomedical informatics because
    of its sensitivity to complex feature associations [17]. Here, we used an extension
    of the original *Relief* algorithm, the *ReliefF* algorithm, which can be applied
    to multi-class classifications. In contrast, the original *Relief* algorithm can
    only be applied to binary classification cases. The snippet below shows the invocation
    of the ‘ReliefFselector’ from the ‘kydavra’ Python package.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*Relief* 的 **独特** 特征是以下想法：对于一个数据样本，找到同一类别中最近的邻居（‘近邻’）和另一类别中最近的邻居（‘近错’）。特征根据它们与‘近邻’的相似程度以及与‘近错’样本的区别程度进行加权。由于其对复杂特征关联的敏感性，*Relief*
    在生物医学信息学中特别有用 [17]。在这里，我们使用了原始 *Relief* 算法的扩展版——*ReliefF* 算法，它可以应用于多类别分类。相比之下，原始的
    *Relief* 算法仅适用于二分类情况。下面的代码片段展示了从 ‘kydavra’ Python 包中调用 ‘ReliefFselector’ 的方法。'
- en: The selected features from the algorithm are shown below.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 从算法中选择的特征如下所示。
- en: Misc Feature Selection Techniques
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 杂项特征选择技术
- en: In this final category, we will discuss the *Featurewiz*, *Selective*, and *PyImpetus*
    packages.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一最终类别中，我们将讨论 *Featurewiz*、*Selective* 和 *PyImpetus* 包。
- en: Why they are important
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 他们的重要性
- en: 'Each package is important for its **unique** reasons: (a) *Featurewiz* is a
    very convenient AutoML package. It selects features with one line of code; (b)
    The *Selective* package offers a wide variety of filter and embedded filter selection
    methods that can be easily invoked with one line of code; (c) The *PyImpetus*
    package is based on an algorithm that is very different from all other feature
    selection techniques, the *Markov Blanket.*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 每个包都有其 **独特** 的原因：（a）*Featurewiz* 是一个非常方便的 AutoML 包。它只需一行代码即可选择特征；（b）*Selective*
    包提供了多种过滤和嵌入式过滤选择方法，可以通过一行代码轻松调用；（c）*PyImpetus* 包基于一种与所有其他特征选择技术非常不同的算法，即 *Markov
    Blanket*。
- en: B.14 The Featurewiz Package
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.14 *Featurewiz* 包
- en: This is an automated feature selection tool [18][19]. Its invocation is as simple
    as shown in the code snippet below. Under the hood, it uses the ‘SULOV’ algorithm
    (*S*earching for *U*ncorrelated *L*ist Of *V*ariables), whose basis is the *MRMR*
    algorithm described above in section B.11\. ‘SULOV’ selects the features with
    the highest mutual information score and the smallest correlation among them.
    Then, the features are passed recursively through *XGBoost* to find the best subset.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个自动化特征选择工具 [18][19]。其调用方式如下面的代码片段所示。该工具在后台使用 ‘SULOV’ 算法（*S*earching for *U*ncorrelated
    *L*ist Of *V*ariables），其基础是上文 B.11 节中描述的 *MRMR* 算法。‘SULOV’ 选择具有最高互信息分数和最小相关性的特征。然后，这些特征递归传递给
    *XGBoost* 以找到最佳子集。
- en: The features selected from *Featurewiz* are shown below.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *Featurewiz* 中选择的特征如下所示。
- en: B.15\. The Selective Feature Selection Library
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.15 *Selective* 特征选择库
- en: 'This library provides numerous feature selection methods for classification
    and regression tasks [20]. Some of the methods offered are: correlation, variance,
    statistical analysis (ANOVA f-test classification, chi-square, etc.), linear methods
    (linear regression, *lasso*, and *ridge* regularization, etc.), and tree-based
    methods (*Random Forest*, *XGBoost*, etc.). An example of this library’s usage
    is shown below.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库提供了多种特征选择方法用于分类和回归任务 [20]。其中一些方法包括：相关性、方差、统计分析（ANOVA f检验分类、卡方检验等）、线性方法（线性回归、*lasso*
    和 *ridge* 正则化等），以及基于树的方法（*Random Forest*、*XGBoost* 等）。以下展示了这个库的一个用例。
- en: 'The selected features using the ‘TreeBased’ method are:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ‘TreeBased’ 方法选择的特征如下：
- en: B.16\. The PyImpetus Package
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.16 *PyImpetus* 包
- en: The **unique** idea of this algorithm is the *Markov Blanket*, which is the
    minimal feature set needed to predict the target variable[21][22]. It can be used
    for both classification and regression tasks. Its implementation for classification
    is shown below.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的 **独特** 思路是 *Markov Blanket*，它是预测目标变量所需的最小特征集 [21][22]。它可用于分类和回归任务。其分类实现如下所示。
- en: Figure 16 shows the selected features and their relative importance.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16 显示了选择的特征及其相对重要性。
- en: '![](../Images/66328477583284f4106cda46e9d5befd.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66328477583284f4106cda46e9d5befd.png)'
- en: Figure 16.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16。
- en: '**Discussion and Conclusion**'
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**讨论与结论**'
- en: 'In this article, we discussed a broad spectrum of feature importance assessment
    techniques from two distinct realms: interpretability and feature selection. Given
    the diversity of the discussed algorithms, a question that arises naturally is:”
    How similar are the features selected as most important by the various algorithms?”'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了从两个不同领域：解释性和特征选择，涉及的广泛特征重要性评估技术。鉴于讨论的算法多样性，自然会出现一个问题：“不同算法选择的最重要特征有多相似？”
- en: Let us take a look at Table 1 below. This table has two columns corresponding
    to the features ‘ST_Slope_up’ and ‘ST_Slope_flat.’ The rows correspond to the
    algorithms and packages we used in the article. The numbers 1,2,3 indicate whether
    the feature was selected as the best, second best, or third best by the algorithm.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看下面的表 1。该表有两列，对应于特征‘ST_Slope_up’和‘ST_Slope_flat’。行对应于我们在文章中使用的算法和包。数字 1、2、3
    表示该特征被算法选择为最佳、第二最佳或第三最佳。
- en: Table 1.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1。
- en: As discussed in the article, some algorithms simply output a set of features
    without any order. In this case, the X in the table indicates that the algorithm
    *selected* the feature. In the case of a gap in the table, the feature was not
    in the three best features selected by the corresponding algorithm. For logistic
    regression, the absolute value of the coefficients was considered. For *CatBoost*,
    we assigned a 1 value to both ‘ST_Slope_up’ and ‘ST_Slop_flat’ because *CatBoost*
    selected ‘ST_Slope’ as the most important feature. Finally, the *OMNIXAI* package
    results were not included because they provided local explanations for a few rows.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如文章中所述，一些算法简单地输出了一组特征而没有任何顺序。在这种情况下，表中的 X 表示算法*选择*了该特征。如果表中有空缺，说明该特征未被相应算法选择为前三个最重要的特征。对于逻辑回归，考虑了系数的绝对值。对于*CatBoost*，我们为‘ST_Slope_up’和‘ST_Slop_flat’分配了
    1，因为*CatBoost*将‘ST_Slope’选为最重要特征。最后，*OMNIXAI*包的结果未包含在内，因为它们仅提供了少数行的局部解释。
- en: An interesting fact emerges from the observation of Table 1\. Except for *LightGBM*,
    the feature ‘ST_Slope_up’ had the *highest* or *second highest* importance inthealgorithms
    that report feature importances. It was also selected by most algorithms that
    reported selected features without importances. The feature ‘ST_Slope_Flat’ also
    performed pretty well because it was either in the first three highest-importance
    features or in the selected feature groups for most algorithms.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从表 1 的观察中出现了一个有趣的事实。除了*LightGBM*，特征‘ST_Slope_up’在报告特征重要性的算法中具有*最高*或*第二高*的重要性。它也被大多数报告选择特征但不报告重要性的算法所选择。特征‘ST_Slope_Flat’也表现得相当好，因为它要么位于前三个最高重要性特征中，要么位于大多数算法选择的特征组中。
- en: Now, let us delve into another interesting insight. These two features had the
    *highest* and *second-highest* *mutual information scores.* As we saw in section
    B.10, this is a straightforward feature computed in one line of code. *So, with
    one line of code, we gained insights into the most important features of our data,
    as reported by the other significantly more computationally complex algorithms.*
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们*深入*探讨另一个有趣的见解。这两个特征具有*最高*和*第二高*的*互信息评分*。正如我们在 B.10 节中看到的，这是一个只需一行代码即可计算的简单特征。*因此，通过一行代码，我们获得了对数据中最重要特征的见解，这些特征由其他计算复杂度更高的算法报告。*
- en: This article discussed twenty-one packages and methods that compute *feature
    importance*, a measure of a feature’s contribution to a model’s predictive ability.
    For further reading, I recommend [23], which discusses another role of features,
    specifically their error contribution to a model.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了二十一种计算*特征重要性*的包和方法，这是衡量特征对模型预测能力贡献的一个指标。欲进一步阅读，我推荐 [23]，它讨论了特征的另一个作用，即其对模型的错误贡献。
- en: The entire code can be found at [https://github.com/theomitsa/Feature_importance/tree/main](https://github.com/theomitsa/Feature_importance/tree/main).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可以在 [https://github.com/theomitsa/Feature_importance/tree/main](https://github.com/theomitsa/Feature_importance/tree/main)
    找到。
- en: Thank you for reading!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: '**Footnotes:**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚注：**'
- en: '**Dataset license:** Open Data Commons Open Database License (ODbL) v1.0 as
    mentioned in [https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)
    and described in [https://opendatacommons.org/licenses/odbl/1-0/](https://opendatacommons.org/licenses/odbl/1-0/)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集许可：** 如在 [https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)
    中提到的，遵循开放数据公共开放数据库许可证 (ODbL) v1.0，并在 [https://opendatacommons.org/licenses/odbl/1-0/](https://opendatacommons.org/licenses/odbl/1-0/)
    中描述。'
- en: I certify that all visualizations/images/graphs in the article are created by
    the author and can be reproduced in the github directory above.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我证实文章中的所有可视化/图像/图表均由作者创作，可以在上述 GitHub 目录中复制。
- en: '**References**'
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '**The Shapash package,**[https://shapash.readthedocs.io/en/latest/](https://shapash.readthedocs.io/en/latest/)'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Shapash 包**，[https://shapash.readthedocs.io/en/latest/](https://shapash.readthedocs.io/en/latest/)'
- en: Molnar, C., **Interpretable Machine Learning,** 2023\. [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Molnar, C.，**可解释的机器学习**，2023年。[https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)
- en: '**The OMNIXAI package,** [https://opensource.salesforce.com/OmniXAI/latest/omnixai.html](https://opensource.salesforce.com/OmniXAI/latest/omnixai.html)'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**OMNIXAI 包**，[https://opensource.salesforce.com/OmniXAI/latest/omnixai.html](https://opensource.salesforce.com/OmniXAI/latest/omnixai.html)'
- en: '**The InterpretML package**, [https://interpret.ml/](https://interpret.ml/)'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**InterpretML 包**，[https://interpret.ml/](https://interpret.ml/)'
- en: '**The Dalex package**, [https://dalex.drwhy.ai/](https://dalex.drwhy.ai/)'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Dalex 包**，[https://dalex.drwhy.ai/](https://dalex.drwhy.ai/)'
- en: '**The Eli5 package,** [https://eli5.readthedocs.io/en/latest/index.html](https://eli5.readthedocs.io/en/latest/index.html)'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Eli5 包**，[https://eli5.readthedocs.io/en/latest/index.html](https://eli5.readthedocs.io/en/latest/index.html)'
- en: 'Mazzanti, S., **Boruta** **Explained Exactly How You Wished Someone Explained
    to You**, Medium: Towards Data Science, March 2020.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Mazzanti, S.，**Boruta** **正如你希望别人解释给你听的那样详细解释**，Medium: Towards Data Science，2020年3月。'
- en: Scornet E., **Trees, Forests, and Impurity-Based Variable Importance,** 2021,
    ffhal-02436169v3f, [https://hal.science/hal-02436169v3/file/importance_variable.pdf](https://hal.science/hal-02436169v3/file/importance_variable.pdf)
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Scornet E.，**树、森林与基于杂质的变量重要性**，2021年，ffhal-02436169v3f，[https://hal.science/hal-02436169v3/file/importance_variable.pdf](https://hal.science/hal-02436169v3/file/importance_variable.pdf)
- en: 'Ke, G. et al., **LightGBM: A Highly Efficient Gradient Boosting Decision Tree,**
    NIPS Conference, pp. 3149–3157, December 2017.'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ke, G. 等人，**LightGBM：一种高效的梯度提升决策树**，NIPS 会议，页码 3149–3157，2017年12月。
- en: Banerjee, P., **A Guide on XGBoost Hyperparameters Tuning**, [https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning](https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Banerjee, P.，**XGBoost 超参数调整指南**，[https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning](https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning)
- en: 'Prokhorenkova, L. et al., **CatBoost: Unbiased Boosting With Categorical Features,**
    NIPS’18: Proceedings of the 32nd International Conference on Neural Information
    Processing Systems, pp. 6639–6649, December 2018.'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Prokhorenkova, L. 等人，**CatBoost：具有类别特征的无偏提升**，NIPS’18：第32届国际神经信息处理系统会议论文集，页码
    6639–6649，2018年12月。
- en: Urbanowicz, R.J. et al., **Benchmarking Relief-Based Feature Selection Methods
    for Bioinformatics** Data Mining, Journal of Biomedical Informatics, vol.85, pp.168–188,
    Sept. 2018.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Urbanowicz, R.J. 等人，**生物信息学数据挖掘中基于 Relief 的特征选择方法基准测试**，《生物医学信息学杂志》，第85卷，页码
    168–188，2018年9月。
- en: Raju, S.K., **Evaluation of Mutual Information and Feature Selection for SARS-CoV-2
    Respiratory Infection,** Bioengineering (Basel), vol. 10, no. 7, July 2023.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Raju, S.K.，**SARS-CoV-2 呼吸道感染的互信息与特征选择评估**，生物工程 (巴塞尔)，第10卷，第7期，2023年7月。
- en: 'Mazzanti, S., “**MRMR” Explained Exactly How You Wished Someone Explained to
    You**, Medium: Towards Data Science, February 2021.'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Mazzanti, S.，“**MRMR** 正如你希望别人解释给你听的那样详细解释”，Medium: Towards Data Science，2021年2月。'
- en: Radovic, M. et al., **Minimum Redundancy Maximum Relevance Feature Selection
    Approach for Temporal Gene Expression Data,** BMC Bioinformatics, January 2017.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Radovic, M. 等人，**用于时间基因表达数据的最小冗余最大相关特征选择方法**，BMC 生物信息学，2017年1月。
- en: 'Kavya, D., **Optimizing Performance: SelectKBest for Efficient Feature Selection
    in Machine Learning**, Medium, February 2023.'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kavya, D.，**优化性能：用于机器学习中高效特征选择的 SelectKBest**，Medium，2023年2月。
- en: 'Urbanowicz, R. J. et al., **Relief-Based Feature Selection: Introduction And
    Review,** Journal of Biomedical Informatics, vol. 85, pp. 189–203, Sept. 2018.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Urbanowicz, R. J. 等，**基于Relief的特征选择：介绍与综述，** 《生物医学信息学杂志》，第85卷，第189–203页，2018年9月。
- en: '**The Featurewiz package,** [https://github.com/AutoViML/featurewiz](https://github.com/AutoViML/featurewiz)'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Featurewiz包，** [https://github.com/AutoViML/featurewiz](https://github.com/AutoViML/featurewiz)'
- en: 'Sharma, H., **Featurewiz: Fast Way to Select the Best Features in Data,** Medium,
    Medium: Towards Data Science, Dec.2020.'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Sharma, H., **Featurewiz：快速选择数据中最佳特征的方法，** Medium，Medium: Towards Data Science，2020年12月。'
- en: '**The Selective Feature Selection Library,** [https://github.com/fidelity/selective](https://github.com/fidelity/selective)'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择性特征选择库，** [https://github.com/fidelity/selective](https://github.com/fidelity/selective)'
- en: '**The PyImpetus package**, [https://github.com/atif-hassan/PyImpetus](https://github.com/atif-hassan/PyImpetus)'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**PyImpetus包，** [https://github.com/atif-hassan/PyImpetus](https://github.com/atif-hassan/PyImpetus)'
- en: 'Hassan, A. et al., **PPFS: Predictive Permutation Feature Selection,** [https://arxiv.org/pdf/2110.10713.pdf](https://arxiv.org/pdf/2110.10713.pdf)'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hassan, A. 等，**PPFS：预测置换特征选择，** [https://arxiv.org/pdf/2110.10713.pdf](https://arxiv.org/pdf/2110.10713.pdf)
- en: 'Mazzanti, S., **Your Features Are Important? It Doesn’t Mean They Are Good,**
    Medium: Towards Data Science, August 2023.'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Mazzanti, S.，**你的特征重要吗？这并不意味着它们好，** Medium: Towards Data Science，2023年8月。'
