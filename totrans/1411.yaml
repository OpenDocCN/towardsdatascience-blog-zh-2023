- en: Leverage LLMs Like GPT to Analyze Your Documents or Transcripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/leverage-llms-like-gpt-to-analyze-your-documents-or-transcripts-c640a266ad52](https://towardsdatascience.com/leverage-llms-like-gpt-to-analyze-your-documents-or-transcripts-c640a266ad52)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Use prompt engineering to analyze your documents with langchain and openai in
    a ChatGPT-like way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://konstantin-rink.medium.com/?source=post_page-----c640a266ad52--------------------------------)[![Konstantin
    Rink](../Images/41bfc069d7382a0fd56f081eea7eb2d9.png)](https://konstantin-rink.medium.com/?source=post_page-----c640a266ad52--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c640a266ad52--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c640a266ad52--------------------------------)
    [Konstantin Rink](https://konstantin-rink.medium.com/?source=post_page-----c640a266ad52--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c640a266ad52--------------------------------)
    ·6 min read·Mar 31, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7dd6687f2e85da1c40832aaf26002fc7.png)'
  prefs: []
  type: TYPE_IMG
- en: (Original) photo by [Laura Rivera](https://unsplash.com/@laurar1vera?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/de/fotos/9ZQzrLWV52M?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText).
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT is definitely one of the most popular Large Language Models (LLMs).
    Since the release of its beta version at the end of 2022, everyone can use the
    convenient chat function to ask questions or interact with the language model.
  prefs: []
  type: TYPE_NORMAL
- en: '**But what if we would like to ask ChatGPT questions about our own documents
    or about a podcast we just listened to?**'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this article is to show you how to leverage LLMs like GPT to analyze
    our documents or transcripts and then ask questions and receive answers in a ChatGPT
    way about the content in the documents.
  prefs: []
  type: TYPE_NORMAL
- en: tl;dr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article uses OpenAI’s ChatGPT `gpt-3.5-turbo` model, which requires an
    [API key](https://platform.openai.com/account/api-keys).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `langchain` package, a framework built around LLMs, is used to load and
    process our documents (Prompt Engineering) and to interact with the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A colab notebook containing the whole code of the article can be found [here](https://colab.research.google.com/drive/1oG6TXgXJTd8qyCj_ncD0Uy1pGEDMlly3?usp=sharing).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before writing all the code, we have to make sure that all the necessary packages
    are installed, API keys are created, and configurations set.
  prefs: []
  type: TYPE_NORMAL
- en: API key
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make use of ChatGPT one needs to create an OpenAI API key first. The key
    can be created under this [link](https://platform.openai.com/account/api-keys)
    and then by clicking on the
  prefs: []
  type: TYPE_NORMAL
- en: '`+ Create new secret key` button.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nothing is free**: Generally OpenAI charges you for every 1,000 tokens. Tokens
    are the result of processed texts and can be words or chunks of characters. The
    prices per 1,000 tokens vary per model (e.g., $0.002 / 1K tokens for gpt-3.5-turbo).
    More details about the pricing options can be found [here](https://openai.com/pricing).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The good thing is that OpenAI grants you a free trial usage of $18 without requiring
    any payment information. An overview of your current usage can be seen in your
    [account](https://platform.openai.com/account/usage).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Installing the OpenAI package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have to also install the official OpenAI package by running the following
    command
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Since OpenAI needs a (valid) API key, we will also have to set the key as a
    environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Installing the langchain package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the tremendous rise of interest in Large Language Models (LLMs) in late
    2022 (release of Chat-GPT), a package named LangChain appeared [around the same
    time](https://github.com/hwchase17/langchain/graphs/commit-activity).
  prefs: []
  type: TYPE_NORMAL
- en: '[LangChain](https://github.com/hwchase17/langchain) is a framework built around
    LLMs like ChatGPT. The aim of this package is to assist in the development of
    applications that combine LLMs with other sources of computation or knowledge.
    It covers the application areas like *Question Answering over specific documents*
    (**goal of this article**)*, Chatbots*, and *Agents*. More information can be
    found in the [documentation](https://langchain.readthedocs.io/en/latest/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The package can be installed with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Prompt Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might be wondering what *Prompt Engineering* is. It is possible to [fine-tune](https://platform.openai.com/docs/guides/fine-tuning)
    GPT-3 by creating a custom model trained on the documents you would like to analyze.
    However, besides costs for training we would also need a lot of high-quality examples,
    ideally vetted by human experts (according to the [documentation](https://platform.openai.com/docs/guides/fine-tuning/general-best-practices)).
  prefs: []
  type: TYPE_NORMAL
- en: This would be overkill for just analyzing our documents or transcripts. So instead
    of training or fine-tuning a model, we pass the text (commonly referred to as
    prompt) that we would like to analyze to it. Producing or creating such high quality
    prompts is called *Prompt Engineering*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: A good article for further reading about Prompt Engineering can be
    found [here](https://docs.cohere.ai/docs/prompt-engineering)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on your use case, `langchain` offers you several “[loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)”
    like `Facebook Chat`, `PDF`, or `DirectoryLoader` to load or read your (unstructured)
    text (files). The package also comes with a `YoutubeLoader` to transcribe youtube
    videos.
  prefs: []
  type: TYPE_NORMAL
- en: The following examples focus on the `DirectoryLoader` and `YoutubeLoader`.
  prefs: []
  type: TYPE_NORMAL
- en: Read text files with DirectoryLoader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `DirectoryLoader` takes as a first argument the **path** and as a second
    a **pattern** to find the documents or document types we are looking for. In our
    case we would load all text files (*.txt*) in the same directory as the script.
    The `load_and_split` function then initiates the loading.
  prefs: []
  type: TYPE_NORMAL
- en: Even though we might only load one text document, it makes sense to do a splitting
    in case we have a large file and to avoid a `NotEnoughElementsException` (minimum
    four documents are needed). More Information can be found [here](https://github.com/hwchase17/langchain/issues/1793#issuecomment-1480765690).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Transcribe youtube videos with YoutubeLoader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangChain comes with a YoutubeLoader module, which makes use of the `youtube_transcript_api`
    [package](https://pypi.org/project/youtube-transcript-api/). This module gathers
    the (generated) subtitles for a given video.
  prefs: []
  type: TYPE_NORMAL
- en: Not every video comes with its own subtitles. In these cases auto-generated
    subtitles are available. However, in some cases they have a bad quality. In these
    cases the usage of [Whisper](/transcribe-audio-files-with-openais-whisper-e973ae348aa7)
    to transcribe audio files could be an alternative.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The code below takes the **video id** and a **language** (default: en) as parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Before we continue…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In case you decide to go with **transcribed youtube videos**, consider a **proper
    cleaning** of, e.g., Latin1 characters (*\xa0*) **first**. I experienced in the
    *Question-Answering* part differences in the answers depending on which format
    of the same source I used.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs like GPT can only handle a certain [amount of tokens](https://platform.openai.com/docs/models/gpt-3-5).
    These limitations are important when working with large(r) documents. In general,
    there are three ways of dealing with these limitations. One is to make use of
    embeddings or `vector space engine`. A second way is to try out different chaining
    methods like `map-reduce` or `refine`. And a third one is a combination of both.
  prefs: []
  type: TYPE_NORMAL
- en: 'A great article that provides more details about the different chaining methods
    and the use of a vector space engine can be found [here](https://dagster.io/blog/chatgpt-langchain).
    Also keep in mind: The more tokens you use, the more you get charged.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the following we combine `embeddings` with the chaining method `stuff` which
    “stuffs” all documents in one single prompt.
  prefs: []
  type: TYPE_NORMAL
- en: First we ingest our transcript ( `docs`) into a vector space by using `OpenAIEmbeddings`.
    The embeddings are then stored in an in-memory embeddings database called [Chroma](https://github.com/chroma-core/chroma).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: After that, we define the **model_name** we would like to use to analyze our
    data. In this case we choose `gpt-3.5-turbo`. A full list of available models
    can be found [here](https://platform.openai.com/docs/models/gpt-3-5). The **temperature**
    parameter defines the sampling temperature. Higher values lead to more random
    outputs, while lower values will make the answers more focused and deterministic.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least we use the`RetrievalQA` (**Q**uestion/**A**nswer) [Retriever](https://blog.langchain.dev/retrieval/)
    and set the respective parameters (`llm`, `chain_type` , `retriever`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Asking questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we are ready to ask the model questions about our documents. The code below
    shows how to define the query.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: What do to with incomplete answers?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases you might experience incomplete answers. The answer text just
    stops after a few words.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for an incomplete answer is most likely the token limitation. If
    the provided prompt is quite long, the model does not have that many tokens left
    to give an (full) answer. One way of handling this could be to switch to a different
    **chain-type** like `refine`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: However, I experienced that when using a different`chain_type`than *stuff* ,
    I get less concrete results. Another way of handling these issues is to rephrase
    the question and make it more concrete.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thanks to the LangChain package, one needs only a few lines of code to analyze
    LLMs like GPT text documents or transcripts. Since the package is relatively new,
    I expect many updates and code changes soon. That might affect the provided code
    snippets in this article.
  prefs: []
  type: TYPE_NORMAL
- en: In case you think about using LLM in your daily work or for a larger private
    project, you should focus on cleaning the data properly, optimizing the number
    of used tokens, and using best practices like setting budget limits or alarms.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed reading this article. A colab notebook with the source code
    can be found [here](https://colab.research.google.com/drive/1oG6TXgXJTd8qyCj_ncD0Uy1pGEDMlly3?usp=sharing).
  prefs: []
  type: TYPE_NORMAL
- en: Sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://www.pinecone.io/learn/langchain-intro/?source=post_page-----c640a266ad52--------------------------------)
    [## LangChain: Introduction and Getting Started | Pinecone'
  prefs: []
  type: TYPE_NORMAL
- en: arge Language M odels (LLMs) entered the world stage with the release of OpenAI's
    GPT-3 in 2020 [1]. Since then…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.pinecone.io](https://www.pinecone.io/learn/langchain-intro/?source=post_page-----c640a266ad52--------------------------------)
    [](https://dagster.io/blog/chatgpt-langchain?source=post_page-----c640a266ad52--------------------------------)
    [## Build a GitHub Support Bot with GPT3, LangChain, and Python | Dagster Blog
  prefs: []
  type: TYPE_NORMAL
- en: January 9, 2023 * 13 minute read * ChatGPT, ever heard of it? ChatGPT came out
    a few months ago and blew everyones'…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: dagster.io](https://dagster.io/blog/chatgpt-langchain?source=post_page-----c640a266ad52--------------------------------)  [##
    Prompt Engineering
  prefs: []
  type: TYPE_NORMAL
- en: Here, we discuss a few principles and techniques for writing prompts (inputs
    for our models) that will help you get the…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: docs.cohere.ai](https://docs.cohere.ai/docs/prompt-engineering?source=post_page-----c640a266ad52--------------------------------)
    [](https://blog.langchain.dev/retrieval/?source=post_page-----c640a266ad52--------------------------------)
    [## Retrieval
  prefs: []
  type: TYPE_NORMAL
- en: 'TL;DR: We are adjusting our abstractions to make it easy for other retrieval
    methods besides the LangChain VectorDB…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: blog.langchain.dev](https://blog.langchain.dev/retrieval/?source=post_page-----c640a266ad52--------------------------------)
  prefs: []
  type: TYPE_NORMAL
