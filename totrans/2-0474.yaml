- en: Catch Up On Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/catch-up-on-large-language-models-8daf784f46f8](https://towardsdatascience.com/catch-up-on-large-language-models-8daf784f46f8)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical guide to large language models without the hype
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcopeixeiro?source=post_page-----8daf784f46f8--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----8daf784f46f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8daf784f46f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8daf784f46f8--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----8daf784f46f8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8daf784f46f8--------------------------------)
    Â·15 min readÂ·Sep 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/adb61491bc3d90e8c3f4676819450577.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Gary Bendig](https://unsplash.com/@kris_ricepees?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: If you are here, it means that like me you were overwhelmed by the constant
    flow of information, and hype posts surrounding **large language models** (LLMs).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: This article is my attempt at helping you catch up on the subject of large language
    models without the hype. After all, it is a transformative technology, and I believe
    it is important for us to understand it, hopefully making you curious to learn
    even more and build something with it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will define what LLMs are and how they work, of
    course covering the Transformer architecture. We also explore the different methods
    of training LLMs and conclude the article with a hands-on project where we use
    Flan-T5 for sentiment analysis using Python.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s get started!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs and generative AI: are they the same thing?'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative AI is a subset of machine learning that focuses on models whoâ€™s
    primary function is to generate *something*: text, images, video, code, etc.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Generative models train on enormous amounts of data created by humans to learn
    patterns and structure which allow them to create new data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of generative models include:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '**Image generation**: DALL-E, Midjourney'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code generation**: OpenAI Codex'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text generation**: GPT-3, Flan-T5, LLaMA'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large language models are part of the generative AI landscape, since they take
    an input text and repeatedly predict the next word until the output is complete.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: However, as language models grew larger, they were able to perform other tasks
    in natural language processing, like summarization, sentiment analysis, named
    entity recognition, translation and more.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, letâ€™s now focus our attention on how LLMs work.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: How LLMs work
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the reasons why we now have large language models is because of the seminal
    work of Google and University of Toronto when they released the paper [Attention
    Is All You Need](https://arxiv.org/abs/1706.03762) in 2017.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: This paper introduced the **Transformer** architecture, which is behind the
    LLMs we know and use today.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: This architecture unlocked large scale models, making it possible to train very
    large models on multiple GPUs, and the models are able to process the inputs in
    parallel, giving them the opportunity to treat very large sequences of data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the Transformer architecture
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following is meant to be a high-level overview of the Transformer architecture.
    There are many resources that dive deeper into it, but the goal here is just to
    understand the way it works so we can understand how different LLMs specialize
    in different tasks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: At any time, for more details, I suggest you read the [original paper](https://arxiv.org/pdf/1706.03762.pdf).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: So, letâ€™s start with a simplified visualization of the Transformer architecture.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8b3a35183236257fa4fe5247b56a9a5.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: A simplified visualization of the Transformer architecture. Image by the author.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can see that the main components of the Transformer
    are the encoder and decoder. Inside each, we also find the **attention** component.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s explore each component in more detail to understand how the Transformer
    architecture works.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Tokenize the inputs
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We know LLMs work with text, but computers work with numbers, not letters. Therefore,
    the input must be *tokenized*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization is the process in which words of a sentence are represented as
    numbers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Basically, every possible word a model can work with is in a dictionary with
    a number associated to it. With tokenization, we can retrieve the number associated
    with the word, to represent a sentence as a sequence of numbers, as shown below.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba7e371a044bec54d4a3dce16126b05a.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Example of tokenization. The sentence is tokenized and then sent to the embedding
    of the Transformer. Image by the author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, we see an example of how the sentence â€œIt rained this morningâ€
    can be tokenized before being set to the embedding layer of the Transformer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Note that there are many ways of tokenizing a sentence. In the example above,
    the tokenizer can represents parts of a word, which is why *rained* is separated
    into *rain* and *ed*. Other tokenizers would have a number for full words only.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: The word embedding layer
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, we have a series of numbers that represent words, but how can
    the computer understand their meaning?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: This is achieved by the word embedding layers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding is a learned representation of words, such that words with a
    similar meaning will have a similar representation. The model will learn different
    properties of words and represent them in a fixed space, where each axis can represent
    the property of a word.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a404c0f4ae58847ff43d149d786629d.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Visualization of word embeddings. We can see that â€œmorningâ€ and â€œsunriseâ€ have
    a similar representation since the angle in the 3D space is smaller. Similarly,
    â€œrainâ€ and â€œthunderâ€ are closer to one another. Image by the author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¯åµŒå…¥çš„å¯è§†åŒ–ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°â€œmorningâ€å’Œâ€œsunriseâ€æœ‰ç›¸ä¼¼çš„è¡¨ç¤ºï¼Œå› ä¸ºå®ƒä»¬åœ¨3Dç©ºé—´ä¸­çš„è§’åº¦è¾ƒå°ã€‚åŒæ ·ï¼Œâ€œrainâ€å’Œâ€œthunderâ€å½¼æ­¤æ›´è¿‘ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: In the figure above, we can see how a 3D word embedding can look like. We see
    that â€œmorningâ€ and â€œsunriseâ€ are closer to one another, and therefore have a similar
    representation. This can be can be computed using cosine similarity.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°3Dè¯åµŒå…¥çš„æ ·å­ã€‚æˆ‘ä»¬çœ‹åˆ°â€œmorningâ€å’Œâ€œsunriseâ€å½¼æ­¤æ›´è¿‘ï¼Œå› æ­¤å…·æœ‰ç›¸ä¼¼çš„è¡¨ç¤ºã€‚è¿™å¯ä»¥é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¾—å‡ºã€‚
- en: On the other hand, â€œrainâ€ and â€œthunderâ€ are close to each other, and far from
    â€œmorningâ€ and â€œsunriseâ€.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œâ€œrainâ€å’Œâ€œthunderâ€å½¼æ­¤è¾ƒè¿‘ï¼Œè€Œä¸â€œmorningâ€å’Œâ€œsunriseâ€ç›¸è·è¾ƒè¿œã€‚
- en: Now, we can only show a 3D space, but in reality, embeddings can have hundreds
    of dimensions. In fact, the original Transformer architecture used an embedding
    space of 512 dimensions. This means that the model could learn 512 different properties
    of words to represent them in a space of 512 dimensions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬åªèƒ½å±•ç¤ºä¸€ä¸ª3Dç©ºé—´ï¼Œä½†å®é™…ä¸Šï¼ŒåµŒå…¥å¯ä»¥æœ‰æ•°ç™¾ä¸ªç»´åº¦ã€‚äº‹å®ä¸Šï¼ŒåŸå§‹çš„Transformeræ¶æ„ä½¿ç”¨äº†512ç»´çš„åµŒå…¥ç©ºé—´ã€‚è¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥å­¦ä¹ 512ä¸ªä¸åŒçš„è¯å±æ€§ï¼Œå°†å®ƒä»¬è¡¨ç¤ºåœ¨ä¸€ä¸ª512ç»´çš„ç©ºé—´ä¸­ã€‚
- en: What about word order?
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè¯åºå‘¢ï¼Ÿ
- en: You may have noticed that by representing words in embeddings, we lose their
    order in the sentence.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œé€šè¿‡è¡¨ç¤ºè¯çš„åµŒå…¥ï¼Œæˆ‘ä»¬ä¼šä¸§å¤±å®ƒä»¬åœ¨å¥å­ä¸­çš„é¡ºåºã€‚
- en: Of course, with natural language, word order is very important, and so thatâ€™s
    why we use positional encoding, so the model knows the order of the words in a
    sentence.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œåœ¨è‡ªç„¶è¯­è¨€ä¸­ï¼Œè¯åºéå¸¸é‡è¦ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨ä½ç½®ç¼–ç ï¼Œä»¥ä¾¿æ¨¡å‹äº†è§£å¥å­ä¸­å•è¯çš„é¡ºåºã€‚
- en: It is the combination of the word embeddings and the positional encoding that
    gets sent to the encoder.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯å°†è¯åµŒå…¥å’Œä½ç½®ç¼–ç ç»“åˆåœ¨ä¸€èµ·å¹¶å‘é€ç»™ç¼–ç å™¨ã€‚
- en: Inside the encoder
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨ç¼–ç å™¨å†…éƒ¨
- en: Our inputs travel inside the encoder where they will go through the **self-attention**
    mechanism.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è¾“å…¥åœ¨ç¼–ç å™¨å†…éƒ¨ä¼ é€’ï¼Œåœ¨é‚£é‡Œå®ƒä»¬ä¼šç»è¿‡**è‡ªæ³¨æ„åŠ›**æœºåˆ¶ã€‚
- en: This is where the model can learn the dependencies between each token in a sentence.
    It learns the importance of each word in relation to all other words in a sentence.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æ¨¡å‹å¯ä»¥å­¦ä¹ å¥å­ä¸­æ¯ä¸ªæ ‡è®°ä¹‹é—´ä¾èµ–å…³ç³»çš„åœ°æ–¹ã€‚å®ƒå­¦ä¹ äº†æ¯ä¸ªè¯ç›¸å¯¹äºå¥å­ä¸­æ‰€æœ‰å…¶ä»–è¯çš„é‡è¦æ€§ã€‚
- en: '![](../Images/f6c0661476ae1f98f0b96aa065648efa.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6c0661476ae1f98f0b96aa065648efa.png)'
- en: Example of an attention map for the word â€œrainedâ€. The stroke width is representative
    of the importance. Here, we can see that â€œrainedâ€ is strongly connected to â€œthisâ€
    and â€œmorningâ€. Image by the author.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å•è¯â€œrainedâ€çš„æ³¨æ„åŠ›å›¾ç¤ºä¾‹ã€‚ç¬”åˆ’å®½åº¦ä»£è¡¨é‡è¦æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°â€œrainedâ€ä¸â€œthisâ€å’Œâ€œmorningâ€ç´§å¯†è¿æ¥ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: In the figure above, we have a stylized example of an attention map for the
    word â€œrainedâ€. The stroke width represents the importance.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å•è¯â€œrainedâ€çš„æ³¨æ„åŠ›å›¾çš„é£æ ¼åŒ–ç¤ºä¾‹ã€‚ç¬”åˆ’å®½åº¦è¡¨ç¤ºé‡è¦æ€§ã€‚
- en: In this example, we can see that self-attention captures the importance of â€œrainedâ€
    with â€œthisâ€ and â€œmorningâ€, meaning that it understands the context of this sentence.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è‡ªæ³¨æ„åŠ›æ•æ‰äº†â€œrainedâ€ä¸â€œthisâ€å’Œâ€œmorningâ€çš„é‡è¦æ€§ï¼Œè¿™æ„å‘³ç€å®ƒç†è§£äº†è¿™ä¸ªå¥å­çš„ä¸Šä¸‹æ–‡ã€‚
- en: While this example remains simple, since we have a very short sentence, the
    self-attention mechanism works very well on longer sentences, effectively capturing
    context and the overall meaning of a sentence.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™ä¸ªä¾‹å­å¾ˆç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬åªæœ‰ä¸€ä¸ªéå¸¸çŸ­çš„å¥å­ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨è¾ƒé•¿çš„å¥å­ä¸­æ•ˆæœå¾ˆå¥½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰ä¸Šä¸‹æ–‡å’Œå¥å­çš„æ•´ä½“å«ä¹‰ã€‚
- en: Furthermore, the model does not have a single attention head. In fact, it has
    multiple attention heads, also called *multi-headed self-attention*, where each
    head can learn a different aspect of language.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæ¨¡å‹å¹¶æ²¡æœ‰ä¸€ä¸ªå•ä¸€çš„æ³¨æ„åŠ›å¤´ã€‚äº‹å®ä¸Šï¼Œå®ƒæœ‰å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œä¹Ÿç§°ä¸º*å¤šå¤´è‡ªæ³¨æ„åŠ›*ï¼Œæ¯ä¸ªå¤´éƒ¨å¯ä»¥å­¦ä¹ è¯­è¨€çš„ä¸åŒæ–¹é¢ã€‚
- en: For example, in the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf),
    the authors found that one head was involved in *anaphora resolution*, which is
    identifying the link between an entity and its repeated references.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨è®ºæ–‡[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)ä¸­ï¼Œä½œè€…å‘ç°ä¸€ä¸ªå¤´éƒ¨æ¶‰åŠåˆ°*æŒ‡ä»£æ¶ˆè§£*ï¼Œå³è¯†åˆ«å®ä½“ä¸å…¶é‡å¤å¼•ç”¨ä¹‹é—´çš„è”ç³»ã€‚
- en: '![](../Images/0027b908a4028d654238ec0de6e4052f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0027b908a4028d654238ec0de6e4052f.png)'
- en: Example of anaphora resolution. Here, the word â€œkeysâ€ is referenced again in
    the sentence as â€œtheyâ€. Image by the author.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‡ä»£æ¶ˆè§£çš„ä¾‹å­ã€‚åœ¨è¿™é‡Œï¼Œå•è¯â€œkeysâ€åœ¨å¥å­ä¸­å†æ¬¡è¢«å¼•ç”¨ä¸ºâ€œtheyâ€ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Above, we see an example of anaphora resolution, where the word â€œkeysâ€ is later
    referenced as â€œtheyâ€, and so one attention head can specialize in identifying
    those links.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢ï¼Œæˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªæŒ‡ä»£è§£æçš„ç¤ºä¾‹ï¼Œå…¶ä¸­å•è¯â€œkeysâ€åæ¥è¢«æåŠä¸ºâ€œtheyâ€ï¼Œå› æ­¤ä¸€ä¸ªæ³¨æ„åŠ›å¤´å¯ä»¥ä¸“é—¨è¯†åˆ«è¿™äº›é“¾æ¥ã€‚
- en: Note that we do not decide what aspect of language each attention head will
    learn.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬å¹¶æœªå†³å®šæ¯ä¸ªæ³¨æ„åŠ›å¤´å°†å­¦ä¹ è¯­è¨€çš„å“ªä¸ªæ–¹é¢ã€‚
- en: At this point, the model has a deep representation of the structure of meaning
    of a sentence. This is sent to the decoder.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œæ¨¡å‹å·²ç»å¯¹å¥å­çš„æ„ä¹‰ç»“æ„æœ‰äº†æ·±å±‚æ¬¡çš„è¡¨ç¤ºã€‚è¿™è¢«å‘é€åˆ°è§£ç å™¨ã€‚
- en: Inside the decoder
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£ç å™¨å†…éƒ¨
- en: The decoder accepts a deep representation of the input tokens. This informs
    the self-attention mechanism inside the decoder.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç å™¨æ¥å—è¾“å…¥ä»¤ç‰Œçš„æ·±å±‚è¡¨ç¤ºã€‚è¿™ä¸ºè§£ç å™¨å†…éƒ¨çš„è‡ªæ³¨æ„æœºåˆ¶æä¾›ä¿¡æ¯ã€‚
- en: As a reminder, here is the Transformer architecture again, so we can remember
    what it looks like.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæé†’ï¼Œè¿™é‡Œå†æ¬¡å±•ç¤ºäº†Transformeræ¶æ„ï¼Œä»¥ä¾¿æˆ‘ä»¬è®°ä½å®ƒçš„æ ·å­ã€‚
- en: '![](../Images/f8b3a35183236257fa4fe5247b56a9a5.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8b3a35183236257fa4fe5247b56a9a5.png)'
- en: A simplified visualization of the Transformer architecture. Image by the author.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Transformeræ¶æ„çš„ç®€åŒ–å¯è§†åŒ–ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: A **start-of-sequence** token is inserted as an input of the decoder, to signal
    it to start generating new tokens.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**åºåˆ—å¼€å§‹**ä»¤ç‰Œè¢«æ’å…¥ä½œä¸ºè§£ç å™¨çš„è¾“å…¥ï¼Œä»¥æŒ‡ç¤ºå…¶å¼€å§‹ç”Ÿæˆæ–°ä»¤ç‰Œã€‚'
- en: New tokens are generated according to the understanding of the input sequence
    generated by the encoder and its self-attention mechanism.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ–°ä»¤ç‰Œæ˜¯æ ¹æ®ç¼–ç å™¨ç”Ÿæˆçš„è¾“å…¥åºåˆ—çš„ç†è§£åŠå…¶è‡ªæ³¨æ„æœºåˆ¶ç”Ÿæˆçš„ã€‚
- en: In the figure above, we can see that the output of the decoder gets sent to
    a softmax layer. This generates a vector of probabilities for each possible token.
    The one with the largest probability is then output by the model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è§£ç å™¨çš„è¾“å‡ºè¢«é€åˆ°ä¸€ä¸ªsoftmaxå±‚ã€‚è¿™ç”Ÿæˆäº†æ¯ä¸ªå¯èƒ½ä»¤ç‰Œçš„æ¦‚ç‡å‘é‡ã€‚å…·æœ‰æœ€å¤§æ¦‚ç‡çš„ä»¤ç‰Œéšåç”±æ¨¡å‹è¾“å‡ºã€‚
- en: That output token is then sent back to the embeddings as an input to the decoder,
    until an **end-of-sequence** token is generated by the model. At that point, the
    output sequence is complete.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è¾“å‡ºä»¤ç‰Œéšåè¢«é€å›åµŒå…¥å±‚ä½œä¸ºè§£ç å™¨çš„è¾“å…¥ï¼Œç›´åˆ°æ¨¡å‹ç”Ÿæˆ**åºåˆ—ç»“æŸ**ä»¤ç‰Œã€‚æ­¤æ—¶ï¼Œè¾“å‡ºåºåˆ—å®Œæˆã€‚
- en: This concludes the basic architecture behind large language models. With the
    Transformer architecture and its ability to process data in parallel, it was possible
    to train models on huge amounts of data, making LLMs a reality.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ€»ç»“äº†å¤§å‹è¯­è¨€æ¨¡å‹èƒŒåçš„åŸºæœ¬æ¶æ„ã€‚é€šè¿‡Transformeræ¶æ„åŠå…¶å¹¶è¡Œå¤„ç†æ•°æ®çš„èƒ½åŠ›ï¼Œä½¿å¾—åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œä½¿LLMsæˆä¸ºç°å®ã€‚
- en: Now, there is more to this, as LLMs do not all use the full Transformer architecture,
    and that influences the way they are trained. Letâ€™s explore this in more detail.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæƒ…å†µæ›´å¤æ‚ï¼Œå› ä¸ºLLMså¹¶ééƒ½ä½¿ç”¨å®Œæ•´çš„Transformeræ¶æ„ï¼Œè¿™å½±å“äº†å®ƒä»¬çš„è®­ç»ƒæ–¹å¼ã€‚è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°æ¢è®¨è¿™ä¸€ç‚¹ã€‚
- en: How LLMs are trained
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMçš„è®­ç»ƒæ–¹å¼
- en: We have seen the underlying mechanisms that power large language models, and
    as mentioned, not all models use the full Transformer architecture.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†æ”¯æ’‘å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºæœ¬æœºåˆ¶ï¼Œå¦‚å‰æ‰€è¿°ï¼Œå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨å®Œæ•´çš„Transformeræ¶æ„ã€‚
- en: In fact, some models may use the encoder portion only, while others use the
    decoder portion only.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œä¸€äº›æ¨¡å‹å¯èƒ½åªä½¿ç”¨ç¼–ç å™¨éƒ¨åˆ†ï¼Œè€Œå…¶ä»–æ¨¡å‹åªä½¿ç”¨è§£ç å™¨éƒ¨åˆ†ã€‚
- en: This means that the models are also trained differently and will therefore specialize
    in particular tasks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æ¨¡å‹çš„è®­ç»ƒæ–¹å¼ä¹Ÿä¸åŒï¼Œå› æ­¤ä¼šä¸“æ³¨äºç‰¹å®šä»»åŠ¡ã€‚
- en: Encoder-only models
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»…ç¼–ç å™¨æ¨¡å‹
- en: Encoder-only models, also called **autoencoding** models are best suited for
    tasks like sentiment analysis, named entity recognition, and word classification
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…ç¼–ç å™¨æ¨¡å‹ï¼Œä¹Ÿç§°ä¸º**è‡ªç¼–ç **æ¨¡å‹ï¼Œæœ€é€‚åˆç”¨äºæƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«å’Œè¯æ±‡åˆ†ç±»ç­‰ä»»åŠ¡ã€‚
- en: Popular examples of autoencoding models are BERT and ROBERTA.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç æ¨¡å‹çš„æµè¡Œç¤ºä¾‹æœ‰BERTå’ŒROBERTAã€‚
- en: Those models are trained using **masked language modeling** (MLM). With that
    training method, words in an input sentence are randomly masked, and the objective
    of the model is then to reconstruct the original text.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¨¡å‹ä½¿ç”¨**æ©ç è¯­è¨€å»ºæ¨¡**ï¼ˆMLMï¼‰è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡è¿™ç§è®­ç»ƒæ–¹æ³•ï¼Œè¾“å…¥å¥å­ä¸­çš„å•è¯ä¼šè¢«éšæœºæ©ç›–ï¼Œæ¨¡å‹çš„ç›®æ ‡æ˜¯é‡å»ºåŸå§‹æ–‡æœ¬ã€‚
- en: '![](../Images/7593549a23d59ccb9ddcef964b473e80.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7593549a23d59ccb9ddcef964b473e80.png)'
- en: Illustrating masked language modeling (MLM) for autoencoding models. Here, a
    random word was masked in the input sentence, and the model must reconstruct the
    original sentence. Image by the author.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´æ˜äº†ç”¨äºè‡ªç¼–ç æ¨¡å‹çš„æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ã€‚åœ¨è¿™é‡Œï¼Œè¾“å…¥å¥å­ä¸­çš„ä¸€ä¸ªéšæœºå•è¯è¢«æ©ç›–ï¼Œæ¨¡å‹å¿…é¡»é‡å»ºåŸå§‹å¥å­ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: In the figure above, we can see what masked language modeling looks like. A
    word is hidden and the sentence is fed to the model, which must then learn to
    predict the right word to get the correct original sentence.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ©è”½è¯­è¨€å»ºæ¨¡çš„æ ·å­ã€‚ä¸€ä¸ªè¯è¢«éšè—ï¼Œå¥å­è¢«è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œæ¨¡å‹å¿…é¡»å­¦ä¹ é¢„æµ‹æ­£ç¡®çš„è¯ä»¥å¾—åˆ°æ­£ç¡®çš„åŸå§‹å¥å­ã€‚
- en: With that method, autoencoding models develop **bidrectional context**, since
    they see what precedes and follows the token they must predict, and not just what
    comes before.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¯¥æ–¹æ³•ï¼Œè‡ªç¼–ç æ¨¡å‹å‘å±•äº†**åŒå‘ä¸Šä¸‹æ–‡**ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥çœ‹åˆ°éœ€è¦é¢„æµ‹çš„æ ‡è®°å‰åçš„å†…å®¹ï¼Œè€Œä¸ä»…ä»…æ˜¯å‰é¢çš„å†…å®¹ã€‚
- en: Again, in the figure above, the model sees â€œit rainedâ€ and â€œmorningâ€, so it
    sees both the beginning and the end of the sentence, allowing it to predict the
    word â€œthisâ€ to reconstruct the sentence correctly.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæ¨¡å‹çœ‹åˆ°â€œit rainedâ€å’Œâ€œmorningâ€ï¼Œå› æ­¤å®ƒçœ‹åˆ°å¥å­çš„å¼€å¤´å’Œç»“å°¾ï¼Œè¿™ä½¿å¾—å®ƒèƒ½å¤Ÿé¢„æµ‹â€œthisâ€è¿™ä¸ªè¯ï¼Œä»è€Œæ­£ç¡®é‡æ„å¥å­ã€‚
- en: Note that with autoencoding models, the input and output sequences have the
    same length.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå¯¹äºè‡ªç¼–ç æ¨¡å‹ï¼Œè¾“å…¥å’Œè¾“å‡ºåºåˆ—çš„é•¿åº¦æ˜¯ç›¸åŒçš„ã€‚
- en: Decoder-only models
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»…è§£ç å™¨æ¨¡å‹
- en: Decoder-only models are also called *autoregressive* models. These models are
    best suited for text generation, but new functions arise when the models get very
    large.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…è§£ç å™¨æ¨¡å‹ä¹Ÿç§°ä¸º*è‡ªå›å½’*æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹æœ€é€‚åˆæ–‡æœ¬ç”Ÿæˆï¼Œä½†å½“æ¨¡å‹å˜å¾—éå¸¸å¤§æ—¶ï¼Œæ–°çš„åŠŸèƒ½å°±ä¼šå‡ºç°ã€‚
- en: Example of autoregressive models are GPT and BLOOM.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªå›å½’æ¨¡å‹çš„ä¾‹å­æœ‰GPTå’ŒBLOOMã€‚
- en: These models are trained using **causal language modeling** (CLM). With causal
    language modeling, the model only sees the tokens preceding the mask, meaning
    that it does not see the end of the sequence.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¨¡å‹ä½¿ç”¨**å› æœè¯­è¨€å»ºæ¨¡**ï¼ˆCLMï¼‰è¿›è¡Œè®­ç»ƒã€‚ä½¿ç”¨å› æœè¯­è¨€å»ºæ¨¡æ—¶ï¼Œæ¨¡å‹åªçœ‹åˆ°æ©è”½ä¹‹å‰çš„æ ‡è®°ï¼Œè¿™æ„å‘³ç€å®ƒçœ‹ä¸åˆ°åºåˆ—çš„ç»“å°¾ã€‚
- en: '![](../Images/dd4503e7d11be727715a89d105e4629a.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd4503e7d11be727715a89d105e4629a.png)'
- en: Illustrating causal language modeling. Here, the model only sees the tokens
    leading to the mask. Then, it must infer the next tokens until the sentence is
    complete. Image by the author.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´æ˜å› æœè¯­è¨€å»ºæ¨¡ã€‚åœ¨è¿™é‡Œï¼Œæ¨¡å‹åªçœ‹åˆ°å¯¼è‡´æ©è”½çš„æ ‡è®°ã€‚ç„¶åï¼Œå®ƒå¿…é¡»æ¨æ–­ä¸‹ä¸€ä¸ªæ ‡è®°ç›´åˆ°å¥å­å®Œæ•´ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: As we see above, with causal language modeling, the model only sees the tokens
    leading to the mask, and not what comes after. Then, it must predict the next
    tokens until the sentence is complete.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šæ‰€ç¤ºï¼Œä½¿ç”¨å› æœè¯­è¨€å»ºæ¨¡æ—¶ï¼Œæ¨¡å‹åªçœ‹åˆ°å¯¼è‡´æ©è”½çš„æ ‡è®°ï¼Œè€Œçœ‹ä¸åˆ°æ©è”½ä¹‹åçš„å†…å®¹ã€‚ç„¶åï¼Œå®ƒå¿…é¡»é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ç›´åˆ°å¥å­å®Œæ•´ã€‚
- en: In the example above, the model would output â€œthisâ€, and that token would be
    fed back as an input, so the model can then predict â€œmorningâ€.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæ¨¡å‹ä¼šè¾“å‡ºâ€œthisâ€ï¼Œç„¶åè¯¥æ ‡è®°ä¼šè¢«åé¦ˆä½œä¸ºè¾“å…¥ï¼Œå› æ­¤æ¨¡å‹å¯ä»¥é¢„æµ‹â€œmorningâ€ã€‚
- en: Unlike masked language modeling, model build unidirectional context, since they
    do not see what comes after the mask.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ©è”½è¯­è¨€å»ºæ¨¡ä¸åŒï¼Œæ¨¡å‹å»ºç«‹äº†å•å‘ä¸Šä¸‹æ–‡ï¼Œå› ä¸ºå®ƒä»¬çœ‹ä¸åˆ°æ©è”½ä¹‹åçš„å†…å®¹ã€‚
- en: Of course, with decoder-only models, the output sequence can have a different
    length than the input sequence.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå¯¹äºä»…è§£ç å™¨æ¨¡å‹ï¼Œè¾“å‡ºåºåˆ—çš„é•¿åº¦å¯èƒ½ä¸è¾“å…¥åºåˆ—çš„é•¿åº¦ä¸åŒã€‚
- en: Encoder-decoder models
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹
- en: Encoder-decoder models are also called *sequence-to-sequence* models, and they
    use the full Transformer architecture.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ä¹Ÿç§°ä¸º*åºåˆ—åˆ°åºåˆ—*æ¨¡å‹ï¼Œå¹¶ä¸”å®ƒä»¬ä½¿ç”¨å®Œæ•´çš„Transformeræ¶æ„ã€‚
- en: Those models are often used for translation, text summarization and question
    answering.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¨¡å‹é€šå¸¸ç”¨äºç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦å’Œé—®ç­”ã€‚
- en: Popular examples of sequence-to-sequence models are T5 and BART.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æµè¡Œçš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„ä¾‹å­æœ‰T5å’ŒBARTã€‚
- en: To train these models, the **span corruption** method is used. Here, a random
    sequence of tokens is masked and designated as a *sentinel* token. Then, the model
    must reconstruct the masked sequence autoregressively.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®­ç»ƒè¿™äº›æ¨¡å‹ï¼Œä½¿ç”¨äº†**è·¨åº¦ç ´å**æ–¹æ³•ã€‚åœ¨è¿™é‡Œï¼Œä¸€ä¸ªéšæœºçš„æ ‡è®°åºåˆ—è¢«æ©è”½å¹¶æŒ‡å®šä¸º*å“¨å…µ*æ ‡è®°ã€‚ç„¶åï¼Œæ¨¡å‹å¿…é¡»è‡ªå›å½’åœ°é‡æ„è¢«æ©è”½çš„åºåˆ—ã€‚
- en: '![](../Images/aa279015ee0385588f31e1a9586423cf.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa279015ee0385588f31e1a9586423cf.png)'
- en: Illustration of span corruption. Here, a sequence of tokens is masked and replaced
    by a sentinel token. The model must then reconstructed the masked sequence autoregressively.
    Image by the author.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è·¨åº¦ç ´åçš„è¯´æ˜ã€‚åœ¨è¿™é‡Œï¼Œä¸€ç³»åˆ—æ ‡è®°è¢«æ©è”½å¹¶ç”¨å“¨å…µæ ‡è®°æ›¿ä»£ã€‚ç„¶åï¼Œæ¨¡å‹å¿…é¡»è‡ªå›å½’åœ°é‡æ„è¢«æ©è”½çš„åºåˆ—ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: In the figure above, we can see that a sequence of two tokens were masked and
    replaced by a sentinel token. The model is then trained to reconstruct the sentinel
    token to obtain the original sentence.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªæ ‡è®°çš„åºåˆ—è¢«æ©è”½å¹¶ç”¨å“¨å…µæ ‡è®°æ›¿ä»£ã€‚ç„¶åï¼Œæ¨¡å‹è¢«è®­ç»ƒä»¥é‡æ„å“¨å…µæ ‡è®°ä»¥è·å¾—åŸå§‹å¥å­ã€‚
- en: Here, the masked input is sent to the encoder, and the decoder is responsible
    for reconstructing the masked sequence.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæ©ç è¾“å…¥è¢«å‘é€åˆ°ç¼–ç å™¨ï¼Œè€Œè§£ç å™¨è´Ÿè´£é‡å»ºæ©ç åºåˆ—ã€‚
- en: A note on model size
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºæ¨¡å‹å¤§å°çš„è¯´æ˜
- en: While we have specified certain tasks for which certain models perform best,
    researchers have observed that large models are capable of various tasks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»¬å·²æŒ‡å®šäº†æŸäº›æ¨¡å‹è¡¨ç°æœ€ä½³çš„ä»»åŠ¡ï¼Œç ”ç©¶äººå‘˜è§‚å¯Ÿåˆ°å¤§å‹æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œå„ç§ä»»åŠ¡ã€‚
- en: Therefore, very large decoder-only models can be very good at translation, even
    though encoder-decoder models specialize in that task.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè™½ç„¶ç¼–ç -è§£ç æ¨¡å‹ä¸“é—¨ç”¨äºç¿»è¯‘ï¼Œä½†éå¸¸å¤§çš„ä»…è§£ç æ¨¡å‹åœ¨ç¿»è¯‘æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚
- en: With all of that in mind, letâ€™s now start working with a large language in Python.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ°è¿™äº›ï¼Œè®©æˆ‘ä»¬ç°åœ¨å¼€å§‹åœ¨ Python ä¸­ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ã€‚
- en: Work with a large language model
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸å¤§å‹è¯­è¨€æ¨¡å‹åˆä½œ
- en: Before we get hands-on experience with a large language model, letâ€™s just cover
    some technical terms involved when working with LLMs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å®é™…æ“ä½œå¤§å‹è¯­è¨€æ¨¡å‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆäº†è§£ä¸€äº›ä¸ LLM ç›¸å…³çš„æŠ€æœ¯æœ¯è¯­ã€‚
- en: First, the text that we feed the LLM is called *prompt*, and the output of the
    model is called *completion*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬æä¾›ç»™ LLM çš„æ–‡æœ¬ç§°ä¸º*æç¤ºï¼ˆpromptï¼‰*ï¼Œæ¨¡å‹çš„è¾“å‡ºç§°ä¸º*å®Œæˆï¼ˆcompletionï¼‰*ã€‚
- en: '![](../Images/a54b6eb231931eccc8e824df03e110e6.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a54b6eb231931eccc8e824df03e110e6.png)'
- en: The prompt is the text we feed to the model with the instructions. The output
    of the model is called completion. Image by the author.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºæ˜¯æˆ‘ä»¬å‘æ¨¡å‹æä¾›çš„åŒ…å«æŒ‡ä»¤çš„æ–‡æœ¬ã€‚æ¨¡å‹çš„è¾“å‡ºç§°ä¸ºå®Œæˆã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Inside the prompt is where we give the instructions to the LLM to achieve the
    task that we want.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æç¤ºä¸­ï¼Œæˆ‘ä»¬å‘ LLM æä¾›æŒ‡ä»¤ï¼Œä»¥å®ç°æˆ‘ä»¬å¸Œæœ›å®Œæˆçš„ä»»åŠ¡ã€‚
- en: This is also where **prompt engineering** is performed. With prompt engineering,
    we can perform **in-context learning**, which is when we give examples to the
    model of how certain tasks should be performed. We will see an example of that
    later on.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿæ˜¯è¿›è¡Œ**æç¤ºå·¥ç¨‹**çš„åœ°æ–¹ã€‚é€šè¿‡æç¤ºå·¥ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œ**ä¸Šä¸‹æ–‡å­¦ä¹ **ï¼Œå³å‘æ¨¡å‹æä¾›å¦‚ä½•æ‰§è¡ŒæŸäº›ä»»åŠ¡çš„ç¤ºä¾‹ã€‚ç¨åæˆ‘ä»¬å°†çœ‹åˆ°ä¸€ä¸ªä¾‹å­ã€‚
- en: For now, letâ€™s interact with an LLM using Python for sentiment analysis.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ Python ä¸ LLM è¿›è¡Œæƒ…æ„Ÿåˆ†æçš„äº’åŠ¨ã€‚
- en: 'Hands-on project: sentiment analysis with Flan-T5'
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®è·µé¡¹ç›®ï¼šä½¿ç”¨ Flan-T5 è¿›è¡Œæƒ…æ„Ÿåˆ†æ
- en: For this mini project, we use Flan-T5 for sentiment analysis of various financial
    news.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªè¿·ä½ é¡¹ç›®ï¼Œæˆ‘ä»¬ä½¿ç”¨ Flan-T5 å¯¹å„ç§é‡‘èæ–°é—»è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚
- en: Flan-T5 is an improved version of the T5 model, which is a sequence-to-sequence
    model. Researchers basically took the T5 model and fine-tuned it on different
    tasks covering more languages. For more details, you can refer to the [original
    paper](https://arxiv.org/pdf/2210.11416.pdf).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Flan-T5 æ˜¯ T5 æ¨¡å‹çš„æ”¹è¿›ç‰ˆï¼ŒT5 æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚ç ”ç©¶äººå‘˜åŸºæœ¬ä¸Šå¯¹ T5 æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä½¿å…¶è¦†ç›–æ›´å¤šè¯­è¨€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[åŸå§‹è®ºæ–‡](https://arxiv.org/pdf/2210.11416.pdf)ã€‚
- en: As for the dataset, we will use the *financial_phrasebank* dataset published
    by Pekka Malo and Ankur Sinha under the Creative Commons Attribute license.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ•°æ®é›†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç”± Pekka Malo å’Œ Ankur Sinha åœ¨ Creative Commons å±æ€§è®¸å¯ä¸‹å‘å¸ƒçš„*financial_phrasebank*
    æ•°æ®é›†ã€‚
- en: The dataset contains a total of 4840 sentences from English language financial
    news that were categorized as positive, negative or neutral. A group of five to
    eight annotators classified each sentence, and depending on the agreement rate,
    the size of the dataset will vary (4850 rows for a 50% agreement rate, and 2260
    rows for a 100% agreement rate).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†åŒ…å«æ¥è‡ªè‹±è¯­é‡‘èæ–°é—»çš„å…±4840ä¸ªå¥å­ï¼Œè¿™äº›å¥å­è¢«åˆ†ç±»ä¸ºç§¯æã€æ¶ˆææˆ–ä¸­ç«‹ã€‚äº”åˆ°å…«åæ³¨é‡Šå‘˜å¯¹æ¯ä¸ªå¥å­è¿›è¡Œåˆ†ç±»ï¼Œæ ¹æ®ä¸€è‡´æ€§ç‡ï¼Œæ•°æ®é›†çš„å¤§å°ä¼šæœ‰æ‰€ä¸åŒï¼ˆ50%ä¸€è‡´ç‡ä¸º4850è¡Œï¼Œ100%ä¸€è‡´ç‡ä¸º2260è¡Œï¼‰ã€‚
- en: For more information on the dataset and how it was compiled, refer to the [full
    dataset details page.](https://huggingface.co/datasets/financial_phrasebank)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ•°æ®é›†åŠå…¶ç¼–åˆ¶æ–¹å¼çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[å®Œæ•´æ•°æ®é›†è¯¦æƒ…é¡µé¢ã€‚](https://huggingface.co/datasets/financial_phrasebank)
- en: Of course, all code show below is available on [GitHub](https://github.com/marcopeix/learn_llm/blob/main/1_llm_get_started.ipynb).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä¸‹é¢æ˜¾ç¤ºçš„æ‰€æœ‰ä»£ç éƒ½å¯ä»¥åœ¨[GitHub](https://github.com/marcopeix/learn_llm/blob/main/1_llm_get_started.ipynb)ä¸Šæ‰¾åˆ°ã€‚
- en: Setup your environment
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½®ä½ çš„ç¯å¢ƒ
- en: 'For the following experiment to work, make sure to have a virtual environment
    with the following packages installed:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿ä»¥ä¸‹å®éªŒæœ‰æ•ˆï¼Œç¡®ä¿æœ‰ä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒï¼Œå¹¶å®‰è£…äº†ä»¥ä¸‹è½¯ä»¶åŒ…ï¼š
- en: torch
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: torch
- en: torchdata
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: torchdata
- en: transformers
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: transformers
- en: datasets
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: datasets
- en: pandas
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: matplotlib
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: matplotlib
- en: scikit-learn
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn
- en: Note that the libraries *transformers* and *datasets* are from HuggingFace,
    making it super easy for us to access and experiment with LLMs.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåº“*transformers*å’Œ*datasets*æ¥è‡ª HuggingFaceï¼Œä½¿æˆ‘ä»¬å¯ä»¥è½»æ¾è®¿é—®å’Œå®éªŒ LLMã€‚
- en: Once the environment is setup, we can start by importing the required libraries.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ç¯å¢ƒè®¾ç½®å¥½ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹å¯¼å…¥æ‰€éœ€çš„åº“ã€‚
- en: '[PRE0]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Load the data
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ•°æ®
- en: Then, we can load our dataset. Here, we use the dataset with 100% agreement
    rate.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½æˆ‘ä»¬çš„æ•°æ®é›†ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å…·æœ‰ 100% ä¸€è‡´æ€§ç‡çš„æ•°æ®é›†ã€‚
- en: '[PRE1]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This dataset contains a total of 2264 sentences.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ•°æ®é›†åŒ…å«æ€»å…± 2264 ä¸ªå¥å­ã€‚
- en: Note that the label is encoded. 1 means neutral, 0 means negative and 2 means
    positive. The count of each label is shown below.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„æ ‡ç­¾å·²è¢«ç¼–ç ã€‚1 ä»£è¡¨ä¸­æ€§ï¼Œ0 ä»£è¡¨è´Ÿé¢ï¼Œ2 ä»£è¡¨æ­£é¢ã€‚æ¯ç§æ ‡ç­¾çš„è®¡æ•°å¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/fe864adf8af515e6c1bc3fc401dd6ccc.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe864adf8af515e6c1bc3fc401dd6ccc.png)'
- en: Frequency of each sentiment in the dataset. Image by the author.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†ä¸­æ¯ç§æƒ…æ„Ÿçš„é¢‘ç‡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Letâ€™s store the actual label of each sentence in a DataFrame, making it easier
    for us to evaluate the model later on.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°†æ¯ä¸ªå¥å­çš„å®é™…æ ‡ç­¾å­˜å‚¨åœ¨ä¸€ä¸ª DataFrame ä¸­ï¼Œä»¥ä¾¿åç»­æ›´å®¹æ˜“è¯„ä¼°æ¨¡å‹ã€‚
- en: '[PRE2]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Load the model
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ¨¡å‹
- en: Now, letâ€™s load the model as well as the tokenizer. As mentioned above, we will
    load the Flan-T5 model. Note that the model is available in different sizes, but
    I decided to use the base version.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†åŠ è½½ Flan-T5 æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œè¯¥æ¨¡å‹æœ‰ä¸åŒçš„å¤§å°ç‰ˆæœ¬ï¼Œä½†æˆ‘å†³å®šä½¿ç”¨åŸºç¡€ç‰ˆã€‚
- en: '[PRE3]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Thatâ€™s it! We can now use this LLM to perform sentiment analysis on our dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ï¼æˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™ä¸ª LLM å¯¹æˆ‘ä»¬çš„æ•°æ®é›†è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚
- en: Prompt the model for sentiment analysis
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘æ¨¡å‹æå‡ºæƒ…æ„Ÿåˆ†æçš„æç¤º
- en: For the model to perform sentiment analysis, we need to do prompt engineering
    to specify that task.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®©æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œæç¤ºå·¥ç¨‹ä»¥æŒ‡å®šè¯¥ä»»åŠ¡ã€‚
- en: In this case we simply use â€œ*Is the following sentence positive, negative or
    neutral?*â€. We then pass the sentence of our dataset and let the model infer.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ç®€å•åœ°ä½¿ç”¨â€œ*ä»¥ä¸‹å¥å­æ˜¯æ­£é¢ã€è´Ÿé¢è¿˜æ˜¯ä¸­æ€§ï¼Ÿ*â€ã€‚ç„¶åæˆ‘ä»¬å°†æ•°æ®é›†ä¸­çš„å¥å­ä¼ é€’ç»™æ¨¡å‹ï¼Œè®©æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚
- en: Note that this is called **zero-shot inference**, since the model was not specifically
    trained for this particular task on this specific dataset.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œè¿™è¢«ç§°ä¸º**é›¶-shot æ¨æ–­**ï¼Œå› ä¸ºæ¨¡å‹æ²¡æœ‰ç‰¹åˆ«é’ˆå¯¹è¿™ä¸ªç‰¹å®šä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚
- en: '[PRE4]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the Python code block above, we loop over each sentence in the dataset and
    pass it in our prompt. The prompt is tokenized and set to the model. We then decode
    the output to obtain a natural language response. Finally, we store the prediction
    of the model in a list.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ Python ä»£ç å—ä¸­ï¼Œæˆ‘ä»¬å¾ªç¯éå†æ•°æ®é›†ä¸­çš„æ¯ä¸ªå¥å­ï¼Œå¹¶å°†å…¶ä¼ é€’åˆ°æˆ‘ä»¬çš„æç¤ºä¸­ã€‚æç¤ºè¢«åˆ†è¯å¹¶è®¾ç½®ç»™æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬è§£ç è¾“å‡ºä»¥è·å¾—è‡ªç„¶è¯­è¨€å“åº”ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ¨¡å‹çš„é¢„æµ‹å­˜å‚¨åœ¨åˆ—è¡¨ä¸­ã€‚
- en: Then, letâ€™s add these predictions to our DataFrame.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè®©æˆ‘ä»¬å°†è¿™äº›é¢„æµ‹æ·»åŠ åˆ°æˆ‘ä»¬çš„ DataFrame ä¸­ã€‚
- en: '[PRE5]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Evaluate the model
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ¨¡å‹
- en: To evaluate our model, letâ€™s display the confusion matrix of the predictions,
    as well as the classification report.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œè®©æˆ‘ä»¬å±•ç¤ºé¢„æµ‹çš„æ··æ·†çŸ©é˜µä»¥åŠåˆ†ç±»æŠ¥å‘Šã€‚
- en: '[PRE6]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/82cccc41147fd11fd4f0fd6cce0e2532.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82cccc41147fd11fd4f0fd6cce0e2532.png)'
- en: Confusion matrix of zero-shot sentiment analysis on financial news using Flan-T5\.
    Image by the author.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Flan-T5 å¯¹é‡‘èæ–°é—»è¿›è¡Œé›¶-shot æƒ…æ„Ÿåˆ†æçš„æ··æ·†çŸ©é˜µã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '![](../Images/b9909c03e18bb8479578ad762a17ba9b.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9909c03e18bb8479578ad762a17ba9b.png)'
- en: Classification report for zero-shot sentiment analysis. Image by the author.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: é›¶-shot æƒ…æ„Ÿåˆ†æçš„åˆ†ç±»æŠ¥å‘Šã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: From the figure above, we can see that the model found all negative sentences,
    at the cost of precision since it mislabelled 611 neutral sentences and 92 positive
    sentences. Also, we can see a clear problem with identifying neutral sentences,
    as it mislabelled the vast majority.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šé¢çš„å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¨¡å‹æ‰¾åˆ°äº†æ‰€æœ‰è´Ÿé¢å¥å­ï¼Œä½†ä»£ä»·æ˜¯ç²¾ç¡®åº¦ï¼Œå› ä¸ºå®ƒé”™è¯¯æ ‡è®°äº† 611 ä¸ªä¸­æ€§å¥å­å’Œ 92 ä¸ªæ­£é¢å¥å­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°è¯†åˆ«ä¸­æ€§å¥å­å­˜åœ¨æ˜æ˜¾çš„é—®é¢˜ï¼Œå› ä¸ºå®ƒé”™è¯¯æ ‡è®°äº†ç»å¤§å¤šæ•°å¥å­ã€‚
- en: Therefore, letâ€™s try to change our prompt to see if we can improve the modelâ€™s
    performance.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè®©æˆ‘ä»¬å°è¯•æ›´æ”¹æç¤ºï¼Œä»¥æŸ¥çœ‹æ˜¯å¦å¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: One-shot inference with in-context learning
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¸¦æœ‰ä¸Šä¸‹æ–‡å­¦ä¹ çš„ä¸€æ¬¡æ€§æ¨æ–­
- en: Here, we modify our prompt to include an example of a neutral sentence. This
    technique is called **in-context learning**, as we pass an example of how the
    model should behave inside the prompt.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¿®æ”¹äº†æˆ‘ä»¬çš„æç¤ºï¼ŒåŠ å…¥äº†ä¸€ä¸ªä¸­æ€§å¥å­çš„ç¤ºä¾‹ã€‚è¿™ç§æŠ€æœ¯ç§°ä¸º**ä¸Šä¸‹æ–‡å­¦ä¹ **ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨æç¤ºä¸­ä¼ é€’äº†æ¨¡å‹åº”å¦‚ä½•è¡¨ç°çš„ç¤ºä¾‹ã€‚
- en: Passing one example is called **one-shot inference**. It is possible to pass
    more examples, in which case it becomes **few-shot inference**.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ é€’ä¸€ä¸ªç¤ºä¾‹ç§°ä¸º**ä¸€æ¬¡æ€§æ¨æ–­**ã€‚å¯ä»¥ä¼ é€’æ›´å¤šç¤ºä¾‹ï¼Œè¿™ç§æƒ…å†µç§°ä¸º**å°‘é‡æ ·æœ¬æ¨æ–­**ã€‚
- en: It is normal to show up to five examples to the LLM. If the performance does
    not improve, then it is likely that we need to fine-tune the model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å‘ LLM å±•ç¤ºæœ€å¤šäº”ä¸ªç¤ºä¾‹æ˜¯æ­£å¸¸çš„ã€‚å¦‚æœæ€§èƒ½æ²¡æœ‰æé«˜ï¼Œé‚£ä¹ˆå¾ˆå¯èƒ½éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
- en: For now, letâ€™s see how one example impacts the performance.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªç¤ºä¾‹å¦‚ä½•å½±å“æ€§èƒ½ã€‚
- en: '[PRE7]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the code block above, we see that we give an example of a neutral sentence
    to help the model identify them. Then, we pass each sentence for the model to
    classify.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ä»£ç å—ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬ç»™å‡ºäº†ä¸€ä¸ªä¸­æ€§å¥å­çš„ç¤ºä¾‹ï¼Œä»¥å¸®åŠ©æ¨¡å‹è¯†åˆ«å®ƒä»¬ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æ¯ä¸ªå¥å­ä¼ é€’ç»™æ¨¡å‹è¿›è¡Œåˆ†ç±»ã€‚
- en: Afterwards, we follow the same steps of adding a new columns containing the
    new predictions, and displaying the confusion matrix.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æŒ‰ç…§ç›¸åŒçš„æ­¥éª¤æ·»åŠ åŒ…å«æ–°é¢„æµ‹çš„æ–°åˆ—ï¼Œå¹¶æ˜¾ç¤ºæ··æ·†çŸ©é˜µã€‚
- en: '[PRE8]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/086bcf3eb3efc80555f4344687861b6f.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/086bcf3eb3efc80555f4344687861b6f.png)'
- en: Confusion matrix of one-shot sentiment analysis of financial news using Flan-T5\.
    Image by the author.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Flan-T5è¿›è¡Œé‡‘èæ–°é—»çš„å•æ¬¡æƒ…æ„Ÿåˆ†æçš„æ··æ·†çŸ©é˜µã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '![](../Images/2ba2fe6cfd211f5f71a249c7c4d5a826.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ba2fe6cfd211f5f71a249c7c4d5a826.png)'
- en: Classification report for one-shot sentiment analysis. Image by the author.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å•æ¬¡æƒ…æ„Ÿåˆ†æçš„åˆ†ç±»æŠ¥å‘Šã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: From the figure above, we can see a slight improvement. The weighted F1-score
    increased from 0.40 to 0.44\. The model did better on the neutral class, but at
    the cost of a worse performance on the positive class.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œç•¥æœ‰æ”¹å–„ã€‚åŠ æƒF1åˆ†æ•°ä»0.40æé«˜åˆ°äº†0.44ã€‚æ¨¡å‹åœ¨ä¸­æ€§ç±»åˆ«ä¸Šçš„è¡¨ç°æ›´å¥½ï¼Œä½†ä»¥ç‰ºç‰²å¯¹æ­£é¢ç±»åˆ«çš„è¡¨ç°ä¸ºä»£ä»·ã€‚
- en: Adding examples of positive, negative, and neutral sentences may help, but I
    did not test it out. Otherwise, fine-tuning the model would be necessary, but
    that is the subject of another article.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ æ­£é¢ã€è´Ÿé¢å’Œä¸­æ€§å¥å­çš„ç¤ºä¾‹å¯èƒ½ä¼šæœ‰å¸®åŠ©ï¼Œä½†æˆ‘æ²¡æœ‰è¿›è¡Œæµ‹è¯•ã€‚å¦åˆ™ï¼Œå°±éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½†é‚£æ˜¯å¦ä¸€ç¯‡æ–‡ç« çš„ä¸»é¢˜ã€‚
- en: Conclusion
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: A lot of concepts were covered in this article, from the understanding the basics
    of LLMs, to actually using Flan-T5 for sentiment analysis in Python.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æ¶µç›–äº†è®¸å¤šæ¦‚å¿µï¼Œä»ç†è§£LLMçš„åŸºç¡€çŸ¥è¯†ï¼Œåˆ°å®é™…ä½¿ç”¨Flan-T5è¿›è¡ŒPythonä¸­çš„æƒ…æ„Ÿåˆ†æã€‚
- en: You now have the foundational knowledge to explore this world on your own and
    see how we can fine-tune LLMs, how we can train one, and how we can build applications
    around them.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ æ‹¥æœ‰äº†æ¢ç´¢è¿™ä¸ªé¢†åŸŸçš„åŸºç¡€çŸ¥è¯†ï¼Œå¯ä»¥è‡ªå·±çœ‹çœ‹å¦‚ä½•å¾®è°ƒLLMï¼Œå¦‚ä½•è®­ç»ƒLLMï¼Œä»¥åŠå¦‚ä½•å›´ç»•å®ƒä»¬æ„å»ºåº”ç”¨ç¨‹åºã€‚
- en: I hope that you learned something new, and that you are curious to learn even
    more.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›ä½ å­¦åˆ°äº†æ–°ä¸œè¥¿ï¼Œå¹¶ä¸”å¯¹å­¦ä¹ æ›´å¤šå……æ»¡å¥½å¥‡ã€‚
- en: Cheers ğŸ»
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: å¹²æ¯ ğŸ»
- en: Support me
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ”¯æŒæˆ‘
- en: Enjoying my work? Show your support with [Buy me a coffee](http://buymeacoffee.com/dswm),
    a simple way for you to encourage me, and I get to enjoy a cup of coffee! If you
    feel like it, just click the button below ğŸ‘‡
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: å–œæ¬¢æˆ‘çš„å·¥ä½œå—ï¼Ÿé€šè¿‡[è¯·æˆ‘å–å’–å•¡](http://buymeacoffee.com/dswm)æ¥æ”¯æŒæˆ‘ï¼Œè¿™æ˜¯ä½ é¼“åŠ±æˆ‘çš„ç®€å•æ–¹å¼ï¼Œè€Œæˆ‘èƒ½äº«å—ä¸€æ¯å’–å•¡ï¼å¦‚æœä½ æ„¿æ„ï¼Œåªéœ€ç‚¹å‡»ä¸‹é¢çš„æŒ‰é’®
    ğŸ‘‡
- en: '[![](../Images/7ad9438bd50b1698fdd722fa6661b16c.png)](http://buymeacoffee.com/dswm)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/7ad9438bd50b1698fdd722fa6661b16c.png)](http://buymeacoffee.com/dswm)'
- en: References
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: '[Attention is All You Need](https://arxiv.org/abs/1706.03762) â€” Ashish Vaswani,
    Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
    Kaiser, Illia Polosukhin'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ³¨æ„åŠ›æœºåˆ¶](https://arxiv.org/abs/1706.03762) â€” Ashish Vaswani, Noam Shazeer, Niki
    Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin'
- en: '[Generative AI with LLM](https://www.deeplearning.ai/courses/generative-ai-with-llms/)s
    â€” deeplearning.ai'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[ç”Ÿæˆå¼AIä¸LLM](https://www.deeplearning.ai/courses/generative-ai-with-llms/) â€”
    deeplearning.ai'
