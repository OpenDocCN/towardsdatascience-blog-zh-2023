- en: Catch Up On Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/catch-up-on-large-language-models-8daf784f46f8](https://towardsdatascience.com/catch-up-on-large-language-models-8daf784f46f8)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical guide to large language models without the hype
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcopeixeiro?source=post_page-----8daf784f46f8--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----8daf784f46f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8daf784f46f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8daf784f46f8--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----8daf784f46f8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8daf784f46f8--------------------------------)
    ·15 min read·Sep 5, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/adb61491bc3d90e8c3f4676819450577.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Gary Bendig](https://unsplash.com/@kris_ricepees?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: If you are here, it means that like me you were overwhelmed by the constant
    flow of information, and hype posts surrounding **large language models** (LLMs).
  prefs: []
  type: TYPE_NORMAL
- en: This article is my attempt at helping you catch up on the subject of large language
    models without the hype. After all, it is a transformative technology, and I believe
    it is important for us to understand it, hopefully making you curious to learn
    even more and build something with it.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will define what LLMs are and how they work, of
    course covering the Transformer architecture. We also explore the different methods
    of training LLMs and conclude the article with a hands-on project where we use
    Flan-T5 for sentiment analysis using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs and generative AI: are they the same thing?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative AI is a subset of machine learning that focuses on models who’s
    primary function is to generate *something*: text, images, video, code, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Generative models train on enormous amounts of data created by humans to learn
    patterns and structure which allow them to create new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of generative models include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image generation**: DALL-E, Midjourney'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code generation**: OpenAI Codex'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text generation**: GPT-3, Flan-T5, LLaMA'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large language models are part of the generative AI landscape, since they take
    an input text and repeatedly predict the next word until the output is complete.
  prefs: []
  type: TYPE_NORMAL
- en: However, as language models grew larger, they were able to perform other tasks
    in natural language processing, like summarization, sentiment analysis, named
    entity recognition, translation and more.
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, let’s now focus our attention on how LLMs work.
  prefs: []
  type: TYPE_NORMAL
- en: How LLMs work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the reasons why we now have large language models is because of the seminal
    work of Google and University of Toronto when they released the paper [Attention
    Is All You Need](https://arxiv.org/abs/1706.03762) in 2017.
  prefs: []
  type: TYPE_NORMAL
- en: This paper introduced the **Transformer** architecture, which is behind the
    LLMs we know and use today.
  prefs: []
  type: TYPE_NORMAL
- en: This architecture unlocked large scale models, making it possible to train very
    large models on multiple GPUs, and the models are able to process the inputs in
    parallel, giving them the opportunity to treat very large sequences of data.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the Transformer architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following is meant to be a high-level overview of the Transformer architecture.
    There are many resources that dive deeper into it, but the goal here is just to
    understand the way it works so we can understand how different LLMs specialize
    in different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: At any time, for more details, I suggest you read the [original paper](https://arxiv.org/pdf/1706.03762.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s start with a simplified visualization of the Transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8b3a35183236257fa4fe5247b56a9a5.png)'
  prefs: []
  type: TYPE_IMG
- en: A simplified visualization of the Transformer architecture. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can see that the main components of the Transformer
    are the encoder and decoder. Inside each, we also find the **attention** component.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore each component in more detail to understand how the Transformer
    architecture works.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenize the inputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We know LLMs work with text, but computers work with numbers, not letters. Therefore,
    the input must be *tokenized*.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization is the process in which words of a sentence are represented as
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, every possible word a model can work with is in a dictionary with
    a number associated to it. With tokenization, we can retrieve the number associated
    with the word, to represent a sentence as a sequence of numbers, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba7e371a044bec54d4a3dce16126b05a.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of tokenization. The sentence is tokenized and then sent to the embedding
    of the Transformer. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, we see an example of how the sentence “It rained this morning”
    can be tokenized before being set to the embedding layer of the Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there are many ways of tokenizing a sentence. In the example above,
    the tokenizer can represents parts of a word, which is why *rained* is separated
    into *rain* and *ed*. Other tokenizers would have a number for full words only.
  prefs: []
  type: TYPE_NORMAL
- en: The word embedding layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, we have a series of numbers that represent words, but how can
    the computer understand their meaning?
  prefs: []
  type: TYPE_NORMAL
- en: This is achieved by the word embedding layers.
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding is a learned representation of words, such that words with a
    similar meaning will have a similar representation. The model will learn different
    properties of words and represent them in a fixed space, where each axis can represent
    the property of a word.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a404c0f4ae58847ff43d149d786629d.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization of word embeddings. We can see that “morning” and “sunrise” have
    a similar representation since the angle in the 3D space is smaller. Similarly,
    “rain” and “thunder” are closer to one another. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, we can see how a 3D word embedding can look like. We see
    that “morning” and “sunrise” are closer to one another, and therefore have a similar
    representation. This can be can be computed using cosine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, “rain” and “thunder” are close to each other, and far from
    “morning” and “sunrise”.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can only show a 3D space, but in reality, embeddings can have hundreds
    of dimensions. In fact, the original Transformer architecture used an embedding
    space of 512 dimensions. This means that the model could learn 512 different properties
    of words to represent them in a space of 512 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: What about word order?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed that by representing words in embeddings, we lose their
    order in the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, with natural language, word order is very important, and so that’s
    why we use positional encoding, so the model knows the order of the words in a
    sentence.
  prefs: []
  type: TYPE_NORMAL
- en: It is the combination of the word embeddings and the positional encoding that
    gets sent to the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our inputs travel inside the encoder where they will go through the **self-attention**
    mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the model can learn the dependencies between each token in a sentence.
    It learns the importance of each word in relation to all other words in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f6c0661476ae1f98f0b96aa065648efa.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of an attention map for the word “rained”. The stroke width is representative
    of the importance. Here, we can see that “rained” is strongly connected to “this”
    and “morning”. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, we have a stylized example of an attention map for the
    word “rained”. The stroke width represents the importance.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we can see that self-attention captures the importance of “rained”
    with “this” and “morning”, meaning that it understands the context of this sentence.
  prefs: []
  type: TYPE_NORMAL
- en: While this example remains simple, since we have a very short sentence, the
    self-attention mechanism works very well on longer sentences, effectively capturing
    context and the overall meaning of a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the model does not have a single attention head. In fact, it has
    multiple attention heads, also called *multi-headed self-attention*, where each
    head can learn a different aspect of language.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf),
    the authors found that one head was involved in *anaphora resolution*, which is
    identifying the link between an entity and its repeated references.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0027b908a4028d654238ec0de6e4052f.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of anaphora resolution. Here, the word “keys” is referenced again in
    the sentence as “they”. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Above, we see an example of anaphora resolution, where the word “keys” is later
    referenced as “they”, and so one attention head can specialize in identifying
    those links.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we do not decide what aspect of language each attention head will
    learn.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the model has a deep representation of the structure of meaning
    of a sentence. This is sent to the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The decoder accepts a deep representation of the input tokens. This informs
    the self-attention mechanism inside the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, here is the Transformer architecture again, so we can remember
    what it looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8b3a35183236257fa4fe5247b56a9a5.png)'
  prefs: []
  type: TYPE_IMG
- en: A simplified visualization of the Transformer architecture. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: A **start-of-sequence** token is inserted as an input of the decoder, to signal
    it to start generating new tokens.
  prefs: []
  type: TYPE_NORMAL
- en: New tokens are generated according to the understanding of the input sequence
    generated by the encoder and its self-attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, we can see that the output of the decoder gets sent to
    a softmax layer. This generates a vector of probabilities for each possible token.
    The one with the largest probability is then output by the model.
  prefs: []
  type: TYPE_NORMAL
- en: That output token is then sent back to the embeddings as an input to the decoder,
    until an **end-of-sequence** token is generated by the model. At that point, the
    output sequence is complete.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the basic architecture behind large language models. With the
    Transformer architecture and its ability to process data in parallel, it was possible
    to train models on huge amounts of data, making LLMs a reality.
  prefs: []
  type: TYPE_NORMAL
- en: Now, there is more to this, as LLMs do not all use the full Transformer architecture,
    and that influences the way they are trained. Let’s explore this in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: How LLMs are trained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen the underlying mechanisms that power large language models, and
    as mentioned, not all models use the full Transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, some models may use the encoder portion only, while others use the
    decoder portion only.
  prefs: []
  type: TYPE_NORMAL
- en: This means that the models are also trained differently and will therefore specialize
    in particular tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-only models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Encoder-only models, also called **autoencoding** models are best suited for
    tasks like sentiment analysis, named entity recognition, and word classification
  prefs: []
  type: TYPE_NORMAL
- en: Popular examples of autoencoding models are BERT and ROBERTA.
  prefs: []
  type: TYPE_NORMAL
- en: Those models are trained using **masked language modeling** (MLM). With that
    training method, words in an input sentence are randomly masked, and the objective
    of the model is then to reconstruct the original text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7593549a23d59ccb9ddcef964b473e80.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustrating masked language modeling (MLM) for autoencoding models. Here, a
    random word was masked in the input sentence, and the model must reconstruct the
    original sentence. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, we can see what masked language modeling looks like. A
    word is hidden and the sentence is fed to the model, which must then learn to
    predict the right word to get the correct original sentence.
  prefs: []
  type: TYPE_NORMAL
- en: With that method, autoencoding models develop **bidrectional context**, since
    they see what precedes and follows the token they must predict, and not just what
    comes before.
  prefs: []
  type: TYPE_NORMAL
- en: Again, in the figure above, the model sees “it rained” and “morning”, so it
    sees both the beginning and the end of the sentence, allowing it to predict the
    word “this” to reconstruct the sentence correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Note that with autoencoding models, the input and output sequences have the
    same length.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder-only models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decoder-only models are also called *autoregressive* models. These models are
    best suited for text generation, but new functions arise when the models get very
    large.
  prefs: []
  type: TYPE_NORMAL
- en: Example of autoregressive models are GPT and BLOOM.
  prefs: []
  type: TYPE_NORMAL
- en: These models are trained using **causal language modeling** (CLM). With causal
    language modeling, the model only sees the tokens preceding the mask, meaning
    that it does not see the end of the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd4503e7d11be727715a89d105e4629a.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustrating causal language modeling. Here, the model only sees the tokens
    leading to the mask. Then, it must infer the next tokens until the sentence is
    complete. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: As we see above, with causal language modeling, the model only sees the tokens
    leading to the mask, and not what comes after. Then, it must predict the next
    tokens until the sentence is complete.
  prefs: []
  type: TYPE_NORMAL
- en: In the example above, the model would output “this”, and that token would be
    fed back as an input, so the model can then predict “morning”.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike masked language modeling, model build unidirectional context, since they
    do not see what comes after the mask.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, with decoder-only models, the output sequence can have a different
    length than the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Encoder-decoder models are also called *sequence-to-sequence* models, and they
    use the full Transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Those models are often used for translation, text summarization and question
    answering.
  prefs: []
  type: TYPE_NORMAL
- en: Popular examples of sequence-to-sequence models are T5 and BART.
  prefs: []
  type: TYPE_NORMAL
- en: To train these models, the **span corruption** method is used. Here, a random
    sequence of tokens is masked and designated as a *sentinel* token. Then, the model
    must reconstruct the masked sequence autoregressively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa279015ee0385588f31e1a9586423cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of span corruption. Here, a sequence of tokens is masked and replaced
    by a sentinel token. The model must then reconstructed the masked sequence autoregressively.
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, we can see that a sequence of two tokens were masked and
    replaced by a sentinel token. The model is then trained to reconstruct the sentinel
    token to obtain the original sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the masked input is sent to the encoder, and the decoder is responsible
    for reconstructing the masked sequence.
  prefs: []
  type: TYPE_NORMAL
- en: A note on model size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While we have specified certain tasks for which certain models perform best,
    researchers have observed that large models are capable of various tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, very large decoder-only models can be very good at translation, even
    though encoder-decoder models specialize in that task.
  prefs: []
  type: TYPE_NORMAL
- en: With all of that in mind, let’s now start working with a large language in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Work with a large language model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get hands-on experience with a large language model, let’s just cover
    some technical terms involved when working with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: First, the text that we feed the LLM is called *prompt*, and the output of the
    model is called *completion*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a54b6eb231931eccc8e824df03e110e6.png)'
  prefs: []
  type: TYPE_IMG
- en: The prompt is the text we feed to the model with the instructions. The output
    of the model is called completion. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the prompt is where we give the instructions to the LLM to achieve the
    task that we want.
  prefs: []
  type: TYPE_NORMAL
- en: This is also where **prompt engineering** is performed. With prompt engineering,
    we can perform **in-context learning**, which is when we give examples to the
    model of how certain tasks should be performed. We will see an example of that
    later on.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s interact with an LLM using Python for sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hands-on project: sentiment analysis with Flan-T5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this mini project, we use Flan-T5 for sentiment analysis of various financial
    news.
  prefs: []
  type: TYPE_NORMAL
- en: Flan-T5 is an improved version of the T5 model, which is a sequence-to-sequence
    model. Researchers basically took the T5 model and fine-tuned it on different
    tasks covering more languages. For more details, you can refer to the [original
    paper](https://arxiv.org/pdf/2210.11416.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: As for the dataset, we will use the *financial_phrasebank* dataset published
    by Pekka Malo and Ankur Sinha under the Creative Commons Attribute license.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset contains a total of 4840 sentences from English language financial
    news that were categorized as positive, negative or neutral. A group of five to
    eight annotators classified each sentence, and depending on the agreement rate,
    the size of the dataset will vary (4850 rows for a 50% agreement rate, and 2260
    rows for a 100% agreement rate).
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the dataset and how it was compiled, refer to the [full
    dataset details page.](https://huggingface.co/datasets/financial_phrasebank)
  prefs: []
  type: TYPE_NORMAL
- en: Of course, all code show below is available on [GitHub](https://github.com/marcopeix/learn_llm/blob/main/1_llm_get_started.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Setup your environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the following experiment to work, make sure to have a virtual environment
    with the following packages installed:'
  prefs: []
  type: TYPE_NORMAL
- en: torch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: torchdata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the libraries *transformers* and *datasets* are from HuggingFace,
    making it super easy for us to access and experiment with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Once the environment is setup, we can start by importing the required libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Load the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Then, we can load our dataset. Here, we use the dataset with 100% agreement
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This dataset contains a total of 2264 sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the label is encoded. 1 means neutral, 0 means negative and 2 means
    positive. The count of each label is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe864adf8af515e6c1bc3fc401dd6ccc.png)'
  prefs: []
  type: TYPE_IMG
- en: Frequency of each sentiment in the dataset. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s store the actual label of each sentence in a DataFrame, making it easier
    for us to evaluate the model later on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Load the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s load the model as well as the tokenizer. As mentioned above, we will
    load the Flan-T5 model. Note that the model is available in different sizes, but
    I decided to use the base version.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! We can now use this LLM to perform sentiment analysis on our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt the model for sentiment analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the model to perform sentiment analysis, we need to do prompt engineering
    to specify that task.
  prefs: []
  type: TYPE_NORMAL
- en: In this case we simply use “*Is the following sentence positive, negative or
    neutral?*”. We then pass the sentence of our dataset and let the model infer.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is called **zero-shot inference**, since the model was not specifically
    trained for this particular task on this specific dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the Python code block above, we loop over each sentence in the dataset and
    pass it in our prompt. The prompt is tokenized and set to the model. We then decode
    the output to obtain a natural language response. Finally, we store the prediction
    of the model in a list.
  prefs: []
  type: TYPE_NORMAL
- en: Then, let’s add these predictions to our DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To evaluate our model, let’s display the confusion matrix of the predictions,
    as well as the classification report.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/82cccc41147fd11fd4f0fd6cce0e2532.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix of zero-shot sentiment analysis on financial news using Flan-T5\.
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9909c03e18bb8479578ad762a17ba9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Classification report for zero-shot sentiment analysis. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can see that the model found all negative sentences,
    at the cost of precision since it mislabelled 611 neutral sentences and 92 positive
    sentences. Also, we can see a clear problem with identifying neutral sentences,
    as it mislabelled the vast majority.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, let’s try to change our prompt to see if we can improve the model’s
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: One-shot inference with in-context learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we modify our prompt to include an example of a neutral sentence. This
    technique is called **in-context learning**, as we pass an example of how the
    model should behave inside the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Passing one example is called **one-shot inference**. It is possible to pass
    more examples, in which case it becomes **few-shot inference**.
  prefs: []
  type: TYPE_NORMAL
- en: It is normal to show up to five examples to the LLM. If the performance does
    not improve, then it is likely that we need to fine-tune the model.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s see how one example impacts the performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the code block above, we see that we give an example of a neutral sentence
    to help the model identify them. Then, we pass each sentence for the model to
    classify.
  prefs: []
  type: TYPE_NORMAL
- en: Afterwards, we follow the same steps of adding a new columns containing the
    new predictions, and displaying the confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/086bcf3eb3efc80555f4344687861b6f.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix of one-shot sentiment analysis of financial news using Flan-T5\.
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ba2fe6cfd211f5f71a249c7c4d5a826.png)'
  prefs: []
  type: TYPE_IMG
- en: Classification report for one-shot sentiment analysis. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can see a slight improvement. The weighted F1-score
    increased from 0.40 to 0.44\. The model did better on the neutral class, but at
    the cost of a worse performance on the positive class.
  prefs: []
  type: TYPE_NORMAL
- en: Adding examples of positive, negative, and neutral sentences may help, but I
    did not test it out. Otherwise, fine-tuning the model would be necessary, but
    that is the subject of another article.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of concepts were covered in this article, from the understanding the basics
    of LLMs, to actually using Flan-T5 for sentiment analysis in Python.
  prefs: []
  type: TYPE_NORMAL
- en: You now have the foundational knowledge to explore this world on your own and
    see how we can fine-tune LLMs, how we can train one, and how we can build applications
    around them.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that you learned something new, and that you are curious to learn even
    more.
  prefs: []
  type: TYPE_NORMAL
- en: Cheers 🍻
  prefs: []
  type: TYPE_NORMAL
- en: Support me
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enjoying my work? Show your support with [Buy me a coffee](http://buymeacoffee.com/dswm),
    a simple way for you to encourage me, and I get to enjoy a cup of coffee! If you
    feel like it, just click the button below 👇
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/7ad9438bd50b1698fdd722fa6661b16c.png)](http://buymeacoffee.com/dswm)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Attention is All You Need](https://arxiv.org/abs/1706.03762) — Ashish Vaswani,
    Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
    Kaiser, Illia Polosukhin'
  prefs: []
  type: TYPE_NORMAL
- en: '[Generative AI with LLM](https://www.deeplearning.ai/courses/generative-ai-with-llms/)s
    — deeplearning.ai'
  prefs: []
  type: TYPE_NORMAL
