# 预训练上下文是你所需的一切

> 原文：[https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358](https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358)

## 现代变换模型背后的驱动力在很大程度上来源于其相关数据，从而允许强大的上下文学习能力。

[](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[![本杰明·图尔雷](../Images/b4c49698c7270c592bf992fc47f75765.png)](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)[![走向数据科学](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------) [本杰明·图尔雷](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------) ·6 分钟阅读·2023年11月27日

--

生成型人工智能及其流行的变换模型如今随处可见，每小时都有新模型发布（见[人工智能的膨胀](https://medium.com/towards-data-science/the-inflation-of-ai-is-more-always-better-8ea1be75e0aa)）。在这个迅速发展的人工智能领域，这些模型可能带来的价值似乎是无穷无尽的。像[chatGPT](http://chat.openai.com)这样的“大型语言模型”（LLM）已经成为每位工程师资源的组成部分，作家们用它们来支持他们的文章，设计师们则用计算机视觉模型的结果来创建初步的视觉效果或寻找灵感。

> 如果这不是魔法，那究竟是什么驱动了这些令人印象深刻的变换模型？

然而，即使成就和实用性都很出色，生成型人工智能提升了生产力，但重要的是要记住，现代机器学习模型（如 LLM 或 Vision Transformers）并没有进行任何魔法（类似于 ML 或统计模型本身从未有过神奇的事实）。尽管模型的卓越能力可能被视为*类似魔法*，一些领域专家甚至谈到模型的*幻觉*，但每个模型的基础仍然只是数学和统计概率（虽然有时复杂，但仍是数学）。这引出了一个根本性的问题：如果这不是魔法，那究竟是什么驱动了这些令人印象深刻的变换模型？

![](../Images/034acf90a6d7f81e53b4a668d97bd141.png)

图 1：展示了 ChatGPT（使用 GPT4）将其“先进技术”和“广泛训练”视为主要性能驱动因素。

# 每个模型的基础是数据

与任何模型（统计模型或机器学习模型）一样，训练数据对模型的最终性能影响最大。如果没有大量高质量的数据来反映你希望模型学习的关系，那么就*没有*可以训练的内容，最终的模型表现会很差（著名的GIGO原则：垃圾进垃圾出）。这一数据建模的基本原则多年来没有改变。每个革命性的新型变换模型背后首先只有一个因素：**数据**。是数据的**数量**、**质量**和**上下文**决定了模型的后续表现。近期研究（见下文）通过展示最新的生成性人工智能模型在提供的上下文属于训练分布时能够很好地进行泛化，而在异质分布学习中表现不佳来支持这一点。

# 同质分布与异质分布学习

重要的是要记住，模型不过是一个巨大的网络、树或关系图。机器学习模型基本上学习的是如何将给定的输入转换为所需的输出（见图2）。

![](../Images/9181ca0727d831aefc6a5acce38a704b.png)

图2：一个超级简单的神经网络结构图，它根据天气和其他上下文预测人流量。在左侧是训练过程中的输入（特征），而右侧是输出（目标）。在它们之间可以有多个转换（层），这些层学习复杂的输入输出关系。

当模型被训练时（换句话说：当这些关系被更新时），输入的上下文和输出的信息量将决定模型的擅长领域。类似于人类能够很好地回答母语中的问题，机器学习模型在处理它们见过的数据时表现较好。这被称为**同质分布**学习。如果在训练过程中，模型接收到大量丰富的上下文，它可以依赖于这种获得的知识，最终的预测结果会显示出准确的表现。

然而，**异质分布**学习描述的是模型需要基于它*未曾*见过的上下文进行预测的情况。你可以想象一个从未学过挪威语的人突然回应用挪威语提出的问题。请查看图3以了解同质分布和异质分布学习的概况。

![](../Images/9271f018d29e44fd241a22c72b9e8a15.png)

图3：展示了同质分布（左）与异质分布（右）学习。左侧的模型在面对不属于原始训练数据的新的上下文（在这种情况下是“政治”）时表现较差，而右侧的模型在处理未见过的上下文时表现良好。机器学习模型通常属于左侧类别，并且在异质分布学习中表现不佳。

现代LLMs和其他ML模型的出色性能来源于原始训练数据中的大量上下文。由于模型的广泛预训练，能够处理的内分布学习的问题范围非常广泛。这使得模型能够回答各种问题，这可能对用户而言看起来像是*魔法*或*人类级智能*，但实际上并非如此。同样，模型的错误或意外回答也并不是真正的*幻觉*，它基本上突出了原始训练数据中的上下文缺口，从而导致分布外学习。总的来说，机器学习模型在其分布外学习能力上非常有限，需要对基础模型进行广泛训练。

# 语言模型中的预训练力量

在谷歌DeepMind成员最近的一篇论文中，作者进一步支持了现代LLMs的上下文学习性能主要源自其预训练分布的观点。论文“[Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models](https://doi.org/10.48550/arXiv.2311.00871)”由**Steve Yadlowsky**、**Lyric Doshi**和**Nilesh Tripuraneni**（2023）撰写，专注于现代变换器模型如何获得其令人印象深刻的上下文学习能力（即应对任何提供的上下文的能力）。

[](https://doi.org/10.48550/arXiv.2311.00871?source=post_page-----f457ffa8a358--------------------------------) [## 预训练数据混合使变换器模型具备狭窄的模型选择能力

### 变换器模型，特别是大型语言模型（LLMs），具有在上下文中学习的卓越能力…

doi.org](https://doi.org/10.48550/arXiv.2311.00871?source=post_page-----f457ffa8a358--------------------------------)

这些发现非常有启发性。当变换器模型在覆盖广泛上下文的数据上进行预训练时，它们在学习新的任务时表现出令人印象深刻的能力，这些任务都属于预训练上下文。这种能力接近最优，展示了在训练分布内的显著泛化和适应能力。然而，当这些模型遇到超出其预训练领域的上下文时，性能受到限制且会出现失败。这突显了在分布外上下文中的减少泛化能力和明显限制。

# 视觉变换器：规模案例研究

在另一项研究中（也是由谷歌DeepMind于2023年进行）题为：“[ConvNets Match Vision Transformers at Scale](https://doi.org/10.48550/arXiv.2310.16764)”的论文中，作者**Samuel L. Smith**、**Andrew Brock**、**Leonard Berrada**和**Soham De**挑战了一个在计算机视觉领域广泛存在的观念，即在大规模下，现代的视觉变换器模型超越了传统的模型，如卷积神经网络（CNNs）。该研究使用类似的计算预算训练了CNNs和视觉变换器，并比较了它们的性能。

[## ConvNets 在规模上匹敌视觉 Transformer](https://doi.org/10.48550/arXiv.2310.16764?source=post_page-----f457ffa8a358--------------------------------)

### 许多研究人员认为，卷积神经网络（ConvNets）在小型或中型数据集上表现良好，但在更大的数据集上不具竞争力……

[doi.org](https://doi.org/10.48550/arXiv.2310.16764?source=post_page-----f457ffa8a358--------------------------------)

结果表明，预训练所使用的计算预算与随后的性能之间存在规模法则。在 ImageNet 上进行微调后，预训练的 CNN 在相当的预算下达到了与视觉 Transformer 相匹配的性能。

# 摘要

这两项研究共同描绘了现代 Transformer 模型令人印象深刻的性能。首先，性能不仅仅受到模型架构的驱动，更主要是由预训练的量驱动。其次，当预训练的上下文涵盖广泛时，生成的模型也会展示出广泛的上下文学习能力。

这些研究强调了一个关键原则：训练数据的量、质量和上下文是任何基础机器学习模型最关键的部分。在不知道预训练涵盖的上下文的情况下，很难事先确定模型在哪些领域表现良好。基准测试可以帮助指示潜在的上下文限制。这些测试并不展示模型的一般性能，它们主要展示了哪些上下文曾经成为模型训练分布的一部分。

总之，随着人工智能时代的到来和数据科学家及工程师在机器学习模型开发中的人数增加，预训练涵盖广泛上下文的重要性变得越来越明显；在许多方面，它不仅是过程的一部分，而且是你所需要的全部。

*所有图像，除非另有说明，均由作者提供。*

***请查看*** [***我的个人主页***](https://medium.com/@benjamin.thuerer/about)***，关注我，或*** [***订阅我的邮件列表***](https://medium.com/@benjamin.thuerer/subscribe) ***，如果你想了解我写的内容或希望及时获得新故事的更新。***
