- en: Categorize Free-Text Bank Transaction Descriptions Using BERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/categorize-free-text-bank-transaction-descriptions-using-bert-44c9cc87735b](https://towardsdatascience.com/categorize-free-text-bank-transaction-descriptions-using-bert-44c9cc87735b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I built myself an Expense Tracking Tool
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jin-cui.medium.com/?source=post_page-----44c9cc87735b--------------------------------)[![Jin
    Cui](../Images/e5ddcbaa6d7da38f960d2c5fea71b538.png)](https://jin-cui.medium.com/?source=post_page-----44c9cc87735b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----44c9cc87735b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----44c9cc87735b--------------------------------)
    [Jin Cui](https://jin-cui.medium.com/?source=post_page-----44c9cc87735b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----44c9cc87735b--------------------------------)
    ·7 min read·Jan 30, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a38931da456d880799b9fea6b82d20c.png)'
  prefs: []
  type: TYPE_IMG
- en: Expense by Category. Chart by author
  prefs: []
  type: TYPE_NORMAL
- en: The Situation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I purchased a property towards the end of calendar year 2022 with a mortgage.
    Given the increase in financial commitments, I wanted to keep a tab on my expenses.
    It had never occurred to me prior to this point, that I actually had no idea where
    I have been spending the most. Figuring this out may be a good starting point
    for my own expense management.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally I turned to the bank transactions data which I downloaded from the
    online banking portal in a *.csv* format. A snippet of this for the last few days
    of 2022 is provided below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2077e5d1baa9ed2ae7aa8b08a0685e57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 1: Writer’s bank transaction data. Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: Based on snippet above, it seems I spent proportionally more on food (as highlighted
    in green). More importantly, the transaction descriptions are free-text based,
    is there a way to automatically classify these into a number of pre-defined expense
    categories (e.g. food, grocery shopping, utilities and etc.)?
  prefs: []
  type: TYPE_NORMAL
- en: There is at least one way using pre-trained Large Language Models like BERT,
    and this article offers a tutorial as to how!
  prefs: []
  type: TYPE_NORMAL
- en: '**A 2023 Introduction to BERT**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whilst ChatGPT being a state-of-the-art **Text Generation** model is attracting
    a lot of attention at this time, it is generally not considered a **General Purpose**
    model — one such as BERT which can be used across multiple Natural Language Understanding
    tasks. Some examples of these are Grammar Detection, Sentiment Classification,
    Text Similarity, Q & A Inference and etc.
  prefs: []
  type: TYPE_NORMAL
- en: BERT was developed and released by Google in 2018\. It’s a pre-trained model
    using text passages on Wikipedia and BookCorpus (to ensure the training data are
    grammatically sound).
  prefs: []
  type: TYPE_NORMAL
- en: The BERT model I’ll be using for the purpose of this tutorial is available on
    Hugging Face through the sentence_transformer library, which is a Python framework
    for creating sentence, text and image embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Steps for Building the Expense Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How do I ultimately convert the free-text transaction descriptions into an
    expense category? There are a couple of strategies I can think of. In this tutorial,
    I’ll be providing a step-by-step guide for building the Expense Classifier based
    on (cosine) similarity of word embeddings. The steps are outlined below:'
  prefs: []
  type: TYPE_NORMAL
- en: Manually label a credible number of transaction descriptions into an expense
    category (e.g. food, entertainment). This creates a set of labelled training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse individual transaction descriptions in the training data above as word
    embeddings using BERT (i.e. convert texts into a numerical vector). **Step 1**
    and **Step 2** collectively ensure that the training data is assigned to a particular
    expense category as well as a word embedding vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat **Step 2** for new transaction descriptions (i.e. convert unseen texts
    into a numerical vector)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pair the word embeddings in **Step 3** with the most similar word embeddings
    from the training data, and assign the same expense category
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Python Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section sets out the Python codes for loading the required packages as
    well as for implementing the steps as outlined above (apart from **Step 1** which
    is a manual labelling step).
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 0: Import the required libraries**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 1: Label training data**'
  prefs: []
  type: TYPE_NORMAL
- en: I manually labelled 200 transaction descriptions into a expense category. For
    instance, the transaction descriptions in **Image 1** were assigned an expense
    category as shown in the image below. I have also assigned categories such as
    utilities (i.e. for electricity and gas), car and gift to other transactions in
    the training data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bea125c7ecded038ac1a2c3f8b867725.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 3: Manual label of training data. Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2: Create word embeddings for training data using BERT**'
  prefs: []
  type: TYPE_NORMAL
- en: We start by defining a function for cleaning the text data. This includes lower-casing
    words, removing special characters including dates (which are not useful in informing
    the expense category).
  prefs: []
  type: TYPE_NORMAL
- en: Stemming, lemmatization or removing stop words which are common practices in
    an NLP data cleaning pipeline are generally not required when using a BERT model
    due to its Byte-Pair Encoding and Attention mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We then apply the function to the transaction descriptions, loaded as *text_raw*
    from the dataframe shown in **Image 1** (*df_transaction_description*).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The snippet below shows an example of a particular transaction before and after
    data cleaning was applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf3838aeaf95973bcf23ea0d0e9ecb45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 2: Data cleaning example. Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: We then run the cleaned texts through BERT. I’ve selected the ‘*paraphrase-mpnet-base-v2*’
    BERT model known for modelling sentence similarity. Per its [documentation on
    Hugging Face](https://huggingface.co/sentence-transformers/paraphrase-mpnet-base-v2),
    it maps sentences and paragraphs to a 768 dimensional dense vector space and can
    be used for tasks like clustering or semantic search.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'A snippet of the word embeddings for the first few transactions is provided
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e34cb4bc7897cf40aca823654d87c18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 4: BERT embeddings. Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Create word embeddings for unseen data**'
  prefs: []
  type: TYPE_NORMAL
- en: I’ve selected 20 transactions from the data which didn’t make the training data
    (for the purpose of this tutorial, randomly selected transactions). These are
    shown in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93028502aadcd1ca7b7ff30b54fd0865.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 5: Unseen transactions. Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: The transaction descriptions above are loaded as *text_test_raw*. Similar to
    **Step 2**, these are run through BERT for embedding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4: Pair unseen data with most similar training data**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: d_output dataframe shows that the unseen data have been assigned a fairly reasonable
    expense category.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eea9109b46c9e201dbfe93d5c908a576.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 6: Unseen data matched with training data. Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: Now whenever new expenses come through, simply feed them to the model!
  prefs: []
  type: TYPE_NORMAL
- en: '**Bonus Step: Plotting expenses by category**'
  prefs: []
  type: TYPE_NORMAL
- en: I have actually applied the steps above to all my expenses in calendar year
    2022\. The plot below shows the resultant expense dollar amounts by the assigned
    category.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a38931da456d880799b9fea6b82d20c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Chart 7: Expense plot by category. Chart by author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Key observations were:'
  prefs: []
  type: TYPE_NORMAL
- en: I spent the most on Food in 2022, followed by Mortgage repayments and Utility
    bills.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although Credit Card repayment had the highest amount, it is assumed that Credit
    Card spending could be attributed to other expense categories in the same proportion.
    This assumption also applies to the PayPal category.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the data, I probably want to cut back on spending on Food for more
    spending on Groceries (i.e. start cooking at home as opposed to dining out) in
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: My spending on Beauty products was probably driven by instances where I went
    shopping with the wife…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, it’s super easy to return the transactions with the highest spending
    within a particular category. For instance, my highest spending in the Food expense
    category in 2022 were shown in the screen print below. I’m happy with the results
    as some of these restaurants weren’t present in the training data. Despite this,
    BERT was still able to allocate these transactions to the Food category.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2cb18ec6e14bec4ad0f30c101ff953a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 7: Top expenses.Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: Concluding Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article provides a comprehensive tutorial in building an expense tracking
    tool. All I’ve done really is translating the free-text transaction descriptions
    to a language the machine understands using BERT, and letting the machine do the
    hard yards!
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach is to replace **Step 4** of this tutorial by passing
    the same wording embeddings through a classification model — something for the
    readers to experiment with further.
  prefs: []
  type: TYPE_NORMAL
- en: If you like this article of mine, feel free to have a read of the [others](https://jin-cui.medium.com/).
  prefs: []
  type: TYPE_NORMAL
- en: '*As I ride the AI/ML wave, I enjoy writing and sharing step-by-step guides
    and how-to tutorials in a comprehensive language with ready-to-run codes. If you
    would like to access all my articles (and articles from other practitioners/writers
    on Medium), you can sign up using* [*the link*](https://medium.com/@jin-cui/membership)
    *here!*'
  prefs: []
  type: TYPE_NORMAL
