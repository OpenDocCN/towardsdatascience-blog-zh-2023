# 使用 Python 探索强化学习的第一步

> 原文：[https://towardsdatascience.com/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3](https://towardsdatascience.com/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3)

## 原始的 Python 实现，展示了如何在强化学习的基本世界之一——网格世界中找到最佳位置

[](https://eligijus-bujokas.medium.com/?source=post_page-----b843b76538e3--------------------------------)[![Eligijus Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----b843b76538e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b843b76538e3--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b843b76538e3--------------------------------) [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----b843b76538e3--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b843b76538e3--------------------------------) ·15 分钟阅读·2023年1月13日

--

![](../Images/b476cbb9f0b2f5a0f39114a7b0ebca24.png)

网格世界矩阵；作者提供的照片

本文的目的是使用 Python 代码和注释介绍强化学习（以下简称**RL**）中的基本概念和定义。

这篇文章深受以下伟大强化学习课程的启发：[https://www.coursera.org/learn/fundamentals-of-reinforcement-learning](https://www.coursera.org/learn/fundamentals-of-reinforcement-learning)

理论见书¹：[http://www.incompleteideas.net/book/RLbook2020.pdf](http://www.incompleteideas.net/book/RLbook2020.pdf)

我所有的强化学习实验代码可以在我的 Gitlab 仓库中查看：[https://github.com/Eligijus112/rl-snake-game](https://github.com/Eligijus112/rl-snake-game)

网格世界问题是强化学习中的一个经典问题，我们希望为代理创建一个优化的策略以穿越网格。

网格是一个方形的单元格矩阵，代理可以在每个单元格中向任意四个方向（上、下、左、右）移动。代理每移动一步会获得-1的奖励，若达到目标单元格则获得+10的奖励。奖励的数值是任意的，可以由用户定义。

在强化学习框架中，代理被正式定义为**做出采取何种行动的决策的组件**。代理在具体的时间步骤中采取行动。

在网格世界设置中，整个动作集合由以下集合定义：

![](../Images/b336858f0f5eab063a10922cc7926120.png)

动作集合

或

![](../Images/4f396e4084c317ccd1fa466164aecb21.png)

动作集合

无论我们的代理处于何处，它只能向左、向右、向上或向下移动。现在让我们定义并可视化我们的网格世界：

[PRE0]

[PRE1]

![](../Images/ddb3ae05c5ca79f13def722c6674a83b.png)

网格世界；作者照片

在上述示例中，我们定义了第一个所需的矩阵——**R**矩阵或**奖励矩阵**。目标位于网格世界的中心和角落。当代理进入其中一个单元格时，它会获得该单元格的奖励值。

让我们定义另一个关键矩阵——状态矩阵**S**：

[PRE2]

![](../Images/51f7e9d43aef3204ba3682c32376d982.png)

状态空间；作者照片

在我们定义的网格世界中，总共有**49个状态**，代理可以处于其中的任何一个状态。每个状态可以通过矩阵中的整数来识别。

假设我们的代理在状态17并向下移动。该动作值**，表示为q，**是：

![](../Images/4e9be0dceaf7766eb7616a668122640c.png)

状态17中动作‘down’的动作值

动作值为10，因为网格24的奖励等于10。因此，当我们使用动作值时，需要注意网格**S**索引和奖励矩阵**G**。可以很容易猜到，从同一状态向右移动的奖励是-1：

![](../Images/0938faa5a0b3ab7968d7ffc4391a783f.png)

状态17中动作‘right’的动作值

一般来说，函数q，称为动作值，将一个数字映射到状态—动作对：

![](../Images/5e7166d1a7357025897ef15cf4d3a83a.png)

动作值函数

数字越高，对代理的“奖励”就越大，因此，代理总是希望采取**最大化****当前状态下的q**的动作。

到目前为止，我们已经定义了矩阵**R**（奖励）和**S**（状态）。另一个关键矩阵是状态值矩阵**V**。**V**矩阵的维度与S和G矩阵相同，每个**V**矩阵中的元素评估给定状态的“好坏”。这里的“好坏”指的是方程：

![](../Images/4c220ff25b15ac721c1892e297056cbf.png)

状态s的值[1]

我们可以将上述方程读作：

> 给定策略pi的状态s的值等于在时间步t给定状态为s时的期望回报。

我们计算所有状态的上述值并将其存储在矩阵**V**中。我们在这里引入了新的变量，所以让我们定义它们。

![](../Images/f046ada703904fdc3e0bf22f7cadd61a.png)

时间步t的总回报[1]

![](../Images/a87ffee384e210ada1ee944a256fde51.png)

折扣因子[1]

**K**索引称为终端状态，其中代理达到网格世界中的任意目标。换句话说，**每个状态中G的值表示从给定状态开始朝向目标的代理路径的折扣奖励总和。值越大，状态越受欢迎。**

状态方程中的pi项称为**策略**，是采取状态s中某一动作的概率：

![](../Images/16502cff059ff88102a32e427a8ce0a8.png)

策略

让我们初始化初始值矩阵**V**：

[PRE3]

![](../Images/faa47e2889ea96314d3c7fe5782f0445.png)

初始状态值矩阵；作者照片

由于我们尚未探索我们创建的网格世界，因此所有状态的回报都是0。

我们需要的最后一个矩阵是策略矩阵**P**。

[PRE4]

[PRE5]

![](../Images/ca4fcca11a20fa8491331e3f6beaa831.png)

初始策略矩阵；作者照片

网格中的每个箭头代表代理可以采取的可用动作。初始矩阵中的概率是均匀的，目标状态中没有可用的移动。

拥有**R, P, S**和**V**矩阵后，我们可以最终开始计算我们RL问题的答案。但我们还需定义RL目标。

**RL算法的目标是使代理找到最优策略P，以最大化每个状态的回报。**

另一种表述是**目标是计算矩阵V中的最优状态值。**

假设我们有一个5乘5的网格，目标在中央：

![](../Images/3276db0f4e58cee158234d9b46fcec5c.png)

示例网格世界；作者照片

为了建立直观理解，我已经计算了最佳值和策略。我们将在本文章的下一部分找到如何实现，但现在让我们解释以下**V**和**P**矩阵：

![](../Images/522e430faa9e355649fec4444715db3f.png)

已解决的值和策略矩阵；作者照片

记住，V矩阵中的每个值是总的累计折扣奖励。因此，我们的代理将始终希望前往具有最高值的状态。用数学表达就是，在每个状态下，代理将根据以下方程选择下一个状态：

![](../Images/a1d5be7b12aebedcd66dfb4efcf11f0f.png)

每个状态的最佳选择 [1]

在每个状态下，我们将选择一个动作，使我们进入状态**s prime**，在这里**r + gamma * (new state value)**是最高的。

更简单的直观理解是，我们可以列出当前状态下所有可用的动作，检查哪个可用状态具有最高的**V(s)**值，然后前往那里。从上面的矩阵中，我们可以看到，例如，状态8有两个最佳选择——**下**和**左**。这是因为这些动作将使代理进入同样好的状态。因此，拥有**V**矩阵后，我们将始终推断出P矩阵。

现在，如何从全零的V矩阵转换为具有值的矩阵？我们需要为每个状态定义贝尔曼方程：

![](../Images/ad0993137593b7dfc5db6303504664b5.png)

状态s的贝尔曼方程与策略pi [1]

上面的方程令人望而生畏，且具有递归特性。对于网格世界示例，我们可以简化方程，并将其写出，而无需中间的条件概率。

![](../Images/66808347cf33d040418b7e026674b640.png)

简化方程 [1]

我们可以这样做，因为当我们在状态s中执行一个动作时，我们可以保证只会进入一个下一个状态。

在已解决的矩阵示例中，回顾一下

![](../Images/06df8d53e68b9b0497e8073ac5968a3f.png)

状态0的值

这意味着从位置 0 开始，我们的代理在长期内将累计 -6.07 的总奖励。为了估计这一点并将递归公式转换为可以通过简单循环评估的公式，我们将使用以下算法：

![](../Images/036a51aa40f92331d08f1e0c65fddf33.png)

价值迭代算法 [1]

我们将简化网格世界问题算法的中间部分：

![](../Images/f0ab254caf5769c503bc05223296f649.png)

价值迭代算法简化；作者拍摄

现在让我们将一切转到 Python 代码中。

[PRE6]

[PRE7]

上述函数找到状态 **s** 的贝尔曼方程值。

[PRE8]

上述代码块实现了价值迭代算法，用于寻找最佳（或接近最佳的） **V** 矩阵。

[PRE9]

我们现在有了所有理论和代码，开始评估网格世界中的所有状态。回顾一下，我们的初始网格世界如下：

![](../Images/07a9e06cbad806a751627013cd488f21.png)

状态空间；作者拍摄

![](../Images/ddb3ae05c5ca79f13def722c6674a83b.png)

网格世界；作者拍摄

![](../Images/a8da08bce9e812777606c180c4a48507.png)

初始值和策略矩阵；作者拍摄

现在让我们更新一个状态——第一个或 **s = 1**。

[PRE10]

价值矩阵和策略矩阵现在看起来如下：

![](../Images/555818070687f34a555a1516f51a5011.png)

一个状态的价值迭代；作者拍摄

在状态 2 或 8 中，最佳策略是移动到状态 1，因为 0 < 2.66，因此状态 1 比其邻居更有价值。

现在让我们更新状态 3，看看会发生什么：

[PRE11]

![](../Images/d7e22b1776ea0246a92ec6717ab79788.png)

更新第三状态；作者拍摄

在状态 3 中的值为 -1，因此，目前在我们的网格世界中，代理会倾向于避免这个状态，相比于其邻居。

价值迭代算法以与上述相同的方式工作，只是针对所有状态（在我们的案例中——从状态 0 到 48）。要实现它，请使用以下代码：

[PRE12]

![](../Images/fb7237cf07e6f7665cd3700e36645e2b.png)

已解决的网格世界；作者拍摄

代理可以从任何非终结状态开始，并沿着策略矩阵中的箭头移动。如果同一状态中有两个或更多箭头，我们可以以相同的概率移动到箭头指向的每一个状态。

总结一下，在一个简单的强化学习问题中，我们有 4 个主要矩阵：

+   奖励矩阵 **R**

+   状态值函数 **V**

+   策略矩阵 **P**

+   状态矩阵 **S**。

此外，我们需要一个有限的动作集合 **A**。

为了理论上评估每个状态，我们使用贝尔曼方程：

![](../Images/ad0993137593b7dfc5db6303504664b5.png)

状态 **s** 的贝尔曼方程与策略 pi

为了实际评估状态值，我们使用价值迭代算法：

![](../Images/f0ab254caf5769c503bc05223296f649.png)

价值迭代算法简化；作者拍摄

强化学习任务的目标是找到我们的代理可以遵循的最佳策略。

随意使用代码并在此处进行调整：[https://github.com/Eligijus112/rl-snake-game](https://github.com/Eligijus112/rl-snake-game)。

祝学习愉快！

[1]

作者：**理查德·S·萨顿，安德鲁·G·巴托**

年份：2018

标题：**强化学习：导论**

链接：[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
