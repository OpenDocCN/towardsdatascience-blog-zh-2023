- en: A Quick Fix for Your Sluggish Python Code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/performance-fix-for-slow-python-lru-cache-f9a454776716](https://towardsdatascience.com/performance-fix-for-slow-python-lru-cache-f9a454776716)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dramatically speed up reusable computationally intensive tasks.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thuwarakesh.medium.com/?source=post_page-----f9a454776716--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page-----f9a454776716--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f9a454776716--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f9a454776716--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page-----f9a454776716--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f9a454776716--------------------------------)
    ·6 min read·Jun 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e339d712f7ddce36ca1ce48ec399b29.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ryan Johnston](https://unsplash.com/@ryanjohnstonco?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Computationally intensive tasks are everywhere now.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: We are using resource-intensive techniques such as LLMs and Generative AI a
    lot these days.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Whoever uses precious resources would know how daunting it is to do the same
    task again, even though we know the results will be the same. You’d blame yourself
    for not storing the results of its previous run.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: This is where the LRU cache helps us. LRU stands for Least Recently Used. It’s
    one of the many caching strategies. Let’s first understand how it works.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[](/python-decorators-for-data-science-6913f717669a?source=post_page-----f9a454776716--------------------------------)
    [## 5 Python Decorators I Use in Almost All My Data Science Projects'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Decorators provide a new and convenient way for everything from caching to sending
    notifications.
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/python-decorators-for-data-science-6913f717669a?source=post_page-----f9a454776716--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: How does @lru_cache work in Python?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine your brain is a small toy box. It can only fit five toys. Your friends
    keep asking you about different toys, and you use your superhero memory to remember
    and tell stories about these toys.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Some toys are easy to find because your friends often ask about them, so they’re
    at the top. Some toys are harder to find because they’re at the bottom of the
    box.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: After you tell a story about a toy, you put it back on top to make things easier.
    That way, the toys your friends ask about the most are always easy to find. This
    is called the “Least Recently Used” or LRU strategy.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: And if you get a new toy, but the box is full, you remove the toy that hasn’t
    been asked about for the longest time. If a friend asks about it, you can still
    find it in your big toy warehouse, which takes longer. That’s how LRU caching
    works!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你得到一个新玩具，但盒子满了，你会移除最久未被询问的玩具。如果一个朋友询问，你仍然可以在你的大玩具仓库中找到它，这需要更长的时间。这就是 LRU 缓存的工作方式！
- en: '[](https://levelup.gitconnected.com/streamlit-openai-gpt3-example-app-b333da955ceb?source=post_page-----f9a454776716--------------------------------)
    [## Create GPT3 Powered Apps in Minutes With Streamlit'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[创建 GPT3 支持的应用程序仅需几分钟，使用 Streamlit](https://levelup.gitconnected.com/streamlit-openai-gpt3-example-app-b333da955ceb?source=post_page-----f9a454776716--------------------------------)
    [## 创建 GPT3 支持的应用程序仅需几分钟，使用 Streamlit'
- en: Learn to build intelligent apps without worrying too much about software development.
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习如何构建智能应用，而不必过于担心软件开发。
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/streamlit-openai-gpt3-example-app-b333da955ceb?source=post_page-----f9a454776716--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/streamlit-openai-gpt3-example-app-b333da955ceb?source=post_page-----f9a454776716--------------------------------)'
- en: Since version 3.2, Python ships with an inbuilt `@lru_cache` decor, which you
    can use on any of your functions. It makes your process work much like the toy
    box example.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从版本 3.2 开始，Python 自带了一个内置的 `@lru_cache` 装饰器，你可以在任何函数上使用它。它使你的过程工作得很像玩具箱的例子。
- en: If a part has been executed before, the results will be pulled from the cache
    during subsequent runs. And just like your toy box getting full, you can specify
    a maximum size for the cache.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某部分之前已经执行过，结果将在后续运行中从缓存中提取。就像你的玩具箱变满了一样，你可以指定缓存的最大大小。
- en: '[](/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=post_page-----f9a454776716--------------------------------)
    [## A Little Pandas Hack to Handle Large Datasets with Limited Memory'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[小型 Pandas 技巧处理有限内存的大数据集](https://a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=post_page-----f9a454776716--------------------------------)
    [## 小型 Pandas 技巧处理有限内存的大数据集'
- en: The Pandas defaults aren’t optimal. A tiny configuration can compress your dataframe
    to fit in your memory.
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pandas 默认设置并不理想。一个小配置可以将你的数据框压缩到适合你的内存中。
- en: towardsdatascience.com](/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=post_page-----f9a454776716--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=post_page-----f9a454776716--------------------------------)'
- en: Where can we use lru_cache in our daily work?
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们在日常工作中可以在哪里使用 `lru_cache`？
- en: 'Consider a **long-running database query**. We’ve all been there: waiting,
    fingers tapping, for the database to finally respond with the needed data. This
    often happens when we need to rerun Jupyter Notebooks.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个 **长期运行的数据库查询**。我们都经历过：等待，手指敲击，直到数据库最终响应所需的数据。这通常发生在我们需要重新运行 Jupyter Notebooks
    时。
- en: Now, imagine doing the waiting once and then retrieving the data instantly the
    next time. That’s the magic of lru_cache.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下，第一次等待一次，然后下次立即检索数据。这就是 `lru_cache` 的魔力。
- en: It’s not only about database queries. What about **APIs**? Especially those
    with a **Pay-as-you-go pricing model** like the OpenAI API. If the prompt looks
    the same, ith lru_cache, we call once, pay for it, and reuse the result without
    paying again.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅是关于数据库查询。那 **API** 呢？尤其是那些具有 **按需付费定价模型** 的 API，如 OpenAI API。如果提示看起来相同，使用
    `lru_cache`，我们调用一次，支付费用，然后重用结果而无需再次支付。
- en: Also, If you’re dealing with external data queries where the connection to the
    source is not always reliable — such as with **web scraping** — lru_cache can
    be a lifesaver. Instead of constantly wrestling with an **unstable connection**,
    you can fetch the data once, cache it, and refer to it without worrying about
    connectivity issues.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你处理的外部数据查询连接到源的可靠性不总是很高——比如 **网页抓取**——`lru_cache` 可以成为救星。你可以一次性获取数据，将其缓存，然后在无需担心连接问题的情况下引用它，而不是不断与
    **不稳定的连接** 做斗争。
- en: '[](https://levelup.gitconnected.com/3-ways-of-web-scraping-in-python-e953c4a96ec2?source=post_page-----f9a454776716--------------------------------)
    [## The Serene Symphony of Python Web Scraping — in 3 Movements'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[Python 网页抓取的宁静交响曲——分为 3 个部分](https://levelup.gitconnected.com/3-ways-of-web-scraping-in-python-e953c4a96ec2?source=post_page-----f9a454776716--------------------------------)
    [## Python 网页抓取的宁静交响曲——分为 3 个部分'
- en: The easiest, the most flexible, and the most comprehensive ways to do web scraping
    in Python
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Python 中进行网页抓取的最简单、最灵活和最全面的方法
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/3-ways-of-web-scraping-in-python-e953c4a96ec2?source=post_page-----f9a454776716--------------------------------)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/3-ways-of-web-scraping-in-python-e953c4a96ec2?source=post_page-----f9a454776716--------------------------------)
- en: And let’s not forget **computationally heavy tasks**. If your function is a
    number cruncher, taking in data and churning out results after heavy computation,
    lru_cache can drastically reduce the load on your system resources. This is often
    the case when your application involves running a heavy ML model like **BERT LLM**
    or having a comprehensive **search functionality**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，不要忘记**计算密集型任务**。如果你的函数需要处理大量数据并进行复杂计算，`lru_cache`可以显著减少对系统资源的负担。当你的应用涉及运行重型机器学习模型如**BERT
    LLM**或拥有全面的**搜索功能**时，情况通常是这样的。
- en: In all these cases, the value proposition is the same. **Time saved, money saved,
    resources saved**. And this is possible as long as we assume that the output stays
    the same within a reasonable time frame. In the grand scheme, Python’s lru_cache
    isn’t a technical solution but a principle of efficiency. And in a world where
    every second counts, efficiency is king.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些情况下，价值主张都是相同的。**节省时间、节省金钱、节省资源**。这在输出保持一致的合理时间范围内是可能的。从宏观上看，Python的`lru_cache`不是一个技术解决方案，而是一种效率原则。在每一秒都很重要的世界里，效率就是至高无上的。
- en: Testing lru_cache on recursive functions
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试`lru_cache`在递归函数上的效果
- en: Here’s a typical example of a Fibonacci number generation. See how long it takes
    to run with and without lru_caching.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个斐波那契数生成的典型示例。看看使用和不使用`lru_cache`的运行时间。
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now let’s import `lru_cache` from `functools` module and annotate the function
    with it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从`functools`模块导入`lru_cache`并对函数进行标注。
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The code achieves significant time-saving by introducing the `lru_cache` decorator
    for memoization compared to the non-memoized version.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入`lru_cache`装饰器来进行备忘录化，与未备忘录化的版本相比，代码实现了显著的时间节省。
- en: In the first example (without `lru_cache`), calculating the 40th Fibonacci number
    took approximately 19.45 seconds. In the second example (with `lru_cache`), calculating
    the 40th Fibonacci number took approximately 8.89e-05 seconds.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例（没有`lru_cache`）中，计算第40个斐波那契数大约花费了19.45秒。在第二个示例（使用`lru_cache`）中，计算第40个斐波那契数仅花费了大约8.89e-05秒。
- en: That’s like 99.99% time saved.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像节省了99.99%的时间。
- en: Here’s a plot of execution time for various Fibonacci numbers. See that the
    non-cached version takes up exponentially higher execution time, whereas the cached
    version takes a teensy little amount of time for even more significant numbers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是不同斐波那契数执行时间的图表。可以看到，非缓存版本的执行时间呈指数增长，而缓存版本即使对于更大的数字也只需要极少的时间。
- en: '![](../Images/1e6147586fd3a0a2f8b769629cddf64c.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e6147586fd3a0a2f8b769629cddf64c.png)'
- en: Image by the Author — how @lru_cache improves execution times of Fibonacci number
    generation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片 — 展示了`@lru_cache`如何提高斐波那契数生成的执行时间。
- en: '[](/sql-on-pandas-usign-duckdb-f7cd238a0a5a?source=post_page-----f9a454776716--------------------------------)
    [## SQL on Pandas — My New Favorite for 10X Speed.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/sql-on-pandas-usign-duckdb-f7cd238a0a5a?source=post_page-----f9a454776716--------------------------------)
    [## SQL on Pandas — 我新的10倍速度的最爱。'
- en: Bringing the best of both worlds together
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将两者的最佳之处结合在一起
- en: towardsdatascience.com](/sql-on-pandas-usign-duckdb-f7cd238a0a5a?source=post_page-----f9a454776716--------------------------------)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/sql-on-pandas-usign-duckdb-f7cd238a0a5a?source=post_page-----f9a454776716--------------------------------)
- en: The @lru_cache is not a silver bullet
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`@lru_cache`并不是万灵药。'
- en: While `@lru_cache` is instrumental in improving performance, but that doesn’t
    mean you can use any function and forget about the performance altogether.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`@lru_cache`在提升性能方面非常重要，但这并不意味着你可以随意使用任何函数而不考虑性能。
- en: '[](/numpy-vectorization-speed-ffdab5deb402?source=post_page-----f9a454776716--------------------------------)
    [## Is Your Python For-loop Slow? Use NumPy Instead'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/numpy-vectorization-speed-ffdab5deb402?source=post_page-----f9a454776716--------------------------------)
    [## 你的Python For-loop很慢？使用NumPy替代'
- en: When speed matters, lists aren’t the best.
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当速度很重要时，列表不是最佳选择。
- en: towardsdatascience.com](/numpy-vectorization-speed-ffdab5deb402?source=post_page-----f9a454776716--------------------------------)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/numpy-vectorization-speed-ffdab5deb402?source=post_page-----f9a454776716--------------------------------)
- en: '**The function being cached must be hashable.**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**被缓存的函数必须是可哈希的。**'
- en: This means it must be immutable and able to be converted into a unique integer
    for dictionary key purposes. This typically isn’t an issue for functions, but
    you can’t cache methods of mutable classes or functions that take mutable arguments.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它必须是不可变的，并且能够转换为用于字典键目的的唯一整数。对于函数来说，这通常不是问题，但你不能缓存可变类的方法或接受可变参数的函数。
- en: 'For instance, the following function is immutable. Hence lru_cache works fine:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下函数是不可变的，因此`lru_cache`工作正常：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: But the next one is not. Calling this would throw an error.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 但下一个则不是。调用它会抛出错误。
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[](/python-3-11-is-indeed-faster-than-3-10-1247531e771b?source=post_page-----f9a454776716--------------------------------)
    [## Python 3.11 Is Indeed Faster Than 3.10'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/python-3-11-is-indeed-faster-than-3-10-1247531e771b?source=post_page-----f9a454776716--------------------------------)
    [## Python 3.11确实比3.10更快'
- en: Comparing side-by-side with bubble sort and recurrence functions confirms it.
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与冒泡排序和递归函数逐一比较确认了这一点。
- en: towardsdatascience.com](/python-3-11-is-indeed-faster-than-3-10-1247531e771b?source=post_page-----f9a454776716--------------------------------)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/python-3-11-is-indeed-faster-than-3-10-1247531e771b?source=post_page-----f9a454776716--------------------------------)
- en: '**lru_cache isn’t great if your function depends on external sources**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你的函数依赖于外部源，`lru_cache`就不太适用了**'
- en: The cache doesn’t automatically expire entries. If your function’s return values
    can change over time (for example, if it’s based on a file system or network operations),
    then using `lru_cache` it could lead to stale results being returned.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存不会自动过期。如果函数的返回值可能会随时间变化（例如，如果它基于文件系统或网络操作），那么使用`lru_cache`可能会导致返回过时的结果。
- en: 'The following Python script simulates an external source and tests this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python脚本模拟了一个外部源并进行了测试：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For such instances, you need a caching technique that supports TTLs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，你需要一种支持TTL的缓存技术。
- en: '[](/github-automated-testing-python-fdfe5aec9446?source=post_page-----f9a454776716--------------------------------)
    [## How to Run Python Tests on Every Commit Using GitHub Actions?'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/github-automated-testing-python-fdfe5aec9446?source=post_page-----f9a454776716--------------------------------)
    [## 如何在每次提交时使用GitHub Actions运行Python测试？'
- en: Automate the boring stuff and ensure your code quality with a CI pipeline.
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动化无聊的任务，确保你的代码质量，通过CI管道。
- en: towardsdatascience.com](/github-automated-testing-python-fdfe5aec9446?source=post_page-----f9a454776716--------------------------------)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/github-automated-testing-python-fdfe5aec9446?source=post_page-----f9a454776716--------------------------------)
- en: Conclusion
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: lru_cache is a very simple technique to store function outputs so that the computations
    don’t have to run again and again. While it’s instrumental in saving expensive
    computational resources, it doesn’t work well in all cases.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`lru_cache`是一种非常简单的技术，用于存储函数输出，以便计算无需重复执行。虽然它在节省昂贵计算资源方面很有用，但并非所有情况下都能很好地工作。'
- en: Yet, as long as your function doesn’t depend on external inputs and it’s hashable,
    lru_cache is definitely a great tool.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，只要你的函数不依赖外部输入且是可哈希的，`lru_cache`绝对是一个很好的工具。
- en: Thanks for reading, friend! Say Hi to me on [**LinkedIn**](https://www.linkedin.com/in/thuwarakesh/),
    [**Twitter**](https://twitter.com/Thuwarakesh), and [**Medium**](https://thuwarakesh.medium.com/).
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 感谢阅读，朋友！在[**LinkedIn**](https://www.linkedin.com/in/thuwarakesh/)、[**Twitter**](https://twitter.com/Thuwarakesh)和[**Medium**](https://thuwarakesh.medium.com/)上向我打个招呼吧。
- en: ''
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not a Medium member yet? Please use this link to [**become a member**](https://thuwarakesh.medium.com/membership)
    because, at no extra cost for you, I earn a small commission for referring you.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 还不是Medium会员？请使用此链接[**成为会员**](https://thuwarakesh.medium.com/membership)，因为在不增加你额外费用的情况下，我可以获得少量佣金作为推荐费。
