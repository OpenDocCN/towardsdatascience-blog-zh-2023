- en: A Quick Fix for Your Sluggish Python Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/performance-fix-for-slow-python-lru-cache-f9a454776716](https://towardsdatascience.com/performance-fix-for-slow-python-lru-cache-f9a454776716)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dramatically speed up reusable computationally intensive tasks.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thuwarakesh.medium.com/?source=post_page-----f9a454776716--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page-----f9a454776716--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f9a454776716--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f9a454776716--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page-----f9a454776716--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f9a454776716--------------------------------)
    ·6 min read·Jun 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e339d712f7ddce36ca1ce48ec399b29.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ryan Johnston](https://unsplash.com/@ryanjohnstonco?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Computationally intensive tasks are everywhere now.
  prefs: []
  type: TYPE_NORMAL
- en: We are using resource-intensive techniques such as LLMs and Generative AI a
    lot these days.
  prefs: []
  type: TYPE_NORMAL
- en: Whoever uses precious resources would know how daunting it is to do the same
    task again, even though we know the results will be the same. You’d blame yourself
    for not storing the results of its previous run.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the LRU cache helps us. LRU stands for Least Recently Used. It’s
    one of the many caching strategies. Let’s first understand how it works.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/python-decorators-for-data-science-6913f717669a?source=post_page-----f9a454776716--------------------------------)
    [## 5 Python Decorators I Use in Almost All My Data Science Projects'
  prefs: []
  type: TYPE_NORMAL
- en: Decorators provide a new and convenient way for everything from caching to sending
    notifications.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/python-decorators-for-data-science-6913f717669a?source=post_page-----f9a454776716--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: How does @lru_cache work in Python?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine your brain is a small toy box. It can only fit five toys. Your friends
    keep asking you about different toys, and you use your superhero memory to remember
    and tell stories about these toys.
  prefs: []
  type: TYPE_NORMAL
- en: Some toys are easy to find because your friends often ask about them, so they’re
    at the top. Some toys are harder to find because they’re at the bottom of the
    box.
  prefs: []
  type: TYPE_NORMAL
- en: After you tell a story about a toy, you put it back on top to make things easier.
    That way, the toys your friends ask about the most are always easy to find. This
    is called the “Least Recently Used” or LRU strategy.
  prefs: []
  type: TYPE_NORMAL
- en: And if you get a new toy, but the box is full, you remove the toy that hasn’t
    been asked about for the longest time. If a friend asks about it, you can still
    find it in your big toy warehouse, which takes longer. That’s how LRU caching
    works!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/streamlit-openai-gpt3-example-app-b333da955ceb?source=post_page-----f9a454776716--------------------------------)
    [## Create GPT3 Powered Apps in Minutes With Streamlit'
  prefs: []
  type: TYPE_NORMAL
- en: Learn to build intelligent apps without worrying too much about software development.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/streamlit-openai-gpt3-example-app-b333da955ceb?source=post_page-----f9a454776716--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Since version 3.2, Python ships with an inbuilt `@lru_cache` decor, which you
    can use on any of your functions. It makes your process work much like the toy
    box example.
  prefs: []
  type: TYPE_NORMAL
- en: If a part has been executed before, the results will be pulled from the cache
    during subsequent runs. And just like your toy box getting full, you can specify
    a maximum size for the cache.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=post_page-----f9a454776716--------------------------------)
    [## A Little Pandas Hack to Handle Large Datasets with Limited Memory'
  prefs: []
  type: TYPE_NORMAL
- en: The Pandas defaults aren’t optimal. A tiny configuration can compress your dataframe
    to fit in your memory.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=post_page-----f9a454776716--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Where can we use lru_cache in our daily work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider a **long-running database query**. We’ve all been there: waiting,
    fingers tapping, for the database to finally respond with the needed data. This
    often happens when we need to rerun Jupyter Notebooks.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine doing the waiting once and then retrieving the data instantly the
    next time. That’s the magic of lru_cache.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not only about database queries. What about **APIs**? Especially those
    with a **Pay-as-you-go pricing model** like the OpenAI API. If the prompt looks
    the same, ith lru_cache, we call once, pay for it, and reuse the result without
    paying again.
  prefs: []
  type: TYPE_NORMAL
- en: Also, If you’re dealing with external data queries where the connection to the
    source is not always reliable — such as with **web scraping** — lru_cache can
    be a lifesaver. Instead of constantly wrestling with an **unstable connection**,
    you can fetch the data once, cache it, and refer to it without worrying about
    connectivity issues.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/3-ways-of-web-scraping-in-python-e953c4a96ec2?source=post_page-----f9a454776716--------------------------------)
    [## The Serene Symphony of Python Web Scraping — in 3 Movements'
  prefs: []
  type: TYPE_NORMAL
- en: The easiest, the most flexible, and the most comprehensive ways to do web scraping
    in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/3-ways-of-web-scraping-in-python-e953c4a96ec2?source=post_page-----f9a454776716--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: And let’s not forget **computationally heavy tasks**. If your function is a
    number cruncher, taking in data and churning out results after heavy computation,
    lru_cache can drastically reduce the load on your system resources. This is often
    the case when your application involves running a heavy ML model like **BERT LLM**
    or having a comprehensive **search functionality**.
  prefs: []
  type: TYPE_NORMAL
- en: In all these cases, the value proposition is the same. **Time saved, money saved,
    resources saved**. And this is possible as long as we assume that the output stays
    the same within a reasonable time frame. In the grand scheme, Python’s lru_cache
    isn’t a technical solution but a principle of efficiency. And in a world where
    every second counts, efficiency is king.
  prefs: []
  type: TYPE_NORMAL
- en: Testing lru_cache on recursive functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here’s a typical example of a Fibonacci number generation. See how long it takes
    to run with and without lru_caching.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s import `lru_cache` from `functools` module and annotate the function
    with it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The code achieves significant time-saving by introducing the `lru_cache` decorator
    for memoization compared to the non-memoized version.
  prefs: []
  type: TYPE_NORMAL
- en: In the first example (without `lru_cache`), calculating the 40th Fibonacci number
    took approximately 19.45 seconds. In the second example (with `lru_cache`), calculating
    the 40th Fibonacci number took approximately 8.89e-05 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: That’s like 99.99% time saved.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a plot of execution time for various Fibonacci numbers. See that the
    non-cached version takes up exponentially higher execution time, whereas the cached
    version takes a teensy little amount of time for even more significant numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e6147586fd3a0a2f8b769629cddf64c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author — how @lru_cache improves execution times of Fibonacci number
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/sql-on-pandas-usign-duckdb-f7cd238a0a5a?source=post_page-----f9a454776716--------------------------------)
    [## SQL on Pandas — My New Favorite for 10X Speed.'
  prefs: []
  type: TYPE_NORMAL
- en: Bringing the best of both worlds together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/sql-on-pandas-usign-duckdb-f7cd238a0a5a?source=post_page-----f9a454776716--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The @lru_cache is not a silver bullet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While `@lru_cache` is instrumental in improving performance, but that doesn’t
    mean you can use any function and forget about the performance altogether.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/numpy-vectorization-speed-ffdab5deb402?source=post_page-----f9a454776716--------------------------------)
    [## Is Your Python For-loop Slow? Use NumPy Instead'
  prefs: []
  type: TYPE_NORMAL
- en: When speed matters, lists aren’t the best.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/numpy-vectorization-speed-ffdab5deb402?source=post_page-----f9a454776716--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**The function being cached must be hashable.**'
  prefs: []
  type: TYPE_NORMAL
- en: This means it must be immutable and able to be converted into a unique integer
    for dictionary key purposes. This typically isn’t an issue for functions, but
    you can’t cache methods of mutable classes or functions that take mutable arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, the following function is immutable. Hence lru_cache works fine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: But the next one is not. Calling this would throw an error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[](/python-3-11-is-indeed-faster-than-3-10-1247531e771b?source=post_page-----f9a454776716--------------------------------)
    [## Python 3.11 Is Indeed Faster Than 3.10'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing side-by-side with bubble sort and recurrence functions confirms it.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/python-3-11-is-indeed-faster-than-3-10-1247531e771b?source=post_page-----f9a454776716--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**lru_cache isn’t great if your function depends on external sources**'
  prefs: []
  type: TYPE_NORMAL
- en: The cache doesn’t automatically expire entries. If your function’s return values
    can change over time (for example, if it’s based on a file system or network operations),
    then using `lru_cache` it could lead to stale results being returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python script simulates an external source and tests this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For such instances, you need a caching technique that supports TTLs.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/github-automated-testing-python-fdfe5aec9446?source=post_page-----f9a454776716--------------------------------)
    [## How to Run Python Tests on Every Commit Using GitHub Actions?'
  prefs: []
  type: TYPE_NORMAL
- en: Automate the boring stuff and ensure your code quality with a CI pipeline.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/github-automated-testing-python-fdfe5aec9446?source=post_page-----f9a454776716--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: lru_cache is a very simple technique to store function outputs so that the computations
    don’t have to run again and again. While it’s instrumental in saving expensive
    computational resources, it doesn’t work well in all cases.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, as long as your function doesn’t depend on external inputs and it’s hashable,
    lru_cache is definitely a great tool.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading, friend! Say Hi to me on [**LinkedIn**](https://www.linkedin.com/in/thuwarakesh/),
    [**Twitter**](https://twitter.com/Thuwarakesh), and [**Medium**](https://thuwarakesh.medium.com/).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not a Medium member yet? Please use this link to [**become a member**](https://thuwarakesh.medium.com/membership)
    because, at no extra cost for you, I earn a small commission for referring you.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
