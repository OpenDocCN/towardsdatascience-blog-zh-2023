- en: Build Industry-Specific LLMs Using Retrieval Augmented Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/build-industry-specific-llms-using-retrieval-augmented-generation-af9e98bb6f68](https://towardsdatascience.com/build-industry-specific-llms-using-retrieval-augmented-generation-af9e98bb6f68)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Organizations are in a race to adopt Large Language Models. Let’s dive into
    how you can build industry-specific LLMs Through RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----af9e98bb6f68--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----af9e98bb6f68--------------------------------)[](https://towardsdatascience.com/?source=post_page-----af9e98bb6f68--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----af9e98bb6f68--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----af9e98bb6f68--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----af9e98bb6f68--------------------------------)
    ·10 min read·May 31, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Companies stand to gain a lot of productivity improvements through LLMs like
    ChatGPT. But try asking ChatGPT “what is the current inflation in the U.S.” and
    it gives:'
  prefs: []
  type: TYPE_NORMAL
- en: I apologize for the confusion, but as an AI language model, I don’t have real-time
    data or browsing capabilities. My responses are based on information available
    up until September 2021\. Therefore, I cannot provide you with the current inflation
    rate in the U.S.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Which is a problem. ChatGPT is clearly missing relevant timely context, which
    could be essential while making informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: How Microsoft Is Solving This
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the Microsoft Build session [Vector Search Isn’t Enough](https://build.microsoft.com/en-US/sessions/038984b3-7c5d-4cc6-b24e-5d9f62bc2f0e?wt.mc_ID=Build2023_esc_corp_em_oo_mto_Marketo_FPnews_Elastic),
    they lay out their product that combines less context-aware LLMs with vector search,
    to create a more engaging experience.
  prefs: []
  type: TYPE_NORMAL
- en: The talk starts from the opposite direction of this piece — from the point of
    view of Elastic Search (or vector search) — and the idea that search by itself
    is limited, and adding the layer of LLMs can vastly improve the search experience.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea is that adding relevant context to LLMs can vastly improve user
    experience especially in most commercial cases, where LLMs haven’t seen these
    sorts of data. Vector search helps in choosing what the relevant context is, when
    you have vast amounts of data, including 100s (or more) documents.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Search 101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/df4126ecf386cf34f81c491832c86c37.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector Search 101 | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: 'Vectorizing is converting data into dimensions. In the case above, you can
    see 2 dimensions: size and type. Size has 2 values (small or big), and type has
    2 values (tree or animal). This is just a conceptual example, and can be scaled
    to hundreds (or more values).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98f0411346236ea58fff19a27e99849e.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector Search 101 With Words | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: You can do the same thing with words or sentences, instead of pictures. Notice
    how in the above example, the vectorization is able to capture the semantic representation
    i.e. it knows that a sentence talking about a bird swooping in on a baby chipmunk
    should be in the (small, animal) quadrant, whereas the sentence talking about
    yesterday’s storm when a large tree fell on the road should be in the (big, tree)
    quadrant. In reality, there are more than 2 dimensions — 384 for sentence transformer
    embeddings (sentences or chunks of text are passed through the BERT transformer,
    and 384 numbers corresponding to the 384 dimensions are outputted.)
  prefs: []
  type: TYPE_NORMAL
- en: Vector Search + LLM (RAG)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vector search is now commonplace in all search (Google, Bing, pick your favorite
    search engine). Think about if your entire search database has only these 4 documents.
    You can ask the question, “Tell me something about a big tree.”
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be:'
  prefs: []
  type: TYPE_NORMAL
- en: During yesterday’s storm, a large tree fell on the road.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Just the most relevant document in the database. But if you asked an LLM the
    same question (and append in the 4 “documents” at the end), this is what you get
    (ChatGPT in this example):'
  prefs: []
  type: TYPE_NORMAL
- en: A big tree is typically a large, towering plant with a sturdy trunk, branches,
    and an extensive network of roots. These trees can reach impressive heights, often
    extending well above the surrounding vegetation. They serve as essential components
    of many ecosystems, providing shade, shelter, and food for various organisms.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The incident you mentioned about a large tree falling on the road during a storm
    indicates the power and impact of severe weather conditions. Storms with strong
    winds can uproot or break trees, leading to road blockages and potential hazards
    for vehicles and pedestrians.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In another scenario you mentioned, while driving on the road in Bandipur, you
    encountered a large elephant crossing the road. Bandipur is a national park and
    wildlife sanctuary located in India, known for its rich biodiversity and diverse
    animal population. Elephants are one of the prominent species found in Bandipur
    and are known to traverse the area, sometimes crossing roads. Encounters like
    these highlight the importance of wildlife conservation and the need for humans
    to coexist with animals in their natural habitats.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Switching topics to Bonsai, they are miniature trees that are grown in small
    pots and meticulously shaped and pruned to maintain their small size. Bonsai cultivation
    is an ancient art form that originated in East Asia and has gained popularity
    worldwide. Due to their limited root space, Bonsai trees require regular watering
    to prevent them from drying out. The small pots can cause the soil to dry quickly,
    so it’s crucial to monitor the moisture levels and water them accordingly.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Finally, you mentioned observing a bird swooping in on a baby chipmunk through
    your window. This observation reflects a moment of nature in action, where a bird
    might have been hunting for food. Birds of prey and certain bird species, such
    as hawks or owls, are known to swoop down on small mammals like chipmunks as part
    of their natural hunting behavior. It’s fascinating to witness such interactions,
    as they provide insights into the dynamics of the animal kingdom.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Overall, these snippets of nature — whether they involve big trees, elephants,
    Bonsai trees, or bird predation — highlight the diversity and wonder of the natural
    world that surrounds us.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: That answer seemed much more well rounded. ChatGPT was using its internal knowledge
    from being trained on trillions of documents, in combination with the context
    it was given.
  prefs: []
  type: TYPE_NORMAL
- en: 'But let’s say you have more than just 4 documents — maybe thousands or millions,
    that you can’t fit into the ChatGPT prompt. In this case, you can use vector search
    to narrow down the context most likely to contain the answer, append it to the
    prompt, and ask the same question as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the (truncated) answer it now gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/916cbd5dcf05aabd6693ebb0a7bb4d78.png)'
  prefs: []
  type: TYPE_IMG
- en: ChatGPT answer | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: 'You could then have a database, that stores documents and embeddings. You can
    have another DB that stores queries, and finds the most relevant documents based
    on the queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9574fca57d2d8141c4113402aa2b5ee7.png)'
  prefs: []
  type: TYPE_IMG
- en: Document DB (Left) and Quey DB (Right) | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: Once you have the most similar document(s) by query, you can feed that to any
    LLM like ChatGPT. By this simple trick, you have augmented your LLM using document
    retrieval! This is also known as retrieval augmented generation (RAG).
  prefs: []
  type: TYPE_NORMAL
- en: Building Industry-Specific Q&A Models Using RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/a9add121e8626cc6254e3c2d362647db.png)'
  prefs: []
  type: TYPE_IMG
- en: RAG Prototype | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: The above diagram outlines how to build a basic RAG that utilizes an LLM over
    custom documents for question answering. The first part is splitting multiple
    documents into manageable chunks, the associated parameter is the ***maximum chunk
    length***. These chunks should be of the typical (minimum) size of text that contain
    the answers to the typical questions asked. This is because sometimes the question
    you ask might have answers at multiple locations within the document. For example,
    you might ask the question “What was X company’s performance from 2015 to 2020?”
    And you might have a large document (or multiple documents) containing specific
    information about company performance over the years in different parts of the
    document. You would ideally want to capture all disparate parts of the document(s)
    containing this information, link them together, and pass to an LLM for answering
    based on these filtered and concatenated document chunks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ***maximum context length*** is basically the maximum length for concatenating
    various chunks together — leaving some space for the question itself, and the
    output answer (remember that LLMs like ChatGPT have a strict length limit that
    includes all the content: question, context, and answer.'
  prefs: []
  type: TYPE_NORMAL
- en: The ***similarity threshold*** is the way to compare the question with document
    chunks, to find the top chunks, most likely to contain the answer. Cosine similarity
    is the typical metric used, but you might want to weight different metrics. Such
    as including a keyword metric to weight contexts with certain keywords more. For
    example, you might want to weight contexts that contain the words “abstract” or
    “summary” when you ask the question to an LLM to summarize a document.
  prefs: []
  type: TYPE_NORMAL
- en: If you want an easy way to test out Generative Q&A over custom documents, check
    out my [API](https://rapidapi.com/skandavivek/api/chatgpt-powered-question-answering-over-documents)
    and [code](https://github.com/skandavivek/web-qa) that uses ChatGPT in the backend.
  prefs: []
  type: TYPE_NORMAL
- en: Prototype ChatGPT boosted by RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s illustrate the usefulness of RAG through an example. [EMAlpha](https://www.emalpha.com/)
    is a company that provides insights into emerging markets — basically the economies
    of up and coming nations like India, China, Brazil, etc (full disclosure — I’m
    an advisor at EMAlpha). The company is building a ChatGPT powered app to generate
    insights into emerging economies based on user inputs. The dashboard looks something
    like this — where you can compare the output of ChatGPT vs the RAG version of
    ChatGPT (EM-GPT) that is able to query financial documents from IMF in the backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15afaf108e3ad7cb76296e37a68b7029.png)'
  prefs: []
  type: TYPE_IMG
- en: EM-GPT from [EMAlpha](https://www.emalpha.com/)| Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is the answer from ChatGPT for the question “What is Nepal’s GDP by year?”:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ac4fe7cefd5459cfa8b5988647b2988.png)'
  prefs: []
  type: TYPE_IMG
- en: ChatGPT Response | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT only returns the GDP until 2019 and it says that if you want more information,
    look at IMF. But if you want to figure out where this data is located in the IMF
    website, it is hard and you need to have an idea about where documents are stored
    on the website. After some searching, you [find the document is here](https://www.imf.org/en/Publications/CR/Issues/2023/05/04/Nepal-Staff-Report-for-the-2023-Article-IV-Consultation-First-and-Second-Reviews-Under-the-533075).
    Even then, figuring out where exactly the GDP information is present needs a lot
    of scrolling.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12c8e35f80594a211a2905f72a983f9c.png)'
  prefs: []
  type: TYPE_IMG
- en: IMF Document on Nepal Economics | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: 'As you see above, it is quite hard to find this data. But when you ask EM-GPT
    the same question, it tracks down the relevant context, and finds the answer as
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/166f3df5dbabead53a551c71c3bad470.png)'
  prefs: []
  type: TYPE_IMG
- en: EM-GPT Answer | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: Below is the exact prompt sent to ChatGPT to answer this question. It’s pretty
    impressive that it managed to make sense of this formatted text, extract the right
    information — and format it into a nice human readable format!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4cb6feca4c004dd7d864f864002c6633.png)'
  prefs: []
  type: TYPE_IMG
- en: ChatGPT prompt using context retrieved based on query | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: I took half an hour to find this information on the IMF website, whereas the
    RAG modified ChatGPT only took a few seconds. Vector search alone would be no
    good, as it would probably at best find the text “Nominal GDP”, and not associate
    the numbers with the years. ChatGPT has been trained on multiple such documents
    in the past, so that once the relevant context is added, it knows which parts
    of the text contain the answer and how to format this answer in a nice readable
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG offers a great way to use LLMs powered on custom documents. Companies like
    [Microsoft](https://www.elastic.co/blog/may-2023-launch-announcement?utm_source=organic-social&utm_medium=twitter&utm_campaign=cee-gc&ultron=t1_launch&blade=twitter&hulk=social&utm_content=10092954746&linkId=215905348),
    Google, and Amazon are in a race to build apps that organizations can use in a
    plug-and-play manner. However, the field is still nascent, and industry specific
    apps that use vector search powered LLMs over their custom documents can be first
    movers and edge out their competition.
  prefs: []
  type: TYPE_NORMAL
- en: 'While I have had people asking me which LLM to use and whether or not to fine-tune
    or completely train models over custom documents, **the role of engineering the
    sync between LLMs and vector search is underrated.** Here are a few considerations
    that can significantly enhance or reduce the quality of responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Length of document chunks.** If it is more likely that the right answers
    are contained in diverse parts of the text and need to be stitched together, documents
    should be separated into smaller chunks so that multiple contexts can be appended
    to queries.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Similarity and retrieval metrics.** Sometimes, plain cosine similarity is
    not enough. For example if many documents contain conflicting information about
    the same topic, you might want to restrict searches to certain documents based
    on metadata within those documents. For this, in addition to similarity, you can
    use other filtering metrics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Architecture:** The architecture I have shown is a prototype. For efficiency
    and scalability, considerations of various aspects have to be made including vector
    embedding model, document database, prompt, LLM model choice, etc.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Avoiding Hallucinations:** You might have noticed that the example I showed
    above was ***almost*** correct. The augmented ChatGPT got the amounts right for
    Nepal’s GDP — but the years wrong. In such cases, there needs to be a lot of feedback
    between choosing prompts, extracting data in a ChatGPT friendly format, and evaluating
    in what percentage of the cases there are hallucinations, and what solutions work
    well.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you know how to apply LLMs to your custom data, go build awesome LLM
    based products!
  prefs: []
  type: TYPE_NORMAL
- en: '*If you like this post, follow me — I write on topics related to applying state-of-the-art
    NLP in real-world applications and, more generally, on the intersections between
    data and society.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/skanda-vivek-01619311b/)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are not yet a Medium member and want to support writers like me, feel
    free to sign-up through my referral link:* [*https://skanda-vivek.medium.com/membership*](https://skanda-vivek.medium.com/membership)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Here are some related articles:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[When Should You Fine-Tune LLMs?](https://medium.com/towards-data-science/when-should-you-fine-tune-llms-2dddc09a404a)'
  prefs: []
  type: TYPE_NORMAL
- en: '[LLM Economics: ChatGPT vs Open-Source](https://medium.com/towards-data-science/llm-economics-chatgpt-vs-open-source-dfc29f69fec1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[How Do You Build A ChatGPT-Powered App?](https://medium.com/geekculture/how-do-you-build-a-chatgpt-powered-app-89c83f3e2143)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Extractive vs Generative Q&A — Which is better for your business?](https://medium.com/towards-data-science/extractive-vs-generative-q-a-which-is-better-for-your-business-5a8a1faab59a)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Fine-Tune Transformer Models For Question Answering On Custom Data](https://medium.com/towards-data-science/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Unleashing the Power of Generative AI For Your Customers](https://medium.com/geekculture/unleashing-the-power-of-generative-ai-for-your-customers-70297f1c9698)'
  prefs: []
  type: TYPE_NORMAL
