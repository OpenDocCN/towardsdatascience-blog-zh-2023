- en: Streaming in Data Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据工程中的流数据
- en: 原文：[https://towardsdatascience.com/streaming-in-data-engineering-2bb2b9b3b603](https://towardsdatascience.com/streaming-in-data-engineering-2bb2b9b3b603)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/streaming-in-data-engineering-2bb2b9b3b603](https://towardsdatascience.com/streaming-in-data-engineering-2bb2b9b3b603)
- en: Streaming data pipelines and real-time analytics
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流数据管道和实时分析
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)
    ·9 min read·Dec 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)
    ·阅读时间9分钟·2023年12月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b5bdd62c71b5b0a4888786ad3318772d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5bdd62c71b5b0a4888786ad3318772d.png)'
- en: Photo by [DESIGNECOLOGIST](https://unsplash.com/@designecologist?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[DESIGNECOLOGIST](https://unsplash.com/@designecologist?utm_source=medium&utm_medium=referral)提供，在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上
- en: '**Streaming** is one of the most popular data pipeline design patterns. Using
    an event as a single data point creates a constant flow of data from one point
    to another enabling an opportunity for real-time data ingestion and analytics.
    If you want to familiarise yourself with data streaming and learn how to build
    real-time data pipelines this story is for you. Learn how to test the solution,
    and mock test data to simulate event streams. This article is a great opportunity
    to acquire some sought-after data engineering skills working with popular streaming
    tools and frameworks, i.e. Kinesis, Kafka and Spark. I would like to speak about
    the benefits, examples, and use cases of Data Streaming.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**流数据**是最受欢迎的数据管道设计模式之一。将事件作为单个数据点创建了从一个点到另一个点的持续数据流，从而为实时数据摄取和分析提供了机会。如果你想了解数据流并学习如何构建实时数据管道，这篇文章适合你。了解如何测试解决方案，并模拟事件流的测试数据。这篇文章是一个绝佳的机会，让你掌握一些受欢迎的数据工程技能，使用流行的流处理工具和框架，即Kinesis、Kafka和Spark。我想谈谈数据流的好处、示例和用例。'
- en: What exactly is data streaming?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据流究竟是什么？
- en: Streaming data, also known as event stream processing, is a data pipeline design
    pattern when data points flow constantly from the source to the destination. It
    can be processed in real-time, enabling real-time analytics capabilities to act
    on data streams and analytics events super fast. Applications can trigger immediate
    responses to new data events thanks to stream processing and typically it would
    be one of the most popular solutions to process the data on an enterprise level.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据，也称为事件流处理，是一种数据管道设计模式，当数据点不断从源头流向目的地时使用。这可以实时处理，使实时分析功能能够快速对数据流和分析事件作出反应。由于流处理，应用程序可以对新数据事件触发即时响应，通常这将是处理企业级数据最受欢迎的解决方案之一。
- en: There is a data pipeline whenever there is data processing between points A
    and B [1].
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 只要在点A和点B之间进行数据处理，就会有一个数据管道 [1]。
- en: '![](../Images/334d567596ab164146151fe1c0d7a31c.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/334d567596ab164146151fe1c0d7a31c.png)'
- en: Streaming data pipeline example. Image by author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据管道示例。图片由作者提供
- en: In this example, we can create an **ELT streaming** data pipeline to **AWS Redshift**.
    AWS **Firehose delivery stream** can offer this type of seamless integration when
    it creates a data feed directly into the data warehouse table. Then data will
    be transformed to create reports with **AWS Quicksight** as a BI tool.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以创建一个**ELT流处理**数据管道到**AWS Redshift**。AWS **Firehose delivery stream**可以提供这种无缝集成，将数据直接创建到数据仓库表中。然后，数据将被转化，以使用**AWS
    Quicksight**作为BI工具生成报告。
- en: Let’s imagine we need to create a reporting dashboard to display revenue streams
    in our company. In many scenarios, a business requirement is to generate insights
    in real-time. This is exactly the case when we would want to use **streaming**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要创建一个报告仪表盘来展示公司中的收入来源。在许多场景中，业务需求是实时生成洞察。这正是我们需要使用**流处理**的情况。
- en: Data streams can be generated by various data sources, i.e. IoT, server data
    streams, marketing in-app events, user activity, payment transactions, etc. This
    data can flow in different formats and often vary in volume. The idea of the streaming
    pattern is to apply ETL on the go and process event streams seamlessly.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流可以由各种数据源生成，例如物联网、服务器数据流、营销应用内事件、用户活动、支付交易等。这些数据可以以不同格式流动，并且经常变化。流处理模式的理念是实时应用ETL并无缝处理事件流。
- en: Whenever we need to act on up-to-the-millisecond data latency streaming is the
    right way to go.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们需要处理毫秒级的数据延迟时，流处理是正确的选择。
- en: Consider this example below to better understand it. All applications use OLTP
    databases [4], MySQL for example. Your app is one of those but you need this data
    in the data warehouse solution (DWH), i.e. Snowflake or BigQuery.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑下面的例子以更好地理解它。所有应用程序都使用OLTP数据库[4]，例如MySQL。你的应用程序也是其中之一，但你需要将这些数据存储到数据仓库解决方案（DWH），即Snowflake或BigQuery。
- en: '[](/data-modelling-for-data-engineers-93d058efa302?source=post_page-----2bb2b9b3b603--------------------------------)
    [## Data Modelling For Data Engineers'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 数据建模为数据工程师](https://example.org/data-modelling-for-data-engineers-93d058efa302?source=post_page-----2bb2b9b3b603--------------------------------)'
- en: The definitive guide for beginners
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初学者的终极指南
- en: towardsdatascience.com](/data-modelling-for-data-engineers-93d058efa302?source=post_page-----2bb2b9b3b603--------------------------------)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://example.org/data-modelling-for-data-engineers-93d058efa302?source=post_page-----2bb2b9b3b603--------------------------------)'
- en: Using a batch data pipeline solution we would want to load from MySQL to DWH
    once a day/hour/every five minutes, etc. Stream opposite to that would use a dedicated
    system, such as Kafka Connect for example. It will process data as soon as it
    lands in our database.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用批量数据管道解决方案，我们可能希望从MySQL加载到DWH一次/每天/每小时/每五分钟等。流处理则相反，它将使用专用系统，例如Kafka Connect。它会在数据进入数据库时立即处理数据。
- en: Popular data streaming tools
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流行的数据流工具
- en: Let’s take a look into popular data streaming platforms and frameworks that
    proved themselves most useful over the last couple of years.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解过去几年中被证明最有用的流数据平台和框架。
- en: '**Apache Spark** — frameworks for distributed data computing for large-scale
    analytics and complex data transformations'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Spark** — 用于大规模分析和复杂数据转换的分布式数据计算框架'
- en: '**Apache Kafka** — a real-time data pipeline tool with a distributed messaging
    system for apps'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Kafka** — 一个实时数据管道工具，具有分布式消息系统用于应用程序'
- en: '**AWS Kinesis** — a real-time streaming platform for analytics and applications'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Kinesis** — 一个用于分析和应用程序的实时流平台'
- en: '**Google Cloud Dataflow** — Google’s streaming platform for real-time event
    processing and analytics pipelines'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Cloud Dataflow** — 谷歌的实时事件处理和分析管道的流平台'
- en: '**Apache Flink** — a distributed streaming data platform designed for low-latency
    data processing.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Flink** — 一个分布式流数据平台，旨在进行低延迟数据处理。'
- en: Almost all of them have their managed cloud-based services (AWs Kinesis, Google
    Cloud Dataflow) and can be seamlessly integrated with other services such as storage
    (S3), queuing (SQS, pub/sub), data warehouses (Redshift) or AI platforms.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有这些平台都有它们的托管云服务（如AWS Kinesis、Google Cloud Dataflow），并且可以与其他服务（如存储（S3）、队列（SQS、pub/sub）、数据仓库（Redshift）或AI平台）无缝集成。
- en: All of them can be deployed on Kubernetes, Docker or Hadoop and aim to solve
    one problem — dealing with high-volume and high-velocity data streams.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些工具都可以部署在Kubernetes、Docker或Hadoop上，旨在解决一个问题——处理高容量和高速度的数据流。
- en: Benefits of data streaming
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据流的好处
- en: Streaming data pipeline design patterns helps organisations to proactively mitigate
    the impact of adverse business events related to delay in data processing, i.e.
    various losses and outages, customer churn and financial downturns. Because of
    the complexity of today’s business needs, conventional **batch data processing**
    is a ‘No Go’ solution, as it can only process data as groups of transactions accumulated
    over time.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据管道设计模式帮助组织主动减轻与数据处理延迟相关的不利业务事件的影响，例如各种损失和中断、客户流失和财务衰退。由于今天业务需求的复杂性，传统的**批处理数据处理**是‘不可行’的解决方案，因为它只能处理累积时间的事务数据组。
- en: 'So here are some business advantages of using data streaming:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以使用数据流的业务优势如下：
- en: '**Increase in customer satisfaction** and as a result increased retention rates'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高客户满意度**，进而提高客户保留率'
- en: '**Reduced operational losses** as it can give real-time insights on system
    outages and breaches'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少操作损失**，因为它可以提供关于系统中断和漏洞的实时洞察。'
- en: '**Increased return on investment** as companies can now act faster on business
    data with increased responsiveness to customer needs and market trends.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投资回报率提高**，因为公司现在可以更快地对业务数据做出反应，对客户需求和市场趋势的响应能力提高。'
- en: The main **technical advantage** is in data processing as it runs event processing
    strictly ***one by one***. Opposite to batch processing, it has a better fault
    tolerance and if one event in the pipeline can’t be processed due to some reason
    then it is only this event. In the batch pipeline, the whole chunk of data processing
    will fail because of this single data point which might have a wrong schema or
    incorrect data format.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的**技术优势**在于数据处理，因为它以严格的***逐个处理***方式运行事件处理。与批处理处理相比，它具有更好的故障容忍性，如果管道中的一个事件因某些原因无法处理，那么只有这个事件会受到影响。在批处理管道中，由于单个数据点可能具有错误的模式或数据格式，整个数据处理块会因此失败。
- en: The main disadvantage of streaming data pipelines is the cost
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 流数据管道的主要缺点是成本
- en: Each time our stream processor hits the endpoint it will require compute power.
    Typically streaming data processing will result in higher costs related to this
    particular data pipeline.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们的流处理器达到端点时，它都需要计算能力。通常，流数据处理会导致与特定数据管道相关的更高成本。
- en: Challenges in building streaming data pipelines
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建流数据管道的挑战
- en: '**Fault Tolerance** — Can we design and build a data platform that handles
    data processing failures from a single point of data event? Very often data comes
    from different data sources. It might be coming even in different formats. Data
    availability and durability becomes important consideration while designing the
    data platform [3] with streaming component.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障容忍性** — 我们能否设计和构建一个能够处理单一数据事件处理失败的数据平台？数据通常来自不同的数据源，甚至可能以不同的格式出现。在设计带有流组件的数据平台时，数据的可用性和持久性成为重要的考虑因素[3]。'
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2bb2b9b3b603--------------------------------)
    [## Data Platform Architecture Types'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 数据平台架构类型](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2bb2b9b3b603--------------------------------)'
- en: How well does it answer your business needs? Dilemma of a choice.
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它能多好地满足你的业务需求？选择的困境。
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2bb2b9b3b603--------------------------------)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2bb2b9b3b603--------------------------------)'
- en: '**Queuing and ordering** — Events in the data stream must be ordered correctly.
    Otherwise, data processing might fail. Indeed, in-app messaging will not make
    sense if ordered incorrectly, for example.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排队和排序** — 数据流中的事件必须正确排序。否则，数据处理可能会失败。例如，如果排序不正确，应用内消息将没有意义。'
- en: '**Scalability** — Applications scale. It is as simple as that. Designing a
    data pipeline that responds well to an increased number of events coming from
    the source is not a trivial task. Being able to add more resources and data processing
    capacity to our data pipeline is a crucial component of a robust data platform.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性** — 应用程序需要扩展。就这么简单。设计一个能够很好应对来自源的事件数量增加的数据管道并非易事。能够为数据管道添加更多资源和数据处理能力是一个强大数据平台的重要组成部分。'
- en: '**Data consistency —** Often in distributed data platforms data is being processed
    in parallel. This might become a challenge as data in one data processor could
    already be modified and become stale in another one.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据一致性 —** 在分布式数据平台中，数据经常是并行处理的。这可能会成为挑战，因为在一个数据处理器中数据可能已经被修改，而在另一个处理器中变得陈旧。'
- en: A real-world example
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现实世界中的一个例子
- en: Let’s take a look at this example of a streaming data pipeline built with AWS
    Kinesis and Redshift.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下这个使用 AWS Kinesis 和 Redshift 构建的流数据管道示例。
- en: '![](../Images/17b2472521ad2fec136aa2de06456924.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17b2472521ad2fec136aa2de06456924.png)'
- en: Example pipeline. Image by author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 示例管道。图片由作者提供
- en: Amazon Kinesis Data Firehose is an ETL service that collects, transforms, and
    distributes streaming data to data lakes, data storage, and analytics services
    with high reliability.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Kinesis Data Firehose 是一个 ETL 服务，可以高可靠性地收集、转换并分发流数据到数据湖、数据存储和分析服务。
- en: We can use it to stream data into Amazon S3 and convert it to the formats needed
    for analysis without having to develop processing pipelines. It is also great
    for Machine learning (ML) pipelines where models are used to examine data and
    forecast inference endpoints as streams flow to their destination.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用它将数据流传输到 Amazon S3，并将数据转换为分析所需的格式，无需开发处理管道。它对于机器学习（ML）管道也非常适合，其中模型用于检查数据并预测推断端点，因为数据流向其目标。
- en: '**Kinesis Data Streams vs Kinesis Data Firehose**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kinesis Data Streams 与 Kinesis Data Firehose**'
- en: Kinesis Data Streams is primarily focused on consuming and storing data streams.
    Kinesis Data Firehose is designed to deliver data streams to specific destinations.
    Both can consume data streams, but which one to use depends on where we want our
    streaming data to go.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis Data Streams 主要关注于消费和存储数据流。Kinesis Data Firehose 旨在将数据流传递到特定的目标。两者都可以消费数据流，但使用哪个取决于我们希望数据流去往何处。
- en: AWS Kinesis Data Firehose allows us to redirect data streams into AWS data storage.
    Kinesis Data Firehose is the most straightforward method for gathering, processing,
    and loading data streams into AWS data storage.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Kinesis Data Firehose 允许我们将数据流重定向到 AWS 数据存储。Kinesis Data Firehose 是收集、处理和加载数据流到
    AWS 数据存储的最直接方法。
- en: Amazon Kinesis Data Firehose supports batch operations, encryption, and compression
    of streaming data, as well as automated scalability in the terabytes per second
    range. Firehose can seamlessly integrate with S3 data lakes, RedShift data warehouse
    solutions or ElasticSearch service.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Kinesis Data Firehose 支持批处理操作、加密和流数据压缩，以及自动化的每秒 TB 级别的可扩展性。Firehose 可以无缝集成
    S3 数据湖、RedShift 数据仓库解决方案或 ElasticSearch 服务。
- en: AWS Kinesis Data Streams is an Amazon Kinesis real-time data streaming solution
    with exceptional scalability and durability where data streams are available 24/7
    for any consumer. It makes it more expensive than the Kinesis Data Firehose.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Kinesis Data Streams 是一个 Amazon Kinesis 实时数据流解决方案，具有卓越的可扩展性和耐用性，数据流全天候 24/7
    可用给任何消费者。这使得它比 Kinesis Data Firehose 更昂贵。
- en: '**How to create a Firehose resource using AWS Cloudformation**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何使用 AWS CloudFormation 创建 Firehose 资源**'
- en: Consider this CloudFormation template below. It deploys the required resources
    including the Firehose we need.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看下面的 CloudFormation 模板。它部署了包括我们需要的 Firehose 在内的所有必要资源。
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It can be deployed in AWS using the AWS CLI tool. We need to run this on our
    command line (replace with unique bucket names in your account):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 AWS CLI 工具在 AWS 中部署它。我们需要在命令行中运行这个（在你的账户中替换为唯一的存储桶名称）：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Our shell script would look like this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 shell 脚本如下所示：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/515379a7556b2ccb17c34b3d84a59dd7.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/515379a7556b2ccb17c34b3d84a59dd7.png)'
- en: Firehose resources created. Image by author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 已创建 Firehose 资源。图片由作者提供
- en: 'Now we would want to create an event producer. We can do it in Python and the
    code for our `app.py` would look like this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要创建一个事件生产者。我们可以使用 Python 完成这个操作，`app.py` 的代码如下：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`put_record_batch` method writes many data records into a delivery stream in
    a single call, allowing for better throughput per producer than single record
    writing. `PutRecord` is used to write single data records into a delivery stream.
    It is up to you which one to choose in this tutorial.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`put_record_batch` 方法可以在一次调用中将多个数据记录写入交付流，这比单条记录写入方式能提供更好的每生产者吞吐量。`PutRecord`
    用于将单条数据记录写入交付流。在本教程中选择哪个方法由你决定。'
- en: We can generate some synthetic data in our `app.py` using this helper function
    below.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 `app.py` 中使用下面的辅助函数生成一些合成数据。
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now this data can be send to our event producer like so:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这些数据可以通过以下方式发送到我们的事件生产者：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Done! We have created a simple streaming data pipeline that outputs aggregated
    results into cloud storage (AWS S3).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 完成！我们已经创建了一个简单的流数据管道，将汇总结果输出到云存储（AWS S3）。
- en: 'Run `python app.py` in your command line:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令行中运行 `python app.py`：
- en: '![](../Images/53af7f1ce72757790dce09aa4c3f438e.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53af7f1ce72757790dce09aa4c3f438e.png)'
- en: Events connector example. Image by author
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 事件连接器示例。作者提供的图片
- en: Check my tutorial below for a more advanced data pipeline example [2]
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 查看下面的教程，了解更高级的数据管道示例 [2]
- en: '[](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2bb2b9b3b603--------------------------------)
    [## Building a Streaming Data Pipeline with Redshift Serverless and Kinesis'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2bb2b9b3b603--------------------------------)
    [## 使用 Redshift Serverless 和 Kinesis 构建流数据管道'
- en: An End-To-End Tutorial for Beginners
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面向初学者的端到端教程
- en: towardsdatascience.com](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2bb2b9b3b603--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2bb2b9b3b603--------------------------------)
- en: Conclusion
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: The ideal streaming data platform for your project doesn’t exist. The streaming
    design has its benefits but also we can see some obvious challenges while using
    it. Which streaming tool to choose is not an easy choice. It depends on your business
    goals and functional data requirements. You would want to try and compare multiple
    streaming platforms based on characteristics such as functionality, performance,
    cost, simplicity of use, and compatibility. Is it going to be a machine-learning
    pipeline? Do we need to work with partitions, windows and joins? Do we need high
    throughput, fault tolerance or low latency?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 项目理想的流数据平台并不存在。流设计有其好处，但在使用时也会遇到一些明显的挑战。选择哪个流工具不是一个容易的决定。这取决于你的业务目标和功能数据需求。你可能需要尝试并比较多个流平台，考虑功能、性能、成本、易用性和兼容性等特征。它会是一个机器学习管道吗？我们需要处理分区、窗口和连接吗？我们需要高吞吐量、容错性还是低延迟？
- en: Different streaming frameworks have different capabilities, for example, Kafka
    has a handy **session** **library** which can be easily integrated into your analytics
    pipeline.
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不同的流框架具有不同的能力，例如，Kafka 有一个方便的**会话** **库**，可以很容易地集成到你的分析管道中。
- en: What frequency of data delivery and consumption do we need in our pipeline?
    Is it going to be a delivery into a DWH solution or into a data lake? Some platforms
    can offer better integration features than others.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的管道需要什么频率的数据传输和消费？它将交付到数据仓库解决方案还是数据湖中？一些平台比其他平台提供更好的集成功能。
- en: Another essential element to consider is the type and **complexity of data processing**
    and analysis that must be performed on your streaming data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的考虑因素是必须对流数据进行的**数据处理**和分析的类型和**复杂性**。
- en: I would recommend creating a prototype based on your own data pipeline scenario
    and requirements gathered from the main stakeholders inside the company. The optimal
    streaming data pipeline would be the one that adds value to the business and also
    meets your data engineering goals.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议根据你自己数据管道场景和公司主要利益相关者收集的需求来创建一个原型。最终的流数据管道应该是能够为业务增值并满足你的数据工程目标的。
- en: 'Recommended read:'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐阅读：
- en: '[1] [https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](/data-pipeline-design-patterns-100afa4b93e3)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](/data-pipeline-design-patterns-100afa4b93e3)'
- en: '[2] [https://towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)'
- en: '[3] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
- en: '[4] [https://medium.com/towards-data-science/data-modelling-for-data-engineers-93d058efa302](https://medium.com/towards-data-science/data-modelling-for-data-engineers-93d058efa302)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://medium.com/towards-data-science/data-modelling-for-data-engineers-93d058efa302](https://medium.com/towards-data-science/data-modelling-for-data-engineers-93d058efa302)'
