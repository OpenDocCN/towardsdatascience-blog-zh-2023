- en: Streaming in Data Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/streaming-in-data-engineering-2bb2b9b3b603](https://towardsdatascience.com/streaming-in-data-engineering-2bb2b9b3b603)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Streaming data pipelines and real-time analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)[![ðŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)
    [ðŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)
    Â·9 min readÂ·Dec 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5bdd62c71b5b0a4888786ad3318772d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [DESIGNECOLOGIST](https://unsplash.com/@designecologist?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '**Streaming** is one of the most popular data pipeline design patterns. Using
    an event as a single data point creates a constant flow of data from one point
    to another enabling an opportunity for real-time data ingestion and analytics.
    If you want to familiarise yourself with data streaming and learn how to build
    real-time data pipelines this story is for you. Learn how to test the solution,
    and mock test data to simulate event streams. This article is a great opportunity
    to acquire some sought-after data engineering skills working with popular streaming
    tools and frameworks, i.e. Kinesis, Kafka and Spark. I would like to speak about
    the benefits, examples, and use cases of Data Streaming.'
  prefs: []
  type: TYPE_NORMAL
- en: What exactly is data streaming?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Streaming data, also known as event stream processing, is a data pipeline design
    pattern when data points flow constantly from the source to the destination. It
    can be processed in real-time, enabling real-time analytics capabilities to act
    on data streams and analytics events super fast. Applications can trigger immediate
    responses to new data events thanks to stream processing and typically it would
    be one of the most popular solutions to process the data on an enterprise level.
  prefs: []
  type: TYPE_NORMAL
- en: There is a data pipeline whenever there is data processing between points A
    and B [1].
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/334d567596ab164146151fe1c0d7a31c.png)'
  prefs: []
  type: TYPE_IMG
- en: Streaming data pipeline example. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we can create an **ELT streaming** data pipeline to **AWS Redshift**.
    AWS **Firehose delivery stream** can offer this type of seamless integration when
    it creates a data feed directly into the data warehouse table. Then data will
    be transformed to create reports with **AWS Quicksight** as a BI tool.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s imagine we need to create a reporting dashboard to display revenue streams
    in our company. In many scenarios, a business requirement is to generate insights
    in real-time. This is exactly the case when we would want to use **streaming**.
  prefs: []
  type: TYPE_NORMAL
- en: Data streams can be generated by various data sources, i.e. IoT, server data
    streams, marketing in-app events, user activity, payment transactions, etc. This
    data can flow in different formats and often vary in volume. The idea of the streaming
    pattern is to apply ETL on the go and process event streams seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we need to act on up-to-the-millisecond data latency streaming is the
    right way to go.
  prefs: []
  type: TYPE_NORMAL
- en: Consider this example below to better understand it. All applications use OLTP
    databases [4], MySQL for example. Your app is one of those but you need this data
    in the data warehouse solution (DWH), i.e. Snowflake or BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-modelling-for-data-engineers-93d058efa302?source=post_page-----2bb2b9b3b603--------------------------------)
    [## Data Modelling For Data Engineers'
  prefs: []
  type: TYPE_NORMAL
- en: The definitive guide for beginners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-modelling-for-data-engineers-93d058efa302?source=post_page-----2bb2b9b3b603--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Using a batch data pipeline solution we would want to load from MySQL to DWH
    once a day/hour/every five minutes, etc. Stream opposite to that would use a dedicated
    system, such as Kafka Connect for example. It will process data as soon as it
    lands in our database.
  prefs: []
  type: TYPE_NORMAL
- en: Popular data streaming tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Letâ€™s take a look into popular data streaming platforms and frameworks that
    proved themselves most useful over the last couple of years.
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Spark** â€” frameworks for distributed data computing for large-scale
    analytics and complex data transformations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Kafka** â€” a real-time data pipeline tool with a distributed messaging
    system for apps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Kinesis** â€” a real-time streaming platform for analytics and applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Cloud Dataflow** â€” Googleâ€™s streaming platform for real-time event
    processing and analytics pipelines'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Flink** â€” a distributed streaming data platform designed for low-latency
    data processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Almost all of them have their managed cloud-based services (AWs Kinesis, Google
    Cloud Dataflow) and can be seamlessly integrated with other services such as storage
    (S3), queuing (SQS, pub/sub), data warehouses (Redshift) or AI platforms.
  prefs: []
  type: TYPE_NORMAL
- en: All of them can be deployed on Kubernetes, Docker or Hadoop and aim to solve
    one problem â€” dealing with high-volume and high-velocity data streams.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of data streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Streaming data pipeline design patterns helps organisations to proactively mitigate
    the impact of adverse business events related to delay in data processing, i.e.
    various losses and outages, customer churn and financial downturns. Because of
    the complexity of todayâ€™s business needs, conventional **batch data processing**
    is a â€˜No Goâ€™ solution, as it can only process data as groups of transactions accumulated
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'So here are some business advantages of using data streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Increase in customer satisfaction** and as a result increased retention rates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduced operational losses** as it can give real-time insights on system
    outages and breaches'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increased return on investment** as companies can now act faster on business
    data with increased responsiveness to customer needs and market trends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main **technical advantage** is in data processing as it runs event processing
    strictly ***one by one***. Opposite to batch processing, it has a better fault
    tolerance and if one event in the pipeline canâ€™t be processed due to some reason
    then it is only this event. In the batch pipeline, the whole chunk of data processing
    will fail because of this single data point which might have a wrong schema or
    incorrect data format.
  prefs: []
  type: TYPE_NORMAL
- en: The main disadvantage of streaming data pipelines is the cost
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Each time our stream processor hits the endpoint it will require compute power.
    Typically streaming data processing will result in higher costs related to this
    particular data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in building streaming data pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Fault Tolerance** â€” Can we design and build a data platform that handles
    data processing failures from a single point of data event? Very often data comes
    from different data sources. It might be coming even in different formats. Data
    availability and durability becomes important consideration while designing the
    data platform [3] with streaming component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2bb2b9b3b603--------------------------------)
    [## Data Platform Architecture Types'
  prefs: []
  type: TYPE_NORMAL
- en: How well does it answer your business needs? Dilemma of a choice.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2bb2b9b3b603--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Queuing and ordering** â€” Events in the data stream must be ordered correctly.
    Otherwise, data processing might fail. Indeed, in-app messaging will not make
    sense if ordered incorrectly, for example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability** â€” Applications scale. It is as simple as that. Designing a
    data pipeline that responds well to an increased number of events coming from
    the source is not a trivial task. Being able to add more resources and data processing
    capacity to our data pipeline is a crucial component of a robust data platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consistency â€”** Often in distributed data platforms data is being processed
    in parallel. This might become a challenge as data in one data processor could
    already be modified and become stale in another one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A real-world example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Letâ€™s take a look at this example of a streaming data pipeline built with AWS
    Kinesis and Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17b2472521ad2fec136aa2de06456924.png)'
  prefs: []
  type: TYPE_IMG
- en: Example pipeline. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kinesis Data Firehose is an ETL service that collects, transforms, and
    distributes streaming data to data lakes, data storage, and analytics services
    with high reliability.
  prefs: []
  type: TYPE_NORMAL
- en: We can use it to stream data into Amazon S3 and convert it to the formats needed
    for analysis without having to develop processing pipelines. It is also great
    for Machine learning (ML) pipelines where models are used to examine data and
    forecast inference endpoints as streams flow to their destination.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kinesis Data Streams vs Kinesis Data Firehose**'
  prefs: []
  type: TYPE_NORMAL
- en: Kinesis Data Streams is primarily focused on consuming and storing data streams.
    Kinesis Data Firehose is designed to deliver data streams to specific destinations.
    Both can consume data streams, but which one to use depends on where we want our
    streaming data to go.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Kinesis Data Firehose allows us to redirect data streams into AWS data storage.
    Kinesis Data Firehose is the most straightforward method for gathering, processing,
    and loading data streams into AWS data storage.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kinesis Data Firehose supports batch operations, encryption, and compression
    of streaming data, as well as automated scalability in the terabytes per second
    range. Firehose can seamlessly integrate with S3 data lakes, RedShift data warehouse
    solutions or ElasticSearch service.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Kinesis Data Streams is an Amazon Kinesis real-time data streaming solution
    with exceptional scalability and durability where data streams are available 24/7
    for any consumer. It makes it more expensive than the Kinesis Data Firehose.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to create a Firehose resource using AWS Cloudformation**'
  prefs: []
  type: TYPE_NORMAL
- en: Consider this CloudFormation template below. It deploys the required resources
    including the Firehose we need.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'It can be deployed in AWS using the AWS CLI tool. We need to run this on our
    command line (replace with unique bucket names in your account):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Our shell script would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/515379a7556b2ccb17c34b3d84a59dd7.png)'
  prefs: []
  type: TYPE_IMG
- en: Firehose resources created. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we would want to create an event producer. We can do it in Python and the
    code for our `app.py` would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`put_record_batch` method writes many data records into a delivery stream in
    a single call, allowing for better throughput per producer than single record
    writing. `PutRecord` is used to write single data records into a delivery stream.
    It is up to you which one to choose in this tutorial.'
  prefs: []
  type: TYPE_NORMAL
- en: We can generate some synthetic data in our `app.py` using this helper function
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now this data can be send to our event producer like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Done! We have created a simple streaming data pipeline that outputs aggregated
    results into cloud storage (AWS S3).
  prefs: []
  type: TYPE_NORMAL
- en: 'Run `python app.py` in your command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53af7f1ce72757790dce09aa4c3f438e.png)'
  prefs: []
  type: TYPE_IMG
- en: Events connector example. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Check my tutorial below for a more advanced data pipeline example [2]
  prefs: []
  type: TYPE_NORMAL
- en: '[](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2bb2b9b3b603--------------------------------)
    [## Building a Streaming Data Pipeline with Redshift Serverless and Kinesis'
  prefs: []
  type: TYPE_NORMAL
- en: An End-To-End Tutorial for Beginners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2bb2b9b3b603--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ideal streaming data platform for your project doesnâ€™t exist. The streaming
    design has its benefits but also we can see some obvious challenges while using
    it. Which streaming tool to choose is not an easy choice. It depends on your business
    goals and functional data requirements. You would want to try and compare multiple
    streaming platforms based on characteristics such as functionality, performance,
    cost, simplicity of use, and compatibility. Is it going to be a machine-learning
    pipeline? Do we need to work with partitions, windows and joins? Do we need high
    throughput, fault tolerance or low latency?
  prefs: []
  type: TYPE_NORMAL
- en: Different streaming frameworks have different capabilities, for example, Kafka
    has a handy **session** **library** which can be easily integrated into your analytics
    pipeline.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What frequency of data delivery and consumption do we need in our pipeline?
    Is it going to be a delivery into a DWH solution or into a data lake? Some platforms
    can offer better integration features than others.
  prefs: []
  type: TYPE_NORMAL
- en: Another essential element to consider is the type and **complexity of data processing**
    and analysis that must be performed on your streaming data.
  prefs: []
  type: TYPE_NORMAL
- en: I would recommend creating a prototype based on your own data pipeline scenario
    and requirements gathered from the main stakeholders inside the company. The optimal
    streaming data pipeline would be the one that adds value to the business and also
    meets your data engineering goals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recommended read:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](/data-pipeline-design-patterns-100afa4b93e3)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [https://medium.com/towards-data-science/data-modelling-for-data-engineers-93d058efa302](https://medium.com/towards-data-science/data-modelling-for-data-engineers-93d058efa302)'
  prefs: []
  type: TYPE_NORMAL
