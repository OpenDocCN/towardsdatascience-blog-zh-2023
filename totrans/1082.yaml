- en: How I Leveraged Open Source LLMs to Achieve Massive Savings on a Large Compute
    Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/how-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267](https://towardsdatascience.com/how-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unlocking Cost-Efficiency in Large Compute Projects with Open Source LLMs and
    GPU Rentals.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)[![Ryan
    Shrott](../Images/186524066383b4b02c994692aebb3ea5.png)](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)
    [Ryan Shrott](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)
    ¬∑6 min read¬∑Aug 30, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/83382f63b21df0de2b0d47d33ad7212d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Alexander Grey](https://unsplash.com/@sharonmccutcheon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the world of large language models (LLMs), the cost of computation can be
    a significant barrier, especially for extensive projects. I recently embarked
    on a project that required running 4,000,000 prompts with an average input length
    of 1000 tokens and an average output length of 200 tokens. That‚Äôs nearly 5 billion
    tokens! The traditional approach of paying per token, as is common with models
    like GPT-3.5 and GPT-4, would have resulted in a hefty bill. However, I discovered
    that by leveraging open source LLMs, I could shift the pricing model to pay per
    hour of compute time, leading to substantial savings. This article will detail
    the approaches I took and compare and contrast each of them. Please note that
    while I share my experience with pricing, these are subject to change and may
    vary depending on your region and specific circumstances. The key takeaway here
    is the potential cost savings when leveraging open source LLMs and renting a GPU
    per hour, rather than the specific prices quoted.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I conducted an initial test using GPT-3.5 and GPT-4 on a small subset of my
    prompt input data. Both models demonstrated commendable performance, but GPT-4
    consistently outperformed GPT-3.5 in a majority of the cases. To give you a sense
    of the cost, running all 4 million prompts using the Open AI API would look something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/35209c957d0dc0f7f6be806ff60f6cd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Total cost of running 4mm prompts with input length of 1000 tokens and 200 token
    output length
  prefs: []
  type: TYPE_NORMAL
- en: While GPT-4 did offer some performance benefits, the cost was disproportionately
    high compared to the incremental performance it added to my outputs. Conversely,
    GPT-3.5 Turbo, although more affordable, fell short in terms of performance, making
    noticeable errors on 2‚Äì3% of my prompt inputs. Given these factors, I wasn‚Äôt prepared
    to invest $7,600 on a project that was essentially a personal endeavor.
  prefs: []
  type: TYPE_NORMAL
- en: Open-Source Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Open-source models to the rescue! With open-source models, the pricing model
    is very different: **you only pay per hour of compute time**. Therefore, your
    goal is to maximize compute iterations per hour. Also, with solutions like Petals.ml,
    you can run your compute for free (with limitations)!'
  prefs: []
  type: TYPE_NORMAL
- en: After trying various models on Hugging Face, I found that a fine-tuned version
    of LLama-2 70B called [Stable Beluga 2](https://huggingface.co/stabilityai/StableBeluga2)
    gave me excellent performance without any fine tuning needed! It performed better
    than GPT-3.5 Turbo, but slightly below GPT-4\. However, this model is still very
    large and requires a very beefy GPU. It‚Äôs often best practice to use the smallest
    model possible to complete your task efficiently. Therefore, I tried out the 7B
    version of Stable Beluga 2\. The performance was not good enough!
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To increase the performance for my task, I used GPT-4 and [Petals.ml](https://github.com/bigscience-workshop/petals)
    (Stable Beluga 2 70B) to generate a fine-tuning dataset. I generated 25K prompt
    completion pairs using Petals.ml and 2K using GPT-4 API. As a reminder Petals.ml
    allows you to run open source LLM models for free using bit-torrent technology.
    The inference time is not great though: it would have taken over a year to do
    4mm iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, Petals.ml hosts this model for free. Therefore, I utilized Petals.ml
    to generate 25,000 prompt completion pairs. I also used the GPT-4 API to generate
    an additional 2K prompt completion pairs. In hindsight, I probably overdid it
    on the training data. I‚Äôve read reports showing that as few as 500 fine tuning
    samples can be enough to improve the performance of an LLM. Anyway, here is the
    total cost of running 2K prompts using Open AI API:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80c052ebe7c81d606c55085777984208.png)'
  prefs: []
  type: TYPE_IMG
- en: Total cost of running 2K prompt completion pairs
  prefs: []
  type: TYPE_NORMAL
- en: 'That‚Äôs right: $84\. Can you guess what happened next? Using this newly acquired
    27K prompt completion pairs, I fine-tuned the 7B variant of the smallest LLama
    2 7B, and voila, this new model performed extremely well for my use case. The
    total fine-tuning cost was only $6.6 as I rented an A100 GPU for 6 hours. **In
    the end, my fine-tuned model performed better than GPT-3.5, and slightly worse
    than GPT-4.**'
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let‚Äôs talk about inference compute costs. My model requires at least 20GB
    VRAM, so we‚Äôll require at least an RTX 3090\. There are four great options currently
    available for GPU rentals: AWS, LambdaLabs, [RunPod](https://runpod.io/?ref=lemrt56t)
    and [Vast.AI](https://cloud.vast.ai/?ref_id=79595). From my experience, Vast.AI
    has the best prices, but the worst reliability. AWS is super reliable but is more
    expensive than the other options. RunPod has great prices and a great UI. I personally
    wanted to keep my costs as low as possible, so I decided to go with Vast.AI for
    this project. Vast.AI is a peer-to-peer GPU rental service, so it‚Äôs likely also
    the least secure. After scouring Vast.AI for the best price to performance ratio
    servers available, here are the best options I found:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/387407f56c236b653480400d7dc503f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Vast.AI total project costs using fune tuned LLama2 7b model. The total cost
    is computed using the number of iterations per second, the price per hour of GPU
    rental, and the total iterations required.
  prefs: []
  type: TYPE_NORMAL
- en: As shown above, the total cost of inference can be less than $99\. In my case,
    I spun up around 10 servers and computed the number of iterations per second I
    could yield with each. For this particular project, which leveraged a fine-tuned
    variant of Llama2‚Äì7B, the RTX A5000 was a great option in terms of price to performance.
    I should also mention that none of this would be possible without the INSANE speeds
    of the open-source library [VLLM](https://github.com/vllm-project/vllm). My compute
    is 100% powered by VLLM under the hood. **Without VLLM, the compute time would
    increase by 20 times!** You may also look at the total runtime and laugh ‚Äî 638
    hours is 26 days; however, you could easily parallelize the task across multiple
    GPUs/servers.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conclusion, the utilization of open-source LLMs and the shift in the pricing
    model from paying per token to paying per hour of compute time resulted in substantial
    cost savings in this large compute project. The total cost, which included prompt
    generation, fine-tuning, and inference, was a mere $189.58\. This is a stark contrast
    to the costs associated with using models like GPT-4 or GPT-3.5 Turbo, which would
    have amounted to $167,810 and $7410.42 respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway from this project is the significant potential for cost reduction
    when leveraging open source LLMs. By renting a GPU per hour, the cost is tied
    to the compute time rather than the number of prompts, which can lead to massive
    savings, especially for extensive projects. This approach not only makes such
    projects more financially feasible but also opens up opportunities for individuals
    and smaller organizations to undertake large compute projects without incurring
    prohibitive costs.
  prefs: []
  type: TYPE_NORMAL
- en: The success of this project underscores the value and potential of open-source
    models in the realm of large language models and AI and serves as a testament
    to the power of innovative, cost-effective solutions in tackling complex computational
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Pricing Note
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pricing information provided in this article is based on my personal experience
    and is intended to serve as a general comparison. Prices may vary depending on
    your region and specific circumstances. **The key takeaway is the potential cost
    savings when leveraging open source LLMs and renting a GPU per hour, rather than
    the specific prices quoted.**
  prefs: []
  type: TYPE_NORMAL
- en: 'üì¢ Hey there! If you found this article helpful, please consider:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: üëè Clapping 50 times (this helps a lot!)
  prefs: []
  type: TYPE_NORMAL
- en: ‚úèÔ∏è Leaving a comment
  prefs: []
  type: TYPE_NORMAL
- en: üåü Highlighting parts you found insightful
  prefs: []
  type: TYPE_NORMAL
- en: üë£ Following me
  prefs: []
  type: TYPE_NORMAL
- en: Any questions? ü§î Don‚Äôt hesitate to ask. Supporting me this way is a free and
    easy way to show appreciation for my detailed tutorial articles! üòä
  prefs: []
  type: TYPE_NORMAL
- en: '**GPU rental recommendations:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Best overall: [RunPod.io GPU Rentals](https://runpod.io/?ref=lemrt56t)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cheapest: [Vast.AI](https://cloud.vast.ai/?ref_id=79595)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Deep Learning Blogs**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Efficiently Serving Open Source LLMs](https://medium.com/p/5f0bf5d8fd59)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Computer Vision by Andrew Ng ‚Äî 11 Lessons Learned](https://medium.com/towards-data-science/efficiently-serving-open-source-llms-5f0bf5d8fd59)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Deep Learning Specialization by Andrew Ng ‚Äî 21 Lessons Learned](/deep-learning-specialization-by-andrew-ng-21-lessons-learned-15ffaaef627c)'
  prefs: []
  type: TYPE_NORMAL
