- en: Feature Subset Selection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征子集选择
- en: 原文：[https://towardsdatascience.com/feature-subset-selection-6de1f05822b0](https://towardsdatascience.com/feature-subset-selection-6de1f05822b0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/feature-subset-selection-6de1f05822b0](https://towardsdatascience.com/feature-subset-selection-6de1f05822b0)
- en: A tutorial on feature selection and a recommended strategy
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于特征选择的教程和推荐策略
- en: '[](https://medium.com/@PadraigC?source=post_page-----6de1f05822b0--------------------------------)[![Pádraig
    Cunningham](../Images/9521fb3dc64c947edf6adf2ce7b80f0f.png)](https://medium.com/@PadraigC?source=post_page-----6de1f05822b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6de1f05822b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6de1f05822b0--------------------------------)
    [Pádraig Cunningham](https://medium.com/@PadraigC?source=post_page-----6de1f05822b0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@PadraigC?source=post_page-----6de1f05822b0--------------------------------)[![Pádraig
    Cunningham](../Images/9521fb3dc64c947edf6adf2ce7b80f0f.png)](https://medium.com/@PadraigC?source=post_page-----6de1f05822b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6de1f05822b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6de1f05822b0--------------------------------)
    [Pádraig Cunningham](https://medium.com/@PadraigC?source=post_page-----6de1f05822b0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6de1f05822b0--------------------------------)
    ·16 min read·Mar 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----6de1f05822b0--------------------------------)
    ·阅读时间16分钟·2023年3月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/eea3aed19636e0bc3cd8b85ca6a2f878.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eea3aed19636e0bc3cd8b85ca6a2f878.png)'
- en: Photo by [gokhan polat](https://unsplash.com/@go_pol?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/qyC7DTbWJJk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[gokhan polat](https://unsplash.com/@go_pol?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    于[Unsplash](https://unsplash.com/photos/qyC7DTbWJJk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: TL;DR
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TL;DR
- en: Feature subset selection is important in supervised machine learning not just
    because it results in better models but also because of the insight it provides.
    This is particularly important now with the emphasis on interpretability in machine
    learning (ML).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 特征子集选择在监督学习中非常重要，不仅因为它可以产生更好的模型，还因为它提供的洞见。随着机器学习（ML）对可解释性强调的增加，这一点尤为重要。
- en: The challenge for practitioners is that there is a bewildering array of feature
    selection methods available. In this post I provide a brief overview of this landscape
    and propose a strategy that will work in most cases. This strategy uses a *Wrapper*
    for the final selection process and, where necessary, *permutation importance*
    as an initial filter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从业者面临的挑战是特征选择方法的种类繁多。在这篇文章中，我简要概述了这一领域，并提出了一种在大多数情况下有效的策略。该策略使用*Wrapper*进行最终选择过程，并在必要时使用*permutation
    importance*作为初步筛选。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: In data analysis, objects described using multiple features may sometimes be
    described using a subset of these features without loss of information. Identifying
    these feature subsets is termed feature selection, variable selection or feature
    subset selection and is a key process in data analysis. This post provides a brief
    overview of feature subset selection (FSS) methods and also proposes a strategy
    that will work in most scenarios. This post is based on a tutorial paper available
    on [arXiv](https://arxiv.org/abs/2106.06437) [1]. Python code for the methods
    presented in that paper is available on [Github](https://github.com/PadraigC/FeatSelTutorial).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析中，使用多个特征描述的对象有时可以仅用这些特征的子集来描述而不丢失信息。识别这些特征子集被称为特征选择、变量选择或特征子集选择，是数据分析中的关键过程。本文简要概述了特征子集选择（FSS）方法，并提出了一种在大多数场景下都有效的策略。本文基于[arXiv](https://arxiv.org/abs/2106.06437)
    [1]上的一篇教程论文。该论文中提出的方法的Python代码可在[Github](https://github.com/PadraigC/FeatSelTutorial)上找到。
- en: 'Feature selection receives a lot of attention in ML because it can deliver
    a number of benefits:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择在机器学习中受到广泛关注，因为它可以带来许多好处：
- en: '**Better classifiers:** The obvious benefit of feature selection is that it
    will improve accuracy because redundant or noisy features can damage accuracy.
    Perhaps surprisingly, improvements in accuracy can be quite limited because powerful
    ML techniques are designed to be robust against noise.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更好的分类器：** 特征选择的明显好处是它会提高准确性，因为冗余或噪声特征可能会损害准确性。或许令人惊讶的是，准确性的提升可能会相当有限，因为强大的机器学习技术被设计得对噪声具有鲁棒性。'
- en: '**Insight:** Perhaps the most enduring benefit of feature selection is the
    insight it provides. Identifying influential features and features that are not
    useful teaches us a lot about the data.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**洞察：** 特征选择最持久的好处可能是它所提供的洞察。识别出有影响力的特征和无用的特征可以让我们对数据有更多了解。'
- en: '**Data Gathering:** In domains where data comes at a cost (e.g. Medical Diagnosis,
    Manufacturing), identifying a minimal set of features for a classification task
    can save money.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据收集：** 在数据有成本的领域（例如医学诊断、制造业），确定用于分类任务的最小特征集可以节省开支。'
- en: '![](../Images/a07a571945feb8fe693ba775f4a25f66.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a07a571945feb8fe693ba775f4a25f66.png)'
- en: '**Figure 1\.** An Overview of the feature selection process: There are three
    main categories, wrappers, filters and ‘embedded’ where feature selection is embedded
    or is a side-effect of the classification algorithm. In turn, feature selection
    can be considered a strategy for dimension reduction. Image by author. [1]'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1\.** 特征选择过程概述：主要有三大类方法，包裹法、过滤法和“嵌入”法，其中特征选择嵌入或是分类算法的副作用。反过来，特征选择可以被视为一种降维策略。图片作者提供。[1]'
- en: 'The main strategies for FSS are summarised in Figure 1\. Other surveys of feature
    selection [2,3] divide feature selection methods into three categories and we
    follow the same structure:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 总结了特征选择方法的主要策略。其他特征选择的调查[2,3]将特征选择方法分为三类，我们遵循相同的结构。
- en: '**Wrappers** are feature selection methods where the classifier is *wrapped*
    in the feature selection process (see Figure 2). This wrapping allows classification
    performance to drive the feature selection process. This has the advantage of
    tying the feature selection to classifier performance but this comes with a significant
    computational cost as very many classifier variants will be evaluated during the
    selection.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包裹法**是特征选择方法的一种，其中分类器在特征选择过程中被*包裹*（见图2）。这种包裹使得分类性能能够驱动特征选择过程。这有一个将特征选择与分类器性能绑定的优点，但这也伴随着显著的计算成本，因为在选择过程中将评估许多分类器变体。'
- en: '**Filters** cover methods that use criteria other than classifier performance
    to guide feature selection. Typically a filter provides a feature ranking and
    then a selection policy uses this ranking to select a feature subset.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤法**涵盖了使用分类器性能以外的标准来指导特征选择的方法。通常，过滤器提供特征排名，然后选择策略使用该排名来选择特征子集。'
- en: '**Embedded** methods refer to any method where the feature selection *emerges*
    as a by-product of the classifier training process. For instance, training a decision
    tree will almost always select a subset of the available features to build a tree.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入法**指的是特征选择作为分类器训练过程的副产品*产生*的任何方法。例如，训练决策树几乎总是会选择一个可用特征的子集来构建树。'
- en: '![](../Images/f182a50f0fcefda3f1d2924ac29e8a28.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f182a50f0fcefda3f1d2924ac29e8a28.png)'
- en: '**Figure 2\.** Wrappers versus Filters: (a) With Wrappers the classifier is
    *wrapped* in the search process. (b) A Filter strategy uses a separate evaluation
    (e.g. information gain) to score features. Image by author.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2\.** 包裹法与过滤法的对比：（a）在包裹法中，分类器在搜索过程中被*包裹*。（b）过滤器策略使用独立评估（例如信息增益）来评分特征。图片作者提供。'
- en: Methodology
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法论
- en: When evaluating the performance of feature selection strategies we typically
    want to know how things will generalise to unseen data. As a proxy for this we
    hold back some data for testing (option (b) in Figure 3). If we wish to assess
    a few different feature selection alternatives as part of the model development
    then these should be tested within the confines of training data and cross validation
    is the most effective way to do this (option (c)). It should be remembered that
    if the objective is to perform feature selection as part of the deployment of
    an ML system then all the available data can be used for feature selection (option
    (a) in Figure 3).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估特征选择策略的性能时，我们通常想知道这些策略如何推广到未见过的数据上。作为这种评估的代理，我们保留一些数据用于测试（图3中的选项(b)）。如果我们希望在模型开发过程中评估几种不同的特征选择方案，那么这些方案应在训练数据的范围内进行测试，而交叉验证是实现这一目标的最有效方式（选项(c)）。需要记住的是，如果目标是在ML系统的部署过程中进行特征选择，那么可以使用所有可用的数据进行特征选择（图3中的选项(a)）。
- en: '![](../Images/7244fce563e2fae416d26cade63f3081.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7244fce563e2fae416d26cade63f3081.png)'
- en: '**Figure 3\.** Evaluation methodology. (a) If an estimation of generalisation
    accuracy is not required then all data can be used for all aspects of model development.
    (b) Test data can be held back from training to get an estimate of generalisation
    accuracy. (c) Cross validation can be used within the training data for Feature
    Selection. Image by author.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3\.** 评估方法论。 (a) 如果不需要估计泛化准确率，则可以使用所有数据进行模型开发的各个方面。 (b) 可以从训练中保留测试数据，以获得泛化准确率的估计。
    (c) 交叉验证可以在训练数据中用于特征选择。图像由作者提供。'
- en: Before proceeding we need to introduce the notation that will be used. Assume
    we have a dataset ***D*** made up of *n* data samples. **D** = ⟨**X, y**⟩ where
    **y** are the class labels. The examples are described by a set of features **F**
    where *p* = |**F**| so there are *n* objects described by *p* features. So **X**
    has dimension *n*×*p* and **y** is a vector of length *n*. The objective is to
    identify a subset **S ⊂ F** that captures the important information in the dataset.
    After feature selection the dataset is reduced to **X′** with dimension *n*×*k*
    where *k* = |**S**|.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们需要介绍将要使用的符号。假设我们有一个数据集***D***，由*n*个数据样本组成。**D** = ⟨**X, y**⟩，其中**y**是类别标签。这些示例由一组特征**F**描述，其中*p*
    = |**F**|，因此有*n*个对象由*p*个特征描述。所以**X**的维度是*n*×*p*，**y**是一个长度为*n*的向量。目标是识别一个子集**S
    ⊂ F**，以捕捉数据集中重要的信息。经过特征选择后，数据集被减少为**X′**，其维度为*n*×*k*，其中*k* = |**S**|。
- en: Some summary statistics of the datasets used in this tutorial as shown in Table
    1\. These datasets are available in the [GitHub repository](https://github.com/PadraigC/FeatSelTutorial).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中使用的数据集的某些摘要统计信息如表1所示。这些数据集可以在[GitHub 仓库](https://github.com/PadraigC/FeatSelTutorial)中找到。
- en: '![](../Images/23c35bb1944a25b1e35241c188eecaf9.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23c35bb1944a25b1e35241c188eecaf9.png)'
- en: '**Table 1\.** Summary statistics on the datasets used in this tutorial.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 1\.** 本教程中使用的数据集的摘要统计信息。'
- en: Wrappers
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 包装器
- en: If |**F**| is small we could in theory try out all possible subsets of features
    and select the best subset. In this case *‘try out’* would mean training and testing
    a classifier using the feature subset. This would follow the protocol presented
    in Figure 3 (c) where cross-validation on the training data would identify a good
    feature subset and then this could be tested on the test data. However the number
    of possibilities is 2*ᵖ* so exhaustive search quickly becomes impossible — for
    instance if *p*=20 there are over 1 million possibilities to consider.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 |**F**| 较小，我们可以在理论上尝试所有可能的特征子集，并选择最佳子集。在这种情况下，*‘尝试’* 意味着使用特征子集进行分类器的训练和测试。这将遵循图3
    (c)中展示的协议，其中在训练数据上进行交叉验证以识别良好的特征子集，然后可以在测试数据上进行测试。然而，可能性数量为2*ᵖ*，因此穷举搜索很快变得不可能——例如，如果*p*=20，则需要考虑的可能性超过100万。
- en: 'Nevertheless this is how a Wrapper feature selection strategy works with the
    important modification that the search can be greedy or stochastic rather than
    exhaustive. The general idea is shown in Figure 2(a), the classifier is *wrapped*
    in the feature selection process, i.e. classifiers trained using the feature subsets
    are used in the search process. The feature subsets will be evaluated using hold-out
    testing or cross-validation testing on classifiers built using the data. The main
    search strategies used with Wrappers are:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这就是 Wrapper 特征选择策略的工作方式，重要的修改在于搜索可以是贪婪的或随机的，而不是穷举的。总体思路如图 2(a) 所示，分类器在特征选择过程中被*包裹*，即使用特征子集训练的分类器用于搜索过程。特征子集将通过对使用数据构建的分类器进行保留测试或交叉验证测试来评估。与
    Wrapper 一起使用的主要搜索策略包括：
- en: '**Exhaustive Search** evaluates every possible feature subset. If the number
    of features to be considered is small it will be possible to consider all feature
    combinations. However, if *p* > 20 there will be millions of feature subsets to
    be considered and an exhaustive search will not be practical.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**穷举搜索** 评估每一个可能的特征子集。如果待考虑的特征数量较少，则可以考虑所有特征组合。然而，如果 *p* > 20，将会有数百万个特征子集需要考虑，这使得穷举搜索变得不切实际。'
- en: '**Sequential Forward Selection (SFS)** starts with no features selected and
    all classifiers incorporating a single feature are considered (see Figure 4 (a)).
    The best of these is selected and then two feature combinations including this
    feature are evaluated. This process proceeds, adding the winning feature at each
    step, until no further improvements can be made.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序前向选择 (SFS)** 从未选择任何特征开始，并考虑所有包含单一特征的分类器（见图 4 (a)）。选择其中表现最佳的分类器，然后评估包括该特征的两个特征组合。这个过程继续进行，在每一步添加获胜的特征，直到无法进一步改进为止。'
- en: '**Backward Elimination (BE)** proceeds in the opposite direction to FSS, it
    starts with all features selected, considers the options with one feature deleted,
    selects the best of these and continues to eliminate features (see Figure 4 (b)).
    Again, the process is terminated when no improvements can be made.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向消除 (BE)** 以与 FSS 相反的方向进行，从选择所有特征开始，考虑删除一个特征的选项，选择其中表现最佳的，然后继续消除特征（见图 4
    (b)）。同样，当无法进一步改进时，过程终止。'
- en: '**Stochastic Search** methods such as genetic algorithms or simulated annealing
    can readily be applied to Wrapper feature selection. Each state can be defined
    by a feature mask on which crossover and mutation can be performed [4]. Given
    this convenient representation, the use of a stochastic search for feature selection
    is quite straightforward although the evaluation of the fitness function (classifier
    accuracy as measured by cross-validation) is expensive.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机搜索** 方法，如遗传算法或模拟退火，可以很方便地应用于 Wrapper 特征选择。每个状态可以通过特征掩码定义，在此掩码上可以进行交叉和变异操作
    [4]。鉴于这种方便的表示方法，虽然评估适应度函数（通过交叉验证测量的分类器准确性）很昂贵，但使用随机搜索进行特征选择仍然非常直接。'
- en: '![](../Images/db619842aee75cf4192a11d6a071b205.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db619842aee75cf4192a11d6a071b205.png)'
- en: '**Figure 4\. F**eature subset selection using wrappers: (a) Sequential Forward
    Selection (b) Backward Elimination. Image by author.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4**\. 使用 Wrapper 进行特征子集选择：(a) 顺序前向选择 (b) 反向消除。作者提供的图像。'
- en: Our exploration of Wrappers will focus on SFS and BE. These are greedy strategies
    that explore the search space of possible feature subsets as shown in Figure 4\.
    SFS starts with an empty set and proceeds forward considering classifiers built
    on single features. The best of these is selected and then pairs of features incorporating
    this feature are considered. The process could terminate when the addition of
    a new feature doesn’t result in any improvement. As the name suggests, Backward
    Elimination works in the opposite direction. It starts with a full set of features
    (Figure 4(b)) and eliminates the least useful feature at each step. For both SFS
    and BE, the feature subsets are evaluated using cross-validation on the training
    data. The evaluation is done on the Segmentation dataset and the classifier used
    is *k*-Nearest Neighbor (*k*-NN) as it is quite sensitive to noisy or redundant
    features. The [Python notebook](https://github.com/PadraigC/FeatSelTutorial/blob/main/FS-Wrappers.ipynb)
    is available on Github.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 Wrapper 方法的探索将集中在 SFS 和 BE 上。这些是贪婪策略，通过探索可能的特征子集的搜索空间（如图 4 所示）。SFS 从一个空集开始，并向前推进，考虑基于单一特征构建的分类器。选择其中最好的一个，然后考虑将这个特征包含在内的特征对。当新增特征无法带来任何改进时，过程可以终止。顾名思义，Backward
    Elimination 的工作方向正好相反。它从一个完整的特征集开始（图 4(b)），并在每一步去除最不有用的特征。对于 SFS 和 BE，特征子集通过对训练数据的交叉验证来评估。评估是在分割数据集上进行的，所使用的分类器是
    *k*-最近邻 (*k*-NN)，因为它对噪声或冗余特征非常敏感。该 [Python notebook](https://github.com/PadraigC/FeatSelTutorial/blob/main/FS-Wrappers.ipynb)
    可在 Github 上获得。
- en: Both methods have their own advantages and disadvantages as can be seen in Figure
    5\. SFS is inclined to require less computation as the models being evaluated
    are smaller, typically a classifier with a small number of features will take
    less time to train and test. SFS is inclined to select less features (see Figure
    5(a)); this parsinomy is typically an advantage. On the other hand, because BE
    starts with larger feature sets, it can do a better job of assessing how features
    work in combination.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图 5 所示，两种方法各有优缺点。SFS 倾向于需要较少的计算，因为被评估的模型较小，通常特征较少的分类器训练和测试所需的时间更短。SFS 倾向于选择较少的特征（见图
    5(a)）；这种简洁通常是一种优势。另一方面，由于 BE 从较大的特征集开始，它可以更好地评估特征的组合效果。
- en: The overall results for SFS and BE are shown in Figure 5(b). SFS selects seven
    features and 11 are selected by BE. Both feature subsets result in improved accuracy
    on the training data but only the SFS subset results in better accuracy on the
    test data. Indeed the gap between train and test accuracy for BE is evidence of
    overfitting — the selection process has fitted too closely to the characteristics
    of the training data at the cost of generalisation accuracy. Indeed overfitting
    is recognised to be a problem with Wrapper-based feature selection [4].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: SFS 和 BE 的总体结果如图 5(b) 所示。SFS 选择了七个特征，而 BE 选择了 11 个特征。两个特征子集都导致了训练数据上的准确率提高，但只有
    SFS 子集在测试数据上表现出了更好的准确率。实际上，BE 的训练和测试准确率之间的差距是过拟合的证据——选择过程过于紧密地拟合了训练数据的特征，从而损害了泛化准确性。确实，过拟合被认为是
    Wrapper 基于特征选择的一个问题 [4]。
- en: '![](../Images/28fb0ddba5092e2558e413e7f8003a5c.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28fb0ddba5092e2558e413e7f8003a5c.png)'
- en: '**Figure 5.**Feature selection example using wrappers. (a) Accuracy on the
    training data as Sequential Forward Selection proceeds measured using cross-validation.
    (b) Accuracy estimates for feature subsets selected by SFS and BE. SFS selects
    7 features and BE selects 11\. Image by author.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.** 使用 Wrapper 的特征选择示例。(a) 随着 Sequential Forward Selection 的进行，训练数据上的准确率通过交叉验证进行测量。(b)
    SFS 和 BE 选择的特征子集的准确率估计。SFS 选择了 7 个特征，而 BE 选择了 11 个特征。图片由作者提供。'
- en: Filters
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Filters
- en: Figure 2 (a) shows how Wrapper strategies use the classification algorithm in
    the feature selection process. Figure 2(b) shows that Filter strategies do not
    use the classifier for feature selection, instead a separate evaluation function
    is used. The fact that Filters are independent of the classifier is a mixed blessing.
    It means that Filters can be much faster than Wrappers but the selected features
    may not be in tune with the inductive bias of the classifier.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 (a) 显示了 Wrapper 策略如何在特征选择过程中使用分类算法。图 2(b) 显示了 Filter 策略如何不使用分类器进行特征选择，而是使用单独的评估函数。Filters
    不依赖于分类器的事实是一种双刃剑。这意味着 Filters 可能比 Wrappers 快得多，但所选择的特征可能与分类器的归纳偏差不一致。
- en: 'A Filter will entail a feature scoring mechanism and then a selection strategy
    based on these scores. The scoring mechanism needs to quantify how much information
    the feature has about the outcome. The selection strategy might be:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器将包含特征评分机制，然后根据这些评分制定选择策略。评分机制需要量化特征对结果的信息量。选择策略可能是：
- en: Select the top ranked *k* features,
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择排名前*k*的特征，
- en: Select top 50%,
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择前50%的特征，
- en: Select features with scores > 50% of the maximum score,
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择评分高于最大评分50%的特征，
- en: Select features with non-zero scores,
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择具有非零评分的特征，
- en: A hybrid Filter/Wrapper strategy whereby features are ranked using a filter
    and then the performance of subsets based on this ranking is evaluated.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种混合的过滤器/包装器策略，通过使用过滤器对特征进行排名，然后评估基于该排名的子集的性能。
- en: We will now look at three Filter strategies — the Chi-square statistic, information
    gain and permutation feature importance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将研究三种过滤器策略——卡方统计量、信息增益和排列特征重要性。
- en: '**The Chi-square statistic** is a measure of independence between a feature
    and the class label. If samples are organised into a contingency table as shown
    in Figure 6, how different are the cell counts to what would be observed by chance?
    The data in Figure 6(a) suggests that handedness is independent of gender because
    the proportions are the same. The data in (b) suggests that gender is predictive
    of handedness.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**卡方统计量**是衡量特征和类别标签之间独立性的指标。如果样本组织成图6所示的列联表，那么单元计数与偶然观察到的计数有多大的不同？图6(a)中的数据表明左右手偏好与性别独立，因为比例是相同的。图6(b)中的数据表明性别可以预测左右手偏好。'
- en: '![](../Images/a62083a93b253bac8176dbda51889728.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a62083a93b253bac8176dbda51889728.png)'
- en: '**Figure 6**: Two contingency tables showing relationships between handedness
    and gender. If handedness is the class then in (a) it is independent of the gender
    feature, in (b) there is a dependence. Image by author.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**图6**：两个列联表显示了左右手偏好和性别之间的关系。如果左右手偏好是类别，那么在（a）中它与性别特征是独立的，在（b）中则存在依赖关系。图片由作者提供。'
- en: 'The Chi-square statistic allows us to quantify this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 卡方统计量允许我们量化这一点：
- en: '![](../Images/de1fe07c3a2db4b9518dd8af7c6039bf.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de1fe07c3a2db4b9518dd8af7c6039bf.png)'
- en: The statistic is a sum over the *m* cells. For each cell we consider the difference
    between the observed count *Oᵢ* and the expected count *Eᵢ* if the feature and
    the class were independent. In Figure 6(a) this difference would be zero because
    the feature and the class are independent. In (b) there would be a difference
    so the statistic would be positive. In general, the greater the dependence the
    larger the statistic. If the feature values are numeric rather than categorical
    then the feature values can be binned to enable the construction of the contingency
    table [5].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 统计量是对*m*个单元的总和。对于每个单元，我们考虑观察到的计数*Oᵢ*与如果特征和类别是独立的情况下预期计数*Eᵢ*之间的差异。在图6(a)中，这个差异将为零，因为特征和类别是独立的。在（b）中将存在差异，因此统计量将是正值。一般来说，依赖关系越大，统计量也越大。如果特征值是数值型而不是分类型，则可以对特征值进行分箱，以便构建列联表[5]。
- en: '**Information gain** is an alternative information-theoretic measure quantifying
    the information a feature contains about a class [6]. In Figure 6(b) by knowing
    the gender we *gain* information about handedness. In a binary classification
    scenario, let’s assume the probability of a positive and negative outcomes are
    respectively *p* and *q*. Then the entropy of a dataset based on these proportions
    is:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**信息增益**是一种信息论度量，用于量化特征包含有关类别的信息[6]。在图6(b)中，通过知道性别我们*获得*了有关左右手偏好的信息。在二分类场景中，假设正负结果的概率分别是*p*和*q*。那么基于这些比例的数据集的熵是：'
- en: '![](../Images/48fe6c4113e8d4272ea6a34553a1a9b8.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48fe6c4113e8d4272ea6a34553a1a9b8.png)'
- en: 'then the information gain for any feature *f* in the dataset in terms of the
    class label is:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，数据集中任何特征*f*相对于类别标签的信息增益是：
- en: '![](../Images/f120095a1b4aee783e4847f406c65106.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f120095a1b4aee783e4847f406c65106.png)'
- en: As with the Chi-square statistic, information gain (I-Gain) allows us to rank
    features for the purpose of feature selection. This is illustrated in Figure 7\.
    This shows the Segmentation features ranked by both measures. The Python notebook
    is available at this GitHub <[link](https://github.com/PadraigC/FeatSelTutorial/blob/main/FS-Filters.ipynb)>.
    The plot shows the scores sorted by I-Gain score. It is clear that the scores
    are well correlated (Pearson correlation score of 0.86) so feature subsets selected
    based on these scores should be reasonably similar.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与卡方统计量类似，信息增益（I-Gain）允许我们对特征进行排序以进行特征选择。这在图 7 中得到了说明。图中显示了根据两种度量对 Segmentation
    特征进行排名的结果。Python 笔记本可以在这个 GitHub <[链接](https://github.com/PadraigC/FeatSelTutorial/blob/main/FS-Filters.ipynb)>
    找到。图表显示了按照 I-Gain 分数排序的分数。显然，分数的相关性很高（皮尔逊相关系数为 0.86），因此根据这些分数选择的特征子集应该是相当相似的。
- en: '![](../Images/4166a86cf90df099f71eb632c3c66cda.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4166a86cf90df099f71eb632c3c66cda.png)'
- en: '**Figure 7:** The Segmentation features ranked by I-Gain and the Chi-square
    statistic. Image by author.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7：** 根据 I-Gain 和卡方统计量对 Segmentation 特征进行排序。图片作者提供。'
- en: This does prove to be the case when we look at the performance of classifiers
    built with feature subsets based on these rankings. In Figure 8 we see the results
    of a range of top *k* selection policies (*k* = 3, 6, 10, 15). At *k* = 10 both
    scores select a feature subset that produces accuracy on the test set equivalent
    to that obtained with the full feature set. The evaluation strategy here conforms
    to pattern (b) in Figure 3, the feature scoring is done using the training data
    and then tested on the test set.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看基于这些排名构建的特征子集的分类器性能时，这确实是这样。在图 8 中，我们可以看到不同 *k* 选择策略的结果（*k* = 3, 6, 10,
    15）。在 *k* = 10 时，这两个分数都选择了一个特征子集，该子集在测试集上的准确率与使用完整特征集获得的准确率相同。这里的评估策略符合图 3 中的模式（b），特征评分使用训练数据完成，然后在测试集上进行测试。
- en: '![](../Images/bdf277890b8874216e9f2c1d75fca0b9.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdf277890b8874216e9f2c1d75fca0b9.png)'
- en: '**Figure 8:** Accuracy estimates for top-*n* features in the Segmentation dataset
    selected using I-Gain and the Chi-squared statistic. Image by author.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8：** 使用 I-Gain 和卡方统计量选择的 Segmentation 数据集中的前-*n* 特征的准确率估计。图片作者提供。'
- en: '**Permutation Feature Importance** is based on the principle that if we want
    to find out how important something is to a process we can break it to see what
    happens. To score the importance of a feature we can *permute* values for that
    feature in a test set to see what is the impact on the overall accuracy. If the
    error increases significantly when a variable is *noised* in this way that variable
    is important. If the error does not increase that variable is not useful for the
    classification. The overall process is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**排列特征重要性** 基于这样的原则：如果我们想了解某物对过程的重要性，我们可以打破它以查看会发生什么。为了评估特征的重要性，我们可以在测试集中对该特征的值进行*置换*，以查看对整体准确率的影响。如果在这种情况下变量被*加噪*时错误显著增加，那么该变量是重要的。如果错误没有增加，那么该变量对分类没有用。整体过程如下：'
- en: Fit a classifier on the data
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据上拟合分类器
- en: Calculate a baseline accuracy
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算基准准确率
- en: Shuffle feature values and calculate accuracy again
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打乱特征值并重新计算准确率
- en: Measure increase in error against error without shuffling
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量与未打乱错误的误差增加
- en: This process is typically repeated multiple times (say 10) to get more stable
    feature importance scores. Permutation importance is used as the first stage in
    the proposed strategy presented at the end of this article.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程通常会重复多次（例如 10 次），以获得更稳定的特征重要性分数。排列重要性被用作本文末尾提出的策略中的第一阶段。
- en: In Figure 9 we see the permutation importance scores for *k-*NN and Gaussian
    Naive Bayes on the Segmentation datasets (notebook <[here](https://github.com/PadraigC/FeatSelTutorial/blob/main/FS-Permutation-FI.ipynb)>).
    We can see that the rankings are reasonably well correlated but by no means the
    same. This difference is down to the different classifiers ‘preferring’ different
    features and to an inherent instability is feature selection methods.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 9 中，我们看到 Segmentation 数据集上 *k-*NN 和高斯朴素贝叶斯的排列重要性分数（笔记本 <[在这里](https://github.com/PadraigC/FeatSelTutorial/blob/main/FS-Permutation-FI.ipynb)>）。我们可以看到排名合理地相关，但并不完全相同。这种差异是由于不同的分类器“偏好”不同的特征以及特征选择方法固有的不稳定性。
- en: '![](../Images/1d06f6eb90ddb8013b029f2f5500b3c7.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d06f6eb90ddb8013b029f2f5500b3c7.png)'
- en: '**Figure 9:** Barcharts showing permutation feature importance scores for k-NN
    and Gaussian Naive Bayes on the Segmentation dataset. Image by author.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Embedded Methods
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we cover feature selection methods that *emerge* naturally from
    the classification algorithm or arise as a side effect of the algorithm. We will
    see that with Decision Trees and Logistic Regression feature selection can be
    an integrated part of the model building process.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision Trees:** The construction of a Decision Tree from a data set will
    very often entail feature selection as some of the features will not appear in
    the tree. Features not included in the tree are effectively selected out. We show
    an example of this on the Penguins dataset in Figure 10.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce2cb3cea5c527fd63cd76405d51a9b8.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: '**Figure 10:** A decision tree for the Penguins dataset. While the data is
    described by four features only three are selected. Image by author.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: In this example the dataset has been divided 50:50 into train and test sets.
    This tree has been trained on the training data and has 93% accuracy on the test
    data (see notebook <[here](https://github.com/PadraigC/FeatSelTutorial/blob/main/FS-D-Tree.ipynb)>).
    This dataset has four features, *flipper length, bill length, bill depth* and
    *body mass*. It is clear from the tree in Figure 10 that three of the four features
    are selected, body mass is not selected.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: This tree has been constructed with the default **scikit-learn** parameters
    so there is no pruning. It is normal in Decision Tree learning to constrain (i.e.
    prune) the size of the tree to prevent overfitting. The use of pruning to prevent
    overfitting will push the feature selection further as even less features will
    be selected in smaller trees.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '**Logistic Regression, Lasso:** In multivariate linear models such as linear
    regression or logistic regression, feature selection can be achieved as a side
    effect of regularization. In ML regularization refers to mechanisms designed to
    simplify models in order to prevent overfitting. Thus regularization can cause
    features to be deselected. Elastic net and Lasso are popular regularization methods
    for linear models. Here we will provide an overview of how Lasso works [7] and
    present examples of Lasso in operation. Starting with the basics, a multivariate
    regression works as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a377cf2d6a71712062b04f5355bae84.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: The dependent variable *y* is a linear function of the input features; for each
    feature *xᵢ* the weight of that feature is determined by the corresponding *βᵢ*
    parameter. For binary classification problems ([0,1] labels) we can use logistic
    regression where the dependent variable is the log odds that an outcome variable
    is 1\. If *pr* is the probability that the label is 1 then the odds is *pr/*(*1-pr*).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/491e9f5a964900f791588b1b7c9e96c7.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: 'So logistic regression provides a class probability:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db18a6bfe0d12185e1c2470143ff5f3d.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: 'Regularization prevents overfitting by limiting model capacity; this is done
    by limiting the size of weights. The two options are L₁ or L₂ regularization:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化通过限制模型容量来防止过拟合；这通过限制权重的大小来实现。两种选择是L₁或L₂正则化：
- en: '![](../Images/dd8c79366ebbcec200abce46a59a02c4.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd8c79366ebbcec200abce46a59a02c4.png)'
- en: So the *β* parameters in are fitted to the training data subject to these L₁
    or L₂ constraints. It transpires that when an L₁ regularization is used the weaker
    weights will go to zero, i.e. those features will be deselected. There is an excellent
    explanation of *why* this happens in the original Lasso paper by Tibshirani [7].
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*β*参数在符合这些L₁或L₂约束的训练数据上进行拟合。结果表明，当使用L₁正则化时，较弱的权重会变为零，即这些特征会被剔除。在Tibshirani
    [7] 的原始Lasso论文中对*为什么*会发生这种情况有很好的解释。
- en: To demonstrate this on our sample datasets we reduce them to binary classification
    problems to make the overall process more transparent (notebook <[here](https://github.com/PadraigC/FeatSelTutorial/blob/main/FS-Lasso.ipynb)>).
    However, feature selection using Lasso also works with multiclass problems. The
    results are shown in Figures 11 and 12\. Because the datasets have been reduced
    to just two classes (Cement and Window for Segmentation and Adelie and Chinstrap
    for Penguins) the accuracies are higher than for the multi-class scenario.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的示例数据集上演示这一点，我们将它们简化为二分类问题，以使整体过程更加透明（笔记本 <[在这里](https://github.com/PadraigC/FeatSelTutorial/blob/main/FS-Lasso.ipynb)>）。然而，使用Lasso进行特征选择也适用于多类别问题。结果见图11和图12。由于数据集被减少到仅有两个类别（分割的水泥和窗户，以及企鹅的阿德利和凤头企鹅），因此准确率比多类别场景更高。
- en: '![](../Images/eae114d3108f1be522dbfc10c46526e4.png)![](../Images/67d29af8b5665e7d28fe922bd3f7ba73.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eae114d3108f1be522dbfc10c46526e4.png)![](../Images/67d29af8b5665e7d28fe922bd3f7ba73.png)'
- en: '**Figure 11:** The impact of Lasso on the Penguins (top) Segmentation datasets.
    Lasso reduces the magnitude of the β parameters; some parameters get reduced to
    zero. Results for Lasso with the default regularization (C=1) and milder regularization
    (C=10) are shown. Images by author.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**图11：** Lasso对企鹅（上图）分割数据集的影响。Lasso减少了β参数的大小；一些参数被减少到零。显示了Lasso在默认正则化（C=1）和较温和正则化（C=10）下的结果。图片由作者提供。'
- en: The extent of the feature reduction with Lasso is controlled by the regularization
    parameter C. Results are included for two levels of regularization, C=10 and C=1\.
    C=10 results in less regularization so more features are retained. In both cases
    the default regularization results in too much feature reduction and generalization
    accuracy is reduced. For the Penguins dataset just two features are retained while
    three are retained in the Segmentation dataset (see Figure 15). The milder regularization
    retains more features resulting in no loss of generalization accuracy.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso特征减少的程度由正则化参数C控制。包括了两个正则化水平的结果，C=10和C=1。C=10会导致较少的正则化，从而保留更多的特征。在这两种情况下，默认正则化导致特征减少过多，泛化准确率降低。对于企鹅数据集，仅保留了两个特征，而在分割数据集中保留了三个特征（见图15）。较温和的正则化保留了更多特征，结果没有损失泛化准确率。
- en: '![](../Images/73a5cd608e8d775688ba3d0ef3f3b9d3.png)![](../Images/8f6dbe4c23fc0f282a1b2a9c79d11980.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73a5cd608e8d775688ba3d0ef3f3b9d3.png)![](../Images/8f6dbe4c23fc0f282a1b2a9c79d11980.png)'
- en: '**Figure 12:** The impact of Lasso on train and test accuracy. Results for
    Lasso with the default regularization (C=1) and milder regularization (C=10) are
    shown. Images by author.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12：** Lasso对训练和测试准确率的影响。显示了Lasso在默认正则化（C=1）和较温和正则化（C=10）下的结果。图片由作者提供。'
- en: Proposed Strategy
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提议策略
- en: As stated at the beginning, our proposed strategy will use *Wrapper* for the
    final selection process and, where necessary, *permutation importance* as an initial
    filter (GitHub <[link](https://github.com/PadraigC/FeatSelTutorial/blob/main/FS-Permutation%2BWrapper.ipynb)>).
    The example we present here used the Ionosphere dataset from the UCI repository.
    This dataset has 34 features so there are over 17 billion possible feature subsets.
    Even with a greedy search strategy (e.g. SFS or BE), applying a Wrapper to the
    full feature set would be very computationally expensive. So we use the two step
    strategy shown in Figure 13; we use permutation importance to reduce the 34 features
    to a candidate set of 18 and then employ a Wrapper.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 正如开头所述，我们提出的策略将使用 *Wrapper* 进行最终选择过程，并在必要时使用 *permutation importance* 作为初始筛选器（GitHub
    <[link](https://github.com/PadraigC/FeatSelTutorial/blob/main/FS-Permutation%2BWrapper.ipynb)>）。我们在这里呈现的示例使用了来自
    UCI 数据库的 Ionosphere 数据集。该数据集有 34 个特征，因此有超过 170 亿个可能的特征子集。即使使用贪心搜索策略（例如 SFS 或 BE），对整个特征集应用
    Wrapper 也会非常计算密集。因此，我们使用图 13 所示的两步策略；我们使用排列重要性将 34 个特征减少到一个候选集 18 个特征，然后使用 Wrapper。
- en: '![](../Images/ad0f9173d58d0e18343a251ce58d5292.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad0f9173d58d0e18343a251ce58d5292.png)'
- en: '**Figure 13\.** The two stage feature selection process. The numbers in green
    indicate the feature counts in the worked example. Image by author.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 13\.** 两阶段特征选择过程。绿色中的数字表示在实例中的特征数量。图像由作者提供。'
- en: The classifier used in this example is *k-*NN and we hold back 50% of the data
    for final testing. The scores from the permutation feature importance stage are
    shown in Figure 14\. We drop the features that don’t have a positive score, leaving
    us with 18 features to pass to the Wrapper stage.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例中使用的分类器是 *k-*NN，我们保留了 50% 的数据用于最终测试。排列特征重要性阶段的评分如图 14 所示。我们丢弃了没有正评分的特征，留下了
    18 个特征用于 Wrapper 阶段。
- en: '![](../Images/179d1e0adffe3826c0849fe2882319e9.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/179d1e0adffe3826c0849fe2882319e9.png)'
- en: '**Figure 14\.** The feature importance scores coming from the Permutation Importance
    stage. In theory these should always be zero or greater; in practice, some scores
    are negative indicating that the results after permuting a feature actually improve
    by chance. It is probably safe to drop these features. Image by author.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 14\.** 来源于排列重要性阶段的特征重要性评分。理论上这些评分应该始终为零或更大；在实际操作中，一些评分为负，表示特征排列后的结果实际上是偶然改进的。可以安全地丢弃这些特征。图像由作者提供。'
- en: We then run a Wrapper search considering these 18 features, using the training
    data only. We use backward elimination as described earlier and it selects 16
    features. In Figure we see accuracy estimates for the three feature sets. The
    estimates on the training data use cross validation and the hold-out estimate
    shows accuracy on the 50% held back from the feature selection process.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们仅使用训练数据对这 18 个特征进行 Wrapper 搜索。我们使用前面描述的向后消除方法，它选择了 16 个特征。在图中我们看到三个特征集的准确性估计。训练数据上的估计使用交叉验证，保留估计显示了从特征选择过程中保留的
    50% 数据的准确性。
- en: '![](../Images/f9fc379ddb9b7c007c142f81ecf6f2b9.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9fc379ddb9b7c007c142f81ecf6f2b9.png)'
- en: '**Figure 15\.** Accuracy scores of the three feature sets. Accuracy on the
    training set (the set used for feature selection) is measured using cross validation.
    The accuracy on the test set held back from the feature selection process is a
    simple hold-out measure. Image by author.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 15\.** 三个特征集的准确性评分。训练集（用于特征选择的集）上的准确性通过交叉验证来测量。测试集上的准确性是从特征选择过程中保留的 50%
    数据的简单保留度量。图像由作者提供。'
- en: We see that both feature selection stages produce accuracy improvements on the
    training data but for the Wrapper stage we see no further improvement on the test
    data. This is in line with other research that shows that in-depth feature selection
    effort can result in overfitting.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到两个特征选择阶段在训练数据上都提高了准确性，但对于 Wrapper 阶段，在测试数据上没有进一步改善。这与其他研究一致，这些研究表明深入的特征选择工作可能导致过拟合。
- en: Conclusion
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'So that concludes our overview of state-of-the-art feature subset collection
    methods. One of the challenges is that there are so many alternative methods to
    choose from; the strategy we propose is:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这就总结了我们对最先进特征子集选择方法的概述。面临的挑战之一是有这么多可供选择的替代方法；我们提出的策略是：
- en: If you have loads of training data, the two stage strategy presented above will
    be worth while.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有大量的训练数据，上述的两阶段策略将是值得的。
- en: If training data is scarce and there is a risk of overfitting you could stop
    feature selection after the permutation importance step.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python implementations (using scikit-learn) for the methods covered here are
    available in this [GitHub repository](https://github.com/PadraigC/FeatSelTutorial).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cunningham, P., Kathirgamanathan, B., & Delany, S. J. (2021). Feature Selection
    Tutorial with Python Examples. *arXiv preprint* [*arXiv:2106.06437*](https://arxiv.org/abs/2106.06437).
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Isabelle Guyon and André Elisseeff. (2003) An introduction to variable and feature
    selection. J*ournal of machine learning research,* 2003.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Luis Carlos Molina Félix, Luis Antonio Belanche Muñoz, and M Àngela Nebot Castells
    (2002) Feature selection algorithms: a survey and experimental evaluation. In
    200*2 IEEE International Conference on Data Mining (ICDM 2002) pag*es 306–313.'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'John Loughrey and Pádraig Cunningham. Overfitting in wrapper-based feature
    subset selection: The harder you try the worse it gets. In I*nternational Conference
    on Innovative Techniques and Applications of Artificial Intelligence,* pages 33–43\.
    Springer, 2004.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Xin Jin et al. “Machine learning techniques and chi- square feature selection
    for cancer classification using SAGE gene expression profiles”. In: *International
    Workshop on Data Mining for Biomedical Applications*. Springer. 2006, pp. 106–115.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'John D Kelleher, Brian Mac Namee, and Aoife D’arcy. *Fundamentals of machine
    learning for predictive data analytics: algorithms, worked examples, and case
    studies*. MIT Press, 2020.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Robert Tibshirani. “Regression shrinkage and selection via the lasso”. In:
    *Journal of the Royal Statistical So- ciety: Series B (Methodological)* 58.1 (1996),
    pp. 267– 288.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
