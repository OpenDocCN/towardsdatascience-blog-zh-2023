- en: Unsupervised Learning Method Series — Exploring K-Means Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/unsupervised-learning-method-series-exploring-k-means-clustering-d129fff3ab6a](https://towardsdatascience.com/unsupervised-learning-method-series-exploring-k-means-clustering-d129fff3ab6a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s explore one of the most famous unsupervised learning methods and how it
    uses distances to map similar instances together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ivopbernardo.medium.com/?source=post_page-----d129fff3ab6a--------------------------------)[![Ivo
    Bernardo](../Images/39887b6f3e63a67c0545e87962ad5df0.png)](https://ivopbernardo.medium.com/?source=post_page-----d129fff3ab6a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d129fff3ab6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d129fff3ab6a--------------------------------)
    [Ivo Bernardo](https://ivopbernardo.medium.com/?source=post_page-----d129fff3ab6a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d129fff3ab6a--------------------------------)
    ·13 min read·Apr 5, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3f57dc819639ea669371626800d21bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [alexlanting](https://unsplash.com/pt-br/@alexlanting) @Unsplash.com
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning is a misterious, yet fun, art. While there is no ground
    truth label to predict and it may be harder to evaluate the solution we come up
    with, unsupervised learning methods are extremely interesting techniques to understand
    our data’s structure and reduce it’s complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Along with visualization and dimensionality reduction techniques, clustering
    is an important group of unsupervised machine learning methods that help us collapse
    single instances into fewer examples by losing some of the original signal from
    our data. In this unsupervised learning series, we’ll first approach `k-means`
    clustering, a very interesting and famous distance-based clustering method.
  prefs: []
  type: TYPE_NORMAL
- en: K-means Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The K-means algorithm works by mapping every observation to a fixed number (*k*)
    of clusters in a dataset based on distances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by visualizing an example where we have customers mapped on a 2
    dimensional plot by `Age`and `Annual Income`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2744cfd3c10f245ffe2621a550600507.png)'
  prefs: []
  type: TYPE_IMG
- en: Age vs. Annual Income Example — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: If we needed to group the customers of our fictional shop (each customer is
    a dot), how many distinct groups should we choose, **given that there is no definitive
    labeling system for these groups?**
  prefs: []
  type: TYPE_NORMAL
- en: To answer these questions, we will first conduct some experiments. **Our initial
    assumption is that there are 2 distinct groups, and we need to allocate our customers
    to them.**
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, we will randomly select two points on the plot, which will serve
    as *cluster centroids* (representing the center of our groups). These points are
    highlighted in orange for ease of identification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/415a38e8d9a9351d574587c93a5f6f80.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster Centroids Example— Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The number of orange points are considered the *k* of k-means. Our solution
    is awful. Why? **Because the orange points fail to represent the underlying data.**
    We have a centroid (bottom left corner) that is just too far away from the data.
    How can we improve this?
  prefs: []
  type: TYPE_NORMAL
- en: The first step of *k-means* consists of allocating every data point to the nearest
    *centroid.* In our case, every customer will be considered part of one of the
    orange dots we see in the picture.
  prefs: []
  type: TYPE_NORMAL
- en: To make this even easier to understand, let’s start by naming one of our points
    — customer *Steve*!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac098e17ea7d6051ae722528db68b5d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Steve vs. Centroids — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '*Steve* is a bit confused — he doesn’t know which group he should join. **Should
    he join the group on the bottom right corner** (represented by the orange dot)
    **or the one on the top left corner** (represented by the other orange dot)?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s give a helping hand to Steve, by drawing the distance between itself
    and each group:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e234e62b6822aa53e8a46f3b8a1d8c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Distance from Steve to First Group — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'One easy way to represent this distance is by calculating the *euclidean distance*
    between Steve and the group, something that is represented by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce5ea57a06d30834de0c3b271fec57f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we substitute *P1* and *P2* by *Steve* and the cluster centroid, we have
    the following calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d25cdc0b220f2d75d5c474bc49d198b.png)'
  prefs: []
  type: TYPE_IMG
- en: The distance of Steve to Centroid 1 is **22.36**. What about the distance from
    Steve to Centroid 2?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da0facb2f05dab85c208983573f9496f.png)'
  prefs: []
  type: TYPE_IMG
- en: Distance from Steve to Second Group — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the distance is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77dc81d4b4c995a8d8524c52f03ab44d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Steve is clearly nearer Group 1** (or Centroid 1)according to the euclidean
    distance, so he’ll get assigned to that group — let’s do that by painting the
    dot representing *Steve* with a red column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf4eb7c92d48670590aefa9e98ea3477.png)'
  prefs: []
  type: TYPE_IMG
- en: Assigning Steve to the First Group — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'If we repeat the same process for all other customers the result will be the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2be5e462c2f3d30ef88052b87dd8508f.png)'
  prefs: []
  type: TYPE_IMG
- en: In the figure above, we mark customers that are near Centroid 1 as red (just
    like *Steve*) and the ones that are near Centroid 2 as green.
  prefs: []
  type: TYPE_NORMAL
- en: Is our clustering solution done? Nope!
  prefs: []
  type: TYPE_NORMAL
- en: The next phase of *k-means* is to recalculate the centroids (orange dots). How
    can we do that? We just compute the average of the points assigned to each cluster!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case:'
  prefs: []
  type: TYPE_NORMAL
- en: Average of all `Ages` assigned to Centroid 1 is 46.9\. Average of all `Annual
    Income` for this group is 39.9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average of all `Ages` for Centroid 2 is 37\. Average of all `Annual Income`
    for this group is 91.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The coordinates *(46.9, 39.9)* and *(37, 91)* will be our new centroids! Let’s
    move them in our 2-D plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa69d317eed10b28581d5a5f9a5ce937.png)'
  prefs: []
  type: TYPE_IMG
- en: Moving our Centroids — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: With the new centroids on our plot, we reset the attribution of customers to
    clusters. *Steve* and friends will have to be allocated again!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86557f68118d32a5efd70d6a848b5ca3.png)'
  prefs: []
  type: TYPE_IMG
- en: Re-setting cluster attribution — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'We calculate the *euclidean distance* between each data point and centroids
    again — after doing that, we’ll have new groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4267631b095efef537e1f97604299135.png)'
  prefs: []
  type: TYPE_IMG
- en: Re-setting cluster attribution — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Notice that something changed between our first and second iteration! Let’s
    visualize our iterations again next to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Iteration 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2be5e462c2f3d30ef88052b87dd8508f.png)'
  prefs: []
  type: TYPE_IMG
- en: 1st Iteration of K-means — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Iteration 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5931d08f715a20996c851eff4d323e51.png)'
  prefs: []
  type: TYPE_IMG
- en: 2nd Iteration of K-means — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Some customers moved from Group 1 to Group 2 between iterations — namely the
    purple points highlighted below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c91aeff17d8f7e5511f3dd96c925ffc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Moving Customers — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This is a central theme in *k-means* clustering as the process will stop when
    **no points change cluster from iteration to iteration.**
  prefs: []
  type: TYPE_NORMAL
- en: In our case, two iterations are enough as no customer will change it’s group
    in the next iteration — *k-means* complete!
  prefs: []
  type: TYPE_NORMAL
- en: After performing a clustering grouping, we will treat our data as two single
    data points , **represented by the centroids!**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1359171cc04507f98ba93b3f03e1c1fd.png)'
  prefs: []
  type: TYPE_IMG
- en: K-means Centroids — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This is a very important step — **we are making the active choice of reducing
    our data points to only 2.** This is a significant loss of variance of the data
    and one of the core ideas behind of clustering.
  prefs: []
  type: TYPE_NORMAL
- en: '**How can we evaluate this solution?** One idea is to compute the *within clusters
    sum of squares,* a metric that calculates the distance between each data point
    and it’s corresponding cluster — visually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30014a329a95733566548ab1efbf5cf9.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing WCSS — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: If we compute all the euclidean distances between our points and their respective
    centroid, we will get a value of around 8850 — this value gives us translates
    into the information we are losing by considering our customers as two clusters.
    Additionally, we can also check the *Between Cluster Sum of Squares(bcss)* that
    measures the average squared distance between all centroids.
  prefs: []
  type: TYPE_NORMAL
- en: '**Naturally, when we add a new centroid, the *WCSS* will be lower, as points
    will have to travel less to their centroid:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ceeb61c03e02a4f04a40c9eccbf2e83.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing WCSS with 3 clusters — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: With the *k-means* intuition in our pocket, we can check the *sklearn* implementation
    in Python. Additionally, we still don’t know how to evaluate an appropriate number
    of clusters (*k*) — something we will see next!
  prefs: []
  type: TYPE_NORMAL
- en: Sklearn Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part we’ll use the [The Airlines Customer Satisfaction](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction)
    dataset, that contains information about customer satisfaction for an airline
    company. Each observation represents a customer, and the variables include things
    like the customer’s demographic information, the travel type (business, etc.),
    and their satisfaction ratings for various aspects of the flight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a look at the first 5 rows and 13 columns of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d17754ed06e12f2be0b1b5511338074a.png)'
  prefs: []
  type: TYPE_IMG
- en: Airline Data Preview — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting our pipeline with preprocessing let’s remove some columns that we
    don’t want to influence our clusters — for that purpose, I’ll remove all categorical
    columns from a possible solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Satisfaction;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gender;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer Type;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type of Travel;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naturally, this is a choice that I’m doing in the data pipeline, for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: I want to keep the focus of this blog post on explaining k-means, and avoid
    building a more complex data pipeline that takes away our attention from that
    purpose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When it comes to categorical variables, we don’t want to have too many dummy
    variables influencing our clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we add more and more binary (also called *dummy*) variables into the k-means
    solution, these variables start to weight a lot in the final clustering distances,
    even after standardization, so it’s very important to be cautious when adding
    this types of data to any *k-means* solution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I also noticed that there are 393 rows with *Arrival Delay in Minutes* as NA
    — *k-means* implementation will not deal very well with this, so we need to do
    some *data imputation*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we zoom-in on these rows, no pattern emerges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c4f93736e875754c381a639d0f4e8888.png)'
  prefs: []
  type: TYPE_IMG
- en: Rows with Arrival Delay in Minutes as Null — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'For these rows, I’ll just assume that the plane arrived with the same delay
    as it departed — applying that rule using `np.where`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The rule is quite simple and we are leaning on `np.where` to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: When `Arrival Delay in Minutes` is `na` , we say that this column should be
    equal to the `Departure Delay in Minutes` , otherwise we use the original value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next step in the preprocessing pipeline is to standardize all our variables
    into a common scale. Particularly in *k-means,* where distances are a crucial
    part of the algorithm, this step may be extremely important to find meaningful
    customers (although testing without standardization may also give you good results,
    depending on how the underlying variable distribution behaves and how large the
    difference in numeric scales).
  prefs: []
  type: TYPE_NORMAL
- en: 'I’m going to apply a *StandardScaler* from `sklearn` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Preprocessing done** — we’re ready to fit our *k-means* solution!'
  prefs: []
  type: TYPE_NORMAL
- en: But..
  prefs: []
  type: TYPE_NORMAL
- en: How many centroids do we choose?
  prefs: []
  type: TYPE_NORMAL
- en: 'Normally, in a *k-means* solution, we would run the algorithm for different
    *k’s* and evaluate each solution *WCSS —* that’s what we will do below, using
    *KMeans* from *sklearn,* and obtaining the `wcss` for each one of them (stored
    in the `inertia_` attribute):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now visualize the evolution of `WCSS` per each solution. There are [several
    methods](https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters/)
    to choose the number of appropriate clusters — in this post, we’ll use the `elbow
    method` that chooses the # of clusters where adding the curve below becomes starts
    to become less steep as this represents the solution where adding a new cluster
    won’t lower `WCSS` so much:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d968a090ee7195422e3f89c865ff33b.png)'
  prefs: []
  type: TYPE_IMG
- en: WCSS Plot— Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to choose 5 as our ideal number of clusters (keep in mind that
    choosing a point in the elbow plot is not scientific and it’s actually a good
    idea to test different solution near the “elbow”):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba186201c26a78cb9c4c17064853a2c7.png)'
  prefs: []
  type: TYPE_IMG
- en: WCSS Plot — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'To fit the solution with 5 clusters, we can pass that value to the parameter
    in the `Kmeans` implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we’ll predict the cluster based on this solution for each customer on
    our filtered data frame — although we should `predict` on the scaled_data (as
    it contains the same scale where the solution was fitted), it’s actually a good
    idea to add the predictions to the original dataframe so that we are able to interpret
    the means of the clusters with scales that make sense:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'How do we analyze the clusters? One cool idea is to compare the means of the
    features across clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To interpret the `cluster_kmeans` , we just compute the averages accross all
    variables for each cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1531fa48172aaff6a653dd268a1173e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster Means — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: For example, cluster index 1 seem very annoyed with their `Seat Comfort` as
    , on average, customers that belong to this group only gave `1.83` points to this
    variable on the survey. Although we can keep doing these comparisons for all variables,
    we still have a lot of dimensions (features) on our clustering solution, making
    it harder to analyze the differences between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'To remove some features from the clustering solutions, some ideas we can apply
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Remove or combine highly correlated variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit a Principal Component Analysis or other dimensionality reduction techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To keep this post light, let’s analyze the correlation matrix between our numeric
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46b91ad255123fd8cbf08239e8669b80.png)'
  prefs: []
  type: TYPE_IMG
- en: Correlation Matrix of Numeric Features — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'From the correlation matrix above, we can identify that “Online Boarding”,
    “Inflight Wifi Service”, “Online Support” and “Ease of Online Booking” seem to
    be correlated together. I’m going to average these 4 variables into a single one
    called `Online and Wi-Fi Satisfaction` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Seat Comfort and Food and Drink can also be combined into a “Comfort & Food”
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, I’m going to drop `Arrival Delay in Minutes` as it has a very high
    correlation with `Departure Delay in Minutes` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we’ve already fitted a clustering solution to this dataset, so let me
    remove that variable as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We’re only left with 13 features! *K-means* solutions may also suffer from the
    curse of dimensionality (particularly when we try to interpret our clusters) and
    trying to reduce the dataset features may be a good idea to interpret our clustering
    solution in a more easy manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the elbow curve based on the new dataset that contains less features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd445aee010af3be18436363b2b7c1d8.png)'
  prefs: []
  type: TYPE_IMG
- en: WCSS Plot for KMeans with Less Features— Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, I’,m going to choose 6 clusters as a solution. Predicting them
    and adding the new `cluster` to the `airline_data_filter` dataset again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/68186a60718757f343623f1a7f5878b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster Profile — First Columns of the Dataset— Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61fa9062d24c34d13d04edf709651381.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster Profile — Other Columns of the Dataset — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'To do a profiling of our customers, it’s relevant to know the average of our
    features, as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21dc44f527c7d80f3794f743c27c2a44.png)'
  prefs: []
  type: TYPE_IMG
- en: Global average of features — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the comparison between the average of each variable, we can now do
    some profilling of our clusters! For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster index 0 consists of a group of customers that is on the average of most
    variables. They do seem a bit unhappy about some parts of the flying experience
    such as `On-board and Leg room Service` , `Baggage Handling` , `Checkin service`
    and `Cleanliness` . How do we know that? Because they gave, on average 2.8 points
    to these variables in the survey, 0.5 p.p below the overall average.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster index 1 is made of very happy customers. These customers rated the airline
    services with above than average points (most variables have an average of above
    4 stars for this group of customers).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, cluster with index 2 seems very unhappy. These customers
    rated the airline services with a below than average rating.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster with index 3 consists of customers with longer trips and that rate the
    airline services a bit below the average. These customers also seem to have been
    impacted more often by a delay on their flight.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cluster with index 4 has customers that are generally happy except for three
    variables: `Departure/Arrival Time Convenience` , `Gate Location` and `Comfort
    & Food` . Probably, there’s some extra variable that may justify these ratings,
    such as these customers travelling low-cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster with index 5 contains the younger customers. It’s very interesting to
    check that they rated most of the variables with average points, except for `Inflight
    Entertainment` , `Online & Wi-Fi Satisfaction` and `Comfort and Food` . Possibly,
    as these customers are younger, they had different expectations regarding online
    services and entertainment that were not met by the airline, something that may
    impact the airline’s ability to captivate younger customers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see, it’s very easy to set up a clustering solution in Python. Here
    are some suggestions of next steps that you can do:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualize the distribution of the categorical variables inside the clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check how these clusters relate to customer satisfaction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build targeted campaigns to improve customer satisfaction. For example, it seemed
    that the last cluster was disappointed about the entertainment and online services
    — why not build a personalized campaign for younger customers offering a better
    experience on these services?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That’s it! Thank you for taking the time to read this blog post.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some things we didn’t discuss were limitations of this algorithm. Let’s use
    this conclusion for that:'
  prefs: []
  type: TYPE_NORMAL
- en: One important thing to keep in mind when using *k-means* is that the algorithm
    is sensitive to the initial placement of the centroids. This is because the algorithm
    may converge to a local minimum, rather than the global minimum, if the initial
    centroids are poorly chosen. **Therefore, it is often a good idea to run the algorithm
    multiple times with different initial centroids and choose the solution that gives
    the lowest overall sum of squared distances.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another limitation of k-means is that it assumes that the clusters are roughly
    spherical and equally sized. This means that it may not work well on datasets
    where the clusters are irregularly shaped or have vastly different sizes. In such
    cases, other clustering algorithms may be more appropriate, such as **hierarchical
    clustering or density-based clustering.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite its limitations, k-means is a popular and effective clustering algorithm
    that has been widely used in many different applications. It is relatively easy
    to implement, explainable and can handle large datasets efficiently. With its
    simple and intuitive approach, it is a good starting point for exploring the structure
    of your data and identifying patterns that may not be immediately obvious.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you would like to drop by my Python courses, feel free to join* ***my free
    course*** *here (*[*Python For Busy People — Python Introduction in 2 Hours*](https://www.udemy.com/course/python-for-busy-people-python-introduction-2-hours/?referralCode=1588B6BF72D40253CDD4)*)*
    ***or a longer 16 hour version*** *(*[*The Complete Python Bootcamp for Beginners*](https://www.udemy.com/course/the-python-for-absolute-beginners-bootcamp/?referralCode=8D25992A055C19079B8A)*).
    My Python courses are suitable for beginners/mid-level developers and I would
    love to have you on my class!*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10080764725e472e5b103d3cf5518065.png)'
  prefs: []
  type: TYPE_IMG
- en: Python for Absolute Beginners Course — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '*The dataset used on this blog post is under the* [*CC0: Public Domain*](https://creativecommons.org/publicdomain/zero/1.0/)
    *license.*'
  prefs: []
  type: TYPE_NORMAL
