- en: Okay, You‚Äôve Trained the Best Machine Learning Model. What‚Äôs Next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/okay-youve-trained-the-best-machine-learning-model-what-s-next-e7b8f167006e](https://towardsdatascience.com/okay-youve-trained-the-best-machine-learning-model-what-s-next-e7b8f167006e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data Science
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An MLOps project beyond modeling in Jupyter Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dwiuzila.medium.com/?source=post_page-----e7b8f167006e--------------------------------)[![Albers
    Uzila](../Images/64f95bb182d878579e23ed4aaec1aafb.png)](https://dwiuzila.medium.com/?source=post_page-----e7b8f167006e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e7b8f167006e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e7b8f167006e--------------------------------)
    [Albers Uzila](https://dwiuzila.medium.com/?source=post_page-----e7b8f167006e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e7b8f167006e--------------------------------)
    ¬∑18 min read¬∑Jun 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4f723cf7bf28815d9d27a97fd601e83.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Elena Mozhvilo](https://unsplash.com/@miracleday?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs say you‚Äôre building a data science project, maybe for work, college, portfolio,
    hobby, or whatever it is. You‚Äôve spent your days solving a problem statement and
    experimenting with Jupyter notebooks. Now, you‚Äôre wondering, ‚ÄúHow do I deploy
    my work as a useful product?‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: To be concrete, assume you have a website that hosts forums. Users can add tags
    to a thread in a forum to ease navigating between forums with different topics.
    You want to better the user experience by suggesting predefined tags hence giving
    context to what the discussion is about.
  prefs: []
  type: TYPE_NORMAL
- en: The forum can be anything, so let‚Äôs be more specific; it always starts with
    a *post* explaining a math problem, followed by thoughts, questions, hints, or
    answers around it. Below is what a thread looks like and its 3 tags, i.e. **induction**,
    **combinatorics unsolved**, and **combinatorics**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5dd6880d744be717451c45c98d3fbcbe.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of a thread in a forum | Image by [author](http://dwiuzila.medium.com/membership)
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you‚Äôve done everything in your notebooks, from understanding
    the problem statement, defining metrics, querying data, cleaning it, preprocessing,
    EDA, building a model, to evaluating and optimizing the model.
  prefs: []
  type: TYPE_NORMAL
- en: You notice there are [so many posts](https://medium.com/towards-data-science/multilabel-text-classification-done-right-using-scikit-learn-and-stacked-generalization-f5df2defc3b5)
    with a huge total number of tags. So to simplify, you filter only 10 tags. The
    models you develop are simple [linear classifiers](/understanding-3-classical-machine-learning-models-once-and-for-all-part-1-32a1ac52c0fd)
    (SVM, logistic regression, etc.) preceded by [TF-IDF vectorization](/all-you-need-to-know-about-bag-of-words-and-word2vec-text-feature-extraction-e386d9ed84aa),
    and you train them with [Stochastic Gradient Descent](https://medium.com/towards-data-science/complete-step-by-step-gradient-descent-algorithm-from-scratch-acba013e8420)
    (SGD).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df8459514d52d33342616cba2be61e1b.png)'
  prefs: []
  type: TYPE_IMG
- en: First 30 frequent tag count | Image by [author](http://dwiuzila.medium.com/membership)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fdf51bf531d6380432968037ab2c7f51.png)'
  prefs: []
  type: TYPE_IMG
- en: Final tag distribution. Notice that geometry is the most common one | Image
    by [author](http://dwiuzila.medium.com/membership)
  prefs: []
  type: TYPE_NORMAL
- en: While notebooks are great and can help you conduct experiments very fast, they‚Äôre
    not production-ready and are sometimes hard to maintain. So, you need to migrate
    your codes into individual Python files, and then step-by-step add other utilities
    while collaborating with your team members.
  prefs: []
  type: TYPE_NORMAL
- en: 'This story will guide you to do exactly that with bite-sized simple steps.
    Before that, you might want to refresh your mind about linear models, TF-IDF,
    and SGD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/understanding-3-classical-machine-learning-models-once-and-for-all-part-1-32a1ac52c0fd?source=post_page-----e7b8f167006e--------------------------------)
    [## Linear Regression, Logistic Regression, and SVM in 10 Minutes'
  prefs: []
  type: TYPE_NORMAL
- en: How does linear regression relate to logistic regression and Support Vector
    Machine?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/understanding-3-classical-machine-learning-models-once-and-for-all-part-1-32a1ac52c0fd?source=post_page-----e7b8f167006e--------------------------------)
    [](/all-you-need-to-know-about-bag-of-words-and-word2vec-text-feature-extraction-e386d9ed84aa?source=post_page-----e7b8f167006e--------------------------------)
    [## All You Need to Know About Bag of Words and Word2Vec ‚Äî Text Feature Extraction
  prefs: []
  type: TYPE_NORMAL
- en: Why Word2Vec is better, and why it‚Äôs not good enough
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/all-you-need-to-know-about-bag-of-words-and-word2vec-text-feature-extraction-e386d9ed84aa?source=post_page-----e7b8f167006e--------------------------------)
    [](/complete-step-by-step-gradient-descent-algorithm-from-scratch-acba013e8420?source=post_page-----e7b8f167006e--------------------------------)
    [## Complete Step-by-Step Gradient Descent Algorithm from Scratch
  prefs: []
  type: TYPE_NORMAL
- en: And its implementation for constant learning rate and line search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/complete-step-by-step-gradient-descent-algorithm-from-scratch-acba013e8420?source=post_page-----e7b8f167006e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a Repository
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let‚Äôs create a new repo on [GitHub](https://github.com/new) called [tagolym-ml](https://github.com/dwiuzila/tagolym-ml/tree/main),
    complete with `README.md`, `.gitignore`, and `LICENSE`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca7476cb2277feb398c1c8acea14c958.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a new GitHub repository | Image by [author](http://dwiuzila.medium.com/membership)
  prefs: []
  type: TYPE_NORMAL
- en: 'To work with the repo, do these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Clone the repo and a folder named `tagolym-ml` will be created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the working directory into this folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a virtual environment called `venv`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Activate the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upgrade `pip`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optionally, you can check the packages currently installed in your environment
    using `pip list`, there will be `pip` and `setuptools`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new git branch called `code_migration` and switch to it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a file `setup.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make some new folders named `config`, `tagolym`, and `credentials`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create files `config.py` and `args.json` inside the `config` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create files `main.py`, `utils.py`, `data.py`, `train.py`, `evaluate.py`, and
    `predict.py` inside the `tagolym` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you don‚Äôt understand how to do those, don‚Äôt worry. Here are all the commands
    you need that you can run on your favorite terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You now have a git repo in your local connected to the remote repo in GitHub.
    The local repo directories will look like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Almost all of these files are empty for now. You‚Äôll fill them in one by one,
    starting with the folder `config`.
  prefs: []
  type: TYPE_NORMAL
- en: Migrate Your Codebase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two main folders within your project, i.e. `config` and `tagolym`.
    You want to copy the necessary codes from your notebooks into files inside these
    folders. Let‚Äôs do it.
  prefs: []
  type: TYPE_NORMAL
- en: config/config.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, you define global variables related to seeds, directories, experiment
    tracking, preprocessing, and label names.
  prefs: []
  type: TYPE_NORMAL
- en: 'When this file is imported somewhere in your code, it will make two new folders
    if not yet created:'
  prefs: []
  type: TYPE_NORMAL
- en: '`data`, in which you store labeled data for the project,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`stores/model`, in which you store the model registry,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: and then connect `stores/model` to a tracking URI for experiment tracking with
    MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: You also define stopwords and additional command words here. The stopwords will
    be the default from `nltk` package, while command words are `["prove", "let",
    "find", "show", "given"]`, which often come up in the post but don‚Äôt give any
    useful signals.
  prefs: []
  type: TYPE_NORMAL
- en: Regex patterns are used for preprocessing. They look intimidating, but you don‚Äôt
    need to understand them. What they basically do is catch any [mathematical expressions](https://www.overleaf.com/learn/latex/Mathematical_expressions)
    and [asymptote syntax](https://asymptote.sourceforge.io/) in LaTeX, which are
    the bread and butter in a math problem post.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, remember you selected only 10 shortlisted labels to work with? You list
    all of them in this file. Some tags have a similar meaning to your labels (e.g.
    ‚Äúinequalities‚Äù ‚Üí ‚Äúinequality‚Äù), so you also have 10 partial labels to catch these
    tags and replace them with appropriate labels. See `tagolym/data.py` below for
    how you do this.
  prefs: []
  type: TYPE_NORMAL
- en: config/args.json
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is where you store all *initial* arguments for the entire process. They
    are coming from different parts of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: What do they mean?
  prefs: []
  type: TYPE_NORMAL
- en: '`nocommand` and `stem` ‚Äî booleans for preprocessing the posts, whether to exclude
    command words and to implement [word stemming](https://en.wikipedia.org/wiki/Stemming).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ngram_max_range` ‚Äî the upper boundary of the range of *n* values for different
    *n*-grams to be extracted during [TF-IDF vectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`loss`, `l1_ratio`, `alpha`, `learning_rate`, `eta0`, `power_t` ‚Äî hyperparameters
    for models using the [SGD classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: tagolym/utils.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The pipeline is a bit convoluted, so you need some utility functions and Python
    classes to streamline your codes. This file contains those:'
  prefs: []
  type: TYPE_NORMAL
- en: '`load_dict` and `save_dict` ‚Äî to load a dictionary from a JSON file and, the
    other way around, dump a dictionary into a JSON file.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`NumpyEncoder` ‚Äî to encode objects with Numpy instances into Python built-in
    instances, used in `save_dict`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`IterativeStratification` ‚Äî when you‚Äôre dealing with multilabel classification
    like this project, the vanilla [train-test-split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)
    method is not ideal for the data. In return, you need what we call [iterative
    stratification](http://scikit.ml/stratification.html), which aims to provide well-balanced
    distribution of evidence of label relations up to a given order. In this project,
    the order is set to 2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: tagolym/data.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All functions regarding data are written in this file, including data split,
    preprocessing, and transformation.
  prefs: []
  type: TYPE_NORMAL
- en: '`preprocess` ‚Äî creates a mapping from tags containing partial labels to one
    of the 10 labels defined in `config/config.py`, then does text processing on all
    posts and labels. This function also drops all samples with an empty post after
    text processing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`binarize` ‚Äî based on model requirements, you may want to binarize your labels
    if you‚Äôre working on a multilabel classification problem. This function converts
    labels into a binary matrix of size (#samples √ó #labels) indicating the presence
    of a tag in the labels. For example, the label containing two tags `["algebra",
    "inequality"]` will be transformed into `[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]`. Besides
    returning the transformed labels, it also returns the `[MultiLabelBinarizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html)`
    object used later, especially for converting the matrix back to labels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`split_data` ‚Äî using `IterativeStratification` from `tagolym/utils.py`, this
    function splits the posts and labels into 3 parts with 70/15/15 proportions, each
    respectively for model training, validation, and testing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: tagolym/train.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It‚Äôs a best practice to have separate files for model training, validation,
    and testing. As the file name suggests, you do all the training here. Since you
    want users to use the model‚Äôs tag recommendations confidently, you want to have
    low false positives.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, false negatives are not your top priority for now. To see
    why, let‚Äôs take an extreme example: the model predicts all 10 labels as negatives,
    hence no tags are recommended and you have a high number of false negatives. But
    then, the users can just create their own tags without hesitation. No big deal.'
  prefs: []
  type: TYPE_NORMAL
- en: So, your objective will be to have a model with high precision.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs discuss what this file has to offer:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train` ‚Äî preprocess the data, binarize the labels, and split the data using
    functions from `tagolym/data.py`. Then, initialize a model, train it, predict
    the labels on all three splits using the trained model, and evaluate the predictions.
    This function accepts `args` which contains all arguments in `config/args.json`,
    to which an additional argument `threshold` may be added before being returned.
    Basically, `threshold` is a list of the best threshold for every label calculated
    by `tune_threshold`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`objective` ‚Äî f1 score is a metric chosen to be optimized in [hyperparameter
    tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization). Using an `args`
    chosen in a trial, this function trains the model and returns the f1 score of
    the validation split. It also sets additional attributes to the trial, including
    precision, recall, and the f1 score of all three splits.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`tune_threshold` ‚Äî the default decision boundary for a binary classification
    problem is 0.5, which may not be optimal depending on the problem. So, besides
    tuning `args`, you also tune the threshold for every label while optimizing the
    f1 score. What it does is try all possible values of the threshold from a grid
    of 0 to 1 and pick the one that has the maximum f1 score.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: tagolym/predict.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What next after model training? Predict! There are two functions in this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`custom_predict` ‚Äî if the model has `predict_proba` attribute, this function
    will predict the probability of each label being a tag. Otherwise, it predicts
    the tag directly using 0.5 threshold. In the former case, if the true labels are
    given, the function will use them to tune the threshold using `tune_threshold`
    from `tagolym/train.py`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`predict` ‚Äî load `args`, the label binarizer, and the trained model. Then,
    preprocess given texts and do predictions on them using `custom_predict`. After
    that, transform the prediction matrix back into tags.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: tagolym/evaluate.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given prediction and true label matrices, the purpose of this file is to calculate
    the precision, recall, f1 score, and number of samples. The performance is computed
    on the overall samples, per-class samples, and per-slice samples. There are 8
    slices you consider:'
  prefs: []
  type: TYPE_NORMAL
- en: short posts, i.e. those that have less than 5 words after preprocessed,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: six slices in which the posts are tagged as a subtopic but not tagged as the
    bigger topic covering the subtopic, and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: posts that don‚Äôt have frequent four-letter-or-more words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: tagolym/main.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the main file that runs everything end-to-end. Here are 5 functions
    you have in this file and what instructions you should write in them:'
  prefs: []
  type: TYPE_NORMAL
- en: '`elt_data` ‚Äî query labeled data and save it to `data` folder in JSON format.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`train_model` ‚Äî load labeled data from `data` folder and train your model.
    Don‚Äôt forget to log the metrics, artifacts, and parameters using MLflow. Save
    also MLflow `run_id` and metrics to the `config` folder.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`optimize` ‚Äî load labeled data from `data` folder and optimize given arguments.
    For search efficiency, the optimization is done in two steps: a) hyperparameters
    in preprocessing, vectorization, and modeling; and b) hyperparameters in the learning
    algorithm. Save also the best arguments based on the objective to the `config`
    folder, name it as `args_opt.json`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`load_artifacts` ‚Äî load the artifacts of a specific `run_id` into memory, including
    arguments, metrics, model, and label binarizer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`predict_tag` ‚Äî given a specific `run_id`, predict the tags of every text it
    receives using preloaded artifacts.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Phew! You just did all migration needed. Now, how do you use these codes?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cee12848735f6ab48433c40563e4fd78.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jason Strull](https://unsplash.com/@jasonstrull?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Package Your Codebase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you used notebooks, you had a preloaded set of packages for experimentation.
    To reproduce it locally and deploy it to production, you want to define your environment
    explicitly instead.
  prefs: []
  type: TYPE_NORMAL
- en: You import many open-source packages in this project but only have `pip` and
    `setuptools` in your environment. So, before running the pipeline, you need to
    install those packages too.
  prefs: []
  type: TYPE_NORMAL
- en: Below is a handy command to do that. Notice I added `[pip-chill](https://pypi.org/project/pip-chill/)`
    at the end to clean up the generation of the package requirements file later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: What‚Äôs cool about `pip-chill` is that it lets you generate a requirements file
    without any packages that depend on other packages in the file, making the requirements
    file clean and accurate. Let‚Äôs just run it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This will create a file `requirements.txt` containing all packages you actually
    need. Note that there are no `pandas`, `scikit-learn`, `regex`, and several other
    packages since these are dependencies of packages already listed in the file.
  prefs: []
  type: TYPE_NORMAL
- en: Now you‚Äôll use `setup.py` to package your codebase, wrapping up all dependencies.
    Inside this file, load all libraries you have in `requirements.txt`, and define
    your package using the `setup` function from `setuptools`.
  prefs: []
  type: TYPE_NORMAL
- en: Your package name will be `tagolym`. You can see other details like its version
    and description in the code below. Libraries you load from `requirements.txt`
    will be used in the `install_requires` parameter and become dependencies of `tagolym`.
  prefs: []
  type: TYPE_NORMAL
- en: You can then install `tagolym` using this command below. A new folder named
    `tagolym.egg-info` will be created which contains the project‚Äôs metadata.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Notice the `-e` or `--editable` flag installs a package in editable mode from
    a local project path. In other words, if you use some functions in the current
    working directory, e.g. using `from tagolym import main`, then you make some changes
    to `tagolym/main.py`, you will able to use this updated version without re-installing
    your package using `pip install`.
  prefs: []
  type: TYPE_NORMAL
- en: Setup Data Source Credential
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There‚Äôs one small problem. The data used in this project is my own data which
    is available in my [BigQuery](https://cloud.google.com/bigquery). After creating
    and downloading a [service account key](https://console.cloud.google.com/iam-admin/serviceaccounts),
    I rename it to `bigquery-key.json`, and place it in the `credentials` folder.
  prefs: []
  type: TYPE_NORMAL
- en: To access the data, you‚Äôd need my credential, which unfortunately is not to
    be shared. But worry not, I‚Äôll provide samples for you to work with.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5559db01c3de6704a8347306c22df26f.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a service account key | Image by [author](http://dwiuzila.medium.com/membership)
  prefs: []
  type: TYPE_NORMAL
- en: 'What you need to do is simple: download the samples `labeled_data.json` [**here**](https://gist.github.com/dwiuzila/74dc99fe6f6d3901dbd1695f77977865)
    and save the file in a folder named `data` in the working directory.'
  prefs: []
  type: TYPE_NORMAL
- en: Run Your Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now you‚Äôre ready! Type `python3` command in the terminal and you‚Äôre good to
    run everything in Python. You‚Äôll only use `tagolym/main.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: First, I query the data using my credential and the `elt_data` function. When
    I see `‚úÖ Saved data!`, I know that the process ran smoothly. As mentioned above,
    you can skip this step and manually put the samples I provided in the `data` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you optimize the model using the `optimize` function by reading the initial
    arguments `config/args.json`. I set the number of trials to 10, but you can try
    something else. A new MLflow study will be created with 20 trials in total since
    you have a two-step optimization process. The best validation f1 score found is
    0.7730.
  prefs: []
  type: TYPE_NORMAL
- en: With a set of optimized arguments `config/args_opt.json`, you train the model
    once again using the `train_model` function and do inference on a list of texts
    using the `predict_tag` function. You can see below that the predictions are spot
    on!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can explore your experiments in a beautiful MLflow UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/32002dbbcf952c1651380ccb18fe5eb8.png)'
  prefs: []
  type: TYPE_IMG
- en: MLflow user interface | Image by [author](http://dwiuzila.medium.com/membership)
  prefs: []
  type: TYPE_NORMAL
- en: Some of these processes create new files in the background, most are the outputs
    of model training. You can see the current project directory in the `README.md`
    file explained in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Miscellaneous
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The security of your project is of the utmost importance. So, credentials should
    only live in your local repo; you don‚Äôt want to push it to GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: As a precaution, add `credentials/` at the end of the `.gitignore` file. This
    will ignore any changes you make in the `credentials` folder when you develop
    your project.
  prefs: []
  type: TYPE_NORMAL
- en: Other things you might want to add to `.gitignore` is `data/` and `stores/`
    since they may contain sensitive information or occupy large disk space. If you‚Äôre
    using macOS, add also `.DS_Store`. It‚Äôs a file that stores custom attributes of
    its containing folder, which isn‚Äôt useful for your project.
  prefs: []
  type: TYPE_NORMAL
- en: After doing all these, optionally, you could update your project description
    in `README.md`. Just type in the high-level process you‚Äôve done in this story
    so everyone could replicate your work easily. Here‚Äôs what it might look like.
  prefs: []
  type: TYPE_NORMAL
- en: Push Your Project to GitHub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your project is cool and all, but is it useful for others? To answer this, you
    can open-source your project so everyone can benefit from it, give feedback, or
    even contribute. And it‚Äôs very simple to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'What you need is the three commands below:'
  prefs: []
  type: TYPE_NORMAL
- en: add every change you made to the Git index,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: commit changes in the index to the local repo, and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: push the local repo to remote, this will create a new branch `code_migration`
    in the remote repo.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You can see the results [**here**](https://github.com/dwiuzila/tagolym-ml/tree/code_migration).
  prefs: []
  type: TYPE_NORMAL
- en: 'Know more about Git:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/a-real-world-case-study-of-using-git-commands-as-a-data-scientist-e7775cccb4ba?source=post_page-----e7b8f167006e--------------------------------)
    [## A Real-World Case Study of Using Git Commands as a Data Scientist'
  prefs: []
  type: TYPE_NORMAL
- en: Complete with Branch Illustration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-real-world-case-study-of-using-git-commands-as-a-data-scientist-e7775cccb4ba?source=post_page-----e7b8f167006e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! üçª You‚Äôve come to the end of this story. You learned how to
    translate your data science experimentation in Jupyter Notebook to a clean and
    maintainable project. Besides that, now you also know how to package your project,
    run your pipeline end-to-end, and work with GitHub and BiqQuery.
  prefs: []
  type: TYPE_NORMAL
- en: But, this is just the beginning of your MLOps journey. There‚Äôs still a long
    way to go. Stay tuned! üìª
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcb1906b329c918a89549438fd041b55.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Matese Fields](https://unsplash.com/@tesecreates?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39d3928bb1b8338b76276e63c0b8b3f8.png)'
  prefs: []
  type: TYPE_IMG
- en: üî• *Hi there! If you enjoy this story and want to support me as a writer, consider*
    [***becoming a member***](https://dwiuzila.medium.com/membership)*. For only $5
    a month, you‚Äôll get unlimited access to all stories on Medium. If you sign up
    using my link, I‚Äôll earn a small commission.*
  prefs: []
  type: TYPE_NORMAL
- en: üîñ *Want to know more about how classical machine learning models work and how
    they optimize their parameters? Or an example of MLOps megaprojects? What about
    cherry-picked top-notch articles of all time? Continue reading:*
  prefs: []
  type: TYPE_NORMAL
- en: '![Albers Uzila](../Images/b4f51438d99b29f789091dd239d7cfa6.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Albers Uzila](https://dwiuzila.medium.com/?source=post_page-----e7b8f167006e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: MLOps Megaproject - Part II
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://dwiuzila.medium.com/list/mlops-megaproject-part-ii-0c6f23f2ddfa?source=post_page-----e7b8f167006e--------------------------------)3
    stories![](../Images/140d7ffae64707753b23e1c5e18dce62.png)![](../Images/1524ee6088d1c77d5bf77d99f6804cc2.png)![](../Images/43560f87b46b65c4bbce9154e5c2b731.png)![Albers
    Uzila](../Images/b4f51438d99b29f789091dd239d7cfa6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Albers Uzila](https://dwiuzila.medium.com/?source=post_page-----e7b8f167006e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning from Scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://dwiuzila.medium.com/list/machine-learning-from-scratch-b35db8650093?source=post_page-----e7b8f167006e--------------------------------)8
    stories![](../Images/4b97f3062e4883b24589972b2dc45d7e.png)![](../Images/ec855bb602dbe7a37aa8ef73fb3df3b7.png)![](../Images/72471337c5c7555442f0cc06985de74d.png)![Albers
    Uzila](../Images/b4f51438d99b29f789091dd239d7cfa6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Albers Uzila](https://dwiuzila.medium.com/?source=post_page-----e7b8f167006e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Optimization Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://dwiuzila.medium.com/list/advanced-optimization-methods-26e264a361e4?source=post_page-----e7b8f167006e--------------------------------)7
    stories![](../Images/99e5c4ac661c0a5c6f0f386ae7986984.png)![](../Images/f84958116968e333daead307a6b3684c.png)![](../Images/c491e53a229df0bc740a49349af2665c.png)![Albers
    Uzila](../Images/b4f51438d99b29f789091dd239d7cfa6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Albers Uzila](https://dwiuzila.medium.com/?source=post_page-----e7b8f167006e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: My Best Stories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://dwiuzila.medium.com/list/my-best-stories-d8243ae80aa0?source=post_page-----e7b8f167006e--------------------------------)10
    stories![](../Images/bff788cd6edf0f1d2e69a785ca11d1dc.png)![](../Images/0d3371edcb1483d694597eb934c27411.png)![](../Images/43560f87b46b65c4bbce9154e5c2b731.png)![Albers
    Uzila](../Images/b4f51438d99b29f789091dd239d7cfa6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Albers Uzila](https://dwiuzila.medium.com/?source=post_page-----e7b8f167006e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Data Science in R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://dwiuzila.medium.com/list/data-science-in-r-0a8179814b50?source=post_page-----e7b8f167006e--------------------------------)7
    stories![](../Images/bad098890d89ce5a89e723add9c14d98.png)![](../Images/5bf2373efc1c05f1939dc9a72c99d786.png)![](../Images/c7d3478ce4cf90499b3e953ba69cd90e.png)'
  prefs: []
  type: TYPE_NORMAL
