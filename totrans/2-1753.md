# 量化及其他：将LLMs的推理时间减少80%

> 原文：[https://towardsdatascience.com/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb](https://towardsdatascience.com/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)

[](https://medium.com/@christopher_karg?source=post_page-----671db9349bdb--------------------------------)[![Christopher Karg](../Images/9d163d59e0c3167732f55d497caf9db2.png)](https://medium.com/@christopher_karg?source=post_page-----671db9349bdb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----671db9349bdb--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----671db9349bdb--------------------------------) [Christopher Karg](https://medium.com/@christopher_karg?source=post_page-----671db9349bdb--------------------------------)

·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----671db9349bdb--------------------------------) ·12分钟阅读·2023年10月27日

--

![](../Images/c44c0f469d07bf09ab4e359dd48265fe.png)

来源：[https://www.pexels.com/photo/cropland-in-autumn-18684338/](https://www.pexels.com/photo/cropland-in-autumn-18684338/)

量化是一种用于多种算法的技术，但随着最近大型语言模型（LLMs）的涌现，这一技术变得越来越普及。在本文中，我旨在提供有关LLMs量化的信息，以及这一技术对在本地运行这些模型的影响。我将探讨量化之外的另一种策略，这可以进一步减少运行这些模型的计算需求。我将解释这些技术为何可能对你有兴趣，并展示一些带有代码示例的基准测试，以说明这些技术的有效性。我还简要介绍了硬件需求/建议以及实现LLM目标的现代工具。在后续文章中，我计划提供逐步说明和代码，用于微调你自己的LLM，请留意。

TL;DR — 通过量化我们的LLM并更改张量*dtype*，我们能够在具有2倍参数的LLM上进行推理，同时将*Wall time*减少80%。

一如既往，如果你希望讨论我在这里提到的任何内容，请[联系我](http://www.linkedin.com/in/-christopherkarg)。

本文中的所有观点均为我个人意见。本文未获得赞助。

# 量化（LLMs的量化）是什么？

量化使我们能够通过将网络的权重和偏差从原始的浮点格式（例如32位）转换为更低精度的格式（例如8位），来减少神经网络的大小。原始的浮点格式可以根据模型的架构和训练过程有所不同。量化的最终目的是减少模型的大小，从而减少内存和计算需求，以运行推理和训练模型。如果你尝试自己量化模型，量化过程可能会变得非常繁琐。这主要是因为某些供应商的硬件不支持。幸运的是，可以通过使用特定的第三方服务和软件来绕过这些限制。

就我个人而言，我在Mac上量化Meta的Llama-2等LLMs时遇到了不少麻烦。这主要是由于对标准库（或任何带有自定义CUDA内核的内容）的支持不足。不过，[optimum](https://github.com/huggingface/optimum) 和 [onnx](https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html) 等第三方工具确实存在，可以使我们的生活稍微轻松一些。

快速且简单的选项是下载[HuggingFace](https://huggingface.co/)（HF）上提供的任何预量化模型。我特别想提到[TheBloke](https://huggingface.co/TheBloke)，他们提供了许多流行LLMs的量化版本，包括本文中将演示的LLama-2模型。有关如何对每个模型进行推理的说明可以在各自的模型卡上找到。

如果你想自己运行量化模型但没有自己的GPU，我建议在以下网站租用NVIDIA硬件：

· [**Runpod.io**](https://www.runpod.io/)

· [**Lambdalabs.com**](https://lambdalabs.com/)

· [**Vast.ai**](https://vast.ai/)— 免责声明 — 使用时请自行判断。这里实际上是租用随机人的GPU。建议在使用此服务时不要共享任何敏感信息。不过，它确实非常便宜。

如果你想购买NVIDIA硬件并希望获得最佳性价比，我建议购买2个二手RTX3090。虽然更新的RTX4090具有[更好的基准性能](https://technical.city/en/video/GeForce-RTX-3090-vs-GeForce-RTX-4090)，但大型语言模型（LLMs）更需要高内存读写速度，而不是更高的处理器速度。3090和4090型号在内存读写速度上差异不大，因此我认为较旧的型号提供了更好的价值。

如果你有预算，选择几乎没有限制。

作为免费的选项，我建议：

· [**Google colab**](https://colab.google/)— 在运行时提供免费的GPU，但有一定的限制（免费层的RAM也有限制，不过你可以付费获得更多）。

· [**Kaggle**](https://www.kaggle.com/) 也在他们的笔记本中提供GPU。

如果你坚持使用Mac硬件，我建议使用M2 Ultra，并配备尽可能多的RAM（理想情况下为64GB以上）。这仍会比上述NVIDIA选项慢，但如果你只是希望在LLM上运行推理而不是训练自己的模型，这绝对是可行的。如果你在Mac硬件上量化自己的模型时遇到问题，我只能推荐[Georgi Greganov的llama.cpp](https://github.com/ggerganov/llama.cpp)。使用这个repo，你可以下载并编译Meta的llama 2模型为C++并将其量化为4位精度。然后我们可以在这些模型上运行推理。该repo的README提供了清晰的操作说明。

# 那么，为什么我们要在本地运行/托管自己的LLM呢？

简短的答案是，一如既往，这取决于情况。撰写本文时，OpenAI的GPT4（通过ChatGPT提供）被广泛[认为是表现最好的LLM](https://paperswithcode.com/paper/gpt-4-technical-report-1)。我认为定价也非常合理，而且模型本身无疑比我上述提到的策略更容易互动。你唯一需要安装的依赖是你的账户信息和信用卡号码；）。

我确实认为运行本地LLM有很强的理由：

**询问关于专有文档/数据的问题。**

你可以使用自己的上下文和数据对LLM进行微调。通过自己进行这些操作，你不会将任何信息分享给第三方，这是一大优势。

**询问关于2021年9月知识截止后的主题** **（GPT4）**。

我看到了一些GPT4在此时间段之后提供详细信息的情况，但模型经常指出知识截止存在。

**对模型进行微调以解决特定于你场景的问题。**

再次回到第一点，你可以调整自己的LLM模型以适应你的需求。

**你可以看到这些LLM的底层工作原理。你可以检查模型架构，并进一步发展你对技术的理解。**

**这很免费（前提是你已经有自己的硬件，并且不计算运行所需的电费）**

量化最终将帮助你在本地运行LLM，使用比在未量化模型上运行推理时更少的计算资源。

# 基准比较Llama-2

我现在将演示量化对 Meta 的 Llama-2 7B 和 13B 模型的影响。我在租用的 GPU 上进行了这些实验，如上所述，但也在 Google Colab 笔记本中进行了测试，以确认结果是可重复的。我们唯一需要做的编辑是运行 7B 参数模型的 8 位量化版本作为我们在 Colab 笔记本中的基准，否则在运行推断时会超出内存限制（在我看来，这已经充分证明了在运行 LLMs 时使用量化的理由！）。不过可以跟着代码示例操作 — 这些代码示例直接来自于我的免费 Colab 笔记本版本。

如果你使用的是 colab 笔记本 — 当安装诸如*accelerate*和*bitsandbytes*等依赖项时，请在笔记本中使用常规的 pip 安装。安装完成后，重启运行时。否则，软件包将无法被识别。另外，不要忘记将运行时更改为 GPU，选择运行时 > 更改运行时类型 > T4 GPU。

我应该补充说明，这一切的前提是你已经获得了 Meta 和 HF 的模型访问权限。为此，你必须首先通过此链接向 Meta 提交申请表：

[https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)

接受确认的时间可能从 2 分钟到 2 天不等。请注意，你用于 Meta 表单和 HF 账户的电子邮件地址必须匹配，以便通过 HF API 使用模型。

一旦确认已收到，你可以登录到 Hugging Face 并开始使用模型。

让我们开始吧。

# 在 8 位量化下对 Llama2–7B 基础模型进行推断。

首先处理我们的导入 — 在此阶段，如果你遇到任何错误消息，请根据需要运行 pip 安装 — 安装完成后不要忘记重启你的运行时（如上所述）。

[PRE0]

接下来我们从 Hugging Face 复制模型名称，以便 API 可以下载相关模型。我们还需要输入我们的 HF 访问令牌。可以通过在 HF 网站的右上角选择你的个人资料 > 设置 > 访问令牌 > 生成令牌或复制现有令牌来找到它。

[PRE1]

现在让我们下载模型：

[PRE2]

在这里我们使用*device_map*参数来选择我们希望使用的硬件。在这种情况下，它选择了第一个可用的 GPU。这里可以加载自定义的*device_maps*，但这超出了本文的范围。还要注意*load_in_8bit*参数。这是我们为了减少推断运行时的内存需求而进行的量化步骤。如果你希望使用 LLMs 构建更大的项目/产品，这个简单的技术可以用于在资源有限的设备（边缘设备或手机等）上部署模型。

接下来我们设置我们的分词器：

[PRE3]

输入你希望的提示。我们使用的基础模型经过了文本补全的训练。

现在让我们对标记化的提示进行推理。如果有任何语法对你来说是新的，请随意查看HF文档。本质上，我们正在解包*toks*对象的内容并将其传递给我们的GPU。输出限制为最多15个标记（如果愿意，可以编辑此参数）。*model.generate()*方法用于使用我们的Llama2模型生成输出。完成后，我们再次将输出传输到CPU内存中，以便查看我们的输出。

[PRE4]

让我们分解这些时间指标，以更好地理解我们所看到的内容。*CPU时间*分解为三个主要组成部分：

1\. *user* — 这表示在用户模式代码中花费的时间。换句话说，就是CPU执行Python代码所需的时间。在这种情况下，花费了7.47秒。这个指标通常也被称为用户时间。

2\. *sys* — 这表示在系统调用或内核模式代码中花费的CPU时间。它是CPU执行操作系统相关任务的时间。在我们的案例中是1.17秒。

3\. *total* — 是用户时间和系统时间的总和。

接下来是*墙时间*。这指的是运行我们代码块所需的‘现实世界’时间。

*CPU时间*和*墙时间*（7.76秒）之间的差异是由于运行模型推理时涉及的其他内存密集型操作。这些操作包括但不限于GPU内存传输、调度、I/O操作等。

让我们解码结果以查看模型的输出：

[PRE5]

太棒了。我们已经成功地在一个基础量化LLM上进行了推理。

我们可以进一步使用的一种技术是将不同的*数据类型*分配给我们在计算过程中使用的Llama2模型中的张量。在之前，我们通过使用*load_in_8bit=True*参数来量化模型的参数，而现在我们将使用*torch_dtype=torch.bfloat16*参数，以减少模型在推理过程中的内存使用。这第二种方法不被认为是量化技术，因为它只改变了模型张量使用的数据类型，而第一种方法涉及通过在加载期间将模型参数的精度降低到8位来进行量化。

这两种方法被认为是减少运行我们的LLM的计算需求的有效技术。让我们看看第二种技术的有效性如何。

让我们用新的参数更新我们的模型：

[PRE6]

此阶段colab可能会提示内存不足。只需通过选择Runtime > Restart runtime来重启运行时，并重新运行笔记本中的所有相关单元格。

现在我们用更新后的张量*数据类型*在模型上运行推理：

[PRE7]

哇。通过调整张量的*数据类型*，我们将总CPU时间减少了6.66秒。我们的墙时间减少了约71%。让我们解码输出，看看是否注意到任何*数据类型*的变化影响：

[PRE8]

我们可以使用各种指标和测试来评估和比较模型的输出。在这篇文章中，我将简单地采用人工评估。两个输出都合格、连贯且相关。考虑到在我们的第二个示例中墙上时间减少了71%，我认为我们目前的技术是成功的。

让我们看看我们能多快在预量化的Llama2–7B模型上运行推理。

# 在具有更新张量数据类型的预量化Llama2–7B上进行推理。

托[TheBloke](https://huggingface.co/TheBloke)的福，我们能够访问Meta的Llama-2模型的预量化版本。有关量化过程的详细信息可以在[模型卡](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ)中找到。

我们将使用相同的张量*数据类型*技术，这种技术使我们在墙上时间上取得了显著的减少。这次使用预量化模型。

让我们更新模型：

[PRE9]

名称末尾的Q表示模型已经完成了量化处理。

现在我们下载具有更新张量*数据类型*的模型：

[PRE10]

更新分词器：

[PRE11]

运行推理：

[PRE12]

我们进一步进行了改进。正如你所见，总CPU时间减少了约14%。墙上时间减少了约8%。

让我们检查输出：

[PRE13]

现在很明显，由于我们的令牌限制设置为15，最终的词被裁剪了。我确认我增加了令牌限制，最终的词被评估为hobby。在人工验证方面，我仍然认为这是合格的。

现在让我们结合我们学到的所有内容，并对更大的Llama-2–13B模型进行推理。该模型的参数数量几乎是我们之前测试的模型的2倍。我们将与我们训练的第一个模型（基础的Llama-2–7B，使用8位量化）进行基准测试，看看两者的比较情况。

# 在具有更新张量数据类型的预量化Llama2–13B上进行推理。

我们将使用相同的语法，但当然会更新模型名称。

[PRE14]

下载具有更新张量*数据类型*的模型：

[PRE15]

更新分词器：

[PRE16]

运行推理：

[PRE17]

让我们将其放入上下文中：

![](../Images/8f5fc6baf18fce88c907b6bd95e07efa.png)

推理时间 Meta-Llama-2–7B（8位量化）与预量化的LLama-2–13B（使用float16张量）

我们几乎将参数数量翻倍（从7B增加到13B）。我们将总CPU时间减少了81%，墙上时间减少了80%。我不会撒谎，我对这个结果非常满意。

让我们获取输出：

[PRE18]

我们不仅通过减少计算需求大幅缩短了推理时间，而且我认为13B模型的输出也比我们运行推理的第一个7B模型更为连贯。

我希望这篇文章能向你展示这些技术在大幅度减少这些大语言模型（LLMs）推理时间方面的有效性。在我们的第一个例子中，甚至在没有应用我们自己的量化方法之前，模型是无法加载到笔记本中的。通过使用这些技术，我们能够部署一个更大的大语言模型（参数数量），将推理时间减少大约80%，并且改善输出。如果这还不是一个积极的结果，我不知道什么是了！

我很高兴讨论和交流这里涉及的任何话题。

*所有图片属于作者，除非另有说明。*
