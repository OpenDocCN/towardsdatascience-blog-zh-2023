- en: 'Scientific Credibility in Machine Translation Research: Pitfalls and Promising
    Trends'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/scientific-credibility-in-machine-translation-research-pitfalls-and-promising-trends-990ddabe8fb9](https://towardsdatascience.com/scientific-credibility-in-machine-translation-research-pitfalls-and-promising-trends-990ddabe8fb9)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Are we at a turning point? My conclusions from the annotation of 1,000+ scientific
    papers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----990ddabe8fb9--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----990ddabe8fb9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----990ddabe8fb9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----990ddabe8fb9--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----990ddabe8fb9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----990ddabe8fb9--------------------------------)
    ·10 min read·May 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/007d926acf400738b07cfcee5bb980d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Pixabay](https://pixabay.com/illustrations/keywords-change-fish-individuality-2488210/)
    — Edited by author
  prefs: []
  type: TYPE_NORMAL
- en: '**How do scientists assess an improvement in machine translation quality?**'
  prefs: []
  type: TYPE_NORMAL
- en: Mostly by using automatic evaluation metrics. A machine translation system with
    a better metric score is considered better.
  prefs: []
  type: TYPE_NORMAL
- en: Since improving metric scores is so important to demonstrate progress in machine
    translation, it is critical to understand how these metrics scores are computed
    and compared.
  prefs: []
  type: TYPE_NORMAL
- en: In 2021, I, with my colleagues Atsushi Fujita and Raphael Rubino, [manually
    annotated and analyzed 761 machine translation evaluations published from 2010
    to 2020](https://aclanthology.org/2021.acl-long.566.pdf) (Marie et al., 2021).
  prefs: []
  type: TYPE_NORMAL
- en: The results of our analysis were published by the ACL 2021, one of the most
    selective venues for machine translation publications. We showed that machine
    translation research lacked scientific credibility. We highlighted several pitfalls
    and concerning trends in machine translation evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: I presented this work during the award session of the ACL 2021.
  prefs: []
  type: TYPE_NORMAL
- en: '**This was 2 years ago.**'
  prefs: []
  type: TYPE_NORMAL
- en: '*How has evaluation in machine translation research evolved since 2021 and
    this publication? Did it improve or get worse? What are the new trends?*'
  prefs: []
  type: TYPE_NORMAL
- en: I updated the analysis to include the machine translation research papers published
    in 2021 and 2022\. The annotation methodology remains the same as the one used
    for the first analysis published at ACL 2021.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset that I created is here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://docs.google.com/spreadsheets/d/1oJ1NUnUKA6sHg08UgZDjnR7oPaX9OlHXrtWFKiUgTO0/edit?usp=sharing&source=post_page-----990ddabe8fb9--------------------------------)
    [## A Meta-Evaluation Dataset of Machine Translation Research (May - 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: Annotations This dataset contains annotations of machine translation evaluations
    conducted in *ACL publications. If you…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: docs.google.com](https://docs.google.com/spreadsheets/d/1oJ1NUnUKA6sHg08UgZDjnR7oPaX9OlHXrtWFKiUgTO0/edit?usp=sharing&source=post_page-----990ddabe8fb9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, I present my main findings.
  prefs: []
  type: TYPE_NORMAL
- en: The annotation of 1,000+ papers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this study, I manually annotated a total of 1,023 papers (+254 papers compared
    to the original study published in 2021).
  prefs: []
  type: TYPE_NORMAL
- en: 'I selected the papers to annotate with the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Published between 2010 and 2022 (included) by one of the top-tier ACL conferences:
    AACL, ACL, EACL, EMLP, NAACL, and CoNLL. I limited the study to ACL publications
    since they are the primary choice of researchers in machine translation to publish
    their best work. The papers submitted to ACL conferences undergo a double-blind
    peer review process. ACL conferences have low acceptance rates (almost always
    below 30% for the past 13 years). In other words, the papers published by the
    ACL are expected to be with high scientific credibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have the words “translation” or “MT” in their title.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Report on the comparison between at least two machine translation systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, this selection leaves out some papers but I judged that it contains
    enough articles to observe trends in machine translation research.
  prefs: []
  type: TYPE_NORMAL
- en: This set of papers doesn't include all the papers published by all the other
    ML/AI conferences that are not focused on NLP. They are far less numerous than
    the machine translation papers published by the ACL conferences, especially before
    2015\. Yet, I believe ML/AI papers focusing on machine translation could have
    their place in this study. Some of these papers are also very cited. I hope I
    will find the time, maybe this year, to extend this study to ICML, NIPS/NeurIPS,
    ICLR, IJCAI, and AAAI conferences.
  prefs: []
  type: TYPE_NORMAL
- en: '*What aspect of machine translation evaluation did I annotate?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Six different aspects of the evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: the automatic metrics used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the use of a human evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the statistical significance testing of the results and comparisons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the use of a framework, SacreBLEU, that facilitates the reproducibility of a
    machine translation evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the comparison of metric scores copied from previous work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the comparison of machine translation systems not trained/validated/tested on
    the exact same datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these aspects are particularly easy to annotate, but they are also informative
    enough to identify pitfalls and trends in machine translation research.
  prefs: []
  type: TYPE_NORMAL
- en: My main observations are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '2021–2022: 100% BLEU'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I listed all the automatic metrics used in evaluations comparing machine translation
    systems. I also regrouped under a single metric identifier all the variants of
    the same metrics. For instance, chrF and chrF++ are both labeled chrF.
  prefs: []
  type: TYPE_NORMAL
- en: BLEU ([Papineni et al., 2002](https://aclanthology.org/P02-1040/)) is an extremely
    popular metric to evaluate machine translation despite its many flaws.
  prefs: []
  type: TYPE_NORMAL
- en: '*How many machine translation papers used BLEU?*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Almost of all them have since 2010.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eea1e5b159d8fd5969cdaff2ce1fa113.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: In 2020, 2021, and 2022, **100%** of the annotated papers used BLEU.
  prefs: []
  type: TYPE_NORMAL
- en: This number doesn’t mean that the papers don’t use other metrics. They do. But
    papers using other metrics in addition to BLEU are a minority.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76fd700482fe98d7c0b5ed731ee4c03f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In 2021 and 2022, respectively 29.9% and 39.1% of the papers used other metrics.
    This is extremely low considering that **countless other and better metrics than
    BLEU are available**.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/traditional-versus-neural-metrics-for-machine-translation-evaluation-2931bd22fe61?source=post_page-----990ddabe8fb9--------------------------------)
    [## Traditional Versus Neural Metrics for Machine Translation Evaluation'
  prefs: []
  type: TYPE_NORMAL
- en: 100+ new metrics since 2010
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/traditional-versus-neural-metrics-for-machine-translation-evaluation-2931bd22fe61?source=post_page-----990ddabe8fb9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: But, on a positive note, we can see that these percentages are much higher than
    in 2020 which was the last year annotated in the first version of this work. Machine
    translation researchers tend to use a more diverse set of metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking more closely, here are the metrics that were the most used in 2022,
    in addition to BLEU:'
  prefs: []
  type: TYPE_NORMAL
- en: 'chrF: 10.0%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'COMET: 10.0%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'METEOR: 5.5%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TER: 4.5%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: METEOR ([Banerjee and Lavie, 2005](https://aclanthology.org/W05-0909.pdf)) and
    TER ([Snover et al., 2006](https://aclanthology.org/2006.amta-papers.25.pdf))
    are old metrics published before 2010\. Neural metrics, which are state-of-the-art
    for machine translation evaluation, are still rarely used. I can’t explain why.
    This is something I don’t understand.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, **COMET (**[**Rei et al., 2020**](https://aclanthology.org/2020.emnlp-main.213.pdf)**),
    a neural metric, is more and more used. This is encouraging and I wouldn’t be
    surprised to see this percentage increasing in 2023**.
  prefs: []
  type: TYPE_NORMAL
- en: The use of chrF ([Popović, 2015](https://aclanthology.org/W15-3049.pdf)) remains
    stable.
  prefs: []
  type: TYPE_NORMAL
- en: The rarity of human evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Almost 100% of machine translation papers rely on automatic evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '*But how many also perform human evaluation?*'
  prefs: []
  type: TYPE_NORMAL
- en: Hiring humans to evaluate machine translation systems is extremely challenging
    and costly. Evaluators have to be bilingual and ideally native speakers of the
    language towards which we want to translate. Available evaluators are rare for
    most language pairs of the world.
  prefs: []
  type: TYPE_NORMAL
- en: This makes machine translation an unusual research area of natural language
    processing in which human evaluation is rare.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, **human evaluation remains ideal** and should be conducted when possible.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/906cd3368bdcb9dd5e22639191e53653.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In 2020, only 7.7% of the papers performed some kind of human evaluation to
    support the results of their automatic evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '**In 2021 and 2022, more than 10% of the papers performed human evaluation**.
    This is a positive trend. I also noticed during my annotations that human evaluations
    are becoming more rigorous, better documented, and conducted at a larger scale
    than before.'
  prefs: []
  type: TYPE_NORMAL
- en: Statistical significance testing is slowly coming back
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When two systems are compared, deciding which one is better is not as simple
    as comparing the metric scores.
  prefs: []
  type: TYPE_NORMAL
- en: A metric score is only an overview of the performance of a system on a specific
    evaluation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We should test for the statistical significance of a score to make sure that
    a system didn’t get a better score by chance.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested, I already discussed why we need statistical significance
    testing in machine translation here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/yes-we-need-statistical-significance-testing-927a8d21f9f0?source=post_page-----990ddabe8fb9--------------------------------)
    [## Yes, We Need Statistical Significance Testing'
  prefs: []
  type: TYPE_NORMAL
- en: A rule of thumb may yield correct results but can’t be scientifically credible
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/yes-we-need-statistical-significance-testing-927a8d21f9f0?source=post_page-----990ddabe8fb9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Do machine translation researchers perform statistical significance testing
    of their results?*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/068ea7d05608e4a55849ff405f9162e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: They were. But since 2015 there was a constant decrease in the adoption of statistical
    significance testing. It is slowly coming back. **In 2022, 43% of the papers tested
    the statistical significance of their results**.
  prefs: []
  type: TYPE_NORMAL
- en: This leaves out 57% of papers claiming improvements in translation quality without
    checking whether their metric scores are coincidental.
  prefs: []
  type: TYPE_NORMAL
- en: This is still a lot. Yet, I think we are going in the right direction here.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: The importance of statistical significance testing is not unanimously
    recognized by the machine translation community. Renowned machine translation
    researchers are clearly against its systematic adoption. More generally, the necessity
    in Science of statistical significance testing is a controversial topic. You can
    find an interesting discussion about it in the work of* [*Wasserstein et al. (2019)*](https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Almost no more copies of scores from previous work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most concerning trends I observed when publishing the first version
    of this work was the copy of scores from several previous works for comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: '*Why is this concerning?*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say paper A published a BLEU score of 30.0 and another paper B published
    a BLEU score of 35.0\. You would conclude that B has a better score, right?
  prefs: []
  type: TYPE_NORMAL
- en: Well, we can’t conclude this. BLEU is not just one metric. It comes with many
    options and parameters that change the way it is computed. Some options artificially
    increase the scores.
  prefs: []
  type: TYPE_NORMAL
- en: This is very well discussed in the work of [Post (2018)](https://aclanthology.org/W18-6319.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: We should never assume that two metrics scores published by two different papers
    are comparable. Very often, they're not.
  prefs: []
  type: TYPE_NORMAL
- en: This is concerning since it means that a paper may conclude that a proposed
    system is better simply because of the use of a BLEU variant that yields higher
    BLEU scores.
  prefs: []
  type: TYPE_NORMAL
- en: I demonstrated several times (e.g., [here](https://medium.com/@bnjmn_marie/science-left-behind-ca0a58231c20)
    and [here](https://medium.com/@bnjmn_marie/comparing-the-uncomparable-to-claim-the-state-of-the-art-a-concerning-trend-3d864522a0ba))
    that the comparison of copied scores can lead to wrong conclusions and make an
    entire evaluation false.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers in machine translation have also a tool at their disposal to make
    sure their BLEU scores are comparable. This tool is [SacreBLEU](https://github.com/mjpost/sacrebleu).
    SacreBLEU generates a signature that shows the parameters and evaluation data
    used for the computation of BLEU, among other information, for replicability.
    Two BLEU scores computed by SacreBLEU, with the same SacreBLEU signature, are
    comparable.
  prefs: []
  type: TYPE_NORMAL
- en: SacreBLEU is the only tool with this feature. It was released in 2018 and is
    used more and more in research papers. In 2022, 47.3% of the papers used SacreBLEU,
    a significant increase compared to 2021\. But still, 52.7% of the research papers
    report on BLEU scores that are not replicable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca3554e26967ae3a3bb816ff9cd7cf1b.png)'
  prefs: []
  type: TYPE_IMG
- en: image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'The good news is that **the comparisons of BLEU scores copied from previous
    work without using SacreBLEU reached a peak in 2019**, with 27.8% of the publications
    who adopted this practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d8b90ccb1a79854fe4c3b8c95e19828.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: We can see in the chart that **we are getting rid of this practice**. In 2022,
    only 6.4% of papers did it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conclusion: Toward more credible machine translation evaluations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s draw the overall picture of the years 2021–2022 of machine translation
    evaluation. I’ll start with the bad:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aea863b2791b36af8032fdb8ae8f0d89.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 43% of the papers still rely **exclusively** on non-replicable BLEU scores to
    conclude that their proposed method improves machine translation quality. They
    are not computed with SacreBLEU and are not supported by human evaluation or statistical
    significance testing.
  prefs: []
  type: TYPE_NORMAL
- en: Considering that [BLEU is a 20-year-old metric with many well-identified flaws](https://medium.com/@bnjmn_marie/12-critical-flaws-of-bleu-1d790ccbe1b1),
    there is here a very large margin for improvement in machine translation evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: When I review machine translation papers for conferences and journals, I now
    systematically request the authors to provide more evidence than just BLEU scores.
  prefs: []
  type: TYPE_NORMAL
- en: I should also highlight that this reliance on BLEU seems to be decreasing, and
    BLEU scores are more and more often supported by other metrics since 2020.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even better, the percentage of papers exhibiting all the different types of
    machine translation evaluation pitfalls is decreasing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef7f8abe6ca13b14e1bc02327de22901.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'These problematic papers are:'
  prefs: []
  type: TYPE_NORMAL
- en: Using only BLEU for automatic evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t use SacreBLEU so their scores are not replicable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t perform a human evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t test the statistical significance of their results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare score(s) copied from different paper(s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directly compare systems trained, validated, and evaluated on different preprocessed
    datasets to claim that their proposed method is better
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 18.4% of the publications were in this category in 2020, against 5.5% in 2022.
  prefs: []
  type: TYPE_NORMAL
- en: This is a huge improvement in my opinion. Will 2023 confirm this trend? Let’s
    hope so!
  prefs: []
  type: TYPE_NORMAL
- en: 'To conclude, here are my recommendations for evaluating machine translation
    with scientific credibility:'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t exclusively rely on BLEU. Ideally, use two different metrics, one that
    I would call “traditional”, such as chrF (preferably not chrF++ which is tokenization-dependant),
    and another one that should be neural, such as COMET or BLEURT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t copy and compare scores published by previous work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform human evaluation (if you have the budget and can find evaluators)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform statistical significance testing. If you don’t do it, we have no way
    to know how significant is the improvement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only compare translations generated by systems trained/validated/evaluated on
    the same datasets. If you change the datasets, we cannot conclude whether the
    improvement comes from the change in the data or from the proposed method. In
    other words, follow the scientific methodology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’ll continue to monitor machine translation evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: The next update will be in 2024!
  prefs: []
  type: TYPE_NORMAL
- en: '*If you like this article and would be interested to read the next ones, the
    best way to support my work is to become a Medium member using this link:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie/membership?source=post_page-----990ddabe8fb9--------------------------------)
    [## Join Medium with my referral link - Benjamin Marie'
  prefs: []
  type: TYPE_NORMAL
- en: Join Our AI Community and Get Access to Cutting-Edge Research This blog aims
    to demystify recent advances in AI for…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@bnjmn_marie/membership?source=post_page-----990ddabe8fb9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are already a member and want to support this work,* [*just follow
    me on Medium*](https://medium.com/@bnjmn_marie)*.*'
  prefs: []
  type: TYPE_NORMAL
