["```py\n# image_encoder - ResNet or Vision Transformer\n# text_encoder - CBOW or Text Transformer\n# I[n, h, w, c] - minibatch of aligned images\n# T[n, l] - minibatch of aligned texts\n# W_i[d_i, d_e] - learned proj of image to embed\n# W_t[d_t, d_e] - learned proj of text to embed\n# t - temperature parameter\n\n# 1) get a batch of aligned images and text\nI, T = get_mini_batch()\n\n# 2) extract feature representations of each modality\nI_f = image_encoder(I) #[n, d_i]\nT_f = text_encoder(T) #[n, d_t]\n\n# 3) joint multimodal embedding [n, d_e]\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1)\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n\n# 4) scaled pairwise cosine similarities [n, n]\nlogits = np.dot(I_e, T_e.T) * np.exp(t)\n\n# 5) symmetric loss function\nlabels = np.arange(n)\nloss_i = cross_entropy_loss(logits, labels, axis=0)\nloss_t = cross_entropy_loss(logits, labels, axis=1)\nloss = (loss_i + loss_t)/2\n```", "```py\n# 5) symmetric loss function\n# logits = nxn matrix of cosin similarity predictions\n# labels = nxn matrix of true values\nlabels = np.arange(n)\nloss_i = cross_entropy_loss(logits, labels, axis=0)\nloss_t = cross_entropy_loss(logits, labels, axis=1)\nloss = (loss_i + loss_t)/2\n```"]