["```py\n\"\"\"\nInitializes an Azure Cognitive Search index with our custom data, using vector search \nand semantic ranking.\n\nTo run this code, you must already have a \"Cognitive Search\" and an \"OpenAI\"\nresource created in Azure.\n\"\"\"\nimport os\n\nimport openai\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.search.documents import SearchClient\nfrom azure.search.documents.indexes import SearchIndexClient\nfrom azure.search.documents.indexes.models import (\n    HnswParameters,\n    HnswVectorSearchAlgorithmConfiguration,\n    PrioritizedFields,\n    SearchableField,\n    SearchField,\n    SearchFieldDataType,\n    SearchIndex,\n    SemanticConfiguration,\n    SemanticField,\n    SemanticSettings,\n    SimpleField,\n    VectorSearch,\n)\nfrom dotenv import load_dotenv\nfrom langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\nfrom langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n\n# Config for Azure Search.\nAZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\nAZURE_SEARCH_KEY = os.getenv(\"AZURE_SEARCH_KEY\")\nAZURE_SEARCH_INDEX_NAME = \"products-index-1\"\n\n# Config for Azure OpenAI.\nAZURE_OPENAI_API_TYPE = \"azure\"\nAZURE_OPENAI_API_BASE = os.getenv(\"AZURE_OPENAI_API_BASE\")\nAZURE_OPENAI_API_VERSION = \"2023-03-15-preview\"\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\nAZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n\nDATA_DIR = \"data/\"\n\ndef load_and_split_documents() -> list[dict]:\n    \"\"\"\n    Loads our documents from disc and split them into chunks.\n    Returns a list of dictionaries.\n    \"\"\"\n    # Load our data.\n    loader = DirectoryLoader(\n        DATA_DIR, loader_cls=UnstructuredMarkdownLoader, show_progress=True\n    )\n    docs = loader.load()\n\n    # Split our documents.\n    splitter = RecursiveCharacterTextSplitter.from_language(\n        language=Language.MARKDOWN, chunk_size=6000, chunk_overlap=100\n    )\n    split_docs = splitter.split_documents(docs)\n\n    # Convert our LangChain Documents to a list of dictionaries.\n    final_docs = []\n    for i, doc in enumerate(split_docs):\n        doc_dict = {\n            \"id\": str(i),\n            \"content\": doc.page_content,\n            \"sourcefile\": os.path.basename(doc.metadata[\"source\"]),\n        }\n        final_docs.append(doc_dict)\n\n    return final_docs\n\ndef get_index(name: str) -> SearchIndex:\n    \"\"\"\n    Returns an Azure Cognitive Search index with the given name.\n    \"\"\"\n    # The fields we want to index. The \"embedding\" field is a vector field that will\n    # be used for vector search.\n    fields = [\n        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n        SimpleField(name=\"sourcefile\", type=SearchFieldDataType.String),\n        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n        SearchField(\n            name=\"embedding\",\n            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n            # Size of the vector created by the text-embedding-ada-002 model.\n            vector_search_dimensions=1536,\n            vector_search_configuration=\"default\",\n        ),\n    ]\n\n    # The \"content\" field should be prioritized for semantic ranking.\n    semantic_settings = SemanticSettings(\n        configurations=[\n            SemanticConfiguration(\n                name=\"default\",\n                prioritized_fields=PrioritizedFields(\n                    title_field=None,\n                    prioritized_content_fields=[SemanticField(field_name=\"content\")],\n                ),\n            )\n        ]\n    )\n\n    # For vector search, we want to use the HNSW (Hierarchical Navigable Small World)\n    # algorithm (a type of approximate nearest neighbor search algorithm) with cosine\n    # distance.\n    vector_search = VectorSearch(\n        algorithm_configurations=[\n            HnswVectorSearchAlgorithmConfiguration(\n                name=\"default\",\n                kind=\"hnsw\",\n                parameters=HnswParameters(metric=\"cosine\"),\n            )\n        ]\n    )\n\n    # Create the search index.\n    index = SearchIndex(\n        name=name,\n        fields=fields,\n        semantic_settings=semantic_settings,\n        vector_search=vector_search,\n    )\n\n    return index\n\ndef initialize(search_index_client: SearchIndexClient):\n    \"\"\"\n    Initializes an Azure Cognitive Search index with our custom data, using vector\n    search.\n    \"\"\"\n    # Load our data.\n    docs = load_and_split_documents()\n    for doc in docs:\n        doc[\"embedding\"] = openai.Embedding.create(\n            engine=AZURE_OPENAI_EMBEDDING_DEPLOYMENT, input=doc[\"content\"]\n        )[\"data\"][0][\"embedding\"]\n\n    # Create an Azure Cognitive Search index.\n    index = get_index(AZURE_SEARCH_INDEX_NAME)\n    search_index_client.create_or_update_index(index)\n\n    # Upload our data to the index.\n    search_client = SearchClient(\n        endpoint=AZURE_SEARCH_ENDPOINT,\n        index_name=AZURE_SEARCH_INDEX_NAME,\n        credential=AzureKeyCredential(AZURE_SEARCH_KEY),\n    )\n    search_client.upload_documents(docs)\n\ndef delete(search_index_client: SearchIndexClient):\n    \"\"\"\n    Deletes the Azure Cognitive Search index.\n    \"\"\"\n    search_index_client.delete_index(AZURE_SEARCH_INDEX_NAME)\n\ndef main():\n    load_dotenv()\n\n    openai.api_type = AZURE_OPENAI_API_TYPE\n    openai.api_base = AZURE_OPENAI_API_BASE\n    openai.api_version = AZURE_OPENAI_API_VERSION\n    openai.api_key = AZURE_OPENAI_API_KEY\n\n    search_index_client = SearchIndexClient(\n        AZURE_SEARCH_ENDPOINT, AzureKeyCredential(AZURE_SEARCH_KEY)\n    )\n\n    initialize(search_index_client)\n    # delete(search_index_client)\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\n\"\"\"\nEntry point for the chatbot.\n\"\"\"\nfrom chatbot_1 import Chatbot\n\ndef main():\n    chatbot = Chatbot()\n    chatbot.ask(\"I need a large backpack. Which one do you recommend?\")\n    chatbot.ask(\"How much does it cost?\")\n    chatbot.ask(\"And how much for a donut?\")\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\n\"\"\"\nChatbot with context and memory.\n\"\"\"\nimport os\n\nimport openai\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.search.documents import SearchClient\nfrom azure.search.documents.models import Vector\nfrom dotenv import load_dotenv\n\n# Config for Azure Search.\nAZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\nAZURE_SEARCH_KEY = os.getenv(\"AZURE_SEARCH_KEY\")\nAZURE_SEARCH_INDEX_NAME = \"products-index-1\"\n\n# Config for Azure OpenAI.\nAZURE_OPENAI_API_TYPE = \"azure\"\nAZURE_OPENAI_API_BASE = os.getenv(\"AZURE_OPENAI_API_BASE\")\nAZURE_OPENAI_API_VERSION = \"2023-03-15-preview\"\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\nAZURE_OPENAI_CHATGPT_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\")\nAZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n\n# Chat roles\nSYSTEM = \"system\"\nUSER = \"user\"\nASSISTANT = \"assistant\"\n\nclass Chatbot:\n    \"\"\"Chat with an LLM using RAG. Keeps chat history in memory.\"\"\"\n\n    chat_history = []\n\n    def __init__(self):\n        load_dotenv()\n        openai.api_type = AZURE_OPENAI_API_TYPE\n        openai.api_base = AZURE_OPENAI_API_BASE\n        openai.api_version = AZURE_OPENAI_API_VERSION\n        openai.api_key = AZURE_OPENAI_API_KEY\n\n    def _summarize_user_intent(self, query: str) -> str:\n        \"\"\"\n        Creates a user message containing the user intent, by summarizing the chat\n        history and user query.\n        \"\"\"\n        chat_history_str = \"\"\n        for entry in self.chat_history:\n            chat_history_str += f\"{entry['role']}: {entry['content']}\\n\"\n        messages = [\n            {\n                \"role\": SYSTEM,\n                \"content\": (\n                    \"You're an AI assistant reading the transcript of a conversation \"\n                    \"between a user and an assistant. Given the chat history and \"\n                    \"user's query, infer user real intent.\"\n                    f\"Chat history: ```", "```py\\n\"\n                    f\"User's query: ```", "```py\\n\"\n                ),\n            }\n        ]\n        chat_intent_completion = openai.ChatCompletion.create(\n            deployment_id=AZURE_OPENAI_CHATGPT_DEPLOYMENT,\n            messages=messages,\n            temperature=0.7,\n            max_tokens=1024,\n            n=1,\n        )\n        user_intent = chat_intent_completion.choices[0].message.content\n\n        return user_intent\n\n    def _get_context(self, user_intent: str) -> list[str]:\n        \"\"\"\n        Gets the relevant documents from Azure Cognitive Search.\n        \"\"\"\n        query_vector = Vector(\n            value=openai.Embedding.create(\n                engine=AZURE_OPENAI_EMBEDDING_DEPLOYMENT, input=user_intent\n            )[\"data\"][0][\"embedding\"],\n            fields=\"embedding\",\n        )\n\n        search_client = SearchClient(\n            endpoint=AZURE_SEARCH_ENDPOINT,\n            index_name=AZURE_SEARCH_INDEX_NAME,\n            credential=AzureKeyCredential(AZURE_SEARCH_KEY),\n        )\n\n        docs = search_client.search(search_text=\"\", vectors=[query_vector], top=1)\n        context_list = [doc[\"content\"] for doc in docs]\n\n        return context_list\n\n    def _rag(self, context_list: list[str], query: str) -> str:\n        \"\"\"\n        Asks the LLM to answer the user's query with the context provided.\n        \"\"\"\n        user_message = {\"role\": USER, \"content\": query}\n        self.chat_history.append(user_message)\n\n        context = \"\\n\\n\".join(context_list)\n        messages = [\n            {\n                \"role\": SYSTEM,\n                \"content\": (\n                    \"You're a helpful assistant.\\n\"\n                    \"Please answer the user's question using only information you can \"\n                    \"find in the context.\\n\"\n                    \"If the user's question is unrelated to the information in the \"\n                    \"context, say you don't know.\\n\"\n                    f\"Context: ```", "```py\\n\"\n                ),\n            }\n        ]\n        messages = messages + self.chat_history\n\n        chat_completion = openai.ChatCompletion.create(\n            deployment_id=AZURE_OPENAI_CHATGPT_DEPLOYMENT,\n            messages=messages,\n            temperature=0.7,\n            max_tokens=1024,\n            n=1,\n        )\n\n        response = chat_completion.choices[0].message.content\n        assistant_message = {\"role\": ASSISTANT, \"content\": response}\n        self.chat_history.append(assistant_message)\n\n        return response\n\n    def ask(self, query: str) -> str:\n        \"\"\"\n        Queries an LLM using RAG.\n        \"\"\"\n        user_intent = self._summarize_user_intent(query)\n        context_list = self._get_context(user_intent)\n        response = self._rag(context_list, query)\n        print(\n            \"*****\\n\"\n            f\"QUESTION:\\n{query}\\n\"\n            f\"USER INTENT:\\n{user_intent}\\n\"\n            f\"RESPONSE:\\n{response}\\n\"\n            \"*****\\n\"\n        )\n\n        return response\n```", "```py\n*****\nQUESTION:\nI need a large backpack. Which one do you recommend?\nUSER INTENT:\nUser's intent: The user is looking for a recommendation for a large backpack.\nRESPONSE:\nBased on the information in the context, the SummitClimber Backpack has a dedicated laptop compartment that can accommodate laptops up to 17 inches and it also has a hydration sleeve and tube port, making it compatible with most hydration bladders for convenient on-the-go hydration. However, it's important to keep in mind the cautionary notes and warranty information provided as well. If you're looking for a backpack larger than the SummitClimber Backpack, I don't have that information available.\n*****\n\n*****\nQUESTION:\nHow much does it cost?\nUSER INTENT:\nUser's real intent: The user wants to know the price of the SummitClimber Backpack that the assistant recommended.\nRESPONSE:\nThe price of the SummitClimber Backpack is $120.\n*****\n\n*****\nQUESTION:\nAnd how much for a donut?\nUSER INTENT:\nUser's real intent: This query does not seem related to the previous conversation about backpacks and may be a joke or a non-serious question.\nRESPONSE:\nI'm sorry, I don't understand your question. Is there anything else I can assist you with?\n*****\n```"]