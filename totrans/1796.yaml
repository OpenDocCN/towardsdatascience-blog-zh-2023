- en: Run ChatGPT and GPT Models on Your Website with PHP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/run-chatgpt-and-gpt-models-on-your-website-with-php-517ea20266d7](https://towardsdatascience.com/run-chatgpt-and-gpt-models-on-your-website-with-php-517ea20266d7)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A very simple solution to deliver AI with GPT models to your users
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----517ea20266d7--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----517ea20266d7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----517ea20266d7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----517ea20266d7--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----517ea20266d7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----517ea20266d7--------------------------------)
    ·12 min read·May 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/674c26153559ff00625bba226d4b3458.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Pixabay](https://pixabay.com/illustrations/media-internet-message-network-3683580/).
  prefs: []
  type: TYPE_NORMAL
- en: GPT models can improve the user experience of websites and web apps. They can
    translate, summarize, answer questions, and do many other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating all these functionalities into your online service is fairly easy
    with OpenAI API. Currently, OpenAI only provides official support for Python and
    NodeJS bindings.
  prefs: []
  type: TYPE_NORMAL
- en: Many [third-party bindings](https://github.com/openai-php/client) have been
    developed by the community to facilitate deployment in other programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will show you how to connect your website to OpenAI’s API
    in PHP. I will also explain how to parse and interpret the results returned by
    the API.
  prefs: []
  type: TYPE_NORMAL
- en: I will only cover GPT models but you can follow the same process for DALL-E
    and Whisper models.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-requisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GPT models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You don’t need to be familiar with the GPT models to understand and implement
    this article, but I still recommend you to read my simple introduction about GPT
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-gentle-introduction-to-gpt-models-e02b093a495b?source=post_page-----517ea20266d7--------------------------------)
    [## A Gentle Introduction to GPT Models'
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the new world of token generators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-gentle-introduction-to-gpt-models-e02b093a495b?source=post_page-----517ea20266d7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: PHP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will only need to know the basics of PHP.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will use a PHP library that we can install with Composer (so you will need
    Composer) and that requires at least PHP 8.1\. *Note: You won’t be able to install
    the library with an older version of PHP.*'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI account
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will need an OpenAI account. If you don’t have one, here is my guide on
    how to create and manage an OpenAI account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie/openai-account-documentation-playground-and-models-hyperparameters-fb1dada13260?source=post_page-----517ea20266d7--------------------------------)
    [## OpenAI Account: Documentation, Playground, and Models’ Hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: All you need to know to start using OpenAI API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@bnjmn_marie/openai-account-documentation-playground-and-models-hyperparameters-fb1dada13260?source=post_page-----517ea20266d7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: You will have to create an API key in your account and have a few cents of credits
    remaining if you want to run the examples.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI PHP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the client maintained by [OpenAI PHP](https://github.com/openai-php/client)
    (MIT license) to communicate with OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other PHP libraries do the same, but I choose this one for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It is listed by OpenAI which is a reasonable guarantee that this library can
    be trusted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has the most stars on GitHub among all the PHP bindings for OpenAI API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is easy to install and use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is regularly updated to take into account the changes in the API and new
    OpenAI models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To install it, open a terminal, go to your website/app parent’s root directory,
    and run composer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you don’t have any errors, you can start using OpenAI API with PHP.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your API Key in PHP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You must create an API key in your OpenAI account.
  prefs: []
  type: TYPE_NORMAL
- en: For safety reasons, I recommend creating a new API key for each web app you
    want to connect to the API.
  prefs: []
  type: TYPE_NORMAL
- en: If one of your products has a security breach, you can then just destroy the
    key in your OpenAI account without affecting your other apps.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should not write this key directly in your PHP file but use an OS environment
    variable to store it. For instance, with Ubuntu/Debian, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In your PHP script you can get the value of this environment variable with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you don’t have access to your OS environment variables, the simplest alternative
    is to define a PHP constant in a separate file that you will require in all your
    PHP scripts using the API.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, create a file “key.php”, preferably not in your website''s main
    directory, and write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then write the following at the top of all your files that will use the API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Completion tasks with GPT models in PHP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI PHP client supports all the tasks accessible through OpenAI API. In this
    article, I will focus on “completion tasks” using GPT models.
  prefs: []
  type: TYPE_NORMAL
- en: A completion task is a task in which we **prompt** the model with a text and
    the API answers by adding text after this prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two different types of completion tasks proposed by the API:'
  prefs: []
  type: TYPE_NORMAL
- en: 'standard: a GPT-3 or GPT-4 model is prompted and then generates tokens following
    this prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'chat: Given a list of messages describing a conversation history, the model
    will return a response. So here, the prompt is a set of messages with information
    about whether it was written by the model or the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I will demonstrate how to use the OpenAI PHP client for these two types of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Completion task with GPT-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we need an objective. What do we want the GPT model to accomplish?
  prefs: []
  type: TYPE_NORMAL
- en: For this article, let’s say that our goal is to “translate” text into emojis.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most critical steps when using GPT models is to find a good prompt
    for our task. If your prompt is not good, the model’s answer won’t be great either.
  prefs: []
  type: TYPE_NORMAL
- en: '*What’s a good prompt?*'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering is a very active research area. I won’t tackle this topic
    here but I plan to do it in my next article.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our task, [inspired by previous machine translation work using large language
    models](https://medium.com/towards-data-science/translate-with-chatgpt-f85609996a7f),
    I propose the following prompt that gave reasonably good results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Translate the following text into emoji:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[TXT]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Where [TXT] will be replaced by the text to translate into emojis.
  prefs: []
  type: TYPE_NORMAL
- en: This prompt has the advantage to be short. It won’t cost much to use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we will try to translate into emojis the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I would like a hamburger without onions.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'So our prompt becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Translate the following text into emoji:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I would like a hamburger without onions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'With the OpenAI PHP client, we can do this with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this code, I assume you are in the root directory of your website.
  prefs: []
  type: TYPE_NORMAL
- en: 'It should print a sequence of emojis. I obtained this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '*🍔🚫🧅*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You may get a different sequence since GPT models are “*non-deterministic*”.
  prefs: []
  type: TYPE_NORMAL
- en: I used the “text-davinci-003” GPT model which is the most powerful GPT-3 model.
  prefs: []
  type: TYPE_NORMAL
- en: You can use a cheaper GPT model if your task is very simple. For instance, we
    can try to replace the model “text-davinci-003” with “ada”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'I got the following answer:'
  prefs: []
  type: TYPE_NORMAL
- en: For example, enter This is the text “Looking For a hamburger
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yes, this is quite bad. There aren’t any emojis in this response. Choosing the
    right model is the most critical choice you will have to make when integrating
    the OpenAI API into your product.
  prefs: []
  type: TYPE_NORMAL
- en: If you choose an old or small model, the result will be of low quality and may
    not complete the task requested.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you choose a bigger model, you may get the best results but for a higher
    cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will have to try several models to figure out which is the best option given
    your objective. As a starting point, OpenAI provides some usage [suggestions along
    with a list of available models](https://platform.openai.com/docs/models/overview).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the model name and prompt, the completion task can take many
    more parameters. They are all described in the [API documentation](https://platform.openai.com/docs/api-reference/completions/create).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can precise for instance the maximum number of tokens in the response as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This shouldn’t generate anything but 1 line break. *Why?*
  prefs: []
  type: TYPE_NORMAL
- en: 1 emoji consists of 3 tokens for text-davinci-003\. So if we set ‘max_tokens’
    to 2, the model can’t even generate 1 emoji.
  prefs: []
  type: TYPE_NORMAL
- en: '*How do I know an emoji is made of 3 tokens?*'
  prefs: []
  type: TYPE_NORMAL
- en: I simply checked it in the playground of my OpenAI user account. For instance,
    if you put there “🍔🚫🧅”, the model will count 9 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the GPT model generates a line break before the sequence of emojis.
    It counts as an additional token. In total, GPT answered me with 10 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the “$result” variable contains all this information. We will have
    a look at it in the next part below.
  prefs: []
  type: TYPE_NORMAL
- en: But before that, let’s have a look at the chat completion task.
  prefs: []
  type: TYPE_NORMAL
- en: Chat completion task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chat completion tasks are slightly different from what we did with GPT-3\. Chat
    tasks are powered by gpt-3.5-turbo, which also powers ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: With gpt-3.5-turbo, the “prompt” parameter is replaced by “messages”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Technically, “messages” are associative arrays with two required keys, and
    one optional, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'role (required): Can be either “system”, “assistant”, or “user”. At the time
    I write this article, “system” is almost ignored according to OpenAI documentation.
    It leaves “assistant” which is the model, and “user” which is a human.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'content (required): This is where we put our prompt, or the context of our
    prompt, for instance, the chat history.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'name (optional): If you want to give a specific name to the author of the message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The length and number of messages are virtually unlimited. That way, the gpt-3.5-turbo
    can accept a very long chat history as input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chat completion can perform similar tasks as the standard GPT-3\. In the documentation,
    OpenAI wrote the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Because `gpt-3.5-turbo` performs at a similar capability to `text-davinci-003`
    but at 10% the price per token, we recommend `gpt-3.5-turbo` for most use cases.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s check it with our task of translating text into emojis.
  prefs: []
  type: TYPE_NORMAL
- en: 'We only have a few modifications to perform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: I obtained the same answer as with text-davinci-003, “🍔🚫🧅”, but at 10% of text-davinci-003's
    price per token.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to communicate with OpenAI API in PHP, we can have a closer
    look at what the API returns. As we will see, there are useful data in the response
    that we can use to monitor the API cost, keep track of the users' activity (e.g.,
    to flag prohibited behavior), etc.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting OpenAI API response with PHP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can make a printable version of the “$result” variables like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'For a chat completion task, it will print this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: I manually masked part of the “id”.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the following entries:'
  prefs: []
  type: TYPE_NORMAL
- en: 'id: A unique ID assigned by OpenAI to the response. This information can help
    to track interactions between the API and your users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'object: The type of task performed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'created: The timestamp of the creation of the response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'model: The model used to generate the response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'choices: By default, you will get only one message for a chat completion task,
    unless you change the “n” option when calling the API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'index: The index, starting at 0, of the message generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'message: Information about the message generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'role: The role of the author of the message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'content: The message itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'finish_reason: The reason why the API stopped the generation of the message.
    By default it will be “stop”, i.e., the model stopped the generation without any
    constraints. It can change if you indicated a “stop” parameter when calling the
    API. The model would then stop the generation after generating one of the tokens
    you mentioned in “stop”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'usage: Information about the length in tokens. It can be used to monitor the
    API cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'prompt_tokens: The number of tokens in your prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'completion_tokens: The number of tokens in the message generated by the API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'total_tokens: The sum of “prompt_tokens” and “completion_tokens”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most important fields are “choices”, since this is what you will have to
    deliver to your users, and “usage” since this is the only metric that will tell
    you how much it cost to generate this answer.
  prefs: []
  type: TYPE_NORMAL
- en: To know the exact cost of an API call, you have to multiply the value of “total_tokens”
    by the cost of the model per token. Note that OpenAI shows pricing for 1,000 tokens
    so you will have to divide this number by 1,000 to get the price per token.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if we use a model costing $0.002 per 1,000 tokens, and “total_tokens”
    is 32, we can compute the total cost as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 0.002 / 1000 * 32 = 0.000064
  prefs: []
  type: TYPE_NORMAL
- en: This API call would cost $0.000064.
  prefs: []
  type: TYPE_NORMAL
- en: The response fields of a standard GPT-3 completion are almost identical to the
    fields of the chat completion task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only notable difference is that a “text.completion” task can also return
    the log probabilities of the *t* most probable tokens. You can indicate “t” when
    you call the API with the “logprobs” parameter. The maximum value of *t* is 5\.
    *Note: OpenAI’s API reference indicates that you can manually request OpenAI a
    greater number if your application needs it.*'
  prefs: []
  type: TYPE_NORMAL
- en: What’s next for an Integration in a web app/website?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned how to communicate with OpenAI API in PHP. Your online service
    can now exploit all the power of GPT models.
  prefs: []
  type: TYPE_NORMAL
- en: The next step would be to implement the front end. You don’t need to do something
    over-complicated for this. A simple AJAX script, using jQuery for instance, would
    be enough to asynchronously get the response from the PHP script that made the
    API call.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be as simple as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This would print the content of the chat completion inside an HTML object with
    the id attribute set to “my_GPT_response”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your PHP script must receive the “prompt” as a $_POST variable, and the API
    answer should be encoded into a JSON object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: To conclude this article, I should mention once again that you must always check
    what you are sending to the API to ensure that you are not violating the policies
    and terms of use of OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: You can exploit the [moderation model](https://platform.openai.com/docs/api-reference/moderations),
    free of use, proposed by OpenAI that can flag unsafe content before you send it
    to a GPT model.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to check the age of your users. [OpenAI’s terms of use](https://openai.com/policies/terms-of-use)
    prohibit the use of their services for children under 13 while children under
    18 can use the services only with the supervision of an adult.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you like this article and would be interested to read the next ones, the
    best way to support my work is to become a Medium member using this link:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie/membership?source=post_page-----517ea20266d7--------------------------------)
    [## Join Medium with my referral link - Benjamin Marie'
  prefs: []
  type: TYPE_NORMAL
- en: Join Our AI Community and Get Access to Cutting-Edge Research This blog aims
    to demystify recent advances in AI for…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@bnjmn_marie/membership?source=post_page-----517ea20266d7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are already a member and want to support this work,* [*just follow
    me on Medium*](https://medium.com/@bnjmn_marie)*.*'
  prefs: []
  type: TYPE_NORMAL
