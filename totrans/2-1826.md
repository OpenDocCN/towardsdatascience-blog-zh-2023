# 将文本分段成段落

> 原文：[https://towardsdatascience.com/segmenting-text-into-paragraphs-e8bed99b6ebd](https://towardsdatascience.com/segmenting-text-into-paragraphs-e8bed99b6ebd)

## 基于监督学习的统计 NLP 方法

[](https://jagota-arun.medium.com/?source=post_page-----e8bed99b6ebd--------------------------------)[![Arun Jagota](../Images/3c3eb142f671b5fb933c2826d8ed78d9.png)](https://jagota-arun.medium.com/?source=post_page-----e8bed99b6ebd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e8bed99b6ebd--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e8bed99b6ebd--------------------------------) [Arun Jagota](https://jagota-arun.medium.com/?source=post_page-----e8bed99b6ebd--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e8bed99b6ebd--------------------------------) ·11 min 阅读·2023年2月25日

--

![](../Images/8e2618a1773575c3a2cb635554d5915f.png)

图片由 [Gordon Johnson](https://pixabay.com/users/gdj-1086657/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4385160) 提供，来自 [Pixabay](https://pixabay.com/)

在之前的 Medium 文章中，我们讨论了将文本分割成句子的问题[3]。现在我们来看一个相关问题：将文本分割成段落。

初看起来，这两个问题似乎本质上是相同的，只是在不同的分块层次上。实际上，将文本分段成段落的问题要有趣得多。

一方面，句子边界有明确的信号，如句号、问号或感叹号。通常，问题在于这些标记中的哪些是实际的边界，哪些是在句子内嵌入的边界。这就是假阳性的难题。

将文本分段成段落更为复杂。这样考虑一下。假设你有一个长句子序列，没有段落分隔符。应该在哪里设置段落边界？这不是一个容易解决的问题，也不一定有唯一的解决方案。这意味着可能存在多种将句子序列分段的合理分法。

将文本分段成段落可以看作是文本分割的一个特例[1]。文本片段是一个连续的段落，保持一定的连贯性，比如说在同一主题上。根据这种连贯性度量，段落会在主题发生变化时过渡到另一个段落。

更广泛的文本分段问题更难解决。原因有几个。其中之一是很难获取标记数据。对于段落分段，有大量的标记数据可用，这些数据以网页和包含段落分隔符的维基百科文章的形式存在。而对于更广泛的文本分段问题情况则不然。

在这篇文章中，我们认为一个能够建议合理分割边界的算法对写作中的人是有帮助的。就像Grammarly对写作的帮助一样。换句话说，精确率和召回率都不需要特别高。精确率需要合理；召回率甚至可以更低。

本文的其余部分应以此视角进行阅读。我们将满足于一个具有合理精确度的解决方案，可能接近50%，以及较低的召回率，可能接近10%。关键是即使这样也在类似Grammarly的环境中是有用的。

即使段落分割建议很少出现，只要它们具有合理的精确度，它们就会增加像Grammarly这样的产品的价值。

不用说，如果我们可以在付出最少努力的情况下获得更好的精确率或召回率，我们自然会选择这样做。

**预测段落分割的概率模型**

我们将从正式描述开始，用简单的英语解释其各个组件。

设X1和X2表示训练语料库中相邻的两个句子。

我们将与(X1, X2)关联一个二进制标签Y。如果X1和X2之间有段落分割，则Y为1，否则为0。

我们将跟踪第三个预测变量*i*。X1将是当前段落中的第*i*个句子。预测变量“*i*”将赋予我们的模型关注段落长度的能力。

我们的训练集将包含实例(X1, X2, *i*, Y)。

从训练集中，我们旨在学习一个模型*P*(Y | X1, X2, *i*)。

*P*(Y=1 | X1, X2, *i*)将表示在X1作为当前段落中的第*i*个句子的情况下，X1和X2之间有段落分割的概率。

P(Y=0|X1, X2, *i*)表示在X1作为当前段落中的第*i*个句子的情况下，X2应该扩展当前段落的概率。

模型P(Y | X1, X2, *i*)非常复杂。这是因为X1和X2是句子，可能非常稀有或很长。这意味着，即使我们的训练集包含了几亿个标记实例，也可能没有足够的数据来估计这个模型。

我们需要做一些假设。

首先，让我们应用贝叶斯规则。

*P*(Y | X1, X2, *i*) = N(X1, X2, *i*, Y)/Z

其中N(X1, X2, *i*, Y)等于*P*(X1, X2, *i* | Y) P(Y)。

Z只是N(X1, X2, *i*, 0) + N(X1, X2, *i*, 1)。

接下来，我们将对N(X1, X2, *i*, Y)进行如下分解。

N(X1, X2, *i*, Y) = *P*(X1 | Y)**P*(X2 | Y)**P*(*i* | Y)**P*(Y)

*P*(X1 | Y=1)是段落中最后句子的分布。*P*(X1 | Y = 0)是段落中非最后句子的分布。

*P*(X2 | Y = 1)是段落中第一句子的分布。*P*(X2 | Y = 0)是段落中非第一句子的分布。

现在考虑*P*(*i* | Y)。

让我们提醒自己 X1 是当前段落中的 *i* 句。因此，*P*(*i* | Y=1) 实际上是段落长度的分布，因为段落必须在 X1，即当前段落中的第 *i* 句之后结束。

*P*(*i* | Y = 1) 将倾向于偏向于较小的 *i*。这是因为大多数段落较短。

*P*(*i* | Y = 0) 将倾向于偏向于更小。这是因为 Y = 0 表示当前段落中的第 *i* 句 X1 不会结束该段落。

概率模型 *P*(X1 | Y) 和 *P*(X2|Y) 都仍然过于复杂。这是因为句子的宇宙是无限的。也就是说，句子可以任意长。也可以任意稀有。

我们可以做进一步的简化假设吗？具体来说，使用的可能性不一定是整个句子，而是它们的前几个词。

让我们从实际例子开始。

首先，让我们看看继续短段落的句子的例子。

假设下一句以“例如”，“例子”，“更准确地说”等开头。如果当前段落足够短，比如一两句，这些前缀在下一句中的出现预测 Y = 0，即扩展段落。

为了支持这个假设，我们邀请读者阅读这些相邻句子的对，例如 [https://en.wikipedia.org/wiki/Deep_learning](https://en.wikipedia.org/wiki/Deep_learning)

> 深度学习算法可以应用于无监督学习任务。这是一个重要的好处，因为未标记的数据比标记的数据更为丰富。**例子** 包括可以以无监督方式训练的深度结构，如深度置信网络。

…

> 深度学习是一类机器学习算法，它[8]: 199–200 使用多个层次逐步从原始输入中提取更高级的特征。**例如**，在图像处理过程中，较低的层次可能识别边缘，而较高的层次可能识别对人类相关的概念，如数字、字母或面孔。

…

> “深度学习”中的“深度”指的是数据转换通过的层数。**更准确地说**，深度学习系统具有实质性的信用分配路径（CAP）深度。

你是否同意每个加粗的词序列预测段落的延续？

这些例子表明，考虑简化 *P*(X1 | Y) 为 *P*(X1 的前几个词 | Y) 是有意义的。

这就引出了“few”在这里的值是多少的问题？我们稍后会解决这个问题。

接下来，让我们看看一句话段落的例子。

为此，我要求 ChatGPT 给我一些一句话段落的例子。似乎它很字面地理解了这个问题。因此我重新表述了问题为

> 给我一些**仅**由一句话构成的段落的例子。

现在我得到了好的例子。

> 沉默。
> 
> 停止。
> 
> 再也不见。
> 
> 为什么？
> 
> 是的！
> 
> 对不起。
> 
> 足够了。
> 
> 记住。
> 
> 帮助！
> 
> 再见。

我们期望这些句子中的每一个都有较高的 *P*(X1 | Y = 1) 的可能性。也就是说，对于其中一些或全部句子，*P*(X1 | Y = 0) 也可能相对较高。这意味着段落不会在它们之后立即结束。

尽管如此，我们在这里展示这些示例，因为它们确实表明 *P*(X1 | Y = 1) 对于这些句子是值得建模的。

接下来，让我们看看以新段落开始的句子的示例。我们从 [https://en.wikipedia.org/wiki/Deep_learning](https://en.wikipedia.org/wiki/Deep_learning) 中挑选了一些段落，并展示了它们第一句话的前几个词。

> 深度学习是一个更广泛领域的一部分…
> 
> 深度学习架构如…
> 
> 人工神经网络（ANNs）是…
> 
> 在深度学习中，每一层学会…
> 
> ANN 基于一组…
> 
> 深度神经网络（DNNs）可以建模复杂的非线性…

这些示例表明，*P*(X2 | Y = 1) 可以简化为

*P*(X2 | Y = 1)

对于“少量”这一选择的合适情况。

让我们将从这些示例中学到的内容形式化。

我们可以简化 *P*(X1 | Y) 和 *P*(X2 | Y) 为

*P*(X1 以 *w*(1)、*w*(2)、…、*w*(*k*) | Y)

以及

*P*(X2 以 *w*’(1)、*w*’(2)、…、*w*’(*k*’) |Y)

分别。

这里 *w*(1)、*w*(2)、…、*w*(k) 和 *w*’(1)、*w*’(2)、…、*w*’(*k*’) 分别是 *k* 和 *k*’ 的词序列。

显而易见的问题是 *k* 和 *k*’ 应该是什么？

解决这个问题的一种方法是不要提前固定 *k* 和 k’，而是推迟决策直到推断时。

方法如下。

首先介绍一些术语。我们将称以句子开始的词序列为句子的前缀。

现在考虑 *P*(X1 | Y=y)。我们将按如下方式近似。

我们将首先找到 X 的最大前缀，称之为 P，并且具有足够的支持。我们将使用 P(X1 以 P 开始 | Y) 作为 P(X1 | Y) 的代理。

X1 前缀 P 对 *P*(X1 | Y) 的支持定义为训练集中 P 作为 X1 前缀的实例数量。

这一近似过程的理念是，我们应该使用 X1 的最长前缀，只要它在训练集中出现的次数足够多（作为 X1 的前缀）。

类似地，我们应该将 *P*(X2 | Y) 估计为 *P*(Q | Y)，其中 Q 是 X2 的最大前缀，其支持度足够大。

我们推断出的形式对模型意味着什么？我们需要跟踪所有 X1 的前缀 P 的概率 *P*(X1 以 P 开始 | Y)。*P*(X2 | Y) 也类似。

内部，对于建模 *P*(X1 |Y) 和 *P*(X2 | Y)，我们需要跟踪大量的词序列。

幸运的是，这些词序列可以收集到所谓的 Trie 数据结构中。这些结构被优化为紧凑地表示大量的词序列。

这些 Tries 在训练过程中如下构建。

我们将分别使用四个 Tries T10、T11、T20 和 T21。T*iy*，*i* = 1 或 2，将存储前缀序列及其对 Y=*y* 的计数，针对 X*i*。

Trie 中的每个节点将存储一个计数。

我们将所有的 Trie 初始化为从一个单独的节点开始，即根节点，其计数设置为零。

现在考虑一个训练集中的实例 (X1, X2, *y*)。我们忽略了 *i*，因为它不会影响 Trie。

将 X1 解释为单词序列时，我们将在 T1*y* 中查找 X1，必要时扩展 Trie 并添加由新节点组成的路径。每次创建新节点时，其计数将初始化为 0。

现在，在 Trie T1*y* 中，我们将沿着表示 X1 的路径递增所有节点的计数，每个节点增加一次。

处理 X2 时，我们将在 Trie T2*y* 上重复相同的过程。

**数值示例**

现在让我们说明这个过程。

四个 Trie 将被初始化为 T10、T11、T20 和 T21，每个 Trie 的初始状态为 {[]:0}。

现在假设我们呈现第一个训练实例为 ([a,b],[A,B,C],1)

> T11 的新状态将是 {[]:1,[a]:1,[a,b]:1}
> 
> T21 的新状态将是 {[]:1,[A]:1, [A,B]:1, [A,B,C]:1}

现在假设我们呈现这个训练实例：([a,d],[A,B,E],1)

> T11 的新状态将是 {[]:2,[a]:2,[a,b]:1,[a,d]:1}
> 
> T21 的新状态将是 {[]:2,[A]:2, [A,B]:2, [A,B,C]:1,[A,B,E]:1}

在上述示例中，为了视觉上的方便，我们将每个 Trie 表示为一个哈希映射，即 Python 中的字典。

实际上，我们可以通过利用（重复的）前缀结构将 Trie 更紧凑地表示为树。

将 Trie 表示为树对于查找与给定序列 X 的所有前缀相关的计数也更高效。我们只需沿着 Trie 包含的唯一路径向下查找 X 的最长前缀。我们说“最长前缀”是因为 X 可能未完全包含在 Trie 中，如果 X 在训练集中从未以这种上下文出现，它会被放置在这个 Trie 中。然而，Trie 中总是存在至少一个 X 的前缀的路径，即使只是空前缀。

**使用 Trie 推理**

假设训练已经完成。现在，对于给定的 (X1, X2, *i*)，我们想计算 *P*(Y=y|X1,X2,i)。

这个计算中涉及 Trie 的部分是 *P*(X1|Y=*y*) 和 *P*(X2|Y=*y*)。

让我们演示如何计算其中之一，其他的计算过程也会类似。

让我们选择 *P*(X1|Y=*y*)。

我们遍历 Trie T10 和 T11 以找到 X1 的最长支持前缀。我们需要使用这两个 Trie，因为 X1 的前缀 P 的支持是 T10 和 T11 中 P 结束的节点上的计数之和。

让我们用 P(X1) 表示 X1 的最长支持前缀。

*P*(P(X1) | Y=*y*) 只是 P 在 Trie T1*y* 中结束的节点上的计数除以 Trie T1*y* 根节点上的计数。这仅仅是训练集中标签为 *y* 且 X1 以 P(X1) 开始的实例数量，除以标签为 *y* 的训练集中实例数量。

**总结**

在这篇文章中，我们介绍了将文本分割成段落的 NLP 问题。我们注意到，这个问题比将文本分割成句子的难度更大，但比将文本分割成以主题为单位的连贯单元的难度要小。

我们将这个问题框架设定为监督学习问题。有大量标记数据可以直接使用。输入是一对相邻句子。结果是这两句之间是否存在段落分隔。因此，这是一个监督学习问题，其中输入是一对序列，结果是二元的。

接下来，我们在简单贝叶斯假设下应用了贝叶斯规则，即在给定结果的情况下，预测变量的条件独立性。然后我们计算了结果公式中的似然性和先验项。

从这里我们注意到，即使在简单假设下，得到的模型也过于复杂。我们讨论了如何通过将输入的两个句子建模为从空前缀到整个序列的一组前缀来应对这种复杂性。在推理时，我们描述了如何使用“正确”的前缀来预测结果。

我们检查了多个实际文本中相邻句子的实例，以支持我们基于前缀而非完整句子的工作方法。

最后，我们注意到，处理句子的所有前缀而不是句子本身可能会导致模型规模激增。为此，我们提出了一种使用 Tries 的方案。在适当的 Tries 中，以紧凑方式表示相同上下文中的序列。我们详细讨论了 Tries 如何在训练过程中学习，以及 Tries 在推理过程中如何使用。

**参考文献**

1.  [文本分割的神经模型](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/reports/final_reports/report001.pdf)

1.  Grammarly

1.  [https://towardsdatascience.com/segmenting-text-into-sentences-using-nlp-35d8ef55c0fd](/segmenting-text-into-sentences-using-nlp-35d8ef55c0fd)

1.  [https://en.wikipedia.org/wiki/Trie](https://en.wikipedia.org/wiki/Trie)
