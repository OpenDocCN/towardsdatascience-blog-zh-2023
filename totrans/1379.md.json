["```py\ndef parse_chord_label(chord_label):\n  # Define a regex pattern for chord symbols\n  pattern = r\"([A-G][#b]?)(m|\\+|%|o|sus)?(6|7|\\^7)?\"\n  # Match the pattern with the input chord\n  match = re.match(pattern, chord_label)\n  if match:\n    # Extract the root, basic chord form and extension from the match obj\n    root = match.group(1)\n    form = match.group(2) or \"M\"\n    ext = match.group(3) or \"\"\n    return root, form, ext\n  else:\n    # Return None if the input is not a valid chord symbol\n    raise ValueError(\"Invalid chord symbol: {}\".format(chord_label))\n```", "```py\ndef parse_jht_to_dep_tree(jht_dict):\n    \"\"\"Parse the python jazz harmony tree dict to a list of dependencies and a list of chord in the leaves.\n    \"\"\"\n    all_leaves = []\n\n    def _iterative_parse_jht(dict_elem):\n        \"\"\"Iterative function to parse the python jazz harmony tree dict to a list of dependencies.\"\"\"\n        children = dict_elem[\"children\"]\n        if children == []:  # recursion ending condition\n            out = (\n                [],\n                {\"index\": len(all_leaves), \"label\": dict_elem[\"label\"]},\n            )\n            # add the label of the current node to the global list of leaves\n            all_leaves.append(dict_elem[\"label\"])\n            return out\n        else:  # recursive call\n            assert len(children) == 2 \n            current_label = noast(dict_elem[\"label\"])\n            out_list = []  # dependency list\n            iterative_result_left = _iterative_parse_jht(children[0])\n            iterative_result_right = _iterative_parse_jht(children[1])\n            # merge the dependencies lists computed deeper\n            out_list.extend(iterative_result_left[0])\n            out_list.extend(iterative_result_right[0])\n            # check if the label correspond to the left or right children and return the corresponding result\n            if iterative_result_right[1][\"label\"] == current_label: # default if both children are equal is to go left-right arch\n                # append the dependency for the current node\n                out_list.append((iterative_result_right[1][\"index\"], iterative_result_left[1][\"index\"]))\n                return out_list, iterative_result_right[1]\n            elif iterative_result_left[1][\"label\"] == current_label: \n                # print(\"right-left arc on label\", current_label)\n                # append the dependency for the current node\n                out_list.append((iterative_result_left[1][\"index\"], iterative_result_right[1][\"index\"]))\n                return out_list, iterative_result_left[1]\n            else:\n                raise ValueError(\"Something went wrong with label\", current_label)\n\n    dep_arcs, root = _iterative_parse_jht(jht_dict)\n    dep_arcs.append((-1,root[\"index\"])) # add connection to the root, with index -1\n    # add self loop to the root\n    dep_arcs.append((-1,-1)) # add loop connection to the root, with index -1\n    return dep_arcs, all_leaves\n```", "```py\nimport torch.nn as nn\n\nclass TransformerEncoder(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        hidden_dim,\n        encoder_depth,\n        n_heads = 4,\n        dropout=0,\n        embedding_dim = 8,\n        activation = \"gelu\",\n    ):\n        super().__init__()\n        self.input_dim = input_dim\n        self.positional_encoder = PositionalEncoding(\n            d_model=input_dim, dropout=dropout, max_len=200\n        )\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, dim_feedforward=hidden_dim, nhead=n_heads, dropout =dropout, activation=activation)\n        encoder_norm = nn.LayerNorm(input_dim)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=encoder_depth, norm=encoder_norm)\n        self.embeddings = nn.ModuleDict({\n                        \"root\": nn.Embedding(12, embedding_dim),\n                        \"form\": nn.Embedding(len(CHORD_FORM), embedding_dim),\n                        \"ext\": nn.Embedding(len(CHORD_EXTENSION), embedding_dim),\n                        \"duration\": nn.Embedding(len(JTB_DURATION), embedding_dim,\n                        \"metrical\": nn.Embedding(METRICAL_LEVELS, embedding_dim)\n                    })\n\n    def forward(self, sequence):\n        root = sequence[:,0]\n        form = sequence[:,1]\n        ext = sequence[:,2]\n        duration = sequence[:,3]\n        metrical = sequence[:,4]\n        # transform categorical features to embedding\n        root = self.embeddings[\"root\"](root.long())\n        form = self.embeddings[\"form\"](form.long())\n        ext = self.embeddings[\"ext\"](ext.long())\n        duration = self.embeddings[\"duration\"](duration.long())\n        metrical = self.embeddings[\"metrical\"](metrical.long())\n        # sum all embeddings\n        z = root + form + ext + duration + metrical\n        # add positional encoding\n        z = self.positional_encoder(z)\n        # reshape to (seq_len, batch = 1, input_dim)\n        z = torch.unsqueeze(z,dim= 1)\n        # run transformer encoder\n        z = self.transformer_encoder(src=z, mask=src_mask)\n        # remove batch dim\n        z = torch.squeeze(z, dim=1)\n        return z, \"\"\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, d_model)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n```", "```py\nclass ArcPredictor(nn.Module):\n    def __init__(self, hidden_channels, activation=F.gelu, dropout=0.3):\n        super().__init__()\n        self.activation = activation\n        self.root_linear = nn.Linear(1, hidden_channels) # linear to produce root features\n        self.lin1 = nn.Linear(2*hidden_channels, hidden_channels)\n        self.lin2 = nn.Linear(hidden_channels, 1)\n        self.dropout = nn.Dropout(dropout)\n        self.norm = nn.LayerNorm(hidden_channels)\n\n    def forward(self, z, pot_arcs):\n        # add column for the root element\n        root_feat = self.root_linear(torch.ones((1,1), device=z.device))\n        z = torch.vstack((root_feat,z))\n        # proceed with the computation\n        z = self.norm(z)\n        # concat the embeddings of the two nodes, shape (num_pot_arcs, 2*hidden_channels)\n        z = torch.cat([z[pot_arcs[:, 0]], z[pot_arcs[:, 1]]], dim=-1)\n        # pass through a linear layer, shape (num_pot_arcs, hidden_channels)\n        z = self.lin1(z)\n        # pass through activation, shape (num_pot_arcs, hidden_channels)\n        z = self.activation(z)\n        # normalize\n        z = self.norm(z)\n        # dropout\n        z = self.dropout(z)\n        # pass through another linear layer, shape (num_pot_arcs, 1)\n        z = self.lin2(z)\n        # return a vector of shape (num_pot_arcs,)\n        return z.view(-1)\n```", "```py\nclass ChordParser(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2, embedding_dim = 8, use_embedding = True, n_heads = 4):\n        super().__init__()\n        self.activation = nn.functional.gelu\n        # initialize the encoder\n        self.encoder = NotesEncoder(input_dim, hidden_dim, num_layers, dropout, embedding_dim, n_heads=n_heads)\n        # initialize the decoder\n        self.decoder = ArcDecoder(input_dim, dropout=dropout)\n\n    def forward(self, note_features, pot_arcs, mask=None):\n        z = self.encoder(note_features)\n        return self.decoder(z, pot_arcs)\n```", "```py\nloss_bce = torch.nn.BCEWithLogitsLoss()\nloss_ce = torch.nn.CrossEntropyLoss(ignore_index=-1)\ntotal_loss = loss_bce + loss_ce\n```", "```py\n# Adapted from https://github.com/HMJW/biaffine-parser\ndef eisner(scores, return_probs = False):\n    \"\"\"Parse using Eisner's algorithm.\n    The matrix follows the following convention:\n        scores[i][j] = p(i=head, j=dep) = p(i --> j)\n    \"\"\"\n    rows, collumns = scores.shape\n    assert rows == collumns, 'scores matrix must be square'\n    num_words = rows - 1  # Number of words (excluding root).\n    # Initialize CKY table.\n    complete = np.zeros([num_words+1, num_words+1, 2])  # s, t, direction (right=1).\n    incomplete = np.zeros([num_words+1, num_words+1, 2])  # s, t, direction (right=1).\n    complete_backtrack = -np.ones([num_words+1, num_words+1, 2], dtype=int)  # s, t, direction (right=1).\n    incomplete_backtrack = -np.ones([num_words+1, num_words+1, 2], dtype=int)  # s, t, direction (right=1).\n    incomplete[0, :, 0] -= np.inf\n    # Loop from smaller items to larger items.\n    for k in range(1, num_words+1):\n        for s in range(num_words-k+1):\n            t = s + k\n            # First, create incomplete items.\n            # left tree\n            incomplete_vals0 = complete[s, s:t, 1] + complete[(s+1):(t+1), t, 0] + scores[t, s]\n            incomplete[s, t, 0] = np.max(incomplete_vals0)\n            incomplete_backtrack[s, t, 0] = s + np.argmax(incomplete_vals0)\n            # right tree\n            incomplete_vals1 = complete[s, s:t, 1] + complete[(s+1):(t+1), t, 0] + scores[s, t]\n            incomplete[s, t, 1] = np.max(incomplete_vals1)\n            incomplete_backtrack[s, t, 1] = s + np.argmax(incomplete_vals1)\n            # Second, create complete items.\n            # left tree\n            complete_vals0 = complete[s, s:t, 0] + incomplete[s:t, t, 0]\n            complete[s, t, 0] = np.max(complete_vals0)\n            complete_backtrack[s, t, 0] = s + np.argmax(complete_vals0)\n            # right tree\n            complete_vals1 = incomplete[s, (s+1):(t+1), 1] + complete[(s+1):(t+1), t, 1]\n            complete[s, t, 1] = np.max(complete_vals1)\n            complete_backtrack[s, t, 1] = s + 1 + np.argmax(complete_vals1)\n    value = complete[0][num_words][1]\n    heads = -np.ones(num_words + 1, dtype=int)\n    backtrack_eisner(incomplete_backtrack, complete_backtrack, 0, num_words, 1, 1, heads)\n    value_proj = 0.0\n    for m in range(1, num_words+1):\n        h = heads[m]\n        value_proj += scores[h, m]\n    if return_probs:\n        return heads, value_proj\n    else:\n        return heads\n\ndef backtrack_eisner(incomplete_backtrack, complete_backtrack, s, t, direction, complete, heads):\n    \"\"\"\n    Backtracking step in Eisner's algorithm.\n    - incomplete_backtrack is a (NW+1)-by-(NW+1) numpy array indexed by a start position,\n    an end position, and a direction flag (0 means left, 1 means right). This array contains\n    the arg-maxes of each step in the Eisner algorithm when building *incomplete* spans.\n    - complete_backtrack is a (NW+1)-by-(NW+1) numpy array indexed by a start position,\n    an end position, and a direction flag (0 means left, 1 means right). This array contains\n    the arg-maxes of each step in the Eisner algorithm when building *complete* spans.\n    - s is the current start of the span\n    - t is the current end of the span\n    - direction is 0 (left attachment) or 1 (right attachment)\n    - complete is 1 if the current span is complete, and 0 otherwise\n    - heads is a (NW+1)-sized numpy array of integers which is a placeholder for storing the\n    head of each word.\n    \"\"\"\n    if s == t:\n        return\n    if complete:\n        r = complete_backtrack[s][t][direction]\n        if direction == 0:\n            backtrack_eisner(incomplete_backtrack, complete_backtrack, s, r, 0, 1, heads)\n            backtrack_eisner(incomplete_backtrack, complete_backtrack, r, t, 0, 0, heads)\n            return\n        else:\n            backtrack_eisner(incomplete_backtrack, complete_backtrack, s, r, 1, 0, heads)\n            backtrack_eisner(incomplete_backtrack, complete_backtrack, r, t, 1, 1, heads)\n            return\n    else:\n        r = incomplete_backtrack[s][t][direction]\n        if direction == 0:\n            heads[s] = t\n            backtrack_eisner(incomplete_backtrack, complete_backtrack, s, r, 1, 1, heads)\n            backtrack_eisner(incomplete_backtrack, complete_backtrack, r+1, t, 0, 1, heads)\n            return\n        else:\n            heads[t] = s\n            backtrack_eisner(incomplete_backtrack, complete_backtrack, s, r, 1, 1, heads)\n            backtrack_eisner(incomplete_backtrack, complete_backtrack, r+1, t, 0, 1, heads)\n            return\n```"]