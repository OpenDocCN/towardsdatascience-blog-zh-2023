- en: Beginner’s Guide to Linear Regression with PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/beginners-guide-to-linear-regression-with-pyspark-bfc39b45a9e9](https://towardsdatascience.com/beginners-guide-to-linear-regression-with-pyspark-bfc39b45a9e9)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A step-by-step tutorial on building a linear regression model with PySpark code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@yazihejazi?source=post_page-----bfc39b45a9e9--------------------------------)[![Yasmine
    Hejazi](../Images/1c280c78e49f62345b3cd0c30b185482.png)](https://medium.com/@yazihejazi?source=post_page-----bfc39b45a9e9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bfc39b45a9e9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bfc39b45a9e9--------------------------------)
    [Yasmine Hejazi](https://medium.com/@yazihejazi?source=post_page-----bfc39b45a9e9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bfc39b45a9e9--------------------------------)
    ·5 min read·Jan 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa8d595a8c5e4b04bbfacc0474adb58d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [eskay lim](https://unsplash.com/@eskaylim?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PySpark is a Python API for Apache Spark. It allows us to code in a high level
    coding language while reaping the benefits of distributed computing. With in-memory
    computation, distributed processing using parallelize, and native machine learning
    libraries, we unlock great data processing efficiency that is essential for data
    scaling.
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial will go step-by-step on how to create a PySpark linear regression
    model using [Diamonds data](https://ggplot2.tidyverse.org/reference/diamonds.html)
    found on [ggplot2](https://ggplot2.tidyverse.org/reference/#data). Databricks
    hosts this dataset on [Databricks Utilities (](https://docs.databricks.com/dev-tools/databricks-utils.html)`[dbutils](https://docs.databricks.com/dev-tools/databricks-utils.html)`[)](https://docs.databricks.com/dev-tools/databricks-utils.html)
    so that it can easily be loaded. This data has both numerical and categorical
    features that we can utilize to build our model. We will handle how to preprocess
    the data so that we can simply train our model and create our predictions. And
    we’ll do this without touching Pandas!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/712aad12eeba673e8726d581a96c4995.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Tahlia Doyle](https://unsplash.com/@tahliaclaire?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: If you are already familiar with coding linear regression in Pandas, the process
    is similar. The regression model will be predicting the **price** of a diamond.
  prefs: []
  type: TYPE_NORMAL
- en: Data Pre-Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let’s load the data. We’ll be using the `diamonds` dataset to predict
    the price of a diamond based on its characteristics. A description of each variable
    can be found [here](https://ggplot2.tidyverse.org/reference/diamonds.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d90228d00c6b9949dc3daed72c3cf1b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by Author
  prefs: []
  type: TYPE_NORMAL
- en: To get our data ready for machine learning, we first need to create a vector
    of numerical features to use as input for our model. This means that if we have
    categorical variables, we need to one-hot encode them into numerical features;
    then, we put all of the features into one vector.
  prefs: []
  type: TYPE_NORMAL
- en: The MLlib library is a wrapper over PySpark that supports many machine learning
    algorithms for classification, regression, clustering, dimensionality reduction,
    and more. This library will really help with our data processing and machine learning
    needs!
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess Categorical Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[StringIndexer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html):
    This is essentially assigning a numeric value to each category (i.e.; Fair: 0,
    Ideal: 1, Good: 2, Very Good: 3, Premium: 4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[OneHotEncoder](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html):
    This converts categories into binary vectors. The result is a SparseVector that
    indicates which index from StringIndexer has the one-hot value of 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Assemble Feature Vector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In PySpark, we need to combine all of our features into one vector column. The
    feature vector column will serve as inputs to our machine-learning models. PySpark
    lets us easily create this vector using [VectorAssembler](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html).
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need to combine numerical features and the categorical sparse vector
    features from before into one vector.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we can run the stages as a pipeline. This runs the data through all the
    feature transformations we’ve defined so far.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Train-test Split
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Randomly split the dataset into train and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Post-split Data Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In linear regression, it is often recommended to standardize your features.
    PySpark’s [StandardScaler](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StandardScaler.html)
    achieves this by removing the mean (set to zero) and scaling to unit variance.
  prefs: []
  type: TYPE_NORMAL
- en: First, fit StandardScaler onto the training data. Then, use the scaler to transform
    the train and test data. The reason why you should fit on the train data is to
    [avoid data leakage](https://machinelearningmastery.com/data-preparation-without-data-leakage/)
    — when applying your model to the real world, you won’t know the distribution
    of your test data yet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Build and Train Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can import from MLlib to start building our model ([pyspark.ml.regression.**LinearRegression**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html)).
    We can start with the default parameter values and adjust these during model tuning.
    The default for some parameters to pay attention to are: `maxIter=100`, `regParam=0.0`,
    `elasticNetParam=0.0`, `loss=''squaredError’`. Then call `fit()` to fit the model
    to the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we need to develop predictions from our data using our new model. To
    get predictions, call `transform()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Why should you also fit your model on the train data?** Comparing your model
    evaluations on both the train and test data can help you identify if you’re overfitting
    or underfitting on your train data. If the evaluations for train and test are
    both similar, you may be underfitting. If your test evaluation is much lower than
    your train evaluation, you may be overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate your model, PySpark has [RegressionEvaluator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.RegressionEvaluator.html)
    at your service. You can choose whichever evaluation metric is most appropriate
    for your use case:'
  prefs: []
  type: TYPE_NORMAL
- en: RMSE — Root mean squared error (Default)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MSE — Mean squared error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R2 — R-squared
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAE — Mean absolute error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Var — Explained variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Train R2: 0.9204 | Test R2: 0.9142'
  prefs: []
  type: TYPE_NORMAL
- en: Analyze Feature Weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To pull out the linear regression weights and coefficients, run the following
    code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: It may be more helpful to match the weights to the feature names and plot them.
    This view is especially of high interest to key stakeholders who want to understand
    key drivers of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Above produces a Pandas dataframe containing the feature names and weights.
    Now let’s plot the weights.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/de201556cccebf06e2cd9cf9e3d7d354.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by Author
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Steps for implementing linear regression with PySpark:'
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encode categorical features using StringIndexer and OneHotEncoder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create input feature vector column using VectorAssembler
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split data into train and test
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale data using StandardScaler
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize and fit LinearRegression model on train data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform model on test data to make predictions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate model with RegressionEvaluator
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyze feature weights to understand and improve model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
