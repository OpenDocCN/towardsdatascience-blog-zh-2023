["```py\nini_squirrel_df = pd.read_csv('/content/drive/MyDrive/SquirrelML/NYC_Squirrels.csv')\n```", "```py\n# Function for Applying Clustering\ndef apply_clustering(df, num_clusters=4):\n    kmeans = KMeans(n_clusters=num_clusters, n_init='auto')\n    df['Cluster'] = kmeans.fit_predict(df[['X', 'Y']])\n    centroids = kmeans.cluster_centers_\n\n    # Assuming 'kmeans' is your trained KMeans model\n    joblib.dump(kmeans, '/content/drive/MyDrive/SquirrelML/squirrel_kmeans.pkl')\n\n    return df, centroids\n```", "```py\nnyc_map = gpd.read_file('geo_export_0a3d2fab-a76c-40da-bc23-6e335dd753dd.shp')\n```", "```py\ngeofeather.to_geofeather(nyc_map,'nyc_map.feather')\n```", "```py\nnyc_map = geofeather.from_geofeather('nyc_map.feather')\nnyc_map.plot()\n```", "```py\nfrom shapely.geometry import Point\n# Calculate the centroid for each geometry\nnyc_map['centroid'] = nyc_map.geometry.centroid\n```", "```py\n# Filter based on centroid's longitude and latitude\nfiltered_nyc_map = nyc_map[(nyc_map['centroid'].x >= -74.0) &\n                           (nyc_map['centroid'].x <= -73.94) &\n                           (nyc_map['centroid'].y >= 40.75) &\n                           (nyc_map['centroid'].y <= 40.82)]\n```", "```py\n# Plot the filtered DataFrame\nfiltered_nyc_map.plot(figsize=(10, 10), aspect='auto')\n```", "```py\n# Convert the 'X' and 'Y' columns into a GeoSeries of Points\npoints = gpd.GeoSeries([Point(xy) for xy in zip(squirrel_df['X'], squirrel_df['Y'])])\n\n# Create a new GeoDataFrame\nsquirrel_geo_df = gpd.GeoDataFrame(squirrel_df, geometry=points)\n\n# Plotting the filtered NYC map\nfig, ax = plt.subplots(figsize=(10, 10))\n\nfiltered_nyc_map.plot(ax=ax, color='#207388')  # Base map in light grey\n\n# Overlay the squirrel data points, color-coded by 'Cluster'\nsquirrel_geo_df.plot(ax=ax, column='Cluster', categorical=True, markersize=20,\n                     cmap='viridis', legend=True, legend_kwds={'title':'Cluster', 'loc': 'upper left'},\n                     facecolors='none' if True else 'black', edgecolors='black')\n\n# Adding labels and title\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nax.set_title('Squirrel Sightings in Central Park by Cluster')\nplt.show()\n```", "```py\n# Define lists of terms for various features\ntree_terms = ['tree', 'trees', 'maple', 'log', 'branch', 'oak', 'willow', 'trunk', 'stump', 'elm']\nshrubbery_terms = ['shrub', 'bush', 'weeds']\nrock_terms = ['rock', 'stone']\ngrassland_terms = ['lawn', 'grass', 'field', 'glen']\npath_terms = ['path', 'road', 'pavement', 'street']\nstructure_terms = ['conservatory', 'fountain', 'bench', 'fence', 'statue', 'bin', 'carousel', 'trellis',\n                  'structure', 'car', 'table', 'ledge', 'railing', 'overpass', 'post', 'can', 'house',\n                  'arch', 'bar', 'sanctuary', 'bridge', 'bike', 'rack', 'construction', 'playground']\nwater_terms = ['water', 'shore', 'pond', 'pool', 'stream']\n\n# Function to create a matching column based on specified terms\ndef create_matching_column(df, column_name, source_column, terms):\n    pattern = '|'.join(terms)\n    df[column_name] = df[source_column].str.contains(pattern, case=False, na=False)\n    return df\n```", "```py\n# Assuming squirrel_df is your DataFrame and 'Approaches' is the target variable\nX = squirrel_df.drop('Approaches', axis=1)  # Features\ny = squirrel_df['Approaches']  # Target\n\n# Stratify the split to maintain the class distribution in train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n```", "```py\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, average_precision_score, roc_auc_score\n\n# Define the parameter grid for Random Forest\nrf_param_grid = {\n    'n_estimators': [50, 100, 200, 500],  # Number of trees in the forest\n    'max_depth': [None, 5, 10, 20],  # Maximum depth of the tree\n    'min_samples_split': [2, 5, 7, 10],  # Minimum number of samples required to split a node\n    'min_samples_leaf': [1, 3, 5, 10]  # Minimum number of samples required at each leaf node\n}\n\n# Initialize Stratified K-Fold\nstratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Define multiple scoring metrics\nscoring_metrics = {\n    'PR-AUC': make_scorer(average_precision_score, needs_proba=True),\n    'ROC-AUC': 'roc_auc'\n}\n\n# Initialize the GridSearchCV object with Stratified K-Fold\nrf_grid_search = GridSearchCV(\n    estimator=RandomForestClassifier(random_state=42, class_weight='balanced'),\n    param_grid=rf_param_grid,\n    cv=stratified_kfold,  # Using Stratified K-Fold here\n    scoring=scoring_metrics,\n    refit='PR-AUC',\n    n_jobs=-1,\n    verbose=2\n)\n\nrf_grid_search.fit(X_train, y_train)\n\n# Best parameters and scores\nprint(\"Best parameters for Random Forest:\", rf_grid_search.best_params_)\nprint(\"Best PR-AUC score:\", rf_grid_search.best_score_)\n\n# To access the results for each metric\ncv_results = rf_grid_search.cv_results_\n\n# Find the index of the best score based on the refit criterion\nbest_index = rf_grid_search.best_index_\n\n# Display the best hyperparameters and corresponding scores\nprint(\"Best Hyperparameters:\", rf_grid_search.best_params_)\nprint(f\"Best Mean Test PR-AUC: {cv_results['mean_test_PR-AUC'][best_index]:.4f}\")\nprint(f\"Best Mean Test ROC-AUC: {cv_results['mean_test_ROC-AUC'][best_index]:.4f}\")\n```", "```py\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.tree  import DecisionTreeClassifier\nfrom sklearn.metrics import make_scorer, average_precision_score, roc_auc_score\n\n# Define the parameter grid for Decision Tree\ndt_param_grid = {\n    'max_depth': [10, 20, None],  # Maximum depth of the tree\n    'min_samples_split': [2, 5],  # Minimum number of samples required to split a node\n    'min_samples_leaf': [1, 2]  # Minimum number of samples required at each leaf node\n}\n\n# Initialize Stratified K-Fold\nstratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Define multiple scoring metrics\nscoring_metrics = {\n    'PR-AUC': make_scorer(average_precision_score, needs_proba=True),\n    'ROC-AUC': 'roc_auc'\n}\n\n# Initialize the GridSearchCV object with Stratified K-Fold\ndt_grid_search  = GridSearchCV(\n    estimator=DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n    param_grid=dt_param_grid,\n    cv=stratified_kfold,  # Using Stratified K-Fold here\n    scoring=scoring_metrics,\n    refit='PR-AUC',\n    n_jobs=-1,\n    verbose=2\n)\n\ndt_grid_search.fit(X_train, y_train)\n\n# Best parameters and scores\nprint(\"Best parameters for Decision Tree:\", dt_grid_search.best_params_)\nprint(\"Best PR-AUC score:\", dt_grid_search.best_score_)\n\n# To access the results for each metric\ndt_cv_results = dt_grid_search.cv_results_\n\n# Find the index of the best score based on the refit criterion\nbest_index = dt_grid_search.best_index_\n\n# Display the best hyperparameters and corresponding scores\nprint(\"Best Hyperparameters:\", dt_grid_search.best_params_)\nprint(f\"Best Mean Test PR-AUC: {dt_cv_results['mean_test_PR-AUC'][best_index]:.4f}\")\nprint(f\"Best Mean Test ROC-AUC: {dt_cv_results['mean_test_ROC-AUC'][best_index]:.4f}\")\n```", "```py\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import brier_score_loss\nfrom sklearn.calibration import calibration_curve\nimport matplotlib.pyplot as plt\n\n# Split your data into training and calibration sets\nX_train_2, X_calib, y_train_2, y_calib = train_test_split(X_train, y_train, test_size=0.2)\n\n# Fit the model on the training data\nbest_rf_classifier.fit(X_train_2, y_train_2)\n\n# Calibrate the model on the calibration data\n# Using Platt scaling (method='sigmoid') or Isotonic regression (method='isotonic')\ncalibrated_rf  = CalibratedClassifierCV(estimator=best_rf_classifier, method='isotonic', cv='prefit')\ncalibrated_rf.fit(X_calib, y_calib)\n\n# Now you can use calibrated_model to make predictions with calibrated probabilities\ncalibrated_probs = calibrated_rf.predict_proba(X_test)\n\n# Predict probabilities on the test set\nbrier_probs = calibrated_rf.predict_proba(X_test)[:, 1]\n\n# Evaluate calibration performance\nbrier_score = brier_score_loss(y_test, brier_probs)\nprint(f\"Brier score: {brier_score:.4f}\")\n\n# Get the calibrated probabilities for the positive class\nprob_true, prob_pred = calibration_curve(y_test, calibrated_probs[:, 1], n_bins=20)\n```", "```py\nimport joblib\n\n# Save the model to a file\njoblib.dump(calibrated_rf, '/content/drive/MyDrive/SquirrelML/cal_Squirrel_RF.pkl')\n```"]