- en: Exploding & Vanishing Gradient Problem in Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b](https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to ensure your neural network doesn‚Äôt ‚Äúdie‚Äù or ‚Äúblow-up‚Äù
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    ¬∑9 min read¬∑Dec 8, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ec02d8429b5e34801dd4ccb016d5e74.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
    title=‚Äùneural network icons.‚Äù Neural network icons created by Paul J. ‚Äî Flaticon.'
  prefs: []
  type: TYPE_NORMAL
- en: What are Vanishing & Exploding Gradients?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In one of my previous posts, we explained neural networks learn through the
    backpropagation algorithm. The main idea is that we start on the output layer
    and move or ‚Äúpropagate‚Äù the error all the way to the input layer updating the
    weights with respect to the loss function as we go. If you are unfamiliar with
    this, then I highly recommend you check that post:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Forward Pass & Backpropagation: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining how neural networks ‚Äútrain‚Äù and ‚Äúlearn‚Äù patterns in data by hand
    and in code using PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The weights are updated using their partial derivative with respect to the loss
    function. The problem is that these gradients get smaller and smaller as we approach
    the lower layers of the network. This leads to the lower layers‚Äô weights barely
    changing when training the network. This is known as the *vanishing gradient problem.*
  prefs: []
  type: TYPE_NORMAL
- en: The opposite can be true where gradients continue getting larger through the
    layers. This is the *exploding gradient problem* whichis mainly an issue in [***recurrent
    neural networks***](https://en.wikipedia.org/wiki/Recurrent_neural_network).
  prefs: []
  type: TYPE_NORMAL
- en: However, a [***paper***](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
    published by [***Xavier Glorot***](https://www.linkedin.com/in/xglorot/?originalSubdomain=ca)
    and [***Yoshua Bengio***](https://en.wikipedia.org/wiki/Yoshua_Bengio) in 2010
    diagnosed several reasons why this is happening to the gradients. The main culprits
    were the [***sigmoid activation function***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)and
    how weights are initialised (typically from the standard normal distribution).
    This combination leads to the variances changing between layers until they *saturate*
    at the extreme edges of the sigmoid function.
  prefs: []
  type: TYPE_NORMAL
- en: Below is the mathematical equation and plot of the sigmoid function. Notice
    that in its extremes, the gradient becomes zero. Therefore, no ‚Äúlearning‚Äù is done
    at these saturation points.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29fff662bec33adab7258d011d2e9db6.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid function. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b71b01559c120a8967c3bf80bc04c16f.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid function. Plot generated by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: We will now go through some techniques that can reduce the chance of our gradients
    vanishing or exploding during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to learn more about activation functions along with their pros
    and cons, check my previous post on the subject:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Activation Functions & Non-Linearity: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining why neural networks can learn (nearly) anything and everything
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----c8f48ec6a80b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Glorot/Xavier Initialisation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fortunately, the authors of the paper suggested methods to neutralise the above
    problem. They suggested a new initialisation method for the weights, named Glorot
    initialisation after the author, that ensures the variance between layers remains
    constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper linked above has the full mathematical details, but their proposed
    initialisation strategy was:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Normal Distribution**'
  prefs: []
  type: TYPE_NORMAL
- en: For a normal distribution*,* ***X ~ N(0, ùúé¬≤)****,* the weights will be initialised
    as follows*:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e33362e36072d25bec235c65713679a.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean and variance for Glorot normal distribution initialisation. Equation by
    author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: If ***n_in*** ***=*** ***n_out*** then we have [***LeCun initialisation***](https://wandb.ai/sauravmaheshkar/initialization/reports/A-Gentle-Introduction-To-Weight-Initialization-for-Neural-Networks--Vmlldzo2ODExMTg)named
    after the computer scientist [***Yann LeCun***](https://en.wikipedia.org/wiki/Yann_LeCun).
    This initialisation was proposed in the 1990s, a decade before Glorot‚Äôs paper.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/742f40143e3cdca52d66f0ba0d4d427b.png)'
  prefs: []
  type: TYPE_IMG
- en: LeCun initialisation. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: '**Uniform Distribution**'
  prefs: []
  type: TYPE_NORMAL
- en: For a uniform distribution ***X ~ U(-a, a)****:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a93c6c22f95680d5b3463ea4a9cbd8d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean and variance for Glorot uniform distribution initialisation. Equation by
    author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our neural networks using these initialisations leads them to converge
    faster since the weights are not too small or large at the beginning of training.
  prefs: []
  type: TYPE_NORMAL
- en: The above expressions are only valid for the sigmoid and [***tanh***](https://medium.com/towards-data-science/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)
    activation functions. For example, for the [***ReLU***](/breaking-linearity-with-relu-d2cfa7ebf264)(rectified
    linear unit) activation function, the normal distribution variance needs to be
    initialised using ***1/n_in.***
  prefs: []
  type: TYPE_NORMAL
- en: A full list of activation functions and their corresponding initialisations
    can be found [***here***](/weight-initialization-and-activation-functions-in-deep-learning-50aac05c3533)for
    the interested reader.
  prefs: []
  type: TYPE_NORMAL
- en: Better Activation Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For most industry standard neural networks, the sigmoid activation function
    has largely been abandoned and replaced with the ReLU as it doesn‚Äôt saturate for
    large positive values (it‚Äôs also more compute efficient):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad1cf21ebe2a1666034bee8102b78d29.png)'
  prefs: []
  type: TYPE_IMG
- en: ReLU function. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf6409fc078754bcab71802d9bbe477e.png)'
  prefs: []
  type: TYPE_IMG
- en: ReLU activation function. Plot generated by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: However, ReLU is not perfect and suffers from the [***dying ReLU problem***](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks)***.***
    This is where neurons start ‚Äúdying‚Äù as they only output zero because their inputted
    weighted sum is always negative. This leads to a zero gradient, so the network
    stops ‚Äúlearning‚Äù anything. This is particularly bad for a large learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'ReLU is easily applied in [***PyTorch***](https://pytorch.org/) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Leaky ReLU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can get around this by using variants of the classic ReLU function such
    as [***‚ÄòLeaky‚Äô ReLU***](https://www.educative.io/answers/what-is-leaky-relu#)
    where the negative inputs are not zero, but rather have some shallow slope with
    gradient ***Œ±***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5dbcbc68a24c8ab953e5b19704fabfa.png)'
  prefs: []
  type: TYPE_IMG
- en: Leaky ReLU function. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d44ec1c36473a7f01361080256878af.png)'
  prefs: []
  type: TYPE_IMG
- en: Leaky ReLU, notice the small gradient for negative x values. Plot generated
    by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: The Leaky ReLU often [outperforms](https://ai.stackexchange.com/questions/40576/why-use-relu-over-leaky-relu)
    the classic ReLU as it reduces the chance of this dying neuron problem, hence
    more robust learning. It is often found that a larger ‚Äúleak‚Äù is better but within
    reason.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other variants that also frequently improve performance over the
    basic ReLU :'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Randomised Leaky ReLU (RReLU)**](https://paperswithcode.com/method/rrelu):
    During training the hyperparameter ***Œ±*** is randomly initialised.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Parametric Leaky ReLU (PReLU):**](https://www.educative.io/answers/what-is-parametric-relu)During
    training the hyperparameter ***Œ±*** is learned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can apply Leaky ReLU in PyTorch as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Exponential Linear Unit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last activation function we will consider is the [***exponential linear
    unit***](https://paperswithcode.com/method/elu) (ELU).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0667299960e38ea18c78404225f18e0.png)'
  prefs: []
  type: TYPE_IMG
- en: ELU function. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1acad18951fc3a113560ef45efc7a38d.png)'
  prefs: []
  type: TYPE_IMG
- en: ELU, notice the exponential value for negative values. Plot generated by author
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key differences between ELU and ReLU are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*For negative values, the ELU function is not zero, so it alleviates the dying
    neuron problem with the vanilla ReLU.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Its gradient is non-zero for negative inputs.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ELU has a slower compute speed due to the exponential than ReLU and its variants.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The ELU generally leads to better performance than the ReLU*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ELU in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Which one to use?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With so many activation functions, it‚Äôs hard to know which one to choose. The
    general rule is ***ELU > Leaky ReLU > ReLU > tanh > sigmoid***. However, compute
    speed may play a factor in your model, so you may have to reconsider which activation
    you go for. Always best to consider your problem at hand and also play around
    with a few to see which one is best.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code used to generate these activation function plots is available at my
    GitHub here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Medium-Articles/Data Science Basics/activation_functions.py at main ¬∑ egorhowell/Medium-Articles'
  prefs: []
  type: TYPE_NORMAL
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----c8f48ec6a80b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Batch Normalisation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Glorot initialisation and the ReLU (and variants) activation function
    helps curtail the chance of vanishing/exploding gradients at the beginning of
    the algorithm, but doesn‚Äôt help during training.
  prefs: []
  type: TYPE_NORMAL
- en: One way to prevent the vanishing/exploding gradients during training is [***Batch
    Normalisation***](https://en.wikipedia.org/wiki/Batch_normalization#:~:text=Batch%20normalization%20%28also%20known%20as,%2Dcentering%20and%20re%2Dscaling.)
    (BN). This process zero centers and re-normalising the outputs or inputs just
    before or after the activation function is applied. Then, it shifts and scales
    this result allowing each layer to have its own ‚Äúlearned‚Äù mean and variance.
  prefs: []
  type: TYPE_NORMAL
- en: This is analogous to why we scale and normalise our features before training.
    It ensures everything is on an even playing field and large valued features won‚Äôt
    drown out the smaller ones. Batch Norm is applying this process after the output
    of each layer as they are inputs into the next!
  prefs: []
  type: TYPE_NORMAL
- en: Great [explanation here](/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)
    about why we need to normalise our features.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The algorithm looks like as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba0f4224ca3afadcc5641717efdf783f.png)'
  prefs: []
  type: TYPE_IMG
- en: Batch Normalisation algorithm. Equation generated in LaTeX by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '***x_i***‚Äã is the input to a batch normalisation layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Œº_B***‚Äã is the mean of the batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***œÉ¬≤_B***‚Äã is the variance of the batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***œµ*** is a small constant added for numerical stability (to avoid division
    by zero).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Œ≥*** is the scale parameter that is learned during training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Œ≤*** is the shift parameter that is learned during training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can walk through it to make it clearer. The first part is just calculating
    the mean and variance of the inputs or outputs of each layer in the network. These
    outputs are then normalised using the mean and variance. The final part is to
    scale, using ***Œ≥*** hyperparameter, and shift, using ***Œ≤*** hyperparameter***.***
    These hyperparameters are learned by the network, which is what makes BN so powerful.
    Each layer will have its custom transformation!
  prefs: []
  type: TYPE_NORMAL
- en: One other important thing to note is that whilst training BN keeps track of
    an [***exponential moving average***](https://en.wikipedia.org/wiki/Moving_average#:~:text=An%20exponential%20moving%20average%20%28EMA,decreases%20exponentially%2C%20never%20reaching%20zero.)
    (EMA) for both the mean and variance. This is used when predicting because you
    can‚Äôt really apply BN to a single prediction row, which is what happens during
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can apply BN in PyTorch as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we insert a batch norm layer in between all the hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalisation has been shown to improve the training of deep neural networks
    and reduce the impact of the vanishing gradients problem. However, training each
    epoch is slower as we need to pass hidden layers outputs through batch normalisation
    layers that increase the total number of parameters in the network.
  prefs: []
  type: TYPE_NORMAL
- en: Batch norm can also act like regulariser!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gradient Clipping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final method to reduce the chance of vanishing/exploding gradient is to
    clip the gradient at some maximum threshold. For example, we can clip the gradient
    at a maximum value of 1\. So, any gradient that is greater than that will be ‚Äúclipped‚Äù
    down to 1\. This technique is often applied to recurrent neural networks (RNN)
    as it is hard to apply batch norm to RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an example of how gradient clipping can be applied in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here `max_norm` is the threshold the gradients are clipped at.
  prefs: []
  type: TYPE_NORMAL
- en: Summary & Further Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vanishing and exploding gradients occur due to the variance change between
    neural network layers and gradients decreasing due to the multiplication effect
    as they are backpropagated. In this post, we discussed three methods to reduce
    the chance of this effect: better activation functions, batch normalisation, and
    gradient clipping. In my opinion, batch normalisation is probably the best option
    combined with a ReLU activation function. Batch normalisation ensures the variance
    across each layer is constant by normalising and scaling the inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Another Thing!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no ‚Äúfluff‚Äù
    or ‚Äúclickbait,‚Äù just pure actionable insights from a practicing Data Scientist.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----c8f48ec6a80b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Connect With Me!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**GitHub**](https://github.com/egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References & Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Andrej Karpathy Neural Network Course*](https://www.youtube.com/watch?v=i94OvYb6noo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*PyTorch site*](https://pytorch.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition.
    Aur√©lien G√©ron. September 2019\. Publisher(s): O‚ÄôReilly Media, Inc. ISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Paper on vanishing gradients study:* [*https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf*](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Great visual explanation of batch norm*](/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Great video on the vanishing gradient problem*](https://www.youtube.com/watch?v=8z3DFk4VxRo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
