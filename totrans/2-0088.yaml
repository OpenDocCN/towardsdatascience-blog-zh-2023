- en: 4 Reasons Why I Won’t Sign the “Existential Risk” New Statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/4-reasons-why-i-wont-sign-the-existential-risk-new-statement-ef658ec699ca](https://towardsdatascience.com/4-reasons-why-i-wont-sign-the-existential-risk-new-statement-ef658ec699ca)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Opinion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fueling fear is a dangerous game
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rafebrena.medium.com/?source=post_page-----ef658ec699ca--------------------------------)[![Rafe
    Brena, Ph.D.](../Images/6bf622a8ce9b3d06d1cb989fd8d625c6.png)](https://rafebrena.medium.com/?source=post_page-----ef658ec699ca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ef658ec699ca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ef658ec699ca--------------------------------)
    [Rafe Brena, Ph.D.](https://rafebrena.medium.com/?source=post_page-----ef658ec699ca--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ef658ec699ca--------------------------------)
    ·5 min read·May 31, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4e36f211c05017db500e8b00ea8f548.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Cash Macanaya](https://unsplash.com/@cashmacanaya?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: What is the new statement?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some weeks ago, [I published](https://medium.com/towards-data-science/why-i-signed-the-pause-giant-ai-experiments-petition-e9711f672d18)
    my pro and con arguments for signing that very well-known [open letter by the
    Future of Life Institute](https://futureoflife.org/open-letter/pause-giant-ai-experiments/?utm_source=pocket_saves)
    — in the end, I signed it, though there were some caveats. A few radio and TV
    hosts interviewed me to explain what all the fuss was about.
  prefs: []
  type: TYPE_NORMAL
- en: 'More recently, I got another email from the Future of Life Institute (FLI in
    the following) asking me to sign a declaration: this time, it was a short statement
    by the Center for AI Safety (CAIS) focused on the existential threats posed by
    recent AI developments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The statement goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: “**Mitigating the risk of extinction from AI should be a global priority alongside
    other societal-scale risks such as pandemics and nuclear war.”**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Very concise indeed; how could there be a problem with this?
  prefs: []
  type: TYPE_NORMAL
- en: Why is this bogus?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the previous FLI statement had weaknesses, this one doubles down on them
    instead of correcting them, making it impossible for me to support it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, I have the four following objections, which for sure are going
    to be a bit longer than the declaration itself:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. It misplaces the real risks of AI and distracts from them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The new statement is essentially a call to panic about AI, and not just to panic
    about some natural consequences of it that we can see right now, but instead about
    hypothetical risks that have been raised by random people who give very vague
    risk estimations like “10 percent risk of human extinction.”
  prefs: []
  type: TYPE_NORMAL
- en: Really? 10% risk of human extinction? Based on what? The survey respondents
    were not asked to justify or explain their reasons, but I suspect many were thinking
    about “Terminator-like” scenarios. You know, horror films are intended to scare
    you, so you go to the movies. But to translate the message to reality is not sound
    reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: The supposed threat to humanity assumes a capability to destroy us that hasn’t
    been explained and an agency—the willingness to erase humankind. How would a machine
    want to kill us when devices don’t have any feelings, be they good or bad? Machines
    don’t “want” this or that.
  prefs: []
  type: TYPE_NORMAL
- en: The real dangers of AI we see playing out right now are very different. One
    of them is the capability of Generative AI to make fake voices, pictures, and
    videos. Can you imagine what you’d do if you received a phone call with your daughter’s
    voice (impersonated with a fake voice) where she asks you to rescue her?
  prefs: []
  type: TYPE_NORMAL
- en: Another one is public misinformation with fake evidence, like counterfeit videos.
    The one with a fake Pope was relatively innocent, but shortly, Twitter will be
    flooded with false declarations, images about events that never occurred, and
    so on. By the way, have you considered that the US elections are approaching?
  prefs: []
  type: TYPE_NORMAL
- en: 'Then there is the exploitation of human-made content that AI algorithms are
    mining all over the internet to produce their “original” images and text: humans’
    work is taken without any financial compensation. In some instances, reference
    to human work is explicit, like in “make this image in the style of X.”'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Man vs. machine is not the right way of framing the risks of AI.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If in the FLI letter of a month ago there were suggestions of a “man vs. machine”
    mindset, it is made very explicit this time. “Extinction from AI,” they call it,
    nothing less.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the real world where we are living—not in apocalyptic Hollywood movies—it’s
    not the machines that damage us or are threatening our existence: it’s more like
    some humans (accidentally, the powerful and rich ones, the owners of big companies)
    leverage new powerful technology to increase their fortunes, and often at the
    expense of the powerless: we have seen how the availability of computer-generated
    graphics has shrunk the small business of graphic artists in places like Fiverr.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Further, the assumption that advanced machine intelligence would try to dethrone
    humans has to be questioned; as Steven Pinker wrote:'
  prefs: []
  type: TYPE_NORMAL
- en: “AI dystopias project a parochial alpha-male psychology onto the concept of
    intelligence. They assume that superhumanly intelligent robots would develop goals
    like deposing their masters or taking over the world.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Yann LeCun—the famous head of AI research at Meta—declared:'
  prefs: []
  type: TYPE_NORMAL
- en: “Humans have all kinds of drives that make them do bad things to each other,
    like the self-preservation instinct… Those drives are programmed into our brain,
    but there is absolutely no reason to build robots that have the same kind of drives.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'No, machines gone rogue will not become our overlords or exterminate us: other
    humans, who are currently our overlords, will increase their domination by leveraging
    the economic and technological means at their disposal—including AI if it’s fitting.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. The comparison of AI with pandemics and nuclear war is not valid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I get that the FLI mentioned the pandemics to relate their statement with something
    that we just lived—and left an emotional scar on many of us— but it’s not a valid
    comparison. Leaving aside some [conspiracy theories](https://journals.sagepub.com/doi/pdf/10.1177/1368430220982068),
    the pandemic we emerged from was not technology—vaccines were. How does the FLI
    assume catastrophic AI would spread? By contagion?
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, nuclear bombs are a technological development, but in the case of
    a nuclear war, we know precisely how and why the bomb would destroy us: it’s not
    speculation, as it is in the case of “rogue AI.”'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Leaders from the very companies bringing the risks are signing the statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One last item that drew my attention was to see the list of people signing the
    statement, starting with Sam Altman. He is the leader of OpenAI, which, with ChatGPT
    since November 2022, put the frantic AI race we live in motion. Even the mighty
    Google struggled to keep pace in this race—didn’t Microsoft’s Satya Nadella say
    he wanted to “make Google dance”? He got his wish at the cost of accelerating
    the AI race.
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t make sense to me that people at the helm of the very companies fueling
    this AI race are also signing this statement. Altman could say that he’s very
    worried by AI developments, but if we see his company keeps going straight at
    full speed, then his preoccupation looks meaningless and incongruous. I don’t
    intend to moralize about Altman’s declarations, but accepting his support at face
    value undermines the statement’s validity –even more so when we consider that
    for Altman’s company leading the race is essential to their financial bottom line.
  prefs: []
  type: TYPE_NORMAL
- en: Final thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s not that machines are going rogue. It’s the use that capitalistic monopolies
    and despotic governments make of AI tools that could damage us. And not in a Hollywood
    dystopic future, but in the real world where we are today.
  prefs: []
  type: TYPE_NORMAL
- en: I won’t endorse a fear-fueled vision of machines that is hypocritical in the
    end because it’s brought by the very companies that try to distract from their
    profit-seeking operating ways. That’s why I’m not signing this new statement endorsed
    by the FLI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Further, I suspect that wealthy and influential leaders can allow themselves
    to examine imaginary threats because they don’t worry about more “mundane” real
    threats like the income reduction of a freelance graphic artist: they know very
    well they will never struggle to make ends meet at the end of the month, nor will
    do their children or grandchildren.'
  prefs: []
  type: TYPE_NORMAL
