- en: Build a Transparent Question-Answering Bot for Your Documents with LangChain
    and GPT-3
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LangChain和GPT-3构建一个透明的文档问答机器人
- en: 原文：[https://towardsdatascience.com/build-a-transparent-question-answering-bot-for-your-documents-with-langchain-and-gpt-3-7f6a71f379f8](https://towardsdatascience.com/build-a-transparent-question-answering-bot-for-your-documents-with-langchain-and-gpt-3-7f6a71f379f8)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/build-a-transparent-question-answering-bot-for-your-documents-with-langchain-and-gpt-3-7f6a71f379f8](https://towardsdatascience.com/build-a-transparent-question-answering-bot-for-your-documents-with-langchain-and-gpt-3-7f6a71f379f8)
- en: Guide to developing an informative QA bot with displayed sources used
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发一个信息丰富的问答机器人指南，并显示所使用的来源
- en: '[](https://konstantin-rink.medium.com/?source=post_page-----7f6a71f379f8--------------------------------)[![Konstantin
    Rink](../Images/41bfc069d7382a0fd56f081eea7eb2d9.png)](https://konstantin-rink.medium.com/?source=post_page-----7f6a71f379f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7f6a71f379f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7f6a71f379f8--------------------------------)
    [Konstantin Rink](https://konstantin-rink.medium.com/?source=post_page-----7f6a71f379f8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://konstantin-rink.medium.com/?source=post_page-----7f6a71f379f8--------------------------------)[![Konstantin
    Rink](../Images/41bfc069d7382a0fd56f081eea7eb2d9.png)](https://konstantin-rink.medium.com/?source=post_page-----7f6a71f379f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7f6a71f379f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7f6a71f379f8--------------------------------)
    [Konstantin Rink](https://konstantin-rink.medium.com/?source=post_page-----7f6a71f379f8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7f6a71f379f8--------------------------------)
    ·11 min read·Jul 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----7f6a71f379f8--------------------------------)
    ·阅读时间11分钟·2023年7月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c24d8ab8c013ee7e02974a92cc9beff8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c24d8ab8c013ee7e02974a92cc9beff8.png)'
- en: Photo by [Justin Ha](https://unsplash.com/@mekanizm?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/de/fotos/XNn3SpMhiNE?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Justin Ha](https://unsplash.com/@mekanizm?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)拍摄，来源于[Unsplash](https://unsplash.com/de/fotos/XNn3SpMhiNE?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)。
- en: A Question Answering system can be of great help in analyzing large amounts
    of your data or documents. However, the sources (i.e., parts of your document)
    that the model used to create the answer are usually not shown in the final answer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 问答系统在分析大量数据或文档时可以大有帮助。然而，模型用来生成答案的来源（即文档的部分内容）通常不会在最终答案中显示。
- en: Understanding the context and origin of responses is valuable not only for users
    seeking accurate information, but also for developers wanting to continuously
    improve their QA bots. With the sources included in the answer, developers gain
    valuable insights into the model’s decision-making process, facilitating iterative
    improvements and fine-tuning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 理解回应的背景和来源不仅对寻求准确信息的用户有价值，也对希望持续改进其QA机器人（问答机器人的开发者）有帮助。通过回答中包含的来源，开发者可以深入了解模型的决策过程，从而促进迭代改进和微调。
- en: '**This article shows how to use LangChain and GPT-3 (text-davinci-003) to create
    a transparent Question-Answering bot that displays the sources used to generate
    the answer by using two examples.**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**本文展示了如何使用LangChain和GPT-3（text-davinci-003）创建一个透明的问答机器人，通过两个示例展示了生成答案时所使用的来源。**'
- en: In the first example, you’ll learn how to create a transparent QA bot that leverages
    your website’s content to answer questions. In the second example, we’ll explore
    the use of transcripts from different YouTube videos, both with and without timestamps.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例中，你将学习如何创建一个透明的QA机器人，利用你的网站内容回答问题。在第二个示例中，我们将探讨使用来自不同YouTube视频的转录文本，包括带有和不带有时间戳的文本。
- en: Process the Data and Create a Vector Store
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数据并创建向量存储
- en: Before we can leverage the capabilities of an LMM like GPT-3, we need to process
    our documents (e.g., website content or YouTube transcripts) in the correct format
    (first chunks, then embeddings) and store them in a vector store. Figure 1 below
    shows the process flow from left to right.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够利用像GPT-3这样的语言模型的能力之前，我们需要将文档（例如网站内容或YouTube转录文本）以正确的格式（首先分块，然后生成嵌入）处理，并将其存储在向量存储中。下图1展示了从左到右的处理流程。
- en: '![](../Images/aa6564c71496116719665fdfbd9b4f15.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa6564c71496116719665fdfbd9b4f15.png)'
- en: Figure 1\. Process flow of data processing and the creation of a vector store
    (image by author).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 数据处理和向量存储创建的流程图（图像来源于作者）。
- en: Website content example
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网站内容示例
- en: In this example, we’ll process the content of the web portal, [*It’s FOSS*](https://itsfoss.com/),
    which specializes in Open Source technologies, with a particular focus on Linux.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将处理网络门户[*It’s FOSS*](https://itsfoss.com/)的内容，该门户专注于开源技术，特别是Linux。
- en: First, we need to obtain a **list of all the articles** we wish to process and
    store in our vector store. The code below reads the *sitemap-posts.xml* file,
    which contains a list of links to all the articles.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要获取一个**所有待处理的文章列表**并存储在我们的向量存储中。下面的代码读取了*sitemap-posts.xml*文件，该文件包含了所有文章的链接列表。
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: At the time of this article, the list contains over 969 links to articles.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文撰写时，列表中包含了超过969个文章链接。
- en: With the list of links, we can now write a little **helper function** called
    `extract_content` that uses `BeautifulSoup` to extract specific elements from
    the article’s page containing the relevant content.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有了链接列表，我们现在可以编写一个小的**辅助函数**，称为`extract_content`，它使用`BeautifulSoup`从文章页面中提取包含相关内容的特定元素。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As a final step, we iterate over the list of links and apply our helper function
    `extract_content` to each URL. For demonstration purposes, I have limited the
    list to 10 elements. If you want to crawl all articles, simply remove `[0:10]`
    from `article_links[0:10]`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步，我们遍历链接列表，并将我们的辅助函数`extract_content`应用于每个URL。为了演示目的，我将列表限制为10个元素。如果你想爬取所有文章，只需从`article_links[0:10]`中删除`[0:10]`。
- en: The `articles` list now contains, for each article, a dictionary with the `"source"`
    (link to the article) and `"content"` (content of the article). The link to the
    article will be displayed later as the source in the final answer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`articles`列表现在包含每篇文章的字典，字典中包括`"source"`（文章链接）和`"content"`（文章内容）。文章的链接将在最终答案中作为来源显示。'
- en: Since GPT-3 comes with a **token limit** (4,096 tokens), it makes sense to split
    long articles into **chunks**. These chunks will later be combined with a prompt
    and sent to GPT-3.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GPT-3有一个**令牌限制**（4,096个令牌），因此将长文章分成**块**是有意义的。这些块将被组合在一起并发送给GPT-3。
- en: The code below splits up the content of the articles into several chunks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码将文章内容拆分成几个块。
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We use the `RecursiveCharacterTextSplitter` here because it aims to keep **semantically
    relevant content together** for as long as possible.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用`RecursiveCharacterTextSplitter`，因为它旨在尽可能长时间地**将语义相关的内容保持在一起**。
- en: Once this is done, all we have to do is execute the following line to store
    the articles and their sources in our vector store.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，我们只需执行以下命令，将文章及其来源存储到我们的向量存储中。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For this example, we use [FAISS](https://github.com/facebookresearch/faiss)
    as a **vector store** and `OpenAIEmbeddings` as our embedding model. Of course,
    it would also be possible to explore other options for vector storage, such as
    [Chroma](https://github.com/chroma-core/chroma), and for embedding models, try
    out solutions from Hugging Face.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用[FAISS](https://github.com/facebookresearch/faiss)作为**向量存储**，并使用`OpenAIEmbeddings`作为我们的嵌入模型。当然，也可以探索其他向量存储选项，例如[Chroma](https://github.com/chroma-core/chroma)，并尝试Hugging
    Face的嵌入模型解决方案。
- en: '**Note**: You can also store your vector store by running `article_store.save_local("your_name")`
    so you don’t have to recreate it every time you use it. See [here](https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss#saving-and-loading)
    for more details.'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意**：你还可以通过运行`article_store.save_local("your_name")`来保存你的向量存储，这样你就不必每次使用时都重新创建它。更多详情见[这里](https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss#saving-and-loading)。'
- en: In case you are **not interested in processing YouTube transcripts**, you can
    **skip the part below** and **proceed to the next section** “Run Transparent Question
    Answering”.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你**不想处理YouTube转录内容**，你可以**跳过下面的部分**并**继续到下一节**“运行透明问答”。
- en: YouTube transcript example
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YouTube转录示例
- en: The transcripts can be processed in two different and independent ways. The
    **first option** demonstrates how to process YouTube transcripts while **preserving
    the links to the videos** as sources (e.g., `[https://youtu.be/XYZ](https://youtu.be/XYZ).)`[).](https://youtu.be/XYZ).)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 转录内容可以通过两种不同且独立的方式处理。**第一个选项**演示了如何处理YouTube转录内容，同时**保留视频链接**作为来源（例如，[https://youtu.be/XYZ](https://youtu.be/XYZ)。）。
- en: The **second part** does the same but illustrates how to **preserve the links,**
    **including the timestamps** e.g., `https://youtu.be/XYZ&t=60`) for more granular
    information.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二部分**做了同样的事情，但说明了如何**保留链接**，**包括时间戳**（例如，`https://youtu.be/XYZ&t=60`）以获取更详细的信息。'
- en: 'For both ways, the transcripts of the following YouTube videos from the channel
    [StatQuest](https://www.youtube.com/@statquest/featured) are used:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种方法，使用了来自频道[StatQuest](https://www.youtube.com/@statquest/featured)的以下YouTube视频的转录：
- en: '[XGBoost Part 1 (of 4): Regression](https://youtu.be/OtD8wVaFm6E)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost 第1部分（共4部分）：回归](https://youtu.be/OtD8wVaFm6E)'
- en: '[XGBoost Part 2 (of 4): Classification](https://youtu.be/8b1JEDvenQU)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost 第2部分（共4部分）：分类](https://youtu.be/8b1JEDvenQU)'
- en: '[XGBoost Part 3 (of 4): Mathematical Details](https://www.youtube.com/watch?v=ZVFeW798-2I)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost 第3部分（共4部分）：数学细节](https://www.youtube.com/watch?v=ZVFeW798-2I)'
- en: '[XGBoost Part 4 (of 4): Crazy Cool Optimizations](https://youtu.be/oRrKeUCEbq8)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost 第4部分（共4部分）：疯狂酷炫的优化](https://youtu.be/oRrKeUCEbq8)'
- en: '**YouTube transcript example (without timestamps)**'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**YouTube 转录示例（不带时间戳）**'
- en: The first part is very straightforward. The code below utilizes LangChain’s
    DocumentLoader `YoutubeLoader`, which incorporates [youtube-transcript-api](https://pypi.org/project/youtube-transcript-api/)
    and [pytube](https://pytube.io/en/latest/).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分非常直接。下面的代码利用了 LangChain 的 DocumentLoader `YoutubeLoader`，它集成了[youtube-transcript-api](https://pypi.org/project/youtube-transcript-api/)和[pytube](https://pytube.io/en/latest/)。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To avoid any conflicts with the token limit, we split the data into several
    chunks by using the `CharacterTextSplitter`. The `add_video_info` is set to *True*
    to also **receive the title and author information** of the video.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免与令牌限制发生冲突，我们使用`CharacterTextSplitter`将数据拆分为几个块。`add_video_info`设置为*True*，以**接收视频的标题和作者信息**。
- en: The returned chunked transcripts are [document](https://api.python.langchain.com/en/latest/schema/langchain.schema.document.Document.html?highlight=document#langchain.schema.document.Document)
    objects. Before creating embeddings and storing them in a vector store, we manipulate
    or extend their metadata by adding information about the *title*, *author*, and
    *link to the video*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的分块转录是[文档](https://api.python.langchain.com/en/latest/schema/langchain.schema.document.Document.html?highlight=document#langchain.schema.document.Document)对象。在创建嵌入并将其存储在向量存储中之前，我们通过添加有关*标题*、*作者*和*视频链接*的信息来操作或扩展它们的元数据。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: YouTube transcript example (with timestamps)
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YouTube 转录示例（带时间戳）
- en: The second way is a bit **more sophisticated**. Here we retrieve the transcript
    with a different package called [youtube-transcript-api](https://pypi.org/project/youtube-transcript-api/).
    The output is a list of dictionaries containing the text, start time, and duration.
    We needed to switch to a different package here, as the `YoutubeLoader` package
    does not return the timestamps.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法稍微**复杂一些**。在这里，我们使用名为[youtube-transcript-api](https://pypi.org/project/youtube-transcript-api/)的不同包来检索转录。输出是一个包含文本、开始时间和持续时间的字典列表。我们需要切换到不同的包，因为`YoutubeLoader`包不返回时间戳。
- en: 'An example can be seen here:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 示例可以在这里看到：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Creating a document object out of each text entry would not make much sense,
    as the entries are **too short** (e.g., 8 words per entry in the example above)
    to be useful later. When searching in a vector store, only a limited number of
    matching documents (e.g., 4) are returned, and the information content is insufficient.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从每个文本条目创建文档对象没有多大意义，因为条目**太短**（例如，上述示例中每个条目只有8个单词），不便于后续使用。在向量存储中搜索时，只返回有限数量的匹配文档（例如，4个），信息内容不足。
- en: Therefore, we need to aggregate or join the text entries into a proper chunk
    of text first. The code snippet below contains a custom helper function.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要首先将文本条目聚合或合并成适当的文本块。下面的代码片段包含一个自定义助手函数。
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The helper applies **resampling** to adjust the frequency of the time **dimension
    to 3 minute steps**. In other words, it **merges the transcripts into 3-minute
    parts of text**. With this function in hand, we can now start fetching and processing
    the transcriptions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 该助手应用**重采样**来调整时间**维度为3分钟步长**。换句话说，它**将转录合并为3分钟的文本部分**。有了这个功能，我们现在可以开始获取和处理转录。
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: An excerpt of the outcome can be seen in the figure below.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的摘录可以在下图中看到。
- en: '![](../Images/9b617d92c19e8202d6e68ff670ca554d.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b617d92c19e8202d6e68ff670ca554d.png)'
- en: Figure 2\. Excerpt of transcripts_df (image by author).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. 转录数据框的摘录（图片由作者提供）。
- en: Since the merged 3-minute parts could now cause issues with the token limits,
    we need to process them with a **splitter** again before generating embeddings
    and storing them in our vector store.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于合并的 3 分钟部分现在可能会导致令牌限制问题，我们需要在生成嵌入并将其存储在向量存储中之前，使用**分割器**再次处理它们。
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Run Transparent Question Answering
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行透明问答
- en: With our filled vector store, we now can focus on the **transparent question
    answering**. The figure below gives an overview of the process.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有了填充的向量存储后，我们现在可以专注于**透明问答**。下面的图给出了该过程的概述。
- en: '![](../Images/011b144a50c651d7977825ab220ddd86.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/011b144a50c651d7977825ab220ddd86.png)'
- en: Figure 3\. Overview of Transparent Question Answering Process (image by author).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 透明问答过程概述（图片由作者提供）。
- en: 'We start by **defining a question**, which is then **converted** by the embedding
    model or API **into an embedding**. The vector store utilizes this question embedding
    to search for ’n’ (default: 4) **similar documents or chunks** in the storage.
    Subsequently, the content of **each document or chunk is combined with a prompt**
    and sent to GPT-3.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先**定义一个问题**，然后由嵌入模型或API**转换**为嵌入。向量存储利用这个问题嵌入来搜索存储中的’n’（默认：4）**相似文档或片段**。随后，将**每个文档或片段的内容与提示组合**并发送到GPT-3。
- en: The results returned from GPT-3 are then **combined with another prompt** in
    a **final step and sent back to GPT-3** once more to obtain the final answer,
    including sources.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 返回的结果然后**与另一个提示组合**，在**最后一步再次发送回 GPT-3**以获取最终答案，包括来源。
- en: Website content example
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网站内容示例
- en: Before using `RetrievalQAWithSourcesChain`, we ensure our bot **memorizes**
    previous conversations by implementing a memory. This enhances contextually relevant
    interactions with users.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`RetrievalQAWithSourcesChain`之前，我们确保通过实施记忆使我们的机器人**记住**之前的对话。这增强了与用户的上下文相关互动。
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To integrate previous chat history into the used prompts, we need to modify
    the existing template.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将之前的聊天记录整合到使用的提示中，我们需要修改现有的模板。
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Afterward, we can utilize the `RetrievalQAWithSourcesChain` to ask questions.
    In this example, we set **k=4**, which means we would query the vector store for
    the 4 most similar documents.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以利用 `RetrievalQAWithSourcesChain` 来提问。在这个示例中，我们设置**k=4**，这意味着我们将查询向量存储以获取4个最相似的文档。
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The result is returned as a dictionary:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 结果以字典形式返回：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can observe that the result contains the sources used to answer the given
    question. To generate this final answer, the API was called **5 times**: 4 times
    to extract relevant information from the 4 most similar chunks, and 1 additional
    time to produce the conclusive answer.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，结果包含了回答给定问题所用的来源。为了生成这个最终答案，API 被调用了**5次**：4次提取最相似的4个片段中的相关信息，额外1次生成最终答案。
- en: We can also ask questions referring to the **previous question**.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以提出引用**之前问题**的问题。
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The outcome would then look as follows.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将如下所示。
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Keep in mind that for this questions the API was also called 5 times.**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**请记住，对于这些问题，API 也被调用了5次。**'
- en: YouTube transcript example (with and without timestamps)
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YouTube 转录示例（有时间戳和没有时间戳）
- en: The code for the YouTube transcript example looks quite similar to the one for
    the website. First, we initialize the ConversationBufferMemory and create a custom
    question prompt template.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube 转录示例的代码与网站的代码非常相似。首先，我们初始化 ConversationBufferMemory，并创建一个自定义问题提示模板。
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Then we create the QA chain with sources.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建带有来源的 QA 链。
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let’s ask a question.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们问一个问题。
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The result for the example without timestamps:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 不带时间戳的示例结果：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The result for the example with timestamps:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 带时间戳的示例结果：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The combination of LangChain’s RetrievalQAWithSourcesChain and GPT-3 is excellent
    for enhancing the transparency of Question Answering. As the process figure illustrates
    (figure 3), **multiple calls** to OpenAI are necessary to obtain the final answer.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 的 RetrievalQAWithSourcesChain 和 GPT-3 的结合非常适合提高问答的透明度。正如过程图所示（图 3），获取最终答案需要**多次调用**
    OpenAI。
- en: Depending on your service usage and the number of similar documents you need
    to process, the number of calls can increase, **resulting in higher costs**. It’s
    definitely worth keeping an eye on that. However, for your hobby projects, this
    shouldn’t be too critical. To keep a better eye on the costs and sent prompts,
    one could consider using [Promptlayer](https://promptlayer.com/home) or [TruLens](https://medium.com/towards-artificial-intelligence/evaluate-and-monitor-the-experiments-with-your-llm-app-df391c0f51c9).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你使用服务的情况和需要处理的类似文档数量，调用次数可能会增加，**导致更高的费用**。这确实值得关注。不过，对于你的爱好项目来说，这不应该太关键。为了更好地关注费用和发送的提示，可以考虑使用
    [Promptlayer](https://promptlayer.com/home) 或 [TruLens](https://medium.com/towards-artificial-intelligence/evaluate-and-monitor-the-experiments-with-your-llm-app-df391c0f51c9)。
- en: 'The Colab notebooks can be found here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Colab 笔记本可以在这里找到：
- en: '[Website example](https://github.com/darinkist/Medium-Article-Transparent-Question-Answering-Bot/blob/main/CodeForArticleWebsiteExample.ipynb)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[网站示例](https://github.com/darinkist/Medium-Article-Transparent-Question-Answering-Bot/blob/main/CodeForArticleWebsiteExample.ipynb)'
- en: '[YouTube example](https://github.com/darinkist/Medium-Article-Transparent-Question-Answering-Bot/blob/main/CodeForArticleYouTubeExample.ipynb)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[YouTube 示例](https://github.com/darinkist/Medium-Article-Transparent-Question-Answering-Bot/blob/main/CodeForArticleYouTubeExample.ipynb)'
- en: Sources
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资料来源
- en: '**LangChain RetrievalQAWithSourcesChain API documentation** [https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain.html](https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LangChain RetrievalQAWithSourcesChain API 文档** [https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain.html](https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain.html)'
- en: '*Owners or creators have been asked in advance whether I am allowed to use
    their content/data as examples for this article.*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有者或创作者已经提前询问是否允许我将他们的内容/数据用作本文的示例。*'
- en: It’s FOSS., “It’s FOSS”, [https://itsfoss.com/](https://itsfoss.com/)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: It’s FOSS., “It’s FOSS”, [https://itsfoss.com/](https://itsfoss.com/)
- en: 'StatQuest. “XGBoost Part 1 (of 4): Regression” *YouTube,* Joshua Starmer, 16
    Dec. 2019, [https://youtu.be/OtD8wVaFm6E](https://youtu.be/OtD8wVaFm6E) .'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatQuest. “XGBoost 第1部分（共4部分）：回归” *YouTube,* Joshua Starmer, 2019年12月16日, [https://youtu.be/OtD8wVaFm6E](https://youtu.be/OtD8wVaFm6E)。
- en: 'StatQuest. “XGBoost Part 2 (of 4): Classification” *YouTube,* Joshua Starmer,
    13 Jan. 2020, [https://youtu.be/8b1JEDvenQU](https://youtu.be/8b1JEDvenQU) .'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatQuest. “XGBoost 第2部分（共4部分）：分类” *YouTube,* Joshua Starmer, 2020年1月13日, [https://youtu.be/8b1JEDvenQU](https://youtu.be/8b1JEDvenQU)。
- en: 'StatQuest. “XGBoost Part 3 (of 4): Mathematical Details” *YouTube,* Joshua
    Starmer, 10 Feb. 2020, [https://youtu.be/ZVFeW798-2I](https://youtu.be/ZVFeW798-2I)
    .'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatQuest. “XGBoost 第3部分（共4部分）：数学细节” *YouTube,* Joshua Starmer, 2020年2月10日,
    [https://youtu.be/ZVFeW798-2I](https://youtu.be/ZVFeW798-2I)。
- en: 'StatQuest. “XGBoost Part 4 (of 4): Crazy Cool Optimizations” *YouTube,* Joshua
    Starmer, 02 Mar. 2020, [https://youtu.be/oRrKeUCEbq8](https://youtu.be/oRrKeUCEbq8)
    .'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatQuest. “XGBoost 第4部分（共4部分）：疯狂炫酷的优化” *YouTube,* Joshua Starmer, 2020年3月2日,
    [https://youtu.be/oRrKeUCEbq8](https://youtu.be/oRrKeUCEbq8)。
