- en: The Power of the Dot Product in Artificial Intelligence
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 点积在人工智能中的力量
- en: 原文：[https://towardsdatascience.com/the-power-of-the-dot-product-in-artificial-intelligence-c002331e1829](https://towardsdatascience.com/the-power-of-the-dot-product-in-artificial-intelligence-c002331e1829)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-power-of-the-dot-product-in-artificial-intelligence-c002331e1829](https://towardsdatascience.com/the-power-of-the-dot-product-in-artificial-intelligence-c002331e1829)
- en: How a simple tool gives rise to astonishing complexity
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单工具如何产生惊人的复杂性
- en: '[](https://manuel-brenner.medium.com/?source=post_page-----c002331e1829--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----c002331e1829--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c002331e1829--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c002331e1829--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----c002331e1829--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://manuel-brenner.medium.com/?source=post_page-----c002331e1829--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----c002331e1829--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c002331e1829--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c002331e1829--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----c002331e1829--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c002331e1829--------------------------------)
    ·9 min read·May 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c002331e1829--------------------------------)
    ·阅读时间 9 分钟·2023年5月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Taking something simple and scaling it up to achieve something more complex
    is a powerful idea, laying the foundation of [life](https://manuel-brenner.medium.com/the-importance-of-noise-327fcab7c4fb),
    computers (based on the simplicity of the [Turing machine](https://medium.com/discourse/a-non-technical-guide-to-turing-machines-f8c6da9596e5)),
    and deep learning techniques (based on the idea of the neurons in a neural network).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 将简单的事物放大以实现更复杂的目标是一个强大的理念，这一理念奠定了[生命](https://manuel-brenner.medium.com/the-importance-of-noise-327fcab7c4fb)、计算机（基于[Turing
    机](https://medium.com/discourse/a-non-technical-guide-to-turing-machines-f8c6da9596e5)的简单性）以及深度学习技术（基于神经网络中神经元的理念）的基础。
- en: '[More is frequently different](https://manuel-brenner.medium.com/more-is-different-a49e833260b3),
    which we continue observing as state-of-the-art architectures are increasingly
    scaled (with large language models like [GPT](https://arxiv.org/abs/2005.14165)
    changing the world as we speak), leading to better generalization and impressive
    results.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[更多往往不同](https://manuel-brenner.medium.com/more-is-different-a49e833260b3)，我们继续观察到随着最先进的架构不断扩展（像[GPT](https://arxiv.org/abs/2005.14165)这样的语言模型正在改变世界），这导致了更好的泛化和令人印象深刻的结果。'
- en: Researchers claim that large language models showcase [emergent abilities](https://openreview.net/pdf?id=yzkSU5zdwD)
    that are achieved without too much architectural innovation, but (arguably) mostly
    from an increase of computational resources and the repetition of simple operations
    at scale (for a caveat, see [this article](https://arxiv.org/abs/2304.15004)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员声称，大型语言模型展示了[涌现能力](https://openreview.net/pdf?id=yzkSU5zdwD)，这些能力无需过多的架构创新，而（可以说）主要来自计算资源的增加和大规模重复简单操作（有关警告，请参见[这篇文章](https://arxiv.org/abs/2304.15004)）。
- en: Although training deep learning models requires ingenuity and techniques that
    the community has built up over years, the fundamental building blocks of most
    deep learning approaches remain quite simple, being composed of only a small number
    of mathematical operations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管训练深度学习模型需要社区多年积累的独创性和技术，但大多数深度学习方法的基本构建块仍然相当简单，仅由少量的数学操作组成。
- en: Perhaps the most important one is the dot product. In the rest of this article,
    I want to explore what the dot product does, why it is so important, and why scaling
    it up has achieved levels of AI that astonish everyone that comes into closer
    contact with it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最重要的是点积。在本文的其余部分，我想深入探讨点积的作用、它为何如此重要，以及为何将其规模化已实现令所有与之接触的人感到惊讶的人工智能水平。
- en: '![](../Images/00f9a2a7d5ef38fedd5289f12d30fc1b.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00f9a2a7d5ef38fedd5289f12d30fc1b.png)'
- en: Artificial Intelligence as the product of dots (DALL-E might have misunderstood
    me a little but it still looks cool).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能作为点的产物（DALL-E 可能有点误解了我，但它仍然看起来很酷）。
- en: Imagine you are working at a company like Netflix, and are tasked with picking
    out movies to recommend to millions of different users. Every user has his/her
    own unique taste, and there are millions of movies to select from, so it’s quite
    the daunting task.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你在像Netflix这样的公司工作，并被分配了选择电影以推荐给数百万不同用户的任务。每个用户都有自己独特的口味，而电影的选择有数百万部，因此这是一项相当艰巨的任务。
- en: One way to tackle this problem is to list a couple of different properties of
    the movies, such as how much the movie aligns with certain genres (e.g. to rate
    how much of an action film or an emotionally taxing drama it is). One could also
    list other kinds of information, such as its comedy content, some external factors
    like its length, whether it was produced by Marvel or Disney, and so on and so
    on. The list of all of the movies’ properties can then be put into a vector.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是列出电影的几个不同属性，例如电影与特定类型（例如评估它是动作片还是情感戏剧）的对齐程度。还可以列出其他信息，例如喜剧内容、一些外部因素，如电影长度、是否由Marvel或Disney制作等。然后将所有电影的属性列表放入一个向量中。
- en: A simple way of getting an idea of a user’s taste is to just take a look at
    that person’s favorite movie(s), or a list of the last couple of movies the user
    watched, which leads to a second vector that gives us an idea of the user’s taste.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 了解用户口味的一个简单方法是直接查看该用户最喜欢的电影或最近观看的几部电影，这将产生第二个向量，从而给我们提供用户口味的一个概念。
- en: New movies can then simply be recommended by computing how similar they are
    to the user’s favorite movies, according to some kind of similarity measure in
    the vector space of the movie’s properties.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以通过计算新电影与用户最喜欢的电影的相似度来简单地推荐新电影，这些相似度是根据电影属性的向量空间中的某种相似度度量来计算的。
- en: This is where the dot product, AI’s favorite similarity measure, enters the
    stage.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是点积，AI最喜爱的相似度度量，登场的地方。
- en: 'The dot product is a simple mathematical operation that measures the similarity
    between two vectors. Mathematically, the dot product of two vectors x and y is
    calculated as:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 点积是一个简单的数学运算，用于测量两个向量之间的相似度。数学上，两个向量x和y的点积计算公式为：
- en: x · y = Σ (x_i * y_i) for i = 1 to n
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`x · y = Σ (x_i * y_i) for i = 1 to n`'
- en: where x_i and y_i are the components of vectors x and y, respectively, and n
    is the dimension of the vectors. It can also be written as
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `x_i` 和 `y_i` 分别是向量x和y的分量，n是向量的维度。它也可以写作：
- en: x · y = |x| * |y| * cos(θ)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`x · y = |x| * |y| * cos(θ)`'
- en: where θ is the angle between the two vectors and || denotes their length. In
    this interpretation, the dot product measures the extent to which x and y are
    aligned. If the two vectors are parallel, the dot product corresponds to their
    length. If the vectors are perpendicular to each other, it goes to zero.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其中θ是两个向量之间的角度，||表示它们的长度。在这种解释下，点积测量了x和y的对齐程度。如果两个向量平行，则点积对应于它们的长度。如果向量彼此垂直，则点积为零。
- en: '![](../Images/a9b8065e1560712994dc2bcb34a33232.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9b8065e1560712994dc2bcb34a33232.png)'
- en: BenFrantzDale at the English Wikipedia, CC BY-SA 3.0
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: BenFrantzDale 在英文维基百科，CC BY-SA 3.0
- en: 'The dot product is often combined with the cosine similarity, which normalizes
    the dot product by the magnitudes of the vectors, providing a similarity measure
    that is invariant to their magnitudes:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 点积通常与余弦相似度结合使用，后者通过向量的大小对点积进行标准化，提供一个对大小不变的相似度度量：
- en: cosine_similarity(y, x) = (y · x) / (|y| * |x|)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`cosine_similarity(y, x) = (y · x) / (|y| * |x|)`'
- en: Geometrically, the dot product can be thought of as the projection of one vector
    onto another. When you compute the dot product of two vectors x and y, you can
    think of it as projecting vector x onto vector y (or vice versa) and then multiplying
    the length of the projection by the length of vector y.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何上讲，点积可以被视为一个向量在另一个向量上的投影。当你计算两个向量x和y的点积时，可以把它看作是将向量x投影到向量y（或反之），然后将投影的长度乘以向量y的长度。
- en: 'Projections are an important concept, and have a nice interpretation: they
    can be thought of as dimension-wise comparisons of different features that the
    vectors encode.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 投影是一个重要概念，并且有一个很好的解释：它们可以被认为是向量所编码的不同特征的维度比较。
- en: 'Returning to our earlier example, consider two movies, Movie A and Movie B.
    For the sake of simplicitiy, let’s say these a characterized by a two-dimensional
    vector based on two distinct properties: Action Intensity (AI) and Emotional Depth
    (ED) (usually, these vectors can of course be much larger). The first component
    of the vector represents Action Intensity, and the second component represents
    Emotional Depth. Let’s say both properties are measured on a scale of -5 to 5,
    with -5 being the least intense or emotionally deep and 5 being the most intense
    or emotionally deep.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 回到之前的例子，考虑两部电影，电影A和电影B。为了简化起见，假设这两部电影通过两个不同的属性来表征：动作强度（AI）和情感深度（ED）（通常，这些向量当然可以大得多）。向量的第一个组件代表动作强度，第二个组件代表情感深度。假设这两个属性的测量范围是-5到5，其中-5表示最不强烈或最不深刻，5表示最强烈或最深刻。
- en: One movie, a relatively shallow popcorn entertainment action film, has an Action
    Intensity of 4 and an Emotional Depth of -3\. Its vector representation would
    be A = [4, -3].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一部电影，一部相对浅显的爆米花娱乐动作片，其动作强度为4，情感深度为-3。它的向量表示为A = [4, -3]。
- en: Movie B is a 4 hour black and white meditation on the Serbian government, [seen
    through the eyes of a pigeon](https://knowyourmeme.com/memes/film-bros-when-you-tell-them-you-want-to-watch-a-marvel-movie).
    It is quite emotionally demanding but does not have a lot of action scenes, scoring
    a rating of B = [-4, 4].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 电影B是一部4小时的黑白冥想片，关于塞尔维亚政府，[从鸽子的视角](https://knowyourmeme.com/memes/film-bros-when-you-tell-them-you-want-to-watch-a-marvel-movie)看待。它在情感上相当要求，但没有很多动作场景，评分为B
    = [-4, 4]。
- en: We can compare both movies using the dot product, evaluating to
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过点积来比较这两部电影，计算结果为
- en: A · B = (4 * -4) + (-3 * 4)= -28
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: A · B = (4 * -4) + (-3 * 4)= -28
- en: Normalizing by the size of both vectors, this gives
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过两向量的大小进行归一化，这得到了
- en: -28/(sqrt(25)*sqrt(32))= -0.99.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: -28/(sqrt(25)*sqrt(32))= -0.99。
- en: As would be expected, the two movies are extremely dissimilar, so their cosine
    (dis)similarity score is close to -1.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，这两部电影的相似度极低，因此它们的余弦（不）相似性得分接近-1。
- en: So why is this simple operation of comparing similarity of two vectors so important
    in AI?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么这个简单的比较两个向量相似性的操作在AI中如此重要？
- en: Machine learning, and maybe at its heart (artificial) intelligence more generally,
    relies on comparing patterns and measuring their similarity on an enormous scale.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，也许更广泛地说，(人工)智能的核心，依赖于在庞大的规模上比较模式并衡量其相似性。
- en: However, it’s not quite trivial to come up with patterns to compare.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，提出用于比较的模式并非易事。
- en: The part I glossed over in the earlier example is how to come up with the labels
    for the movies. The ratings Action Intensity and Emotional Depth are not straightforward
    to extract from the ground-truth data of the movie, but are human assessments,
    who perform the complex cognitive (subjective) task of watching the movie and
    transforming it into a significantly lower-dimensional representation in which
    it becomes apparent that the movies are quite different. From the perspective
    of the pixel space of the movies (for a 2 hour movie in 4K, these are something
    like 1.4*10¹² pixels), this is a tremendous dimensionality reduction that includes
    individual biases and priors.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我在之前的示例中略过的一部分是如何为电影制定标签。动作强度和情感深度并不是从电影的真实数据中直接提取的，而是由人类评估得出的，他们进行复杂的认知（主观）任务——观看电影并将其转化为明显低维的表示，从中可以看出这些电影的差异。从电影的像素空间的角度来看（对于一部4K的2小时电影，这些像素大约是1.4*10¹²），这是一种巨大的维度降低，其中包含个人的偏见和先验知识。
- en: The concept of complex nonlinear transformations is a second important ingredient
    necessary for making dot products powerful. It transforms inputs into representations
    in which comparing their similarity becomes meaningful in the context of the respective
    learning problem, such as a classification task or regression problem.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂非线性变换的概念是使点积强大的第二个重要因素。它将输入转换为在相应学习问题的背景下比较其相似性变得有意义的表示，例如分类任务或回归问题。
- en: Combining nonlinear transformation with dot products is at the heart of what
    neural networks do. Here the dot product is used in calculating the weighted sum
    of inputs in each neuron which, combined with a nonlinear transformation (the
    activation function, such as the tanh or ReLU function, plus the learned weights
    of the network), constitute the basic mechanism by which all neural networks operate.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将非线性变换与点积相结合是神经网络核心功能的一部分。在这里，点积用于计算每个神经元中输入的加权和，这与非线性变换（激活函数，如tanh或ReLU函数，加上网络的学习权重）结合，构成了所有神经网络运行的基本机制。
- en: This combination becomes even clearer in the context of kernel functions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种组合在核函数的背景下变得更加清晰。
- en: A kernel function allows us to compare the similarity of points in a new, transformed
    space. It does so by computing a dot product between the transformed data points
    in a (usually higher-dimensional) projection space. The neat thing is that the
    “kernel trick” allows this computation to be performed without explicitly calculating
    the transformation, which can save a lot of resources, and finds application in
    Support Vector Machines (SVMs), which, combined with the kernel trick, constitutes
    one of the most powerful and influential algorithms in recent decades.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数使我们能够比较新变换空间中点的相似性。它通过计算在（通常是更高维度的）投影空间中变换后的数据点之间的点积来实现这一点。巧妙之处在于，“核技巧”允许在不显式计算变换的情况下进行这一计算，这可以节省大量资源，并在支持向量机（SVMs）中找到应用，结合核技巧，构成了近年来最强大和最具影响力的算法之一。
- en: '![](../Images/3a21754c52d3435b783d0cfd3741c690.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a21754c52d3435b783d0cfd3741c690.png)'
- en: 'Kernel machines combine a nonlinear transformation to find a feature space
    in which the class labels are easily separable. Original: Alisneaky Vector: Zirguezi,
    CC BY-SA 4.0'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '核机器结合了非线性变换，以找到一个特征空间，在这个空间中类别标签容易被分开。原作者：Alisneaky Vector: Zirguezi, CC BY-SA
    4.0'
- en: 'These basic insights can be connected to the hottest models on the market right
    now: large language models.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基本见解可以与当前市场上的最热门模型连接起来：大型语言模型。
- en: 'A standard task for a large language model is the translation of a sentence
    between two languages, say between English and German:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的一个标准任务是翻译两种语言之间的句子，比如在英语和德语之间：
- en: “I am reading an article about the importance of the dot product in AI.”
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我正在阅读一篇关于点积在人工智能中重要性的文章。”
- en: ''
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “Ich lese einen Artikel über die Bedeutung des Skalarproduktes für die KI.”
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “Ich lese einen Artikel über die Bedeutung des Skalarproduktes für die KI。”
- en: Both sentences carry approximately the same meaning but are significantly different
    in their representation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 两个句子的含义大致相同，但它们的表示方式却显著不同。
- en: The translation task can be framed as finding a nonlinear transformation of
    the words that correspond to approximately the same location in the latent semantic
    space that captures their “meaning”. The quality of the translation can then be
    measured by the achieved similarity.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译任务可以被表述为找到单词的非线性变换，这种变换对应于潜在语义空间中大致相同的位置，从而捕捉它们的“意义”。翻译的质量可以通过实现的相似度来衡量。
- en: If “measuring similarity” doesn’t make your spidey senses tingle by now, I haven’t
    done a good job with this article.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果“测量相似性”还没有让你产生强烈的兴趣，那么我在这篇文章中的表现还不够好。
- en: And indeed, the dot product shows up at the heart of transformer models, which
    have become the foundation of modern natural language processing (NLP) and many
    other machine-learning tasks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，点积在变压器模型的核心中出现，这些模型已经成为现代自然语言处理（NLP）以及许多其他机器学习任务的基础。
- en: The self-attention mechanism is a key component of transformers. Self-attention
    allows the model to weigh the importance of different input elements with respect
    to each other. This allows them to capture long-range dependencies and complex
    relationships in the data. In the self-attention mechanism, the dot product is
    used in calculating the attention scores and forming context-aware representations
    of the input elements.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制是变压器模型的一个关键组成部分。自注意力机制使模型能够权衡不同输入元素之间的重要性。这使得模型能够捕捉数据中的长程依赖关系和复杂关系。在自注意力机制中，点积被用来计算注意力得分和形成上下文感知的输入元素表示。
- en: 'The input elements (usually embeddings/tokenized versions of the input text)
    are first linearly projected into three different spaces: Query (Q), Key (K),
    and Value (V) using separate learned weight matrices. This results in three sets
    of vectors for each input element: query vectors, key vectors, and value vectors.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 输入元素（通常是输入文本的嵌入/分词版本）首先被线性投影到三个不同的空间：查询（Q）、键（K）和值（V），使用各自的学习权重矩阵。这会为每个输入元素生成三组向量：查询向量、键向量和值向量。
- en: The dot product is then used to compute attention scores between each pair of
    query and key vectors (score_ij = q_i · k_j).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 点积随后用于计算每对查询向量和键向量之间的注意力分数（score_ij = q_i · k_j）。
- en: This measures the similarity between the query and key vectors, which determines
    how much attention the model pays to each input element with respect to all the
    other elements.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这衡量了查询向量和键向量之间的相似性，决定了模型对每个输入元素相对于其他所有元素的关注程度。
- en: 'After computing all similarity scores, the scores are scaled and sent through
    a ***similarity function***, and can then be used to compute a context function,
    which is again a simple sum over attention scores and values: (context_i = Σ (attention_ij
    * v_j))'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算所有相似性分数后，分数会被缩放并通过一个***相似性函数***，然后可以用来计算上下文函数，该函数又是注意力分数和值的简单总和：(context_i
    = Σ (attention_ij * v_j))
- en: '[The usual choice for the similarity function is the softmax, which can be
    thought of as a kernel function](https://arxiv.org/pdf/2112.04035.pdf), a nonlinear
    transformation that enables the comparison between elements, and that estimates
    which elements might be useful for prediction. Other kernel functions are possible,
    depending on the problem at hand. In a more fundamental sense, transformers can
    be viewed as kernel machines (more precisely as [Deep Infinite-Dimensional Non-Mercer
    Binary Kernel Machines](https://arxiv.org/abs/2106.01506), as this paper discusses).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[通常选择的相似性函数是softmax，可以看作是一种核函数](https://arxiv.org/pdf/2112.04035.pdf)，它是一种非线性变换，可以在元素之间进行比较，并估计哪些元素可能对预测有用。根据具体问题，也可以使用其他核函数。从更根本的角度来看，变换器可以被视为核机器（更准确地说是[深度无限维非墨瑟二元核机器](https://arxiv.org/abs/2106.01506)，如本文讨论）。'
- en: As with the other examples, the dot product, combined with nonlinear transformations
    of the input and output text and projections, defines the self-attention mechanism.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他示例一样，点积与输入和输出文本的非线性变换及投影结合，定义了自注意力机制。
- en: I hope I could convince you that the dot product plays a pivotal role in AI,
    especially in its most recent instantiations. While in some specific contexts,
    other distance/similarity measures between vectors are more appropriate (Euclidian,
    Chebyshev, Mahalanobis distance, etc.), the dot product is probably the most widely
    used across a wide range of applications, and constitutes the fundamental building
    block for quantifying similarity, relationships, and dependencies within all kinds
    of data. When combined with nonlinear transformations, the dot product sits at
    the heart of a wide array of algorithms and models, from Support Vector Machines
    to neural networks to the self-attention mechanism in Transformers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望能够说服你，点积在人工智能中扮演了关键角色，尤其是在其最新的实例中。虽然在某些特定背景下，其他向量之间的距离/相似性度量（如欧几里得距离、切比雪夫距离、马哈拉诺比斯距离等）可能更为合适，但点积可能是最广泛使用的，并且构成了量化数据中相似性、关系和依赖性的基础构件。当与非线性变换结合时，点积处于各种算法和模型的核心，从支持向量机到神经网络，再到Transformers中的自注意力机制。
- en: With the impressive strides AI has been taking in recent years, I find it fascinating
    to reflect on the underlying simplicity and adaptability of the operations that
    make it up.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能近年来取得的显著进展，我发现反思构成其基础的操作的简单性和适应性非常有趣。
- en: 'It can at times be a little scary to see how effective this strategy has been:
    [in a recent podcast](https://www.youtube.com/watch?v=VcVfceTsD0A), Max Tegmark
    commented on the surprising simplicity/stupidity behind training large language
    models (just give them loads of text and make them predict the next words/sentences
    using transformers with self-attention). In some ways, [intelligence might be
    simpler than we think](/why-intelligence-might-be-simpler-than-we-think-1d3d7feb5d34),
    as most who have played with ChatGPT confirm.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有时看到这种策略的有效性可能有些令人害怕：[在最近的播客中](https://www.youtube.com/watch?v=VcVfceTsD0A)，Max
    Tegmark 评论了训练大型语言模型背后令人惊讶的简单性/愚蠢性（只需给它们大量文本，并使用自注意力的变换器预测下一个单词/句子）。在某些方面，[智能可能比我们想象的更简单](/why-intelligence-might-be-simpler-than-we-think-1d3d7feb5d34)，正如大多数与
    ChatGPT 互动过的人所确认的那样。
- en: 'This has important implications: computers are very good at doing something
    very simple reliably and fast and scaling it up to the Nth degree. With the promise
    of Moore’s law, we will continue scaling these models.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这具有重要的意义：计算机非常擅长以可靠和快速的方式执行一些非常简单的任务，并将其扩展到Nth级别。随着摩尔定律的承诺，我们将继续扩展这些模型。
- en: These are fascinating and possibly dangerous times ahead, and it’s likely that
    the dot product will sit right at their center.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是令人着迷且可能危险的时期，很可能点积将位于其中的核心。
