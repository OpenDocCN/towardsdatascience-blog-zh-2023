# 机器学习基础（第4部分）：决策树

> 原文：[https://towardsdatascience.com/ml-basics-part-4-decision-trees-cc37d07137b2](https://towardsdatascience.com/ml-basics-part-4-decision-trees-cc37d07137b2)

## **什么是决策树，如何构建和应用决策树于不同的分类任务**

[](https://azad-wolf.medium.com/?source=post_page-----cc37d07137b2--------------------------------)[![J. Rafid Siddiqui, PhD](../Images/02280890ed87239c75cbcbfa7c5d686c.png)](https://azad-wolf.medium.com/?source=post_page-----cc37d07137b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cc37d07137b2--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cc37d07137b2--------------------------------) [J. Rafid Siddiqui, PhD](https://azad-wolf.medium.com/?source=post_page-----cc37d07137b2--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cc37d07137b2--------------------------------) ·8分钟阅读·2023年1月4日

--

![](../Images/854204edb68a795f1f9cc7135d68d19a.png)

图1：决策树分类器（来源：作者）

在之前的文章中，我们探讨了[*回归*](https://azad-wolf.medium.com/ml-basics-part-1-regression-a-gateway-method-to-machine-learning-36d54d233907)、[*支持向量机*](https://azad-wolf.medium.com/ml-basics-part-2-support-vector-machines-ac4defba2615)*以及* [*人工神经网络*](https://azad-wolf.medium.com/ml-basics-part-3-artificial-neural-networks-879851bcd217)*。在本文中，我们将介绍另一种机器学习概念——*决策树*。您可以通过上述链接查看其他方法。

**介绍**

决策树也许是机器学习工具箱中最简单和最直观的分类方法之一。决策树的首次出现是在*威廉·贝尔森*于1959年的一篇出版物中。早期对决策树的使用主要限于分类法，因为它们与这种数据类型的自然相似性。后来，决策树分类器的适应版本应运而生，它们不再局限于特定的数据类型（例如，名义型数据），也适用于其他数据类型（例如，数值型数据）。决策树是一种简单却有效的工具，用于许多分类问题，在某些情况下，它们可以超越其他更复杂的方法，这些复杂的方法对于简单的数据集可能显得过于繁琐。

**决策节点、约束和叶节点**

决策树——顾名思义，是一种树形数据结构，具有一组决策节点和一组在有向图中连接的边。决策节点包含决策标准（即约束），决定从节点到树中其他节点的出边路径。最常见的决策树是二叉树，每个决策节点只有两条边，但也可以有多于两条边的树（例如，基于名义数据构建的树）。约束可以简单到由一个变量组成（例如，**X₁** **≤** C），也可以由多个特征的线性组合组成（例如，**X₁** **≤ X** **₂ +** C）。同样，约束也可以是非线性的。树的叶节点包含分类标签。这意味着，当我们从训练数据集遍历构建的树并到达一个叶节点时，我们会得到测试数据点的分类标签。在图2中，我们可以看到一个示例数据集以及从该数据中学习到的决策树。

![](../Images/754897976473b07e3a965f21b4121f14.png)

图2：左侧：来自两个类别的一组随机数据点形成的簇，右侧：从数据中学习到的决策树图示——数字仅用于后续参考标记（来源：作者）

**学习树**

正如前一节所提到的，决策树是一组以树形排列的决策约束，其中叶节点为特定数据实例提供分类。最重要的步骤是找出我们应该以树形排列的最佳约束。我们通过在每个节点找到最佳约束来做到这一点，这样可以为其子节点提供最佳的数据拆分。更具体地说，在任何给定节点，我们选择一个特征，然后通过迭代地遍历该特征在训练集中的所有变量值来生成一组候选约束。例如，如果我们选择一个变量**X₀**，那么候选约束将通过将该变量与该变量的所有值进行比较来生成（即，**X₀ ≤ C**）。其中C是该变量在训练集中的常数值。

![](../Images/46b1c8f589e09f34d25e47e4bd0ee408.png)

图3：数据簇、决策树以及数据拆分的约束可视化（来源：作者）

对所有特征变量重复相同的过程，获得所有候选约束。为了找出最佳约束，我们在该节点的数据上应用该约束，得到两个数据子集：一个是左分支，满足该约束，另一个是右分支，否定该约束。我们现在需要一个度量来评估拆分的效果。换句话说，我们想知道拆分数据是否提高了找到真实数据标签的机会。为此，我们使用熵度量。

![](../Images/ae3ad3326d1535abe356fb4745bd12ee.png)

图4：熵的方程式（来源：作者）

熵是系统中不确定性的度量。在信息理论中，熵是数据中存在的信息量（惊讶/不确定性）。例如，一个表示随机变量结果的二元集**X ∈** [1,1,1,1,1,1]的熵为零。这意味着它包含最少量的信息，因此我们对该集合中所有未来发生的事件的值都很确定。然而，对于值集**X ∈** [1,1,1,0,0,0]，熵为*1.0*，这意味着它包含最多的信息，我们完全不确定新事件的值是什么。

现在我们测量每个数据分割中分类标签的熵，并将其与父节点的熵进行比较。更具体地说，我们计算一个叫做*信息增益*的量度，它测量通过观察另一个随机变量的结果来获得关于一个随机变量的信息量。在决策树的背景下，这意味着相对于父节点，子节点的熵减少。因此，它可以通过从父节点的熵中减去子节点的熵来计算。如果有多个子节点，则从父节点的熵中减去子节点熵的加权平均值。权重可以通过计算左子节点和右子节点相对于父节点的概率来计算。

![](../Images/21919f25a9df3cc289c0cbb3e46328b6.png)

图5：信息增益计算（来源：作者）

我们为每个候选约束计算*信息增益*，并选择具有最大*信息增益*值的约束。直观地说，我们试图找到那些在类别标签方面变化最小的数据分割。我们一直这样做，直到到达叶子节点，我们希望最终得到只有一个类别的分类标签。然而，根据我们构建的树的深度，叶子节点可能没有单一的类别标签，在这种情况下，我们将取数据分割中多数类别的类别标签作为最终分类标签。在图3中，你可以看到树的构建过程，包括最佳约束、数据分割和叶子节点的分类标签。

![](../Images/4f5ae8937f0bbae556a242a4dfa315dd.png)

图6：决策树算法（来源：作者）

**带线性约束的决策树**

使用前面描述的过程，我们可以使用*numpy*在Python中构建决策树，并将其应用于各种类型的数据。我们从创建一个随机数据集开始，并将其分割为训练集和测试集，如图7所示。

![](../Images/2123f86b9c1753899773e10652aca5e0.png)

图7：一组随机数据点分割为训练集和测试集（来源：作者）

我们可以在训练集上训练一个决策树分类器。从训练集构建的决策树如图8所示。

![](../Images/6fd311c87895323e1d499730f1efd97a.png)

图 8: 单变量决策树分类器与示例预测（来源：作者）

一旦构建了树，我们可以使用它来预测测试点。例如，X=(0.5,-0.5) 的一个实例在遍历树后将被预测为属于‘0’类。预测过程仅包含一组应用于数据点的条件。训练树也是一个简单直观的过程，但对于大型数据集，它可能变得耗时且繁琐。

**具有非线性约束的决策树**

在前面的部分，我们对线性可分数据应用了决策树分类器。然而，在大多数情况下，数据并不是线性可分的。例如，图 9 中的数据就不是线性可分的。

![](../Images/750a8fabca60be967e9eb2c25b31f985.png)

图 9: 非线性数据示例（来源：作者）

我们可以像以前一样使用单个变量来构建树，但仅使用正交约束会在准确找到决策边界时遇到问题。因此，我们将从变量的线性组合中构建约束。基于这些约束学习到的决策树如图 10 所示。请注意，以这种方式，我们将每个约束建模为一条直线，并可以找到倾斜的决策边界。与使用单变量约束构建的决策树相比，这种方法对相同数据的准确分类所需的树深度也较少。

![](../Images/6721da8de1ca74c2b4a8b3d79cabd98e.png)

图 10: 用于非线性数据的多变量决策树（来源：作者）

在大多数情况下，使用线性约束构建的决策树已经足够。因为多个线性约束可以准确地创建非线性决策边界。然而，在某些情况下，你可能希望尝试非线性约束以获得更精确的分类。具有非线性约束的决策树如图 11 所示。

![](../Images/4eb482af4c11a806e712b1c4cc98e7ef.png)

图 11: 具有非线性约束的决策树（来源：作者）

决策树的结果，无论是多变量线性约束还是非线性约束，都可以在图 12 中看到。正如你可能注意到的，两种分类边界之间几乎没有差别。这是因为，使用多变量线性约束构建的决策树分类器本身就是一个非线性分类器。然而，对于分类性能可能有显著差异的其他数据集，使用这种分类器可能值得尝试。

![](../Images/c635f844af8ff483a8850478cc9112b2.png)

图 12: 左侧: 多变量线性约束的分类，右侧: 非线性约束的分类（来源：作者）

**结论**

在这篇文章中，你了解了决策树以及如何构建决策树。你还学会了如何对不同数据类型应用决策树分类器。决策树分类器可能是机器学习工具箱中最简单和最直观的分类器。然而，在某些情况下，这就是获得准确分类所需的一切。你可以在以下的 GitHub 仓库中找到使用 *numpy* 实现决策树的代码。

**代码：**

[https://www.github.com/azad-academy/MLBasics-DecisionTrees](https://www.github.com/azad-academy/MLBasics-DecisionTrees)

**成为 Patreon 支持者：**

[https://www.patreon.com/azadacademy](https://www.patreon.com/azadacademy)

**在 Substack 上找到我：**

[https://azadwolf.substack.com](https://azadwolf.substack.com)

**关注 Twitter 以获取更新：**

[https://twitter.com/azaditech](https://twitter.com/azaditech)
