- en: All Languages Are NOT Created (Tokenized) Equal
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/all-languages-are-not-created-tokenized-equal-cd87694a97c1](https://towardsdatascience.com/all-languages-are-not-created-tokenized-equal-cd87694a97c1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Language models cost much more in some languages than others
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@artfish?source=post_page-----cd87694a97c1--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----cd87694a97c1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cd87694a97c1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cd87694a97c1--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----cd87694a97c1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cd87694a97c1--------------------------------)
    ·12 min read·May 3, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5d82c8ce5ba774594d82455e802916d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: “hey” translated to 52 different languages. The size of the text is scaled to
    corresponds to the number of tokens needed to represent the message in the corresponding
    language. Image created by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '*Original article was posted on my* [*blog*](https://www.artfish.ai/p/all-languages-are-not-created-tokenized)*.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Large language models such as ChatGPT process and generate text sequences by
    first splitting the text into smaller units called **tokens**. In the image below,
    each colored block represents a unique token. Short or common words such as “you”,
    “say”, “loud”, and “always” are its own token, whereas longer or less common words
    such as “atrocious”, “precocious”, and “supercalifragilisticexpialidocious” are
    broken into smaller subwords.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17d355ca37bbda6bda1137a1956d5f38.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Visualization of tokenization of a short text using [OpenAI’s tokenizer website](https://platform.openai.com/tokenizer).
    Screenshot taken by author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: This process of **tokenization** is not uniform across languages, leading to
    disparities in the number of tokens produced for equivalent expressions in different
    languages. For example, **a sentence in Burmese or Amharic may require 10x more
    tokens than a similar message in English.**
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77a9be17c8b6729603e33808832bd519.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: An example of the same message translated into five languages and the corresponding
    number of tokens required to tokenize that message (using OpenAI’s tokenizer).
    The text comes from [Amazon’s MASSIVE dataset](https://www.amazon.science/blog/amazon-releases-51-language-dataset-for-language-understanding).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I explore the tokenization process and how it varies across
    different languages:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of token distributions in a parallel dataset of short messages that
    have been translated into 52 different languages
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some languages, such as Armenian or Burmese, require **9 to 10 times more tokens
    than English** to tokenize comparable messages
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些语言，如亚美尼亚语或缅甸语，标记长度是英语的**9 到 10 倍**，以标记类似的消息
- en: The impact of this language disparity
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言差异的影响
- en: '**This phenomenon is not new to AI** — this is consistent with what we observe
    in Morse code and computer fonts'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**这一现象在 AI 中并不新鲜**——这与我们在摩尔斯电码和计算机字体中观察到的情况一致'
- en: Try it yourself!
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亲自尝试一下！
- en: '[Try out the exploratory dashboard I made, available on HuggingFace spaces](https://huggingface.co/spaces/yenniejun/tokenizers-languages).
    Here, you can compare the token lengths for different languages and for different
    tokenizers (which was not explored in this article, but which I explore the reader
    to do on their own).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[尝试一下我在 HuggingFace Spaces 上制作的探索性仪表板](https://huggingface.co/spaces/yenniejun/tokenizers-languages)。在这里，你可以比较不同语言和不同
    Tokenizers 的标记长度（这在本文中没有探讨，但我鼓励读者自行探索）。'
- en: '![](../Images/ef69e8025dc85f2213a7476d2a23f453.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef69e8025dc85f2213a7476d2a23f453.png)'
- en: Screenshot of the language tokenizers dashboard.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 语言 Tokenizers 仪表板的截图。
- en: Data
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: '[MASSIVE](https://arxiv.org/abs/2204.08582) is a parallel dataset [introduced
    by Amazon](https://github.com/alexa/massive) consisting of 1 million realistic,
    parallel short texts translated across 52 languages and 18 domains. I used the
    `dev` split of the dataset, which consists of **2033 texts translated into each
    of the languages**. The dataset is [available on HuggingFace](https://huggingface.co/datasets/AmazonScience/massive)
    and is licensed under the [CC BY 4.0 license](https://huggingface.co/datasets/AmazonScience/massive/blob/main/massive.py).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[MASSIVE](https://arxiv.org/abs/2204.08582) 是一个由 [Amazon](https://github.com/alexa/massive)
    介绍的平行数据集，包含 100 万个现实的、跨 52 种语言和 18 个领域的短文本翻译。我使用了数据集的 `dev` 切分，包含**2033 个翻译成每种语言的文本**。该数据集
    [可在 HuggingFace 上获取](https://huggingface.co/datasets/AmazonScience/massive) 并根据
    [CC BY 4.0 许可证](https://huggingface.co/datasets/AmazonScience/massive/blob/main/massive.py)
    进行授权。'
- en: A focus on OpenAI’s Tokenizers
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专注于 OpenAI 的 Tokenizers
- en: 'While many other language model tokenizers exist, this article mainly focuses
    on [OpenAI’s Byte Pair Encoding (BPE) tokenizer](https://platform.openai.com/tokenizer)
    (used by ChatGPT and GPT-4) for three main reasons:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然存在许多其他语言模型 Tokenizer，但本文主要关注 [OpenAI 的 Byte Pair Encoding (BPE) Tokenizer](https://platform.openai.com/tokenizer)（ChatGPT
    和 GPT-4 使用的）有三个主要原因：
- en: First, [Denys Linkov’s article](https://denyslinkov.medium.com/why-is-gpt-3-15-77x-more-expensive-for-certain-languages-2b19a4adc4bc)
    compared several tokenizers and found that GPT-2’s tokenizer had the highest token
    length disparity among different languages. This prompted me to concentrate on
    OpenAI models, including GPT-2 and its successors.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，[Denys Linkov 的文章](https://denyslinkov.medium.com/why-is-gpt-3-15-77x-more-expensive-for-certain-languages-2b19a4adc4bc)
    比较了几种 Tokenizer，并发现 GPT-2 的 Tokenizer 在不同语言之间的 Token 长度差异最大。这促使我专注于 OpenAI 模型，包括
    GPT-2 及其后续版本。
- en: Second, since we lack insight into ChatGPT’s full training dataset, investigating
    OpenAI’s black box models and tokenizers help to better understand their behaviors
    and outputs.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，由于我们缺乏对 ChatGPT 完整训练数据集的深入了解，调查 OpenAI 的黑箱模型和 Tokenizers 有助于更好地理解它们的行为和输出。
- en: Finally, the widespread adoption of ChatGPT in various applications (from language
    learning platforms like [Duolingo](https://blog.duolingo.com/duolingo-max/) to
    social media apps like [Snapchat](https://newsroom.snap.com/say-hi-to-my-ai))
    highlights the importance of understanding tokenization nuances to ensure equitable
    language processing across diverse linguistic communities.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，ChatGPT 在各种应用中的广泛采用（从语言学习平台如 [Duolingo](https://blog.duolingo.com/duolingo-max/)
    到社交媒体应用如 [Snapchat](https://newsroom.snap.com/say-hi-to-my-ai)）突显了理解 Tokenization
    细微差别的重要性，以确保在不同语言社区中公平的语言处理。
- en: To calculate the number of tokens a text contains, I use the `cl100k_base` tokenizer
    available on [tiktoken](https://github.com/openai/tiktoken), which is the BPE
    tokenizer used by OpenAI’s ChatGPT models (`gpt-3.5-turbo` and `gpt-4`).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算文本包含的 Token 数量，我使用了 [tiktoken](https://github.com/openai/tiktoken) 上的 `cl100k_base`
    Tokenizer，这是 OpenAI ChatGPT 模型（`gpt-3.5-turbo` 和 `gpt-4`）使用的 BPE Tokenizer。
- en: Results
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: Some languages consistently tokenize to longer lengths
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些语言的标记长度始终较长
- en: The following distribution plot compares the distribution of token lengths for
    five languages. The curve for English is tall and narrow, meaning that English
    texts consistently tokenize to a smaller number of tokens. On the other hand,
    the curve for languages such as Hindi and Burmese are short and wide, meaning
    that these languages tokenize texts into many more tokens.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下分布图比较了五种语言的标记长度分布。英语的曲线高而窄，这意味着英语文本通常被标记为较少的标记数。另一方面，像印地语和缅甸语这样的语言的曲线短而宽，这意味着这些语言的文本标记数要多得多。
- en: '![](../Images/ad96619815dd74d0591c968737b50f6e.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad96619815dd74d0591c968737b50f6e.png)'
- en: Distribution of token lengths for all 2033 messages and 52 languages. Five of
    the languages have been bolded and colored; the rest are shown in gray. Figure
    created by author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所有2033条消息和52种语言的标记长度分布。五种语言被加粗并着色，其余的用灰色显示。图表由作者创建。
- en: English has the shortest median token length
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 英语的中位数标记长度最短
- en: For each language, I calculated the median token length for all of the texts
    in the dataset. The following chart compares a subset of the languages. English
    texts had the smallest median length of 7 tokens and Burmese texts had the largest
    median length of 72 tokens. Romance languages such as Spanish, French, and Portuguese
    tended to result in a similar number of tokens as English.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种语言，我计算了数据集中所有文本的中位数标记长度。以下图表比较了部分语言。英语文本的中位数长度最小，为7个标记，而缅甸语文本的中位数长度最大，为72个标记。西班牙语、法语和葡萄牙语等罗曼语族语言通常产生的标记数与英语相似。
- en: '![](../Images/e09f188755d7a54c4697575672b3362a.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e09f188755d7a54c4697575672b3362a.png)'
- en: A subset of the 52 languages and their median token length. Figure created by
    author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 52种语言的一个子集及其标记长度的中位数。图表由作者创建。
- en: As English had the shortest median token length, I calculated the ratio of the
    other languages’ median token length to that of English. Languages such as Hindi
    and Bengali (over 800 million people speak either of these languages) resulted
    in a median token length of about 5 times that of English. The ratio is 9 times
    that of English for Armenian and over 10 times that of English for Burmese. In
    other words, **to express the same sentiment, some languages require up to 10
    times more tokens**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于英语的中位数标记长度最短，我计算了其他语言的中位数标记长度与英语的比例。像印地语和孟加拉语这样的语言（有超过8亿人讲这两种语言）其标记长度约为英语的5倍。亚美尼亚语的比例是英语的9倍，缅甸语则超过了英语的10倍。换句话说，**为了表达相同的情感，一些语言需要多达10倍的标记数**。
- en: '![](../Images/db10fc30f4280e3a990c81316c801cba.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db10fc30f4280e3a990c81316c801cba.png)'
- en: A subset of the 52 languages and the ratio of that language’s median token length
    to that of English. Figure created by author.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 52种语言的一个子集及这些语言的中位数标记长度与英语的中位数标记长度之比。图表由作者创建。
- en: Discussion
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论
- en: Implications of tokenization language disparity
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记语言差异的影响
- en: 'Overall, requiring more tokens (to tokenize the same message in a different
    language) means:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，需要更多标记（在不同语言中标记相同的信息）意味着：
- en: You’re limited by how much information you can put in the prompt (because the
    context window is fixed). As of March 2023, GPT-3 could take up to 4K tokens and
    GPT-4 could take up to 8K or 32K tokens in its input [[1](#footnote-1)]
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你受限于可以在提示中放入的信息量（因为上下文窗口是固定的）。截至2023年3月，GPT-3最多可以处理4K个标记，而GPT-4最多可以处理8K或32K个标记
    [[1](#footnote-1)]
- en: It costs more money
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本更高
- en: It takes longer to run
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行时间更长
- en: OpenAI’s models are increasingly being used in countries where English is not
    the dominant language. According to SimilarWeb.com, the United States only accounted
    for 10% of the traffic sent to ChatGPT in Jan-March 2023.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的模型在英语不是主流语言的国家越来越多地被使用。根据SimilarWeb.com的数据，美国在2023年1月至3月仅占ChatGPT流量的10%。
- en: '![](../Images/6f74a6266c72c05e5887bf3aff7d588f.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f74a6266c72c05e5887bf3aff7d588f.png)'
- en: Top 5 countries sending the most traffic to chat.openai.com in Jan-March 2023\.
    Sourced from [similarweb.com](https://www.similarweb.com/website/chat.openai.com/#traffic)
    on May 2, 2023\. Screenshot taken by author.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年1月至3月，向chat.openai.com发送流量最多的前五个国家。数据来源于[similarweb.com](https://www.similarweb.com/website/chat.openai.com/#traffic)，数据截取于2023年5月2日。截图由作者拍摄。
- en: Additionally, ChatGPT was used [in Pakistan to grant bail in a juvenile kidnapping
    case](https://interestingengineering.com/culture/pakistani-court-utilizes-chatgpt-4-to-grant-bail)
    and [in Japan for administrative tasks](https://www.japantimes.co.jp/news/2023/04/20/national/chatgpt-yokosuka-trial/).
    As ChatGPT and similar models are becoming increasingly integrated into products
    and services worldwide, it is crucial to understand and address such inequalities.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ChatGPT曾被用于[在巴基斯坦为一桩青少年绑架案件申请保释](https://interestingengineering.com/culture/pakistani-court-utilizes-chatgpt-4-to-grant-bail)和[在日本进行行政任务](https://www.japantimes.co.jp/news/2023/04/20/national/chatgpt-yokosuka-trial/)。随着ChatGPT和类似模型在全球产品和服务中的日益融合，理解和解决这些不平等现象至关重要。
- en: Language Disparity in Natural Language Processing
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理中的语言差异
- en: This digital divide in natural language processing (NLP) is an active area of
    research. 70% of research papers published in a computational linguistics conference
    only evaluated English.[2](#footnote-2) Multilingual models perform worse on several
    NLP tasks on low resource languages than on high resource languages such as English.[3](#footnote-3)
    According to [W3Techs](https://w3techs.com/) (World Wide Web Technology Surveys),
    English dominates more than half (55.6%) of the content on the Internet.[4](#footnote-4)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）中的数字鸿沟是一个活跃的研究领域。70%的计算语言学会议上发表的研究论文仅评估了英语。[2](#footnote-2) 多语言模型在低资源语言上的NLP任务表现不如在高资源语言（如英语）上的表现。[3](#footnote-3)
    根据[W3Techs](https://w3techs.com/)（全球互联网技术调查），英语在互联网内容中占据了超过一半（55.6%）。[4](#footnote-4)
- en: '![](../Images/5ce6f6662d13b419b39bfae2e6cb9442.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ce6f6662d13b419b39bfae2e6cb9442.png)'
- en: 'Percentages of websites using various content languages (as of April 30, 2023).
    Data source: [https://w3techs.com/technologies/overview/content_language.](https://w3techs.com/technologies/overview/content_language.)
    Figure created by author.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 各种内容语言的网站百分比（截至2023年4月30日）。数据来源：[https://w3techs.com/technologies/overview/content_language.](https://w3techs.com/technologies/overview/content_language.)
    图表由作者创建。
- en: Similarly, English makes up [over 46% of the Common Crawl corpus](https://commoncrawl.github.io/cc-crawl-statistics/plots/languages)
    (billions of webpages from the Internet [crawled for over a decade](https://commoncrawl.org/the-data/)),
    versions of which have been used to train many large languages such as Google’s
    T5 and OpenAI’s GPT-3 (and likely ChatGPT and GPT-4). Common Crawl makes up 60%
    of GPT-3 training data.[5](#footnote-5)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，英语占[超过46%的Common Crawl语料库](https://commoncrawl.github.io/cc-crawl-statistics/plots/languages)（来自互联网的数十亿网页[被抓取了十多年](https://commoncrawl.org/the-data/)），其版本已被用于训练许多大型语言模型，如谷歌的T5和OpenAI的GPT-3（以及可能的ChatGPT和GPT-4）。Common
    Crawl占GPT-3训练数据的60%。[5](#footnote-5)
- en: Addressing the digital divide in NLP is crucial to ensure equitable language
    representation and performance in AI-driven technologies. Bridging this gap calls
    for a concerted effort from researchers, developers, and linguists to prioritize
    and invest in the development of low-resource languages, fostering a more inclusive
    and diverse linguistic landscape in the realm of natural language processing.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 解决NLP中的数字鸿沟对确保AI驱动技术中的公平语言表现和表现至关重要。弥合这一差距需要研究人员、开发者和语言学家共同努力，优先考虑和投资于低资源语言的发展，从而在自然语言处理领域促进更加包容和多样化的语言环境。
- en: 'Historical example: Representing Chinese Typography using Morse Code'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 历史例子：使用摩尔斯电码表示中文排版
- en: Such a disparity of technological costs for different languages is not new to
    AI or even to computing.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术成本的语言差异对于人工智能甚至计算机领域来说并不新鲜。
- en: Over a hundred years ago, telegraphy, a revolutionary technology of its time
    (“the internet of its era”), faced language inequities similar to those we see
    in today’s large language models. Despite its promises of open exchange and collaboration,
    telegraphy exhibited discrepancies in speed and cost across languages. For instance,
    encoding and transmitting a message in Chinese (compared to an equivalent message
    in English) was
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一百多年前，电报作为当时的革命性技术（“那个时代的互联网”），面临着与今天的大型语言模型类似的语言不平等现象。尽管电报承诺开放交流和合作，但在不同语言之间的速度和成本上存在差异。例如，用中文编码和传输一条消息（与用英语传输的同等消息相比）需要
- en: 2 times as expensive
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本是原来的2倍
- en: Took 15–20 times longer
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花费了15到20倍的时间
- en: Sound familiar?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很熟悉？
- en: Telegraphy was “designed first and foremostfor Western alphabetic languages,
    English above all.”[6](#footnote-6) Morse code assigned different lengths and
    costs to dots and dashes, resulting in a cost-efficient system for English. However,
    the Chinese language, which relies on ideograms, faced challenges in telegraphy.
    A Frenchman named Viguier devised a mapping system for Chinese characters to Morse
    code.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 电报通信“首先为西方字母语言，尤其是英语设计”。[6](#footnote-6) 摩尔斯电码为点和划线分配了不同的长度和成本，从而形成了对英语成本高效的系统。然而，依赖表意文字的中文在电报通信中面临了挑战。一位名叫Viguier的法国人设计了一个将汉字映射到摩尔斯电码的系统。
- en: Essentially, each Chinese ideogram was mapped to a four-digit code, which had
    to then be translated into Morse code. This was took a long time looking up the
    codes in the codebook (which lacked meaningful correlations) and was more costly
    to transmit (as each character was represented by four digits, and a single digit
    was more expensive to transmit than a single letter). This practice put the Chinese
    language at a disadvantage compared to other languages in terms of telegraphic
    speed and cost.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，每个汉字被映射为四位数的代码，然后必须转换为摩尔斯电码。这需要很长时间查找代码簿中的代码（其中缺乏有意义的关联），而且传输成本更高（因为每个字符由四个数字表示，而单个数字比单个字母的传输成本更高）。这一做法使得中文在电报速度和成本上相对于其他语言处于劣势。
- en: '![](../Images/dfb454208313c88edf79d3edbb9c3e1d.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfb454208313c88edf79d3edbb9c3e1d.png)'
- en: Manuscript on left from Zhang Deyim *Dianxin xinfa* 電信新法, 1873\. Danish National
    Archives. [http://www5.kb.dk/permalink/2006/manus/350/eng/32/.](http://www5.kb.dk/permalink/2006/manus/350/eng/32/.)
    Red circle drawn in by author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧手稿来自张德霖的 *电信新法*，1873年。丹麦国家档案馆。[http://www5.kb.dk/permalink/2006/manus/350/eng/32/.](http://www5.kb.dk/permalink/2006/manus/350/eng/32/.)
    红圈由作者绘制。
- en: 'Another example: Inequity in representing fonts'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另一个例子：字体表现的不平等
- en: Initially, I tried to visualize all 52 languages in a single word cloud. I ended
    up with something like this, where a majority of the languages were not rendered
    properly.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，我尝试在一个词云中可视化所有52种语言。结果得到了类似这样的效果，其中大多数语言渲染不正确。
- en: '![](../Images/73a33e8723f379efc0adab7f8aa10f98.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73a33e8723f379efc0adab7f8aa10f98.png)'
- en: Word cloud visualizing “hey” in 52 languages. Many of the languages (including
    Arabic, Hindi, and Korean) cannot be rendered using a single font (depicted is
    the [default WordCloud font](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html)
    DroidSansMono). Size corresponds to the number of tokens required to represent
    “hey” in that language. Figure created by author.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 词云可视化了“hey”在52种语言中的表现。许多语言（包括阿拉伯语、印地语和韩语）无法通过单一字体渲染（所示为[默认的WordCloud字体](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html)
    DroidSansMono）。大小与表示“hey”在该语言中所需的令牌数量相对应。图形由作者创建。
- en: This led me down a rabbit hole of trying to find a font that could render all
    of the language scripts. I went on Google Fonts to find this perfect font and
    found that one did not exist. Below is a screenshot showing how these 52 languages
    would render in 3 different fonts from Google Fonts.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我陷入了寻找可以渲染所有语言脚本的字体的困境。我去了 Google Fonts 查找这个完美的字体，发现并不存在这样一种字体。下图展示了这些52种语言在Google
    Fonts的三种不同字体中的渲染效果。
- en: '![](../Images/8322277624175d809f7cc33ec664149d.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8322277624175d809f7cc33ec664149d.png)'
- en: To generate the word cloud at the beginning of this article, I (ehm) manually
    downloaded the 17 font files necessary to render all of the language scripts and
    displayed words one at a time. While I got the desired effect, it was a lot more
    work than it would have been if, for example, all of my languages used the same
    script (such as the Latin alphabet).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成本文开头的词云，我（嗯）手动下载了17个字体文件，这些字体文件是渲染所有语言脚本和显示单词所必需的。虽然我得到了预期的效果，但这比起例如所有语言都使用相同脚本（如拉丁字母）要费时得多。
- en: Conclusion
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, I explored the language disparity in language models by looking
    at how they process text through tokenization.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我通过观察语言模型如何通过分词处理文本，探讨了语言模型中的语言差异。
- en: Using a dataset of parallel texts translated into 52 languages, I showed that
    some languages require up to 10 times more tokens to express the same message
    in English
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用翻译成52种语言的平行文本数据集，我展示了有些语言表达相同的信息需要比英语多出多达10倍的令牌。
- en: I shared a [dashboard where you can explore different languages and tokenizers](https://huggingface.co/spaces/yenniejun/tokenizers-languages)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我分享了一个[可以探索不同语言和分词器的仪表板](https://huggingface.co/spaces/yenniejun/tokenizers-languages)
- en: I discussed the impacts of this disparity on certain languages in terms of performance,
    monetary cost, and time
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I showed how this pattern of linguistic technological disparity is not new,
    comparing the phenomenon to the historical case of Chinese Morse code and telegraphy
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language disparities in NLP tokenization reveal a pressing issue in AI: equity
    and inclusivity. As models like ChatGPT are predominantly trained on English,
    non-Indo-European and non-Latin script languages face barriers due to prohibitive
    tokenization costs. Addressing these disparities is essential to ensure a more
    inclusive and accessible future for artificial intelligence, ultimately benefiting
    diverse linguistic communities worldwide.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '*Thank you for reading this article! If you liked it, please check out similar
    articles on my* [*blog*](https://blog.yenniejun.com/)!'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: APPENDIX
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Byte-Pair Encoding Tokenization
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the realm of natural language processing, tokenizers play a crucial role
    in enabling language models to process and understand text. Different models use
    different methods for tokenizing a sentence, such as splitting it into words,
    into characters, or into parts of words (also known as subwords; e.g. splitting
    “constantly” into “constant” and “ly”).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: One common tokenization is called [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)
    (BPE). This is the encoding used by OpenAI for their ChatGPT models. BPE is meant
    to decompose rare words into meaningful subwords while keeping frequently used
    words intact. A comprehensive explanation of the BPE algorithm can be found on
    the [HuggingFace Transformers course](https://huggingface.co/docs/transformers/tokenizer_summary).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Deeper Dive into Token Distribution for Languages
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I augmented Amazon’s MASSIVE dataset by using information about each of the
    52 languages using the infobox section of that language’s Wikipedia page, obtaining
    information such as writing script (e.g. Latin, Arabic alphabet) and main geographic
    region the language is predominant in (if relevant). I additionally use metadata
    from [The World Atlas of Language Structures](https://wals.info) to obtain information
    such as [language family](https://en.wikipedia.org/wiki/Language_family) (e.g.
    Indo-European, Sino-Tibetan).[7](#footnote-7)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Note that the following analyses in this article uphold the assumptions made
    by Wikipedia, The World Atlas of Language Structures, and by the Amazon MASSIVE
    dataset. Since I am not a linguistics expert, I had to assume that whatever on
    Wikipedia and the World Atlas were canonically accepted as correct with regards
    to dominant geographic region or language family.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Also, there are debates about what constitutes a language versus a dialect.
    For example, while languages such as Chinese and Arabic have different forms that
    people may not understand, they are still called single languages. On the other
    hand, Hindi and Urdu are very similar and are sometimes grouped together as one
    language called Hindustani. Because of these challenges, we need to be careful
    when deciding what counts as a language or a dialect.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '**Breakdown by language.** I chose the [12 most spoken languages](https://en.wikipedia.org/wiki/List_of_languages_by_total_number_of_speakers)
    (a combination of both first-language and second-language speakers).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e148df217ba668d4cbfec343071f951.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: Token distribution by language. Figure created by author.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '**Breakdown by language family.** Indo-European (e.g. Swedish, French), Austronesian
    languages (e.g. Indonesian, Tagalog), and Uralic languages (e.g. Hungarian, Finnish)
    resulted in shorter tokens. Dravidian languages (e.g. Tamil, Kannada) tended to
    have longer tokens.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc49a50a083e6c4aca8a872d8e1a59d1.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: Token distribution by language family. Figure created by author.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '**Breakdown by main geographic region.** Not all languages were specific to
    a single geographic region (such as Arabic, English, and Spanish, which are spread
    across many regions) — these languages were removed from this section. Languages
    spoken mostly in Europe tend to be shorter in token length, while languages spoken
    mostly in the Middle East, Central Asia, and the Horn of Africa tended to be longer
    in token length.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aeaff5dd8c825faf20eaf3c42c5154b6.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: Token distribution by main geographic region. Figure created by author.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '**Breakdown by writing script.** Other than the Latin, Arabic, and Cyrillic
    alphabets, all other languages use their own unique script. While the latter combines
    many very different unique scripts (such as Korean, Hebrew, and Georgian scripts),
    these unique scripts definitely tokenize to longer values. Compared to Latin-based
    scripts, which tokenize to shorter values.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55596b9c31ca8132f67d82c225d5ec00.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: Token distribution by writing script. Figure created by author.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'English almost always ranks #1'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For each text in the dataset, I ranked all languages based on number of tokens
    — the language with the least tokens was ranked #1 and the one with the most tokens
    was ranked #52\. Then, I plotted the distribution of each language’s *ranking*.
    Essentially, this should show how each language’s token length compares with the
    other languages in this dataset. In the below figure, I labeled a few of the languages
    (the other languages show up as gray lines in the background).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'While there were a few cases where some languages’ tokens were fewer than that
    of English (such as a few examples in Indonesian or Norwegian), English almost
    always ranked number one. Does this come as a surprise to anyone? What surprised
    me most was that there was no clear #2 or #3\. English language texts consistently
    produce the shortest tokens, and the ranking fluctuates a bit more for other languages.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有少数情况下某些语言的标记比英语少（例如印尼语或挪威语中的一些例子），但英语几乎总是排名第一。这对任何人来说有惊讶吗？让我最惊讶的是没有明确的第二或第三名。英语文本一致地生成最短的标记，而其他语言的排名波动较大。
- en: '![](../Images/f7ee92a742558e62dd51567056c69201.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7ee92a742558e62dd51567056c69201.png)'
- en: Distribution of token ranking with respect to other languages. Figure created
    by author.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其他语言的标记排名分布。图由作者制作。
- en: Quantifying token distributions differences using Earth Mover’s Distance
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用地球移动者距离量化标记分布的差异
- en: To quantify how different the token length distribution between two languages
    were, I calculated the [earth mover’s distance](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)
    (also known as the [Wasserstein distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wasserstein_distance.html))
    between two distributions. Essentially, this metric calculates the minimum amount
    of “work” required to transform one distribution into another. Larger values mean
    the distributions are farther apart (more different) while smaller values mean
    the distributions are quite similar.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化两种语言之间标记长度分布的差异，我计算了[地球移动者距离](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)（也称为[瓦瑟斯坦距离](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wasserstein_distance.html)）。本质上，这个度量计算了将一个分布转换为另一个分布所需的最小“工作量”。较大的值意味着分布之间的距离较远（差异较大），而较小的值则意味着分布之间非常相似。
- en: Here is a small subset of languages. Note that the distance says nothing about
    the length of the tokens, just how similar the distribution of token lengths are
    for two languages. For example, Arabic and Russian have similar distributions
    even though the languages themselves are not similar in a linguistic sense.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是语言的一个小子集。请注意，距离并不表示标记的长度，只是两个语言的标记长度分布的相似程度。例如，阿拉伯语和俄语的分布相似，即使这些语言在语言学上并不相似。
- en: '![](../Images/f5574c2aa21645c2be176deaa6d8365c.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5574c2aa21645c2be176deaa6d8365c.png)'
- en: Heatmap showing Earth Mover’s Distance among a subset of languages. Figure created
    by author.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 显示子集语言之间地球移动者距离的热图。图由作者制作。
- en: '[1](#footnote-anchor-1). OpenAI. [“Models”](https://platform.openai.com/docs/models).
    *OpenAI API*. [Archived](https://web.archive.org/web/20230317000210/https://platform.openai.com/docs/models)
    from the original on March 17, 2023\. Retrieved March 18, 2023.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](#footnote-anchor-1). OpenAI。[“模型”](https://platform.openai.com/docs/models)。*OpenAI
    API*。[存档](https://web.archive.org/web/20230317000210/https://platform.openai.com/docs/models)于2023年3月17日的原始页面。检索于2023年3月18日。'
- en: '[2](#footnote-anchor-2). Sebastian Ruder, Ivan Vulić, and Anders Søgaard. 2022.
    [Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research
    Manifold](https://aclanthology.org/2022.findings-acl.184). In *Findings of the
    Association for Computational Linguistics: ACL 2022*, pages 2340–2354, Dublin,
    Ireland. Association for Computational Linguistics.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](#footnote-anchor-2). 塞巴斯蒂安·鲁德尔、伊万·弗利奇和安德斯·索高德。2022年。[自然语言处理中的方差偏差：朝向多维度研究范畴的探索](https://aclanthology.org/2022.findings-acl.184)。见于*计算语言学协会：ACL
    2022*，第2340–2354页，都柏林，爱尔兰。计算语言学协会。'
- en: '[3](#footnote-anchor-3). Shijie Wu and Mark Dredze. 2020. [Are All Languages
    Created Equal in Multilingual BERT?](https://aclanthology.org/2020.repl4nlp-1.16).
    In *Proceedings of the 5th Workshop on Representation Learning for NLP*, pages
    120–130, Online. Association for Computational Linguistics.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](#footnote-anchor-3). 吴世杰和马克·德雷泽。2020年。[多语言BERT中所有语言是否平等？](https://aclanthology.org/2020.repl4nlp-1.16)。见于*第五届自然语言处理表示学习研讨会论文集*，第120–130页，在线。计算语言学协会。'
- en: '[4](#footnote-anchor-4). [Usage statistics of content languages for websites”](https://w3techs.com/technologies/overview/content_language).
    [Archived](https://archive.ph/RzLBr) from the original on 30 April 2023.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](#footnote-anchor-4). [网站内容语言的使用统计](https://w3techs.com/technologies/overview/content_language)。[存档](https://archive.ph/RzLBr)于2023年4月30日的原始页面。'
- en: '[5](#footnote-anchor-5). Brown, Tom, et al. “Language models are few-shot learners.”
    *Advances in neural information processing systems* 33 (2020): 1877–1901.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](#footnote-anchor-5). 布朗，汤姆，等。“语言模型是少样本学习者。”*神经信息处理系统进展* 33 (2020)：1877–1901。'
- en: '[6](#footnote-anchor-6). Jin Tsu. Kingdom of Characters: The Language Revolution
    That Made China Modern. New York: Riverhead Books, 2022 (p. 124).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[6](#footnote-anchor-6). Jin Tsu. 《文字的王国：使中国现代化的语言革命》。纽约：Riverhead Books, 2022（第124页）。'
- en: '[7](#footnote-anchor-7). Dryer, Matthew S. & Haspelmath, Martin (eds.) 2013\.
    WALS Online (v2020.3) [Data set]. Zenodo. [https://doi.org/10.5281/zenodo.7385533.](https://doi.org/10.5281/zenodo.7385533.)
    Available online at [https://wals.info,](https://wals.info,) Accessed on 2023–04–30.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[7](#footnote-anchor-7). Dryer, Matthew S. & Haspelmath, Martin (编辑) 2013\.
    WALS Online (v2020.3) [数据集]。Zenodo。 [https://doi.org/10.5281/zenodo.7385533.](https://doi.org/10.5281/zenodo.7385533.)
    在线获取 [https://wals.info,](https://wals.info,) 访问日期为2023年4月30日。'
