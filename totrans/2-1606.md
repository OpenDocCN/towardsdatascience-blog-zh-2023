# 使用 C 优化 LLM，并在您的笔记本电脑上运行 GPT、Llama 和 Whisper

> 原文：[https://towardsdatascience.com/optimizing-llms-with-c-and-running-gpt-lama-whisper-on-your-laptop-460c8bdd047e](https://towardsdatascience.com/optimizing-llms-with-c-and-running-gpt-lama-whisper-on-your-laptop-460c8bdd047e)

## 在这篇文章中，我们将深入了解由 Georgi Gerganov 创建的出色张量库`ggml`。它是如何工作的？张量创建过程是什么？我们可以从一些简单的例子开始吗？

[](https://stefanobosisio1.medium.com/?source=post_page-----460c8bdd047e--------------------------------)[![Stefano Bosisio](../Images/450d904024a4cbf1adf8a625886d852e.png)](https://stefanobosisio1.medium.com/?source=post_page-----460c8bdd047e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----460c8bdd047e--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----460c8bdd047e--------------------------------) [Stefano Bosisio](https://stefanobosisio1.medium.com/?source=post_page-----460c8bdd047e--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----460c8bdd047e--------------------------------) ·15 分钟阅读·2023年9月23日

--

![](../Images/3de95b94179dd7cec4a2cc9abeefedba.png)

图片来源：[Aryo Yarahmadi](https://unsplash.com/@aryo_yarahmadi) 在 [Unsplash](https://unsplash.com/photos/ylMP3TetKoQ)

## 目录

1.  [实现一个简单的数学函数](#a6d9)

    1.1 [上下文的定义](#c2ca)

    1.2 [初始化张量](#36c0)

    [1.3 前向计算和计算图](#3427)

    1.4 [编译和运行](#be10)

1.  [对第一部分的最终备注](#0308)

1.  [支持我的写作](#a985)

大型语言模型（LLMs）正随处引起关注。报纸上充斥着大量的文字描述这个即将到来的新世界，保证“AI终于来了”。尽管 LLMs 对我们的生活产生了切实的影响，但我们必须保持冷静，并对整个情况进行批判性分析。LLMs 的炒作让我想起了几年前“数据科学家”职位的炒作。2014年，当我开始攻读博士学位时，我看到数据科学家职位的稳步增加，直到2018年左右达到顶峰。当时，新闻再次炒作，写道：“数据科学家：100万美元的职业”或“21世纪最性感的工作”——这些标题是否让你联想到 LLM 的标题？

一方面，LLM（大语言模型）是一项伟大的技术，是迈向更通用AI框架的一步。这些模型是深入AI的起点，我相信有一天大多数应用程序和技术将依赖于这些模型。然而，我常常在Medium上看到，对于这些模型，有时缺乏明确性。尽管这些模型的能力和惊人的成果无可置疑，但它们过于庞大，不易运行或训练。因此，公司在决定任何战略业务方向之前，需要对LLM有非常透彻的了解。最尖锐的问题之一是这些模型的巨大内存成本、大规模基础设施需求以及推理时所需的昂贵基础设施。

如果我们考虑基本的LLM结构，即转换器，我们可以识别出经典的编码器-解码器结构。在推理时，解码器需要有一个内存机制来确定给特定输入标记分配多少注意力分数。这个分数基于标记在句子中的可能位置以及它与剩余上下文的一致性。这种机制称为KV缓存。鉴于这个矩阵的大小，对于简单模型而言，2048的上下文长度很容易就会占用3TB的内存。为了进一步加速计算，我们需要在GPU上运行。最后，整个解码器结构很难并行化。

鉴于此前言，我们是否可以找到一种折衷或权衡的解决方案，使我们能够在更简单的基础设施上运行这些计算？本文展示了Georgi Gerganov如何实现一个新的优化的基于C的张量库，称为`ggml`。我在文中提到的提交是提交[0f4e99b](https://github.com/ggerganov/ggml/commit/0f4e99b1cc357cfff21178dfd5027e70162d7ed6)，这是2022年9月的提交，标志着`ggml`冒险的开始。其逻辑是使用基础代码，以便让你对整个包有一个强有力的理解。

# 实现一个简单的数学函数

在跳到LLM之前（它将在第二篇文章中介绍），让我们尝试分解库的关键元素，以便计算一个非常简单的函数，如：*f = ax²*。

## 上下文的定义

`ggml`中的一切都始于一个上下文。上下文定义了内存要求，以适应给定模型中的所有张量。上下文是从全局状态开始创建的：

[PRE0]

全局状态构建为一个`context_container`，它是：

[PRE1]

在容器中，我们可以注意到`ggml`第一个版本的核心元素，即`ggml_context`的存在：

[PRE2]

`ggml_context`包含了有关我们可以使用多少内存以及内存缓冲区的所有信息，以便在我们不知道张量可能占用多少字节的情况下，我们可以拥有足够的内存。

然后，上下文用于初始化整个过程。`ggml_init`启动初始化过程并返回：

[PRE3]

`*ctx`是一个新的上下文指针。我们可以使用`GGML_PRINT`在源代码中调查`*ctx`的输入对象，例如：

[PRE4]

在我的Apple MacBook M2 Pro上，上下文已经初始化为16 GB内存、0个对象，并且内存布局是`objects_begin`和`objects_end`的地址为0x0。

`objects_begin`和`objects_end`确实是下一步，即在`ggml_context`中创建张量的内存地址。

## 初始化张量

对于`ggml`中的所有函数，总会找到一个协议实现，例如：

`function_with_attribute` → `function_general` → `function_implementation`

`function_with_attribute`是具有特定任务的函数，例如`ggml_new_tensor_1d`或`ggml_new_tensor_2d`，分别生成1D或2D张量。这个特定函数调用`function_general`，即实现的通用布局，例如`ggml_new_tensor_Xd`将调用`ggml_new_tensor`。最后，`function_general`调用实现`function_implementation`。这样，每次需要修改代码时，我们只需对实现进行操作，而不是修改所有特定函数。

要创建一个1D张量，我们可以使用`ggml_new_tensor1d`。从实现协议中，我们可以看到`ggml_new_tensor_1d`的代码如下：

[PRE5]

如你所见，我们有`ggml_new_tensor_1d`调用`ggml_new_tensor`，然后调用实现`ggml_new_tensor_impl`。新张量的创建类似于列表的创建。正如Georgi所述，所有的新张量对象将*放置在当前内存池的末尾*，给定一个上下文，上下文的末尾将是对象指向的位置，其中`ggml_object`定义为：

[PRE6]

起初，所有张量都初始化为`data == NULL`。核心是数据类型，在`ggml`中可以是：`sizeof(int8_t), sizeof(int16_t), sizeof(int32_t)`或`sizeof(float)`。这些大小决定了在上下文中所需的内存量，因此每个张量都在内存段中得到了完美分配。

最后，创建了一个包含所有检索信息的对象：

[PRE7]

一旦计算出新张量的数据缓冲区`struct ggml_tensor* const result = (struct ggml_tensor*)(memb_buffer + obj_new-> offset);`，就会返回分配的张量：

[PRE8]

让我们看一个简单的示例，操作1D张量，通过定义一个数学函数*f = ax²*：

[PRE9]

在定义输入张量之前，我们需要指定内存参数。在这种情况下，我们假设要使用16 GB的内存，并且这将成为上下文`ggml_context * ctx`的一部分。然后，我们可以开始定义第一个张量`x`，它将是参考变量（例如，我们想计算相对于`x`的梯度）。为了让`ggml`知道`x`是我们的主要变量，我们可以将其作为参数添加到上下文中`ggml_set_param(ctx, x);`

目前我们没有执行任何计算。我们只是指示 `ggml` 关于我们的函数（或模型）及张量如何相互作用。需要理解的是，每个张量都有一个特定的 `.op` 操作。所有新张量初始化时都为 `GGML_OP_NONE`。一旦我们对张量调用任何新操作，这一点会被修改。这会进入计算图，以便用户可以决定是否计算函数值或相对于输入变量的函数梯度（例如，在我们的案例中，我们可以要求计算相对于 `x` 的梯度）。

例如，`ggml_mul` 执行输入张量操作的变体。最初，`tensor -> op` 从 `GGML_NONE` 转换为 `GGML_OP_MUL`：

[PRE10]

这些计算被封装在一个图计算结构中，该结构在推理时为我们处理的每个模型提供 `forward` 计算。

## 前向计算和计算图

目前我们只实现了函数 *f = ax²*。要执行实际操作，我们需要创建图计算。具体操作如下：

[PRE11]

`ggml_build_forward` 构建前向计算图。在前向步骤中，我们正在构建实际的计算图，这个图遍历所有节点并返回一个结构 `ggml_cgraph`：

[PRE12]

对于上述示例，代码返回一个包含 3 个节点的图，分别是 `x`、`x^2` 和 `a*x^2` 以及 1 个叶子。可以通过 `ggml_graph_dump_dot` 函数获得图的可视化表示：

[PRE13]

其中 `&gf` 是对图结构的引用，“name_of_your_graph” 指代 `ggml` 生成的 `dot` 文件的名称。如果你想将其转换为图像，只需运行：

[PRE14]

对于我们的示例，图形为：

![](../Images/9e0f4b84ff017c22c47d65375802853e.png)

图 1：函数 f=ax² 的计算图。第一个节点是输入参数 x，然后 x 乘以自身，最后与变量 a 相乘。变量 a 是图中的一个叶子，其值为 3.00

如后面所述，我们可以给变量赋值（例如在此情况下 `a = 3.0`），我们可以看到图形具有以下内容：

1.  一个初始的黄色节点，具有 `GGML_OP_NONE` 操作以定义 `x`

1.  一个 `GGML_OP_MUL` 操作，即 `x*x`

1.  一个粉色的叶子，指代另一个变量的值 (`a`)

1.  最终节点，绿色，另一个 `GGML_OP_MUL` 操作为 `a*x^2`

一旦所有张量被分配，我们将拥有一个最终的图，其中包含所有操作，从参数变量 `x` 开始。

## 计算图中的操作

`ggml_compute_forward` 是所有计算运行的地方。

此函数的输入参数是 `struct ggml_compute_params * params, struct ggml_tensor * tensor`。`params` 指定了图中与张量相关联的操作。通过 `switch...case` 循环调用任何前向操作：

[PRE15]

每个操作都是根据张量的输入类型编码的：

[PRE16]

对于[0f4e99b](https://github.com/ggerganov/ggml/commit/0f4e99b1cc357cfff21178dfd5027e70162d7ed6)提交，仅实现了`GGML_TYPE_F32`。这调用了主要的乘法实现。

[PRE17]

操作的核心在于`for`循环。在这个循环中，我们处理结果张量`x`、乘法项`src0`和乘数`src1`。特别是：

+   `(char *) dst->data`将`dst`的数据指针转换为`char*`。这样做是因为指针算术应以字节为单位进行，而`char*`是最灵活的类型。

+   `i * (dst->nb[1])`计算当前行的字节偏移量。由于`i`在每次迭代中递增，这实际上是根据步幅信息移动到内存中的下一行。

+   最后，使用`(float *)`将结果强制转换回`float*`，以确保这些指针被解释为指向浮点值的指针。

在数值计算和张量操作的上下文中，步幅指的是沿张量的特定维度连续元素之间的步长，通常以字节为单位。理解和正确处理步幅对于高效的张量操作和内存管理至关重要。

操作`ggml_vec_mul_f32`执行最终的乘法如下：

[PRE18]

内联函数是C语言提供的一种机制（通过`inline`关键字），用于建议编译器在调用点“原地”展开特定函数，而不是作为单独的函数调用。当你调用一个常规函数时，会有一些开销。这包括将参数压入栈中、设置新的栈帧以及执行返回操作。对于非常小且频繁使用的函数，这些开销可能相对昂贵。内联消除了这些开销，因为代码直接插入调用点。内联允许编译器执行在正常调用情况下不可能进行的优化。例如，当函数被内联时，编译器可以在调用者的上下文中查看其代码并进行相应优化。这可能包括常量折叠、死代码消除和其他优化，从而使代码运行得更快。

## 最终的简单代码

我们现在准备在`ggml`中实现一个完整的代码，计算某些值的函数*f = ax²*。在`examples`文件夹下，我们可以创建一个名为`simple_example`的新文件夹。在那里，我们将有主文件`main.cpp`：

[PRE19]

在同一文件夹中，我们需要一个`CMakeLists.txt`文件，以便我们可以使用`ggml`库编译代码：

[PRE20]

最后，在文件`examples/CMakeLists.txt`的末尾添加以下行：`add_subdirectory(simple_example)`

现在，一切都是相互关联的，并且可以正确编译和运行。

## 编译和运行

回到`ggml`文件夹，按照`README.md`文件中的说明，创建一个名为`build`的文件夹并运行以下命令：

[PRE21]

这将编译`ggml`库，并生成一个`simple_example`示例代码的二进制文件。我们只需输入`./bin/simple_example`即可运行我们的代码。代码将执行计算，并打印出图形信息的形式，包括所有节点和叶子及其相关操作。对于每个操作，将给出计算时间的估算。记住，如果你想绘制最终图形，你需要运行`dot -Tpng final_graph -o final_graph.png && open final_graph.png`

# 第一部分的最终说明

在这篇第一篇文章中，我们开始深入了解`ggml`的工作原理及其基本理念。特别是，我们深入探讨了：

+   `ggml_context`以及在`ggml`库中如何初始化和使用内存

+   如何初始化一个新的1D张量以及`ggml`中的协议实现

+   图形计算如何工作，检索图形计算并绘制出来

+   一个简单的示例，初始化一个数学函数并获取其计算图

在下一篇文章中，我们将处理LLM，特别是GPT。我们将看到如何在`ggml`中实现和使用它们，最后，在我们的笔记本电脑上运行GPT模型。

# 支持我的写作

如果你喜欢我的文章，请通过以下链接支持我的写作，加入 Medium 的会员计划 :)

[](https://stefanobosisio1.medium.com/membership?source=post_page-----460c8bdd047e--------------------------------) [## 通过我的推荐链接加入 Medium - Stefano Bosisio

### 作为 Medium 会员，你的一部分会员费用会用于你阅读的作者，同时你可以获得对每个故事的完全访问权限…

stefanobosisio1.medium.com](https://stefanobosisio1.medium.com/membership?source=post_page-----460c8bdd047e--------------------------------)
