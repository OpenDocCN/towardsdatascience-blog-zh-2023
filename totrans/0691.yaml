- en: Decoupled Frontend — Backend Microservices Architecture for a ChatGPT-Based
    LLM Chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/decoupled-frontend-backend-microservices-architecture-for-chatgpt-based-llm-chatbot-61637dc5c7ea](https://towardsdatascience.com/decoupled-frontend-backend-microservices-architecture-for-chatgpt-based-llm-chatbot-61637dc5c7ea)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**A practical guide to building a headless ChatGPT application with Streamlit,
    FastAPI, and the OpenAI API**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://stephen-leo.medium.com/?source=post_page-----61637dc5c7ea--------------------------------)[![Marie
    Stephen Leo](../Images/c5669d884da5ff5c965f98904257d379.png)](https://stephen-leo.medium.com/?source=post_page-----61637dc5c7ea--------------------------------)[](https://towardsdatascience.com/?source=post_page-----61637dc5c7ea--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----61637dc5c7ea--------------------------------)
    [Marie Stephen Leo](https://stephen-leo.medium.com/?source=post_page-----61637dc5c7ea--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----61637dc5c7ea--------------------------------)
    ·8 min read·May 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64f56820c6449617c8f28c080da1275e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image generated by Author using Midjourney V5.1 using the prompt: “decoupled
    frontend backend software application”'
  prefs: []
  type: TYPE_NORMAL
- en: In [my previous post](https://medium.com/towards-data-science/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e),
    I wrote about the differences between monolithic and microservices architecture
    patterns for LLM-based chatbot applications. One of the significant advantages
    of picking the microservices architecture pattern is that it allows the separation
    of frontend code from the data science logic so that a data scientist can focus
    on the data science logic without worrying about the frontend code. In this post,
    I will show you how to build a microservice chatbot application with Streamlit,
    FastAPI, and the OpenAI API. We will decouple the frontend and backend codes from
    each other to easily swap out the frontend for another frontend framework like
    React, Swift, Dash, Gradio, etc.
  prefs: []
  type: TYPE_NORMAL
- en: First, create a new conda environment and install the necessary libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Backend: Data Science Logic**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like my previous blog post, we’ll build the backend using FastAPI. The most
    crucial part of any API is the API contract which defines the input format that
    the API accepts and the output format the API will send back to the client. Defining
    and aligning to a robust API contract allow frontend developers to work independently
    of API developers as long as both parties respect the contract. This is the beauty
    of decoupling the frontend from the backend. FastAPI lets us easily specify and
    validate the API contract using Pydantic models. The API contract for our backend
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9db859fa99931c586aa7bb73965cbf0a.png)'
  prefs: []
  type: TYPE_IMG
- en: API contract details. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'The backend will be responsible for the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize a new FastAPI app, load the OpenAI API key, and define
    a system prompt that will inform ChatGPT of the role we want it to play. In this
    case, we want ChatGPT to play the role of a comic book assistant, so we prompt
    it as such. Feel free to “engineer” different prompts and see how ChatGPT responds!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we create two Pydantic models, `Conversation` and `ConversationHistory`,
    to validate the API payload. The `Conversation` model will validate each message
    in the conversation history, while the `ConversationHistory` model is just a list
    of conversations to validate the entire conversation history. The OpenAI ChatGPT
    API can only accept `assistant` or `user` in the `role` parameter, so we specify
    that restriction in the `Conversation` model. If you try sending any other value
    in the `role` parameter, the API will return an error. Validation is one of the
    many benefits of using Pydantic models with FastAPI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we reserve the root route for a health check.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we define a `/chat` route that accepts a `POST` request. The route
    will receive a `ConversationHistory` payload, which is a list of conversations.
    The route will then convert the payload to a Python dictionary, initialize the
    conversation history with the system prompt and list of messages from the payload,
    generate a response using the OpenAI ChatGPT API, and return the generated response
    and the token usage back to the API caller.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! We can now run the backend on our local machine using `uvicorn backend:app
    — reload` and test it using the Swagger UI at [http://127.0.0.1:8000/docs.](http://127.0.0.1:8000/docs.)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46bbd7111851acebf2ca5fffa50dd1b2.png)'
  prefs: []
  type: TYPE_IMG
- en: FastAPI docs for the backend. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Frontend: User Interface**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll build the frontend headless and completely independent of the backend.
    We only have to respect the API contract used by the backend. Before building
    the frontend user interface, let’s define a few helper functions.
  prefs: []
  type: TYPE_NORMAL
- en: '`display_conversation()` will help us display the conversation history using
    the [native streamlit chat elements](https://docs.streamlit.io/library/api-reference/chat).
    We can choose unique avatars for both the user and assistant messages using emojis
    or paths to files.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`clear_conversation()` will help us clear the conversation history. It will
    also initialize the `conversation_history` session state variable to store the
    conversation history and the `total_cost` session state variable to hold the total
    conversation cost.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`download_conversation()` will allow us to download the conversation history
    as a CSV file.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`calc_cost()`: will help us calculate the cost of the conversation based on
    the number of tokens used. The [OpenAI API charges $0.002 per 1000 output tokens
    and $0.0015 per 1000 input tokens](https://openai.com/pricing#chat), so we’ll
    use that to calculate the conversation cost.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now we have everything we need to build the user interface using Streamlit.
    Let’s create a frontend.py file and import the helper functions we defined above.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll define the URL of our FastAPI backend.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`openai_llm_response()` will append the latest user input to the `conversation_history`
    session state variable using the `user` role. Then, we’ll create the payload in
    the format our backend FastAPI app expects with a `history` field. Finally, we’ll
    send the payload to the backend and append the generated response with the cost
    of the individual API call to the `conversation_history` session state variable.
    We’ll also increment the total cost with the cost of the generated response.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`main()`: is the bulk of the UI design. Below the title, we add buttons for
    clearing and downloading the conversation using the helper functions in utils.py.
    Then we have a chat input field where the user can enter their question. Pressing
    enter will send the text typed in the input field to the backend. Finally, we
    display the cost of the conversation and the conversation history.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! We’ve completed our frontend app. We can now test it using `streamlit
    run frontend.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c5887d6217d4671781ef04e23b45722.png)'
  prefs: []
  type: TYPE_IMG
- en: Streamlit App interface. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building a chatbot using the OpenAI API following a microservices architecture
    is easy by decoupling the frontend from the backend. Here are some thoughts on
    when to consider a decoupled architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Your app is relatively complex or needs to support mid to large-scale traffic.
    The decoupled architecture allows for independent scaling of the frontend and
    backend to handle large-scale traffic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You have dedicated frontend developer resources to build the UI or need to serve
    external customers requiring a highly polished UI. In this tutorial, we used Streamlit
    to construct a simple user interface, but it can get difficult or even impossible
    to build more complex UIs. It’s best to build customer-facing apps using specialized
    UI frameworks like React, Swift, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You want to improve the data science logic independent of the frontend. For
    example, you can update the prompts or add multiple microservices, all orchestrated
    by an API server entry point, without worrying about the frontend code as long
    as you respect the same API contract you’ve aligned with the frontend engineers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'However, there may be situations when there are better architectural choices
    than decoupling for your app. Here are some thoughts on when NOT to use decoupled
    architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Your app is simple or has low traffic. You can have a monolithic app since scaling
    is not an issue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You do not have dedicated frontend developer resources to build the UI, or your
    app only serves internal customers who might be more forgiving of a rough UI design.
    This is especially true while building a minimal viable product or prototype.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You’re a unicorn wanting to simultaneously improve the data science logic and
    frontend interface!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
