- en: Time Series Forecasting with Deep Learning in PyTorch (LSTM-RNN)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c](https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An in depth tutorial on forecasting a univariate time series using deep learning
    with PyTorch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://zainbaq.medium.com/?source=post_page-----1ba339885f0c--------------------------------)[![Zain
    Baquar](../Images/27c7941771179fe5731641930c403ff2.png)](https://zainbaq.medium.com/?source=post_page-----1ba339885f0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ba339885f0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ba339885f0c--------------------------------)
    [Zain Baquar](https://zainbaq.medium.com/?source=post_page-----1ba339885f0c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ba339885f0c--------------------------------)
    ·12 min read·Feb 9, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d2d7047e533c03e443b61973f43ce21a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: '[Unsplash: Maxim Hopman](https://unsplash.com/@nampoh)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Believe it or not, humans are constantly predicting things passively — even
    the most minuscule or seemingly trivial things. When crossing the road, we forecast
    where the cars will be to cross the road safely, or we try to predict exactly
    where a ball will be when we try to catch it. We don’t need to know the exact
    velocity of the car or the precise wind direction affecting the ball in order
    to perform these tasks — they come more or less naturally and obviously to us.
    These abilities are tuned by a handful of events, which over years of experience
    and practice allow us to navigate the unpredictable reality we live in. Where
    we fail in this regard, is when there are simply too many factors to take into
    consideration when we are actively predicting a large scale phenomenon, like the
    weather or how the economy will perform one year down the line.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: This is where the power of computing comes into focus — to fill the gap of our
    inability to take even the most seemingly random of occurrences and relate them
    to a future event. As we all know, computers are extremely good at doing a specific
    task over numerous iterations — which we can leverage in order to predict the
    future.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '**What is a ‘time series’?**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: A time series is any quantifiable metric or event that takes place over a period
    of time. As trivial as this sounds, almost anything can be thought of as a time
    series. Your average heart rate per hour over a month or the daily closing value
    of a stock over a year or the number vehicle accidents in a certain city per week
    over a year. Recording this information over any uniform period of time is considered
    as a time series. The astute would note that for each of these examples, there
    is a **frequency** (daily, weekly, hourly etc) of the event and a **length** of
    time (a month, year, day etc) over which the event takes place.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: For a time series, the metric is recorded with a uniform frequency throughout
    the length of time over which we are observing the metric. In other words, the
    time in between each record should be the same.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will explore how to use past data in the form of a time
    series to forecast what may happen in the future.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '**Objective**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The objective of the algorithm is to be able to take in a sequence of values,
    and predict the next value in the sequence. The simplest way to do this is to
    use an **Auto-Regressive** model, however, this has been covered extensively by
    other authors, and so we will focus on a more deep learning approach to this problem,
    using **recurrent neural networks**. I’ve linked the implementation notebook [here](https://drive.google.com/file/d/1ZzxQISX0519T347j3Sx71iHKsdYDOzcl/view?usp=sharing).
    The dataset used in this tutorial was used in a Kaggle competition, and is found
    [here](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=oil.csv).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Preparation**'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s have a look at a sample time series. The plot below shows some data on
    the price of oil from 2013 to 2018.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48c4e48fcbff37198877292c230c971f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: This is simply a plot of a single sequence of numbers on a date axis. The table
    below shows the first 10 entries of this time series. Just looking at the date
    column, it is apparent that we have price data at a daily frequency.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Many machine learning models perform much better on normalized data. The standard
    way to normalize data is to transform it such that for each column, the mean is
    0 and the standard deviation is 1\. The code below provides a way to do this using
    the **scikit-learn** library.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We also want to ensure that our data has a uniform frequency — in this example,
    we have the price of oil on each day across these 5 years, so this works out nicely.
    If, for your data, this is not the case, Pandas has a few different ways to resample
    your data to fit a uniform frequency.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequencing**'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once this is achieved, we are going to use the time series and generate clips,
    or **sequences** of fixed length. While recording these sequences, we will also
    record the value that occurred right after that sequence. For example: let’s say
    we have a sequence: [1, 2, 3, 4, 5, 6].'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'By choosing a sequence length of 3, we can generate the following sequences,
    and their associated targets:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '**[Sequence]: Target**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[1, 2, 3] → 4'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[2, 3, 4] → 5'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[3, 4, 5] → 6'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Another way to look at this is that we are defining how many steps back to look
    in order to predict the next value. We will call this value the **training window**
    and the number of values to predict, the **prediction window**. In this example,
    these are 3 and 1 respectively. The function below details how this is accomplished.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*PyTorch* requires us to store our data in a Dataset class in the following
    way:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can then use a *PyTorch* DataLoader to iterate through the data. The benefit
    of using a DataLoader is that it handles batching and shuffling internally, so
    we don’t have to worry about implementing it for ourselves.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'The training batches are finally ready after the following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At each iteration the DataLoader will yield 16 (batch size) sequences with their
    associated targets which we will pass into the model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Architecture**'
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The class below defines this architecture in *PyTorch*. We’ll be using a single
    LSTM layer, followed by some dense layers for the regressive part of the model
    with dropout layers in between them. The model will output a single value for
    each training input.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This class is a plug n’ play Python class that I built to be able to dynamically
    build a neural network (of this type) of any size, based on the parameters we
    choose — so feel free to tune the parameters n_hidden and n_deep_players to add
    or remove parameters from your model. More parameters means more model complexity
    and longer training times, so be sure to refer to your use-case for what’s best
    for your data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: As an arbitrary selection, lets create a Long Short-Term Memory model with 5
    fully connected layers which 50 neurons each, ending with a single output value
    for each training example in each batch. Here, **sequence_len** refers to the
    training window and **nout** defines how many steps to predict; setting **sequence_len**
    as 180 and **nout** as 1, means that the model will look at 180 days (half a year)
    back to predict what will happen tomorrow.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Model Training**'
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our model defined, we can choose our loss function and optimizer, set
    our learning rate and number of epochs, and begin our training loop. Since this
    is regressive problem (i.e. we are trying to predict a continuous value), a safe
    choice is Mean Squared Error for the loss function. This provides a robust way
    to calculate the error between the actual values and what the model predicts.
    This is given by:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13a66e22dc6bd9018198abeea20afb76.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: Image snipped from Google.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer object stores and calculates all the gradients needed for back
    propagation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here’s the training loop. In each training iteration, we will calculate the
    loss on both the training and validation sets we created earlier:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/17028744e646c14abaed45ad0b4bd290.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Sample output for training loop showing the training and validation loss at
    each epoch.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Now that the model is trained, we can evaluate our predictions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '**Inference**'
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here we will simply call our trained model to predict on our un-shuffled data
    and see how different the predictions are from the true observations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/9239b1333d89793b8eabc6441c36b78b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: Normalized Predicted vs Actual price of oil historically. Image by author.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: For a first try, our predictions don’t look too bad! And it helps that our validation
    loss is as low as our training loss, showing that we did not overfit the model
    and thus, the model can be considered to generalize well — which is important
    for any predictive system.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: With a somewhat decent estimator for the price of oil with respect to time in
    this time period, let’s see if we can use it to forecast what lies ahead.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '**Forecasting**'
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we define history as the series until the moment of the forecast, the algorithm
    is simple:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Get the latest valid sequence from the history (of training window length).
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Input that latest sequence to the model and predict the next value.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Append the predicted value on to the history.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat from step 1 for any number of iterations.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One caveat here is that depending on the parameters chosen upon training the
    model, the further out you forecast, the more the model succumbs to it’s own biases
    and starts to predict the mean value. So we don’t want to always predict too far
    ahead if unnecessary, as it takes away from the accuracy of the forecast.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'This is implemented in the functions below:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let’s try a few cases.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Lets forecast from different places in the middle of the series so we can compare
    the forecast to what actually happened. The way we have coded the forecaster,
    we can forecast from anywhere and for any reasonable number of steps. The red
    line shows the forecast. Keep in mind, the plots show the normalized prices on
    the y-axis.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2028b2f4fb4dfc5eaf331e7ed03fc01.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Forecasting 200 days from Q3 2013\. Image by author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f4df4fdfb8c12661b24deb6cfec9f99.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: Forecasting 200 days from EOY 2014/15\. Image by author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a75676534e172e00b151e99b305c259.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: Forecasting 200 days from Q1 2016\. Image by author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8258d44173c3fc71c54deaa3865a56f9.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: Forecasting 200 days from the last day of the data. Image by author.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: And this was just the first model configuration we tried! Experimenting more
    with the architecture and implementation would definitely allow your model to
    train better and forecast more accurately.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There we have it! A model that can predict what will happen next in a **univariate**
    time series. It’s pretty cool when thinking about all the ways and places in which
    this can be applied. Yes, this article only handled **univariate** timeseries,
    in which there is a single sequence of values. However there are ways to use multiple
    series measuring different things together to make predictions. This is called
    **multivariate** time series forecasting, it mainly just needs a few tweaks to
    the model architecture which I will cover in a future article.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！一个可以预测**单变量**时间序列中接下来会发生什么的模型。考虑到这种模型可以应用的各种方式和场景，真的很酷。是的，这篇文章仅处理了**单变量**时间序列，其中只有一个值序列。然而，也有方法可以使用多个测量不同事物的序列来进行预测。这被称为**多变量**时间序列预测，它主要只需要对模型架构进行一些调整，这些我将在未来的文章中进行介绍。
- en: The true magic of this kind of forecasting model is in the LSTM layer of the
    model, and how it handles and remembers sequences as a **recurrent** layer of
    the neural network. For more information on Neural Networks of different kinds,
    I would highly recommend [3blue1browns video](https://www.youtube.com/watch?v=aircAruvnKk)
    on the topic. He has a great series detailing how these algorithms work internally,
    which is very visually intuitive.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这种预测模型的真正魔力在于模型的LSTM层，它如何处理和记忆序列作为神经网络的**递归**层。有关不同类型神经网络的更多信息，我强烈推荐[3blue1brown的视频](https://www.youtube.com/watch?v=aircAruvnKk)。他有一个很棒的系列，详细介绍了这些算法如何在内部工作，非常直观。
- en: Thanks for reading, and be sure to check out my other articles!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，确保查看我的其他文章！
- en: '**References:**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考资料：**'
- en: Time series data — [https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=oil.csv](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=oil.csv)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据 — [https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=oil.csv](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=oil.csv)
