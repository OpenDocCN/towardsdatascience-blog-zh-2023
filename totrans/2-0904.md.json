["```py\ndef array_index_to_matplot_coords(i: int, j: int, n_cols: int) -> Tuple[int, int]:\n    \"\"\"Converts an array index to a matplot coordinate\"\"\"\n    x = j\n    y = n_cols - i - 1\n    return x, y\n\ndef plot_matrix(\n    M: np.array, \n    goal_coords: list = [],\n    img_width: int = 5, \n    img_height: int = 5, \n    title: str = None,\n    annotate_goal: bool = True\n    ) -> None: \n    \"\"\"\n    Plots a matrix as an image.\n    \"\"\"\n    height, width = M.shape\n\n    fig = plt.figure(figsize=(img_width, img_width))\n    ax = fig.add_subplot(111, aspect='equal')\n\n    for y in range(height):\n        for x in range(width):\n            # By default, the (0, 0) coordinate in matplotlib is the bottom left corner,\n            # so we need to invert the y coordinate to plot the matrix correctly\n            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n\n            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n            if (x, y) in goal_coords:\n                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n                if annotate_goal:\n                    ax.annotate(str(M[x][y]), xy=(matplot_x, matplot_y), ha='center', va='center')\n            else: \n                ax.annotate(str(M[x][y]), xy=(matplot_x, matplot_y), ha='center', va='center')\n\n    offset = .5    \n    ax.set_xlim(-offset, width - offset)\n    ax.set_ylim(-offset, height - offset)\n\n    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n\n    plt.title(title)\n    plt.show()\n```", "```py\n# Importing the array library\nimport numpy as np\n\n# Defining the number of blocks of a n x n grid \nn = 7\n\n# Defining the value for the hole and the goal\ngoal = 10\nstep = -1\n\n# Initiating an empty dataframe of size n x n\nR = np.ones((n,n))\n\n# Defining the coordinates of the goal\ngoal_coords = [(0, n-1), (n-1, 0), (0, 0), (n-1, n-1), (n // 2, n // 2)]\n\n# Adding the goal values to the center and the corners\nfor goal_coord in goal_coords:\n    R[goal_coord[1], goal_coord[0]] = goal\n\n# Every other step is -1\nR[R == 1] = step\n\n# Converting the G matrix to int \nR = R.astype(int)\n\n# Ploting\nplot_matrix(R, goal_cords, title='Gridworld')\n```", "```py\nS = np.arange(0, n*n).reshape(n, n)\n\nplot_matrix(S, goal_coords, title='State space')\n```", "```py\n# Initiating the empty Value function \nV = np.zeros((n, n))\n\nplot_matrix(V, goal_coords, title='Value function')\n```", "```py\ndef plot_policy_matrix(P: dict, S:np.array, goal_coords: list = [], img_width: int = 5, img_height: int = 5, title: str = None) -> None: \n    \"\"\" \n    Plots the policy matrix out of the dictionary provided; The dictionary values are used to draw the arrows \n    \"\"\"\n    height, width = S.shape\n\n    fig = plt.figure(figsize=(img_width, img_width))\n    ax = fig.add_subplot(111, aspect='equal')\n    for y in range(height):\n        for x in range(width):\n            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n\n            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n            if (x, y) in goal_coords:\n                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n\n            else:\n                # Adding the arrows to the plot\n                if 'up' in P[S[x, y]]:\n                    plt.arrow(matplot_x, matplot_y, 0, 0.3, head_width = 0.05, head_length = 0.05)\n                if 'down' in P[S[x, y]]:\n                    plt.arrow(matplot_x, matplot_y, 0, -0.3, head_width = 0.05, head_length = 0.05)\n                if 'left' in P[S[x, y]]:\n                    plt.arrow(matplot_x, matplot_y, -0.3, 0, head_width = 0.05, head_length = 0.05)\n                if 'right' in P[S[x, y]]:\n                    plt.arrow(matplot_x, matplot_y, 0.3, 0, head_width = 0.05, head_length = 0.05)\n\n    offset = .5    \n    ax.set_xlim(-offset, width - offset)\n    ax.set_ylim(-offset, height - offset)\n\n    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n\n    plt.title(title)\n\n# Saving all the unique states to a vector \nstates = np.unique(S)\n\n# Dictionary to hold each action for a given state\nP = {}\nfor s in states: \n    s_dict = {}\n\n    # Checking which index is the current state in the S matrix \n    s_index = np.where(S == s)\n\n    # If the state is in the top left corner, we can only move right and down\n    if s_index == (0, 0):\n        s_dict['right'] = 0.5\n        s_dict['down'] = 0.5\n\n    # If the state is in the top right corner, we can only move left and down\n    elif s_index == (0, n - 1):\n        s_dict['left'] = 0.5\n        s_dict['down'] = 0.5\n\n    # If the state is in the bottom left corner, we can only move right and up\n    elif s_index == (n - 1, 0):\n        s_dict['right'] = 0.5\n        s_dict['up'] = 0.5\n\n    # If the state is in the bottom right corner, we can only move left and up\n    elif s_index == (n - 1, n - 1):\n        s_dict['left'] = 0.5\n        s_dict['up'] = 0.5\n\n    # If the state is in the first row, we can only move left, right, and down\n    elif s_index[0] == 0:\n        s_dict['left'] = 0.333\n        s_dict['right'] = 0.333\n        s_dict['down'] = 0.333\n\n    # If the state is in the last row, we can only move left, right, and up\n    elif s_index[0] == n - 1:\n        s_dict['left'] =  0.333\n        s_dict['right'] = 0.333\n        s_dict['up'] = 0.333\n\n    # If the state is in the first column, we can only move up, down, and right\n    elif s_index[1] == 0:\n        s_dict['up'] = 0.333\n        s_dict['down'] = 0.333\n        s_dict['right'] = 0.333\n\n    # If the state is in the last column, we can only move up, down, and left\n    elif s_index[1] == n - 1:\n        s_dict['up'] = 0.333\n        s_dict['down'] = 0.333\n        s_dict['left'] = 0.333\n\n    # If the state is in the middle, we can move in all directions\n    else:\n        s_dict['up'] = 0.25\n        s_dict['down'] = 0.25\n        s_dict['left'] = 0.25\n        s_dict['right'] = 0.25\n\n    # Saving the current states trasition probabilities\n    P[s] = s_dict\n```", "```py\n# Drawing a plot for the policy matrix with arrows; In one cell there can be the maximum of 4 arrows each indicating the action an agent can take \nplot_policy_matrix(P, S, goal_coords, title='Policy matrix')\n```", "```py\ndef get_next_state(a: str, s: int, S: np.array): \n    \"\"\" \n    Function that returns the next state's coordinates given an action and a state \n    \"\"\"\n    # Getting the current indexes \n    s_index = np.where(S == s)\n    s_row = s_index[0][0]\n    s_col = s_index[1][0]\n\n    # Defining the indexes of the next state\n    next_row = s_row \n    next_col = s_col\n\n    if a == 'up':\n        next_row = s_row - 1\n        next_col = s_col\n    elif a == 'down':\n        next_row = s_row + 1\n        next_col = s_col\n    elif a == 'left':\n        next_row = s_row\n        next_col = s_col - 1\n    elif a == 'right':\n        next_row = s_row\n        next_col = s_col + 1\n\n    return next_row, next_col\n```", "```py\ndef bellman_value(\n    s: int, \n    S: np.array, \n    P: dict, \n    G: np.array, \n    V: np.array, \n    gamma: float = 0.9\n    ) -> Tuple: \n    \"\"\"\n    Calculates the Belman equation value for the given state\n    \"\"\"\n    # Extracting all the available actions for the given state\n    actions = P[s]\n\n    # Placeholder to hold the sum \n    sum = 0\n    for action in actions: \n        # Extracting the probability of the given action \n        prob = actions[action]\n\n        # Getting the next states indexes\n        next_row, next_col = get_next_state(action, s, S)\n\n        # Extracting the expected reward \n        reward = G[next_row, next_col]\n\n        # Extracting the value of the next state\n        value_prime = V[next_row, next_col]\n\n        # Adding to the sum \n        sum += prob * (reward + gamma * value_prime)\n\n    return sum\n```", "```py\ndef get_max_return(s: int, S: np.array, P: dict, G: np.array, V: np.array, gamma: float = 0.9) -> Tuple:\n    \"\"\"\n    Returns the best action and the Bellman's value for the given state\n    \"\"\"\n    # Extracting all the available actions for the given state\n    actions = P[s]\n\n    # Placeholder to hold the best action and the max return \n    best_action = None\n    max_return = -np.inf\n\n    for action in actions: \n        # Getting the probability of the action \n        prob = actions[action]\n\n        # Getting the next states indexes\n        next_row, next_col = get_next_state(action, s, S)\n\n        # Extracting the expected reward \n        reward = G[next_row, next_col]\n\n        # Extracting the value of the next state\n        value_prime = V[next_row, next_col]\n\n        # Calculating the return \n        _return = prob * (reward + gamma * value_prime)\n\n        # Checking if the return is greater than the current max return\n        if _return > max_return:\n            best_action = action\n            max_return = _return\n\n    return best_action, max_return\n\ndef update_value(s, S, P, G, V, gamma) -> float:\n    \"\"\"\n    Updates the value function for the given state\n    \"\"\"\n    # Getting the indexes of s in S \n    s_index = np.where(S == s)\n    s_row = s_index[0][0]\n    s_col = s_index[1][0]\n\n    # Getting the best action and the Bellman's value \n    _, max_return = get_max_return(s, S, P, G, V, gamma)\n\n    # Rounding up the bellman value\n    max_return = np.round(max_return, 2)\n\n    # Updating the value function with a rounded value\n    V[s_row, s_col] = max_return\n\n    return max_return\n\ndef value_iteration(\n    S: np.array, \n    P: np.array, \n    G: np.array, \n    V: np.array, \n    gamma: float = 0.9, \n    epsilon: float = 0.0001,\n    n_iter: int = None \n    ) -> None: \n    \"\"\"\n    Function that performs the value iteration algorithm\n\n    The function updates the V matrix inplace \n    \"\"\"\n    # Iteration tracker \n    iteration = 0\n\n    # Iterating until the difference between the value functions is less than epsilon \n    iterate = True\n    while iterate: \n        # Placeholder for the maximum difference between the value functions \n        delta = 0\n\n        # Updating the iteration tracker\n        iteration += 1 \n        # Iterating over the states \n        for s in S.flatten():\n            # Getting the indexes of s in S \n            s_index = np.where(S == s)\n            s_row = s_index[0][0]\n            s_col = s_index[1][0]\n\n            # Saving the current value for the state\n            v_init = V[s_row, s_col].copy()\n\n            # Updating the value function\n            v_new = update_value(s, S, P, G, V, gamma)\n\n            # Updating the delta \n            delta = np.max([delta, np.abs(v_new - v_init)])\n\n            if (delta < epsilon) and (n_iter is None): \n                iterate = False\n                break\n\n        if (n_iter is not None) and (iteration >= n_iter):\n            iterate = False\n\n    # Printing the iteration tracker\n    print(f\"Converged in {iteration} iterations\")\n\n    return None\n```", "```py\ndef update_policy(S, P, V): \n    \"\"\"\n    Function that updates the policy given the value function \n    \"\"\"\n    # Iterating over the states \n    for s in S.flatten(): \n        # Listing all the actions \n        actions = P[s]\n\n        # For each available action, getting the Bellman's value\n        values = {}\n        for action in actions.keys():\n            # Getting the next state indexes\n            next_row, next_col = get_next_state(action, s, S)\n\n            # Saving the value function of that nex t state\n            values[action] = V[next_row, next_col]\n\n        # Extracting the maximum key value of the values dictionary \n        max_value = max(values.values())        \n\n        # Leaving the keys that are equal to the maximum value\n        best_actions = [key for key in values if values[key] == max_value]\n\n        # Getting the length of the dictionary \n        length = len(values)\n\n        # Creating the final dictionary with all the best actions in it \n        p_star = {}\n        for action in best_actions:\n            p_star[action] = 1/length\n\n        # Updating the policy \n        P[s] = p_star\n```", "```py\nupdate_value(1, S, P, G, V, gamma=0.9)\nupdate_policy(S, P, V)\n```", "```py\nupdate_value(3, S, P, G, V, gamma=0.9)\nupdate_policy(S, P, V)\n```", "```py\nvalue_iteration(S, P, G, V, epsilon=10**-16)\nupdate_policy(S, P, V)\n```"]