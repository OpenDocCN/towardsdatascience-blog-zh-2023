- en: 'Gradient Descent vs. Gradient Boosting: A Side-by-Side Comparison'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712](https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From Initialization to Convergence in simple English
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)
    ·5 min read·Feb 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient descent and gradient boosting are two popular machine learning algorithms.
    Despite their different approaches and applications, both gradient descent and
    gradient boosting algorithms are founded on gradient calculations and share several
    common steps. The main aim of this article is to provide a detailed comparison
    of these two algorithms to help readers gain a better understanding of their similarities
    and differences.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c776904768c4cbd3516306acf0a29c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Gregoire Jeanneau](https://unsplash.com/es/@gregjeanneau?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradient descent is a common optimization algorithm used in machine learning
    to minimize a cost function. The goal is to find the best set of parameters that
    minimize the error between the predicted and actual values. The process starts
    by randomly initializing the weights or coefficients of the model. Then, it iteratively
    updates the weights in the direction of the steepest descent of the cost function
    by calculating the gradient of the cost function with respect to each parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradient boosting is an ensemble method that combines multiple weak models to
    create a stronger predictive model. It works by iteratively fitting a new model
    to the residual errors of the previous model. The final prediction is the sum
    of the predictions of all the models. In gradient boosting, the focus is on the
    errors made by the previous models.
  prefs: []
  type: TYPE_NORMAL
- en: Different Yet Similar
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To provide a comprehensive comparison of gradient descent and gradient boosting,
    we will first explain the algorithms separately and then provide a step-by-step
    comparison of each algorithm’s approach to optimization. This approach will help
    readers gain a better understanding of the similarities and differences between
    the two algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent algorithm in simple English
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are a few steps that explain gradient descent in simple English:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose a starting point: Gradient descent starts with a random or predefined
    initial set of weights or coefficients for the model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the gradient: The gradient is the direction of the steepest ascent
    or descent of a function. In gradient descent, we calculate the gradient of the
    cost function with respect to each parameter. The cost function measures how well
    the model fits the training data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the weights: Once we have the gradient, we update the weights of the
    model in the opposite direction of the gradient. The size of the update is determined
    by a learning rate, which controls how much the weights are adjusted in each iteration.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat until convergence: We repeat steps 2 and 3 until we reach the minimum
    of the cost function, which corresponds to the best set of weights for the model.
    The convergence criteria may vary, such as reaching a certain number of iterations
    or when the change in the cost function becomes small enough.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By iteratively adjusting the weights in the direction of the steepest descent
    of the cost function, gradient descent aims to find the best set of parameters
    that minimize the error between the predicted and actual values.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting algorithm in simple English
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are a few steps that explain gradient boosting in simple English:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a weak model: We start by training a weak model, such as a decision tree
    or a regression model, on the training data. The weak model may not perform well
    on its own but can make some predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the error: We calculate the error between the predicted and actual
    values of the weak model. This error becomes the target for the next model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Train a new model: We train a new model to predict the error made by the previous
    model. This new model is fitted on the residuals or errors of the previous model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Combine the models: We combine the predictions of all the models to make the
    final prediction. The final prediction is the sum of the predictions of all the
    models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat until convergence: We repeat steps 2 to 4, adding new models to the
    ensemble, until we reach a predefined number of models or until the performance
    on a validation set stops improving.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By iteratively fitting new models to the residual errors of the previous model,
    gradient boosting aims to improve the accuracy of the model. The final prediction
    is a combination of the predictions of all the models, each correcting the errors
    of the previous model. Gradient boosting can handle non-linear relationships,
    missing values, and outliers effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Side-by-side comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a side-by-side comparison of gradient descent and gradient boosting
    for each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient Descent: Random or predefined initialization of the weights or coefficients
    of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gradient Boosting: Training of a weak model, such as a decision tree or a regression
    model, on the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2\. Calculation of Error:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient Descent: Calculation of the error or loss between the predicted and
    actual values of the model on the entire training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gradient Boosting: Calculation of the error or residuals between the predicted
    and actual values of the weak model on the training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3\. Update or Fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient Descent: Update the weights of the model in the opposite direction
    of the gradient, based on the learning rate and gradient of the cost function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gradient Boosting: Fit a new model to predict the residual errors of the previous
    model, based on the error of the weak model and the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '4\. Combination:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient Descent: No combination is needed as the goal is to optimize the parameters
    of a single model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gradient Boosting: Combine the predictions of all the models to make the final
    prediction. The final prediction is the sum of the predictions of all the models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '5\. Convergence:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient Descent: Repeat steps 2 to 4 until convergence is reached, which may
    vary depending on the criteria such as the number of iterations or the change
    in the cost function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gradient Boosting: Repeat steps 2 to 4, adding new models to the ensemble until
    a predefined number of models or until the performance on a validation set stops
    improving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both gradient descent and gradient boosting rely on gradient calculations to
    optimize models, but they differ in their approach and purpose. Gradient descent
    focuses on minimizing the cost function of a single model, while gradient boosting
    aims to improve the accuracy of an ensemble of models.
  prefs: []
  type: TYPE_NORMAL
- en: While gradient descent and gradient boosting have different optimization goals,
    they share a common algorithmic foundation based on gradient descent. In gradient
    descent, the algorithm optimizes the parameters of a single model to minimize
    a cost function. In contrast, gradient boosting aims to optimize an ensemble of
    models by iteratively adding new models to minimize the cost function of the ensemble.
    However, both algorithms use gradient descent as the fundamental optimization
    technique.
  prefs: []
  type: TYPE_NORMAL
