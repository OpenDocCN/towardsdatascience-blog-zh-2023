["```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\ntf.random.set_seed(42)\n```", "```py\ndef create_model(mean, var, verbose=False):\n    \"\"\"Definition of a DeepONet with fully connected branch and trunk layers.\n\n    Args:\n    ----\n    mean: dictionary, mean values of the inputs\n    var: dictionary, variance values of the inputs\n    verbose: boolean, indicate whether to show the model summary\n\n    Outputs:\n    --------\n    model: the DeepONet model\n    \"\"\"\n\n    # Branch net\n    branch_input = tf.keras.Input(shape=(len(mean['forcing'])), name=\"forcing\")\n    branch = tf.keras.layers.Normalization(mean=mean['forcing'], variance=var['forcing'])(branch_input)\n    for i in range(3):\n        branch = tf.keras.layers.Dense(50, activation=\"tanh\")(branch)\n\n    # Trunk net\n    trunk_input = tf.keras.Input(shape=(len(mean['time'])), name=\"time\")\n    trunk = tf.keras.layers.Normalization(mean=mean['time'], variance=var['time'])(trunk_input)   \n    for i in range(3):\n        trunk = tf.keras.layers.Dense(50, activation=\"tanh\")(trunk)\n\n    # Compute the dot product between branch and trunk net\n    dot_product = tf.reduce_sum(tf.multiply(branch, trunk), axis=1, keepdims=True)\n\n    # Add the bias\n    output = BiasLayer()(dot_product)\n\n    # Create the model\n    model = tf.keras.models.Model(inputs=[branch_input, trunk_input], outputs=output)\n\n    if verbose:\n        model.summary()\n\n    return model \n```", "```py\nclass BiasLayer(tf.keras.layers.Layer):\n    def build(self, input_shape):\n        self.bias = self.add_weight(shape=(1,),\n                                    initializer=tf.keras.initializers.Zeros(),\n                                    trainable=True)\n    def call(self, inputs):\n        return inputs + self.bias\n```", "```py\n@tf.function\ndef ODE_residual_calculator(t, u, u_t, model):\n    \"\"\"ODE residual calculation.\n\n    Args:\n    ----\n    t: temporal coordinate\n    u: input function evaluated at discrete temporal coordinates\n    u_t: input function evaluated at t\n    model: DeepONet model\n\n    Outputs:\n    --------\n    ODE_residual: residual of the governing ODE\n    \"\"\"\n\n    with tf.GradientTape() as tape:\n        tape.watch(t)\n        s = model({\"forcing\": u, \"time\": t})\n\n    # Calculate gradients\n    ds_dt = tape.gradient(s, t)\n\n    # ODE residual\n    ODE_residual = ds_dt - u_t\n\n    return ODE_residual\n```", "```py\n@tf.function\ndef train_step(X, X_init, IC_weight, ODE_weight, model):\n    \"\"\"Calculate gradients of the total loss with respect to network model parameters.\n\n    Args:\n    ----\n    X: training dataset for evaluating ODE residuals\n    X_init: training dataset for evaluating initial conditions\n    IC_weight: weight for initial condition loss\n    ODE_weight: weight for ODE loss\n    model: DeepONet model\n\n    Outputs:\n    --------\n    ODE_loss: calculated ODE loss\n    IC_loss: calculated initial condition loss\n    total_loss: weighted sum of ODE loss and initial condition loss\n    gradients: gradients of the total loss with respect to network model parameters.\n    \"\"\"\n    with tf.GradientTape() as tape:\n        tape.watch(model.trainable_weights)\n\n        # Initial condition prediction\n        y_pred_IC = model({\"forcing\": X_init[:, 1:-1], \"time\": X_init[:, :1]})\n\n        # Equation residual\n        ODE_residual = ODE_residual_calculator(t=X[:, :1], u=X[:, 1:-1], u_t=X[:, -1:], model=model)\n\n        # Calculate loss\n        IC_loss = tf.reduce_mean(keras.losses.mean_squared_error(0, y_pred_IC))\n        ODE_loss = tf.reduce_mean(tf.square(ODE_residual))\n\n        # Total loss\n        total_loss = IC_loss*IC_weight + ODE_loss*ODE_weight\n\n    gradients = tape.gradient(total_loss, model.trainable_variables)\n\n    return ODE_loss, IC_loss, total_loss, gradients\n```", "```py\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\n\ndef create_samples(length_scale, sample_num):\n    \"\"\"Create synthetic data for u(·)\n\n    Args:\n    ----\n    length_scale: float, length scale for RNF kernel\n    sample_num: number of u(·) profiles to generate\n\n    Outputs:\n    --------\n    u_sample: generated u(·) profiles\n    \"\"\"\n\n    # Define kernel with given length scale\n    kernel = RBF(length_scale)\n\n    # Create Gaussian process regressor\n    gp = GaussianProcessRegressor(kernel=kernel)\n\n    # Collocation point locations\n    X_sample = np.linspace(0, 1, 100).reshape(-1, 1) \n\n    # Create samples\n    u_sample = np.zeros((sample_num, 100))\n    for i in range(sample_num):\n        # sampling from the prior directly\n        n = np.random.randint(0, 10000)\n        u_sample[i, :] = gp.sample_y(X_sample, random_state=n).flatten()  \n\n    return u_sample\n```", "```py\nfrom tqdm import tqdm\nfrom scipy.integrate import solve_ivp\n\ndef generate_dataset(N, length_scale, ODE_solve=False):\n    \"\"\"Generate dataset for Physics-informed DeepONet training.\n\n    Args:\n    ----\n    N: int, number of u(·) profiles\n    length_scale: float, length scale for RNF kernel\n    ODE_solve: boolean, indicate whether to compute the corresponding s(·)\n\n    Outputs:\n    --------\n    X: the dataset for t, u(·) profiles, and u(t)\n    y: the dataset for the corresponding ODE solution s(·)\n    \"\"\"\n\n    # Create random fields\n    random_field = create_samples(length_scale, N)\n\n    # Compile dataset\n    X = np.zeros((N*100, 100+2))\n    y = np.zeros((N*100, 1))\n\n    for i in tqdm(range(N)):\n        u = np.tile(random_field[i, :], (100, 1))\n        t = np.linspace(0, 1, 100).reshape(-1, 1)\n\n        # u(·) evaluated at t\n        u_t = np.diag(u).reshape(-1, 1)\n\n        # Update overall matrix\n        X[i*100:(i+1)*100, :] = np.concatenate((t, u, u_t), axis=1)\n\n        # Solve ODE\n        if ODE_solve:\n            sol = solve_ivp(lambda var_t, var_s: np.interp(var_t, t.flatten(), random_field[i, :]), \n                            t_span=[0, 1], y0=[0], t_eval=t.flatten(), method='RK45')\n            y[i*100:(i+1)*100, :] = sol.y[0].reshape(-1, 1)\n\n    return X, y\n```", "```py\n# Create training dataset\nN_train = 2000\nlength_scale_train = 0.4\nX_train, y_train = generate_dataset(N_train, length_scale_train)\n\n# Create validation dataset\nN_val = 100\nlength_scale_test = 0.4\nX_val, y_val = generate_dataset(N_val, length_scale_test)\n\n# Create testing dataset\nN_test = 100\nlength_scale_test = 0.4\nX_test, y_test = generate_dataset(N_test, length_scale_test, ODE_solve=True)\n```", "```py\n# Determine batch size\nini_batch_size = int(2000/100)\ncol_batch_size = 2000\n\n# Create dataset object (initial conditions)\nX_train_ini = tf.convert_to_tensor(X_train[X_train[:, 0]==0], dtype=tf.float32)\nini_ds = tf.data.Dataset.from_tensor_slices((X_train_ini))\nini_ds = ini_ds.shuffle(5000).batch(ini_batch_size)\n\n# Create dataset object (collocation points)\nX_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train))\ntrain_ds = train_ds.shuffle(100000).batch(col_batch_size)\n\n# Scaling \nmean = {\n    'forcing': np.mean(X_train[:, 1:-1], axis=0),\n    'time': np.mean(X_train[:, :1], axis=0)\n}\n\nvar = {\n    'forcing': np.var(X_train[:, 1:-1], axis=0),\n    'time': np.var(X_train[:, :1], axis=0)\n}\n```", "```py\nfrom collections import defaultdict\n\nclass LossTracking:\n\n    def __init__(self):\n        self.mean_total_loss = keras.metrics.Mean()\n        self.mean_IC_loss = keras.metrics.Mean()\n        self.mean_ODE_loss = keras.metrics.Mean()\n        self.loss_history = defaultdict(list)\n\n    def update(self, total_loss, IC_loss, ODE_loss):\n        self.mean_total_loss(total_loss)\n        self.mean_IC_loss(IC_loss)\n        self.mean_ODE_loss(ODE_loss)\n\n    def reset(self):\n        self.mean_total_loss.reset_states()\n        self.mean_IC_loss.reset_states()\n        self.mean_ODE_loss.reset_states()\n\n    def print(self):\n        print(f\"IC={self.mean_IC_loss.result().numpy():.4e}, \\\n              ODE={self.mean_ODE_loss.result().numpy():.4e}, \\\n              total_loss={self.mean_total_loss.result().numpy():.4e}\")\n\n    def history(self):\n        self.loss_history['total_loss'].append(self.mean_total_loss.result().numpy())\n        self.loss_history['IC_loss'].append(self.mean_IC_loss.result().numpy())\n        self.loss_history['ODE_loss'].append(self.mean_ODE_loss.result().numpy())\n```", "```py\n# Set up training configurations\nn_epochs = 300\nIC_weight= tf.constant(1.0, dtype=tf.float32)   \nODE_weight= tf.constant(1.0, dtype=tf.float32)\nloss_tracker = LossTracking()\nval_loss_hist = []\n\n# Set up optimizer\noptimizer = keras.optimizers.Adam(learning_rate=1e-3)\n\n# Instantiate the PINN model\nPI_DeepONet= create_model(mean, var)\nPI_DeepONet.compile(optimizer=optimizer)\n\n# Configure callbacks\n_callbacks = [keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=30),\n             tf.keras.callbacks.ModelCheckpoint('NN_model.h5', monitor='val_loss', save_best_only=True)]\ncallbacks = tf.keras.callbacks.CallbackList(\n                _callbacks, add_history=False, model=PI_DeepONet)\n\n# Start training process\nfor epoch in range(1, n_epochs + 1):  \n    print(f\"Epoch {epoch}:\")\n\n    for X_init, X in zip(ini_ds, train_ds):\n\n        # Calculate gradients\n        ODE_loss, IC_loss, total_loss, gradients = train_step(X, X_init, \n                                                            IC_weight, ODE_weight,\n                                                            PI_DeepONet)\n        # Gradient descent\n        PI_DeepONet.optimizer.apply_gradients(zip(gradients, PI_DeepONet.trainable_variables))\n\n        # Loss tracking\n        loss_tracker.update(total_loss, IC_loss, ODE_loss)\n\n    # Loss summary\n    loss_tracker.history()\n    loss_tracker.print()\n    loss_tracker.reset()\n\n    ####### Validation\n    val_res = ODE_residual_calculator(X_val[:, :1], X_val[:, 1:-1], X_val[:, -1:], PI_DeepONet)\n    val_ODE = tf.cast(tf.reduce_mean(tf.square(val_res)), tf.float32)\n\n    X_val_ini = X_val[X_val[:, 0]==0]\n    pred_ini_valid = PI_DeepONet.predict({\"forcing\": X_val_ini[:, 1:-1], \"time\": X_val_ini[:, :1]}, batch_size=12800)\n    val_IC = tf.reduce_mean(keras.losses.mean_squared_error(0, pred_ini_valid))\n    print(f\"val_IC: {val_IC.numpy():.4e}, val_ODE: {val_ODE.numpy():.4e}, lr: {PI_DeepONet.optimizer.lr.numpy():.2e}\")\n\n    # Callback at the end of epoch\n    callbacks.on_epoch_end(epoch, logs={'val_loss': val_IC+val_ODE})\n    val_loss_hist.append(val_IC+val_ODE)\n\n    # Re-shuffle dataset\n    ini_ds = tf.data.Dataset.from_tensor_slices((X_train_ini))\n    ini_ds = ini_ds.shuffle(5000).batch(ini_batch_size)\n\n    train_ds = tf.data.Dataset.from_tensor_slices((X_train))\n    train_ds = train_ds.shuffle(100000).batch(col_batch_size)\n```", "```py\n# History\nfig, ax = plt.subplots(1, 3, figsize=(12, 4))\nax[0].plot(range(n_epochs), loss_tracker.loss_history['IC_loss'])\nax[1].plot(range(n_epochs), loss_tracker.loss_history['ODE_loss'])\nax[2].plot(range(n_epochs), val_loss_hist)\nax[0].set_title('IC Loss')\nax[1].set_title('ODE Loss')\nax[2].set_title('Val Loss')\nfor axs in ax:\n    axs.set_yscale('log')\n```"]