- en: 'Hands-on Generative AI with GANs using Python: Autoencoders'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-autoencoders-c77232b402fc](https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-autoencoders-c77232b402fc)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/6fa218e9894ae7f7c804d4ccfef9edc1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [GR Stocks](https://unsplash.com/@grstocks?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Start with Autoencoders to better understand GANs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----c77232b402fc--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----c77232b402fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c77232b402fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c77232b402fc--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----c77232b402fc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c77232b402fc--------------------------------)
    ·6 min read·Mar 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, generative models have gained popularity due to Artificial
    Intelligent’s ability to produce synthetic instances that are almost indistinguishable
    from real data. Neural Networks like Chat GPT, which can generate text, and DALLE,
    which can generate wholly original graphics, may be familiar to you.
  prefs: []
  type: TYPE_NORMAL
- en: The website [thispersondoesnotexist.com](https://this-person-does-not-exist.com/en),
    where an AI-generated image of a person who doesn’t exist shows each time you
    visit the link, is one well-known example of generative networks. This is only
    one among many illustrations of the amazing possibilities of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Over time, Generative AI has evolved and as research advanced different architectures
    were born to solve many application cases. But in order to start learning about
    the topic of generative AI, you need to be familiar with one architecture: Generative
    Adversarial Networks (GANs).'
  prefs: []
  type: TYPE_NORMAL
- en: GANs Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ultimate **goal of a generative network is to generate new data that has
    the same distribution as its training set.** Generative networks are typically
    considered part of unsupervised learning in machine learning because they do not
    require labelled data. The Generative Adversarial Network (GAN) concept, proposed
    by Ian Goodfellow in 2014, is a popular paper “[*Generative Adversarial Nets*](https://arxiv.org/pdf/1406.2661.pdf)”.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, the GAN architecture was based on fully connected layers that were
    trained to **generate low-resolution images**, such as handwritten digits. Since
    then, there have been numerous improvements and applications of GANs. They have
    been used for tasks such as image-to-image translation, image super-resolution,
    and image inpainting, where the network learns to reconstruct missing parts of
    an image.
  prefs: []
  type: TYPE_NORMAL
- en: GANs can also be used in supervised and semi-supervised learning tasks. For
    example, Conditional GANs can generate data based on certain conditions, such
    as generating images of different animals based on user input. Semi-supervised
    GANs use labelled data to improve the quality of generated data.
  prefs: []
  type: TYPE_NORMAL
- en: The applications of GANs extend far beyond image generation. These models have
    been used in NLP (Natural Language Processing), music generation, and even drug
    discovery! The potential of generative models is huge, and as technology continues
    to advance, we can expect even more innovative applications to emerge.
  prefs: []
  type: TYPE_NORMAL
- en: GANs are attractive because they can generate data with the same distribution
    as their training data.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders Before GANs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To fully understand how these Generative Adversarial Networks work, it’s helpful
    to first start with Autoencoders. Autoencoders are a type of neural network that
    can compress and decompress training data, making them useful for data compression
    and feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Standard Autoencoders are not capable of generating new data**, but they
    serve as a useful starting point for understanding GANs. Autoencoders consist
    of two concatenated networks — an encoder network and a decoder network. The encoder
    network receives a **d-dimensional input feature x and encodes it into a p-dimensional
    vector z**. In other words, the role of the encoder is to learn how to model the
    function z= f(X). **Vector z is also called latent vector**. Usually, the dimension
    of the latent vector is lower than the original input vector, so **p<d**'
  prefs: []
  type: TYPE_NORMAL
- en: The **decoder** network **takes** the encoded vector **z and reconstructs the
    original input feature x**. **The objective of the autoencoder is to minimize
    the difference between the original input feature and the reconstructed feature**.
    By doing so, **the autoencoder learns to compress and decompress the input data
    while preserving its essential features**.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see a picture representing the autoencoder architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2870a78625634c5309e753a7607f6e4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Autoencoder Architecture (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: While **autoencoders can be used for data compression and feature extraction,
    they are not capable of generating new data like GANs.**
  prefs: []
  type: TYPE_NORMAL
- en: In this simple example, both the encoder and decoder are simple linear layers
    that compress and decompress space. **More complex architectures can have multiple
    layers and contain different types of layers, such as convolutional ones if we
    are applying the model to images.**
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see a trivial implementation of autoencoder in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The AutoEncoder class extends nn.Module as usual and consists of an encoder
    and a decoder both linear layers, which take an input a vector x of size input_shape
    (e.g. 784) reduces it to a latent space of size 128 and eventually the original
    size vector is reconstructed.
  prefs: []
  type: TYPE_NORMAL
- en: Other Types of AutoEncoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen that commonly the size of the latent vector is smaller than that
    of the input vector, so compression takes place, i.e., **p<d**. These types of
    autoencoders are called **undercomplete**.
  prefs: []
  type: TYPE_NORMAL
- en: But we can create a latent vector with a size larger than the input vector,
    **p>d**. Of course, **overcomplete autoencoders**! But what are they used for?
    They can be used for **noise reduction**.
  prefs: []
  type: TYPE_NORMAL
- en: During the training of these networks, noise is added to the input data, think
    of images that are blurred for example, and the network must be able to reconstruct
    the noise-free image. This particular architecture is called denoising autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c5fd6e04e9d564a956c473fa40100e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Basic Denoising Architecture (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Real Example of Autoencoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us now look at an example of how to implement a more complex Autoencoder
    using PyTorch to generate synthetic data similar to the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: First of all as usual we install and import the libraries we will need.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now we simply import the dataset. In Pytorch this is very easy because the library
    provides methods to download the dataset quickly. So we instantiate both the dataset
    and then the dataloader that we will need to train the network. We also define
    a transformation to convert images to tensors when they are processed by the network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now it’s time to create the AutoEncoder class as we did before. But in this
    case, both the encoder and the decoder will be deeper since they will be composed
    of more layers so as to better capture image features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see it is not much more complicated than the trivial example seen
    at the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: Now as is always the case when we train a model, we instantiate the class, and
    define a loss and an optimizer. MSELoss and Adam here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The moment of training the network has come. We have to iterate over our dataloader
    and reshape the input so it matches the model architecture. We then calculate
    the output and loss obtained, and save everything aside on a list that we can
    plot at the end of the training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Okay, our network is trained! Now we can plot the original images against their
    reconstructed images from the network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9d062ed06421a4b2e062a271196a1bc9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Original vs Reconstructed (src: [https://arxiv.org/pdf/2003.05991.pdf](https://arxiv.org/pdf/2003.05991.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning about Autoencoders is very helpful to understand how GANs work. In
    this article, we saw a little bit about the theory of these architectures and
    then we saw how they can be used to reconstruct the output of MNIST images. They’re
    a lot of fun to use, plus they’re also useful for various reasons, some of which
    include compressing and decompressing input or denoising of images as we’ve seen.
    **In the next article, I will explain how Autoencoders are related to GANs and
    we will see how to implement them.** Follow me for future articles![😉](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  prefs: []
  type: TYPE_NORMAL
- en: The End
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Marcello Politi*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [CV](https://march-08.github.io/digital-cv/)'
  prefs: []
  type: TYPE_NORMAL
