- en: 'Hands-on Generative AI with GANs using Python: Autoencoders'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-autoencoders-c77232b402fc](https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-autoencoders-c77232b402fc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/6fa218e9894ae7f7c804d4ccfef9edc1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Photo by [GR Stocks](https://unsplash.com/@grstocks?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Start with Autoencoders to better understand GANs
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----c77232b402fc--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----c77232b402fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c77232b402fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c77232b402fc--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----c77232b402fc--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c77232b402fc--------------------------------)
    Â·6 min readÂ·Mar 21, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, generative models have gained popularity due to Artificial
    Intelligentâ€™s ability to produce synthetic instances that are almost indistinguishable
    from real data. Neural Networks like Chat GPT, which can generate text, and DALLE,
    which can generate wholly original graphics, may be familiar to you.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The website [thispersondoesnotexist.com](https://this-person-does-not-exist.com/en),
    where an AI-generated image of a person who doesnâ€™t exist shows each time you
    visit the link, is one well-known example of generative networks. This is only
    one among many illustrations of the amazing possibilities of generative AI.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Over time, Generative AI has evolved and as research advanced different architectures
    were born to solve many application cases. But in order to start learning about
    the topic of generative AI, you need to be familiar with one architecture: Generative
    Adversarial Networks (GANs).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: GANs Overview
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ultimate **goal of a generative network is to generate new data that has
    the same distribution as its training set.** Generative networks are typically
    considered part of unsupervised learning in machine learning because they do not
    require labelled data. The Generative Adversarial Network (GAN) concept, proposed
    by Ian Goodfellow in 2014, is a popular paper â€œ[*Generative Adversarial Nets*](https://arxiv.org/pdf/1406.2661.pdf)â€.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Initially, the GAN architecture was based on fully connected layers that were
    trained to **generate low-resolution images**, such as handwritten digits. Since
    then, there have been numerous improvements and applications of GANs. They have
    been used for tasks such as image-to-image translation, image super-resolution,
    and image inpainting, where the network learns to reconstruct missing parts of
    an image.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åˆï¼ŒGANçš„æ¶æ„åŸºäºå…¨è¿æ¥å±‚ï¼Œæ—¨åœ¨**ç”Ÿæˆä½åˆ†è¾¨ç‡å›¾åƒ**ï¼Œå¦‚æ‰‹å†™æ•°å­—ã€‚ä»é‚£æ—¶èµ·ï¼ŒGANç»å†äº†ä¼—å¤šæ”¹è¿›å’Œåº”ç”¨ã€‚å®ƒä»¬å·²è¢«ç”¨äºå›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ã€å›¾åƒè¶…åˆ†è¾¨ç‡å’Œå›¾åƒä¿®è¡¥ç­‰ä»»åŠ¡ï¼Œå…¶ä¸­ç½‘ç»œå­¦ä¹ é‡å»ºå›¾åƒçš„ç¼ºå¤±éƒ¨åˆ†ã€‚
- en: GANs can also be used in supervised and semi-supervised learning tasks. For
    example, Conditional GANs can generate data based on certain conditions, such
    as generating images of different animals based on user input. Semi-supervised
    GANs use labelled data to improve the quality of generated data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: GANsä¹Ÿå¯ä»¥ç”¨äºç›‘ç£å­¦ä¹ å’ŒåŠç›‘ç£å­¦ä¹ ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œæ¡ä»¶GANså¯ä»¥æ ¹æ®æŸäº›æ¡ä»¶ç”Ÿæˆæ•°æ®ï¼Œå¦‚æ ¹æ®ç”¨æˆ·è¾“å…¥ç”Ÿæˆä¸åŒåŠ¨ç‰©çš„å›¾åƒã€‚åŠç›‘ç£GANsä½¿ç”¨æ ‡è®°æ•°æ®æ¥æé«˜ç”Ÿæˆæ•°æ®çš„è´¨é‡ã€‚
- en: The applications of GANs extend far beyond image generation. These models have
    been used in NLP (Natural Language Processing), music generation, and even drug
    discovery! The potential of generative models is huge, and as technology continues
    to advance, we can expect even more innovative applications to emerge.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: GANsçš„åº”ç”¨è¿œä¸æ­¢äºå›¾åƒç”Ÿæˆã€‚è¿™äº›æ¨¡å‹å·²è¢«åº”ç”¨äºNLPï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰ã€éŸ³ä¹ç”Ÿæˆç”šè‡³è¯ç‰©å‘ç°ï¼ç”Ÿæˆæ¨¡å‹çš„æ½œåŠ›å·¨å¤§ï¼Œéšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œæˆ‘ä»¬å¯ä»¥æœŸå¾…æ›´å¤šåˆ›æ–°åº”ç”¨çš„å‡ºç°ã€‚
- en: GANs are attractive because they can generate data with the same distribution
    as their training data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GANså¾ˆæœ‰å¸å¼•åŠ›ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ç”Ÿæˆä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒç›¸åŒçš„æ•°æ®ã€‚
- en: Autoencoders Before GANs
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨ä¸GANs
- en: To fully understand how these Generative Adversarial Networks work, itâ€™s helpful
    to first start with Autoencoders. Autoencoders are a type of neural network that
    can compress and decompress training data, making them useful for data compression
    and feature extraction.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å®Œå…¨ç†è§£è¿™äº›ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„å·¥ä½œåŸç†ï¼Œé¦–å…ˆäº†è§£è‡ªç¼–ç å™¨æ˜¯å¾ˆæœ‰å¸®åŠ©çš„ã€‚è‡ªç¼–ç å™¨æ˜¯ä¸€ç§å¯ä»¥å‹ç¼©å’Œè§£å‹è®­ç»ƒæ•°æ®çš„ç¥ç»ç½‘ç»œç±»å‹ï¼Œä½¿å…¶åœ¨æ•°æ®å‹ç¼©å’Œç‰¹å¾æå–æ–¹é¢éå¸¸æœ‰ç”¨ã€‚
- en: '**Standard Autoencoders are not capable of generating new data**, but they
    serve as a useful starting point for understanding GANs. Autoencoders consist
    of two concatenated networks â€” an encoder network and a decoder network. The encoder
    network receives a **d-dimensional input feature x and encodes it into a p-dimensional
    vector z**. In other words, the role of the encoder is to learn how to model the
    function z= f(X). **Vector z is also called latent vector**. Usually, the dimension
    of the latent vector is lower than the original input vector, so **p<d**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ ‡å‡†è‡ªç¼–ç å™¨æ— æ³•ç”Ÿæˆæ–°æ•°æ®**ï¼Œä½†å®ƒä»¬ä½œä¸ºç†è§£GANsçš„æœ‰ç”¨èµ·ç‚¹ã€‚è‡ªç¼–ç å™¨ç”±ä¸¤ä¸ªä¸²è”çš„ç½‘ç»œç»„æˆâ€”â€”ç¼–ç å™¨ç½‘ç»œå’Œè§£ç å™¨ç½‘ç»œã€‚ç¼–ç å™¨ç½‘ç»œæ¥æ”¶**dç»´è¾“å…¥ç‰¹å¾xï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºpç»´å‘é‡z**ã€‚æ¢å¥è¯è¯´ï¼Œç¼–ç å™¨çš„è§’è‰²æ˜¯å­¦ä¹ å¦‚ä½•å»ºæ¨¡å‡½æ•°z
    = f(X)ã€‚**å‘é‡zä¹Ÿç§°ä¸ºæ½œåœ¨å‘é‡**ã€‚é€šå¸¸ï¼Œæ½œåœ¨å‘é‡çš„ç»´åº¦ä½äºåŸå§‹è¾“å…¥å‘é‡ï¼Œå› æ­¤**p < d**'
- en: The **decoder** network **takes** the encoded vector **z and reconstructs the
    original input feature x**. **The objective of the autoencoder is to minimize
    the difference between the original input feature and the reconstructed feature**.
    By doing so, **the autoencoder learns to compress and decompress the input data
    while preserving its essential features**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**è§£ç å™¨**ç½‘ç»œ**æ¥æ”¶**ç¼–ç åçš„å‘é‡**zå¹¶é‡å»ºåŸå§‹è¾“å…¥ç‰¹å¾x**ã€‚**è‡ªç¼–ç å™¨çš„ç›®æ ‡æ˜¯æœ€å°åŒ–åŸå§‹è¾“å…¥ç‰¹å¾ä¸é‡å»ºç‰¹å¾ä¹‹é—´çš„å·®å¼‚**ã€‚é€šè¿‡è¿™æ ·åšï¼Œ**è‡ªç¼–ç å™¨å­¦ä¹ åœ¨å‹ç¼©å’Œè§£å‹è¾“å…¥æ•°æ®çš„åŒæ—¶ä¿ç•™å…¶æœ¬è´¨ç‰¹å¾**ã€‚'
- en: Letâ€™s see a picture representing the autoencoder architecture.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªè¡¨ç¤ºè‡ªç¼–ç å™¨æ¶æ„çš„å›¾ç‰‡ã€‚
- en: '![](../Images/2870a78625634c5309e753a7607f6e4b.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2870a78625634c5309e753a7607f6e4b.png)'
- en: Autoencoder Architecture (Image By Author)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨æ¶æ„ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰
- en: While **autoencoders can be used for data compression and feature extraction,
    they are not capable of generating new data like GANs.**
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**è™½ç„¶è‡ªç¼–ç å™¨å¯ä»¥ç”¨äºæ•°æ®å‹ç¼©å’Œç‰¹å¾æå–ï¼Œä½†å®ƒä»¬æ— æ³•åƒGANsé‚£æ ·ç”Ÿæˆæ–°æ•°æ®ã€‚**'
- en: In this simple example, both the encoder and decoder are simple linear layers
    that compress and decompress space. **More complex architectures can have multiple
    layers and contain different types of layers, such as convolutional ones if we
    are applying the model to images.**
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç®€å•çš„ä¾‹å­ä¸­ï¼Œç¼–ç å™¨å’Œè§£ç å™¨éƒ½æ˜¯ç®€å•çš„çº¿æ€§å±‚ï¼Œç”¨äºå‹ç¼©å’Œè§£å‹ç©ºé—´ã€‚**æ›´å¤æ‚çš„æ¶æ„å¯ä»¥åŒ…å«å¤šä¸ªå±‚ï¼Œå¹¶ä¸”å¯èƒ½åŒ…å«ä¸åŒç±»å‹çš„å±‚ï¼Œä¾‹å¦‚åœ¨åº”ç”¨äºå›¾åƒæ¨¡å‹æ—¶ä½¿ç”¨å·ç§¯å±‚ã€‚**
- en: Letâ€™s see a trivial implementation of autoencoder in PyTorch.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹åœ¨PyTorchä¸­è‡ªç¼–ç å™¨çš„ä¸€ä¸ªç®€å•å®ç°ã€‚
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The AutoEncoder class extends nn.Module as usual and consists of an encoder
    and a decoder both linear layers, which take an input a vector x of size input_shape
    (e.g. 784) reduces it to a latent space of size 128 and eventually the original
    size vector is reconstructed.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: AutoEncoder ç±»åƒå¾€å¸¸ä¸€æ ·ç»§æ‰¿ nn.Moduleï¼ŒåŒ…æ‹¬ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ï¼Œå®ƒä»¬éƒ½æ˜¯çº¿æ€§å±‚ï¼Œæ¥å—ä¸€ä¸ªå¤§å°ä¸º input_shapeï¼ˆä¾‹å¦‚
    784ï¼‰çš„è¾“å…¥å‘é‡ xï¼Œå°†å…¶å‡å°‘åˆ°å¤§å°ä¸º 128 çš„æ½œåœ¨ç©ºé—´ï¼Œæœ€ç»ˆé‡å»ºåŸå§‹å¤§å°çš„å‘é‡ã€‚
- en: Other Types of AutoEncoders
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¶ä»–ç±»å‹çš„ AutoEncoders
- en: We have seen that commonly the size of the latent vector is smaller than that
    of the input vector, so compression takes place, i.e., **p<d**. These types of
    autoencoders are called **undercomplete**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œé€šå¸¸æ½œåœ¨å‘é‡çš„å¤§å°å°äºè¾“å…¥å‘é‡çš„å¤§å°ï¼Œå› æ­¤å‘ç”Ÿå‹ç¼©ï¼Œå³ **p<d**ã€‚è¿™ç±» autoencoders è¢«ç§°ä¸º **undercomplete**ã€‚
- en: But we can create a latent vector with a size larger than the input vector,
    **p>d**. Of course, **overcomplete autoencoders**! But what are they used for?
    They can be used for **noise reduction**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªæ¯”è¾“å…¥å‘é‡æ›´å¤§çš„æ½œåœ¨å‘é‡ï¼Œ**p>d**ã€‚å½“ç„¶ï¼Œ**overcomplete autoencoders**ï¼ä½†å®ƒä»¬çš„ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿå®ƒä»¬å¯ä»¥ç”¨äº
    **é™å™ª**ã€‚
- en: During the training of these networks, noise is added to the input data, think
    of images that are blurred for example, and the network must be able to reconstruct
    the noise-free image. This particular architecture is called denoising autoencoder.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¾“å…¥æ•°æ®ä¸­ä¼šæ·»åŠ å™ªå£°ï¼Œä¾‹å¦‚æ¨¡ç³Šçš„å›¾åƒï¼Œç½‘ç»œå¿…é¡»èƒ½å¤Ÿé‡å»ºæ— å™ªå£°çš„å›¾åƒã€‚è¿™ç§ç‰¹å®šçš„æ¶æ„ç§°ä¸ºå»å™ª autoencoderã€‚
- en: '![](../Images/1c5fd6e04e9d564a956c473fa40100e1.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c5fd6e04e9d564a956c473fa40100e1.png)'
- en: Basic Denoising Architecture (Image By Author)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬å»å™ªæ¶æ„ï¼ˆå›¾ç‰‡ä½œè€…ï¼‰
- en: Real Example of Autoencoder
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Autoencoder çš„å®é™…ç¤ºä¾‹
- en: Let us now look at an example of how to implement a more complex Autoencoder
    using PyTorch to generate synthetic data similar to the MNIST dataset.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªå¦‚ä½•ä½¿ç”¨ PyTorch å®ç°æ›´å¤æ‚çš„ Autoencoder çš„ä¾‹å­ï¼Œè¯¥ Autoencoder ç”¨äºç”Ÿæˆç±»ä¼¼äº MNIST æ•°æ®é›†çš„åˆæˆæ•°æ®ã€‚
- en: First of all as usual we install and import the libraries we will need.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œåƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬å®‰è£…å¹¶å¯¼å…¥æ‰€éœ€çš„åº“ã€‚
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now we simply import the dataset. In Pytorch this is very easy because the library
    provides methods to download the dataset quickly. So we instantiate both the dataset
    and then the dataloader that we will need to train the network. We also define
    a transformation to convert images to tensors when they are processed by the network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬åªéœ€å¯¼å…¥æ•°æ®é›†ã€‚åœ¨ Pytorch ä¸­è¿™éå¸¸ç®€å•ï¼Œå› ä¸ºåº“æä¾›äº†å¿«é€Ÿä¸‹è½½æ•°æ®é›†çš„æ–¹æ³•ã€‚æ‰€ä»¥æˆ‘ä»¬å®ä¾‹åŒ–æ•°æ®é›†ï¼Œç„¶åæ˜¯æˆ‘ä»¬éœ€è¦è®­ç»ƒç½‘ç»œçš„ dataloaderã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€ä¸ªè½¬æ¢ï¼Œå°†å›¾åƒåœ¨è¢«ç½‘ç»œå¤„ç†æ—¶è½¬æ¢ä¸ºå¼ é‡ã€‚
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now itâ€™s time to create the AutoEncoder class as we did before. But in this
    case, both the encoder and the decoder will be deeper since they will be composed
    of more layers so as to better capture image features.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯æ—¶å€™åˆ›å»º AutoEncoder ç±»äº†ï¼Œå°±åƒä¹‹å‰ä¸€æ ·ã€‚ä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¼–ç å™¨å’Œè§£ç å™¨éƒ½ä¼šæ›´æ·±ï¼Œå› ä¸ºå®ƒä»¬å°†ç”±æ›´å¤šçš„å±‚ç»„æˆï¼Œä»¥æ›´å¥½åœ°æ•æ‰å›¾åƒç‰¹å¾ã€‚
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see it is not much more complicated than the trivial example seen
    at the beginning.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œå®ƒå¹¶æ²¡æœ‰æ¯”æœ€åˆçœ‹åˆ°çš„ç®€å•ä¾‹å­å¤æ‚å¤šå°‘ã€‚
- en: Now as is always the case when we train a model, we instantiate the class, and
    define a loss and an optimizer. MSELoss and Adam here.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬è®­ç»ƒæ¨¡å‹æ—¶æ€»æ˜¯è¿™æ ·ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–ç±»ï¼Œå¹¶å®šä¹‰ä¸€ä¸ªæŸå¤±å‡½æ•°å’Œä¸€ä¸ªä¼˜åŒ–å™¨ã€‚åœ¨è¿™é‡Œæ˜¯ MSELoss å’Œ Adamã€‚
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The moment of training the network has come. We have to iterate over our dataloader
    and reshape the input so it matches the model architecture. We then calculate
    the output and loss obtained, and save everything aside on a list that we can
    plot at the end of the training.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒç½‘ç»œçš„æ—¶åˆ»æ¥äº†ã€‚æˆ‘ä»¬å¿…é¡»éå†æˆ‘ä»¬çš„ dataloaderï¼Œå¹¶è°ƒæ•´è¾“å…¥ä»¥åŒ¹é…æ¨¡å‹æ¶æ„ã€‚ç„¶åè®¡ç®—è¾“å‡ºå’Œè·å¾—çš„æŸå¤±ï¼Œå¹¶å°†æ‰€æœ‰å†…å®¹ä¿å­˜åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒç»“æŸæ—¶ç»˜åˆ¶ã€‚
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Okay, our network is trained! Now we can plot the original images against their
    reconstructed images from the network.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½äº†ï¼Œæˆ‘ä»¬çš„ç½‘ç»œå·²ç»è®­ç»ƒå®Œæˆï¼ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†åŸå§‹å›¾åƒä¸ç½‘ç»œé‡å»ºçš„å›¾åƒè¿›è¡Œå¯¹æ¯”ã€‚
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/9d062ed06421a4b2e062a271196a1bc9.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d062ed06421a4b2e062a271196a1bc9.png)'
- en: 'Original vs Reconstructed (src: [https://arxiv.org/pdf/2003.05991.pdf](https://arxiv.org/pdf/2003.05991.pdf))'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'åŸå§‹ vs é‡å»ºï¼ˆæ¥æº: [https://arxiv.org/pdf/2003.05991.pdf](https://arxiv.org/pdf/2003.05991.pdf)ï¼‰'
- en: Final Thoughts
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæ€è€ƒ
- en: Learning about Autoencoders is very helpful to understand how GANs work. In
    this article, we saw a little bit about the theory of these architectures and
    then we saw how they can be used to reconstruct the output of MNIST images. Theyâ€™re
    a lot of fun to use, plus theyâ€™re also useful for various reasons, some of which
    include compressing and decompressing input or denoising of images as weâ€™ve seen.
    **In the next article, I will explain how Autoencoders are related to GANs and
    we will see how to implement them.** Follow me for future articles![ğŸ˜‰](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ è‡ªç¼–ç å™¨å¯¹äºç†è§£ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„å·¥ä½œåŸç†éå¸¸æœ‰å¸®åŠ©ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬äº†è§£äº†ä¸€äº›å…³äºè¿™äº›æ¶æ„çš„ç†è®ºï¼Œç„¶åçœ‹åˆ°å®ƒä»¬å¦‚ä½•ç”¨äºé‡æ„MNISTå›¾åƒçš„è¾“å‡ºã€‚ä½¿ç”¨å®ƒä»¬éå¸¸æœ‰è¶£ï¼Œè€Œä¸”å®ƒä»¬ä¹Ÿæœ‰å„ç§ç”¨é€”ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬å‹ç¼©å’Œè§£å‹è¾“å…¥æˆ–å»å™ªå›¾åƒï¼Œæ­£å¦‚æˆ‘ä»¬æ‰€è§ã€‚**åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†è§£é‡Šè‡ªç¼–ç å™¨å¦‚ä½•ä¸GANsç›¸å…³ï¼Œå¹¶ä¸”æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å®ç°å®ƒä»¬ã€‚**
    å…³æ³¨æˆ‘ä»¥è·å–æœªæ¥çš„æ–‡ç« ![ğŸ˜‰](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
- en: The End
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æŸ
- en: '*Marcello Politi*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*Marcello Politi*'
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [CV](https://march-08.github.io/digital-cv/)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [CV](https://march-08.github.io/digital-cv/)'
