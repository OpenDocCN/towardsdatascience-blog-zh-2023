- en: 'Temporal Differences with Python: First Sample-Based Reinforcement Learning
    Algorithm'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee](https://towardsdatascience.com/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Coding up and understanding the TD(0) algorithm using Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------)
    ·13 min read·Jan 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36f274e9692bf335a901977271968109.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Kurt Cotoaga](https://unsplash.com/@kydroon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a continuation article from my previous article:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----54c11745a0ee--------------------------------)
    [## First Steps in the World Of Reinforcement Learning using Python'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Original Python implementation of how to find the best places to be in one of
    the fundamental worlds of reinforcement…
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----54c11745a0ee--------------------------------)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I want to familiarize the reader with the sample-based algorithm
    logic in Reinforcement Learning (**RL**). To do this, we will create a grid world
    with holes (much like the one in the thumbnail) and let our agent freely traverse
    our created world.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, by the end of the agent's journey, he will have learnt where in the
    world is a good place to be and which locations should be avoided. To help our
    agent in the learning process we will use the famous **TD(0)** algorithm.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into the algorithms, let us define the objective that we want
    to solve.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will create a grid world with 5 rows and 7 columns, meaning,
    our agent will be able to be in one of 35 states. The rules of movement are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The agent cannot go outside the grid world boundaries.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent, at each time step, can only move up, down, left or right.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent starts in the top left corner of our grid world.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the agent reaches his goal or falls into the hole, the game ends and he is
    returned to the starting state.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each movement earns a reward of -1.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Falling into a hole earns a reward of -10.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reaching the goal earns a reward of 10.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ultimate goal of our agent is to evaluate each state he can be in as well
    as possible. In other words, **our agent wants to evaluate the value of each state
    given a certain policy of movement.**
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'The bellow code snipped initializes the environment stated in the previous
    section:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The objects that we need to start learning are:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The state matrix S
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value matrix V
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward matrix R
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The policy dictionary P
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, the code snippet above initiates a world with a random policy.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '**A random policy means that our agent chooses to go from one state to another
    by a uniform probability distribution.**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us create our world and explore in more detail the matrices:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The bellow code snippet is used to plot the matrices:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let us first visualize the state matrix:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/a010780e57982499b646f6fd03f4aad8.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: State matrix; Photo by author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The red states indicate the hole coordinates — these are the state our agent
    wants to avoid.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: The grey state indicates the goal — this is where our agent wants to be.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Our agent will always start its journey from state 0.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'The reward matrix is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/1aa2a275f8ba0f3ab10a817dbbfb6caa.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Reward matrix; Photo by author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The reward matrix for transitioning into a certain state is visualized above.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Going from state 1 -> 8 would give a reward of -1
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going from state 9 -> 10 would give a reward of -10
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going from state 33 -> 34 would give a reward of 10
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And so on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'The policy that our agent will follow is a random policy — transitioning into
    each of the states is uniform:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/2555fd4214edb5815094a9072e2b5f46.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: The policy matrix; Photo by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'The grey states in the policy matrix represent the terminal states: if the
    agent chooses to go to that state, the episode will end and the agent will be
    reset to state 0.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**The goal of the TD(0) algorithm is to evaluate each state’s value for the
    given policy.**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, we want to fill out the value matrix values:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/a3440272edf8dca009a125014db17b4c.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Initial value matrix; Photo by author
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The **TD(0)** algorithm is short for **one step temporal differences** algorithm.
    In order to start building intuition and putting it broadly, in this algorithm
    our agent makes one step following the given policy, observes the reward and updates
    the estimates of the state value after such a step.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'To put it mathematically, the update step is the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc019e96ebe04404524df5596f5c0a45.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: TD(0) update equation
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'Here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '**s prime** — the state to which our agent goes from the current state s.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward **r** is equal to the reward of transitioning to s prime.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gamma** is the discount rate (more than 0, less than or equal to 1).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alpha** is the size (more than 0, less than or equal to 1).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The full algorithm¹ is the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77bdaf7b738af3153323baee021c0110.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Full TD(0); Photo by author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: The TD(0) algorithm is a **prediction** algorithm. In RL, a prediction algorithm
    refers to an algorithm that tries to estimate the state values while **not** changing
    the given policy (probabilities of transition).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: It is also a **bootstrap** algorithm because we are using the current estimate
    of the value function to estimate the value function for the next state.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we are only interested in the state values — the total expected accumulated
    reward if the agent moves from the current state:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c69146c3676f9e8d5c3aae5cc0b9a450.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: State value
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Now let us start the implementation of the algorithm.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that our agent needs is to make a move based on our created
    policy:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When an agent is in state s, he can only go to states that are present in the
    policy matrix dictionary. For example, all the actions in state 1 are:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f28b8b65ac45770c6f867938c3719b6c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: All the possible actions from state 1
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: The sum of all the probabilities is equal to 1 and our agent chooses randomly
    from the actions **right, left or down** (refer to the state matrix plot to see
    where the state is).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'The above action is all that is needed to start updating the value function.
    When our agent makes a move, it transitions to another state and collects the
    reward of that state. We then apply the equation:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc019e96ebe04404524df5596f5c0a45.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: TD(0) update equation
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The final thing to do is to wrap everything into a **while loop** and only
    stop exploring when our agent transitions into a terminal state:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We now have everything we need to implement the full TD(0) algorithm.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Let us define 10000 episodes and let our agent learn!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The number of actions our agent took before termination:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/43bc9ae571c1200b08fa3af801a9413e.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: Number of moves; Graph by author
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: On average, our agent took 10 moves before bumping into a terminal state.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'The final evaluated state value matrix:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1394931721b63e545ef719d8b174129b.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: Evaluated V using TD(0) and a random policy; Graph by author
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, following the given policy, it is a very bad state to be in where
    our agent starts our journey. On average, starting from that state, the agent
    accumulates only a -9.96 reward. As we move closer to the goal state, however,
    the value increases.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Note that the goal state and the hole states have a value of 0 because there
    is no exploration from these states — every time the agent transitions there the
    game ends.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'What would happen if we chose another policy? For example, more often chose
    the direction to the ‘right’:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/66602a243f165f8344cf6d401141b74b.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Number of moves with a different policy
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3cbab1ef6cc6bcd0ba876784e9803804.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: Value matrix of the different policy
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The random policies state value matrix sum is equal to **-249.29** while the
    policy with a bigger probability to go to the right has a sum of **-213.51**.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 随机策略的状态价值矩阵总和为**-249.29**，而更高概率向右的策略总和为**-213.51**。
- en: In that sense, we can say that moving more often to the right is a better policy!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个意义上说，我们可以说更频繁地向右移动是一种更好的策略！
- en: In this article, I presented the first sample-based algorithm in RL — the one-step
    temporal difference algorithm or TD(0).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我介绍了RL中的第一个基于样本的算法——一步时序差分算法或TD(0)。
- en: It is a prediction algorithm, meaning, it is only used to evaluate the states
    for the given policy. Changing a policy will yield different state-value results.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种预测算法，即仅用于评估给定策略的状态。改变策略会得到不同的状态价值结果。
- en: Happy learning and happy coding everyone!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 祝大家学习愉快，编程快乐！
- en: '[1]'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]'
- en: 'Author: **Richard S. Sutton, Andrew G. Barto**'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '作者: **理查德·S·萨顿，安德鲁·G·巴托**'
- en: 'Year: **2018**'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '年份: **2018**'
- en: 'Page: **120**'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '页码: **120**'
- en: 'Title: **Reinforcement Learning: An Introduction**'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '书名: **强化学习：一种介绍**'
- en: URL:[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: URL:[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
