- en: 'Temporal Differences with Python: First Sample-Based Reinforcement Learning
    Algorithm'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee](https://towardsdatascience.com/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Coding up and understanding the TD(0) algorithm using Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------)
    ·13 min read·Jan 27, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36f274e9692bf335a901977271968109.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Kurt Cotoaga](https://unsplash.com/@kydroon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a continuation article from my previous article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----54c11745a0ee--------------------------------)
    [## First Steps in the World Of Reinforcement Learning using Python'
  prefs: []
  type: TYPE_NORMAL
- en: Original Python implementation of how to find the best places to be in one of
    the fundamental worlds of reinforcement…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----54c11745a0ee--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I want to familiarize the reader with the sample-based algorithm
    logic in Reinforcement Learning (**RL**). To do this, we will create a grid world
    with holes (much like the one in the thumbnail) and let our agent freely traverse
    our created world.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, by the end of the agent's journey, he will have learnt where in the
    world is a good place to be and which locations should be avoided. To help our
    agent in the learning process we will use the famous **TD(0)** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into the algorithms, let us define the objective that we want
    to solve.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will create a grid world with 5 rows and 7 columns, meaning,
    our agent will be able to be in one of 35 states. The rules of movement are:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent cannot go outside the grid world boundaries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent, at each time step, can only move up, down, left or right.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent starts in the top left corner of our grid world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the agent reaches his goal or falls into the hole, the game ends and he is
    returned to the starting state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each movement earns a reward of -1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Falling into a hole earns a reward of -10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reaching the goal earns a reward of 10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ultimate goal of our agent is to evaluate each state he can be in as well
    as possible. In other words, **our agent wants to evaluate the value of each state
    given a certain policy of movement.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The bellow code snipped initializes the environment stated in the previous
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The objects that we need to start learning are:'
  prefs: []
  type: TYPE_NORMAL
- en: The state matrix S
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value matrix V
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward matrix R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The policy dictionary P
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, the code snippet above initiates a world with a random policy.
  prefs: []
  type: TYPE_NORMAL
- en: '**A random policy means that our agent chooses to go from one state to another
    by a uniform probability distribution.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us create our world and explore in more detail the matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The bellow code snippet is used to plot the matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us first visualize the state matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a010780e57982499b646f6fd03f4aad8.png)'
  prefs: []
  type: TYPE_IMG
- en: State matrix; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: The red states indicate the hole coordinates — these are the state our agent
    wants to avoid.
  prefs: []
  type: TYPE_NORMAL
- en: The grey state indicates the goal — this is where our agent wants to be.
  prefs: []
  type: TYPE_NORMAL
- en: Our agent will always start its journey from state 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reward matrix is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1aa2a275f8ba0f3ab10a817dbbfb6caa.png)'
  prefs: []
  type: TYPE_IMG
- en: Reward matrix; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: The reward matrix for transitioning into a certain state is visualized above.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Going from state 1 -> 8 would give a reward of -1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going from state 9 -> 10 would give a reward of -10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going from state 33 -> 34 would give a reward of 10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The policy that our agent will follow is a random policy — transitioning into
    each of the states is uniform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2555fd4214edb5815094a9072e2b5f46.png)'
  prefs: []
  type: TYPE_IMG
- en: The policy matrix; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The grey states in the policy matrix represent the terminal states: if the
    agent chooses to go to that state, the episode will end and the agent will be
    reset to state 0.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The goal of the TD(0) algorithm is to evaluate each state’s value for the
    given policy.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, we want to fill out the value matrix values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a3440272edf8dca009a125014db17b4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial value matrix; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: The **TD(0)** algorithm is short for **one step temporal differences** algorithm.
    In order to start building intuition and putting it broadly, in this algorithm
    our agent makes one step following the given policy, observes the reward and updates
    the estimates of the state value after such a step.
  prefs: []
  type: TYPE_NORMAL
- en: 'To put it mathematically, the update step is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc019e96ebe04404524df5596f5c0a45.png)'
  prefs: []
  type: TYPE_IMG
- en: TD(0) update equation
  prefs: []
  type: TYPE_NORMAL
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**s prime** — the state to which our agent goes from the current state s.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward **r** is equal to the reward of transitioning to s prime.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gamma** is the discount rate (more than 0, less than or equal to 1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alpha** is the size (more than 0, less than or equal to 1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The full algorithm¹ is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77bdaf7b738af3153323baee021c0110.png)'
  prefs: []
  type: TYPE_IMG
- en: Full TD(0); Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: The TD(0) algorithm is a **prediction** algorithm. In RL, a prediction algorithm
    refers to an algorithm that tries to estimate the state values while **not** changing
    the given policy (probabilities of transition).
  prefs: []
  type: TYPE_NORMAL
- en: It is also a **bootstrap** algorithm because we are using the current estimate
    of the value function to estimate the value function for the next state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we are only interested in the state values — the total expected accumulated
    reward if the agent moves from the current state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c69146c3676f9e8d5c3aae5cc0b9a450.png)'
  prefs: []
  type: TYPE_IMG
- en: State value
  prefs: []
  type: TYPE_NORMAL
- en: Now let us start the implementation of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that our agent needs is to make a move based on our created
    policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When an agent is in state s, he can only go to states that are present in the
    policy matrix dictionary. For example, all the actions in state 1 are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f28b8b65ac45770c6f867938c3719b6c.png)'
  prefs: []
  type: TYPE_IMG
- en: All the possible actions from state 1
  prefs: []
  type: TYPE_NORMAL
- en: The sum of all the probabilities is equal to 1 and our agent chooses randomly
    from the actions **right, left or down** (refer to the state matrix plot to see
    where the state is).
  prefs: []
  type: TYPE_NORMAL
- en: 'The above action is all that is needed to start updating the value function.
    When our agent makes a move, it transitions to another state and collects the
    reward of that state. We then apply the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc019e96ebe04404524df5596f5c0a45.png)'
  prefs: []
  type: TYPE_IMG
- en: TD(0) update equation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The final thing to do is to wrap everything into a **while loop** and only
    stop exploring when our agent transitions into a terminal state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We now have everything we need to implement the full TD(0) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Let us define 10000 episodes and let our agent learn!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of actions our agent took before termination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/43bc9ae571c1200b08fa3af801a9413e.png)'
  prefs: []
  type: TYPE_IMG
- en: Number of moves; Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: On average, our agent took 10 moves before bumping into a terminal state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final evaluated state value matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1394931721b63e545ef719d8b174129b.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluated V using TD(0) and a random policy; Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, following the given policy, it is a very bad state to be in where
    our agent starts our journey. On average, starting from that state, the agent
    accumulates only a -9.96 reward. As we move closer to the goal state, however,
    the value increases.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the goal state and the hole states have a value of 0 because there
    is no exploration from these states — every time the agent transitions there the
    game ends.
  prefs: []
  type: TYPE_NORMAL
- en: 'What would happen if we chose another policy? For example, more often chose
    the direction to the ‘right’:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/66602a243f165f8344cf6d401141b74b.png)'
  prefs: []
  type: TYPE_IMG
- en: Number of moves with a different policy
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3cbab1ef6cc6bcd0ba876784e9803804.png)'
  prefs: []
  type: TYPE_IMG
- en: Value matrix of the different policy
  prefs: []
  type: TYPE_NORMAL
- en: The random policies state value matrix sum is equal to **-249.29** while the
    policy with a bigger probability to go to the right has a sum of **-213.51**.
  prefs: []
  type: TYPE_NORMAL
- en: In that sense, we can say that moving more often to the right is a better policy!
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I presented the first sample-based algorithm in RL — the one-step
    temporal difference algorithm or TD(0).
  prefs: []
  type: TYPE_NORMAL
- en: It is a prediction algorithm, meaning, it is only used to evaluate the states
    for the given policy. Changing a policy will yield different state-value results.
  prefs: []
  type: TYPE_NORMAL
- en: Happy learning and happy coding everyone!
  prefs: []
  type: TYPE_NORMAL
- en: '[1]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Author: **Richard S. Sutton, Andrew G. Barto**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Year: **2018**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Page: **120**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Title: **Reinforcement Learning: An Introduction**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL:[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
