# 6 种聚类方法 — 概述

> 原文：[https://towardsdatascience.com/6-types-of-clustering-methods-an-overview-7522dba026ca](https://towardsdatascience.com/6-types-of-clustering-methods-an-overview-7522dba026ca)

## 聚类方法和算法的类型以及何时使用它们

[](https://kayjanwong.medium.com/?source=post_page-----7522dba026ca--------------------------------)[![Kay Jan Wong](../Images/28e803eca6327d97b6aa97ee4095d7bd.png)](https://kayjanwong.medium.com/?source=post_page-----7522dba026ca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7522dba026ca--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7522dba026ca--------------------------------) [Kay Jan Wong](https://kayjanwong.medium.com/?source=post_page-----7522dba026ca--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7522dba026ca--------------------------------) ·8 分钟阅读·2023年3月24日

--

![](../Images/78a95243080c867f084198a587c15efc.png)

由 [Kier in Sight](https://unsplash.com/@kierinsight?utm_source=medium&utm_medium=referral) 拍摄的照片，发布于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

聚类是无监督学习的一个分支，其中**未标记的数据**被划分为不同的组，相似的数据实例被分配到同一聚类，而不相似的数据实例则被分配到不同的聚类。聚类在市场细分、异常检测和网络分析等领域有广泛的应用。

不同类型的聚类方法各有其优缺点。本文介绍了不同类型的聚类方法及算法示例，以及何时使用每种算法。

# 目录

+   基于质心/划分 (*K-means*)

+   基于连通性的 (*层次聚类*)

+   基于密度的 (*DBSCAN*)

+   图基的 (*亲和传播*)

+   基于分布的 (*高斯混合模型*)

+   基于压缩的 (*谱聚类, BIRCH*)

# 基于质心/划分

> 基于质心的方法通过数据点到质心（聚类中心）的接近程度来将数据点分组

![](../Images/c8c964bd30a5eddbc2e8dd1378b8b3dd.png)

图 1：基于质心的方法（K-means）的聚类输出示例 — 图片来自 [sklearn](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html)

数据点到质心的接近程度通过以下方式测量

+   **欧几里得距离**：最短（直线）距离，适用于数值特征，这是默认的距离度量

+   **曼哈顿距离**：绝对差异的总和，适用于分类特征

+   **汉明距离**：不同位数的百分比，适用于二进制特征

+   **马哈拉诺比斯距离**：欧几里得距离的标准化形式

+   **闵可夫斯基距离**：欧几里得距离和曼哈顿距离的广义形式

+   **切比雪夫距离**：最大绝对差异

无论使用哪种距离度量，执行聚类时数值特征都应始终标准化！

## K-means

K-means 涉及初始化并执行迭代期望最大化（EM）步骤，直到收敛或达到最大迭代次数。

在初始化期间，`k` 个质心是根据 `k-means++` 优化算法分配的。在期望（E）步骤中，数据点根据其最近的（欧几里得距离）质心分配到聚类中。在最大化（M）步骤中，质心被更新以最小化惯性或聚类内平方和（WCSS）。EM 步骤重复进行，直到收敛到局部最小值，聚类分配和质心不再变化。

## 何时使用 K-means

+   你希望**可解释性**：K-means 易于理解和解释。

+   聚类是**均匀大小和球形**的：K-means 在聚类是良好分离的球形时表现良好，但如果聚类是长而不规则的形状，它的表现则不佳。

## 何时不要使用 K-means

+   你**不确定聚类的数量**：K-means 需要预定义聚类的数量。通常使用肘部法，绘制 WCSS 与聚类数量的关系图，以确定最优的聚类数量。

+   你的数据中有**离群点**：所有数据点都被分配到一个聚类中，因此离群点的存在可能会扭曲结果，必须去除或转换。

+   你希望**计算效率**：K-means 的计算成本随数据规模增加而增加，因为它的运行时间是 `O(tkn)`，其中 `t` 是迭代次数，`k` 是聚类数量，`n` 是数据点数量。使用降维算法如 PCA 可以加速计算。

> **其他基于质心的算法**：K-Medoids、均值漂移、模糊聚类（软K-means；数据点可以属于多个聚类）
> 
> **编辑**：还有方差比准则（VRC）和贝叶斯信息准则（BIC）作为比肘部法更鲁棒的替代方法（感谢 Victor 提供的信息 — 论文链接到 [paper](https://ar5iv.labs.arxiv.org/html/2212.12189)）。

# 基于连通性

> 基于连通性的方法根据聚类之间的接近度将数据点分组在一起

![](../Images/8b314755a33b723683abaf21331467f4.png)

图 2：基于连通性的方法（层次聚类）的聚类输出示例 — 图片来自 [sklearn](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html)

连结标准计算聚类之间的接近度，

+   **单一连结**：聚类间最接近点的距离；聚类间的最小距离。适用于检测任意形状的聚类，但无法检测重叠的聚类。计算效率高，但对噪声数据不够鲁棒。

+   **完全/最大连结**：聚类间最远点之间的距离；聚类间的最大距离。适用于检测重叠聚类，但无法检测任意形状的聚类

+   **平均连结**：两个聚类间所有距离的平均值

+   **质心连结**：两个聚类中心之间的距离

+   **Ward连结**：每个数据点到其分配聚类的质心的平方距离总和。这导致合并聚类时，所有聚类内总方差增加最小。Ward连结是默认的连结标准

## 层次聚类

凝聚层次聚类通过进行迭代的**自下而上方法**工作，其中每个数据点被视为一个单独的聚类，并且两个最接近（按连接标准）的聚类被迭代合并，直到只剩下一个大聚类。

分裂层次聚类的做法正好相反，执行迭代的**自上而下方法**，从一个大聚类开始，不断分解为两个较小的聚类，直到每个数据点成为一个单独的聚类。

## 何时使用层次聚类

+   你对**聚类数量不确定**：层次聚类需要预定义聚类数量，但可以在树状图上查看聚类，并且可以在不重新计算的情况下调整聚类数量。

+   你想要**计算效率**：在任何中间阶段的聚类标签可以被恢复，因此如果聚类数量改变，则不必重新计算相似度矩阵。

+   你有**高维数据**：输出可以通过树状图进行可视化，这可以用于更高维的数据。

# 基于密度

> 基于密度的方法根据密度而不是距离将数据点分组

![](../Images/eb1555c77c6d282903a4bb6da175c1fe.png)

图 3：基于密度的方法（DBSCAN）的聚类输出示例 — 图片来自 [sklearn](https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html)

## 基于密度的空间聚类应用于噪声（DBSCAN）

DBSCAN算法将密集区域中的数据点聚集在一起，用低密度区域分隔开。位于`eps`单位内且有超过`min_samples`数量的数据点的样本称为**核心样本**。

## 何时使用DBSCAN

+   你对**聚类数量不确定**：DBSCAN不需要预定义聚类数量。

+   你想要**任意形状的聚类**：聚类由密集区域决定，因此聚类形状可以是奇怪或复杂的。

+   你想要**检测离群点**：那些未被分配到任何聚类的数据点，因为它们不落在任何密集区域，将被视为离群点。

## 何时不使用DBSCAN

+   当你想要**稳定的性能**：DBSCAN的输出受到`eps`参数的高度影响和敏感，必须为数据集选择适当的`eps`值。

> **其他基于密度的算法**：排序点以识别聚类结构（OPTICS）、层次DBSCAN（HDBSCAN）

# 基于图

> 基于图的方法根据图距离将数据点分组。

![](../Images/b08720086f04f4f4676ce89fc418e3c2.png)

图 4：基于图的方法（亲和传播）的聚类输出示例 — 图片来自 [sklearn](https://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html)

## 亲和传播

亲和传播通过数据点之间的成对消息传递直到收敛。选择代表周围数据点的最佳点作为样本点，并将每个点分配到其最近样本点的簇中。

## 何时使用亲和传播

+   你**不确定簇的数量**：亲和传播不要求预定义簇的数量。

## **何时不使用亲和传播**

+   你想要**计算效率**：亲和传播的时间复杂度为`O(tn²)`，其中`t`是迭代次数，`n`是数据点数量，因为消息在每个数据点之间成对发送。

+   你想要**空间效率**：如果使用密集相似度矩阵，亲和传播会占用`O(n²)`的内存，因为在每个数据点之间发送的消息被视为‘投票’，以选举样本点。

# 基于分布

> 基于分布的方法根据数据点属于同一概率分布的可能性将数据点分组。

![](../Images/f2001805fbeef5503b21d7baae18ca9a.png)

图 5：基于分布的方法（GMM）的聚类输出示例 — 图片来自 [sklearn](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_selection.html)

基于分布的方法使用统计推断来对数据进行聚类，使得数据点距离中心点越近，被分配到该簇的概率越高。

与考虑距离度量的聚类方法相比，基于分布的方法在确定簇的形状方面更具灵活性，前提是簇遵循预定义的分布，如合成数据或模拟数据。如果簇是噪声的，结果会过拟合。

## 高斯混合模型（GMM）

GMM 假设每个数据点都是由多个具有未知参数的高斯分布生成的，并通过迭代的期望最大化（EM）步骤来拟合数据点。

在期望（E）步骤中，数据点被分配到假设随机选择的高斯参数的簇中。在最大化（M）步骤中，高斯分布的参数被更新，以最佳地拟合簇中的数据点。

## 何时使用高斯混合模型

+   你想要**计算效率**：GMM 是最快的基于分布的算法。

+   你的簇遵循**高斯分布**：分布可以调整以适应球形、对角线、绑定或完全协方差矩阵。

## 何时不使用高斯混合模型

+   你有**每个簇的数据不足**：当高斯分布中的数据点不足时，计算协方差矩阵会很困难。

# 基于压缩的

> 基于压缩的方法将数据点转换为嵌入，然后在低维数据上执行聚类。

## 谱聚类

谱聚类将数据点之间的亲和矩阵转换为低维嵌入，然后再进行聚类（如K均值）。

## 何时使用谱聚类

+   你希望**计算效率**：如果亲和矩阵稀疏且使用`pyamg`求解器，谱聚类速度很快。

## 何时不使用谱聚类

+   你**不确定簇的数量**：谱聚类需要预定义簇的数量，并且簇的数量应该尽可能小。

## BIRCH

BIRCH对数据点进行有损压缩，生成一组聚类特征节点（CF Nodes），形成聚类特征树（CFT）。新数据点被‘插入’到树中，直到到达叶子节点，并在节点内存储有关子簇的信息。

最终簇质心的信息从叶子节点读取，其他聚类算法（如层次聚类）可以随之执行。

## 何时不使用BIRCH

+   你有**高维数据**：BIRCH对高维数据的扩展性较差。

有趣的是，通过完全不同的方法和算法可以实现获取簇的最终目标。本文并不旨在覆盖所有可能的聚类算法或深入探讨每种算法中的数学公式，但希望能提供一些关于聚类方法类型及其应用场景的高层次细节。

# 相关故事

聚类的另一个方面是评估聚类算法，以确定哪个算法在统计度量上表现*最佳*。

[](/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=post_page-----7522dba026ca--------------------------------) [## 7 聚类算法评估指标

### 使用Python示例深入解释无监督学习评估指标

towardsdatascience.com](/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=post_page-----7522dba026ca--------------------------------)

# 相关链接

+   `sklearn` 聚类文档：[https://scikit-learn.org/stable/modules/clustering.html](https://scikit-learn.org/stable/modules/clustering.html)
