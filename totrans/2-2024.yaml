- en: 'The History of Open-Source LLMs: Early Days (Part One)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源LLMs的历史：早期阶段（第一部分）
- en: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8](https://towardsdatascience.com/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8](https://towardsdatascience.com/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8)
- en: Understanding GPT-Neo, GPT-J, GLM, OPT, BLOOM, and more…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解GPT-Neo、GPT-J、GLM、OPT、BLOOM等…
- en: '[](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)
    ·20 min read·Nov 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)
    ·阅读时间20分钟·2023年11月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/05d30e1fd65a03fd2cb858285b0d7f58.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05d30e1fd65a03fd2cb858285b0d7f58.png)'
- en: (Photo by [Chris Lawton](https://unsplash.com/@chrislawton?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/stack-of-six-brown-hardbound-books-9T346Ij4kGk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由 [Chris Lawton](https://unsplash.com/@chrislawton?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来自 [Unsplash](https://unsplash.com/photos/stack-of-six-brown-hardbound-books-9T346Ij4kGk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)）
- en: Research on language modeling has a long history that dates back to models like
    GTP and GPT-2 or even RNN-based techniques (e.g., [ULMFit](https://arxiv.org/abs/1801.06146))
    that predate modern, transformer-based language models. Despite this long history,
    however, language models have only become popular relatively recently. The first
    rise in popularity came with the proposal of GPT-3 [1], which showed that impressive
    few-shot learning performance could be achieved across many tasks via a combination
    of self-supervised pre-training and in-context learning; see below.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模的研究有着悠久的历史，可以追溯到类似GTP和GPT-2的模型，甚至是早于现代变换器基础语言模型的RNN技术（例如，[ULMFit](https://arxiv.org/abs/1801.06146)）。尽管历史悠久，但语言模型在相对最近才变得流行。第一次流行的兴起来自于GPT-3的提出，[1]展示了通过自监督预训练和上下文学习的结合，可以在许多任务中实现令人印象深刻的少量学习性能；见下文。
- en: '![](../Images/3052b7fe6011e83507c05a91e32c5cda.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3052b7fe6011e83507c05a91e32c5cda.png)'
- en: (from [1])
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: After this, the recognition garnered by GPT-3 led to the proposal of a swath
    of large language models (LLMs). Shortly after, research on language model alignment
    led to the creation of even more impressive models like InstructGPT [19] and,
    most notably, its sister model ChatGPT. The impressive performance of these models
    led to a flood of interest in language modeling and generative AI.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，GPT-3获得的认可促使了大量大型语言模型（LLMs）的提出。不久之后，语言模型对齐的研究导致了更为出色的模型的创建，如InstructGPT
    [19]，以及最著名的其姊妹模型ChatGPT。这些模型的出色表现引发了对语言建模和生成式AI的广泛兴趣。
- en: Despite being incredibly powerful, many early developments in LLM research have
    one common property — *they are closed source*. When language models first began
    to gain widespread recognition, many of the most powerful LLMs were only accessible
    via paid APIs (e.g., the [OpenAI API](https://openai.com/blog/openai-api)) and
    the ability to research and develop such models was restricted to select individuals
    or labs. Such an approach is markedly different from typical AI research practices,
    where openness and idea sharing is usually encouraged to promote forward progress.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管非常强大，但许多早期 LLM 研究中的发展有一个共同特点——*它们是闭源的*。当语言模型首次开始获得广泛认可时，许多最强大的 LLM 只能通过付费
    API 访问（例如，[OpenAI API](https://openai.com/blog/openai-api)），而研究和开发这些模型的能力仅限于特定的个人或实验室。这种方法明显不同于典型的
    AI 研究实践，后者通常鼓励开放和思想共享，以促进前进的进步。
- en: “This restricted access has limited researchers’ ability to understand how and
    why these large language models work, hindering progress on efforts to improve
    their robustness and mitigate known issues such as bias and toxicity.” *— from
    [4]*
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “这种受限的访问限制了研究人员理解这些大型语言模型如何以及为何工作的能力，阻碍了改进其鲁棒性和缓解已知问题（如偏见和毒性）的努力。” *——来自 [4]*
- en: '**This overview.** Despite the initial emphasis upon proprietary technology,
    the LLM research community slowly began to create open-source variants of popular
    language models like GPT-3\. Although the first open-source language models lagged
    behind the best proprietary models, they laid the foundation for improved transparency
    within LLM research and catalyzed the development of many subsequent models that
    were more powerful (e.g., Falcon [10] and [LLaMA-2](https://ai.meta.com/llama/)).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**这一概述。** 尽管最初强调的是专有技术，LLM 研究社区慢慢开始创建流行语言模型如 GPT-3 的开源变体。尽管首批开源语言模型在性能上落后于最佳的专有模型，但它们为
    LLM 研究中的改进透明度奠定了基础，并催化了许多更强大的后续模型的发展（例如，Falcon [10] 和 [LLaMA-2](https://ai.meta.com/llama/)）。'
- en: This overview is part of a three part series exploring the history of open-source
    language models. Here, we will learn about the beginning of this history, including
    several initial attempts at creating open-source language models. Although these
    models left something to be desired in terms of performance, they are incredibly
    important to understand, as the revolution of open-source LLMs that ensued was
    entirely based upon these models. In the following two parts of the series, we
    will learn more about recent open-source LLMs, as well as how imitation and alignment
    techniques have been used to improve their performance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这一概述是一个三部分系列的一部分，探索开源语言模型的历史。在这里，我们将了解这一历史的开端，包括若干初步尝试创建开源语言模型的努力。尽管这些模型在性能上有所欠缺，但它们对理解至关重要，因为随后的开源
    LLM 革命完全基于这些模型。在系列的接下来的两部分中，我们将深入了解最近的开源 LLM 以及如何使用模仿和对齐技术来提高它们的性能。
- en: '![](../Images/b4a5c7276cf2250f14c9aa6ede26289b.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4a5c7276cf2250f14c9aa6ede26289b.png)'
- en: (from [12, 20])
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [12, 20]）
- en: The Mechanics of a Language Model
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型的机制
- en: Open-source LLM research catalyzed transparency and idea sharing, creating an
    environment in which researchers could collaborate and innovate more quickly.
    Put simply, *the beauty of open-source LLM research is that it gives us the potential
    to study these incredible models and develop a deeper understanding of how they
    work*. There are no unknown tricks hidden behind a paid API or black box. Open-source
    LLMs allow us to look at the code, run experiments, and even try out our own ideas
    and modifications — we have full access to the underlying model!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 开源 LLM 研究催化了透明度和思想共享，创造了一个研究人员能够更快地合作和创新的环境。简单来说，*开源 LLM 研究的美在于它让我们有可能研究这些令人难以置信的模型，并深入理解它们的工作原理*。没有隐藏在付费
    API 或黑箱中的未知技巧。开源 LLM 允许我们查看代码，进行实验，甚至尝试我们自己的想法和修改——我们对基础模型拥有完全的访问权限！
- en: “A much broader segment of the AI community needs access to these models in
    order to conduct reproducible research and collectively drive the field forward.”
    *— from [4]*
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “AI 社区的更广泛领域需要访问这些模型，以便进行可重复的研究，并共同推动该领域的发展。” *——来自 [4]*
- en: But, to build a deep understanding of such models, we first need to understand
    the basics behind how they work. Within this section, we will overview these ideas,
    attempting to provide a (relatively) comprehensive understanding of LLMs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但要深入理解这些模型，我们首先需要了解它们的基本工作原理。在本节中，我们将概述这些理念，尝试提供对大型语言模型（LLMs）的（相对）全面理解。
- en: The Language Modeling Objective
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言建模目标
- en: '![](../Images/c5c67554d791155729ae174c08d04176.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5c67554d791155729ae174c08d04176.png)'
- en: Pre-training with a language modeling objective (created by author)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用语言建模目标进行预训练（由作者创建）
- en: At the core of language modeling is next token prediction (also called the standard
    language modeling objective), which is used to train nearly all language models.
    To train a language model using next token prediction, we need a large corpus
    of raw text. Using this corpus, we train the model by *i)* sampling some text
    from the dataset and *ii)* training the model to predict the next word; see above.
    Because the ground truth next token can always be deduced from the raw text, next
    token prediction is a form of self-supervised learning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模的核心是下一个令牌预测（也称为标准语言建模目标），它用于训练几乎所有的语言模型。为了使用下一个令牌预测训练语言模型，我们需要一个大规模的原始文本语料库。利用这个语料库，我们通过*
    i)* 从数据集中抽取一些文本和* ii)* 训练模型以预测下一个单词来训练模型；见上文。因为真实的下一个令牌总是可以从原始文本中推断出来，所以下一个令牌预测是一种自监督学习的方法。
- en: '**What is a token?** One can roughly consider next token prediction to be predicting
    the next word in a sequence, given a few preceding words as context. However,
    this analogy is not perfect, as tokens and words are not exactly equal. When a
    language model receives text as input, the raw text is first tokenized (i.e.,
    converted into a sequence of discrete words or sub-words); see below.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是令牌？** 可以大致将下一个令牌预测理解为在给定前几个单词作为上下文的情况下预测序列中的下一个单词。然而，这种类比并不完全准确，因为令牌和单词并不完全相等。当语言模型接收文本作为输入时，原始文本首先被分词（即，转换为离散的单词或子单词序列）；见下文。'
- en: '![](../Images/6c211f85a2ac29498592c09a5374c7cd.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c211f85a2ac29498592c09a5374c7cd.png)'
- en: Converting raw text into a sequence of tokens (created by author)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始文本转换为令牌序列（由作者创建）
- en: The tokenizer associated with a language model typically has a fixed-size vocabulary,
    or set of viable tokens that can be created from a textual sequence.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与语言模型相关联的分词器通常具有固定大小的词汇表，或可以从文本序列中创建的可用令牌集合。
- en: '**Predicting next tokens.** Once a sequence of tokens has been created, the
    language model has an embedding layer that stores a unique and learnable vector
    embedding for every token within the tokenizer’s vocabulary. Using this embedding
    layer, we can convert each token within the input sequence into a corresponding
    vector embedding, forming a sequence of token vectors; see below.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测下一个令牌。** 一旦创建了令牌序列，语言模型就具有一个嵌入层，为分词器词汇表中的每个令牌存储一个唯一且可学习的向量嵌入。利用这个嵌入层，我们可以将输入序列中的每个令牌转换为相应的向量嵌入，形成令牌向量序列；见下文。'
- en: '![](../Images/625fd4aecf4cab6c12ca2f211743b89d.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/625fd4aecf4cab6c12ca2f211743b89d.png)'
- en: Tokenizing and embedding raw text data (created by author)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对原始文本数据进行分词和嵌入（由作者创建）
- en: After adding positional embeddings to each token, we can pass this sequence
    of token vectors into a decoder-only transformer (more explanation will follow),
    which transforms (no pun intended) each of these token vectors and produces a
    corresponding output vector for each token. Notably, the number of output vectors
    is the same as the number of input vectors; see below.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个令牌上添加位置嵌入后，我们可以将这个令牌向量序列传递给仅解码器的变换器（更多解释将随后介绍），它将这些令牌向量进行转换（无双关意味），并为每个令牌生成相应的输出向量。值得注意的是，输出向量的数量与输入向量的数量相同；见下文。
- en: '![](../Images/ba20b8238f80f8417a7af042de5c78dd.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba20b8238f80f8417a7af042de5c78dd.png)'
- en: Processing tokens with a decoder-only transformer (created by author)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用仅解码器的变换器处理令牌（由作者创建）
- en: Now that we have an output representation for each token, we are ready to perform
    next-token prediction! For each token in the sequence, we simply take its output
    token vector and use this to predict the token that comes next in the sequence!
    An illustration of this process is shown below. In practice, this next token prediction
    objective is simultaneously computed over all tokens in the sequence (and over
    all sequences in a mini-batch!) to maximize efficiency.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们为每个令牌有了输出表示，我们准备进行下一个令牌预测了！对于序列中的每个令牌，我们只需取其输出令牌向量并使用它来预测序列中接下来的令牌！下面展示了这一过程的示意图。在实际应用中，这一目标同时在序列中的所有令牌（以及在一个小批量中的所有序列）上进行计算，以最大化效率。
- en: '![](../Images/2e4f766f68636c5671f11d86eec16030.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e4f766f68636c5671f11d86eec16030.png)'
- en: Computing the next token prediction training objective (created by author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 计算下一个令牌预测训练目标（由作者创建）
- en: Due to the use of causal (or masked) self-attention, each output token vector
    only considers the current token and those that is come before it in the sequence
    when computing its representation. If we were to use bidirectional self-attention,
    each output token vector would be computed by looking at the entire sequence of
    vectors, which would allow the model to cheat and solve next token prediction
    by just copying the token that comes next in the sequence. As such, *masked self-attention
    is needed for next-token prediction*. But, what is self-attention and — more fundamentally
    — what is a transformer? Let’s dive into this next.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用了因果（或掩蔽）自注意，每个输出令牌向量在计算其表示时只考虑当前令牌以及序列中其之前的令牌。如果我们使用双向自注意，每个输出令牌向量将通过查看整个向量序列来计算，这将允许模型作弊，通过仅仅复制序列中接下来的令牌来解决下一个令牌预测。因此，*掩蔽自注意是下一个令牌预测所必需的*。但是，自注意到底是什么，更根本地说，变换器是什么？接下来我们将深入探讨。
- en: '**A quick note.** The phrase “language model” may sometimes be used to refer
    to models beyond those that specialize in performing next token prediction. For
    example, BERT [18] is considered by some to be a “language model”, but it is trained
    using a [Cloze](https://en.wikipedia.org/wiki/Cloze_test)-style objective and
    is not a generative model. As such, language models that specialize in next token
    prediction are oftentimes distinguished as “causal” language models. Here, we
    will use both of these terms interchangeably to refer to models that specialize
    in next token prediction.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**一个快速说明。** “语言模型”这一术语有时可能被用来指代那些不仅专注于执行下一个令牌预测的模型。例如，BERT [18] 被一些人认为是“语言模型”，但它是使用[Cloze](https://en.wikipedia.org/wiki/Cloze_test)风格的目标进行训练的，并不是生成模型。因此，专注于下一个令牌预测的语言模型通常被称为“因果”语言模型。在这里，我们将这两个术语交替使用，指代那些专注于下一个令牌预测的模型。'
- en: The Transformer Architecture and its Variants
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器架构及其变体
- en: '![](../Images/d4063e56b4061772fa70fc6774e8504c.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4063e56b4061772fa70fc6774e8504c.png)'
- en: (from [17])
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [17])
- en: 'All language models use some variant of the transformer architecture. This
    architecture (shown above) was originally proposed in [17] for solving sequence-to-sequence
    tasks. However, it was subsequently extended to solve a variety of different problems,
    from assessing the semantic similarity of text to classifying images. In its original
    form, the transformer architecture has two components:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所有语言模型都使用某种变体的变换器架构。这种架构（如上图所示）最初在[17]中提出，用于解决序列到序列任务。然而，它随后被扩展以解决各种不同的问题，从评估文本的语义相似性到图像分类。在其原始形式中，变换器架构有两个组成部分：
- en: '*Encoder*: each block performs bidirectional self-attention and a pointwise
    feed-forward transformation, which are separated with a residual connection and
    LayerNorm.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*编码器*：每个块执行双向自注意和一个逐点前馈变换，这些操作之间通过残差连接和LayerNorm分隔。'
- en: '*Decoder*: each block performs causal self-attention, [cross attention](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html#cross-attention)
    (i.e., self-attention across encoder and decoder tokens), and a pointwise feed-forward
    transformation, each separated by a residual connection and LayerNorm.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解码器*：每个块执行因果自注意，[交叉注意](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html#cross-attention)（即跨编码器和解码器令牌的自注意），以及一个逐点前馈变换，每个部分之间通过残差连接和LayerNorm分隔。'
- en: When both components of the architecture are present, the encoder processes
    the input sequence and produces an output sequence. Then, the decoder generates
    its own output sequence, given the encoder’s output sequence as input. In other
    words, the encoder processes the entire input sequence to form a representation
    that the decoder uses as context when generating output. As a whole, the transformer
    takes a sequence as input and produces a new sequence as output.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当架构的两个组件都存在时，编码器处理输入序列并生成一个输出序列。然后，解码器生成自己的输出序列，输入为编码器的输出序列。换句话说，编码器处理整个输入序列以形成解码器在生成输出时使用的表示。整体上，转换器接受一个序列作为输入并生成一个新的序列作为输出。
- en: '![](../Images/ff8ee0466c2a6832b16e3a042577e9d5.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff8ee0466c2a6832b16e3a042577e9d5.png)'
- en: (from [17])
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [17]）
- en: '**Decoder-only and encoder-only transformers.** Nearly all causal language
    models use a decoder-only transformer as their underlying architecture, which
    is just a normal transformer with the encoder-portion of the architecture removed;
    see above. Additionally, the cross attention portion of each decoder block is
    removed due to the lack of an encoder (i.e., we can’t attend to an encoder that
    doesn’t exist)! Alternatively, one could form an encoder-only architecture by
    just using the encoder portion of the architecture. Encoder-only architectures
    (e.g., BERT [18]) excel at solving a variety of discriminative natural language
    tasks, but they are not used for generating text. To learn more, check out the
    link [here](/language-understanding-with-bert-c17a453ada1a).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**仅解码器和仅编码器的转换器。** 几乎所有因果语言模型都使用仅解码器转换器作为其基础架构，这只是一个普通的转换器，其编码器部分被移除；见上文。此外，由于缺少编码器（即，我们不能关注不存在的编码器），每个解码器块的交叉注意力部分也被移除！或者，可以通过仅使用架构中的编码器部分来形成仅编码器架构。仅编码器架构（例如，BERT
    [18]）在解决各种判别自然语言任务方面表现出色，但它们不用于生成文本。要了解更多信息，请查看[这里](/language-understanding-with-bert-c17a453ada1a)的链接。'
- en: '**Why the decoder?** The choice of using the decoder-only architecture (as
    opposed to encoder-only or the full encoder-decoder transformer) for LLMs is not
    arbitrary. Rather, this choice is driven by the use of next-token prediction for
    training language models. The use of masked self-attention within the decoder
    ensures that the model cannot look forward in the sequence when predicting the
    next token. Otherwise, next-token prediction would be trivial, as the model could
    simply copy the next token; see below.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么选择解码器？** 选择仅解码器架构（与仅编码器或完整的编码器-解码器转换器相比）用于大语言模型并非随意决定。相反，这一选择是由于训练语言模型时使用下一个词预测。解码器中使用的掩码自注意力确保模型在预测下一个词时无法向前查看序列。否则，下一个词预测将变得非常简单，因为模型可以直接复制下一个词；见下文。'
- en: '![](../Images/3d35c55158d6953194bda3c49b455440.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d35c55158d6953194bda3c49b455440.png)'
- en: Causal self-attention is used for next-token prediction (created by author)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因果自注意力用于下一个词预测（作者创建）
- en: To perform next token prediction without cheating, both encoder-only and encoder-decoder
    transformers would have to avoid including any ground truth next token in their
    input sequence. To do this, we could *i)* ingest a prefix and *ii)* predict the
    token that follows this prefix. However, this approach is a bit inefficient because
    we can only predict a single next token at a time. In contrast, decoder-only models,
    due to their use of masked self-attention, can ingest an entire sequence of tokens
    and apply a language modeling objective to every token within the sequence. Plus,
    several papers [12] have shown practically that decoder-only architectures yield
    the best performance for next token prediction.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在不作弊的情况下进行下一个词预测，无论是仅编码器还是编码器-解码器转换器，都必须避免在其输入序列中包含任何真实的下一个词。为此，我们可以 *i)*
    处理一个前缀，并 *ii)* 预测跟随该前缀的词。然而，这种方法效率较低，因为我们一次只能预测一个下一个词。相比之下，只有解码器的模型由于使用掩码自注意力，可以处理整个词序列，并将语言建模目标应用于序列中的每个词。此外，一些论文
    [12] 实际上表明，仅解码器架构在下一个词预测中表现最佳。
- en: '**How do we generate text?** Given the decoder-only architecture outlined above,
    generating text follows a simple autoregressive process. We just continually predict
    the next token, add this token to our input, and repeat; see below.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们如何生成文本？** 鉴于上面概述的仅解码器架构，生成文本遵循简单的自回归过程。我们只需不断预测下一个词，将此词添加到输入中，并重复进行；见下文。'
- en: '![](../Images/0eb74f1f7b9395dd27740f1c6cf3e9b5.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0eb74f1f7b9395dd27740f1c6cf3e9b5.png)'
- en: Generating text with a language model (created by author)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用语言模型生成文本（作者创建）
- en: Training and Using Language Models
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和使用语言模型
- en: To complete our understanding of language models, we need to quickly explore
    how these models are typically trained and used in practice. Although a lot of
    research has been done in this area, most language models are trained according
    to a few standard techniques, as proposed in [19]; see below.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面理解语言模型，我们需要快速探索这些模型在实际中通常是如何训练和使用的。尽管在这一领域做了大量的研究，但大多数语言模型都是按照 [19] 中提出的一些标准技术进行训练的；详见下文。
- en: '![](../Images/39249c2721cd023c5fa54ec2e5dfdcda.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39249c2721cd023c5fa54ec2e5dfdcda.png)'
- en: LLM training components (from [19])
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 训练组件（来自 [19]）
- en: Language models can learn in a variety of different ways. Here, we will focus
    on pre-training, alignment, and in-context learning, which collectively encompass
    most of what’s required to train an LLM and use it in a practical application.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型可以通过多种不同方式进行学习。在这里，我们将重点讨论预训练、对齐和上下文学习，这些方法共同涵盖了训练大型语言模型（LLM）和在实际应用中使用它所需的大部分内容。
- en: '**Pre-training.** The pre-training process is the initial and most computationally
    expensive step of creating an LLM. Beginning with a randomly-initialized LLM,
    we must train this model — using a language modeling objective — over a massive
    corpus of raw text that is curated from a variety of different sources. Prior
    research [1] has shown us that by pre-training a very large model (i.e., lots
    of parameters) over a large dataset, we can obtain a [foundation model](https://crfm.stanford.edu/)
    that can accurately solve a variety of different tasks by performing next token
    prediction. To get the best results, we need scale in terms of both data and model
    size.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**预训练。** 预训练过程是创建 LLM 的初步且计算开销最大的步骤。从一个随机初始化的 LLM 开始，我们必须使用语言建模目标对这个模型进行训练——在从各种不同来源中整理出的海量原始文本语料库上。之前的研究
    [1] 向我们展示，通过在一个大型数据集上对一个非常大的模型（即，参数很多）进行预训练，我们可以获得一个 [基础模型](https://crfm.stanford.edu/)，它可以通过执行下一个
    token 预测来准确地解决各种不同的任务。为了获得最佳结果，我们需要在数据和模型规模方面进行扩展。'
- en: '**What else do we need?** Language models that solely undergo pre-training
    can be powerful. Look at GPT-3 [1] and Chinchilla [15] for a few examples. However,
    there is a reason that LLMs did not explode in popularity until the proposal of
    models like ChatGPT — *just performing next token prediction is not very interesting*.
    Oftentimes, predicting the statistically-correct next token, although it leads
    to reasonable text being generated, produces output that is repetitive, simple,
    and generally not helpful. We needed some way to make LLMs craft outputs that
    are more helpful and interesting to us as humans!'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们还需要什么？** 仅仅经历预训练的语言模型可以是强大的。看看 GPT-3 [1] 和 Chinchilla [15] 的一些例子。然而，LLM
    直到像 ChatGPT 这样的模型提出后才突然流行起来是有原因的——*仅仅进行下一个 token 预测并不是非常有趣*。通常，预测统计上正确的下一个 token，虽然生成了合理的文本，却会产生重复、简单且通常不太有用的输出。我们需要某种方法来使
    LLM 生成对我们人类更有帮助和更有趣的输出！'
- en: '![](../Images/c4bb95493aad5c7c6ec0d92b5ab366d0.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4bb95493aad5c7c6ec0d92b5ab366d0.png)'
- en: (from [19])
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [19]）
- en: 'Alignment refers to the process of fine-tuning an LLM to better align with
    the desires of human users. This is accomplished primarily via two techniques:
    supervised fine-tuning (SFT) and/or reinforcement learning from human feedback
    (RLHF). The desired behavior of an LLM depends a lot on the context or application
    in which it is deployed. However, alignment is a generic tool that can be used
    to arbitrarily fine-tune an LLM to behave in a certain way; see above. Recent
    research indicates that models do not learn new information during alignment.
    Rather, this process simply teaches the model how to properly format or present
    the knowledge that it has already gained from the pre-training process.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐是指对LLM进行微调的过程，以更好地符合人类用户的期望。这主要通过两种技术实现：监督微调（SFT）和/或来自人类反馈的强化学习（RLHF）。LLM的期望行为很大程度上取决于其部署的上下文或应用。然而，对齐是一种通用工具，可以用来随意地微调LLM，使其表现出特定的行为；详见上文。近期研究表明，模型在对齐过程中不会学习到新的信息。相反，这个过程只是教会模型如何正确地格式化或呈现其从预训练过程中已经获得的知识。
- en: '**Using LLMs in practice.** After we have pre-trained and fine-tuned (or aligned)
    our language model, the final step is to specialize the model to our desired application.
    This process may require extra fine-tuning over domain-specific data. More training
    is not always necessary, however, as we can accomplish a lot by just using in-context
    learning; see below.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**实际应用 LLM。** 在我们对语言模型进行预训练和微调（或对齐）之后，最后一步是将模型专门化为我们所需的应用。这一过程可能需要对领域特定的数据进行额外的微调。然而，并不总是需要更多的训练，因为我们可以通过使用上下文学习来完成很多工作；见下文。'
- en: '![](../Images/072087b61567a23057e21513b694a91d.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/072087b61567a23057e21513b694a91d.png)'
- en: (from [1])
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Put simply, in-context learning refers to the idea of solving a variety of different
    problems using a single, general-purpose foundation model (e.g., a pre-trained
    LLM). Given the generic text-to-text structure of a language model, this can actually
    be done quite easily. We just need to construct a textual problem-solving prompt
    that can be provided as input to the LLM; see below.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，上下文学习指的是使用一个单一的通用基础模型（例如，预训练的 LLM）来解决各种不同的问题。鉴于语言模型的通用文本到文本结构，这实际上是相当容易做到的。我们只需构建一个可以作为输入提供给
    LLM 的文本问题解决提示；见下文。
- en: '![](../Images/2ddf4727d70335b75398cbbc03591665.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ddf4727d70335b75398cbbc03591665.png)'
- en: Different prompt variants for solving an arithmetic problem (created by author)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 解决算术问题的不同提示变体（由作者创建）
- en: 'Then, the LLM should generate the answer to our problem as output. As such,
    we can solve many different problems by just modifying the input prompt! The process
    of constructing good prompts for solving problems is referred to as prompt engineering,
    and we have explored this idea extensively in previous posts:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，LLM 应该生成我们问题的答案作为输出。因此，我们可以通过仅仅修改输入提示来解决许多不同的问题！构建有效提示以解决问题的过程称为提示工程，我们在之前的文章中对此进行了广泛探讨：
- en: Practical Prompt Engineering [[link](/practical-prompt-engineering-74e96130abc4)]
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实用提示工程 [[link](/practical-prompt-engineering-74e96130abc4)]
- en: Advanced Prompt Engineering [[link](/advanced-prompt-engineering-f07f9e55fe01)]
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级提示工程 [[link](/advanced-prompt-engineering-f07f9e55fe01)]
- en: Initial Attempts at Open-Source LLMs
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源 LLM 的初步尝试
- en: Given the [expense of pre-training](https://www.mosaicml.com/blog/gpt-3-quality-for-500k),
    it took some time for the research community to pursue the creation of an open-source
    LLM, causing proprietary models like GPT-3 to become the standard. However, once
    the first few models were proposed, the floodgates opened and research on open-source
    LLM progressed rapidly (almost *too* rapidly). We will learn about a few of the
    early models here, while more recent open-source LLMs will be covered in future
    parts of the series.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 [预训练的高成本](https://www.mosaicml.com/blog/gpt-3-quality-for-500k)，研究社区花了一些时间才开始追求创建开源
    LLM，使得像 GPT-3 这样的专有模型成为标准。然而，一旦提出了最初的几个模型，闸门就打开了，开源 LLM 的研究迅速推进（几乎 *过于* 迅速）。我们将在这里了解一些早期模型，而更多最近的开源
    LLM 将在系列的未来部分中介绍。
- en: '[GPT-NeoX-20B](https://arxiv.org/abs/2204.06745) [6]'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[GPT-NeoX-20B](https://arxiv.org/abs/2204.06745) [6]'
- en: One of the first open-source LLMs — a 20 billion parameter model called GPT-NeoX-20B
    [6] — was created by [EleutherAI](https://www.eleuther.ai/). GPT-NeoX-20B was
    created after the initial GPT-Neo model (2.7 billion parameters) [22], was pre-trained
    over [the Pile](https://huggingface.co/datasets/EleutherAI/pile), and achieves
    impressive few-show learning performance (comparable to GPT-3) on a variety of
    natural language benchmarks. Although this model is somewhat small compared to
    GPT-3 (i.e., 20 billion parameters vs. 175 billion parameters), it was the largest
    open-source language model to be released at the time. Plus, all of code for training
    and evaluating the model was released alongside its weights under an [Apache 2.0
    license](https://www.planetcrust.com/what-does-apache-2-0-license-mean), which
    permits commercial use.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个开源 LLM 之一——一个名为 GPT-NeoX-20B 的 200 亿参数模型[6]——由 [EleutherAI](https://www.eleuther.ai/)
    创建。GPT-NeoX-20B 是在初始的 GPT-Neo 模型（27 亿参数）[22] 之后创建的，经过 [the Pile](https://huggingface.co/datasets/EleutherAI/pile)
    进行预训练，并在各种自然语言基准测试中表现出色的少量示例学习性能（可与 GPT-3 相媲美）。尽管与 GPT-3（即 200 亿参数对比 1750 亿参数）相比，这个模型在某种程度上较小，但它在当时是最大的开源语言模型。此外，所有训练和评估模型的代码都与其权重一起以
    [Apache 2.0 许可证](https://www.planetcrust.com/what-does-apache-2-0-license-mean)
    发布，允许商业使用。
- en: '![](../Images/464b0594abbfedc8df9295cfbbc47328.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/464b0594abbfedc8df9295cfbbc47328.png)'
- en: (from [8])
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [8]）
- en: '**The model.** GPT-NeoX-20B [6] uses a standard decoder-only transformer architecture,
    but makes the following two changes:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型。** GPT-NeoX-20B [6] 使用了标准的仅解码器变换器架构，但进行了以下两项更改：'
- en: RoPE Embeddings
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoPE 嵌入
- en: Parallel Attention and Feed Forward Layers
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行注意力和前馈层
- en: Improving upon standard position embeddings, RoPE embeddings (shown above) provide
    a new methodology for injecting positional information into the self-attention
    operation. This approach finds a better balance between absolute and relative
    position information and is used in a variety of other models (e.g., PaLM [9]
    and Falcon-40B [10]) due to its ability to improve performance on tasks with long
    sequence lengths. Additionally, the use of parallel attention and feed forward
    layers (see below) leads to a 15% improvement in training throughput with minimal
    performance degradation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 改进了标准位置嵌入，RoPE嵌入（如上所示）提供了一种将位置信息注入自注意力操作的新方法。这种方法在绝对位置和相对位置信息之间找到了更好的平衡，并由于其在处理长序列任务中的表现能力，被用于多种其他模型（例如PaLM
    [9]和Falcon-40B [10]）。此外，使用并行注意力和前馈层（见下文）使训练吞吐量提高了15%，性能下降最小。
- en: '![](../Images/e8925011b742dda3dc495518dd9b76ea.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8925011b742dda3dc495518dd9b76ea.png)'
- en: Performing attention and feed forward layers in parallel (created by author)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 并行执行注意力层和前馈层（由作者创建）
- en: Interestingly, a custom tokenizer is created for GPT-NeoX-20B. This tokenizer
    is comparable to that of GPT-2 [11], but it is trained from scratch on the Pile
    — a large and diverse corpus of text — and is modified to more consistently tokenize
    whitespace characters. As such, the resulting tokenizer, in addition to being
    trained on a high-quality corpus, is especially effective at tokenizing code (i.e.,
    there are a lot of whitespace characters in code!). As a result, several open-source
    models (e.g., MPT-7B [5]) adopt this tokenizer even today.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，为GPT-NeoX-20B创建了一个自定义分词器。这个分词器与GPT-2的分词器[11]相当，但它是从头开始在Pile（一个大型多样的文本语料库）上训练的，并且经过修改以更一致地分词空白字符。因此，最终的分词器除了在高质量语料库上进行训练外，还特别有效于分词代码（即，代码中有很多空白字符！）。因此，几种开源模型（例如MPT-7B
    [5]）即使在今天也采用了这个分词器。
- en: '![](../Images/71f8929ce4bdba04222ea1cbbed33238.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71f8929ce4bdba04222ea1cbbed33238.png)'
- en: (from [6])
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [6]）
- en: '**The performance.** GPT-NeoX-20B was compared to both GPT-3 and other open-source
    models, such as [GPT-J](https://www.eleuther.ai/artifacts/gpt-j). In these evaluations,
    we see that GPT-NeoX-20B performs quite well (even when compared to proprietary
    models) on common language modeling tasks; see above. Notably, GPT-3 tends to
    achieve the best performance. However, GPT-NeoX-20B performs quite well relative
    to its size and even outperforms proprietary models with a similar number of parameters.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能。** GPT-NeoX-20B与GPT-3和其他开源模型（如 [GPT-J](https://www.eleuther.ai/artifacts/gpt-j)）进行了比较。在这些评估中，我们看到GPT-NeoX-20B在常见的语言建模任务上表现相当好（即使与专有模型相比）；见上文。值得注意的是，GPT-3往往实现了最佳性能。然而，GPT-NeoX-20B相对于其规模表现相当好，甚至在参数数量相近的专有模型中也表现优异。'
- en: '![](../Images/b10f5839e8a9ffc18e19a389388b406f.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b10f5839e8a9ffc18e19a389388b406f.png)'
- en: (from [6])
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [6]）
- en: The performance of GPT-NeoX-20B is not quite state-of-the-art, but the model
    performs surprisingly well for its size, even when compared to recent models!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-NeoX-20B的性能虽然不是最先进的，但在其规模下表现出乎意料地好，即使与近期模型相比也是如此！
- en: '[Open Pre-Trained Transformers (OPT) Language Models](https://arxiv.org/abs/2205.01068)
    [4]'
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[开放预训练变换器（OPT）语言模型](https://arxiv.org/abs/2205.01068) [4]'
- en: '![](../Images/40a10d026d0ee315ab08580a8211ab62.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40a10d026d0ee315ab08580a8211ab62.png)'
- en: Components of the OPT release (created by author)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: OPT版本的组件（由作者创建）
- en: In a previous overview, we have discussed the details of the Open Pre-trained
    Transformers (OPT) library in depth. See [here](/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)
    for a link.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的概述中，我们深入讨论了开放预训练变换器（OPT）库的细节。请见 [这里](/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)
    的链接。
- en: OPT, which was proposed by [Meta AI](https://ai.meta.com/), was created as an
    initiative to democratize access of powerful LLMs to the public and is comprised
    of several different LLMs with sizes ranging from 125 million to 175 billion parameters.
    These models are pre-trained over a curated dataset compiled from sources like
    [Reddit](https://arxiv.org/abs/2001.08435), [the Pile](https://arxiv.org/abs/2101.00027),
    and [BooksCorpus](https://yknzhu.wixsite.com/mbweb), and the largest model in
    this suite — OPT-175B — was one of the first truly *large* language models to
    be open-sourced. Going further, the models are accompanied by a [code repository](https://github.com/facebookresearch/metaseq)
    and even a logbook that details the pre-training process of all models. Although
    OPT models are [not commercially-usable](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md),
    they are an incredibly resource that heavily influenced the open availability
    of LLMs for research.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: OPT由[Meta AI](https://ai.meta.com/)提出，旨在将强大的LLMs民主化地提供给公众，包括几个不同的LLMs，规模从1.25亿到1750亿参数不等。这些模型在由[Reddit](https://arxiv.org/abs/2001.08435)、[the
    Pile](https://arxiv.org/abs/2101.00027)和[BooksCorpus](https://yknzhu.wixsite.com/mbweb)等来源编制的精选数据集上进行预训练，而这个套件中的最大模型——OPT-175B——是首批真正的*大型*开源语言模型之一。此外，这些模型还配有一个[代码库](https://github.com/facebookresearch/metaseq)和一个详细记录所有模型预训练过程的日志。尽管OPT模型[不用于商业](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md)，但它们是对LLMs开放研究的巨大资源。
- en: '**The impact.** The OPT language models were the first large-scale effort to
    make massive language models accessible to the research community — *LLMs were
    now fully-available to anyone*, rather than being hidden behind an API. Additionally,
    OPT’s open-source training code makes a highly efficient training framework, using
    common techniques like [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/)
    and [tensor parallelism](https://github.com/NVIDIA/Megatron-LM?fbclid=IwAR3SvXpTaLseZacJv_Bntwg0czNNYj8hEhcho3R_mo8ABDS8zmszw4mdZ3E),
    readily available. This code achieves resource utilization that is 17% better
    than research published directly by NVIDIA [3], making it a great resource for
    training LLMs.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**影响。** OPT语言模型是首次大规模努力使庞大的语言模型对研究社区可访问的——*LLMs现在完全对任何人开放*，而不是隐藏在API后面。此外，OPT的开源训练代码提供了一个高效的训练框架，使用了像[FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/)和[张量并行](https://github.com/NVIDIA/Megatron-LM?fbclid=IwAR3SvXpTaLseZacJv_Bntwg0czNNYj8hEhcho3R_mo8ABDS8zmszw4mdZ3E)等常见技术。这段代码实现了比NVIDIA直接发布的研究[3]高出17%的资源利用率，是训练LLMs的极佳资源。'
- en: '![](../Images/0552d7057a25f7f263fc74ffc6965c5f.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0552d7057a25f7f263fc74ffc6965c5f.png)'
- en: (from [5])
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于[5]）
- en: The training [notes](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles?fbclid=IwAR3qONxU4mENL_HAVcf9LJCwwqijGCVMk87C8Sm9_q3y6TZS3kZiY6Fd5dY)
    and [logbook](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf?fbclid=IwAR1gSseT67AGnNprJRdiW91Pf7eW1b82Z3pYshE4CYGT_-AKVnCUdaIdmm8)
    associated with OPT provide a massive amount of (previously unknown) insight into
    the LLM training process. From these resources, we can better understand the full
    cost of training an LLM and the many struggles that may occur in this process
    (e.g., loss spikes, hardware failures, and other “mid flight” training adjustments
    that are required). Such difficulties with training LLMs became a topic of conversation
    and have since been (mostly) resolved by subsequent work on open-source LLMs;
    see above.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 训练[笔记](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles?fbclid=IwAR3qONxU4mENL_HAVcf9LJCwwqijGCVMk87C8Sm9_q3y6TZS3kZiY6Fd5dY)和[日志](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf?fbclid=IwAR1gSseT67AGnNprJRdiW91Pf7eW1b82Z3pYshE4CYGT_-AKVnCUdaIdmm8)与OPT相关，提供了大量（先前未知的）对LLM训练过程的见解。通过这些资源，我们可以更好地理解训练LLM的全部成本以及在此过程中可能遇到的许多困难（例如，损失峰值、硬件故障以及其他“中途”训练调整）。这些训练LLM的困难成为了讨论的话题，并且已经通过随后的开源LLM工作（大部分）得到解决；详见上述内容。
- en: '![](../Images/eeedf7b8ce8fd018b3f9256b75bc0a90.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eeedf7b8ce8fd018b3f9256b75bc0a90.png)'
- en: (from [4])
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于[4]）
- en: '**Does it perform well?** OPT-175B was extensively compared to popular models
    at the time of its proposal and found to achieve comparable performance to GPT-3
    in zero and few-shot learning settings; see above. Overall, OPT’s performance
    is not notable — *the model is widely considered to lag behind proprietary models
    in terms of quality*. Despite its lackluster performance, however, OPT was a massive
    step forward for AI research and significantly boosted the level of interest in
    open-source LLMs. This impact should not be understated, as it came at a time
    when the dominance of proprietary models had been accepted as a new standard.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**它的表现如何？** OPT-175B 在提议时与当时流行的模型进行了广泛比较，并发现其在零样本和少样本学习设置下的性能与 GPT-3 相当；详见上文。总体而言，OPT
    的性能并不突出——*该模型被普遍认为在质量上落后于专有模型*。尽管其表现平平，然而，OPT 对 AI 研究而言是一个巨大的进步，并显著提升了对开源 LLM
    的兴趣。这一影响不容低估，因为它出现在专有模型主导地位被接受为新标准的时期。'
- en: '[BLOOM: An Open, Multilingual Language Model](https://bigscience.huggingface.co/blog/bloom)
    [12]'
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[BLOOM: 一个开放的多语言语言模型](https://bigscience.huggingface.co/blog/bloom) [12]'
- en: “Academia, nonprofits and smaller companies’ research labs find it difficult
    to create, study, or even use LLMs as only a few industrial labs with the necessary
    resources and exclusive rights can fully access them.” *— from [12]*
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “学术界、非营利组织和小型公司的研究实验室发现创建、研究甚至使用 LLM 非常困难，因为只有少数具备必要资源和专有权利的工业实验室才能完全访问它们。”
    *—来自 [12]*
- en: Proposed in [12], BLOOM is an 176 billion parameter LLM that was trained as
    part of a massive, open collaboration of AI researchers (i.e., over 1000 researchers
    participated!), called the [Big Science Research Workshop](https://bigscience.huggingface.co/).
    Running over the timespan of one year (May 2021 to May 2022), the goal of this
    workshop was to create *i)* a massive multilingual text dataset and *ii)* a large
    multilingual language model that is trained on this dataset. The resulting model,
    which is slightly larger than GPT-3 and is open-sourced under the [Responsible
    AI License](https://bigscience.huggingface.co/blog/the-bigscience-rail-license)
    (RAIL), can generate text in 46 different languages[8](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-8-135273362)
    and 13 programming languages.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在[12]中提出，BLOOM 是一个 1760 亿参数的 LLM，它作为一个大规模开放协作的 AI 研究项目（即，超过 1000 名研究人员参与！），称为[大科学研究工作坊](https://bigscience.huggingface.co/)。该工作坊持续了一年（2021
    年 5 月至 2022 年 5 月），其目标是创建 *i)* 一个大规模的多语言文本数据集，以及 *ii)* 一个在该数据集上训练的大型多语言语言模型。最终生成的模型，略大于
    GPT-3，并在[负责任 AI 许可证](https://bigscience.huggingface.co/blog/the-bigscience-rail-license)（RAIL）下开源，可以生成
    46 种不同语言[8](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-8-135273362)和
    13 种编程语言的文本。
- en: '**The dataset** developed for training BLOOM, called the [ROOTS corpus](https://arxiv.org/abs/2303.03915),
    is comprised of 498 HuggingFace datasets and contains over 1.6 terabytes of text
    that spans 46 natural languages and 13 programming languages. The distribution
    of this dataset across the different languages is shown in the figure below.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**为训练 BLOOM 开发的数据集**，称为[ROOTS 语料库](https://arxiv.org/abs/2303.03915)，由 498
    个 HuggingFace 数据集组成，包含超过 1.6 TB 的文本，涵盖 46 种自然语言和 13 种编程语言。该数据集在不同语言中的分布如下图所示。'
- en: '![](../Images/086efab8d0133082762b58c2ac406cf9.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/086efab8d0133082762b58c2ac406cf9.png)'
- en: (from [12])
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [12])
- en: After obtaining the raw data, the authors apply a pipeline of different quality
    filters to remove text that is not natural language. The exact filtering components
    that are used, which are further outlined in Section 3.1.3 of [12], change depending
    on the source of the data. However, the overall pipeline shares a common goal
    of filtering out as much low-quality text as possible.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得原始数据后，作者应用了不同的质量过滤器管道，以去除非自然语言的文本。使用的具体过滤组件在[12]的第 3.1.3 节中进一步阐述，根据数据源的不同而有所变化。然而，整体管道的共同目标是尽可能多地过滤掉低质量文本。
- en: '![](../Images/7d2bd4c46a9310ef6a98766cfb9c9a3b.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d2bd4c46a9310ef6a98766cfb9c9a3b.png)'
- en: (from [12])
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [12])
- en: '**The architecture** used by BLOOM is a standard decoder-only transformer.
    As shown above, however, a few modifications are made to this architecture, such
    as:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**BLOOM 使用的架构** 是标准的解码器单一 Transformer。然而，如上所示，对该架构进行了少许修改，例如：'
- en: '*ALiBi* [13]: This aids the model in generalizing to longer context lengths
    than those seen during training.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ALiBi* [13]：这帮助模型将训练中见过的上下文长度泛化到更长的上下文长度。'
- en: '*Embedding Layer Norm*: An extra [layer norm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
    is placed after the model’s embedding layer, which is empirically found to improve
    training stability.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*嵌入层归一化*：在模型的嵌入层后添加了额外的[层归一化](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)，这在经验上发现能够提高训练稳定性。'
- en: Overall, this model is not much different than most LLMs. Interestingly, authors
    in [12] perform an extensive analysis between different types of transformer architectures
    (e.g., encoder-only models, encoder-decoder models, and decoder-only models),
    finding that the decoder-only model (used by nearly all causal language models)
    achieves the best performance after pre-training.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这个模型与大多数LLM并没有太大区别。有趣的是，[12]中的作者对不同类型的变换器架构（例如，仅编码器模型、编码器-解码器模型和仅解码器模型）进行了广泛分析，发现仅解码器模型（几乎所有因果语言模型都使用这种模型）在预训练后表现最佳。
- en: “Our results show that immediately after pre-training, causal decoder-only models
    performed best — validating the choice of state-of-the-art LLMs.” *— from [12]*
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们的结果显示，预训练后，因果解码器模型表现最佳——验证了选择最先进LLM的决策。” *— 来源于 [12]*
- en: '**Does it perform well?** Compared to other open-source LLMs, BLOOM performs
    relatively well. It achieves comparable, or improved, results relative to OPT
    in natural language benchmarks and tends to excel at machine translation tasks
    given that it was trained on a multilingual corpus; see below.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**它表现如何？** 与其他开源LLM相比，BLOOM的表现相对较好。在自然语言基准测试中，它的结果与OPT相当或有所改进，并且由于其在多语言语料库上进行训练，通常在机器翻译任务中表现优异；见下文。'
- en: '![](../Images/a931372cfd9177c919402852aa8c24b3.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a931372cfd9177c919402852aa8c24b3.png)'
- en: (from [12])
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [12])
- en: However, BLOOM’s performance falls below that of top proprietary models. For
    example, we see in results on the HumanEval benchmark (shown below) that the model’s
    coding abilities fall far short of alternatives like Codex [14]. Additionally,
    when we compare the performance of BLOOM to models like Chinchilla [15] and PaLM
    [9], we quickly see that the performance of open-source models falls short of
    their proprietary counterparts. In other words, *research in open-source LLMs
    was still lagging at the time when BLOOM was proposed*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，BLOOM的表现低于顶级专有模型。例如，我们在下方的HumanEval基准测试结果中看到，该模型的编码能力远远落后于Codex [14]等替代品。此外，当我们将BLOOM与Chinchilla
    [15]和PaLM [9]等模型进行比较时，我们很快发现开源模型的性能逊色于其专有对应物。换句话说，*在BLOOM提出时，开源LLM的研究仍然滞后*。
- en: '![](../Images/52f1b3c499c80044940a223632231145.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52f1b3c499c80044940a223632231145.png)'
- en: (from [12])
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [12])
- en: Other Notable Models
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他显著模型
- en: We tried to cover several notable models that were proposed during the early
    days of open-source LLM research. But, there are still a few models not covered
    in this overview that are worth mentioning. Let’s take a quick look at a few of
    them.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试覆盖早期开源LLM研究中提出的几个显著模型。但仍有一些模型未包含在本概述中，值得一提。让我们快速了解其中的一些。
- en: '**GPT-J [21]** is a 6 billion parameter, English-only causal language model
    that was proposed prior to GPT-NeoX-20B [6]. Similar to GPT-NeoX-20B, this model
    was pre-trained on the Pile. At the time of its release, GPT-J-6B was the largest
    publicly-available GPT-3-style language model.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-J [21]** 是一个60亿参数的仅英语因果语言模型，在GPT-NeoX-20B [6]之前提出。与GPT-NeoX-20B类似，该模型在Pile上进行预训练。在发布时，GPT-J-6B是最大的公开GPT-3风格语言模型。'
- en: '![](../Images/faed1a07b5d94c956ac692853a70d524.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/faed1a07b5d94c956ac692853a70d524.png)'
- en: (from [20])
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [20])
- en: '**GLM [20]** is more of a pre-training objective rather than a language model.
    This work explores the idea of unifying different pre-training techniques (e.g.,
    from BERT, T5, and GPT) by proposing a autoregressive blank infilling objective.
    In other words, we predict masked words in a sentence in an autoregressive manner,
    similar to a language model; see above. The resulting model, which is quite small
    (<1 billion parameters), is found to outperform BERT, T5 and GPT on several popular
    natural language processing benchmarks.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**GLM [20]** 更像是一个预训练目标，而不是一个语言模型。该工作探索了通过提出自回归空白填充目标来统一不同的预训练技术（例如BERT、T5和GPT）的想法。换句话说，我们以自回归方式预测句子中的掩码词，类似于语言模型；见上文。结果模型相当小（<10亿参数），在几个流行的自然语言处理基准测试中表现优于BERT、T5和GPT。'
- en: Where do we go from here?
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们接下来要做什么？
- en: '![](../Images/d1a8829aad7d90260ea20581ae9b7e16.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1a8829aad7d90260ea20581ae9b7e16.png)'
- en: The evolution of open-source LLM research (created by author)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 开源LLM研究的发展（由作者创建）
- en: 'Given that initial attempts at open-source LLMs yielded models that did not
    perform nearly as well as proprietary counterparts, we might reasonably wonder:
    *What should we do to make these models better?* As this research area has evolved,
    we have seen effort invested into two primary areas:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于早期对开源LLMs的尝试产生的模型表现远不如专有对手，我们不禁要问：*我们应该做什么来提升这些模型？* 随着这一研究领域的发展，我们看到主要投入了两个方向的努力：
- en: Creating better base LLMs
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建更好的基础LLMs
- en: Fine-tuning open-source LLMs (i.e., alignment and imitation)
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调开源LLMs（即对齐和模仿）
- en: Given that open-source LLMs are accessible to everyone, research in these areas
    progressed at a shocking pace — *we went from OPT to near state-of-the-art models
    (e.g., LLaMA-2 or Falcon-40B [10]) in less than a year*!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由于开源LLMs对所有人开放，这些领域的研究进展速度令人震惊——*我们在不到一年的时间里从OPT进展到了接近最先进的模型（如LLaMA-2或Falcon-40B
    [10]）*！
- en: “We argue that the highest leverage action for improving open-source models
    is to tackle the difficult challenge of developing better base LMs” *— from [16]*
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们认为，提升开源模型的最高杠杆行动是应对开发更好的基础语言模型这一艰巨挑战” *— 出自[16]*
- en: Both of the research directions outlined above were explored in parallel during
    this time, and each resulted in the development of useful techniques for AI practitioners.
    Within the next two parts of this survey, we will overview each of these areas
    and the key contributions of each, exploring how initial attempts at open-source
    LLMs evolved into incredibly-capable models such as LLaMA-2.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两条研究方向在此期间是并行探索的，每条方向都产生了对AI从业者有用的技术。在本调查的接下来的两个部分中，我们将概述这些领域及其关键贡献，探讨最初对开源LLMs的尝试如何演变成极其强大的模型，如LLaMA-2。
- en: Connect with me!
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接我！
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. If you liked this overview, subscribe
    to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I help readers understand AI research via overviews of relevant topics from
    the ground up. You can also follow me on [X](https://twitter.com/cwolferesearch)
    and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/), or
    check out my [other writings](https://medium.com/@wolfecameron) on medium!
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢阅读这篇文章。我是[Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)的AI总监。我研究深度学习的实证和理论基础。如果你喜欢这个概述，请订阅我的[Deep
    (Learning) Focus通讯](https://cameronrwolfe.substack.com/)，我通过从基础开始的相关主题概述帮助读者理解AI研究。你也可以在[X](https://twitter.com/cwolferesearch)和[LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)上关注我，或查看我在medium上的[其他文章](https://medium.com/@wolfecameron)！
- en: Bibliography
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Brown, Tom, et al. “Language models are few-shot learners.” *Advances in
    neural information processing systems* 33 (2020): 1877–1901.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Brown, Tom, 等. “语言模型是少样本学习者。” *神经信息处理系统进展* 33 (2020)：1877–1901。'
- en: '[2] Rae, Jack W., et al. “Scaling language models: Methods, analysis & insights
    from training gopher.” *arXiv preprint arXiv:2112.11446* (2021).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Rae, Jack W., 等. “语言模型的扩展：方法、分析与训练gopher的见解。” *arXiv预印本 arXiv:2112.11446*
    (2021)。'
- en: '[3] Smith, Shaden, et al. “Using deepspeed and megatron to train megatron-turing
    nlg 530b, a large-scale generative language model.” *arXiv preprint arXiv:2201.11990*
    (2022).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Smith, Shaden, 等. “使用deepspeed和megatron训练megatron-turing nlg 530b，一个大规模生成语言模型。”
    *arXiv预印本 arXiv:2201.11990* (2022)。'
- en: '[4] Zhang, Susan, et al. “OPT: Open Pre-trained Transformer Language Models.”
    *arXiv preprint arXiv:2205.01068* (2022).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Zhang, Susan, 等. “OPT: 开源预训练变换器语言模型。” *arXiv预印本 arXiv:2205.01068* (2022)。'
- en: '[5] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable
    Llms.” *MosaicML*, 5 May 2023, [www.mosaicml.com/blog/mpt-7b.](http://www.mosaicml.com/blog/mpt-7b.)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] “介绍MPT-7B：开源商业可用LLMs的新标准。” *MosaicML*，2023年5月5日， [www.mosaicml.com/blog/mpt-7b.](http://www.mosaicml.com/blog/mpt-7b.)'
- en: '[6] Black, Sid, et al. “Gpt-neox-20b: An open-source autoregressive language
    model.” *arXiv preprint arXiv:2204.06745* (2022).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Black, Sid, 等. “Gpt-neox-20b：一个开源自回归语言模型。” *arXiv预印本 arXiv:2204.06745*
    (2022)。'
- en: '[7] Gao, Leo, et al. “The pile: An 800gb dataset of diverse text for language
    modeling.” *arXiv preprint arXiv:2101.00027* (2020).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Gao, Leo, 等. “The pile: 一个800GB的多样文本数据集用于语言建模。” *arXiv预印本 arXiv:2101.00027*
    (2020)。'
- en: '[8] Su, Jianlin, et al. “Roformer: Enhanced transformer with rotary position
    embedding.” *arXiv preprint arXiv:2104.09864* (2021).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Su, Jianlin, 等. “Roformer: 带旋转位置嵌入的增强变换器。” *arXiv预印本 arXiv:2104.09864*
    (2021)。'
- en: '[9] Chowdhery, Aakanksha, et al. “Palm: Scaling language modeling with pathways.”
    *arXiv preprint arXiv:2204.02311* (2022).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Chowdhery, Aakanksha 等. “Palm：通过路径扩展语言建模。” *arXiv 预印本 arXiv:2204.02311*
    (2022)。'
- en: '[10] “Introducing Falcon LLM”, *Technology Innovation Institute*, 7 June 2023,
    [https://falconllm.tii.ae/.](https://falconllm.tii.ae/.)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] “介绍 Falcon LLM”，*技术创新研究所*，2023年6月7日，[https://falconllm.tii.ae/.](https://falconllm.tii.ae/.)'
- en: '[11] Radford, Alec, et al. “Language Models are Unsupervised Multitask Learners.”'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Radford, Alec 等. “语言模型是无监督的多任务学习者。”'
- en: '[12] Scao, Teven Le, et al. “Bloom: A 176b-parameter open-access multilingual
    language model.” *arXiv preprint arXiv:2211.05100* (2022).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Scao, Teven Le 等. “Bloom：一个 176b-参数的开放访问多语言模型。” *arXiv 预印本 arXiv:2211.05100*
    (2022)。'
- en: '[13] Press, Ofir, Noah A. Smith, and Mike Lewis. “Train short, test long: Attention
    with linear biases enables input length extrapolation.” *arXiv preprint arXiv:2108.12409*
    (2021).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Press, Ofir, Noah A. Smith 和 Mike Lewis. “短期训练，长期测试：具有线性偏差的注意力机制实现输入长度外推。”
    *arXiv 预印本 arXiv:2108.12409* (2021)。'
- en: '[14] Chen, Mark, et al. “Evaluating large language models trained on code.”
    *arXiv preprint arXiv:2107.03374* (2021).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Chen, Mark 等. “评估训练有代码的大型语言模型。” *arXiv 预印本 arXiv:2107.03374* (2021)。'
- en: '[15] Hoffmann, Jordan, et al. “Training compute-optimal large language models.”
    *arXiv preprint arXiv:2203.15556* (2022).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Hoffmann, Jordan 等. “训练计算最优的大型语言模型。” *arXiv 预印本 arXiv:2203.15556* (2022)。'
- en: '[16] Gudibande, Arnav, et al. “The false promise of imitating proprietary llms.”
    *arXiv preprint arXiv:2305.15717* (2023).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Gudibande, Arnav 等. “模仿专有 llms 的虚假承诺。” *arXiv 预印本 arXiv:2305.15717* (2023)。'
- en: '[17] Vaswani, Ashish, et al. “Attention is all you need.” *Advances in neural
    information processing systems* 30 (2017).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Vaswani, Ashish 等. “注意力机制是你所需的一切。” *神经信息处理系统进展* 30 (2017)。'
- en: '[18] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Devlin, Jacob 等. “Bert：用于语言理解的深度双向转换器预训练。” *arXiv 预印本 arXiv:1810.04805*
    (2018)。'
- en: '[19] Ouyang, Long, et al. “Training language models to follow instructions
    with human feedback.” *Advances in Neural Information Processing Systems* 35 (2022):
    27730–27744.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Ouyang, Long 等. “训练语言模型以遵循指令和人类反馈。” *神经信息处理系统进展* 35 (2022)：27730–27744。'
- en: '[20] Du, Zhengxiao, et al. “Glm: General language model pretraining with autoregressive
    blank infilling.” *arXiv preprint arXiv:2103.10360* (2021).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Du, Zhengxiao 等. “Glm：自回归空白填充的通用语言模型预训练。” *arXiv 预印本 arXiv:2103.10360*
    (2021)。'
- en: '[21] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 billion parameter autoregressive
    language model, 2021.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Ben Wang 和 Aran Komatsuzaki. GPT-J-6B：一个拥有 60 亿参数的自回归语言模型，2021。'
- en: '[22] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021\.
    GPT-Neo: Large scale autoregressive language modeling with MeshTensorflow.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Sid Black, Leo Gao, Phil Wang, Connor Leahy 和 Stella Biderman. 2021\.
    GPT-Neo：使用 MeshTensorflow 的大规模自回归语言建模。'
