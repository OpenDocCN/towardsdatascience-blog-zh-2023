- en: 'The History of Open-Source LLMs: Early Days (Part One)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8](https://towardsdatascience.com/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding GPT-Neo, GPT-J, GLM, OPT, BLOOM, and more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----d782bcd8f7e8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d782bcd8f7e8--------------------------------)
    ·20 min read·Nov 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05d30e1fd65a03fd2cb858285b0d7f58.png)'
  prefs: []
  type: TYPE_IMG
- en: (Photo by [Chris Lawton](https://unsplash.com/@chrislawton?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/stack-of-six-brown-hardbound-books-9T346Ij4kGk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: Research on language modeling has a long history that dates back to models like
    GTP and GPT-2 or even RNN-based techniques (e.g., [ULMFit](https://arxiv.org/abs/1801.06146))
    that predate modern, transformer-based language models. Despite this long history,
    however, language models have only become popular relatively recently. The first
    rise in popularity came with the proposal of GPT-3 [1], which showed that impressive
    few-shot learning performance could be achieved across many tasks via a combination
    of self-supervised pre-training and in-context learning; see below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3052b7fe6011e83507c05a91e32c5cda.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  prefs: []
  type: TYPE_NORMAL
- en: After this, the recognition garnered by GPT-3 led to the proposal of a swath
    of large language models (LLMs). Shortly after, research on language model alignment
    led to the creation of even more impressive models like InstructGPT [19] and,
    most notably, its sister model ChatGPT. The impressive performance of these models
    led to a flood of interest in language modeling and generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Despite being incredibly powerful, many early developments in LLM research have
    one common property — *they are closed source*. When language models first began
    to gain widespread recognition, many of the most powerful LLMs were only accessible
    via paid APIs (e.g., the [OpenAI API](https://openai.com/blog/openai-api)) and
    the ability to research and develop such models was restricted to select individuals
    or labs. Such an approach is markedly different from typical AI research practices,
    where openness and idea sharing is usually encouraged to promote forward progress.
  prefs: []
  type: TYPE_NORMAL
- en: “This restricted access has limited researchers’ ability to understand how and
    why these large language models work, hindering progress on efforts to improve
    their robustness and mitigate known issues such as bias and toxicity.” *— from
    [4]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**This overview.** Despite the initial emphasis upon proprietary technology,
    the LLM research community slowly began to create open-source variants of popular
    language models like GPT-3\. Although the first open-source language models lagged
    behind the best proprietary models, they laid the foundation for improved transparency
    within LLM research and catalyzed the development of many subsequent models that
    were more powerful (e.g., Falcon [10] and [LLaMA-2](https://ai.meta.com/llama/)).'
  prefs: []
  type: TYPE_NORMAL
- en: This overview is part of a three part series exploring the history of open-source
    language models. Here, we will learn about the beginning of this history, including
    several initial attempts at creating open-source language models. Although these
    models left something to be desired in terms of performance, they are incredibly
    important to understand, as the revolution of open-source LLMs that ensued was
    entirely based upon these models. In the following two parts of the series, we
    will learn more about recent open-source LLMs, as well as how imitation and alignment
    techniques have been used to improve their performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4a5c7276cf2250f14c9aa6ede26289b.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [12, 20])
  prefs: []
  type: TYPE_NORMAL
- en: The Mechanics of a Language Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open-source LLM research catalyzed transparency and idea sharing, creating an
    environment in which researchers could collaborate and innovate more quickly.
    Put simply, *the beauty of open-source LLM research is that it gives us the potential
    to study these incredible models and develop a deeper understanding of how they
    work*. There are no unknown tricks hidden behind a paid API or black box. Open-source
    LLMs allow us to look at the code, run experiments, and even try out our own ideas
    and modifications — we have full access to the underlying model!
  prefs: []
  type: TYPE_NORMAL
- en: “A much broader segment of the AI community needs access to these models in
    order to conduct reproducible research and collectively drive the field forward.”
    *— from [4]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But, to build a deep understanding of such models, we first need to understand
    the basics behind how they work. Within this section, we will overview these ideas,
    attempting to provide a (relatively) comprehensive understanding of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The Language Modeling Objective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/c5c67554d791155729ae174c08d04176.png)'
  prefs: []
  type: TYPE_IMG
- en: Pre-training with a language modeling objective (created by author)
  prefs: []
  type: TYPE_NORMAL
- en: At the core of language modeling is next token prediction (also called the standard
    language modeling objective), which is used to train nearly all language models.
    To train a language model using next token prediction, we need a large corpus
    of raw text. Using this corpus, we train the model by *i)* sampling some text
    from the dataset and *ii)* training the model to predict the next word; see above.
    Because the ground truth next token can always be deduced from the raw text, next
    token prediction is a form of self-supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is a token?** One can roughly consider next token prediction to be predicting
    the next word in a sequence, given a few preceding words as context. However,
    this analogy is not perfect, as tokens and words are not exactly equal. When a
    language model receives text as input, the raw text is first tokenized (i.e.,
    converted into a sequence of discrete words or sub-words); see below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c211f85a2ac29498592c09a5374c7cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting raw text into a sequence of tokens (created by author)
  prefs: []
  type: TYPE_NORMAL
- en: The tokenizer associated with a language model typically has a fixed-size vocabulary,
    or set of viable tokens that can be created from a textual sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '**Predicting next tokens.** Once a sequence of tokens has been created, the
    language model has an embedding layer that stores a unique and learnable vector
    embedding for every token within the tokenizer’s vocabulary. Using this embedding
    layer, we can convert each token within the input sequence into a corresponding
    vector embedding, forming a sequence of token vectors; see below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/625fd4aecf4cab6c12ca2f211743b89d.png)'
  prefs: []
  type: TYPE_IMG
- en: Tokenizing and embedding raw text data (created by author)
  prefs: []
  type: TYPE_NORMAL
- en: After adding positional embeddings to each token, we can pass this sequence
    of token vectors into a decoder-only transformer (more explanation will follow),
    which transforms (no pun intended) each of these token vectors and produces a
    corresponding output vector for each token. Notably, the number of output vectors
    is the same as the number of input vectors; see below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba20b8238f80f8417a7af042de5c78dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Processing tokens with a decoder-only transformer (created by author)
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an output representation for each token, we are ready to perform
    next-token prediction! For each token in the sequence, we simply take its output
    token vector and use this to predict the token that comes next in the sequence!
    An illustration of this process is shown below. In practice, this next token prediction
    objective is simultaneously computed over all tokens in the sequence (and over
    all sequences in a mini-batch!) to maximize efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e4f766f68636c5671f11d86eec16030.png)'
  prefs: []
  type: TYPE_IMG
- en: Computing the next token prediction training objective (created by author)
  prefs: []
  type: TYPE_NORMAL
- en: Due to the use of causal (or masked) self-attention, each output token vector
    only considers the current token and those that is come before it in the sequence
    when computing its representation. If we were to use bidirectional self-attention,
    each output token vector would be computed by looking at the entire sequence of
    vectors, which would allow the model to cheat and solve next token prediction
    by just copying the token that comes next in the sequence. As such, *masked self-attention
    is needed for next-token prediction*. But, what is self-attention and — more fundamentally
    — what is a transformer? Let’s dive into this next.
  prefs: []
  type: TYPE_NORMAL
- en: '**A quick note.** The phrase “language model” may sometimes be used to refer
    to models beyond those that specialize in performing next token prediction. For
    example, BERT [18] is considered by some to be a “language model”, but it is trained
    using a [Cloze](https://en.wikipedia.org/wiki/Cloze_test)-style objective and
    is not a generative model. As such, language models that specialize in next token
    prediction are oftentimes distinguished as “causal” language models. Here, we
    will use both of these terms interchangeably to refer to models that specialize
    in next token prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer Architecture and its Variants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/d4063e56b4061772fa70fc6774e8504c.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [17])
  prefs: []
  type: TYPE_NORMAL
- en: 'All language models use some variant of the transformer architecture. This
    architecture (shown above) was originally proposed in [17] for solving sequence-to-sequence
    tasks. However, it was subsequently extended to solve a variety of different problems,
    from assessing the semantic similarity of text to classifying images. In its original
    form, the transformer architecture has two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Encoder*: each block performs bidirectional self-attention and a pointwise
    feed-forward transformation, which are separated with a residual connection and
    LayerNorm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Decoder*: each block performs causal self-attention, [cross attention](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html#cross-attention)
    (i.e., self-attention across encoder and decoder tokens), and a pointwise feed-forward
    transformation, each separated by a residual connection and LayerNorm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When both components of the architecture are present, the encoder processes
    the input sequence and produces an output sequence. Then, the decoder generates
    its own output sequence, given the encoder’s output sequence as input. In other
    words, the encoder processes the entire input sequence to form a representation
    that the decoder uses as context when generating output. As a whole, the transformer
    takes a sequence as input and produces a new sequence as output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff8ee0466c2a6832b16e3a042577e9d5.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [17])
  prefs: []
  type: TYPE_NORMAL
- en: '**Decoder-only and encoder-only transformers.** Nearly all causal language
    models use a decoder-only transformer as their underlying architecture, which
    is just a normal transformer with the encoder-portion of the architecture removed;
    see above. Additionally, the cross attention portion of each decoder block is
    removed due to the lack of an encoder (i.e., we can’t attend to an encoder that
    doesn’t exist)! Alternatively, one could form an encoder-only architecture by
    just using the encoder portion of the architecture. Encoder-only architectures
    (e.g., BERT [18]) excel at solving a variety of discriminative natural language
    tasks, but they are not used for generating text. To learn more, check out the
    link [here](/language-understanding-with-bert-c17a453ada1a).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Why the decoder?** The choice of using the decoder-only architecture (as
    opposed to encoder-only or the full encoder-decoder transformer) for LLMs is not
    arbitrary. Rather, this choice is driven by the use of next-token prediction for
    training language models. The use of masked self-attention within the decoder
    ensures that the model cannot look forward in the sequence when predicting the
    next token. Otherwise, next-token prediction would be trivial, as the model could
    simply copy the next token; see below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d35c55158d6953194bda3c49b455440.png)'
  prefs: []
  type: TYPE_IMG
- en: Causal self-attention is used for next-token prediction (created by author)
  prefs: []
  type: TYPE_NORMAL
- en: To perform next token prediction without cheating, both encoder-only and encoder-decoder
    transformers would have to avoid including any ground truth next token in their
    input sequence. To do this, we could *i)* ingest a prefix and *ii)* predict the
    token that follows this prefix. However, this approach is a bit inefficient because
    we can only predict a single next token at a time. In contrast, decoder-only models,
    due to their use of masked self-attention, can ingest an entire sequence of tokens
    and apply a language modeling objective to every token within the sequence. Plus,
    several papers [12] have shown practically that decoder-only architectures yield
    the best performance for next token prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '**How do we generate text?** Given the decoder-only architecture outlined above,
    generating text follows a simple autoregressive process. We just continually predict
    the next token, add this token to our input, and repeat; see below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0eb74f1f7b9395dd27740f1c6cf3e9b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Generating text with a language model (created by author)
  prefs: []
  type: TYPE_NORMAL
- en: Training and Using Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete our understanding of language models, we need to quickly explore
    how these models are typically trained and used in practice. Although a lot of
    research has been done in this area, most language models are trained according
    to a few standard techniques, as proposed in [19]; see below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39249c2721cd023c5fa54ec2e5dfdcda.png)'
  prefs: []
  type: TYPE_IMG
- en: LLM training components (from [19])
  prefs: []
  type: TYPE_NORMAL
- en: Language models can learn in a variety of different ways. Here, we will focus
    on pre-training, alignment, and in-context learning, which collectively encompass
    most of what’s required to train an LLM and use it in a practical application.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-training.** The pre-training process is the initial and most computationally
    expensive step of creating an LLM. Beginning with a randomly-initialized LLM,
    we must train this model — using a language modeling objective — over a massive
    corpus of raw text that is curated from a variety of different sources. Prior
    research [1] has shown us that by pre-training a very large model (i.e., lots
    of parameters) over a large dataset, we can obtain a [foundation model](https://crfm.stanford.edu/)
    that can accurately solve a variety of different tasks by performing next token
    prediction. To get the best results, we need scale in terms of both data and model
    size.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What else do we need?** Language models that solely undergo pre-training
    can be powerful. Look at GPT-3 [1] and Chinchilla [15] for a few examples. However,
    there is a reason that LLMs did not explode in popularity until the proposal of
    models like ChatGPT — *just performing next token prediction is not very interesting*.
    Oftentimes, predicting the statistically-correct next token, although it leads
    to reasonable text being generated, produces output that is repetitive, simple,
    and generally not helpful. We needed some way to make LLMs craft outputs that
    are more helpful and interesting to us as humans!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4bb95493aad5c7c6ec0d92b5ab366d0.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [19])
  prefs: []
  type: TYPE_NORMAL
- en: 'Alignment refers to the process of fine-tuning an LLM to better align with
    the desires of human users. This is accomplished primarily via two techniques:
    supervised fine-tuning (SFT) and/or reinforcement learning from human feedback
    (RLHF). The desired behavior of an LLM depends a lot on the context or application
    in which it is deployed. However, alignment is a generic tool that can be used
    to arbitrarily fine-tune an LLM to behave in a certain way; see above. Recent
    research indicates that models do not learn new information during alignment.
    Rather, this process simply teaches the model how to properly format or present
    the knowledge that it has already gained from the pre-training process.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Using LLMs in practice.** After we have pre-trained and fine-tuned (or aligned)
    our language model, the final step is to specialize the model to our desired application.
    This process may require extra fine-tuning over domain-specific data. More training
    is not always necessary, however, as we can accomplish a lot by just using in-context
    learning; see below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/072087b61567a23057e21513b694a91d.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, in-context learning refers to the idea of solving a variety of different
    problems using a single, general-purpose foundation model (e.g., a pre-trained
    LLM). Given the generic text-to-text structure of a language model, this can actually
    be done quite easily. We just need to construct a textual problem-solving prompt
    that can be provided as input to the LLM; see below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ddf4727d70335b75398cbbc03591665.png)'
  prefs: []
  type: TYPE_IMG
- en: Different prompt variants for solving an arithmetic problem (created by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the LLM should generate the answer to our problem as output. As such,
    we can solve many different problems by just modifying the input prompt! The process
    of constructing good prompts for solving problems is referred to as prompt engineering,
    and we have explored this idea extensively in previous posts:'
  prefs: []
  type: TYPE_NORMAL
- en: Practical Prompt Engineering [[link](/practical-prompt-engineering-74e96130abc4)]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced Prompt Engineering [[link](/advanced-prompt-engineering-f07f9e55fe01)]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initial Attempts at Open-Source LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the [expense of pre-training](https://www.mosaicml.com/blog/gpt-3-quality-for-500k),
    it took some time for the research community to pursue the creation of an open-source
    LLM, causing proprietary models like GPT-3 to become the standard. However, once
    the first few models were proposed, the floodgates opened and research on open-source
    LLM progressed rapidly (almost *too* rapidly). We will learn about a few of the
    early models here, while more recent open-source LLMs will be covered in future
    parts of the series.
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT-NeoX-20B](https://arxiv.org/abs/2204.06745) [6]'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first open-source LLMs — a 20 billion parameter model called GPT-NeoX-20B
    [6] — was created by [EleutherAI](https://www.eleuther.ai/). GPT-NeoX-20B was
    created after the initial GPT-Neo model (2.7 billion parameters) [22], was pre-trained
    over [the Pile](https://huggingface.co/datasets/EleutherAI/pile), and achieves
    impressive few-show learning performance (comparable to GPT-3) on a variety of
    natural language benchmarks. Although this model is somewhat small compared to
    GPT-3 (i.e., 20 billion parameters vs. 175 billion parameters), it was the largest
    open-source language model to be released at the time. Plus, all of code for training
    and evaluating the model was released alongside its weights under an [Apache 2.0
    license](https://www.planetcrust.com/what-does-apache-2-0-license-mean), which
    permits commercial use.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/464b0594abbfedc8df9295cfbbc47328.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [8])
  prefs: []
  type: TYPE_NORMAL
- en: '**The model.** GPT-NeoX-20B [6] uses a standard decoder-only transformer architecture,
    but makes the following two changes:'
  prefs: []
  type: TYPE_NORMAL
- en: RoPE Embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel Attention and Feed Forward Layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving upon standard position embeddings, RoPE embeddings (shown above) provide
    a new methodology for injecting positional information into the self-attention
    operation. This approach finds a better balance between absolute and relative
    position information and is used in a variety of other models (e.g., PaLM [9]
    and Falcon-40B [10]) due to its ability to improve performance on tasks with long
    sequence lengths. Additionally, the use of parallel attention and feed forward
    layers (see below) leads to a 15% improvement in training throughput with minimal
    performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e8925011b742dda3dc495518dd9b76ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Performing attention and feed forward layers in parallel (created by author)
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, a custom tokenizer is created for GPT-NeoX-20B. This tokenizer
    is comparable to that of GPT-2 [11], but it is trained from scratch on the Pile
    — a large and diverse corpus of text — and is modified to more consistently tokenize
    whitespace characters. As such, the resulting tokenizer, in addition to being
    trained on a high-quality corpus, is especially effective at tokenizing code (i.e.,
    there are a lot of whitespace characters in code!). As a result, several open-source
    models (e.g., MPT-7B [5]) adopt this tokenizer even today.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/71f8929ce4bdba04222ea1cbbed33238.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [6])
  prefs: []
  type: TYPE_NORMAL
- en: '**The performance.** GPT-NeoX-20B was compared to both GPT-3 and other open-source
    models, such as [GPT-J](https://www.eleuther.ai/artifacts/gpt-j). In these evaluations,
    we see that GPT-NeoX-20B performs quite well (even when compared to proprietary
    models) on common language modeling tasks; see above. Notably, GPT-3 tends to
    achieve the best performance. However, GPT-NeoX-20B performs quite well relative
    to its size and even outperforms proprietary models with a similar number of parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b10f5839e8a9ffc18e19a389388b406f.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [6])
  prefs: []
  type: TYPE_NORMAL
- en: The performance of GPT-NeoX-20B is not quite state-of-the-art, but the model
    performs surprisingly well for its size, even when compared to recent models!
  prefs: []
  type: TYPE_NORMAL
- en: '[Open Pre-Trained Transformers (OPT) Language Models](https://arxiv.org/abs/2205.01068)
    [4]'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/40a10d026d0ee315ab08580a8211ab62.png)'
  prefs: []
  type: TYPE_IMG
- en: Components of the OPT release (created by author)
  prefs: []
  type: TYPE_NORMAL
- en: In a previous overview, we have discussed the details of the Open Pre-trained
    Transformers (OPT) library in depth. See [here](/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)
    for a link.
  prefs: []
  type: TYPE_NORMAL
- en: OPT, which was proposed by [Meta AI](https://ai.meta.com/), was created as an
    initiative to democratize access of powerful LLMs to the public and is comprised
    of several different LLMs with sizes ranging from 125 million to 175 billion parameters.
    These models are pre-trained over a curated dataset compiled from sources like
    [Reddit](https://arxiv.org/abs/2001.08435), [the Pile](https://arxiv.org/abs/2101.00027),
    and [BooksCorpus](https://yknzhu.wixsite.com/mbweb), and the largest model in
    this suite — OPT-175B — was one of the first truly *large* language models to
    be open-sourced. Going further, the models are accompanied by a [code repository](https://github.com/facebookresearch/metaseq)
    and even a logbook that details the pre-training process of all models. Although
    OPT models are [not commercially-usable](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md),
    they are an incredibly resource that heavily influenced the open availability
    of LLMs for research.
  prefs: []
  type: TYPE_NORMAL
- en: '**The impact.** The OPT language models were the first large-scale effort to
    make massive language models accessible to the research community — *LLMs were
    now fully-available to anyone*, rather than being hidden behind an API. Additionally,
    OPT’s open-source training code makes a highly efficient training framework, using
    common techniques like [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/)
    and [tensor parallelism](https://github.com/NVIDIA/Megatron-LM?fbclid=IwAR3SvXpTaLseZacJv_Bntwg0czNNYj8hEhcho3R_mo8ABDS8zmszw4mdZ3E),
    readily available. This code achieves resource utilization that is 17% better
    than research published directly by NVIDIA [3], making it a great resource for
    training LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0552d7057a25f7f263fc74ffc6965c5f.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [5])
  prefs: []
  type: TYPE_NORMAL
- en: The training [notes](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles?fbclid=IwAR3qONxU4mENL_HAVcf9LJCwwqijGCVMk87C8Sm9_q3y6TZS3kZiY6Fd5dY)
    and [logbook](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf?fbclid=IwAR1gSseT67AGnNprJRdiW91Pf7eW1b82Z3pYshE4CYGT_-AKVnCUdaIdmm8)
    associated with OPT provide a massive amount of (previously unknown) insight into
    the LLM training process. From these resources, we can better understand the full
    cost of training an LLM and the many struggles that may occur in this process
    (e.g., loss spikes, hardware failures, and other “mid flight” training adjustments
    that are required). Such difficulties with training LLMs became a topic of conversation
    and have since been (mostly) resolved by subsequent work on open-source LLMs;
    see above.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eeedf7b8ce8fd018b3f9256b75bc0a90.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [4])
  prefs: []
  type: TYPE_NORMAL
- en: '**Does it perform well?** OPT-175B was extensively compared to popular models
    at the time of its proposal and found to achieve comparable performance to GPT-3
    in zero and few-shot learning settings; see above. Overall, OPT’s performance
    is not notable — *the model is widely considered to lag behind proprietary models
    in terms of quality*. Despite its lackluster performance, however, OPT was a massive
    step forward for AI research and significantly boosted the level of interest in
    open-source LLMs. This impact should not be understated, as it came at a time
    when the dominance of proprietary models had been accepted as a new standard.'
  prefs: []
  type: TYPE_NORMAL
- en: '[BLOOM: An Open, Multilingual Language Model](https://bigscience.huggingface.co/blog/bloom)
    [12]'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “Academia, nonprofits and smaller companies’ research labs find it difficult
    to create, study, or even use LLMs as only a few industrial labs with the necessary
    resources and exclusive rights can fully access them.” *— from [12]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Proposed in [12], BLOOM is an 176 billion parameter LLM that was trained as
    part of a massive, open collaboration of AI researchers (i.e., over 1000 researchers
    participated!), called the [Big Science Research Workshop](https://bigscience.huggingface.co/).
    Running over the timespan of one year (May 2021 to May 2022), the goal of this
    workshop was to create *i)* a massive multilingual text dataset and *ii)* a large
    multilingual language model that is trained on this dataset. The resulting model,
    which is slightly larger than GPT-3 and is open-sourced under the [Responsible
    AI License](https://bigscience.huggingface.co/blog/the-bigscience-rail-license)
    (RAIL), can generate text in 46 different languages[8](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-8-135273362)
    and 13 programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: '**The dataset** developed for training BLOOM, called the [ROOTS corpus](https://arxiv.org/abs/2303.03915),
    is comprised of 498 HuggingFace datasets and contains over 1.6 terabytes of text
    that spans 46 natural languages and 13 programming languages. The distribution
    of this dataset across the different languages is shown in the figure below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/086efab8d0133082762b58c2ac406cf9.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [12])
  prefs: []
  type: TYPE_NORMAL
- en: After obtaining the raw data, the authors apply a pipeline of different quality
    filters to remove text that is not natural language. The exact filtering components
    that are used, which are further outlined in Section 3.1.3 of [12], change depending
    on the source of the data. However, the overall pipeline shares a common goal
    of filtering out as much low-quality text as possible.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d2bd4c46a9310ef6a98766cfb9c9a3b.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [12])
  prefs: []
  type: TYPE_NORMAL
- en: '**The architecture** used by BLOOM is a standard decoder-only transformer.
    As shown above, however, a few modifications are made to this architecture, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ALiBi* [13]: This aids the model in generalizing to longer context lengths
    than those seen during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Embedding Layer Norm*: An extra [layer norm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
    is placed after the model’s embedding layer, which is empirically found to improve
    training stability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, this model is not much different than most LLMs. Interestingly, authors
    in [12] perform an extensive analysis between different types of transformer architectures
    (e.g., encoder-only models, encoder-decoder models, and decoder-only models),
    finding that the decoder-only model (used by nearly all causal language models)
    achieves the best performance after pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: “Our results show that immediately after pre-training, causal decoder-only models
    performed best — validating the choice of state-of-the-art LLMs.” *— from [12]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Does it perform well?** Compared to other open-source LLMs, BLOOM performs
    relatively well. It achieves comparable, or improved, results relative to OPT
    in natural language benchmarks and tends to excel at machine translation tasks
    given that it was trained on a multilingual corpus; see below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a931372cfd9177c919402852aa8c24b3.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [12])
  prefs: []
  type: TYPE_NORMAL
- en: However, BLOOM’s performance falls below that of top proprietary models. For
    example, we see in results on the HumanEval benchmark (shown below) that the model’s
    coding abilities fall far short of alternatives like Codex [14]. Additionally,
    when we compare the performance of BLOOM to models like Chinchilla [15] and PaLM
    [9], we quickly see that the performance of open-source models falls short of
    their proprietary counterparts. In other words, *research in open-source LLMs
    was still lagging at the time when BLOOM was proposed*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52f1b3c499c80044940a223632231145.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [12])
  prefs: []
  type: TYPE_NORMAL
- en: Other Notable Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We tried to cover several notable models that were proposed during the early
    days of open-source LLM research. But, there are still a few models not covered
    in this overview that are worth mentioning. Let’s take a quick look at a few of
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-J [21]** is a 6 billion parameter, English-only causal language model
    that was proposed prior to GPT-NeoX-20B [6]. Similar to GPT-NeoX-20B, this model
    was pre-trained on the Pile. At the time of its release, GPT-J-6B was the largest
    publicly-available GPT-3-style language model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/faed1a07b5d94c956ac692853a70d524.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [20])
  prefs: []
  type: TYPE_NORMAL
- en: '**GLM [20]** is more of a pre-training objective rather than a language model.
    This work explores the idea of unifying different pre-training techniques (e.g.,
    from BERT, T5, and GPT) by proposing a autoregressive blank infilling objective.
    In other words, we predict masked words in a sentence in an autoregressive manner,
    similar to a language model; see above. The resulting model, which is quite small
    (<1 billion parameters), is found to outperform BERT, T5 and GPT on several popular
    natural language processing benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: Where do we go from here?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/d1a8829aad7d90260ea20581ae9b7e16.png)'
  prefs: []
  type: TYPE_IMG
- en: The evolution of open-source LLM research (created by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that initial attempts at open-source LLMs yielded models that did not
    perform nearly as well as proprietary counterparts, we might reasonably wonder:
    *What should we do to make these models better?* As this research area has evolved,
    we have seen effort invested into two primary areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating better base LLMs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tuning open-source LLMs (i.e., alignment and imitation)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given that open-source LLMs are accessible to everyone, research in these areas
    progressed at a shocking pace — *we went from OPT to near state-of-the-art models
    (e.g., LLaMA-2 or Falcon-40B [10]) in less than a year*!
  prefs: []
  type: TYPE_NORMAL
- en: “We argue that the highest leverage action for improving open-source models
    is to tackle the difficult challenge of developing better base LMs” *— from [16]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Both of the research directions outlined above were explored in parallel during
    this time, and each resulted in the development of useful techniques for AI practitioners.
    Within the next two parts of this survey, we will overview each of these areas
    and the key contributions of each, exploring how initial attempts at open-source
    LLMs evolved into incredibly-capable models such as LLaMA-2.
  prefs: []
  type: TYPE_NORMAL
- en: Connect with me!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. If you liked this overview, subscribe
    to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I help readers understand AI research via overviews of relevant topics from
    the ground up. You can also follow me on [X](https://twitter.com/cwolferesearch)
    and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/), or
    check out my [other writings](https://medium.com/@wolfecameron) on medium!
  prefs: []
  type: TYPE_NORMAL
- en: Bibliography
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Brown, Tom, et al. “Language models are few-shot learners.” *Advances in
    neural information processing systems* 33 (2020): 1877–1901.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Rae, Jack W., et al. “Scaling language models: Methods, analysis & insights
    from training gopher.” *arXiv preprint arXiv:2112.11446* (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Smith, Shaden, et al. “Using deepspeed and megatron to train megatron-turing
    nlg 530b, a large-scale generative language model.” *arXiv preprint arXiv:2201.11990*
    (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Zhang, Susan, et al. “OPT: Open Pre-trained Transformer Language Models.”
    *arXiv preprint arXiv:2205.01068* (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable
    Llms.” *MosaicML*, 5 May 2023, [www.mosaicml.com/blog/mpt-7b.](http://www.mosaicml.com/blog/mpt-7b.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Black, Sid, et al. “Gpt-neox-20b: An open-source autoregressive language
    model.” *arXiv preprint arXiv:2204.06745* (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Gao, Leo, et al. “The pile: An 800gb dataset of diverse text for language
    modeling.” *arXiv preprint arXiv:2101.00027* (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Su, Jianlin, et al. “Roformer: Enhanced transformer with rotary position
    embedding.” *arXiv preprint arXiv:2104.09864* (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Chowdhery, Aakanksha, et al. “Palm: Scaling language modeling with pathways.”
    *arXiv preprint arXiv:2204.02311* (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] “Introducing Falcon LLM”, *Technology Innovation Institute*, 7 June 2023,
    [https://falconllm.tii.ae/.](https://falconllm.tii.ae/.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Radford, Alec, et al. “Language Models are Unsupervised Multitask Learners.”'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Scao, Teven Le, et al. “Bloom: A 176b-parameter open-access multilingual
    language model.” *arXiv preprint arXiv:2211.05100* (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Press, Ofir, Noah A. Smith, and Mike Lewis. “Train short, test long: Attention
    with linear biases enables input length extrapolation.” *arXiv preprint arXiv:2108.12409*
    (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] Chen, Mark, et al. “Evaluating large language models trained on code.”
    *arXiv preprint arXiv:2107.03374* (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] Hoffmann, Jordan, et al. “Training compute-optimal large language models.”
    *arXiv preprint arXiv:2203.15556* (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[16] Gudibande, Arnav, et al. “The false promise of imitating proprietary llms.”
    *arXiv preprint arXiv:2305.15717* (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[17] Vaswani, Ashish, et al. “Attention is all you need.” *Advances in neural
    information processing systems* 30 (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '[18] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '[19] Ouyang, Long, et al. “Training language models to follow instructions
    with human feedback.” *Advances in Neural Information Processing Systems* 35 (2022):
    27730–27744.'
  prefs: []
  type: TYPE_NORMAL
- en: '[20] Du, Zhengxiao, et al. “Glm: General language model pretraining with autoregressive
    blank infilling.” *arXiv preprint arXiv:2103.10360* (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[21] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 billion parameter autoregressive
    language model, 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[22] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021\.
    GPT-Neo: Large scale autoregressive language modeling with MeshTensorflow.'
  prefs: []
  type: TYPE_NORMAL
