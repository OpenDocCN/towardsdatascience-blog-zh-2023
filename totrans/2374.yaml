- en: Why More Is More (in Artificial Intelligence)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5](https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How Large Neural Networks Generalize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    ·10 min read·Aug 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Less is more.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: -***Ludwig Mies van der Rohe***
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Less is more only when more is too much.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- ***Frank Loyd Wright***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep neural networks (DNNs) have profoundly transformed the landscape of machine
    learning, often becoming synonymous with the broader fields of artificial intelligence
    and machine learning. Yet, their rise would have been unimaginable without their
    partner-in-crime: stochastic gradient descent (SGD).'
  prefs: []
  type: TYPE_NORMAL
- en: 'SGD, along with its derivative optimizers, forms the core of many self-learning
    algorithms. At its heart, the concept is straightforward: calculate the task’s
    loss using training data, determine the gradients of this loss in relation to
    its parameters, and then adjust the parameters in a direction that minimizes the
    loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It sounds simple, but in applications, it has proven to be immensely powerful:
    SGD can find solutions for all kinds of complex problems and training data, given
    it is used in conjunction with a sufficiently expressive architecture. It’s particularly
    good at finding parameter sets that make the network perform perfectly on the
    training data, something called the **interpolation regime**. But under which
    conditions are neural networks thought to **generalize well**, meaning that they
    perform well on unseen test data?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcd15cc481d3711756d2673df3509c41.png)'
  prefs: []
  type: TYPE_IMG
- en: The quest to generalize lies at the heart of machine learning. Envisioned by
    DALL-E.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some ways, it’s almost too powerful: SGD''s abilities aren’t only limited
    to training data that can be expected to lead to good generalization. It has been
    shown e.g. [in this influential paper](https://arxiv.org/abs/1611.03530), that
    SGD can make a network perfectly memorize a set of images that were randomly labeled
    ([there is a deep relationship between memory and generalization that I have written
    about previously](/memory-and-generalization-in-artificial-intelligence-35e006ca0a9a)).
    Although this might appear challenging — given the mismatch between labels and
    image content — it’s surprisingly straightforward for neural networks trained
    with SGD. In fact, it’s not much more challenging than fitting genuine data.'
  prefs: []
  type: TYPE_NORMAL
- en: This ability indicates that NNs, trained with SGD, run the risk of overfitting,
    and measures for regularizing overfitting, such as norms, early stopping, and
    **reducing model size** become crucial to avoid it.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the point of classical statistics, less is more, and so more is less,
    as summarized concisely in this DALL-E painting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7ed758643d413a422f2bbdb18d99eec.png)'
  prefs: []
  type: TYPE_IMG
- en: More is Less, by DALL-E.
  prefs: []
  type: TYPE_NORMAL
- en: '**“Everything should be made as simple as possible, but not simpler.”** *—
    Albert Einstein*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'But here is the rub: one of the most surprising properties of state-of-the-art
    deep learning is that most architectures work best in a **highly overparameterized
    regime, meaning when they have many more parameters than data points**. This is
    surprising because standard learning theories, supported by basic intuitions,
    suggest that with so many parameters, DNNs should just over-adjust to the specific
    training data, and be poor at dealing with new, unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is captured nicely in the famous **“double-descent”** curve, popularized
    by the 2019 paper by Belkin et al. “Reconciling modern machine learning practice
    and the bias-variance trade-off”:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ab908493f79a3909e9e6f2a48d4741d.png)'
  prefs: []
  type: TYPE_IMG
- en: Classical U-shaped learning curve and double descent observed for high capacity
    function classes. [Taken from Belkin et al., 2019.](https://arxiv.org/pdf/1812.11118.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: While in the “classical” regime, increasing capacity leads to a reduction of
    both train and test loss, beyond the sweet spot, test risk increases again, while
    training risk goes to zero (“overfitting regime”).
  prefs: []
  type: TYPE_NORMAL
- en: 'But modern machine learning algorithms have pushed beyond this threshold: moving
    to the modern interpolating regime with highly over-parameterized models, test
    risk decreases again, even falling below levels that were feasible in the classical
    regime, achieving unheard-of generalization.'
  prefs: []
  type: TYPE_NORMAL
- en: The miracle of the appropriateness of the language of mathematics for the formulation
    of the laws of physics is a wonderful gift which we neither understand nor deserve.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Eugene Wigner***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The fact that neural networks generalize, even with their inherent propensity
    to overfit, indicates the presence of what is generally known as an **inductive
    bias**. An inductive bias refers to the (usually unspoken) set of assumptions
    that a learning algorithm makes to predict outputs for unseen data points, based
    on the data it was trained on. Much like with our often unconscious (and often
    rather problematic) cognitive biases, the inductive bias of the architecture determines
    **its preference for certain solutions**, which guides it when there’s uncertainty
    or multiple solutions are possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'As it turns out, we are very lucky: much research points towards the fact that
    DNNs trained with SGD have an inductive bias towards functions that **generalize
    well** (on structured data, which counts for most of the data we are interested
    in applying DNNs to). This is among the set of surprises that make deep learning
    work so well, discussed by Terrence Sejnowski as [“The unreasonable effectiveness
    of deep learning in artificial intelligence”](https://www.pnas.org/doi/10.1073/pnas.1907373117),
    echoing Eugene Wigner’s famous paper from six decades prior on the [unreasonable
    effectiveness of mathematics in the natural sciences](https://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences)
    that we “neither understand nor deserve”.'
  prefs: []
  type: TYPE_NORMAL
- en: “It is a profound and necessary truth that the deep things in science are not
    found because they are useful; they are found because it was possible to find
    them.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***J. Robert Oppenheimer***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This unreasonable effectiveness has enabled many groundbreaking applications
    of deep learning that might have seemed impossible.
  prefs: []
  type: TYPE_NORMAL
- en: 'It might also be something we don’t deserve, or rather, **should not (yet)
    deserve**: given AI’s rapid evolution and the increasing chorus of cautionary
    voices urging a pause in AI research, the success of deep learning has also opened
    Pandora’s box that will likely define the path of mankind in the coming century.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dde2c148ab8d78d258af32f164873c9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Opening of Pandora’s box, as envisioned by DALL-E.
  prefs: []
  type: TYPE_NORMAL
- en: But, in contrast to the metaphysical question Wigner raised about the underlying
    relationship between mathematics and physics, between mind and reality, the success
    of deep learning is at least something we might be able to grasp.
  prefs: []
  type: TYPE_NORMAL
- en: While much theoretical work has gone into understanding why neural networks
    generalize so well, theory in deep learning is lagging behind, and there is still
    no clear-cut, universally agreed-upon explanation for why DNNs generalize so well.
  prefs: []
  type: TYPE_NORMAL
- en: Many theoretical proposals, such as the insight that local minima are global
    minima, often hold for only limited architectures, [such as linear neural networks](http://proceedings.mlr.press/v80/laurent18a/laurent18a.pdf)
    or simple non-linear networks. Often, results derived from learning theory can
    be at odds with practical experiences that actually make deep neural networks
    work, such as the existence of many suboptimal local minima in actual deep neural
    networks (explored in this aptly titled paper on [truth or backpropaganda](https://arxiv.org/pdf/1910.00359.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: But while there are no universally agreed-upon explanations yet, there are many
    fascinating insights from deep learning theory that can advance our understanding
    and intuitions about why generalization in DNNs works.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the rest of this article, I want to explore a couple of explanatory attempts
    at understanding why ***more is frequently more*** for neural network training
    (a quick note: I will focus on in-distribution generalization, and not address
    the growing and equally exciting field looking at distribution shifts, called
    out-of-distribution/OOD generalization).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the beginning, there are roughly speaking, two potential culprits:
    **SGD/optimization** and the **Neural Networks** themselves.'
  prefs: []
  type: TYPE_NORMAL
- en: The possibility that SGD might be responsible for the inductive bias that leads
    to good generalization has sparked a wealth of research. This research delves
    into the various mechanisms and optimizers under which these generalization properties
    manifest. Interestingly, among the myriad of optimizer variants available today,
    from SGD to Adam to Adagrad to LBFGS, it appears that the choice of optimizer
    **plays a minimal role in influencing generalization**. [Most, if not all, manage
    to achieve comparable generalization outcomes across diverse tasks](https://www.youtube.com/watch?v=kcVWAKf7UAg).
    And while it has been theorized that the inherently stochastic nature of SGD helps
    models escape from sharp, non-generalizing minima, pushing them towards broader,
    more generalizing regions of the loss landscape, and has a regularizing effect
    on parameters, even full batch SGD (performing training on the whole train data
    at once) generalizes well in many cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'This suggests that generalization is perhaps more intertwined with the **characteristics
    of the loss landscape** than with the specific optimizer steering the learning
    algorithm’s journey. It raises an interesting question: how does the generalization
    capability of a particular minimum correlate with the surrounding loss landscape’s
    attributes?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A popular [explanation that’s been around since the 90s](https://www.bioinf.jku.at/publications/older/3304.pdf)
    posits that **sharp minima tend to generalize poorly compared to their flat counterparts**.
    Intuitively, this relates to how close decision boundaries are to data points
    during classification: In scenarios where generalization is suboptimal and overfitting
    is rampant, decision boundaries are nestled closely to data points, so small changes
    in parameters can lead to sharp changes in loss, since they also change the classification
    of several points at once. In turn, flat minima lead to wide decision margins,
    reminiscent of the margins aimed for in Support Vector Machines.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e6581d694363be95b7f032eaaea348b.png)'
  prefs: []
  type: TYPE_IMG
- en: Rugged loss landscapes with spiky, sharp local minima vs. smoother and wider
    local minima. Smoothness of the loss landscape also depends on training techniques
    and regulaization techniques, such as drop-out. [Adopted from our recent ICML
    paper](https://proceedings.mlr.press/v202/hess23a.html).
  prefs: []
  type: TYPE_NORMAL
- en: Since gradient descent is more **likely to run into flat minima** during optimization,
    generalizing solutions are naturally preferred, an effect exacerbated by the usually
    very high-dimensional loss landscapes. Sharp minima become the proverbial *needle
    in the high-dimensional haystack*, while flat minima are, well, the hay in the
    haystack, and given the high-dimensional nature of the problem [**it can be several
    thousands of orders** harder to find a sharp, overfitting minimum than a flat
    one](https://www.youtube.com/watch?v=kcVWAKf7UAg).
  prefs: []
  type: TYPE_NORMAL
- en: On top, flatter minima are also simply larger than sharp minima volume-wise,
    and again the counter-intuitive scaling of volume in high-dimensional spaces means
    that this can lead to many orders of magnitude differences in their respective
    volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another phenomenon frequently observed in practice can further come in handy:
    [local minima are in practice often thought to be connected by valleys of small
    loss](http://proceedings.mlr.press/v80/draxler18a/draxler18a.pdf), known as “loss
    valleys.” SGD, with its iterative nature, can traverse these valleys and is more
    likely to find and settle in these broader, more generalizable regions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having discussed SGD and its interplay with loss landscapes, it’s time to think
    about neural networks directly: could it be that randomly initialized deep neural
    networks themselves already exhibit some kind of **inductive bias for “simple/generalizing”
    solutions**, whatever simple means?'
  prefs: []
  type: TYPE_NORMAL
- en: If simple networks lead to simple solutions, which many of the processes modeled
    by DNNs actually are (i.e. much lower-dimensional than fitting e.g. random noise),
    then the propensity of a neural network to represent a simple solution would already
    give us a lot.
  prefs: []
  type: TYPE_NORMAL
- en: A nice way to check whether SGD has much to do with the success of neural networks
    is to see how they would solve problems without being explicitly trained via SGD.
  prefs: []
  type: TYPE_NORMAL
- en: A recent paper ([Is SGD a Bayesian sampler? Well, almost.](https://arxiv.org/pdf/2006.15191.pdf))
    proposes that the inductive bias, which nudges DNNs towards “simpler” solutions,
    might not primarily be a special property of SGD, but rather an **inherent characteristic
    of the networks**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper’s central premise revolves around two probabilities: P_SGD(f∣S)—
    the likelihood that an overparameterized DNN, trained using SGD or its variants,
    will converge to a function f consistent with a training set S; and P_Bayesian(f∣S)
    — the Bayesian posterior probability that the function f is expressed by a DNN
    when its parameters are randomly initialized, given the training set S.'
  prefs: []
  type: TYPE_NORMAL
- en: Leaving details aside, the study discovered a significant correlation between
    these two probabilities. More importantly, **the Bayesian sampler showcased a
    strong inclination towards functions with low error and minimal complexity**.
    This finding suggests that the DNNs’ inherent structure and its vastness, rather
    than the specificities of SGD, could be the dominant factors behind their ability
    to generalize in overparameterized settings.
  prefs: []
  type: TYPE_NORMAL
- en: However, the paper also highlights that while these two probabilities are closely
    related, there still exist nuanced differences sensitive to hyperparameters. This
    means that factors like architecture variations, batch size, learning rate, and
    optimizer choice can still matter a lot in DNN performance.
  prefs: []
  type: TYPE_NORMAL
- en: “Luck is what happens when preparation meets opportunity.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***— Seneca***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The fact that architecture could prefer simple solutions also connects nicely
    with another new insight on overparameterized networks: the so-called [Lottery
    Ticket Hypothesis](https://arxiv.org/abs/1803.03635).'
  prefs: []
  type: TYPE_NORMAL
- en: It proposes that within large, over-parameterized networks, there exist smaller,
    sparser subnetworks that are primed for success. These “winning tickets”, as they
    are called, are not just lucky finds but are already initialized in a way that
    makes them particularly good at learning the specific task at hand. Within the
    overparameterized model, there exist numerous “winning tickets” or optimal subnetworks.
    These substructures are predisposed to learn efficiently and generalize well.
    The overarching network’s redundancy provides a form of ensemble learning, where
    multiple “good” subnetworks collectively converge towards robust solutions. Furthermore,
    the overabundance of parameters might aid in the exploration of the loss landscape,
    enabling the model to discover and leverage these subnetworks effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my previous article, I have delved into the [Free Lunch Theorem](https://medium.com/towards-data-science/why-there-kind-of-is-free-lunch-56f3d3c4279f),
    and how transfer learning of large pre-trained models gives us something akin
    to free lunch. This connects nicely with the topic of generalization: large pre-trained
    models often exhibit remarkable generalization capabilities, even with limited
    domain-specific data. This phenomenon seemingly defies the traditional bias-variance
    trade-off, echoing the broader theme of overparameterized models generalizing
    well. Models like GPT-4 have shown us that having more parameters when complemented
    by diverse enough data, can indeed lead to models that not only fit the training
    data well but also excel in seemingly novel, unseen scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But the quality of the data that is trained on is perhaps the most crucial
    ingredient to generalization. Sam Altman frequently emphasizes that data curation
    was instrumental in the effectiveness of GPT-4\. And “overfitting” on the right
    kind of data can be enough to give you what you are looking for. As Letitia Parcalabescu,
    the host of [AI Coffee Break](https://www.youtube.com/c/aicoffeebreak), said in
    a [recent podcast episode](https://open.spotify.com/episode/2sUYtwQKvLleRshLio4t1g?si=2b8af33b7d3b4bd9)
    we recorded together: “If you overfit on the entire world, you are basically done”.'
  prefs: []
  type: TYPE_NORMAL
- en: It is likely that large neural networks generalize well for a number of reasons.
    It is something most people did not expect, coming from a decade-old, deeply ingrained
    tradition of statistical thinking about the bias-variance tradeoff in the 20th
    century.
  prefs: []
  type: TYPE_NORMAL
- en: But it works. And it’s fascinating that despite all the models and algorithms
    having been developed by humans, we still are not sure what‘s going on. Maybe,
    someday in the future, we will find out. And if we don’t, I’m sure AI algorithms,
    in a quest to know themselves, will.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: If you like my writing, please [subscribe to get my stories via mail](https://manuel-brenner.medium.com/subscribe),
    or [consider becoming a referred member](https://manuel-brenner.medium.com/membership).
  prefs: []
  type: TYPE_NORMAL
