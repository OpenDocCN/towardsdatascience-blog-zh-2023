["```py\ndef translate_text(\n    text: Union[str, list],\n    target_language: Union[str, list],\n    source_language: Optional[str] = None,\n    api_version: str = '3.0') -> tuple:\n    \"\"\"translates txt using the microsoft translate API\n\n    :param text: text to be translated. Either single or multiple (stored in a list)\n    :param target_language: ISO format of target translation languages\n    :param source_language: ISO format of source language. If not provided is inferred by the translator, defaults to None\n    :param api_version: api version to use, defaults to \"3.0\"\n    :return: for successful response, (status_code, [{\"translations\": [{\"text\": translated_text_1, \"to\": lang_1}, ...]}, ...]))        \n    \"\"\"\n\n    url = f'{MICROSOFT_TRANSLATE_URL}/translate?api-version={api_version}'\n\n    if isinstance(target_language, str):\n        target_language = [target_language]\n\n    # extend url with array parameters, e.g. f\"{url}&to=de&to=ru\"\n    url = add_array_api_parameters(\n        url,\n        param_name='to',\n        param_values=target_language\n    )\n\n    if source_language:\n        url = f'{url}&from={source_language}'\n\n    if isinstance(text, str):\n        text = [text]\n\n    body = [{'text': text_} for text_ in text]\n\n    LOGGER.info(f'Translating {len(text)} texts to {len(target_language)} languages')\n    resp = requests.post(url, headers=HEADERS, json=body)\n    status_code = resp.status_code\n\n    if is_request_valid(status_code):\n        return resp.json(), status_code\n\n    return resp.text, status_code\n```", "```py\ntexts = [\"Hi this is a dummy text\", \"Oh no dummy text\"]\ntarget_languages = [\"de\", \"ru\"]\nn_target_langs = len(target_languages)\ntext_sizes = [len(text) for text in texts]\ntotal_size = sum(text_sizes) * n_target_langs\nassert total_size <= 50000\n```", "```py\nfrom typing import Dict, List\n\ndef batch_by_size(sizes: Dict[int, int], limit: int, sort_docs: bool = False) -> List[Dict[str, Union[int, List[int]]]]:\n    \"\"\"Given a size mapping such {document_id: size_of_document}, batches documents such that the total size of a batch of documents does not exceed pre-specified limit\n\n    :param sizes: mapping that gives document size for each document_id\n    :param limit: size limit for each batch\n    :sort_doc: if True sorts `sizes` in descending order\n    :return: [{'idx': [ids_for_batch], 'total_size': total_size_of_documents_in_batch}, ...]\n\n    Example:\n        >>> documents = ['Joe Smith is cool', 'Django', 'Hi']\n        >>> sizes = {i: len(doc) for i, doc in enumerate(documents)}\n        >>> limit = 10\n        >>> batch_by_size(sizes, limit)\n        [{'idx': [0], 'total_size': 17}, {'idx': [1, 2], 'total_size': 8}]\n    \"\"\"\n    if sort_docs:\n        sizes = {key: size for key, size in sorted(sizes.items(), key=lambda x: x[1], reverse=True)}\n\n    batched_items = []\n    sizes_iter = iter(sizes)\n    key = next(sizes_iter)\n    while key is not None:\n        if not batched_items:\n            batched_items.append({\n                'idx': [key],\n                'total_size': sizes[key]\n            })\n        else:\n            size = sizes[key]\n            if size > limit:\n                LOGGER.warning(f'Document {key} exceeds max limit size: {size}>{limit}')\n            total_size = batched_items[-1]['total_size'] + size\n            if total_size > limit:\n                batched_items.append({\n                    'idx': [key],\n                    'total_size': size\n                })\n            else:\n                batched_items[-1]['idx'].append(key)\n                batched_items[-1]['total_size'] = total_size\n        key = next(sizes_iter, None)\n\n    return batched_items\n```", "```py\nMAX_CHARACTER_LIMITS = 50000\ndef translate_text(\n    text: Union[str, list],\n    target_language: Union[str, list],\n    source_language: Optional[str] = None,\n    api_version: str = '3.0') -> tuple:\n    \"\"\"translates txt using the microsoft translate API\n\n    :param text: text to be translated. Either single or multiple (stored in a list)\n    :param target_language: ISO format of target translation languages\n    :param source_language: ISO format of source language. If not provided is inferred by the translator, defaults to None\n    :param api_version: api version to use, defaults to \"3.0\"\n    :return: for successful response, (status_code, [{\"translations\": [{\"text\": translated_text_1, \"to\": lang_1}, ...]}, ...]))        \n    \"\"\"\n\n    url = f'{MICROSOFT_TRANSLATE_URL}/translate?api-version={api_version}'\n\n    if isinstance(target_language, str):\n        target_language = [target_language]\n\n    # extend url with array parameters, e.g. f\"{url}&to=de&to=ru\"\n    url = add_array_api_parameters(\n        url,\n        param_name='to',\n        param_values=target_language\n    )\n\n    if source_language:\n        url = f'{url}&from={source_language}'\n\n    if isinstance(text, str):\n        text = [text]\n\n    ### start of new code --------------------------------------------\n    n_target_langs = len(target_language)\n\n    # get maximum size of each document\n\n    sizes_dict = {i: len(text_)*n_target_langs for i, text_ in enumerate(text)}\n    batched_texts = batch_by_size(sizes_dict, MAX_CHARACTER_LIMITS)\n\n    # for each batch, translate the texts. If successful append to list\n    translation_outputs = []\n    for batch in batched_texts:\n        doc_ids = batch['idx']\n        batch_texts = [text[doc_id] for doc_id in doc_ids]\n        body = [{'text': text_} for text_ in batch_texts]\n        LOGGER.info(f'Translating {len(text)} texts to {len(target_language)} languages')\n        resp = requests.post(url, headers=HEADERS, json=body)\n        status_code = resp.status_code\n        if not is_request_valid(status_code):\n            raise Exception(f'Translation failed for texts {doc_ids}')\n\n        translation_output = resp.json()\n        translation_outouts += translation_output\n\n    return translation_outouts, status_code\n    ### end of new code ----------------------------------------------\n```", "```py\nmax_limit = 10\ntext = \"Hello!\"\ntext_size = len(text)  # 6\ntarget_languages = [\"de\", \"ru\"]\nn_target_langs = len(target_languages)\ntotal_size = text_size * n_target_langs  # 12\nassert total_size <= max_limit  # 12 <= 10, raises error!\n```", "```py\nMAX_CHARACTER_LIMITS = 50000\ndef translate_text(\n    text: Union[str, list],\n    target_language: Union[str, list],\n    source_language: Optional[str] = None,\n    api_version: str = '3.0') -> tuple:\n    \"\"\"translates txt using the microsoft translate API\n\n    :param text: text to be translated. Either single or multiple (stored in a list)\n    :param target_language: ISO format of target translation languages\n    :param source_language: ISO format of source language. If not provided is inferred by the translator, defaults to None\n    :param api_version: api version to use, defaults to \"3.0\"\n    :return: for successful response, (status_code, [{\"translations\": [{\"text\": translated_text_1, \"to\": lang_1}, ...]}, ...]))        \n    \"\"\"\n\n    url = f'{MICROSOFT_TRANSLATE_URL}/translate?api-version={api_version}'\n\n    if isinstance(target_language, str):\n        target_language = [target_language]\n\n    if source_language:\n        url = f'{url}&from={source_language}'\n\n    if isinstance(text, str):\n        text = [text]\n\n    n_target_langs = len(target_language)\n\n    # get maximum size of each document\n\n    sizes_dict = {i: len(text_)*n_target_langs for i, text_ in enumerate(texts)}\n    batched_texts = batch_by_size(sizes_dict, MAX_CHARACTER_LIMITS)\n\n    # for each batch, translate the texts. If successful append to list\n    translation_outputs = []\n    for batch in batched_texts:\n        doc_ids = batch['idx']\n        total_size = batch['total_size']\n        ### start of new code --------------------------------------------\n\n        # case where single doc too big to be translated to all languages, but small enough that it can be translated to some languages\n        if total_size > MAX_CHARACTER_LIMITS:\n            translation_output = [dict(translations=[])]\n            doc_size = sizes_dict[doc_ids[0]]  # necessarily only single document in batch\n            batch_size = MAX_CHARACTER_LIMITS // doc_size\n            batch_range = range(0, n_target_langs, batch_size)\n            n_batches = len(batch_range)\n\n            # batch by target languages\n            for batch_id, start_lang_idx in enumerate(batch_range):\n                end_lang_idx = start_lang_idx + batch_size\n                target_languages_ = target_language[start_lang_idx: end_lang_idx]\n\n                # rebuild the url for subset of langs\n                url_ = add_array_api_parameters(\n                    url,\n                    param_name='to',\n                    param_values=target_languages_\n                )\n                body = [{'text': text[doc_ids[0]]}]\n                LOGGER.debug(f'Translating batch {batch_id+1}/{n_batches} of text with idx={doc_ids[0]}. Target languages: {target_languages_}')\n                resp = requests.post(url_, headers=HEADERS, json=body)\n                status_code = resp.status_code\n                if not is_request_valid(status_code):\n                    raise Exception(f'Translation failed for texts {doc_ids[0]}')\n                partial_translation_output = resp.json()\n                # concatenate outputs in correct format\n                translation_output['translations'] += partial_translation_output['translations']\n\n        else:\n            # -- code as before, except translation_output now part of else\n            batch_texts = [text[doc_id] for doc_id in doc_ids]\n            body = [{'text': text_} for text_ in batch_texts]\n            LOGGER.info(f'Translating {len(text)} texts to {len(target_language)} languages')\n            # rebuild url for all languages\n            url_ = add_array_api_parameters(\n                url,\n                param_name='to',\n                param_values=target_language\n            )\n            resp = requests.post(url_, headers=HEADERS, json=body)\n            status_code = resp.status_code\n            if not is_request_valid(status_code):\n                raise Exception(f'Translation failed for texts {doc_ids}')\n\n            translation_output = resp.json()\n        ### end of new code ----------------------------------------------\n        translation_outputs += translation_output\n\n    return translation_outputs, status_code\n```", "```py\n if total_size > MAX_CHARACTER_LIMITS:\n            # -- start of new code\n            doc_size = sizes_dict[doc_ids[0]]  # necessarily only single document in batch\n            batch_size = MAX_CHARACTER_LIMITS // doc_size\n            if not batch_size:\n                msg = f'Text `{doc_ids[0]}` too large to be translated'\n                if raise_error_on_translation_failure:\n                    raise Exception(msg)\n                LOGGER.error(msg)\n            else:            \n                batch_range = range(0, n_target_langs, batch_size)\n                translation_output = [dict(translations=[])]\n                # -- end of new code\n```", "```py\nMAX_CHARACTER_LIMITS_PER_HOUR = 2000000\ndef _sleep(time_of_last_success, request_size):\n    if time_of_last_success:\n        time_diff_since_first_request = time.time() - time_of_last_success\n        time_diff_needed_for_next_request = request_size / (MAX_CHARACTER_LIMITS_PER_HOUR / 3600) \n        sleep_time = time_diff_needed_for_next_request - time_diff_since_first_request\n        if sleep_time > 0:\n            LOGGER.debug(f'Sleeping {sleep_time:.3g} seconds...')\n            time.sleep(sleep_time)\n```", "```py\ndef batch_by_size_min_buckets(sizes: Dict[Union[int, str], int], limit: int, sort_docs: bool = True) -> List[Dict[str, Union[int, List[int]]]]:\n    \"\"\"Given dictionary of documents and their sizes {doc_id: doc_size}, batch documents such that the total size of each batch <= limit. Algorithm designed to decrease number of batches, but does not guarantee that it will be an optimal fit\n\n    :param sizes: mapping that gives document size for each document_id, {doc_1: 10, doc_2: 20, ...}\n    :param limit: size limit for each batch\n    :sort_doc: if True sorts `sizes` in descending order\n    :return: [{'idx': [ids_for_batch], 'total_size': total_size_of_documents_in_batch}, ...]\n\n    Example:\n        >>> documents = ['Joe Smith is cool', 'Django', 'Hi']\n        >>> sizes = {i: len(doc) for i, doc in enumerate(documents)}\n        >>> limit = 10\n        >>> batch_by_size(sizes, limit)\n        [{'idx': [0], 'total_size': 17}, {'idx': [1, 2], 'total_size': 8}]\n    \"\"\"\n    if sort_docs:\n        sizes = {key: size for key, size in sorted(sizes.items(), key=lambda x: x[1], reverse=True)}\n\n    batched_items = []\n    sizes_iter = iter(sizes)\n    key = next(sizes_iter)  # doc_id\n\n    # -- helpers\n    def _add_doc(key):\n        batched_items.append({\n            'idx': [key],\n            'total_size': sizes[key]\n        })\n\n    def _append_doc_to_batch(batch_id, key):\n        batched_items[batch_id]['idx'].append(key)\n        batched_items[batch_id]['total_size'] += sizes[key]\n\n    while key is not None:\n\n        # initial condition\n        if not batched_items:\n            _add_doc(key)\n        else:\n            size = sizes[key]\n\n            if size > limit:\n                LOGGER.warning(f'Document {key} exceeds max limit size: {size}>{limit}')\n                _add_doc(key)\n            else:\n                # find the batch that fits the current doc best\n                batch_id = -1\n                total_capacity = limit - size  # how much we can still fit\n                min_capacity = total_capacity\n                for i, batched_item in enumerate(batched_items):\n                    total_size = batched_item['total_size']\n                    remaining_capacity =  total_capacity - total_size  # we want to minimise this\n\n                    # current batch too large for doc, go to next batch\n                    if remaining_capacity < 0:\n                        continue\n                    # current batch is a better fit for doc, save batch_id\n                    elif remaining_capacity < min_capacity:\n                        min_capacity = remaining_capacity\n                        batch_id = i\n\n                    # if perfect fit, break loop\n                    if remaining_capacity == 0:\n                        break\n\n                if batch_id == -1:\n                    _add_doc(key)\n                else:\n                    _append_doc_to_batch(batch_id, key)\n\n        key = next(sizes_iter, None) \n    return batched_items\n```", "```py\ndef translate_text(\n    text: Union[str, list],\n    target_language: Union[str, list],\n    source_language: Optional[str] = None,\n    api_version: str = '3.0', raise_error_on_translation_failure=True, include_partials_in_output=False) -> tuple:\n    \"\"\"translates txt using the microsoft translate API\n\n    :param text: text to be translated. Either single or multiple (stored in a list)\n    :param target_language: ISO format of target translation languages\n    :param source_language: ISO format of source language. If not provided is inferred by the translator, defaults to None\n    :param api_version: api version to use, defaults to \"3.0\"\n    :param raise_error_on_translation_failure: if `True`, raises errors on translation failure. If `False` ignores failed translations in output\n    :pram include_partials_in_output: if `True` includes partially translated texts in output, otherwise ignores them\n    :return: for successful response, (status_code, [{\"translations\": [{\"text\": translated_text_1, \"to\": lang_1}, ...]}, ...]))        \n    \"\"\"\n\n    url = f'{MICROSOFT_TRANSLATE_URL}/translate?api-version={api_version}'\n\n    if isinstance(target_language, str):\n        target_language = [target_language]\n\n    if source_language:\n        url = f'{url}&from={source_language}'\n\n    if isinstance(text, str):\n        text = [text]\n\n    n_target_langs = len(target_language)\n\n    sizes_dict = {i: len(text_)*n_target_langs for i, text_ in enumerate(text)}\n    batched_texts = batch_by_size_min_buckets(sizes_dict, CHARACTER_LIMITS)\n\n    translation_outputs = []\n    for batch in batched_texts:\n        translation_output = []\n        doc_ids = batch['idx']\n        total_size = batch['total_size']\n\n        if total_size > CHARACTER_LIMITS:\n            doc_size = sizes_dict[doc_ids[0]]\n            batch_size = CHARACTER_LIMITS // doc_size\n\n            if not batch_size:\n                msg = f'Text `{doc_ids[0]}` too large to be translated'\n                # -- new code ------------------------------------------\n                if raise_error_on_translation_failure:\n                    raise Exception(msg)\n                LOGGER.error(msg)\n                # -- end of new code ------------------------------------------\n\n            else:\n                _translation_output = dict(translations=[])\n                batch_range = range(0, n_target_langs, batch_size)\n                n_batches = len(batch_range)\n\n                _translation_failed = False  # to track translations at language batching level\n                for batch_id, start_lang_idx in enumerate(batch_range):\n                    end_lang_idx = start_lang_idx + batch_size\n                    target_languages_ = target_language[start_lang_idx: end_lang_idx]\n\n                    url_ = add_array_api_parameters(url, param_name='to', param_values=target_languages_)\n                    body = [{'text': text[doc_ids[0]]}]\n\n                    LOGGER.info(f'Translating batch {batch_id+1}/{n_batches} of text with idx={doc_ids[0]}. Target languages: {target_languages_}')\n                    resp = requests.post(url_, headers=HEADERS, json=body)\n                    status_code = resp.status_code\n                    if not is_request_valid(status_code):\n                        # -- new code ------------------------------------------\n                        msg = f'Partial translation of text `{doc_ids[0]}` to languages {target_languages_} failed.'\n                        if raise_error_on_translation_failure:\n                            raise Exception(msg)\n                        LOGGER.error(msg)\n                        _translation_failed = True\n                        if not include_partials_in_output:\n                            break\n                        # -- end of new code-------------------------------------\n\n                    partial_translation_output = resp.json()\n                    _translation_output['translations'] += partial_translation_output['translations']\n\n                # -- new code -------------------------------------------\n                if not _translation_failed or include_partials_in_output:\n                    translation_output.append(_translation_output)\n                # -- end of new code ------------------------------------\n\n        else:\n            batch_texts = [text[doc_id] for doc_id in doc_ids]\n            body = [{'text': text_} for text_ in batch_texts]\n            LOGGER.info(f'Translating {len(text)} texts to {len(target_language)} languages')\n            # rebuild url for all languages\n            url_ = add_array_api_parameters(\n                url,\n                param_name='to',\n                param_values=target_language\n            )\n            resp = requests.post(url_, headers=HEADERS, json=body)\n            status_code = resp.status_code\n            if not is_request_valid(status_code):\n                # -- new code -----------------------------------\n                msg = f'Translation failed for texts {doc_ids}. Reason: {resp.text}'\n                if raise_error_on_translation_failure:\n                    raise Exception(msg)\n                LOGGER.error(msg)\n                # -- end of new code --------------------------------\n            else:\n                translation_output += resp.json()\n\n        translation_outputs += translation_output\n\n    return translation_outputs, status_code\n```", "```py\nCHARACTER_LIMITS = 50000\nMAX_CHARACTER_LIMITS_PER_HOUR = 2000000\n\nclass MicrosoftTranslator:\n    \"\"\"Class for translating text using the Microsoft Translate API\n\n    :param api_version: api version to use, defaults to \"3.0\"\n    :param ignore_on_translation_failure: if `False`, returns failed translations with error and status code. If `False` ignores failed translations in output, defaults to False\n    :param include_partials_in_output: if `True` includes partially translated texts in output, otherwise ignores them, defaults to False\n\n    \"\"\"\n    def __init__(\n        self,\n        api_version: str = '3.0',\n        ignore_on_translation_failure: bool = False,\n        include_partials_in_output: bool = False\n    ):\n\n        base_url = f'{MICROSOFT_TRANSLATE_URL}/translate?api-version={api_version}'\n\n        self.base_url = base_url\n        self.api_version = api_version\n        self.ignore_on_translation_failure = ignore_on_translation_failure\n        self.include_partials_in_output = include_partials_in_output\n\n    def translate_text(\n        self,\n        texts: Union[str, list],\n        target_languages: Union[str, list],\n        source_language: Optional[str] = None\n    ) -> tuple:\n        \"\"\"translates txt using the microsoft translate API\n\n        :param texts: text(s) to be translated. Can either be a single text (str) or multiple (list)\n        :param target_languages: ISO format of target translation language(s). Can be single lang (str) or multiple (list)\n        :param source_language: ISO format of source language. If not provided is inferred by the translator, defaults to None\n        :return: for successful response, (status_code, [{\"translations\": [{\"text\": translated_text_1, \"to\": lang_1}, ...]}, ...]))         \n        \"\"\"\n\n        # -- create storage for translation failures and flag for failed translation\n        self._set_request_default\n\n        # add source language to url\n        if source_language:\n            base_url = f'{self.base_url}&from={source_language}'\n        else:\n            base_url = self.base_url\n\n        # standardise target_languages and texts types\n        if isinstance(target_languages, str):\n            target_languages = [target_languages]\n\n        if isinstance(texts, str):\n            texts = [texts]\n\n        # batch texts for translation, based on doc_size = len(doc)*n_target_langs\n        n_target_langs = len(target_languages)\n        sizes_dict = {i: len(text)*n_target_langs for i, text in enumerate(texts)}\n\n        profile_texts = self._profile_texts(sizes_dict)\n        if profile_texts:\n            return profile_texts\n\n        batched_texts = batch_by_size_min_buckets(sizes_dict, CHARACTER_LIMITS, sort_docs=True)\n\n        translation_outputs = []  # variable to store all translation outputs\n\n        for batch in batched_texts:\n            batch_translation_output = []  # variable to store translation output for single batch\n            doc_ids = batch['idx']\n            total_size = batch['total_size']\n            texts_in_batch = [texts[doc_id] for doc_id in doc_ids]\n            # -- case when batch exceeds character limits\n            if total_size > CHARACTER_LIMITS:\n                assert len(doc_ids) == 1, 'Critical error: batching function is generating batches that exceed max limit with more than 1 text. Revisit the function and fix this.'\n\n                doc_id = doc_ids[0]\n                doc_size = sizes_dict[doc_id] // n_target_langs\n                batch_size = CHARACTER_LIMITS // doc_size\n\n                # -- case when a single doc is too large to be translated\n                if not batch_size:\n                    process_error = self._process_error(f'Text idx={doc_id} too large to be translated', 'Max character limit for request', 400, doc_ids, target_languages)\n                    if process_error:\n                        return process_error\n\n                # -- case when single doc too big to be translated to all language, but small enough that it can be translated to some languages\n                else:\n                    _translation_output = dict()  # variable to store translations for single text but different language batches\n                    _translation_failed = False  # variable to track if translation is partial\n\n                    batch_range = range(0, n_target_langs, batch_size)\n                    n_batches = len(batch_range)\n\n                    # batch by target languages\n                    for batch_id, start_lang_idx in enumerate(batch_range):\n                        end_lang_idx = start_lang_idx + batch_size\n                        target_languages_ = target_languages[start_lang_idx: end_lang_idx]\n                        total_size_ = doc_size * len(target_languages_)\n\n                        response_output, status_code = self._post_request(\n                            f'Translating batch {batch_id+1}/{n_batches} of text with idx={doc_id}. Target languages: {target_languages_}',\n                            base_url, target_languages_, texts_in_batch, total_size_\n                        )\n                        if not is_request_valid(status_code):\n                            process_error = self._process_error(\n                                f'Partial translation of text idx={doc_id} to languages {target_languages_} failed. Reason: {response_output}',\n                                response_output, status_code, doc_ids, target_languages_)\n                            if process_error:\n                                return process_error\n\n                            # failure indicates translation is partial. Break loop if we don't case about partials in output\n                            _translation_failed = True\n                            if not self.include_partials_in_output:\n                                break\n                        else:\n                            self._update_partial_translation(response_output, _translation_output, source_language)\n\n                    if not _translation_failed or self.include_partials_in_output:\n                        if _translation_output:\n                            batch_translation_output.append(_translation_output)\n\n            # -- case when batch does not exceed character limits\n            else:\n                response_output, status_code = self._post_request(\n                    f'Translating {len(texts)} texts to {len(target_languages)} languages',\n                    base_url, target_languages, texts_in_batch, total_size\n                )\n\n                if not is_request_valid(status_code):\n                    process_error = self._process_error(\n                        f'Translation failed for texts {doc_ids}. Reason: {response_output}',\n                        response_output, status_code, doc_ids, target_languages\n                    )\n                    if process_error:\n                        return process_error\n                else:\n                    batch_translation_output += response_output\n\n            translation_outputs += batch_translation_output\n\n        if self.no_failures:\n            status_code = 200\n        # case when all translations failed, so return translation errors instead\n        elif not translation_outputs:\n            translation_outputs = self.translation_errors\n            status_code = 400\n        # case when translations partially failed, modify status code\n        else:\n            status_code = 206\n\n        return translation_outputs, status_code\n\n    @property\n    def _set_request_default(self):\n        \"\"\"Function for resetting translation errors and no_failures flag\n        \"\"\"\n\n        self.translation_errors = {}\n        self.no_failures = True\n\n    @property\n    def _set_no_failures_to_false(self):\n        \"\"\"Function to explicitly set no failures to False\n        \"\"\"\n        self.no_failures = False\n\n    @property\n    def _set_success_request_time(self):\n        \"\"\"Function to set the time a request is made\n        \"\"\"\n        self.time_of_last_success_request = time.time()\n\n    def _update_partial_translation(self, response_output, partial_translation_output, source_language):\n\n        # concatenate outputs in correct format\n        if 'translations' not in partial_translation_output:\n            partial_translation_output['translations'] = response_output['translations']\n        else:\n            partial_translation_output['translations'] += response_output['translations']\n\n        if not source_language:\n            if 'detectedLanguage' not in partial_translation_output:\n                partial_translation_output['detectedLanguage'] = response_output['detectedLanguage']\n            else:\n                partial_translation_output['detectedLanguage'] += response_output['detectedLanguage']\n\n        return partial_translation_output\n\n    def _update_translation_errors(self, response_text: str, status_code: int, doc_ids: list, target_languages: list):\n        \"\"\"Add failed translation to errors dictionary\n\n        :param response_text: response text from failed request (status_code not beginning with 2)\n        :param status_code: status code from failed request\n        :param doc_ids: documents that were to be translated\n        :param target_languages: target languages used in request\n        \"\"\"\n        doc_ids = tuple(doc_ids)\n        if doc_ids not in self.translation_errors:\n            self.translation_errors[doc_ids] = dict(\n                reason=response_text,\n                status_code=status_code,\n                target_languages=target_languages\n            )\n        else:\n            self.translation_errors[doc_ids]['target_languages'] += target_languages\n            self.translation_errors[doc_ids]['status_code'] = status_code\n            self.translation_errors[doc_ids]['reason'] = response_text\n\n    def _process_error(self, msg: str, response_text: str, status_code: int, doc_ids: list, target_languages: list):\n        \"\"\"Processes failed request based on `ignore_on_translation_failure` strategy\n\n        :param msg: message to return or log depending on `ignore_on_translation_failure` strategy\n        :param response_text: response text from failed request (status_code not beginning with 2)\n        :param status_code: status code from failed request\n        :param doc_ids: documents that were to be translated\n        :param target_languages: target languages used in request\n        \"\"\"\n\n        self._set_no_failures_to_false\n\n        self._update_translation_errors(response_text, status_code, doc_ids, target_languages)\n\n        if not self.ignore_on_translation_failure:\n            return response_text, status_code\n\n        LOGGER.error(msg)\n\n    def _profile_texts(self, sizes: Dict[Union[int, str], int]):\n        \"\"\"Profiles texts to see if the request can be translated\n\n        :param sizes: size mapping for each document, {doc_id_1: size, ...}\n        \"\"\"\n        num_texts = len(sizes)\n        total_request_size = sum(sizes.values())\n        if total_request_size > MAX_CHARACTER_LIMITS_PER_HOUR:\n            return 'Your texts exceed max character limits per hour', 400\n\n        LOGGER.info(f'Detected `{num_texts}` texts with total request size `{total_request_size}`')\n\n    def _sleep(self, request_size: int):\n        \"\"\"Function to sleep prior to requests being made based on the size of the request and the time last successful request was made in order to avoid overloading Microsoft servers\n\n        :param request_size: size of the request being made\n        \"\"\"\n        if hasattr(self, 'time_of_last_success_request'):\n            time_diff_since_first_request = time.time() - self.time_of_last_success_request\n            time_diff_needed_for_next_request = request_size / (MAX_CHARACTER_LIMITS_PER_HOUR / 3600) \n            sleep_time = time_diff_needed_for_next_request - time_diff_since_first_request\n            if sleep_time > 0:\n                LOGGER.debug(f'Sleeping {sleep_time:.3g} seconds...')\n                time.sleep(sleep_time)\n\n    def _post_request(self, msg: str, base_url: str, target_languages: list, texts: List[str], request_size: Optional[int] = None) -> Tuple[Union[dict, str], int]:\n        \"\"\"Internal function to post requests to microsoft API\n\n        :param msg: message to log\n        :param base_url: base url for making the request\n        :param target_languages: list of target languages to translate text to\n        :param texts: texts to translate\n        :param request_size: size of the request being made for calculating sleep period, defaults to None\n\n        :return: for successful response, (status_code, [{\"translations\": [{\"text\": translated_text_1, \"to\": lang_1}, ...]}, ...]))\n        \"\"\"\n        if not request_size:\n            n_target_langs = len(target_languages)\n            request_size = sum([len(text)*n_target_langs for text in texts])\n\n        self._sleep(request_size)\n\n        url = add_array_api_parameters(base_url, param_name='to', param_values=target_languages)\n        LOGGER.info(msg)\n        body = [{'text': text} for text in texts]\n        resp = requests.post(url, headers=HEADERS, json=body)\n        status_code = resp.status_code\n        if is_request_valid(status_code):\n            self._set_success_request_time\n            return resp.json(), status_code\n        return resp.text, status_code\n```"]