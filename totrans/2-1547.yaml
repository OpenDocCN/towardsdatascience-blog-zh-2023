- en: Multiple GPU training in PyTorch and Gradient Accumulation as an alternative
    to it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91](https://towardsdatascience.com/multiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Code and Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alexml0123?source=post_page-----e578b3fc5b91--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----e578b3fc5b91--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e578b3fc5b91--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e578b3fc5b91--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----e578b3fc5b91--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e578b3fc5b91--------------------------------)
    ·7 min read·Jul 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed6fca5026469bec09b620c9620bc331.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://unsplash.com/photos/vBzJ0UFOA70](https://unsplash.com/photos/vBzJ0UFOA70)'
  prefs: []
  type: TYPE_NORMAL
- en: In this article we are going to first see the differences between Data Parallelism
    (DP) and Distributed Data Parallelism (DDP) algorithms, then we will explain what
    Gradient Accumulation (GA) is to finally show how DDP and GA are implemented in
    PyTorch and how they lead to the same result.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training a deep neural network (DNN), one important hyperparameter is the
    batch size. Normally, the batch size should not be too big because the network
    would tend to overfit, but also not too small because it will result in slow convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with images of high resolution or other types of data that occupy
    a lot of memory, assuming that today most training of big DNN models are done
    on GPUs, fitting small batch size can be problematic depending on the memory of
    the available GPU. Because, as we said, small batch sizes result in slow convergence,
    there are three main methods we can use to increase the effective batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple small GPUs running the model in parallel on mini-batches — DP
    or DDP algorithms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a larger GPU (expensive)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accumulate the gradient over multiple steps
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s now look at 1\. and 3\. in more details — if you are lucky to have a large
    GPU that you can fit all the data you need on it, you can read the DDP part and
    see how it’s implemented in PyTorch in the Full Code section skipping the rest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we want an effective batch size of 30 but we can only fit 10 data points
    (mini-batch size) on each GPU. We have two choices: Data Parallelism or Distributed
    Data Parallelism:'
  prefs: []
  type: TYPE_NORMAL
- en: Data Parallelism (DP)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we define the master GPU. Then, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Move 10 data points (mini-batch) and the replica of the model to other 2 GPUs
    from master GPU
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do a forward pass on each GPU and pass to master GPU the outputs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the total loss on Master GPU and then send back the loss to each GPU
    to compute the gradients for the parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Send the gradients back (these are the average of the gradients for all training
    examples) to Master GPU, sum them up to get the average gradient for the entire
    batch of 30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the parameters on the Master GPU and send these updates to the other
    2 GPUs for the next iteration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are some problems & inefficiencies with this process:'
  prefs: []
  type: TYPE_NORMAL
- en: Data are passed from the Master GPU before being split between other GPUs. Also,
    Master GPU is utilized more than other GPUs as computation of total loss and parameters
    updates happen on Master GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to synchronize the models on other GPUs at each iteration which can
    slow down the training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed Data Parallel (DDP)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Distributed Data Parallel was introduced to improve on inefficiencies of Data
    Parallel algorithm. We still have the same settings as before — 30 data points
    for each batch with 3 GPUs. The differences are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It does not have the Master GPU
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because we don’t have the Master GPU anymore, we load the data on each GPU in
    a *non-overlapping* way in parallel directly from the disk/RAM — *DistributedSampler*
    does this job for us. Under the hood it uses the local rank (GPU id) to distribute
    the data across GPUs — given 30 data points, first GPU will use points [0, 3,
    6, … , 27], 2nd GPU [1, 4, 7, .., 28] and 3rd GPU [2, 5, 8, .. , 29]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Forward pass, loss computation and backward passes are executed on each
    GPU independently and the gradients are asynchronously reduced calculating the
    mean and then the update follows across all GPUs
  prefs: []
  type: TYPE_NORMAL
- en: Because of the advantages of DDP over DP, DDP usage is preferred nowadays, thus
    we will only show the DDP implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Accumulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we have only one GPU but still want to use a larger batch size, an alternative
    option is to accumulate the gradients for a certain number of steps, effectively
    accumulating the gradients for certain number of mini-batches increasing the effective
    batch size. From the above example, we could accumulate the gradients of 10 data
    points for 3 iterations to achieve the same results as what we described in DDP
    training with an effective batch size of 30.
  prefs: []
  type: TYPE_NORMAL
- en: '**DDP process** Code'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Below I will only go through the differences when implementing DDP compared
    to 1 GPU code. The full code can be found some sections below. First we initialize
    the process group that allows different processes to communicate between them.
    With *int(os.environ[“LOCAL_RANK”])* we retrieve the GPU used in a given process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then, we need to wrap the model in *DistributedDataParallel* that enables multi-gpu
    training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The final part is to define *DistributedSampler* that I mentioned in DDP section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the training stays the same — I will include the full code at the
    end of this article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient Accumulation Code**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When backpropagation happens, after we call *loss.backward(),* gradients are
    stored in their respective Tensors. The actual update happens when *optimizer.step()*
    is called and then the gradients stored in the Tensors are set to zero with *optimizer.zero_grad()*
    to run the next iteration of backpropagation and parameters update. Thus, to accumulate
    the gradient we call *loss.backward()* for the number of gradient accumulations
    we need without setting gradients to zero so that they accumulate across multiple
    iterations, and then **we average them to get the average gradient across accumulated
    gradient iterations** (*loss = loss/ACC_STEPS*). After that we call *optimizer.step()*
    and zero the gradients to start the next accumulation of gradients. In code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Full code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we run these two scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*python3 ddp.py — epochs 2 — batch_size 4 — data_size 8 — verbose — acc_steps
    2*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*torchrun — standalone — nproc_per_node=2 ddp.py — epochs 2 — distributed —
    batch_size 4 — data_size 8 — verbose*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'we will see that we obtain the exact same final model parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article we have briefly introduced and gave an intuition behind DP,
    DDP algorithms and Gradient Accumulation and have shown how to increase the size
    effective batch size even without multiple GPUs. One important thing to notice
    is that even if we obtain the same final results, training with multiple GPUs
    is much faster than using gradient accumulation, thus if the training speed is
    important then multiple GPUs is the only way to speed up the training.
  prefs: []
  type: TYPE_NORMAL
