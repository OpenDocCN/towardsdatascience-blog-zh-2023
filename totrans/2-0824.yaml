- en: Essential Evaluation Metrics for Classification Problems in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/essential-evaluation-metrics-for-classification-problems-in-machine-learning-69e90665375b](https://towardsdatascience.com/essential-evaluation-metrics-for-classification-problems-in-machine-learning-69e90665375b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Comprehensive Guide to Understanding and Evaluating the Performance of Classification
    Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://aaron-zhu.medium.com/?source=post_page-----69e90665375b--------------------------------)[![Aaron
    Zhu](../Images/42e9690c4b4aad63f396b171a74e29f7.png)](https://aaron-zhu.medium.com/?source=post_page-----69e90665375b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----69e90665375b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----69e90665375b--------------------------------)
    [Aaron Zhu](https://aaron-zhu.medium.com/?source=post_page-----69e90665375b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----69e90665375b--------------------------------)
    ·10 min read·Mar 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1067a61f9cd203144c29c260ca6388d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Are you confused about the terms used in evaluating the performance of machine
    learning models? Do you get lost in the sea of confusion when you come across
    the terms confusion matrix, precision, recall, specificity and sensitivity? Well,
    worry no more, because in this blog post we will dive deep into these evaluation
    metrics and help you make sense of these terms.
  prefs: []
  type: TYPE_NORMAL
- en: What is a Confusion Matrix?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with the confusion matrix. It is a table that is used to evaluate
    the performance of a classification model. It contains four values: true positives
    (**TP**), true negatives (**TN**), false positives (**FP**) and false negatives
    (**FN**). A true positive is when the model correctly predicts the positive class,
    a true negative is when the model correctly predicts the negative class, a false
    positive is when the model predicts the positive class but it is actually negative,
    and a false negative is when the model predicts the negative class but it is actually
    positive.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e858e19e7cdb87ed020d37c10bf86638.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: What is Accuracy?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Accuracy** is a measure of how well a model is performing overall. It is
    the proportion of correct predictions made by the model out of all the predictions
    made. In other words, it is the number of true positives and true negatives divided
    by the total number of predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy = (TP+TN)/(TP+TN+FP+FN)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is Precision?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Precision** is a measure of how accurate the positive predictions of a model
    are. It is calculated as the ratio of true positives to the sum of true positives
    and false positives. A high precision indicates that the model correctly predicts
    the positive class most of the time.'
  prefs: []
  type: TYPE_NORMAL
- en: Precision = TP / (TP + FP)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is Recall?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Recall** is a measure of how well the model is able to identify the positive
    class. It is calculated as the ratio of true positives to the sum of true positives
    and false negatives. A high recall indicates that the model is able to identify
    most of the positive instances.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall = TP / (TP + FN)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is Specificity?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Specificity** is a measure of how well the model is able to identify the
    negative class. It is calculated as the ratio of true negatives to the sum of
    true negatives and false positives. A high specificity indicates that the model
    is able to identify most of the negative instances.'
  prefs: []
  type: TYPE_NORMAL
- en: Specificity = TN / (TN + FP)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You may think of specificity as the recall with a different definition of the
    positive and negative labels, i.e., the positive label is considered negative,
    and the negative label is considered positive.
  prefs: []
  type: TYPE_NORMAL
- en: What is Sensitivity?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Sensitivity** is another term used for recall, especially in medical contexts
    where it refers to the ability of a medical test to detect a disease or condition
    in people who actually have the disease or condition.'
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity = TP / (TP + FN)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Summary Table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa901f75f5c2167956718a5bf5ab86e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: When is Precision-Recall a better measure than Accuracy?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Precision-Recall is used instead of accuracy when the data is **imbalanced**,
    meaning there are significantly more samples of one class than the other.
  prefs: []
  type: TYPE_NORMAL
- en: In such cases, accuracy can be misleading, as a model can achieve high accuracy
    by simply predicting the majority class. For example, in a binary classification
    problem with 90% of the samples belonging to the negative class, a model that
    always predicts negative will have an accuracy of 90%, even if it is not making
    any correct positive predictions.
  prefs: []
  type: TYPE_NORMAL
- en: When the data is imbalanced, we could have a model with very high accuracy.
    But the model is useless because of low precision-recall values.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Precision-Recall is a better measure of a model’s performance in such cases
    because it takes into account the proportion of true positives and true negatives,
    as well as false positives and false negatives, which are more critical in imbalanced
    datasets. Precision measures the proportion of true positive predictions made
    out of all positive predictions made, while recall measures the proportion of
    true positive predictions made out of all actual positive samples.
  prefs: []
  type: TYPE_NORMAL
- en: Precision-Recall is more suitable for evaluating the performance of a model
    in imbalanced datasets, while accuracy is more appropriate when the classes are
    balanced.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is Precision-Recall Trade-off?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ideally, we would like to have both high precision and high recall for our model.
    Achieving both simultaneously often is not possible.
  prefs: []
  type: TYPE_NORMAL
- en: The Precision-Recall trade-off arises because optimizing one metric often comes
    at the expense of the other.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Here is why
  prefs: []
  type: TYPE_NORMAL
- en: If a model is more conservative in its predictions, it may achieve a **higher
    precision** by reducing the number of false positives, but this may also result
    in a lower recall, since it may miss some true positive instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversely, suppose a model is more aggressive in its predictions. In that case,
    it may achieve a **higher recall** by capturing more true positive instances,
    but this may also result in a lower precision, since it may make more false positive
    predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, to determine whether we should prioritize the precision or recall
    value, we need to evaluate the cost of false positives and false negatives.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In case 1, a medical test is designed to detect a disease in people.
  prefs: []
  type: TYPE_NORMAL
- en: The cost of false negative cases might be — patients who are sick don’t receive
    the right treatment, which might cause more people to be infected if the disease
    is contagious.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, the cost of false positive cases might be — wasting resources
    treating healthy people and unnecessarily quarantining themselves.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, the cost of false negative cases is much higher than the cost of
    false positive cases. In this case, paying more attention to the recall value
    makes more sense.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97c0aae8ec053fb3877a5a79f3186269.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In case 2, a bank designs an ML model to detect credit card fraud.
  prefs: []
  type: TYPE_NORMAL
- en: The cost of false negative cases might be — the bank loses money on the fraudulent
    transactions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost of false positive cases might be — the false fraud alert hurts the
    customers’ experience, which causes a decrease in customer retention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, the cost of false positive cases is much higher than the cost of
    false negative cases. Based on a study about credit card fraud, false-positive
    credit card fraud costs 13 times more in lost income than true fraud. In this
    case, paying more attention to the precision value makes more sense.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48f9e0833e14b9e5418e7fb251a72254.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: What is F1-Score?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the above examples, we try to prioritize either recall or precision at the
    expense of the other measure. But there are also many situations, where both **recall
    and precision are** **equally important**. In such cases, we should use another
    measure, called F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: The **F1 score** takes into account both precision and recall, and provides
    a single score that summarizes the model’s overall performance. It ranges from
    0 to 1, with a score of 1 indicating perfect precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for calculating the F1 score is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0890623c7974fe4e87d056c37fec34ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It can also be calculated as the harmonic mean of precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b7b4a6d57e8f811b81d6374b0477584.png)'
  prefs: []
  type: TYPE_IMG
- en: What are Receiver Operating Characteristic Curve (ROC Curve) and **Area Under
    the Curve** (**AUC**)?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Receiver Operating Characteristic (ROC) Curve is a graphical representation
    of the performance of a binary classification model that predicts the probability
    of an event occurring. The ROC curve is created by plotting the true positive
    rate (TPR) against the false positive rate (FPR) at **various threshold** settings.
  prefs: []
  type: TYPE_NORMAL
- en: TPR=TP/(TP+FN)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: FPR=FP/(FP+TN)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s consider an example of a binary classification problem where we want to
    predict whether an email is spam or not spam. Let’s say we have a model that predicts
    the **probability** that an email is spam, and we want to use an ROC curve to
    evaluate its performance.
  prefs: []
  type: TYPE_NORMAL
- en: To create the ROC curve, we need to set a **threshold** value (i.e., from 0
    to 1) for the predicted probability above which we classify an email as spam,
    and below which we classify it as not spam. The threshold value is a decision
    boundary that determines the trade-off between the true positive rate (TPR) and
    the false positive rate (FPR).
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we set the threshold to 0.5, then any email with a predicted
    probability greater than 0.5 would be classified as spam, and any email with a
    predicted probability less than or equal to 0.5 would be classified as not spam.
    This threshold value would give us a certain TPR and FPR.
  prefs: []
  type: TYPE_NORMAL
- en: In general, as the threshold increases, TPR and FPR decrease. In the most extreme
    case, when the threshold value is 0, all predicted values are positive, therefore,
    TPR=FPR=1\. Conversely, when the threshold value is 1, all predicted values are
    negative, therefore, TPR=FPR=0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that, for a given dataset, we calculate the TPR and FPR for **various
    threshold values**, and we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37721065980b5ead2dbf5a1ac5516358.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot these values on an ROC curve, with TPR on the y-axis and FPR on
    the x-axis, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a88c859d3bd567bacab8bc6e49c0e459.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the plot, the ROC curve is a trade-off between the TPR and
    FPR for different threshold values.
  prefs: []
  type: TYPE_NORMAL
- en: The **area under the curve** (**AUC**) measures the overall performance of the
    model, with an AUC of 1 indicating perfect performance, and an AUC of 0.5 indicating
    random guessing (i.e., the diagonal line which represents a classifier making
    random predictions).
  prefs: []
  type: TYPE_NORMAL
- en: 'Key takeaways of ROC:'
  prefs: []
  type: TYPE_NORMAL
- en: ROC works better in evaluating different models when the classes are **balanced**
    and the cost of false positives and false negatives are similar. The higher the
    AUC-ROC is, the better the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the ROC, we can pick the **optimal threshold value**, which depends on
    how the classifier is intended to be applied — if the cost of false positives
    and false negatives are similar, the threshold that is close to the upper left
    corner of the ROC is the optimal value. If the cost of false positives and false
    negatives is higher, we can pick a higher threshold value. Conversely, if the
    cost of false negatives and false positives is higher, we can pick a lower threshold
    value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROC is **threshold-invariant**. It measures the performance of a model across
    a range of thresholds. It means we don’t need to determine a threshold using ROC
    in advance, unlike precision, recall, accuracy, and F1 score which are based on
    a specific threshold.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Precision-Recall Curve (PRC)?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In contrast to the ROC curve that plots TPR and FPR, the Precision-Recall Curve
    (PRC) curve plots precision on the y-axis and recall on the x-axis. The PRC curve
    shows how well the model can identify positive cases while avoiding false positives.
  prefs: []
  type: TYPE_NORMAL
- en: The area under the PRC curve can measure the performance of a model. The higher
    the AUC-PRC, the better the model. A model with an AUC-PRC of 0.5 is no better
    than random guessing, while a model with an AUC-PRC of 1.0 is perfect.
  prefs: []
  type: TYPE_NORMAL
- en: In general, as the threshold increases, the precision would increase and the
    recall would decrease. In the most extreme case, when the threshold value is 0,
    all predicted values are positive, therefore, recall = 1 and precision = 0\. Conversely,
    when the threshold value is 1, all predicted values are negative, therefore, recall
    = 0 and precision = 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that, for a given dataset, we calculate the precision and recall for
    **various threshold values**, and we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e687e008df1fd8694d5fd10614dc1a7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63201c9b57f1ad33a72e5a8d8594152b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Key takeaways of PRC:'
  prefs: []
  type: TYPE_NORMAL
- en: The choice between ROC and PRC curves depends on the problem at hand. The ROC
    curve is useful when the classes are balanced and the cost of false positives
    and false negatives are similar. The PRC curve is useful when the classes are
    imbalanced or the cost of false positives and false negatives are different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By looking at the PRC, you can choose an **optimal threshold** that balances
    precision and recall according to your specific use case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve covered many evaluation metrics for classification problems. These metrics
    are interrelated, and each has its strengths and weaknesses in measuring the model’s
    accuracy. Overall, understanding these metrics is crucial in developing effective
    machine learning models and making informed decisions based on their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you would like to explore more posts related to **Statistics**, please check
    out my articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Type I & II Errors and Sample Size Calculation in Hypothesis Testing**](/type-i-ii-errors-and-sample-size-calculation-in-hypothesis-testing-760dae42a065)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**7 Most Asked Questions on Central Limit Theorem**](/7-most-asked-questions-on-central-limit-theorem-82e95eb7d964)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Standard Deviation vs Standard Error: What’s the Difference?**](/standard-deviation-vs-standard-error-whats-the-difference-ae969f48adef)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**3 Most Common Misinterpretations: Hypothesis Testing, Confidence Interval,
    P-Value**](/the-most-common-misinterpretations-hypothesis-testing-confidence-interval-p-value-4548a10a5b72)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Are the Error Terms Normally Distributed in a Linear Regression Model?**](/are-the-error-terms-normally-distributed-in-a-linear-regression-model-15e6882298a4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Are the OLS Estimators Normally Distributed in a Linear Regression Model?**](/are-ols-estimators-normally-distributed-in-a-linear-regression-model-89b688fa8dc3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**What is Regularization: Bias-Variance Tradeoff**](/machine-learning-bias-variance-tradeoff-and-regularization-94846f945131)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Variance vs Covariance vs Correlation: What is the Difference?**](https://medium.com/geekculture/variance-vs-covariance-vs-correlation-what-is-the-difference-95adff96d542)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Confidence Interval vs Prediction Interval: What is the Difference?**](/confidence-interval-vs-prediction-interval-what-is-the-difference-64c45146d47d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Which is Worse, Type I or Type II errors?**](https://medium.com/geekculture/which-is-worse-type-i-or-type-ii-errors-f40a0f040fcc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you enjoy this article, please click the **Clap** icon. If you would like
    to see more articles from me and thousands of other writers on Medium. You can:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Subscribe**](https://aaron-zhu.medium.com/subscribe) to my newsletter to
    get an email notification whenever I post a new article.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sign up for a [**membership**](https://aaron-zhu.medium.com/membership) to unlock
    full access to everything on Medium.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
