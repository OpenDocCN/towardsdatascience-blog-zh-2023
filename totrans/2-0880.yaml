- en: Feature Encoding Techniques in Machine Learning with Python Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/feature-encoding-techniques-in-machine-learning-with-python-implementation-dbf933e64aa](https://towardsdatascience.com/feature-encoding-techniques-in-machine-learning-with-python-implementation-dbf933e64aa)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 6 feature encoding techniques to consider for your data science workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://kayjanwong.medium.com/?source=post_page-----dbf933e64aa--------------------------------)[![Kay
    Jan Wong](../Images/28e803eca6327d97b6aa97ee4095d7bd.png)](https://kayjanwong.medium.com/?source=post_page-----dbf933e64aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dbf933e64aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dbf933e64aa--------------------------------)
    [Kay Jan Wong](https://kayjanwong.medium.com/?source=post_page-----dbf933e64aa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dbf933e64aa--------------------------------)
    ·10 min read·Jan 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0cc329e9db60722e304729377be4544.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Susan Holt Simpson](https://unsplash.com/@shs521?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '***Feature Encoding*** converts categorical variables to numerical variables
    as part of the feature engineering step to make the data compatible with Machine
    Learning models. There are various ways to perform feature encoding, depending
    on the type of categorical variable and other considerations.'
  prefs: []
  type: TYPE_NORMAL
- en: This article introduces tips to perform feature encoding in general, elaborating
    on 6 feature encoding techniques that you can consider in your Data Science workflows,
    with comments on when to use them, and finally how to implement them in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b373a0a726614c75547f877115df740f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 1: Summary of Feature Encoding Techniques — Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: The cheat sheet summary of the 6 feature encoding techniques is summarized in
    Fig 1; read on for a detailed explanation and implementation of each method.
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Label / Ordinal Encoding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One-Hot / Dummy Encoding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Target Encoding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Count / Frequency Encoding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Binary / BaseN Encoding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hash Encoding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature Encoding Tips
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tip 1: Prevent Data Leakage'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given that the purpose of feature encoding is to convert categorical variables
    to numerical variables, we can encode categories such as “cat”, “dog”, and “horse”
    to numbers such as 0, 1, 2, etc. However, we must keep in mind the problem of
    **data leakage** which happens when information in test data is leaked into the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder must be fitted on only the training data, such that the encoder
    only learns the categories that exist in the training set, and then be used to
    transform the validation/test data. **Do not fit the encoder on the whole dataset!**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The question naturally follows “*What if I have missing or new categories in
    the validation/test data?*”, we can handle this in two ways — by removing these
    unseen categories since the model is not trained on these unseen categories anyway.
    Alternatively, we can encode them as `-1` or other arbitrary values to indicate
    that these are unseen categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip 2: Save your Encoders'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the previous tip, encoders are fitted on training data (with
    method `.fit`) and used to transform validation/test data (with method `.transform`).
    It is best to save the encoder to transform the validation/test data later on.
  prefs: []
  type: TYPE_NORMAL
- en: Other benefits of saving encoders are the ability to retrieve the categories
    or to transform the encoded values back to their categories (with method `.inverse_transform`)
    if applicable and required.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's dive into the feature encoding techniques!
  prefs: []
  type: TYPE_NORMAL
- en: №1\. Label / Ordinal Encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Label Encoder and Ordinal Encoder encode categories into numerical values directly
    (refer to Fig 2).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Label Encoder is used for ***nominal*** categorical variables (categories without
    order i.e., red, green, blue) while Ordinal Encoder is used for ***ordinal***
    categorical variables (categories with order i.e., small, medium, large).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e8d46ea0373347ddaf5bfe7bfcef4ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 2: Example of Label Encoding — Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: Given that the cardinality (number of categories) is `n`, Label and Ordinal
    Encoder encodes the values from `0` to `n-1`.
  prefs: []
  type: TYPE_NORMAL
- en: When to use Label / Ordinal Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Nominal/Ordinal Variables**: Label Encoder is used for nominal categorical
    variables, while Ordinal Encoder is used for ordinal categorical variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supports High Cardinality**: Label and Ordinal Encoder can be used in cases
    where there are many different categories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unseen Variables**: Ordinal Encoder can encode unseen variables in the validation/test
    set with an arbitrary value (refer to the code example below), by default it throws
    a value error'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons of Label / Ordinal Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Unseen Variables**: Label Encoder does not encode unseen variables in the
    validation/test set and will throw a value error, special error handling must
    be done to avoid this error'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categories interpreted as numerical values**: Machine Learning models would
    read the encoded columns as numerical variables instead of interpreting them as
    distinct categories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Label Encoder, it can only encode one column at a time and multiple label
    encoders must be initialized for each categorical column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For Ordinal Encoder, it can encode multiple columns at once, and the order of
    the categories can be specified.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: №2\. One Hot / Dummy Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In One-Hot Encoding and Dummy Encoding, the categorical column is split into
    multiple columns consisting of ones and zeros (refer to Fig 3).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This addresses the drawback to Label and Ordinal Encoding where columns are
    now read in as categorical columns due to encoded data being represented as multiple
    boolean columns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0204fe69d55eca38233d97ce66a8d435.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3: Example of One-Hot Encoding — Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: Given that the cardinality (number of categories) is `n`, One-Hot Encoder encodes
    the data by creating `n` additional columns. In Dummy Encoding, we can drop the
    last column as it would be a dummy variable, and this will result in `n-1` columns.
  prefs: []
  type: TYPE_NORMAL
- en: When to use One-Hot / Dummy Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Nominal Variables**: One-Hot Encoder is used for nominal categorical variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low to Medium Cardinality**: As new columns are created for each category,
    it is recommended to use One-Hot encoding where there is a low to a medium number
    of categories such that the resulting data would not be too sparse'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing or Unseen Variables**: One-Hot Encoder from the sklearn package can
    handle missing or unseen variables by creating columns for missing variables and
    omitting columns for unseen variables so that the feature columns remain consistent,
    by default it throws a value error'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons of One-Hot / Dummy Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Dummy Variable Trap**: It may result in a phenomenon where the features are
    highly correlated since the encoded data is sparse'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Large dataset**: One-hot encoding increases the number of columns in the
    dataset, which may in turn affect the training speed and is not optimal for tree-based
    models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot encoding can be done with `OneHotEncoder` from the sklearn package or
    using the pandas `get_dummies` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Using the pandas built-in `get_dummies` method, missing and unseen variables
    in validation/test data must be manually handled.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: №3\. Target Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Target Encoding uses Bayesian posterior probability to encode categorical variables
    to the mean of the target variable (numerical variable). Smoothing techniques
    are applied to prevent target leakage.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Compared to Label and Ordinal Encoding, Target Encoding encodes the data with
    values that explains the target instead of arbitrary numbers 0, 1, 2, etc. Other
    similar encodings can be to encode categorical variables with Information Value
    (IV) or Weight of Evidence (WOE).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/554464b6423e3f286920f32e04b4ea9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 4: Example of Target Encoding — Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to implement target encoding
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean Encoding: The encoded values are the mean of the target values with smoothing
    applied'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Leave-One-Out Encoding: The encoded values are the mean of the target values
    except for the data point that we want to predict'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to use Target Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Nominal Variables**: Target Encoder is used for nominal categorical variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supports High Cardinality**: Target Encoder can be used in cases where there
    are many different categories, and it is better if there are multiple data samples
    for each category'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unseen Variables**: Target Encoder can handle unseen variables by encoding
    them with the mean of the target variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons of Target Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Target Leakage**: Even with smoothing, this may result in target leakage
    and overfitting. Leave-One-Out Encoding and introducing Gaussian noise in the
    target variable can be used to address the overfitting problem'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uneven Category Distribution**: The category distribution can differ in train
    and validation/test data and result in categories being encoded with incorrect
    or extreme values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Target Encoding requires installing the `category_encoders` python package using
    the command `pip install category_encoders`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: №4\. Count / Frequency Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Count and Frequency Encoding encodes categorical variables to the count of occurrences
    and frequency (normalized count) of occurrences respectively.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/5fac27741fe852ba6bb4d84099316ccf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 5: Example of Count and Frequency Encoding — Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: When to use Count / Frequency Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Nominal Variables**: Frequency and Count Encoder is effective for nominal
    categorical variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unseen Variables**: Frequency and Count Encoder can handle unseen variables
    by encoding them with a `0` value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons of Count / Frequency Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Similar encodings**: If all categories have similar counts, the encoded values
    will be the same'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: №5\. Binary / BaseN Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Binary Encoding encodes categorical variables into integers, then converts them
    to binary code. The output is similar to One-Hot Encoding, but lesser columns
    are created.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This addresses the drawback to One-Hot Encoding where a cardinality of `n` does
    not result in `n` number of columns, but `log2(n)` columns. BaseN Encoding follows
    the same idea but uses other base values instead of 2, resulting in `logN(n)`
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e81081ecfdb661632192a68fea03c21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 6: Example of Binary Encoding — Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: When to use Binary Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Nominal Variables**: Binary and BaseN Encoder are used for nominal categorical
    variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Cardinality**: Binary and BaseN encoding works well with a high number
    of categories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing or Unseen Variables**: Binary and BaseN Encoder can handle unseen
    variables by encoding them with `0` values across all columns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: №6\. Hash Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hash Encoding encodes categorical variables into distinct hash values using
    a hash function. The output is similar to One-Hot Encoding, but you can choose
    the number of columns created.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hash Encoding is similar to Binary Encoding such that they are more space-efficient
    than One-Hot Encoding, but Hash Encoding makes use of a hash function instead
    of binary numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/465b520aae5b9b37b3af018d62048f45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 7: Example of Hash Encoding using 2 columns — Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: Hash encoding can encode high-cardinality data to a fixed-sized array as the
    number of new columns is manually specified. This is also similar to dimensionality
    reduction algorithms such as [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)
    or [Spectral Embedding](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html)
    methods which construct fixed-sized arrays from eigenvalues and other distance
    measures.
  prefs: []
  type: TYPE_NORMAL
- en: When to use Hash Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Nominal Variables**: Hash Encoder is used for nominal categorical variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Cardinality**: Hash encoding works well with a high number of categories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing or Unseen Variables**: Hash Encoder can handle unseen variables by
    encoding them with null values across all columns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons of Hash Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Irreversible**: Hashing functions are one-direction such that the original
    input can be hashed into a hash value, but the original input cannot be retrieved
    from the hash value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information Loss or Collision**: If too few columns are created, hash encoding
    can lead to loss of information as multiple different inputs may result in the
    same output from the hash function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hash encoding can be done with `FeatureHasher` from the sklearn package or with
    `HashingEncoder` from the category encoders package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Using `category_encoders`,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Hope you have learned more about the different ways to encode your categorical
    data into numerical data. To choose between which feature encoding technique to
    use, it is important to consider the **type of categorical data** (nominal or
    ordinal), the **Machine Learning model used**, and the **pros and cons** of each
    method.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to consider missing or unseen variables during testing
    due to changing patterns or trends so that the data science workflow does not
    fail in production!
  prefs: []
  type: TYPE_NORMAL
- en: Useful Links
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`sklearn` Documentation: [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`category_encoders` Documentation: [https://contrib.scikit-learn.org/category_encoders/](https://contrib.scikit-learn.org/category_encoders/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
