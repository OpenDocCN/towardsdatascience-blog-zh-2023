- en: 'Efficient Deep Learning: Unleashing the Power of Model Compression'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效深度学习：释放模型压缩的力量
- en: 原文：[https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06](https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06](https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06)
- en: '![](../Images/e3f104a88962263b5a88baa0f644362f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3f104a88962263b5a88baa0f644362f.png)'
- en: Image By Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Accelerate model inference speed in production
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速生产中的模型推理速度
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    ·9 min read·Sep 3, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    ·9分钟阅读·2023年9月3日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: When a Machine Learning model is deployed into production there are often requirements
    to be met that are not taken into account in a prototyping phase of the model.
    For example, the model in production will have to handle lots of requests from
    different users running the product. So you will want to optimize for instance
    latency and/o throughput.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习模型投入生产时，通常会有一些在模型原型阶段未考虑的要求。例如，生产中的模型必须处理来自不同用户的大量请求。因此，你需要优化例如延迟和/或吞吐量。
- en: '**Latency**: is the time it takes for a task to get done, like how long it
    takes to load a webpage after you click a link. It’s the waiting time between
    starting something and seeing the result.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟**：是完成任务所需的时间，例如点击链接后加载网页所需的时间。这是从开始某项工作到看到结果的等待时间。'
- en: '**Throughput**: is how much requests a system can handle in a certain time.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**吞吐量**：是系统在一定时间内能够处理的请求数量。'
- en: This means that the Machine Learning model has to be very fast at making its
    predictions, and for this there are various techniques that serve to increase
    the speed of model inference, let’s look at the most important ones in this article.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着机器学习模型必须非常快速地进行预测，为此有各种技术可以提高模型推理的速度，让我们在本文中探讨其中最重要的几种。
- en: Model Compression
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型压缩
- en: There are techniques that aim to make **models smaller**, which is why they
    are called **model compression** techniques, while others that focus on making
    models **faster at inference** and thus fall under the field of **model optimization**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些技术旨在使**模型更小**，这就是为什么它们被称为**模型压缩**技术，而其他技术则专注于使模型**推理更快**，因此属于**模型优化**领域。
- en: But often making models smaller also helps with inference speed, so it is a
    very blurred line that separates these two fields of study.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但通常使模型更小也有助于推理速度，因此这两个研究领域之间的界限非常模糊。
- en: Low Rank Factorization
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低秩分解
- en: This is the first method we see, and it is being studied a lot, in fact many
    papers have recently come out concerning it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们看到的第一种方法，实际上这方面的研究很多，最近有许多相关论文发表。
- en: '**The basic idea is to replace the matrices of a neural network** (the matrices
    representing the layers of the network) **with matrices that have a lower dimensionality**,
    although it would be more correct to talk about tensors, because we can often
    have matrices of more than 2 dimensions. In this way we will have fewer network
    parameters and faster inference.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: A trivial case is in a CNN network of replacing 3x3 convolutions with 1x1 convolutions.
    Such techniques are used by networks such as [SqueezeNet](https://arxiv.org/abs/1602.07360).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Lately, similar ideas are being applied for other purposes, such as allowing
    fine-tuning of large language models with limited resources.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: When fine-tuning a pretrained model for a downstreamtask, one still has to train
    the model on all the parameters of the pretrained model, which can be very expensive.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: So the idea of the method called “[Low Rank Adaptation Of Large Language Models](https://arxiv.org/abs/2106.09685)”,
    or LoRA, is to replace matrices from the original model with pairs of smaller
    matrices (using matrix decomposition) that have a smaller size. This way only
    these new matrices need to be retrained to fit the pretrained model to more downstream
    tasks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b21d32443e3691de45e54500a439b131.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: 'Matric Decomposition in LoRA( src: https://arxiv.org/pdf/2106.09685.pdf)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Lets now see how to implement fine tuning using LoRA with the [PEFT](https://huggingface.co/docs/peft/index)
    library from Hugging Face 🤗.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we want to fine-tune the `[bigscience/mt0-large](https://huggingface.co/bigscience/mt0-large)`
    using LoRA. We must first take care of importing what we will need.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The next step will be to create a configuration for LoRA to be applied during
    fine-tuning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We now instantiate the model using the base model of the Transformers library
    and the configuration object we created for LoRA.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Knowledge Distillation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is another method that allows us to put a “small” and therefore fast model
    into production.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to have a **large model called the teacher**, and a s**maller model
    called the student**, and we will **use the** **teacher’s knowledge to teach the
    student what to predict**. That way we can put only the student into production.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3f862323f2d9b3c00b3c9ae076faab1.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: 'Knowledge Distillation (src: [https://www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/](https://www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/))'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: A classic example of a model developed in this way is [DistillBERT](https://huggingface.co/docs/transformers/model_doc/distilbert),
    which is the student model of [BERT](https://arxiv.org/abs/1810.04805). DistilBERT
    is 40% smaller than BERT, but retains 97% of the language comprehension capabilities
    and is 60% faster in inference.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: One downside of this method is that you still need to have the large techer
    model available in order to train the student, and you may not have the resources
    to train a model like the teacher.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a simple example of knowledge distillation in Python. A key concept
    to understand is [Kullback–Leibler divergenc](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)e,
    which is a mathematical concept for understanding the difference between two distributions,
    and in fact in our case we want to understand the difference between the predictions
    of the two models, so the loss function of the training will be based on this
    mathematical concept.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Pruning
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pruning is a model compression method that I studied for my graduate thesis,
    and in fact I have previously published an article on how to implement Pruning
    in Julia: [Iterative Pruning Methods for Artificial Neural Networks in Julia](/iterative-pruning-methods-for-artificial-neural-networks-in-julia-c605f547a485).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Pruning was born to address overfitting in Decision Trees, in fact branches
    were cut off to decrease tree depth. The concept was later used in Neural Networks
    in which edges and/or nodes in the network are removed ( depending on whether
    unstructured pruning or structured pruning is performed).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5512b117100950116c5961541a85284.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: 'Neural Network Pruning (src: [https://towardsdatascience.com/pruning-neural-networks-1bb3ab5791f9](/pruning-neural-networks-1bb3ab5791f9))'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Suppose to remove entire nodes from the network, the matrices representing the
    layers will become smaller along with your model, and thus also faster.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if we remove individual edges, the size of the matrices will remain
    the same, but we will place zeros in correspondence with the removed edges, and
    thus we will have very sparse matrices. In unstructured pruning therefore the
    advantage lies not in increased speed, but in memory, because saving sparse matrices
    in memory takes up much less space than saving dense matrices.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'But what are the nodes or edges that we want to prune? The most unnecessary
    ones… There are a lot of research about this, and I’d like to link you to two
    papers in particular:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[Optimal Brain Damage](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=17c0a7de3c17d31f79589d245852b57d083d386e)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimal Brain Surgeon and general network pruning](https://ieeexplore.ieee.org/document/298572)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see a simple Python script on how to implement Pruning for a simple MNIST
    Model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Quantization
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I don’t think I am wrong in saying that quantization is probably the most widely
    used comrpession technique at the moment. Again, the basic idea is simple. **We
    generally represent of parameters of our neural network using 32bit float numbers.
    But what if we used less than that? We could use 16bit, 8, bit, 4 bit, or even
    1bit and have binary networks!**
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: What does this imply? By using lower precision numbers, the model will weigh
    less and be smaller, however it will also lose precision giving more approximate
    results than the original model. This is a technique used a lot when we need to
    deploy an on edge devices, on some particular hardware like a smartphone, because
    it allows us to shrink the size of the network a lot. Many frameworks allow to
    easily apply quantization such as [TensorFlow Lite](https://www.tensorflow.org/lite),
    [PyTorch](https://pytorch.org/) or [TensorRT](https://developer.nvidia.com/tensorrt).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Quantization can be applied pre-training, so we directly transect a network
    whose parameters can only take parameters in a certain range, or post-training,
    so at the end we round up the value of the parameters.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Here again we quickly see how to apply quantization in Python.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Conclusion
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we looked at several model compression methodologies in order
    to speed up the model inference phase, which can be a critical requirement for
    models in production. In particular, we focused on Low Rank Factorization, Knowledge
    Distillation, Pruning and Quantization, explaining the basic ideas and showing
    a simple implementation in Python.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Model compression is also particularly useful for deploying models on particular
    hardware such as smartphones that have few resources (RAM, GPU, etc…).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: One use case that I am very passionate about is also to use model compression
    to deploy models on satellites and spacecraft, which is very useful particularly
    in the field of earth observation, for example to allow the satellite to recognize
    autonomously which data or images to discard so as not to have too much traffic
    when this data is then sent to the ground segment on the for the data analysis.
    I hope this article has been helpful to you in better understanding this topic.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this article, follow me on Medium! 😄
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 💼 [Linkedin](https://www.linkedin.com/in/marcello-politi/) ️| 🐦 [Twitter](https://twitter.com/_March08_)
    | [💻](https://emojiterra.com/laptop-computer/) [Website](https://marcello-politi.super.site/)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
