- en: 'Efficient Deep Learning: Unleashing the Power of Model Compression'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效深度学习：释放模型压缩的力量
- en: 原文：[https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06](https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06](https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06)
- en: '![](../Images/e3f104a88962263b5a88baa0f644362f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3f104a88962263b5a88baa0f644362f.png)'
- en: Image By Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Accelerate model inference speed in production
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速生产中的模型推理速度
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    ·9 min read·Sep 3, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    ·9分钟阅读·2023年9月3日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: When a Machine Learning model is deployed into production there are often requirements
    to be met that are not taken into account in a prototyping phase of the model.
    For example, the model in production will have to handle lots of requests from
    different users running the product. So you will want to optimize for instance
    latency and/o throughput.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习模型投入生产时，通常会有一些在模型原型阶段未考虑的要求。例如，生产中的模型必须处理来自不同用户的大量请求。因此，你需要优化例如延迟和/或吞吐量。
- en: '**Latency**: is the time it takes for a task to get done, like how long it
    takes to load a webpage after you click a link. It’s the waiting time between
    starting something and seeing the result.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟**：是完成任务所需的时间，例如点击链接后加载网页所需的时间。这是从开始某项工作到看到结果的等待时间。'
- en: '**Throughput**: is how much requests a system can handle in a certain time.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**吞吐量**：是系统在一定时间内能够处理的请求数量。'
- en: This means that the Machine Learning model has to be very fast at making its
    predictions, and for this there are various techniques that serve to increase
    the speed of model inference, let’s look at the most important ones in this article.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着机器学习模型必须非常快速地进行预测，为此有各种技术可以提高模型推理的速度，让我们在本文中探讨其中最重要的几种。
- en: Model Compression
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型压缩
- en: There are techniques that aim to make **models smaller**, which is why they
    are called **model compression** techniques, while others that focus on making
    models **faster at inference** and thus fall under the field of **model optimization**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些技术旨在使**模型更小**，这就是为什么它们被称为**模型压缩**技术，而其他技术则专注于使模型**推理更快**，因此属于**模型优化**领域。
- en: But often making models smaller also helps with inference speed, so it is a
    very blurred line that separates these two fields of study.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但通常使模型更小也有助于推理速度，因此这两个研究领域之间的界限非常模糊。
- en: Low Rank Factorization
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低秩分解
- en: This is the first method we see, and it is being studied a lot, in fact many
    papers have recently come out concerning it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们看到的第一种方法，实际上这方面的研究很多，最近有许多相关论文发表。
- en: '**The basic idea is to replace the matrices of a neural network** (the matrices
    representing the layers of the network) **with matrices that have a lower dimensionality**,
    although it would be more correct to talk about tensors, because we can often
    have matrices of more than 2 dimensions. In this way we will have fewer network
    parameters and faster inference.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**基本思想是将神经网络中的矩阵**（代表网络层的矩阵）**替换为具有较低维度的矩阵**，尽管更准确的说法是张量，因为我们常常会有超过2维的矩阵。通过这种方式，我们将拥有更少的网络参数和更快的推理速度。'
- en: A trivial case is in a CNN network of replacing 3x3 convolutions with 1x1 convolutions.
    Such techniques are used by networks such as [SqueezeNet](https://arxiv.org/abs/1602.07360).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的例子是在CNN网络中用1x1卷积替代3x3卷积。这样的技术被[ SqueezeNet](https://arxiv.org/abs/1602.07360)等网络使用。
- en: Lately, similar ideas are being applied for other purposes, such as allowing
    fine-tuning of large language models with limited resources.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，类似的思想被应用于其他目的，例如在资源有限的情况下允许对大型语言模型进行微调。
- en: When fine-tuning a pretrained model for a downstreamtask, one still has to train
    the model on all the parameters of the pretrained model, which can be very expensive.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在对预训练模型进行微调以用于下游任务时，仍需对预训练模型的所有参数进行训练，这可能会非常昂贵。
- en: So the idea of the method called “[Low Rank Adaptation Of Large Language Models](https://arxiv.org/abs/2106.09685)”,
    or LoRA, is to replace matrices from the original model with pairs of smaller
    matrices (using matrix decomposition) that have a smaller size. This way only
    these new matrices need to be retrained to fit the pretrained model to more downstream
    tasks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，名为“[大规模语言模型的低秩适应](https://arxiv.org/abs/2106.09685)”或LoRA的方法的想法是将原始模型中的矩阵替换为一对对尺寸较小的矩阵（使用矩阵分解）。这样，只有这些新的矩阵需要重新训练，以便将预训练模型调整到更多的下游任务中。
- en: '![](../Images/b21d32443e3691de45e54500a439b131.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b21d32443e3691de45e54500a439b131.png)'
- en: 'Matric Decomposition in LoRA( src: https://arxiv.org/pdf/2106.09685.pdf)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA中的矩阵分解（来源： https://arxiv.org/pdf/2106.09685.pdf）
- en: Lets now see how to implement fine tuning using LoRA with the [PEFT](https://huggingface.co/docs/peft/index)
    library from Hugging Face 🤗.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用Hugging Face 🤗 的[PEFT](https://huggingface.co/docs/peft/index)库来实现微调。
- en: Suppose we want to fine-tune the `[bigscience/mt0-large](https://huggingface.co/bigscience/mt0-large)`
    using LoRA. We must first take care of importing what we will need.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想使用LoRA对`[bigscience/mt0-large](https://huggingface.co/bigscience/mt0-large)`进行微调。我们必须首先处理导入我们需要的内容。
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The next step will be to create a configuration for LoRA to be applied during
    fine-tuning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步将是创建一个LoRA配置，以便在微调过程中应用。
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We now instantiate the model using the base model of the Transformers library
    and the configuration object we created for LoRA.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用Transformers库的基础模型和我们为LoRA创建的配置对象来实例化模型。
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Knowledge Distillation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识蒸馏
- en: This is another method that allows us to put a “small” and therefore fast model
    into production.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一种方法，它允许我们将一个“小型”且因此速度更快的模型投入生产。
- en: The idea is to have a **large model called the teacher**, and a s**maller model
    called the student**, and we will **use the** **teacher’s knowledge to teach the
    student what to predict**. That way we can put only the student into production.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法的核心思想是有一个**称为教师的大型模型**和一个**称为学生的小型模型**，我们将**利用教师的知识来教学生如何进行预测**。这样我们就可以只将学生模型投入生产。
- en: '![](../Images/e3f862323f2d9b3c00b3c9ae076faab1.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3f862323f2d9b3c00b3c9ae076faab1.png)'
- en: 'Knowledge Distillation (src: [https://www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/](https://www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/))'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏（来源：[https://www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/](https://www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/)）
- en: A classic example of a model developed in this way is [DistillBERT](https://huggingface.co/docs/transformers/model_doc/distilbert),
    which is the student model of [BERT](https://arxiv.org/abs/1810.04805). DistilBERT
    is 40% smaller than BERT, but retains 97% of the language comprehension capabilities
    and is 60% faster in inference.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式开发的经典模型示例是[DistillBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)，它是[
    BERT](https://arxiv.org/abs/1810.04805)的学生模型。DistilBERT比BERT小40%，但保留了97%的语言理解能力，并且在推理时快60%。
- en: One downside of this method is that you still need to have the large techer
    model available in order to train the student, and you may not have the resources
    to train a model like the teacher.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个缺点是，你仍然需要拥有大型教师模型以训练学生，而你可能没有资源训练像教师那样的模型。
- en: Let’s look at a simple example of knowledge distillation in Python. A key concept
    to understand is [Kullback–Leibler divergenc](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)e,
    which is a mathematical concept for understanding the difference between two distributions,
    and in fact in our case we want to understand the difference between the predictions
    of the two models, so the loss function of the training will be based on this
    mathematical concept.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个Python中的知识蒸馏的简单示例。一个关键概念是[KL散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)，这是一个理解两个分布之间差异的数学概念，实际上在我们的案例中，我们想要了解两个模型的预测之间的差异，因此训练的损失函数将基于这个数学概念。
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Pruning
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪枝
- en: 'Pruning is a model compression method that I studied for my graduate thesis,
    and in fact I have previously published an article on how to implement Pruning
    in Julia: [Iterative Pruning Methods for Artificial Neural Networks in Julia](/iterative-pruning-methods-for-artificial-neural-networks-in-julia-c605f547a485).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝是一种模型压缩方法，我在研究生论文中研究过这个方法，实际上我之前已经发布了一篇关于如何在Julia中实现剪枝的文章：[Julia中的迭代剪枝方法](/iterative-pruning-methods-for-artificial-neural-networks-in-julia-c605f547a485)。
- en: Pruning was born to address overfitting in Decision Trees, in fact branches
    were cut off to decrease tree depth. The concept was later used in Neural Networks
    in which edges and/or nodes in the network are removed ( depending on whether
    unstructured pruning or structured pruning is performed).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝诞生是为了应对决策树中的过拟合，实际上，剪去了分支以减少树的深度。这个概念后来被应用于神经网络中，在其中网络中的边和/或节点被移除（取决于是进行非结构化剪枝还是结构化剪枝）。
- en: '![](../Images/a5512b117100950116c5961541a85284.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5512b117100950116c5961541a85284.png)'
- en: 'Neural Network Pruning (src: [https://towardsdatascience.com/pruning-neural-networks-1bb3ab5791f9](/pruning-neural-networks-1bb3ab5791f9))'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络剪枝（来源：[https://towardsdatascience.com/pruning-neural-networks-1bb3ab5791f9](/pruning-neural-networks-1bb3ab5791f9)）
- en: Suppose to remove entire nodes from the network, the matrices representing the
    layers will become smaller along with your model, and thus also faster.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从网络中移除整个节点，表示层的矩阵会变得更小，模型也会变得更快。
- en: In contrast, if we remove individual edges, the size of the matrices will remain
    the same, but we will place zeros in correspondence with the removed edges, and
    thus we will have very sparse matrices. In unstructured pruning therefore the
    advantage lies not in increased speed, but in memory, because saving sparse matrices
    in memory takes up much less space than saving dense matrices.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果我们移除单个边，矩阵的大小将保持不变，但我们会在与被移除的边对应的位置放置零，因此我们将拥有非常稀疏的矩阵。因此，在非结构化剪枝中，优势不在于提高速度，而在于节省内存，因为在内存中保存稀疏矩阵占用的空间比保存密集矩阵少得多。
- en: 'But what are the nodes or edges that we want to prune? The most unnecessary
    ones… There are a lot of research about this, and I’d like to link you to two
    papers in particular:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们要剪枝的节点或边是什么呢？最不必要的那些……关于这一点有很多研究，我特别想给你推荐两篇论文：
- en: '[Optimal Brain Damage](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=17c0a7de3c17d31f79589d245852b57d083d386e)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Optimal Brain Damage](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=17c0a7de3c17d31f79589d245852b57d083d386e)'
- en: '[Optimal Brain Surgeon and general network pruning](https://ieeexplore.ieee.org/document/298572)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Optimal Brain Surgeon 和通用网络剪枝](https://ieeexplore.ieee.org/document/298572)'
- en: Let’s see a simple Python script on how to implement Pruning for a simple MNIST
    Model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的Python脚本，了解如何为一个简单的MNIST模型实现剪枝。
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Quantization
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化
- en: I don’t think I am wrong in saying that quantization is probably the most widely
    used comrpession technique at the moment. Again, the basic idea is simple. **We
    generally represent of parameters of our neural network using 32bit float numbers.
    But what if we used less than that? We could use 16bit, 8, bit, 4 bit, or even
    1bit and have binary networks!**
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我不认为我说错了，量化可能是目前使用最广泛的压缩技术。再一次，基本概念很简单。**我们通常用32位浮点数表示神经网络的参数。但如果我们使用更少的位数呢？我们可以使用16位、8位、4位，甚至1位，甚至可以拥有二进制网络！**
- en: What does this imply? By using lower precision numbers, the model will weigh
    less and be smaller, however it will also lose precision giving more approximate
    results than the original model. This is a technique used a lot when we need to
    deploy an on edge devices, on some particular hardware like a smartphone, because
    it allows us to shrink the size of the network a lot. Many frameworks allow to
    easily apply quantization such as [TensorFlow Lite](https://www.tensorflow.org/lite),
    [PyTorch](https://pytorch.org/) or [TensorRT](https://developer.nvidia.com/tensorrt).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着什么？通过使用较低精度的数字，模型的体积会更小，但也会失去精度，结果会比原始模型更为近似。这是一种在需要将模型部署到边缘设备上时常用的技术，特别是智能手机等特定硬件上，因为它能大幅缩小网络的尺寸。许多框架允许轻松应用量化，例如
    [TensorFlow Lite](https://www.tensorflow.org/lite)、[PyTorch](https://pytorch.org/)
    或 [TensorRT](https://developer.nvidia.com/tensorrt)。
- en: Quantization can be applied pre-training, so we directly transect a network
    whose parameters can only take parameters in a certain range, or post-training,
    so at the end we round up the value of the parameters.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 量化可以在训练前应用，即我们直接截断参数只能取特定范围内的值，或者在训练后应用，即在结束时对参数的值进行四舍五入。
- en: Here again we quickly see how to apply quantization in Python.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们快速展示了如何在 Python 中应用量化。
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Conclusion
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we looked at several model compression methodologies in order
    to speed up the model inference phase, which can be a critical requirement for
    models in production. In particular, we focused on Low Rank Factorization, Knowledge
    Distillation, Pruning and Quantization, explaining the basic ideas and showing
    a simple implementation in Python.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了几种模型压缩方法，以加快模型推理阶段，这对于生产环境中的模型可能是一个关键要求。特别地，我们关注了低秩分解、知识蒸馏、剪枝和量化，解释了基本概念，并展示了
    Python 中的简单实现。
- en: Model compression is also particularly useful for deploying models on particular
    hardware such as smartphones that have few resources (RAM, GPU, etc…).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩对于在资源有限（如 RAM、GPU 等）的特定硬件上部署模型尤其有用，例如智能手机。
- en: One use case that I am very passionate about is also to use model compression
    to deploy models on satellites and spacecraft, which is very useful particularly
    in the field of earth observation, for example to allow the satellite to recognize
    autonomously which data or images to discard so as not to have too much traffic
    when this data is then sent to the ground segment on the for the data analysis.
    I hope this article has been helpful to you in better understanding this topic.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常热衷的一个应用案例是使用模型压缩来在卫星和航天器上部署模型，这在地球观测领域尤其有用，例如使卫星能够自主识别需要丢弃的数据或图像，以避免在数据传输到地面段进行分析时产生过多流量。希望这篇文章对你更好地理解这个话题有所帮助。
- en: If you liked this article, follow me on Medium! 😄
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，请在 Medium 上关注我！😄
- en: 💼 [Linkedin](https://www.linkedin.com/in/marcello-politi/) ️| 🐦 [Twitter](https://twitter.com/_March08_)
    | [💻](https://emojiterra.com/laptop-computer/) [Website](https://marcello-politi.super.site/)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 💼 [Linkedin](https://www.linkedin.com/in/marcello-politi/) ️| 🐦 [Twitter](https://twitter.com/_March08_)
    | [💻](https://emojiterra.com/laptop-computer/) [网站](https://marcello-politi.super.site/)
