["```py\ndef sample_gumbel(shape, eps=1e-20):\n    # sample from a uniform distribution\n    U = torch.rand(shape)\n    if is_cuda:\n        U = U.cuda()\n    return -torch.log(-torch.log(U + eps) + eps)\n\ndef gumbel_softmax_sample(logits, temperature):\n    y = logits + sample_gumbel(logits.size())\n    return F.softmax(y / temperature, dim=-1)\n\ndef gumbel_softmax(logits, temperature, hard=False):\n    y = gumbel_softmax_sample(logits, temperature)\n\n    if not hard:\n        return y.view(-1, latent_dim * categorical_dim)\n\n    shape = y.size()\n    _, ind = y.max(dim=-1)\n    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n    y_hard.scatter_(1, ind.view(-1, 1), 1)\n    y_hard = y_hard.view(*shape)\n    # skip the gradient of y_hard\n    y_hard = (y_hard - y).detach() + y \n    return y_hard.view(-1, latent_dim * categorical_dim)\n```", "```py\nskip_d = False\n\na = torch.Tensor([1])\na.requires_grad = True\n\nb = torch.Tensor([2])\nb.requires_grad = True\n\nc = 2 * (a + b)\n\nif skip_d:\n    d = c ** 2\n    d = (d - c).detach() + c\nelse:\n    d = c ** 2\n\nf = d * 4\nf.retain_grad()\nd.retain_grad()\nc.retain_grad()\n\nloss = f * 3\nloss.backward()\n\nprint(loss)\nprint(a.grad, b.grad, c.grad, d.grad, f.grad)\n# Loss value: tensor([432.])\n# (tensor([288.]), tensor([288.]), tensor([144.]), tensor([12.]), tensor([3.]))\n\n# Running the same with skip_d = True we get:\n# tensor([432.])\n# (tensor([24.]), tensor([24.]), tensor([12.]), tensor([12.]), tensor([3.]))\n```", "```py\nclass VAE_model(nn.Module):\n    def __init__(self):\n        super(VAE_model, self).__init__()\n        self.fc1 = nn.Linear(784, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, latent_dim * categorical_dim)\n        self.fc4 = nn.Linear(latent_dim * categorical_dim, 256)\n        self.fc5 = nn.Linear(256, 512)\n        self.fc6 = nn.Linear(512, 784)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def encode(self, x):\n        h1 = self.relu(self.fc1(x))\n        h2 = self.relu(self.fc2(h1))\n        return self.relu(self.fc3(h2))\n\n    def decode(self, z):\n        h4 = self.relu(self.fc4(z))\n        h5 = self.relu(self.fc5(h4))\n        return self.sigmoid(self.fc6(h5))\n```", "```py\nlogits_z = self.encode(data.view(-1, \nlogits_z = logits_z.view(-1, latent_dim, categorical_dim)\nlatent_z = gumbel_softmax(logits_z, temp)\nlatent_z = latent_z.view(-1, latent_dim * categorical_dim)\n```", "```py\nprobs_x = self.decode(latent_z)\n# we assumed distribution of the data is Bernoulli\ndist_x = torch.distributions.Bernoulli(probs=probs_x, validate_args=False)\n```", "```py\n# reconstruction loss - log probabilities of the data\nrec_loss = dist_x.log_prob(data.view(-1, 784)).sum(dim=-1)\n```", "```py\n# KL divergence loss\nKL = (posterior_distrib.probs * (logits_z_log - prior_distrib.probs.log())).view(-1, latent_dim * categorical_dim).sum(dim=-1)\n```", "```py\ntorch.manual_seed(0)\n\nbatch_size = 100\ntemperature = 1.0\nseed = 0\nlog_interval = 10\nhard = False\nis_cuda = torch.cuda.is_available()\ntorch.manual_seed(seed)\nif is_cuda:\n    torch.cuda.manual_seed(seed)\nkwargs = {'num_workers': 1, 'pin_memory': True} if is_cuda else {}\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('./data/MNIST', train=True, download=True,\n                   transform=transforms.ToTensor()),\n    batch_size=batch_size, shuffle=True, **kwargs)\n\ndef sample_gumbel(shape, eps=1e-20):\n    # sample from a uniform distribution\n    U = torch.rand(shape)\n    if is_cuda:\n        U = U.cuda()\n    return -torch.log(-torch.log(U + eps) + eps)\n\ndef gumbel_softmax_sample(logits, temperature):\n    y = logits + sample_gumbel(logits.size())\n    return F.softmax(y / temperature, dim=-1)\n\ndef gumbel_softmax(logits, temperature, hard=False):\n    y = gumbel_softmax_sample(logits, temperature)\n\n    if not hard:\n        return y.view(-1, latent_dim * categorical_dim)\n\n    shape = y.size()\n    _, ind = y.max(dim=-1)\n    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n    y_hard.scatter_(1, ind.view(-1, 1), 1)\n    y_hard = y_hard.view(*shape)\n    # skip the gradient of y_hard\n    y_hard = (y_hard - y).detach() + y \n    return y_hard.view(-1, latent_dim * categorical_dim)\n\nclass VAE_model(nn.Module):\n    def __init__(self):\n        super(VAE_model, self).__init__()\n        self.fc1 = nn.Linear(784, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, latent_dim * categorical_dim)\n        self.fc4 = nn.Linear(latent_dim * categorical_dim, 256)\n        self.fc5 = nn.Linear(256, 512)\n        self.fc6 = nn.Linear(512, 784)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def sample_img(self, img, temp, random=True):\n        # evaluation\n        with torch.no_grad():\n            logits_z = self.encode(img.view(-1, 784))\n            logits_z = logits_z.view(-1, latent_dim, categorical_dim)\n            if random:\n                latent_z = gumbel_softmax(logits_z, temp, True)\n            else:\n                latent_z = logits_z.view(-1, latent_dim * categorical_dim)\n            logits_x = self.decode(latent_z)\n            # probs instead of logits because we have sigmoid activation \n            # in the decoder\n            dist_x = torch.distributions.Bernoulli(probs=logits_x)\n            sampled_img = dist_x.sample()\n        return sampled_img\n\n    def encode(self, x):\n        h1 = self.relu(self.fc1(x))\n        h2 = self.relu(self.fc2(h1))\n        return self.relu(self.fc3(h2))\n\n    def decode(self, z):\n        h4 = self.relu(self.fc4(z))\n        h5 = self.relu(self.fc5(h4))\n        return self.sigmoid(self.fc6(h5))\n\n    def forward(self, data, temp, hard):\n        logits_z = self.encode(data.view(-1, 784))\n        logits_z = logits_z.view(-1, latent_dim, categorical_dim)\n\n        # estimated posterior probabiity coefficients\n        probs_z = F.softmax(logits_z, dim=-1)\n        posterior_distrib = torch.distributions.Categorical(probs=probs_z)\n        # categorical prior\n        probs_prior = torch.ones_like(logits_z)/categorical_dim\n        prior_distrib = torch.distributions.Categorical(probs=probs_prior)\n\n        latent_z = gumbel_softmax(logits_z, temp)\n        latent_z = latent_z.view(-1, latent_dim * categorical_dim)\n\n        probs_x = self.decode(latent_z)\n        # we assumed distribution of the data is Bernoulli\n        dist_x = torch.distributions.Bernoulli(probs=probs_x, validate_args=False)\n        # Losses\n        # reconstruction loss - log probabilities of the data\n        rec_loss = dist_x.log_prob(data.view(-1, 784)).sum(dim=-1)\n        logits_z_log = F.log_softmax(logits_z, dim=-1)\n        # KL divergence loss\n        KL = (posterior_distrib.probs * (logits_z_log - prior_distrib.probs.log())).view(-1, latent_dim * categorical_dim).sum(dim=-1)\n        elbo = rec_loss - KL\n        loss = -elbo.mean()\n        return loss\n\ndef train(epoch, model, optimizer):\n    model.train()\n    train_loss = 0\n    temp = temperature\n    for batch_idx, (data, _) in enumerate(train_loader):\n        if is_cuda:\n            data = data.cuda()\n        optimizer.zero_grad()\n        loss = model(data, temp, hard)\n        loss.backward()\n        train_loss += loss.item() * len(data)\n        optimizer.step()\n        if batch_idx % 100 == 1:\n            temp = np.maximum(temp * np.exp(-ANNEAL_RATE * batch_idx), temp_min)\n        if batch_idx % log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                       100\\. * batch_idx / len(train_loader),\n                       loss.item()))\n            print(\"Temperature : \", temp)\n\n            sampled = model.sample_img(data[0].view(-1, 28*28), temp).view(28, 28).detach().cpu()\n            fig, axs = plt.subplots(1, 2, figsize=(6,4))\n            fig.suptitle('Reconstructed vs Real')\n            axs[0].imshow(sampled.reshape(28,28))\n            axs[0].axis('off')\n            axs[1].imshow(data[0].reshape(28,28).detach().cpu())\n            axs[1].axis('off')\n            plt.show()\n    print('====> Epoch: {} Average loss: {:.4f}'.format(\n        epoch, train_loss / len(train_loader.dataset)))\n\n### Train\ntemp_min = 0.5\nANNEAL_RATE = 0.00003\nlatent_dim = 20\ncategorical_dim = 10\nmy_model = VAE_model()\nmy_model.to('cuda:0')\noptimizer = optim.Adam(my_model.parameters(), lr=1e-3)\nfor epoch in range(3):\n    train(epoch, my_model, optimizer)\n```"]