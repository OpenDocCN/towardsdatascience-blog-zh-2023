- en: Introduction to PCA in Python with Sklearn, Pandas, and Matplotlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/introduction-to-pca-in-python-with-sklearn-pandas-and-matplotlib-476880f30238](https://towardsdatascience.com/introduction-to-pca-in-python-with-sklearn-pandas-and-matplotlib-476880f30238)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Learn the intuition behind PCA in Python and Sklearn by transforming a multidimensional
    dataset into an arbitrary number of dimensions and visualizing the reduced data
    with Matplotlib*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@theDrewDag?source=post_page-----476880f30238--------------------------------)[![Andrea
    D''Agostino](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----476880f30238--------------------------------)[](https://towardsdatascience.com/?source=post_page-----476880f30238--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----476880f30238--------------------------------)
    [Andrea D''Agostino](https://medium.com/@theDrewDag?source=post_page-----476880f30238--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----476880f30238--------------------------------)
    ·13 min read·Sep 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/acfc1cd8e8829588471b9dc0c9f7508a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nivenn Lanos](https://unsplash.com/@nivenn?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: As data analysts and scientists, we are often faced with complex challenges
    due to the growing amount of information available.
  prefs: []
  type: TYPE_NORMAL
- en: It is undeniable that the accumulation of data from various sources has become
    a constant in our lives. Data scientist or not, **everyone practically describes
    a phenomenon as a collection of variables or attributes.**
  prefs: []
  type: TYPE_NORMAL
- en: It is very rare to work on solving an analytical challenge without having to
    deal with a multidimensional data set — this is especially evident today, where
    data collection is increasingly automated and technology allows us to acquire
    information from a wide range of sources, including **sensors, IoT devices, social
    media, online transactions and much more.**
  prefs: []
  type: TYPE_NORMAL
- en: But as the complexity of a phenomenon grows, so do the challenges that the data
    scientist has to face to achieve his or her goals.
  prefs: []
  type: TYPE_NORMAL
- en: These challenges might include…
  prefs: []
  type: TYPE_NORMAL
- en: '**High dimensionality**: Having many columns can lead to high dimensionality
    problems, which can make models more complex and difficult to interpret.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noisy data**: The automatic collection of data can lead to the presence of
    errors, missing data, or unreliable data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretation**: High dimensionality means low interpretability — it is
    difficult to understand what the most influential features are for a certain problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting**: Too complex models can suffer from overfitting, i.e. excessive
    adaptation to training data, with consequent low ability to generalize new data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational resources**: The analysis of large and complex datasets often
    requires significant computational resources. Scalability is an important consideration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communication of the results**: Explaining the discoveries understandably
    obtained from a multidimensional dataset is an important challenge, especially
    when you communicate with non-technical stakeholders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I wrote an article that connects to this topic, which you can read here
  prefs: []
  type: TYPE_NORMAL
- en: '[](/why-having-many-features-can-hinder-your-models-performance-865369b6b8b1?source=post_page-----476880f30238--------------------------------)
    [## Why having many features can hinder your model’s performance'
  prefs: []
  type: TYPE_NORMAL
- en: The activity of feature engineering can be very useful for improving the performance
    of a predictive model. However, it…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/why-having-many-features-can-hinder-your-models-performance-865369b6b8b1?source=post_page-----476880f30238--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Multidimensionality in the data science and machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In data science and machine learning, a multidimensional dataset is a collection
    of organized data that include multiple columns or attributes, each of which represents
    a characteristic (or feature) of the phenomenon object of study.
  prefs: []
  type: TYPE_NORMAL
- en: A dataset that contains information on houses is a concrete example of a multidimensional
    dataset. In fact, each house can be described with its square meters, the number
    of rooms, if there is a garage or not and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, **we will explore how to use the PCA to simplify and visualize
    multidimensional data effectively**, making complex multidimensional information
    accessible.
  prefs: []
  type: TYPE_NORMAL
- en: 'By following this guide, you will learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The intuition behind the PCA algorithm**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apply the PCA with Sklearn** on a toy dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Matplotlib to **visualize reduced data**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **main use cases of PCA** in data science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Fundamental intuition of the PCA algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Principal Component Analysis, *PCA*, is an unsupervised statistical technique
    for **the decomposition of multidimensional data.**
  prefs: []
  type: TYPE_NORMAL
- en: Its main purpose is to reduce our multidimensional dataset in a number of arbitrary
    variables in order to
  prefs: []
  type: TYPE_NORMAL
- en: '**Select important features** in the original dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increase the signal / noise ratio**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create new features** to provide to a Machine Learning model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visualize multidimensional data**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the number of components that we choose, the PCA algorithm allows the
    reduction of the number of variables in the original dataset by preserving those
    that *best explain the total variance of the dataset itself.* PCA fights the so
    infamous *curse of dimensionality.*
  prefs: []
  type: TYPE_NORMAL
- en: The **curse of dimensionality** is a concept in machine learning that refers
    to the difficulty encountered when working with high dimensionality data.
  prefs: []
  type: TYPE_NORMAL
- en: As the number of size increases, the number of data necessary to represent a
    set of data reliably increases exponentially. **This can make it difficult to
    find interesting patterns in data and can cause overfitting problems in automatic
    learning models.**
  prefs: []
  type: TYPE_NORMAL
- en: The transformation applied by the PCA reduces the size of the dataset by creating
    components that best explain the variance of the original data. This allows to
    isolate the most relevant variables and to reduce the complexity of the dataset.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**PCA is typically a difficult technique to understand**, especially for new
    practitioners in the field of data science and analytics.'
  prefs: []
  type: TYPE_NORMAL
- en: The motivation of this difficulty must be sought in the strictly mathematical
    bases of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '*So what does the PCA do from a mathematical point of view?*'
  prefs: []
  type: TYPE_NORMAL
- en: PCA allows us to project a n-dimensional dataset in a lower dimensional plane.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'It seems complex, but in reality it is not. Let’s try with a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: When we draw something on a sheet of paper, we are actually taking a mental
    representation (which we can represent in 3 dimensions) and projecting it on the
    sheet. **In doing so, we reduce the quality and precision of the representation.**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, the representation on sheet remains understandable and even shareable
    with our peers.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, during the act of drawing we represent forms, lines and shadows in
    order to allow the observer to understand what we are thinking about in our brain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take this image of a shark as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/675ee3f512121bac7e2e3121c97b43de.png)'
  prefs: []
  type: TYPE_IMG
- en: Foto di [David Clode](https://unsplash.com/it/@davidclode?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    su [Unsplash](https://unsplash.com/it/foto/OCWu7r9XP-Q?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: 'If we wanted to draw it on a sheet of paper, based on our level of skill (mine
    is very low as you can see), we could represent it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69ac1269fe86ee1eb627e68df6146667.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The point is that despite the representation is not perfectly 1:1, **an observer
    can easily understand that the drawing represents a shark.**
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the “mental” algorithm that we used is similar to the PCA — we have
    reduced the dimensionality, therefore the characteristics of the shark in photography,
    and **used only the most relevant dimensions to communicate the concept of “shark”
    on the sheet of paper.**
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically speaking therefore, we don’t just want to project our object
    into a lower dimensional plan, **but we also want to preserve the most relevant
    information as possible.**
  prefs: []
  type: TYPE_NORMAL
- en: Data compression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use a simple dataset to proceed with an example. This dataset contains
    structural information on houses, such as size in square meters, number of rooms
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fb8bd17ae158e11b23d7e3c29819a96.png)'
  prefs: []
  type: TYPE_IMG
- en: Example dataset. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The goal here is to show how easy it is to approach the **limitations of data
    visualization** when dealing with a multidimensional dataset and how PCA can help
    us overcome these limitations.
  prefs: []
  type: TYPE_NORMAL
- en: The dimensionality of a dataset can be understood simply as the number of columns
    within it. A column represents an attribute, a characteristic, of the phenomenon
    we are studying. The more dimensions there are, the more complex the phenomenon.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this case we have a dataset with 5 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '*But what are these limitations in the data visualization*? Let me explain
    it to you by analyzing the *square_metres* variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dcbfaa72caa7feab7dc9671902bfc37e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Houses 1 and 2 have a low *square_meters* values of while all the others are
    around or higher than 100\. **This is a one-dimensional graph precisely because
    we take into consideration only one variable.**
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s add a dimension to the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a62435cca953954897339760b639b92.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: This type of graph, called a *scatterplot*, shows the relationship between two
    variables. It is very useful for visualizing correlations and interactions between
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: This visualization is already starting to introduce a good level of interpretative
    complexity, as it requires careful inspection to understand the relationship between
    the variables even by expert analysts.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s go insert yet another variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0b80bfbfaba7cdd0a857443a7aac557.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**This is definitely a complex image to process**. Mathematically though, this
    is a visualization that makes perfect sense. From a perceptual and interpretative
    point of view, **we are at the limit of human understanding.**'
  prefs: []
  type: TYPE_NORMAL
- en: We all know how our interpretation of the world stops at the three-dimensional.
    However, we also know that this dataset is characterized by 5 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '*But how do we view them all?*'
  prefs: []
  type: TYPE_NORMAL
- en: We can’t, unless we visualize two-dimensional relationships between all the
    variables, side by side.
  prefs: []
  type: TYPE_NORMAL
- en: In the example below, we see how square_metres are related in two dimensions
    to *n_rooms* and *n_neighbors*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4dfe66a36640402c73c247d2d21ee3f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s imagine putting all the possible combinations side by side…we would
    soon be overwhelmed by the large amount of information to keep in mind.
  prefs: []
  type: TYPE_NORMAL
- en: This is where PCA comes into play. Using Python (we’ll see later), we can apply
    Sklearn’s PCA class to this dataset and obtain such a graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7166c3f91799f90e8b4a277980e0b0e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: What we see here is a graph showing the principal components returned by the
    PCA.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the PCA algorithm **performs a linear transformation on the data
    in order to find the linear combination of features that best explains the total
    variance of the dataset.**
  prefs: []
  type: TYPE_NORMAL
- en: This combination of features is called the **principal component**. The process
    is repeated for each major component until the desired number of components is
    reached.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using PCA is that it allows us to reduce the dimensionality
    of the data by keeping the most important information, eliminating the less relevant
    ones and making the data easier to visualize and to use to build machine learning
    models.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If you are interested in digging deeper into the mathematics behind PCA, I
    suggest the following resources in English:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PCA Explained Step-By-Step from StatQuest](https://www.youtube.com/watch?v=FgakZw6K1QQ&ref=diariodiunanalista.it)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Principal Component Analysis (PCA) Explained Visually with Zero Math](/principal-component-analysis-pca-explained-visually-with-zero-math-1cbf392b9e7d?ref=diariodiunanalista.it)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To apply PCA in Python, we can use scikit-learn, which offers a simple and effective
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: At this link you can read the [PCA documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?ref=diariodiunanalista.it).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the *wine dataset* as toy dataset for the example. The wine dataset
    is part of Scikit-Learn and under the creative commons license, attribution 4.0,
    making it free to use and share ([license can be read here](https://archive.ics.uci.edu/dataset/109/wine)).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started on the essential libraries
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0fbca795fe15c63001caa691843bd888.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The dimensionality of the data is (178, 14) — it means that there are 178 rows
    (examples that a machine learning model can learn from) each of them described
    by 14 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: We need to apply data normalization before applying PCA. You can do this with
    Sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '***It is important to normalize our data when using PCA:*** *it calculates
    a new projection of the dataset and the new axis is based on the standard deviation
    of the variables.*'
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to reduce the size. We can apply PCA simply like this
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can specify any number of PCA output dimensions as long as they are less
    than 14, which is the total number of dimensions in the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s organize the small version of the dataframe into a new Pandas Dataframe
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cd025271dada18dc5d11e857f375877d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component 1 and 2 are the output dimensions of the PCA, which will
    now be possible to visualize with a scatterplot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3bdfd71e5e37cc21c32447906d6e13cf.png)'
  prefs: []
  type: TYPE_IMG
- en: How a reduced dataset looks like. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: And there it is. This graph shows the difference between the wines described
    by the 14 initial variables, but reduced to 2 by the PCA. **The PCA retained the
    relevant information and in the meantime reduced the noise in the dataset.**
  prefs: []
  type: TYPE_NORMAL
- en: Here is the whole code to apply PCA with Sklearn, Pandas and Matplotlib in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Use cases for PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Below is a list of the most common PCA use cases in data science.
  prefs: []
  type: TYPE_NORMAL
- en: Improve machine learning model training speeds
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data compressed by the PCA provides the important information and is much
    more digestible by a machine learning model, which now bases its learning on a
    small number of features instead of all the features present in the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA is essentially a feature selection tool. When we go to apply it, we look
    for the features that explain the dataset variance best.
  prefs: []
  type: TYPE_NORMAL
- en: We can create a ranking of the principal components and sort them by importance,
    with the first component explaining the most variance and the last component explaining
    the least.
  prefs: []
  type: TYPE_NORMAL
- en: By analyzing the main components it is possible to go back to the original features
    and exclude those that do not contribute to preserving the information in the
    reduced dimensional plan created by the PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA is often used in anomaly identification because it can help identify patterns
    in the data that are not easily discerned with the naked eye.
  prefs: []
  type: TYPE_NORMAL
- en: Anomalies often appear as data points away from the main group in lower dimensional
    space, making them easier to detect.
  prefs: []
  type: TYPE_NORMAL
- en: Signal detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In contrast to anomaly identification, PCA is also very useful for signal detection.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, just as PCA can highlight outliers, it can also remove the “noise” that
    does not contribute to the total variability of the data. In the context of speech
    recognition, this allows the user to better isolate speech traces and to improve
    speech-based person identification systems.
  prefs: []
  type: TYPE_NORMAL
- en: Image compression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Working with images can be expensive if we have particular constraints, such
    as saving the image in a certain format. Without going into detail, PCA can be
    useful for compressing images while still maintaining the information present
    in them.
  prefs: []
  type: TYPE_NORMAL
- en: This allows machine learning algorithms to train faster at the expense of compressed
    information of a certain quality.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for your attention 🙏I hope you enjoyed reading and learned something
    new.
  prefs: []
  type: TYPE_NORMAL
- en: To recap,
  prefs: []
  type: TYPE_NORMAL
- en: you learned what dimensionality of a dataset means and the limitations that
    come with having many dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: you learned how the PCA algorithm works intuitively step by step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: you learned how to implement it in Python with Sklearn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and finally, you learned about the most common PCA use cases in data science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you found this article useful, share it with your passionate friends or colleagues.
  prefs: []
  type: TYPE_NORMAL
- en: Until next time,
  prefs: []
  type: TYPE_NORMAL
- en: Andrew
  prefs: []
  type: TYPE_NORMAL
- en: '**If you want to support my content creation activity, feel free to follow
    my referral link below and join Medium’s membership program**. I will receive
    a portion of your investment and you’ll be able to access Medium’s plethora of
    articles on data science and more in a seamless way.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@theDrewDag/membership?source=post_page-----476880f30238--------------------------------)
    [## Join Medium with my referral link - Andrea D''Agostino'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@theDrewDag/membership?source=post_page-----476880f30238--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Recommended Reads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the interested, here are a list of books that I recommended for each ML-related
    topic. There are ESSENTIAL books in my opinion and have greatly impacted my professional
    career.
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: these are Amazon affiliate links. I will receive a small commission
    from Amazon for referring you these items. Your experience won’t change and you
    won’t be charged more, but it will help me scale my business and produce even
    more content around AI.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intro to ML:** [*Confident Data Skills: Master the Fundamentals of Working
    with Data and Supercharge Your Career*](https://amzn.to/3ZzKTz6)by Kirill Eremenko'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sklearn / TensorFlow:** [*Hands-On Machine Learning with Scikit-Learn, Keras,
    and TensorFlow*](https://amzn.to/433F4Nm) by Aurelien Géron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NLP:** [*Text as Data: A New Framework for Machine Learning and the Social
    Sciences*](https://amzn.to/3zvH43j)by Justin Grimmer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sklearn / PyTorch:** [*Machine Learning with PyTorch and Scikit-Learn: Develop
    machine learning and deep learning models with Python*](https://amzn.to/3Gcavve)
    by Sebastian Raschka'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Viz:** [*Storytelling with Data: A Data Visualization Guide for Business
    Professionals*](https://amzn.to/3HUtGtB) by Cole Knaflic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Useful Links (written by me)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Learn how to perform a top-tier Exploratory Data Analysis in Python**: [*Exploratory
    Data Analysis in Python — A Step-by-Step Process*](/exploratory-data-analysis-in-python-a-step-by-step-process-d0dfa6bf94ee)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learn the basics of TensorFlow**: [*Get started with TensorFlow 2.0 — Introduction
    to deep learning*](https://medium.com/towards-data-science/a-comprehensive-introduction-to-tensorflows-sequential-api-and-model-for-deep-learning-c5e31aee49fa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perform text clustering with TF-IDF in Python**: [*Text Clustering with TF-IDF
    in Python*](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
