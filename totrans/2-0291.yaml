- en: An Entry Point Into HuggingFace
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进入 HuggingFace 的切入点
- en: 原文：[https://towardsdatascience.com/an-entry-point-into-huggingface-2f3d1e60ad5a](https://towardsdatascience.com/an-entry-point-into-huggingface-2f3d1e60ad5a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-entry-point-into-huggingface-2f3d1e60ad5a](https://towardsdatascience.com/an-entry-point-into-huggingface-2f3d1e60ad5a)
- en: A step by step guide of basics for beginners
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为初学者提供的基础知识的逐步指南
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----2f3d1e60ad5a--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----2f3d1e60ad5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f3d1e60ad5a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f3d1e60ad5a--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----2f3d1e60ad5a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page-----2f3d1e60ad5a--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----2f3d1e60ad5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f3d1e60ad5a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f3d1e60ad5a--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----2f3d1e60ad5a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f3d1e60ad5a--------------------------------)
    ·15 min read·Nov 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f3d1e60ad5a--------------------------------)
    ·15 分钟阅读·2023 年 11 月 26 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a6112804c83872912af719222f58fd92.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6112804c83872912af719222f58fd92.png)'
- en: Image from [unsplash](https://unsplash.com/photos/person-holding-ac-receiver-_J3oTl6acVg)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [unsplash](https://unsplash.com/photos/person-holding-ac-receiver-_J3oTl6acVg)
- en: HuggingFace can be complex and complicated if you don’t know where to start
    to learn it. One entry point into HuggingFace repository are [run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py)
    and [run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py)
    scripts.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道从哪里开始学习，HuggingFace 可能会显得复杂和困难。进入 HuggingFace 仓库的一个切入点是 [run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py)
    和 [run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py)
    脚本。
- en: In this post, we will go through [run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py)
    script. This script picks a masked language model from HuggingFace and fine tune
    it on a dataset (or train it from scratch). If you are a beginner and you have
    very little exposure to HuggingFace codes, this post will help you understand
    the basics.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将通过 [run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py)
    脚本进行讲解。这个脚本从 HuggingFace 中选择一个掩码语言模型，并在数据集上微调它（或者从头开始训练）。如果你是初学者，对 HuggingFace
    代码了解甚少，这篇文章将帮助你理解基础知识。
- en: We will pick a masked language model and load a dataset from HuggingFace and
    fine tune the model on the dataset. At the end, we will evaluate the model. This
    is all for the sake of understanding the code structure, so our focus is not on
    any specific usecase.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们将选择一个掩码语言模型，从 HuggingFace 加载数据集，并在数据集上微调模型。最后，我们将评估模型。这一切都是为了理解代码结构，因此我们的重点不在于任何特定的应用案例。
- en: Let’s get started.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Few Words About Fine Tuning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于微调的几句话
- en: Fine-tuning is a common technique in deep learning to take a pre-trained neural
    network model and tweak it to better suit a new dataset or task.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是深度学习中的一种常见技术，用于调整预训练的神经网络模型，以更好地适应新的数据集或任务。
- en: Fine tuning works well when your dataset is not large enough to train a deep
    model from scratch! So you start from an already learned base model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的数据集不足以从头训练一个深度模型时，微调效果很好！因此，你可以从一个已经训练好的基础模型开始。
- en: In fine tuning, you take a model pre-trained on a large data source (e.g. ImageNet
    for images or BooksCorpus for NLP), then continue training it on your dataset
    to adapt the mode for your task. This requires much less additional data and epochs
    than training from random weights.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调中，你会采用一个在大数据源上预训练的模型（例如，图像的 ImageNet 或 NLP 的 BooksCorpus），然后在你的数据集上继续训练，以使模型适应你的任务。这比从随机权重开始训练需要的额外数据和周期要少得多。
- en: Fine Tuning In HuggingFace
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调在 HuggingFace 中
- en: 'HuggingFace (HF) has a lot of built-in functions that allow us to fine tune
    a pre-trained model in few lines of codes. The major steps are as following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace (HF) 提供了许多内置功能，让我们可以用少量代码微调预训练模型。主要步骤如下：
- en: load the pre-trained model
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练模型
- en: load the pre-trained tokenizer
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练的 tokenizer
- en: load the dataset you want to use for fine tuning
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载你想用于微调的数据集
- en: tokenize above dataset using the tokenizer
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 tokenizer 对上述数据集进行分词
- en: use Trainer object to train the pre-trained model on the tokenized dataset
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Trainer 对象在分词数据集上训练预训练模型
- en: Let’s see each step in code. We will intentionally leave out many details to
    just give an overview of how the overall structure look.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看代码中的每个步骤。我们将故意省略许多细节，以便给出整体结构的概览。
- en: '1) HF: Load the pre-trained model'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1) HF：加载预训练模型
- en: 'For example, to load the `bert-base` model, write the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要加载`bert-base`模型，请编写以下内容：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Visit the full list of model names at [*https://huggingface.co/models*](https://huggingface.co/models)
    .
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 访问模型名称的完整列表请见[*https://huggingface.co/models*](https://huggingface.co/models)。
- en: '2) HF: Load the pre-trained tokenizer'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2) HF：加载预训练的 tokenizer
- en: 'Often the tokenizer name is the same as model name:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，tokenizer 的名称与模型名称相同：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '3) HF: load a dataset for fine tuning'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3) HF：加载用于微调的数据集
- en: Here, we are loading the squad dataset in non-streaming fashion.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们以非流式方式加载 squad 数据集。
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '4) HF: tokenize the dataset'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4) HF：对数据集进行分词
- en: We defined a tokenized function and pass samples in batch to it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个分词函数，并将样本批量传递给它。
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '5) HF: trainer to train the model'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5) HF：训练模型的 trainer
- en: And last but not least, is the trainer object that is in charge of training
    the model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是负责训练模型的 Trainer 对象。
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Putting these 5 steps together we can fine tune a model from HuggingFace.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将这五个步骤结合起来，我们可以从 HuggingFace 微调模型。
- en: 'Before we do so, we start with one detail that we left out: **Input Arguments**.
    As you see above, there are many hyper-parameters involved in the code e.g. `model_name`,
    `dataset_name`, `training_args`, etc. These hyper-parameters are input arguments
    that we should specify before taking any step above. Let’s see what all arguments
    exist in HuggingFace.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，我们从一个被忽略的细节开始：**输入参数**。如上所示，代码中涉及许多超参数，例如 `model_name`、`dataset_name`、`training_args`
    等。这些超参数是我们在进行上述任何步骤之前应指定的输入参数。让我们看看 HuggingFace 中有哪些参数。
- en: Group of Arguments in HuggingFace
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HuggingFace 中的参数组
- en: 'There are usually three or four different argument groups:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通常有三到四个不同的参数组：
- en: '`ModelArguments`: Arguments related to the model/config/tokenizer we are going
    to fine-tune, or train from scratch.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ModelArguments`：与我们将微调或从头训练的模型/配置/tokenizer 相关的参数。'
- en: '`DataTrainingArguments`: Arguments related to the training data and evaluation
    data.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataTrainingArguments`：与训练数据和评估数据相关的参数。'
- en: '`TrainingArguments` : Arguments related to the training hyper-parameters and
    configurations.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`TrainingArguments`：与训练超参数和配置相关的参数。'
- en: '`PEFTArguments` : Arguments related to parameter efficient training of the
    model. This one is optional, you may choose not to train the model in parameter
    efficient mode and so you won’t have this group of arguments.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`PEFTArguments`：与模型的参数高效训练相关的参数。这是可选的，你可以选择不以参数高效模式训练模型，这样你将没有这个参数组。'
- en: 'You might have seen each of these defined as a `dataclass` before. For example,
    `ModelArguments` is as following : (it’s a bit long, but it is just all hyper-parameters
    related to modeling)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能之前见过每一个参数都被定义为 `dataclass`。例如，`ModelArguments` 如下：（有点长，但只是所有与建模相关的超参数）
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In order to initialize these and parse them, we use `HFArgumentParser` .
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了初始化这些并解析它们，我们使用 `HFArgumentParser`。
- en: HfArgumentParser
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HfArgumentParser
- en: '`HfArgumentParser` is the **HuggingFace argument parser**. You see it defined
    as following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`HfArgumentParser` 是**HuggingFace 参数解析器**。你可以看到它的定义如下：'
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Let’s see how we can pass arguments**: You can pass arguments in three ways:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**让我们看看如何传递参数**：你可以通过三种方式传递参数：'
- en: '**1) via command line**: Open a terminal and pass arguments in a command line.
    Here is an example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**1) 通过命令行**：打开终端并在命令行中传递参数。以下是一个示例：'
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then read it as following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按照以下方式读取：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The parser will automatically assign these arguments to the right groups.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器将自动将这些参数分配到正确的组。
- en: '**2) via passing a json file**: The JSON file should have keys corresponding
    to the argument names. An example of a json file is this:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**2) 通过传递 json 文件**：JSON 文件应包含与参数名称对应的键。以下是一个 json 文件的示例：'
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Pay attention to not put a `,` after the last line, otherwise you’ll get an
    error. Call the `train.py` script as following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意不要在最后一行后面加`,`，否则会出错。按如下方式调用`train.py`脚本：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And receive arguments in the `train.py` as following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 并在`train.py`中接收参数如下：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**3) passing via a dictionary:**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**3) 通过字典传递：**'
- en: 'Of course if you can pass via a json file, you can pass the arguments via a
    dictionary too:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果你可以通过json文件传递，你也可以通过字典传递参数：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The point is that the `HfArgumentParser` allows flexible passing of arguments.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要点是`HfArgumentParser`允许灵活传递参数。
- en: The overall code
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 整体代码
- en: 'Let’s put all five steps above together:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 将以上五个步骤综合起来：
- en: First import all necessary libraries.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先导入所有必要的库。
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Second, we define three groups of arguments. Note we have already imported `TrainingArguments`.
    So no need to define that. First, we define `ModelArguments` .
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们定义了三组参数。注意我们已经导入了`TrainingArguments`。所以不需要再定义它。首先，我们定义`ModelArguments`。
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And `DataTrainingArguments` class:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以及`DataTrainingArguments`类：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We then call the `HfArgumentParser` to parse the input arguments into each
    class:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后调用`HfArgumentParser`将输入参数解析到各个类中：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'But for our usecase, let’s define them as a dictionary and pass them:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但为了我们的使用场景，我们将它们定义为一个字典并传递：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, lets print `model_args, data_args, training_args` :'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打印`model_args, data_args, training_args`：
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'training_args is a long list of attributes. I print it here partially:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`training_args`是一个长属性列表。我在这里部分打印出来：'
- en: '[PRE20]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Next, we load a dataset.** Here, we are loading `glue — cola` dataset which
    is the “Corpus of Linguistic Acceptability”. It consists of English acceptability
    judgments drawn from books and journal articles on linguistic theory. Each example
    is a sequence of words annotated with whether it is a grammatical English sentence
    [1].'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**接下来，我们加载一个数据集。** 在这里，我们加载的是`glue — cola`数据集，即“语言接受性语料库”。它包含了从语言学理论的书籍和期刊文章中提取的英文接受性判断。每个示例是一个带有语法标注的单词序列[1]。'
- en: '[PRE21]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The dataset has three splits: train — validation — test. Notice that we have
    8551 data points in train. Also notice this is a supervised dataset since it has
    a label; but in this post we are not going to use the label and we will use this
    dataset in *self-supervised manner* (masking tokens) to fine tune the model.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集有三个分割：train — validation — test。注意我们在训练集中有8551个数据点。同时注意这是一个监督数据集，因为它有一个标签；但在这篇文章中我们不会使用标签，我们将以*自监督的方式*（掩蔽标记）使用此数据集来微调模型。
- en: '![](../Images/e73f842838f7952533da5c1018f0415d.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e73f842838f7952533da5c1018f0415d.png)'
- en: Image by author
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Let’s look at one example in train:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个训练中的例子：
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/8d58fba321e1cfd86daadab0a57266b9.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d58fba321e1cfd86daadab0a57266b9.png)'
- en: Image by author
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: We see the `label = 1` because the `sentence` is grammatically correct. Now
    let’s see another example with `label = 0`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到`label = 1`因为`sentence`在语法上是正确的。现在让我们看另一个`label = 0`的例子。
- en: '![](../Images/170694c84b5d4e9179647c24e434d9e9.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/170694c84b5d4e9179647c24e434d9e9.png)'
- en: Image by author
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: This sentence “They drank the pub” is grammatically incorrect and so the `label
    = 0`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这句话“They drank the pub”在语法上是不正确的，所以`label = 0`。
- en: '**Okay, enough of the data. Let’s load the tokenizer**:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**好了，数据部分讲解完了。让我们加载分词器**：'
- en: '[PRE23]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Printing the tokenizer shows the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 打印分词器显示如下：
- en: '[PRE24]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Next, load the model** and check the model `embedding_size` ; this shows
    the embedding dimensions for model and if it matches the tokenizer vocab size.
    If they do not match, we resize the model embedding matrix.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**接下来，加载模型**并检查模型的`embedding_size`；这显示了模型的嵌入维度，并且如果它与分词器的词汇表大小匹配。如果不匹配，我们将调整模型的嵌入矩阵。'
- en: '[PRE25]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In our case, since we have not added any special tokens to the tokenizer, they
    match and both are *30522*. So we are good.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，由于我们没有向分词器添加任何特殊标记，它们匹配且都是*30522*。所以没问题。
- en: '**Next, lets set the context length and tell tokenizer which column from the
    data to read:**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**接下来，设置上下文长度并告知分词器从数据中读取哪一列：**'
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Next, write the tokenization function:**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**接下来，编写分词函数：**'
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s look at the tokenized data:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看分词后的数据：
- en: '![](../Images/3e407dd03e294dd9ac86a4463812430b.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e407dd03e294dd9ac86a4463812430b.png)'
- en: Image by author
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'And the first data point looks like this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数据点如下所示：
- en: '![](../Images/ef0c44611f8b6bb92babbd2d6fc01925.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef0c44611f8b6bb92babbd2d6fc01925.png)'
- en: Image by author
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'And the 10th data point looks like this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第10个数据点如下所示：
- en: '![](../Images/5d15b1454036edde4f534f77f1cee1f4.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d15b1454036edde4f534f77f1cee1f4.png)'
- en: Image by author
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Notice that naturally different data points have different sizes (input_ids
    have different length in different data points). Later, data collator will pad
    or truncate them to make sure they are of the same length.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，不同的数据点自然具有不同的大小（input_ids在不同数据点中的长度不同）。稍后，数据收集器会对其进行填充或截断，以确保它们的长度相同。
- en: '**And the packing function**:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**以及打包函数**：'
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The data after packing looks as following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 打包后的数据如下所示：
- en: '![](../Images/b07031fd286ce472bd50e55d7e518127.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b07031fd286ce472bd50e55d7e518127.png)'
- en: Image by author
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: You see that size of data has decrease from 8551 to 185 rows. This is because
    packing forms sequences of length `context_length=512` . So after `group_texts`
    function we have 185 data point in train each of length `512` tokens.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到数据的大小从8551行减少到185行。这是因为打包形成了长度为`context_length=512`的序列。因此，在`group_texts`函数之后，我们在训练集中有185个数据点，每个数据点的长度为`512`个tokens。
- en: 'Now that data is tokenized and packed into batches of context length, let’s
    take our train and eval dataset:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经被标记化并打包成上下文长度的批次，让我们来看看我们的训练集和评估集：
- en: '[PRE29]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Last, we define the data collator and the trainer object. If you are not familiar
    with data collator take a look at [this post](https://medium.com/towards-data-science/data-collators-in-huggingface-a0c76db798d2).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了数据收集器和训练器对象。如果你不熟悉数据收集器，可以查看[这篇文章](https://medium.com/towards-data-science/data-collators-in-huggingface-a0c76db798d2)。
- en: '[](/data-collators-in-huggingface-a0c76db798d2?source=post_page-----2f3d1e60ad5a--------------------------------)
    [## Data Collators in HuggingFace'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-collators-in-huggingface-a0c76db798d2?source=post_page-----2f3d1e60ad5a--------------------------------)
    [## HuggingFace中的数据收集器'
- en: What they are and what they do
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它们是什么以及它们的作用
- en: towardsdatascience.com](/data-collators-in-huggingface-a0c76db798d2?source=post_page-----2f3d1e60ad5a--------------------------------)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/data-collators-in-huggingface-a0c76db798d2?source=post_page-----2f3d1e60ad5a--------------------------------)
- en: Just know that data collator is in charge of ensuring all sequences in a batch
    have same length, so it does truncation and padding.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 只需知道数据收集器负责确保批次中的所有序列具有相同的长度，因此它会进行截断和填充。
- en: '[PRE31]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let’s train the model and see the results:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练模型并查看结果：
- en: '[PRE32]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](../Images/e17c3e68c92fc39b615b6234b33392aa.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e17c3e68c92fc39b615b6234b33392aa.png)'
- en: Image by author
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'You see we have the validation result and the metric (accuracy as we chose)
    in every 100 steps, because in the `data_args` we set the following:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们每100步都有验证结果和指标（我们选择的是准确率），这是因为在`data_args`中我们设置了以下内容：
- en: '[PRE33]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Conclusion
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, we looked at a brief version of [run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py)
    script which is an entry point into HuggingFace. We saw the fundamental steps
    into fine-tuning (or training from scratch) a model on HuggingFace. If you have
    had very little exposure to HuggingFace, this post can help you learning the basic
    steps.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们简要介绍了[run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py)脚本，这个脚本是进入HuggingFace的入口点。我们看到了在HuggingFace上微调（或从头开始训练）模型的基本步骤。如果你对HuggingFace了解甚少，这篇文章可以帮助你了解基本步骤。
- en: References
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://huggingface.co/datasets/glue](https://huggingface.co/datasets/glue)'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/datasets/glue](https://huggingface.co/datasets/glue)'
