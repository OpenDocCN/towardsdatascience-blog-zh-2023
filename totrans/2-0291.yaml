- en: An Entry Point Into HuggingFace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-entry-point-into-huggingface-2f3d1e60ad5a](https://towardsdatascience.com/an-entry-point-into-huggingface-2f3d1e60ad5a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A step by step guide of basics for beginners
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----2f3d1e60ad5a--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----2f3d1e60ad5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f3d1e60ad5a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f3d1e60ad5a--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----2f3d1e60ad5a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f3d1e60ad5a--------------------------------)
    ·15 min read·Nov 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6112804c83872912af719222f58fd92.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [unsplash](https://unsplash.com/photos/person-holding-ac-receiver-_J3oTl6acVg)
  prefs: []
  type: TYPE_NORMAL
- en: HuggingFace can be complex and complicated if you don’t know where to start
    to learn it. One entry point into HuggingFace repository are [run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py)
    and [run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py)
    scripts.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will go through [run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py)
    script. This script picks a masked language model from HuggingFace and fine tune
    it on a dataset (or train it from scratch). If you are a beginner and you have
    very little exposure to HuggingFace codes, this post will help you understand
    the basics.
  prefs: []
  type: TYPE_NORMAL
- en: We will pick a masked language model and load a dataset from HuggingFace and
    fine tune the model on the dataset. At the end, we will evaluate the model. This
    is all for the sake of understanding the code structure, so our focus is not on
    any specific usecase.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Few Words About Fine Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-tuning is a common technique in deep learning to take a pre-trained neural
    network model and tweak it to better suit a new dataset or task.
  prefs: []
  type: TYPE_NORMAL
- en: Fine tuning works well when your dataset is not large enough to train a deep
    model from scratch! So you start from an already learned base model.
  prefs: []
  type: TYPE_NORMAL
- en: In fine tuning, you take a model pre-trained on a large data source (e.g. ImageNet
    for images or BooksCorpus for NLP), then continue training it on your dataset
    to adapt the mode for your task. This requires much less additional data and epochs
    than training from random weights.
  prefs: []
  type: TYPE_NORMAL
- en: Fine Tuning In HuggingFace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HuggingFace (HF) has a lot of built-in functions that allow us to fine tune
    a pre-trained model in few lines of codes. The major steps are as following:'
  prefs: []
  type: TYPE_NORMAL
- en: load the pre-trained model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: load the pre-trained tokenizer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: load the dataset you want to use for fine tuning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: tokenize above dataset using the tokenizer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: use Trainer object to train the pre-trained model on the tokenized dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s see each step in code. We will intentionally leave out many details to
    just give an overview of how the overall structure look.
  prefs: []
  type: TYPE_NORMAL
- en: '1) HF: Load the pre-trained model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For example, to load the `bert-base` model, write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Visit the full list of model names at [*https://huggingface.co/models*](https://huggingface.co/models)
    .
  prefs: []
  type: TYPE_NORMAL
- en: '2) HF: Load the pre-trained tokenizer'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Often the tokenizer name is the same as model name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '3) HF: load a dataset for fine tuning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we are loading the squad dataset in non-streaming fashion.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '4) HF: tokenize the dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We defined a tokenized function and pass samples in batch to it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '5) HF: trainer to train the model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: And last but not least, is the trainer object that is in charge of training
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Putting these 5 steps together we can fine tune a model from HuggingFace.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Before we do so, we start with one detail that we left out: **Input Arguments**.
    As you see above, there are many hyper-parameters involved in the code e.g. `model_name`,
    `dataset_name`, `training_args`, etc. These hyper-parameters are input arguments
    that we should specify before taking any step above. Let’s see what all arguments
    exist in HuggingFace.'
  prefs: []
  type: TYPE_NORMAL
- en: Group of Arguments in HuggingFace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are usually three or four different argument groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ModelArguments`: Arguments related to the model/config/tokenizer we are going
    to fine-tune, or train from scratch.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataTrainingArguments`: Arguments related to the training data and evaluation
    data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`TrainingArguments` : Arguments related to the training hyper-parameters and
    configurations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`PEFTArguments` : Arguments related to parameter efficient training of the
    model. This one is optional, you may choose not to train the model in parameter
    efficient mode and so you won’t have this group of arguments.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You might have seen each of these defined as a `dataclass` before. For example,
    `ModelArguments` is as following : (it’s a bit long, but it is just all hyper-parameters
    related to modeling)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In order to initialize these and parse them, we use `HFArgumentParser` .
  prefs: []
  type: TYPE_NORMAL
- en: HfArgumentParser
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`HfArgumentParser` is the **HuggingFace argument parser**. You see it defined
    as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Let’s see how we can pass arguments**: You can pass arguments in three ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1) via command line**: Open a terminal and pass arguments in a command line.
    Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then read it as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The parser will automatically assign these arguments to the right groups.
  prefs: []
  type: TYPE_NORMAL
- en: '**2) via passing a json file**: The JSON file should have keys corresponding
    to the argument names. An example of a json file is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Pay attention to not put a `,` after the last line, otherwise you’ll get an
    error. Call the `train.py` script as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And receive arguments in the `train.py` as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**3) passing via a dictionary:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course if you can pass via a json file, you can pass the arguments via a
    dictionary too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The point is that the `HfArgumentParser` allows flexible passing of arguments.
  prefs: []
  type: TYPE_NORMAL
- en: The overall code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s put all five steps above together:'
  prefs: []
  type: TYPE_NORMAL
- en: First import all necessary libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Second, we define three groups of arguments. Note we have already imported `TrainingArguments`.
    So no need to define that. First, we define `ModelArguments` .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'And `DataTrainingArguments` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We then call the `HfArgumentParser` to parse the input arguments into each
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'But for our usecase, let’s define them as a dictionary and pass them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, lets print `model_args, data_args, training_args` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'training_args is a long list of attributes. I print it here partially:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Next, we load a dataset.** Here, we are loading `glue — cola` dataset which
    is the “Corpus of Linguistic Acceptability”. It consists of English acceptability
    judgments drawn from books and journal articles on linguistic theory. Each example
    is a sequence of words annotated with whether it is a grammatical English sentence
    [1].'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset has three splits: train — validation — test. Notice that we have
    8551 data points in train. Also notice this is a supervised dataset since it has
    a label; but in this post we are not going to use the label and we will use this
    dataset in *self-supervised manner* (masking tokens) to fine tune the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e73f842838f7952533da5c1018f0415d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at one example in train:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8d58fba321e1cfd86daadab0a57266b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We see the `label = 1` because the `sentence` is grammatically correct. Now
    let’s see another example with `label = 0`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/170694c84b5d4e9179647c24e434d9e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: This sentence “They drank the pub” is grammatically incorrect and so the `label
    = 0`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Okay, enough of the data. Let’s load the tokenizer**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Printing the tokenizer shows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**Next, load the model** and check the model `embedding_size` ; this shows
    the embedding dimensions for model and if it matches the tokenizer vocab size.
    If they do not match, we resize the model embedding matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In our case, since we have not added any special tokens to the tokenizer, they
    match and both are *30522*. So we are good.
  prefs: []
  type: TYPE_NORMAL
- en: '**Next, lets set the context length and tell tokenizer which column from the
    data to read:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '**Next, write the tokenization function:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the tokenized data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e407dd03e294dd9ac86a4463812430b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'And the first data point looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef0c44611f8b6bb92babbd2d6fc01925.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'And the 10th data point looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d15b1454036edde4f534f77f1cee1f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Notice that naturally different data points have different sizes (input_ids
    have different length in different data points). Later, data collator will pad
    or truncate them to make sure they are of the same length.
  prefs: []
  type: TYPE_NORMAL
- en: '**And the packing function**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The data after packing looks as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b07031fd286ce472bd50e55d7e518127.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: You see that size of data has decrease from 8551 to 185 rows. This is because
    packing forms sequences of length `context_length=512` . So after `group_texts`
    function we have 185 data point in train each of length `512` tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that data is tokenized and packed into batches of context length, let’s
    take our train and eval dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Last, we define the data collator and the trainer object. If you are not familiar
    with data collator take a look at [this post](https://medium.com/towards-data-science/data-collators-in-huggingface-a0c76db798d2).
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-collators-in-huggingface-a0c76db798d2?source=post_page-----2f3d1e60ad5a--------------------------------)
    [## Data Collators in HuggingFace'
  prefs: []
  type: TYPE_NORMAL
- en: What they are and what they do
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-collators-in-huggingface-a0c76db798d2?source=post_page-----2f3d1e60ad5a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Just know that data collator is in charge of ensuring all sequences in a batch
    have same length, so it does truncation and padding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s train the model and see the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e17c3e68c92fc39b615b6234b33392aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'You see we have the validation result and the metric (accuracy as we chose)
    in every 100 steps, because in the `data_args` we set the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we looked at a brief version of [run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py)
    script which is an entry point into HuggingFace. We saw the fundamental steps
    into fine-tuning (or training from scratch) a model on HuggingFace. If you have
    had very little exposure to HuggingFace, this post can help you learning the basic
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://huggingface.co/datasets/glue](https://huggingface.co/datasets/glue)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
