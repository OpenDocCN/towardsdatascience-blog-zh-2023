["```py\n\"\"\"\nPlotting positional encoding for each index.\nA positional encoding for a single token would be a horizontal row in the image\n\ninspired by https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#these would be defined based on the vector embedding and sequence\nsequence_length = 512\nembedding_dimension = 1000\n\n#generating a positional encodings\ndef gen_positional_encodings(sequence_length, embedding_dimension):\n    #creating an empty placeholder\n    positional_encodings = np.zeros((sequence_length, embedding_dimension))\n\n    #itterating over each element in the sequence\n    for i in range(sequence_length):\n\n        #calculating the values of this sequences position vector\n        #as defined in section 3.5 of the attention is all you need\n        #paper: https://arxiv.org/pdf/1706.03762.pdf\n        for j in np.arange(int(embedding_dimension/2)):\n            denominator = np.power(sequence_length, 2*j/embedding_dimension)\n            positional_encodings[i, 2*j] = np.sin(i/denominator)\n            positional_encodings[i, 2*j+1] = np.cos(i/denominator)\n\n    return positional_encodings\n\n#rendering\nfig, ax = plt.subplots(figsize=(15,5))\nax.set_ylabel('Sequence Index')\nax.set_xlabel('Positional Encoding')\ncax = ax.matshow(gen_positional_encodings(sequence_length, embedding_dimension))\nfig.colorbar(cax, pad=0.01)\n```", "```py\n\"\"\"\nRendering out a few individual examples\n\ninspired by https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n\"\"\"\npositional_encodings = gen_positional_encodings(100, 50)\nfig = plt.figure(figsize=(15, 4))    \nfor i in range(4):\n    ax = plt.subplot(141 + i)\n    idx = i*10\n    plt.plot(positional_encodings[:,idx])\n    ax.set_title(f'positional encoding {idx}')\nplt.show()\n```"]