- en: Introduction of Four Types of Item Similarity Measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/introduction-of-four-types-of-item-similarity-measures-e0aea70da335](https://towardsdatascience.com/introduction-of-four-types-of-item-similarity-measures-e0aea70da335)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data Mining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Covers how to choose the similarity measure when item embeddings are available
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jhwang1992m?source=post_page-----e0aea70da335--------------------------------)[![Jiahui
    Wang](../Images/91350774d661092f429d1b0591af95f4.png)](https://medium.com/@jhwang1992m?source=post_page-----e0aea70da335--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e0aea70da335--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e0aea70da335--------------------------------)
    [Jiahui Wang](https://medium.com/@jhwang1992m?source=post_page-----e0aea70da335--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e0aea70da335--------------------------------)
    ·5 min read·Feb 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b75c9c4e4fd412a9e5b9ad5ec3a51357.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [James Yarema](https://unsplash.com/@jamesyarema?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Recommendation algorithms are applied everywhere in our daily lives, from the
    order of videos that appear on our Youtube homepage, to the arrangement of item
    shelves in Walmart. These algorithms learn the potential preferences of the users
    and recommend the most relevant items to the users. To achieve this, we need to
    describe the users and items in terms of numerical vectors, so that the algorithms
    can measure the user similarity and item similarity to make further recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common methods to learn the numerical vector representations
    of the users and items: [content-based filtering and collaborative filtering](https://developers.google.com/machine-learning/recommendation/overview/candidate-generation).
    In content-based filtering, vector representations of the items are learnt by
    manually crafting features based on the expert knowledge of the items. Let’s take
    the mobile apps for example, each app on Google Play will have an app name, a
    category and some text description provided by the developers. Then expert knowledge
    can be applied to extract features from these information to build vector representation
    of the apps. The advantage of content-based filtering is that it does not rely
    on existing user-item data. In contrast, collaborative filtering does not require
    expert knowledge of the items and purely relies on the existing user-item data.
    When it comes to collaborative filtering, data speaks for itself. As long as we
    have enough user-item data, user and item representations can be extracted by
    using matrix factorization.'
  prefs: []
  type: TYPE_NORMAL
- en: After getting the numerical vector representations of the items, the next task
    is to measure similarities between the items to make recommendation. In this post,
    we will focus on the various similarity measures and discuss how to choose the
    right similarity measure. Meanwhile, Spark code examples on how to calculate the
    similarities will be provided, as recommendation algorithms are usually applied
    on big data.
  prefs: []
  type: TYPE_NORMAL
- en: Without further ado, let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction on the Example Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we will study the app similarities by analysing the installed
    app lists of a large number of users. Suppose we have m users with a total number
    of n apps installed, then we can construct a user-item matrix of m×n. Unlike the
    collaborative filtering method, which will learn a low dimensional representation
    of the users and the items, in this example, we will directly use the m dimensional
    representation for each app. Collaborative filtering learns low dimensional representations
    of the users and items to do quick calculation for real-time recommendation tasks,
    but here we are focused on demonstrate how to calculate similarities after we
    get the numerical vector representations. Thus, we simplify the process to get
    the item vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for demonstration purpose, we will only use a dummy example of
    4 users and 4 apps, which can be generated with the code:'
  prefs: []
  type: TYPE_NORMAL
- en: Code to generate the installed app list example
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9977b1853375ca690ab610488ecafc8.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of the installed app list
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataframe is then converted into rows of install list for each user:'
  prefs: []
  type: TYPE_NORMAL
- en: Code to convert the installed app list to userid as key
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4b7435c0d7504638b279f24c9339497.png)'
  prefs: []
  type: TYPE_IMG
- en: Results of transformed installed app list
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Various Similarity Measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The calculation of the various similarity measures relies on two common statistical
    results: cosine similarity of the app pairs and Euclidean magnitudes of the app
    vectors, which can be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: Code of generating cosine similarity and Euclidean magnitudes of app pairs
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0911d966499749e0333c10a595ba293.png)'
  prefs: []
  type: TYPE_IMG
- en: Result of cosine similarity and Euclidean magnitudes of app pairs
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Cosine Similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cosine similarity measures the angle between two vectors. It can be easily calculated
    by calling the Spark’s native function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76408abd15063f085d7bba3ff4725c09.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation of cosine similarity
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Dot Product
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dot product is cosine similarity multiplied by the euclidean magnitudes of the
    two vectors. It can be understood as the projection of one vector on the other
    vector. The euclidean magnitude of a vector measures the popularity of the app
    among the users. When more users install an app, the euclidean magnitude of this
    app will become larger. Thus, unlike cosine similarity, dot product is affected
    by the popularity of the two apps. Dot product of a popular app and an unpopular
    app will be small, as if we imagine when the short vector is projected on the
    long vector, the projection length won’t be long. Thus, on top of the cosine similarity,
    dot product takes consideration of the popularity of the two apps.
  prefs: []
  type: TYPE_NORMAL
- en: Code of generating dot product
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Jaccard Similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jaccard similarity is the intersaction of the user sets divided by the union
    of the user sets who install the two apps. The intersaction of the user sets measures
    the similarity of the two apps, while the union of the user sets measures the
    diversity of the two apps. In this sense, we can consider Jaccard similarity as
    a measurement that is normalized by the diversity of the two apps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b60d05bf6691575a86e12200379a647.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of Jaccard similarity
  prefs: []
  type: TYPE_NORMAL
- en: 'By using the euclidean magnitudes of two vectors and the calculated dot product,
    Jaccard similarity can be easily calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: Code to generate Jaccard similarity
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Conditional Probability Lift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Conditional probability lift measures to what extent the installment of app
    B helps with the installment of app A.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a88cade78c5db93568968a1d8662a78d.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation of conditional probability lift
  prefs: []
  type: TYPE_NORMAL
- en: By using the Euclidean magnitudes of the two vectors, conditional probability
    can be easily calculated.
  prefs: []
  type: TYPE_NORMAL
- en: Code to calculate conditional probability lift
  prefs: []
  type: TYPE_NORMAL
- en: After mapping back the app_id, the final results can be obtained.
  prefs: []
  type: TYPE_NORMAL
- en: Code to merge the four similartiy results
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67f81211645142cec1034a77cb5c80f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Merged similarity results
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we covered four types of similarity measures on the embedding,
    namely cosine similarity, dot product, Jaccard similarity, and conditional probability
    lift. Depending on the definition of embedding similarity, we can choose to use
    the most appropriate measure.
  prefs: []
  type: TYPE_NORMAL
