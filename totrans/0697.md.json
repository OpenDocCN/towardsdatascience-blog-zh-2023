["```py\nfrom keras.models import Sequential\nfrom keras.layers import (Dense,\n                          LSTM,\n                          TimeDistributed,\n                          RepeatVector,\n                          Dropout)\n\n# Number of variables in the time series. \n# 1 means the time series is univariate\nN_FEATURES = 1\n# Number of lags in the auto-regressive model\nN_LAGS = 24\n# Number of future steps to be predicted\nHORIZON = 12\n\n# 'Sequential' instance is used to create a linear stack of layers \n# ... each layer feeds into the next one.\nmodel = Sequential()\n# Adding an LSTM layer with 32 units and relu activation\nmodel.add(LSTM(32, activation='relu', input_shape=(N_LAGS, N_FEATURES)))\n# Using dropout to avoid overfitting \nmodel.add(Dropout(.2))\n# Repeating the input vector HORIZON times to match the shape of the output.\nmodel.add(RepeatVector(HORIZON))\n# Another LSTM layer, this time with 16 units\n# Also returning the output of each time step (return_sequences=True)\nmodel.add(LSTM(16, activation='relu', return_sequences=True))\n# Using dropout again with 0.2 dropout rate\nmodel.add(Dropout(.2))\n# Adding a standard fully connected neural network layer\n# And distributing the layer to each time step\nmodel.add(TimeDistributed(Dense(N_FEATURES)))\n\n# Compiling the model using ADAM and setting the objective to minimize MSE\nmodel.compile(optimizer='adam', loss='mse')\n```", "```py\nimport pandas as pd\n\n# https://github.com/vcerqueira/blog/tree/main/data\ndata = pd.read_csv('data/daily_energy_demand.csv', \n                   parse_dates=['Datetime'], \n                   index_col='Datetime')\n\nprint(data.head())\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\n# leaving last 20% of observations for testing\ntrain, test = train_test_split(data, test_size=0.2, shuffle=False)\n\n# computing the average of each series in the training set\nmean_by_series = train.mean()\n\n# mean-scaling: dividing each series by its mean value\ntrain_scaled = train / mean_by_series\ntest_scaled = test / mean_by_series\n```", "```py\nimport numpy as np\n\nclass LogTransformation:\n\n    @staticmethod\n    def transform(x):\n        xt = np.sign(x) * np.log(np.abs(x) + 1)\n\n        return xt\n\n    @staticmethod\n    def inverse_transform(xt):\n        x = np.sign(xt) * (np.exp(np.abs(xt)) - 1)\n\n        return x\n\n# log transformation\ntrain_scaled_log = LogTransformation.transform(train_scaled)\ntest_scaled_log = LogTransformation.transform(test_scaled)\n```", "```py\n# src module here: https://github.com/vcerqueira/blog/tree/main/src\nfrom src.tde import time_delay_embedding\n\nN_FEATURES = 1 # time series is univariate\nN_LAGS = 3 # number of lags\nHORIZON = 2 # forecasting horizon\n\n# transforming time series for supervised learning\ntrain_by_series, test_by_series = {}, {}\n# iterating over each time series\nfor col in data:\n    train_series = train_scaled_log[col]\n    test_series = test_scaled_log[col]\n\n    train_series.name = 'Series'\n    test_series.name = 'Series'\n\n    # creating observations using a sliding window method\n    train_df = time_delay_embedding(train_series, n_lags=N_LAGS, horizon=HORIZON)\n    test_df = time_delay_embedding(test_series, n_lags=N_LAGS, horizon=HORIZON)\n\n    train_by_series[col] = train_df\n    test_by_series[col] = test_df\n```", "```py\ntrain_df = pd.concat(train_by_series, axis=0)\n\nprint(train_df)\n```", "```py\n# defining target (Y) and explanatory variables (X)\npredictor_variables = train_df.columns.str.contains('\\(t\\-|\\(t\\)')\ntarget_variables = train_df.columns.str.contains('\\(t\\+')\nX_tr = train_df.iloc[:, predictor_variables]\nY_tr = train_df.iloc[:, target_variables]\n\n# transforming the data from matrix into a 3-d format for deep learning\nX_tr_3d = from_matrix_to_3d(X_tr)\nY_tr_3d = from_matrix_to_3d(Y_tr)\n\n# defining the neural network\nmodel = Sequential()\nmodel.add(LSTM(32, activation='relu', input_shape=(N_LAGS, N_FEATURES)))\nmodel.add(Dropout(.2))\nmodel.add(RepeatVector(HORIZON))\nmodel.add(LSTM(16, activation='relu', return_sequences=True))\nmodel.add(Dropout(.2))\nmodel.add(TimeDistributed(Dense(N_FEATURES)))\nmodel.compile(optimizer='adam', loss='mse')\n\n# spliting training into a development and validation set\nX_train, X_valid, Y_train, Y_valid = \\\n    train_test_split(X_tr_3d, Y_tr_3d, test_size=.2, shuffle=False)\n\n# training the neural network\nmodel.fit(X_train, Y_train, validation_data=(X_valid,Y_valid), epochs=100)\n```", "```py\nfrom keras.callbacks import ModelCheckpoint\n\nmodel_checkpoint = ModelCheckpoint(\n    filepath='best_model_weights.h5',\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True)\n\nmodel = Sequential()\nmodel.add(LSTM(32, activation='relu', input_shape=(N_LAGS, N_FEATURES)))\nmodel.add(Dropout(.2))\nmodel.add(RepeatVector(HORIZON))\nmodel.add(LSTM(16, activation='relu', return_sequences=True))\nmodel.add(Dropout(.2))\nmodel.add(TimeDistributed(Dense(N_FEATURES)))\nmodel.compile(optimizer='adam', loss='mse')\n\nhistory = model.fit(X_train, Y_train,\n                    epochs=300,\n                    validation_data=(X_valid,Y_valid),\n                    callbacks=[model_checkpoint])\n```", "```py\n# The best model weights are loaded into the model.\nmodel.load_weights('best_model_weights.h5')\n\n# Inference on DAYTON region\ntest_dayton = test_by_series['DAYTON']\n\n# spliting target variables from explanatory ones\nX_ts = test_df.iloc[:, predictor_variables]\nY_ts = test_df.iloc[:, target_variables]\nX_ts_3d = from_matrix_to_3d(X_ts)\n\n# predicting on normalized data\npreds = model.predict_on_batch(X_ts_3d)\npreds_df = from_3d_to_matrix(preds, Y_ts.columns)\n\n# reverting log transformation\npreds_df = LogTransformation.inverse_transform(preds_df)\n# reverting mean scaling\npreds_df *= mean_by_series['DAYTON']\n```"]