["```py\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\n# Generate sample data\ndata, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n\ndistortions = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters=i, random_state=0)\n    km.fit(data)\n    distortions.append(km.inertia_)\n\n# Plot the elbow\nplt.plot(range(1, 11), distortions, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal number of clusters')\nplt.show()\n```", "```py\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Generate a binary classification dataset.\nX, y = make_classification(n_samples=1000, n_classes=2, weights=[1,1], random_state=1)\n\n# Split into train/test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\n\n# Train a logistic regression model\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(X_train, y_train)\n\n# Predict probabilities\nprobs = model.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n\n# Compute the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n\n# Compute the Area Under the Receiver Operating Characteristic Curve (AUROC)\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate or (1 - Specifity)')\nplt.ylabel('True Positive Rate or (Sensitivity)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n```", "```py\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the data\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Compute PCA\npca = PCA(n_components=4)\nX_pca = pca.fit_transform(X)\n\n# Plot the explained variances\nfeatures = range(pca.n_components_)\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\nplt.bar(features, explained_variance, alpha=0.5,\n        align='center', label='individual explained variance')\nplt.step(features, explained_variance, where='mid',\n         label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()\n```", "```py\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Generate a binary classification dataset.\nX, y = make_classification(n_samples=5000, n_classes=2, weights=[1,1], random_state=1)\n\n# Split into train/test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\n\n# Train a logistic regression model\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(X_train, y_train)\n\n# Predict probabilities\nprobs = model.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n\n# Compute the precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_test, probs)\n# Compute the area under the precision-recall curve\npr_auc = auc(recall, precision)\n\nplt.plot(recall, precision, label='PRC curve (area = %0.2f)' % pr_auc)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n```"]