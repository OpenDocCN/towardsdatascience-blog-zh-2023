- en: Is This the Solution to P-Hacking?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/is-this-the-solution-to-p-hacking-a04e6ed2b6a7](https://towardsdatascience.com/is-this-the-solution-to-p-hacking-a04e6ed2b6a7)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/268cfbc4852390989c44eab83f520315.png)'
  prefs: []
  type: TYPE_IMG
- en: Is E the new P? Image created with Dall·E by the author.
  prefs: []
  type: TYPE_NORMAL
- en: E-values, a better alternative for p-values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://hennie-de-harder.medium.com/?source=post_page-----a04e6ed2b6a7--------------------------------)[![Hennie
    de Harder](../Images/3e4f2cccd6cb976ca3f8bf15597daea8.png)](https://hennie-de-harder.medium.com/?source=post_page-----a04e6ed2b6a7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a04e6ed2b6a7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a04e6ed2b6a7--------------------------------)
    [Hennie de Harder](https://hennie-de-harder.medium.com/?source=post_page-----a04e6ed2b6a7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a04e6ed2b6a7--------------------------------)
    ·11 min read·Nov 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '**In scientific research, the manipulation of data and peeking at results have
    been problems for as long as the field has existed. Researchers often aim for
    a significant p-value to get published, which can lead to the temptation of stopping
    data collection early or manipulating the data. This practice, known as p-hacking,
    was the focus of** [**my previous post**](/sneaky-science-data-dredging-exposed-26a445f00e5c)**.
    If researchers decide to deliberately change data values or fake complete datasets,
    there is not much we can do about it. However, for some instances of p-hacking,
    there might be a solution available!**'
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we dive into the topic of safe testing. Safe tests have some strong
    advantages over the old (current) way of hypothesis testing. For example, this
    method of testing allows for the combination of results from multiple studies.
    Another advantage is that you can stop the experiment optionally, at any time
    you like. To illustrate safe testing, we will use the R package [safestats](https://cran.r-project.org/package=safestats),
    developed by the researchers who proposed the theory. First, we will introduce
    e-values and explain the problem they can solve. E-values are already used by
    companies like Netflix and Amazon because of their benefits.
  prefs: []
  type: TYPE_NORMAL
- en: I will not delve into the proofs of the theory; instead, this post takes a more
    practical approach, showing how you can use e-values in your own tests. For proofs
    and a thorough explanation of safe testing, [the original paper](https://arxiv.org/pdf/1906.07801.pdf)
    is a good resource.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An Introduction to E-values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In hypothesis testing, which you can brush up on [here](/sneaky-science-data-dredging-exposed-26a445f00e5c),
    you assess whether to retain the null hypothesis or to accept the alternative.
    Usually, the p-value is used for this. If the p-value is smaller than the predetermined
    significance level, alpha, you accept the alternative hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'E-values function differently from p-values but are related. The easiest interpretation
    of e-values is like this: **Suppose you are gambling against the null hypothesis.
    You invest 1$, and the return value is equal to E$. If the e-value E is between
    0 and 1, you lose, and the null hypothesis holds true. On the other hand, if the
    e-value is higher than 1, you win! The null hypothesis loses the game.** A modest
    E of 1.1 implies limited evidence against the null, whereas a substantial E, say
    1000, denotes overwhelming evidence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some main points of e-values to be aware of:'
  prefs: []
  type: TYPE_NORMAL
- en: An e-value can take *any positive value*, and you can use e-values as an *alternative
    to p-values* in hypothesis testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An e-value E, is interpretable as a traditional p-value p, by the relation
    1/E = p. Beware: It will not give you the same result as a standard p-value, but
    you can *interpret* it like a p-value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In traditional tests, you have alpha, also known as the significance level.
    Often this value is equal to 0.05\. E-values work a bit different, and you can
    look at them as evidence against the null. The *higher* the e-value, the more
    evidence against the null.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At *any point in time (!)* you can stop data collection and draw a conclusion
    during the test if you are using e-values. This is known as *e-processes*, and
    the use of e-processes ensures validity under optional stopping and allows sequential
    updates of statistical evidence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fun fact: E-values are not as ‘new’ as you might think. The [first paper](https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=40194&option_lang=eng)
    on it was written in 1976\. The values were not called e-values at that time.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/48ac105bfaa884d6d5b54dea42c79dea.png)'
  prefs: []
  type: TYPE_IMG
- en: A researcher gambling against... a hypothesis?! Image created with Dall·E 3
    by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Why should I care about E-values?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That is a valid question. What is wrong with traditional p-values? Is there
    a need to replace them with e-values? Why learn something new if there is nothing
    wrong with the current way of testing?
  prefs: []
  type: TYPE_NORMAL
- en: Actually, there *is* something wrong with p-values. There is a ton of criticism
    on traditional p-values. Some statisticians (over 800) want to [abandon p-values
    completely](https://media.nature.com/original/magazine-assets/d41586-019-00857-9/d41586-019-00857-9.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s illustrate why with a classic example.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are a junior researcher for a pharmaceutical company. You need to
    test the efficacy of a medicine the company developed. You search for test candidates,
    and half of them receive the medicine, while the other half takes a placebo. You
    determine how many test candidates you need to be able to draw conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The experiment starts, and you struggle a bit finding new participants. You
    are under time pressure, and your boss asks on a regular basis, “Do you have the
    results for me? We want to ship this product to the market!” Because of the pressure,
    you decide to peek at the results and calculate the p-value, although you haven’t
    reached the minimum number of test candidates! Looking at the p-value, now there
    are two options:'
  prefs: []
  type: TYPE_NORMAL
- en: The p-value is *not significant*. This means you cannot prove that the medicine
    works. Obviously, you don’t share these results! You wait a bit longer, hoping
    the p-value will become significant…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes! You find a *significant* p-value! But what is your next step? Do you stop
    the experiment? Do you continue until you reach the correct number of test candidates?
    Do you share the results with your boss?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After you looked at the data once, it’s tempting to do it more often. You calculate
    the p-value, and sometimes it’s significant, sometimes it isn’t… It might seem
    innocent to do this, while in fact, you are sabotaging the process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d23cd150a88005a58fbdb7848f180271.png)'
  prefs: []
  type: TYPE_IMG
- en: Significant or not? Image created with Dall·E 3 by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Why is it wrong to *only look* at the data and the corresponding p-value a few
    times before the experiment has officially ended? One simple and intuitive reason
    is because *if you would have done something* with other results (e.g. if you
    find a significant p-value you stop the experiment), you are messing with the
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a theoretical perspective: **You violate the Type I error guarantee.**
    The Type I error guarantee refers to how certain you can be that you will not
    mistakenly reject a true null hypothesis (= find a significant result). It’s like
    a promise about how often you’ll cry wolf when there’s no wolf around. The risk
    of this happening is ≤ alpha. But only for one experiment! If you are looking
    at the data more often, you cannot trust this value anymore: the risk of a Type
    I error becomes higher.'
  prefs: []
  type: TYPE_NORMAL
- en: This relates to the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem).
    If you do multiple independent tests to proof the same hypothesis, you should
    correct the value of alpha to keep the risk of a Type I error low. There are different
    ways of fixing this, like [Bonferroni](https://en.wikipedia.org/wiki/Bonferroni_correction),
    [Tukey’s range test](https://en.wikipedia.org/wiki/Tukey%27s_range_test) or [Scheffé’s
    method](https://en.wikipedia.org/wiki/Scheffé%27s_method).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/edca1f35c6b04697fc55034e76058787.png)'
  prefs: []
  type: TYPE_IMG
- en: The family-wise error rate for multiple independent tests. For one tests it
    is equal to alpha. Note that for 10 tests, the error rate has increased to 40%,
    and for 60 tests, it’s 95%. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize: P-values can be used, but it can be tempting for researchers
    to look at the data before the sample size is reached. This is wrong and increases
    the risk of a Type I error. To guarantee the quality and robustness of an experiment,
    e-values are the better alternative. Because of characteristics of e-values, you
    don’t need to doubt these experiments (or at least less, a researcher can always
    decide to fabricate data 😢).'
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of using E-values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, we can use e-values in the same way as p-values. One major
    difference in that case is that large e-values are comparable with low p-values.
    Recall that 1/E = p. If you want to use e-values in the same way as p-values and
    you use a significance level of 0.05, you can reject the null hypothesis if an
    e-value is higher than 20 (1/0.05).
  prefs: []
  type: TYPE_NORMAL
- en: But of course, there are more use cases and benefits of e-values! If there are
    several experiments that test the same hypothesis, we can multiply the e-values
    for those tests to get a new e-value that can be used for testing. This can never
    be done for p-values. But for e-values, it works.
  prefs: []
  type: TYPE_NORMAL
- en: You can also look at the data and results during the experiment. If you want
    to stop with the test, because the results don’t look promising, that’s okay.
    Another possibility is to continue with a test if it does look promising.
  prefs: []
  type: TYPE_NORMAL
- en: We can also create *anytime valid confidence intervals* with e-values. What
    does this mean? It means that the confidence intervals will work for any sample
    size (so during the whole experiment). They will be a bit broader than a regular
    confidence interval, but the good thing is that you can trust them at anytime.
  prefs: []
  type: TYPE_NORMAL
- en: Usage of the safestats package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last part of the post, we get more practical. Let’s calculate our own
    e-values. For this, we use the R-package [safestats](https://cran.r-project.org/package=safestats).
    To install and load it, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The case we will solve is a classic one: We will compare different versions
    of a website. If a person buys, we log success (1), and if a person does not buy
    anything, we log failure (0). We show the old version of the website to 50% of
    the visitors (group A), and the new version of the website to the other 50% (group
    B). In this use case, wel will look at different things that can happen. It can
    happen that the null hypothesis is true (no difference between the website or
    the old version is better), and sometimes the alternative hypothesis is true (the
    new website is better).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in creating a safe test is creating the design objective. In
    this variable, you specify values for `alpha`, `beta` and `delta`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In many cases, `delta` is set to a higher number. But for comparing different
    versions of a website with a lot of traffic, it makes sense to set it small, because
    it’s easy to get many observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can recognize the values we chose, but the package also calculated the `nBlocksPlan`
    parameter. This is the number of data points (blocks) we need to observe, it’s
    based on the delta and beta parameter. Also check the decision rule, based on
    the value of `alpha`. If the `e-value` is greater than 20 (1 divided by 0.05),
    we reject the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Test case: Alternative Hypothesis is True'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s generate some fake data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9889bcf9882976a9955dc5c66e00e310.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution A and B for success probabilities 0.05 and 0.08, respectively.
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to perform our first safe test!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `e-value` is equal to 77658, which means we can reject the null hypothesis.
    Enough evidence to reject it!
  prefs: []
  type: TYPE_NORMAL
- en: 'A question that might arise: “Could we have stopped earlier?” That is a nice
    benefit of e-values. Peeking at the data is allowed before the planned sample
    size is reached, so you can decide to quit or continue an experiment at any time.
    We can plot the e-values, e.g. cumulative for every 50 new samples. The first
    40 e-values plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a45423fc7c74504f72fd2641784e803.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the beginning there is no evidence against the null, corresponding to low
    e-values. But with gathering more samples the evidence starts to show: the e-values
    exceed the decision boundary of 20\. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29fb964e949b3afb669e2f801e1d2bd2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can be sure: the null hypothesis should be rejected. All e-values except
    the last one. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test case: Null Hypothesis is True'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we change the fake data and make the probabilities equal to each other (success
    probability for version A and B equal to 0.05), we should detect no significant
    e- or p-value. The distributions of version A and B look similar and the null
    hypothesis is true. This is reflected in the e-values plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36c77e726e224fdba2d1b3dd8ca2ac4e.png)'
  prefs: []
  type: TYPE_IMG
- en: No effect. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: But what if we compare this with p-values? How often will we reject the null
    hypothesis, although in reality we shouldn’t? Let’s test it. We will repeat the
    experiment 1000 times, and see in how many cases we rejected the null hypothesis
    for p-values and e-values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output if we sum the `pValuesRejected` and the `eValuesRejected`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The p-value was significant in 48 of the cases (around 5%, this is what we
    would expect with an alpha of 0.05). On the other hand, the e-value does a great
    job: It never rejects the null hypothesis. In case you weren’t convinced of using
    e-values yet, I hope you are now!'
  prefs: []
  type: TYPE_NORMAL
- en: If you are curious for other examples, I can recommend the [vignettes](https://cran.r-project.org/web/packages/safestats/index.html)
    from the safestats package.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: E-values present a compelling alternative to traditional p-values, offering
    several advantages. They provide the flexibility to either continue or halt an
    experiment at any stage. Additionally, their combinability is a benefit, and the
    freedom to review experimental results at any point is a big plus. The comparison
    of p-values and e-values revealed that e-values are more reliable; p-values carry
    a greater risk of falsely identifying significant differences when none exist.
    The safestats R package is a useful tool for implementing these robust tests.
  prefs: []
  type: TYPE_NORMAL
- en: I am convinced of the merits of e-values and look forward to the development
    of a Python package that supports their implementation! 😄
  prefs: []
  type: TYPE_NORMAL
- en: Related
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](/sneaky-science-data-dredging-exposed-26a445f00e5c?source=post_page-----a04e6ed2b6a7--------------------------------)
    [## Sneaky Science: Data Dredging Exposed'
  prefs: []
  type: TYPE_NORMAL
- en: Delve into the motivations and consequences of P-hacking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/sneaky-science-data-dredging-exposed-26a445f00e5c?source=post_page-----a04e6ed2b6a7--------------------------------)
    [](/how-to-compare-ml-solutions-effectively-28384e2cbca1?source=post_page-----a04e6ed2b6a7--------------------------------)
    [## How to Compare ML Solutions Effectively?
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the chances of getting a model to production
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-compare-ml-solutions-effectively-28384e2cbca1?source=post_page-----a04e6ed2b6a7--------------------------------)
    [](/simplify-your-machine-learning-projects-ab171d19c9ef?source=post_page-----a04e6ed2b6a7--------------------------------)
    [## Simplify Your Machine Learning Projects
  prefs: []
  type: TYPE_NORMAL
- en: Why spending a lot of time and effort on a complex model is a bad idea, and
    what to do instead
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/simplify-your-machine-learning-projects-ab171d19c9ef?source=post_page-----a04e6ed2b6a7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
