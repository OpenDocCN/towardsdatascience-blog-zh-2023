- en: How Generative AI Can Support Food Industry Businesses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-generative-ai-can-support-food-industry-businesses-993872b4a6ce](https://towardsdatascience.com/how-generative-ai-can-support-food-industry-businesses-993872b4a6ce)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learning from past mistakes and using ChatGPT to build better machine learning
    models for Food Industry Companies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ben-mccloskey20.medium.com/?source=post_page-----993872b4a6ce--------------------------------)[![Benjamin
    McCloskey](../Images/7118f5933f2affe2a7a4d3375452fa4c.png)](https://ben-mccloskey20.medium.com/?source=post_page-----993872b4a6ce--------------------------------)[](https://towardsdatascience.com/?source=post_page-----993872b4a6ce--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----993872b4a6ce--------------------------------)
    [Benjamin McCloskey](https://ben-mccloskey20.medium.com/?source=post_page-----993872b4a6ce--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----993872b4a6ce--------------------------------)
    ·12 min read·Jul 19, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/526e267b04847504f33254d628ce1049.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mae Mu](https://unsplash.com/@picoftasty?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The journey I am about to take you on is important for two reasons.
  prefs: []
  type: TYPE_NORMAL
- en: It will show you how you can use ChatGPT to help support companies working in
    the food industry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Arguably the most important reason, I am going to walk through a post I made
    almost two years ago, ***point out the problems with that article***, and attempt
    the fix them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes, I argue that the second reason is more important. Why? Looking back past
    ways and processes you analyze data is important because it allows you to learn
    how to fix your failures which ultimately leads to success. I am in no way perfect,
    and I personally look for the wrong things I have done in the past in the hopes
    of learning from my mistakes and developing *stronger* models for the clients
    I support.
  prefs: []
  type: TYPE_NORMAL
- en: The Original Publication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I first published “Machine Learning is Not Just for Big Tech” in July of 2021.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/machine-learning-is-not-just-for-big-tech-using-natural-language-processing-to-support-small-8f571249c073?source=post_page-----993872b4a6ce--------------------------------)
    [## Machine Learning is Not Just for Big Tech'
  prefs: []
  type: TYPE_NORMAL
- en: Using Natural Language Processing to Support Small Businesses.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/machine-learning-is-not-just-for-big-tech-using-natural-language-processing-to-support-small-8f571249c073?source=post_page-----993872b4a6ce--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the article was to show how a company in the food industry could
    be supported by the various uses of machine learning (ML). I used Natural Language
    Processing (NLP) techniques to work with reviews across the internet about the
    company. Some of the methods I used from NLP were Topic Modeling Analysis to gain
    a better understanding of what customers were talking about and Sentiment Analysis
    to create a model that could help predict the sentiment of future reviews and
    provide feedback to the company. The analysis showed both methods were capable
    of being performed on a *small corpus of data.*
  prefs: []
  type: TYPE_NORMAL
- en: AH! The big mistake.
  prefs: []
  type: TYPE_NORMAL
- en: My data was not great. Not only was the dataset small, but it was also **biased
    toward positive reviews.** This led to models almost always predicting a review
    to be positive (not helpful for the company) and was overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '*Solution*? I thought about using a Generative Adversarial Network (GAN) to
    create new synthetic reviews, but then I thought, could I just ask ChatGPT? **Boom**.
    The first mistake from my original work was solved. I was able to use ChatGPT
    to create artificial positive and negative reviews which ultimately balanced my
    Italian Food Company Review Dataset!'
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Luckily, I did my due diligence and verified the usability of the data I created
    **before** training any models or doing any sort of analysis. Check out the post
    below which provides a more in-depth analysis of the real as well artificial data.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/chatgpt-generated-food-industry-reviews-realism-assessment-2ee28155970f?source=post_page-----993872b4a6ce--------------------------------)
    [## ChatGPT Generated Food Industry Reviews: Realism Assessment'
  prefs: []
  type: TYPE_NORMAL
- en: Investigating how review and survey collection by food industry companies can
    be supported by ChatGPT-generated data.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/chatgpt-generated-food-industry-reviews-realism-assessment-2ee28155970f?source=post_page-----993872b4a6ce--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Positive Review Creation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the dataset, I wanted to have an equal balance of positive and negative
    reviews. First, using ChatGPT, I queried it to create positive reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Create 500 different positive reviews about different Italian foods and products
    purchased from an Italian market and put them in a CSV file.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I did this at least 5 times for two reasons. One, ChatGPT kept timing out. Two,
    I wanted to make sure I got enough different reviews. Additionally, to increase
    the diversity of generated data, I would change the query each time. For example,
    I would say *Italian Desserts* or *Italian Wines* in place of *Italian foods and
    products.* Let’s take a look at a fake positive review generated by ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: '*“The Pecorino Toscano cheese I bought had a robust and savory taste. Its firm
    and crumbly texture, with a hint of grassiness, made it a great choice for grating,
    shaving, or enjoying on its own.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not bad if you ask me!
  prefs: []
  type: TYPE_NORMAL
- en: Negative Review Creation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For creating the negative reviews, I followed the same process. I did have to
    explicitly tell ChatGPT I was not making the negative reviews to harm anyone,
    which is the truth! I simply wanted a classification model and analysis that ***generalizes***
    to all types of data the model may encounter (positive and negative reviews).
  prefs: []
  type: TYPE_NORMAL
- en: Create 100 negative reviews about different Italian foods purchased from an
    Italian market and put them in a CSV file.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Example of a generated negative review:'
  prefs: []
  type: TYPE_NORMAL
- en: ”The prosciutto and arugula pizza I tried had wilted arugula and the prosciutto
    was tough. It wasn’t appetizing.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Again, not bad if you ask me!
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**First error:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*cannot import name ‘pad_sequences’ from ‘keras.preprocessing.sequence’ (/usr/local/lib/python3.10/dist-packages/keras/preprocessing/sequence.py)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution:**'
  prefs: []
  type: TYPE_NORMAL
- en: → Instead of the import
  prefs: []
  type: TYPE_NORMAL
- en: '`from keras.preprocessing.sequence import pad_sequences`'
  prefs: []
  type: TYPE_NORMAL
- en: '→ Use the import:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from keras.utils import pad_sequences`'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the code had worked as before which was surprising. With the recent
    release of Keras 3.0, some of this code may be depreciated depending on what packages
    and IDE you are using to run your code.
  prefs: []
  type: TYPE_NORMAL
- en: Data Cleaning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There was some additional cleaning that needed to be done on the reviews before
    a model could be trained. Luckily, “0” for negative reviews and “1” for positive
    reviews could be appended to each review in a sentiment column.
  prefs: []
  type: TYPE_NORMAL
- en: Original Cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the following line would be acceptable to implement if I was adding a
    new column, I decided to use a lambda function instead for cleaning the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Also, I decided that a rating of *3* is negative which luckily helped balance
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I noticed through the cleaning process I had undefined variables in the original
    post, which is a problem I use to have a lot of (honestly I still do). Currently,
    I attempt to mitigate this problem by resetting my kernel and double-checking
    my work before integrating it into one of my posts.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the Original Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/95104fcee3ae2b9d6dc274f28db53a9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Dataset Imbalances (Image from Author)
  prefs: []
  type: TYPE_NORMAL
- en: The biggest issue with the dataset was the giant imbalance between the positive
    and negative reviews. Originally, there were 512 positive reviews and 132 negative
    reviews. *Why is this a problem?* Training a model with this dataset will more
    than likely lead to a model which mostly (almost always) predicts reviews as positive.
    I definitely overlooked this in my original post and should have done a better
    job of addressing this issue. While I do not shy away from imbalanced datasets,
    I will not initially use them until I have exhausted all efforts and tried various
    techniques to balance them (including further data collection!).
  prefs: []
  type: TYPE_NORMAL
- en: What if we did not train a model at all and just used BERT?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we can see, the dataset in its current state is not supportive of a strong
    sentiment analysis model. I*S BERT capable of accurately predicting the sentiment
    of the reviews with no training? Or is another approach needed to be taken?*
  prefs: []
  type: TYPE_NORMAL
- en: With no training, BERT was accurate **68.27%** of the time. Higher accuracies
    may not have been achieved due to the unstructured nature and fuzzy language used
    in many of the reviews which BERT could not comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04f8797a768b464b66da03bf8f685485.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT Prediction Confusion Matrix (Image from Author)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine the confusion matrix in pursuit of understanding the BERT predictions.
    For the negative reviews, BERT was correct 76 times. 56 labels were positive which
    should have been negative. This could be due to how the reviews may have been
    more neutral in their given state but leaned towards a positive sentiment. For
    example, *“This place used to be good but the pizza was not very good today.”*
    As humans, we read this sentence and understand there is a mostly negative sentiment
    being expressed. BERT on the other hand may interpret the continuous use of *“good”*
    in the sentence as a means for predicting the sentence as positive.
  prefs: []
  type: TYPE_NORMAL
- en: For the positive reviews, BERT accomplishes more accurate classifications. 363
    times BERT correctly predicts a review to be positive. One failure, however, is
    there are 149 predictions where BERT believed a positive review to be negative.
  prefs: []
  type: TYPE_NORMAL
- en: I wanted to outline and discuss the information from the confusion matrix since
    it shows how difficult it can be for algorithms to understand human language,
    especially when different linguistic features are incorporated, such as sarcasm
    and the sentiment the speaker wishes to communicate**. The purpose of using BERT
    with no training was to see if a model was needed to be fine-tuned for the company
    or if an off the shelf model could be used. With an accuracy below 80%, I would
    recommend fine-tuning a model to be more aligned with the company’s data.**
  prefs: []
  type: TYPE_NORMAL
- en: Model Creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I noticed in my original post I used Convolutional Neural Networks (CNN) for
    developing my model. From what I recall, I had done this because I was working
    a lot with CNNs for my published research ([*Benefits of using blended generative
    adversarial network images to augment classification model training data sets*](https://journals.sagepub.com/doi/abs/10.1177/15485129231170225?journalCode=dmsa)*).*
    While it’s not wrong to use CNNs (1-D in this case), I wanted to also look at
    other models which may provide better predictions for the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: New Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While creating my own model was beneficial for my own learning, I decided to
    conduct transfer learning and train on the data with a well-known model [*BERT*](https://arxiv.org/abs/1810.04805)for
    my new SA model.
  prefs: []
  type: TYPE_NORMAL
- en: '*See the full BERT code I used below.*'
  prefs: []
  type: TYPE_NORMAL
- en: Why *Transfer Learning?* **Why reinvent the wheel when there are powerful models
    readily available which can be tailored to your given problem?** Whenever you
    face a problem, always do your research to see what others have accomplished in
    the past. You may be surprised to find out many people have experienced the same,
    if not similar problem and have already solved it for you!
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overall, I missed the mark on (or really, did not outline) how the dataset I
    originally had led to a model that was *biased* and *heavily overfit* on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training: No Dataset Augmentation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As stated, the original model was overfitting to the dataset due to the extreme
    imbalances in the positive and negative labels. Some of the changes that led to
    a “better” model were going from an 80/20 split to a 70/30 split for the training
    and test set as well as adding more dropout into the BERT model (I used 0.5).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf2d7b99fd5705d004ff4cca0df17aed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Training/Test Loss & Accuracy: Original Dataset (Image from Author)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there was a stable loss of 50% (somewhat expected) throughout
    the model training. When I trained with a 70/30 split, the biggest difference
    was the loss value for the training set went from ~80% to ~60%. Overall, the models
    did not perform well with the imbalanced dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset with Augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**How does BERT interpret fake reviews?**'
  prefs: []
  type: TYPE_NORMAL
- en: BERT underperformed with an accuracy of 38.22%. I do not put the full fault
    on the BERT model. The generated data from ChatGPT lacked distinction between
    what constitutes a review as positive and negative, a clear indication that ChatGPT
    generates a lot of its data off of past patterns and is not sentient.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7547480915c0e1616b7fbd8375133994.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT Confusion Matrix with Generated Data (Image from Author)
  prefs: []
  type: TYPE_NORMAL
- en: The most significant problem in the overhead confusion matrix is where BERT
    classified many reviews as positive, when in fact they were negative. *Why*? Well,
    for starters more negative reviews had to be generated by ChatGPT than positive
    reviews. Clearly, the patterns which ChatGPT used for creating its negative reviews
    were too similar to the positive reviews, encompassing one of the downfalls of
    data generation and ChatGPT’s inability to produce diversity within different
    categories of data distinct and aligned with real-world information.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning BERT with the Augmented Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0e8e9faf6b8563b92b47984d39222368.png)'
  prefs: []
  type: TYPE_IMG
- en: Balanced Dataset (Image from Author)
  prefs: []
  type: TYPE_NORMAL
- en: With ChatGPT, the dataset was balanced to contain 1,126 positive reviews and
    1,124 negative reviews (an additional 614 positive reviews and 992 negative reviews).
    ***One benefit of using Generative AI algorithms is their ability to balance datasets,
    especially those with huge imbalances like this one.*** *The downfall*? The newly
    generated data may not be representative of the original data and this needs to
    be taken into consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Once the dataset is balanced, we can attempt to finetune BERT again for sentiment
    analysis using the same processes as before. One distinct change was the number
    of words for each embedding decreased in size due to the average length being
    reduced by the generated reviews (150 → 60).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f4d85a07168d4c16cf76653c32f2c0c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Training/Test Loss & Accuracy: Augmented Dataset (Image from Author)'
  prefs: []
  type: TYPE_NORMAL
- en: Success! Dataset augmentation improved the fine-tuning of the BERT model. After
    5 epochs, an accuracy of **99.67%** was achieved with little loss. The test lost
    stayed consistently around 13.3% with an accuracy of 97.3% at epoch 5.
  prefs: []
  type: TYPE_NORMAL
- en: '*How would this model benefit a company in the food industry that uses it?*
    While using the data from just the company could lead to a model that is more
    *personable* and *aligned* with their processes, using external data can help
    provide a model to the company which is more *generalizable* and *adaptable* to
    real-world changes. These changes, which are almost guaranteed to occur, may be
    alien to the model, and having a model which can adapt to the unpredictability
    of the real world mitigates its failure and works as a stopgap against undesired
    decisions taken by the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI has recently exploded, and finding appropriate use cases which
    are positive and beneficial to companies in different industries is important.
    For companies in the food industry, or any company which has a product and reviews
    about said product, ChatGPT can help support models which can flag reviews as
    positive and negative to support business operations and product development.
    While ChatGPT can be used for automation, we should be wary of its power, taking
    a *human-on-the-loop* approach and evaluate the generated data. Whatever Generative
    AI technique you use for your dataset development, ensure the data is realistic
    to the real world information it wishes to capture in hopes of create the strongest
    performing model possible.
  prefs: []
  type: TYPE_NORMAL
- en: On a more personal note, today showed how we can always try to get better, and
    we must learn from our poor past performances to get better. Learning from your
    mistakes and weaknesses is one of the most important parts of being a Data Scientist,
    and ultimately will foster a career makred with excellence and focused on continuous
    development.
  prefs: []
  type: TYPE_NORMAL
- en: '**If you enjoyed today’s reading, PLEASE give me a follow and let me know if
    there is another topic you would like me to explore! If you do not have a Medium
    account, sign up through my link** [**here**](https://ben-mccloskey20.medium.com/membership)
    **(I receive a small commission when you do this)! Additionally, add me on** [**LinkedIn**](https://www.linkedin.com/in/benjamin-mccloskey-169975a8/),
    **or feel free to reach out! Thanks for reading!**'
  prefs: []
  type: TYPE_NORMAL
- en: Sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data usage approved by Altomontes Inc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
