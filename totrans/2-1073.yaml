- en: How Generative AI Can Support Food Industry Businesses
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成性人工智能如何支持食品行业企业
- en: 原文：[https://towardsdatascience.com/how-generative-ai-can-support-food-industry-businesses-993872b4a6ce](https://towardsdatascience.com/how-generative-ai-can-support-food-industry-businesses-993872b4a6ce)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-generative-ai-can-support-food-industry-businesses-993872b4a6ce](https://towardsdatascience.com/how-generative-ai-can-support-food-industry-businesses-993872b4a6ce)
- en: Learning from past mistakes and using ChatGPT to build better machine learning
    models for Food Industry Companies
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从过去的错误中学习，并利用 ChatGPT 为食品行业公司构建更好的机器学习模型
- en: '[](https://ben-mccloskey20.medium.com/?source=post_page-----993872b4a6ce--------------------------------)[![Benjamin
    McCloskey](../Images/7118f5933f2affe2a7a4d3375452fa4c.png)](https://ben-mccloskey20.medium.com/?source=post_page-----993872b4a6ce--------------------------------)[](https://towardsdatascience.com/?source=post_page-----993872b4a6ce--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----993872b4a6ce--------------------------------)
    [Benjamin McCloskey](https://ben-mccloskey20.medium.com/?source=post_page-----993872b4a6ce--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ben-mccloskey20.medium.com/?source=post_page-----993872b4a6ce--------------------------------)[![Benjamin
    McCloskey](../Images/7118f5933f2affe2a7a4d3375452fa4c.png)](https://ben-mccloskey20.medium.com/?source=post_page-----993872b4a6ce--------------------------------)[](https://towardsdatascience.com/?source=post_page-----993872b4a6ce--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----993872b4a6ce--------------------------------)
    [Benjamin McCloskey](https://ben-mccloskey20.medium.com/?source=post_page-----993872b4a6ce--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----993872b4a6ce--------------------------------)
    ·12 min read·Jul 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----993872b4a6ce--------------------------------)
    ·12分钟阅读·2023年7月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/526e267b04847504f33254d628ce1049.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/526e267b04847504f33254d628ce1049.png)'
- en: Photo by [Mae Mu](https://unsplash.com/@picoftasty?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于[Mae Mu](https://unsplash.com/@picoftasty?utm_source=medium&utm_medium=referral)
    [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The journey I am about to take you on is important for two reasons.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我将带你踏上的旅程有两个重要原因。
- en: It will show you how you can use ChatGPT to help support companies working in
    the food industry.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将展示如何使用 ChatGPT 来支持食品行业的公司。
- en: Arguably the most important reason, I am going to walk through a post I made
    almost two years ago, ***point out the problems with that article***, and attempt
    the fix them.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以说最重要的原因，我将详细讲解我在将近两年前发表的一篇文章，***指出那篇文章中的问题***，并尝试解决它们。
- en: Yes, I argue that the second reason is more important. Why? Looking back past
    ways and processes you analyze data is important because it allows you to learn
    how to fix your failures which ultimately leads to success. I am in no way perfect,
    and I personally look for the wrong things I have done in the past in the hopes
    of learning from my mistakes and developing *stronger* models for the clients
    I support.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我认为第二个原因更为重要。为什么？回顾过去的方法和过程，分析数据是重要的，因为它可以让你学习如何修正失败，这最终会导致成功。我绝非完美，我个人寻找过去做错的事情，希望从错误中学习，并为我支持的客户开发出*更强大的*模型。
- en: The Original Publication
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始出版物
- en: I first published “Machine Learning is Not Just for Big Tech” in July of 2021.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我第一次发布《机器学习不仅仅是大科技的专利》是在 2021 年 7 月。
- en: '[](/machine-learning-is-not-just-for-big-tech-using-natural-language-processing-to-support-small-8f571249c073?source=post_page-----993872b4a6ce--------------------------------)
    [## Machine Learning is Not Just for Big Tech'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/machine-learning-is-not-just-for-big-tech-using-natural-language-processing-to-support-small-8f571249c073?source=post_page-----993872b4a6ce--------------------------------)
    [## 机器学习不仅仅是大科技的专利'
- en: Using Natural Language Processing to Support Small Businesses.
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自然语言处理来支持小企业。
- en: towardsdatascience.com](/machine-learning-is-not-just-for-big-tech-using-natural-language-processing-to-support-small-8f571249c073?source=post_page-----993872b4a6ce--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/machine-learning-is-not-just-for-big-tech-using-natural-language-processing-to-support-small-8f571249c073?source=post_page-----993872b4a6ce--------------------------------)
- en: The purpose of the article was to show how a company in the food industry could
    be supported by the various uses of machine learning (ML). I used Natural Language
    Processing (NLP) techniques to work with reviews across the internet about the
    company. Some of the methods I used from NLP were Topic Modeling Analysis to gain
    a better understanding of what customers were talking about and Sentiment Analysis
    to create a model that could help predict the sentiment of future reviews and
    provide feedback to the company. The analysis showed both methods were capable
    of being performed on a *small corpus of data.*
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是展示食品行业中的公司如何通过各种机器学习（ML）应用得到支持。我使用了自然语言处理（NLP）技术来处理关于公司的网络评论。我使用的一些NLP方法包括主题建模分析，以更好地了解客户在谈论什么，以及情感分析，以创建一个可以帮助预测未来评论情感并向公司提供反馈的模型。分析显示这两种方法都能够在*小型数据集*上进行。
- en: AH! The big mistake.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 啊！大错误。
- en: My data was not great. Not only was the dataset small, but it was also **biased
    toward positive reviews.** This led to models almost always predicting a review
    to be positive (not helpful for the company) and was overfitting.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我的数据并不理想。数据集不仅小，而且**倾向于正面评论**。这导致模型几乎总是预测评论为正面（对公司没有帮助），并且存在过拟合。
- en: '*Solution*? I thought about using a Generative Adversarial Network (GAN) to
    create new synthetic reviews, but then I thought, could I just ask ChatGPT? **Boom**.
    The first mistake from my original work was solved. I was able to use ChatGPT
    to create artificial positive and negative reviews which ultimately balanced my
    Italian Food Company Review Dataset!'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案*？我考虑使用生成对抗网络（GAN）来创建新的合成评论，但随后我想，能不能直接问ChatGPT？**轰**。我原始工作中的第一个错误被解决了。我能够利用ChatGPT创建人工的正面和负面评论，这**最终**平衡了我的意大利食品公司评论数据集！'
- en: The Dataset
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: Luckily, I did my due diligence and verified the usability of the data I created
    **before** training any models or doing any sort of analysis. Check out the post
    below which provides a more in-depth analysis of the real as well artificial data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我在训练任何模型或进行任何分析之前，已经**仔细**验证了我创建的数据的可用性。请查看下面的帖子，提供对真实数据和人工数据的更深入分析。
- en: '[](/chatgpt-generated-food-industry-reviews-realism-assessment-2ee28155970f?source=post_page-----993872b4a6ce--------------------------------)
    [## ChatGPT Generated Food Industry Reviews: Realism Assessment'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/chatgpt-generated-food-industry-reviews-realism-assessment-2ee28155970f?source=post_page-----993872b4a6ce--------------------------------)
    [## ChatGPT生成的食品行业评论：真实性评估'
- en: Investigating how review and survey collection by food industry companies can
    be supported by ChatGPT-generated data.
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调查食品行业公司如何通过ChatGPT生成的数据来支持评论和调查收集。
- en: towardsdatascience.com](/chatgpt-generated-food-industry-reviews-realism-assessment-2ee28155970f?source=post_page-----993872b4a6ce--------------------------------)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/chatgpt-generated-food-industry-reviews-realism-assessment-2ee28155970f?source=post_page-----993872b4a6ce--------------------------------)'
- en: Positive Review Creation
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正面评论创建
- en: For the dataset, I wanted to have an equal balance of positive and negative
    reviews. First, using ChatGPT, I queried it to create positive reviews.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集，我希望正面和负面评论的比例均衡。首先，使用ChatGPT，我查询它来创建正面评论。
- en: Create 500 different positive reviews about different Italian foods and products
    purchased from an Italian market and put them in a CSV file.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 创建500条关于不同意大利食品和产品的正面评论，并将其放入CSV文件中。
- en: I did this at least 5 times for two reasons. One, ChatGPT kept timing out. Two,
    I wanted to make sure I got enough different reviews. Additionally, to increase
    the diversity of generated data, I would change the query each time. For example,
    I would say *Italian Desserts* or *Italian Wines* in place of *Italian foods and
    products.* Let’s take a look at a fake positive review generated by ChatGPT.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我至少这样做了5次，原因有二。一是ChatGPT不断超时。二是我想确保获得足够不同的评论。此外，为了增加生成数据的多样性，我每次都会更改查询。例如，我会将*意大利食品和产品*改为*意大利甜点*或*意大利葡萄酒*。让我们看看ChatGPT生成的一个虚假正面评论。
- en: '*“The Pecorino Toscano cheese I bought had a robust and savory taste. Its firm
    and crumbly texture, with a hint of grassiness, made it a great choice for grating,
    shaving, or enjoying on its own.*'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“我购买的佩科里诺托斯卡纳奶酪味道浓郁美味。它的质地坚实且易碎，带有一丝草味，非常适合刨丝、刮片或直接享用。*'
- en: Not bad if you ask me!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你问我，不错！
- en: Negative Review Creation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负面评论创建
- en: For creating the negative reviews, I followed the same process. I did have to
    explicitly tell ChatGPT I was not making the negative reviews to harm anyone,
    which is the truth! I simply wanted a classification model and analysis that ***generalizes***
    to all types of data the model may encounter (positive and negative reviews).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建负面评论，我遵循了相同的过程。我确实需要明确告诉ChatGPT我制作负面评论并不是为了伤害任何人，这确实是事实！我只是想要一个能***泛化***到模型可能遇到的所有数据类型（正面和负面评论）的分类模型和分析。
- en: Create 100 negative reviews about different Italian foods purchased from an
    Italian market and put them in a CSV file.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 创建100条关于不同意大利食品的负面评论，并将它们放入CSV文件中。
- en: 'Example of a generated negative review:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的负面评论示例：
- en: ”The prosciutto and arugula pizza I tried had wilted arugula and the prosciutto
    was tough. It wasn’t appetizing.”
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我尝试的意大利火腿和芝麻菜比萨上有枯萎的芝麻菜，意大利火腿也很硬。它并不令人垂涎。”
- en: Again, not bad if you ask me!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你问我，结果还不错！
- en: Analysis
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析
- en: '**First error:**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一个错误：**'
- en: '*cannot import name ‘pad_sequences’ from ‘keras.preprocessing.sequence’ (/usr/local/lib/python3.10/dist-packages/keras/preprocessing/sequence.py)*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*无法从‘keras.preprocessing.sequence’导入名称‘pad_sequences’ (/usr/local/lib/python3.10/dist-packages/keras/preprocessing/sequence.py)*'
- en: '**Solution:**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案：**'
- en: → Instead of the import
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: → 替代导入
- en: '`from keras.preprocessing.sequence import pad_sequences`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`from keras.preprocessing.sequence import pad_sequences`'
- en: '→ Use the import:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: → 使用导入：
- en: '`from keras.utils import pad_sequences`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`from keras.utils import pad_sequences`'
- en: Most of the code had worked as before which was surprising. With the recent
    release of Keras 3.0, some of this code may be depreciated depending on what packages
    and IDE you are using to run your code.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数代码如以前一样工作，这很令人惊讶。由于Keras 3.0的最新发布，部分代码可能会被弃用，这取决于你使用的包和IDE。
- en: Data Cleaning
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据清理
- en: There was some additional cleaning that needed to be done on the reviews before
    a model could be trained. Luckily, “0” for negative reviews and “1” for positive
    reviews could be appended to each review in a sentiment column.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练之前，还需要对评论进行一些额外的清理。幸运的是，可以在情感列中将“0”用于负面评论，将“1”用于正面评论，附加到每条评论中。
- en: Original Cleaning
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始清理
- en: While the following line would be acceptable to implement if I was adding a
    new column, I decided to use a lambda function instead for cleaning the dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然如果我添加新列，以下代码是可以实施的，但我决定使用lambda函数来清理数据集。
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Also, I decided that a rating of *3* is negative which luckily helped balance
    the dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我决定将*3*的评分视为负面评分，这幸运地帮助平衡了数据集。
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I noticed through the cleaning process I had undefined variables in the original
    post, which is a problem I use to have a lot of (honestly I still do). Currently,
    I attempt to mitigate this problem by resetting my kernel and double-checking
    my work before integrating it into one of my posts.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我在清理过程中注意到原始帖子中有未定义的变量，这是我以前经常遇到的问题（说实话，我现在仍然有）。目前，我尝试通过重置内核并在将工作集成到我的帖子之前仔细检查来减轻这个问题。
- en: Problems with the Original Dataset
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始数据集的问题
- en: '![](../Images/95104fcee3ae2b9d6dc274f28db53a9c.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95104fcee3ae2b9d6dc274f28db53a9c.png)'
- en: Dataset Imbalances (Image from Author)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集不平衡（图像来自作者）
- en: The biggest issue with the dataset was the giant imbalance between the positive
    and negative reviews. Originally, there were 512 positive reviews and 132 negative
    reviews. *Why is this a problem?* Training a model with this dataset will more
    than likely lead to a model which mostly (almost always) predicts reviews as positive.
    I definitely overlooked this in my original post and should have done a better
    job of addressing this issue. While I do not shy away from imbalanced datasets,
    I will not initially use them until I have exhausted all efforts and tried various
    techniques to balance them (including further data collection!).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集最大的问题是正面评论和负面评论之间的巨大不平衡。最初有512条正面评论和132条负面评论。*这为什么是个问题？* 使用这个数据集训练模型很可能会导致一个大多数（几乎总是）将评论预测为正面的模型。我在原始帖子中确实忽视了这一点，应该更好地解决这个问题。虽然我不回避不平衡的数据集，但在我用尽所有努力并尝试各种技术（包括进一步的数据收集！）来平衡它们之前，我不会最初使用它们。
- en: What if we did not train a model at all and just used BERT?
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果我们完全不训练模型而仅使用BERT会怎样？
- en: As we can see, the dataset in its current state is not supportive of a strong
    sentiment analysis model. I*S BERT capable of accurately predicting the sentiment
    of the reviews with no training? Or is another approach needed to be taken?*
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，目前数据集不支持强大的情感分析模型。*BERT在没有训练的情况下能够准确预测评论的情感吗？还是需要采取其他方法？*
- en: With no training, BERT was accurate **68.27%** of the time. Higher accuracies
    may not have been achieved due to the unstructured nature and fuzzy language used
    in many of the reviews which BERT could not comprehend.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有训练的情况下，BERT的准确率为**68.27%**。由于许多评论使用了非结构化和模糊的语言，BERT无法理解，因此可能未能实现更高的准确率。
- en: '![](../Images/04f8797a768b464b66da03bf8f685485.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04f8797a768b464b66da03bf8f685485.png)'
- en: BERT Prediction Confusion Matrix (Image from Author)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: BERT预测混淆矩阵（图片来自作者）
- en: Let’s examine the confusion matrix in pursuit of understanding the BERT predictions.
    For the negative reviews, BERT was correct 76 times. 56 labels were positive which
    should have been negative. This could be due to how the reviews may have been
    more neutral in their given state but leaned towards a positive sentiment. For
    example, *“This place used to be good but the pizza was not very good today.”*
    As humans, we read this sentence and understand there is a mostly negative sentiment
    being expressed. BERT on the other hand may interpret the continuous use of *“good”*
    in the sentence as a means for predicting the sentence as positive.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过混淆矩阵来深入了解BERT的预测。对于消极评论，BERT正确了76次。56个标签为积极，但应该是消极的。这可能是由于评论在给定状态下可能更为中立，但倾向于积极情感。例如，*“这个地方以前不错，但今天的比萨饼不太好。”*
    作为人类，我们阅读这句话会理解其表达了主要的消极情感。另一方面，BERT可能会将句子中连续使用的*“好”* 作为预测该句子为积极的依据。
- en: For the positive reviews, BERT accomplishes more accurate classifications. 363
    times BERT correctly predicts a review to be positive. One failure, however, is
    there are 149 predictions where BERT believed a positive review to be negative.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于积极评论，BERT实现了更准确的分类。BERT正确预测了363次评论为积极。然而，一个失败是，BERT将149个预测为积极的评论误认为是消极的。
- en: I wanted to outline and discuss the information from the confusion matrix since
    it shows how difficult it can be for algorithms to understand human language,
    especially when different linguistic features are incorporated, such as sarcasm
    and the sentiment the speaker wishes to communicate**. The purpose of using BERT
    with no training was to see if a model was needed to be fine-tuned for the company
    or if an off the shelf model could be used. With an accuracy below 80%, I would
    recommend fine-tuning a model to be more aligned with the company’s data.**
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我想概述并讨论混淆矩阵中的信息，因为它显示了算法理解人类语言的困难，特别是当不同的语言特征被纳入时，例如讽刺和说话者希望传达的情感**。使用BERT而不进行训练的目的是查看是否需要对公司进行模型微调，还是可以使用现成的模型。在准确率低于80%的情况下，我建议微调一个模型，以更好地与公司的数据对齐。**
- en: Model Creation
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型创建
- en: I noticed in my original post I used Convolutional Neural Networks (CNN) for
    developing my model. From what I recall, I had done this because I was working
    a lot with CNNs for my published research ([*Benefits of using blended generative
    adversarial network images to augment classification model training data sets*](https://journals.sagepub.com/doi/abs/10.1177/15485129231170225?journalCode=dmsa)*).*
    While it’s not wrong to use CNNs (1-D in this case), I wanted to also look at
    other models which may provide better predictions for the dataset.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我注意到在我最初的帖子中，我使用了卷积神经网络（CNN）来开发我的模型。根据我的记忆，我这样做是因为我在已发布的研究中大量使用CNN（[*使用混合生成对抗网络图像来增强分类模型训练数据集的好处*](https://journals.sagepub.com/doi/abs/10.1177/15485129231170225?journalCode=dmsa)*）。虽然使用CNN（在这种情况下是1-D）并没有错，但我还想看看其他可能对数据集提供更好预测的模型。
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: New Model
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新模型
- en: While creating my own model was beneficial for my own learning, I decided to
    conduct transfer learning and train on the data with a well-known model [*BERT*](https://arxiv.org/abs/1810.04805)for
    my new SA model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然创建自己的模型对我的学习很有帮助，但我决定进行迁移学习，并在一个知名模型[*BERT*](https://arxiv.org/abs/1810.04805)上训练我的新情感分析（SA）模型。
- en: '*See the full BERT code I used below.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*请参见我使用的完整BERT代码。*'
- en: Why *Transfer Learning?* **Why reinvent the wheel when there are powerful models
    readily available which can be tailored to your given problem?** Whenever you
    face a problem, always do your research to see what others have accomplished in
    the past. You may be surprised to find out many people have experienced the same,
    if not similar problem and have already solved it for you!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么 *迁移学习？* **为什么要重新发明轮子，当有强大的模型可以根据你的问题进行调整时？** 每当遇到问题时，总是要做研究，看看其他人在过去完成了什么。你可能会惊讶地发现，很多人遇到过相同的或类似的问题，并且已经为你解决了！
- en: Results
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: Overall, I missed the mark on (or really, did not outline) how the dataset I
    originally had led to a model that was *biased* and *heavily overfit* on the dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我没有准确描述（或者说，根本没有概述）我原始数据集如何导致模型*偏向*和*严重过拟合*数据集。
- en: 'Training: No Dataset Augmentation'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练：无数据增强
- en: As stated, the original model was overfitting to the dataset due to the extreme
    imbalances in the positive and negative labels. Some of the changes that led to
    a “better” model were going from an 80/20 split to a 70/30 split for the training
    and test set as well as adding more dropout into the BERT model (I used 0.5).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，原始模型因正负标签的极端不平衡而过拟合数据集。导致“更好”模型的一些变化包括将训练和测试集的划分从 80/20 改为 70/30，以及在 BERT
    模型中添加更多的 dropout（我使用了 0.5）。
- en: '![](../Images/cf2d7b99fd5705d004ff4cca0df17aed.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf2d7b99fd5705d004ff4cca0df17aed.png)'
- en: 'Training/Test Loss & Accuracy: Original Dataset (Image from Author)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 训练/测试损失与准确度：原始数据集（图片来自作者）
- en: As you can see, there was a stable loss of 50% (somewhat expected) throughout
    the model training. When I trained with a 70/30 split, the biggest difference
    was the loss value for the training set went from ~80% to ~60%. Overall, the models
    did not perform well with the imbalanced dataset.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在模型训练过程中，损失稳定在 50%（这是可以预期的）。当我使用 70/30 的划分进行训练时，最大的变化是训练集的损失值从 ~80% 降至
    ~60%。总体而言，模型在不平衡的数据集上表现不佳。
- en: Dataset with Augmentation
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增强数据集
- en: '**How does BERT interpret fake reviews?**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT 如何解读虚假评论？**'
- en: BERT underperformed with an accuracy of 38.22%. I do not put the full fault
    on the BERT model. The generated data from ChatGPT lacked distinction between
    what constitutes a review as positive and negative, a clear indication that ChatGPT
    generates a lot of its data off of past patterns and is not sentient.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的表现不佳，准确率为 38.22%。我不把全部责任归咎于 BERT 模型。ChatGPT 生成的数据在区分正面和负面评论方面缺乏明确性，这清楚地表明
    ChatGPT 生成的数据很大程度上基于过去的模式，而不具备意识。
- en: '![](../Images/7547480915c0e1616b7fbd8375133994.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7547480915c0e1616b7fbd8375133994.png)'
- en: BERT Confusion Matrix with Generated Data (Image from Author)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 混淆矩阵与生成数据（图片来自作者）
- en: The most significant problem in the overhead confusion matrix is where BERT
    classified many reviews as positive, when in fact they were negative. *Why*? Well,
    for starters more negative reviews had to be generated by ChatGPT than positive
    reviews. Clearly, the patterns which ChatGPT used for creating its negative reviews
    were too similar to the positive reviews, encompassing one of the downfalls of
    data generation and ChatGPT’s inability to produce diversity within different
    categories of data distinct and aligned with real-world information.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在总体混淆矩阵中，最显著的问题是 BERT 将许多评论错误分类为正面，而实际上它们是负面的。*为什么*？首先，ChatGPT 需要生成的负面评论比正面评论更多。显然，ChatGPT
    用于创建负面评论的模式与正面评论过于相似，这突显了数据生成的一个缺陷，以及 ChatGPT 无法在不同类别的数据中产生多样性，这些数据与现实世界的信息不一致。
- en: Fine-tuning BERT with the Augmented Dataset
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用增强数据集对 BERT 进行微调
- en: '![](../Images/0e8e9faf6b8563b92b47984d39222368.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e8e9faf6b8563b92b47984d39222368.png)'
- en: Balanced Dataset (Image from Author)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡数据集（图片来自作者）
- en: With ChatGPT, the dataset was balanced to contain 1,126 positive reviews and
    1,124 negative reviews (an additional 614 positive reviews and 992 negative reviews).
    ***One benefit of using Generative AI algorithms is their ability to balance datasets,
    especially those with huge imbalances like this one.*** *The downfall*? The newly
    generated data may not be representative of the original data and this needs to
    be taken into consideration.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ChatGPT 后，数据集被平衡为包含 1,126 条正面评论和 1,124 条负面评论（额外的 614 条正面评论和 992 条负面评论）。***使用生成式
    AI 算法的一个好处是它们能够平衡数据集，特别是像这样的巨大不平衡数据集。*** *缺点*？新生成的数据可能无法代表原始数据，这需要考虑到。
- en: Once the dataset is balanced, we can attempt to finetune BERT again for sentiment
    analysis using the same processes as before. One distinct change was the number
    of words for each embedding decreased in size due to the average length being
    reduced by the generated reviews (150 → 60).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集平衡，我们可以尝试再次微调BERT进行情感分析，使用之前相同的过程。一个明显的变化是，由于生成的评论（150 → 60）导致每个嵌入的单词数量减少了。
- en: '![](../Images/4f4d85a07168d4c16cf76653c32f2c0c.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f4d85a07168d4c16cf76653c32f2c0c.png)'
- en: 'Training/Test Loss & Accuracy: Augmented Dataset (Image from Author)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 训练/测试损失与准确率：增强数据集（图片来自作者）
- en: Success! Dataset augmentation improved the fine-tuning of the BERT model. After
    5 epochs, an accuracy of **99.67%** was achieved with little loss. The test lost
    stayed consistently around 13.3% with an accuracy of 97.3% at epoch 5.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！数据集增强改善了BERT模型的微调。在5个周期后，准确率达到了**99.67%**，损失很小。测试损失始终维持在约13.3%左右，第五周期的准确率为97.3%。
- en: '*How would this model benefit a company in the food industry that uses it?*
    While using the data from just the company could lead to a model that is more
    *personable* and *aligned* with their processes, using external data can help
    provide a model to the company which is more *generalizable* and *adaptable* to
    real-world changes. These changes, which are almost guaranteed to occur, may be
    alien to the model, and having a model which can adapt to the unpredictability
    of the real world mitigates its failure and works as a stopgap against undesired
    decisions taken by the model.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*这个模型如何惠及使用它的食品行业公司？* 虽然使用公司内部的数据可能会导致一个更*个性化*、*与其流程对齐*的模型，但使用外部数据可以帮助公司提供一个更*通用*、*适应*现实世界变化的模型。这些变化几乎是不可避免的，模型可能会对此感到陌生，拥有一个能够适应现实世界不可预测性的模型可以减少失败，并作为对模型做出不良决策的缓冲。'
- en: Conclusion
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Generative AI has recently exploded, and finding appropriate use cases which
    are positive and beneficial to companies in different industries is important.
    For companies in the food industry, or any company which has a product and reviews
    about said product, ChatGPT can help support models which can flag reviews as
    positive and negative to support business operations and product development.
    While ChatGPT can be used for automation, we should be wary of its power, taking
    a *human-on-the-loop* approach and evaluate the generated data. Whatever Generative
    AI technique you use for your dataset development, ensure the data is realistic
    to the real world information it wishes to capture in hopes of create the strongest
    performing model possible.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI最近迅猛发展，找到适合并对不同产业公司有积极和有益的用例非常重要。对于食品行业的公司，或任何拥有产品及其评论的公司，ChatGPT可以帮助支持能够将评论标记为正面和负面的模型，以支持业务运营和产品开发。虽然ChatGPT可以用于自动化，但我们应该警惕其强大功能，采取*人类参与*的方法，评估生成的数据。不论你使用何种生成式AI技术进行数据集开发，确保数据真实反映现实世界信息，以期创造出表现最强的模型。
- en: On a more personal note, today showed how we can always try to get better, and
    we must learn from our poor past performances to get better. Learning from your
    mistakes and weaknesses is one of the most important parts of being a Data Scientist,
    and ultimately will foster a career makred with excellence and focused on continuous
    development.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 从个人角度来看，今天展示了我们如何不断努力变得更好，我们必须从过去的不佳表现中学习以取得进步。从错误和弱点中学习是数据科学家最重要的部分之一，*最终*将促成一个以卓越和持续发展为核心的职业生涯。
- en: '**If you enjoyed today’s reading, PLEASE give me a follow and let me know if
    there is another topic you would like me to explore! If you do not have a Medium
    account, sign up through my link** [**here**](https://ben-mccloskey20.medium.com/membership)
    **(I receive a small commission when you do this)! Additionally, add me on** [**LinkedIn**](https://www.linkedin.com/in/benjamin-mccloskey-169975a8/),
    **or feel free to reach out! Thanks for reading!**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你喜欢今天的阅读，请关注我，并告诉我是否有其他主题你希望我深入探讨！如果你没有Medium账号，通过我的链接** [**这里**](https://ben-mccloskey20.medium.com/membership)
    **（这样我会获得少量佣金）注册吧！此外，可以在** [**LinkedIn**](https://www.linkedin.com/in/benjamin-mccloskey-169975a8/)
    **上添加我，或者随时联系我！感谢阅读！**'
- en: Sources
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来源
- en: Data usage approved by Altomontes Inc.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据使用经Altomontes Inc.批准。
- en: Code
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
