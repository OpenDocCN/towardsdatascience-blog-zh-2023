- en: An AI-Powered Analysis of our Postal Service Through Tweets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/an-ai-powered-analysis-of-our-postal-service-through-tweets-fa1764409905](https://towardsdatascience.com/an-ai-powered-analysis-of-our-postal-service-through-tweets-fa1764409905)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deciphering Customer Voices with AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Delving into Machine Learning, Topic Modeling, and Sentiment Analysis to Uncover
    Valuable Customer Perspectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://johnadeojo.medium.com/?source=post_page-----fa1764409905--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----fa1764409905--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fa1764409905--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fa1764409905--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----fa1764409905--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fa1764409905--------------------------------)
    ¬∑13 min read¬∑Mar 22, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ca21fa5a67996b27ed77e77c334b936.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: AI generated sentiment and topic for #royalmail'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My partner and I usually experience an excellent postal service. Most of the
    time letters arrive to our home un-opened and delivered in a timely fashion. That‚Äôs
    why when our post didn‚Äôt arrive for a few weeks we thought it was quite strange.
    After some diligent web searching, we discovered the most likely cause to this
    service disruption was strikes. As a data scientist this whole episode got me
    thinking‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: Is there a way to leverage online data to track these types of incidents?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The answer to this question is yes, and I have already built a prototype which
    is available for you to play with. I recommend doing so before reading on as it
    will give you a feel for things before getting into the technical details.
  prefs: []
  type: TYPE_NORMAL
- en: üåè [Explore the m(app)](https://john-adeojo-royalmail-dash-scriptsstreamlitstreamlit-rm-o2f2wo.streamlit.app/)
    ‚Äî This is best opened on a computer, although it will work on a mobile phone.
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôll spend the remainder of this write up walking you through how I went about
    answering this question. This is pretty much an end to end machine learning project
    exploring aspects of software engineering, social media data mining, topic modelling,
    transformers, custom loss functions, transfer learning, and data visualisation.
    If that sounds interesting to you at all grab a snack or a drink and get comfortable
    because this might be quite a long one but hopefully worth the read.
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer***: This article is an independent analysis of tweets containing
    the #royalmail hashtag and is not affiliated with, endorsed, or sponsored by Royal
    Mail Group Ltd. The opinions and findings expressed within this article are solely
    those of the author and do not represent the views or official positions of Royal
    Mail Group Ltd or any of its subsidiaries.*'
  prefs: []
  type: TYPE_NORMAL
- en: Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When seeking to understand what people think, Twitter is always a good starting
    point. Much of what people post on Twitter is public and easily accessible through
    their API. It‚Äôs the kind of no holds barred verbal arena you would expect to find
    plenty of insights on customer service. I got curious and conducted a quick twitter
    search myself starting simply with ‚Äò#royalmail‚Äô. And voila! a tonne of tweets.
  prefs: []
  type: TYPE_NORMAL
- en: With my data source identified, the next thing I did was to figure out how I
    would ‚Äúmine‚Äù issues raised from those tweets. Topic modelling came to mind immediately
    as something to try. I figured that using some kind of clustering on the tweets
    could reveal some latent topics. I‚Äôll spend the remainder of the write up going
    into some technical details. This won‚Äôt be a step-by-step, but rather a peek over
    my shoulder and a window into my thought process in putting this project together.
  prefs: []
  type: TYPE_NORMAL
- en: Software Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Development environment**: I do the majority of my ML projects in python
    so my preferred IDE is Jupyter labs. I find it useful to be able to quickly toggle
    between Jupyter notebooks, python scripts, and the terminal.'
  prefs: []
  type: TYPE_NORMAL
- en: '**File structure**: This is a rather complex project, if I do say so myself.
    There are several processes to consider here and therefore it‚Äôs not something
    that could just be done from the safety of a Jupyter notebook. Listing out all
    of these we have; data extraction, data processing, topic modeling, machine learning,
    and data visualisation. To help create some order I usually start by establishing
    an appropriate file structure. You can, and probably should leverage bash scripting
    to do this.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Modularisation**: I broke each process down into modules making it easy to
    re-use, adapt and tweak things for different use cases. Modules also help keep
    your code ‚Äòclean‚Äô. Without the modular approach I would have ended up with a Jupyter
    notebook or python script thousands of lines long, very unappealing and difficult
    to de-bug.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Version control**: With complex projects, you do not want to lose your progress,
    overwrite something important, or mess up beyond repair. GitHub is really the
    perfect solution for this as it makes it hard to mess up badly. I get started
    by creating a remote repo and cloning it to my local machine allowing me to sleep
    easy knowing all my hard work is backed up. GitHub desk top allows me to carefully
    track any changes before committing them back to the remote repository.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Packages:** I leveraged a tonne of open source packages, I‚Äôll list the key
    ones below and provide links.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Transformers](https://huggingface.co/docs/transformers/installation): API
    for hugging face large language model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pytorch](https://pytorch.org/get-started/locally/): Framework for building
    and customising transformers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Streamlit](https://streamlit.io/): For building web applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Scikit Learn](https://scikit-learn.org/stable/install.html): Framework for
    machine learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[UMAP](https://umap-learn.readthedocs.io/en/latest/): Open source implementation
    of the UMAP algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html):
    Open source implementation of the HDSCAN algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Folium](https://python-visualization.github.io/folium/#:~:text=folium%20makes%20it%20easy%20to,as%20markers%20on%20the%20map.):
    For geographic data visualisation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CUDA](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/):
    Parallel computing platform for leveraging the power of your GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Seaborn](https://seaborn.pydata.org/): A library for data visualisation in
    python.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pandas](https://pandas.pydata.org/): A library for handling structured data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Numpy](https://numpy.org/): A library for performing numeric operations in
    python.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment management**: Having access to a wealth of libraries on the internet
    is fantastic, but your environment can quickly run away with you. To manage this
    complexity I like to enforce upon myself a clean environment policy whenever I
    start a new project. It‚Äôs strictly one environment per project. I choose to use
    [Anaconda](https://www.anaconda.com/) as my choice of environment manager because
    of the flexibility it gives me.'
  prefs: []
  type: TYPE_NORMAL
- en: '*note: for the purposes of this project I did create separate environments
    and GitHub repositories for the streamlet web application and the topic modeling.*'
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I used the Twitter API to extract around 30k publicly available tweets searching
    #royalmail. I want to stress here that only data that is publicly available can
    be extracted with the Twitter API alleviating some of the data privacy concerns
    one may have.'
  prefs: []
  type: TYPE_NORMAL
- en: Twitter data is incredibly messy and notoriously difficult to work with for
    any natural language processing (nlp) tasks. It‚Äôs social media data loaded with
    emoji‚Äôs, grammatical inconsistencies, special characters, expletives, URLS, and
    every other hurdle that comes with free form text. I wrote my own custom scripts
    to clean the data for this particular project. It was primarily getting rid of
    URLs and annoying stop words. I have given a snippet for the ‚Äúlite‚Äù version, but
    I did also use a more heavy duty version during clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Module for cleaning URLs from tweets
  prefs: []
  type: TYPE_NORMAL
- en: '*Please note that this is within Twitters terms of service. They allow analysis,
    aggregation of publicly available data via* [*their API*](https://developer.twitter.com/en/developer-terms/agreement-and-policy)*.
    The data is permitted for both* [*non-commercial and commercial use*](https://developer.twitter.com/en/developer-terms/commercial-terms#:~:text=Know%20that%20if%20you%20are,access%20are%20available%20for%20free).&text=Your%20product%20or%20service%20is%20monetized%20if%20you%20earn%20money%20from%20it.)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Topic Modelling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The topic modelling approach I used draws inspiration from BERT topic¬π. I had
    initially tried Latent Dirichlet Allocation , but struggled to get anything coherent.
    BERT topic was a great reference point, but I had noticed that it hadn‚Äôt explicitly
    been designed to extract topics from messy Twitter data. Following many of the
    same logical steps as BERT topic, I adapted the approach a little bit for the
    task.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level BERT topic uses the BERT model to generate embeddings, performs
    dimensionality reduction and clustering to reveal latent topics in documents.
  prefs: []
  type: TYPE_NORMAL
- en: My approach leveraged the twitter-xlm-roberta-base¬≤ model to generate embeddings.
    This transformer has been pretrained on twitter data and captures all the messy
    nuances, emojis and all. Embeddings, are simply a way to represent sentences in
    numeric form such that both syntactical and semantical information is preserved.
    Embeddings are learnt by transformers through self-attention. The amazing thing
    about all the recent innovation in the large language model space is that one
    can leverage state-of-the-art models to generate embeddings for one‚Äôs own purposes.
  prefs: []
  type: TYPE_NORMAL
- en: I used the UMAP algorithm to project the tweet embeddings into a two dimensional
    space and HDBSCAN to identify clusters. Treating each cluster as a document, I
    generated TF-IDF scores to extract a list of key words that roughly ‚Äòdefine‚Äô each
    cluster forming my initial topics.
  prefs: []
  type: TYPE_NORMAL
- en: '*TF-IDF is a handy way to measure a word‚Äôs significance in a cluster, considering
    how often it appears in that specific cluster and how rare it is in a larger group
    of clusters. It helps identify words that are unique and meaningful in each cluster.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Some of these dimensionality reductions can be hard to make sense of at first.
    I found these resources useful for helping me get to grips with the algorithms.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Understanding UMAP*](https://pair-code.github.io/understanding-umap/) *‚Äî
    An excellent resource that helps you visualise and understand the impact of adjusting
    hyperparameters.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[*HDBSCAN Documentation*](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)
    *‚Äî The most coherent explanation of HDBSCAN I could find was provided in the documentation
    itself.*'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, I tested the coherence of the topics generated by scoring the cosine
    similarity between the topics and the tweets themselves. This sounds rather formulaic
    on paper, but I can assure you this was no straight forward task. Unsupervised
    machine learning of this nature is just trial and error. It took me dozens of
    iterations and manual effort to find the right parameters to get coherent topics
    out of these tweets. So rather than going into the specifics of all the hyperparameters
    I used, I will just talk about the four critical ones that were really a make
    or break for this approach.
  prefs: []
  type: TYPE_NORMAL
- en: '**Distance metrics**: for topic modelling the distance metric is really the
    difference between forming coherent topics and just generating a random list of
    words. For both UMAP and HDBSCAN I chose cosine distance. The choice here was
    a no-brainer considering my objective, to model topics. Topics are semantically
    similar groups of text, and the best way to measure semantic similarity is cosine
    distance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of words**: after generating the clusters I wanted to understand the
    ‚Äúcontents‚Äù of those clusters through TF-IDF. The key metric of choice here is
    how many words to return for each cluster. This could range from one to the number
    of unique words in the whole corpus of text. Too many words, and your topics become
    incoherent, too few and you end up with poor coverage of your cluster. Selecting
    this was a matter of trial and error, after several iterations I landed on 4 words
    per topic.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scoring**: Topic modelling isn‚Äôt an exact science, so some manual intervention
    is required to make sure topics made sense. I could do this for a few hundred
    or even a few thousand tweets, but tens of thousands? That‚Äôs not practically feasible.
    So I used a numeric ‚Äúhack‚Äù by scoring the cosine similarity between the TFIDF
    topics generated and the tweets themselves. Again this was a lot of trial and
    error but after several iterations I found an appropriate cut off for cosine similarity
    to be around 0.9\. This left me with around 3k from the original 30k that were
    fairly well categorised. Most importantly, it was a large enough sample size to
    do some supervised machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Topics in 2d:** UMAP provides a convenient way to visualise the topics. What
    we can see is that there is a mass of topics in the centre that have been clustered
    together with some smaller niche topics on the edge. It actually reminds me a
    bit of a galaxy. After doing some detective work (manual trawling through spreadsheets)
    I found this to make sense. The mass of topics in the centre are mainly around
    customer service, often complaints. What I thought was particularly fascinating
    was the model‚Äôs ability to actually isolate very niche areas. These included politics,
    economics, employment, and philately (which isn‚Äôt some minor celebrity, but the
    collection of stamps!). Of course, topics returned by TFIDF were no where near
    this coherent, but I was able to identify 6 well categorised topics from the analysis.
    My final 6 topics were customer service, politics, royal reply, jobs, financial
    news, and philately.'
  prefs: []
  type: TYPE_NORMAL
- en: '**List of four words topics generated by TF-IDF on the clusters and taking
    the 0.9+ cosine similarity to tweets.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'apprenticeship, jinglejobs, job, label: **Jobs**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'biggest, boss, revolt, year: **Politics**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'birth, reply, royalletters, royalreply: **Royal Reply**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'collecting, pack, philatelist, philately: **Philately**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'declares, plc, position, short: **Financial News**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'definitive, philatelist, philately, presentation: **Philately**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'driving, infoapply, job, office: **Jobs**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'driving, job, sm1jobs, suttonjobs: **Jobs**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ftse, rmg, share, stock: **Financial News**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'germany, royal, royalletter, royalreply: **Royal Reply**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'gradjobs, graduatescheme, jobsearch, listen: **Jobs**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'labour, libdems, tory, uk: **Politics**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'letter, mail, service, strike: **Customer Service**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'luxembourg, royal, royalletter, royalreply: **Royal Reply**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'new, profit, shareholder, world: **Financial News**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'plc, position, reduced, wace: **Financial News**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8bc6976977c0bb1f8442f12aae195bf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: A 2d representation of the embedding after applying UMAP'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8bd6950f177d003c98cf88bfbe2c538b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: A view of the the topics after applying HDBSCAN. Yellow mass
    is customer service related'
  prefs: []
  type: TYPE_NORMAL
- en: '*The topic modelling was fiddly and definitely not something you want to rely
    on continuously for generating insights. As far as I‚Äôm concerned it should be
    an exercise that you conduct once every few months or so (depending on the fidelity
    of your data), just in case anything new comes up.*'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having performed the arduous task of topic modelling, I had some labels and
    a decent sized data set of just under 3k observations for training a model. Leveraging
    a pretrained transformer means not having to train from scratch, not having to
    build my own architecture and harnessing the power of the model‚Äôs existing knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Data Splitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I proceeded with the standard Train, Validation, and Test splits with 80% of
    the observations being allocated to train. See script below:'
  prefs: []
  type: TYPE_NORMAL
- en: Data splitting script
  prefs: []
  type: TYPE_NORMAL
- en: Implementing focal loss with a custom trainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model training turned out to be less straight forward than I had anticipated,
    and this wasn‚Äôt because of the hardware requirements but rather the data itself.
    What I was dealing with was a highly imbalanced multiclass classification problem.
    Customer service observations were at least ten times as prominent in the data
    set than the next most prominent class. This caused the model performance to be
    overwhelmed by the customer service class leading to low recall and precision
    for the less prominent classes.
  prefs: []
  type: TYPE_NORMAL
- en: I started with something simple initially applying class weights and cross entropy
    loss, but this didn‚Äôt do the trick. After a quick google search I discovered that
    the loss function focal loss has been used successfully to solve class imbalance.
    Focal loss reshapes the cross entropy loss to ‚Äúdown-weight‚Äù the loss assigned
    to well classified examples¬≥.
  prefs: []
  type: TYPE_NORMAL
- en: '*The original paper on focal loss focussed on computer vision tasks where images
    had shallow depth of field. The image below is an example of shallow depth of
    field, the foreground is prominent but the background very low res. This type
    of extreme imbalance between foreground and background is analogous to the imbalance
    I had to deal with to classify the tweets.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00dea754e944d517bbed76b7eeea6859.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Raphael Wild](https://unsplash.com/@veloradio?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Below I have laid out my implementation of focal loss within a custom trainer
    object.
  prefs: []
  type: TYPE_NORMAL
- en: '*note that the class weights (alpha) are hard coded. You will need to adjust
    these if you want to use this for you own purposes.*'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of the focal loss. Custom trainer is just standard trainer with
    added focal loss
  prefs: []
  type: TYPE_NORMAL
- en: Model Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After a bit of customisation I was able to fit a model (and in under 7 minutes
    thanks to my GPU and CUDA). Focal loss vs. time gives us some evidence that the
    model was close to converging.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a714d949b1f46e3251ba33e5b444f62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Focal loss vs time step'
  prefs: []
  type: TYPE_NORMAL
- en: Model training script. Notice the customer trainer is imported and implemented
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Model Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model was assessed on the test data set which included 525 randomly selected
    labelled examples. The performance appears impressive, with fairly high precision
    and recall across all classes. I would caveat that test performance is probably
    optimistic due to the small sample size and there is likely to be more variance
    in the nature of these tweets outside of our sample. However, we are dealing with
    a relatively narrow domain (#royalmail) so variance is likely to be narrower than
    it would be for something more general purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b42ce43a03c1d467a4a40e164bf175a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: confusion matrix (test dataset)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/387cd526d73841e894dd49cb85406f8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: model performance metrics on Test'
  prefs: []
  type: TYPE_NORMAL
- en: Geographic Visualisation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To effectively visualize the wealth of information I gathered, I decided to
    create a sentiment map. By utilizing my trained model, I generated topics for
    tweets posted between January and March 2023\. Additionally, I employed the pretrained
    `[twitter-roberta-base-sentiment](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)`
    model from Cardiff NLP to assess the sentiment of each tweet. To build the final
    web application, I used Streamlit.
  prefs: []
  type: TYPE_NORMAL
- en: script for generating the Streamlit web application
  prefs: []
  type: TYPE_NORMAL
- en: Business Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The current app serves as a basic prototype, but it can be expanded to uncover
    more profound insights. I‚Äôll briefly discuss a few potential extensions below:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Temporal Filtering**: Incorporate a date range filter, allowing users to
    explore tweets within specific time periods. This can help identify trends and
    changes in sentiment over time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Interactive Visualizations**: Implement interactive charts and visualizations
    that enable users to explore relationships between sentiment, topics, and other
    factors in the dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Real-time Data**: Connect the app to live Twitter data, enabling real-time
    analysis and visualization of sentiment and topics as they emerge.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Advanced Filtering**: Provide more advanced filtering options, such as filtering
    by user, hashtag, or keyword, to allow for more targeted analysis of specific
    conversations and trends.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By extending the app with these features, you can provide users with a more
    powerful and insightful tool for exploring and understanding sentiment and topics
    in tweets.
  prefs: []
  type: TYPE_NORMAL
- en: '[GitHub Repo](https://github.com/john-adeojo/twitter_issues_dashboard)'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@johnadeojo/membership?source=post_page-----fa1764409905--------------------------------)
    [## Join Medium with my referral link - John Adeojo'
  prefs: []
  type: TYPE_NORMAL
- en: I share data science projects, experiences, and expertise to assist you on your
    journey You can sign up to medium via‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@johnadeojo/membership?source=post_page-----fa1764409905--------------------------------)
    [](https://www.john-adeojo.com/?source=post_page-----fa1764409905--------------------------------)
    [## Home | John Adeojo
  prefs: []
  type: TYPE_NORMAL
- en: About Me An experienced data scientist and machine learning (ML) expert specialising
    in building bespoke ML powered‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.john-adeojo.com](https://www.john-adeojo.com/?source=post_page-----fa1764409905--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Citations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1]Grootendorst, M. (2022). *BERTopic: Neural topic modeling with a class-based
    TF-IDF procedure*. Paperswithcode.com. [https://paperswithcode.com/paper/bertopic-neural-topic-modeling-with-a-class](https://paperswithcode.com/paper/bertopic-neural-topic-modeling-with-a-class)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2]Barbieri, F., Anke, L. E., & Camacho-Collados, J. (2022). *XLM-T: Multilingual
    Language Models in Twitter for Sentiment Analysis and Beyond*. Paperswithcode.com.
    [https://arxiv.org/abs/2104.12250](https://arxiv.org/abs/2104.12250)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3]Lin, T.-Y., Goyal, P., Girshick, R., He, K. and Dollar, P. (2018). Focal
    Loss for Dense Object Detection. *Facebook AI Research (FAIR)*. [online] Available
    at: [https://arxiv.org/pdf/1708.02002.pdf](https://arxiv.org/pdf/1708.02002.pdf)
    [Accessed 21 Mar. 2023].'
  prefs: []
  type: TYPE_NORMAL
