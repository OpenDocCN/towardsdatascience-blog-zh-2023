- en: Transformers — Intuitively and Exhaustively Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb](https://towardsdatascience.com/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Exploring the modern wave of machine learning: taking apart the transformer
    step by step'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----58a5c5df8dbb--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----58a5c5df8dbb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58a5c5df8dbb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58a5c5df8dbb--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----58a5c5df8dbb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58a5c5df8dbb--------------------------------)
    ·15 min read·Sep 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/edb20757aa401a62d13a932e35ee4b95.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Image by author using MidJourney. All images by the author unless otherwise
    specified.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In this post you will learn about the transformer architecture, which is at
    the core of the architecture of nearly all cutting-edge large language models.
    We’ll start with a brief chronology of some relevant natural language processing
    concepts, then we’ll go through the transformer step by step and uncover how it
    works.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '**Who is this useful for?** Anyone interested in natural language processing
    (NLP).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '**How advanced is this post?** This is not a complex post, but there are a
    lot of concepts, so it might be daunting to less experienced data scientists.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-requisites:** A good working understanding of a standard neural network.
    Some cursory experience with embeddings, encoders, and decoders would probably
    also be helpful.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: A Brief Chronology of NLP Up To The Transformer
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following sections contain useful concepts and technologies to know before
    getting into transformers. Feel free to skip ahead if you feel confident.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Word Vector Embeddings
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A conceptual understanding of word vector embeddings is pretty much fundamental
    to understanding natural language processing. In essence, a word vector embedding
    takes individual words and translates them into a vector which somehow represents
    its meaning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc19100e60b99abf50827b475f328713.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: 'The job of a word to vector embedder: turn words into numbers which somehow
    capture their general meaning.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The details can vary from implementation to implementation, but the end result
    can be thought of as a “space of words”, where the space obeys certain convenient
    relationships. **Words are hard to do math on, but vectors which contain information
    about a word, and how they relate to other words, are significantly easier to
    do math on.** This task of converting words to vectors is often referred to as
    an “embedding”.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vect, a landmark paper in the natural language processing space, sought
    to create an embedding which obeyed certain useful characteristics. Essentially,
    they wanted to be able to do algebra with words, and created an embedding to facilitate
    that. With Word2Vect, you could embed the word “king”, subtract the embedding
    for “man”, add the embedding for “woman”, and you would get a vector who’s nearest
    neighbor was the embedding for “queen”.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7c80912fb726a665edb7051aa12138d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: A conceptual demonstration of doing algebra on word embeddings. If you think
    of each of the points as a vector from the origin, if you subtracted the vector
    for “man” from the vector for “king”, and added the vector for “woman”, the resultant
    vector would be near the word queen. In actuality these embedding spaces are of
    much higher dimensions, and the measurement for “closeness” can be a bit less
    intuitive (like cosine similarity), but the intuition remains the same.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: As the state of the art has progressed, word embeddings have maintained an important
    tool, with GloVe, Word2Vec, and FastText all being popular choices. Sub-word embeddings
    are generally much more powerful than full word embeddings, but are out of scope
    of this post.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Networks (RNNs)
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we can convert words into numbers which hold some meaning, we can start
    analyzing sequences of words. One of the early strategies was using a recurrent
    neural network, where you would train a neural network that would feed into itself
    over sequential inputs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3a6d104fc11d9c3d2ebc8f65e8f1fd4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: The general idea of an RNN, which is a normal fully connected neural network
    which feeds into itself.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d8049d2ef520a6125a2fec62ffbf303.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: What an RNN might look like if it had 3 hidden neurons and was used across 2
    inputs. The arrows in red are the recursive connections which connect the information
    from subsequent recurrent layers. The blue arrows are internal connections, like
    a dense layer. The neural network is copied for illustrative purposes, but keep
    in mind the network is actually feeding back into itself, meaning the parameters
    for the second (and subsequent) modules would be the same as the first.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Unlike a traditional neural network, because recurrent networks feed into themselves
    they can be used for sequences of arbitrary length. they will have the same number
    of parameters for a sequences of length 10 or a sequence of length 100 because
    they reuse the same parameters for each recursive connection.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: This network style was employed across numerous modeling problems which could
    generally be categorized as sequence to sequence modeling, sequence to vector
    modeling, vector to sequence modeling, and sequence to vector to sequence modeling.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/936d21d358a8f635f28ff1d1bdf1ce66.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Conceptual diagrams of a few applications of different modeling strategies which
    might use RNNs. Sequence to Sequence might be predicting the next word for text
    complete. Sequence to vector might be scoring how satisfied a customer was with
    a review. Vector to sequence might be compressing an image into a vector and asking
    the model to describe that image as a sequence of text. Sequence to vector to
    sequence might be text translation, where you need to understand a sentence, compress
    it into some representation, then construct a translation of that compressed representation
    in a different language.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: While the promise of infinite length sequence modeling is enticing, it’s not
    practical. Because each layer shares the same weights it’s easy for recurrent
    models to forget the content of inputs. As a result, RNNs could only practically
    be used for very short sequences of words.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: There were some attempts to solve this problem by using “gated” and “leaky”
    RNNs. The most famous of these was the LSTM, which is described in the next section.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Long/Short Term Memory (LSTMs)
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The LSTM was created as an attempt to improve the ability of recurrent networks
    to recall important information. LSTM’s have a short term and long term memory,
    where certain information can be checked into or removed from the long term memory
    at any given element in the sequence.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3991fe7436ae9116f390d042247bc17.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: an LSTM in a nutshell
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Conceptually, an LSTM has three key subcomponents, the “forget gate” which is
    used to forget previous long term memories, the “input gate” which is used to
    commit things to long term memory, and the “output gate” which is used to formulate
    the short term memory for the next iteration.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3cef86252cf77623f233230baefe8610.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: The parameters in an LSTM. This particular LSTM expects an input vector of dimension
    3, and holds internal state vectors of dimension 2\. the dimension of vectors
    is a configurable hyperparameter. Also, notice the “S” and “T” at the end of each
    of the gates. These stand for sigmoid or tanh activation functions, which are
    used to squash values into certain ranges, like 0 to 1 or -1 to 1\. This “squashing”
    is what allows the network to “forget” and “commit to memory” certain information.
    image by the author, heavily inspired by [Source](/animated-rnn-lstm-and-gru-ef124d06cf45)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs, and similar architectures like GRUs, proved to be a significant improvement
    on the classic RNN discussed in the previous section. The ability to hold memory
    as a separate concept which is checked in and checked out of proved to be incredibly
    powerful. However, while LSTMs could model longer sequences, they were too forgetful
    for many language modeling tasks. Also, because they relied on previous inputs
    (like RNNs), their training was difficult to parallelize and, as a result, slow.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Attention Through Alignment
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Landmark Paper, [Neural Machine Translation by Jointly Learning to Align
    and Translate](https://arxiv.org/abs/1409.0473) popularized the general concept
    of attention and was the conceptual precursor to the multi-headed self attention
    mechanisms used in transformers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: I have a whole [article on this specific topic](https://medium.com/roundtableml/attention-from-alignment-practically-explained-548ef6588aa4),
    along with example code in PyTorch. In a nutshell, the attention mechanism in
    this paper looks at all potential inputs and decides which one to present to an
    RNN at any given output. **In other words, it decides which inputs are currently
    relevant, and which inputs are not currently relevant**.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://blog.roundtableml.com/attention-from-alignment-practically-explained-548ef6588aa4?source=post_page-----58a5c5df8dbb--------------------------------)
    [## Attention from Alignment, Practically Explained'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Learn from what matters, Ignore what doesn’t.
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: blog.roundtableml.com](https://blog.roundtableml.com/attention-from-alignment-practically-explained-548ef6588aa4?source=post_page-----58a5c5df8dbb--------------------------------)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: This approach proved to have a massive impact, particularly in translation tasks.
    It allowed recurrent networks to figure out which information is currently relevant,
    thus allowing previously unprecedented performance in translation tasks specifically.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e96ba2aa559332e97758204b267938f2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: A figure from the linked article. The squares represent the word vector embeddings,
    and the circles represent intermediary vector representations. The red and blue
    circles are hidden states from a recurrent network, and the white circles are
    hidden states created by the attention through alignment mechanism. The punchline
    is that the attention mechanism can choose the right inputs to present to the
    output at any given step.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections we covered some forest through the trees knowledge.
    Now we’ll look at the transformer, which used a combination of previously successful
    and novel ideas to revolutionize natural language processing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67d76e5da4c7da11abe8fa358593a263.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: The transformer diagram. [source](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: We’ll go through the transformer element by element and discuss how each module
    works. There’s a lot to go over, but it’s not math-heavy and the concepts are
    pretty approachable.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: High Level Architecture
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At it’s most fundamental, the transformer is an encoder/decoder style model,
    kind of like the sequence to vector to sequence model we discussed previously.
    The encoder takes some input and compresses it to a representation which encodes
    the meaning of the entire input. The decoder then takes that embedding and recurrently
    constructs the output.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ab4230b9e11ee438636995bbba2317e.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: A transformer working in a sequence to vector to sequence task, in a nutshell.
    The input (I am a manager) is compressed to some abstract representation that
    encodes the meaning of the entire input. The decoder works recurrently, like our
    RNNs previously discussed, to construct the output.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Input Embedding and Positional Encoding
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eac5999b4ebbaaab2705a3f3fc62f80b.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Input embedding within the original diagram. [source](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The input embedding for a transformer is similar to previously discussed strategies;
    a word space embedder similar to word2vect converts all input words into a vector.
    This embedding is trained alongside the model itself, as essentially a lookup
    table which is improved through model training. So, there would be a randomly
    initialized vector corresponding to each word in the vocabulary, and this vector
    would change as the model learned about each word.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Unlike recurrent strategies, transformers encode the entire input in one shot.
    As a result the encoder might lose information about the location of words in
    an input. To resolve this, transformers also use positional encoders, which is
    a vector encoding information about where a particular word was in the sequence.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/2a3b13933101dc0757176687c913fdc6.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: Example of positional encoding. The Y axis represents subsequent words, and
    the x axis represents values within a particular words positional encoding. Each
    row in this image represents an individual word.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/7a19f1ecdb46cf79387936d3962b1b5d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: Values of the position vector relative to different indexes in a sequence. K
    represents the index in a sequence, and the graph represents values in a vector.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'This system uses the sin and cosin function in unison to encode position, which
    you can gain some intuition about in this article:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://blog.roundtableml.com/use-frequency-more-frequently-14715714de38?source=post_page-----58a5c5df8dbb--------------------------------)
    [## Use Frequency More Frequently'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: A handbook from simple to advanced frequency analysis. Exploring a vital tool
    which is widely underutilized in data…
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: blog.roundtableml.com](https://blog.roundtableml.com/use-frequency-more-frequently-14715714de38?source=post_page-----58a5c5df8dbb--------------------------------)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: I won’t harp on it, but a fascinating note; this system of encoding position
    is remarkably similarity to positional encoders used in motors, where two sin
    waves offset by 90 degrees allow a motor driver to understand position, direction,
    and speed of a motor.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The vector used to encode the position of a word is added to the embedding of
    that word, creating a vector which contains both information about where that
    word is in a sentence, and the word itself. You might think “if your adding these
    wiggly waves to the embedding vector, wouldn’t that mask some of the meaning of
    the original embedding, and maybe confuse the model”? To that, I would say that
    neural networks (which the transformer employs for it’s learnable parameters)
    are incredibly good at understanding and manipulating smooth and continuous functions,
    so this is practically of little consequence for a sufficiently large model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-Headed Self Attention: High Level'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is probably the most important sub-component of the transformer mechanism.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9db82d332f334cc053e58aafcdfad9c4.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: The multi-headed self attention mechanism within the original diagram. [source](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: In this authors humble opinion, calling this an “attention” mechanism is a bit
    of a misnomer. Really, it’s a “co-relation” and “contextualization” mechanism.
    It allows words to interact with other words to transform the input (which is
    a list of embedded vectors for each word) into a matrix which represents the meaning
    of the entire input.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d9141780d17ff587a29b0cac4b7ac59.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: Multi Headed self attention, in a nutshell. The mechanism mathematically combines
    the vectors for different words, creating a matrix which encodes a deeper meaning
    of the entire input.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'This mechanism can be thought of four individual steps:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Creation of the Query, Key, and Value
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Division into multiple heads
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Attention Head
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Composing the final output
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multi Head Self Att. Step 1) Creation of the Query, Key, and Value
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, don’t be too worried about the names “Query”, “Key”, and “Value”.
    These are vaguely inspired by databases, but really only in the most obtuse sense.
    The query, key, and value are essentially different representations of the embedded
    input which will be co-related to each-other throughout the attention mechanism.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5437782be2ff6d394a82062848453bce.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: Turning the embedded input into the query key and value. The input has a dimension
    which is num_words by embedding_size, The query, key, and value all have the same
    dimensions as the input. In essence, a dense network projects the input into a
    tensor with three times the number of features while maintaining sequence length.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: The dense network shown above includes the only learnable parameters in the
    multi headed self attention mechanism. Multi headed self attention can be thought
    of as a function, and the model learns the inputs (Query, Key, and Value) which
    maximizes the performance of that function for the final modeling task.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Multi Head Self Att. Step 2) Division into multiple heads
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we do the actual contextualization which makes self attention so powerful,
    we’re going to divide the query, key, and value into chunks. The core idea is
    that instead of co-relating our words one way, we can co-relate our words numerous
    different ways. In doing so we can encode more subtle and complex meaning.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e6aa573cebef5051a49a9638e442374.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: In this example we have 3 attention heads. As a result the query, key, and value
    are divided into 3 sections and passed to each head. Note that we’re dividing
    along the feature axis, not the word axis. Different aspects of each word’s embedding
    are passed to a different attention head, but each word is still present for each
    attention head.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Multi Head Self Att. Step 3) The Attention Head
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the sub-components of the query, key, and value which is passed
    to an attention head, we can discuss how the attention heads combines values in
    order to contextualize results. In Attention is all you need, this is done with
    matrix multiplication.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82ef6e9249baeccc399ca73acd5d45af.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: Matrix Multiplication. [Source](https://en.wikipedia.org/wiki/Matrix_multiplication#/media/File:Matrix_multiplication_diagram_2.svg)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: In matrix multiplication rows in one matrix get combined with columns in another
    matrix via a dot product to create a resultant matrix. In the attention mechanism
    the Query and Key are matrix multiplied together to create what I refer to as
    the “Attention Matrix”.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b758201a67271bc4aaabe5167f51d6b.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Calculating the attention matrix with the query and key. Note that the key is
    transposed to allow for matrix multiplication to result in the correct attention
    matrix shape.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: This is a fairly simple operation, and as a result it’s easy to underestimate
    its impact. The usage of a matrix multiplication at this point forces the representations
    of each word to be combined with the representations of each other word. Because
    the Query and Key are defined by a dense network, the attention mechanism learns
    how to translate the query and key to optimize the content of this matrix.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the attention matrix, it can be multiplied by the value matrix.
    This serves three key purposes:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Add a bit more contextualization by relating another representation of the input.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a system such that the query and key function to transform the value which
    allows for either self attention or cross attention depending on where the query,
    key, and value come from.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perhaps most importantly, it make the output of the attention mechanism the
    same size as the input, which makes certain implementation details easier to handle.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Important Correction**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: The attention matrix is softmaxed row wise before being multiplying by the value
    matrix. This single mathematical detail completely changes the conceptual implications
    of the attention matrix and its relationship with the value matrix.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: because each row in the attention matrix is softmaxed, each row becomes a probability,
    This is very similar to the attention through alignment concept I cover in a different
    article.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a53d42bdbb6ea8bc3af514762660db7.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: From my [attention through alignment article](/attention-from-alignment-practically-explained-548ef6588aa4).
    Each row is a probability distribution which sums to 1, forcing the most important
    things to be co-related with other important things.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: This detail is frequently lost in greater explanations on transformers, but
    it is arguably the most important operation in the transformer architecture as
    it turns vague correlation into something with sparse and meaningful choices.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3fda4dcbbf3a775146f918a8e50f21f.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: The attention matrix (which is the matrix multiplication of the query and key)
    multiplied by the value matrix to yield the final result of the attention mechanism.
    Because of the shape of the attention matrix, the result is the same shape as
    the value matrix. Keep in mind, this is the result from a single attention head.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Multi Head Self Att. Step 4) Composing the final output
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last section we used the query, key, and value to construct a new result
    matrix which has the same shape as the value matrix, but with significantly more
    context awareness.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the attention head only computes the attention for a subcomponent
    of the input space (divided along the feature axis).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e6aa573cebef5051a49a9638e442374.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Recall that the inputs were split into multiple attention heads. In this example,
    3 heads.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Each of these heads now outputs a different result, which can then be concatenated
    together.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72e8e1c0a4cd4c927303e588cc1c5ecc.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: The results of each attention head gets concatenated together
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: The shape of matrix is the same exact shape as the input matrix. However, unlike
    the input where each row related cleanly with a singular word, this matrix is
    much more abstract.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d9141780d17ff587a29b0cac4b7ac59.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: Recall that the attention mechanism, in a nutshell, transforms the embedded
    input into an abstract context rich representation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Add and Norm
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/a2c6c90e29890ac11c551427f41512a0.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: Add and Norm within the original diagram. [source](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The Add and Norm operations are applied twice in the encoder, and both times
    its effect is the same. There’s really two key concepts at play here; skip connections
    and layer normalization.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Skip connections are used all over the shop in machine learning. my favorite
    example is in image segmentation using a U-net architecture, if you’re familiar
    with that. Basically, when you do complex operations, it’s easy for the model
    to “get away from itself”. This has all sorts of fancy mathematical definitions
    like gradient explosions and rank collapse, but conceptually it’s pretty simple;
    a model can overthink a problem, and as a result it can be useful to re-introduce
    older data to re-introduce some of that simpler structure.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78a32038112aa03fa990fbe210a37c82.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: 'What a skip connected add might look like. In this example the matrix on the
    left represents the original encoded input. The matrix in the middle represents
    the hyper contextual result from the attention matrix. The Right represents the
    result of a skip connection: a context aware matrix which still contains some
    order from the original input.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Layer normalization is similar to skip connections in that it, conceptually,
    reigns in wonkiness. A lot of operations have been done to this data, which has
    resulted in who knows how large and small of values. If you do data science on
    this matrix, you might have to deal with both incredibly small and massively large
    values. This is known to be problematic.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Layer normalization computes the mean and standard deviation (how widely distributed
    the values are) and uses those values to squash the data back into a reasonable
    distribution.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Feed Forward
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/b28950a18ae9c716142281a3c741c1d0.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: Feed Forward within the original diagram. [source](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: This part’s simple. We can take the output from the add norm after the attention
    mechanism and pass it through a simple dense network. I like to see this as a
    projection, where the model can learn how to project the attention output into
    a format which will be useful for the decoder.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The output of the feed forward network is then passed through another add norm
    layer, and that results in the final output. This final output will be used by
    the decoder to generate output.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: General Function of the Decoder
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve completely covered the encoder, and have a highly contextualized representation
    of the input. Now we’ll discuss how the decoder uses that representation to generate
    some output.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58b4ebd22d397ff600a3fac8532c0db0.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: high level representation of how the output of the encoder relates to the decoder.
    the decoder references the encoded input for every recursive loop of the output.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: The decoder is very similar to the encoder with a few minor variations. Before
    we talk about the variations, let's talk about similarities
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67d76e5da4c7da11abe8fa358593a263.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: The Transformer Architecture [source](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in the image above, the decoder uses the same word to vector
    embedding approach, and employs the same positional encoder. The decoder uses
    “Masked” multi headed self attention, which we’ll discuss in the next section,
    and uses another multi-headed attention block.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: The second multi-headed self attention uses the encoded input for the key and
    value, and uses the decoder input to generate the query. As a result, the attention
    matrix gets calculated from the embedding for the encoder and the decoder, which
    then gets applied to the value from the encoder. This allows the decoder to decide
    what it should finally output based on both the encoder input and the decoder
    input.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The rest is the same boiler plate you might find on any other model. The results
    pass through another feed forward, an add norm, a linear layer, and a softmax.
    This softmax would then output probabilities for a bag of words, for instance,
    allowing the model to decide on a word to output.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Masked Multi Headed Self Attention
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So the only thing really new about the decoder is the “masked” attention. This
    has a to do with how these models are trained.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: One of the core flaws of recurrent neural networks is that you need to train
    them sequentially. An RNN intimately relies on the analysis of the previous step
    to inform the next step.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d8049d2ef520a6125a2fec62ffbf303.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: RNNs intimate dependencies between steps
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: This makes training RNNs incredibly slow as each sequence in the training set
    needs to be sequentially fed through the model one by one. With some careful modifications
    to the attention mechanism transformers can get around this problem, allowing
    the model to be trained for an entire sequence in parallel.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'The details can get a bit fiddly, but the essence is this: When training a
    model, you have access to the desired output sequence. As a result, you can feed
    the entire output sequence to the decoder (including outputs you havent predicted
    yet) and use a mask to hide them from the model. This allows you to train on all
    positions of the sequence simultaneously.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f23f8faa166561e07e311fe4585d8504.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: 'A diagram of the mask working in mask multiheaded self attention, for an english
    to french translation task. The input of this task is the phrase “I am a manager”
    and the desired output is the phrase “Je suis directeur”. Note, for simplicities
    sake, I’ve generally neglected the concept of utiltiy tokens. They’re pretty easy
    to understand, though: start the sequence, end the sequence, etc.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And that’s it! We broke down some of the technical innovations that lead to
    the discovery of the transformer and how the transformer works, then we went over
    the transformer’s high level architecture as an encoder-decoder model and discussed
    important sub-components like multi-headed self attention, input embeddings, positional
    encoding, skip connections, and normalization.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Follow For More!
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I describe papers and concepts in the ML space, with an emphasis on practical
    and intuitive explanations.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我描述了机器学习领域的论文和概念，重点在于实用和直观的解释。
- en: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----58a5c5df8dbb--------------------------------)
    [## Get an email whenever Daniel Warfield publishes'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----58a5c5df8dbb--------------------------------)
    [## 每当丹尼尔·沃菲尔德发布文章时获取邮件'
- en: High quality data science articles straight to your inbox. Get an email whenever
    Daniel Warfield publishes. By signing up, you…
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高质量的数据科学文章直接送到您的收件箱。每当丹尼尔·沃菲尔德发布文章时获取邮件。通过注册，您…
- en: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----58a5c5df8dbb--------------------------------)
    [![](../Images/1f6f4c8a07d69cf53e055e0130a85b03.png)](https://www.buymeacoffee.com/danielwarfield)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----58a5c5df8dbb--------------------------------)
    [![](../Images/1f6f4c8a07d69cf53e055e0130a85b03.png)](https://www.buymeacoffee.com/danielwarfield)
- en: Never expected, always appreciated. By donating you allow me to allocate more
    time and resources towards more frequent and higher quality articles. [Learn More](https://www.buymeacoffee.com/danielwarfield)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从未预期，总是感激。通过捐赠，您使我能够将更多时间和资源分配到更频繁和更高质量的文章上。[了解更多](https://www.buymeacoffee.com/danielwarfield)
- en: '**Attribution:** All of the images in this document were created by Daniel
    Warfield, unless a source is otherwise provided. You can use any images in this
    post for your own non-commercial purposes, so long as you reference this article,
    [https://danielwarfield.dev](https://danielwarfield.dev/), or both.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**归属:** 本文档中的所有图像均由**丹尼尔·沃菲尔德**创建，除非另有来源说明。您可以将本文中的任何图像用于您的非商业用途，只要您引用了这篇文章，[https://danielwarfield.dev](https://danielwarfield.dev/)，或两者都引用。'
