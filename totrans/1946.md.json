["```py\n# Step 1: Neural net\n# simple MLP with two hidden layers, ReLU activations, batch norm and dropout\n\nin_features = x_train.shape[1]\nnum_nodes = [32, 32]\nout_features = 1\nbatch_norm = True\ndropout = 0.1\noutput_bias = False\n\nnet = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm,\n                              dropout, output_bias=output_bias)\n\nmodel = CoxPH(net, tt.optim.Adam)\n\n# Step 2: Model training\n\nbatch_size = 256\nepochs = 512\ncallbacks = [tt.callbacks.EarlyStopping()]\nverbose = True\n\nmodel.optimizer.set_lr(0.01)\n\nlog = model.fit(x_train, y_train, batch_size, epochs, callbacks, verbose,\n                val_data=val, val_batch_size=batch_size)\n\n# Step 3: Prediction\n\n_ = model.compute_baseline_hazards()\nsurv = model.predict_surv_df(x_test)\n\n# Step 4: Evaluation\n\nev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\nev.concordance_td()\n```"]