- en: Leveraging LLMs to Complete Recommendation Knowledge Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/augmenting-intelligence-leveraging-llms-to-complete-recommendation-knowledge-graphs-a0585e311d3f](https://towardsdatascience.com/augmenting-intelligence-leveraging-llms-to-complete-recommendation-knowledge-graphs-a0585e311d3f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alcarazanthony1?source=post_page-----a0585e311d3f--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----a0585e311d3f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a0585e311d3f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a0585e311d3f--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----a0585e311d3f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a0585e311d3f--------------------------------)
    ·6 min read·Nov 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  prefs: []
  type: TYPE_NORMAL
- en: With the rapid growth of the internet and online platforms, users are inundated
    with choices. Recommender systems have become essential in cutting through this
    information overload by predicting users’ preferences and suggesting relevant
    content. However, providing accurate and personalized recommendations remains
    a persistent challenge.
  prefs: []
  type: TYPE_NORMAL
- en: The crux of the problem lies in understanding users’ true interests and intents
    by modeling their behavior. Recommender systems rely on patterns gleaned from
    user data like browsing history, purchases, ratings, and interactions. But real-world
    user data is often sparse and limited, lacking crucial contextual signals needed
    to capture the nuances of user intent.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, recommender models fail to learn comprehensive user and item representations.
    Their suggestions end up being too generic, repetitive, or irrelevant. The cold
    start problem compounds matters for new users with minimal activity history. Businesses
    also suffer from subpar customer experience leading to lost revenue.
  prefs: []
  type: TYPE_NORMAL
- en: This calls for solutions that can unlock deeper insights from user data. An
    emerging approach is to use knowledge graphs that encapsulate facts and connections
    between entities. Well-constructed knowledge graphs hold tremendous potential
    for addressing critical challenges in recommendation systems.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graphs go beyond just modeling user-item interactions. They encode
    diverse contextual metadata, attributes, and relationships across multiple entities.
    This multidimensional linked structure mimics how human memory stores world knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: By training specialized graph neural network models on such interconnected knowledge,
    recommender systems can learn more informative representations of user behavior
    and item characteristics. The enriched understanding leads to suggestions tailored
    to nuanced user needs and scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: However, a roadblock to this vision is that real-world knowledge graphs are
    far from perfect. They suffer from incompleteness, lacking crucial connections
    and details. This inhibits recommendation models from truly grasping user contexts
    and intents.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, recent advances in language models provide a ray of hope. Pre-trained
    models like GPT-3 have demonstrated remarkable natural language generation capabilities,
    powered by their vast stores of world knowledge. Early exploration of leveraging
    such models with in context learning to enhance knowledge graphs shows great promise
    (Wei et al., 2023).
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will dive deeper into how the augmented intelligence of
    language models can transform knowledge graphs. We will explore techniques like
    relationship prediction and attribute enrichment powered by models like GPT-3\.
    Through comprehensive examples, we will demonstrate how language model-enhanced
    knowledge graphs unlock the next level of intelligent recommendation systems.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Graphs — Encoding Connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A knowledge graph represents entities (users, products) as nodes and their relationships
    as edges. Connecting users to their interests, demographics, purchase history
    etc. allows recommendation systems to learn better representations.
  prefs: []
  type: TYPE_NORMAL
- en: However, real-world knowledge graphs derived from user data often suffer from
    sparsity and incompleteness. Many potential connections are simply missing, which
    limits the system’s ability to truly understand user intent.
  prefs: []
  type: TYPE_NORMAL
- en: This is where large language models (LLMs) promise a breakthrough.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs — Augmenting Intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs have gained immense popularity due to their ability to generate remarkably
    human-like text. But what’s more impressive is the vast knowledge encoded in their
    parameters through pre-training on massive text corpora.
  prefs: []
  type: TYPE_NORMAL
- en: Recent research has explored leveraging this knowledge to improve recommender
    systems powered by graph neural networks (GNNs). The key idea is to use LLMs to
    augment existing knowledge graphs by reinforcing edges and enhancing node attributes.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs Reinforce Graph Connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs can predict potential connections between users and items that may not
    be explicitly present in the source data. For instance, by analyzing a user’s
    purchase history, they can suggest relevant products the user may be interested
    in.
  prefs: []
  type: TYPE_NORMAL
- en: These LLM-predicted links help densify the sparse graphs, providing crucial
    signals for preference modeling. Reinforcing edges strengthens neighborhoods and
    allows collaborative patterns to emerge.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs Enhance Node Attributes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nodes in knowledge graphs represent entities like users and items. LLMs can
    augment attributes for these nodes based on the textual data associated with them.
  prefs: []
  type: TYPE_NORMAL
- en: For example, product descriptions and reviews can be processed by LLMs to extract
    missing specs or tags. User comments and posts can be similarly analyzed to fill
    in sparse profile information.
  prefs: []
  type: TYPE_NORMAL
- en: This results in nodes with rich feature vectors, overcoming cold start issues.
    The enhanced attributes improve semantics for better recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Improved Modeling with Augmented Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By training graph neural networks on LLM-enhanced knowledge graphs, recommendation
    systems can learn superior user and item representations.
  prefs: []
  type: TYPE_NORMAL
- en: The improved structure and node features lead to embeddings that capture nuanced
    preferences and item characteristics. This addresses key challenges like sparsity
    and cold start that plague many recommendation engines.
  prefs: []
  type: TYPE_NORMAL
- en: Studies show significant gains on metrics like recall and lower latency from
    augmenting graphs with LLMs before feeding them to GNN architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLMRec technique :'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The LLMRec techniques for augmenting knowledge graphs using equations in a
    step-by-step manner:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Construct Prompts for the LLM'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we need to create prompts that provide context for the LLM to generate
    useful augmentations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For reinforcing user-item connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PUI: User-item interaction prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'D: Task description'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'H: User’s historical interactions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'C: Candidate items'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'F: Desired output format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For enhancing node attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PA: Attribute enhancement prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'D: Task description'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'E: Available entity attributes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'F: Output format for missing attributes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Obtain Augmentations from LLM'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now use the prompts to get augmented data from the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: 'EA: Augmented user-item interactions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AA: Augmented attributes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM(): Language model (e.g. GPT-3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3: Incorporate Augmentations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The augmented data can be incorporated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: 'E’: Union of original and augmented interactions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A’: Union of original and augmented attributes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 4: Train Enhanced Recommender'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The recommender model is then trained on the improved graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: 'θ*: Optimized model parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P(): Posterior probability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data robustification technique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Denoised data robustification technique used in LLMRec to handle noise in the
    augmented data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Noisy User-Item Interaction Pruning**'
  prefs: []
  type: TYPE_NORMAL
- en: Sort the loss values calculated using the augmented user-item pairs in ascending
    order after each training iteration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prune or discard a certain percentage of the pairs with the highest loss values.
    These likely correspond to noisy or unreliable samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retain only the most reliable pairs with lowest losses for training in the next
    iteration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mathematically, this is implemented by sorting and slicing the loss tensor:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Where N is the number of samples to retain after pruning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Enhancing Augmented Features via MAE**'
  prefs: []
  type: TYPE_NORMAL
- en: Mask a subset of the augmented node features in the graph using [MASK] tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reconstruct the original features from the masked versions using a masked autoencoder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The feature reconstruction loss between original and masked features acts as
    regularization to improve feature quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mathematically, the loss is computed as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Where f is the original feature, f’ is the masked feature, and V is the set
    of masked nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Together, these techniques prune unreliable augmentations and impose constraints
    to ensure the noisy artificial data does not degrade performance. This results
    in a clean, robust training process using the high-quality augmented graph.
  prefs: []
  type: TYPE_NORMAL
- en: Limitless Possibilities with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowledge graphs represent an incredibly promising direction for building more
    intelligent and contextual next-generation recommender systems. By encoding multifaceted
    connections between diverse entities, they can capture nuanced user behavior patterns
    and item relationships.
  prefs: []
  type: TYPE_NORMAL
- en: However, real-world knowledge graphs often suffer from critical issues like
    sparsity and incompleteness that limit their effectiveness. This is where large
    language models provide a game-changing opportunity through their ability to predict
    missing connections and generate missing descriptive attributes.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen through detailed examples, techniques like relationship reinforcement
    and attribute enhancement powered by LLMs can significantly augment existing knowledge
    graphs. The augmented intelligence acts as a missing jigsaw puzzle piece, connecting
    the dots to create a more holistic picture.
  prefs: []
  type: TYPE_NORMAL
- en: Training graph neural networks on such enriched representations unlocks the
    full potential of knowledge graphs. It allows learning sophisticated user and
    item embeddings that capture subtleties and semantics.
  prefs: []
  type: TYPE_NORMAL
- en: The result is recommender systems that truly understand user contexts and intents.
    LLM-powered knowledge graphs pave the path for intelligent assistants that can
    cater to nuanced user needs and scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: As language models continue to evolve, so will their capabilities for knowledge
    augmentation. With advances like causal reasoning and conversational interaction,
    they could help construct explanatory graphs that link recommendations to user
    behaviors and rationales.
  prefs: []
  type: TYPE_NORMAL
- en: Adoption at scale does require addressing challenges like computational overhead
    and algorithmic biases that could creep in. But the possibilities make this one
    of the most promising directions for future recommender systems.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge is power. In the domain of recommendation systems, knowledge graphs
    complemented by language models looks set to unleash that power. It marks the
    beginning of a new era of intelligent recommendation. An era where systems transcend
    simply pattern matching, but exhibit a deeper understanding of user contexts and
    needs for thoughtful suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80fa24d339b7d18a5ce26a715d2778a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
