- en: Understanding Causal Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解因果树
- en: 原文：[https://towardsdatascience.com/understanding-causal-trees-920177462149](https://towardsdatascience.com/understanding-causal-trees-920177462149)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-causal-trees-920177462149](https://towardsdatascience.com/understanding-causal-trees-920177462149)
- en: '[CAUSAL DATA SCIENCE](https://towardsdatascience.com/tagged/causal-data-science)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[因果数据科学](https://towardsdatascience.com/tagged/causal-data-science)'
- en: '*How to use regression trees to estimate heterogeneous treatment effects*'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*如何使用回归树来估计异质性处理效应*'
- en: '[](https://medium.com/@matteo.courthoud?source=post_page-----920177462149--------------------------------)[![Matteo
    Courthoud](../Images/d873eab35a0cf9fc696658c0bee16b33.png)](https://medium.com/@matteo.courthoud?source=post_page-----920177462149--------------------------------)[](https://towardsdatascience.com/?source=post_page-----920177462149--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----920177462149--------------------------------)
    [Matteo Courthoud](https://medium.com/@matteo.courthoud?source=post_page-----920177462149--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@matteo.courthoud?source=post_page-----920177462149--------------------------------)[![Matteo
    Courthoud](../Images/d873eab35a0cf9fc696658c0bee16b33.png)](https://medium.com/@matteo.courthoud?source=post_page-----920177462149--------------------------------)[](https://towardsdatascience.com/?source=post_page-----920177462149--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----920177462149--------------------------------)
    [Matteo Courthoud](https://medium.com/@matteo.courthoud?source=post_page-----920177462149--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----920177462149--------------------------------)
    ·15 min read·Feb 3, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----920177462149--------------------------------)
    ·阅读时间 15 分钟·2023年2月3日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7b179d1c87c2a54be0d5266b5a9d9071.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b179d1c87c2a54be0d5266b5a9d9071.png)'
- en: Cover, image by Author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 封面，图片由作者提供
- en: In causal inference, we are usually interested in estimating the causal effect
    of a treatment (a drug, ad, product, …) on an outcome of interest (a disease,
    firm revenue, customer satisfaction, …). However, knowing that a treatment works
    on average is often not sufficient and we would like to know for which subjects
    (patients, users, customers, …) it works better or worse, i.e. we would like to
    estimate **heterogeneous treatment effects**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在因果推断中，我们通常关注的是估计处理（药物、广告、产品等）对感兴趣结果（疾病、公司收入、客户满意度等）的因果效应。然而，知道处理在平均情况下有效通常是不够的，我们希望了解对哪些对象（患者、用户、客户等）效果更好或更差，即我们希望估计**异质性处理效应**。
- en: 'Estimating heterogeneous treatment effects allows us to use the treatment selectively
    and more efficiently through **targeting**. Knowing which customers are more likely
    to react to a discount allows a company to spend less money by offering fewer
    but better-targeted discounts. This works also for negative effects: knowing for
    which patients a certain drug has side effects allows a pharmaceutical company
    to warn or exclude them from the treatment. There is also a more subtle advantage
    of estimating heterogeneous treatment effects: knowing **for whom** a treatment
    works allow us to better understand **how** a treatment works. Knowing that the
    effect of a discount does not depend on the income of its recipient but rather
    on its buying habits tells us that maybe it is not a matter of money, but rather
    a matter of attention or loyalty.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 估计异质性处理效应使我们能够通过**目标定位**选择性地和更有效地使用处理。了解哪些客户更可能对折扣做出反应可以使公司通过提供更少但更精准的折扣来节省开支。这同样适用于负面效应：知道哪些患者对某种药物有副作用可以使制药公司警告或将他们排除在治疗之外。估计异质性处理效应还有一个更微妙的优势：了解**谁**对处理有效可以帮助我们更好地理解**如何**处理有效。知道折扣的效果不依赖于接受者的收入而是依赖于其购买习惯，告诉我们也许这不仅仅是钱的问题，而是关注度或忠诚度的问题。
- en: In this article, we will explore the estimation of heterogeneous treatment effects
    using a modified version of regression trees (and forests). From a machine-learning
    perspective, there are two fundamental **differences between causal trees and
    predictive trees**. First of all, the target is the treatment effect, which is
    an inherently unobservable object. Second, we are interested in doing inference,
    which means quantifying the uncertainty of our estimates.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨使用回归树（及其森林）改进版来估计异质性处理效应。从机器学习的角度来看，**因果树与预测树之间有两个基本差异**。首先，目标是处理效应，这本质上是一个不可观察的对象。其次，我们关注的是进行推断，这意味着量化我们估计的不确定性。
- en: Online Discounts and Targeting
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线折扣与目标定位
- en: 'For the rest of the article, we are going to use a toy example, for the sake
    of exposition: suppose we were an **online shop** and we are interested in understanding
    whether offering discounts to new customers increases their expenditure. In particular,
    we would like to know if offering discounts is more effective for some customers
    with respect to others since we would prefer not to give discounts to customers
    that would spend anyways. Moreover, it could also be that spamming customers with
    pop-ups could deter them from buying, having the opposite effect.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在文章的其余部分，我们将使用一个示例进行说明：假设我们是一个**在线商店**，并且我们希望了解是否对新客户提供折扣会增加他们的支出。特别是，我们希望知道对某些客户提供折扣是否比对其他客户更有效，因为我们不希望对那些即使没有折扣也会消费的客户进行折扣。此外，向客户发送弹窗广告可能会让他们反感，从而产生相反的效果。
- en: '![](../Images/6a3434d66f1b72c454fca46a3b7d94b6.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a3434d66f1b72c454fca46a3b7d94b6.png)'
- en: Image generated by Author using [NightCafé](https://creator.nightcafe.studio/)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用[NightCafé](https://creator.nightcafe.studio/)生成
- en: 'To understand whether and how much the discounts are effective we run an **A/B
    test**: whenever a new user visits our online shop, we randomly decide whether
    to offer them the discount or not. I import the data-generating process `dgp_online_discounts()`
    from `[src.dgp](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py)`.
    With respect to previous articles, I generated a new DGP parent class that handles
    randomization and data generation, while its children classes contain specific
    use cases. I also import some plotting functions and libraries from `[src.utils](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py)`.
    To include not only code but also data and tables, I use [Deepnote](https://deepnote.com/),
    a Jupyter-like web-based collaborative notebook environment.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解折扣的效果以及效果的大小，我们进行了一项**A/B 测试**：每当一个新用户访问我们的在线商店时，我们会随机决定是否向他们提供折扣。我从`[src.dgp](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py)`导入数据生成过程`dgp_online_discounts()`。与之前的文章相比，我生成了一个新的DGP父类来处理随机化和数据生成，而其子类包含具体的使用案例。我还从`[src.utils](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py)`导入了一些绘图函数和库。为了包括代码、数据和表格，我使用了[Deepnote](https://deepnote.com/)，这是一个类似Jupyter的基于Web的协作笔记本环境。
- en: We have data on 100.000 website visitors, for whom we observe the `time` of
    the day, the `device` they use, their `browser` and their geographical `region`.
    We also see whether they were offered the `discount`, our treatment, and what
    is their `spend`, the outcome of interest.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有100,000名网站访问者的数据，我们观察他们的`time`（时间）、使用的`device`（设备）、`browser`（浏览器）以及他们的地理`region`（区域）。我们还记录了他们是否获得了`discount`（折扣），我们的处理，以及他们的`spend`（支出），这是我们的关注点。
- en: Since the treatment was randomly assigned, we can use a simple **difference-in-means**
    estimator to estimate the treatment effect. We expect the treatment and control
    group to be similar, except for the `discount`, therefore we can causally attribute
    any difference in `spend` to the `discount`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于处理是随机分配的，我们可以使用简单的**均值差异**估计量来估计处理效应。我们期望处理组和对照组在`discount`（折扣）之外是相似的，因此我们可以将`spend`（支出）的任何差异归因于`discount`（折扣）。
- en: 'The discount seems to be effective: on average the spending in the treatment
    group increases by 1.95$. But are all customers equally affected?'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣似乎有效：在处理组中，平均支出增加了1.95美元。但所有客户的反应是否相同？
- en: To answer this question, we would like to estimate **heterogeneous treatment
    effects**, possibly at the individual level.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们希望估计**异质处理效应**，可能在个体层面上。
- en: Heterogeneous Treatment Effects
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异质处理效应
- en: There are many possible ways to estimate heterogeneous treatment effects. The
    most common is to split the population into groups based on some observable characteristic,
    which in our case could be the `device`, the `browser` or the geographical `region`.
    Once you have decided which variable to split your data on, you can simply interact
    with the treatment variable (`discount`) with the dimension of treatment heterogeneity.
    Let's take `device` for example.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 估计异质处理效应有多种方法。最常见的方法是根据一些可观察的特征将人群划分为不同的组，在我们的案例中，这些特征可以是`device`（设备）、`browser`（浏览器）或地理`region`（区域）。一旦决定了数据划分的变量，你可以简单地将处理变量（`discount`）与处理异质性的维度进行交互。以`device`为例。
- en: How do we interpret the regression results? The effect of the `discount` on
    customers' `spend` is 1.22$ but it increases by a further 1.44$ if the customer
    is accessing the website from a mobile `device`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解读回归结果？`discount`对客户`spend`的影响是1.22$，但如果客户通过移动`device`访问网站，这一影响会增加至1.44$。
- en: Splitting is easy for categorical variables, but for a continuous variable like
    `time` it is not intuitive where to split. Every hour? And which dimension is
    more informative? It would be tempting to try all possible splits, but the more
    we split the data, the more it is likely that we find spurious results (i.e. we
    overfit, in machine learning lingo). It would be great if we could **let the data
    speak** and select the minimum and most informative splits.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类变量，划分很容易，但对于像`time`这样的连续变量来说，不直观如何划分。每小时划分一次？哪个维度更具信息性？虽然很诱人尝试所有可能的划分，但我们对数据的划分越多，发现虚假结果（即在机器学习术语中，我们过拟合）的可能性就越大。如果我们能**让数据说话**并选择最小且信息量最大的划分，那就太好了。
- en: In a [separate post](https://medium.com/towards-data-science/understanding-meta-learners-8a9c1e340832),
    I have shown how the so-called **meta-learners** take this approach to causal
    inference. The idea is to predict the outcome conditional on the treatment status
    for each observation, and then compare the predicted conditional on treatment,
    with the predicted outcome conditional on control. The difference is the individual
    treatment effect.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在[另一篇文章](https://medium.com/towards-data-science/understanding-meta-learners-8a9c1e340832)中，我展示了所谓的**元学习者**如何采取这种因果推断方法。思路是根据每个观察的治疗状态预测结果，然后将预测的条件治疗结果与预测的对照结果进行比较。二者之间的差异就是个体治疗效应。
- en: The problem with meta-learners is that they use all their [degrees of freedom](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics))
    in predicting the outcome. However, we are interested to predict treatment effect
    heterogeneity. If most of the variation in the outcome is *not* in the treatment
    dimension, we will get very poor estimates of the treatment effects.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习者的问题在于，它们在预测结果时使用了所有的[自由度](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics))。然而，我们感兴趣的是预测治疗效应的异质性。如果结果的大部分变异*不*在治疗维度上，我们将得到非常差的治疗效应估计。
- en: Is it possible to instead directly concentrate on the **prediction of individual
    treatment effects**? Let’s define *Y* as the outcome of interest `spend`, *D*
    the treatment `discount`, and *X* other observable characteristics. The *ideal*
    loss function is
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 是否可以直接集中在**个体治疗效应的预测**上？我们将*Y*定义为感兴趣的结果`spend`，*D*为治疗`discount`，以及*X*为其他可观察特征。*理想*的损失函数是
- en: '![](../Images/625014656a5a9c75f9c9c8425affd810.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/625014656a5a9c75f9c9c8425affd810.png)'
- en: Ideal loss function, image by Author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的损失函数，图片由作者提供
- en: where *τᵢ* is the treatment effect of individual *i*. However, this objective
    function is **unfeasible** since we do not observe *τᵢ*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*τᵢ*是个体*i*的治疗效应。然而，这个目标函数是**不可行的**，因为我们无法观察到*τᵢ*。
- en: But, turns out that there is a way to get an unbiased estimate of the **individual
    treatment effect**. The **idea** is to use an auxiliary outcome variable, whose
    expected value for each individual is the individual treatment effect. This variable
    is
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但事实证明，有一种方法可以获得**个体治疗效应**的无偏估计。**思路**是使用一个辅助结果变量，其每个个体的期望值即为个体治疗效应。这个变量是
- en: '![](../Images/8c6ee904e46dcad7945f99fe47a9c19b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c6ee904e46dcad7945f99fe47a9c19b.png)'
- en: Auxiliary outcome variable, image by Author
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助结果变量，图片由作者提供
- en: where *p*(*Xᵢ*) is the [**propensity score**](https://en.wikipedia.org/wiki/Propensity_score_matching)
    of observation *i*, i.e. its probability of being treated.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*p*(*Xᵢ*)是观察*i*的[**倾向评分**](https://en.wikipedia.org/wiki/Propensity_score_matching)，即其被治疗的概率。
- en: In randomized experiments, the propensity score is known since randomization
    is fully under the control of the experimenter. For example, in our case, the
    probability of treatment was 50%. In quasi-experimental studies instead, when
    the treatment probability is not known, it has to be estimated. Even in randomized
    experiments, it is always better to estimate rather than impute the propensity
    scores, since it guards against sampling variation in the randomization. For more
    details on the propensity scores and how they are used in causal inference, I
    have a separate post [here](https://medium.com/towards-data-science/matching-weighting-or-regression-99bf5cffa0d9).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机化实验中，倾向得分是已知的，因为随机化完全在实验者的控制之下。例如，在我们的案例中，治疗的概率是50%。而在准实验研究中，当治疗概率未知时，需要进行估计。即使在随机化实验中，估计倾向得分总是比填补更好，因为它能防止随机化中的抽样变异。有关倾向得分及其在因果推断中的使用的更多详细信息，请参阅我在[这里](https://medium.com/towards-data-science/matching-weighting-or-regression-99bf5cffa0d9)的单独帖子。
- en: Let’s first generate dummy variables for our categorical variables, `device`,
    `browser` and `region`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为类别变量`device`、`browser`和`region`生成虚拟变量。
- en: We fit a `LogisticRegression` and use it to predict the treatment probability,
    i.e. construct the propensity score.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拟合了一个`LogisticRegression`并用它来预测治疗概率，即构建倾向得分。
- en: '![](../Images/afa6eb8ef5d4eff1aef7264b9cd1a95e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afa6eb8ef5d4eff1aef7264b9cd1a95e.png)'
- en: Distribution of estimated propensity scores, image by Author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 估计的倾向得分分布，图片来源：作者
- en: As expected, most propensity scores are very close to 0.5, the probability of
    treatment used in randomization. Moreover, the distribution is almost identical
    across the treatment and control groups, further confirming that randomization
    worked. If it had not been the case, we would have needed to make further assumptions
    in order to conduct a causal analysis. The most common one is **unconfoundedness**,
    also known as ignorability or selection on observables. In short, we will assume
    that conditional on some observables 𝑋 the treatment assignment is as good as
    random.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的，大多数倾向得分接近0.5，这是随机化中使用的治疗概率。此外，治疗组和对照组的分布几乎完全相同，进一步确认了随机化的有效性。如果情况不是这样，我们将需要做出进一步假设以进行因果分析。最常见的假设是**无混淆性**，也称为可忽略性或基于可观测变量的选择。简而言之，我们将假设在某些可观测变量𝑋的条件下，治疗分配是随机的。
- en: '![](../Images/7a455561d68df293eac18c050dbd3a7a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a455561d68df293eac18c050dbd3a7a.png)'
- en: Unconfoundedness assumption, image by Author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**无混淆假设**，图片来源：作者'
- en: However, in our case, the treatment probability is known and it seems that nothing
    went wrong in the randomization process.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们的案例中，治疗概率是已知的，并且似乎随机化过程中没有出现问题。
- en: We now have all the elements to compute our auxiliary outcome variable *Y**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有计算辅助结果变量*Y**的所有元素。
- en: As we said before, the idea is to use *Y** as the target of a prediction problem,
    since the expected value is exactly the individual treatment effect. Let’s check
    its average in the data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说，目的是将*Y**作为预测问题的目标，因为其期望值正好是个体治疗效果。让我们检查数据中的平均值。
- en: Indeed its average is almost identical to the previously estimated average treatment
    effect of 1.94$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，它的平均值几乎与之前估计的1.94$的平均治疗效果相同。
- en: How is it possible that, with a single observation and an estimate of the propensity
    score, we can estimate the individual treatment effect? What are the drawbacks?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在只有一个观察值和倾向得分估计的情况下估计个体治疗效果？有什么缺点？
- en: 'The **intuition** is to approach the problem from a different perspective:
    *ex-ante*, before the experiment. Imagine that our dataset had a single observation,
    *i*. We know that the treatment probability is *p*(*Xᵢ*), the propensity score.
    Therefore, in expectation, our dataset has *p*(*Xᵢ*) observations in the treatment
    group and *1–p*(*Xᵢ*) observations in the control group. The rest is business
    as usual: we estimate the treatment effect as the difference in average outcomes
    between the two groups! And indeed that is what we would do:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**直观**的想法是从不同的角度来处理问题：*事前*，在实验之前。设想我们的数据集只有一个观察值，*i*。我们知道治疗概率是*p*(*Xᵢ*)，即倾向得分。因此，期望中，我们的数据集中治疗组有*p*(*Xᵢ*)个观察值，对照组有*1–p*(*Xᵢ*)个观察值。其余的照常处理：我们通过两组之间的平均结果差异来估计治疗效果！这确实是我们会做的：'
- en: '![](../Images/a61c82853d3864a67fa2af6e2ef190ee.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a61c82853d3864a67fa2af6e2ef190ee.png)'
- en: Auxiliary outcome variable, image by Author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助结果变量，图片来源：作者
- en: The only difference is that we have a single observation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别是我们只有一个观察值。
- en: 'This trick comes at a cost: *Yᵢ** is an unbiased estimator for the individual
    treatment effect but has a very **high variance**. This is immediately visible
    by plotting its distribution.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技巧有一个代价：*Yᵢ* 是个体处理效应的无偏估计量，但具有非常**高的方差**。通过绘制其分布，这一点可以立即显现出来。
- en: '![](../Images/631c1029186de6b42501d772234bf081.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/631c1029186de6b42501d772234bf081.png)'
- en: Distribution of the auxiliary variable, image by Author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助变量的分布，图像由作者提供
- en: We are now ready to estimate **heterogeneous treatment effects**, by translating
    the causal inference problem into a prediction problem, predicting the auxiliary
    outcome *Y**, given observable characteristics *X*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备估计**异质性处理效应**，通过将因果推断问题转化为预测问题，预测给定可观察特征*X*的辅助结果*Y*。
- en: '![](../Images/4f2b5684285e0ab0fdf2e3c667945ccf.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f2b5684285e0ab0fdf2e3c667945ccf.png)'
- en: Image generated by Author using [NightCafé](https://creator.nightcafe.studio/)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者使用[NightCafé](https://creator.nightcafe.studio/)生成
- en: Causal Trees
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 因果树
- en: In the previous section, we have seen that we can transform the estimation of
    **heterogeneous treatment effects** into a prediction problem, where the outcome
    is the auxiliary outcome variable
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们已经看到，我们可以将**异质性处理效应**的估计转化为预测问题，其中结果是辅助结果变量。
- en: '![](../Images/8c6ee904e46dcad7945f99fe47a9c19b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c6ee904e46dcad7945f99fe47a9c19b.png)'
- en: Auxiliary outcome variable, image by Author
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助结果变量，图像由作者提供
- en: We can in principle use any machine learning algorithm at this point to estimate
    individual treatment effects. However, [**regression trees**](https://en.wikipedia.org/wiki/Decision_tree_learning)
    have particularly convenient characteristics.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，我们可以使用任何机器学习算法来估计个体处理效应。然而，[**回归树**](https://en.wikipedia.org/wiki/Decision_tree_learning)具有特别便利的特征。
- en: First of all, how do regression trees work? Classification and regression trees
    (CART) are algorithms that recursively **partition the data in bins** based on
    covariates *X* such that the outcome *Y* *within* each bin is as homogeneous as
    possible and the outcome *across* bins is as heterogeneous as possible. The predicted
    values are simply the outcome averages within each bin, in our case the *auxiliary*
    outcome variable *Y**, whose expected value for each observation is equal to the
    individual treatment effect. Therefore, by averaging *Y** within each bin, we
    can compute the **conditional (on X) heterogeneous treatment effect** 𝔼[*τᵢ |
    Xᵢ*] for observations that fall within that bin**.**
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，回归树是如何工作的？分类和回归树（CART）是基于协变量*X*递归**对数据进行分箱**的算法，使得每个箱中的结果*Y*在*箱内*尽可能同质，而*箱间*的结果尽可能异质。预测值只是每个箱中的结果平均值，在我们的情况下是*辅助*结果变量*Y*，每个观察值的期望值等于个体处理效应。因此，通过对每个箱中的*Y*进行平均，我们可以计算**条件（基于X）的异质性处理效应**
    𝔼[*τᵢ | Xᵢ*] 对于落在该箱中的观察值。
- en: The **averaging** part is one of the big advantages of regression trees for
    inference since we know very well how to do inference with averages, thanks to
    the [**Central Limit Theorem**](https://en.wikipedia.org/wiki/Central_limit_theorem).
    The second advantage of regression trees over other machine learning algorithms
    is that trees are very **interpretable** since we can directly plot the data partition
    as a tree structure. We will see more of this later. Last but not least, regression
    trees are still at the core of the [best-performing predictive algorithms](https://arxiv.org/abs/2207.08815)
    with tabular data, as of 2022.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均化**部分是回归树推断的一个主要优势，因为我们非常清楚如何使用平均值进行推断，这要归功于[**中心极限定理**](https://en.wikipedia.org/wiki/Central_limit_theorem)。回归树相对于其他机器学习算法的第二个优势是树非常**可解释**，因为我们可以直接将数据分区绘制为树结构。我们稍后会详细了解这一点。最后但同样重要的是，截至2022年，回归树仍然是[表现最佳的预测算法](https://arxiv.org/abs/2207.08815)的核心。'
- en: Let’s use the `[DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)`
    function from `sklearn` to fit our regression tree and estimate the heterogeneous
    treatment effects of `discounts` on customers' `spend`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`sklearn`中的`[DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)`函数来拟合我们的回归树，并估计`discounts`对顾客`spend`的异质性处理效应。
- en: We have restricted the tree to have a maximum depth of 2 and at least 30 observations
    per partition (also called *leaf*) so that we can easily plot the tree and visualize
    the estimated groups and treatment effects.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将树的最大深度限制为2，并且每个分区（也称为*叶子*）至少包含30个观察值，以便我们可以轻松绘制树并可视化估计的组和处理效应。
- en: '![](../Images/caff3ccb47368f02118a113b301a7874.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/caff3ccb47368f02118a113b301a7874.png)'
- en: Regression tree on auxiliary outcome variable Y*, image by Author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助结果变量Y*上的回归树，作者提供的图片
- en: How should we **interpret** the tree? On the top, we can see the average *Y**
    in the data, 1.945$, corresponding with the average treatment effect. Starting
    from there, the data gets split into different branches, according to the rules
    highlighted at the top of each node. For example, the first node splits the data
    into two groups of size 51,156 and 48,844 depending on whether the `time` is later
    than 12.325\. At the bottom, we have our final partitions, with the heterogeneous
    treatment effects. For example, the leftmost leaf contains 43,876 observations
    with `time` earlier than 12.325 and non-Safari `browser`, for which we predict
    an effect on `spend` of 0.295$. In short, every node contains an estimate of the
    **conditional average treatment effect** 𝔼[*τᵢ | Xᵢ*], where darker node colors
    indicate higher prediction values.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何**解读**这棵树？在顶部，我们可以看到数据中的平均值*Y**为1.945$，对应于平均处理效应。从那里开始，数据根据每个节点顶部突出显示的规则被分成不同的分支。例如，第一个节点将数据分成两个大小分别为51,156和48,844的组，具体取决于`time`是否晚于12.325。在底部，我们有最终的分区，包含异质的处理效应。例如，最左边的叶子包含43,876个观察值，其中`time`早于12.325且`browser`不是Safari，我们预测对`spend`的影响为0.295$。简而言之，每个节点包含**条件平均处理效应**
    𝔼[*τᵢ | Xᵢ*]的估计，其中节点颜色越深表示预测值越高。
- en: Should we believe these estimates? Not really, because of a couple of reasons.
    The **first problem** is that we have an unbiased estimate of the average treatment
    effect only if, *within each leaf*, we have the same number of treated and control
    units. This is not automatically the case with an off-the-shelf `DecisionTreeRegressor()`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该相信这些估计吗？不完全是，因为有几个原因。**第一个问题**是，只有在*每个叶子内部*我们有相同数量的处理组和对照组单元时，我们才有平均处理效应的无偏估计。这在使用现成的`DecisionTreeRegressor()`时并非自动成立。
- en: Honest Trees
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 诚实树
- en: 'Another problem with our naive procedure is that we have used the **same data**
    to generate the tree and evaluate it. This generates bias because a simple difference
    in means estimator would not take into account the fact that the partition is
    endogenous, i.e. generated on the same data. In machine learning terms, we are
    overfitting. The solution is simple: we can split the sample into two separate
    subsamples and use different data to generate the tree and compute the predictions.
    These trees are called **honest trees**.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的简单方法的另一个问题是我们使用了**相同的数据**来生成和评估树。这会产生偏差，因为简单的均值差异估计量不会考虑分区是内生的，即在相同的数据上生成的。用机器学习术语来说，我们是在过拟合。解决方案很简单：我们可以将样本拆分为两个独立的子样本，并使用不同的数据生成树和计算预测。这些树被称为**诚实树**。
- en: This solution is as simple as effective since it allows us, at the inference
    stage, to treat each sample within a leaf as independent from the tree structure.
    At that point, our estimator is a **difference in means** estimator for an independent
    subsample we can simply use the Central Limit Theorem for inference. One drawback
    of splitting the data is that we lose [power](https://en.wikipedia.org/wiki/Power_of_a_test),
    i.e. the ability to detect non-spurious heterogeneous treatment effects because
    of the smaller sample. The solution is to repeat the procedure twice, swapping
    the sample used to build the tree and the sample used to compute the within-leaf
    averages. Then, we can average the two estimates for each individual and adjust
    the estimated standard errors accordingly.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案既简单又有效，因为它允许我们在推断阶段，将每个叶子中的样本视为与树结构独立。此时，我们的估计量是对独立子样本的**均值差异**估计量，我们可以简单地使用中心极限定理进行推断。拆分数据的一个缺点是我们失去了[统计功效](https://en.wikipedia.org/wiki/Power_of_a_test)，即由于样本较小而无法检测到虚假的异质处理效应。解决方案是重复该过程两次，交换用于构建树和计算叶子内均值的样本。然后，我们可以对每个个体的两个估计值取平均，并相应地调整估计的标准误差。
- en: '![](../Images/5775862581c9150a1b868f1e86ff8f39.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5775862581c9150a1b868f1e86ff8f39.png)'
- en: Sample splitting procedure, image by Author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 样本拆分过程，作者提供的图片
- en: Generating Splits
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成拆分
- en: Last but not least, how should the tree be generated? The default rule to generate
    **splits** with the `DecisionTreeRegressor` function is the `squared_error` and
    there is no restriction on the minimum number of observations per leaf. Other
    commonly used rules include, mean absolute error, Gini’s impurity, and Shannon’s
    information. Which one performs better depends on the specific application, but
    the general objective is always prediction accuracy, broadly defined.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，树应该如何生成？`DecisionTreeRegressor`函数生成**分裂**的默认规则是`squared_error`，并且对每个叶子中的最小观测数没有限制。其他常用规则包括平均绝对误差、基尼
    impurity 和香农信息。哪个表现更好取决于具体应用，但总体目标始终是预测准确性，广义上定义。
- en: 'In our case instead, the objective is **inference**: we want to uncover heterogeneous
    treatment effects that are statistically different from each other. There is no
    value in generating different treatment effects if they are statistically indistinguishable.
    Moreover (but strongly related), when building the tree and generating the data
    partitions, we have to take into account that, since we use honest trees, we will
    use different data to estimate the within-leaf treatment effects.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在我们的案例中，目标是**推断**：我们希望揭示在统计上不同的异质性处理效应。如果处理效应在统计上不可区分，那么生成不同的处理效应就没有意义。此外（但与之紧密相关），在构建树和生成数据分区时，我们必须考虑，由于我们使用的是诚实树，我们将使用不同的数据来估计叶内处理效应。
- en: '[Athey and Imbens (2016)](https://www.pnas.org/doi/abs/10.1073/pnas.1510489113)
    use a modified version of the [Mean Squared Error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error)
    as a splitting criterion, the **Expanded Mean Squared Error (EMSE)**:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[Athey和Imbens (2016)](https://www.pnas.org/doi/abs/10.1073/pnas.1510489113)使用了[均方误差
    (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error)的修改版本作为分裂标准，即**扩展均方误差
    (EMSE)**：'
- en: '![](../Images/8609eaa4c5d55a4b510cd46719c8ba2a.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8609eaa4c5d55a4b510cd46719c8ba2a.png)'
- en: Expanded Root Mean Squared Error, image by Author
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展均方根误差，作者提供的图片
- en: where *μ* is the estimated conditional expectation *μ*(*X*) *=* 𝔼 [*Y** | *X*]
    and the difference with respect to the MSE is the additional term *Yᵢ²*, the squared
    outcome variable. In our setting, we can rewrite it as
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*μ*是估计的条件期望*μ*(*X*) *=* 𝔼 [*Y* | *X*]，与MSE的差异是额外的项*Yᵢ²*，即平方的结果变量。在我们的设置中，我们可以将其重写为
- en: '![](../Images/f948a88295633b7ec28416436a817605.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f948a88295633b7ec28416436a817605.png)'
- en: Expanded Root Mean Squared Error for causal trees, image by Author
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展均方根误差用于因果树，作者提供的图片
- en: Why is this a sensible error loss? Because we can rewrite it as the expected
    variance of the conditional treatment effects, minus the squared expected value.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这是一个合理的误差损失？因为我们可以将其重写为条件处理效应的期望方差减去平方期望值。
- en: '![](../Images/da70d2e82e72ba2f85787338a85176f8.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da70d2e82e72ba2f85787338a85176f8.png)'
- en: Expanded Root Mean Squared Error for causal trees, image by Author
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展均方根误差用于因果树，作者提供的图片
- en: This formulation of the EMSE makes clear that the objective is to **minimize
    the within-leaf variance** of the estimated conditional conditional treatment
    effects *τ*(*X*) (the first term). In other words, small leaves are automatically
    penalized. The second term is just a normalizing factor. Note that both terms
    are unknown and have to be estimated from the training data, used to generate
    the tree.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这种EMSE的公式明确了目标是**最小化估计条件处理效应*τ*(*X*)的叶内方差**（第一个项）。换句话说，小的叶子会被自动惩罚。第二个项只是一个归一化因子。请注意，这两个项都是未知的，必须从训练数据中估计，用于生成树。
- en: '**Implementation**'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**实现**'
- en: Luckily, there are multiple libraries for causal trees. We import `CausalForestDML`
    from Microsoft's [EconML](https://econml.azurewebsites.net/) library, one of the
    best libraries for causal inference.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有多个因果树库可供选择。我们从微软的[EconML](https://econml.azurewebsites.net/)库中导入`CausalForestDML`，这是最好的因果推断库之一。
- en: We have restricted the number of estimators to 1 to have a single tree instead
    of multiple ones, the so-called [**random forests**](https://en.wikipedia.org/wiki/Random_forest)
    that we will cover in a separate article.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将估计器的数量限制为1，以便得到一棵树，而不是多棵树，即所谓的[**随机森林**](https://en.wikipedia.org/wiki/Random_forest)，我们将在另一篇文章中介绍。
- en: '![](../Images/f2df52c573a7a93e249b1a45ceabaeac.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2df52c573a7a93e249b1a45ceabaeac.png)'
- en: Estimated causal tree, image by Author
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 估计的因果树，作者提供的图片
- en: As we can see, the tree representation looks extremely similar to the one we
    got before using the `DecisionTreeRegressor` function. However, now the model
    not only reports estimates of the conditional average treatment effects but also
    the standard errors of the estimates (at the bottom). How were they computed?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，树形表示与之前使用`DecisionTreeRegressor`函数得到的结果非常相似。然而，现在模型不仅报告条件平均处理效应的估计值，还有这些估计值的标准误差（在底部）。这些是如何计算的？
- en: Inference
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推断
- en: 'Honest trees, besides improving the out-of-sample prediction accuracy of the
    model, have another great implication: they allow us to **compute standard errors
    as if the tree structure was exogenous**. In fact, since the data used to compute
    the predictions is independent of the data used to build the tree (split the data),
    we can just treat the tree structure as independent from the estimated treatment
    effects. As a consequence, we can estimate the standard errors of the estimates
    as standard errors of difference between sample averages, as in a standard AB
    test.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 诚实树除了提高模型的样本外预测准确性外，还有另一个重要意义：它们允许我们**像树结构是外生的一样计算标准误差**。实际上，由于用于计算预测的数据与用于构建树的数据（分割数据）是独立的，我们可以将树结构视为与估计的处理效应独立。因此，我们可以将估计的标准误差视为样本均值差异的标准误差，就像标准的AB测试一样。
- en: If we had used the same data to build the tree and estimate the treatment effects,
    we would have introduced **bias**, because of the spurious correlation between
    the covariates and the outcomes. This bias usually disappears for very large sample
    sizes, but honest trees do not require that.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用相同的数据来构建树并估计处理效应，我们将引入**偏差**，由于协变量与结果之间的虚假相关性。这个偏差通常在非常大的样本量中消失，但诚实树并不需要这样。
- en: Performance
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: 'How well does the model perform? Since we control the data-generating process,
    we can do something that is not possible with real data: check the predicted treatment
    effects against the true ones. The `add_treatment_effect()` function gives us
    the “true” treatment effects for each observation in the data.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 模型表现如何？由于我们控制了数据生成过程，我们可以做一些真实数据无法做到的事情：将预测的处理效应与真实值进行比较。`add_treatment_effect()`函数为数据中的每个观测值提供了“真实”处理效应。
- en: We can now check how well the causal tree is able to estimate the individual
    treatment effects. Let’s start with the categorical variables. I plot the true
    and estimated average treatment effect conditional on each value of `device`,
    `browser` and `region`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以检查因果树在估计个体处理效应方面的能力。让我们从分类变量开始。我绘制了基于`device`、`browser`和`region`每个值的真实和估计的平均处理效应。
- en: '![](../Images/f3e6cce7dd1271dcdfb076dacf145acb.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3e6cce7dd1271dcdfb076dacf145acb.png)'
- en: True and estimated treatment effects for each categorical value, image by Author
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分类值的真实和估计处理效应，图片由作者提供
- en: The causal tree is pretty good at detecting the heterogeneous treatment effects
    for the categorical variables. It overestimates the effect of mobile devices and
    Safari browser but does generally a good job.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因果树在检测分类变量的异质处理效应方面表现相当好。它高估了移动设备和Safari浏览器的效应，但总体上表现不错。
- en: 'However, this is also where we expect a tree model to perform particularly
    well: where the effects are **discrete**. How well does it do on our continuous
    variable, time? First, let’s again isolate the predicted treatment effects on
    `time` and ignore the other covariates.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这也是我们期望树模型表现特别好的地方：在**离散**的效应上。它在我们的连续变量时间上的表现如何？首先，让我们再次隔离`time`上的预测处理效应，并忽略其他协变量。
- en: We now plot the predicted treatment effects against the true ones, along the
    `time` dimension.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将预测的处理效应与真实值沿`time`维度进行绘图。
- en: '![](../Images/1eced6305e633e3542f0a586b28d149f.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1eced6305e633e3542f0a586b28d149f.png)'
- en: True and estimated treatment effects along the time dimension, image by Author
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 沿时间维度的真实和估计处理效应，图片由作者提供
- en: 'From the plot, we can appreciate the discrete nature of causal trees: the model
    is only able to split the continuous variable into 4bins. These bins are close
    to the true treatment effects, but they fail to capture a big chunk of the treatment
    effect heterogeneity.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中，我们可以欣赏到因果树的离散特性：模型只能将连续变量分割成4个区间。这些区间接近真实的处理效应，但未能捕捉到处理效应异质性的较大部分。
- en: Can these predictions be improved? The answer is yes, and we will explore how
    in the next post.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测能得到改进吗？答案是肯定的，我们将在下一篇文章中探讨如何改进。
- en: Conclusion
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have seen how to use causal trees to estimate **heterogeneous
    treatment effects**. The main insight comes from the definition of an auxiliary
    outcome variable that allows us to frame the inference problem as a prediction
    problem. While we can then use any algorithm to predict treatment effects, regression
    trees are particularly useful because of their interpretability, prediction accuracy,
    and feature of generating prediction as subsample averages.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了如何使用因果树来估计**异质性处理效应**。主要的洞见来自于辅助结果变量的定义，这使我们能够将推断问题框架设为预测问题。虽然我们可以使用任何算法来预测处理效应，但回归树特别有用，因为它们具有良好的可解释性、预测准确性，并且能够生成作为子样本平均值的预测。
- en: The work by [Athey and Imbens (2016)](https://www.pnas.org/doi/abs/10.1073/pnas.1510489113)
    on regression trees to compute heterogeneous treatment effects brought together
    two separate literatures, causal inference, and machine learning in a very fruitful
    **synergy**. The causal inference literature (re)discovered the inference benefits
    of sample splitting, which allows us to do correct inference even when the data
    partition is complex and hard to analyze. On the other hand, splitting the tree
    generation phase from the within-leaf prediction phase has strong benefits in
    terms of prediction accuracy, by safeguarding against overfitting.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[Athey 和 Imbens (2016)](https://www.pnas.org/doi/abs/10.1073/pnas.1510489113)
    关于回归树计算异质性处理效应的工作，将因果推断和机器学习这两个不同的文献结合在了一起，形成了非常有成效的**协同效应**。因果推断文献（重新）发现了样本分割的推断好处，这使我们能够在数据分割复杂且难以分析时进行正确的推断。另一方面，将树生成阶段与叶内预测阶段分开，有助于提高预测准确性，防止过拟合。'
- en: References
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: S. Athey, G. Imbens, [Recursive partitioning for heterogeneous causal effects](https://www.pnas.org/doi/abs/10.1073/pnas.1510489113)
    (2016), *PNAS*.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Athey, G. Imbens, [异质因果效应的递归分割](https://www.pnas.org/doi/abs/10.1073/pnas.1510489113)
    (2016), *PNAS*。
- en: S. Wager, S. Athey, [Estimation and Inference of Heterogeneous Treatment Effects
    using Random Forests](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839)
    (2018), *Journal of the American Statistical Association*.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Wager, S. Athey, [使用随机森林的异质性处理效应的估计和推断](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839)
    (2018), *美国统计协会期刊*。
- en: S. Athey, J. Tibshirani, S. Wager, [Generalized Random Forests](https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full)
    (2019). *The Annals of Statistics*.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Athey, J. Tibshirani, S. Wager, [广义随机森林](https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full)
    (2019). *统计年鉴*。
- en: M. Oprescu, V. Syrgkanis, Z. Wu, [Orthogonal Random Forest for Causal Inference](http://proceedings.mlr.press/v97/oprescu19a.html?ref=https%3A%2F%2Fgithubhelp.com)
    (2019). *Proceedings of the 36th International Conference on Machine Learning*.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M. Oprescu, V. Syrgkanis, Z. Wu, [用于因果推断的正交随机森林](http://proceedings.mlr.press/v97/oprescu19a.html?ref=https%3A%2F%2Fgithubhelp.com)
    (2019). *第36届国际机器学习会议论文集*。
- en: Related Articles
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关文章
- en: '[DAGs and Control Variables](/b63dc69e3d8c)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DAG 和控制变量](/b63dc69e3d8c)'
- en: '[Matching, Weighting, or Regression?](/99bf5cffa0d9)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[匹配、加权还是回归？](/99bf5cffa0d9)'
- en: '[Understanding Meta Learners](/8a9c1e340832)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解元学习者](/8a9c1e340832)'
- en: '[Understanding AIPW, the Doubly-Robust Estimator](/ed4097dab27a)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解 AIPW，双重稳健估计量](/ed4097dab27a)'
- en: Code
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码
- en: 'You can find the original Jupyter Notebook here:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 Jupyter Notebook 可以在这里找到：
- en: '[https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb)'
- en: Thank you for reading!
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: '*I really appreciate it!* 🤗 *If you liked the post and would like to see more,
    consider* [***following me***](https://medium.com/@matteo.courthoud)*. I post
    once a week on topics related to causal inference and data analysis. I try to
    keep my posts simple but precise, always providing code, examples, and simulations.*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*我非常感激！* 🤗 *如果你喜欢这篇文章并希望看到更多内容，请考虑* [***关注我***](https://medium.com/@matteo.courthoud)*。我每周发布一次与因果推断和数据分析相关的话题。我尽量保持帖子简单但精确，始终提供代码、示例和模拟。*'
- en: '*Also, a small* ***disclaimer****: I write to learn so mistakes are the norm,
    even though I try my best. Please, when you spot them, let me know. I also appreciate
    suggestions on new topics!*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*此外，一个小小的* ***免责声明****：我写作是为了学习，因此错误是常见的，尽管我尽力而为。如果你发现错误，请告诉我。我也欢迎对新话题的建议！*'
