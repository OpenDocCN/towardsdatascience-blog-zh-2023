- en: Understanding Causal Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£å› æœæ ‘
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/understanding-causal-trees-920177462149](https://towardsdatascience.com/understanding-causal-trees-920177462149)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/understanding-causal-trees-920177462149](https://towardsdatascience.com/understanding-causal-trees-920177462149)
- en: '[CAUSAL DATA SCIENCE](https://towardsdatascience.com/tagged/causal-data-science)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[å› æœæ•°æ®ç§‘å­¦](https://towardsdatascience.com/tagged/causal-data-science)'
- en: '*How to use regression trees to estimate heterogeneous treatment effects*'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*å¦‚ä½•ä½¿ç”¨å›å½’æ ‘æ¥ä¼°è®¡å¼‚è´¨æ€§å¤„ç†æ•ˆåº”*'
- en: '[](https://medium.com/@matteo.courthoud?source=post_page-----920177462149--------------------------------)[![Matteo
    Courthoud](../Images/d873eab35a0cf9fc696658c0bee16b33.png)](https://medium.com/@matteo.courthoud?source=post_page-----920177462149--------------------------------)[](https://towardsdatascience.com/?source=post_page-----920177462149--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----920177462149--------------------------------)
    [Matteo Courthoud](https://medium.com/@matteo.courthoud?source=post_page-----920177462149--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@matteo.courthoud?source=post_page-----920177462149--------------------------------)[![Matteo
    Courthoud](../Images/d873eab35a0cf9fc696658c0bee16b33.png)](https://medium.com/@matteo.courthoud?source=post_page-----920177462149--------------------------------)[](https://towardsdatascience.com/?source=post_page-----920177462149--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----920177462149--------------------------------)
    [Matteo Courthoud](https://medium.com/@matteo.courthoud?source=post_page-----920177462149--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----920177462149--------------------------------)
    Â·15 min readÂ·Feb 3, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----920177462149--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 15 åˆ†é’ŸÂ·2023å¹´2æœˆ3æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7b179d1c87c2a54be0d5266b5a9d9071.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b179d1c87c2a54be0d5266b5a9d9071.png)'
- en: Cover, image by Author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å°é¢ï¼Œå›¾ç‰‡ç”±ä½œè€…æä¾›
- en: In causal inference, we are usually interested in estimating the causal effect
    of a treatment (a drug, ad, product, â€¦) on an outcome of interest (a disease,
    firm revenue, customer satisfaction, â€¦). However, knowing that a treatment works
    on average is often not sufficient and we would like to know for which subjects
    (patients, users, customers, â€¦) it works better or worse, i.e. we would like to
    estimate **heterogeneous treatment effects**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å› æœæ¨æ–­ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å…³æ³¨çš„æ˜¯ä¼°è®¡å¤„ç†ï¼ˆè¯ç‰©ã€å¹¿å‘Šã€äº§å“ç­‰ï¼‰å¯¹æ„Ÿå…´è¶£ç»“æœï¼ˆç–¾ç—…ã€å…¬å¸æ”¶å…¥ã€å®¢æˆ·æ»¡æ„åº¦ç­‰ï¼‰çš„å› æœæ•ˆåº”ã€‚ç„¶è€Œï¼ŒçŸ¥é“å¤„ç†åœ¨å¹³å‡æƒ…å†µä¸‹æœ‰æ•ˆé€šå¸¸æ˜¯ä¸å¤Ÿçš„ï¼Œæˆ‘ä»¬å¸Œæœ›äº†è§£å¯¹å“ªäº›å¯¹è±¡ï¼ˆæ‚£è€…ã€ç”¨æˆ·ã€å®¢æˆ·ç­‰ï¼‰æ•ˆæœæ›´å¥½æˆ–æ›´å·®ï¼Œå³æˆ‘ä»¬å¸Œæœ›ä¼°è®¡**å¼‚è´¨æ€§å¤„ç†æ•ˆåº”**ã€‚
- en: 'Estimating heterogeneous treatment effects allows us to use the treatment selectively
    and more efficiently through **targeting**. Knowing which customers are more likely
    to react to a discount allows a company to spend less money by offering fewer
    but better-targeted discounts. This works also for negative effects: knowing for
    which patients a certain drug has side effects allows a pharmaceutical company
    to warn or exclude them from the treatment. There is also a more subtle advantage
    of estimating heterogeneous treatment effects: knowing **for whom** a treatment
    works allow us to better understand **how** a treatment works. Knowing that the
    effect of a discount does not depend on the income of its recipient but rather
    on its buying habits tells us that maybe it is not a matter of money, but rather
    a matter of attention or loyalty.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼°è®¡å¼‚è´¨æ€§å¤„ç†æ•ˆåº”ä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡**ç›®æ ‡å®šä½**é€‰æ‹©æ€§åœ°å’Œæ›´æœ‰æ•ˆåœ°ä½¿ç”¨å¤„ç†ã€‚äº†è§£å“ªäº›å®¢æˆ·æ›´å¯èƒ½å¯¹æŠ˜æ‰£åšå‡ºååº”å¯ä»¥ä½¿å…¬å¸é€šè¿‡æä¾›æ›´å°‘ä½†æ›´ç²¾å‡†çš„æŠ˜æ‰£æ¥èŠ‚çœå¼€æ”¯ã€‚è¿™åŒæ ·é€‚ç”¨äºè´Ÿé¢æ•ˆåº”ï¼šçŸ¥é“å“ªäº›æ‚£è€…å¯¹æŸç§è¯ç‰©æœ‰å‰¯ä½œç”¨å¯ä»¥ä½¿åˆ¶è¯å…¬å¸è­¦å‘Šæˆ–å°†ä»–ä»¬æ’é™¤åœ¨æ²»ç–—ä¹‹å¤–ã€‚ä¼°è®¡å¼‚è´¨æ€§å¤„ç†æ•ˆåº”è¿˜æœ‰ä¸€ä¸ªæ›´å¾®å¦™çš„ä¼˜åŠ¿ï¼šäº†è§£**è°**å¯¹å¤„ç†æœ‰æ•ˆå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£**å¦‚ä½•**å¤„ç†æœ‰æ•ˆã€‚çŸ¥é“æŠ˜æ‰£çš„æ•ˆæœä¸ä¾èµ–äºæ¥å—è€…çš„æ”¶å…¥è€Œæ˜¯ä¾èµ–äºå…¶è´­ä¹°ä¹ æƒ¯ï¼Œå‘Šè¯‰æˆ‘ä»¬ä¹Ÿè®¸è¿™ä¸ä»…ä»…æ˜¯é’±çš„é—®é¢˜ï¼Œè€Œæ˜¯å…³æ³¨åº¦æˆ–å¿ è¯šåº¦çš„é—®é¢˜ã€‚
- en: In this article, we will explore the estimation of heterogeneous treatment effects
    using a modified version of regression trees (and forests). From a machine-learning
    perspective, there are two fundamental **differences between causal trees and
    predictive trees**. First of all, the target is the treatment effect, which is
    an inherently unobservable object. Second, we are interested in doing inference,
    which means quantifying the uncertainty of our estimates.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä½¿ç”¨å›å½’æ ‘ï¼ˆåŠå…¶æ£®æ—ï¼‰æ”¹è¿›ç‰ˆæ¥ä¼°è®¡å¼‚è´¨æ€§å¤„ç†æ•ˆåº”ã€‚ä»æœºå™¨å­¦ä¹ çš„è§’åº¦æ¥çœ‹ï¼Œ**å› æœæ ‘ä¸é¢„æµ‹æ ‘ä¹‹é—´æœ‰ä¸¤ä¸ªåŸºæœ¬å·®å¼‚**ã€‚é¦–å…ˆï¼Œç›®æ ‡æ˜¯å¤„ç†æ•ˆåº”ï¼Œè¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä¸å¯è§‚å¯Ÿçš„å¯¹è±¡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯è¿›è¡Œæ¨æ–­ï¼Œè¿™æ„å‘³ç€é‡åŒ–æˆ‘ä»¬ä¼°è®¡çš„ä¸ç¡®å®šæ€§ã€‚
- en: Online Discounts and Targeting
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨çº¿æŠ˜æ‰£ä¸ç›®æ ‡å®šä½
- en: 'For the rest of the article, we are going to use a toy example, for the sake
    of exposition: suppose we were an **online shop** and we are interested in understanding
    whether offering discounts to new customers increases their expenditure. In particular,
    we would like to know if offering discounts is more effective for some customers
    with respect to others since we would prefer not to give discounts to customers
    that would spend anyways. Moreover, it could also be that spamming customers with
    pop-ups could deter them from buying, having the opposite effect.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ–‡ç« çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç¤ºä¾‹è¿›è¡Œè¯´æ˜ï¼šå‡è®¾æˆ‘ä»¬æ˜¯ä¸€ä¸ª**åœ¨çº¿å•†åº—**ï¼Œå¹¶ä¸”æˆ‘ä»¬å¸Œæœ›äº†è§£æ˜¯å¦å¯¹æ–°å®¢æˆ·æä¾›æŠ˜æ‰£ä¼šå¢åŠ ä»–ä»¬çš„æ”¯å‡ºã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¸Œæœ›çŸ¥é“å¯¹æŸäº›å®¢æˆ·æä¾›æŠ˜æ‰£æ˜¯å¦æ¯”å¯¹å…¶ä»–å®¢æˆ·æ›´æœ‰æ•ˆï¼Œå› ä¸ºæˆ‘ä»¬ä¸å¸Œæœ›å¯¹é‚£äº›å³ä½¿æ²¡æœ‰æŠ˜æ‰£ä¹Ÿä¼šæ¶ˆè´¹çš„å®¢æˆ·è¿›è¡ŒæŠ˜æ‰£ã€‚æ­¤å¤–ï¼Œå‘å®¢æˆ·å‘é€å¼¹çª—å¹¿å‘Šå¯èƒ½ä¼šè®©ä»–ä»¬åæ„Ÿï¼Œä»è€Œäº§ç”Ÿç›¸åçš„æ•ˆæœã€‚
- en: '![](../Images/6a3434d66f1b72c454fca46a3b7d94b6.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a3434d66f1b72c454fca46a3b7d94b6.png)'
- en: Image generated by Author using [NightCafÃ©](https://creator.nightcafe.studio/)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…ä½¿ç”¨[NightCafÃ©](https://creator.nightcafe.studio/)ç”Ÿæˆ
- en: 'To understand whether and how much the discounts are effective we run an **A/B
    test**: whenever a new user visits our online shop, we randomly decide whether
    to offer them the discount or not. I import the data-generating process `dgp_online_discounts()`
    from `[src.dgp](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py)`.
    With respect to previous articles, I generated a new DGP parent class that handles
    randomization and data generation, while its children classes contain specific
    use cases. I also import some plotting functions and libraries from `[src.utils](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py)`.
    To include not only code but also data and tables, I use [Deepnote](https://deepnote.com/),
    a Jupyter-like web-based collaborative notebook environment.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†äº†è§£æŠ˜æ‰£çš„æ•ˆæœä»¥åŠæ•ˆæœçš„å¤§å°ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹**A/B æµ‹è¯•**ï¼šæ¯å½“ä¸€ä¸ªæ–°ç”¨æˆ·è®¿é—®æˆ‘ä»¬çš„åœ¨çº¿å•†åº—æ—¶ï¼Œæˆ‘ä»¬ä¼šéšæœºå†³å®šæ˜¯å¦å‘ä»–ä»¬æä¾›æŠ˜æ‰£ã€‚æˆ‘ä»`[src.dgp](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py)`å¯¼å…¥æ•°æ®ç”Ÿæˆè¿‡ç¨‹`dgp_online_discounts()`ã€‚ä¸ä¹‹å‰çš„æ–‡ç« ç›¸æ¯”ï¼Œæˆ‘ç”Ÿæˆäº†ä¸€ä¸ªæ–°çš„DGPçˆ¶ç±»æ¥å¤„ç†éšæœºåŒ–å’Œæ•°æ®ç”Ÿæˆï¼Œè€Œå…¶å­ç±»åŒ…å«å…·ä½“çš„ä½¿ç”¨æ¡ˆä¾‹ã€‚æˆ‘è¿˜ä»`[src.utils](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py)`å¯¼å…¥äº†ä¸€äº›ç»˜å›¾å‡½æ•°å’Œåº“ã€‚ä¸ºäº†åŒ…æ‹¬ä»£ç ã€æ•°æ®å’Œè¡¨æ ¼ï¼Œæˆ‘ä½¿ç”¨äº†[Deepnote](https://deepnote.com/)ï¼Œè¿™æ˜¯ä¸€ä¸ªç±»ä¼¼Jupyterçš„åŸºäºWebçš„åä½œç¬”è®°æœ¬ç¯å¢ƒã€‚
- en: We have data on 100.000 website visitors, for whom we observe the `time` of
    the day, the `device` they use, their `browser` and their geographical `region`.
    We also see whether they were offered the `discount`, our treatment, and what
    is their `spend`, the outcome of interest.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰100,000åç½‘ç«™è®¿é—®è€…çš„æ•°æ®ï¼Œæˆ‘ä»¬è§‚å¯Ÿä»–ä»¬çš„`time`ï¼ˆæ—¶é—´ï¼‰ã€ä½¿ç”¨çš„`device`ï¼ˆè®¾å¤‡ï¼‰ã€`browser`ï¼ˆæµè§ˆå™¨ï¼‰ä»¥åŠä»–ä»¬çš„åœ°ç†`region`ï¼ˆåŒºåŸŸï¼‰ã€‚æˆ‘ä»¬è¿˜è®°å½•äº†ä»–ä»¬æ˜¯å¦è·å¾—äº†`discount`ï¼ˆæŠ˜æ‰£ï¼‰ï¼Œæˆ‘ä»¬çš„å¤„ç†ï¼Œä»¥åŠä»–ä»¬çš„`spend`ï¼ˆæ”¯å‡ºï¼‰ï¼Œè¿™æ˜¯æˆ‘ä»¬çš„å…³æ³¨ç‚¹ã€‚
- en: Since the treatment was randomly assigned, we can use a simple **difference-in-means**
    estimator to estimate the treatment effect. We expect the treatment and control
    group to be similar, except for the `discount`, therefore we can causally attribute
    any difference in `spend` to the `discount`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå¤„ç†æ˜¯éšæœºåˆ†é…çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç®€å•çš„**å‡å€¼å·®å¼‚**ä¼°è®¡é‡æ¥ä¼°è®¡å¤„ç†æ•ˆåº”ã€‚æˆ‘ä»¬æœŸæœ›å¤„ç†ç»„å’Œå¯¹ç…§ç»„åœ¨`discount`ï¼ˆæŠ˜æ‰£ï¼‰ä¹‹å¤–æ˜¯ç›¸ä¼¼çš„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†`spend`ï¼ˆæ”¯å‡ºï¼‰çš„ä»»ä½•å·®å¼‚å½’å› äº`discount`ï¼ˆæŠ˜æ‰£ï¼‰ã€‚
- en: 'The discount seems to be effective: on average the spending in the treatment
    group increases by 1.95$. But are all customers equally affected?'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æŠ˜æ‰£ä¼¼ä¹æœ‰æ•ˆï¼šåœ¨å¤„ç†ç»„ä¸­ï¼Œå¹³å‡æ”¯å‡ºå¢åŠ äº†1.95ç¾å…ƒã€‚ä½†æ‰€æœ‰å®¢æˆ·çš„ååº”æ˜¯å¦ç›¸åŒï¼Ÿ
- en: To answer this question, we would like to estimate **heterogeneous treatment
    effects**, possibly at the individual level.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¸Œæœ›ä¼°è®¡**å¼‚è´¨å¤„ç†æ•ˆåº”**ï¼Œå¯èƒ½åœ¨ä¸ªä½“å±‚é¢ä¸Šã€‚
- en: Heterogeneous Treatment Effects
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼‚è´¨å¤„ç†æ•ˆåº”
- en: There are many possible ways to estimate heterogeneous treatment effects. The
    most common is to split the population into groups based on some observable characteristic,
    which in our case could be the `device`, the `browser` or the geographical `region`.
    Once you have decided which variable to split your data on, you can simply interact
    with the treatment variable (`discount`) with the dimension of treatment heterogeneity.
    Let's take `device` for example.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼°è®¡å¼‚è´¨å¤„ç†æ•ˆåº”æœ‰å¤šç§æ–¹æ³•ã€‚æœ€å¸¸è§çš„æ–¹æ³•æ˜¯æ ¹æ®ä¸€äº›å¯è§‚å¯Ÿçš„ç‰¹å¾å°†äººç¾¤åˆ’åˆ†ä¸ºä¸åŒçš„ç»„ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œè¿™äº›ç‰¹å¾å¯ä»¥æ˜¯`device`ï¼ˆè®¾å¤‡ï¼‰ã€`browser`ï¼ˆæµè§ˆå™¨ï¼‰æˆ–åœ°ç†`region`ï¼ˆåŒºåŸŸï¼‰ã€‚ä¸€æ—¦å†³å®šäº†æ•°æ®åˆ’åˆ†çš„å˜é‡ï¼Œä½ å¯ä»¥ç®€å•åœ°å°†å¤„ç†å˜é‡ï¼ˆ`discount`ï¼‰ä¸å¤„ç†å¼‚è´¨æ€§çš„ç»´åº¦è¿›è¡Œäº¤äº’ã€‚ä»¥`device`ä¸ºä¾‹ã€‚
- en: How do we interpret the regression results? The effect of the `discount` on
    customers' `spend` is 1.22$ but it increases by a further 1.44$ if the customer
    is accessing the website from a mobile `device`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•è§£è¯»å›å½’ç»“æœï¼Ÿ`discount`å¯¹å®¢æˆ·`spend`çš„å½±å“æ˜¯1.22$ï¼Œä½†å¦‚æœå®¢æˆ·é€šè¿‡ç§»åŠ¨`device`è®¿é—®ç½‘ç«™ï¼Œè¿™ä¸€å½±å“ä¼šå¢åŠ è‡³1.44$ã€‚
- en: Splitting is easy for categorical variables, but for a continuous variable like
    `time` it is not intuitive where to split. Every hour? And which dimension is
    more informative? It would be tempting to try all possible splits, but the more
    we split the data, the more it is likely that we find spurious results (i.e. we
    overfit, in machine learning lingo). It would be great if we could **let the data
    speak** and select the minimum and most informative splits.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†ç±»å˜é‡ï¼Œåˆ’åˆ†å¾ˆå®¹æ˜“ï¼Œä½†å¯¹äºåƒ`time`è¿™æ ·çš„è¿ç»­å˜é‡æ¥è¯´ï¼Œä¸ç›´è§‚å¦‚ä½•åˆ’åˆ†ã€‚æ¯å°æ—¶åˆ’åˆ†ä¸€æ¬¡ï¼Ÿå“ªä¸ªç»´åº¦æ›´å…·ä¿¡æ¯æ€§ï¼Ÿè™½ç„¶å¾ˆè¯±äººå°è¯•æ‰€æœ‰å¯èƒ½çš„åˆ’åˆ†ï¼Œä½†æˆ‘ä»¬å¯¹æ•°æ®çš„åˆ’åˆ†è¶Šå¤šï¼Œå‘ç°è™šå‡ç»“æœï¼ˆå³åœ¨æœºå™¨å­¦ä¹ æœ¯è¯­ä¸­ï¼Œæˆ‘ä»¬è¿‡æ‹Ÿåˆï¼‰çš„å¯èƒ½æ€§å°±è¶Šå¤§ã€‚å¦‚æœæˆ‘ä»¬èƒ½**è®©æ•°æ®è¯´è¯**å¹¶é€‰æ‹©æœ€å°ä¸”ä¿¡æ¯é‡æœ€å¤§çš„åˆ’åˆ†ï¼Œé‚£å°±å¤ªå¥½äº†ã€‚
- en: In a [separate post](https://medium.com/towards-data-science/understanding-meta-learners-8a9c1e340832),
    I have shown how the so-called **meta-learners** take this approach to causal
    inference. The idea is to predict the outcome conditional on the treatment status
    for each observation, and then compare the predicted conditional on treatment,
    with the predicted outcome conditional on control. The difference is the individual
    treatment effect.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[å¦ä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/understanding-meta-learners-8a9c1e340832)ä¸­ï¼Œæˆ‘å±•ç¤ºäº†æ‰€è°“çš„**å…ƒå­¦ä¹ è€…**å¦‚ä½•é‡‡å–è¿™ç§å› æœæ¨æ–­æ–¹æ³•ã€‚æ€è·¯æ˜¯æ ¹æ®æ¯ä¸ªè§‚å¯Ÿçš„æ²»ç–—çŠ¶æ€é¢„æµ‹ç»“æœï¼Œç„¶åå°†é¢„æµ‹çš„æ¡ä»¶æ²»ç–—ç»“æœä¸é¢„æµ‹çš„å¯¹ç…§ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚äºŒè€…ä¹‹é—´çš„å·®å¼‚å°±æ˜¯ä¸ªä½“æ²»ç–—æ•ˆåº”ã€‚
- en: The problem with meta-learners is that they use all their [degrees of freedom](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics))
    in predicting the outcome. However, we are interested to predict treatment effect
    heterogeneity. If most of the variation in the outcome is *not* in the treatment
    dimension, we will get very poor estimates of the treatment effects.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å…ƒå­¦ä¹ è€…çš„é—®é¢˜åœ¨äºï¼Œå®ƒä»¬åœ¨é¢„æµ‹ç»“æœæ—¶ä½¿ç”¨äº†æ‰€æœ‰çš„[è‡ªç”±åº¦](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics))ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯é¢„æµ‹æ²»ç–—æ•ˆåº”çš„å¼‚è´¨æ€§ã€‚å¦‚æœç»“æœçš„å¤§éƒ¨åˆ†å˜å¼‚*ä¸*åœ¨æ²»ç–—ç»´åº¦ä¸Šï¼Œæˆ‘ä»¬å°†å¾—åˆ°éå¸¸å·®çš„æ²»ç–—æ•ˆåº”ä¼°è®¡ã€‚
- en: Is it possible to instead directly concentrate on the **prediction of individual
    treatment effects**? Letâ€™s define *Y* as the outcome of interest `spend`, *D*
    the treatment `discount`, and *X* other observable characteristics. The *ideal*
    loss function is
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯å¦å¯ä»¥ç›´æ¥é›†ä¸­åœ¨**ä¸ªä½“æ²»ç–—æ•ˆåº”çš„é¢„æµ‹**ä¸Šï¼Ÿæˆ‘ä»¬å°†*Y*å®šä¹‰ä¸ºæ„Ÿå…´è¶£çš„ç»“æœ`spend`ï¼Œ*D*ä¸ºæ²»ç–—`discount`ï¼Œä»¥åŠ*X*ä¸ºå…¶ä»–å¯è§‚å¯Ÿç‰¹å¾ã€‚*ç†æƒ³*çš„æŸå¤±å‡½æ•°æ˜¯
- en: '![](../Images/625014656a5a9c75f9c9c8425affd810.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/625014656a5a9c75f9c9c8425affd810.png)'
- en: Ideal loss function, image by Author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç†æƒ³çš„æŸå¤±å‡½æ•°ï¼Œå›¾ç‰‡ç”±ä½œè€…æä¾›
- en: where *Ï„áµ¢* is the treatment effect of individual *i*. However, this objective
    function is **unfeasible** since we do not observe *Ï„áµ¢*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­*Ï„áµ¢*æ˜¯ä¸ªä½“*i*çš„æ²»ç–—æ•ˆåº”ã€‚ç„¶è€Œï¼Œè¿™ä¸ªç›®æ ‡å‡½æ•°æ˜¯**ä¸å¯è¡Œçš„**ï¼Œå› ä¸ºæˆ‘ä»¬æ— æ³•è§‚å¯Ÿåˆ°*Ï„áµ¢*ã€‚
- en: But, turns out that there is a way to get an unbiased estimate of the **individual
    treatment effect**. The **idea** is to use an auxiliary outcome variable, whose
    expected value for each individual is the individual treatment effect. This variable
    is
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†äº‹å®è¯æ˜ï¼Œæœ‰ä¸€ç§æ–¹æ³•å¯ä»¥è·å¾—**ä¸ªä½“æ²»ç–—æ•ˆåº”**çš„æ— åä¼°è®¡ã€‚**æ€è·¯**æ˜¯ä½¿ç”¨ä¸€ä¸ªè¾…åŠ©ç»“æœå˜é‡ï¼Œå…¶æ¯ä¸ªä¸ªä½“çš„æœŸæœ›å€¼å³ä¸ºä¸ªä½“æ²»ç–—æ•ˆåº”ã€‚è¿™ä¸ªå˜é‡æ˜¯
- en: '![](../Images/8c6ee904e46dcad7945f99fe47a9c19b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c6ee904e46dcad7945f99fe47a9c19b.png)'
- en: Auxiliary outcome variable, image by Author
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¾…åŠ©ç»“æœå˜é‡ï¼Œå›¾ç‰‡ç”±ä½œè€…æä¾›
- en: where *p*(*Xáµ¢*) is the [**propensity score**](https://en.wikipedia.org/wiki/Propensity_score_matching)
    of observation *i*, i.e. its probability of being treated.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­*p*(*Xáµ¢*)æ˜¯è§‚å¯Ÿ*i*çš„[**å€¾å‘è¯„åˆ†**](https://en.wikipedia.org/wiki/Propensity_score_matching)ï¼Œå³å…¶è¢«æ²»ç–—çš„æ¦‚ç‡ã€‚
- en: In randomized experiments, the propensity score is known since randomization
    is fully under the control of the experimenter. For example, in our case, the
    probability of treatment was 50%. In quasi-experimental studies instead, when
    the treatment probability is not known, it has to be estimated. Even in randomized
    experiments, it is always better to estimate rather than impute the propensity
    scores, since it guards against sampling variation in the randomization. For more
    details on the propensity scores and how they are used in causal inference, I
    have a separate post [here](https://medium.com/towards-data-science/matching-weighting-or-regression-99bf5cffa0d9).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨éšæœºåŒ–å®éªŒä¸­ï¼Œå€¾å‘å¾—åˆ†æ˜¯å·²çŸ¥çš„ï¼Œå› ä¸ºéšæœºåŒ–å®Œå…¨åœ¨å®éªŒè€…çš„æ§åˆ¶ä¹‹ä¸‹ã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæ²»ç–—çš„æ¦‚ç‡æ˜¯50%ã€‚è€Œåœ¨å‡†å®éªŒç ”ç©¶ä¸­ï¼Œå½“æ²»ç–—æ¦‚ç‡æœªçŸ¥æ—¶ï¼Œéœ€è¦è¿›è¡Œä¼°è®¡ã€‚å³ä½¿åœ¨éšæœºåŒ–å®éªŒä¸­ï¼Œä¼°è®¡å€¾å‘å¾—åˆ†æ€»æ˜¯æ¯”å¡«è¡¥æ›´å¥½ï¼Œå› ä¸ºå®ƒèƒ½é˜²æ­¢éšæœºåŒ–ä¸­çš„æŠ½æ ·å˜å¼‚ã€‚æœ‰å…³å€¾å‘å¾—åˆ†åŠå…¶åœ¨å› æœæ¨æ–­ä¸­çš„ä½¿ç”¨çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æˆ‘åœ¨[è¿™é‡Œ](https://medium.com/towards-data-science/matching-weighting-or-regression-99bf5cffa0d9)çš„å•ç‹¬å¸–å­ã€‚
- en: Letâ€™s first generate dummy variables for our categorical variables, `device`,
    `browser` and `region`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºç±»åˆ«å˜é‡`device`ã€`browser`å’Œ`region`ç”Ÿæˆè™šæ‹Ÿå˜é‡ã€‚
- en: We fit a `LogisticRegression` and use it to predict the treatment probability,
    i.e. construct the propensity score.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‹Ÿåˆäº†ä¸€ä¸ª`LogisticRegression`å¹¶ç”¨å®ƒæ¥é¢„æµ‹æ²»ç–—æ¦‚ç‡ï¼Œå³æ„å»ºå€¾å‘å¾—åˆ†ã€‚
- en: '![](../Images/afa6eb8ef5d4eff1aef7264b9cd1a95e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afa6eb8ef5d4eff1aef7264b9cd1a95e.png)'
- en: Distribution of estimated propensity scores, image by Author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼°è®¡çš„å€¾å‘å¾—åˆ†åˆ†å¸ƒï¼Œå›¾ç‰‡æ¥æºï¼šä½œè€…
- en: As expected, most propensity scores are very close to 0.5, the probability of
    treatment used in randomization. Moreover, the distribution is almost identical
    across the treatment and control groups, further confirming that randomization
    worked. If it had not been the case, we would have needed to make further assumptions
    in order to conduct a causal analysis. The most common one is **unconfoundedness**,
    also known as ignorability or selection on observables. In short, we will assume
    that conditional on some observables ğ‘‹ the treatment assignment is as good as
    random.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚é¢„æœŸçš„ï¼Œå¤§å¤šæ•°å€¾å‘å¾—åˆ†æ¥è¿‘0.5ï¼Œè¿™æ˜¯éšæœºåŒ–ä¸­ä½¿ç”¨çš„æ²»ç–—æ¦‚ç‡ã€‚æ­¤å¤–ï¼Œæ²»ç–—ç»„å’Œå¯¹ç…§ç»„çš„åˆ†å¸ƒå‡ ä¹å®Œå…¨ç›¸åŒï¼Œè¿›ä¸€æ­¥ç¡®è®¤äº†éšæœºåŒ–çš„æœ‰æ•ˆæ€§ã€‚å¦‚æœæƒ…å†µä¸æ˜¯è¿™æ ·ï¼Œæˆ‘ä»¬å°†éœ€è¦åšå‡ºè¿›ä¸€æ­¥å‡è®¾ä»¥è¿›è¡Œå› æœåˆ†æã€‚æœ€å¸¸è§çš„å‡è®¾æ˜¯**æ— æ··æ·†æ€§**ï¼Œä¹Ÿç§°ä¸ºå¯å¿½ç•¥æ€§æˆ–åŸºäºå¯è§‚æµ‹å˜é‡çš„é€‰æ‹©ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬å°†å‡è®¾åœ¨æŸäº›å¯è§‚æµ‹å˜é‡ğ‘‹çš„æ¡ä»¶ä¸‹ï¼Œæ²»ç–—åˆ†é…æ˜¯éšæœºçš„ã€‚
- en: '![](../Images/7a455561d68df293eac18c050dbd3a7a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a455561d68df293eac18c050dbd3a7a.png)'
- en: Unconfoundedness assumption, image by Author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ— æ··æ·†å‡è®¾**ï¼Œå›¾ç‰‡æ¥æºï¼šä½œè€…'
- en: However, in our case, the treatment probability is known and it seems that nothing
    went wrong in the randomization process.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæ²»ç–—æ¦‚ç‡æ˜¯å·²çŸ¥çš„ï¼Œå¹¶ä¸”ä¼¼ä¹éšæœºåŒ–è¿‡ç¨‹ä¸­æ²¡æœ‰å‡ºç°é—®é¢˜ã€‚
- en: We now have all the elements to compute our auxiliary outcome variable *Y**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨æ‹¥æœ‰è®¡ç®—è¾…åŠ©ç»“æœå˜é‡*Y**çš„æ‰€æœ‰å…ƒç´ ã€‚
- en: As we said before, the idea is to use *Y** as the target of a prediction problem,
    since the expected value is exactly the individual treatment effect. Letâ€™s check
    its average in the data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€è¯´ï¼Œç›®çš„æ˜¯å°†*Y**ä½œä¸ºé¢„æµ‹é—®é¢˜çš„ç›®æ ‡ï¼Œå› ä¸ºå…¶æœŸæœ›å€¼æ­£å¥½æ˜¯ä¸ªä½“æ²»ç–—æ•ˆæœã€‚è®©æˆ‘ä»¬æ£€æŸ¥æ•°æ®ä¸­çš„å¹³å‡å€¼ã€‚
- en: Indeed its average is almost identical to the previously estimated average treatment
    effect of 1.94$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®ï¼Œå®ƒçš„å¹³å‡å€¼å‡ ä¹ä¸ä¹‹å‰ä¼°è®¡çš„1.94$çš„å¹³å‡æ²»ç–—æ•ˆæœç›¸åŒã€‚
- en: How is it possible that, with a single observation and an estimate of the propensity
    score, we can estimate the individual treatment effect? What are the drawbacks?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨åªæœ‰ä¸€ä¸ªè§‚å¯Ÿå€¼å’Œå€¾å‘å¾—åˆ†ä¼°è®¡çš„æƒ…å†µä¸‹ä¼°è®¡ä¸ªä½“æ²»ç–—æ•ˆæœï¼Ÿæœ‰ä»€ä¹ˆç¼ºç‚¹ï¼Ÿ
- en: 'The **intuition** is to approach the problem from a different perspective:
    *ex-ante*, before the experiment. Imagine that our dataset had a single observation,
    *i*. We know that the treatment probability is *p*(*Xáµ¢*), the propensity score.
    Therefore, in expectation, our dataset has *p*(*Xáµ¢*) observations in the treatment
    group and *1â€“p*(*Xáµ¢*) observations in the control group. The rest is business
    as usual: we estimate the treatment effect as the difference in average outcomes
    between the two groups! And indeed that is what we would do:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›´è§‚**çš„æƒ³æ³•æ˜¯ä»ä¸åŒçš„è§’åº¦æ¥å¤„ç†é—®é¢˜ï¼š*äº‹å‰*ï¼Œåœ¨å®éªŒä¹‹å‰ã€‚è®¾æƒ³æˆ‘ä»¬çš„æ•°æ®é›†åªæœ‰ä¸€ä¸ªè§‚å¯Ÿå€¼ï¼Œ*i*ã€‚æˆ‘ä»¬çŸ¥é“æ²»ç–—æ¦‚ç‡æ˜¯*p*(*Xáµ¢*)ï¼Œå³å€¾å‘å¾—åˆ†ã€‚å› æ­¤ï¼ŒæœŸæœ›ä¸­ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†ä¸­æ²»ç–—ç»„æœ‰*p*(*Xáµ¢*)ä¸ªè§‚å¯Ÿå€¼ï¼Œå¯¹ç…§ç»„æœ‰*1â€“p*(*Xáµ¢*)ä¸ªè§‚å¯Ÿå€¼ã€‚å…¶ä½™çš„ç…§å¸¸å¤„ç†ï¼šæˆ‘ä»¬é€šè¿‡ä¸¤ç»„ä¹‹é—´çš„å¹³å‡ç»“æœå·®å¼‚æ¥ä¼°è®¡æ²»ç–—æ•ˆæœï¼è¿™ç¡®å®æ˜¯æˆ‘ä»¬ä¼šåšçš„ï¼š'
- en: '![](../Images/a61c82853d3864a67fa2af6e2ef190ee.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a61c82853d3864a67fa2af6e2ef190ee.png)'
- en: Auxiliary outcome variable, image by Author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¾…åŠ©ç»“æœå˜é‡ï¼Œå›¾ç‰‡æ¥æºï¼šä½œè€…
- en: The only difference is that we have a single observation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å”¯ä¸€çš„åŒºåˆ«æ˜¯æˆ‘ä»¬åªæœ‰ä¸€ä¸ªè§‚å¯Ÿå€¼ã€‚
- en: 'This trick comes at a cost: *Yáµ¢** is an unbiased estimator for the individual
    treatment effect but has a very **high variance**. This is immediately visible
    by plotting its distribution.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŠ€å·§æœ‰ä¸€ä¸ªä»£ä»·ï¼š*Yáµ¢* æ˜¯ä¸ªä½“å¤„ç†æ•ˆåº”çš„æ— åä¼°è®¡é‡ï¼Œä½†å…·æœ‰éå¸¸**é«˜çš„æ–¹å·®**ã€‚é€šè¿‡ç»˜åˆ¶å…¶åˆ†å¸ƒï¼Œè¿™ä¸€ç‚¹å¯ä»¥ç«‹å³æ˜¾ç°å‡ºæ¥ã€‚
- en: '![](../Images/631c1029186de6b42501d772234bf081.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/631c1029186de6b42501d772234bf081.png)'
- en: Distribution of the auxiliary variable, image by Author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¾…åŠ©å˜é‡çš„åˆ†å¸ƒï¼Œå›¾åƒç”±ä½œè€…æä¾›
- en: We are now ready to estimate **heterogeneous treatment effects**, by translating
    the causal inference problem into a prediction problem, predicting the auxiliary
    outcome *Y**, given observable characteristics *X*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡ä¼°è®¡**å¼‚è´¨æ€§å¤„ç†æ•ˆåº”**ï¼Œé€šè¿‡å°†å› æœæ¨æ–­é—®é¢˜è½¬åŒ–ä¸ºé¢„æµ‹é—®é¢˜ï¼Œé¢„æµ‹ç»™å®šå¯è§‚å¯Ÿç‰¹å¾*X*çš„è¾…åŠ©ç»“æœ*Y*ã€‚
- en: '![](../Images/4f2b5684285e0ab0fdf2e3c667945ccf.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f2b5684285e0ab0fdf2e3c667945ccf.png)'
- en: Image generated by Author using [NightCafÃ©](https://creator.nightcafe.studio/)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒç”±ä½œè€…ä½¿ç”¨[NightCafÃ©](https://creator.nightcafe.studio/)ç”Ÿæˆ
- en: Causal Trees
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å› æœæ ‘
- en: In the previous section, we have seen that we can transform the estimation of
    **heterogeneous treatment effects** into a prediction problem, where the outcome
    is the auxiliary outcome variable
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œæˆ‘ä»¬å¯ä»¥å°†**å¼‚è´¨æ€§å¤„ç†æ•ˆåº”**çš„ä¼°è®¡è½¬åŒ–ä¸ºé¢„æµ‹é—®é¢˜ï¼Œå…¶ä¸­ç»“æœæ˜¯è¾…åŠ©ç»“æœå˜é‡ã€‚
- en: '![](../Images/8c6ee904e46dcad7945f99fe47a9c19b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c6ee904e46dcad7945f99fe47a9c19b.png)'
- en: Auxiliary outcome variable, image by Author
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¾…åŠ©ç»“æœå˜é‡ï¼Œå›¾åƒç”±ä½œè€…æä¾›
- en: We can in principle use any machine learning algorithm at this point to estimate
    individual treatment effects. However, [**regression trees**](https://en.wikipedia.org/wiki/Decision_tree_learning)
    have particularly convenient characteristics.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åŸåˆ™ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•æœºå™¨å­¦ä¹ ç®—æ³•æ¥ä¼°è®¡ä¸ªä½“å¤„ç†æ•ˆåº”ã€‚ç„¶è€Œï¼Œ[**å›å½’æ ‘**](https://en.wikipedia.org/wiki/Decision_tree_learning)å…·æœ‰ç‰¹åˆ«ä¾¿åˆ©çš„ç‰¹å¾ã€‚
- en: First of all, how do regression trees work? Classification and regression trees
    (CART) are algorithms that recursively **partition the data in bins** based on
    covariates *X* such that the outcome *Y* *within* each bin is as homogeneous as
    possible and the outcome *across* bins is as heterogeneous as possible. The predicted
    values are simply the outcome averages within each bin, in our case the *auxiliary*
    outcome variable *Y**, whose expected value for each observation is equal to the
    individual treatment effect. Therefore, by averaging *Y** within each bin, we
    can compute the **conditional (on X) heterogeneous treatment effect** ğ”¼[*Ï„áµ¢ |
    Xáµ¢*] for observations that fall within that bin**.**
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå›å½’æ ‘æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿåˆ†ç±»å’Œå›å½’æ ‘ï¼ˆCARTï¼‰æ˜¯åŸºäºåå˜é‡*X*é€’å½’**å¯¹æ•°æ®è¿›è¡Œåˆ†ç®±**çš„ç®—æ³•ï¼Œä½¿å¾—æ¯ä¸ªç®±ä¸­çš„ç»“æœ*Y*åœ¨*ç®±å†…*å°½å¯èƒ½åŒè´¨ï¼Œè€Œ*ç®±é—´*çš„ç»“æœå°½å¯èƒ½å¼‚è´¨ã€‚é¢„æµ‹å€¼åªæ˜¯æ¯ä¸ªç®±ä¸­çš„ç»“æœå¹³å‡å€¼ï¼Œåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹æ˜¯*è¾…åŠ©*ç»“æœå˜é‡*Y*ï¼Œæ¯ä¸ªè§‚å¯Ÿå€¼çš„æœŸæœ›å€¼ç­‰äºä¸ªä½“å¤„ç†æ•ˆåº”ã€‚å› æ­¤ï¼Œé€šè¿‡å¯¹æ¯ä¸ªç®±ä¸­çš„*Y*è¿›è¡Œå¹³å‡ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—**æ¡ä»¶ï¼ˆåŸºäºXï¼‰çš„å¼‚è´¨æ€§å¤„ç†æ•ˆåº”**
    ğ”¼[*Ï„áµ¢ | Xáµ¢*] å¯¹äºè½åœ¨è¯¥ç®±ä¸­çš„è§‚å¯Ÿå€¼ã€‚
- en: The **averaging** part is one of the big advantages of regression trees for
    inference since we know very well how to do inference with averages, thanks to
    the [**Central Limit Theorem**](https://en.wikipedia.org/wiki/Central_limit_theorem).
    The second advantage of regression trees over other machine learning algorithms
    is that trees are very **interpretable** since we can directly plot the data partition
    as a tree structure. We will see more of this later. Last but not least, regression
    trees are still at the core of the [best-performing predictive algorithms](https://arxiv.org/abs/2207.08815)
    with tabular data, as of 2022.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¹³å‡åŒ–**éƒ¨åˆ†æ˜¯å›å½’æ ‘æ¨æ–­çš„ä¸€ä¸ªä¸»è¦ä¼˜åŠ¿ï¼Œå› ä¸ºæˆ‘ä»¬éå¸¸æ¸…æ¥šå¦‚ä½•ä½¿ç”¨å¹³å‡å€¼è¿›è¡Œæ¨æ–­ï¼Œè¿™è¦å½’åŠŸäº[**ä¸­å¿ƒæé™å®šç†**](https://en.wikipedia.org/wiki/Central_limit_theorem)ã€‚å›å½’æ ‘ç›¸å¯¹äºå…¶ä»–æœºå™¨å­¦ä¹ ç®—æ³•çš„ç¬¬äºŒä¸ªä¼˜åŠ¿æ˜¯æ ‘éå¸¸**å¯è§£é‡Š**ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ç›´æ¥å°†æ•°æ®åˆ†åŒºç»˜åˆ¶ä¸ºæ ‘ç»“æ„ã€‚æˆ‘ä»¬ç¨åä¼šè¯¦ç»†äº†è§£è¿™ä¸€ç‚¹ã€‚æœ€åä½†åŒæ ·é‡è¦çš„æ˜¯ï¼Œæˆªè‡³2022å¹´ï¼Œå›å½’æ ‘ä»ç„¶æ˜¯[è¡¨ç°æœ€ä½³çš„é¢„æµ‹ç®—æ³•](https://arxiv.org/abs/2207.08815)çš„æ ¸å¿ƒã€‚'
- en: Letâ€™s use the `[DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)`
    function from `sklearn` to fit our regression tree and estimate the heterogeneous
    treatment effects of `discounts` on customers' `spend`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨`sklearn`ä¸­çš„`[DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)`å‡½æ•°æ¥æ‹Ÿåˆæˆ‘ä»¬çš„å›å½’æ ‘ï¼Œå¹¶ä¼°è®¡`discounts`å¯¹é¡¾å®¢`spend`çš„å¼‚è´¨æ€§å¤„ç†æ•ˆåº”ã€‚
- en: We have restricted the tree to have a maximum depth of 2 and at least 30 observations
    per partition (also called *leaf*) so that we can easily plot the tree and visualize
    the estimated groups and treatment effects.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ ‘çš„æœ€å¤§æ·±åº¦é™åˆ¶ä¸º2ï¼Œå¹¶ä¸”æ¯ä¸ªåˆ†åŒºï¼ˆä¹Ÿç§°ä¸º*å¶å­*ï¼‰è‡³å°‘åŒ…å«30ä¸ªè§‚å¯Ÿå€¼ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è½»æ¾ç»˜åˆ¶æ ‘å¹¶å¯è§†åŒ–ä¼°è®¡çš„ç»„å’Œå¤„ç†æ•ˆåº”ã€‚
- en: '![](../Images/caff3ccb47368f02118a113b301a7874.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/caff3ccb47368f02118a113b301a7874.png)'
- en: Regression tree on auxiliary outcome variable Y*, image by Author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¾…åŠ©ç»“æœå˜é‡Y*ä¸Šçš„å›å½’æ ‘ï¼Œä½œè€…æä¾›çš„å›¾ç‰‡
- en: How should we **interpret** the tree? On the top, we can see the average *Y**
    in the data, 1.945$, corresponding with the average treatment effect. Starting
    from there, the data gets split into different branches, according to the rules
    highlighted at the top of each node. For example, the first node splits the data
    into two groups of size 51,156 and 48,844 depending on whether the `time` is later
    than 12.325\. At the bottom, we have our final partitions, with the heterogeneous
    treatment effects. For example, the leftmost leaf contains 43,876 observations
    with `time` earlier than 12.325 and non-Safari `browser`, for which we predict
    an effect on `spend` of 0.295$. In short, every node contains an estimate of the
    **conditional average treatment effect** ğ”¼[*Ï„áµ¢ | Xáµ¢*], where darker node colors
    indicate higher prediction values.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åº”è¯¥å¦‚ä½•**è§£è¯»**è¿™æ£µæ ‘ï¼Ÿåœ¨é¡¶éƒ¨ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ•°æ®ä¸­çš„å¹³å‡å€¼*Y**ä¸º1.945$ï¼Œå¯¹åº”äºå¹³å‡å¤„ç†æ•ˆåº”ã€‚ä»é‚£é‡Œå¼€å§‹ï¼Œæ•°æ®æ ¹æ®æ¯ä¸ªèŠ‚ç‚¹é¡¶éƒ¨çªå‡ºæ˜¾ç¤ºçš„è§„åˆ™è¢«åˆ†æˆä¸åŒçš„åˆ†æ”¯ã€‚ä¾‹å¦‚ï¼Œç¬¬ä¸€ä¸ªèŠ‚ç‚¹å°†æ•°æ®åˆ†æˆä¸¤ä¸ªå¤§å°åˆ†åˆ«ä¸º51,156å’Œ48,844çš„ç»„ï¼Œå…·ä½“å–å†³äº`time`æ˜¯å¦æ™šäº12.325ã€‚åœ¨åº•éƒ¨ï¼Œæˆ‘ä»¬æœ‰æœ€ç»ˆçš„åˆ†åŒºï¼ŒåŒ…å«å¼‚è´¨çš„å¤„ç†æ•ˆåº”ã€‚ä¾‹å¦‚ï¼Œæœ€å·¦è¾¹çš„å¶å­åŒ…å«43,876ä¸ªè§‚å¯Ÿå€¼ï¼Œå…¶ä¸­`time`æ—©äº12.325ä¸”`browser`ä¸æ˜¯Safariï¼Œæˆ‘ä»¬é¢„æµ‹å¯¹`spend`çš„å½±å“ä¸º0.295$ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæ¯ä¸ªèŠ‚ç‚¹åŒ…å«**æ¡ä»¶å¹³å‡å¤„ç†æ•ˆåº”**
    ğ”¼[*Ï„áµ¢ | Xáµ¢*]çš„ä¼°è®¡ï¼Œå…¶ä¸­èŠ‚ç‚¹é¢œè‰²è¶Šæ·±è¡¨ç¤ºé¢„æµ‹å€¼è¶Šé«˜ã€‚
- en: Should we believe these estimates? Not really, because of a couple of reasons.
    The **first problem** is that we have an unbiased estimate of the average treatment
    effect only if, *within each leaf*, we have the same number of treated and control
    units. This is not automatically the case with an off-the-shelf `DecisionTreeRegressor()`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åº”è¯¥ç›¸ä¿¡è¿™äº›ä¼°è®¡å—ï¼Ÿä¸å®Œå…¨æ˜¯ï¼Œå› ä¸ºæœ‰å‡ ä¸ªåŸå› ã€‚**ç¬¬ä¸€ä¸ªé—®é¢˜**æ˜¯ï¼Œåªæœ‰åœ¨*æ¯ä¸ªå¶å­å†…éƒ¨*æˆ‘ä»¬æœ‰ç›¸åŒæ•°é‡çš„å¤„ç†ç»„å’Œå¯¹ç…§ç»„å•å…ƒæ—¶ï¼Œæˆ‘ä»¬æ‰æœ‰å¹³å‡å¤„ç†æ•ˆåº”çš„æ— åä¼°è®¡ã€‚è¿™åœ¨ä½¿ç”¨ç°æˆçš„`DecisionTreeRegressor()`æ—¶å¹¶éè‡ªåŠ¨æˆç«‹ã€‚
- en: Honest Trees
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯šå®æ ‘
- en: 'Another problem with our naive procedure is that we have used the **same data**
    to generate the tree and evaluate it. This generates bias because a simple difference
    in means estimator would not take into account the fact that the partition is
    endogenous, i.e. generated on the same data. In machine learning terms, we are
    overfitting. The solution is simple: we can split the sample into two separate
    subsamples and use different data to generate the tree and compute the predictions.
    These trees are called **honest trees**.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç®€å•æ–¹æ³•çš„å¦ä¸€ä¸ªé—®é¢˜æ˜¯æˆ‘ä»¬ä½¿ç”¨äº†**ç›¸åŒçš„æ•°æ®**æ¥ç”Ÿæˆå’Œè¯„ä¼°æ ‘ã€‚è¿™ä¼šäº§ç”Ÿåå·®ï¼Œå› ä¸ºç®€å•çš„å‡å€¼å·®å¼‚ä¼°è®¡é‡ä¸ä¼šè€ƒè™‘åˆ†åŒºæ˜¯å†…ç”Ÿçš„ï¼Œå³åœ¨ç›¸åŒçš„æ•°æ®ä¸Šç”Ÿæˆçš„ã€‚ç”¨æœºå™¨å­¦ä¹ æœ¯è¯­æ¥è¯´ï¼Œæˆ‘ä»¬æ˜¯åœ¨è¿‡æ‹Ÿåˆã€‚è§£å†³æ–¹æ¡ˆå¾ˆç®€å•ï¼šæˆ‘ä»¬å¯ä»¥å°†æ ·æœ¬æ‹†åˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å­æ ·æœ¬ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„æ•°æ®ç”Ÿæˆæ ‘å’Œè®¡ç®—é¢„æµ‹ã€‚è¿™äº›æ ‘è¢«ç§°ä¸º**è¯šå®æ ‘**ã€‚
- en: This solution is as simple as effective since it allows us, at the inference
    stage, to treat each sample within a leaf as independent from the tree structure.
    At that point, our estimator is a **difference in means** estimator for an independent
    subsample we can simply use the Central Limit Theorem for inference. One drawback
    of splitting the data is that we lose [power](https://en.wikipedia.org/wiki/Power_of_a_test),
    i.e. the ability to detect non-spurious heterogeneous treatment effects because
    of the smaller sample. The solution is to repeat the procedure twice, swapping
    the sample used to build the tree and the sample used to compute the within-leaf
    averages. Then, we can average the two estimates for each individual and adjust
    the estimated standard errors accordingly.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè§£å†³æ–¹æ¡ˆæ—¢ç®€å•åˆæœ‰æ•ˆï¼Œå› ä¸ºå®ƒå…è®¸æˆ‘ä»¬åœ¨æ¨æ–­é˜¶æ®µï¼Œå°†æ¯ä¸ªå¶å­ä¸­çš„æ ·æœ¬è§†ä¸ºä¸æ ‘ç»“æ„ç‹¬ç«‹ã€‚æ­¤æ—¶ï¼Œæˆ‘ä»¬çš„ä¼°è®¡é‡æ˜¯å¯¹ç‹¬ç«‹å­æ ·æœ¬çš„**å‡å€¼å·®å¼‚**ä¼°è®¡é‡ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°ä½¿ç”¨ä¸­å¿ƒæé™å®šç†è¿›è¡Œæ¨æ–­ã€‚æ‹†åˆ†æ•°æ®çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯æˆ‘ä»¬å¤±å»äº†[ç»Ÿè®¡åŠŸæ•ˆ](https://en.wikipedia.org/wiki/Power_of_a_test)ï¼Œå³ç”±äºæ ·æœ¬è¾ƒå°è€Œæ— æ³•æ£€æµ‹åˆ°è™šå‡çš„å¼‚è´¨å¤„ç†æ•ˆåº”ã€‚è§£å†³æ–¹æ¡ˆæ˜¯é‡å¤è¯¥è¿‡ç¨‹ä¸¤æ¬¡ï¼Œäº¤æ¢ç”¨äºæ„å»ºæ ‘å’Œè®¡ç®—å¶å­å†…å‡å€¼çš„æ ·æœ¬ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ¯ä¸ªä¸ªä½“çš„ä¸¤ä¸ªä¼°è®¡å€¼å–å¹³å‡ï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´ä¼°è®¡çš„æ ‡å‡†è¯¯å·®ã€‚
- en: '![](../Images/5775862581c9150a1b868f1e86ff8f39.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5775862581c9150a1b868f1e86ff8f39.png)'
- en: Sample splitting procedure, image by Author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ ·æœ¬æ‹†åˆ†è¿‡ç¨‹ï¼Œä½œè€…æä¾›çš„å›¾ç‰‡
- en: Generating Splits
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ‹†åˆ†
- en: Last but not least, how should the tree be generated? The default rule to generate
    **splits** with the `DecisionTreeRegressor` function is the `squared_error` and
    there is no restriction on the minimum number of observations per leaf. Other
    commonly used rules include, mean absolute error, Giniâ€™s impurity, and Shannonâ€™s
    information. Which one performs better depends on the specific application, but
    the general objective is always prediction accuracy, broadly defined.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä½†åŒæ ·é‡è¦çš„æ˜¯ï¼Œæ ‘åº”è¯¥å¦‚ä½•ç”Ÿæˆï¼Ÿ`DecisionTreeRegressor`å‡½æ•°ç”Ÿæˆ**åˆ†è£‚**çš„é»˜è®¤è§„åˆ™æ˜¯`squared_error`ï¼Œå¹¶ä¸”å¯¹æ¯ä¸ªå¶å­ä¸­çš„æœ€å°è§‚æµ‹æ•°æ²¡æœ‰é™åˆ¶ã€‚å…¶ä»–å¸¸ç”¨è§„åˆ™åŒ…æ‹¬å¹³å‡ç»å¯¹è¯¯å·®ã€åŸºå°¼
    impurity å’Œé¦™å†œä¿¡æ¯ã€‚å“ªä¸ªè¡¨ç°æ›´å¥½å–å†³äºå…·ä½“åº”ç”¨ï¼Œä½†æ€»ä½“ç›®æ ‡å§‹ç»ˆæ˜¯é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¹¿ä¹‰ä¸Šå®šä¹‰ã€‚
- en: 'In our case instead, the objective is **inference**: we want to uncover heterogeneous
    treatment effects that are statistically different from each other. There is no
    value in generating different treatment effects if they are statistically indistinguishable.
    Moreover (but strongly related), when building the tree and generating the data
    partitions, we have to take into account that, since we use honest trees, we will
    use different data to estimate the within-leaf treatment effects.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œç›®æ ‡æ˜¯**æ¨æ–­**ï¼šæˆ‘ä»¬å¸Œæœ›æ­ç¤ºåœ¨ç»Ÿè®¡ä¸Šä¸åŒçš„å¼‚è´¨æ€§å¤„ç†æ•ˆåº”ã€‚å¦‚æœå¤„ç†æ•ˆåº”åœ¨ç»Ÿè®¡ä¸Šä¸å¯åŒºåˆ†ï¼Œé‚£ä¹ˆç”Ÿæˆä¸åŒçš„å¤„ç†æ•ˆåº”å°±æ²¡æœ‰æ„ä¹‰ã€‚æ­¤å¤–ï¼ˆä½†ä¸ä¹‹ç´§å¯†ç›¸å…³ï¼‰ï¼Œåœ¨æ„å»ºæ ‘å’Œç”Ÿæˆæ•°æ®åˆ†åŒºæ—¶ï¼Œæˆ‘ä»¬å¿…é¡»è€ƒè™‘ï¼Œç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯è¯šå®æ ‘ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸åŒçš„æ•°æ®æ¥ä¼°è®¡å¶å†…å¤„ç†æ•ˆåº”ã€‚
- en: '[Athey and Imbens (2016)](https://www.pnas.org/doi/abs/10.1073/pnas.1510489113)
    use a modified version of the [Mean Squared Error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error)
    as a splitting criterion, the **Expanded Mean Squared Error (EMSE)**:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[Atheyå’ŒImbens (2016)](https://www.pnas.org/doi/abs/10.1073/pnas.1510489113)ä½¿ç”¨äº†[å‡æ–¹è¯¯å·®
    (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error)çš„ä¿®æ”¹ç‰ˆæœ¬ä½œä¸ºåˆ†è£‚æ ‡å‡†ï¼Œå³**æ‰©å±•å‡æ–¹è¯¯å·®
    (EMSE)**ï¼š'
- en: '![](../Images/8609eaa4c5d55a4b510cd46719c8ba2a.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8609eaa4c5d55a4b510cd46719c8ba2a.png)'
- en: Expanded Root Mean Squared Error, image by Author
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©å±•å‡æ–¹æ ¹è¯¯å·®ï¼Œä½œè€…æä¾›çš„å›¾ç‰‡
- en: where *Î¼* is the estimated conditional expectation *Î¼*(*X*) *=* ğ”¼ [*Y** | *X*]
    and the difference with respect to the MSE is the additional term *Yáµ¢Â²*, the squared
    outcome variable. In our setting, we can rewrite it as
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­*Î¼*æ˜¯ä¼°è®¡çš„æ¡ä»¶æœŸæœ›*Î¼*(*X*) *=* ğ”¼ [*Y* | *X*]ï¼Œä¸MSEçš„å·®å¼‚æ˜¯é¢å¤–çš„é¡¹*Yáµ¢Â²*ï¼Œå³å¹³æ–¹çš„ç»“æœå˜é‡ã€‚åœ¨æˆ‘ä»¬çš„è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶é‡å†™ä¸º
- en: '![](../Images/f948a88295633b7ec28416436a817605.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f948a88295633b7ec28416436a817605.png)'
- en: Expanded Root Mean Squared Error for causal trees, image by Author
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©å±•å‡æ–¹æ ¹è¯¯å·®ç”¨äºå› æœæ ‘ï¼Œä½œè€…æä¾›çš„å›¾ç‰‡
- en: Why is this a sensible error loss? Because we can rewrite it as the expected
    variance of the conditional treatment effects, minus the squared expected value.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¿™æ˜¯ä¸€ä¸ªåˆç†çš„è¯¯å·®æŸå¤±ï¼Ÿå› ä¸ºæˆ‘ä»¬å¯ä»¥å°†å…¶é‡å†™ä¸ºæ¡ä»¶å¤„ç†æ•ˆåº”çš„æœŸæœ›æ–¹å·®å‡å»å¹³æ–¹æœŸæœ›å€¼ã€‚
- en: '![](../Images/da70d2e82e72ba2f85787338a85176f8.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da70d2e82e72ba2f85787338a85176f8.png)'
- en: Expanded Root Mean Squared Error for causal trees, image by Author
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©å±•å‡æ–¹æ ¹è¯¯å·®ç”¨äºå› æœæ ‘ï¼Œä½œè€…æä¾›çš„å›¾ç‰‡
- en: This formulation of the EMSE makes clear that the objective is to **minimize
    the within-leaf variance** of the estimated conditional conditional treatment
    effects *Ï„*(*X*) (the first term). In other words, small leaves are automatically
    penalized. The second term is just a normalizing factor. Note that both terms
    are unknown and have to be estimated from the training data, used to generate
    the tree.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§EMSEçš„å…¬å¼æ˜ç¡®äº†ç›®æ ‡æ˜¯**æœ€å°åŒ–ä¼°è®¡æ¡ä»¶å¤„ç†æ•ˆåº”*Ï„*(*X*)çš„å¶å†…æ–¹å·®**ï¼ˆç¬¬ä¸€ä¸ªé¡¹ï¼‰ã€‚æ¢å¥è¯è¯´ï¼Œå°çš„å¶å­ä¼šè¢«è‡ªåŠ¨æƒ©ç½šã€‚ç¬¬äºŒä¸ªé¡¹åªæ˜¯ä¸€ä¸ªå½’ä¸€åŒ–å› å­ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸¤ä¸ªé¡¹éƒ½æ˜¯æœªçŸ¥çš„ï¼Œå¿…é¡»ä»è®­ç»ƒæ•°æ®ä¸­ä¼°è®¡ï¼Œç”¨äºç”Ÿæˆæ ‘ã€‚
- en: '**Implementation**'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å®ç°**'
- en: Luckily, there are multiple libraries for causal trees. We import `CausalForestDML`
    from Microsoft's [EconML](https://econml.azurewebsites.net/) library, one of the
    best libraries for causal inference.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œæœ‰å¤šä¸ªå› æœæ ‘åº“å¯ä¾›é€‰æ‹©ã€‚æˆ‘ä»¬ä»å¾®è½¯çš„[EconML](https://econml.azurewebsites.net/)åº“ä¸­å¯¼å…¥`CausalForestDML`ï¼Œè¿™æ˜¯æœ€å¥½çš„å› æœæ¨æ–­åº“ä¹‹ä¸€ã€‚
- en: We have restricted the number of estimators to 1 to have a single tree instead
    of multiple ones, the so-called [**random forests**](https://en.wikipedia.org/wiki/Random_forest)
    that we will cover in a separate article.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä¼°è®¡å™¨çš„æ•°é‡é™åˆ¶ä¸º1ï¼Œä»¥ä¾¿å¾—åˆ°ä¸€æ£µæ ‘ï¼Œè€Œä¸æ˜¯å¤šæ£µæ ‘ï¼Œå³æ‰€è°“çš„[**éšæœºæ£®æ—**](https://en.wikipedia.org/wiki/Random_forest)ï¼Œæˆ‘ä»¬å°†åœ¨å¦ä¸€ç¯‡æ–‡ç« ä¸­ä»‹ç»ã€‚
- en: '![](../Images/f2df52c573a7a93e249b1a45ceabaeac.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2df52c573a7a93e249b1a45ceabaeac.png)'
- en: Estimated causal tree, image by Author
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼°è®¡çš„å› æœæ ‘ï¼Œä½œè€…æä¾›çš„å›¾ç‰‡
- en: As we can see, the tree representation looks extremely similar to the one we
    got before using the `DecisionTreeRegressor` function. However, now the model
    not only reports estimates of the conditional average treatment effects but also
    the standard errors of the estimates (at the bottom). How were they computed?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæ ‘å½¢è¡¨ç¤ºä¸ä¹‹å‰ä½¿ç”¨`DecisionTreeRegressor`å‡½æ•°å¾—åˆ°çš„ç»“æœéå¸¸ç›¸ä¼¼ã€‚ç„¶è€Œï¼Œç°åœ¨æ¨¡å‹ä¸ä»…æŠ¥å‘Šæ¡ä»¶å¹³å‡å¤„ç†æ•ˆåº”çš„ä¼°è®¡å€¼ï¼Œè¿˜æœ‰è¿™äº›ä¼°è®¡å€¼çš„æ ‡å‡†è¯¯å·®ï¼ˆåœ¨åº•éƒ¨ï¼‰ã€‚è¿™äº›æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿ
- en: Inference
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨æ–­
- en: 'Honest trees, besides improving the out-of-sample prediction accuracy of the
    model, have another great implication: they allow us to **compute standard errors
    as if the tree structure was exogenous**. In fact, since the data used to compute
    the predictions is independent of the data used to build the tree (split the data),
    we can just treat the tree structure as independent from the estimated treatment
    effects. As a consequence, we can estimate the standard errors of the estimates
    as standard errors of difference between sample averages, as in a standard AB
    test.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¯šå®æ ‘é™¤äº†æé«˜æ¨¡å‹çš„æ ·æœ¬å¤–é¢„æµ‹å‡†ç¡®æ€§å¤–ï¼Œè¿˜æœ‰å¦ä¸€ä¸ªé‡è¦æ„ä¹‰ï¼šå®ƒä»¬å…è®¸æˆ‘ä»¬**åƒæ ‘ç»“æ„æ˜¯å¤–ç”Ÿçš„ä¸€æ ·è®¡ç®—æ ‡å‡†è¯¯å·®**ã€‚å®é™…ä¸Šï¼Œç”±äºç”¨äºè®¡ç®—é¢„æµ‹çš„æ•°æ®ä¸ç”¨äºæ„å»ºæ ‘çš„æ•°æ®ï¼ˆåˆ†å‰²æ•°æ®ï¼‰æ˜¯ç‹¬ç«‹çš„ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ ‘ç»“æ„è§†ä¸ºä¸ä¼°è®¡çš„å¤„ç†æ•ˆåº”ç‹¬ç«‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¼°è®¡çš„æ ‡å‡†è¯¯å·®è§†ä¸ºæ ·æœ¬å‡å€¼å·®å¼‚çš„æ ‡å‡†è¯¯å·®ï¼Œå°±åƒæ ‡å‡†çš„ABæµ‹è¯•ä¸€æ ·ã€‚
- en: If we had used the same data to build the tree and estimate the treatment effects,
    we would have introduced **bias**, because of the spurious correlation between
    the covariates and the outcomes. This bias usually disappears for very large sample
    sizes, but honest trees do not require that.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„æ•°æ®æ¥æ„å»ºæ ‘å¹¶ä¼°è®¡å¤„ç†æ•ˆåº”ï¼Œæˆ‘ä»¬å°†å¼•å…¥**åå·®**ï¼Œç”±äºåå˜é‡ä¸ç»“æœä¹‹é—´çš„è™šå‡ç›¸å…³æ€§ã€‚è¿™ä¸ªåå·®é€šå¸¸åœ¨éå¸¸å¤§çš„æ ·æœ¬é‡ä¸­æ¶ˆå¤±ï¼Œä½†è¯šå®æ ‘å¹¶ä¸éœ€è¦è¿™æ ·ã€‚
- en: Performance
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ€§èƒ½
- en: 'How well does the model perform? Since we control the data-generating process,
    we can do something that is not possible with real data: check the predicted treatment
    effects against the true ones. The `add_treatment_effect()` function gives us
    the â€œtrueâ€ treatment effects for each observation in the data.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¡¨ç°å¦‚ä½•ï¼Ÿç”±äºæˆ‘ä»¬æ§åˆ¶äº†æ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥åšä¸€äº›çœŸå®æ•°æ®æ— æ³•åšåˆ°çš„äº‹æƒ…ï¼šå°†é¢„æµ‹çš„å¤„ç†æ•ˆåº”ä¸çœŸå®å€¼è¿›è¡Œæ¯”è¾ƒã€‚`add_treatment_effect()`å‡½æ•°ä¸ºæ•°æ®ä¸­çš„æ¯ä¸ªè§‚æµ‹å€¼æä¾›äº†â€œçœŸå®â€å¤„ç†æ•ˆåº”ã€‚
- en: We can now check how well the causal tree is able to estimate the individual
    treatment effects. Letâ€™s start with the categorical variables. I plot the true
    and estimated average treatment effect conditional on each value of `device`,
    `browser` and `region`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥æ£€æŸ¥å› æœæ ‘åœ¨ä¼°è®¡ä¸ªä½“å¤„ç†æ•ˆåº”æ–¹é¢çš„èƒ½åŠ›ã€‚è®©æˆ‘ä»¬ä»åˆ†ç±»å˜é‡å¼€å§‹ã€‚æˆ‘ç»˜åˆ¶äº†åŸºäº`device`ã€`browser`å’Œ`region`æ¯ä¸ªå€¼çš„çœŸå®å’Œä¼°è®¡çš„å¹³å‡å¤„ç†æ•ˆåº”ã€‚
- en: '![](../Images/f3e6cce7dd1271dcdfb076dacf145acb.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3e6cce7dd1271dcdfb076dacf145acb.png)'
- en: True and estimated treatment effects for each categorical value, image by Author
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªåˆ†ç±»å€¼çš„çœŸå®å’Œä¼°è®¡å¤„ç†æ•ˆåº”ï¼Œå›¾ç‰‡ç”±ä½œè€…æä¾›
- en: The causal tree is pretty good at detecting the heterogeneous treatment effects
    for the categorical variables. It overestimates the effect of mobile devices and
    Safari browser but does generally a good job.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å› æœæ ‘åœ¨æ£€æµ‹åˆ†ç±»å˜é‡çš„å¼‚è´¨å¤„ç†æ•ˆåº”æ–¹é¢è¡¨ç°ç›¸å½“å¥½ã€‚å®ƒé«˜ä¼°äº†ç§»åŠ¨è®¾å¤‡å’ŒSafariæµè§ˆå™¨çš„æ•ˆåº”ï¼Œä½†æ€»ä½“ä¸Šè¡¨ç°ä¸é”™ã€‚
- en: 'However, this is also where we expect a tree model to perform particularly
    well: where the effects are **discrete**. How well does it do on our continuous
    variable, time? First, letâ€™s again isolate the predicted treatment effects on
    `time` and ignore the other covariates.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬æœŸæœ›æ ‘æ¨¡å‹è¡¨ç°ç‰¹åˆ«å¥½çš„åœ°æ–¹ï¼šåœ¨**ç¦»æ•£**çš„æ•ˆåº”ä¸Šã€‚å®ƒåœ¨æˆ‘ä»¬çš„è¿ç»­å˜é‡æ—¶é—´ä¸Šçš„è¡¨ç°å¦‚ä½•ï¼Ÿé¦–å…ˆï¼Œè®©æˆ‘ä»¬å†æ¬¡éš”ç¦»`time`ä¸Šçš„é¢„æµ‹å¤„ç†æ•ˆåº”ï¼Œå¹¶å¿½ç•¥å…¶ä»–åå˜é‡ã€‚
- en: We now plot the predicted treatment effects against the true ones, along the
    `time` dimension.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å°†é¢„æµ‹çš„å¤„ç†æ•ˆåº”ä¸çœŸå®å€¼æ²¿`time`ç»´åº¦è¿›è¡Œç»˜å›¾ã€‚
- en: '![](../Images/1eced6305e633e3542f0a586b28d149f.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1eced6305e633e3542f0a586b28d149f.png)'
- en: True and estimated treatment effects along the time dimension, image by Author
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¿æ—¶é—´ç»´åº¦çš„çœŸå®å’Œä¼°è®¡å¤„ç†æ•ˆåº”ï¼Œå›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'From the plot, we can appreciate the discrete nature of causal trees: the model
    is only able to split the continuous variable into 4bins. These bins are close
    to the true treatment effects, but they fail to capture a big chunk of the treatment
    effect heterogeneity.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ¬£èµåˆ°å› æœæ ‘çš„ç¦»æ•£ç‰¹æ€§ï¼šæ¨¡å‹åªèƒ½å°†è¿ç»­å˜é‡åˆ†å‰²æˆ4ä¸ªåŒºé—´ã€‚è¿™äº›åŒºé—´æ¥è¿‘çœŸå®çš„å¤„ç†æ•ˆåº”ï¼Œä½†æœªèƒ½æ•æ‰åˆ°å¤„ç†æ•ˆåº”å¼‚è´¨æ€§çš„è¾ƒå¤§éƒ¨åˆ†ã€‚
- en: Can these predictions be improved? The answer is yes, and we will explore how
    in the next post.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›é¢„æµ‹èƒ½å¾—åˆ°æ”¹è¿›å—ï¼Ÿç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­æ¢è®¨å¦‚ä½•æ”¹è¿›ã€‚
- en: Conclusion
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we have seen how to use causal trees to estimate **heterogeneous
    treatment effects**. The main insight comes from the definition of an auxiliary
    outcome variable that allows us to frame the inference problem as a prediction
    problem. While we can then use any algorithm to predict treatment effects, regression
    trees are particularly useful because of their interpretability, prediction accuracy,
    and feature of generating prediction as subsample averages.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨å› æœæ ‘æ¥ä¼°è®¡**å¼‚è´¨æ€§å¤„ç†æ•ˆåº”**ã€‚ä¸»è¦çš„æ´è§æ¥è‡ªäºè¾…åŠ©ç»“æœå˜é‡çš„å®šä¹‰ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†æ¨æ–­é—®é¢˜æ¡†æ¶è®¾ä¸ºé¢„æµ‹é—®é¢˜ã€‚è™½ç„¶æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•ç®—æ³•æ¥é¢„æµ‹å¤„ç†æ•ˆåº”ï¼Œä½†å›å½’æ ‘ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§ã€é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¹¶ä¸”èƒ½å¤Ÿç”Ÿæˆä½œä¸ºå­æ ·æœ¬å¹³å‡å€¼çš„é¢„æµ‹ã€‚
- en: The work by [Athey and Imbens (2016)](https://www.pnas.org/doi/abs/10.1073/pnas.1510489113)
    on regression trees to compute heterogeneous treatment effects brought together
    two separate literatures, causal inference, and machine learning in a very fruitful
    **synergy**. The causal inference literature (re)discovered the inference benefits
    of sample splitting, which allows us to do correct inference even when the data
    partition is complex and hard to analyze. On the other hand, splitting the tree
    generation phase from the within-leaf prediction phase has strong benefits in
    terms of prediction accuracy, by safeguarding against overfitting.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[Athey å’Œ Imbens (2016)](https://www.pnas.org/doi/abs/10.1073/pnas.1510489113)
    å…³äºå›å½’æ ‘è®¡ç®—å¼‚è´¨æ€§å¤„ç†æ•ˆåº”çš„å·¥ä½œï¼Œå°†å› æœæ¨æ–­å’Œæœºå™¨å­¦ä¹ è¿™ä¸¤ä¸ªä¸åŒçš„æ–‡çŒ®ç»“åˆåœ¨äº†ä¸€èµ·ï¼Œå½¢æˆäº†éå¸¸æœ‰æˆæ•ˆçš„**ååŒæ•ˆåº”**ã€‚å› æœæ¨æ–­æ–‡çŒ®ï¼ˆé‡æ–°ï¼‰å‘ç°äº†æ ·æœ¬åˆ†å‰²çš„æ¨æ–­å¥½å¤„ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æ•°æ®åˆ†å‰²å¤æ‚ä¸”éš¾ä»¥åˆ†ææ—¶è¿›è¡Œæ­£ç¡®çš„æ¨æ–­ã€‚å¦ä¸€æ–¹é¢ï¼Œå°†æ ‘ç”Ÿæˆé˜¶æ®µä¸å¶å†…é¢„æµ‹é˜¶æ®µåˆ†å¼€ï¼Œæœ‰åŠ©äºæé«˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚'
- en: References
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: S. Athey, G. Imbens, [Recursive partitioning for heterogeneous causal effects](https://www.pnas.org/doi/abs/10.1073/pnas.1510489113)
    (2016), *PNAS*.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Athey, G. Imbens, [å¼‚è´¨å› æœæ•ˆåº”çš„é€’å½’åˆ†å‰²](https://www.pnas.org/doi/abs/10.1073/pnas.1510489113)
    (2016), *PNAS*ã€‚
- en: S. Wager, S. Athey, [Estimation and Inference of Heterogeneous Treatment Effects
    using Random Forests](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839)
    (2018), *Journal of the American Statistical Association*.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Wager, S. Athey, [ä½¿ç”¨éšæœºæ£®æ—çš„å¼‚è´¨æ€§å¤„ç†æ•ˆåº”çš„ä¼°è®¡å’Œæ¨æ–­](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839)
    (2018), *ç¾å›½ç»Ÿè®¡åä¼šæœŸåˆŠ*ã€‚
- en: S. Athey, J. Tibshirani, S. Wager, [Generalized Random Forests](https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full)
    (2019). *The Annals of Statistics*.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Athey, J. Tibshirani, S. Wager, [å¹¿ä¹‰éšæœºæ£®æ—](https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full)
    (2019). *ç»Ÿè®¡å¹´é‰´*ã€‚
- en: M. Oprescu, V. Syrgkanis, Z. Wu, [Orthogonal Random Forest for Causal Inference](http://proceedings.mlr.press/v97/oprescu19a.html?ref=https%3A%2F%2Fgithubhelp.com)
    (2019). *Proceedings of the 36th International Conference on Machine Learning*.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M. Oprescu, V. Syrgkanis, Z. Wu, [ç”¨äºå› æœæ¨æ–­çš„æ­£äº¤éšæœºæ£®æ—](http://proceedings.mlr.press/v97/oprescu19a.html?ref=https%3A%2F%2Fgithubhelp.com)
    (2019). *ç¬¬36å±Šå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®è®ºæ–‡é›†*ã€‚
- en: Related Articles
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›¸å…³æ–‡ç« 
- en: '[DAGs and Control Variables](/b63dc69e3d8c)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DAG å’Œæ§åˆ¶å˜é‡](/b63dc69e3d8c)'
- en: '[Matching, Weighting, or Regression?](/99bf5cffa0d9)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åŒ¹é…ã€åŠ æƒè¿˜æ˜¯å›å½’ï¼Ÿ](/99bf5cffa0d9)'
- en: '[Understanding Meta Learners](/8a9c1e340832)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ç†è§£å…ƒå­¦ä¹ è€…](/8a9c1e340832)'
- en: '[Understanding AIPW, the Doubly-Robust Estimator](/ed4097dab27a)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ç†è§£ AIPWï¼ŒåŒé‡ç¨³å¥ä¼°è®¡é‡](/ed4097dab27a)'
- en: Code
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»£ç 
- en: 'You can find the original Jupyter Notebook here:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹çš„ Jupyter Notebook å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š
- en: '[https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb)'
- en: Thank you for reading!
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼
- en: '*I really appreciate it!* ğŸ¤— *If you liked the post and would like to see more,
    consider* [***following me***](https://medium.com/@matteo.courthoud)*. I post
    once a week on topics related to causal inference and data analysis. I try to
    keep my posts simple but precise, always providing code, examples, and simulations.*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘éå¸¸æ„Ÿæ¿€ï¼* ğŸ¤— *å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« å¹¶å¸Œæœ›çœ‹åˆ°æ›´å¤šå†…å®¹ï¼Œè¯·è€ƒè™‘* [***å…³æ³¨æˆ‘***](https://medium.com/@matteo.courthoud)*ã€‚æˆ‘æ¯å‘¨å‘å¸ƒä¸€æ¬¡ä¸å› æœæ¨æ–­å’Œæ•°æ®åˆ†æç›¸å…³çš„è¯é¢˜ã€‚æˆ‘å°½é‡ä¿æŒå¸–å­ç®€å•ä½†ç²¾ç¡®ï¼Œå§‹ç»ˆæä¾›ä»£ç ã€ç¤ºä¾‹å’Œæ¨¡æ‹Ÿã€‚*'
- en: '*Also, a small* ***disclaimer****: I write to learn so mistakes are the norm,
    even though I try my best. Please, when you spot them, let me know. I also appreciate
    suggestions on new topics!*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ­¤å¤–ï¼Œä¸€ä¸ªå°å°çš„* ***å…è´£å£°æ˜****ï¼šæˆ‘å†™ä½œæ˜¯ä¸ºäº†å­¦ä¹ ï¼Œå› æ­¤é”™è¯¯æ˜¯å¸¸è§çš„ï¼Œå°½ç®¡æˆ‘å°½åŠ›è€Œä¸ºã€‚å¦‚æœä½ å‘ç°é”™è¯¯ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚æˆ‘ä¹Ÿæ¬¢è¿å¯¹æ–°è¯é¢˜çš„å»ºè®®ï¼*'
