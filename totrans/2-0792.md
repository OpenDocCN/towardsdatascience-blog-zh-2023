# 高效服务开源 LLM

> 原文：[https://towardsdatascience.com/efficiently-serving-open-source-llms-5f0bf5d8fd59](https://towardsdatascience.com/efficiently-serving-open-source-llms-5f0bf5d8fd59)

[](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)[![Ryan Shrott](../Images/186524066383b4b02c994692aebb3ea5.png)](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------) [Ryan Shrott](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)

·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------) ·阅读时长 5 分钟·2023 年 8 月 14 日

--

![](../Images/c16d4589aafa99416a26da8bff1b5afe.png)

图片来源：[Mariia Shalabaieva](https://unsplash.com/@maria_shalabaieva?utm_source=medium&utm_medium=referral) 于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

本文解释了我个人使用 6 种常见方法服务开源 LLM 的经验：AWS Sage Maker、Hugging Face、Together.AI、VLLM 和 Petals.ml。

## 挣扎…

你已经感受到了服务自己微调的开源 LLM 的痛苦、挣扎和荣耀，但你最终因为成本、推理时间、可靠性和技术挑战而决定回到 Open AI 或 Anthropic :( 你也放弃了租用 A100 GPU（许多供应商的 GPU 已经被预订到 2023 年底！）。你也没有 10 万美元去购买一个 2 级 A100 服务器。尽管如此，你仍在梦想，你真的希望开源能够为你的解决方案服务。也许你的公司不愿意将私人数据发送给 Open AI，或者你需要一个针对特定任务微调的模型？在本文中，我将概述并比较一些最有效的推理方法/平台，用于服务开源 LLM。在 2023 年，我将对 6 种方法进行比较，并解释何时应该使用其中一种或另一种。我亲自尝试了这 6 种方法，并将详细介绍我的个人经验：**AWS Sage Maker、Hugging Face 推理端点、Together.AI、VLLM 和 Petals.ml**。我没有所有的答案，但我会尽力详细说明我的经验。我与这些供应商没有任何经济联系，仅仅是分享我的经验以造福他人。请分享你的经验！

## 为什么选择开源？

开源模型有许多优点，包括控制、隐私和潜在的成本降低。例如，你可以针对特定的使用案例微调一个较小的开源模型，从而获得准确的结果和快速的推理时间。隐私控制意味着推理可以在自己的服务器上完成。另一方面，成本降低比你想象的要困难得多。OpenAI 拥有规模经济，定价具有竞争力。他们的 GPT-3.5 Turbo 定价模式很难与之竞争，并且已被证明类似于电力成本。不过，你仍然可以采用一些方法和技巧来节省开支，并用开源模型获得优秀的结果。例如，我的微调模型 [Stable Beluga 2](https://huggingface.co/stabilityai/StableBeluga2) 目前显著优于 GPT-3.5 Turbo，并且在我的应用中更便宜。因此，我强烈建议你尝试使用开源模型。

## Hugging Face 推理端点

这是服务开源 LLM 最常见且最简单的方法。只需点击几下即可，且几乎没有错误。毕竟，Hugging Face 最初是一家 NLP 公司。你的模型很可能已经存在于 Hugging Face 上，因此这是快速测试模型的首选选项。GPU 服务器成本往往较高。例如，如果你仅使用 RunPod.io 部署模型，你将有更多的提供商选择，并且成本更低。Hugging Face 已开源了他们的 Transformers 推理库，并提供了易于修改的 Docker 镜像。因此，如果你需要更多控制，可以选择在 RunPod 上的自定义解决方案。 [这是一个](https://www.youtube.com/watch?v=FdcXJ7d3WQU&t=224s&ab_channel=VenelinValkov)关于如何在 RunPod 上操作的教程。

## VLLM

[这个解决方案](https://github.com/vllm-project/vllm)由于其推理速度而非常有趣。他们声称比 Hugging Face 的 Transformers 快 24 倍！在我个人使用时，发现速度大约是 Hugging Face Transformers 的 10 倍。不过，我发现这里有一些小 bug。这个项目正在积极开发中，尚未完善。不过，我仍然强烈建议你试试这个解决方案。由于推理速度更快，相较于 HF Transformers，成本将显著降低。

![](../Images/f05be35a29639954eae27b280d6dc2af.png)

来源: [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)

## Petals.ml

[这个](https://github.com/bigscience-workshop/petals)是最有趣的解决方案。Petals.ml的开发者发现了一种在家运行LLMs的方法，类似于BitTorrent。这使得微调和推理的速度比卸载快最多10倍。实际上，这意味着模型的只有一小部分会加载到你自己的GPU上，其余部分会存在于GPU网络群中。换句话说，一个GPU网络将协作进行计算。这非常有趣，因为它在一定程度上使LLM的使用得到民主化，即任何人都可以运行大型LLM而不花一分钱！相关技术的论文可以[在这里找到](https://arxiv.org/abs/2209.01188)。我强烈建议你试试Petals.ml！

## Together.AI

[他们](https://together.ai/)提供了一个具有出色定价的开源模型API。你可以使用Together.AI计算集群对开源模型进行微调和部署。他们的定价是AWS的20%。他们的平台简单直观，容易上手。因此，我强烈推荐这个平台。他们的API价格大约是[GPT-3.5 turbo的1/10](https://together.ai/pricing)。这是我现在最喜欢的开源模型部署方式！

## AWS Sagemaker

部署ML模型的成熟方法。Sagemaker对初学者不太友好，而且与上述方法相比，它可能是最昂贵的。它也是最复杂的。然而，如果你的业务已经在使用AWS，这可能是你唯一的选择。此外，如果你像我一样在AWS上有免费的计算资源，为什么不尝试一下呢？这是AI Anytime的教程：[https://www.youtube.com/watch?v=A9Pu4xg-Nas&ab_channel=AIAnytime](https://www.youtube.com/watch?v=A9Pu4xg-Nas&ab_channel=AIAnytime)。

## 结论：

总结来说，我强烈建议尝试Together.AI和Petals.ml，因为使用这些平台有许多优势。如果你需要隐私和非常快的推理速度，我建议使用VLLM。如果你被迫使用AWS，那么选择SageMaker。如果你想要简单高效的方案（特别是用于测试），可以选择HF transformers端点。

# 📢 嗨！如果你觉得这篇文章有帮助，请考虑：

👏 鼓掌50次（这很有帮助！）

✏️ 留下评论

🌟 突出你觉得有见地的部分

👣 关注我

有任何问题吗？🤔 不要犹豫，尽管问。以这种方式支持我是对我的详细教程文章的一种免费而简单的感谢方式！😊

# 最终说明

如果你对仅使用Python开发全栈AI应用感兴趣，请随时[注册这里](https://saas.mixo.io/)。一如既往，请在下面留下你的经验和评论。我期待阅读所有评论。

就这些了，如果你读到这里，请在下面评论并在[LinkedIn](https://www.linkedin.com/in/ryanshrott/)上添加我。

我的Github在[这里](https://github.com/ryanshrott)。

**其他深度学习博客**

[安德鲁·吴的序列模型 — 11个经验教训](https://medium.com/towards-data-science/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b)

[安德鲁·吴的计算机视觉 — 11个经验教训](/computer-vision-by-andrew-ng-11-lessons-learned-7d05c18a6999)

[安德鲁·吴的深度学习专业化 — 21个经验教训](/deep-learning-specialization-by-andrew-ng-21-lessons-learned-15ffaaef627c)
