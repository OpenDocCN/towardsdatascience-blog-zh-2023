["```py\npython = \"^3.8\"\nDjango = \"^4.2.1\"\nlightgbm = \"^3.3.5\"\npandas = \"^2.0.1\"\ndjangorestframework = \"^3.14.0\"\npytrends = \"^4.9.2\"\ndrf-extensions = \"^0.7.1\"\n```", "```py\n# Clone the project.\ngit clone git@github.com:davide-burba/code-collection.git\n\n# Move to the right folder.\ncd code-collection/examples/api-example-django\n\n# Launch the app.\ndocker compose up -d\n\n# Apply the migrations.\ndocker compose exec django ./manage.py migrate\n\n# Interactively create a (super)user for your app.\ndocker compose exec django ./manage.py createsuperuser \n```", "```py\nclass TimeSeries(models.Model):\n    name = models.CharField(unique=True, max_length=64)\n    source = models.CharField(max_length=32, choices=DataSource.choices)\n\nclass TSVersion(models.Model):\n    timeseries = models.ForeignKey(TimeSeries, on_delete=CASCADE)\n    created_at = models.DateTimeField(auto_now_add=True)\n    expired = models.BooleanField(default=False)\n\nclass TSValue(models.Model):\n    version = models.ForeignKey(TSVersion, on_delete=CASCADE)\n    time = models.DateTimeField()\n    value = models.FloatField()\n\nclass DataSource(models.TextChoices):\n    GOOGLE_TRENDS = \"GOOGLE_TRENDS\"\n```", "```py\nclass DataConfig(models.Model):\n    name = models.CharField(unique=True, max_length=64)\n\nclass DataFeatures(models.Model):\n    config = models.ForeignKey(\n        DataConfig, on_delete=CASCADE, related_name=\"features\"\n    )\n    timeseries = models.ForeignKey(TimeSeries, on_delete=PROTECT)\n\nclass DataTargets(models.Model):\n    config = models.ForeignKey(\n        DataConfig, on_delete=CASCADE, related_name=\"targets\"\n    )\n    timeseries = models.ForeignKey(TimeSeries, on_delete=PROTECT)\n```", "```py\nclass PreprocessingConfig(models.Model):\n    name = models.CharField(unique=True, max_length=64)\n    params = models.JSONField()\n\nclass MLConfig(models.Model):\n    params = models.JSONField()\n```", "```py\nclass MLModel(models.Model):\n    name = models.CharField(unique=True, max_length=64)\n    ml_config = models.ForeignKey(MLConfig, on_delete=PROTECT)\n    data_config = models.ForeignKey(DataConfig, on_delete=PROTECT)\n    preprocess_config = models.ForeignKey(\n        PreprocessingConfig, on_delete=PROTECT\n    )\n\nclass MLModelVersion(models.Model):\n    ml_model = models.ForeignKey(MLModel, on_delete=CASCADE)\n    ml_file = models.FileField(upload_to=\"ml_models\")\n    created_at = models.DateTimeField(auto_now_add=True)\n    metadata = models.JSONField()\n```", "```py\nimport datetime as dt\nfrom abc import ABC, abstractmethod\n\nimport pandas as pd\nfrom gtrends.models import TimeSeries\nfrom pytrends.request import TrendReq\n\ndef download_data(timeseries: TimeSeries) -> pd.DataFrame:\n    return DATASOURCE_MAP[timeseries.source](timeseries).download()\n\nclass DataSource(ABC):\n    def __init__(self, timeseries: TimeSeries):\n        self.timeseries = timeseries\n\n    @abstractmethod\n    def download(self) -> pd.DataFrame:\n        \"\"\"Returns a dataframe with time as index and value as column.\"\"\"\n\nclass GTrendSource(DataSource):\n    START_DATE = \"2022-01-01\"\n\n    def download(self) -> pd.DataFrame:\n        # Download data.\n        name = self.timeseries.name\n        data = self.download_interest_over_time(name)\n        # Format new data.\n        data = (\n            data[~data.isPartial]\n            .reset_index()\n            .rename(columns={name: \"value\", \"date\": \"time\"})\n            .set_index(\"time\")\n        )[[\"value\"]]\n        data[\"value\"] = data[\"value\"].astype(float)\n        return data\n\n    @classmethod\n    def download_interest_over_time(cls, search_term: str) -> pd.DataFrame:\n        \"\"\"Download Google Trends data.\"\"\"\n        pytrends = TrendReq()\n        timeframe = (\n            cls.START_DATE + \" \" + dt.datetime.now().strftime(\"%Y-%m-%d\")\n        )\n        pytrends.build_payload([search_term], timeframe=timeframe)\n        return pytrends.interest_over_time()\n\nDATASOURCE_MAP = {\n    \"GOOGLE_TRENDS\": GTrendSource,\n}\n```", "```py\nclass BasePreprocessor(ABC):\n    @abstractmethod\n    def build_x_y(self, data: Dict) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"Return features and target ready for training.\"\"\"\n\n    @abstractmethod\n    def build_x_latest(self, data: Dict) -> pd.DataFrame:\n        \"\"\"Return only latest values of features, useful for inference\"\"\"\n```", "```py\ndef _build_lags(\n    df: pd.DataFrame, column: str, lags: List[int], prefix: str\n) -> pd.DataFrame:\n    return pd.concat(\n        [\n            df[[column]]\n            .shift(lag)\n            .rename(columns={column: f\"{prefix}_lag_{lag}\"})\n            for lag in lags\n        ],\n        axis=1,\n    )\n```", "```py\n@dataclass\nclass Preprocessor(BasePreprocessor):\n    horizon: int\n    target_lags: List[int]\n    feature_lags: List[int]\n```", "```py\ndef _build_x_lags_targets(\n        self, target_data: Dict\n    ) -> Optional[pd.DataFrame]:\n        if not self.target_lags:\n            return None\n\n        x = []\n        for df in target_data.values():\n            x.append(\n                _build_lags(\n                    df=df,\n                    column=\"value\",\n                    lags=self.target_lags,\n                    prefix=\"target\",\n                )\n            )\n        return pd.concat(x, axis=1)\n\ndef _build_x_lags_features(\n        self, feature_data: Dict, target_data: Dict\n    ) -> Optional[pd.DataFrame]:\n        if not self.feature_lags:\n            return None\n\n        x = []\n        for name, df in feature_data.items():\n            x.append(\n                _build_lags(\n                    df=df,\n                    column=\"value\",\n                    lags=self.feature_lags,\n                    prefix=name,\n                )\n            )\n\n        # Concat features on axis 1.\n        x = pd.concat(\n            [df.reset_index().drop(columns=[\"ts_name\"]) for df in x], axis=1\n        )\n\n        # Use target to \"reindex\" on axis 0.\n        for_reindex = pd.concat(target_data.values(), axis=1).reset_index()\n        x = pd.merge(for_reindex, x, how=\"left\", on=\"time\")\n\n        return x.drop(columns=[\"value\"]).set_index([\"time\", \"ts_name\"])\n```", "```py\ndef build_x(self, data: Dict) -> pd.DataFrame:\n    target_data = data[\"targets\"]\n    feature_data = data[\"features\"]\n\n    # Build x_target and x_features.\n    x_targ = self._build_x_lags_targets(target_data)\n    x_feat = self._build_x_lags_features(feature_data, target_data)\n    # Combine x_target and x_features.\n    if x_feat is None and x_targ is None:\n        raise ValueError(\"Cannot have no target lags and no feature lags.\")\n    elif x_feat is None:\n        return x_targ\n    elif x_targ is None:\n        return x_feat\n    return pd.merge(\n        x_targ, x_feat, left_index=True, right_index=True, how=\"left\"\n    )\n```", "```py\ndef build_y(self, target_data: Dict) -> pd.DataFrame:\n    y = {}\n    for name, df in target_data.items():\n        y[name] = (\n            df[\"value\"]\n            .shift(-self.horizon)\n            .rename(f\"horizon_{self.horizon}\")\n        )\n    return pd.concat(y.values())\n```", "```py\ndef build_x_y(self, data: Dict) -> Tuple[pd.DataFrame, pd.Series]:\n    # Build x and y.\n    x = self.build_x(data)\n    y = self.build_y(data[\"targets\"])\n    # Align x indexes with y indexes.\n    x = pd.merge(y, x, left_index=True, right_index=True, how=\"left\")\n    x = x.iloc[:, 1:]\n    # Drop missing values generated by lags/horizon.\n    idx = ~(x.isnull().any(axis=1) | y.isnull())\n    x = x.loc[idx]\n    y = y.loc[idx]\n    return x, y\n\ndef build_x_latest(self, data: Dict) -> pd.DataFrame:\n    x = self.build_x(data)\n    return x[x.index == x.index.max()]\n```", "```py\nimport lightgbm\n\ndef save_engine(model, path):\n    model.booster_.save_model(path)\n\ndef load_engine(path):\n    return lightgbm.Booster(model_file=path)\n```", "```py\nfrom typing import Tuple\nimport pandas as pd\nfrom gtrends.services.data_sources import download_data\nfrom gtrends.models import TimeSeries, TSValue, TSVersion\n\ndef update_timeseries(timeseries: TimeSeries) -> Tuple[bool, int]:\n    \"\"\"Update timeseries values.\n\n    Either add the new values (if past values are the same) to the latest\n    version, or create a new version.\n\n    Args:\n        timeseries: A timeseries object.\n\n    Returns:\n        A pair with:\n            - bool: True if it created a new version, False otherwise\n            - int: The number of new values added.\n    \"\"\"\n    new_data = download_data(timeseries)\n\n    # Assign version.\n    new_version = True\n    versions = timeseries.tsversion_set.order_by(\"created_at\")\n    if versions:\n        version = versions.last()\n        old_data = _build_old_data(version)\n\n        if _is_old_data_in_new_data(old_data, new_data):\n            # If old values match, just keep the new values.\n            new_version = False\n            new_data = new_data.loc[~new_data.index.isin(old_data.index)]\n        else:\n            # Else, set the old version to expired.\n            version.expired = True\n            version.save()\n\n    if new_version:\n        version = TSVersion(timeseries=timeseries)\n        version.save()\n\n    # Store new data.\n    objs = [\n        TSValue(version=version, time=d[0], value=d[1].value)\n        for d in new_data.iterrows()\n    ]\n    TSValue.objects.bulk_create(objs)\n    return new_version, len(objs)\n\ndef _build_old_data(version: TSVersion) -> pd.DataFrame:\n    old_data = pd.DataFrame(\n        version.tsvalue_set.values(\"time\", \"value\")\n    ).set_index(\"time\")\n    old_data.index = old_data.index.tz_localize(None)\n    return old_data\n\ndef _is_old_data_in_new_data(\n    old_data: pd.DataFrame, new_data: pd.DataFrame\n) -> bool:\n    if old_data.index.isin(new_data.index).all() and new_data.loc[\n        old_data.index\n    ].equals(old_data):\n        return True\n    return False\n```", "```py\ndef update_all_timeseries():\n    \"\"\"Update all time-series values.\"\"\"\n    for ts in TimeSeries.object.all():\n        update_timeseries(ts)\n```", "```py\nfrom typing import Dict, List\nimport pandas as pd\nfrom gtrends.models import TimeSeries\n\ndef load_data(\n    target_ts: List[TimeSeries], feature_ts: List[TimeSeries]\n) -> Dict[str, Dict[str, pd.DataFrame]]:\n    targ_feat = {\n        \"targets\": target_ts,\n        \"features\": feature_ts,\n    }\n    data = {\"targets\": {}, \"features\": {}}\n    metadata = {\"targets\": {}, \"features\": {}}\n    for key, items in targ_feat.items():\n        for item in items:\n            ts = item.timeseries\n            version = ts.tsversion_set.last()\n            values = version.tsvalue_set.all().values(\"time\", \"value\")\n\n            df = pd.DataFrame(values)\n            df[\"ts_name\"] = ts.name\n            df = df.set_index([\"time\", \"ts_name\"])\n\n            data[key][ts.name] = df\n            metadata[key][ts.name] = version.id\n\n    return data, metadata\n```", "```py\ndef preprocess(data: Dict, prep_params: Dict) -> Tuple[pd.DataFrame, pd.Series]:\n    return Preprocessor(**prep_params).build_x_y(data)\n\ndef build_x_latest(data: Dict, prep_params: Dict) -> pd.DataFrame:\n    return Preprocessor(**prep_params).build_x_latest(data)\n```", "```py\nfrom lightgbm import LGBMRegressor\n\ndef train(x, y, model_params):\n    model = LGBMRegressor(**model_params)\n    model.fit(x, y)\n    return model\n```", "```py\nimport os\nfrom typing import Dict\nfrom uuid import uuid4\n\nfrom django.core.files import File\nfrom gtrends import models\nfrom gtrends.ml import save_engine\nfrom lightgbm import LGBMRegressor\n\ndef save_mlmodelversion(\n    engine: LGBMRegressor, ml_model: models.MLModel, metadata: Dict\n) -> models.MLModelVersion:\n    filename = str(uuid4()).replace(\"-\", \"\")[:10] + \".txt\"\n    tmp_path = \"/tmp/\" + filename\n    save_engine(engine, tmp_path)\n\n    with open(tmp_path, \"r\") as f:\n        ml_model_file = File(f, filename)\n        ml_model_version = models.MLModelVersion(\n            ml_model=ml_model,\n            ml_file=ml_model_file,\n            metadata=metadata,\n        )\n        ml_model_version.save()\n\n    os.remove(tmp_path)\n    return ml_model_version\n```", "```py\nfrom gtrends.models import MLModel, MLModelVersion\nfrom gtrends.services.tasks import (\n    load_data,\n    preprocess,\n    save_mlmodelversion,\n    train,\n)\n\ndef train_pipeline(ml_model: MLModel) -> MLModelVersion:\n    target_ts = ml_model.data_config.targets.all()\n    feature_ts = ml_model.data_config.features.all()\n    prep_params = ml_model.preprocess_config.params\n    model_params = ml_model.ml_config.params\n\n    data, metadata = load_data(target_ts, feature_ts)\n    x, y = preprocess(data, prep_params)\n    engine = train(x, y, model_params)\n    ml_model_version = save_mlmodelversion(engine, ml_model, metadata)\n\n    return ml_model_version\n```", "```py\nfrom typing import Dict\nfrom gtrends.models import MLModel\nfrom gtrends.services.ml import load_engine\nfrom gtrends.services.tasks import build_x_latest, load_data\n\ndef inference_pipeline(ml_model: MLModel) -> Dict:\n    if ml_model.mlmodelversion_set.count() == 0:\n        raise ValueError(\"No model has been trained yet!\")\n\n    target_ts = ml_model.data_config.targets.all()\n    feature_ts = ml_model.data_config.features.all()\n    prep_params = ml_model.preprocess_config.params\n\n    data, _ = load_data(target_ts, feature_ts)\n    x = build_x_latest(data, prep_params)\n    engine = load_engine(ml_model.mlmodelversion_set.last().ml_file.path)\n    y_pred = engine.predict(x)\n\n    # Format predictions.\n    horizon = ml_model.preprocess_config.params[\"horizon\"]\n    preds = {\n        name: {\"last_date\": time, \"prediction\": pred, \"horizon\": horizon}\n        for time, name, pred in zip(\n            x.index.get_level_values(\"time\"),\n            x.index.get_level_values(\"ts_name\"),\n            y_pred,\n        )\n    }\n    for k, v in preds.items():\n        last_value = data[\"targets\"][k].loc[v[\"last_date\"]][\"value\"].item()\n        preds[k][\"last_value\"] = last_value\n        preds[k][\"predicted_delta\"] = v[\"prediction\"] - last_value\n\n    return preds\n```", "```py\nclass TSVersionSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = models.TSVersion\n        fields = \"__all__\"\n```", "```py\nclass TimeSeriesSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = models.TimeSeries\n        fields = \"__all__\"\n\n    def create(self, validated_data):\n        ts = super().create(validated_data)\n        # Download data for the newly created timeseries.\n        update_timeseries(ts)\n        return ts\n```", "```py\nclass DataConfigSerializer(serializers.ModelSerializer):\n    features = DataFeaturesSerializer(many=True)\n    targets = DataTargetsSerializer(many=True, allow_null=False)\n\n    class Meta:\n        model = models.DataConfig\n        fields = (\"id\", \"name\", \"features\", \"targets\")\n\n    def validate_targets(self, value):\n        if len(value) == 0:\n            raise serializers.ValidationError(\"Must have at least one target.\")\n        return value\n\n    def create(self, validated_data):\n        config = models.DataConfig.objects.create(**validated_data)\n\n        features_data = validated_data.pop(\"features\")\n        targets_data = validated_data.pop(\"targets\")\n        for feature_data in features_data:\n            models.DataFeatures.objects.create(config=config, **feature_data)\n        for target_data in targets_data:\n            models.DataTargets.objects.create(config=config, **target_data)\n\n        return config\n```", "```py\nclass PreprocessingConfigSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = models.PreprocessingConfig\n        fields = \"__all__\"\n\n    def validate_params(self, value):\n        expected_keys = {\"horizon\", \"target_lags\", \"feature_lags\"}\n        if set(value.keys()) != expected_keys:\n            raise serializers.ValidationError(f\"Expected keys: {expected_keys}\")\n\n        if not isinstance(value[\"horizon\"], int):\n            raise serializers.ValidationError(\"Horizon must be an int.\")\n\n        for key in [\"target_lags\", \"feature_lags\"]:\n            if not isinstance(value[key], list) or not all(\n                isinstance(lag, int) for lag in value[key]\n            ):\n                raise serializers.ValidationError(f\"{key} must be a list[int].\")\n\n        return value\n```", "```py\nclass MLConfigSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = models.MLConfig\n        fields = \"__all__\"\n\n    def validate_params(self, value):\n        # At the moment we only support LightGBM model.\n        valid_params = LGBMRegressor().get_params()\n        invalid_params = [k for k in value if k not in valid_params]\n        if invalid_params:\n            raise serializers.ValidationError(f\"Invalid: {invalid_params}\")\n        return value\n```", "```py\nclass TimeSeriesViewSet(viewsets.ModelViewSet):\n    queryset = models.TimeSeries.objects.prefetch_related(\"tsversion_set\").all()\n    serializer_class = serializers.TimeSeriesSerializer\n    http_method_names = [\"get\", \"post\", \"head\", \"delete\"]\n\n    @action(detail=True, url_path=\"latest-values\")\n    def latest_values(self, request, pk):\n        \"\"\"Get timeseries values for the last version.\"\"\"\n        versions = self.queryset.get(pk=pk).tsversion_set.order_by(\"created_at\")\n        if not versions:\n            return Response([])\n        return Response(versions.first().tsvalue_set.values(\"time\", \"value\"))\n\n    @action(detail=True, url_path=\"update-values\")\n    @transaction.atomic\n    def update_values(self, request, pk):\n        \"\"\"Update values for one timeseries.\"\"\"\n        timeseries = self.queryset.get(pk=pk)\n        try:\n            new_version, how_many = update_timeseries(timeseries)\n        except ResponseError:\n            return Response(\n                \"Data download failed!\", status=status.HTTP_400_BAD_REQUEST\n            )\n        msg = f\"Data updated, {how_many} new values. \"\n        if new_version:\n            msg += \" New version created.\"\n        else:\n            msg += \" Updated latest version.\"\n        return Response(msg)\n\n    @action(detail=False, url_path=\"update-all-values\")\n    @transaction.atomic\n    def update_all_values(self, request):\n        \"\"\"Update values for all timeseries.\"\"\"\n        try:\n            for timeseries in self.queryset:\n                update_timeseries(timeseries)\n        except ResponseError:\n            return Response(\n                \"Data download failed!\", status=status.HTTP_400_BAD_REQUEST\n            )\n        msg = f\"Data updated, {len(self.queryset)} timeseries updated.\"\n        return Response(msg)\n```", "```py\nclass TSVersionViewSet(viewsets.ModelViewSet):\n    queryset = models.TSVersion.objects.prefetch_related(\"tsvalue_set\").all()\n    serializer_class = serializers.TSVersionSerializer\n    http_method_names = [\"get\", \"head\"]\n\n    @action(detail=True)\n    def values(self, request, pk, **kwargs):\n        \"\"\"Get timeseries values.\"\"\"\n        values = self.queryset.get(pk=pk).tsvalue_set.values(\"time\", \"value\")\n        return Response(values)\n```", "```py\nclass MLConfigViewSet(viewsets.ModelViewSet):\n    queryset = models.MLConfig.objects.all()\n    serializer_class = serializers.MLConfigSerializer\n    http_method_names = [\"get\", \"post\", \"head\", \"delete\"]\n\nclass DataConfigViewSet(viewsets.ModelViewSet):\n    queryset = models.DataConfig.objects.all()\n    serializer_class = serializers.DataConfigSerializer\n    http_method_names = [\"get\", \"post\", \"head\", \"delete\"]\n\nclass PreprocessingConfigViewSet(viewsets.ModelViewSet):\n    queryset = models.PreprocessingConfig.objects.all()\n    serializer_class = serializers.PreprocessingConfigSerializer\n    http_method_names = [\"get\", \"post\", \"head\", \"delete\"]\n```", "```py\nclass MLModelViewSet(viewsets.ModelViewSet):\n    queryset = (\n        models.MLModel.objects.select_related(\"preprocess_config\", \"ml_config\")\n        .prefetch_related(\n            \"data_config__targets\",\n            \"data_config__features\",\n            \"mlmodelversion_set\",\n        )\n        .all()\n    )\n    serializer_class = serializers.MLModelSerializer\n    http_method_names = [\"get\", \"post\", \"head\", \"delete\"]\n\n    @action(detail=True)\n    def train(self, request, pk, **kwargs):\n        ml_model = self.queryset.get(pk=pk)\n        ml_model_version = train_pipeline(ml_model)\n        return Response(\n            serializers.MLModelVersionSerializer(ml_model_version).data\n        )\n\n    @action(detail=True)\n    def predict(self, request, pk, **kwargs):\n        ml_model = self.queryset.get(pk=pk)\n        if ml_model.mlmodelversion_set.count() == 0:\n            return Response(\n                \"No model has been trained yet!\",\n                status=status.HTTP_404_NOT_FOUND,\n            )\n\n        predictions = inference_pipeline(ml_model)\n        return Response(predictions)\n```", "```py\nclass MLModelVersionViewSet(viewsets.ModelViewSet):\n    queryset = models.MLModelVersion.objects.all()\n    serializer_class = serializers.MLModelVersionSerializer\n    http_method_names = [\"get\", \"head\"]\n```", "```py\nfrom django.urls import include, path\nfrom gtrends import views\nfrom rest_framework_extensions.routers import ExtendedDefaultRouter\n\nrouter = ExtendedDefaultRouter()\n\nrouter.register(\"model-config\", views.MLConfigViewSet)\nrouter.register(\"data-config\", views.DataConfigViewSet)\nrouter.register(\"preprocessing-config\", views.PreprocessingConfigViewSet)\n\nmodels = router.register(\"model\", views.MLModelViewSet)\nmodels.register(\n    \"versions\",\n    views.MLModelVersionViewSet,\n    basename=\"version\",\n    parents_query_lookups=\"model_id\",\n)\n\ntimeseries = router.register(\"timeseries\", views.TimeSeriesViewSet)\ntimeseries.register(\n    \"versions\",\n    views.TSVersionViewSet,\n    basename=\"version\",\n    parents_query_lookups=\"timeseries_id\",\n)\n\nurlpatterns = [\n    path(\"\", include(router.urls)),\n]\n```"]