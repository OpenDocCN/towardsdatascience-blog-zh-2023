- en: Audio Classification with Deep Learning in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/audio-classification-with-deep-learning-in-python-cf752b22ba07](https://towardsdatascience.com/audio-classification-with-deep-learning-in-python-cf752b22ba07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[The Kaggle Blueprints](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning image models to tackle domain shift and class imbalance with PyTorch
    and torchaudio in audio data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie?source=post_page-----cf752b22ba07--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----cf752b22ba07--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cf752b22ba07--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cf752b22ba07--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----cf752b22ba07--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cf752b22ba07--------------------------------)
    ·10 min read·Apr 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5658cabbf5faa49cd94151fa8d262288.png)'
  prefs: []
  type: TYPE_IMG
- en: Classifying bird calls in soundscapes with Machine Learning (Image drawn by
    the author)
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to another edition of “[The Kaggle Blueprints](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)”,
    where we will analyze [Kaggle](https://www.kaggle.com/) competitions’ winning
    solutions for lessons we can apply to our own data science projects.
  prefs: []
  type: TYPE_NORMAL
- en: This edition will review the techniques and approaches from the [“BirdCLEF 2022”](https://www.kaggle.com/competitions/birdclef-2022)
    competition, which ended in May 2022.
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem Statement: Audio Classification with Domain Shift'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of the [“BirdCLEF 2022”](https://www.kaggle.com/competitions/birdclef-2022)
    competition was to identify Hawaiian bird species by sound. The competitors were
    given short audio files of single bird calls and were asked to predict whether
    a specific bird was present in a longer recording.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.kaggle.com/competitions/birdclef-2022/?source=post_page-----cf752b22ba07--------------------------------)
    [## BirdCLEF 2022'
  prefs: []
  type: TYPE_NORMAL
- en: Identify bird calls in soundscapes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.kaggle.com](https://www.kaggle.com/competitions/birdclef-2022/?source=post_page-----cf752b22ba07--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to a vanilla audio classification problem, this competition added
    flavor with the following challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain shift** — The training data consisted of clean audio recordings of
    a single bird call separated from any additional sounds (a few seconds, different
    lengths). However, the test data consisted of “unclean” longer (1 minute) recordings
    taken “in the wild” and contained different sounds other than bird calls (e.g.,
    wind, rain, other animals, etc.).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/704679818fe7a7fed4bc9e911776597f.png)'
  prefs: []
  type: TYPE_IMG
- en: Domain shift in audio data
  prefs: []
  type: TYPE_NORMAL
- en: '**Class imbalance/Few-shot learning** —As some birds are less common than others,
    we are dealing with a long-tailed class distribution where some birds only have
    one sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/77ac7d7f41edd2c292be144740016d79.png)'
  prefs: []
  type: TYPE_IMG
- en: Long-tailed class distribution
  prefs: []
  type: TYPE_NORMAL
- en: '*Insert your data here!* — To follow along in this article, your dataset should
    look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d498603d50162c179f1679b9a89ea6e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Insert your data here: How your audio dataset dataframe should be formatted'
  prefs: []
  type: TYPE_NORMAL
- en: Approaching Audio Classification as an Image Classification Problem with Deep
    Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A popular approach among competitors to this audio classification problem was
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Converting the audio classification problem to an image classification problem](#e2c2)
    by converting the audio from waveform to a Mel spectrogram and applying a Deep
    Learning model'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Applying data augmentations to the audio data in waveform and in spectrograms](#8f8b)
    to tackle the domain shift and class imbalance'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Finetune a pre-trained image classification model](#daf1) to tackle class
    imbalance'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This article will use PyTorch (version 1.13.0) for the Deep Learning framework
    and `[torchaudio](https://pytorch.org/audio/stable/index.html)` (version 0.13.0)
    and `[librosa](https://librosa.org/doc/main/index.html)` (version 0.10.0) for
    audio processing. Additionally, we will be using `[timm](https://timm.fast.ai/)`
    (version 0.6.12) for fine-tuning with pre-trained image models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Preparations: Getting Familiar with Audio Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before getting started with solving an audio classification problem, let’s first
    get familiar with working with audio data. You can load the audio and its sampling
    rate from different file formats (e.g., .wav, .ogg, etc.) with the `.load()` method
    from the `torchaudio` library or the `librosa` library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you want to listen to the loaded audio directly in a Jupyter notebook for
    explorations, the following code will provide you with an audio player.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2bf7e7fca69596cc56af095a4ab4a080.png)'
  prefs: []
  type: TYPE_IMG
- en: Displaying audio player for loaded data in Jupyter notebook
  prefs: []
  type: TYPE_NORMAL
- en: The `[librosa](https://librosa.org/doc/main/index.html)` library also provides
    various methods to display the audio data for exploration purposes quickly. If
    you used `[torchaudio](https://pytorch.org/audio/stable/index.html)` to load the
    audio file, make sure to convert the tensors to NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5256d3c24fab2123ee1348e4c2b25ae3.png)'
  prefs: []
  type: TYPE_IMG
- en: Original audio data of the word “stop” in waveform from the “Speech Commands”
    dataset [0]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Convert the Audio Classification Problem to an Image Classification
    Problem'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A popular method to model audio data with a Deep Learning model is to convert
    the *“computer hearing”* problem to a *computer vision* problem [2]. Specifically,
    the waveform audio is converted to a Mel spectrogram (which is a type of image)
    as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ee81e347b6f6c708e569e49da5e7cbf.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting an audio file from waveform (time domain) to Mel spectrogram (frequency
    domain)
  prefs: []
  type: TYPE_NORMAL
- en: Usually, you would use a Fast Fourier Transform (FFT) to computationally convert
    an audio signal from the time domain (waveform) to the frequency domain (spectrogram).
  prefs: []
  type: TYPE_NORMAL
- en: However, the FFT will give you the overall frequency components for the entire
    time series of the audio signal as a whole. Thus, you are losing the time information
    when converting audio data from the time domain to the frequency domain.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of the FFT, you can use the Short-Time Fourier Transform (STFT) to preserve
    the time information. The STFT is a variant of the FFT that breaks up the audio
    signal into smaller sections by using a sliding time window. It takes the FFT
    on each section and then combines them.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_fft` —length of the sliding window (default: 2048)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hop_length` — number of samples by which to slide the window (default: 512).
    The `hop_length`will directly impact the resulting image size. If your audio data
    has a fixed length and you want to convert the waveform to a fixed image size,
    you can set `hop_length = audio_length // (image_size[1] — 1)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/f4ee73d6d2965c96bfb5d8b8b97df960.png)'
  prefs: []
  type: TYPE_IMG
- en: Short-Time Fourier Transform (STFT)
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will convert the amplitude to decibels and bin the frequencies according
    to the Mel scale. For this purpose, `n_mels` is the number of frequency bands
    (Mel bins). This will be the height of the resulting spectrogram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82405a51d8414924699fa9b74e0da69a.png)'
  prefs: []
  type: TYPE_IMG
- en: Convert amplitude to decibels and apply Mel binning to the spectrum
  prefs: []
  type: TYPE_NORMAL
- en: 'For an in-depth explanation of the Mel spectrogram, I recommend this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53?source=post_page-----cf752b22ba07--------------------------------)
    [## Understanding the Mel Spectrogram'
  prefs: []
  type: TYPE_NORMAL
- en: (and Other Topics in Signal Processing)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53?source=post_page-----cf752b22ba07--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Below you can see an example PyTorch `Dataset` which loads an audio file and
    converts the waveform to a Mel spectrogram after some preprocessing steps.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Your resulting dataset should produce samples that look something like this
    before we feed it to the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0c54b387ffda9de0c7cb8c156863f58.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample structure from the Audio Dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Apply Augmentations to Audio Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One technique to tackle this competition’s challenges of domain shift and class
    imbalance was to apply data augmentations to the training data [5, 8, 10, 11].
    You can apply data augmentations for audio data in the waveform and the spectrogram.
    The `[torchaudio](https://pytorch.org/audio/stable/index.html)` library already
    provides a lot of different data augmentations for audio data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Popular data augmentation techniques for audio data in **waveform (time domain)**
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Noise injection like white noise, colored noise, or background noise (`[AddNoise](https://pytorch.org/audio/stable/generated/torchaudio.transforms.AddNoise.html#torchaudio.transforms.AddNoise)`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shifting time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing speed (`[Speed](https://pytorch.org/audio/stable/generated/torchaudio.transforms.Speed.html#torchaudio.transforms.Speed)`;
    alternatively use `[TimeStretch](https://pytorch.org/audio/stable/generated/torchaudio.transforms.TimeStretch.html#torchaudio.transforms.TimeStretch)`
    in frequency domain)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing pitch (`[PitchShift](https://pytorch.org/audio/stable/generated/torchaudio.transforms.PitchShift.html#torchaudio.transforms.PitchShift)`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2c9d74a61d35695672531f166551ec88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Overview of different data augmentation techniques for audio in waveform: Noise
    injection (white noise, colored noise, background noise), shifting time, changing
    speed and pitch'
  prefs: []
  type: TYPE_NORMAL
- en: 'Popular data augmentation techniques for audio data in the **spectrogram (frequency
    domain)** are:'
  prefs: []
  type: TYPE_NORMAL
- en: Popular image augmentation techniques like Mixup [13] or Cutmix [3]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/d0193c5f5d8b516c9d6b5cff68fde6ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Data Augmentation for Spectrogram: Mixup [13]'
  prefs: []
  type: TYPE_NORMAL
- en: SpecAugment [7] (`[FrequencyMasking](https://pytorch.org/audio/stable/generated/torchaudio.transforms.FrequencyMasking.html#torchaudio.transforms.FrequencyMasking)`
    and `[TimeMasking](https://pytorch.org/audio/stable/generated/torchaudio.transforms.TimeMasking.html#torchaudio.transforms.TimeMasking)`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/23daa65bbcea6fa1c6a6d52efe3baffe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Data Augmentation for Spectrogram: SpecAugment [7]'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see while providing a lot of audio augmentations, `[torchaudio](https://pytorch.org/audio/stable/index.html)`
    doesn’t provide all of the proposed data augmentations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, if you want to inject a specific type of noise, shift the time, or apply
    Mixup [13] or Cutmix [12] augmentations, you must [write a custom data augmentation
    in PyTorch](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms).
    You can reference this [collection of audio data augmentation techniques](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c)
    for their implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c?source=post_page-----cf752b22ba07--------------------------------)
    [## Data Augmentation Techniques for Audio Data in Python'
  prefs: []
  type: TYPE_NORMAL
- en: How to augment audio in waveform (time domain) and as spectrograms (frequency
    domain) with librosa, numpy, and PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-augmentation-techniques-for-audio-data-in-python-15505483c63c?source=post_page-----cf752b22ba07--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example PyTorch `Dataset` class from before, you can apply the data
    augmentations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Fine-tune a Pretrained Image Classification Model for Few-Shot Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this competition, we are dealing with a class imbalance. As some classes
    only have one sample, we are dealing with a few-shot learning problem. Nakamura
    and Harada [6] showed in 2019 that fine-tuning could be an effective approach
    to few-shot learning.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of competitors [2, 5, 8, 10, 11] fine-tuned common pre-trained image classification
    models such as
  prefs: []
  type: TYPE_NORMAL
- en: EfficientNet (e.g., `tf_efficientnet_b3_ns`) [9],
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SE-ResNext (e.g., `se_resnext50_32x4d`) [3],
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NFNet (e.g., `eca_nfnet_l0`) [1]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can load any pre-trained image classification model with the `[timm](https://timm.fast.ai/)`
    library for fine-tuning. Make sure to set `in_chans = 1` as we are not working
    with 3-channel images but 1-channel Mel spectrograms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Other competitors reported successes from fine-tuning models pre-trained on
    similar audio classification problems [4, 10].
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning is done with a cosine annealing learning rate scheduler (`[CosineAnnealingLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR)`)
    for a few epochs [2, 8].
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9ae13b38a3e843bc46c227a590baa3f2.png)'
  prefs: []
  type: TYPE_IMG
- en: PyTorch Cosine Annealing / Decay Learning Rate Scheduler (Image by the author,
    originally published in [“A Visual Guide to Learning Rate Schedulers in PyTorch”](https://medium.com/towards-data-science/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863#fad1))
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more tips and best practices in this [guide for fine-tuning Deep
    Learning models](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f):'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----cf752b22ba07--------------------------------)
    [## Intermediate Deep Learning with Transfer Learning'
  prefs: []
  type: TYPE_NORMAL
- en: A practical guide for fine-tuning Deep Learning models for computer vision and
    natural language processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----cf752b22ba07--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many more lessons to be learned from reviewing the learning resources
    Kagglers have created during the course of the [“BirdCLEF 2022”](https://www.kaggle.com/competitions/birdclef-2022)
    competition. There are also many different solutions for this type of problem
    statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we focused on the general approach that was popular among
    many competitors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Converting the audio classification problem to an image classification problem](#e2c2)
    by converting the audio from waveform to a Mel spectrogram and applying a Deep
    Learning model'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Applying data augmentations to the audio data in waveform and in spectrograms](#8f8b)
    to tackle the domain shift and class imbalance'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Finetune a pre-trained image classification model](#daf1) to tackle class
    imbalance'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enjoyed This Story?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----cf752b22ba07--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----cf752b22ba07--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the original competition data does not allow commercial use, examples are
    done with the following dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[0] Warden P. Speech Commands: A public dataset for single-word speech recognition,
    2017\. Available from [http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz](http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz)'
  prefs: []
  type: TYPE_NORMAL
- en: 'License: CC-BY-4.0'
  prefs: []
  type: TYPE_NORMAL
- en: Image References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If not otherwise stated, all images are created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Web & Literature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Brock, A., De, S., Smith, S. L., & Simonyan, K. (2021, July). High-performance
    large-scale image recognition without normalization. In *International Conference
    on Machine Learning* (pp. 1059–1071). PMLR.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Chai Time Data Science](https://www.youtube.com/@ChaiTimeDataScience)
    (2022). [BirdCLEF 2022: 11th Pos Gold Solution | Gilles Vandewiele](https://www.youtube.com/watch?v=MXZKNnuoQXw)
    (accessed March 13th, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Hu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation networks. In
    *Proceedings of the IEEE conference on computer vision and pattern recognition*
    (pp. 7132–7141).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Kramarenko Vladislav (2022). [4th place](https://www.kaggle.com/competitions/birdclef-2022/discussion/326987)
    in Kaggle Discussions (accessed March 13th, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] LeonShangguan (2022). [[Public #1 Private #2] + [Private #7/8 (potential)]
    solutions. The host wins.](https://www.kaggle.com/competitions/birdclef-2022/discussion/326950)
    in Kaggle Discussions (accessed March 13th, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Nakamura, A., & Harada, T. (2019). Revisiting fine-tuning for few-shot
    learning. *arXiv preprint arXiv:1910.00216*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Park, D. S., Chan, W., Zhang, Y., Chiu, C. C., Zoph, B., Cubuk, E. D.,
    & Le, Q. V. (2019). Specaugment: A simple data augmentation method for automatic
    speech recognition. *arXiv preprint arXiv:1904.08779*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] slime (2022). [3rd place solution](https://www.kaggle.com/competitions/birdclef-2022/discussion/327193)
    in Kaggle Discussions (accessed March 13th, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Tan, M., & Le, Q. (2019, May). Efficientnet: Rethinking model scaling for
    convolutional neural networks. In *International conference on machine learning*
    (pp. 6105–6114). PMLR.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Volodymyr (2022). [1st place solution models (it’s not all BirdNet)](https://www.kaggle.com/competitions/birdclef-2022/discussion/327047)
    in Kaggle Discussions (accessed March 13th, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] yokuyama (2022). [5th place solution](https://www.kaggle.com/competitions/birdclef-2022/discussion/327044)
    in Kaggle Discussions (accessed March 13th, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., & Yoo, Y. (2019). Cutmix:
    Regularization strategy to train strong classifiers with localizable features.
    In *Proceedings of the IEEE/CVF international conference on computer vision* (pp.
    6023–6032).'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017) mixup: Beyond
    empirical risk minimization. arXiv preprint arXiv:1710.09412.'
  prefs: []
  type: TYPE_NORMAL
