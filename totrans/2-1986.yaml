- en: 'The A-Z of Transformers: Everything You Need to Know'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å˜æ¢å™¨çš„Aåˆ°Zï¼šä½ éœ€è¦çŸ¥é“çš„ä¸€åˆ‡
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/the-a-z-of-transformers-everything-you-need-to-know-c9f214c619ac](https://towardsdatascience.com/the-a-z-of-transformers-everything-you-need-to-know-c9f214c619ac)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/the-a-z-of-transformers-everything-you-need-to-know-c9f214c619ac](https://towardsdatascience.com/the-a-z-of-transformers-everything-you-need-to-know-c9f214c619ac)
- en: Everything you need to know about Transformers, and how to implement them
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦äº†è§£çš„å…³äºå˜æ¢å™¨çš„ä¸€åˆ‡ï¼Œä»¥åŠå¦‚ä½•å®ç°å®ƒä»¬
- en: '[](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)[![FranÃ§ois
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)
    [FranÃ§ois Porcher](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)[![FranÃ§ois
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)
    [FranÃ§ois Porcher](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)
    Â·16 min readÂ·Oct 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨åœ¨[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)
    Â·16åˆ†é’Ÿé˜…è¯»Â·2023å¹´10æœˆ25æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/be68574b989226c3e6ac62c933050cf5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be68574b989226c3e6ac62c933050cf5.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ä½œè€…
- en: Why another tutorial on Transformers?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¿˜è¦å¦å†™ä¸€ç¯‡å…³äºå˜æ¢å™¨çš„æ•™ç¨‹ï¼Ÿ
- en: You have probably already heard of Transformers, and everyone talks about it,
    so why making a new article about it?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½å·²ç»å¬è¯´è¿‡å˜æ¢å™¨ï¼Œæ¯ä¸ªäººéƒ½åœ¨è°ˆè®ºå®ƒï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆè¿˜è¦å†å†™ä¸€ç¯‡å…³äºå®ƒçš„æ–‡ç« å‘¢ï¼Ÿ
- en: Well, I am a researcher, and this requires me to have a very deep understanding
    of the tools I use (because if you donâ€™t understand them, how can you identify
    where they are wrong and how you can improve them, right?).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ˜¯ä¸€åç ”ç©¶å‘˜ï¼Œè¿™è¦æ±‚æˆ‘å¯¹æˆ‘ä½¿ç”¨çš„å·¥å…·æœ‰éå¸¸æ·±å…¥çš„ç†è§£ï¼ˆå› ä¸ºå¦‚æœä½ ä¸ç†è§£å®ƒä»¬ï¼Œä½ æ€ä¹ˆèƒ½è¯†åˆ«å®ƒä»¬çš„é—®é¢˜ä»¥åŠå¦‚ä½•æ”¹è¿›å®ƒä»¬ï¼Œå¯¹å§ï¼Ÿï¼‰ã€‚
- en: As I ventured deeper into the world of Transformers, I found myself buried under
    a mountain of resources. And yet, despite all that reading, I was left with a
    general sense of the architecture and a trail of lingering questions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘æ·±å…¥æ¢ç´¢å˜æ¢å™¨çš„ä¸–ç•Œæ—¶ï¼Œæˆ‘å‘ç°è‡ªå·±åŸ‹åœ¨ä¸€å †èµ„æºä¸‹ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå°½ç®¡é˜…è¯»äº†è¿™ä¹ˆå¤šï¼Œæˆ‘ä»ç„¶å¯¹æ¶æ„æœ‰ä¸€ä¸ªå¤§è‡´çš„äº†è§£ï¼Œå¹¶ç•™ä¸‹äº†ä¸€è¿ä¸²æ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚
- en: In this guide, I aim to bridge that knowledge gap. A guide that will give you
    a strong intuition on Transformers, a deep dive into the architecture, and the
    implementation from scratch.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘æ—¨åœ¨å¼¥è¡¥è¿™ç§çŸ¥è¯†å·®è·ã€‚ä¸€ä¸ªå°†ç»™ä½ æä¾›å¯¹å˜æ¢å™¨çš„å¼ºç›´è§‰ã€å¯¹æ¶æ„çš„æ·±å…¥æ¢è®¨ä»¥åŠä»é›¶å¼€å§‹å®ç°çš„æŒ‡å—ã€‚
- en: 'I strongly advise you to follow the code on [Github](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¼ºçƒˆå»ºè®®ä½ å…³æ³¨[Github](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch)ä¸Šçš„ä»£ç ï¼š
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch?source=post_page-----c9f214c619ac--------------------------------)
    [## awesome-ai-tutorials/NLP/007 - Transformers From Scratch at main Â·â€¦'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch?source=post_page-----c9f214c619ac--------------------------------)
    [## awesome-ai-tutorials/NLP/007 - Transformers From Scratch at main Â·â€¦'
- en: The best collection of AI tutorials to make you a boss of Data Science! - awesome-ai-tutorials/NLP/007
    - Transformersâ€¦
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ€ä½³AIæ•™ç¨‹åˆé›†ï¼Œè®©ä½ æˆä¸ºæ•°æ®ç§‘å­¦çš„ä¸“å®¶ï¼ - awesome-ai-tutorials/NLP/007 - Transformersâ€¦
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch?source=post_page-----c9f214c619ac--------------------------------)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch?source=post_page-----c9f214c619ac--------------------------------)
- en: Enjoy! ğŸ¤—
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: äº«å—å§ï¼ğŸ¤—
- en: 'A little bit of History first:'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¦–å…ˆä¸€ç‚¹å†å²ï¼š
- en: Many attribute the concept of the attention mechanism to the renowned paper
    â€œAttention is All You Needâ€ by the Google Brain team. However, **this is only
    part of the story.**
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šäººå°†æ³¨æ„åŠ›æœºåˆ¶çš„æ¦‚å¿µå½’åŠŸäºGoogle Brainå›¢é˜Ÿçš„è‘—åè®ºæ–‡â€œAttention is All You Needâ€ã€‚ç„¶è€Œï¼Œ**è¿™åªæ˜¯éƒ¨åˆ†äº‹å®**ã€‚
- en: The roots of the attention mechanism can be traced back to an earlier paper
    titled â€œ[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)â€
    authored by Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶çš„æ ¹æºå¯ä»¥è¿½æº¯åˆ°ä¸€ç¯‡æ—©æœŸçš„è®ºæ–‡ï¼Œé¢˜ä¸ºâ€œ[Neural Machine Translation by Jointly Learning to Align
    and Translate](https://arxiv.org/abs/1409.0473)â€ï¼Œç”±Dzmitry Bahdanauã€KyungHyun Choå’ŒYoshua
    Bengioæ’°å†™ã€‚
- en: Bahdanauâ€™s primary challenge was addressing the limitations of Recurrent Neural
    Networks (RNNs). Specifically, when encoding lengthy sentences into vectors using
    RNNs, **crucial information was often lost.**
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Bahdanauçš„ä¸»è¦æŒ‘æˆ˜æ˜¯è§£å†³å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„å±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ä½¿ç”¨RNNå°†é•¿å¥ç¼–ç ä¸ºå‘é‡æ—¶ï¼Œ**å…³é”®çš„ä¿¡æ¯ç»å¸¸ä¸¢å¤±ã€‚**
- en: Drawing parallels from translation exercises â€” where one often revisits the
    source sentence while translating â€” Bahdanau aimed to allocate weights to the
    hidden states within the RNN. This approach yielded **impressive outcomes**, and
    is depicted in the following diagram.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç¿»è¯‘ç»ƒä¹ ä¸­æ±²å–çš„ç»éªŒâ€”â€”åœ¨ç¿»è¯‘æ—¶ç»å¸¸ä¼šé‡æ–°å®¡è§†æºå¥â€”â€”Bahdanauçš„ç›®æ ‡æ˜¯ä¸ºRNNä¸­çš„éšè—çŠ¶æ€åˆ†é…æƒé‡ã€‚è¿™ç§æ–¹æ³•å–å¾—äº†**ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœ**ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
- en: '![](../Images/021d4865ae1f5167428caeb2caea1723.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/021d4865ae1f5167428caeb2caea1723.png)'
- en: Image from [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºäº[Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf)
- en: 'However, Bahdanau wasnâ€™t the only one tackling this issue. Taking cues from
    his groundbreaking work, the Google Brain team posited a bold idea:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼ŒBahdanauå¹¶ä¸æ˜¯å”¯ä¸€ä¸€ä¸ªè§£å†³è¿™ä¸ªé—®é¢˜çš„äººã€‚å€Ÿé‰´äº†ä»–çš„å¼€åˆ›æ€§å·¥ä½œï¼ŒGoogle Brainå›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªå¤§èƒ†çš„æƒ³æ³•ï¼š
- en: â€œWhy not strip everything down and focus solely on the attention mechanism?â€
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œä¸ºä»€ä¹ˆä¸å»æ‰ä¸€åˆ‡ï¼Œåªä¸“æ³¨äºæ³¨æ„åŠ›æœºåˆ¶å‘¢ï¼Ÿâ€
- en: They believed it wasnâ€™t the RNN but the attention mechanism that was the primary
    driver behind the success.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬è®¤ä¸ºï¼ŒæˆåŠŸçš„ä¸»è¦é©±åŠ¨å› ç´ ä¸æ˜¯RNNï¼Œè€Œæ˜¯æ³¨æ„åŠ›æœºåˆ¶ã€‚
- en: This conviction culminated in their paper, aptly titled â€œAttention is All You
    Needâ€.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ä¿¡å¿µæœ€ç»ˆåœ¨ä»–ä»¬çš„è®ºæ–‡ä¸­å¾—åˆ°äº†ä½“ç°ï¼Œè¿™ç¯‡è®ºæ–‡æ°å¦‚å…¶åˆ†åœ°å‘½åä¸ºâ€œAttention is All You Needâ€ã€‚
- en: Fascinating, right?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¸å¼•äººï¼Œå¯¹å§ï¼Ÿ
- en: The Transformer Architecture
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformeræ¶æ„
- en: 1\. First things first, Embeddings
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. é¦–å…ˆï¼ŒåµŒå…¥
- en: This diagram represents the Transformer architecture. Donâ€™t worry if you donâ€™t
    understand anything at first, we will cover absolutely everything.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå›¾è¡¨ç¤ºäº†Transformeræ¶æ„ã€‚ä¸è¦æ‹…å¿ƒå¦‚æœä½ ä¸€å¼€å§‹ä¸ç†è§£ä»»ä½•ä¸œè¥¿ï¼Œæˆ‘ä»¬ä¼šæ¶µç›–æ‰€æœ‰å†…å®¹ã€‚
- en: '![](../Images/dcfe23dc844b3b9aa870462b629be554.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcfe23dc844b3b9aa870462b629be554.png)'
- en: Embeddings, Image from article modified by author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥ï¼Œæ–‡ç« ä¸­çš„å›¾ç‰‡ç»è¿‡ä½œè€…ä¿®æ”¹
- en: 'From Text to Vectors â€” The Embedding Process: Imagine our input is a sequence
    of words, say â€œThe cat drinks milkâ€. This sequence has a length termed as `seq_len`.
    Our immediate task is to convert these words into a form that the model can understand,
    specifically vectors. That''s where the Embedder comes in.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ–‡æœ¬åˆ°å‘é‡â€”â€”åµŒå…¥è¿‡ç¨‹ï¼šå‡è®¾æˆ‘ä»¬çš„è¾“å…¥æ˜¯ä¸€ä¸ªå•è¯åºåˆ—ï¼Œæ¯”å¦‚â€œçŒ«å–ç‰›å¥¶â€ã€‚è¿™ä¸ªåºåˆ—çš„é•¿åº¦ç§°ä¸º`seq_len`ã€‚æˆ‘ä»¬ç«‹å³è¦åšçš„å°±æ˜¯å°†è¿™äº›å•è¯è½¬æ¢æˆæ¨¡å‹å¯ä»¥ç†è§£çš„å½¢å¼ï¼Œå…·ä½“æ¥è¯´ï¼Œå°±æ˜¯å‘é‡ã€‚è¿™å°±æ˜¯Embedderçš„ä½œç”¨ã€‚
- en: Each word undergoes a transformation to become a vector. This transformation
    process is termed as â€˜embeddingâ€™. Each of these vectors or â€˜embeddingsâ€™ has a
    size of `d_model = 512`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªè¯éƒ½ä¼šç»å†ä¸€ä¸ªå˜æ¢ï¼Œæˆä¸ºä¸€ä¸ªå‘é‡ã€‚è¿™ä¸ªå˜æ¢è¿‡ç¨‹è¢«ç§°ä¸ºâ€œåµŒå…¥â€ã€‚è¿™äº›å‘é‡æˆ–â€œåµŒå…¥â€çš„å¤§å°ä¸º`d_model = 512`ã€‚
- en: Now, what exactly is this Embedder? At its core, the Embedder is a linear mapping
    (matrix), denoted by `E`. You can visualize it as a matrix of size `(d_model,
    vocab_size)`, where `vocab_size` is the size of our vocabulary.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œè¿™ä¸ªEmbedderåˆ°åº•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿä»æœ¬è´¨ä¸Šè®²ï¼ŒEmbedderæ˜¯ä¸€ä¸ªçº¿æ€§æ˜ å°„ï¼ˆçŸ©é˜µï¼‰ï¼Œç”¨`E`è¡¨ç¤ºã€‚ä½ å¯ä»¥å°†å…¶è§†ä¸ºä¸€ä¸ªå¤§å°ä¸º`(d_model,
    vocab_size)`çš„çŸ©é˜µï¼Œå…¶ä¸­`vocab_size`æ˜¯æˆ‘ä»¬è¯æ±‡è¡¨çš„å¤§å°ã€‚
- en: After the embedding process, we end up with a collection of vectors of size
    `d_model` each. Itâ€™s crucial to understand this format, as itâ€™s a recurrent theme
    â€” youâ€™ll see it across various stages like encoder input, encoder output, and
    so on.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åµŒå…¥è¿‡ç¨‹ä¹‹åï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ç»„å¤§å°ä¸º`d_model`çš„å‘é‡ã€‚ç†è§£è¿™ç§æ ¼å¼è‡³å…³é‡è¦ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªåå¤å‡ºç°çš„ä¸»é¢˜â€”â€”ä½ ä¼šåœ¨ç¼–ç å™¨è¾“å…¥ã€ç¼–ç å™¨è¾“å‡ºç­‰å„ä¸ªé˜¶æ®µçœ‹åˆ°å®ƒã€‚
- en: 'Letâ€™s code this part:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç¼–å†™è¿™ä¸€éƒ¨åˆ†ä»£ç ï¼š
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note: we multiply by *d_model* for normalization purposes (explained later)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨ï¼šæˆ‘ä»¬ä¹˜ä»¥*d_model*æ˜¯ä¸ºäº†å½’ä¸€åŒ–ï¼ˆç¨åè§£é‡Šï¼‰
- en: 'Note 2: I personally wondered if we used a pre-trained embedder, or at least
    start from a pre-trained one and fine-tune it. **But no, the embedding is fully
    learned from scratch and initialized randomly.**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨2ï¼šæˆ‘ä¸ªäººæƒ³çŸ¥é“æˆ‘ä»¬æ˜¯å¦ä½¿ç”¨äº†é¢„è®­ç»ƒçš„åµŒå…¥å™¨ï¼Œæˆ–è€…è‡³å°‘ä»ä¸€ä¸ªé¢„è®­ç»ƒçš„åµŒå…¥å™¨å¼€å§‹å¹¶è¿›è¡Œå¾®è°ƒã€‚**ä½†ä¸ï¼ŒåµŒå…¥å®Œå…¨æ˜¯ä»å¤´å­¦ä¹ çš„ï¼Œå¹¶ä¸”æ˜¯éšæœºåˆå§‹åŒ–çš„ã€‚**
- en: Positional Encoding
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½ç½®ç¼–ç 
- en: Why Do We Need Positional Encoding?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦ä½ç½®ç¼–ç ï¼Ÿ
- en: 'Given our current setup, we possess a list of vectors representing words. If
    fed as-is to a transformer model, thereâ€™s a **key element missing: the sequential
    order of words.** Words in natural languages often derive meaning from their position.
    â€œJohn loves Maryâ€ carries a different sentiment from â€œMary loves John.â€ To ensure
    our model captures this order, we introduce Positional Encoding.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å½“å‰çš„è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬æ‹¥æœ‰ä¸€ä¸ªè¡¨ç¤ºå•è¯çš„å‘é‡åˆ—è¡¨ã€‚å¦‚æœæŒ‰åŸæ ·è¾“å…¥åˆ°å˜æ¢å™¨æ¨¡å‹ä¸­ï¼Œæœ‰ä¸€ä¸ª **å…³é”®è¦ç´ ç¼ºå¤±ï¼šå•è¯çš„é¡ºåºã€‚** è‡ªç„¶è¯­è¨€ä¸­çš„å•è¯å¾€å¾€ä»å…¶ä½ç½®ä¸­è·å–æ„ä¹‰ã€‚â€œJohn
    loves Maryâ€å’Œâ€œMary loves Johnâ€è¡¨è¾¾çš„æƒ…æ„Ÿä¸åŒã€‚ä¸ºäº†ç¡®ä¿æˆ‘ä»¬çš„æ¨¡å‹æ•æ‰åˆ°è¿™ç§é¡ºåºï¼Œæˆ‘ä»¬å¼•å…¥äº†ä½ç½®ç¼–ç ã€‚
- en: 'Now, you might wonder, â€œWhy not just add a simple increment like +1 for the
    first word, +2 for the second, and so on?â€ There are several challenges with this
    approach:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä½ å¯èƒ½ä¼šæƒ³ï¼Œâ€œä¸ºä»€ä¹ˆä¸ç®€å•åœ°å¯¹ç¬¬ä¸€ä¸ªè¯åŠ  +1ï¼Œå¯¹ç¬¬äºŒä¸ªè¯åŠ  +2ï¼Œä»¥æ­¤ç±»æ¨å‘¢ï¼Ÿâ€è¿™ç§æ–¹æ³•æœ‰å‡ ä¸ªæŒ‘æˆ˜ï¼š
- en: '**Multidimensionality:** Each token is represented in 512 dimensions. A mere
    increment would not suffice to capture this complex space.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¤šç»´åº¦æ€§ï¼š** æ¯ä¸ªä»¤ç‰Œåœ¨ 512 ç»´åº¦ä¸­è¡¨ç¤ºã€‚ç®€å•çš„å¢é‡æ— æ³•æ•æ‰è¿™ä¸ªå¤æ‚çš„ç©ºé—´ã€‚'
- en: '**Normalization Concerns:** Ideally, we want our values to lie between -1 and
    1\. So, directly adding large numbers (like +2000 for a long text) would be problematic.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å½’ä¸€åŒ–é—®é¢˜ï¼š** ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›å€¼åœ¨ -1 å’Œ 1 ä¹‹é—´ã€‚å› æ­¤ï¼Œç›´æ¥æ·»åŠ å¤§æ•°å€¼ï¼ˆä¾‹å¦‚ï¼Œé•¿æ–‡æœ¬ä¸­çš„ +2000ï¼‰ä¼šå¸¦æ¥é—®é¢˜ã€‚'
- en: '**Sequence Length Dependency:** Using direct increments is not scale-agnostic.
    For a long text, where the position might be +5000, this number does **not truly
    reflect the relative position** of the token in its associated sentence. And **the
    meaning of a world depends more on its relative position in a sentence, than its
    absolute position in a text.**'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åºåˆ—é•¿åº¦ä¾èµ–æ€§ï¼š** ç›´æ¥å¢é‡çš„ä½¿ç”¨ä¸æ˜¯å°ºåº¦æ— å…³çš„ã€‚å¯¹äºé•¿æ–‡æœ¬ï¼Œå…¶ä¸­ä½ç½®å¯èƒ½æ˜¯ +5000ï¼Œè¿™ä¸ªæ•°å­—å¹¶ **ä¸èƒ½çœŸæ­£åæ˜ ä»¤ç‰Œåœ¨å…¶å…³è”å¥å­ä¸­çš„ç›¸å¯¹ä½ç½®**ã€‚è€Œä¸”
    **ä¸€ä¸ªè¯çš„æ„ä¹‰æ›´ä¾èµ–äºå®ƒåœ¨å¥å­ä¸­çš„ç›¸å¯¹ä½ç½®ï¼Œè€Œéåœ¨æ–‡æœ¬ä¸­çš„ç»å¯¹ä½ç½®ã€‚**'
- en: If you studied mathematics, the idea of circular coordinates â€” **specifically,
    sine and cosine functions** â€” should resonate with your intuition. These functions
    provide a unique way to encode position that meets our needs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å­¦ä¹ è¿‡æ•°å­¦ï¼Œåœ†å½¢åæ ‡çš„æ¦‚å¿µ â€” **ç‰¹åˆ«æ˜¯æ­£å¼¦å’Œä½™å¼¦å‡½æ•°** â€” åº”è¯¥ä¸ä½ çš„ç›´è§‰ç›¸å¥‘åˆã€‚è¿™äº›å‡½æ•°æä¾›äº†ä¸€ç§ç‹¬ç‰¹çš„æ–¹å¼æ¥ç¼–ç ä½ç½®ï¼Œä»¥æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚ã€‚
- en: Given our matrix of size `(seq_len, d_model)`, our aim is to add another matrix,
    the Positional Encoding, of the same size.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šæˆ‘ä»¬å¤§å°ä¸º `(seq_len, d_model)` çš„çŸ©é˜µï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ·»åŠ å¦ä¸€ä¸ªç›¸åŒå¤§å°çš„çŸ©é˜µï¼Œå³ä½ç½®ç¼–ç ã€‚
- en: 'Hereâ€™s the core concept:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ ¸å¿ƒæ¦‚å¿µï¼š
- en: For every token, the authors suggest providing a **sine** coordinate of the
    pairwise dimensions (2k) a **cosine** coordinate to (2k+1).
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªä»¤ç‰Œï¼Œä½œè€…å»ºè®®æä¾›ä¸€å¯¹ç»´åº¦ï¼ˆ2kï¼‰çš„ **æ­£å¼¦** åæ ‡å’Œ (2k+1) çš„ **ä½™å¼¦** åæ ‡ã€‚
- en: If we fix the token position, and we move the dimension, we can see that the
    sine/cosine decrease in frequency
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å›ºå®šä»¤ç‰Œä½ç½®ï¼Œå¹¶ç§»åŠ¨ç»´åº¦ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ­£å¼¦/ä½™å¼¦é¢‘ç‡çš„ä¸‹é™
- en: If we look at a token that is further in the text, this phenomenon happens more
    rapidly (the frequency is increased)
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æŸ¥çœ‹æ–‡æœ¬ä¸­è¾ƒè¿œçš„ä»¤ç‰Œï¼Œè¿™ç§ç°è±¡ä¼šæ›´è¿…é€Ÿåœ°å‘ç”Ÿï¼ˆé¢‘ç‡å¢åŠ ï¼‰
- en: '![](../Images/f46bdcaafa0040d4f409a5db8908caf7.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f46bdcaafa0040d4f409a5db8908caf7.png)'
- en: Image from article
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡ç« ä¸­çš„å›¾ç‰‡
- en: This is summed up in the following graph (but donâ€™t scratch your head too much
    on this). The Key take away is that Positional Encoding is a mathematical function
    that allows the Transformer to keep an idea of the order of tokens in the sentence.
    This is a very active area or research.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨ä»¥ä¸‹å›¾ä¸­å¾—åˆ°äº†æ€»ç»“ï¼ˆä½†ä¸å¿…å¤ªè´¹è„‘ç­‹ï¼‰ã€‚å…³é”®ç‚¹æ˜¯ä½ç½®ç¼–ç æ˜¯ä¸€ä¸ªæ•°å­¦å‡½æ•°ï¼Œå®ƒå…è®¸å˜æ¢å™¨ä¿æŒå¯¹å¥å­ä¸­ä»¤ç‰Œé¡ºåºçš„ç†è§£ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸æ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚
- en: '![](../Images/18a0ef4eaab2b74148416745751809cf.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18a0ef4eaab2b74148416745751809cf.png)'
- en: Positional Embedding, Image by author
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ç½®åµŒå…¥ï¼Œä½œè€…æä¾›çš„å›¾ç‰‡
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Attention Mechanism (Single Head)
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå•å¤´ï¼‰
- en: 'Letâ€™s dive into the core concept of Googleâ€™s paper: the Attention Mechanism'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ·±å…¥äº†è§£è°·æ­Œè®ºæ–‡çš„æ ¸å¿ƒæ¦‚å¿µï¼šæ³¨æ„åŠ›æœºåˆ¶
- en: 'High-Level Intuition:'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é«˜çº§ç›´è§‰ï¼š
- en: At its core, the attention mechanism is a **communication mechanism between
    vectors/tokens.** It allows a model to focus on specific parts of the input when
    producing an output. Think of it as shining a spotlight on certain parts of your
    input data. This â€œspotlightâ€ can be brighter on more relevant parts (giving them
    more attention) and dimmer on less relevant parts.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æœ¬è´¨ä¸Šè®²ï¼Œæ³¨æ„åŠ›æœºåˆ¶æ˜¯ **å‘é‡/ä»¤ç‰Œä¹‹é—´çš„é€šä¿¡æœºåˆ¶**ã€‚å®ƒå…è®¸æ¨¡å‹åœ¨ç”Ÿæˆè¾“å‡ºæ—¶ä¸“æ³¨äºè¾“å…¥çš„ç‰¹å®šéƒ¨åˆ†ã€‚å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯å¯¹è¾“å…¥æ•°æ®çš„æŸäº›éƒ¨åˆ†è¿›è¡Œèšç„¦çš„â€œèšå…‰ç¯â€ã€‚è¿™ä¸ªâ€œèšå…‰ç¯â€å¯ä»¥åœ¨æ›´ç›¸å…³çš„éƒ¨åˆ†æ›´äº®ï¼ˆç»™äºˆæ›´å¤šå…³æ³¨ï¼‰ï¼Œåœ¨ä¸å¤ªç›¸å…³çš„éƒ¨åˆ†æ›´æš—ã€‚
- en: For a sentence, attention helps determine the relationship between words. Some
    words are closely related to each other in meaning or function within a sentence,
    while others are not. **The attention mechanism quantifies these relationships.**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€ä¸ªå¥å­ï¼Œæ³¨æ„åŠ›æœ‰åŠ©äºç¡®å®šå•è¯ä¹‹é—´çš„å…³ç³»ã€‚æŸäº›å•è¯åœ¨å¥å­ä¸­åœ¨æ„ä¹‰æˆ–åŠŸèƒ½ä¸Šç´§å¯†ç›¸å…³ï¼Œè€Œå…¶ä»–å•è¯åˆ™ä¸ç„¶ã€‚**æ³¨æ„åŠ›æœºåˆ¶é‡åŒ–äº†è¿™äº›å…³ç³»ã€‚**
- en: 'Example:'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: 'Consider the sentence: â€œShe gave him her book.â€'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘å¥å­ï¼šâ€œShe gave him her book.â€
- en: 'If we focus on the word â€œherâ€, the attention mechanism might determine that:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å…³æ³¨å•è¯â€œherâ€ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯èƒ½ä¼šç¡®å®šï¼š
- en: It has a strong connection with â€œbookâ€ because â€œherâ€ is indicating possession
    of the â€œbookâ€.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒä¸â€œbookâ€æœ‰å¾ˆå¼ºçš„è”ç³»ï¼Œå› ä¸ºâ€œherâ€è¡¨ç¤ºå¯¹â€œbookâ€çš„æ‰€æœ‰æƒã€‚
- en: It has a medium connection with â€œSheâ€ because â€œSheâ€ and â€œherâ€ likely refer to
    the same entity.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒä¸â€œSheâ€æœ‰ä¸­ç­‰çš„è”ç³»ï¼Œå› ä¸ºâ€œSheâ€å’Œâ€œherâ€å¯èƒ½æŒ‡çš„æ˜¯åŒä¸€ä¸ªå®ä½“ã€‚
- en: It has a weaker connection with other words like â€œgaveâ€ or â€œhimâ€.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒä¸å…¶ä»–è¯å¦‚â€œgaveâ€æˆ–â€œhimâ€çš„è”ç³»è¾ƒå¼±ã€‚
- en: Technical Dive into the Attention mechanism
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·±å…¥äº†è§£æ³¨æ„åŠ›æœºåˆ¶
- en: '![](../Images/963c9565bade1e73604dfc61e9593e68.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/963c9565bade1e73604dfc61e9593e68.png)'
- en: Scaled Dot-Product Attention, image from [article](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œå›¾ç‰‡æ¥æºäº[æ–‡ç« ](https://arxiv.org/pdf/1706.03762.pdf)
- en: '**For each token, we generate three vectors:**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯¹äºæ¯ä¸ªæ ‡è®°ï¼Œæˆ‘ä»¬ç”Ÿæˆä¸‰ä¸ªå‘é‡ï¼š**'
- en: '**Query (Q):**'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Query (Q)ï¼š**'
- en: '**Intuition**: Think of the query as a â€œ**question**â€ that a token poses. It
    represents the current word and tries to find out which parts of the sequence
    are relevant to it.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›´è§‰**ï¼šå°†æŸ¥è¯¢è§†ä¸ºä¸€ä¸ªæ ‡è®°æå‡ºçš„â€œ**é—®é¢˜**â€ã€‚å®ƒä»£è¡¨å½“å‰çš„å•è¯ï¼Œå¹¶å°è¯•æ‰¾å‡ºåºåˆ—ä¸­å“ªäº›éƒ¨åˆ†ä¸å…¶ç›¸å…³ã€‚'
- en: '**2\. Key (K):**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. Key (K)ï¼š**'
- en: '**Intuition**: The key can be thought of as an â€œ**identifier**â€ for each word
    in the sequence. When the query â€œasksâ€ its question, the key helps in â€œansweringâ€
    by determining **how relevant each word in the sequence is to the query.**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›´è§‰**ï¼šå¯ä»¥å°†é”®è§†ä¸ºåºåˆ—ä¸­æ¯ä¸ªå•è¯çš„â€œ**æ ‡è¯†ç¬¦**â€ã€‚å½“æŸ¥è¯¢â€œæé—®â€æ—¶ï¼Œé”®é€šè¿‡ç¡®å®š**åºåˆ—ä¸­æ¯ä¸ªå•è¯ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§**æ¥â€œå›ç­”â€ã€‚'
- en: '**3\. Value (V):**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. Value (V)ï¼š**'
- en: '**Intuition**: Once the relevance of each word (via its key) to the query is
    determined, we need actual information or content from those words to assist the
    current token. This is where the value comes in. **It represents the content of
    each word.**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›´è§‰**ï¼šä¸€æ—¦ç¡®å®šäº†æ¯ä¸ªå•è¯ï¼ˆé€šè¿‡å…¶é”®ï¼‰ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼Œæˆ‘ä»¬éœ€è¦ä»è¿™äº›å•è¯ä¸­è·å–å®é™…çš„ä¿¡æ¯æˆ–å†…å®¹æ¥è¾…åŠ©å½“å‰æ ‡è®°ã€‚è¿™å°±æ˜¯å€¼çš„ä½œç”¨ã€‚**å®ƒä»£è¡¨äº†æ¯ä¸ªå•è¯çš„å†…å®¹ã€‚**'
- en: How are Q, K, V generated?
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Qã€Kã€Væ˜¯å¦‚ä½•ç”Ÿæˆçš„ï¼Ÿ
- en: '![](../Images/a2579e468f9c1df6324091ff86705c7f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2579e468f9c1df6324091ff86705c7f.png)'
- en: Q, K, V generation, image by author
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Qã€Kã€Vç”Ÿæˆï¼Œä½œè€…æä¾›çš„å›¾åƒ
- en: The similarity between a query and a key is a dot product (measures the similarity
    between 2 vectors), divided by the standard deviation of this random variable,
    to have everything normalized.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥è¯¢ä¸é”®ä¹‹é—´çš„ç›¸ä¼¼æ€§æ˜¯ç‚¹ç§¯ï¼ˆæµ‹é‡ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼‰ï¼Œé™¤ä»¥è¯¥éšæœºå˜é‡çš„æ ‡å‡†å·®ï¼Œä»¥ä¾¿ä¸€åˆ‡æ ‡å‡†åŒ–ã€‚
- en: '![](../Images/a0751dcbf8f0088625ba917b7a7dbeeb.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0751dcbf8f0088625ba917b7a7dbeeb.png)'
- en: Attention formula, Image from [article](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›å…¬å¼ï¼Œå›¾ç‰‡æ¥æºäº[æ–‡ç« ](https://arxiv.org/pdf/1706.03762.pdf)
- en: 'Letâ€™s illustrate this with an example:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç¤ºä¾‹æ¥è¯´æ˜ï¼š
- en: 'Letâ€™s image we have one query, and want to figure the result of the attention
    with K and V:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾æƒ³æˆ‘ä»¬æœ‰ä¸€ä¸ªæŸ¥è¯¢ï¼Œå¹¶ä¸”æƒ³è¦è®¡ç®—ä¸Kå’ŒVçš„æ³¨æ„åŠ›ç»“æœï¼š
- en: '![](../Images/ea25d0ec9ac6bef4ab16db1a428ca6cc.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea25d0ec9ac6bef4ab16db1a428ca6cc.png)'
- en: Q, K, V, Image by author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Qã€Kã€Vï¼Œä½œè€…æä¾›çš„å›¾åƒ
- en: 'Now letâ€™s compute the similarities between q1 and the keys:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è®¡ç®—q1å’Œé”®ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼š
- en: '![](../Images/5b1d818e83c98702b71576756c495c6b.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b1d818e83c98702b71576756c495c6b.png)'
- en: Dot Product, Image by author
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹ç§¯ï¼Œä½œè€…æä¾›çš„å›¾åƒ
- en: While the numbers 3/2 and 1/8 might seem relatively close, the softmax functionâ€™s
    exponential nature would amplify their difference.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ•°å­—3/2å’Œ1/8çœ‹èµ·æ¥ç›¸å¯¹æ¥è¿‘ï¼Œä½†softmaxå‡½æ•°çš„æŒ‡æ•°ç‰¹æ€§ä¼šæ”¾å¤§å®ƒä»¬ä¹‹é—´çš„å·®å¼‚ã€‚
- en: '![](../Images/188dd4d52274b8982de5d9af84882131.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/188dd4d52274b8982de5d9af84882131.png)'
- en: Attention weights, Image by author
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æƒé‡ï¼Œä½œè€…æä¾›çš„å›¾åƒ
- en: This differential suggests that q1 has a more pronounced connection to k1 than
    k2.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå·®å¼‚è¡¨æ˜q1ä¸k1çš„è”ç³»æ¯”ä¸k2çš„è”ç³»æ›´ä¸ºæ˜¾è‘—ã€‚
- en: Now letâ€™s look at the result of attention, which is a weighted (attention weights)
    combination of the values
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹æ³¨æ„åŠ›çš„ç»“æœï¼Œå®ƒæ˜¯å€¼çš„åŠ æƒï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰ç»„åˆã€‚
- en: '![](../Images/606d749bd87cd8e042826277d90de621.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/606d749bd87cd8e042826277d90de621.png)'
- en: Attention, Image by author
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›ï¼Œä½œè€…æä¾›çš„å›¾åƒ
- en: Great! Repeating this operation for every token (q1 through qn) yields a collection
    of *n* vectors.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å¤ªæ£’äº†ï¼å¯¹æ¯ä¸ªæ ‡è®°ï¼ˆq1åˆ°qnï¼‰é‡å¤è¿™ä¸ªæ“ä½œä¼šå¾—åˆ°ä¸€ä¸ª*n*ä¸ªå‘é‡çš„é›†åˆã€‚
- en: In practice this operation is vectorized into a matrix multiplication for more
    effectiveness.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œè¿™ä¸ªæ“ä½œè¢«çŸ¢é‡åŒ–ä¸ºçŸ©é˜µä¹˜æ³•ï¼Œä»¥æé«˜æ•ˆç‡ã€‚
- en: 'Letâ€™s code it:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥ç¼–å†™ä»£ç ï¼š
- en: '[PRE2]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Multi-Headed Attention
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šå¤´æ³¨æ„åŠ›
- en: Whatâ€™s the Issue with Single-Headed Attention?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å•å¤´æ³¨æ„åŠ›æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ
- en: With the single-headed attention approach, every token gets to pose **just one
    query.** This generally translates to it deriving a strong relationship with just
    one other token, given that the softmax tends to **heavily weigh one value while
    diminishing others close to zero.** Yet, when you think about language and sentence
    structures, **a single word often has connections to multiple other words, not
    just one.**
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å•å¤´æ³¨æ„åŠ›æ–¹æ³•æ—¶ï¼Œæ¯ä¸ªæ ‡è®°åªèƒ½æå‡º **ä¸€ä¸ªæŸ¥è¯¢ã€‚** è¿™é€šå¸¸æ„å‘³ç€å®ƒåªèƒ½ä¸å¦ä¸€ä¸ªæ ‡è®°å»ºç«‹å¼ºå…³ç³»ï¼Œå› ä¸º softmax å€¾å‘äº **é‡åº¦åŠ æƒä¸€ä¸ªå€¼ï¼ŒåŒæ—¶å°†å…¶ä»–å€¼å‹ç¼©åˆ°æ¥è¿‘é›¶ã€‚**
    ç„¶è€Œï¼Œå½“ä½ è€ƒè™‘è¯­è¨€å’Œå¥å­ç»“æ„æ—¶ï¼Œ**ä¸€ä¸ªå•è¯é€šå¸¸ä¸å¤šä¸ªå…¶ä»–å•è¯æœ‰å…³ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªã€‚**
- en: To tackle this limitation, we introduce **multi-headed attention**. The core
    idea? Letâ€™s allow each token to pose multiple questions (queries) simultaneously
    by running the attention process in parallel for â€˜hâ€™ times. The original Transformer
    uses 8 heads.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº† **å¤šå¤´æ³¨æ„åŠ›**ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿè®©æˆ‘ä»¬å…è®¸æ¯ä¸ªæ ‡è®°åŒæ—¶æå‡ºå¤šä¸ªé—®é¢˜ï¼ˆæŸ¥è¯¢ï¼‰ï¼Œé€šè¿‡å¹¶è¡Œè¿›è¡Œâ€˜hâ€™æ¬¡æ³¨æ„åŠ›è¿‡ç¨‹ã€‚åŸå§‹ Transformer
    ä½¿ç”¨äº†8ä¸ªå¤´ã€‚
- en: '![](../Images/87353cc6805b860d5489fc7d9ac85ed1.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87353cc6805b860d5489fc7d9ac85ed1.png)'
- en: Multi-Headed attention, image from [article](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå¤´æ³¨æ„åŠ›ï¼Œå›¾åƒæ¥è‡ª[æ–‡ç« ](https://arxiv.org/pdf/1706.03762.pdf)
- en: Once we get the results of the 8 heads, we concatenate them into a matrix.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬å¾—åˆ°8ä¸ªå¤´çš„ç»“æœï¼Œå°±å°†å®ƒä»¬è¿æ¥æˆä¸€ä¸ªçŸ©é˜µã€‚
- en: '![](../Images/a3bb101441fa44f16c78c014742a49e1.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3bb101441fa44f16c78c014742a49e1.png)'
- en: Multi-Headed attention, image from article
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå¤´æ³¨æ„åŠ›ï¼Œå›¾åƒæ¥è‡ª[æ–‡ç« ](https://arxiv.org/pdf/1706.03762.pdf)
- en: 'This is also straightforward to code, we just have to be careful with the dimensions:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç ä¹Ÿå¾ˆç®€å•ï¼Œæˆ‘ä»¬åªéœ€æ³¨æ„ç»´åº¦ï¼š
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You should start to understand why Transformers are so powerful now, they exploit
    parallelism to the fullest.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ åº”è¯¥å¼€å§‹ç†è§£ä¸ºä»€ä¹ˆ Transformers å¦‚æ­¤å¼ºå¤§ï¼Œå®ƒä»¬å……åˆ†åˆ©ç”¨äº†å¹¶è¡Œæ€§ã€‚
- en: Assembling the pieces of the Transformer
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»„è£… Transformer çš„å„ä¸ªéƒ¨åˆ†
- en: 'On the high-level, a Transformer is the combination of 3 elements: an **Encoder**,
    a **Decoder**, and a **Generator**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼ŒTransformer æ˜¯ç”±ä¸‰ä¸ªå…ƒç´ ç»„æˆçš„ï¼š**ç¼–ç å™¨**ã€**è§£ç å™¨**å’Œ**ç”Ÿæˆå™¨**
- en: '![](../Images/05ffe38e98c719a712414f6d0d062ceb.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05ffe38e98c719a712414f6d0d062ceb.png)'
- en: Endoder, Decoder, Generator, Image from article modified by author
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Endoderã€Decoderã€Generatorï¼Œå›¾åƒæ¥è‡ªæ–‡ç« ï¼Œä½œè€…ä¿®æ”¹
- en: '**1\. The Encoder**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. ç¼–ç å™¨**'
- en: 'Purpose: Convert an input sequence into a new sequence (usually of smaller
    dimension) that captures the essence of the original data.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®çš„ï¼šå°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºä¸€ä¸ªæ–°çš„åºåˆ—ï¼ˆé€šå¸¸ç»´åº¦è¾ƒå°ï¼‰ï¼Œä»¥æ•æ‰åŸå§‹æ•°æ®çš„ç²¾é«“ã€‚
- en: 'Note: If youâ€™ve heard of the **BERT** model, it uses just this encoding part
    of the Transformer.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå¦‚æœä½ å¬è¯´è¿‡ **BERT** æ¨¡å‹ï¼Œå®ƒåªä½¿ç”¨ Transformer çš„ç¼–ç éƒ¨åˆ†ã€‚
- en: '**2\. The Decoder**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. è§£ç å™¨**'
- en: 'Purpose: Generate an output sequence using the encoded sequence from the Encoder.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®çš„ï¼šä½¿ç”¨ç¼–ç å™¨ç¼–ç çš„åºåˆ—ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚
- en: 'Note: The decoder in the Transformer is different from the typical autoencoderâ€™s
    decoder. In the Transformer, **the decoder not only looks at the encoded output
    but also considers the tokens it has generated so far.**'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šTransformer ä¸­çš„è§£ç å™¨ä¸åŒäºå…¸å‹è‡ªç¼–ç å™¨çš„è§£ç å™¨ã€‚åœ¨ Transformer ä¸­ï¼Œ**è§£ç å™¨ä¸ä»…æŸ¥çœ‹ç¼–ç åçš„è¾“å‡ºï¼Œè¿˜è€ƒè™‘äº†å®ƒè¿„ä»Šä¸ºæ­¢ç”Ÿæˆçš„æ ‡è®°ã€‚**
- en: '**3\. The Generator**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. ç”Ÿæˆå™¨**'
- en: 'Purpose: Convert a vector to a token. It does this by projecting the vector
    to the size of the vocabulary and then picking the most likely token with the
    softmax function.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®çš„ï¼šå°†å‘é‡è½¬æ¢ä¸ºæ ‡è®°ã€‚å®ƒé€šè¿‡å°†å‘é‡æŠ•å½±åˆ°è¯æ±‡è¡¨çš„å¤§å°ï¼Œç„¶åä½¿ç”¨softmaxå‡½æ•°é€‰æ‹©æœ€å¯èƒ½çš„æ ‡è®°æ¥å®ç°ã€‚
- en: 'Letâ€™s code that:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥ç¼–å†™ä»£ç ï¼š
- en: '[PRE4]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'One remark here: â€œsrcâ€ refers to the input sequence, and â€œtargetâ€ refers to
    the sequence being generated. Remember that we generate the output in an autoregressive
    manner, token by token, so we need to keep track of the target sequence as well.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªå¤‡æ³¨ï¼šâ€œsrcâ€æŒ‡çš„æ˜¯è¾“å…¥åºåˆ—ï¼Œâ€œtargetâ€æŒ‡çš„æ˜¯æ­£åœ¨ç”Ÿæˆçš„åºåˆ—ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬ä»¥è‡ªå›å½’çš„æ–¹å¼ç”Ÿæˆè¾“å‡ºï¼Œä¸€ä¸ªæ ‡è®°ä¸€ä¸ªæ ‡è®°ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è·Ÿè¸ªç›®æ ‡åºåˆ—ã€‚
- en: Stacking Encoders
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å †å ç¼–ç å™¨
- en: 'The Transformerâ€™s Encoder isnâ€™t just one layer. Itâ€™s actually a stack of *N*
    layers. Specifically:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer çš„ç¼–ç å™¨ä¸ä»…ä»…æ˜¯ä¸€å±‚ã€‚å®ƒå®é™…ä¸Šæ˜¯ä¸€ä¸ªç”± *N* å±‚ç»„æˆçš„å †æ ˆã€‚å…·ä½“æ¥è¯´ï¼š
- en: Encoder in the original Transformer model consists of a stack of *N=6* identical
    layers.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸå§‹ Transformer æ¨¡å‹ä¸­çš„ç¼–ç å™¨ç”± *N=6* ä¸ªç›¸åŒçš„å±‚ç»„æˆã€‚
- en: 'Inside the Encoder layer, we can see that there are two Sublayer blocks which
    are very similar ((1) and (2)): A **residual connection followed by a layer norm.**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¼–ç å™¨å±‚ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ‰ä¸¤ä¸ªSublayerå—éå¸¸ç›¸ä¼¼ï¼ˆï¼ˆ1ï¼‰å’Œï¼ˆ2ï¼‰ï¼‰ï¼š**æ®‹å·®è¿æ¥åè·Ÿå±‚å½’ä¸€åŒ–**ã€‚
- en: '**Block (1) Self-Attention Mechanism:** Helps the encoder focus on different
    words in the input when generating the encoded representation.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å—ï¼ˆ1ï¼‰è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šå¸®åŠ©ç¼–ç å™¨åœ¨ç”Ÿæˆç¼–ç è¡¨ç¤ºæ—¶å…³æ³¨è¾“å…¥ä¸­çš„ä¸åŒè¯æ±‡ã€‚'
- en: '**Block (2) Feed-Forward Neural Network:** A small neural network applied independently
    to each position.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å—ï¼ˆ2ï¼‰å‰é¦ˆç¥ç»ç½‘ç»œ**ï¼šä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œç‹¬ç«‹åº”ç”¨äºæ¯ä¸ªä½ç½®ã€‚'
- en: '![](../Images/63659de9ad14fe9b7d83ee2d23d61c80.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63659de9ad14fe9b7d83ee2d23d61c80.png)'
- en: Encoder Layer, residual connections, and Layer Norm,Image from article modified
    by author
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨å±‚ï¼Œæ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ï¼Œå›¾ç‰‡æ¥è‡ªæ–‡ç« ï¼Œä½œè€…ä¿®æ”¹
- en: 'Now letâ€™s code that:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ç¼–å†™è¿™ä¸ªï¼š
- en: 'SublayerConnection first:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆæ˜¯SublayerConnectionï¼š
- en: We follow the general architecture, and we can change â€œsublayerâ€ by either â€œself-attentionâ€
    or â€œFFNâ€
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éµå¾ªé€šç”¨æ¶æ„ï¼Œå¯ä»¥å°†â€œsublayerâ€æ›´æ”¹ä¸ºâ€œself-attentionâ€æˆ–â€œFFNâ€ã€‚
- en: '[PRE5]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we can define the full Encoder layer:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å®šä¹‰å®Œæ•´çš„ç¼–ç å™¨å±‚ï¼š
- en: '[PRE6]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The Encoder Layer is ready, now letâ€™s just chain them together to form the
    full Encoder:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨å±‚å·²å‡†å¤‡å¥½ï¼Œç°åœ¨å°†å®ƒä»¬é“¾æ¥åœ¨ä¸€èµ·å½¢æˆå®Œæ•´çš„ç¼–ç å™¨ï¼š
- en: '[PRE7]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Decoder
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è§£ç å™¨
- en: The Decoder, just like the Encoder, is structured with multiple identical layers
    stacked on top of each other. The number of these layers is typically 6 in the
    original Transformer model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç å™¨ä¸ç¼–ç å™¨ä¸€æ ·ï¼Œç”±å¤šä¸ªç›¸åŒçš„å±‚å †å è€Œæˆã€‚è¿™äº›å±‚çš„æ•°é‡åœ¨åŸå§‹Transformeræ¨¡å‹ä¸­é€šå¸¸ä¸º6ã€‚
- en: How is the Decoder different from the Encoder?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç å™¨ä¸ç¼–ç å™¨æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ
- en: 'A third SubLayer is added to interact with the encoder: this is **Cross-Attention**'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ äº†ç¬¬ä¸‰ä¸ªSubLayerä¸ç¼–ç å™¨è¿›è¡Œäº¤äº’ï¼šè¿™æ˜¯**äº¤å‰æ³¨æ„åŠ›**ã€‚
- en: SubLayer (1) is the same as the Encoder. Itâ€™s the **Self-Attention** mechanism,
    meaning that we generate everything (Q, K, V) from the tokens fed into the Decoder
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SubLayer (1) ä¸ç¼–ç å™¨ç›¸åŒã€‚è¿™æ˜¯**è‡ªæ³¨æ„åŠ›**æœºåˆ¶ï¼Œæ„å‘³ç€æˆ‘ä»¬ä»è¾“å…¥åˆ°è§£ç å™¨çš„æ ‡è®°ä¸­ç”Ÿæˆä¸€åˆ‡ï¼ˆQã€Kã€Vï¼‰ã€‚
- en: 'SubLayer (2) is the new communication mechanism: **Cross-Attention.** It is
    called that way because we use the **output from (1) to generate the Queries**,
    and we use the **output from the Encoder to generate the Keys and Values (K, V)**.
    In other words, to generate a sentence we have to look both at what we have generated
    so far by the Decoder (self-attention), and what we asked in the first place in
    the Encoder (cross-attention)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SubLayer (2) æ˜¯æ–°çš„é€šä¿¡æœºåˆ¶ï¼š**äº¤å‰æ³¨æ„åŠ›**ã€‚ä¹‹æ‰€ä»¥è¿™æ ·ç§°å‘¼ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬ä½¿ç”¨**ï¼ˆ1ï¼‰çš„è¾“å‡ºç”ŸæˆæŸ¥è¯¢**ï¼Œå¹¶ä½¿ç”¨**ç¼–ç å™¨çš„è¾“å‡ºç”Ÿæˆé”®å’Œå€¼ï¼ˆKï¼ŒVï¼‰**ã€‚æ¢å¥è¯è¯´ï¼Œä¸ºäº†ç”Ÿæˆä¸€ä¸ªå¥å­ï¼Œæˆ‘ä»¬å¿…é¡»åŒæ—¶æŸ¥çœ‹è§£ç å™¨åˆ°ç›®å‰ä¸ºæ­¢ç”Ÿæˆçš„å†…å®¹ï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰å’Œç¼–ç å™¨æœ€åˆè¦æ±‚çš„å†…å®¹ï¼ˆäº¤å‰æ³¨æ„åŠ›ï¼‰ã€‚
- en: SubLayer (3) is identical as in the Encoder.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SubLayer (3) ä¸ç¼–ç å™¨ä¸­çš„ç›¸åŒã€‚
- en: '![](../Images/a2d11ce51d4babcc3c8f2a513e853b14.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2d11ce51d4babcc3c8f2a513e853b14.png)'
- en: Decoder Layer, self attention, cross attention, Image from article modified
    by author
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç å™¨å±‚ï¼Œè‡ªæ³¨æ„åŠ›ï¼Œäº¤å‰æ³¨æ„åŠ›ï¼Œå›¾ç‰‡æ¥è‡ªæ–‡ç« ï¼Œä½œè€…ä¿®æ”¹
- en: Now letâ€™s code the DecoderLayer. If you understood the mechanism in the EncoderLayer,
    this should be quite straightforward.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ç¼–å†™DecoderLayerçš„ä»£ç ã€‚å¦‚æœä½ ç†è§£äº†EncoderLayerä¸­çš„æœºåˆ¶ï¼Œè¿™åº”è¯¥å¾ˆç®€å•ã€‚
- en: '[PRE8]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And now we can chain the N=6 DecoderLayers to form the Decoder:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†N=6çš„DecoderLayersé“¾æ¥èµ·æ¥å½¢æˆè§£ç å™¨ï¼š
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'At this point you have understood around 90% of what a Transformer is. There
    are still a few details:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ å·²ç»äº†è§£äº†Transformerçš„90%ã€‚è¿˜æœ‰ä¸€äº›ç»†èŠ‚ï¼š
- en: Transformer Model Details
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformeræ¨¡å‹è¯¦ç»†ä¿¡æ¯
- en: 'Padding:'
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¡«å……ï¼š
- en: In a typical transformer, thereâ€™s a maximum length for sequences (e.g., â€œmax_len=5000â€).
    This defines the **longest sequence the model can handle.**
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å…¸å‹çš„Transformerä¸­ï¼Œåºåˆ—æœ‰ä¸€ä¸ªæœ€å¤§é•¿åº¦ï¼ˆä¾‹å¦‚ï¼Œâ€œmax_len=5000â€ï¼‰ã€‚è¿™å®šä¹‰äº†**æ¨¡å‹å¯ä»¥å¤„ç†çš„æœ€é•¿åºåˆ—**ã€‚
- en: However, real-world sentences can vary in length. To handle shorter sentences,
    we use padding.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„å¥å­é•¿åº¦å¯ä»¥æœ‰æ‰€ä¸åŒã€‚ä¸ºäº†å¤„ç†è¾ƒçŸ­çš„å¥å­ï¼Œæˆ‘ä»¬ä½¿ç”¨å¡«å……ã€‚
- en: Padding is the addition of special â€œpadding tokensâ€ to make all sequences in
    a batch the same length.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¡«å……æ˜¯æ·»åŠ ç‰¹æ®Šçš„â€œå¡«å……æ ‡è®°â€ï¼Œä»¥ä½¿æ‰¹é‡ä¸­çš„æ‰€æœ‰åºåˆ—é•¿åº¦ç›¸åŒã€‚
- en: '![](../Images/4f36cb8ce900c84c7cdbd19bcd3c08ec.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f36cb8ce900c84c7cdbd19bcd3c08ec.png)'
- en: Padding, image by author
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å¡«å……ï¼Œå›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Masking
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ©ç 
- en: Masking ensures that during the attention computation, certain tokens are ignored.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æ©ç ç¡®ä¿åœ¨æ³¨æ„åŠ›è®¡ç®—æœŸé—´ï¼ŒæŸäº›æ ‡è®°è¢«å¿½ç•¥ã€‚
- en: 'Two scenarios for masking:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ç§æ©ç åœºæ™¯ï¼š
- en: '**src_masking**: Since weâ€™ve added padding tokens to sequences, we donâ€™t want
    the model to pay attention to these meaningless tokens. Hence, we mask them out.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**src_masking**ï¼šç”±äºæˆ‘ä»¬å·²å‘åºåˆ—ä¸­æ·»åŠ äº†å¡«å……æ ‡è®°ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›æ¨¡å‹å…³æ³¨è¿™äº›æ— æ„ä¹‰çš„æ ‡è®°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å®ƒä»¬æ©è”½æ‰ã€‚'
- en: '**tgt_masking** or **Look-Ahead/Causal Masking**: In the decoder, when generating
    tokens sequentially, each token should only be influenced by previous tokens and
    not future ones. For instance, when generating the 5th word in a sentence, it
    shouldnâ€™t know about the 6th word. This ensures a sequential generation of tokens.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tgt_masking** æˆ– **å‰ç»/å› æœæ©ç **ï¼šåœ¨è§£ç å™¨ä¸­ï¼Œç”Ÿæˆæ ‡è®°æ—¶ï¼Œæ¯ä¸ªæ ‡è®°åº”ä»…å—ä¹‹å‰æ ‡è®°çš„å½±å“ï¼Œè€Œä¸å—æœªæ¥æ ‡è®°çš„å½±å“ã€‚ä¾‹å¦‚ï¼Œåœ¨ç”Ÿæˆå¥å­çš„ç¬¬5ä¸ªè¯æ—¶ï¼Œå®ƒä¸åº”äº†è§£ç¬¬6ä¸ªè¯ã€‚è¿™ç¡®ä¿äº†æ ‡è®°çš„é¡ºåºç”Ÿæˆã€‚'
- en: '![](../Images/b6b33c8914fe205aad88c533215a33b0.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6b33c8914fe205aad88c533215a33b0.png)'
- en: Causal Masking/Look-Ahead masking, image by author
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: å› æœæ©ç /å‰ç»æ©ç ï¼Œå›¾ç‰‡æ¥æºä½œè€…
- en: 'We then use this mask to add minus infinity so that the corresponding token
    is ignored. This example should clarify things:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ©ç æ¥æ·»åŠ è´Ÿæ— ç©·å¤§ï¼Œä»¥ä½¿ç›¸åº”çš„æ ‡è®°è¢«å¿½ç•¥ã€‚è¿™ä¸ªä¾‹å­åº”è¯¥èƒ½æ¾„æ¸…ä¸€äº›é—®é¢˜ï¼š
- en: '![](../Images/0a6265d3ffcbae569aca4b8d88dc0a87.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a6265d3ffcbae569aca4b8d88dc0a87.png)'
- en: Masking, a trick in the softmax, image by author
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: æ©ç ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨ softmax ä¸­çš„æŠ€å·§ï¼Œå›¾ç‰‡æ¥æºä½œè€…
- en: 'FFN: Feed Forward Network'
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FFNï¼šå‰é¦ˆç½‘ç»œ
- en: The â€œFeed Forwardâ€ layer in the Transformerâ€™s diagram is a tad misleading. Itâ€™s
    not just one operation, but a sequence of them.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer å›¾ä¸­çš„â€œå‰é¦ˆâ€å±‚æœ‰äº›è¯¯å¯¼ã€‚å®ƒä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ“ä½œï¼Œè€Œæ˜¯ä¸€ä¸ªæ“ä½œåºåˆ—ã€‚
- en: The FFN consists of two linear layers. Interestingly, the input data, which
    might be of dimension `d_model=512`, is first transformed into a higher dimension
    `d_ff=2048` and then mapped back to its original dimension (`d_model=512`).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FFN ç”±ä¸¤ä¸ªçº¿æ€§å±‚ç»„æˆã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¾“å…¥æ•°æ®ï¼Œå¯èƒ½ä¸ºç»´åº¦ `d_model=512`ï¼Œé¦–å…ˆè¢«è½¬æ¢ä¸ºæ›´é«˜ç»´åº¦ `d_ff=2048`ï¼Œç„¶åå†æ˜ å°„å›å…¶åŸå§‹ç»´åº¦ï¼ˆ`d_model=512`ï¼‰ã€‚
- en: This can be visualized as the data being â€œexpandedâ€ in the middle of the operation
    before being â€œcompressedâ€ back to its original size.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥è¢«å¯è§†åŒ–ä¸ºæ•°æ®åœ¨æ“ä½œä¸­é—´è¢«â€œæ‰©å±•â€ï¼Œç„¶åå†â€œå‹ç¼©â€å›å…¶åŸå§‹å¤§å°ã€‚
- en: '![](../Images/a7be031e77a2b0e6be3d5a27316900c5.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7be031e77a2b0e6be3d5a27316900c5.png)'
- en: Image from article modified by author
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥è‡ªæ–‡ç« ï¼Œå·²ç”±ä½œè€…ä¿®æ”¹
- en: 'This is easy to code:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆå®¹æ˜“ç¼–ç ï¼š
- en: '[PRE10]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Conclusion
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: 'The unparalleled success and popularity of the Transformer model can be attributed
    to several key factors:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer æ¨¡å‹çš„æ— ä¸ä¼¦æ¯”çš„æˆåŠŸå’Œå—æ¬¢è¿ç¨‹åº¦å¯ä»¥å½’å› äºå‡ ä¸ªå…³é”®å› ç´ ï¼š
- en: '**Flexibility.** Transformers can work with any sequence of vectors. These
    vectors can be embeddings for words. It is easy to transpose this to Computer
    Vision by converting an image to different patches, and unfolding a patch into
    a vector. Or even in Audio, we can split an audio into different pieces and vectorize
    them.'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**çµæ´»æ€§**ï¼šTransformers å¯ä»¥å¤„ç†ä»»ä½•åºåˆ—çš„å‘é‡ã€‚è¿™äº›å‘é‡å¯ä»¥æ˜¯è¯çš„åµŒå…¥ã€‚é€šè¿‡å°†å›¾åƒè½¬æ¢ä¸ºä¸åŒçš„è¡¥ä¸ï¼Œå¹¶å°†è¡¥ä¸å±•å¼€ä¸ºå‘é‡ï¼Œå¯ä»¥è½»æ¾åœ°å°†å…¶è½¬åŒ–ä¸ºè®¡ç®—æœºè§†è§‰ã€‚ç”šè‡³åœ¨éŸ³é¢‘ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†éŸ³é¢‘æ‹†åˆ†ä¸ºä¸åŒçš„ç‰‡æ®µå¹¶è¿›è¡Œå‘é‡åŒ–ã€‚'
- en: '**Generality**: With minimal inductive bias, the Transformer is **free** to
    capture intricate and nuanced patterns in data, thereby enabling it to learn and
    **generalize better.**'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€šç”¨æ€§**ï¼šTransformer ç”±äºæœ€å°çš„å½’çº³åå·®ï¼Œ**èƒ½å¤Ÿ**æ•æ‰æ•°æ®ä¸­çš„å¤æ‚å’Œå¾®å¦™çš„æ¨¡å¼ï¼Œä»è€Œä½¿å…¶èƒ½å¤Ÿ**æ›´å¥½åœ°å­¦ä¹ å’Œæ³›åŒ–**ã€‚'
- en: '**Speed & Efficiency:** Leveraging the immense computational power of GPUs,
    Transformers are designed for **parallel processing.**'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€Ÿåº¦ä¸æ•ˆç‡**ï¼šåˆ©ç”¨ GPU çš„å·¨å¤§è®¡ç®—èƒ½åŠ›ï¼ŒTransformers è®¾è®¡ä¸º**å¹¶è¡Œå¤„ç†**ã€‚'
- en: 'Thanks for reading! Before you go:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼åœ¨ä½ ç¦»å¼€ä¹‹å‰ï¼š
- en: You can run the experiments with my Transformer Github Repository.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä½¿ç”¨æˆ‘çš„ Transformer GitHub ä»“åº“è¿è¡Œå®éªŒã€‚
- en: For more awesome tutorials, check my [compilation of AI tutorials](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
    on Github
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬²è·å–æ›´å¤šç²¾å½©æ•™ç¨‹ï¼Œè¯·æŸ¥çœ‹æˆ‘çš„ [AI æ•™ç¨‹åˆé›†](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
    åœ¨ GitHub ä¸Š
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----c9f214c619ac--------------------------------)
    [## GitHub â€” FrancoisPorcher/awesome-ai-tutorials: The best collection of AI tutorials
    to make you aâ€¦'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub â€” FrancoisPorcher/awesome-ai-tutorials: The best collection of AI
    tutorials to make you aâ€¦](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----c9f214c619ac--------------------------------)'
- en: The best collection of AI tutorials to make you a boss of Data Science! â€” GitHub
    â€¦
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ€ä½³çš„ AI æ•™ç¨‹åˆé›†ï¼Œè®©ä½ æˆä¸ºæ•°æ®ç§‘å­¦é¢†åŸŸçš„é«˜æ‰‹ï¼â€” GitHub â€¦
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----c9f214c619ac--------------------------------)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----c9f214c619ac--------------------------------)'
- en: Y*ou should get my articles in your inbox.* [***Subscribe here.***](https://medium.com/@francoisporcher/subscribe)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä½ åº”è¯¥è®¢é˜…æˆ‘çš„æ–‡ç« åˆ°ä½ çš„æ”¶ä»¶ç®±ã€‚* [***åœ¨è¿™é‡Œè®¢é˜…ã€‚***](https://medium.com/@francoisporcher/subscribe)'
- en: '*If you want to have access to premium articles on Medium, you only need a
    membership for $5 a month. If you sign up* [***with my link***](https://medium.com/@francoisporcher/membership)*,
    you support me with a part of your fee without additional costs.*'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ æƒ³è®¿é—® Medium ä¸Šçš„ä¼˜è´¨æ–‡ç« ï¼Œåªéœ€æ¯æœˆ $5 è®¢é˜…ä¼šå‘˜ã€‚å¦‚æœä½ æ³¨å†Œ* [***é€šè¿‡æˆ‘çš„é“¾æ¥***](https://medium.com/@francoisporcher/membership)*ï¼Œä½ åœ¨ä¸å¢åŠ é¢å¤–è´¹ç”¨çš„æƒ…å†µä¸‹ï¼Œç”¨ä½ çš„ä¸€éƒ¨åˆ†è´¹ç”¨æ”¯æŒæˆ‘ã€‚*'
- en: If you found this article insightful and beneficial, please consider following
    me and leaving a clap for more in-depth content! Your support helps me continue
    producing content that aids our collective understanding.
  id: totrans-202
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰æ´å¯ŸåŠ›ä¸”æœ‰ç›Šï¼Œè¯·è€ƒè™‘å…³æ³¨æˆ‘å¹¶ç•™ä¸‹æŒå£°ä»¥è·å–æ›´å¤šæ·±å…¥å†…å®¹ï¼ä½ çš„æ”¯æŒå¸®åŠ©æˆ‘ç»§ç»­åˆ¶ä½œæœ‰åŠ©äºæˆ‘ä»¬é›†ä½“ç†è§£çš„å†…å®¹ã€‚
- en: References
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: '[Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)'
- en: '[The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
    (a good portion of the code is inspired from their blog post)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ³¨é‡Š Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)ï¼ˆå¤§éƒ¨åˆ†ä»£ç çµæ„Ÿæ¥è‡ªä»–ä»¬çš„åšå®¢æ–‡ç« ï¼‰'
- en: '[Andrej Karpathy Stanford lecture](https://www.youtube.com/watch?v=L4DC7e6g2iI)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Andrej Karpathy æ–¯å¦ç¦è®²åº§](https://www.youtube.com/watch?v=L4DC7e6g2iI)'
- en: To go further
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥æ¢ç´¢
- en: 'Even with a comprehensive guide, there are many other areas linked with Transformers.
    Here are some ideas you may want to explore:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æœ‰å…¨é¢çš„æŒ‡å—ï¼Œä»æœ‰è®¸å¤šä¸ Transformers ç›¸å…³çš„å…¶ä»–é¢†åŸŸã€‚è¿™é‡Œæœ‰ä¸€äº›ä½ å¯èƒ½æƒ³æ¢ç´¢çš„æƒ³æ³•ï¼š
- en: '**Positional Encoding:** significant improvements have been made, you may want
    to check â€œRelative positional Encodingâ€ and â€œRotary Positional Embedding (RoPE)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä½ç½®ç¼–ç ï¼š** å·²ç»å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä½ å¯èƒ½æƒ³äº†è§£â€œç›¸å¯¹ä½ç½®ç¼–ç â€å’Œâ€œæ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰â€'
- en: '**Layer Norm**, and the differences with batch norm, group norm'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å±‚å½’ä¸€åŒ–**ï¼Œä»¥åŠä¸æ‰¹å½’ä¸€åŒ–ã€ç»„å½’ä¸€åŒ–çš„åŒºåˆ«'
- en: '**Residual connections**, and their effect on smoothing the gradient'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ®‹å·®è¿æ¥**åŠå…¶å¯¹æ¢¯åº¦å¹³æ»‘çš„å½±å“'
- en: Improvements made to BERT (Roberta, ELECTRA, Camembert)
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹ BERT çš„æ”¹è¿›ï¼ˆRobertaã€ELECTRAã€Camembertï¼‰
- en: '**Distillation** of large models into smaller models'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°†å¤§æ¨¡å‹è’¸é¦ä¸ºå°æ¨¡å‹**'
- en: Applications of Transformers in other domains (mainly vision and audio)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers åœ¨å…¶ä»–é¢†åŸŸçš„åº”ç”¨ï¼ˆä¸»è¦æ˜¯è§†è§‰å’ŒéŸ³é¢‘ï¼‰
- en: The link between Transformers and Graph Neural Networks
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers ä¸å›¾ç¥ç»ç½‘ç»œä¹‹é—´çš„è”ç³»
