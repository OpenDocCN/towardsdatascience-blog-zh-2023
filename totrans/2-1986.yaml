- en: 'The A-Z of Transformers: Everything You Need to Know'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器的A到Z：你需要知道的一切
- en: 原文：[https://towardsdatascience.com/the-a-z-of-transformers-everything-you-need-to-know-c9f214c619ac](https://towardsdatascience.com/the-a-z-of-transformers-everything-you-need-to-know-c9f214c619ac)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-a-z-of-transformers-everything-you-need-to-know-c9f214c619ac](https://towardsdatascience.com/the-a-z-of-transformers-everything-you-need-to-know-c9f214c619ac)
- en: Everything you need to know about Transformers, and how to implement them
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你需要了解的关于变换器的一切，以及如何实现它们
- en: '[](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)
    ·16 min read·Oct 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)
    ·16分钟阅读·2023年10月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/be68574b989226c3e6ac62c933050cf5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be68574b989226c3e6ac62c933050cf5.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者
- en: Why another tutorial on Transformers?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么还要另写一篇关于变换器的教程？
- en: You have probably already heard of Transformers, and everyone talks about it,
    so why making a new article about it?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经听说过变换器，每个人都在谈论它，那么为什么还要再写一篇关于它的文章呢？
- en: Well, I am a researcher, and this requires me to have a very deep understanding
    of the tools I use (because if you don’t understand them, how can you identify
    where they are wrong and how you can improve them, right?).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我是一名研究员，这要求我对我使用的工具有非常深入的理解（因为如果你不理解它们，你怎么能识别它们的问题以及如何改进它们，对吧？）。
- en: As I ventured deeper into the world of Transformers, I found myself buried under
    a mountain of resources. And yet, despite all that reading, I was left with a
    general sense of the architecture and a trail of lingering questions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当我深入探索变换器的世界时，我发现自己埋在一堆资源下。尽管如此，尽管阅读了这么多，我仍然对架构有一个大致的了解，并留下了一连串悬而未决的问题。
- en: In this guide, I aim to bridge that knowledge gap. A guide that will give you
    a strong intuition on Transformers, a deep dive into the architecture, and the
    implementation from scratch.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本指南中，我旨在弥补这种知识差距。一个将给你提供对变换器的强直觉、对架构的深入探讨以及从零开始实现的指南。
- en: 'I strongly advise you to follow the code on [Github](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议你关注[Github](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch)上的代码：
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch?source=post_page-----c9f214c619ac--------------------------------)
    [## awesome-ai-tutorials/NLP/007 - Transformers From Scratch at main ·…'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch?source=post_page-----c9f214c619ac--------------------------------)
    [## awesome-ai-tutorials/NLP/007 - Transformers From Scratch at main ·…'
- en: The best collection of AI tutorials to make you a boss of Data Science! - awesome-ai-tutorials/NLP/007
    - Transformers…
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳AI教程合集，让你成为数据科学的专家！ - awesome-ai-tutorials/NLP/007 - Transformers…
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch?source=post_page-----c9f214c619ac--------------------------------)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch?source=post_page-----c9f214c619ac--------------------------------)
- en: Enjoy! 🤗
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 享受吧！🤗
- en: 'A little bit of History first:'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 首先一点历史：
- en: Many attribute the concept of the attention mechanism to the renowned paper
    “Attention is All You Need” by the Google Brain team. However, **this is only
    part of the story.**
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人将注意力机制的概念归功于Google Brain团队的著名论文“Attention is All You Need”。然而，**这只是部分事实**。
- en: The roots of the attention mechanism can be traced back to an earlier paper
    titled “[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)”
    authored by Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的根源可以追溯到一篇早期的论文，题为“[Neural Machine Translation by Jointly Learning to Align
    and Translate](https://arxiv.org/abs/1409.0473)”，由Dzmitry Bahdanau、KyungHyun Cho和Yoshua
    Bengio撰写。
- en: Bahdanau’s primary challenge was addressing the limitations of Recurrent Neural
    Networks (RNNs). Specifically, when encoding lengthy sentences into vectors using
    RNNs, **crucial information was often lost.**
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Bahdanau的主要挑战是解决循环神经网络（RNN）的局限性。具体来说，在使用RNN将长句编码为向量时，**关键的信息经常丢失。**
- en: Drawing parallels from translation exercises — where one often revisits the
    source sentence while translating — Bahdanau aimed to allocate weights to the
    hidden states within the RNN. This approach yielded **impressive outcomes**, and
    is depicted in the following diagram.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从翻译练习中汲取的经验——在翻译时经常会重新审视源句——Bahdanau的目标是为RNN中的隐藏状态分配权重。这种方法取得了**令人印象深刻的结果**，如下图所示。
- en: '![](../Images/021d4865ae1f5167428caeb2caea1723.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/021d4865ae1f5167428caeb2caea1723.png)'
- en: Image from [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于[Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf)
- en: 'However, Bahdanau wasn’t the only one tackling this issue. Taking cues from
    his groundbreaking work, the Google Brain team posited a bold idea:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Bahdanau并不是唯一一个解决这个问题的人。借鉴了他的开创性工作，Google Brain团队提出了一个大胆的想法：
- en: “Why not strip everything down and focus solely on the attention mechanism?”
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “为什么不去掉一切，只专注于注意力机制呢？”
- en: They believed it wasn’t the RNN but the attention mechanism that was the primary
    driver behind the success.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 他们认为，成功的主要驱动因素不是RNN，而是注意力机制。
- en: This conviction culminated in their paper, aptly titled “Attention is All You
    Need”.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信念最终在他们的论文中得到了体现，这篇论文恰如其分地命名为“Attention is All You Need”。
- en: Fascinating, right?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 很吸引人，对吧？
- en: The Transformer Architecture
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer架构
- en: 1\. First things first, Embeddings
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 首先，嵌入
- en: This diagram represents the Transformer architecture. Don’t worry if you don’t
    understand anything at first, we will cover absolutely everything.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表示了Transformer架构。不要担心如果你一开始不理解任何东西，我们会涵盖所有内容。
- en: '![](../Images/dcfe23dc844b3b9aa870462b629be554.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcfe23dc844b3b9aa870462b629be554.png)'
- en: Embeddings, Image from article modified by author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入，文章中的图片经过作者修改
- en: 'From Text to Vectors — The Embedding Process: Imagine our input is a sequence
    of words, say “The cat drinks milk”. This sequence has a length termed as `seq_len`.
    Our immediate task is to convert these words into a form that the model can understand,
    specifically vectors. That''s where the Embedder comes in.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本到向量——嵌入过程：假设我们的输入是一个单词序列，比如“猫喝牛奶”。这个序列的长度称为`seq_len`。我们立即要做的就是将这些单词转换成模型可以理解的形式，具体来说，就是向量。这就是Embedder的作用。
- en: Each word undergoes a transformation to become a vector. This transformation
    process is termed as ‘embedding’. Each of these vectors or ‘embeddings’ has a
    size of `d_model = 512`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每个词都会经历一个变换，成为一个向量。这个变换过程被称为“嵌入”。这些向量或“嵌入”的大小为`d_model = 512`。
- en: Now, what exactly is this Embedder? At its core, the Embedder is a linear mapping
    (matrix), denoted by `E`. You can visualize it as a matrix of size `(d_model,
    vocab_size)`, where `vocab_size` is the size of our vocabulary.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这个Embedder到底是什么呢？从本质上讲，Embedder是一个线性映射（矩阵），用`E`表示。你可以将其视为一个大小为`(d_model,
    vocab_size)`的矩阵，其中`vocab_size`是我们词汇表的大小。
- en: After the embedding process, we end up with a collection of vectors of size
    `d_model` each. It’s crucial to understand this format, as it’s a recurrent theme
    — you’ll see it across various stages like encoder input, encoder output, and
    so on.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入过程之后，我们得到一组大小为`d_model`的向量。理解这种格式至关重要，因为这是一个反复出现的主题——你会在编码器输入、编码器输出等各个阶段看到它。
- en: 'Let’s code this part:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写这一部分代码：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note: we multiply by *d_model* for normalization purposes (explained later)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注：我们乘以*d_model*是为了归一化（稍后解释）
- en: 'Note 2: I personally wondered if we used a pre-trained embedder, or at least
    start from a pre-trained one and fine-tune it. **But no, the embedding is fully
    learned from scratch and initialized randomly.**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注2：我个人想知道我们是否使用了预训练的嵌入器，或者至少从一个预训练的嵌入器开始并进行微调。**但不，嵌入完全是从头学习的，并且是随机初始化的。**
- en: Positional Encoding
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 位置编码
- en: Why Do We Need Positional Encoding?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要位置编码？
- en: 'Given our current setup, we possess a list of vectors representing words. If
    fed as-is to a transformer model, there’s a **key element missing: the sequential
    order of words.** Words in natural languages often derive meaning from their position.
    “John loves Mary” carries a different sentiment from “Mary loves John.” To ensure
    our model captures this order, we introduce Positional Encoding.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的设置中，我们拥有一个表示单词的向量列表。如果按原样输入到变换器模型中，有一个 **关键要素缺失：单词的顺序。** 自然语言中的单词往往从其位置中获取意义。“John
    loves Mary”和“Mary loves John”表达的情感不同。为了确保我们的模型捕捉到这种顺序，我们引入了位置编码。
- en: 'Now, you might wonder, “Why not just add a simple increment like +1 for the
    first word, +2 for the second, and so on?” There are several challenges with this
    approach:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能会想，“为什么不简单地对第一个词加 +1，对第二个词加 +2，以此类推呢？”这种方法有几个挑战：
- en: '**Multidimensionality:** Each token is represented in 512 dimensions. A mere
    increment would not suffice to capture this complex space.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多维度性：** 每个令牌在 512 维度中表示。简单的增量无法捕捉这个复杂的空间。'
- en: '**Normalization Concerns:** Ideally, we want our values to lie between -1 and
    1\. So, directly adding large numbers (like +2000 for a long text) would be problematic.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**归一化问题：** 理想情况下，我们希望值在 -1 和 1 之间。因此，直接添加大数值（例如，长文本中的 +2000）会带来问题。'
- en: '**Sequence Length Dependency:** Using direct increments is not scale-agnostic.
    For a long text, where the position might be +5000, this number does **not truly
    reflect the relative position** of the token in its associated sentence. And **the
    meaning of a world depends more on its relative position in a sentence, than its
    absolute position in a text.**'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**序列长度依赖性：** 直接增量的使用不是尺度无关的。对于长文本，其中位置可能是 +5000，这个数字并 **不能真正反映令牌在其关联句子中的相对位置**。而且
    **一个词的意义更依赖于它在句子中的相对位置，而非在文本中的绝对位置。**'
- en: If you studied mathematics, the idea of circular coordinates — **specifically,
    sine and cosine functions** — should resonate with your intuition. These functions
    provide a unique way to encode position that meets our needs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你学习过数学，圆形坐标的概念 — **特别是正弦和余弦函数** — 应该与你的直觉相契合。这些函数提供了一种独特的方式来编码位置，以满足我们的需求。
- en: Given our matrix of size `(seq_len, d_model)`, our aim is to add another matrix,
    the Positional Encoding, of the same size.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 给定我们大小为 `(seq_len, d_model)` 的矩阵，我们的目标是添加另一个相同大小的矩阵，即位置编码。
- en: 'Here’s the core concept:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是核心概念：
- en: For every token, the authors suggest providing a **sine** coordinate of the
    pairwise dimensions (2k) a **cosine** coordinate to (2k+1).
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个令牌，作者建议提供一对维度（2k）的 **正弦** 坐标和 (2k+1) 的 **余弦** 坐标。
- en: If we fix the token position, and we move the dimension, we can see that the
    sine/cosine decrease in frequency
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们固定令牌位置，并移动维度，我们可以看到正弦/余弦频率的下降
- en: If we look at a token that is further in the text, this phenomenon happens more
    rapidly (the frequency is increased)
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们查看文本中较远的令牌，这种现象会更迅速地发生（频率增加）
- en: '![](../Images/f46bdcaafa0040d4f409a5db8908caf7.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f46bdcaafa0040d4f409a5db8908caf7.png)'
- en: Image from article
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 文章中的图片
- en: This is summed up in the following graph (but don’t scratch your head too much
    on this). The Key take away is that Positional Encoding is a mathematical function
    that allows the Transformer to keep an idea of the order of tokens in the sentence.
    This is a very active area or research.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下图中得到了总结（但不必太费脑筋）。关键点是位置编码是一个数学函数，它允许变换器保持对句子中令牌顺序的理解。这是一个非常活跃的研究领域。
- en: '![](../Images/18a0ef4eaab2b74148416745751809cf.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18a0ef4eaab2b74148416745751809cf.png)'
- en: Positional Embedding, Image by author
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 位置嵌入，作者提供的图片
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Attention Mechanism (Single Head)
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力机制（单头）
- en: 'Let’s dive into the core concept of Google’s paper: the Attention Mechanism'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解谷歌论文的核心概念：注意力机制
- en: 'High-Level Intuition:'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级直觉：
- en: At its core, the attention mechanism is a **communication mechanism between
    vectors/tokens.** It allows a model to focus on specific parts of the input when
    producing an output. Think of it as shining a spotlight on certain parts of your
    input data. This “spotlight” can be brighter on more relevant parts (giving them
    more attention) and dimmer on less relevant parts.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，注意力机制是 **向量/令牌之间的通信机制**。它允许模型在生成输出时专注于输入的特定部分。可以把它看作是对输入数据的某些部分进行聚焦的“聚光灯”。这个“聚光灯”可以在更相关的部分更亮（给予更多关注），在不太相关的部分更暗。
- en: For a sentence, attention helps determine the relationship between words. Some
    words are closely related to each other in meaning or function within a sentence,
    while others are not. **The attention mechanism quantifies these relationships.**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个句子，注意力有助于确定单词之间的关系。某些单词在句子中在意义或功能上紧密相关，而其他单词则不然。**注意力机制量化了这些关系。**
- en: 'Example:'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：
- en: 'Consider the sentence: “She gave him her book.”'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑句子：“She gave him her book.”
- en: 'If we focus on the word “her”, the attention mechanism might determine that:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们关注单词“her”，注意力机制可能会确定：
- en: It has a strong connection with “book” because “her” is indicating possession
    of the “book”.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它与“book”有很强的联系，因为“her”表示对“book”的所有权。
- en: It has a medium connection with “She” because “She” and “her” likely refer to
    the same entity.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它与“She”有中等的联系，因为“She”和“her”可能指的是同一个实体。
- en: It has a weaker connection with other words like “gave” or “him”.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它与其他词如“gave”或“him”的联系较弱。
- en: Technical Dive into the Attention mechanism
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解注意力机制
- en: '![](../Images/963c9565bade1e73604dfc61e9593e68.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/963c9565bade1e73604dfc61e9593e68.png)'
- en: Scaled Dot-Product Attention, image from [article](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放点积注意力，图片来源于[文章](https://arxiv.org/pdf/1706.03762.pdf)
- en: '**For each token, we generate three vectors:**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于每个标记，我们生成三个向量：**'
- en: '**Query (Q):**'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Query (Q)：**'
- en: '**Intuition**: Think of the query as a “**question**” that a token poses. It
    represents the current word and tries to find out which parts of the sequence
    are relevant to it.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**直觉**：将查询视为一个标记提出的“**问题**”。它代表当前的单词，并尝试找出序列中哪些部分与其相关。'
- en: '**2\. Key (K):**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. Key (K)：**'
- en: '**Intuition**: The key can be thought of as an “**identifier**” for each word
    in the sequence. When the query “asks” its question, the key helps in “answering”
    by determining **how relevant each word in the sequence is to the query.**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**直觉**：可以将键视为序列中每个单词的“**标识符**”。当查询“提问”时，键通过确定**序列中每个单词与查询的相关性**来“回答”。'
- en: '**3\. Value (V):**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. Value (V)：**'
- en: '**Intuition**: Once the relevance of each word (via its key) to the query is
    determined, we need actual information or content from those words to assist the
    current token. This is where the value comes in. **It represents the content of
    each word.**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**直觉**：一旦确定了每个单词（通过其键）与查询的相关性，我们需要从这些单词中获取实际的信息或内容来辅助当前标记。这就是值的作用。**它代表了每个单词的内容。**'
- en: How are Q, K, V generated?
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q、K、V是如何生成的？
- en: '![](../Images/a2579e468f9c1df6324091ff86705c7f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2579e468f9c1df6324091ff86705c7f.png)'
- en: Q, K, V generation, image by author
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Q、K、V生成，作者提供的图像
- en: The similarity between a query and a key is a dot product (measures the similarity
    between 2 vectors), divided by the standard deviation of this random variable,
    to have everything normalized.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 查询与键之间的相似性是点积（测量两个向量之间的相似性），除以该随机变量的标准差，以便一切标准化。
- en: '![](../Images/a0751dcbf8f0088625ba917b7a7dbeeb.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0751dcbf8f0088625ba917b7a7dbeeb.png)'
- en: Attention formula, Image from [article](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力公式，图片来源于[文章](https://arxiv.org/pdf/1706.03762.pdf)
- en: 'Let’s illustrate this with an example:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例来说明：
- en: 'Let’s image we have one query, and want to figure the result of the attention
    with K and V:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 设想我们有一个查询，并且想要计算与K和V的注意力结果：
- en: '![](../Images/ea25d0ec9ac6bef4ab16db1a428ca6cc.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea25d0ec9ac6bef4ab16db1a428ca6cc.png)'
- en: Q, K, V, Image by author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Q、K、V，作者提供的图像
- en: 'Now let’s compute the similarities between q1 and the keys:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算q1和键之间的相似性：
- en: '![](../Images/5b1d818e83c98702b71576756c495c6b.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b1d818e83c98702b71576756c495c6b.png)'
- en: Dot Product, Image by author
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 点积，作者提供的图像
- en: While the numbers 3/2 and 1/8 might seem relatively close, the softmax function’s
    exponential nature would amplify their difference.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数字3/2和1/8看起来相对接近，但softmax函数的指数特性会放大它们之间的差异。
- en: '![](../Images/188dd4d52274b8982de5d9af84882131.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/188dd4d52274b8982de5d9af84882131.png)'
- en: Attention weights, Image by author
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力权重，作者提供的图像
- en: This differential suggests that q1 has a more pronounced connection to k1 than
    k2.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个差异表明q1与k1的联系比与k2的联系更为显著。
- en: Now let’s look at the result of attention, which is a weighted (attention weights)
    combination of the values
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看注意力的结果，它是值的加权（注意力权重）组合。
- en: '![](../Images/606d749bd87cd8e042826277d90de621.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/606d749bd87cd8e042826277d90de621.png)'
- en: Attention, Image by author
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力，作者提供的图像
- en: Great! Repeating this operation for every token (q1 through qn) yields a collection
    of *n* vectors.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！对每个标记（q1到qn）重复这个操作会得到一个*n*个向量的集合。
- en: In practice this operation is vectorized into a matrix multiplication for more
    effectiveness.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这个操作被矢量化为矩阵乘法，以提高效率。
- en: 'Let’s code it:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来编写代码：
- en: '[PRE2]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Multi-Headed Attention
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多头注意力
- en: What’s the Issue with Single-Headed Attention?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 单头注意力有什么问题？
- en: With the single-headed attention approach, every token gets to pose **just one
    query.** This generally translates to it deriving a strong relationship with just
    one other token, given that the softmax tends to **heavily weigh one value while
    diminishing others close to zero.** Yet, when you think about language and sentence
    structures, **a single word often has connections to multiple other words, not
    just one.**
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单头注意力方法时，每个标记只能提出 **一个查询。** 这通常意味着它只能与另一个标记建立强关系，因为 softmax 倾向于 **重度加权一个值，同时将其他值压缩到接近零。**
    然而，当你考虑语言和句子结构时，**一个单词通常与多个其他单词有关，而不仅仅是一个。**
- en: To tackle this limitation, we introduce **multi-headed attention**. The core
    idea? Let’s allow each token to pose multiple questions (queries) simultaneously
    by running the attention process in parallel for ‘h’ times. The original Transformer
    uses 8 heads.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一限制，我们引入了 **多头注意力**。核心思想是什么？让我们允许每个标记同时提出多个问题（查询），通过并行进行‘h’次注意力过程。原始 Transformer
    使用了8个头。
- en: '![](../Images/87353cc6805b860d5489fc7d9ac85ed1.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87353cc6805b860d5489fc7d9ac85ed1.png)'
- en: Multi-Headed attention, image from [article](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力，图像来自[文章](https://arxiv.org/pdf/1706.03762.pdf)
- en: Once we get the results of the 8 heads, we concatenate them into a matrix.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到8个头的结果，就将它们连接成一个矩阵。
- en: '![](../Images/a3bb101441fa44f16c78c014742a49e1.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3bb101441fa44f16c78c014742a49e1.png)'
- en: Multi-Headed attention, image from article
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力，图像来自[文章](https://arxiv.org/pdf/1706.03762.pdf)
- en: 'This is also straightforward to code, we just have to be careful with the dimensions:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码也很简单，我们只需注意维度：
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You should start to understand why Transformers are so powerful now, they exploit
    parallelism to the fullest.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该开始理解为什么 Transformers 如此强大，它们充分利用了并行性。
- en: Assembling the pieces of the Transformer
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组装 Transformer 的各个部分
- en: 'On the high-level, a Transformer is the combination of 3 elements: an **Encoder**,
    a **Decoder**, and a **Generator**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，Transformer 是由三个元素组成的：**编码器**、**解码器**和**生成器**
- en: '![](../Images/05ffe38e98c719a712414f6d0d062ceb.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05ffe38e98c719a712414f6d0d062ceb.png)'
- en: Endoder, Decoder, Generator, Image from article modified by author
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Endoder、Decoder、Generator，图像来自文章，作者修改
- en: '**1\. The Encoder**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 编码器**'
- en: 'Purpose: Convert an input sequence into a new sequence (usually of smaller
    dimension) that captures the essence of the original data.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：将输入序列转换为一个新的序列（通常维度较小），以捕捉原始数据的精髓。
- en: 'Note: If you’ve heard of the **BERT** model, it uses just this encoding part
    of the Transformer.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意：如果你听说过 **BERT** 模型，它只使用 Transformer 的编码部分。
- en: '**2\. The Decoder**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 解码器**'
- en: 'Purpose: Generate an output sequence using the encoded sequence from the Encoder.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：使用编码器编码的序列生成输出序列。
- en: 'Note: The decoder in the Transformer is different from the typical autoencoder’s
    decoder. In the Transformer, **the decoder not only looks at the encoded output
    but also considers the tokens it has generated so far.**'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意：Transformer 中的解码器不同于典型自编码器的解码器。在 Transformer 中，**解码器不仅查看编码后的输出，还考虑了它迄今为止生成的标记。**
- en: '**3\. The Generator**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 生成器**'
- en: 'Purpose: Convert a vector to a token. It does this by projecting the vector
    to the size of the vocabulary and then picking the most likely token with the
    softmax function.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：将向量转换为标记。它通过将向量投影到词汇表的大小，然后使用softmax函数选择最可能的标记来实现。
- en: 'Let’s code that:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来编写代码：
- en: '[PRE4]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'One remark here: “src” refers to the input sequence, and “target” refers to
    the sequence being generated. Remember that we generate the output in an autoregressive
    manner, token by token, so we need to keep track of the target sequence as well.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个备注：“src”指的是输入序列，“target”指的是正在生成的序列。请记住，我们以自回归的方式生成输出，一个标记一个标记，因此我们需要跟踪目标序列。
- en: Stacking Encoders
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠编码器
- en: 'The Transformer’s Encoder isn’t just one layer. It’s actually a stack of *N*
    layers. Specifically:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的编码器不仅仅是一层。它实际上是一个由 *N* 层组成的堆栈。具体来说：
- en: Encoder in the original Transformer model consists of a stack of *N=6* identical
    layers.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始 Transformer 模型中的编码器由 *N=6* 个相同的层组成。
- en: 'Inside the Encoder layer, we can see that there are two Sublayer blocks which
    are very similar ((1) and (2)): A **residual connection followed by a layer norm.**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器层中，我们可以看到有两个Sublayer块非常相似（（1）和（2））：**残差连接后跟层归一化**。
- en: '**Block (1) Self-Attention Mechanism:** Helps the encoder focus on different
    words in the input when generating the encoded representation.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**块（1）自注意力机制**：帮助编码器在生成编码表示时关注输入中的不同词汇。'
- en: '**Block (2) Feed-Forward Neural Network:** A small neural network applied independently
    to each position.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**块（2）前馈神经网络**：一个小型神经网络独立应用于每个位置。'
- en: '![](../Images/63659de9ad14fe9b7d83ee2d23d61c80.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63659de9ad14fe9b7d83ee2d23d61c80.png)'
- en: Encoder Layer, residual connections, and Layer Norm,Image from article modified
    by author
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器层，残差连接和层归一化，图片来自文章，作者修改
- en: 'Now let’s code that:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写这个：
- en: 'SublayerConnection first:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是SublayerConnection：
- en: We follow the general architecture, and we can change “sublayer” by either “self-attention”
    or “FFN”
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循通用架构，可以将“sublayer”更改为“self-attention”或“FFN”。
- en: '[PRE5]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we can define the full Encoder layer:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义完整的编码器层：
- en: '[PRE6]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The Encoder Layer is ready, now let’s just chain them together to form the
    full Encoder:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器层已准备好，现在将它们链接在一起形成完整的编码器：
- en: '[PRE7]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Decoder
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码器
- en: The Decoder, just like the Encoder, is structured with multiple identical layers
    stacked on top of each other. The number of these layers is typically 6 in the
    original Transformer model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器与编码器一样，由多个相同的层堆叠而成。这些层的数量在原始Transformer模型中通常为6。
- en: How is the Decoder different from the Encoder?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器与编码器有什么不同？
- en: 'A third SubLayer is added to interact with the encoder: this is **Cross-Attention**'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了第三个SubLayer与编码器进行交互：这是**交叉注意力**。
- en: SubLayer (1) is the same as the Encoder. It’s the **Self-Attention** mechanism,
    meaning that we generate everything (Q, K, V) from the tokens fed into the Decoder
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SubLayer (1) 与编码器相同。这是**自注意力**机制，意味着我们从输入到解码器的标记中生成一切（Q、K、V）。
- en: 'SubLayer (2) is the new communication mechanism: **Cross-Attention.** It is
    called that way because we use the **output from (1) to generate the Queries**,
    and we use the **output from the Encoder to generate the Keys and Values (K, V)**.
    In other words, to generate a sentence we have to look both at what we have generated
    so far by the Decoder (self-attention), and what we asked in the first place in
    the Encoder (cross-attention)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SubLayer (2) 是新的通信机制：**交叉注意力**。之所以这样称呼，是因为我们使用**（1）的输出生成查询**，并使用**编码器的输出生成键和值（K，V）**。换句话说，为了生成一个句子，我们必须同时查看解码器到目前为止生成的内容（自注意力）和编码器最初要求的内容（交叉注意力）。
- en: SubLayer (3) is identical as in the Encoder.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SubLayer (3) 与编码器中的相同。
- en: '![](../Images/a2d11ce51d4babcc3c8f2a513e853b14.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2d11ce51d4babcc3c8f2a513e853b14.png)'
- en: Decoder Layer, self attention, cross attention, Image from article modified
    by author
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器层，自注意力，交叉注意力，图片来自文章，作者修改
- en: Now let’s code the DecoderLayer. If you understood the mechanism in the EncoderLayer,
    this should be quite straightforward.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写DecoderLayer的代码。如果你理解了EncoderLayer中的机制，这应该很简单。
- en: '[PRE8]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And now we can chain the N=6 DecoderLayers to form the Decoder:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将N=6的DecoderLayers链接起来形成解码器：
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'At this point you have understood around 90% of what a Transformer is. There
    are still a few details:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经了解了Transformer的90%。还有一些细节：
- en: Transformer Model Details
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer模型详细信息
- en: 'Padding:'
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充：
- en: In a typical transformer, there’s a maximum length for sequences (e.g., “max_len=5000”).
    This defines the **longest sequence the model can handle.**
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在典型的Transformer中，序列有一个最大长度（例如，“max_len=5000”）。这定义了**模型可以处理的最长序列**。
- en: However, real-world sentences can vary in length. To handle shorter sentences,
    we use padding.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，现实世界的句子长度可以有所不同。为了处理较短的句子，我们使用填充。
- en: Padding is the addition of special “padding tokens” to make all sequences in
    a batch the same length.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充是添加特殊的“填充标记”，以使批量中的所有序列长度相同。
- en: '![](../Images/4f36cb8ce900c84c7cdbd19bcd3c08ec.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f36cb8ce900c84c7cdbd19bcd3c08ec.png)'
- en: Padding, image by author
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 填充，图片由作者提供
- en: Masking
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掩码
- en: Masking ensures that during the attention computation, certain tokens are ignored.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码确保在注意力计算期间，某些标记被忽略。
- en: 'Two scenarios for masking:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种掩码场景：
- en: '**src_masking**: Since we’ve added padding tokens to sequences, we don’t want
    the model to pay attention to these meaningless tokens. Hence, we mask them out.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**src_masking**：由于我们已向序列中添加了填充标记，我们不希望模型关注这些无意义的标记。因此，我们将它们掩蔽掉。'
- en: '**tgt_masking** or **Look-Ahead/Causal Masking**: In the decoder, when generating
    tokens sequentially, each token should only be influenced by previous tokens and
    not future ones. For instance, when generating the 5th word in a sentence, it
    shouldn’t know about the 6th word. This ensures a sequential generation of tokens.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tgt_masking** 或 **前瞻/因果掩码**：在解码器中，生成标记时，每个标记应仅受之前标记的影响，而不受未来标记的影响。例如，在生成句子的第5个词时，它不应了解第6个词。这确保了标记的顺序生成。'
- en: '![](../Images/b6b33c8914fe205aad88c533215a33b0.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6b33c8914fe205aad88c533215a33b0.png)'
- en: Causal Masking/Look-Ahead masking, image by author
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因果掩码/前瞻掩码，图片来源作者
- en: 'We then use this mask to add minus infinity so that the corresponding token
    is ignored. This example should clarify things:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用这个掩码来添加负无穷大，以使相应的标记被忽略。这个例子应该能澄清一些问题：
- en: '![](../Images/0a6265d3ffcbae569aca4b8d88dc0a87.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a6265d3ffcbae569aca4b8d88dc0a87.png)'
- en: Masking, a trick in the softmax, image by author
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码，这是一个在 softmax 中的技巧，图片来源作者
- en: 'FFN: Feed Forward Network'
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FFN：前馈网络
- en: The “Feed Forward” layer in the Transformer’s diagram is a tad misleading. It’s
    not just one operation, but a sequence of them.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 图中的“前馈”层有些误导。它不仅仅是一个操作，而是一个操作序列。
- en: The FFN consists of two linear layers. Interestingly, the input data, which
    might be of dimension `d_model=512`, is first transformed into a higher dimension
    `d_ff=2048` and then mapped back to its original dimension (`d_model=512`).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FFN 由两个线性层组成。有趣的是，输入数据，可能为维度 `d_model=512`，首先被转换为更高维度 `d_ff=2048`，然后再映射回其原始维度（`d_model=512`）。
- en: This can be visualized as the data being “expanded” in the middle of the operation
    before being “compressed” back to its original size.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可以被可视化为数据在操作中间被“扩展”，然后再“压缩”回其原始大小。
- en: '![](../Images/a7be031e77a2b0e6be3d5a27316900c5.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7be031e77a2b0e6be3d5a27316900c5.png)'
- en: Image from article modified by author
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自文章，已由作者修改
- en: 'This is easy to code:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这很容易编码：
- en: '[PRE10]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Conclusion
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'The unparalleled success and popularity of the Transformer model can be attributed
    to several key factors:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型的无与伦比的成功和受欢迎程度可以归因于几个关键因素：
- en: '**Flexibility.** Transformers can work with any sequence of vectors. These
    vectors can be embeddings for words. It is easy to transpose this to Computer
    Vision by converting an image to different patches, and unfolding a patch into
    a vector. Or even in Audio, we can split an audio into different pieces and vectorize
    them.'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**灵活性**：Transformers 可以处理任何序列的向量。这些向量可以是词的嵌入。通过将图像转换为不同的补丁，并将补丁展开为向量，可以轻松地将其转化为计算机视觉。甚至在音频中，我们可以将音频拆分为不同的片段并进行向量化。'
- en: '**Generality**: With minimal inductive bias, the Transformer is **free** to
    capture intricate and nuanced patterns in data, thereby enabling it to learn and
    **generalize better.**'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**通用性**：Transformer 由于最小的归纳偏差，**能够**捕捉数据中的复杂和微妙的模式，从而使其能够**更好地学习和泛化**。'
- en: '**Speed & Efficiency:** Leveraging the immense computational power of GPUs,
    Transformers are designed for **parallel processing.**'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**速度与效率**：利用 GPU 的巨大计算能力，Transformers 设计为**并行处理**。'
- en: 'Thanks for reading! Before you go:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！在你离开之前：
- en: You can run the experiments with my Transformer Github Repository.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用我的 Transformer GitHub 仓库运行实验。
- en: For more awesome tutorials, check my [compilation of AI tutorials](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
    on Github
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 欲获取更多精彩教程，请查看我的 [AI 教程合集](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
    在 GitHub 上
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----c9f214c619ac--------------------------------)
    [## GitHub — FrancoisPorcher/awesome-ai-tutorials: The best collection of AI tutorials
    to make you a…'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub — FrancoisPorcher/awesome-ai-tutorials: The best collection of AI
    tutorials to make you a…](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----c9f214c619ac--------------------------------)'
- en: The best collection of AI tutorials to make you a boss of Data Science! — GitHub
    …
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳的 AI 教程合集，让你成为数据科学领域的高手！— GitHub …
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----c9f214c619ac--------------------------------)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----c9f214c619ac--------------------------------)'
- en: Y*ou should get my articles in your inbox.* [***Subscribe here.***](https://medium.com/@francoisporcher/subscribe)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*你应该订阅我的文章到你的收件箱。* [***在这里订阅。***](https://medium.com/@francoisporcher/subscribe)'
- en: '*If you want to have access to premium articles on Medium, you only need a
    membership for $5 a month. If you sign up* [***with my link***](https://medium.com/@francoisporcher/membership)*,
    you support me with a part of your fee without additional costs.*'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你想访问 Medium 上的优质文章，只需每月 $5 订阅会员。如果你注册* [***通过我的链接***](https://medium.com/@francoisporcher/membership)*，你在不增加额外费用的情况下，用你的一部分费用支持我。*'
- en: If you found this article insightful and beneficial, please consider following
    me and leaving a clap for more in-depth content! Your support helps me continue
    producing content that aids our collective understanding.
  id: totrans-202
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章有洞察力且有益，请考虑关注我并留下掌声以获取更多深入内容！你的支持帮助我继续制作有助于我们集体理解的内容。
- en: References
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)'
- en: '[The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
    (a good portion of the code is inspired from their blog post)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[注释 Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)（大部分代码灵感来自他们的博客文章）'
- en: '[Andrej Karpathy Stanford lecture](https://www.youtube.com/watch?v=L4DC7e6g2iI)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Andrej Karpathy 斯坦福讲座](https://www.youtube.com/watch?v=L4DC7e6g2iI)'
- en: To go further
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步探索
- en: 'Even with a comprehensive guide, there are many other areas linked with Transformers.
    Here are some ideas you may want to explore:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有全面的指南，仍有许多与 Transformers 相关的其他领域。这里有一些你可能想探索的想法：
- en: '**Positional Encoding:** significant improvements have been made, you may want
    to check “Relative positional Encoding” and “Rotary Positional Embedding (RoPE)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置编码：** 已经取得了显著的改进，你可能想了解“相对位置编码”和“旋转位置嵌入（RoPE）”'
- en: '**Layer Norm**, and the differences with batch norm, group norm'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层归一化**，以及与批归一化、组归一化的区别'
- en: '**Residual connections**, and their effect on smoothing the gradient'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差连接**及其对梯度平滑的影响'
- en: Improvements made to BERT (Roberta, ELECTRA, Camembert)
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 BERT 的改进（Roberta、ELECTRA、Camembert）
- en: '**Distillation** of large models into smaller models'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将大模型蒸馏为小模型**'
- en: Applications of Transformers in other domains (mainly vision and audio)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers 在其他领域的应用（主要是视觉和音频）
- en: The link between Transformers and Graph Neural Networks
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers 与图神经网络之间的联系
