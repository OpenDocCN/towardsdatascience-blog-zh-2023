["```py\npip install darts\n```", "```py\nimport darts\n```", "```py\n# multiprocessing\nfrom joblib import Parallel, delayed\n\n# data manipulation\nimport numpy as np\nimport pandas as pd\nfrom darts import TimeSeries\nfrom darts.utils.timeseries_generation import datetime_attribute_timeseries\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\n# transformers and preprocessing\nfrom darts.dataprocessing.transformers import Scaler\n\n# models\nfrom darts.models import NaiveSeasonal, StatsForecastAutoARIMA, ExponentialSmoothing, Prophet #local\nfrom darts.models import LightGBMModel, RNNModel, NBEATSModel, TFTModel #global\n\n# likelihood\nfrom darts.utils.likelihood_models import GaussianLikelihood\n\n# evaluation\nfrom darts.metrics import mape\n\n# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport logging\nlogging.disable(logging.CRITICAL)\n```", "```py\ndataset = pd.read_csv('store_item_demand.csv')\n\n# set the column type for column with date\ndataset['date'] = pd.to_datetime(dataset['date'], format='%Y-%m-%d')\n# sort values and reset index\ndataset.sort_values(by=['date', 'store', 'item'], inplace=True)\ndataset.reset_index(drop=True, inplace=True)\n# creation of an auxiliary table with hierarchy and aggregated sales totals\nhierarchy_df = dataset.groupby(['store', 'item'])[['sales']].sum()\nhierarchy_df = hierarchy_df.reset_index(drop=False).sort_values(by=['sales'],\n  ascending=False).reset_index(drop=True)\n```", "```py\nfig, ax = plt.subplots(figsize=(30, 10))\n\nfor single_ts in list(np.arange(0, 10)) + list(np.arange(245, 255)) + list(np.arange(490, 500)):\n    single_ts_df = pd.merge(dataset, hierarchy_df.loc[[single_ts], ['store', 'item']], how='inner', on=['store', 'item'])\n    ax.plot(single_ts_df['date'], single_ts_df['sales'], color='black', alpha=0.25)\nax.set_xlim([dataset['date'].min(), dataset['date'].max()])\nax.set_ylim([0, dataset['sales'].max()])\nplt.show()\n```", "```py\n# make copy of df\ndataset_scaled_EDA = dataset.copy()\n\n# min max value calculation\ndataset_scaled_EDA['min_sales'] = dataset_scaled_EDA.groupby(['store', 'item'])['sales'].transform(lambda x: x.min())\ndataset_scaled_EDA['max_sales'] = dataset_scaled_EDA.groupby(['store', 'item'])['sales'].transform(lambda x: x.max())\n# scale\ndataset_scaled_EDA['sales_scaled'] = (dataset_scaled_EDA['sales'] - dataset_scaled_EDA['min_sales'])/(dataset_scaled_EDA['max_sales'] - dataset_scaled_EDA['min_sales'])\n# add info about year, week of year and day of week\ndataset_scaled_EDA['year'] = dataset_scaled_EDA['date'].dt.year\ndataset_scaled_EDA['month'] = dataset_scaled_EDA['date'].dt.month\ndataset_scaled_EDA['day_of_week'] = [d.strftime('%A') for d in dataset_scaled_EDA['date']]\ndataset_scaled_EDA['day_of_week'] = pd.Categorical(dataset_scaled_EDA['day_of_week'], \n  categories=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], \n  ordered=True)\n\n# visualize\nfig, ax = plt.subplots(1, 3, figsize=(30, 10))\nsns.boxplot(x='year', y='sales_scaled', data=dataset_scaled_EDA, ax=ax[0]).set(\n    xlabel='Year', \n    ylabel='Scaled Sales'\n)\nax[0].set_title('Box plot for years (trend)')\nsns.boxplot(x='month', y='sales_scaled', data=dataset_scaled_EDA, ax=ax[1]).set(\n    xlabel='Month', \n    ylabel='Scaled Sales'\n)\nax[1].set_title('Box plot for months (seasonality)')\nsns.boxplot(x='day_of_week', y='sales_scaled', data=dataset_scaled_EDA, ax=ax[2]).set(\n    xlabel='Day of week', \n    ylabel='Scaled Sales'\n)\nax[2].set_title('Box plot for day of week (seasonality)')\nax[2].set_xticklabels(ax[2].get_xticklabels(), rotation=30)\nplt.show()\n```", "```py\ndataset_ts = dataset.copy()\ndataset_ts = TimeSeries.from_group_dataframe(df=dataset_ts, \n                                             group_cols=['store', 'item'],\n                                             time_col='date', \n                                             value_cols='sales')\ndataset_ts\n```", "```py\nfirst_test_date = pd.Timestamp('2017-01-01')\ntrain_dataset_ts, test_dataset_ts = [], []\n\nfor single_ts in tqdm(dataset_ts):\n    # split into train and test tests\n    single_train_ts, single_test_ts = single_ts.split_before(first_test_date)\n    train_dataset_ts.append(single_train_ts)\n    test_dataset_ts.append(single_test_ts)\n```", "```py\nmodel.gridsearch(\n  parameters={}, \n  series= ,\n  start= , \n  forecast_horizon= ,\n  stride= ,\n  metric= \n)\n```", "```py\nbacktests_results = model.historical_forecasts(\n  series= , \n  start= ,\n  forecast_horizon= ,\n  retrain=True,\n  overlap_end=True,\n  last_points_only=False,\n)\n```", "```py\ndef take_backtest_horizon(backtests, horizon, last_horizon, observations):\n    backtests_horizon_dates = [i.time_index[-1+horizon] for i in backtests[last_horizon-horizon:][:observations]]\n    backtests_horizon_values = [i.values()[-1+horizon] for i in backtests[last_horizon-horizon:][:observations]]\n\n    backtests_horizon = TimeSeries.from_times_and_values(times=pd.DatetimeIndex(backtests_horizon_dates), values=np.array(backtests_horizon_values))\n    return backtests_horizon\n\nbacktests_5W = take_backtest_horizon(\n  backtests=backtests_results, \n  horizon=5, # for which horizon we want a forecast, the number must be less than forecast_horizon\n  last_horizon=forecast_horizon, # forecast_horizon from historical_forecasts method\n  observations=len(test_series))\n```", "```py\nscaler = Scaler() # MinMaxScaler\ntrain_dataset_ts_prepared = scaler.fit_transform(train_dataset_ts)\ntest_dataset_ts_prepared = scaler.transform(test_dataset_ts)\ndataset_ts_prepared = scaler.transform(dataset_ts)\n```", "```py\nforecast_horizons = len(test_dataset_ts[0])\n\ndef _backtests_local_estimator(_estimator, _ts_set, _split_date, _horizons, _single_forecast):\n    model = _estimator\n    if _single_forecast:\n        model.fit(_ts_set.split_before(_split_date)[0])\n        backtests_single_ts = model.predict(_horizons)\n\n    else:\n        backtests_single_ts = model.historical_forecasts(series=_ts_set, \n                                                         start=_split_date - np.timedelta64(_horizons-1, 'D'), \n                                                         verbose=False, \n                                                         overlap_end=False,\n                                                         last_points_only=True, \n                                                         forecast_horizon=_horizons,\n                                                         retrain=True)\n\n    return backtests_single_ts\n\ndef backtests_multiple_local_estimators(estimator, multiple_ts_sets=dataset_ts, split_date=first_test_date, horizons=forecast_horizons, single_forecast=True):\n    backtests_multiple_ts = Parallel(n_jobs=-1,\n                                     verbose=5, \n                                     backend = 'multiprocessing',\n                                     pre_dispatch='1.5*n_jobs')(\n            delayed(_backtests_local_estimator)(\n                _estimator=estimator,\n                _ts_set=single_ts_set,\n                _split_date=split_date,\n                _horizons=horizons,\n                _single_forecast=single_forecast\n            )\n        for single_ts_set in multiple_ts_sets\n    )\n\n    return backtests_multiple_ts\n```", "```py\ndef get_overall_MAPE(prediction_series, test_series=test_dataset_ts):\n    return np.round(np.mean(mape(actual_series=test_series, \n                                 pred_series=prediction_series, n_jobs=-1)),\n                    2)\n```", "```py\nbacktests_baseline_model = backtests_multiple_local_estimators(estimator=NaiveSeasonal(K=365))\nprint(f'overall MAPE: {get_overall_MAPE(backtests_baseline_model)}%')\n```", "```py\nbacktests_arima = backtests_multiple_local_estimators(estimator=StatsForecastAutoARIMA())\nprint(f'overall MAPE: {get_overall_MAPE(backtests_arima)}%')\n```", "```py\nfig, ax = plt.subplots(figsize=(30, 10))\ntest_dataset_ts[0].plot(label='True value', color='black')\nbacktests_arima[0].plot(label='Forecast', color='green')\nplt.show()\n```", "```py\nbacktests_exponential_smoothing  = backtests_multiple_local_estimators(estimator=ExponentialSmoothing())\nprint(f'overall MAPE: {get_overall_MAPE(backtests_exponential_smoothing)}%')\n```", "```py\nfig, ax = plt.subplots(figsize=(30, 10))\ntest_dataset_ts[0].plot(label='True value', color='black')\nbacktests_exponential_smoothing[0].plot(label='Forecast', color='green')\nplt.show()\n```", "```py\nbacktests_prophet = backtests_multiple_local_estimators(estimator=Prophet())\nprint(f'overall MAPE: {get_overall_MAPE(backtests_prophet)}%')\n```", "```py\nfig, ax = plt.subplots(figsize=(30, 10))\ntest_dataset_ts[0].plot(label='True value', color='black')\nbacktests_prophet[0].plot(label='Forecast', color='green')\nplt.show()\n```", "```py\nmodel_lightGBM = LightGBMModel(lags=14, \n                               output_chunk_length=365, \n                               random_state=0,\n                               multi_models=False, \n                               add_encoders={\"cyclic\": {\"future\": [\"month\"]}, \n                                             'datetime_attribute': {'future': ['dayofweek']},\n                                             'position': {'past': ['relative'], 'future': ['relative']}, \n                                             'transformer': Scaler()\n                 })\n\nmodel_lightGBM.fit(series=train_dataset_ts_prepared)\nbacktests_lightGBM = model_lightGBM.predict(n=forecast_horizons, series=train_dataset_ts_prepared)\n# The model predicts scaled values, but then we have to reverse the transformation\nbacktests_lightGBM = scaler.inverse_transform(backtests_lightGBM) # \nprint(f'overall MAPE: {get_overall_MAPE(backtests_lightGBM)}%')\n```", "```py\nfig, ax = plt.subplots(figsize=(30, 10))\ntest_dataset_ts[0].plot(label='True value', color='black')\nbacktests_lightGBM[0].plot(label='Forecast', color='green')\nplt.show()\n```", "```py\nday_series = datetime_attribute_timeseries(\n    dataset_ts[0], attribute=\"weekday\", one_hot=True, dtype=np.float32\n)\nmonth_series = datetime_attribute_timeseries(\n    dataset_ts[0], attribute=\"month\", one_hot=True, dtype=np.float32\n)\n\nday_month_series = day_series.concatenate(month_series, axis=1, ignore_static_covariates=True)\n\nmodel_deepar = RNNModel(\n    model=\"LSTM\",\n    hidden_dim=10,\n    n_rnn_layers=2,\n    dropout=0.2,\n    batch_size=32,\n    n_epochs=5,\n    optimizer_kwargs={\"lr\": 1e-3},\n    random_state=0,\n    training_length=21,\n    input_chunk_length=14,\n    likelihood=GaussianLikelihood(),\n    pl_trainer_kwargs={\n        \"accelerator\": \"gpu\",\n        \"devices\": [0]\n    }\n)\n\nmodel_deepar.fit(series=train_dataset_ts_prepared, future_covariates=[train_day_month]*len(train_dataset_ts_prepared), verbose=True)\n\nbacktests_deepar = model_deepar.predict(n=forecast_horizons, series=train_dataset_ts_prepared, \n                                        future_covariates=[day_month_series]*len(train_dataset_ts_prepared), \n                                        num_samples=1000, verbose=True)\nbacktests_deepar = scaler.inverse_transform(backtests_deepar)\nprint(f'overall MAPE: {get_overall_MAPE(backtests_deepar)}%')\n```", "```py\nfig, ax = plt.subplots(figsize=(30, 10))\ntest_dataset_ts[0].plot(label='True value', color='black')\nbacktests_deepar[0].plot(label='Forecast', color='green')\nplt.show()\n```", "```py\nmodel_nbeats = NBEATSModel(\n    input_chunk_length=178,\n    output_chunk_length=356,\n    generic_architecture=False,\n    num_blocks=3,\n    num_layers=4,\n    layer_widths=512,\n    n_epochs=10,\n    nr_epochs_val_period=1,\n    batch_size=800,\n    model_name=\"nbeats_interpretable_run\",\n    random_state=0,\n    pl_trainer_kwargs={\n      \"accelerator\": \"gpu\",\n      \"devices\": [0]\n    }\n)\n\nmodel_nbeats.fit(series=train_dataset_ts_prepared, verbose=True)\n\nbacktests_nbeats = model_nbeats.predict(n=forecast_horizons, series=train_dataset_ts_prepared, verbose=True)\nbacktests_nbeats = scaler.inverse_transform(backtests_nbeats)\nprint(f'overall MAPE: {get_overall_MAPE(backtests_nbeats)}%')\n```", "```py\nfig, ax = plt.subplots(figsize=(30, 10))\ntest_dataset_ts[0].plot(label='True value', color='black')\nbacktests_nbeats[0].plot(label='Forecast', color='green')\nplt.show()\n```", "```py\nmodel_tft = TFTModel(\n    input_chunk_length=28,\n    output_chunk_length=356,\n    hidden_size=16,\n    lstm_layers=1,\n    num_attention_heads=3,\n    dropout=0.1,\n    batch_size=32,\n    n_epochs=5,\n    add_encoders={\"cyclic\": {\"future\": [\"month\"]}, \n                  'datetime_attribute': {'future': ['dayofweek']},\n                  'position': {'past': ['relative'], 'future': ['relative']}, \n                  'transformer': Scaler()\n                 },\n    add_relative_index=False,\n    optimizer_kwargs={\"lr\": 1e-3},\n    random_state=0,\n    pl_trainer_kwargs={\n      \"accelerator\": \"gpu\",\n      \"devices\": [0]\n    }\n)\n\nmodel_tft.fit(series=train_dataset_ts_prepared, verbose=True)\n\nbacktests_tft = model_tft.predict(n=forecast_horizons, series=train_dataset_ts_prepared, \n                                  num_samples=1000, verbose=True)\nbacktests_tft = scaler.inverse_transform(backtests_tft)\nprint(f'overall MAPE: {get_overall_MAPE(backtests_tft)}%')\n```", "```py\nfig, ax = plt.subplots(figsize=(30, 10))\ntest_dataset_ts[0].plot(label='True value', color='black')\nbacktests_tft[0].plot(label='Forecast', color='green')\nplt.show()\n```"]