# 在编写 Apache Beam 管道时使用示例进行 Map、Filter 和 CombinePerKey 转换

> 原文：[https://towardsdatascience.com/map-filter-and-combineperkey-transforms-in-writing-apache-beam-pipelines-with-examples-e06926124a02](https://towardsdatascience.com/map-filter-and-combineperkey-transforms-in-writing-apache-beam-pipelines-with-examples-e06926124a02)

![](../Images/3c0bc71612c5632ec1b351c55f68aa03.png)

图片由 [JJ Ying](https://unsplash.com/@jjying?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

## 让我们用一些真实数据进行练习

[](https://rashida00.medium.com/?source=post_page-----e06926124a02--------------------------------)[![Rashida Nasrin Sucky](../Images/42bd057e8eca255907c43c29a498f2ca.png)](https://rashida00.medium.com/?source=post_page-----e06926124a02--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e06926124a02--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e06926124a02--------------------------------) [Rashida Nasrin Sucky](https://rashida00.medium.com/?source=post_page-----e06926124a02--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e06926124a02--------------------------------) ·8分钟阅读·2023年7月12日

--

Apache Beam 作为统一的编程模型，在高效和可移植的大数据处理管道中越来越受欢迎。它可以处理批量数据和流数据。这也是名字的由来。Beam 是 Batch 和 Stream 两个词的组合：

B（来自**B**atch）+ eam（来自 str**eam**）= Beam

便携性也是一个很棒的特性。你只需专注于运行管道，它可以在任何地方运行，例如 Spark、Flink、Apex 或 Cloud Dataflow。你不需要更改逻辑或语法。

在这篇文章中，我们将专注于学习如何通过示例编写一些ETL管道。我们将尝试使用一个好的数据集进行一些转换操作，希望你会发现这些转换操作在工作中也非常有用。

请随意下载这个[公共数据集](https://creativecommons.org/publicdomain/zero/1.0/)并跟随练习：

[示例销售数据 | Kaggle](https://www.kaggle.com/datasets/kyanyoga/sample-sales-data)

这个练习使用了 Google Colab notebook。因此，安装非常简单。只需使用这一行代码：

[PRE0]

安装完成后，我为这个练习创建了一个名为‘data’的目录：

[PRE1]

让我们深入探讨今天的话题——转换操作。首先，我们将处理一个最简单的管道，即读取CSV文件并将其写入文本文件。

这不像 Padas 的 read_csv() 方法那么简单。它需要一个 coder() 操作。首先，在这里定义了一个 CustomCoder() 类，该类首先将对象编码为字节字符串，然后将字节解码为其对应的对象，并最终指定这个 coder 是否保证对值进行确定性编码。请查看 [文档这里。](https://beam.apache.org/releases/pydoc/2.2.0/apache_beam.coders.coders.html)

如果这是你的第一个管道，请注意管道的语法。在 CustomCoder() 类之后是最简单的管道。我们首先将空管道初始化为‘p1’。然后我们编写了‘sales’管道，其中首先从我们之前创建的数据文件夹中读取 CSV 文件。在 Apache beam 中，管道中的每个转换操作都以 | 符号开始。读取 CSV 文件中的数据后，我们只是将其写入文本文件。最后，通过 run() 方法我们运行了管道。这是 Apache beam 中标准和常用的管道语法。

[PRE2]

如果你现在检查你的‘data’文件夹，你会看到一个‘output-00000-of-00001’文件。从这个文件中打印前 5 行以检查数据：

[PRE3]

输出：

[PRE4]

## Map

让我们来看一下如何在上述管道中使用 Map 转换。这是最常见的转换操作。你在 Map 中指定的转换将应用于 PCollection 中的每一个元素。

例如，我想添加一个 split 方法以从 PCollection 中的每个元素创建列表。在这里，我们将使用 lambda 进行 Map 转换。如果你不熟悉 lambda，请查看这里的 lambda 代码。lambda 后我们提到‘row’，任何其他变量名也可以。我们对‘row’应用的任何函数或方法将应用于 PCollection 中的每个元素。

[PRE5]

看，它是完全相同的语法。只是我在读取和写入操作之间多了一行代码。再次打印输出的前 5 行进行检查：

[PRE6]

输出：

[PRE7]

看，每个元素都变成了一个列表。

## Filter

接下来，我将把 Filter 转换也添加到上述代码块中。这里的 lambda 也可以用于过滤。我们将过滤掉所有数据，只保留 Produc line 中的‘经典汽车’数据。数据集的第 11 列是产品线。正如你所知，Python 是零索引的。所以，列号的计数也从零开始。

[PRE8]

如之前所示，打印前 5 行进行检查：

[PRE9]

输出：

[PRE10]

查看上面输出中每个列表的第 11 个元素。它是‘经典汽车’。

## 回答一些问题

> **每种类型的汽车订购了多少数量？**

为了找出这一点，我们首先将创建元组，其中第一个元素或键将来自数据集的第 11 个元素，第二个元素即值将是数据集的第二个元素，即‘订购数量’。在下一步中，我们将使用 CombinePerKey() 方法。正如名字所示，它将为每个键结合具有聚合函数的值。

当你看到代码时会更清楚。这里是代码。

[PRE11]

如你所见，我们在这里使用了两次 Map 函数。第一次是分割并像之前一样生成列表，然后从每行数据中提取第 10 列的产品线和第二列的数量。

这里是输出：

[PRE12]

只是打印了输出的前 10 行。如你所见，这里每行数据的订单数量都列出了。回答上述问题的下一步也是最后一步是将每项的所有值结合起来。Apache Beam 中有 CombinePerKey 方法可以实现。顾名思义，它会为每个键使用聚合函数来结合值。在这种情况下，我们需要的是“总和”。

[PRE13]

输出：

[PRE14]

所以，我们得到了每个产品的总订单数量。

> **哪些州的订单数量超过了 2000 个？**

这是一个有趣的问题，我们需要每一个之前做过的变换加上另一个过滤变换。我们需要计算每个州的总订单数量，就像在前面的例子中计算每个产品的总订单数量一样。然后，将数量超过 2000 的订单进行过滤。

在所有之前的例子中，lambda 函数被用于 Map 和 Filter 变换。这里我们将看到如何定义一个函数并在 Map 或 Filter 函数中使用它。这里定义了一个函数 quantity_filter()，它返回值数量大于 2000 的项。

[PRE15]

输出：

[PRE16]

这是输出，其中如果数量不超过 2000，则返回‘None’。我不喜欢看到所有这些‘None’值。我将添加另一个过滤变换来过滤掉这些‘None’值。

[PRE17]

输出：

[PRE18]

所以，我们总共有 5 个返回的值，其中订单数量大于 2000。

## 结论

在本教程中，我想展示如何在 Apache Beam 中使用 Map、Filter 和 CombinePerKey 变换来编写 ETL 管道。希望它们足够清晰，能够在你的项目中使用。我将在下一篇文章中解释如何使用 ParDo。

随意关注我 [Twitter](https://twitter.com/rashida048) 并点赞我的 [Facebook](https://www.facebook.com/rashida.smith.161) 页面。

## 相关阅读

[关于 Python 中多项式回归的详细教程、概述、实现和过拟合 | 作者：Rashida Nasrin Sucky | 2023年6月 | Towards AI](https://pub.towardsai.net/a-detailed-tutorial-on-polynomial-regression-in-python-overview-implementation-and-overfitting-e319fc7e5b8f)

[迷你 VGG 网络图像识别的完整实现 | 作者：Rashida Nasrin Sucky | Towards Data Science](/complete-implementation-of-a-mini-vgg-network-for-image-recognition-849299480356)

[如何在 TensorFlow 中定义自定义层、激活函数和损失函数 | 作者：Rashida Nasrin Sucky | Towards Data Science](/how-to-define-custom-layer-activation-function-and-loss-function-in-tensorflow-bdd7e78eb67)

[在 TensorFlow 中开发多输出模型的逐步教程 | 作者：拉希达·纳斯林·苏基 | 数据科学前沿](/a-step-by-step-tutorial-to-develop-a-multi-output-model-in-tensorflow-ec9f13e5979c)

[OpenCV Python 中的简单边缘检测方法 | 作者：拉希达·纳斯林·苏基 | 数据科学前沿](/easy-method-of-edge-detection-in-opencv-python-db26972deb2d)
