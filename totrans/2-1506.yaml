- en: 'Meet Gemini: Google’s Largest and Most Powerful AI Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/meet-gemini-googles-largest-and-most-powerful-ai-model-2ffd2f07490f](https://towardsdatascience.com/meet-gemini-googles-largest-and-most-powerful-ai-model-2ffd2f07490f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This next-gen AI model outperformed ChatGPT on almost all academic benchmarks.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://natassha6789.medium.com/?source=post_page-----2ffd2f07490f--------------------------------)[![Natassha
    Selvaraj](../Images/adea0c904ea1a62e8961d82e4d0dd643.png)](https://natassha6789.medium.com/?source=post_page-----2ffd2f07490f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2ffd2f07490f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2ffd2f07490f--------------------------------)
    [Natassha Selvaraj](https://natassha6789.medium.com/?source=post_page-----2ffd2f07490f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2ffd2f07490f--------------------------------)
    ·6 min read·Dec 9, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c9beca1db4c3e59c67d1b0bb75d5e28.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mitchell Luo](https://unsplash.com/@mitchel3uo?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/google-logo-neon-light-signage-jz4ca36oJ_M?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: When OpenAI released ChatGPT last November, there was a pressing question on
    everyone’s minds — what are the tech giants doing?
  prefs: []
  type: TYPE_NORMAL
- en: When will companies like Google respond to this development?
  prefs: []
  type: TYPE_NORMAL
- en: We now have our answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'On December 6, 2023, Google announced their latest AI model: Gemini.'
  prefs: []
  type: TYPE_NORMAL
- en: According to the company’s CEO Sundar Pichai, this technology is a huge leap
    forward in artificial intelligence, and will affect virtually all of Google’s
    products.
  prefs: []
  type: TYPE_NORMAL
- en: Gemini comes in 3 sizes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The current version of the model is called Gemini 1.0\. It can work with text,
    images, videos, and audio, and comes in 3 different sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Gemini Nano** is a small, more efficient version that can be run natively
    and on Android devices.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gemini Pro**, the middle tier version, strikes a balance between between
    capability and efficiency. This model performs significantly better than Google’s
    previous flagship model, PaLM-2\. It currently powers the Bard chatbot.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, **Gemini Ultra** is the most powerful model in the series. It excels
    at complex reasoning, and has outperformed OpenAI’s GPT-4 model on various benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Gemini Ultra hasn’t been made publicly available just yet. Google has announced
    that the model will be launched early next year, although no specific time-frame
    has been provided.
  prefs: []
  type: TYPE_NORMAL
- en: How does Gemini differ from OpenAI’s GPT Models?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI’s ChatGPT is currently powered by 2 AI models — GPT-3.5 for the free
    version and GPT-4 for the paid version.
  prefs: []
  type: TYPE_NORMAL
- en: A few months ago, OpenAI announced that GPT-4 had multimodal capabilities (i.e.
    it was able to [process text, audio, and images](https://openai.com/blog/chatgpt-can-now-see-hear-and-speak)).
  prefs: []
  type: TYPE_NORMAL
- en: However, although it can process various data types, the model’s primary design
    and functionality is focused on **text-based inputs and outputs**.
  prefs: []
  type: TYPE_NORMAL
- en: This means that GPT-4 is a text-based framework, and vision and audio processing
    models are built on top of it as a secondary stage.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For instance, combining **GPT-4** with an image generation model like **DALLE-3**
    allows you to translate text into an image that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6978ba99da710593480d920255bb0c3.png)'
  prefs: []
  type: TYPE_IMG
- en: While GPT-4’s training for processing other modalities is separate from its
    text-based training, Gemini is trained on a diverse dataset of text, images, videos,
    and audio from the start.
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, **multimodal capabilities are built into Gemini** from the
    ground up, to ensure that it natively understands all data types.
  prefs: []
  type: TYPE_NORMAL
- en: This architectural difference means that Gemini is able to generalize more easily,
    as it understands information from text, images, audio and video.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Models like GPT-4 and DALLE-3, on the other hand, are fine-tuned for **specific
    tasks** (GPT-4 for text, DALLE-3 for images).
  prefs: []
  type: TYPE_NORMAL
- en: 'In Google Deepmind’s research paper on Gemini, the following question is posed:'
  prefs: []
  type: TYPE_NORMAL
- en: One open question is whether this joint training can result in a model which
    has strong capabilities in each domain — even when compared to models and approaches
    that are narrowly tailored to single domains.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This question addresses a fundamental challenge in the field of AI: the tradeoff
    between specialized and generalized models.'
  prefs: []
  type: TYPE_NORMAL
- en: Models that are designed for a single domain usually perform better for that
    specific task, as compared to a model developed through joint training.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now compare Gemini’s performance against state-of-the-art models that
    have been tailored to specific domains.
  prefs: []
  type: TYPE_NORMAL
- en: Gemini outperforms ChatGPT in almost all benchmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Google Deepmind’s latest [report](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)
    on Gemini, the model’s performance is evaluated against other algorithms like
    Claude 2, PaLM-2, GPT-3.5, and GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how Gemini fares against the best models in various domains:'
  prefs: []
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Gemini’s performance on text benchmarks against existing models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14357c9ccd0e41831ef95cca25f2cf60.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Deepmind’s technical report](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: In a number of text-based benchmarks that cover reasoning abilities, reading
    comprehension, STEM, and coding, notice that Gemini Ultra outperforms OpenAI’s
    GPT-4 model in 8 out of 9 assessments.
  prefs: []
  type: TYPE_NORMAL
- en: According to the Google Deepmind report, the model performed best when a technique
    called “**chain-of-thought**” prompting was used.
  prefs: []
  type: TYPE_NORMAL
- en: In chain-of-thought prompting, you break down a problem and guide AI through
    a step-by-step reasoning process, similar to the way a human figures things out.
  prefs: []
  type: TYPE_NORMAL
- en: This tends to be more effective than simply throwing an entire question at an
    AI model.
  prefs: []
  type: TYPE_NORMAL
- en: You can read [this](https://www.promptingguide.ai/techniques/cot) guide to learn
    more about chain-of-thought prompting.
  prefs: []
  type: TYPE_NORMAL
- en: Also, notice how **Gemini Pro** (the middle-tier version), which currently powers
    Bard, appears to surpass GPT-3.5 in almost every benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: I have been experimenting with Gemini Pro ever since its release, and personally,
    find its responses to be on par with that of GPT-3.5.
  prefs: []
  type: TYPE_NORMAL
- en: I plan to perform a more detailed comparison between their reasoning and coding
    capabilities, and will publish a follow-up article soon.
  prefs: []
  type: TYPE_NORMAL
- en: Image Understanding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even without prior training on specific tasks, Gemini Ultra’s vision capabilities
    surpassed other models that were fine-tuned for those benchmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7bba453c75a96927269895677efe3d79.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Deepmind’s technical report](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: The first benchmark, [MMMU](https://mmmu-benchmark.github.io/), consists of
    college-level questions across 6 disciplines — business, science, humanities,
    art, tech, and medicine.
  prefs: []
  type: TYPE_NORMAL
- en: Not only does this benchmark demand strong reasoning skills and college-level
    expertise, but the questions are also **uniquely based on images**.
  prefs: []
  type: TYPE_NORMAL
- en: Answering these questions accurately isn’t easy since the model must do two
    things —interpret visual elements and perform complex textual analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Gemini Ultra surpassed all other AI models in this benchmark, highlighting its
    strong multimodal capabilities and the ability to generalize.
  prefs: []
  type: TYPE_NORMAL
- en: Video Understanding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gemini Ultra also achieved state-of-the-art performance in video-related tasks
    — it was able to **add captions** and **answer questions based on videos**.
  prefs: []
  type: TYPE_NORMAL
- en: For example, given [this](https://www.youtube.com/watch?v=VmWxjmJ3mvs) video
    of a person playing soccer, the model was asked to provide recommendations as
    to how the player could improve their technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is Gemini’s response upon analyzing the video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, Gemini Ultra demonstrated exceptional performance in image generation
    and audio understanding tasks. You can read the complete report [here](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: While the model’s performance seems promising on academic benchmarks, keep in
    mind that a 4–5% improvement in a research environment may not necessarily make
    any real-world impact.
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion, integrating AI models into other applications and productizing
    them for specific use-cases (like automated data analysis or enhanced search capabilities)
    is going to have a larger impact than a marginal performance improvement.
  prefs: []
  type: TYPE_NORMAL
- en: I’m looking forward to see how Google is going to integrate this new model into
    its suite of products, especially Google Analytics, to generate more advanced
    insights and predictions.
  prefs: []
  type: TYPE_NORMAL
- en: My biggest takeaway from the paper on Gemini is that a model built with multimodality
    at its core can surpass algorithms designed for specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This approach could be a significant step towards achieving **artificial general
    intelligence** (AGI), where the goal is to build an AI model that can apply its
    intelligence to a broad range of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how you can get started with Gemini
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier in this article, you can start using Gemini Pro today by
    accessing the [Bard chatbot](https://bard.google.com/chat).
  prefs: []
  type: TYPE_NORMAL
- en: Google has also announced that Gemini Nano (the lightest version) will soon
    be [integrated into Pixel smartphones](https://store.google.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/),
    starting with the Pixel 8 Pro.
  prefs: []
  type: TYPE_NORMAL
- en: This will allow you to create automated summaries of conversations using the
    Recorder app, and generate high-quality response suggestions on WhatsApp.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Gemini will be made available to developers on December 13th, through
    Google Generative AI Studio. You can also sign up to this live session by Google
    on how you can start [building applications using Gemini](https://www.googlecloudcommunity.com/gc/Cloud-Events/Building-Transformative-Applications-with-Gemini-on-Google-Cloud/ec-p/677873#M437)
    on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
