- en: Building a Streaming Data Pipeline with Redshift Serverless and Kinesis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](https://towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An End-To-End Tutorial for Beginners
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----04e09d7e85b2--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----04e09d7e85b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----04e09d7e85b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----04e09d7e85b2--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----04e09d7e85b2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----04e09d7e85b2--------------------------------)
    ·9 min read·Oct 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1700c0485714244a17aec09305461e6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sebastian Pandelache](https://unsplash.com/@pandelache?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will talk about one of the most popular data pipeline design
    patterns — event streaming. Among other benefits, it enables lightning-fast data
    analytics and we can create reporting dashboards that update results in real-time.
    I will demonstrate how it can be achieved by building a streaming data pipeline
    with AWS Kinesis and Redshift which can be deployed with just a few clicks using
    infrastructure as code. We will use AWS CloudFormation to describe our data platform
    architecture and simplify deployment.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that as a data engineer, you are tasked to create a data pipeline that
    connects server event streams with a data warehouse solution (Redshift) to transform
    the data and create an analytics dashboard.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19d9e86d0773049b03bcd3c4ea66b9ab.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Pipeline Infrastructure. Image by author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: What is a data pipeline?
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is a sequence of data processing steps. Due to ***logical data flow connections***
    between these stages, each stage generates an **output** that serves as an **input**
    for the following stage.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I previously wrote about it in this article:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----04e09d7e85b2--------------------------------)
    [## Data pipeline design patterns'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right architecture with examples
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----04e09d7e85b2--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: For example, event data can be created by a source at the back end, an event
    stream built with Kinesis Firehose or Kafka stream. It can then feed a number
    of various consumers or destinations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，事件数据可以由后端的源创建，使用 Kinesis Firehose 或 Kafka 流构建事件流。然后它可以馈送到多个不同的消费者或目的地。
- en: Streaming is a “must-have” solution for enterprise data due to its streaming
    data processing capabilities. It enables real-time data analytics.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 流式处理是企业数据的“必备”解决方案，因其流数据处理能力。它能够实现实时数据分析。
- en: In our use-case scenario we can set up an **ELT streaming** data pipeline to
    AWS Redshift. AWS Firehose stream can offer this type of seamless integration
    when streaming data will be uploaded directly into the data warehouse table. Then
    data can be transformed to create reports with AWS Quicksight as a BI tool for
    example.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的用例场景中，我们可以设置一个**ELT 流式**数据管道到 AWS Redshift。AWS Firehose 流可以提供这种无缝集成，当数据流被直接上传到数据仓库表时。然后，数据可以被转换以使用
    AWS Quicksight 作为 BI 工具来创建报告，例如。
- en: '![](../Images/90a8cb4b073924f3113fce5df13ab20c.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90a8cb4b073924f3113fce5df13ab20c.png)'
- en: Added BI component. Image by author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了 BI 组件。图片来源于作者。
- en: This tutorial assumes that learners are familiar with AWS CLI and have minimal
    Python knowledge.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本教程假设学习者熟悉 AWS CLI 并且具有基本的 Python 知识。
- en: Workflow
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作流程
- en: 1\. Firstly, we will create Kinesis data stream using AWS CloudFormation
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 首先，我们将使用 AWS CloudFormation 创建 Kinesis 数据流。
- en: 2\. We will send sample data events to this event stream using AWS Lambda.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 我们将使用 AWS Lambda 向此事件流发送示例数据事件。
- en: 3\. Finally, we will provision AWS Redshift cluster and test our streaming pipeline.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 最后，我们将配置 AWS Redshift 集群并测试我们的流式管道。
- en: Create an AWS Kinesis Data Stream
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 AWS Kinesis 数据流
- en: AWS Kinesis Data Streams is an Amazon Kinesis real-time data streaming solution.
    It offers great scalability and durability where data streams are available for
    any consumer.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Kinesis Data Streams 是一个 Amazon Kinesis 实时数据流解决方案。它提供了出色的可扩展性和耐用性，数据流可以被任何消费者访问。
- en: 'We can create it with CloudFormation template. The commandline script below
    will trigger AWS CLI command to deploy it:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 CloudFormation 模板来创建它。下面的命令行脚本将触发 AWS CLI 命令进行部署：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And the template kinesis-data-stream.yaml will look as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 并且模板 kinesis-data-stream.yaml 将如下所示：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Very simple. If everything goes well we will see our Kinesis stream being deployed:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 非常简单。如果一切顺利，我们将看到我们的 Kinesis 流被部署：
- en: '![](../Images/0f33f2db4f4b45e501787735db96cb76.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f33f2db4f4b45e501787735db96cb76.png)'
- en: Stream created. Image by author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 流已创建。图片来源于作者。
- en: 2\. Create AWS Lambda function to simulate an event stream
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 创建 AWS Lambda 函数以模拟事件流
- en: Now we would want to send some events to our Kinesis Data Stream. For this,
    we can create a serverless application, such as AWS Lambda. We will use `boto3`
    library (The AWS SDK for Python) to build a data connector with AWS Kinesis at
    source.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们希望将一些事件发送到我们的 Kinesis 数据流。为此，我们可以创建一个无服务器应用程序，例如 AWS Lambda。我们将使用`boto3`库（AWS
    的 Python SDK）来构建一个数据连接器与 AWS Kinesis 进行数据源连接。
- en: '![](../Images/611621ecd4f8ba97807a73dcbfff5e82.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/611621ecd4f8ba97807a73dcbfff5e82.png)'
- en: Running app locally to simulate event stream. Image by author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本地运行应用以模拟事件流。图片来源于作者。
- en: 'Our application folder structure can look like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用程序文件夹结构可以如下所示：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Our `app.py` must be able to send events to Kinesis Data Stream:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`app.py`必须能够向 Kinesis 数据流发送事件：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We would like to add a helper function to generate some random event data.
    For instance:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望添加一个帮助函数来生成一些随机事件数据。例如：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can use `python-lambda-local` library to run and test AWS Lambda locally
    like so:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`python-lambda-local`库本地运行和测试 AWS Lambda，方法如下：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`env.json` is just an event payload to run Lambda locally.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`env.json` 只是一个事件负载，用于本地运行 Lambda。'
- en: '`config/staging.yaml` can contain any environment specific setting our application
    might require in future. For example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`config/staging.yaml` 可以包含我们应用程序未来可能需要的任何环境特定设置。例如：'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If you need to use `requirements.txt` it can look like this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要使用`requirements.txt`，它可以如下所示：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run this in your command line:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的命令行中运行这个：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This approach is useful because we might want to deploy our serverless application
    in the cloud and schedule it. We can use CloudFormation template for this. I prevoiusly
    wrote about it here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法很有用，因为我们可能希望将无服务器应用程序部署到云中并进行调度。我们可以使用 CloudFormation 模板来实现这一点。我之前在这里写过：
- en: '[](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)
    [## Infrastructure as Code for Beginners'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Deploy Data Pipelines like a pro with these templates
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use a CloudFormation template the application can be deployed with
    a shell script like so:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It is a flexible setup allowing use to create robust CI/CD pipelines. I remember
    I created one in this post below.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
    [## Continuous Integration and Deployment for Data Platforms'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD for data engineers and ML Ops
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Create Redshift Serverless resources
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we need to create Redshift Serverless cluster for our streaming data pipeline.
    We can provision Redshift Workgroup, create a Namespace and other required resources
    either manually or with CloudFormation template.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Redshift Serverless is just a data warehouse solution. It enables the execution
    of analytics workloads of any size without the need for data warehouse infrastructure
    management. Redshift is fast and generates insights from enormous volumes of data
    in seconds. It scales automatically to provide quick performance for even the
    most demanding applications.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e11cbe2d920bab18ad3e3c56a7a0a94.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: Example view with events from our application. Image by author.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: In our case we can deploy the Redshift resources using CloudFormation template
    definitions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So if we run this in the command line it will deploy this stack:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Typically we would want to deploy databases in a private subnet. However, in
    the early stages of a development, you might want to have a direct access to Redshift
    from dev machine.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: This is not recommended for production environments, but in this development
    case, you can start off by putting Redshift into our `default` VPC subnet.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now when all required pipeline resources were successfully provisioned we can
    connect our Kinesis stream and Redshift data warehouse.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Then we can use SQL statements to create `kinesis_data` schema in Redshift:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The first part of this SQL will set AWS Kinesis as source. The second one will
    create a view with event data from our application.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to create AWS Redshift role with added `AmazonRedshiftAllCommandsFullAccess`
    AWS managed policy.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'That’s it. Everything is ready to run the application to simulate the event
    data stream. These events will appear straight away in the Redshift view we have
    just created:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/611621ecd4f8ba97807a73dcbfff5e82.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: Application running locally. Image by author.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e11cbe2d920bab18ad3e3c56a7a0a94.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: Example view with events from our application. Image by author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We created a simple and reliable streaming data pipeline from a serverless application
    created with AWS Lambda to AWS Redshift data warehouse where data is transformed
    and ingested in real time. It enables capturing, processing, and storing data
    streams of any size with ease. It is great for any Machine learning (ML) pipeline
    where models are used to examine data and forecast inference endpoints as streams
    flow to their destination.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: We used infrastructure as code to deploy data pipeline resources. This is a
    preferable approach to deploying resources in different data environments.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Recommended read
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
    [## Continuous Integration and Deployment for Data Platforms'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD for data engineers and ML Ops
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
    [](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----04e09d7e85b2--------------------------------)
    [## Data Platform Architecture Types
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: How well does it answer your business needs? Dilemma of a choice.
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----04e09d7e85b2--------------------------------)
    [](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)
    [## Infrastructure as Code for Beginners
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Deploy Data Pipelines like a pro with these templates
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
