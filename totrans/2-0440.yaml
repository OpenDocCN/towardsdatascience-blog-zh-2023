- en: Building a Streaming Data Pipeline with Redshift Serverless and Kinesis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](https://towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An End-To-End Tutorial for Beginners
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----04e09d7e85b2--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----04e09d7e85b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----04e09d7e85b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----04e09d7e85b2--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----04e09d7e85b2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----04e09d7e85b2--------------------------------)
    Â·9 min readÂ·Oct 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1700c0485714244a17aec09305461e6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sebastian Pandelache](https://unsplash.com/@pandelache?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will talk about one of the most popular data pipeline design
    patterns â€” event streaming. Among other benefits, it enables lightning-fast data
    analytics and we can create reporting dashboards that update results in real-time.
    I will demonstrate how it can be achieved by building a streaming data pipeline
    with AWS Kinesis and Redshift which can be deployed with just a few clicks using
    infrastructure as code. We will use AWS CloudFormation to describe our data platform
    architecture and simplify deployment.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that as a data engineer, you are tasked to create a data pipeline that
    connects server event streams with a data warehouse solution (Redshift) to transform
    the data and create an analytics dashboard.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19d9e86d0773049b03bcd3c4ea66b9ab.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Pipeline Infrastructure. Image by author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: What is a data pipeline?
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is a sequence of data processing steps. Due to ***logical data flow connections***
    between these stages, each stage generates an **output** that serves as an **input**
    for the following stage.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I previously wrote about it in this article:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----04e09d7e85b2--------------------------------)
    [## Data pipeline design patterns'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right architecture with examples
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----04e09d7e85b2--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: For example, event data can be created by a source at the back end, an event
    stream built with Kinesis Firehose or Kafka stream. It can then feed a number
    of various consumers or destinations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œäº‹ä»¶æ•°æ®å¯ä»¥ç”±åç«¯çš„æºåˆ›å»ºï¼Œä½¿ç”¨ Kinesis Firehose æˆ– Kafka æµæ„å»ºäº‹ä»¶æµã€‚ç„¶åå®ƒå¯ä»¥é¦ˆé€åˆ°å¤šä¸ªä¸åŒçš„æ¶ˆè´¹è€…æˆ–ç›®çš„åœ°ã€‚
- en: Streaming is a â€œmust-haveâ€ solution for enterprise data due to its streaming
    data processing capabilities. It enables real-time data analytics.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æµå¼å¤„ç†æ˜¯ä¼ä¸šæ•°æ®çš„â€œå¿…å¤‡â€è§£å†³æ–¹æ¡ˆï¼Œå› å…¶æµæ•°æ®å¤„ç†èƒ½åŠ›ã€‚å®ƒèƒ½å¤Ÿå®ç°å®æ—¶æ•°æ®åˆ†æã€‚
- en: In our use-case scenario we can set up an **ELT streaming** data pipeline to
    AWS Redshift. AWS Firehose stream can offer this type of seamless integration
    when streaming data will be uploaded directly into the data warehouse table. Then
    data can be transformed to create reports with AWS Quicksight as a BI tool for
    example.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ç”¨ä¾‹åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ª**ELT æµå¼**æ•°æ®ç®¡é“åˆ° AWS Redshiftã€‚AWS Firehose æµå¯ä»¥æä¾›è¿™ç§æ— ç¼é›†æˆï¼Œå½“æ•°æ®æµè¢«ç›´æ¥ä¸Šä¼ åˆ°æ•°æ®ä»“åº“è¡¨æ—¶ã€‚ç„¶åï¼Œæ•°æ®å¯ä»¥è¢«è½¬æ¢ä»¥ä½¿ç”¨
    AWS Quicksight ä½œä¸º BI å·¥å…·æ¥åˆ›å»ºæŠ¥å‘Šï¼Œä¾‹å¦‚ã€‚
- en: '![](../Images/90a8cb4b073924f3113fce5df13ab20c.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90a8cb4b073924f3113fce5df13ab20c.png)'
- en: Added BI component. Image by author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ äº† BI ç»„ä»¶ã€‚å›¾ç‰‡æ¥æºäºä½œè€…ã€‚
- en: This tutorial assumes that learners are familiar with AWS CLI and have minimal
    Python knowledge.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹å‡è®¾å­¦ä¹ è€…ç†Ÿæ‚‰ AWS CLI å¹¶ä¸”å…·æœ‰åŸºæœ¬çš„ Python çŸ¥è¯†ã€‚
- en: Workflow
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å·¥ä½œæµç¨‹
- en: 1\. Firstly, we will create Kinesis data stream using AWS CloudFormation
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ AWS CloudFormation åˆ›å»º Kinesis æ•°æ®æµã€‚
- en: 2\. We will send sample data events to this event stream using AWS Lambda.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. æˆ‘ä»¬å°†ä½¿ç”¨ AWS Lambda å‘æ­¤äº‹ä»¶æµå‘é€ç¤ºä¾‹æ•°æ®äº‹ä»¶ã€‚
- en: 3\. Finally, we will provision AWS Redshift cluster and test our streaming pipeline.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. æœ€åï¼Œæˆ‘ä»¬å°†é…ç½® AWS Redshift é›†ç¾¤å¹¶æµ‹è¯•æˆ‘ä»¬çš„æµå¼ç®¡é“ã€‚
- en: Create an AWS Kinesis Data Stream
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»º AWS Kinesis æ•°æ®æµ
- en: AWS Kinesis Data Streams is an Amazon Kinesis real-time data streaming solution.
    It offers great scalability and durability where data streams are available for
    any consumer.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Kinesis Data Streams æ˜¯ä¸€ä¸ª Amazon Kinesis å®æ—¶æ•°æ®æµè§£å†³æ–¹æ¡ˆã€‚å®ƒæä¾›äº†å‡ºè‰²çš„å¯æ‰©å±•æ€§å’Œè€ç”¨æ€§ï¼Œæ•°æ®æµå¯ä»¥è¢«ä»»ä½•æ¶ˆè´¹è€…è®¿é—®ã€‚
- en: 'We can create it with CloudFormation template. The commandline script below
    will trigger AWS CLI command to deploy it:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ CloudFormation æ¨¡æ¿æ¥åˆ›å»ºå®ƒã€‚ä¸‹é¢çš„å‘½ä»¤è¡Œè„šæœ¬å°†è§¦å‘ AWS CLI å‘½ä»¤è¿›è¡Œéƒ¨ç½²ï¼š
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And the template kinesis-data-stream.yaml will look as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”æ¨¡æ¿ kinesis-data-stream.yaml å°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Very simple. If everything goes well we will see our Kinesis stream being deployed:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: éå¸¸ç®€å•ã€‚å¦‚æœä¸€åˆ‡é¡ºåˆ©ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°æˆ‘ä»¬çš„ Kinesis æµè¢«éƒ¨ç½²ï¼š
- en: '![](../Images/0f33f2db4f4b45e501787735db96cb76.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f33f2db4f4b45e501787735db96cb76.png)'
- en: Stream created. Image by author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æµå·²åˆ›å»ºã€‚å›¾ç‰‡æ¥æºäºä½œè€…ã€‚
- en: 2\. Create AWS Lambda function to simulate an event stream
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. åˆ›å»º AWS Lambda å‡½æ•°ä»¥æ¨¡æ‹Ÿäº‹ä»¶æµ
- en: Now we would want to send some events to our Kinesis Data Stream. For this,
    we can create a serverless application, such as AWS Lambda. We will use `boto3`
    library (The AWS SDK for Python) to build a data connector with AWS Kinesis at
    source.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¸Œæœ›å°†ä¸€äº›äº‹ä»¶å‘é€åˆ°æˆ‘ä»¬çš„ Kinesis æ•°æ®æµã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªæ— æœåŠ¡å™¨åº”ç”¨ç¨‹åºï¼Œä¾‹å¦‚ AWS Lambdaã€‚æˆ‘ä»¬å°†ä½¿ç”¨`boto3`åº“ï¼ˆAWS
    çš„ Python SDKï¼‰æ¥æ„å»ºä¸€ä¸ªæ•°æ®è¿æ¥å™¨ä¸ AWS Kinesis è¿›è¡Œæ•°æ®æºè¿æ¥ã€‚
- en: '![](../Images/611621ecd4f8ba97807a73dcbfff5e82.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/611621ecd4f8ba97807a73dcbfff5e82.png)'
- en: Running app locally to simulate event stream. Image by author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬åœ°è¿è¡Œåº”ç”¨ä»¥æ¨¡æ‹Ÿäº‹ä»¶æµã€‚å›¾ç‰‡æ¥æºäºä½œè€…ã€‚
- en: 'Our application folder structure can look like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºæ–‡ä»¶å¤¹ç»“æ„å¯ä»¥å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Our `app.py` must be able to send events to Kinesis Data Stream:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„`app.py`å¿…é¡»èƒ½å¤Ÿå‘ Kinesis æ•°æ®æµå‘é€äº‹ä»¶ï¼š
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We would like to add a helper function to generate some random event data.
    For instance:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›æ·»åŠ ä¸€ä¸ªå¸®åŠ©å‡½æ•°æ¥ç”Ÿæˆä¸€äº›éšæœºäº‹ä»¶æ•°æ®ã€‚ä¾‹å¦‚ï¼š
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can use `python-lambda-local` library to run and test AWS Lambda locally
    like so:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`python-lambda-local`åº“æœ¬åœ°è¿è¡Œå’Œæµ‹è¯• AWS Lambdaï¼Œæ–¹æ³•å¦‚ä¸‹ï¼š
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`env.json` is just an event payload to run Lambda locally.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`env.json` åªæ˜¯ä¸€ä¸ªäº‹ä»¶è´Ÿè½½ï¼Œç”¨äºæœ¬åœ°è¿è¡Œ Lambdaã€‚'
- en: '`config/staging.yaml` can contain any environment specific setting our application
    might require in future. For example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`config/staging.yaml` å¯ä»¥åŒ…å«æˆ‘ä»¬åº”ç”¨ç¨‹åºæœªæ¥å¯èƒ½éœ€è¦çš„ä»»ä½•ç¯å¢ƒç‰¹å®šè®¾ç½®ã€‚ä¾‹å¦‚ï¼š'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If you need to use `requirements.txt` it can look like this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ éœ€è¦ä½¿ç”¨`requirements.txt`ï¼Œå®ƒå¯ä»¥å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run this in your command line:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½ çš„å‘½ä»¤è¡Œä¸­è¿è¡Œè¿™ä¸ªï¼š
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This approach is useful because we might want to deploy our serverless application
    in the cloud and schedule it. We can use CloudFormation template for this. I prevoiusly
    wrote about it here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½å¸Œæœ›å°†æ— æœåŠ¡å™¨åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ°äº‘ä¸­å¹¶è¿›è¡Œè°ƒåº¦ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ CloudFormation æ¨¡æ¿æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æˆ‘ä¹‹å‰åœ¨è¿™é‡Œå†™è¿‡ï¼š
- en: '[](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)
    [## Infrastructure as Code for Beginners'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Deploy Data Pipelines like a pro with these templates
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use a CloudFormation template the application can be deployed with
    a shell script like so:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It is a flexible setup allowing use to create robust CI/CD pipelines. I remember
    I created one in this post below.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
    [## Continuous Integration and Deployment for Data Platforms'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD for data engineers and ML Ops
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Create Redshift Serverless resources
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we need to create Redshift Serverless cluster for our streaming data pipeline.
    We can provision Redshift Workgroup, create a Namespace and other required resources
    either manually or with CloudFormation template.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Redshift Serverless is just a data warehouse solution. It enables the execution
    of analytics workloads of any size without the need for data warehouse infrastructure
    management. Redshift is fast and generates insights from enormous volumes of data
    in seconds. It scales automatically to provide quick performance for even the
    most demanding applications.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e11cbe2d920bab18ad3e3c56a7a0a94.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: Example view with events from our application. Image by author.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: In our case we can deploy the Redshift resources using CloudFormation template
    definitions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So if we run this in the command line it will deploy this stack:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Typically we would want to deploy databases in a private subnet. However, in
    the early stages of a development, you might want to have a direct access to Redshift
    from dev machine.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: This is not recommended for production environments, but in this development
    case, you can start off by putting Redshift into our `default` VPC subnet.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now when all required pipeline resources were successfully provisioned we can
    connect our Kinesis stream and Redshift data warehouse.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Then we can use SQL statements to create `kinesis_data` schema in Redshift:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The first part of this SQL will set AWS Kinesis as source. The second one will
    create a view with event data from our application.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to create AWS Redshift role with added `AmazonRedshiftAllCommandsFullAccess`
    AWS managed policy.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Thatâ€™s it. Everything is ready to run the application to simulate the event
    data stream. These events will appear straight away in the Redshift view we have
    just created:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/611621ecd4f8ba97807a73dcbfff5e82.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: Application running locally. Image by author.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e11cbe2d920bab18ad3e3c56a7a0a94.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: Example view with events from our application. Image by author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We created a simple and reliable streaming data pipeline from a serverless application
    created with AWS Lambda to AWS Redshift data warehouse where data is transformed
    and ingested in real time. It enables capturing, processing, and storing data
    streams of any size with ease. It is great for any Machine learning (ML) pipeline
    where models are used to examine data and forecast inference endpoints as streams
    flow to their destination.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: We used infrastructure as code to deploy data pipeline resources. This is a
    preferable approach to deploying resources in different data environments.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Recommended read
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
    [## Continuous Integration and Deployment for Data Platforms'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD for data engineers and ML Ops
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
    [](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----04e09d7e85b2--------------------------------)
    [## Data Platform Architecture Types
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: How well does it answer your business needs? Dilemma of a choice.
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----04e09d7e85b2--------------------------------)
    [](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)
    [## Infrastructure as Code for Beginners
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Deploy Data Pipelines like a pro with these templates
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
