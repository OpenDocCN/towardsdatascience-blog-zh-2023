["```py\nimport numpy as np \nfrom scipy.special import softmax\nimport math\n\nsentence = \"Today is sunday\"\n\nvocabulary = ['Today', 'is', 'sunday', 'saturday']\n\n# Initial Embedding (one-hot encoding):\n\nx_1 = [1,0,0,0] # Today \nx_2 = [0,1,0,0] # is \nx_3 = [0,0,1,0] # Sunday\nx_4 = [0,0,0,1] # Saturday\n\nX_example = np.stack([x_1, x_2, x_3], axis=0)\n```", "```py\nW_Q = np.random.uniform(-1, 1, size=(4, 4))\nW_K = np.random.uniform(-1, 1, size=(4, 4))\nW_V = np.random.uniform(-1, 1, size=(4, 4))\n```", "```py\nclass W_matrices:\n    def __init__(self, n_lines, n_cols):\n        self.W_Q = np.random.uniform(low=-1, high=1, size=(n_lines, n_cols))\n        self.W_K = np.random.uniform(low=-1, high=1, size=(n_lines, n_cols))\n        self.W_V = np.random.uniform(low=-1, high=1, size=(n_lines, n_cols))\n\n    def print_W_matrices(self):\n        print('W_Q : \\n', self.W_Q)\n        print('W_K : \\n', self.W_K)\n        print('W_V : \\n', self.W_V)\n```", "```py\nQ = np.matmul(X_example, W_Q)\nK = np.matmul(X_example, W_K)\nV = np.matmul(X_example, W_V)\n```", "```py\nAttention_scores = np.matmul(Q, np.transpose(K))\n```", "```py\nAttention_scores = Attention_scores / 2\n```", "```py\nSoftmax_Attention_Matrix = np.apply_along_axis(softmax, 1, Attention_scores)\n```", "```py\nOne_Head_Attention = np.matmul(Softmax_Attention_Matrix,V)\n```", "```py\nclass One_Head_Attention:\n    def __init__(self, d_model, X):\n        self.d_model = d_model\n        self.W_mat = W_matrices(d_model, d_model)\n\n        self.Q = np.matmul(X, self.W_mat.W_Q)\n        self.K = np.matmul(X, self.W_mat.W_K)\n        self.V = np.matmul(X, self.W_mat.W_V)\n\n    def print_QKV(self):\n        print('Q : \\n', self.Q)\n        print('K : \\n', self.K)\n        print('V : \\n', self.V)\n\n    def compute_1_head_attention(self):\n        Attention_scores = np.matmul(self.Q, np.transpose(self.K)) \n        print('Attention_scores before normalization : \\n', Attention_scores)\n        Attention_scores = Attention_scores / np.sqrt(self.d_model) \n        print('Attention scores after Renormalization: \\n ', Attention_scores)\n        Softmax_Attention_Matrix = np.apply_along_axis(softmax, 1, Attention_scores)\n        print('result after softmax: \\n', Softmax_Attention_Matrix)\n        # print('Softmax shape: ', Softmax_Attention_Matrix.shape)\n\n        result = np.matmul(Softmax_Attention_Matrix, self.V)\n        print('softmax result multiplied by V: \\n', result)\n\n        return result\n\n    def _backprop(self):\n        # do smth to update W_mat\n        pass\n```", "```py\nclass Multi_Head_Attention:\n    def __init__(self, n_heads, d_model, X):\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_concat = self.d_model*self.n_heads # 4*8\n        self.W_0 = np.random.uniform(-1, 1, size=(self.d_concat, self.d_model))\n        # print('W_0 shape : ', self.W_0.shape)\n        self.heads = []\n        self.heads_results = []\n        i = 0\n        while i < self.n_heads:\n            self.heads.append(One_Head_Attention(self.d_model, X))\n            i += 1\n\n    def print_W_0(self):\n        print('W_0 : \\n', self.W_0)\n\n    def print_QKV_each_head(self):\n        i = 0\n        while i < self.n_heads:\n            print(f'Head {i}: \\n')\n            self.heads[i].print_QKV()\n            i += 1\n\n    def print_W_matrices_each_head(self):\n        i = 0\n        while i < self.n_heads:\n            print(f'Head {i}: \\n')\n            self.heads[i].W_mat.print_W_matrices()\n            i += 1\n\n    def compute(self):\n        for head in self.heads:\n            self.heads_results.append(head.compute_1_head_attention())\n            # print('head: ', self.heads_results[-1].shape)\n\n        multi_head_results = np.concatenate(self.heads_results, axis=1)\n        # print('multi_head_results shape = ', multi_head_results.shape)\n\n        V_updated = np.matmul(multi_head_results, self.W_0)\n        return V_updated\n\n    def back_propagate(self):\n        # backpropagate W_0\n        # call _backprop for each head\n        pass\n```", "```py\nclass Positional_Encoding:\n    def __init__(self, X):\n        self.PE_shape = X.shape\n        self.PE = np.empty(self.PE_shape)\n        self.d_model = self.PE_shape[1]\n\n    def compute(self, X):\n        for i in range(self.PE_shape[0]): \n            for j in range(self.PE_shape[1]):\n                self.PE[i,2*j] = math.sin(i/(10000**(2*j/self.d_model)))\n                self.PE[i,2*j+1] = math.cos(i/(10000**(2*j/self.d_model)))\n                # this way we are assuming that the vectors are ordered stacked in X\n\n        return X + self.PE\n```", "```py\nclass FFN:\n    def __init__(self, V_updated, layer1_sz, layer2_sz):\n        self.layer1_sz = layer1_sz\n        self.layer2_sz = layer2_sz\n        self.layer1_weights = np.random.uniform(low=-1, high=1, size=(V_updated.shape[1], layer1_sz))\n        self.layer2_weights = np.random.uniform(low=-1, high=1, size=(layer2_sz, V_updated.shape[1]))\n\n    def compute(self, V_updated):\n        result = np.matmul(V_updated, self.layer1_weights)\n        result = np.matmul(result, self.layer2_weights)\n\n        return result\n\n    def backpropagate_ffn(self):\n        pass\n```"]