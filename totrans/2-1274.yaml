- en: How to Write Expert Prompts for ChatGPT (GPT-4) and Other Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550](https://towardsdatascience.com/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A beginner-friendly guide to prompt engineering with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://nabil-alouani.medium.com/?source=post_page-----23133dc85550--------------------------------)[![Nabil
    Alouani](../Images/8ceea018e9b15413d318bfb710bb0011.png)](https://nabil-alouani.medium.com/?source=post_page-----23133dc85550--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23133dc85550--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23133dc85550--------------------------------)
    [Nabil Alouani](https://nabil-alouani.medium.com/?source=post_page-----23133dc85550--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23133dc85550--------------------------------)
    ¬∑63 min read¬∑Nov 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d2088f1c56684c0f65cee8b56f3ca2d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author via Midjourney.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering is a fancy way to say, ‚ÄúWrite better and better instructions
    for an AI model until it does exactly what you want.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: Writing prompts is a bit like riding a bike. You don‚Äôt need a Ph.D. in mechanical
    physics to learn how to keep your balance. A bit of theory can help but the most
    important part is trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: Consider this guide as the ‚Äúbit of theory‚Äù that will help you prepare for riding
    the AI bike. You‚Äôll find a list of techniques illustrated with explanations, examples,
    and templates you can test on your favorite models.
  prefs: []
  type: TYPE_NORMAL
- en: The guide focuses on ChatGPT (GPT-4), but every single technique shared below
    applies to other Large Language Models (LLMs) like Claude and LLaMA.
  prefs: []
  type: TYPE_NORMAL
- en: Table of contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Disclaimer:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although this guide is skewed towards ChatGPT, I have no personal interest in
    promoting the product. I‚Äôm not sponsored by OpenAI, Microsoft, or anyone else.
  prefs: []
  type: TYPE_NORMAL
- en: What‚Äôs in this guide?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The guide includes seven ‚ÄúGood to know‚Äù sections and more than 25 prompt engineering
    techniques. Each section starts with an icon that indicates its type, as shown
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: üí° ‚ÄúGood to know‚Äù section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üü¢ Beginner-friendly technique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üü° Intermediate technique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üü† Advanced technique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚ÄúGood to know‚Äù sections provide specific points to keep in mind when writing
    prompts. Beginner-friendly techniques can be used right away. Intermediate and
    advanced techniques require preparation and a pinch of patience.
  prefs: []
  type: TYPE_NORMAL
- en: All of the techniques discussed in the guide derive from AI research papers
    and first-hand experience. Citations and direct links are available at the end
    of the guide. There are two techniques I (re)named to make them easier to remember.
    They are marked with a star like this*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each prompt has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here‚Äôs an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: üí° Why should you care about Prompt Engineering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI continues to make its way into every digital tool we use every
    day ‚Äî like web browsers, social media apps, and even PowerPoint presentations.
    The better Language Models get, the more we‚Äôll use them.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering is the language we use to interact with AI models to achieve
    an increasing number of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Picture the near future as a foreign country. You know you‚Äôll have to live there
    for a couple of years, but you don‚Äôt speak the local language. Every time you
    buy groceries, book a trip, or negotiate a deal, you have to hire someone to do
    the talking for you.
  prefs: []
  type: TYPE_NORMAL
- en: It sounds inefficient and unfun, doesn‚Äôt it?
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine you could learn the local language before you moved to this foreign
    country. Instead of hiring someone who speaks the language of AI, you learn to
    write prompts. Pick the right words and AI will give you what you want‚Äî be it
    emails, code, or monthly reports.
  prefs: []
  type: TYPE_NORMAL
- en: But hold on, you may say. Isn‚Äôt Prompt Engineering bound to disappear?
  prefs: []
  type: TYPE_NORMAL
- en: Probably yes, because the goal is to make it easier for humans to use AI systems.
    Perhaps in the future, messy voice prompts could be enough to create whatever
    crosses your mind. Heck, we may even develop AI that can read your mind in real
    time.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, Prompt Engineering becomes a temporary skill. It‚Äôll become obsolete
    as soon as AI learns to guess human intentions based on half-baked sentences and
    brain signals.
  prefs: []
  type: TYPE_NORMAL
- en: You can wait for that to happen, or you can get a headstart.
  prefs: []
  type: TYPE_NORMAL
- en: üí° Why is Prompt Engineering harder than you think?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The short answer is ‚Äúan illusion of ease.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: Since we use natural language to write prompts, we don‚Äôt see it as a complex
    skill that requires practice. All you have to do is write instructions in plain
    English, right?
  prefs: []
  type: TYPE_NORMAL
- en: Well, not exactly.
  prefs: []
  type: TYPE_NORMAL
- en: Yes, when you talk to humans, you prompt them the same way you‚Äôd prompt an AI.
    Both humans and AI use inner models to respond to your prompt. Except humans and
    AI systems have different models of reality.
  prefs: []
  type: TYPE_NORMAL
- en: The human model draws on cognitive abilities, past experiences, theoretical
    knowledge, and real-time sensory data. Language Models, on the other hand, rely
    solely on language patterns.
  prefs: []
  type: TYPE_NORMAL
- en: You could say that human software and AI software run on different operating
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: So when you interact with a Language Model, it‚Äôs as if you‚Äôre interacting with
    an alien. Sure the alien responds to your natural language but it doesn‚Äôt behave
    the same way a fellow human would ‚Äî which means you‚Äôll have to adapt your prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to humans, AI systems require more details, precise explanations, clearer
    instructions, and a bit of repetition. You want to be intentional with each word
    you write ‚Äî almost as if you were writing code.
  prefs: []
  type: TYPE_NORMAL
- en: The analogy of ‚ÄúEnglish as a programming language‚Äù is spot-on until you consider
    one crucial detail. Unlike with code, every single prompt gets a response, regardless
    of its quality. Even prompts that include typos and unclear sentences generate
    *something*. In contrast, incorrect code doesn‚Äôt generate anything (except for
    frustrating error messages).
  prefs: []
  type: TYPE_NORMAL
- en: This always-generate-something property makes the illusion of ease even stronger.
    Messy prompts produce results, so why bother improving them?
  prefs: []
  type: TYPE_NORMAL
- en: Casual AI users don‚Äôt feel the need to work on their prompt engineering skills,
    so they mistakenly believe it‚Äôs an easy feat.
  prefs: []
  type: TYPE_NORMAL
- en: In a way, prompt engineering is like dancing. You watch a tutorial on TikTok
    where a professional dancer demonstrates a few hip-hop steps and you think ‚ÄúOh
    this is actually easy!‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: Notice how easy this move looks.
  prefs: []
  type: TYPE_NORMAL
- en: But once you try to reproduce those very same ‚Äúeasy steps,‚Äù you realize your
    body has its own laws of physics. All of a sudden, moving sideways while bouncing
    your body and waving your arms seems impossible.
  prefs: []
  type: TYPE_NORMAL
- en: Just like dancing, Prompt Engineering is harder than it looks, but it gets easier
    with every prompt you write.
  prefs: []
  type: TYPE_NORMAL
- en: üí° You don‚Äôt need prompt ideas, you need problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The best prompts come from good ideas, and good ideas are born from interesting
    problems. Here‚Äôs a simple framework you can use to find problems worth solving:'
  prefs: []
  type: TYPE_NORMAL
- en: List all the tasks you carry out on a computer (for work or otherwise).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isolate the tasks that require text analysis and/or writing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the results you want to achieve from each task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example of a useful problem:** ‚ÄúEvery Monday, I file a performance report
    to my client for each advertisement I ran the week before. I need to comment on
    every Key Performance Indicator (KPI) to make the data easier to understand by
    non-experts.‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: '**How prompting can help:** Write a prompt that comments on each KPI you submit.
    Ask your AI model to use storytelling techniques and fun analogies to make the
    report lighter and more accessible.'
  prefs: []
  type: TYPE_NORMAL
- en: üí° Watch out for AI hallucinations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whatever the model you pick, remember that chatbots often hallucinate, writing
    information that‚Äôs factually wrong. This behavior stems from LLMs being trained
    on giant datasets filled with logical errors and nonsense.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning and Reinforcement Learning based on Human Feedback (RLHF) helps
    LLMs reduce inaccuracies and bias, but there are still loopholes.
  prefs: []
  type: TYPE_NORMAL
- en: 'I took a few swings on the hallucination problem in other pieces. You‚Äôll find
    the links below. For now, here‚Äôs a sneak peek:'
  prefs: []
  type: TYPE_NORMAL
- en: You may think chatbots tell the truth by default, and sometimes, they happen
    to hallucinate. But it‚Äôs more accurate to say chatbots write pure bullshit that
    sometimes matches reality.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By ‚Äúbullshit,‚Äù I don‚Äôt mean the slang we use to describe nonsense, but a philosophical
    definition brought by Harry Frankfurt.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Harry Frankfurt described bullshit as information that has no relationship to
    reality. When you lie, you distort reality. When you tell the truth, you describe
    your representation of reality. But when you bullshit, you make things up with
    no consideration of what reality might be like.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*‚ÄúIt is just this lack of connection to a concern with truth ‚Äî* ***this indifference
    to how things really are*** *‚Äî that I regard as of the essence of bullshit.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*This points to a similar and fundamental aspect of the essential nature of
    bullshit:* ***although it is produced without concern with the truth, it need
    not be false****.‚Äù*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*‚Äî* [*On Bullshit by Harry Frankfurt*](https://www.goodreads.com/author/quotes/219.Harry_G_Frankfurt)
    *[Emphasis mine].*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Language Models are highly-plausible bullshitters that often land on truth,
    so you can‚Äôt fully trust them. You must fact-check their outputs, especially when
    dealing with non-fiction.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://nabilalouani.substack.com/p/artificial-disinformation-can-chatbots?source=post_page-----23133dc85550--------------------------------)
    [## Artificial Disinformation: Can Chatbots Destroy Trust on the Internet?'
  prefs: []
  type: TYPE_NORMAL
- en: Soon after ChatGPT came out, a clock started ticking inside my head. It's like
    when you see a bolt of lightning slice‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: nabilalouani.substack.com](https://nabilalouani.substack.com/p/artificial-disinformation-can-chatbots?source=post_page-----23133dc85550--------------------------------)
    [](https://nabilalouani.substack.com/p/chatgpt-hype-is-proof-nobody-really?source=post_page-----23133dc85550--------------------------------)
    [## ChatGPT Hype is Proof Nobody Really Understands AI
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models are dumber than your neighbor's cat
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: nabilalouani.substack.com](https://nabilalouani.substack.com/p/chatgpt-hype-is-proof-nobody-really?source=post_page-----23133dc85550--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: üü¢ The Basics of Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each prompt is a bridge between what you want and what your Language Model generates.
    The shape of your bridge depends on the problem you want to solve, but the underlying
    structure remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Picture this structure as six pillars:'
  prefs: []
  type: TYPE_NORMAL
- en: Be specific.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use placeholders <like_this> to build flexible templates. (More on this in a
    dedicated section).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prioritize **what to do** over what not to do.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the desired format of the output. (More on this in a dedicated section).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use double hashtags like this ## to separate different parts of your prompt.
    A prompt can include instructions, examples, and the desired format.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Revise your prompt to remove the fluff.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here‚Äôs an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: üü¢ Specify the context (also called ‚ÄúPriming‚Äù)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For each question you write, your Large Language Model can generate thousands
    of different answers. When you provide context, you help your LLM narrow down
    the range of possible outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Say you want a non-boring meal plan for the upcoming week. Adding your diet
    restrictions and personal preferences makes it more likely to get relevant suggestions
    for every single meal.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways you can introduce context into your prompt. It‚Äôs like
    mentally preparing your Language Model for the task, hence the name ‚ÄúPriming.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: üü¢ Specify the desired format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This one is straightforward. All you have to do is add a sentence to your prompt
    where you describe the format you want.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs a list you can draw from:'
  prefs: []
  type: TYPE_NORMAL
- en: Bullet-points;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Articles and blog posts;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essays and research papers;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Short stories and creative writing pieces;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poems and song lyrics;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newsletters and press releases;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social media posts and captions;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advertisements and marketing copy;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Email templates and business correspondence;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Product descriptions and reviews;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tutorials and how-to guides;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequently Asked Questions (FAQs);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transcripts and interviews;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reports and memos;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Screenplays and scripts for plays or podcasts;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speeches and presentations;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summaries and abstracts;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical documentation and manuals;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Educational materials, such as lesson plans or course syllabi;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opinion pieces and editorials;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personal statements, cover letters, and resumes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Below are three examples of how to introduce format inside a basic prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Note:** if you use a non-specialized Language Model to generate legal contracts,
    make sure you run them by legal experts.'
  prefs: []
  type: TYPE_NORMAL
- en: üü¢ Use <placeholders>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Placeholders <like_this> help you achieve two separate goals.
  prefs: []
  type: TYPE_NORMAL
- en: Use <placeholders> to write flexible prompts that can take different inputs.
    You have to indicate the content of each placeholder in your prompt. In this case,
    **a placeholder is a parameter.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use empty <placeholders> to illustrate the desired format. Here you don‚Äôt have
    to write the content of each placeholder. Your LLM will guess what each placeholder
    stands for, especially when you use known frameworks like User Stories or cover
    letters. In this case, **a placeholder is an instruction.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: üü¢ How to use placeholders as parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: üü¢ How to use placeholders as instructions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: üü¢ Specify the style/tone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each chatbot has a default style defined by its creators. For instance, ChatGPT
    sounds friendly and nuanced, but you can ask it to change its tone to fit your
    preferences and needs.
  prefs: []
  type: TYPE_NORMAL
- en: You can even ask your Language Model to mimic the tone of a fictional/real person.
    Usually, the result is an over-the-top parody of whoever ChatGPT tries to emulate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few examples of styles you can pick from:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generic styles:** formal, informal, persuasive, conversational, sarcastic,
    dramatic, condescending, nuanced, biased, humorous, optimistic, pessimistic, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Domain-specific styles:** academic, legal, political, technical, medical,
    news, scientific, marketing, creative, instructional, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mimicking the style of a real person:** Agatha Christie, Daniel Kahneman,
    J.K Rowling, James Baldwin, Hajime Isayama, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here‚Äôs how you can specify the style in a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: üü¢ Specify the length of the desired response
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Length is a proxy for the level of detail you want in a response. Length is
    also a constraint you sometimes must consider when writing specific formats like
    tweets, SEO descriptions, and titles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are three examples of how you can specify length in a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: üü¢Specify the target audience
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Language Models are trained on billions of words taken from diverse sources,
    including Wikipedia, research papers, and Reddit. Each source has its own audience,
    and each audience consumes information differently.
  prefs: []
  type: TYPE_NORMAL
- en: When you specify the target audience, you tell your model to adapt the content,
    the examples, and the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider two potential audiences for a prompt about the benefits of exercise:
    general adult readers and medical professionals.'
  prefs: []
  type: TYPE_NORMAL
- en: For the first audience, you want your Language Model to use relatable examples
    and simple explanations. In contrast, the second audience would expect you to
    evoke studies and use technical terminology.
  prefs: []
  type: TYPE_NORMAL
- en: Even if the topic remains the same, the desired output can be extremely different.
    That‚Äôs why you want to indicate the target audience in your prompts/
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are what the prompts would look like for the ‚Äúbenefits of exercise‚Äù example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: One common mistake people make when writing prompts is to consider ‚Äústyle‚Äù and
    ‚Äútarget audience‚Äù as the same parameter. In reality, the style determines *how
    the text sounds* and the target audience decides *which words to use*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is another set of examples of how to introduce the target audience in
    a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: üü¢ Many-Examples Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I stole this technique from [Simon Willison](https://twitter.com/simonw), who
    uses Many-Examples prompting to push Language Models beyond their comfort zone.
    When ChatGPT and other models write a response, they predict the most likely answer
    ‚Äî but the most likely answer is often the least creative.
  prefs: []
  type: TYPE_NORMAL
- en: If you ask your model to exhaust its reserve of common answers, however, it‚Äôll
    have no choice but to explore new possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many-Examples prompting is particularly useful for tasks that involve imagination.
    Here are a few instances where the technique can shine:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimize code by examining a list of possible variations;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brainstorm ideas for new products/features;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inspire creative names for products/domains/companies;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reformulate sentences or create slogans;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find a destination for your holidays;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrade your CV by looking for new skills to learn;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find new hobbies;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Browse of list of books/movies/songs based on your personal preferences. (Bing
    works better than ChatGPT for this one);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore diverse perspectives on the same topic;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate as much information as possible for a given topic. Here you can combine
    Many-Examples prompting with Knowledge Generation (more on this later).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are two basic formulations you can use Many-Examples prompting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the second option when the first one fails.Sometimes when you ask for a
    large number of examples, ChatGPT will complain. It‚Äôll say something like: ‚ÄúAs
    an AI language model, I am not aware of 50 distinct possible answers. However,
    I can provide you with a list of several answers that can be helpful.‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: From there, ChatGPT will stop at 20 possible answers, but you can use the ‚ÄúPlease
    add 5 more distinct answers‚Äù to keep the generative ball rolling. Once the same
    answers keep showing up, it means you‚Äôve emptied your model‚Äôs creative jar.
  prefs: []
  type: TYPE_NORMAL
- en: üü¢ Temperature Control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Temperature is a parameter that influences the ‚Äúrandomness‚Äù of the response
    generated by your language model. It typically ranges from 0 to 1, but in some
    instances, you can bring the temperature beyond 1.
  prefs: []
  type: TYPE_NORMAL
- en: Lower temperatures (between 0.1 and 0.3) produce the most likely response. In
    other words, you get the most ‚Äúconservative‚Äù output. **Low temperatures are particularly
    useful when generating code because you get the most stable output.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher temperatures (between 0.7 and 0.9) lead to more creative responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One way to memorize the use of temperature: ‚ÄúCold for code; hot for prose.‚Äù
    Here‚Äôs how you can introduce it in a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: üü¢ Zero-Shot Prompting (no examples)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Zero-shot prompting is to write an instruction for your AI model without providing
    context or examples. The basic format of zero-shot involves two parts often called
    ‚ÄúText‚Äù and ‚ÄúDesired Result.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are two examples of zero-shot prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This particular format of zero-shot prompting is rare outside of AI labs where
    experts use the technique to test the capabilities of their models.
  prefs: []
  type: TYPE_NORMAL
- en: The most common format of zero-shot prompting is the one you use naturally.
    You just type your question. You don‚Äôt need the ‚ÄúText + Desired output‚Äù format.
    That‚Äôs because user-friendly models like ChatGPT and Bard are optimized for dialogue
    ‚Äî and every dialogue is a series of zero-shots.
  prefs: []
  type: TYPE_NORMAL
- en: You could say chatbots are zero-shot machines.
  prefs: []
  type: TYPE_NORMAL
- en: üü¢ Few-Shot Prompting (several high-quality examples)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Few-shot prompting is also known as in-context learning. You give your Language
    Model a bunch of high-quality examples to improve its ‚Äúguesses.‚Äù The number of
    examples depends on your model, but you can start with three to five inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'It‚Äôs not necessary to add a number to each example (like #1, #2, #3), but doing
    so can improve the output. Another element you want to add to your examples is
    ‚Äúnoise.‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: Noise is information that‚Äôs not useful for the task given to your Language Model.
    In the ‚ÄúTone‚Äù examples, I introduced misleading sentences to confuse the system
    and force it to focus on the ‚Äúsignal.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: If you make the task too obvious for your Language Model, it may underperform
    when faced with complex examples.
  prefs: []
  type: TYPE_NORMAL
- en: üü¢ Zero-Shot/Few-Shot ‚Äî The simple version
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to remember something from zero-shot and few-shot, remember the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**When your Language Model fails to give you the desired response, add high-quality
    examples to your prompt.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs an illustration of how few-shot can help you improve ChatGPT‚Äôs output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: üí° In-context Learning vs. Chat History
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first usable version of every Language Model is often a jack of all trades.
    It can perform a variety of tasks at an average-ish level. If you want to specialize
    your model (and consequently improve its output), you have two options. You could
    either retrain it using new specific data or use in-context learning. AI people
    usually use a combination of both.
  prefs: []
  type: TYPE_NORMAL
- en: In-context learning is a prompting technique that allows you to steer the responses
    of your LLMs in a specific direction. All you need are a few examples, just like
    few-shot prompting.
  prefs: []
  type: TYPE_NORMAL
- en: The reason AI experts love in-context learning is efficiency. Instead of using
    a ton of high-quality data to adapt a raw model, you can use a very limited number
    of well-formatted examples.
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a summary of In-Context Learning published by Princeton University.
  prefs: []
  type: TYPE_NORMAL
- en: In-context learning was popularized in the original GPT-3 paper as a way to
    use language models to learn tasks given only a few examples.[[1]](http://ai.stanford.edu/blog/understanding-incontext/#f1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: During in-context learning, we give the LM a prompt that consists of a list
    of input-output pairs that demonstrate a task. At the end of the prompt, we append
    a test input and allow the LM to make a prediction just by conditioning on the
    prompt and predicting the next tokens.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To correctly answer the two prompts below, the model needs to read the training
    examples to figure out the input distribution (financial or general news), output
    distribution (Positive/Negative or topic), input-output mapping (sentiment or
    topic classification), and the formatting.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/97294ea04f70fdc5faf9722c5942496f.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples of In-Context Learning. [Source](http://ai.stanford.edu/blog/understanding-incontext/)
  prefs: []
  type: TYPE_NORMAL
- en: You can derive numerous applications from in-context learning ‚Äî such as generating
    code, automated spreadsheets, and numerous other text-oriented tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatGPT, however, is another story. OpenAI sacrificed ChatGPT‚Äôs ability to
    use in-context learning to introduce a new feature: Chat history. Sure, you lose
    the flexibility of the model, but you get a user-friendly interface that allows
    for lengthy conversations.'
  prefs: []
  type: TYPE_NORMAL
- en: You could argue chat history is a variant of in-context learning because ChatGPT‚Äôs
    responses evolve depending on the content of the chat history tab you‚Äôre using.
    For instance, if you feed a list of recipes into a ChatGPT tab, it‚Äôll be able
    to perform specific tasks on your input*.* This involves summary, continuation,
    and editing.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this important?
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your needs and future discoveries, you may need to pick one of
    two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Use in-context learning to fine-tune a ‚Äúraw‚Äù model like GPT-4, OpenLLaMa, or
    Falcon. In other words, you can create a customized chatbot but the process can
    be tedious.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use chat history to leverage ‚Äúmemory‚Äù and ‚Äúlong conversations.‚Äù It‚Äôs easier
    to customize your output but the quality may go down over time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: üü° Chain of Thought Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chain of Thought (CoT) prompting means you tell your Language Model to **reason
    step by step** before arriving at a final response. It‚Äôs as if you ask your model
    to think out loud.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose I ask you to calculate 4x3\. You could instantly compute the operation
    inside your head and say, ‚Äú12.‚Äù But if I ask you to use a ‚Äúchain of thought,‚Äù
    you‚Äôd split your reasoning into four steps.
  prefs: []
  type: TYPE_NORMAL
- en: 4x3 = 4+4+4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4+4+4 = (4+4) + 4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (4+4) + 4 = 8+4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 8+4 = 12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CoT prompts are typically used to solve logical riddles. The idea is to break
    down complex problems into smaller, more manageable questions.
  prefs: []
  type: TYPE_NORMAL
- en: Language Models predict the next token in a sequence of words, and their predictions
    are more accurate when they deal with common patterns found in abundance inside
    training data. But sometimes, you need to tap into uncommon patterns to answer
    uncommon questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following riddle: ‚ÄúIf eggs are $0.12 a dozen, how many eggs can
    you get for a dollar?‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you force ChatGPT to give an immediate response, it‚Äôll write: ‚ÄùYou can get
    10 dozen eggs for a dollar,‚Äù which is a wrong answer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a46f56a22764a947e872e1b4cd6744b.png)'
  prefs: []
  type: TYPE_IMG
- en: The prompt forces ChatGPT to give an immediate answer (no chain of thought).
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you ask ChatGPT to reason step by step, it gives a different answer
    ‚Äî the right answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56c5edbad984077a2d309882e6a4b28b.png)'
  prefs: []
  type: TYPE_IMG
- en: The latest versions of ChatGPT often (but not always) use CoT when they respond
    to prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs another example that illustrates the difference between standard prompting
    and Chain of Thought.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6673d19d2c756c392c58499590e51e37.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice the addition of a chain of thought in the second input (highlighted in
    blue). [Source](https://learnprompting.org/docs/intermediate/chain_of_thought#fn-1)
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways you can use Chain of Thought prompting.
  prefs: []
  type: TYPE_NORMAL
- en: üü¢ Zero-Shot Chain of Thought
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Add one sentence at the end of your prompt to make your Language Model apply
    CoT. The top-performing sentences I found are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**‚Äú‚Ä¶.Let‚Äôs think step by step.‚Äù**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**‚Äú‚Ä¶.Please proceed step by step to be sure you arrive at the right answer.‚Äù**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here‚Äôs how you can incorporate them in your prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Usually, Zero-shot CoT is enough to solve logic puzzles. But if your Language
    Model fails, you can try the second flavor of CoT prompting.
  prefs: []
  type: TYPE_NORMAL
- en: üü° **Few-Shot Chain of Thought**
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Much like the standard few-shot prompting, you want to feed your Language Model
    high-quality examples before submitting your question. Each example must include
    multiple steps of reasoning‚Äî and the more logical steps you add, the better the
    response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs an example of a prompt that combines Few-Shot and Chain of Thought:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: üü¢ Role Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Assigning a specific role to your Language Model helps it capture more and
    better semantic relationships (ie: logic and meaning).'
  prefs: []
  type: TYPE_NORMAL
- en: In a way, Role Prompting helps you nudge your model to focus on specific information
    inside its training data. It‚Äôs a shortcut to specify many variables at once ‚Äî
    like context, style, perspective, and vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the task at hand, you can use different versions of Role Prompting.
    Below are a few examples that may inspire you.
  prefs: []
  type: TYPE_NORMAL
- en: Mimic a personal style.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emulate specific expertise like a lawyer or a strategic planner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emulate your counterpart in a conversation like your professor, your ex, or
    your boss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate multiple points of view.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Behave like a mini-app that corrects typos, compiles your code, or generates
    Excel formulas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: There‚Äôs an advanced version of role prompting that we‚Äôll explore in a specific
    section called ‚ÄúAll-In-One Prompting.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: üü¢ Knowledge Generation Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of Knowledge Generation prompting is to make your Language Model retrieve
    specific bits of information from its giant pool of training data. Picture this
    technique as asking your model to do some research before writing a final response.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you want your model to write a blog post about growing flowers on your
    balcony. Instead of asking your model to write the blog right away, you can prompt
    it to generate key points about gardening, flowers, and space management.
  prefs: []
  type: TYPE_NORMAL
- en: Once you get the desired key point, make sure to attend to your fact-checking
    duties. From there, prompt your model to use the ‚Äúknowledge‚Äù it generated to write
    an article.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Generation improves the output quality because it forces your model
    to focus on specific points instead of trying to answer a vague prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs how you can introduce Knowledge Generation into your prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Knowledge Generation Prompting and ChatGPT Plugins**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use ChatGPT plugins to both generate knowledge and help with fact-checking.
    Make sure to try as many plugins as possible because most of them are still clunky.
  prefs: []
  type: TYPE_NORMAL
- en: üü† Knowledge Integration Prompting*
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main weakness of Knowledge Generation prompting is the timeline. GPT-4‚Äôs
    training data stops in September 2021, which means all the content that came afterward
    is unknown to the model.
  prefs: []
  type: TYPE_NORMAL
- en: The cutoff date isn‚Äôt a problem when you deal with timeless topics like gardening,
    writing, and cooking, but if you‚Äôre chasing the latest information, you need a
    complementary trick.
  prefs: []
  type: TYPE_NORMAL
- en: You can use plugins, chatbots with online browsing, or Knowledge Integration
    prompting.
  prefs: []
  type: TYPE_NORMAL
- en: All you have to do is feed recent data into your model to help it catch up with
    the news. In a way, you make your offline model integrate new knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: For API users, [GPT-4 can process up to 32,000 tokens](https://www.semrush.com/blog/gpt-4/),
    which represent about 25,000 words. This includes both the user prompt and the
    answer. For users of ChatGPT Plus, GPT-4 can take up to 4096 tokens as input,
    which is approximately 3,000 words.
  prefs: []
  type: TYPE_NORMAL
- en: You can use these 3,000 words and the chat history feature to ‚Äúteach‚Äù ChatGPT-4
    new information. The model itself won‚Äôt integrate the data, but you can generate
    prompts that leverage the ‚Äúnew information‚Äù you just added.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is a framework you can use to apply Knowledge Integration prompting:'
  prefs: []
  type: TYPE_NORMAL
- en: Find a relevant source, like a research paper or a documented article.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the most informative parts of the paper at hand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cut the parts into chunks of 3,000 words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feed the chunks into ChatGPT-4 and ask it to explain each section in simple
    words. You can also ask for quotes and examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use ChatGPT-4's output for a new prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs say you‚Äôre an AI researcher specializing in Large Language Models. Your
    current task is to reference material that‚Äôs relevant to your thesis.
  prefs: []
  type: TYPE_NORMAL
- en: You found an interesting paper titled *Language Models Can Solve Computer Tasks*.
    You want to take notes before skimming the other 122 papers you bookmarked last
    week.
  prefs: []
  type: TYPE_NORMAL
- en: Here are the steps you can follow to get ChatGPT to help you take quick notes.
  prefs: []
  type: TYPE_NORMAL
- en: First, identify the passage you want to summarize. In this example, we‚Äôll select
    the discussion part which makes for about 1,000 words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/f0a39e870bbe222ab75222daf50028f4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://arxiv.org/abs/2302.08399).'
  prefs: []
  type: TYPE_NORMAL
- en: Cut these lengthy passages into chunks of 3,000 words (not needed in this example).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feed these chunks of text to ChatGPT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ask ChatGPT to write a summary of the text you provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the process for all the papers you want to summarize.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don‚Äôt forget to fact-check.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use your freshly created reading notes to find common threads, and confront
    opposing results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here‚Äôs what the framework looks like in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**Note:** if the final output is too long, ChatGPT will stop writing its response
    midway. In this case, you can prompt it with the word ‚ÄúContinue,‚Äù and it will
    resume writing from the point it was cut off.'
  prefs: []
  type: TYPE_NORMAL
- en: üü¢ **Knowledge Integration* and Microsoft Edge**
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using Knowledge Integration prompts, you can use the ‚ÄúChat‚Äù feature of
    Microsoft Edge for more efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of navigating the material yourself, you can open a web page or a PDF
    in Edge and use the Chat feature to summarize the content. From there, inject
    the summary into ChatGPT and use it for another prompt like the one we saw in
    the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs a prompt you can use to summarize a document using Microsoft Edge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: üü† Directional Stimulus (DS) Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The original version of DS Prompting involves two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Train a Language Model to generate a **specific type of hint** from a given
    text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use **another Language Model** to summarize the same text using the hint generated
    by the first Language Model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For this guide, we‚Äôll use a simpler version of DS Prompting where we **use the
    same model to perform both tasks**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f78e881ebb239c356e1eecd86b892c54.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison between Standard Prompting and Direction Stimulus Prompting. [Source](https://arxiv.org/abs/2302.11520)
  prefs: []
  type: TYPE_NORMAL
- en: The best (and perhaps only) use case for DS Prompting is to summarize research
    papers and other academic-like texts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you want to summarize a paper titled *Guiding Large Language Models
    via Directional Stimulus Prompting*. Here are the steps you want to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the passage you want to summarize.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cut these lengthy passages into chunks your Language Model can handle. (3,000
    words in the case of ChatGPT-4).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feed the chunks of text into ChatGPT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ask ChatGPT to write a hint about the paper involving key information like names,
    addresses, dates, places, and events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Few-Shot prompting to show ChatGPT the desired output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you get the ‚Äúhint,‚Äù fact-check it and adjust your Few-Shot prompt if necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Submit your ‚Äúhint‚Äù generating prompt in a new tab.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the prompt to get a ‚Äúhint‚Äù for the text you want.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the same tab, prompt ChatGPT to summarize the last submitted text using the
    ‚Äúhint.‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here‚Äôs what Directional Stimilus prompting looks like in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: üü° Recursive Criticism and Improvement (RCI) Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Despite the fancy name, Recursive Criticism and Improvement prompting is a
    simple technique. You can break it down into three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt your LLM to generate an output based on an initial prompt (often a Zero-Shot).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prompt your model to identify potential problems with the output it generated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prompt your model to generate an updated output based on the identified problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RCI enhances your LLMs‚Äô reasoning abilities in situations like writing code
    and solving logic puzzles. In fact, researchers found that RCI can outperform
    Chain of Thought (CoT) in some instances.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e55390bc1927cbb3543c5bba8feaa118.png)'
  prefs: []
  type: TYPE_IMG
- en: An example where RCI outperforms Chain of Thought prompting. [Source](https://arxiv.org/abs/2303.17491)
  prefs: []
  type: TYPE_NORMAL
- en: Even better, when you combine RCI with CoT prompting (‚ÄúLet‚Äôs think step by step‚Äù),
    you get better results than either of the methods used separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs a framework you can apply to practice RCI:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt your model with an instruction of your choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the same tab, add a new prompt where you ask your model to ‚Äú**review the
    previous answer and find potential problems in it**.‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If there‚Äôs an error, ChatGPT will try to find it and correct it at the same
    time. Other models can stop after describing the error. In such a case, you can
    add the following prompt: ‚Äú**Based on the problems you found, improve your answer**.‚Äù'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another option is to combine the two previous prompts into one: ‚Äú**Please review
    your answer and find every potential problem within it**. **Based on the problems
    you found, improve your answer.**‚Äù'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs an example where RCI prompting helped ‚Äúoptimize‚Äù a simple Python script.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/356eed96ddba204cd7bab98928487a8b.png)![](../Images/965fb4a13ef4d494f2802cc1075953e5.png)![](../Images/08806cc37a0a5b7975244543ff5fa424.png)![](../Images/d15f4757b1102b1431bcbc8989ff206b.png)'
  prefs: []
  type: TYPE_IMG
- en: üü° Self-Refinement Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Self-refinement is a variant of RCI prompting. But instead of asking your model
    to find *problems*, you ask it to provide *feedback*. The two techniques are identical
    when the goal is precision and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if you seek creativity and subjective responses, Self-Refinement
    prompts give you more options than RCI.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, when you prompt your model to generate a poem, what you‚Äôre looking
    for isn‚Äôt a clear-cut formula. Rather, it‚Äôs an emotion-charged depiction of a
    human experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, you start by prompting your model to write a poem and then engage
    in a refinement loop where you:'
  prefs: []
  type: TYPE_NORMAL
- en: Ask your model to give feedback on its own response. Use a role prompt to improve
    the quality of the feedback, then add the desired instruction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Ask your model to improve its response based on its own feedback.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Add your own feedback if needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the previous steps until you reach the desired outcome.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here‚Äôs an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1ddc6eff6b34693531ca32ffa3a41ab7.png)![](../Images/d098a59d548a1f9b579c1bbbd7b81ca1.png)![](../Images/b7b4505794f10c643ff1c94f8c94b063.png)![](../Images/49e3993a7e50a759bcc342d861efed64.png)![](../Images/37ed832b4959c4b04246c37fdf7e190b.png)![](../Images/8410316a2c525b922c1a0703eb3b894c.png)'
  prefs: []
  type: TYPE_IMG
- en: ChatGPT‚Äôs output for a series of Self-Refinement prompts about writing a Linkedin
    post.
  prefs: []
  type: TYPE_NORMAL
- en: I kept the quality of the prompts low to illustrate a crucial point of self-refinement
    prompting. Depending on how you steer the feedback loop, **you may end up degrading
    the output.** It takes practice and domain-specific expertise to maximize the
    potential of self-refinement prompting.
  prefs: []
  type: TYPE_NORMAL
- en: There‚Äôs a dedicated section about iterations and refinement. It‚Äôs titled ‚ÄúIterate
    until you have to revert.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: üü° Reverse Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reverse engineering is the art of building things backward ‚Äî and you can use
    it on prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of writing a prompt to generate a response, start with a high-quality
    version of the desired response and work your way back to a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to highlight the difference between classic prompting and reverse
    prompt engineering is to turn each technique into a question.
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional prompting: ‚ÄúHere are the directions. Can you get me there?‚Äù'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reverse-Engineered prompting: ‚ÄúHere‚Äôs the destination I want to reach. Can
    you show me the directions to get there?‚Äù'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method shines in two situations. The first is when seek inspiration to
    write your prompt. The second is when your goal is to generate output with very
    specific formats ‚Äî like a board game, a landing page, or a recipe. Let‚Äôs explore
    an example involving the latter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: üü° Prompt Revision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This technique may seem similar to Reverse Prompt Engineering, but there‚Äôs a
    tiny difference. Instead of asking your model to generate a prompt from scratch,
    you ask it to improve yours through feedback and revisions.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Revision is useful for intermediate and expert prompt engineers. Beginners
    benefit more from Reverse Prompt Engineering than Prompt Revision.
  prefs: []
  type: TYPE_NORMAL
- en: '**When you‚Äôre a beginner**, **you don‚Äôt have enough skills to recognize your
    mistakes.** Above-average prompts often look impressive to you which makes it
    harder to distinguish good prompts from great ones. That‚Äôs why you want to stick
    to the basics until you develop reflexes and intuitions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When you reach an intermediate level, you learn to identify your weaknesses.**
    Prompt Revision helps you identify and overcome your blind spots. It can also
    provide subtle changes that can improve your prompts‚Äô output. Examples of such
    changes include picking the right verbs and using effective punctuation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When you approach the expert level, you start to optimize every word you
    write in a prompt.** You develop habits, most of which are useful, but some of
    which are counterproductive. In a way, prompting is a bit like cycling ‚Äî at the
    beginning, you master the correct posture but you later find (bad) shortcuts that
    work just for you. Prompt Revision helps you make up for potential gaps by rewriting
    your prompts using the top-performing guidelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs a Prompt Revision example shared by [Alex Albert,](https://twitter.com/alexalbert__?lang=en)
    a prompt engineer and jailbreaker.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: üü° Program Simulation Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Program Simulation is a particular case of Role prompting where you ask your
    model to behave like a mini app.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, your model won‚Äôt develop an app from scratch and run it on some
    invisible server. Instead, your LLM will behave like a small program that operates
    inside a chat tab.
  prefs: []
  type: TYPE_NORMAL
- en: The emulated program functions like an automated voice message system. Think
    of when you call a fancy restaurant and hear a recorded voice telling you to ‚Äútype
    1 to make a new reservation or type 2 to cancel an existing reservation.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1e2767521f1cc6b9fc8756c14f6d2b9.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see the output of a Program Simulation prompt as a decision tree where
    every possible outcome is pre-determined by a script. Your prompt makes for the
    script or at least part of it.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, you can write a prompt that defines the main branches of the decision
    tree and let your LLM fill in the blanks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs illustrate with an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the prompt only defines the five main branches of the desired decision
    tree. The description of each of these five branches is an implicit instruction
    for your LLM to complete the decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs what the output of the previous prompt looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1aa6b2e28a7230ac97b5073f6b34ff4a.png)![](../Images/3461dec317e5a6fbffc25ac53fa601ca.png)![](../Images/7b3e99cf7342181387e5f4fddcb89122.png)![](../Images/fb811e3b30b89c0bddf8aee337739e2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Program Simulation prompting.
  prefs: []
  type: TYPE_NORMAL
- en: üü† All-In-One (AIO) Prompting*
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All-In-One (AIO) is an extended version of Role Prompting where you give your
    model a detailed list of instructions, turning it into a ‚Äúspecialized‚Äù version
    of itself.
  prefs: []
  type: TYPE_NORMAL
- en: I called it All-In-One because you can combine all of the previous techniques
    to emulate a specialized LLM inside another LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft used a similar approach to create Bing Chat out of GPT-4\. They fine-tuned
    some version of GPT-4 and equipped it with extra functions like online search
    and annotations.
  prefs: []
  type: TYPE_NORMAL
- en: All-In-One prompting involves two steps.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt your model to behave as if it were a raw version of itself. Technically
    speaking, it won‚Äôt do that, but it‚Äôll pretend as if it does, which is good enough.
    You‚Äôll be able to steer the behavior of ChatGPT (and other models) in a specific
    direction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prompt your model as if you‚Äôre ‚Äúfine-tuning‚Äù it. Note that fine-tuning is much
    more complex than writing a lengthy prompt. The real process involves training
    your model on new data, applying reinforcement learning through human feedback,
    and other adjustments. In the case of AIO prompting, you‚Äôll only ask your LLM
    to behave in a very precise manner, similar to Role prompting but with a more
    significant amount of details.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let‚Äôs illustrate AIO prompting with an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As discussed above, the second part of AIO combines many techniques. For instance,
    there‚Äôs role prompting, temperature control, chain-of-thought, and knowledge integration.
    You don‚Äôt have to use every single prompting technique you know, only the relevant
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs go through a simple template you can use to generate your own AI assistants
    using AIO prompting.
  prefs: []
  type: TYPE_NORMAL
- en: üü¢ Template for All-In-One (AIO) Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When writing an AIO prompt, be as specific as possible and adjust the prompt
    to the problem you want to solve. (You can also ask Bernard to help you improve
    your original prompt).
  prefs: []
  type: TYPE_NORMAL
- en: Test your prompt over and over again until you reach a satisfying result. Remember,
    Language Models like ChatGPT (GPT-4) have powerful capabilities, but most of these
    capabilities remain asleep until you wake them up.
  prefs: []
  type: TYPE_NORMAL
- en: The following template won‚Äôt take you there right away, but it‚Äôll put you on
    track. You‚Äôll find generic labels like <Topic_name> to make it easier to understand
    AIO prompting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here‚Äôs how to use the template.
  prefs: []
  type: TYPE_NORMAL
- en: Replace <Model_name> with a name of your choosing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace <Field_name> with the expertise you want your model to emulate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace <Topic_name> with a specific topic within the area of expertise you
    chose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace <Tone_type> with the desired tone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace <Style_type> with the desired style.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove the name of the techniques from the initial template.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add your own instructions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test your prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjust it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterate until you reach a satisfying result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üü¢ More examples of All-In-One Prompting*
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following is a list of AIO examples covering various applications. Modify
    them to make them your own. You‚Äôll find a few hints to help you get started.
  prefs: []
  type: TYPE_NORMAL
- en: Dolores, the Email Muse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Robert Ford, the Coding Master
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Lee Sizemore, the Outlandish Chef
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Maeve the Mastermind
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The secret sauce of AIO prompting is your professional expertise. The more you
    know about a topic, the higher the quality of your prompt because you‚Äôll think
    of details that others may miss.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering is an empirical science, and the best way to learn it is
    through regular practice. Hit the keyboard as much as possible and make sure to
    keep in touch with AI literature to upgrade your techniques.
  prefs: []
  type: TYPE_NORMAL
- en: üí° Iterate until you have to revert
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The output of Language Models is like a decision tree with thousands of possible
    outcomes. Each word predicted by the model branches out into a set of new possibilities,
    most of which are invisible to you. The only part that‚Äôs under your control is
    the starting point ‚Äî and that‚Äôs your prompt.
  prefs: []
  type: TYPE_NORMAL
- en: One major difference between Language Models and decision trees is the presence
    of randomness. The same prompt doesn‚Äôt always generate the same response. It‚Äôs
    the price we pay for creativity.
  prefs: []
  type: TYPE_NORMAL
- en: There‚Äôs also the alignment tax, where the model‚Äôs behavior (and capability)
    can change to meet (new) restrictions. And to top things off, nobody really knows
    what‚Äôs happening inside Language Models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, when you use a Language Model, you‚Äôre interacting with an unpredictable
    black box. You can‚Äôt really rely on exact science: trial and error is your best
    option.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The rule is simple: Iterate on your prompt until the latest version of your
    output becomes worse than the previous one. In other words, iterate until you
    have to revert.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Iteration comes in two flavors: either try different versions of the same prompt
    or guide the model through a succession of prompts. In most cases, you‚Äôll use
    a combination of both.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5595a1f463d23797dc7226e13debd22.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of how the quality of your output evolves with prompt iterations.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand how the iterative process works, picture prompting as a
    concave function (or a bell curve). Your first iterations are likely to get you
    better results, but at some point, your new prompt will start to generate worse
    output compared to its predecessors.
  prefs: []
  type: TYPE_NORMAL
- en: Pay attention to the inflection point, and when you reach it, you want to either
    settle or start a new chain of prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab92c7ad4c30803f0a95414186fe7b02.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of how successive chains of prompt iterations can improve your
    final prompt.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the following framework to get yourself started with the iterative
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Use Many-Examples prompting to generate ideas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**‚ÄúPlease provide me with a list of 50 suggestions on how to improve this prompt/response.‚Äù**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use **Prompt Revision/Bernard** to improve your prompts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rewrite the same prompt using **different words** and examine the responses.
    Different words trigger different responses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create a library of prompts for each model you use**. Make sure to update
    your library every now and then.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Study how Language Models work to understand how they generate responses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Whenever your output is stuck in the mud, give your prompts a few tweaks to
    push it out. Try different verbs. Mix prompting techniques. Switch models. Sleep
    on it. Start again tomorrow.
  prefs: []
  type: TYPE_NORMAL
- en: üí° With great power‚Ä¶
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We let the AI genie out of the bottle before we could figure out the risks involved,
    let alone how to deal with them.
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôm particularly worried about large-scale misinformation. But there are other
    risks like rapid displacements in the job market, prompt injection attacks, scams,
    and ultimately the alignment problem.
  prefs: []
  type: TYPE_NORMAL
- en: For now, the best we can do is to learn how to use generative AI to create more
    good than harm.
  prefs: []
  type: TYPE_NORMAL
- en: Below you‚Äôll find six short ethical rules that will seem obvious but are still
    worth mentioning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Maintain ethical standards:** Uphold your moral principles and avoid using
    AI-generated content that promotes harmful ideas or misinformation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Be transparent:** Clearly disclose the use of AI in your content creation
    process so your audience knows the origin of the information they‚Äôre consuming.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Use AI as an upgrade, not a replacement:** Tech companies will make a choice:
    either achieve the same results with ten times fewer people or keep the same number
    of employees and produce ten times more. The latter option paints a brighter future.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Verify facts and accuracy:** Always double-check the information generated
    by AI chatbots. The same goes for code. Misinformation and harmful code can make
    their way into your output. Switch on your skeptic mode.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Never disclose sensitive data:** OpenAI[keeps a record](https://www.androidauthority.com/does-chatgpt-save-data-conversations-3310883/)
    of all of your exchanges with ChatGPT. The same may apply to other companies.
    Assume everything you type into a chatbot can leak.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Watch for biases:** Chatbots can inadvertently perpetuate biases found in
    their training data. Be vigilant. You can use specific prompts to limit bias by
    asking a Language Model to consider multiple perspectives or provide a balanced
    view.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompt Engineering split the world into two camps. The first pictures it as
    a bug ‚Äî and the underlying argument is that AI models will get better and better
    at responding to natural language. There‚Äôs a possible future where AI models will
    be able to guess what we want them to do, similar to how social media algorithms
    can guess which content we want to see.
  prefs: []
  type: TYPE_NORMAL
- en: The second camp pictures Prompt Engineering as an essential skill in tomorrow‚Äôs
    job market. The argument is generative AI models will be everywhere. We‚Äôll use
    them to write code, generate reports, analyze data, and even prepare seminars.
    In such a scenario, Prompt Engineering becomes as essential as writing (non-boring)
    emails.
  prefs: []
  type: TYPE_NORMAL
- en: The world is never black or white but always some shade of gray ‚Äî and the future
    of Prompt Engineering is no exception. You can take a bet or you can play it sage.
    If you increase your Prompt Engineering skills and it turns out to be a short-lived
    skill, then you‚Äôll lose a few dozen hours of your life.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if you ignore Prompt Engineering, it later becomes a non-negotiable
    skill you may miss out on potential career upgrades. So it‚Äôs probably worth the
    detour.
  prefs: []
  type: TYPE_NORMAL
- en: References (in no particular order)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E.,
    Le, Q., & Zhou, D.* [***Chain of Thought Prompting Elicits Reasoning in Large
    Language Models***](https://arxiv.org/abs/2201.11903) ***‚Äî*** *(2022).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y.* [***Large Language
    Models are Zero-Shot Reasoner***](https://arxiv.org/abs/2205.11916) ***‚Äî*** *(2022).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Bras, R. L., Choi, Y., &
    Hajishirzi, H.* [***Generated Knowledge Prompting for Commonsense Reasoning***](https://arxiv.org/abs/2110.08387)
    ***‚Äî*** *(2021).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery,
    A., & Zhou, D.* [***Self-Consistency Improves Chain of Thought Reasoning in Language
    Model***](https://arxiv.org/abs/2203.11171) ***‚Äî*** *(2022).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zhou, D., Sch√§rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans,
    D., Cui, C., Bousquet, O., Le, Q., & Chi, E.* [***Least-to-Most Prompting Enables
    Complex Reasoning in Large Language Models***](https://arxiv.org/abs/2205.10625)
    ***‚Äî*** *(2022).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geunwoo Kim, Pierre Baldi, Stephen McAleer. [***Language Models Can Solve Computer
    Tasks***](https://arxiv.org/abs/2303.17491) ‚Äî*(2023)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Murray Shanahan, Kyle McDonell, Laria Reynolds. [Role-Play with Large Language
    Models](https://arxiv.org/abs/2305.16367) ‚Äî*(2023)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alex Albert. [***Jailbreak Chat***](https://www.jailbreakchat.com/) ‚Äî*(2022‚Äì2023)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sander Schulhoff, Anaum Khan, Fady Yanni. [***LearnPrompting.com***](https://learnprompting.org/)
    ‚Äî*(2023)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu
    Chen. [***What Makes Good In-Context Examples for GPT-3?***](https://arxiv.org/abs/2101.06804)
    ‚Äî*(2021)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lilian Weng. [***Prompt Engineering***](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)‚Äî*(2023)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cognitive Revolution Youtube Channel. [***The Art of Prompting ChatGPT With
    Riley Goodside***](https://www.youtube.com/watch?v=zg3H-9nxkyI&ab_channel=CognitiveRevolution%22HowAIChangesEverything%22)
    ‚Äî*(2023)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Giuseppe Scalamogna; [***New ChatGPT Prompt Engineering Technique: Program
    Simulation***](/new-chatgpt-prompt-engineering-technique-program-simulation-56f49746aa7b)
    ‚Äî*(2023)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sang Michael Xie, Sewon Min. [***How Does In-Context Learning Work?***](http://ai.stanford.edu/blog/understanding-incontext/)‚Äî*(2022)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alberto Romero. [***Prompt Engineering Is Probably More Important Than You Think***](https://thealgorithmicbridge.substack.com/p/prompt-engineering-is-probably-more)‚Äî(2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan.[***Guiding
    Large Language Models via Directional Stimulus Prompting***](https://arxiv.org/abs/2302.11520)*‚Äî(2023).*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao Fu, Hao Peng, Tushar Khot. [***How does GPT Obtain its Ability?***](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)
    *‚Äî(2022).*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note regarding the images:** Unless otherwise noted, all images and screenshots
    are by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: Contact section
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Become a Medium member to support me here](https://nabil-alouani.medium.com/membership).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Join my Subtack](https://nabilalouani.substack.com/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Write me an email: nabil@nabilalouani.com.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
