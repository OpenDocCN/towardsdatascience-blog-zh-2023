- en: How to Write Expert Prompts for ChatGPT (GPT-4) and Other Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何为ChatGPT（GPT-4）和其他语言模型编写专家提示
- en: 原文：[https://towardsdatascience.com/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550](https://towardsdatascience.com/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550](https://towardsdatascience.com/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550)
- en: A beginner-friendly guide to prompt engineering with LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与LLMs一起进行提示工程的适合初学者的指南
- en: '[](https://nabil-alouani.medium.com/?source=post_page-----23133dc85550--------------------------------)[![Nabil
    Alouani](../Images/8ceea018e9b15413d318bfb710bb0011.png)](https://nabil-alouani.medium.com/?source=post_page-----23133dc85550--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23133dc85550--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23133dc85550--------------------------------)
    [Nabil Alouani](https://nabil-alouani.medium.com/?source=post_page-----23133dc85550--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://nabil-alouani.medium.com/?source=post_page-----23133dc85550--------------------------------)[![Nabil
    Alouani](../Images/8ceea018e9b15413d318bfb710bb0011.png)](https://nabil-alouani.medium.com/?source=post_page-----23133dc85550--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23133dc85550--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23133dc85550--------------------------------)
    [Nabil Alouani](https://nabil-alouani.medium.com/?source=post_page-----23133dc85550--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23133dc85550--------------------------------)
    ·63 min read·Nov 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----23133dc85550--------------------------------)
    ·阅读63分钟·2023年11月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1d2088f1c56684c0f65cee8b56f3ca2d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d2088f1c56684c0f65cee8b56f3ca2d.png)'
- en: Image by author via Midjourney.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者通过Midjourney提供的图片。
- en: Prompt Engineering is a fancy way to say, “Write better and better instructions
    for an AI model until it does exactly what you want.”
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是一种花哨的说法，意思是“为AI模型编写更好的指令，直到它完全按照你的要求执行”。
- en: Writing prompts is a bit like riding a bike. You don’t need a Ph.D. in mechanical
    physics to learn how to keep your balance. A bit of theory can help but the most
    important part is trial and error.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 编写提示有点像骑自行车。你不需要在机械物理学上有博士学位才能学会保持平衡。一点理论可以帮助，但最重要的部分是反复试验。
- en: Consider this guide as the “bit of theory” that will help you prepare for riding
    the AI bike. You’ll find a list of techniques illustrated with explanations, examples,
    and templates you can test on your favorite models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 把这个指南看作是为骑AI自行车做准备的“一点理论”。你会找到一系列技术，包括解释、示例和模板，你可以在你喜欢的模型上进行测试。
- en: The guide focuses on ChatGPT (GPT-4), but every single technique shared below
    applies to other Large Language Models (LLMs) like Claude and LLaMA.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南侧重于ChatGPT（GPT-4），但下面分享的每一种技术都适用于其他大型语言模型（LLMs），如Claude和LLaMA。
- en: Table of contents
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Disclaimer:'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 免责声明：
- en: Although this guide is skewed towards ChatGPT, I have no personal interest in
    promoting the product. I’m not sponsored by OpenAI, Microsoft, or anyone else.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本指南偏向于ChatGPT，但我并没有个人利益来推广这个产品。我没有得到OpenAI、微软或其他任何人的赞助。
- en: What’s in this guide?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这个指南包括什么内容？
- en: 'The guide includes seven “Good to know” sections and more than 25 prompt engineering
    techniques. Each section starts with an icon that indicates its type, as shown
    below:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南包括七个“好好知道”部分和超过25种提示工程技术。每个部分都以一个图标开头，表示其类型，如下所示：
- en: 💡 “Good to know” section.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 💡 “好好知道”部分。
- en: 🟢 Beginner-friendly technique.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🟢 适合初学者的技术。
- en: 🟡 Intermediate technique.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🟡 中级技术。
- en: 🟠 Advanced technique.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🟠 高级技术。
- en: “Good to know” sections provide specific points to keep in mind when writing
    prompts. Beginner-friendly techniques can be used right away. Intermediate and
    advanced techniques require preparation and a pinch of patience.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: “好好知道”部分提供了在编写提示时要记住的具体要点。适合初学者的技术可以立即使用。中级和高级技术需要准备和一点耐心。
- en: All of the techniques discussed in the guide derive from AI research papers
    and first-hand experience. Citations and direct links are available at the end
    of the guide. There are two techniques I (re)named to make them easier to remember.
    They are marked with a star like this*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 指南中讨论的所有技术都来源于AI研究论文和第一手经验。引用和直接链接可在指南末尾找到。有两种技术我重新命名，以便记忆。它们用星号标记，就像这样*。
- en: 'Each prompt has the following format:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每个提示的格式如下：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here’s an example:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 💡 Why should you care about Prompt Engineering?
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💡 为什么你应该关心提示工程？
- en: Generative AI continues to make its way into every digital tool we use every
    day — like web browsers, social media apps, and even PowerPoint presentations.
    The better Language Models get, the more we’ll use them.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI继续进入我们每天使用的每个数字工具中，如网络浏览器、社交媒体应用甚至PowerPoint演示文稿。语言模型越好，我们就会越多地使用它们。
- en: Prompt engineering is the language we use to interact with AI models to achieve
    an increasing number of tasks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是我们用来与AI模型交互以完成越来越多任务的语言。
- en: Picture the near future as a foreign country. You know you’ll have to live there
    for a couple of years, but you don’t speak the local language. Every time you
    buy groceries, book a trip, or negotiate a deal, you have to hire someone to do
    the talking for you.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 把近期未来想象成一个外国国家。你知道你将在那里生活几年，但你不会说当地的语言。每次你购买食品、预订旅行或谈判交易时，你都必须雇佣人来替你说话。
- en: It sounds inefficient and unfun, doesn’t it?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来效率低下且不好玩，不是吗？
- en: Now imagine you could learn the local language before you moved to this foreign
    country. Instead of hiring someone who speaks the language of AI, you learn to
    write prompts. Pick the right words and AI will give you what you want— be it
    emails, code, or monthly reports.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，在你搬到这个外国国家之前，你可以学习当地的语言。与其雇佣会说AI语言的人，你学会写提示。选择合适的词语，AI将给你想要的东西——无论是电子邮件、代码还是月度报告。
- en: But hold on, you may say. Isn’t Prompt Engineering bound to disappear?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但是等等，你可能会说。提示工程不是注定要消失吗？
- en: Probably yes, because the goal is to make it easier for humans to use AI systems.
    Perhaps in the future, messy voice prompts could be enough to create whatever
    crosses your mind. Heck, we may even develop AI that can read your mind in real
    time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是的，因为目标是使人类更容易使用AI系统。也许在未来，混乱的语音提示就足以创造你想象的任何东西。我们甚至可能开发出能够实时读取你的思维的AI。
- en: In these cases, Prompt Engineering becomes a temporary skill. It’ll become obsolete
    as soon as AI learns to guess human intentions based on half-baked sentences and
    brain signals.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，提示工程成为一种临时技能。一旦AI能够根据不完整的句子和脑电信号猜测人类意图，它将变得过时。
- en: You can wait for that to happen, or you can get a headstart.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以等待这种情况发生，或者你可以提前开始。
- en: 💡 Why is Prompt Engineering harder than you think?
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💡 为什么提示工程比你想象的更难？
- en: The short answer is “an illusion of ease.”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的答案是“一种轻松的错觉”。
- en: Since we use natural language to write prompts, we don’t see it as a complex
    skill that requires practice. All you have to do is write instructions in plain
    English, right?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用自然语言编写提示，我们不认为它是一项需要练习的复杂技能。你所要做的就是用简单的英语写出指令，对吗？
- en: Well, not exactly.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，不完全是这样。
- en: Yes, when you talk to humans, you prompt them the same way you’d prompt an AI.
    Both humans and AI use inner models to respond to your prompt. Except humans and
    AI systems have different models of reality.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，当你与人类交谈时，你会以与提示AI相同的方式提示他们。人类和AI都使用内部模型来回应你的提示。只是人类和AI系统对现实有不同的模型。
- en: The human model draws on cognitive abilities, past experiences, theoretical
    knowledge, and real-time sensory data. Language Models, on the other hand, rely
    solely on language patterns.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 人类模型依赖于认知能力、过去的经验、理论知识和实时感官数据。而语言模型则仅依赖于语言模式。
- en: You could say that human software and AI software run on different operating
    systems.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以说人类软件和AI软件运行在不同的操作系统上。
- en: So when you interact with a Language Model, it’s as if you’re interacting with
    an alien. Sure the alien responds to your natural language but it doesn’t behave
    the same way a fellow human would — which means you’ll have to adapt your prompts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当你与语言模型交互时，就好像你在与一个外星人交互。当然，这个外星人会回应你的自然语言，但它的行为方式与人类不同，这意味着你需要调整你的提示。
- en: Compared to humans, AI systems require more details, precise explanations, clearer
    instructions, and a bit of repetition. You want to be intentional with each word
    you write — almost as if you were writing code.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类相比，AI系统需要更多细节、精确的解释、清晰的指示和一些重复。你希望每个写出的词都是有意义的，就像你在编写代码一样。
- en: The analogy of “English as a programming language” is spot-on until you consider
    one crucial detail. Unlike with code, every single prompt gets a response, regardless
    of its quality. Even prompts that include typos and unclear sentences generate
    *something*. In contrast, incorrect code doesn’t generate anything (except for
    frustrating error messages).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: “英语作为一种编程语言”的类比在考虑一个至关重要的细节之前是完美的。与代码不同，每个提示都会得到回应，无论其质量如何。即使包含拼写错误和不清晰句子的提示也会产生*某种东西*。相比之下，不正确的代码不会产生任何东西（除了令人沮丧的错误消息）。
- en: This always-generate-something property makes the illusion of ease even stronger.
    Messy prompts produce results, so why bother improving them?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种总是产生结果的特性使得轻松的错觉更加强烈。混乱的提示也会产生结果，那么为什么要费心改进它们呢？
- en: Casual AI users don’t feel the need to work on their prompt engineering skills,
    so they mistakenly believe it’s an easy feat.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一般的AI用户不觉得有必要提高他们的提示工程技能，所以他们错误地认为这是一件容易的事情。
- en: In a way, prompt engineering is like dancing. You watch a tutorial on TikTok
    where a professional dancer demonstrates a few hip-hop steps and you think “Oh
    this is actually easy!”
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，提示工程就像跳舞。你在TikTok上看了一个专业舞者演示了一些嘻哈舞步的教程，你会觉得“哦，这其实很容易！”
- en: Notice how easy this move looks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这一举动看起来多么容易。
- en: But once you try to reproduce those very same “easy steps,” you realize your
    body has its own laws of physics. All of a sudden, moving sideways while bouncing
    your body and waving your arms seems impossible.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 但是一旦你尝试重现那些“容易的步骤”，你会意识到你的身体有自己的物理规律。突然间，一边移动身体，一边挥舞着手臂似乎是不可能的。
- en: Just like dancing, Prompt Engineering is harder than it looks, but it gets easier
    with every prompt you write.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 就像跳舞一样，提示工程比看起来更难，但每写一个提示都会变得更容易。
- en: 💡 You don’t need prompt ideas, you need problems
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💡 你不需要提示的想法，你需要问题
- en: 'The best prompts come from good ideas, and good ideas are born from interesting
    problems. Here’s a simple framework you can use to find problems worth solving:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的提示来自好的想法，而好的想法源于有趣的问题。以下是一个简单的框架，你可以用它来找到值得解决的问题：
- en: List all the tasks you carry out on a computer (for work or otherwise).
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出你在计算机上进行的所有任务（工作或其他）。
- en: Isolate the tasks that require text analysis and/or writing.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分离需要文本分析和/或撰写的任务。
- en: Describe the results you want to achieve from each task.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述你希望从每个任务中实现的结果。
- en: '**Example of a useful problem:** “Every Monday, I file a performance report
    to my client for each advertisement I ran the week before. I need to comment on
    every Key Performance Indicator (KPI) to make the data easier to understand by
    non-experts.”'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**有用问题的例子：** “每个星期一，我向客户提交上周运行的每个广告的绩效报告。我需要评论每个关键绩效指标（KPI），以便非专业人士更容易理解数据。”'
- en: '**How prompting can help:** Write a prompt that comments on each KPI you submit.
    Ask your AI model to use storytelling techniques and fun analogies to make the
    report lighter and more accessible.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示如何帮助：** 撰写每个提交的KPI的提示。要求你的AI模型使用讲故事的技巧和有趣的类比，使报告更轻松易懂。'
- en: 💡 Watch out for AI hallucinations
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💡 小心AI幻觉
- en: Whatever the model you pick, remember that chatbots often hallucinate, writing
    information that’s factually wrong. This behavior stems from LLMs being trained
    on giant datasets filled with logical errors and nonsense.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择哪种模型，记住聊天机器人经常会产生幻觉，写出事实错误的信息。这种行为源自LLMs在巨大的数据集上训练，这些数据集充满了逻辑错误和胡言乱语。
- en: Fine-tuning and Reinforcement Learning based on Human Feedback (RLHF) helps
    LLMs reduce inaccuracies and bias, but there are still loopholes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于人类反馈的微调和强化学习（RLHF）有助于LLMs减少不准确性和偏见，但仍然存在漏洞。
- en: 'I took a few swings on the hallucination problem in other pieces. You’ll find
    the links below. For now, here’s a sneak peek:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我在其他文章中已经对幻觉问题进行了一些尝试。你可以在下面找到链接。现在，这里是一个预览：
- en: You may think chatbots tell the truth by default, and sometimes, they happen
    to hallucinate. But it’s more accurate to say chatbots write pure bullshit that
    sometimes matches reality.
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可能认为聊天机器人默认会说实话，有时候他们会产生幻觉。但更准确地说，聊天机器人写出纯粹的胡说八道，有时候与现实相匹配。
- en: ''
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By “bullshit,” I don’t mean the slang we use to describe nonsense, but a philosophical
    definition brought by Harry Frankfurt.
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过“胡说八道”，我不是指我们用来描述无意义的俚语，而是哈里·弗兰克福提出的一个哲学定义。
- en: ''
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Harry Frankfurt described bullshit as information that has no relationship to
    reality. When you lie, you distort reality. When you tell the truth, you describe
    your representation of reality. But when you bullshit, you make things up with
    no consideration of what reality might be like.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 哈里·弗兰克福特将胡说八道描述为与现实没有关系的信息。当你撒谎时，你扭曲了现实。当你说真话时，你描述了你对现实的表达。但是当你胡说八道时，你编造了一些与现实无关的东西。
- en: ''
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*“It is just this lack of connection to a concern with truth —* ***this indifference
    to how things really are*** *— that I regard as of the essence of bullshit.*'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“正是这种与真相关联的缺乏连接——* ***对事物真实情况的漠不关心*** *——我认为是胡说八道的本质。”*'
- en: ''
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*This points to a similar and fundamental aspect of the essential nature of
    bullshit:* ***although it is produced without concern with the truth, it need
    not be false****.”*'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*这指向了胡说八道的一个类似而基本的方面：* ***尽管它是在不关心真相的情况下产生的，但它不一定是错误的****。”*'
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*—* [*On Bullshit by Harry Frankfurt*](https://www.goodreads.com/author/quotes/219.Harry_G_Frankfurt)
    *[Emphasis mine].*'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*—* [*《胡说八道》哈里·弗兰克福特著*](https://www.goodreads.com/author/quotes/219.Harry_G_Frankfurt)
    *[Emphasis mine].*'
- en: Language Models are highly-plausible bullshitters that often land on truth,
    so you can’t fully trust them. You must fact-check their outputs, especially when
    dealing with non-fiction.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是高度可信的胡说八道者，通常能够接近真相，因此你不能完全信任它们。在处理非虚构内容时，你必须对它们的输出进行事实核查。
- en: '[](https://nabilalouani.substack.com/p/artificial-disinformation-can-chatbots?source=post_page-----23133dc85550--------------------------------)
    [## Artificial Disinformation: Can Chatbots Destroy Trust on the Internet?'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://nabilalouani.substack.com/p/artificial-disinformation-can-chatbots?source=post_page-----23133dc85550--------------------------------)
    [## 人工虚假信息：聊天机器人能破坏互联网上的信任吗？'
- en: Soon after ChatGPT came out, a clock started ticking inside my head. It's like
    when you see a bolt of lightning slice…
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ChatGPT发布后不久，我脑海中开始倒计时。就像当你看到一道闪电划过时...
- en: nabilalouani.substack.com](https://nabilalouani.substack.com/p/artificial-disinformation-can-chatbots?source=post_page-----23133dc85550--------------------------------)
    [](https://nabilalouani.substack.com/p/chatgpt-hype-is-proof-nobody-really?source=post_page-----23133dc85550--------------------------------)
    [## ChatGPT Hype is Proof Nobody Really Understands AI
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: nabilalouani.substack.com](https://nabilalouani.substack.com/p/artificial-disinformation-can-chatbots?source=post_page-----23133dc85550--------------------------------)
    [](https://nabilalouani.substack.com/p/chatgpt-hype-is-proof-nobody-really?source=post_page-----23133dc85550--------------------------------)
    [## 聊天GPT的炒作证明没有人真正理解人工智能
- en: Large Language Models are dumber than your neighbor's cat
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型比你邻居的猫更笨
- en: nabilalouani.substack.com](https://nabilalouani.substack.com/p/chatgpt-hype-is-proof-nobody-really?source=post_page-----23133dc85550--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: nabilalouani.substack.com](https://nabilalouani.substack.com/p/chatgpt-hype-is-proof-nobody-really?source=post_page-----23133dc85550--------------------------------)
- en: 🟢 The Basics of Prompting
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 提示的基础知识
- en: Each prompt is a bridge between what you want and what your Language Model generates.
    The shape of your bridge depends on the problem you want to solve, but the underlying
    structure remains the same.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 每个提示都是你想要解决的问题和你的语言模型生成的内容之间的桥梁。你的桥的形状取决于你想要解决的问题，但基本结构保持不变。
- en: 'Picture this structure as six pillars:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这个结构就像是六根柱子：
- en: Be specific.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要具体明确。
- en: Use placeholders <like_this> to build flexible templates. (More on this in a
    dedicated section).
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用占位符<like_this>构建灵活的模板。（在专门的部分中详细介绍）。
- en: Prioritize **what to do** over what not to do.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优先考虑**要做什么**而不是不要做什么。
- en: Specify the desired format of the output. (More on this in a dedicated section).
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定所需输出的格式。（在专门的部分中详细介绍）。
- en: 'Use double hashtags like this ## to separate different parts of your prompt.
    A prompt can include instructions, examples, and the desired format.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用双井号（##）来分隔提示的不同部分。提示可以包括说明、示例和所需格式。
- en: Revise your prompt to remove the fluff.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改你的提示以去除废话。
- en: 'Here’s an example:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子：
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 🟢 Specify the context (also called “Priming”)
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 指定上下文（也称为“启动”）
- en: For each question you write, your Large Language Model can generate thousands
    of different answers. When you provide context, you help your LLM narrow down
    the range of possible outcomes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你写的每个问题，你的大型语言模型可以生成成千上万个不同的答案。当你提供上下文时，你帮助你的LLM缩小了可能结果的范围。
- en: Say you want a non-boring meal plan for the upcoming week. Adding your diet
    restrictions and personal preferences makes it more likely to get relevant suggestions
    for every single meal.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要一个即将到来的一周中不无聊的饮食计划。添加你的饮食限制和个人偏好可以更有可能获得每餐相关的建议。
- en: There are multiple ways you can introduce context into your prompt. It’s like
    mentally preparing your Language Model for the task, hence the name “Priming.”
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方式可以在提示中引入上下文。这就像是为任务做心理准备，因此被称为“启动”。
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 🟢 Specify the desired format
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 指定所需格式
- en: This one is straightforward. All you have to do is add a sentence to your prompt
    where you describe the format you want.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个很简单。你只需要在提示中添加一句描述所需格式的句子。
- en: 'Here’s a list you can draw from:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个你可以参考的列表：
- en: Bullet-points;
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简要说明；
- en: Articles and blog posts;
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文章和博客文章；
- en: Essays and research papers;
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文和研究论文；
- en: Short stories and creative writing pieces;
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短篇小说和创意写作作品；
- en: Poems and song lyrics;
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诗歌和歌词；
- en: Newsletters and press releases;
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通讯和新闻稿；
- en: Social media posts and captions;
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交媒体帖子和标题；
- en: Advertisements and marketing copy;
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广告和营销文案；
- en: Email templates and business correspondence;
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件模板和商务往来；
- en: Product descriptions and reviews;
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品描述和评论；
- en: Tutorials and how-to guides;
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教程和指南；
- en: Frequently Asked Questions (FAQs);
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见问题解答（FAQ）；
- en: Transcripts and interviews;
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录和采访；
- en: Reports and memos;
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告和备忘录；
- en: Screenplays and scripts for plays or podcasts;
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电影剧本和戏剧或播客脚本；
- en: Speeches and presentations;
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演讲和展示；
- en: Summaries and abstracts;
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要和概述；
- en: Technical documentation and manuals;
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术文档和手册；
- en: Educational materials, such as lesson plans or course syllabi;
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教育材料，如教案或课程大纲；
- en: Opinion pieces and editorials;
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观点文章和社论；
- en: Personal statements, cover letters, and resumes.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个人陈述、求职信和简历。
- en: 'Below are three examples of how to introduce format inside a basic prompt:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何在基本提示中引入格式的三个示例：
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Note:** if you use a non-specialized Language Model to generate legal contracts,
    make sure you run them by legal experts.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：**如果你使用非专业的语言模型生成法律合同，请确保请法律专家审核。'
- en: 🟢 Use <placeholders>
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 使用<占位符>
- en: Placeholders <like_this> help you achieve two separate goals.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <像这样的>占位符可以帮助你实现两个独立的目标。
- en: Use <placeholders> to write flexible prompts that can take different inputs.
    You have to indicate the content of each placeholder in your prompt. In this case,
    **a placeholder is a parameter.**
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用<占位符>编写灵活的提示，可以接受不同的输入。你必须在提示中指示每个占位符的内容。在这种情况下，**占位符是一个参数**。
- en: Use empty <placeholders> to illustrate the desired format. Here you don’t have
    to write the content of each placeholder. Your LLM will guess what each placeholder
    stands for, especially when you use known frameworks like User Stories or cover
    letters. In this case, **a placeholder is an instruction.**
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用空的<占位符>来说明所需格式。在这里，你不需要编写每个占位符的内容。你的LLM会猜测每个占位符代表什么，特别是当你使用像用户故事或求职信这样的已知框架时。在这种情况下，**占位符是一条指令**。
- en: 🟢 How to use placeholders as parameters
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 🟢 如何将占位符用作参数
- en: '[PRE6]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 🟢 How to use placeholders as instructions
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 🟢 如何将占位符用作指令
- en: '[PRE7]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 🟢 Specify the style/tone
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 指定风格/语气
- en: Each chatbot has a default style defined by its creators. For instance, ChatGPT
    sounds friendly and nuanced, but you can ask it to change its tone to fit your
    preferences and needs.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 每个聊天机器人都有一个默认的风格，由其创建者定义。例如，ChatGPT听起来友好而细致，但你可以要求它改变语气以适应你的偏好和需求。
- en: You can even ask your Language Model to mimic the tone of a fictional/real person.
    Usually, the result is an over-the-top parody of whoever ChatGPT tries to emulate.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以要求你的语言模型模仿一个虚构/真实人物的语气。通常，结果是ChatGPT试图模仿的人的夸张模仿。
- en: 'Here are a few examples of styles you can pick from:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些你可以选择的风格示例：
- en: '**Generic styles:** formal, informal, persuasive, conversational, sarcastic,
    dramatic, condescending, nuanced, biased, humorous, optimistic, pessimistic, etc.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用风格：**正式、非正式、有说服力的、对话式的、讽刺的、戏剧性的、居高临下的、细微的、有偏见的、幽默的、乐观的、悲观的等。'
- en: '**Domain-specific styles:** academic, legal, political, technical, medical,
    news, scientific, marketing, creative, instructional, etc.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域特定风格：**学术、法律、政治、技术、医学、新闻、科学、营销、创意、教学等。'
- en: '**Mimicking the style of a real person:** Agatha Christie, Daniel Kahneman,
    J.K Rowling, James Baldwin, Hajime Isayama, etc.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模仿真实人物的风格：**阿加莎·克里斯蒂、丹尼尔·卡尼曼、J.K.罗琳、詹姆斯·鲍德温、諫山創等。'
- en: 'Here’s how you can specify the style in a prompt:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何在提示中指定风格的示例：
- en: '[PRE8]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 🟢 Specify the length of the desired response
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 指定所需响应的长度
- en: Length is a proxy for the level of detail you want in a response. Length is
    also a constraint you sometimes must consider when writing specific formats like
    tweets, SEO descriptions, and titles.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 长度是你希望回应中包含的详细程度的代理。长度有时也是你在写特定格式（如推文、SEO描述和标题）时必须考虑的约束。
- en: 'Here are three examples of how you can specify length in a prompt:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何在提示中指定长度的三个示例：
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 🟢Specify the target audience
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢指定目标受众
- en: Language Models are trained on billions of words taken from diverse sources,
    including Wikipedia, research papers, and Reddit. Each source has its own audience,
    and each audience consumes information differently.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是在包括维基百科、研究论文和Reddit在内的各种来源中训练的数十亿字。每个来源都有自己的受众，每个受众都以不同的方式消费信息。
- en: When you specify the target audience, you tell your model to adapt the content,
    the examples, and the vocabulary.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当你指定目标受众时，你告诉你的模型要调整内容、例子和词汇。
- en: 'Consider two potential audiences for a prompt about the benefits of exercise:
    general adult readers and medical professionals.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个关于运动好处的提示的两个潜在受众：一般成年读者和医学专业人士。
- en: For the first audience, you want your Language Model to use relatable examples
    and simple explanations. In contrast, the second audience would expect you to
    evoke studies and use technical terminology.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个受众，你希望你的语言模型使用易于理解的例子和简单的解释。相比之下，第二个受众希望你唤起研究并使用专业术语。
- en: Even if the topic remains the same, the desired output can be extremely different.
    That’s why you want to indicate the target audience in your prompts/
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 即使主题保持不变，期望的输出也可能完全不同。这就是为什么你要在提示中指明目标受众的原因。
- en: 'Here are what the prompts would look like for the “benefits of exercise” example:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是“运动好处”示例的提示：
- en: '[PRE10]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: One common mistake people make when writing prompts is to consider “style” and
    “target audience” as the same parameter. In reality, the style determines *how
    the text sounds* and the target audience decides *which words to use*.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 人们在写提示时常犯的一个错误是将“风格”和“目标受众”视为相同的参数。实际上，风格决定了*文本听起来的样子*，而目标受众决定了*使用哪些词语*。
- en: 'Below is another set of examples of how to introduce the target audience in
    a prompt:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何在提示中介绍目标受众的另一组示例：
- en: '[PRE11]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 🟢 Many-Examples Prompting
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 Many-Examples提示
- en: I stole this technique from [Simon Willison](https://twitter.com/simonw), who
    uses Many-Examples prompting to push Language Models beyond their comfort zone.
    When ChatGPT and other models write a response, they predict the most likely answer
    — but the most likely answer is often the least creative.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我从[Simon Willison](https://twitter.com/simonw)那里偷了这个技巧，他使用Many-Examples提示来推动语言模型超越其舒适区。当ChatGPT和其他模型写出回应时，它们会预测最可能的答案，但最可能的答案往往是最不创造性的。
- en: If you ask your model to exhaust its reserve of common answers, however, it’ll
    have no choice but to explore new possibilities.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要求你的模型耗尽常见答案的储备，它将别无选择，只能探索新的可能性。
- en: 'Many-Examples prompting is particularly useful for tasks that involve imagination.
    Here are a few instances where the technique can shine:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Many-Examples提示对涉及想象力的任务特别有用。以下是一些技术可以发挥作用的实例：
- en: Optimize code by examining a list of possible variations;
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过检查可能的变体列表来优化代码；
- en: Brainstorm ideas for new products/features;
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为新产品/功能进行头脑风暴；
- en: Inspire creative names for products/domains/companies;
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激发产品/域名/公司的创意名称；
- en: Reformulate sentences or create slogans;
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新构造句子或创建口号；
- en: Find a destination for your holidays;
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找一个度假的目的地；
- en: Upgrade your CV by looking for new skills to learn;
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过寻找要学习的新技能来升级你的简历；
- en: Find new hobbies;
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找新的爱好；
- en: Browse of list of books/movies/songs based on your personal preferences. (Bing
    works better than ChatGPT for this one);
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据个人喜好浏览书籍/电影/歌曲列表。（Bing在这方面比ChatGPT更有效）；
- en: Explore diverse perspectives on the same topic;
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索同一主题的多元化观点；
- en: Generate as much information as possible for a given topic. Here you can combine
    Many-Examples prompting with Knowledge Generation (more on this later).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为给定主题生成尽可能多的信息。在这里，你可以将Many-Examples提示与知识生成结合起来（稍后会详细介绍）。
- en: 'Here are two basic formulations you can use Many-Examples prompting:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你可以使用的两种基本Many-Examples提示的公式：
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Use the second option when the first one fails.Sometimes when you ask for a
    large number of examples, ChatGPT will complain. It’ll say something like: “As
    an AI language model, I am not aware of 50 distinct possible answers. However,
    I can provide you with a list of several answers that can be helpful.”'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当第一个选项失败时使用第二个选项。有时当你要求大量的例子时，ChatGPT会抱怨。它会说类似于：“作为一个AI语言模型，我不知道50个不同的可能答案。但是，我可以为你提供一些有用的答案列表。”
- en: From there, ChatGPT will stop at 20 possible answers, but you can use the “Please
    add 5 more distinct answers” to keep the generative ball rolling. Once the same
    answers keep showing up, it means you’ve emptied your model’s creative jar.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里开始，ChatGPT将停在20个可能的答案上，但你可以使用“请再添加5个不同的答案”来保持生成的连续性。一旦相同的答案不断出现，这意味着你已经耗尽了模型的创造力。
- en: 🟢 Temperature Control
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 温度控制
- en: Temperature is a parameter that influences the “randomness” of the response
    generated by your language model. It typically ranges from 0 to 1, but in some
    instances, you can bring the temperature beyond 1.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 温度是影响语言模型生成的响应“随机性”的参数。它通常范围从0到1，但在某些情况下，你可以将温度提高到1以上。
- en: Lower temperatures (between 0.1 and 0.3) produce the most likely response. In
    other words, you get the most “conservative” output. **Low temperatures are particularly
    useful when generating code because you get the most stable output.**
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低温（在0.1和0.3之间）产生最可能的响应。换句话说，你会得到最“保守”的输出。**在生成代码时，低温特别有用，因为你会得到最稳定的输出。**
- en: Higher temperatures (between 0.7 and 0.9) lead to more creative responses.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高的温度（在0.7和0.9之间）会导致更有创意的响应。
- en: 'One way to memorize the use of temperature: “Cold for code; hot for prose.”
    Here’s how you can introduce it in a prompt:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 记住使用温度的一种方法：“冷用于代码；热用于散文。”以下是你可以在提示中介绍它的方式：
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 🟢 Zero-Shot Prompting (no examples)
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 零样本提示（没有例子）
- en: Zero-shot prompting is to write an instruction for your AI model without providing
    context or examples. The basic format of zero-shot involves two parts often called
    “Text” and “Desired Result.”
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本提示是为你的AI模型编写指令，而不提供上下文或例子。零样本的基本格式通常包括通常称为“文本”和“期望结果”的两部分。
- en: 'Here are two examples of zero-shot prompts:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是两个零样本提示的例子：
- en: '[PRE14]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This particular format of zero-shot prompting is rare outside of AI labs where
    experts use the technique to test the capabilities of their models.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特定格式的零样本提示在AI实验室之外很少见，专家们在那里使用这种技术来测试他们的模型的能力。
- en: The most common format of zero-shot prompting is the one you use naturally.
    You just type your question. You don’t need the “Text + Desired output” format.
    That’s because user-friendly models like ChatGPT and Bard are optimized for dialogue
    — and every dialogue is a series of zero-shots.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本提示的最常见格式是你自然而然地使用的格式。你只需输入你的问题。你不需要“文本+期望输出”的格式。这是因为像ChatGPT和Bard这样用户友好的模型都经过了对话优化
    —— 每一次对话都是一次零样本。
- en: You could say chatbots are zero-shot machines.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以说聊天机器人是零样本机器。
- en: 🟢 Few-Shot Prompting (several high-quality examples)
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 少样本提示（几个高质量的例子）
- en: Few-shot prompting is also known as in-context learning. You give your Language
    Model a bunch of high-quality examples to improve its “guesses.” The number of
    examples depends on your model, but you can start with three to five inputs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本提示也被称为上下文学习。你给你的语言模型一堆高质量的例子来改善它的“猜测”。例子的数量取决于你的模型，但你可以从三到五个输入开始。
- en: 'Here’s an example:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个例子：
- en: '[PRE15]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It’s not necessary to add a number to each example (like #1, #2, #3), but doing
    so can improve the output. Another element you want to add to your examples is
    “noise.”'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要为每个例子添加编号（如#1、#2、#3），但这样做可以改善输出。你想要添加到你的例子中的另一个元素是“噪音”。
- en: Noise is information that’s not useful for the task given to your Language Model.
    In the “Tone” examples, I introduced misleading sentences to confuse the system
    and force it to focus on the “signal.”
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 噪音是对你的语言模型给定的任务没有用的信息。在“语气”例子中，我引入了误导性的句子来混淆系统，并迫使它专注于“信号”。
- en: If you make the task too obvious for your Language Model, it may underperform
    when faced with complex examples.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你让任务对你的语言模型来说太明显，当面对复杂的例子时，它可能表现不佳。
- en: 🟢 Zero-Shot/Few-Shot — The simple version
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 零样本/少样本 — 简单版本
- en: 'If you want to remember something from zero-shot and few-shot, remember the
    following:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想记住零样本和少样本中的一些东西，记住以下内容：
- en: '**When your Language Model fails to give you the desired response, add high-quality
    examples to your prompt.**'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当你的语言模型无法给出你想要的响应时，向你的提示添加高质量的例子。**'
- en: Here’s an illustration of how few-shot can help you improve ChatGPT’s output.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是少量提示如何帮助你改进ChatGPT的输出的示例。
- en: '[PRE16]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 💡 In-context Learning vs. Chat History
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💡 上下文学习 vs. 聊天历史
- en: The first usable version of every Language Model is often a jack of all trades.
    It can perform a variety of tasks at an average-ish level. If you want to specialize
    your model (and consequently improve its output), you have two options. You could
    either retrain it using new specific data or use in-context learning. AI people
    usually use a combination of both.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 每种语言模型的第一个可用版本通常是万金油。它可以以一种平均水平执行各种任务。如果你想让你的模型专业化（从而提高其输出），你有两个选择。你可以使用新的特定数据对其进行重新训练，或者使用上下文学习。AI人员通常会两者结合使用。
- en: In-context learning is a prompting technique that allows you to steer the responses
    of your LLMs in a specific direction. All you need are a few examples, just like
    few-shot prompting.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习是一种提示技术，可以让你引导语言模型的响应朝特定方向发展。你只需要一些示例，就像少量提示一样。
- en: The reason AI experts love in-context learning is efficiency. Instead of using
    a ton of high-quality data to adapt a raw model, you can use a very limited number
    of well-formatted examples.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: AI专家喜欢上下文学习的原因是效率。你不需要使用大量高质量的数据来调整原始模型，只需使用非常有限数量的格式良好的示例即可。
- en: Here’s a summary of In-Context Learning published by Princeton University.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这是普林斯顿大学发布的上下文学习摘要。
- en: In-context learning was popularized in the original GPT-3 paper as a way to
    use language models to learn tasks given only a few examples.[[1]](http://ai.stanford.edu/blog/understanding-incontext/#f1)
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 上下文学习在原始GPT-3论文中被推广为一种只使用少量示例就能让语言模型学习任务的方法。[[1]](http://ai.stanford.edu/blog/understanding-incontext/#f1)
- en: ''
  id: totrans-206
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: During in-context learning, we give the LM a prompt that consists of a list
    of input-output pairs that demonstrate a task. At the end of the prompt, we append
    a test input and allow the LM to make a prediction just by conditioning on the
    prompt and predicting the next tokens.
  id: totrans-207
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在上下文学习期间，我们给语言模型一个提示，其中包含一系列展示任务的输入-输出对。在提示的末尾，我们附加一个测试输入，并允许语言模型仅通过对提示进行条件化并预测下一个标记来进行预测。
- en: ''
  id: totrans-208
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To correctly answer the two prompts below, the model needs to read the training
    examples to figure out the input distribution (financial or general news), output
    distribution (Positive/Negative or topic), input-output mapping (sentiment or
    topic classification), and the formatting.
  id: totrans-209
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要正确回答下面的两个提示，模型需要阅读训练示例以找出输入分布（财经或一般新闻）、输出分布（积极/消极或主题）、输入-输出映射（情感或主题分类）和格式。
- en: '![](../Images/97294ea04f70fdc5faf9722c5942496f.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97294ea04f70fdc5faf9722c5942496f.png)'
- en: Examples of In-Context Learning. [Source](http://ai.stanford.edu/blog/understanding-incontext/)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习的例子。[来源](http://ai.stanford.edu/blog/understanding-incontext/)
- en: You can derive numerous applications from in-context learning — such as generating
    code, automated spreadsheets, and numerous other text-oriented tasks.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从上下文学习中得到许多应用，比如生成代码、自动化电子表格和许多其他面向文本的任务。
- en: 'ChatGPT, however, is another story. OpenAI sacrificed ChatGPT’s ability to
    use in-context learning to introduce a new feature: Chat history. Sure, you lose
    the flexibility of the model, but you get a user-friendly interface that allows
    for lengthy conversations.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，ChatGPT又是另一回事。OpenAI牺牲了ChatGPT使用上下文学习的能力，引入了一个新功能：聊天历史。当然，你会失去模型的灵活性，但你会得到一个用户友好的界面，可以进行长时间的对话。
- en: You could argue chat history is a variant of in-context learning because ChatGPT’s
    responses evolve depending on the content of the chat history tab you’re using.
    For instance, if you feed a list of recipes into a ChatGPT tab, it’ll be able
    to perform specific tasks on your input*.* This involves summary, continuation,
    and editing.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以认为聊天历史是上下文学习的一种变体，因为ChatGPT的响应会根据你使用的聊天历史标签的内容而演变。例如，如果你将一系列食谱输入到ChatGPT标签中，它将能够对你的输入执行特定任务*。*
    这涉及摘要、延续和编辑。
- en: Why is this important?
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这为什么重要？
- en: 'Depending on your needs and future discoveries, you may need to pick one of
    two options:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的需求和未来的发现，你可能需要选择以下两个选项中的一个：
- en: Use in-context learning to fine-tune a “raw” model like GPT-4, OpenLLaMa, or
    Falcon. In other words, you can create a customized chatbot but the process can
    be tedious.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上下文学习来微调像GPT-4、OpenLLaMa或Falcon这样的“原始”模型。换句话说，你可以创建一个定制的聊天机器人，但这个过程可能会很繁琐。
- en: Use chat history to leverage “memory” and “long conversations.” It’s easier
    to customize your output but the quality may go down over time.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用“记忆”和“长对话”来使用聊天历史。这样更容易定制你的输出，但随着时间的推移，质量可能会下降。
- en: 🟡 Chain of Thought Prompting
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟡 思维链提示
- en: Chain of Thought (CoT) prompting means you tell your Language Model to **reason
    step by step** before arriving at a final response. It’s as if you ask your model
    to think out loud.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链（CoT）提示意味着在最终响应之前告诉你的语言模型**逐步推理**。就好像你要求你的模型大声思考一样。
- en: Suppose I ask you to calculate 4x3\. You could instantly compute the operation
    inside your head and say, “12.” But if I ask you to use a “chain of thought,”
    you’d split your reasoning into four steps.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我让你计算 4x3。你可以立即在脑海中计算这个操作，并说：“12”。但如果我要求你使用“思维链”，你会将你的推理分成四个步骤。
- en: 4x3 = 4+4+4
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4x3 = 4+4+4
- en: 4+4+4 = (4+4) + 4
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4+4+4 = (4+4) + 4
- en: (4+4) + 4 = 8+4
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4+4) + 4 = 8+4
- en: 8+4 = 12
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 8+4 = 12
- en: CoT prompts are typically used to solve logical riddles. The idea is to break
    down complex problems into smaller, more manageable questions.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 提示通常用于解决逻辑谜题。其思想是将复杂的问题分解为更小、更易管理的问题。
- en: Language Models predict the next token in a sequence of words, and their predictions
    are more accurate when they deal with common patterns found in abundance inside
    training data. But sometimes, you need to tap into uncommon patterns to answer
    uncommon questions.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型预测单词序列中的下一个标记，当它们处理训练数据中丰富的常见模式时，它们的预测更准确。但有时，你需要利用不常见的模式来回答不常见的问题。
- en: 'Consider the following riddle: “If eggs are $0.12 a dozen, how many eggs can
    you get for a dollar?”'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下谜题：“如果鸡蛋一打卖 0.12 美元，你可以用一美元买多少个鸡蛋？”
- en: 'If you force ChatGPT to give an immediate response, it’ll write: ”You can get
    10 dozen eggs for a dollar,” which is a wrong answer.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你强迫 ChatGPT 立即给出答案，它会写道：“你可以用一美元买 10 打鸡蛋”，这是一个错误的答案。
- en: '![](../Images/7a46f56a22764a947e872e1b4cd6744b.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a46f56a22764a947e872e1b4cd6744b.png)'
- en: The prompt forces ChatGPT to give an immediate answer (no chain of thought).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 提示强制 ChatGPT 立即给出答案（没有思维链）。
- en: Now, if you ask ChatGPT to reason step by step, it gives a different answer
    — the right answer.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你要求 ChatGPT 逐步推理，它会给出一个不同的答案——正确的答案。
- en: '![](../Images/56c5edbad984077a2d309882e6a4b28b.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56c5edbad984077a2d309882e6a4b28b.png)'
- en: The latest versions of ChatGPT often (but not always) use CoT when they respond
    to prompts.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 的最新版本通常（但不总是）在响应提示时使用 CoT。
- en: Here’s another example that illustrates the difference between standard prompting
    and Chain of Thought.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是另一个例子，说明了标准提示和思维链之间的区别。
- en: '![](../Images/6673d19d2c756c392c58499590e51e37.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6673d19d2c756c392c58499590e51e37.png)'
- en: Notice the addition of a chain of thought in the second input (highlighted in
    blue). [Source](https://learnprompting.org/docs/intermediate/chain_of_thought#fn-1)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 注意第二个输入中思维链的添加（以蓝色突出显示）。[来源](https://learnprompting.org/docs/intermediate/chain_of_thought#fn-1)
- en: There are two ways you can use Chain of Thought prompting.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用思维链提示的两种方式。
- en: 🟢 Zero-Shot Chain of Thought
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 🟢 零提示思维链
- en: 'Add one sentence at the end of your prompt to make your Language Model apply
    CoT. The top-performing sentences I found are:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的提示末尾添加一句话，让你的语言模型应用 CoT。我发现的表现最好的句子是：
- en: '**“….Let’s think step by step.”**'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“….让我们逐步思考。”**'
- en: '**“….Please proceed step by step to be sure you arrive at the right answer.”**'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“….请逐步进行，确保你得到正确的答案。”**'
- en: 'Here’s how you can incorporate them in your prompts:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何在你的提示中加入它们：
- en: '[PRE17]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Usually, Zero-shot CoT is enough to solve logic puzzles. But if your Language
    Model fails, you can try the second flavor of CoT prompting.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，零提示思维链足以解决逻辑谜题。但如果你的语言模型失败了，你可以尝试第二种思维链提示。
- en: 🟡 **Few-Shot Chain of Thought**
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 🟡 **少量提示思维链**
- en: Much like the standard few-shot prompting, you want to feed your Language Model
    high-quality examples before submitting your question. Each example must include
    multiple steps of reasoning— and the more logical steps you add, the better the
    response.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 就像标准的少量提示一样，你需要在提交问题之前向你的语言模型提供高质量的示例。每个示例必须包括多个推理步骤，而且你添加的逻辑步骤越多，响应就越好。
- en: 'Here’s an example of a prompt that combines Few-Shot and Chain of Thought:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个结合了少量提示和思维链的提示的例子：
- en: '[PRE18]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 🟢 Role Prompting
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 角色提示
- en: 'Assigning a specific role to your Language Model helps it capture more and
    better semantic relationships (ie: logic and meaning).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 为你的语言模型分配一个特定的角色有助于它捕捉更多和更好的语义关系（即：逻辑和含义）。
- en: In a way, Role Prompting helps you nudge your model to focus on specific information
    inside its training data. It’s a shortcut to specify many variables at once —
    like context, style, perspective, and vocabulary.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，角色提示帮助你促使你的模型专注于其训练数据中的特定信息。这是一种一次性指定多个变量的快捷方式，比如上下文、风格、观点和词汇。
- en: Depending on the task at hand, you can use different versions of Role Prompting.
    Below are a few examples that may inspire you.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 根据手头的任务，你可以使用不同版本的角色提示。以下是一些可能会启发你的例子。
- en: Mimic a personal style.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模仿个人风格。
- en: Emulate specific expertise like a lawyer or a strategic planner.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模仿像律师或战略规划师这样的特定专业知识。
- en: Emulate your counterpart in a conversation like your professor, your ex, or
    your boss.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模仿你在对话中的对应者，比如你的教授、前任或老板。
- en: Generate multiple points of view.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成多个观点。
- en: Behave like a mini-app that corrects typos, compiles your code, or generates
    Excel formulas.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表现得像一个纠正拼写错误、编译你的代码或生成Excel公式的迷你应用程序。
- en: '[PRE19]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: There’s an advanced version of role prompting that we’ll explore in a specific
    section called “All-In-One Prompting.”
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在一个名为“全能提示”的特定部分中探讨角色提示的高级版本。
- en: 🟢 Knowledge Generation Prompting
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 知识生成提示
- en: The goal of Knowledge Generation prompting is to make your Language Model retrieve
    specific bits of information from its giant pool of training data. Picture this
    technique as asking your model to do some research before writing a final response.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 知识生成提示的目标是让你的语言模型从其庞大的训练数据中检索特定的信息片段。想象一下这种技术就像是在写最终回答之前要求你的模型进行一些研究。
- en: Suppose you want your model to write a blog post about growing flowers on your
    balcony. Instead of asking your model to write the blog right away, you can prompt
    it to generate key points about gardening, flowers, and space management.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想让你的模型写一篇关于在阳台上种花的博客文章。与其立即要求你的模型写博客，不如提示它生成关于园艺、花卉和空间管理的关键要点。
- en: Once you get the desired key point, make sure to attend to your fact-checking
    duties. From there, prompt your model to use the “knowledge” it generated to write
    an article.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你得到了期望的关键点，一定要履行你的事实核查职责。从那里，提示你的模型使用它生成的“知识”来撰写一篇文章。
- en: Knowledge Generation improves the output quality because it forces your model
    to focus on specific points instead of trying to answer a vague prompt.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 知识生成可以提高输出质量，因为它迫使你的模型专注于特定的要点，而不是试图回答模糊的提示。
- en: 'Here’s how you can introduce Knowledge Generation into your prompts:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何将知识生成引入到你的提示中的方法：
- en: '[PRE20]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Knowledge Generation Prompting and ChatGPT Plugins**'
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**知识生成提示和ChatGPT插件**'
- en: You can use ChatGPT plugins to both generate knowledge and help with fact-checking.
    Make sure to try as many plugins as possible because most of them are still clunky.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用ChatGPT插件来生成知识并帮助核实事实。一定要尝试尽可能多的插件，因为它们中的大多数仍然有些笨拙。
- en: 🟠 Knowledge Integration Prompting*
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟠 知识整合提示*
- en: The main weakness of Knowledge Generation prompting is the timeline. GPT-4’s
    training data stops in September 2021, which means all the content that came afterward
    is unknown to the model.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 知识生成提示的主要弱点是时间表。GPT-4的训练数据止于2021年9月，这意味着之后出现的所有内容对模型来说都是未知的。
- en: The cutoff date isn’t a problem when you deal with timeless topics like gardening,
    writing, and cooking, but if you’re chasing the latest information, you need a
    complementary trick.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 截止日期对于处理园艺、写作和烹饪等永恒话题并不是问题，但如果你追求最新信息，你需要一个补充的技巧。
- en: You can use plugins, chatbots with online browsing, or Knowledge Integration
    prompting.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用插件、带有在线浏览功能的聊天机器人，或者知识整合提示。
- en: All you have to do is feed recent data into your model to help it catch up with
    the news. In a way, you make your offline model integrate new knowledge.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 你所需要做的就是将最新的数据输入到你的模型中，以帮助它跟上新闻。在某种程度上，你让你的离线模型整合新的知识。
- en: For API users, [GPT-4 can process up to 32,000 tokens](https://www.semrush.com/blog/gpt-4/),
    which represent about 25,000 words. This includes both the user prompt and the
    answer. For users of ChatGPT Plus, GPT-4 can take up to 4096 tokens as input,
    which is approximately 3,000 words.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 对于API用户，[GPT-4可以处理高达32,000个标记](https://www.semrush.com/blog/gpt-4/)，大约相当于25,000个单词。这包括用户提示和答案。对于ChatGPT
    Plus的用户，GPT-4可以接受高达4096个标记的输入，大约相当于3,000个单词。
- en: You can use these 3,000 words and the chat history feature to “teach” ChatGPT-4
    new information. The model itself won’t integrate the data, but you can generate
    prompts that leverage the “new information” you just added.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用这 3,000 个单词和聊天记录功能来“教导” ChatGPT-4 新的信息。模型本身不会整合数据，但你可以生成利用刚刚添加的“新信息”的提示。
- en: 'Below is a framework you can use to apply Knowledge Integration prompting:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是你可以应用知识整合提示的框架：
- en: Find a relevant source, like a research paper or a documented article.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到相关的来源，比如研究论文或已记录的文章。
- en: Identify the most informative parts of the paper at hand.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定手头论文中最具信息量的部分。
- en: Cut the parts into chunks of 3,000 words.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些部分切成 3,000 个单词的片段。
- en: Feed the chunks into ChatGPT-4 and ask it to explain each section in simple
    words. You can also ask for quotes and examples.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些片段输入到 ChatGPT-4 中，并要求它用简单的话语解释每个部分。你也可以要求引用和例子。
- en: Use ChatGPT-4's output for a new prompt.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 ChatGPT-4 的输出进行新的提示。
- en: 'Example:'
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 例如：
- en: Let’s say you’re an AI researcher specializing in Large Language Models. Your
    current task is to reference material that’s relevant to your thesis.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一位专门研究大型语言模型的人工智能研究员。你目前的任务是引用与你的论文相关的材料。
- en: You found an interesting paper titled *Language Models Can Solve Computer Tasks*.
    You want to take notes before skimming the other 122 papers you bookmarked last
    week.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 你找到了一篇有趣的论文，标题为*语言模型可以解决计算机任务*。在浏览你上周收藏的其他 122 篇论文之前，你想先做笔记。
- en: Here are the steps you can follow to get ChatGPT to help you take quick notes.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你可以遵循的步骤，让 ChatGPT 帮助你快速记录笔记。
- en: First, identify the passage you want to summarize. In this example, we’ll select
    the discussion part which makes for about 1,000 words.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，确定你想要总结的段落。在这个例子中，我们将选择讨论部分，大约有 1,000 个单词。
- en: '![](../Images/f0a39e870bbe222ab75222daf50028f4.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0a39e870bbe222ab75222daf50028f4.png)'
- en: '[Source](https://arxiv.org/abs/2302.08399).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/2302.08399)。'
- en: Cut these lengthy passages into chunks of 3,000 words (not needed in this example).
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些冗长的段落切成 3,000 个单词的片段（在这个例子中不需要）。
- en: Feed these chunks of text to ChatGPT.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些文本片段输入到 ChatGPT 中。
- en: Ask ChatGPT to write a summary of the text you provided.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求 ChatGPT 写出你提供的文本的摘要。
- en: Repeat the process for all the papers you want to summarize.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复这个过程，总结你想要概括的所有论文。
- en: Don’t forget to fact-check.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要忘记事实核查。
- en: Use your freshly created reading notes to find common threads, and confront
    opposing results.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用你新创建的阅读笔记找出共同的线索，并对抗相反的结果。
- en: 'Here’s what the framework looks like in practice:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，框架看起来是这样的：
- en: '[PRE21]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Note:** if the final output is too long, ChatGPT will stop writing its response
    midway. In this case, you can prompt it with the word “Continue,” and it will
    resume writing from the point it was cut off.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：**如果最终输出太长，ChatGPT 将在中途停止写作。在这种情况下，你可以提示它使用“继续”这个词，它将从被切断的地方继续写作。'
- en: 🟢 **Knowledge Integration* and Microsoft Edge**
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 🟢 **知识整合* 和 Microsoft Edge**
- en: When using Knowledge Integration prompts, you can use the “Chat” feature of
    Microsoft Edge for more efficiency.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用知识整合提示时，你可以使用 Microsoft Edge 的“聊天”功能来提高效率。
- en: Instead of navigating the material yourself, you can open a web page or a PDF
    in Edge and use the Chat feature to summarize the content. From there, inject
    the summary into ChatGPT and use it for another prompt like the one we saw in
    the previous example.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以打开 Edge 中的网页或 PDF，并使用聊天功能来总结内容，而不是自己浏览材料。然后，将总结注入到 ChatGPT 中，并用它进行另一个提示，就像我们在前面的例子中看到的那样。
- en: 'Here’s a prompt you can use to summarize a document using Microsoft Edge:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是你可以使用 Microsoft Edge 总结文档的提示：
- en: '[PRE22]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 🟠 Directional Stimulus (DS) Prompting
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟠 方向刺激（DS）提示
- en: 'The original version of DS Prompting involves two steps:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: DS Prompting 的原始版本包括两个步骤：
- en: Train a Language Model to generate a **specific type of hint** from a given
    text.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个语言模型，从给定的文本中生成**特定类型的提示**。
- en: Use **another Language Model** to summarize the same text using the hint generated
    by the first Language Model.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**另一个语言模型**来使用第一个语言模型生成的提示来总结相同的文本。
- en: For this guide, we’ll use a simpler version of DS Prompting where we **use the
    same model to perform both tasks**.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本指南，我们将使用 DS Prompting 的简化版本，在这个版本中，我们**使用相同的模型执行两项任务**。
- en: '![](../Images/f78e881ebb239c356e1eecd86b892c54.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f78e881ebb239c356e1eecd86b892c54.png)'
- en: Comparison between Standard Prompting and Direction Stimulus Prompting. [Source](https://arxiv.org/abs/2302.11520)
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 标准提示和方向刺激提示的比较。[来源](https://arxiv.org/abs/2302.11520)
- en: The best (and perhaps only) use case for DS Prompting is to summarize research
    papers and other academic-like texts.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: DS提示的最佳（也许是唯一的）用例是总结研究论文和其他类似学术文本。
- en: 'Suppose you want to summarize a paper titled *Guiding Large Language Models
    via Directional Stimulus Prompting*. Here are the steps you want to follow:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要总结一篇名为《通过定向刺激提示引导大型语言模型》的论文。以下是你想要遵循的步骤：
- en: Identify the passage you want to summarize.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定你想要总结的段落。
- en: Cut these lengthy passages into chunks your Language Model can handle. (3,000
    words in the case of ChatGPT-4).
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些冗长的段落切成你的语言模型可以处理的块。（在ChatGPT-4的情况下为3,000个单词）。
- en: Feed the chunks of text into ChatGPT.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本块输入ChatGPT。
- en: Ask ChatGPT to write a hint about the paper involving key information like names,
    addresses, dates, places, and events.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求ChatGPT写一条关于涉及关键信息（如姓名、地址、日期、地点和事件）的论文的提示。
- en: Use Few-Shot prompting to show ChatGPT the desired output.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Few-Shot提示向ChatGPT展示所需的输出。
- en: Once you get the “hint,” fact-check it and adjust your Few-Shot prompt if necessary.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦你得到了“提示”，对其进行事实核查，并根据需要调整你的Few-Shot提示。
- en: Submit your “hint” generating prompt in a new tab.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在新标签页中提交你的“提示”生成提示。
- en: Run the prompt to get a “hint” for the text you want.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行提示以获得你想要的文本的“提示”。
- en: In the same tab, prompt ChatGPT to summarize the last submitted text using the
    “hint.”
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在同一个标签页中，提示ChatGPT使用“提示”来总结最后提交的文本。
- en: 'Here’s what Directional Stimilus prompting looks like in practice:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这是实践中的定向刺激提示的样子：
- en: '[PRE23]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 🟡 Recursive Criticism and Improvement (RCI) Prompting
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟡 递归批评和改进（RCI）提示
- en: 'Despite the fancy name, Recursive Criticism and Improvement prompting is a
    simple technique. You can break it down into three steps:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名字很花哨，递归批评和改进提示是一种简单的技术。你可以将其分解为三个步骤：
- en: Prompt your LLM to generate an output based on an initial prompt (often a Zero-Shot).
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示你的LLM根据初始提示（通常是零提示）生成输出。
- en: Prompt your model to identify potential problems with the output it generated.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示你的模型识别生成的输出中的潜在问题。
- en: Prompt your model to generate an updated output based on the identified problem.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示你的模型根据确定的问题生成更新的输出。
- en: RCI enhances your LLMs’ reasoning abilities in situations like writing code
    and solving logic puzzles. In fact, researchers found that RCI can outperform
    Chain of Thought (CoT) in some instances.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: RCI可以增强你的LLMs在编写代码和解决逻辑谜题等情况下的推理能力。事实上，研究人员发现，在某些情况下，RCI可以胜过思维链（CoT）。
- en: '![](../Images/e55390bc1927cbb3543c5bba8feaa118.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e55390bc1927cbb3543c5bba8feaa118.png)'
- en: An example where RCI outperforms Chain of Thought prompting. [Source](https://arxiv.org/abs/2303.17491)
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 一个RCI优于思维链提示的例子。[来源](https://arxiv.org/abs/2303.17491)
- en: Even better, when you combine RCI with CoT prompting (“Let’s think step by step”),
    you get better results than either of the methods used separately.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，当你将RCI与CoT提示（“让我们逐步思考”）结合起来时，你会获得比单独使用任何一种方法都更好的结果。
- en: 'Here’s a framework you can apply to practice RCI:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可以应用于实践RCI的框架：
- en: Prompt your model with an instruction of your choice.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用你选择的指令提示你的模型。
- en: In the same tab, add a new prompt where you ask your model to “**review the
    previous answer and find potential problems in it**.”
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在同一个标签页中，添加一个新的提示，要求你的模型“**审查之前的答案并找出其中的潜在问题**。”
- en: 'If there’s an error, ChatGPT will try to find it and correct it at the same
    time. Other models can stop after describing the error. In such a case, you can
    add the following prompt: “**Based on the problems you found, improve your answer**.”'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有错误，ChatGPT将尝试在描述错误后找到并纠正它。在这种情况下，你可以添加以下提示：“**根据你找到的问题，改进你的答案**。”
- en: 'Another option is to combine the two previous prompts into one: “**Please review
    your answer and find every potential problem within it**. **Based on the problems
    you found, improve your answer.**”'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个选项是将前两个提示合并为一个：“**请检查你的答案并找出其中的每个潜在问题**。**根据你找到的问题，改进你的答案**。”
- en: Here’s an example where RCI prompting helped “optimize” a simple Python script.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个RCI提示帮助“优化”一个简单的Python脚本的例子。
- en: '[PRE24]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/356eed96ddba204cd7bab98928487a8b.png)![](../Images/965fb4a13ef4d494f2802cc1075953e5.png)![](../Images/08806cc37a0a5b7975244543ff5fa424.png)![](../Images/d15f4757b1102b1431bcbc8989ff206b.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/356eed96ddba204cd7bab98928487a8b.png)![](../Images/965fb4a13ef4d494f2802cc1075953e5.png)![](../Images/08806cc37a0a5b7975244543ff5fa424.png)![](../Images/d15f4757b1102b1431bcbc8989ff206b.png)'
- en: 🟡 Self-Refinement Prompting
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟡 自我完善提示
- en: Self-refinement is a variant of RCI prompting. But instead of asking your model
    to find *problems*, you ask it to provide *feedback*. The two techniques are identical
    when the goal is precision and accuracy.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 自我改进是 RCI 提示的一种变体。但与其要求模型找到*问题*不同，你要求它提供*反馈*。当目标是精确性和准确性时，这两种技术是相同的。
- en: On the other hand, if you seek creativity and subjective responses, Self-Refinement
    prompts give you more options than RCI.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你寻求创造力和主观回答，自我改进提示比 RCI 提供更多选项。
- en: For instance, when you prompt your model to generate a poem, what you’re looking
    for isn’t a clear-cut formula. Rather, it’s an emotion-charged depiction of a
    human experience.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当你提示你的模型生成一首诗时，你寻求的不是一个明确的公式，而是一个充满情感的人类体验的描绘。
- en: 'In this case, you start by prompting your model to write a poem and then engage
    in a refinement loop where you:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你首先提示你的模型写一首诗，然后参与一个改进循环，其中你：
- en: Ask your model to give feedback on its own response. Use a role prompt to improve
    the quality of the feedback, then add the desired instruction.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请你的模型对自己的回答进行反馈。使用角色提示来提高反馈的质量，然后添加所需的指示。
- en: '[PRE25]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Ask your model to improve its response based on its own feedback.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请你的模型根据自己的反馈改进回答。
- en: '[PRE26]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Add your own feedback if needed.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如有需要，请添加你自己的反馈。
- en: Repeat the previous steps until you reach the desired outcome.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复前面的步骤，直到达到期望的结果。
- en: 'Here’s an example:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: '[PRE27]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/1ddc6eff6b34693531ca32ffa3a41ab7.png)![](../Images/d098a59d548a1f9b579c1bbbd7b81ca1.png)![](../Images/b7b4505794f10c643ff1c94f8c94b063.png)![](../Images/49e3993a7e50a759bcc342d861efed64.png)![](../Images/37ed832b4959c4b04246c37fdf7e190b.png)![](../Images/8410316a2c525b922c1a0703eb3b894c.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ddc6eff6b34693531ca32ffa3a41ab7.png)![](../Images/d098a59d548a1f9b579c1bbbd7b81ca1.png)![](../Images/b7b4505794f10c643ff1c94f8c94b063.png)![](../Images/49e3993a7e50a759bcc342d861efed64.png)![](../Images/37ed832b4959c4b04246c37fdf7e190b.png)![](../Images/8410316a2c525b922c1a0703eb3b894c.png)'
- en: ChatGPT’s output for a series of Self-Refinement prompts about writing a Linkedin
    post.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 对一系列关于撰写领英帖子的自我改进提示的输出。
- en: I kept the quality of the prompts low to illustrate a crucial point of self-refinement
    prompting. Depending on how you steer the feedback loop, **you may end up degrading
    the output.** It takes practice and domain-specific expertise to maximize the
    potential of self-refinement prompting.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我将提示的质量保持较低，以说明自我改进提示的一个关键点。根据你如何引导反馈循环，**你可能会降低输出的质量**。要最大限度地发挥自我改进提示的潜力，需要实践和领域专业知识。
- en: There’s a dedicated section about iterations and refinement. It’s titled “Iterate
    until you have to revert.”
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个专门讲述迭代和改进的部分，标题为“迭代直到必须还原”。
- en: 🟡 Reverse Prompt Engineering
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟡 逆向提示工程
- en: Reverse engineering is the art of building things backward — and you can use
    it on prompts.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 逆向工程是向后构建事物的艺术，你可以在提示上使用它。
- en: Instead of writing a prompt to generate a response, start with a high-quality
    version of the desired response and work your way back to a prompt.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 不要写一个提示来生成回答，而是从期望的回答的高质量版本开始，然后逐步回溯到提示。
- en: Another way to highlight the difference between classic prompting and reverse
    prompt engineering is to turn each technique into a question.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种突出经典提示和逆向提示工程之间差异的方法是将每种技术转化为一个问题。
- en: 'Traditional prompting: “Here are the directions. Can you get me there?”'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统提示：“这是方向。你能帮我到那里吗？”
- en: 'Reverse-Engineered prompting: “Here’s the destination I want to reach. Can
    you show me the directions to get there?”'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逆向工程提示：“这是我想要达到的目的地。你能给我展示到达那里的路线吗？”
- en: This method shines in two situations. The first is when seek inspiration to
    write your prompt. The second is when your goal is to generate output with very
    specific formats — like a board game, a landing page, or a recipe. Let’s explore
    an example involving the latter.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在两种情况下非常出色。第一种情况是当你寻求灵感来写你的提示时。第二种情况是当你的目标是生成具有非常特定格式的输出，比如棋盘游戏、落地页或食谱。让我们探索一个涉及后者的例子。
- en: '[PRE28]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 🟡 Prompt Revision
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟡 提示修订
- en: This technique may seem similar to Reverse Prompt Engineering, but there’s a
    tiny difference. Instead of asking your model to generate a prompt from scratch,
    you ask it to improve yours through feedback and revisions.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术可能与逆向提示工程相似，但有一个微小的区别。你不是要求你的模型从头开始生成提示，而是通过反馈和修订来改进你的提示。
- en: Prompt Revision is useful for intermediate and expert prompt engineers. Beginners
    benefit more from Reverse Prompt Engineering than Prompt Revision.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 提示修订对中级和专家级提示工程师很有用。初学者比起提示修订更适合逆向提示工程。
- en: '**When you’re a beginner**, **you don’t have enough skills to recognize your
    mistakes.** Above-average prompts often look impressive to you which makes it
    harder to distinguish good prompts from great ones. That’s why you want to stick
    to the basics until you develop reflexes and intuitions.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当你是一个初学者时**，**你没有足够的技能来识别你的错误。**对你来说，高于平均水平的提示通常看起来令人印象深刻，这使得区分好的提示和优秀的提示更加困难。这就是为什么你要坚持基础知识，直到你培养出反应和直觉。'
- en: '**When you reach an intermediate level, you learn to identify your weaknesses.**
    Prompt Revision helps you identify and overcome your blind spots. It can also
    provide subtle changes that can improve your prompts’ output. Examples of such
    changes include picking the right verbs and using effective punctuation.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当你达到中级水平时，你学会了识别自己的弱点。**提示修订可以帮助你识别和克服你的盲点。它还可以提供微小的变化，以改善你的提示输出。这些变化的例子包括选择正确的动词和使用有效的标点符号。'
- en: '**When you approach the expert level, you start to optimize every word you
    write in a prompt.** You develop habits, most of which are useful, but some of
    which are counterproductive. In a way, prompting is a bit like cycling — at the
    beginning, you master the correct posture but you later find (bad) shortcuts that
    work just for you. Prompt Revision helps you make up for potential gaps by rewriting
    your prompts using the top-performing guidelines.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当你达到专家级别时，你开始优化你在提示中写的每一个字。**你养成了一些习惯，其中大部分是有用的，但有些是适得其反的。在某种程度上，提示有点像骑自行车——一开始，你掌握了正确的姿势，但后来你发现了（不好的）捷径，这些捷径只适合你自己。提示修订可以通过使用表现最佳的指南来重写你的提示，弥补潜在的差距。'
- en: Here’s a Prompt Revision example shared by [Alex Albert,](https://twitter.com/alexalbert__?lang=en)
    a prompt engineer and jailbreaker.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个由[亚历克斯·阿尔伯特](https://twitter.com/alexalbert__?lang=en)（一位提示工程师和越狱者）分享的提示修订示例。
- en: '[PRE29]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 🟡 Program Simulation Prompting
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟡程序模拟提示
- en: Program Simulation is a particular case of Role prompting where you ask your
    model to behave like a mini app.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 程序模拟是角色提示的一个特殊情况，你要求你的模型像一个小型应用程序一样运行。
- en: Technically, your model won’t develop an app from scratch and run it on some
    invisible server. Instead, your LLM will behave like a small program that operates
    inside a chat tab.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，你的模型不会从头开始开发一个应用程序并在某个看不见的服务器上运行。相反，你的LLM将像一个在聊天标签内运行的小程序一样运行。
- en: The emulated program functions like an automated voice message system. Think
    of when you call a fancy restaurant and hear a recorded voice telling you to “type
    1 to make a new reservation or type 2 to cancel an existing reservation.”
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟的程序的功能类似于自动语音留言系统。想象一下当你打电话给一个高档餐厅时，听到一个录音告诉你“按1键进行新预订，按2键取消现有预订。”
- en: '![](../Images/b1e2767521f1cc6b9fc8756c14f6d2b9.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1e2767521f1cc6b9fc8756c14f6d2b9.png)'
- en: You can see the output of a Program Simulation prompt as a decision tree where
    every possible outcome is pre-determined by a script. Your prompt makes for the
    script or at least part of it.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将程序模拟提示的输出看作是一个决策树，其中每个可能的结果都是由脚本预先确定的。你的提示就是脚本，或者至少是其中的一部分。
- en: In fact, you can write a prompt that defines the main branches of the decision
    tree and let your LLM fill in the blanks.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你可以编写一个定义决策树主要分支的提示，然后让你的LLM填充空白。
- en: 'Let’s illustrate with an example:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来说明：
- en: '[PRE30]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Notice how the prompt only defines the five main branches of the desired decision
    tree. The description of each of these five branches is an implicit instruction
    for your LLM to complete the decision tree.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 注意提示只定义了所需决策树的五个主要分支。每个分支的描述都是对你的LLM完成决策树的隐含指令。
- en: 'Here’s what the output of the previous prompt looks like:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前一个提示的输出结果：
- en: '![](../Images/1aa6b2e28a7230ac97b5073f6b34ff4a.png)![](../Images/3461dec317e5a6fbffc25ac53fa601ca.png)![](../Images/7b3e99cf7342181387e5f4fddcb89122.png)![](../Images/fb811e3b30b89c0bddf8aee337739e2e.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1aa6b2e28a7230ac97b5073f6b34ff4a.png)![](../Images/3461dec317e5a6fbffc25ac53fa601ca.png)![](../Images/7b3e99cf7342181387e5f4fddcb89122.png)![](../Images/fb811e3b30b89c0bddf8aee337739e2e.png)'
- en: Example of Program Simulation prompting.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 程序模拟提示的示例。
- en: 🟠 All-In-One (AIO) Prompting*
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟠全能提示（AIO）*
- en: All-In-One (AIO) is an extended version of Role Prompting where you give your
    model a detailed list of instructions, turning it into a “specialized” version
    of itself.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: All-In-One（AIO）是角色提示的扩展版本，你可以给你的模型提供详细的指令列表，将其转变为一个“专门”的版本。
- en: I called it All-In-One because you can combine all of the previous techniques
    to emulate a specialized LLM inside another LLM.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我称之为全能提示，因为你可以将之前的所有技巧结合起来，模拟一个专门的LLM在另一个LLM内部运行。
- en: Microsoft used a similar approach to create Bing Chat out of GPT-4\. They fine-tuned
    some version of GPT-4 and equipped it with extra functions like online search
    and annotations.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 微软使用类似的方法，将Bing Chat从GPT-4中创建出来。他们对某个版本的GPT-4进行了微调，并配备了在线搜索和注释等额外功能。
- en: All-In-One prompting involves two steps.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: All-In-One提示包括两个步骤。
- en: Prompt your model to behave as if it were a raw version of itself. Technically
    speaking, it won’t do that, but it’ll pretend as if it does, which is good enough.
    You’ll be able to steer the behavior of ChatGPT (and other models) in a specific
    direction.
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示你的模型表现得好像它是一个原始版本的自己。从技术上讲，它不会这样做，但它会假装自己这样做，这已经足够好了。你将能够引导ChatGPT（和其他模型）的行为朝特定方向发展。
- en: Prompt your model as if you’re “fine-tuning” it. Note that fine-tuning is much
    more complex than writing a lengthy prompt. The real process involves training
    your model on new data, applying reinforcement learning through human feedback,
    and other adjustments. In the case of AIO prompting, you’ll only ask your LLM
    to behave in a very precise manner, similar to Role prompting but with a more
    significant amount of details.
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示你的模型就像你在“微调”它一样。请注意，微调比编写冗长的提示要复杂得多。真正的过程涉及在新数据上训练模型，通过人类反馈应用强化学习以及其他调整。在AIO提示的情况下，你只需要求你的LLM以非常精确的方式行为，类似于角色提示，但具有更多的细节。
- en: 'Let’s illustrate AIO prompting with an example:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来说明AIO提示：
- en: '[PRE31]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As discussed above, the second part of AIO combines many techniques. For instance,
    there’s role prompting, temperature control, chain-of-thought, and knowledge integration.
    You don’t have to use every single prompting technique you know, only the relevant
    ones.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，AIO的第二部分结合了许多技术。例如，有角色提示、温度控制、思维链和知识整合。你不必使用你所知道的每一种提示技术，只需使用相关的技术即可。
- en: Now let’s go through a simple template you can use to generate your own AI assistants
    using AIO prompting.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过一个简单的模板来生成你自己的AI助手，使用AIO提示。
- en: 🟢 Template for All-In-One (AIO) Prompting
  id: totrans-398
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 All-In-One（AIO）提示模板
- en: When writing an AIO prompt, be as specific as possible and adjust the prompt
    to the problem you want to solve. (You can also ask Bernard to help you improve
    your original prompt).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写AIO提示时，尽可能具体，并根据你想解决的问题调整提示。（你也可以请伯纳德帮助你改进原始提示）。
- en: Test your prompt over and over again until you reach a satisfying result. Remember,
    Language Models like ChatGPT (GPT-4) have powerful capabilities, but most of these
    capabilities remain asleep until you wake them up.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 反复测试你的提示，直到达到满意的结果。记住，ChatGPT（GPT-4）等语言模型具有强大的能力，但大部分这些能力在你唤醒它们之前都处于休眠状态。
- en: The following template won’t take you there right away, but it’ll put you on
    track. You’ll find generic labels like <Topic_name> to make it easier to understand
    AIO prompting.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模板不会立即帮助你达到目标，但它会让你走上正轨。你会发现像<Topic_name>这样的通用标签，以便更容易理解AIO提示。
- en: '[PRE32]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here’s how to use the template.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何使用模板的方法。
- en: Replace <Model_name> with a name of your choosing.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用你选择的名称替换<Model_name>。
- en: Replace <Field_name> with the expertise you want your model to emulate.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用你想要你的模型模拟的专业知识替换<Field_name>。
- en: Replace <Topic_name> with a specific topic within the area of expertise you
    chose.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用你选择的专业领域中的特定主题替换<Topic_name>。
- en: Replace <Tone_type> with the desired tone.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用所需的语气类型替换<Tone_type>。
- en: Replace <Style_type> with the desired style.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用所需的风格类型替换<Style_type>。
- en: Remove the name of the techniques from the initial template.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从初始模板中删除技术名称。
- en: Add your own instructions.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加你自己的说明。
- en: Test your prompt.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试你的提示。
- en: Adjust it.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整它。
- en: Iterate until you reach a satisfying result.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反复迭代，直到达到满意的结果。
- en: 🟢 More examples of All-In-One Prompting*
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🟢 更多All-In-One提示示例*
- en: The following is a list of AIO examples covering various applications. Modify
    them to make them your own. You’ll find a few hints to help you get started.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是涵盖各种应用的AIO示例列表。修改它们使其成为你自己的。你会找到一些提示来帮助你入门。
- en: Dolores, the Email Muse
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多洛丽丝，电子邮件灵感
- en: '[PRE33]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Robert Ford, the Coding Master
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 罗伯特·福特，编码大师
- en: '[PRE34]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Lee Sizemore, the Outlandish Chef
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 李·西兹莫尔，奇异的厨师
- en: '[PRE35]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Maeve the Mastermind
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梅芙，策划大师
- en: '[PRE36]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The secret sauce of AIO prompting is your professional expertise. The more you
    know about a topic, the higher the quality of your prompt because you’ll think
    of details that others may miss.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: AIO提示的秘密武器是你的专业知识。你对一个主题了解得越多，你的提示质量就越高，因为你会想到其他人可能会忽略的细节。
- en: Prompt engineering is an empirical science, and the best way to learn it is
    through regular practice. Hit the keyboard as much as possible and make sure to
    keep in touch with AI literature to upgrade your techniques.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是一门经验科学，学习它的最佳方式是通过定期练习。尽可能多地敲击键盘，并确保与人工智能文献保持联系，以提升你的技术。
- en: 💡 Iterate until you have to revert
  id: totrans-426
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💡 迭代直到你不得不回退
- en: The output of Language Models is like a decision tree with thousands of possible
    outcomes. Each word predicted by the model branches out into a set of new possibilities,
    most of which are invisible to you. The only part that’s under your control is
    the starting point — and that’s your prompt.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的输出就像一个决策树，有成千上万种可能的结果。模型预测的每个词都会分支出一系列新的可能性，其中大部分对你来说是不可见的。唯一在你控制之下的是起点
    —— 也就是你的提示。
- en: One major difference between Language Models and decision trees is the presence
    of randomness. The same prompt doesn’t always generate the same response. It’s
    the price we pay for creativity.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型和决策树之间的一个主要区别是存在随机性。同一个提示并不总是生成相同的响应。这是我们为创造力付出的代价。
- en: There’s also the alignment tax, where the model’s behavior (and capability)
    can change to meet (new) restrictions. And to top things off, nobody really knows
    what’s happening inside Language Models.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 还有对齐税，模型的行为（和能力）可能会改变以满足（新的）限制。更糟的是，没有人真正知道语言模型内部发生了什么。
- en: 'In short, when you use a Language Model, you’re interacting with an unpredictable
    black box. You can’t really rely on exact science: trial and error is your best
    option.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，当你使用语言模型时，你正在与一个不可预测的黑匣子进行交互。你不能真正依赖精确的科学：试错是你最好的选择。
- en: 'The rule is simple: Iterate on your prompt until the latest version of your
    output becomes worse than the previous one. In other words, iterate until you
    have to revert.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 规则很简单：在你的提示上进行迭代，直到最新版本的输出变得比之前的更糟。换句话说，迭代直到你不得不回退。
- en: 'Iteration comes in two flavors: either try different versions of the same prompt
    or guide the model through a succession of prompts. In most cases, you’ll use
    a combination of both.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代有两种方式：要么尝试同一个提示的不同版本，要么通过一系列提示引导模型。在大多数情况下，你会同时使用两种方式。
- en: '![](../Images/f5595a1f463d23797dc7226e13debd22.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5595a1f463d23797dc7226e13debd22.png)'
- en: Illustration of how the quality of your output evolves with prompt iterations.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 说明了随着提示迭代次数的增加，你的输出质量是如何发展的。
- en: To better understand how the iterative process works, picture prompting as a
    concave function (or a bell curve). Your first iterations are likely to get you
    better results, but at some point, your new prompt will start to generate worse
    output compared to its predecessors.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地理解迭代过程的工作原理，把提示想象成一个凹函数（或者钟形曲线）。你的第一次迭代很可能会带来更好的结果，但在某个时刻，你的新提示将开始生成比之前更糟的输出。
- en: Pay attention to the inflection point, and when you reach it, you want to either
    settle or start a new chain of prompts.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 注意拐点，当你到达拐点时，你要么停下来，要么开始一个新的提示链。
- en: '![](../Images/ab92c7ad4c30803f0a95414186fe7b02.png)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab92c7ad4c30803f0a95414186fe7b02.png)'
- en: Illustration of how successive chains of prompt iterations can improve your
    final prompt.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 说明了连续的提示迭代链如何改进你的最终提示。
- en: You can use the following framework to get yourself started with the iterative
    process.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下框架来开始迭代过程。
- en: Use Many-Examples prompting to generate ideas.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用多例提示来生成想法。
- en: '**“Please provide me with a list of 50 suggestions on how to improve this prompt/response.”**'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**“请给我提供一个关于如何改进这个提示/响应的建议列表。”**'
- en: Use **Prompt Revision/Bernard** to improve your prompts.
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**提示修订/伯纳德**来改进你的提示。
- en: Rewrite the same prompt using **different words** and examine the responses.
    Different words trigger different responses.
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用**不同的词**重写相同的提示，并检查响应。不同的词触发不同的响应。
- en: '**Create a library of prompts for each model you use**. Make sure to update
    your library every now and then.'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为你使用的每个模型创建一个提示库**。确保定期更新你的库。'
- en: Study how Language Models work to understand how they generate responses.
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 研究语言模型的工作原理，了解它们如何生成响应。
- en: Whenever your output is stuck in the mud, give your prompts a few tweaks to
    push it out. Try different verbs. Mix prompting techniques. Switch models. Sleep
    on it. Start again tomorrow.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你的输出陷入僵局时，对你的提示进行一些调整来推动它。尝试不同的动词。混合提示技巧。切换模型。睡一觉。明天重新开始。
- en: 💡 With great power…
  id: totrans-447
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💡 力量越大…
- en: We let the AI genie out of the bottle before we could figure out the risks involved,
    let alone how to deal with them.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在弄清楚涉及的风险之前就把人工智能精灵放出了瓶子，更不用说如何处理它们了。
- en: I’m particularly worried about large-scale misinformation. But there are other
    risks like rapid displacements in the job market, prompt injection attacks, scams,
    and ultimately the alignment problem.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 我特别担心大规模的错误信息。但还有其他风险，比如工作市场的快速转移，提示注入攻击，诈骗，以及最终的对齐问题。
- en: For now, the best we can do is to learn how to use generative AI to create more
    good than harm.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们能做的最好的事情就是学会如何利用生成式人工智能创造更多的利大于弊。
- en: Below you’ll find six short ethical rules that will seem obvious but are still
    worth mentioning.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 下面你会发现六条短小的道德规则，它们似乎是显而易见的，但仍然值得一提。
- en: '**Maintain ethical standards:** Uphold your moral principles and avoid using
    AI-generated content that promotes harmful ideas or misinformation.'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**保持道德标准：**坚持你的道德原则，避免使用促进有害思想或错误信息的AI生成内容。'
- en: '**Be transparent:** Clearly disclose the use of AI in your content creation
    process so your audience knows the origin of the information they’re consuming.'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**透明：**清楚地披露在你的内容创作过程中使用人工智能，这样你的观众就知道他们消费的信息的来源。'
- en: '**Use AI as an upgrade, not a replacement:** Tech companies will make a choice:
    either achieve the same results with ten times fewer people or keep the same number
    of employees and produce ten times more. The latter option paints a brighter future.'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将AI用作升级，而不是替代：**科技公司将做出选择：要么用十分之一的人员达到相同的结果，要么保持相同数量的员工，产出十倍的成果。后者选项描绘了一个更光明的未来。'
- en: '**Verify facts and accuracy:** Always double-check the information generated
    by AI chatbots. The same goes for code. Misinformation and harmful code can make
    their way into your output. Switch on your skeptic mode.'
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**验证事实和准确性：**始终要仔细检查由AI聊天机器人生成的信息。同样适用于代码。错误信息和有害代码可能会进入你的输出。打开你的怀疑模式。'
- en: '**Never disclose sensitive data:** OpenAI[keeps a record](https://www.androidauthority.com/does-chatgpt-save-data-conversations-3310883/)
    of all of your exchanges with ChatGPT. The same may apply to other companies.
    Assume everything you type into a chatbot can leak.'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**永远不要泄露敏感数据：**OpenAI[记录](https://www.androidauthority.com/does-chatgpt-save-data-conversations-3310883/)了你与ChatGPT的所有交流。其他公司可能也是如此。假设你输入到聊天机器人中的一切都可能泄露。'
- en: '**Watch for biases:** Chatbots can inadvertently perpetuate biases found in
    their training data. Be vigilant. You can use specific prompts to limit bias by
    asking a Language Model to consider multiple perspectives or provide a balanced
    view.'
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注意偏见：**聊天机器人可能无意中延续其训练数据中存在的偏见。要保持警惕。你可以使用特定提示来限制偏见，通过要求语言模型考虑多个观点或提供一个平衡的观点。'
- en: '[PRE37]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Conclusion
  id: totrans-459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Prompt Engineering split the world into two camps. The first pictures it as
    a bug — and the underlying argument is that AI models will get better and better
    at responding to natural language. There’s a possible future where AI models will
    be able to guess what we want them to do, similar to how social media algorithms
    can guess which content we want to see.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程将世界分为两个阵营。第一个将其视为一个bug——其根本论点是AI模型将越来越擅长回应自然语言。有可能的未来是，AI模型将能够猜测我们想让它们做什么，类似于社交媒体算法可以猜测我们想看到哪些内容。
- en: The second camp pictures Prompt Engineering as an essential skill in tomorrow’s
    job market. The argument is generative AI models will be everywhere. We’ll use
    them to write code, generate reports, analyze data, and even prepare seminars.
    In such a scenario, Prompt Engineering becomes as essential as writing (non-boring)
    emails.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个阵营将提示工程视为明天工作市场上的一项必不可少的技能。其论点是生成式人工智能模型将无处不在。我们将使用它们来编写代码，生成报告，分析数据，甚至准备研讨会。在这种情况下，提示工程就像写（非无聊的）电子邮件一样重要。
- en: The world is never black or white but always some shade of gray — and the future
    of Prompt Engineering is no exception. You can take a bet or you can play it sage.
    If you increase your Prompt Engineering skills and it turns out to be a short-lived
    skill, then you’ll lose a few dozen hours of your life.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 世界从来不是黑白分明的，而总是一些灰色的阴影——Prompt Engineering的未来也不例外。你可以赌一把，也可以谨慎行事。如果你提高了你的提示工程技能，结果证明它是一个短暂的技能，那么你将失去几十个小时的生活。
- en: In contrast, if you ignore Prompt Engineering, it later becomes a non-negotiable
    skill you may miss out on potential career upgrades. So it’s probably worth the
    detour.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果你忽视了提示工程，那么以后它可能会成为你错过潜在职业晋升的一项不可妥协的技能。所以这可能是值得绕道而行的。
- en: References (in no particular order)
  id: totrans-464
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献（无特定顺序）
- en: '*Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E.,
    Le, Q., & Zhou, D.* [***Chain of Thought Prompting Elicits Reasoning in Large
    Language Models***](https://arxiv.org/abs/2201.11903) ***—*** *(2022).*'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*魏, J., 王, X., Schuurmans, D., Bosma, M., Ichter, B., 夏, F., 齐, E., 乐, Q.,
    & 周, D.* [***思维链提示引发大型语言模型的推理***](https://arxiv.org/abs/2201.11903) ***—*** *(2022).*'
- en: '*Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y.* [***Large Language
    Models are Zero-Shot Reasoner***](https://arxiv.org/abs/2205.11916) ***—*** *(2022).*'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*小岛, T., 顾, S. S., Reid, M., 松尾, Y., & 岩沢, Y.* [***大型语言模型是零-shot推理器***](https://arxiv.org/abs/2205.11916)
    ***—*** *(2022).*'
- en: '*Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Bras, R. L., Choi, Y., &
    Hajishirzi, H.* [***Generated Knowledge Prompting for Commonsense Reasoning***](https://arxiv.org/abs/2110.08387)
    ***—*** *(2021).*'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*刘, J., 刘, A., 卢, X., Welleck, S., West, P., Bras, R. L., Choi, Y., & Hajishirzi,
    H.* [***生成的知识提示用于常识推理***](https://arxiv.org/abs/2110.08387) ***—*** *(2021).*'
- en: '*Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery,
    A., & Zhou, D.* [***Self-Consistency Improves Chain of Thought Reasoning in Language
    Model***](https://arxiv.org/abs/2203.11171) ***—*** *(2022).*'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*王, X., 魏, J., Schuurmans, D., 乐, Q., 齐, E., Narang, S., Chowdhery, A., & 周,
    D.* [***自洽性改善语言模型的思维链推理***](https://arxiv.org/abs/2203.11171) ***—*** *(2022).*'
- en: '*Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans,
    D., Cui, C., Bousquet, O., Le, Q., & Chi, E.* [***Least-to-Most Prompting Enables
    Complex Reasoning in Large Language Models***](https://arxiv.org/abs/2205.10625)
    ***—*** *(2022).*'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*周, D., Schärli, N., 侯, L., 魏, J., Scales, N., 王, X., Schuurmans, D., 崔, C.,
    Bousquet, O., 乐, Q., & 齐, E.* [***从最少到最多提示使大型语言模型能够进行复杂推理***](https://arxiv.org/abs/2205.10625)
    ***—*** *(2022).*'
- en: Geunwoo Kim, Pierre Baldi, Stephen McAleer. [***Language Models Can Solve Computer
    Tasks***](https://arxiv.org/abs/2303.17491) —*(2023)*.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geunwoo Kim, Pierre Baldi, Stephen McAleer. [***语言模型可以解决计算机任务***](https://arxiv.org/abs/2303.17491)
    —*(2023)*.
- en: Murray Shanahan, Kyle McDonell, Laria Reynolds. [Role-Play with Large Language
    Models](https://arxiv.org/abs/2305.16367) —*(2023)*.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murray Shanahan, Kyle McDonell, Laria Reynolds. [大型语言模型的角色扮演](https://arxiv.org/abs/2305.16367)
    —*(2023)*.
- en: Alex Albert. [***Jailbreak Chat***](https://www.jailbreakchat.com/) —*(2022–2023)*.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alex Albert. [***越狱聊天***](https://www.jailbreakchat.com/) —*(2022–2023)*.
- en: Sander Schulhoff, Anaum Khan, Fady Yanni. [***LearnPrompting.com***](https://learnprompting.org/)
    —*(2023)*.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sander Schulhoff, Anaum Khan, Fady Yanni. [***LearnPrompting.com***](https://learnprompting.org/)
    —*(2023)*.
- en: Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu
    Chen. [***What Makes Good In-Context Examples for GPT-3?***](https://arxiv.org/abs/2101.06804)
    —*(2021)*.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu
    Chen. [***GPT-3的上下文示例是什么样的？***](https://arxiv.org/abs/2101.06804) —*(2021)*.
- en: Lilian Weng. [***Prompt Engineering***](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)—*(2023)*.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lilian Weng. [***提示工程***](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)—*(2023)*.
- en: Cognitive Revolution Youtube Channel. [***The Art of Prompting ChatGPT With
    Riley Goodside***](https://www.youtube.com/watch?v=zg3H-9nxkyI&ab_channel=CognitiveRevolution%22HowAIChangesEverything%22)
    —*(2023)*.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认知革命Youtube频道。 [***用Riley Goodside提示ChatGPT的艺术***](https://www.youtube.com/watch?v=zg3H-9nxkyI&ab_channel=CognitiveRevolution%22HowAIChangesEverything%22)
    —*(2023)*.
- en: 'Giuseppe Scalamogna; [***New ChatGPT Prompt Engineering Technique: Program
    Simulation***](/new-chatgpt-prompt-engineering-technique-program-simulation-56f49746aa7b)
    —*(2023)*.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Giuseppe Scalamogna; [***新的ChatGPT提示工程技术：程序模拟***](/new-chatgpt-prompt-engineering-technique-program-simulation-56f49746aa7b)
    —*(2023)*.
- en: Sang Michael Xie, Sewon Min. [***How Does In-Context Learning Work?***](http://ai.stanford.edu/blog/understanding-incontext/)—*(2022)*.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sang Michael Xie, Sewon Min. [***上下文学习是如何工作的？***](http://ai.stanford.edu/blog/understanding-incontext/)—*(2022)*.
- en: Alberto Romero. [***Prompt Engineering Is Probably More Important Than You Think***](https://thealgorithmicbridge.substack.com/p/prompt-engineering-is-probably-more)—(2023).
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alberto Romero. [***提示工程可能比你想象的更重要***](https://thealgorithmicbridge.substack.com/p/prompt-engineering-is-probably-more)—(2023).
- en: Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan.[***Guiding
    Large Language Models via Directional Stimulus Prompting***](https://arxiv.org/abs/2302.11520)*—(2023).*
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan.[***通过定向刺激提示引导大型语言模型***](https://arxiv.org/abs/2302.11520)*—(2023).*
- en: Yao Fu, Hao Peng, Tushar Khot. [***How does GPT Obtain its Ability?***](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)
    *—(2022).*
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao Fu, Hao Peng, Tushar Khot. [***GPT是如何获得其能力的？***](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)
    *—(2022).*
- en: '**Note regarding the images:** Unless otherwise noted, all images and screenshots
    are by the author.'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于图片的说明：** 除非另有说明，所有图片和截图均由作者提供。'
- en: Contact section
  id: totrans-483
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系方式
- en: '[Become a Medium member to support me here](https://nabil-alouani.medium.com/membership).'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为Medium会员，在这里支持我](https://nabil-alouani.medium.com/membership)。'
- en: '[Join my Subtack](https://nabilalouani.substack.com/).'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[加入我的Subtack](https://nabilalouani.substack.com/)。'
- en: 'Write me an email: nabil@nabilalouani.com.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给我写封邮件：nabil@nabilalouani.com。
