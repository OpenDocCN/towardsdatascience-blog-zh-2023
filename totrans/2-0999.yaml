- en: 'Google’s MusicLM: from text description to music'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/googles-musiclm-from-text-description-to-music-23794ab6955c](https://towardsdatascience.com/googles-musiclm-from-text-description-to-music-23794ab6955c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A new model is generating impressive music from just text prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----23794ab6955c--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----23794ab6955c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23794ab6955c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23794ab6955c--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----23794ab6955c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23794ab6955c--------------------------------)
    ·8 min read·Feb 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/effa592eb7e806938eac69eab439c2d5.png)'
  prefs: []
  type: TYPE_IMG
- en: image generated by the authors using OpenAI DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Google has released a new model capable of generating [music from a textual
    description](https://arxiv.org/pdf/2301.11325.pdf). The result? impressive.
  prefs: []
  type: TYPE_NORMAL
- en: '**Actually, the idea of applying generative AI to music is not a new concept.**
    There have been several attempts in recent years, including Riffusion, [Dance
    Diffusion](https://techcrunch.com/2022/10/07/ai-music-generator-dance-diffusion/),
    Microsoft’s Museformer, and [OpenAI’s Jukebox](https://openai.com/blog/jukebox/).
    Google itself had previously released a model called [AudioML](https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html).
    **Why would this model be different?**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/mlearning-ai/microsofts-museformer-ai-music-is-the-new-frontier-8dc5cb24459c?source=post_page-----23794ab6955c--------------------------------)
    [## Microsoft’s Museformer: AI music is the new frontier'
  prefs: []
  type: TYPE_NORMAL
- en: AI art is exploding, music can be next.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/mlearning-ai/microsofts-museformer-ai-music-is-the-new-frontier-8dc5cb24459c?source=post_page-----23794ab6955c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Why is hard to generate music with AI?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/bdab379e9038b789e6a153a8df2fdc7a.png)'
  prefs: []
  type: TYPE_IMG
- en: image by [Marius Masalar](https://unsplash.com/it/@marius) at unsplash.com
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, previous models showed obvious technical and quality problems. The
    result was stale, the songs were not very complex, often repetitive, and still
    not high-fidelity.
  prefs: []
  type: TYPE_NORMAL
- en: Generating music is not easy, [as numerous attempts have shown.](https://www.vice.com/en/article/qvq54v/why-is-ai-generated-music-still-so-bad)
    Authors often use MIDIs, but generating high-fidelity music is another matter.
    **Music also has a complex structure**, you have to consider melodies and harmonies,
    and there are patterns that repeat over time and over a long distance.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the authors of the article point out, it is easier to generate text-to-image,
    and text-to-music was attempted for: “simple acoustic scenes, consisting of few
    acoustic events over a period of seconds.”'
  prefs: []
  type: TYPE_NORMAL
- en: Here we are talking about starting with a single text caption and generating
    complex audio with a long-term structure. **How did they accomplish this?**
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/googles-audiolm-generating-music-by-hearing-a-song-s-snippet-c9512a9290cd?source=post_page-----23794ab6955c--------------------------------)
    [## Google’s Audiolm: Generating Music by Hearing a Song’s Snippet'
  prefs: []
  type: TYPE_NORMAL
- en: Whether music or speech, Google's new model can continue playing what is hearing.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/googles-audiolm-generating-music-by-hearing-a-song-s-snippet-c9512a9290cd?source=post_page-----23794ab6955c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, as they explain AudioLM was used as a starting point. The previous
    model was able to take a melody and continue it in a coherent way. **Nevertheless,
    there are several technical limitations to overcome**:'
  prefs: []
  type: TYPE_NORMAL
- en: The first main limitation of such a model is the “scarcity of paired audio-text
    data.” In fact, text-to-image training was facilitated by the fact that there
    are so many images, and alt text can be used as captions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not easy to be able to describe in a few words the salient features of
    music such as acoustic scenes or the timbre of a melody.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, the music unfolds on a temporal dimension, so there is a risk of a weak
    link between the caption and the music (in contrast, an image is static).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MusicLM: structure, training, results and limitations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/a1555ff223923fa9f981e34c6cd8a49c.png)'
  prefs: []
  type: TYPE_IMG
- en: image by [James Stamler](https://unsplash.com/it/@jamesstamler) at unsplash.com
  prefs: []
  type: TYPE_NORMAL
- en: '**The first component of this model is MuLan** (and also the core part). This
    model is used to construct a music-text joint embedding and consists of two embedding
    towers (one for textual input and one for musical input. The two towers are pre-trained
    [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) and a variant of [ResNeT-50](https://www.kaggle.com/datasets/keras/resnet50)
    for audio.'
  prefs: []
  type: TYPE_NORMAL
- en: MuLan was trained on pairs of music clips and their corresponding textual annotations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c44a1e8372e26a7efcff602f99325887.png)'
  prefs: []
  type: TYPE_IMG
- en: 'the three components of MusicLm have been independently pre-trained. The other
    two components are discussed below. Image source: [here](https://arxiv.org/pdf/2301.11325.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in the image the authors there are two other components. As the authors
    explain:'
  prefs: []
  type: TYPE_NORMAL
- en: We use the self-supervised audio representations of SoundStream as acoustic
    tokens to enable high-fidelity synthesis, and w2vBERT , as semantic tokens to
    facilitate long-term coherent generation. For representing the conditioning, we
    rely on the MuLan music embedding during training and the MuLan text embedding
    at inference time.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'As complicated as, this system may seem, it allows several advantages: it can
    scale quickly, and the use of [contrastive loss](https://paperswithcode.com/method/supervised-contrastive-loss)
    for embedding training increases robustness. In addition, **having pre-trained
    models separately allows for better conditioning of music with textual input**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**During training**, the model learns to convert the token mapping produced
    by MuLan to semantic tokens ([w2w-BERT](https://arxiv.org/abs/2108.06209)). Then
    the acoustic token is conditioned on both the MuLan audio tokens and the semantic
    tokens ([SoundStream](https://arxiv.org/abs/2107.03312)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad0f9c2b66f7140c496a4559afe7afed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'model during training. Image source: [here](https://arxiv.org/pdf/2301.11325.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**During inference** the process is to provide a textual description to MuLAn
    which transforms it into conditional signaling, this in turn is transformed into
    an audio token by w2w-BERT and then transformed into waveforms by the SoundStream
    decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b9a6d25c39fd712f1ce6ec88928946d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'model during inference. Image source: [here](https://arxiv.org/pdf/2301.11325.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: What also makes MusicLM so capable is that it has been trained on 5 million
    audio clips (a total of 280,000 hours of audio). In addition, the authors have
    created a dataset of 5.5 thousand music clips with captions written by professional
    musicians (this dataset has been released [here](https://www.kaggle.com/datasets/googleai/musiccaps)).
    Each of these captions describes the music with four sentences and is followed
    by a list of music aspects (genre, mood, rhythm, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: As the results show, MusicML proves to be superior to previous models (Mubert
    and [Riffusion](https://techcrunch.com/2022/12/15/try-riffusion-an-ai-model-that-composes-music-by-visualizing-it/)),
    both in terms of audio quality and text faithfulness. Moreover, in terms of listening
    quality as well. In fact, listeners were shown clips and asked to choose which
    clip best represented the textual description (a win means that the listener preferred
    the model in a side-by-side comparison)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5196c7011595bca45bbf5e39d318eff.png)'
  prefs: []
  type: TYPE_IMG
- en: '“Evaluation of generated samples using captions from the MusicCaps dataset.
    Models are compared in terms of audio quality, by means of Frechet Audio Distance
    (FAD), and faithfulness to the ´ text description, using Kullback–Leibler Divergence
    (KLD) and MuLan Cycle Consistency (MCC), and counts of wins in pairwise human
    listening tests (Wins).” Image source: [here](https://arxiv.org/pdf/2301.11325.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f05c58aa78510760d95f4a20116631b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The user interface for the human listener study. Image source: [here](https://arxiv.org/pdf/2301.11325.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listening to the results, it is hard not to be impressed,** considering that
    there are no musicians in the loop. After all, the model manages to capture musical
    nuances such as instrumental riffs and melodies.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the model is not limited to textual descriptions but can take other
    audio as input and continue them (“which is provided in the form of humming, singing,
    whistling, or playing an instrument”).
  prefs: []
  type: TYPE_NORMAL
- en: '**The authors also describe an approach called “story mode”**, which is to
    generate long audio sequences where the textual description changes over time.
    The model generates musical sequences with “smooth transitions which are tempo
    consistent and semantically plausible while changing music context according to
    the text description.” These descriptions can also contain descriptions com “time
    to meditate” or “time to run” creating a narrative in which appropriate music
    is associated.'
  prefs: []
  type: TYPE_NORMAL
- en: The model in short is not limited to the description of an instrument or genre
    but can be conditioned with descriptions that are inspired by activities, epochs,
    places, or moods. MusicLM could therefore also be used for movie soundtracks,
    workout apps, etc.
  prefs: []
  type: TYPE_NORMAL
- en: At this link, you can also read the captions and listen to the model result.
  prefs: []
  type: TYPE_NORMAL
- en: '[## MusicLM'
  prefs: []
  type: TYPE_NORMAL
- en: Andrea Agostinelli, Timo I. Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti,
    Antoine Caillon, Qingqing Huang, Aren…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: google-research.github.io](https://google-research.github.io/seanet/musiclm/examples/?source=post_page-----23794ab6955c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Though impressive the model is not perfect**, and some of the audio clips
    have a distorted quality (but this could be improved in the future with training
    modifications). In addition, the model can also generate choirs and people singing,
    but often the lyrics are not even in English but in a kind of gibberish and the
    voice sounds more like an amalgam of singers than a coherent human voice. Thus,
    how the authors state:'
  prefs: []
  type: TYPE_NORMAL
- en: Future work may focus on lyrics generation, along with improvement of text conditioning
    and vocal quality. Another aspect is the modeling of high-level song structure
    like introduction, verse, and chorus. Modeling the music at a higher sample rate
    is an additional goal.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Google decided not to distribute the model: “there are several risks associated
    with our model and the use-case it tackles.” As the authors note, the model reflects
    the biases that are present within the training data, posing problems for generating
    music for cultures that are underrepresented within the dataset. They also point
    out that the model raises ethical questions about cultural appropriation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91c6cb3068c877ccfccb293aaa961ea2.png)'
  prefs: []
  type: TYPE_IMG
- en: '“Genre distribution of all 5.5k examples of MusicCaps, according to an AudioSet
    classifier”. Image source: [here](https://arxiv.org/pdf/2301.11325.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Another problem is that in 1% of cases, the model generates music that it has
    heard during training, replicating parts of existing songs (even elements that
    are copyrighted). [TechCrunch suggests that this is enough](https://techcrunch.com/2023/01/27/google-created-an-ai-that-can-generate-music-from-text-descriptions-but-wont-release-it/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAK6OO6H73AxKjuqHai2HzS-KZFESx33sBYlc-8NKW0jnv6BUQlduIQ2-LqwaZr6ryfZq0BQ6F03HrFCY3mDS5CnYUWLZiNskZK-YzWxP943yOaC4gaKFfI7owLanSlNFG_kazxH79ZN6G_71N5ZVyK7AtziTDGsXAUVoNW_daWxZ)
    to discourage Google from releasing the model (especially now that institutions
    are interested in regulating AI and generative tools).
  prefs: []
  type: TYPE_NORMAL
- en: 'They acknowledge the risk of potential misappropriation of creative content
    linked with the use case and further highlight the necessity of deep studies and
    analysis, concluding:'
  prefs: []
  type: TYPE_NORMAL
- en: We strongly emphasize the need for more future work in tackling these risks
    associated to music generation — we have no plans to release models at this point
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/mlearning-ai/the-eu-wants-to-regulate-your-favorite-ai-tools-7419d985f2f2?source=post_page-----23794ab6955c--------------------------------)
    [## The EU wants to regulate your favorite AI tools'
  prefs: []
  type: TYPE_NORMAL
- en: EU is preparing a new AI bill and generative AI is included
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/mlearning-ai/the-eu-wants-to-regulate-your-favorite-ai-tools-7419d985f2f2?source=post_page-----23794ab6955c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusions**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MusicLM is impressive in its ability to generate music from the textual description,
    a quantum leap compared to previous models. It generates coherent music, long
    sequences capable of capturing music nuances. Although it has been trained with
    an impressive amount of data, it is still not perfect.
  prefs: []
  type: TYPE_NORMAL
- en: Also, as described by the authors since the model produces copyrighted music
    they have no plans to release it. This is probably also derived from the fact
    that some programmers recently [have sued GitHub Copilot](https://www.theverge.com/2022/11/8/23446821/microsoft-openai-github-copilot-class-action-lawsuit-ai-copyright-violation-training-data)
    and [Getty images filed lawsuits against Stability AI](https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit)
    and MidJourney.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, as noted by the authors, generative music models are incorporating
    bias from the training set. MusicLM is not only generating instrumental music
    but can also generate vocals and chorus, which can lead to incorporating bias
    and harmful content in the generated music.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, as noted by several musicians, streaming has thinned the earnings
    of musicians and composers, and [generative AI could reduce their income still
    further](https://biz.crast.net/what-happens-to-songwriters-when-ai-can-generate-music/).
  prefs: []
  type: TYPE_NORMAL
- en: Instead, the authors suggest that rather than replacing musicians and composers
    these models will soon be a set of tools to assist humans.
  prefs: []
  type: TYPE_NORMAL
- en: 'if you have found it interesting:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can look for my other articles, you can also [**subscribe**](https://salvatore-raieli.medium.com/subscribe)
    to get notified when I publish articles, and you can also connect or reach me
    on[**LinkedIn**](https://www.linkedin.com/in/salvatore-raieli/)**.** Thanks for
    your support!
  prefs: []
  type: TYPE_NORMAL
- en: Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----23794ab6955c--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----23794ab6955c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
