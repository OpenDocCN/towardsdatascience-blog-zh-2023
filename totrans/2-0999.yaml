- en: 'Google’s MusicLM: from text description to music'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google 的 MusicLM：从文本描述到音乐
- en: 原文：[https://towardsdatascience.com/googles-musiclm-from-text-description-to-music-23794ab6955c](https://towardsdatascience.com/googles-musiclm-from-text-description-to-music-23794ab6955c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/googles-musiclm-from-text-description-to-music-23794ab6955c](https://towardsdatascience.com/googles-musiclm-from-text-description-to-music-23794ab6955c)
- en: A new model is generating impressive music from just text prompt
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个新模型仅凭文本提示就能生成令人印象深刻的音乐
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----23794ab6955c--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----23794ab6955c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23794ab6955c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23794ab6955c--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----23794ab6955c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----23794ab6955c--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----23794ab6955c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23794ab6955c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23794ab6955c--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----23794ab6955c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23794ab6955c--------------------------------)
    ·8 min read·Feb 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23794ab6955c--------------------------------)
    ·阅读时间 8 分钟·2023年2月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/effa592eb7e806938eac69eab439c2d5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/effa592eb7e806938eac69eab439c2d5.png)'
- en: image generated by the authors using OpenAI DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 OpenAI DALL-E 生成
- en: Google has released a new model capable of generating [music from a textual
    description](https://arxiv.org/pdf/2301.11325.pdf). The result? impressive.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Google 发布了一种新模型，能够从 [文本描述生成音乐](https://arxiv.org/pdf/2301.11325.pdf)。结果？令人印象深刻。
- en: '**Actually, the idea of applying generative AI to music is not a new concept.**
    There have been several attempts in recent years, including Riffusion, [Dance
    Diffusion](https://techcrunch.com/2022/10/07/ai-music-generator-dance-diffusion/),
    Microsoft’s Museformer, and [OpenAI’s Jukebox](https://openai.com/blog/jukebox/).
    Google itself had previously released a model called [AudioML](https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html).
    **Why would this model be different?**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**实际上，将生成性人工智能应用于音乐并不是一个新概念。** 近年来已经有几个尝试，包括 Riffusion、[Dance Diffusion](https://techcrunch.com/2022/10/07/ai-music-generator-dance-diffusion/)、微软的
    Museformer 和 [OpenAI 的 Jukebox](https://openai.com/blog/jukebox/)。Google 自身之前发布过一个名为
    [AudioML](https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html)
    的模型。**为什么这个模型会有所不同？**'
- en: '[](https://medium.com/mlearning-ai/microsofts-museformer-ai-music-is-the-new-frontier-8dc5cb24459c?source=post_page-----23794ab6955c--------------------------------)
    [## Microsoft’s Museformer: AI music is the new frontier'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/mlearning-ai/microsofts-museformer-ai-music-is-the-new-frontier-8dc5cb24459c?source=post_page-----23794ab6955c--------------------------------)
    [## Microsoft 的 Museformer：人工智能音乐是新前沿'
- en: AI art is exploding, music can be next.
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工智能艺术正在蓬勃发展，音乐可能是下一个。
- en: medium.com](https://medium.com/mlearning-ai/microsofts-museformer-ai-music-is-the-new-frontier-8dc5cb24459c?source=post_page-----23794ab6955c--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/mlearning-ai/microsofts-museformer-ai-music-is-the-new-frontier-8dc5cb24459c?source=post_page-----23794ab6955c--------------------------------)'
- en: Why is hard to generate music with AI?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么用人工智能生成音乐如此困难？
- en: '![](../Images/bdab379e9038b789e6a153a8df2fdc7a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdab379e9038b789e6a153a8df2fdc7a.png)'
- en: image by [Marius Masalar](https://unsplash.com/it/@marius) at unsplash.com
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Marius Masalar](https://unsplash.com/it/@marius) 在 unsplash.com 提供
- en: Meanwhile, previous models showed obvious technical and quality problems. The
    result was stale, the songs were not very complex, often repetitive, and still
    not high-fidelity.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，之前的模型显示出明显的技术和质量问题。结果陈旧，歌曲复杂度不高，常常重复，并且音质依然不高。
- en: Generating music is not easy, [as numerous attempts have shown.](https://www.vice.com/en/article/qvq54v/why-is-ai-generated-music-still-so-bad)
    Authors often use MIDIs, but generating high-fidelity music is another matter.
    **Music also has a complex structure**, you have to consider melodies and harmonies,
    and there are patterns that repeat over time and over a long distance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 生成音乐并不容易，[正如众多尝试所示。](https://www.vice.com/en/article/qvq54v/why-is-ai-generated-music-still-so-bad)
    作者们常使用MIDI，但生成高保真音乐是另一回事。**音乐还有复杂的结构**，你必须考虑旋律和和声，并且有重复的模式，这些模式在时间上和距离上都会重复。
- en: 'As the authors of the article point out, it is easier to generate text-to-image,
    and text-to-music was attempted for: “simple acoustic scenes, consisting of few
    acoustic events over a period of seconds.”'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如文章的作者指出的那样，生成文本到图像相对容易，而文本到音乐的尝试仅限于：“由几个声音事件组成的简单声学场景，持续几秒钟。”
- en: Here we are talking about starting with a single text caption and generating
    complex audio with a long-term structure. **How did they accomplish this?**
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们谈论的是从单一文本说明开始，并生成具有长期结构的复杂音频。**他们是如何实现这一点的？**
- en: '[](https://pub.towardsai.net/googles-audiolm-generating-music-by-hearing-a-song-s-snippet-c9512a9290cd?source=post_page-----23794ab6955c--------------------------------)
    [## Google’s Audiolm: Generating Music by Hearing a Song’s Snippet'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pub.towardsai.net/googles-audiolm-generating-music-by-hearing-a-song-s-snippet-c9512a9290cd?source=post_page-----23794ab6955c--------------------------------)
    [## 谷歌的 AudioLM：通过听歌曲片段生成音乐'
- en: Whether music or speech, Google's new model can continue playing what is hearing.
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无论是音乐还是语音，谷歌的新模型都能继续播放所听到的内容。
- en: pub.towardsai.net](https://pub.towardsai.net/googles-audiolm-generating-music-by-hearing-a-song-s-snippet-c9512a9290cd?source=post_page-----23794ab6955c--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/googles-audiolm-generating-music-by-hearing-a-song-s-snippet-c9512a9290cd?source=post_page-----23794ab6955c--------------------------------)'
- en: 'Meanwhile, as they explain AudioLM was used as a starting point. The previous
    model was able to take a melody and continue it in a coherent way. **Nevertheless,
    there are several technical limitations to overcome**:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，正如他们所解释的，AudioLM被用作起点。之前的模型能够将旋律继续下去，并且保持一致性。**然而，仍然有几个技术限制需要克服**：
- en: The first main limitation of such a model is the “scarcity of paired audio-text
    data.” In fact, text-to-image training was facilitated by the fact that there
    are so many images, and alt text can be used as captions.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种模型的第一个主要限制是“音频-文本配对数据的稀缺”。事实上，文本到图像的训练得以实现是因为有大量的图像，且可以使用替代文本作为说明。
- en: It is not easy to be able to describe in a few words the salient features of
    music such as acoustic scenes or the timbre of a melody.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用几句话描述音乐的显著特征，如声学场景或旋律的音色，并不容易。
- en: Also, the music unfolds on a temporal dimension, so there is a risk of a weak
    link between the caption and the music (in contrast, an image is static).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，音乐在时间维度上展开，因此有可能出现说明和音乐之间的链接较弱的风险（与静态图像相比）。
- en: 'MusicLM: structure, training, results and limitations'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MusicLM：结构、训练、结果和限制
- en: '![](../Images/a1555ff223923fa9f981e34c6cd8a49c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1555ff223923fa9f981e34c6cd8a49c.png)'
- en: image by [James Stamler](https://unsplash.com/it/@jamesstamler) at unsplash.com
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[James Stamler](https://unsplash.com/it/@jamesstamler) 于 unsplash.com
- en: '**The first component of this model is MuLan** (and also the core part). This
    model is used to construct a music-text joint embedding and consists of two embedding
    towers (one for textual input and one for musical input. The two towers are pre-trained
    [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) and a variant of [ResNeT-50](https://www.kaggle.com/datasets/keras/resnet50)
    for audio.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**这个模型的第一个组件是 MuLan**（也是核心部分）。这个模型用于构建音乐-文本联合嵌入，由两个嵌入塔组成（一个用于文本输入，一个用于音乐输入）。这两个塔是预训练的[
    BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) 和 [ResNeT-50](https://www.kaggle.com/datasets/keras/resnet50)
    的变体，用于音频。'
- en: MuLan was trained on pairs of music clips and their corresponding textual annotations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MuLan 在音乐片段及其相应的文本注释对上进行训练。
- en: '![](../Images/c44a1e8372e26a7efcff602f99325887.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c44a1e8372e26a7efcff602f99325887.png)'
- en: 'the three components of MusicLm have been independently pre-trained. The other
    two components are discussed below. Image source: [here](https://arxiv.org/pdf/2301.11325.pdf)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: MusicLM 的三个组件已分别进行预训练。其他两个组件如下文所述。图片来源：[这里](https://arxiv.org/pdf/2301.11325.pdf)
- en: 'As we saw in the image the authors there are two other components. As the authors
    explain:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在图像中看到的那样，作者提到还有两个其他组件。正如作者解释的：
- en: We use the self-supervised audio representations of SoundStream as acoustic
    tokens to enable high-fidelity synthesis, and w2vBERT , as semantic tokens to
    facilitate long-term coherent generation. For representing the conditioning, we
    rely on the MuLan music embedding during training and the MuLan text embedding
    at inference time.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们使用SoundStream的自监督音频表示作为声学标记，以实现高保真合成，并使用w2vBERT作为语义标记，以促进长期一致的生成。在条件表示方面，我们在训练期间依赖MuLan音乐嵌入，在推断时依赖MuLan文本嵌入。
- en: 'As complicated as, this system may seem, it allows several advantages: it can
    scale quickly, and the use of [contrastive loss](https://paperswithcode.com/method/supervised-contrastive-loss)
    for embedding training increases robustness. In addition, **having pre-trained
    models separately allows for better conditioning of music with textual input**.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个系统看起来很复杂，但它有几个优点：它可以快速扩展，并且使用[对比损失](https://paperswithcode.com/method/supervised-contrastive-loss)进行嵌入训练可以提高鲁棒性。此外，**单独拥有预训练模型可以更好地处理带有文本输入的音乐**。
- en: '**During training**, the model learns to convert the token mapping produced
    by MuLan to semantic tokens ([w2w-BERT](https://arxiv.org/abs/2108.06209)). Then
    the acoustic token is conditioned on both the MuLan audio tokens and the semantic
    tokens ([SoundStream](https://arxiv.org/abs/2107.03312)).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**在训练期间**，模型学习将MuLan生成的标记映射转换为语义标记（[w2w-BERT](https://arxiv.org/abs/2108.06209)）。然后，声学标记在MuLan音频标记和语义标记（[SoundStream](https://arxiv.org/abs/2107.03312)）的条件下进行处理。'
- en: '![](../Images/ad0f9c2b66f7140c496a4559afe7afed.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad0f9c2b66f7140c496a4559afe7afed.png)'
- en: 'model during training. Image source: [here](https://arxiv.org/pdf/2301.11325.pdf)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 训练期间的模型。图片来源：[这里](https://arxiv.org/pdf/2301.11325.pdf)
- en: '**During inference** the process is to provide a textual description to MuLAn
    which transforms it into conditional signaling, this in turn is transformed into
    an audio token by w2w-BERT and then transformed into waveforms by the SoundStream
    decoder.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**在推断期间**，过程是将文本描述提供给MuLan，MuLan将其转换为条件信号，接着w2w-BERT将其转换为音频标记，然后由SoundStream解码器将其转换为波形。'
- en: '![](../Images/0b9a6d25c39fd712f1ce6ec88928946d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b9a6d25c39fd712f1ce6ec88928946d.png)'
- en: 'model during inference. Image source: [here](https://arxiv.org/pdf/2301.11325.pdf)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 推断期间的模型。图片来源：[这里](https://arxiv.org/pdf/2301.11325.pdf)
- en: What also makes MusicLM so capable is that it has been trained on 5 million
    audio clips (a total of 280,000 hours of audio). In addition, the authors have
    created a dataset of 5.5 thousand music clips with captions written by professional
    musicians (this dataset has been released [here](https://www.kaggle.com/datasets/googleai/musiccaps)).
    Each of these captions describes the music with four sentences and is followed
    by a list of music aspects (genre, mood, rhythm, etc.)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使MusicLM如此强大的原因之一是它在500万条音频剪辑上进行了训练（总共280,000小时的音频）。此外，作者还创建了一个包含5500个由专业音乐家编写字幕的音乐剪辑的数据集（该数据集已发布[这里](https://www.kaggle.com/datasets/googleai/musiccaps)）。每个字幕描述了音乐的四句话，并附有音乐方面的列表（如流派、情绪、节奏等）。
- en: As the results show, MusicML proves to be superior to previous models (Mubert
    and [Riffusion](https://techcrunch.com/2022/12/15/try-riffusion-an-ai-model-that-composes-music-by-visualizing-it/)),
    both in terms of audio quality and text faithfulness. Moreover, in terms of listening
    quality as well. In fact, listeners were shown clips and asked to choose which
    clip best represented the textual description (a win means that the listener preferred
    the model in a side-by-side comparison)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如结果所示，MusicML在音频质量和文本一致性方面都优于之前的模型（Mubert和[Riffusion](https://techcrunch.com/2022/12/15/try-riffusion-an-ai-model-that-composes-music-by-visualizing-it/)）。而且在听觉质量方面也是如此。实际上，听众被展示了剪辑并被要求选择哪个剪辑最能代表文本描述（胜利意味着听众在对比中更喜欢该模型）。
- en: '![](../Images/b5196c7011595bca45bbf5e39d318eff.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5196c7011595bca45bbf5e39d318eff.png)'
- en: '“Evaluation of generated samples using captions from the MusicCaps dataset.
    Models are compared in terms of audio quality, by means of Frechet Audio Distance
    (FAD), and faithfulness to the ´ text description, using Kullback–Leibler Divergence
    (KLD) and MuLan Cycle Consistency (MCC), and counts of wins in pairwise human
    listening tests (Wins).” Image source: [here](https://arxiv.org/pdf/2301.11325.pdf)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: “使用来自MusicCaps数据集的字幕对生成样本进行评估。通过Frechet音频距离（FAD）比较音频质量，使用Kullback–Leibler散度（KLD）和MuLan循环一致性（MCC）以及人类听力测试中的胜利次数（Wins）来比较与文本描述的一致性。”
    图片来源：[这里](https://arxiv.org/pdf/2301.11325.pdf)
- en: '![](../Images/f05c58aa78510760d95f4a20116631b3.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f05c58aa78510760d95f4a20116631b3.png)'
- en: 'The user interface for the human listener study. Image source: [here](https://arxiv.org/pdf/2301.11325.pdf)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 人类听众研究的用户界面。图片来源：[这里](https://arxiv.org/pdf/2301.11325.pdf)
- en: '**Listening to the results, it is hard not to be impressed,** considering that
    there are no musicians in the loop. After all, the model manages to capture musical
    nuances such as instrumental riffs and melodies.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**听到结果时，难免令人印象深刻**，考虑到模型中没有音乐家参与。毕竟，该模型能够捕捉到音乐细节，如乐器即兴演奏和旋律。'
- en: Moreover, the model is not limited to textual descriptions but can take other
    audio as input and continue them (“which is provided in the form of humming, singing,
    whistling, or playing an instrument”).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该模型不仅限于文本描述，还可以以其他音频作为输入并继续处理（“可以以哼唱、歌唱、吹口哨或演奏乐器的形式提供”）。
- en: '**The authors also describe an approach called “story mode”**, which is to
    generate long audio sequences where the textual description changes over time.
    The model generates musical sequences with “smooth transitions which are tempo
    consistent and semantically plausible while changing music context according to
    the text description.” These descriptions can also contain descriptions com “time
    to meditate” or “time to run” creating a narrative in which appropriate music
    is associated.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者还描述了一种称为“故事模式”的方法**，即生成长音频序列，其中文本描述随着时间的推移而变化。该模型生成具有“平滑过渡、节奏一致且语义合理的音乐序列，同时根据文本描述改变音乐背景”的音乐序列。这些描述还可以包含“冥想时间”或“跑步时间”的描述，从而创建一个将适当音乐与之关联的叙事。'
- en: The model in short is not limited to the description of an instrument or genre
    but can be conditioned with descriptions that are inspired by activities, epochs,
    places, or moods. MusicLM could therefore also be used for movie soundtracks,
    workout apps, etc.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，该模型并不仅限于对某一乐器或音乐流派的描述，还可以根据活动、时代、地点或情绪的描述进行调整。因此，MusicLM 也可以用于电影配乐、锻炼应用等。
- en: At this link, you can also read the captions and listen to the model result.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在此链接中，你还可以阅读字幕并听到模型的结果。
- en: '[## MusicLM'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[## MusicLM'
- en: Andrea Agostinelli, Timo I. Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti,
    Antoine Caillon, Qingqing Huang, Aren…
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Andrea Agostinelli, Timo I. Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti,
    Antoine Caillon, Qingqing Huang, Aren…
- en: google-research.github.io](https://google-research.github.io/seanet/musiclm/examples/?source=post_page-----23794ab6955c--------------------------------)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[google-research.github.io](https://google-research.github.io/seanet/musiclm/examples/?source=post_page-----23794ab6955c--------------------------------)'
- en: '**Though impressive the model is not perfect**, and some of the audio clips
    have a distorted quality (but this could be improved in the future with training
    modifications). In addition, the model can also generate choirs and people singing,
    but often the lyrics are not even in English but in a kind of gibberish and the
    voice sounds more like an amalgam of singers than a coherent human voice. Thus,
    how the authors state:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**尽管模型令人印象深刻，但并不完美**，一些音频片段的质量存在失真（但未来可能通过训练调整得到改善）。此外，该模型还可以生成合唱和人声，但歌词通常不是英语，而是一种无意义的语言，并且声音听起来更像是多位歌手的混合，而非连贯的人声。因此，作者如是说：'
- en: Future work may focus on lyrics generation, along with improvement of text conditioning
    and vocal quality. Another aspect is the modeling of high-level song structure
    like introduction, verse, and chorus. Modeling the music at a higher sample rate
    is an additional goal.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 未来的工作可能会集中在歌词生成上，同时改进文本条件和声音质量。另一个方面是建模高级的歌曲结构，如引言、诗句和副歌。以更高的采样率建模音乐是一个额外的目标。
- en: 'Google decided not to distribute the model: “there are several risks associated
    with our model and the use-case it tackles.” As the authors note, the model reflects
    the biases that are present within the training data, posing problems for generating
    music for cultures that are underrepresented within the dataset. They also point
    out that the model raises ethical questions about cultural appropriation.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Google 决定不分发该模型：“我们的模型及其处理的用例存在一些风险。”正如作者所指出的，该模型反映了训练数据中存在的偏见，这在为数据集中代表性不足的文化生成音乐时会带来问题。他们还指出，该模型提出了关于文化挪用的伦理问题。
- en: '![](../Images/91c6cb3068c877ccfccb293aaa961ea2.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91c6cb3068c877ccfccb293aaa961ea2.png)'
- en: '“Genre distribution of all 5.5k examples of MusicCaps, according to an AudioSet
    classifier”. Image source: [here](https://arxiv.org/pdf/2301.11325.pdf)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: “MusicCaps 中所有 5.5k 示例的流派分布，基于 AudioSet 分类器”。图片来源：[这里](https://arxiv.org/pdf/2301.11325.pdf)
- en: Another problem is that in 1% of cases, the model generates music that it has
    heard during training, replicating parts of existing songs (even elements that
    are copyrighted). [TechCrunch suggests that this is enough](https://techcrunch.com/2023/01/27/google-created-an-ai-that-can-generate-music-from-text-descriptions-but-wont-release-it/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAK6OO6H73AxKjuqHai2HzS-KZFESx33sBYlc-8NKW0jnv6BUQlduIQ2-LqwaZr6ryfZq0BQ6F03HrFCY3mDS5CnYUWLZiNskZK-YzWxP943yOaC4gaKFfI7owLanSlNFG_kazxH79ZN6G_71N5ZVyK7AtziTDGsXAUVoNW_daWxZ)
    to discourage Google from releasing the model (especially now that institutions
    are interested in regulating AI and generative tools).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，在1%的情况下，模型生成了在训练期间听到的音乐，复制了现有歌曲的部分（甚至是受版权保护的元素）。[TechCrunch 认为这足以](https://techcrunch.com/2023/01/27/google-created-an-ai-that-can-generate-music-from-text-descriptions-but-wont-release-it/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAK6OO6H73AxKjuqHai2HzS-KZFESx33sBYlc-8NKW0jnv6BUQlduIQ2-LqwaZr6ryfZq0BQ6F03HrFCY3mDS5CnYUWLZiNskZK-YzWxP943yOaC4gaKFfI7owLanSlNFG_kazxH79ZN6G_71N5ZVyK7AtziTDGsXAUVoNW_daWxZ)劝阻谷歌发布模型（尤其是在机构有意规范人工智能和生成工具的情况下）。
- en: 'They acknowledge the risk of potential misappropriation of creative content
    linked with the use case and further highlight the necessity of deep studies and
    analysis, concluding:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 他们认识到与使用案例相关的创意内容潜在的误用风险，并进一步强调了深入研究和分析的必要性，得出结论：
- en: We strongly emphasize the need for more future work in tackling these risks
    associated to music generation — we have no plans to release models at this point
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们强烈强调需要更多的未来工作来应对与音乐生成相关的这些风险——目前我们没有发布模型的计划。
- en: '[](https://medium.com/mlearning-ai/the-eu-wants-to-regulate-your-favorite-ai-tools-7419d985f2f2?source=post_page-----23794ab6955c--------------------------------)
    [## The EU wants to regulate your favorite AI tools'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 欧盟想要规范你最喜欢的人工智能工具](https://medium.com/mlearning-ai/the-eu-wants-to-regulate-your-favorite-ai-tools-7419d985f2f2?source=post_page-----23794ab6955c--------------------------------)'
- en: EU is preparing a new AI bill and generative AI is included
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欧盟正在准备一项新的人工智能法案，生成式人工智能也被包含在内。
- en: medium.com](https://medium.com/mlearning-ai/the-eu-wants-to-regulate-your-favorite-ai-tools-7419d985f2f2?source=post_page-----23794ab6955c--------------------------------)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/mlearning-ai/the-eu-wants-to-regulate-your-favorite-ai-tools-7419d985f2f2?source=post_page-----23794ab6955c--------------------------------)'
- en: '**Conclusions**'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: MusicLM is impressive in its ability to generate music from the textual description,
    a quantum leap compared to previous models. It generates coherent music, long
    sequences capable of capturing music nuances. Although it has been trained with
    an impressive amount of data, it is still not perfect.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: MusicLM 在从文本描述生成音乐方面表现出色，相较于之前的模型是一个质的飞跃。它生成连贯的音乐，长序列能够捕捉音乐的细微差别。尽管它已经用大量数据进行了训练，但仍然不完美。
- en: Also, as described by the authors since the model produces copyrighted music
    they have no plans to release it. This is probably also derived from the fact
    that some programmers recently [have sued GitHub Copilot](https://www.theverge.com/2022/11/8/23446821/microsoft-openai-github-copilot-class-action-lawsuit-ai-copyright-violation-training-data)
    and [Getty images filed lawsuits against Stability AI](https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit)
    and MidJourney.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，正如作者所描述的，由于模型生成了受版权保护的音乐，他们没有计划发布它。这可能也源于一些程序员最近[起诉了 GitHub Copilot](https://www.theverge.com/2022/11/8/23446821/microsoft-openai-github-copilot-class-action-lawsuit-ai-copyright-violation-training-data)和[Getty
    images 起诉了 Stability AI](https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit)以及
    MidJourney。
- en: Moreover, as noted by the authors, generative music models are incorporating
    bias from the training set. MusicLM is not only generating instrumental music
    but can also generate vocals and chorus, which can lead to incorporating bias
    and harmful content in the generated music.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如作者所指出的，生成音乐模型正在从训练集中引入偏见。MusicLM不仅生成器乐音乐，还可以生成声乐和合唱，这可能导致生成的音乐中包含偏见和有害内容。
- en: In addition, as noted by several musicians, streaming has thinned the earnings
    of musicians and composers, and [generative AI could reduce their income still
    further](https://biz.crast.net/what-happens-to-songwriters-when-ai-can-generate-music/).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如多位音乐家所指出的，流媒体已经稀薄了音乐家和作曲家的收入，而[生成式人工智能可能进一步减少他们的收入](https://biz.crast.net/what-happens-to-songwriters-when-ai-can-generate-music/)。
- en: Instead, the authors suggest that rather than replacing musicians and composers
    these models will soon be a set of tools to assist humans.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，作者建议这些模型将不会取代音乐家和作曲家，而是很快成为辅助人类的一组工具。
- en: 'if you have found it interesting:'
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得有趣：
- en: You can look for my other articles, you can also [**subscribe**](https://salvatore-raieli.medium.com/subscribe)
    to get notified when I publish articles, and you can also connect or reach me
    on[**LinkedIn**](https://www.linkedin.com/in/salvatore-raieli/)**.** Thanks for
    your support!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查找我的其他文章，你也可以[**订阅**](https://salvatore-raieli.medium.com/subscribe)以在我发布新文章时收到通知，还可以在[**LinkedIn**](https://www.linkedin.com/in/salvatore-raieli/)**上联系我**。感谢你的支持！
- en: Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我GitHub代码库的链接，我计划在这里收集与机器学习、人工智能等相关的代码和资源。
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----23794ab6955c--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…](https://github.com/SalvatoreRa/tutorial?source=post_page-----23794ab6955c--------------------------------)'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 包含数学解释和可重用代码（Python）的机器学习、人工智能、数据科学教程……
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----23794ab6955c--------------------------------)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…](https://github.com/SalvatoreRa/tutorial?source=post_page-----23794ab6955c--------------------------------)'
