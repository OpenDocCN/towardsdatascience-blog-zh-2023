- en: A Step-by-step Guide to Solving 4 Real-life Problems With Transformers and Hugging
    Face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/4-real-life-problems-solved-using-transformers-and-hugging-face-a-complete-guide-e45fe698cc4d](https://towardsdatascience.com/4-real-life-problems-solved-using-transformers-and-hugging-face-a-complete-guide-e45fe698cc4d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understand Transformers and harness their power to solve real-life problems
    in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://zoumanakeita.medium.com/?source=post_page-----e45fe698cc4d--------------------------------)[![Zoumana
    Keita](../Images/34a15c1d03687816dbdbc065f5719f80.png)](https://zoumanakeita.medium.com/?source=post_page-----e45fe698cc4d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e45fe698cc4d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e45fe698cc4d--------------------------------)
    [Zoumana Keita](https://zoumanakeita.medium.com/?source=post_page-----e45fe698cc4d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e45fe698cc4d--------------------------------)
    ·11 min read·Jan 31, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/429c05a1b2db762b98d5755b81d78326.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Aditya Vyas](https://unsplash.com/@aditya1702) on [Unsplash](https://unsplash.com/photos/B9MULm2UZIk)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the field of Natural Language Processing (NLP), researchers have made significant
    contributions over the past decades, resulting in innovative advancements in various
    domains. Some examples of NLP in practice are provided below:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Siri**, a personal assistant developed by Apple, can assist users with tasks
    like setting alarms, sending texts, and answering questions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the **medical field**, NLP is being utilized to speed up drug discovery.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, NLP is also being used to bridge language barriers through **translation**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The purpose of this article is to discuss Transformers, an extremely powerful
    model in Natural Language Processing. It will begin by highlighting the advantages
    of Transformers over recurrent neural networks, furthering your comprehension
    of the model. Then, it will provide practical examples of using Huggingface transformers
    in real-world scenarios
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Network — the shinning era before Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before delving into the fundamental idea of transformers, it’s important to
    gain a basic understanding of recurrent models, including their limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent networks use an encoder-decoder structure and are typically used for
    tasks involving input and output sequences in a specific order. Some of the most
    common applications of recurrent networks include machine translation and modeling
    time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with recurrent networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an example, let’s take a French sentence and translate it into English. The
    encoder receives the original French sentence as input, and the decoder produces
    the translated output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c8639426bdc40dfafc77c5b86aa8163.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple illustration of the recurrent network (By Author)
  prefs: []
  type: TYPE_NORMAL
- en: The encoder processes the input French sentence word by word, and the decoder
    generates word embeddings in the same sequence, making them time-consuming to
    train.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hidden state of the current word is dependent on the hidden states of the
    previous words, making it impossible to perform parallel computation, regardless
    of the computational power available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence-to-sequence neural networks are prone to experiencing exploding gradients
    when the network is too large, resulting in poor performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another type of recurrent network, Long Short-Term Memory (LSTM) networks, were
    developed to address the issue of vanishing gradients, but they are even slower
    than traditional sequence models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wouldn’t it be beneficial to have a model that combines the advantages of recurrent
    networks and enables parallel computation?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Here is where transformers come in handy.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What are transformers in NLP?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2017, Google Brain introduced Transformers, a new, powerful neural network
    architecture, in their renowned research paper “Attention is all you need.” It
    is based on the attention mechanism rather than the sequential computation found
    in recurrent networks.
  prefs: []
  type: TYPE_NORMAL
- en: What are the main components of transformers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like recurrent networks, transformers also consist of two main components:
    an encoder and a decoder, each incorporating a self-attention mechanism. The following
    section provides an overall understanding of the primary elements of each component
    of transformers'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d25a9de3087b7b52d0c3fcd3f74227cd.png)'
  prefs: []
  type: TYPE_IMG
- en: General [Architecture of Transformers](https://arxiv.org/abs/1706.03762) (adapted
    by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Input sentence preprocessing stage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section involves two primary steps: (1) creating the embeddings of the
    input sentence, and (2) calculating the positional vector of each word in the
    input sentence. These computations are carried out in the same way for both the
    input sentence (prior to the encoder block) and the output sentence (before the
    decoder block).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Embedding of the input data**'
  prefs: []
  type: TYPE_NORMAL
- en: Before creating the embeddings of the input data, we begin by tokenizing it,
    then creating the embedding for each individual word without considering their
    relationship within the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '**Positional encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: The tokenization process eliminates any sense of connections that existed in
    the input sentence. The positional encoding aims to restore the original cyclical
    nature by creating a context vector for each word.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder bloc
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a result of the previous step, we obtain a combination of two vectors for
    each word: (1) the embedding and (2) its context vector. These vectors are combined
    to create a single vector for each word, which is then sent to the encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-head attention**'
  prefs: []
  type: TYPE_NORMAL
- en: As previously stated, all sense of relationship is lost. The purpose of the
    attention layer is to identify the contextual connections between different words
    in the input sentence. This step ultimately results in the creation of an attention
    vector for each word.
  prefs: []
  type: TYPE_NORMAL
- en: '**Position-wise feed-forward net (FFN)**'
  prefs: []
  type: TYPE_NORMAL
- en: In this step, a feed-forward neural network is applied to each attention vector
    to change them into a format that can be used by the following multi-head attention
    layer in the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder block
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The decoder block is made up of three main layers: a masked multi-head attention
    layer, a multi-head attention layer, and a position-wise feed-forward network.
    The last two layers are similar to those in the encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The decoder is used during training and takes two inputs: the attention vectors
    of the input sentence being translated and the corresponding target sentences
    in English.'
  prefs: []
  type: TYPE_NORMAL
- en: '**So, what is the masked multi-head attention layer responsible for?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: During the generation of the next English word, the network is allowed to use
    all the words from the French word. However, when dealing with a given word in
    the target sequence (English translation), the network only has to access the
    previous words, because making the next ones available will lead the network to
    “cheat” and not make any effort to learn properly. Here is where the masked multi-head
    attention layer has all its benefits. It masks those next words by transforming
    them into zeros so that they can’t be used by the attention network.
  prefs: []
  type: TYPE_NORMAL
- en: The result of the masked multi-head attention layer passes through the rest
    of the layers in order to predict the next word by generating a probability score.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning in NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training deep neural networks such as transformers from scratch is not an easy
    task, and might present the following challenges: (1) finding the required amount
    of data for the target problem can be time-consuming, and (2) getting the necessary
    computation resources like GPUs to train such deep networks can be very costly.'
  prefs: []
  type: TYPE_NORMAL
- en: Image building a model from scratch to translate ***Mandingo language into Wolof,***
    which arebothlow resources languages***.*** Gathering data related to those languages
    is costly. Instead of going through all these challenges, one can re-use pre-trained
    deep-neural networks as the starting point for training the new model.
  prefs: []
  type: TYPE_NORMAL
- en: Such models have been trained on a huge corpus of data, made available by someone
    else (moral person, organization, etc.), and evaluated to work very well on language
    translation tasks such as French to English.
  prefs: []
  type: TYPE_NORMAL
- en: But what do you mean by re-use deep-neural networks?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The re-use of the model involves choosing the pre-trained model that is similar
    to your use case, refining the input-output pair data of your target task, and
    retraining the higher layers of the pre-trained model by using your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The introduction of transformers has led to the development of state-of-the-art
    transfer learning models such as:'
  prefs: []
  type: TYPE_NORMAL
- en: BERT short for **B**idirectional **E**ncoder **R**epresentations from **T**ransformers
    was developed by Google researchers in 2018\. It helps to solve the most common
    language tasks such as named entity recognition, sentiment analysis, question-answering,
    text summarization, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT3 (Generative Pre-Training-3), proposed by OpenAI researchers. It is a multi-layer
    transformer, mainly used to generate any type of text. GPT models are capable
    of producing human-like text responses to a given question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to Hugging Face Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hugging Face is an AI community and Machine Learning platform created in 2016
    by Julien Chaumond, Clément Delangue, and Thomas Wolf. It aims to democratize
    NLP by providing Data Scientists, AI practitioners, and Engineers immediate access
    to over 20000 pre-trained models based on the state-of-the-art transformer architecture.
    These models can be applied to:'
  prefs: []
  type: TYPE_NORMAL
- en: Text in over 100 languages, for performing tasks such as classification, information
    extraction, question answering, generation, generation, and translation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech, for tasks such as object audio classification and speech recognition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vision for object detection, image classification, and segmentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face transformers also provides almost 2000 data sets and layered APIs
    allowing programmers to easily interact with those models using the three most
    popular deep learning libraries: Pytorch, Tensorflow, and Jax.'
  prefs: []
  type: TYPE_NORMAL
- en: Other components of the Hugging Face transformers are the [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines).
    These are objects that abstract the complexity of the code from the library. They
    make it easy to use all these models for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face Tutorial
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you have a better understanding of transformers, and the Hugging Face
    platform, we will walk you through the following real-world scenarios: sequence
    classification, language translation, text generation, question answering, named
    entity recognition, and text summarization.'
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step is to install the transformers library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be using the Internet News and Consumer Engagement data set from [Kaggle](https://www.kaggle.com/datasets/szymonjanowski/internet-articles-data-with-users-engagement).
    This data set is made freely availably by [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)
    and was created to predict the popularity of an article before its publication.'
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity’s sake, the tutorial will be using only three examples from the
    data, and the analysis is based on the description column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8bd3cd5bb79bd7e0bd60b35fd7b967ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Original news in English (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Language Translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MariamMT is an efficient Machine Translation framework. It uses the [MarianNMT](https://marian-nmt.github.io/)
    engine under the hood, which is purely developed in C++ by Microsoft and many
    academic institutions such as the University of Edinburgh, and Adam Mickiewicz
    University in Poznań. The same engine is currently behind the [Microsoft Translator](http://translator.microsoft.com/)
    service.
  prefs: []
  type: TYPE_NORMAL
- en: The [NLP group from the University of Helsinki](https://blogs.helsinki.fi/language-technology/)
    open-sourced multiple translation models on Hugging Face Transformers and they
    are all in the following format `Helsinki-NLP/opus-mt-{src}-{tgt}`where `{src}`
    and `{tgt}`correspond respectively to the source and target languages.
  prefs: []
  type: TYPE_NORMAL
- en: So, in our case, the source language is English (en) and the target language
    is French (fr)
  prefs: []
  type: TYPE_NORMAL
- en: MarianMT is one of those models previously trained using Marian on parallel
    data collected at [Opus](https://opus.nlpl.eu/).
  prefs: []
  type: TYPE_NORMAL
- en: 'MarianMT requires `sentencepiece` in addition to Transformers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Select the pre-trained model, get the tokenizer and load the pre-trained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Add the special token `>>{tgt}<<`in front of each source (English) text with
    the help of the following function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the batch translation logic with the help of the following function,
    a batch being a list of texts to be translated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/f9f8f7fef13b97168fc274c8e6575fca.png)'
  prefs: []
  type: TYPE_IMG
- en: Translated texts
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the time, training a Machine Learning model requires all the candidate
    labels/targets to be known beforehand, meaning that if your training labels are
    **science**, **politics**, or **education**, you will not be able to predict the
    **healthcare** label unless you retrain your model, taking into consideration
    that label and the corresponding input data.
  prefs: []
  type: TYPE_NORMAL
- en: This powerful approach makes it possible to predict the target of a text in
    about 15 languages without having seen any of the candidate labels. We can use
    this model by simply loading it from the hub.
  prefs: []
  type: TYPE_NORMAL
- en: The goal here is to try to classify the category of each of the previous descriptions,
    whether it is ***tech****,* ***politics****,* ***security,***or***finance***.
  prefs: []
  type: TYPE_NORMAL
- en: Import the pipeline module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Define candidate labels. These correspond to what we want to predict: ***tech****,*
    ***politics****,* ***business,***or***finance***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Define the classifier with the multi-lingual option
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Implement the prediction logic using this helper function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the predictions on the first and the last descriptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b623f00081e60981cd526c2504213bf0.png)'
  prefs: []
  type: TYPE_IMG
- en: Original textual description
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/793ab28e5f86f849926116e2cb691e7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Text predicted to be mainly about finance (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: This previous result shows that the text is overall about finance at 81%.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the last description, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7bc527410c6eaba16da38414e18b7c31.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a8f6409eb75d5aa89b802f3ebb35ddd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Text predicted to be mainly about tech (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: This previous result shows that the text is overall about tech at 95%.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most models performing sentiment classification require proper training. The
    hugging Face pipeline module makes it easy to run sentiment analysis predictions
    by using a specific model available on the hub by specifying its name.
  prefs: []
  type: TYPE_NORMAL
- en: Load the pipeline module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Chose the task to perform and load the corresponding model. Here, we want to
    perform sentiment classification, using distill BERT base model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The model is ready! Let's analyze the underlying sentiments behind the last
    two sentences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b745e22c0ab4a2fdf757d446d8dc82e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Sentiment scores (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: The model predicted the first text to have a negative sentiment with 96% of
    confidence, and the second one is predicted with positive sentiment at 52% of
    confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Question Answering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine dealing with a report much longer than the one about Apple. And, all
    you are interested in is the date of the event being mentioned. Instead of reading
    the whole report to find the key information, we can use a question-answering
    model from Hugging Face that will provide the answer we are interested in.
  prefs: []
  type: TYPE_NORMAL
- en: This can be done by providing the model with proper context (Apple’s report)and
    the question we are interested in finding the answer to.
  prefs: []
  type: TYPE_NORMAL
- en: Import the question-answering class and tokenizer from transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Instantiate the model using its name and its tokenizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Request the model by asking the question and specifying the context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Get the result of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/15915fd60f52c1f919d5e604f43d34c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Question Answering Result (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: The model answered that Apple’s event is on September 10th with high confidence
    of 97%. It even specifies where the answer is in the text by providing the starting
    and ending locations.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we’ve covered the evolution of natural language technology
    from recurrent networks to transformers and how Hugging Face has democratized
    the use of NLP through its platform.
  prefs: []
  type: TYPE_NORMAL
- en: If you are still hesitant about using transformers, we believe it is time to
    give them a try and add value to your business cases.
  prefs: []
  type: TYPE_NORMAL
- en: Also, If you like reading my stories and wish to support my writing, consider
    [becoming a Medium member](https://zoumanakeita.medium.com/membership). With a
    $ 5-a-month commitment, you unlock unlimited access to stories on Medium.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to follow me on [Medium](https://zoumanakeita.medium.com/), [Twitter](https://twitter.com/zoumana_keita_),
    and [YouTube](https://www.youtube.com/channel/UC9xKdy8cz6ZuJU5FTNtM_pQ), or say
    Hi on [LinkedIn](https://www.linkedin.com/in/zoumana-keita/). It is always a pleasure
    to discuss AI, ML, Data Science, NLP, and MLOps stuff!
  prefs: []
  type: TYPE_NORMAL
- en: The article [When Should You Consider Using Datatable Instead of Pandas to Process
    Large Data?](/when-should-you-consider-using-datatable-instead-of-pandas-to-process-large-data-29a4245f67c6)
    could be a good next step.
  prefs: []
  type: TYPE_NORMAL
