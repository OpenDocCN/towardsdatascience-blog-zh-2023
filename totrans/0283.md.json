["```py\nimport requests\nfrom bs4 import BeautifulSoup\n\nurl = \"https://en.wikipedia.org/wiki/GPT-4\"\nresponse = requests.get(url)\n\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# find all the text on the page\ntext = soup.get_text()\n\n# find the content div\ncontent_div = soup.find('div', {'class': 'mw-parser-output'})\n\n# remove unwanted elements from div\nunwanted_tags = ['sup', 'span', 'table', 'ul', 'ol']\nfor tag in unwanted_tags:\n    for match in content_div.findAll(tag):\n        match.extract()\n\nprint(content_div.get_text())\n```", "```py\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\narticle_text = content_div.get_text()\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    # Set a really small chunk size, just to show.\n    chunk_size = 100,\n    chunk_overlap  = 20,\n    length_function = len,\n)\n\ntexts = text_splitter.create_documents([article_text])\nprint(texts[0])\nprint(texts[1])\n```", "```py\nimport openai\n\nprint(texts[0])\n\nembedding = openai.Embedding.create(\n    input=texts[0].page_content, model=\"text-embedding-ada-002\"\n)[\"data\"][0][\"embedding\"]\n\nlen(embedding)\n```", "```py\nimport numpy as np\nfrom numpy.linalg import norm\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport openai\n\n####################################################################\n# load documents\n####################################################################\n# URL of the Wikipedia page to scrape\nurl = 'https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom'\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Parse the HTML content using BeautifulSoup\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Find all the text on the page\ntext = soup.get_text()\n\n####################################################################\n# split text\n####################################################################\ntext_splitter = RecursiveCharacterTextSplitter(\n    # Set a really small chunk size, just to show.\n    chunk_size = 100,\n    chunk_overlap  = 20,\n    length_function = len,\n)\n\ntexts = text_splitter.create_documents([text])\n\n####################################################################\n# calculate embeddings\n####################################################################\n# create new list with all text chunks\ntext_chunks=[]\n\nfor text in texts:\n    text_chunks.append(text.page_content)\n\ndf = pd.DataFrame({'text_chunks': text_chunks})\n\n####################################################################\n# get embeddings from text-embedding-ada model\n####################################################################\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\n   text = text.replace(\"\\n\", \" \")\n   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n\ndf['ada_embedding'] = df.text_chunks.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n\n####################################################################\n# calculate the embeddings for the user's question\n####################################################################\nusers_question = \"What is GPT-4?\"\n\nquestion_embedding = get_embedding(text=users_question, model=\"text-embedding-ada-002\")\n\n# create a list to store the calculated cosine similarity\ncos_sim = []\n\nfor index, row in df.iterrows():\n   A = row.ada_embedding\n   B = question_embedding\n\n   # calculate the cosine similarity\n   cosine = np.dot(A,B)/(norm(A)*norm(B))\n\n   cos_sim.append(cosine)\n\ndf[\"cos_sim\"] = cos_sim\ndf.sort_values(by=[\"cos_sim\"], ascending=False)\n```", "```py\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"testapikey213412\"\n```", "```py\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(temperature=0.7)\n```", "```py\nllm.__dict__\n```", "```py\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport numpy as np\nfrom numpy.linalg import norm\nimport pandas as pd\nimport openai\n\n####################################################################\n# load documents\n####################################################################\n# URL of the Wikipedia page to scrape\nurl = 'https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom'\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Parse the HTML content using BeautifulSoup\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Find all the text on the page\ntext = soup.get_text()\n\n####################################################################\n# split text\n####################################################################\ntext_splitter = RecursiveCharacterTextSplitter(\n    # Set a really small chunk size, just to show.\n    chunk_size = 100,\n    chunk_overlap  = 20,\n    length_function = len,\n)\n\ntexts = text_splitter.create_documents([text])\n\n####################################################################\n# calculate embeddings\n####################################################################\n# create new list with all text chunks\ntext_chunks=[]\n\nfor text in texts:\n    text_chunks.append(text.page_content)\n\ndf = pd.DataFrame({'text_chunks': text_chunks})\n\n# get embeddings from text-embedding-ada model\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\n   text = text.replace(\"\\n\", \" \")\n   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n\ndf['ada_embedding'] = df.text_chunks.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n\n####################################################################\n# calculate similarities to the user's question\n####################################################################\n# calcuate the embeddings for the user's question\nusers_question = \"Who is the current Prime Minister of the UK?\"\nquestion_embedding = get_embedding(text=users_question, model=\"text-embedding-ada-002\")\n```", "```py\nfrom langchain import PromptTemplate\nfrom langchain.llms import OpenAI\n\n# calcuate the embeddings for the user's question\nusers_question = \"Who is the current Prime Minister of the UK?\"\nquestion_embedding = get_embedding(text=users_question, model=\"text-embedding-ada-002\")\n\n# create a list to store the calculated cosine similarity\ncos_sim = []\n\nfor index, row in df.iterrows():\n   A = row.ada_embedding\n   B = question_embedding\n\n   # calculate the cosine similiarity\n   cosine = np.dot(A,B)/(norm(A)*norm(B))\n\n   cos_sim.append(cosine)\n\ndf[\"cos_sim\"] = cos_sim\ndf.sort_values(by=[\"cos_sim\"], ascending=False)\n```", "```py\nfrom langchain import PromptTemplate\nfrom langchain.llms import OpenAI\nimport openai\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport numpy as np\nfrom numpy.linalg import norm\nimport pandas as pd\nimport openai\n\n####################################################################\n# load documents\n####################################################################\n# URL of the Wikipedia page to scrape\nurl = 'https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom'\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Parse the HTML content using BeautifulSoup\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Find all the text on the page\ntext = soup.get_text()\n\n####################################################################\n# split text\n####################################################################\ntext_splitter = RecursiveCharacterTextSplitter(\n    # Set a really small chunk size, just to show.\n    chunk_size = 100,\n    chunk_overlap  = 20,\n    length_function = len,\n)\n\ntexts = text_splitter.create_documents([text])\n\n####################################################################\n# calculate embeddings\n####################################################################\n# create new list with all text chunks\ntext_chunks=[]\n\nfor text in texts:\n    text_chunks.append(text.page_content)\n\ndf = pd.DataFrame({'text_chunks': text_chunks})\n\n# get embeddings from text-embedding-ada model\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\n   text = text.replace(\"\\n\", \" \")\n   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n\ndf['ada_embedding'] = df.text_chunks.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n\n####################################################################\n# calculate similarities to the user's question\n####################################################################\n# calcuate the embeddings for the user's question\nusers_question = \"Who is the current Prime Minister of the UK?\"\nquestion_embedding = get_embedding(text=users_question, model=\"text-embedding-ada-002\")\n\n# create a list to store the calculated cosine similarity\ncos_sim = []\n\nfor index, row in df.iterrows():\n   A = row.ada_embedding\n   B = question_embedding\n\n   # calculate the cosine similiarity\n   cosine = np.dot(A,B)/(norm(A)*norm(B))\n\n   cos_sim.append(cosine)\n\ndf[\"cos_sim\"] = cos_sim\ndf.sort_values(by=[\"cos_sim\"], ascending=False)\n\n####################################################################\n# build a suitable prompt and send it\n####################################################################\n# define the LLM you want to use\nllm = OpenAI(temperature=1)\n\n# define the context for the prompt by joining the most relevant text chunks\ncontext = \"\"\n\nfor index, row in df[0:50].iterrows():\n    context = context + \" \" + row.text_chunks\n\n# define the prompt template\ntemplate = \"\"\"\nYou are a chat bot who loves to help people! Given the following context sections, answer the\nquestion using only the given context. If you are unsure and the answer is not\nexplicitly writting in the documentation, say \"Sorry, I don't know how to help with that.\"\n\nContext sections:\n{context}\n\nQuestion:\n{users_question}\n\nAnswer:\n\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])\n\n# fill the prompt template\nprompt_text = prompt.format(context = context, users_question = users_question)\nllm(prompt_text)\n```", "```py\nusers_question = \"Who was the first Prime Minister of the UK?\"\n```", "```py\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.document_loaders import TextLoader\n\n# URL of the Wikipedia page to scrape\nurl = 'https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom'\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Parse the HTML content using BeautifulSoup\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Find all the text on the page\ntext = soup.get_text()\ntext = text.replace('\\n', '')\n\n# Open a new file called 'output.txt' in write mode and store the file object in a variable\nwith open('output.txt', 'w', encoding='utf-8') as file:\n    # Write the string to the file\n    file.write(text)\n```", "```py\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# load the document\nwith open('./output.txt', encoding='utf-8') as f:\n    text = f.read()\n\n# define the text splitter\ntext_splitter = RecursiveCharacterTextSplitter(    \n    chunk_size = 500,\n    chunk_overlap  = 100,\n    length_function = len,\n)\n\ntexts = text_splitter.create_documents([text])\n```", "```py\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\n\n# define the embeddings model\nembeddings = OpenAIEmbeddings()\n\n# use the text chunks and the embeddings model to fill our vector store\ndb = Chroma.from_documents(texts, embeddings)\n```", "```py\nfrom langchain.llms import OpenAI\nfrom langchain import PromptTemplate\n\nusers_question = \"Who is the current Prime Minister of the UK?\"\n\n# use our vector store to find similar text chunks\nresults = db.similarity_search(\n    query=user_question,\n    n_results=5\n)\n\n# define the prompt template\ntemplate = \"\"\"\nYou are a chat bot who loves to help people! Given the following context sections, answer the\nquestion using only the given context. If you are unsure and the answer is not\nexplicitly writting in the documentation, say \"Sorry, I don't know how to help with that.\"\n\nContext sections:\n{context}\n\nQuestion:\n{users_question}\n\nAnswer:\n\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])\n\n# fill the prompt template\nprompt_text = prompt.format(context = results, users_question = users_question)\n\n# ask the defined LLM\nllm(prompt_text)\n```"]