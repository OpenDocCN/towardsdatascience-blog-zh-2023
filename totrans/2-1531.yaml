- en: Modern Data Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/modern-data-engineering-e202776fb9a9](https://towardsdatascience.com/modern-data-engineering-e202776fb9a9)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Platform Specific Tools and Advanced Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----e202776fb9a9--------------------------------)[![üí°Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----e202776fb9a9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e202776fb9a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e202776fb9a9--------------------------------)
    [üí°Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----e202776fb9a9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e202776fb9a9--------------------------------)
    ¬∑12 min read¬∑Nov 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4dcaf0e3ae7f465c2b949ddc1ac7edf.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Christopher Burns](https://unsplash.com/@christopher__burns?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The modern data ecosystem keeps evolving and new data tools emerge now and then.
    In this article, I want to talk about crucial things that affect data engineers.
    We will discuss how to use this knowledge to power advanced analytics pipelines
    and operational excellence.
  prefs: []
  type: TYPE_NORMAL
- en: 'I‚Äôd like to discuss some popular Data engineering questions:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modern data engineering (DE). What is it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does your DE work well enough to fuel advanced data pipelines and Business intelligence
    (BI)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are your data pipelines efficient?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is required from the technological point of view to enable operational
    excellence?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Back in October, I wrote about the rise of the Data Engineer, the role, its
    challenges, responsibilities, daily routine and how to become successful in this
    field. The data engineering landscape is constantly changing but major trends
    seem to remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-become-a-data-engineer-c0319cb226c2?source=post_page-----e202776fb9a9--------------------------------)
    [## How to Become a Data Engineer'
  prefs: []
  type: TYPE_NORMAL
- en: A shortcut for beginners in 2024
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-become-a-data-engineer-c0319cb226c2?source=post_page-----e202776fb9a9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: As a data engineer, I am tasked to design efficient data processes almost every
    day. So here are a few things to consider that can help us answer these questions.
  prefs: []
  type: TYPE_NORMAL
- en: Modern data engineering trends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ETL vs ELT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simplified data connectors and API integrations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ETL frameworks explosion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data infrastructure as code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Mesh and decentralized data management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Democratization of Business intelligence pipelines using AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on data literacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ELT vs ETL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Popular SQL data transformation tools like **Dataform** and **DBT** made a significant
    contribution to the popularisation of the ELT approach [1]. It simply makes sense
    to perform required data transformations, such as cleansing, enrichment and extraction
    in the place where data is being stored. Often it is a data warehouse solution
    (DWH) in the central part of our infrastructure. Cloud platform leaders made DWH
    (Snowflake, BigQuery, Redshift, Firebolt) infrastructure management really simple
    and in many scenarios they will outperform and dedicated in-house infrastructure
    management team in terms of cost-effectiveness and speed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4fbc3d51212c640c74e34cc21f6baff.png)'
  prefs: []
  type: TYPE_IMG
- en: Data warehouse exmaple. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: It also might be a datalake in the center and it depends on the type of our
    data platform and tools we use. In this case, SQL stops being an option in many
    cases making it difficult to query the data for those users who are not familiar
    with programming. Tools like Databricks, Tabular and Galaxy try to solve this
    problem and it really feels like the future. Indeed, datalakes can store all types
    of data including unstructured ones and we still need to be able to analyse these
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/121483bb3ae93ecd7fd90f7d796c3cb0.png)'
  prefs: []
  type: TYPE_IMG
- en: Datalake example. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Just imagine transactionally consistent datalake tables with point-in-time snapshot
    isolation.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I previously wrote about it in one of my stories on Apache Iceberg table format
    [2].
  prefs: []
  type: TYPE_NORMAL
- en: '[](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----e202776fb9a9--------------------------------)
    [## Introduction to Apache Iceberg Tables'
  prefs: []
  type: TYPE_NORMAL
- en: A few Compelling Reasons to Choose Apache Iceberg for Data Lakes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----e202776fb9a9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Simplified data integrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Managed solutions like **Fivetran** and **Stitch** were built to manage third-party
    API integrations with ease. These days many companies choose this approach to
    simplify data interactions with their external data sources. This would be the
    right way to go for data analyst teams that are not familiar with coding.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, why would we build a data connector from scratch if it already exists
    and is being managed in the cloud?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The downside of this approach is it‚Äôs pricing model though.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Very often it is row-based and might become quite expensive on an enterprise
    level of data ingestion, i.e. big data pipelines. This is where open-source alternatives
    come into play. Frameworks like **Airbyte** and **Meltano** might be an easy and
    quick solution to deploy a data source **integration** microservice.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don‚Äôt have time to learn a new ETL framework you can create a simple
    data connector yourself. If you know a bit of Python it would be a trivial task.
    In one of my previous articles I wrote how easy it is to create a microservice
    that pulls data from NASA API [3]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----e202776fb9a9--------------------------------)
    [## Python for Data Engineers'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced ETL techniques for beginners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----e202776fb9a9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Consider this code snippet for `app.py`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It can be deployed in any cloud vendor platform and scheduled to run with the
    required frequency. It‚Äôs always a good practice to use something like **Terraform**
    to deploy our data pipeline applications.
  prefs: []
  type: TYPE_NORMAL
- en: ETL frameworks explosion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can witness a ‚ÄúCambrian explosion‚Äù of various ETL frameworks for data extraction
    and transformation. It‚Äôs not a surprise that many of them are open-source and
    are Python-based.
  prefs: []
  type: TYPE_NORMAL
- en: '**Luigi** [8] is one of them and it helps to create ETL pipelines. It was created
    by Spotify to manage massive data processing workloads. It has a command line
    interface and great visualization features. However, even basic ETL pipelines
    would require a certain level of Python programming skills. From my experience,
    I can tell that it‚Äôs great for strict and straightforward pipelines. I find it
    particularly difficult to implement complex branching logic using Luigi but it
    works great in many scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python ETL (PETL) [9]** is one of the most widely used open-source ETL frameworks
    for straightforward data transformations. It is invaluable working with tables,
    extracting data from external data sources and performing basic ETL on data. In
    many ways, it is similar to **Pandas** but the latter has more analytics capabilities
    under the hood. PETL is great for aggregation and row-level ETL.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bonobo** [10] is another open-source lightweight data processing tool which
    is great for rapid development, automation and parallel execution of batch-processing
    data pipelines. What I like about it is that it makes it really easy to work with
    various data file formats, i.e. SQL, XML, XLS, CSV and JSON. It will be a great
    tool for those with minimal Python knowledge. Among other benefits, I like that
    it works well with semi-complex data schemas. It is ideal for simple ETL and can
    run in Docker containers (it has a Docker extension).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pandas** is an absolute beast in the world of data and there is no need to
    cover it‚Äôs capabilities in this story. It‚Äôs worth mentioning that its data frame
    transformations have been included in one of the basic methods of data loading
    for many modern data warehouses. Consider this data loading sample into the BigQuery
    data warehouse solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Apache Airflow**, for example, is not an ETL tool per se but it helps to
    organize our ETL pipelines into a nice visualization of dependency graphs (DAGs)
    to describe the relationships between tasks. Typical Airflow architecture includes
    a schduler based on metadata, executors, workers and tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can run ml_engine_training_op after we export data into the
    cloud storage (bq_export_op) and make this workflow run daily or weekly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/137358971d4273ca60da3554c13fd586.png)'
  prefs: []
  type: TYPE_IMG
- en: ML model training using Airflow. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Consider this example below.
  prefs: []
  type: TYPE_NORMAL
- en: It creates a simple data pipeline graph to export data into a cloud storage
    bucket and then trains the ML model using MLEngineTrainingOperator.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Bubbles** [11] is another open-source tool for ETL in the Python world. It‚Äôs
    great for rapid development and I like how it works with metadata to describe
    data pipelines. The creators of Bubbles call it an ‚Äúabstract framework‚Äù and say
    that it can be used from many other programming languages, not exclusively from
    Python.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many other tools with more specific applications, i.e. extracting
    data from web pages (PyQuery, BeautifulSoup, etc.) and parallel data processing.
    It can be a topic for another story but I wrote about some of them before, i.e.
    `joblib` library [12]
  prefs: []
  type: TYPE_NORMAL
- en: Data infrastructure as code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Infrastructure as code** (IaC) is a popular and very functional approach
    for **managing data platform resources**. Even for data, it is pretty much a standard
    right now, and it definitely looks great on your CV telling your potential employers
    that you are familiar with DevOps standards. Using tools like Terraform (platform
    agnostic) and CloudFormation we can integrate our development work and deployments
    (operations) with ease.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, we would want to have staging and production data environments for
    our data pipelines. It helps to test our pipelines and facilitate collaboration
    between teams.
  prefs: []
  type: TYPE_NORMAL
- en: Consider this diagram below. It explains how data environments work.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75b7d0ef1c9e75fbc698ea6d609ab4c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Data environments. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Often we might need an extra sandbox for testing purposes or to run data transformation
    unit tests when our ETL services trigger CI/CD workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 'I previously wrote about it here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----e202776fb9a9--------------------------------)
    [## Infrastructure as Code for Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy Data Pipelines like a pro with these templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----e202776fb9a9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Using AWS CloudFormation template files we can describe required resources and
    their dependencies so we can launch and configure them together as a single stack.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you are a **data professional** this approach will definitely help working
    with different data environments and replicate data platform resources faster
    and more consistently without errors.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that many data practitioners are not familiar with IaC and it
    creates a lot of errors during the development process.
  prefs: []
  type: TYPE_NORMAL
- en: Data Mesh and decentralized data management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data space has significantly evolved during the last decade and now we have
    lots of data tools and frameworks. **Data Mesh** defines the state when we have
    different data domains (company departments) with their own teams and shared data
    resources. Each team has their own goals, KPIs, data roles and responsibilities.
  prefs: []
  type: TYPE_NORMAL
- en: For a long period of time, data bureaucracy has been a real pain for many companies.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This data platform type [4] might seem a bit chaotic but it was meant to become
    a successful and efficient choice for companies where decentralization enables
    different teams to access cross-domain datasets and run analytics or ETL tasks
    on their own.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, Snowflake might be your favourite data warehouse solution if you are
    a data analyst and not familiar with Spark. However, often it‚Äôs a trivial problem
    when you might want to read datalake data without data engineering help. In this
    scenario, a bunch of metadata records on datasets could be extremely useful and
    that‚Äôs why Data Mesh is so successful.
  prefs: []
  type: TYPE_NORMAL
- en: It enables users with knowledge about data, its origins and how other teams
    can make the best of those datasets they weren‚Äôt previously aware of.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes datasets and data source connections become very intricate and it
    is always a good practice to have a single-source-of-truth data silo or repository
    with metadata and dataset descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: In one of my previous stories [5] I wrote about the role of SQL as a unified
    querying language for teams and data. Indeed, it analytical, self-descriptive
    and come be even dynamic which makes it a perfect tool for all data users.
  prefs: []
  type: TYPE_NORMAL
- en: '**Often it all turns into a big mes(s/h)**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This fact makes SQL-based templating engines like DBT, Jinja and Dataform very
    popular. **Just imagine you have an SQL-like platform where all datasets and their
    transformations are described and defined thoroughly [6].**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0baeff3ede9d729968e3c27de50bdcaf.png)'
  prefs: []
  type: TYPE_IMG
- en: Dataform‚Äôs dependency graph and metadata. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: It might be a big challenge to understand how data teams relate to data sources
    and schemas. Very often it is all tangled in spaghetti of dataset dependencies
    and ETL transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering plays a critical role in mentoring, improving data literacy
    and empowering the rest of the company with state-of-the-art data processing techniques
    and best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Democratization of Business Intelligence pipelines using AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Improving data accessibility has always been a popular topic in the data space
    but it is interesting to see how the whole data pipeline design process is becoming
    increasingly accessible to teams that weren‚Äôt familiar with data before. Now almost
    every department can utilize built-in AI capabilities to create complex BI transformations
    on data.
  prefs: []
  type: TYPE_NORMAL
- en: All they need is to describe what they want BI-wise in their own words
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For example, BI tools like **Thoughspot** use AI with an intuitive ‚ÄúGoogle-like
    search interface‚Äù [7] to gain insights from data stored in any modern DWH solution
    such as Google Big Query, Redshift, Snowflake or Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: Modern Data Stack includes BI tools that help with data modelling and visualization.
    Many of them already have these built-in AI capabilities to gain data insights
    faster based on user behaviour.
  prefs: []
  type: TYPE_NORMAL
- en: I believe it‚Äôs a fairly easy task to integrate GPT and BI. In the next couple
    of years, we will see many new products using this tech.
  prefs: []
  type: TYPE_NORMAL
- en: GPT can pre-process text data to generate a SQL query that understands your
    intent and answers your question.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, I tried to give a very high-level overview of major data trends
    that affect data engineering role these days. Data Mesh and templated SQL with
    dependency graphs to facilitate data literacy democratized the whole analytics
    process. Advanced data pipelines with intricate ETL techniques and transformations
    can be transparent for everyone in the organisation now. Data pipelines are becoming
    increasingly accessible for other teams and they don‚Äôt need to know programming
    to learn and understand the complexity of ETL. Data Mesh and metadata help to
    solve this problem. From my experience, I can tell that I keep seeing more and
    more people learning SQL to contribute to the transformation layer. Companies
    born during the ‚Äúadvanced data analytics‚Äù age have the luxury of easy access to
    cloud vendor products and their managed services. It definitely helps to acquire
    the required data skills and improve them to gain a competitive advantage.
  prefs: []
  type: TYPE_NORMAL
- en: Recommended read
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [https://medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3](https://medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://towardsdatascience.com/introduction-to-apache-iceberg-tables-a791f1758009](/introduction-to-apache-iceberg-tables-a791f1758009)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd](/python-for-data-engineers-f3d5db59b6dd)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488](https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] [https://medium.com/towards-data-science/easy-way-to-create-live-and-staging-environments-for-your-data-e4f03eb73365](https://medium.com/towards-data-science/easy-way-to-create-live-and-staging-environments-for-your-data-e4f03eb73365)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] [https://docs.thoughtspot.com/cloud/latest/search-sage](https://docs.thoughtspot.com/cloud/latest/search-sage)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] [https://github.com/spotify/luigi](https://github.com/spotify/luigi)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] [https://petl.readthedocs.io/en/stable/](https://petl.readthedocs.io/en/stable/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] [https://www.bonobo-project.org](https://www.bonobo-project.org)'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] [http://bubbles.databrewery.org/](http://bubbles.databrewery.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] [https://medium.com/towards-data-science/how-to-become-a-data-engineer-c0319cb226c2](https://medium.com/towards-data-science/how-to-become-a-data-engineer-c0319cb226c2)'
  prefs: []
  type: TYPE_NORMAL
