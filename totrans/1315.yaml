- en: An Intro to Hugging Face With Implementation of 6 NLP Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/implement-nlp-tasks-using-hugging-face-77dfdcad65fd](https://towardsdatascience.com/implement-nlp-tasks-using-hugging-face-77dfdcad65fd)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An introductory tutorial to use Hugging Face for NLP tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fmnobar?source=post_page-----77dfdcad65fd--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----77dfdcad65fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77dfdcad65fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----77dfdcad65fd--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----77dfdcad65fd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----77dfdcad65fd--------------------------------)
    ·12 min read·Apr 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a227cd780125742369e775c5e6ccf84.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Duy Pham](https://unsplash.com/@miinyuii?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/Cecb0_8Hx-o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: '[Hugging Face](https://huggingface.co/) is an open-source AI community for
    and by machine learning practitioners with a focus on Natural Language Processing
    (NLP), computer vision and audio/speech processing tasks. Whether you already
    work in one of these areas or aspire to enter this realm in the future, you will
    benefit from learning how to use Hugging Face tools and models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post we are going to go over six of the most frequently used NLP tasks
    by leveraging pre-trained models available on Hugging Face, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation (a.k.a. Language Modeling)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Question Answering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sentiment Analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Text Classification
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Text Summarization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Machine Translation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before jumping into the tasks, let’s take a minute to talk about the distinction
    between “Training” and “Inference”, which are two important concepts in machine
    learning, in order to clarify what we will be working on today.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fmnobar/membership?source=post_page-----77dfdcad65fd--------------------------------)
    [## Join Medium with my referral link'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Farzad (and other writers on Medium). Your membership
    fee directly supports Farzad and other…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@fmnobar/membership?source=post_page-----77dfdcad65fd--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Training vs. Inference in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training is the process of feeding a machine learning model with large amounts
    of data. During this process the model “learns” from the provided data (by optimizing
    an objective function) and hence this process is called “Training”. Once we have
    a trained model, we can use it to make predictions in new data that model has
    not seen before. This process is called “Inference”. In short, training is the
    learning process for the model, while inference is the model making predictions
    (i.e. when we actually use the model).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the distinction between training and inference, we can
    more concretely define what we will be working on today. In this post, we will
    be using various pre-trained models for inference. In other words, we would not
    be going through the expensive process of training any new models here. On the
    other hand, we are going to leverage the myriad of existing pre-trained models
    in the Hugging Face Hub and use those for inference (i.e. to make predictions).
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Text Generation (a.k.a. Language Modeling)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I decided to start with this task, given the recent hiked interest about Generative
    AI such as ChatGPT. This task is usually called language modeling and the task
    that the models perform is to predict missing parts of text (this can be a word,
    token or larger strings of text). What has attracted a lot of interest recently
    is that the models can generate text without necessarily having seen such prompts
    before.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how it works in practice!
  prefs: []
  type: TYPE_NORMAL
- en: 1.1\. Text Generation — Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to implement text generation, we will import `pipeline` from `transformers`
    library, use one of the GPT models and take the steps below. I have also added
    comments in the code so that you can more easily follow the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import libraries
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the name of the pre-trained model to be used for this specific task
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the sentence, which will be completed by the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an instance of `pipeline` as `generator`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the text generation and store the results as `output`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code block below follows these steps.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5de8984992318dd87aacdbbaf11cd913.png)'
  prefs: []
  type: TYPE_IMG
- en: Text Generation Results
  prefs: []
  type: TYPE_NORMAL
- en: We can see in the results that the model took our provided input text and generated
    additional text, given the data it has been trained on and the sentence that we
    provided. Note that I limited the length of the output using the `max_new_tokens`
    to 30 tokens to prevent a lengthy response. The generated text sounds reasonable
    and relevant to context.
  prefs: []
  type: TYPE_NORMAL
- en: But what about a case where we would like to ask a question from the model?
    Can the model answer a question, instead of just completing an incomplete sentence?
    Let’s explora that next.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Question Answering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Question answering, as the name suggests, is a task where the model answers
    a question provided by the user. There are generally two types of question answering
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extractive (i.e. context-dependent):** Where the user describes a situation
    to the model in the question/prompt and ask the model to generate a response,
    given that provided information. In this scenario, the model picks the relevant
    parts of the information from the prompt and returns the results'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Abstractive (i.e. context-independent):** Where the user asks a question
    from the model, without providing any context'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s look at how question answering can be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Question Answering — Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implementation process is similar to the language modeling task. We will use
    two different models to be able to compare the results.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the `distilbert-base-cased-distilled-squad` ([link](https://huggingface.co/distilbert-base-cased-distilled-squad)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df755d3f89fe5dac99a188cf302b4bcb.png)'
  prefs: []
  type: TYPE_IMG
- en: Question Answering Results (distilbert-base-cased-distilled-squad `Model)`
  prefs: []
  type: TYPE_NORMAL
- en: We can see the summary and the model was able to determine which part of the
    context was relevant to answer the question.
  prefs: []
  type: TYPE_NORMAL
- en: let’s implement the same problem using a different model, named `deepset/roberta-base-squad2`
    ([link](https://huggingface.co/deepset/roberta-base-squad2)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e158182b62c0f846aae0b86dd9e56a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Question Answering Results (`deepset/roberta-base-squad2 Model`)
  prefs: []
  type: TYPE_NORMAL
- en: As we see in the above example, the second model was also able to identify that
    NLP stands for natural language processing, given the context that we provided.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue our journey in the NLP tasks by looking at sentiment analysis
    next.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Sentiment Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sentiment analysis is the process of categorizing the sentiment of a text into
    positive, negative or neutral. There is a wide range of applications for sentiment
    analysis in different industries, such as monitoring customers’ sentiment from
    product reviews or even in politics, such as gauging public interest in a given
    topic during an election year. Focus of this post is to use Hugging Face for various
    tasks so we will not dive deeper into each topic but if you are interested in
    learning more about sentiment analysis in depth, you can refer to this post:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/sentiment-analysis-intro-and-implementation-ddf648f79327?source=post_page-----77dfdcad65fd--------------------------------)
    [## Sentiment Analysis — Intro and Implementation'
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment Analysis using NLTK, scikit-learn and TextBlob
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/sentiment-analysis-intro-and-implementation-ddf648f79327?source=post_page-----77dfdcad65fd--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Sentiment Analysis — Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to implement sentiment analysis, we will again rely on`pipeline` from
    `transformers` library and take the steps below. I have also added comments in
    the code so that you can more easily follow the steps.
  prefs: []
  type: TYPE_NORMAL
- en: Import libraries
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the name of the pre-trained model to be used for this specific task
    (i.e. sentiment analysis)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the task (i.e. sentiment analysis)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the sentence, which will be sentiment analyzed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an instance of `pipeline` as `analyzer`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the sentiment analysis and save the results as `output`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e5bf1b80072f1bb437635989b5c8451.png)'
  prefs: []
  type: TYPE_IMG
- en: Sentiment Analysis Results
  prefs: []
  type: TYPE_NORMAL
- en: The results indicate that the sentiment of the sentence is a positive one with
    a score of ~85%. The sentence sounds pretty positive to me so I like the results
    so far. Feel free to replicate the process for other sentences and test it out!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to a different type of text classification.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Text Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentiment analysis, which we just covered, can be considered a special case
    of text classification, where the categories (or classes) are only positive, negative
    or neutral. Text classification is more generic in that it can classify (or categorize)
    the incoming text (e.g. sentence, paragraph or document) into pre-defined classes.
    Let’s see what this means in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Text Classification — Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the same `pipeline` package and take steps very similar to what
    we did for sentiment analysis, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd38a8780d752b47257237e4dea5e5c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Text Classification Results
  prefs: []
  type: TYPE_NORMAL
- en: Results are quite interesting! The scores correspond to each label, sorted from
    the largest to the smallest for ease of reading. For example, the results indicate
    that our sentence is labeled as “education” with a score of ~40%, followed by
    “business” by ~22%, while labels for “music”, “sports” and “politics” have very
    low scores, which makes sense to me overall.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to our next task, which is summarization.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Text Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text summarization is the task of automatically summarizing textual input, while
    still conveying the main points/gist of the incoming text. One example of the
    business intuition behind the need for such summarization models is the situations
    where humans read incoming text communications (e.g. customer emails) and using
    a summarization model can save human time. For example, these human representatives
    can read the summary of the customer emails instead of the entire emails, resulting
    in improved operational efficiency by saving human time and cost.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how we can implement text summarization.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Text Summarization — Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to other tasks, we will use the `pipeline` for a summarization task.
    For this specific task, we will first use a text-to-text pre-rained model from
    Google named [T5](https://github.com/google-research/text-to-text-transfer-transformer)
    to summarize the description that we just read about “Text Summarization” in the
    section above. We will then repeat the same exercise using a different model from
    Google to see how the results vary. Let’s see how we can implement this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/732cdbd9d24b13790eec08fb126dbb1c.png)'
  prefs: []
  type: TYPE_IMG
- en: Text Summarization Results (T5 Model)
  prefs: []
  type: TYPE_NORMAL
- en: As you see in the results, the T5 model took the input text, which was rather
    long, and returned a brief summary of what it considered the main points of the
    input text. I like the summary since it explains what text summarization is and
    what benefits it can provide — that’s a good summary!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try another model from Google named [Pegasus](https://huggingface.co/google/pegasus-cnn_dailymail)
    to see if and how the results change, when we use a different model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8aebfbfd0c88e203b79cc25583fce843.png)'
  prefs: []
  type: TYPE_IMG
- en: Text Summarization Results (Pegasus Model)
  prefs: []
  type: TYPE_NORMAL
- en: As expected, resulting outputs of the two models defer, since they are each
    trained using specific data and training objectives but both somehow accomplished
    the task. I personally prefer the outcome of the T5 model, since it more succinctly
    states the point of the input text.
  prefs: []
  type: TYPE_NORMAL
- en: Last, but not the least task that we will be looking at is machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Machine Translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine translation is the task of generating the translation of an input text
    in a target language. This is similar to what Google Translate or other similar
    translation engines provide. One of the benefits of using Hugging Face for machine
    translation is that we get to choose what models to use for our translation, which
    can potentially provide a more accurate translation for the specific language
    that we are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the implementation of machine translation in Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Machine Translation — Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to generate translations, we will use two of the most common pre-trained
    models to translate the same sentence from English to French. Implementation of
    each slightly varies but the overall process is the same as other tasks that we
    have implemented so far.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1\. T5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[T5](https://huggingface.co/docs/transformers/model_doc/t5) is an encoder-decoder
    pre-trained model developed by Google, which works well on multiple tasks, including
    machine translation. In order to prompt T5 to perform a tasks such as translation
    from language X to language Y, we will add a string (called a “prefix”) to the
    sentence to the input of each task follows: `"translate X to Y: sentence_to_be_translated"`.'
  prefs: []
  type: TYPE_NORMAL
- en: This is actually easier in practice to understand so let’s just translate a
    sentence from English to French using T5 and see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff09beef413b5d75dcf118b24cfc6259.png)'
  prefs: []
  type: TYPE_IMG
- en: Machine Translation Results (T5 Model)
  prefs: []
  type: TYPE_NORMAL
- en: I looked up this translation on Google Translate and this looks like a good
    translation! I was concerned about this verification methodology, since T5 is
    also developed by Google. I do not know what exact model Google Translate uses
    for translation but we will see how much the results vary when we run the same
    translation task using mBART in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2\. mBART
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[mBART](https://huggingface.co/docs/transformers/model_doc/mbart) is a multilingual
    encoder-decoder pre-trained model developed by Meta, which is primarily intended
    for machine translation tasks. mBART, unlike T5, does not require the prefix in
    the prompt but we need to identify the original and target languges to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement the same task in mBART.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/01ceddbf75c2b303d82c706352e78920.png)'
  prefs: []
  type: TYPE_IMG
- en: Machine Translation Results (mBART Model)
  prefs: []
  type: TYPE_NORMAL
- en: Results seem very similar to what T5 generated, with the exception of “poste”
    having been replaced by “post”. Regardless of the difference between the two outcomes,
    the main point of the exercise was to demonstrate how these pre-trained models
    can generate machine translation, which we have accomplished using both models.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post we introduced Hugging Face, an open-source AI community used by
    and for many machine learning practitioners in NLP, computer vision and audio/speech
    processing tasks. We then walked through the implementation of such pre-trained
    models within the Hugging Face platform to accomplish downstream NLP tasks, such
    as text generation, question answering, sentiment analysis, text classification,
    text summarization and machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for Reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you found this post helpful, please [follow me on Medium](https://medium.com/@fmnobar)
    and subscribe to receive my latest posts!
  prefs: []
  type: TYPE_NORMAL
- en: '*(All images, unless otherwise noted, are by the author.)*'
  prefs: []
  type: TYPE_NORMAL
