- en: Unit testing PySpark code using Pytest
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Pytest对PySpark代码进行单元测试
- en: 原文：[https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415](https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415](https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415)
- en: '[](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)[![Julian
    West](../Images/c7740cd407ed17e84af2c49a37dbbe92.png)](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)
    [Julian West](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)[![Julian
    West](../Images/c7740cd407ed17e84af2c49a37dbbe92.png)](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)
    [Julian West](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)
    ·10 min read·Jan 16, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)
    ·10分钟阅读·2023年1月16日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/314b8ebf1cb2596498d48e1f09861a50.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/314b8ebf1cb2596498d48e1f09861a50.png)'
- en: Photo by [Jez Timms](https://unsplash.com/@jeztimms?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/_Ch_onWf38o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于[Jez Timms](https://unsplash.com/@jeztimms?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在[Unsplash](https://unsplash.com/photos/_Ch_onWf38o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: '**I am a big fan of unit-testing.**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**我非常喜欢单元测试。**'
- en: '**Reading two books —** [**The Pragmatic Programmer**](https://engineeringfordatascience.com/book-notes/pragmatic-programmer/)
    **and** [**Refactoring**](https://engineeringfordatascience.com/book-notes/refactoring/)
    **— completely changed the way I viewed unit-testing.**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**阅读了两本书——** [**《务实程序员》**](https://engineeringfordatascience.com/book-notes/pragmatic-programmer/)
    **和** [**《重构》**](https://engineeringfordatascience.com/book-notes/refactoring/)
    **——彻底改变了我对单元测试的看法。**'
- en: “Testing is not about finding bugs.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “测试不是为了找出漏洞。
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We believe that the major benefits of testing happen when you think about and
    write the tests, not when you run them.”
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们相信，测试的主要好处发生在你思考和编写测试的时候，而不是运行它们的时候。”
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*— The Pragmatic Programmer, David Thomas and Andrew Hunt*'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*— 《务实程序员》，大卫·托马斯和安德鲁·亨特*'
- en: Instead of seeing testing as a chore to complete after I have finished my data
    pipelines, I see it as a powerful tool to improve the design of my code, reduce
    coupling, iterate more quickly and build trust with others in my work.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我不再把测试视为在完成数据管道后需要完成的繁琐工作，而是将其视为一个强大的工具，用于改善代码设计，减少耦合，更快速地迭代，并与工作中的其他人建立信任。
- en: However, writing good tests for data applications can be difficult.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为数据应用程序编写良好的测试可能很困难。
- en: Unlike traditional software applications with relatively well defined inputs,
    data applications in production depend on large and constantly changing input
    data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的软件应用程序具有相对明确的输入不同，生产环境中的数据应用程序依赖于大量且不断变化的输入数据。
- en: It can be very challenging to accurately represent this data in a test environment
    in enough detail to cover all edge cases. Some people argue there is little point
    in unit-testing data pipelines, and focus on data validation techniques instead.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试环境中准确地表示这些数据，以足够的细节覆盖所有边界情况，可能是非常具有挑战性的。有些人认为对数据管道进行单元测试没有什么意义，而是更关注数据验证技术。
- en: I strongly believe in implementing both unit-testing and data validation in
    your data pipelines. **Unit-testing isn’t just about finding bugs, it is about
    creating better designed code and building trust with colleagues and end users.**
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我坚信在数据管道中同时实现单元测试和数据验证。**单元测试不仅仅是为了找出漏洞，更是为了创建设计更好的代码，并与同事和最终用户建立信任。**
- en: If you can get in the habit of writing tests, you will write better designed
    code, save time in the long run and reduce the pain of pipelines failing or giving
    incorrect results in production.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能养成编写测试的习惯，你将编写出设计更好的代码，节省长远的时间，并减少管道失败或在生产中给出不正确结果的痛苦。
- en: Challenges with Unit testing PySpark code
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单元测试 PySpark 代码的挑战
- en: 'A good unit-test should have the following characteristics:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的单元测试应具备以下特征：
- en: '**Focused**. Each test should test a single behaviour/functionality.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专注**。每个测试应测试一个单一的行为/功能。'
- en: '**Fast**. Allowing you to iterate and gain feedback quickly'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速**。允许你快速迭代并获得反馈。'
- en: '**Isolated**. Each test should be responsible for testing a specific functionality
    and not depend on external factors in order to run successfully'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**孤立**。每个测试应该负责测试特定的功能，并且不依赖于外部因素以便成功运行。'
- en: '**Concise**. Creating a test shouldn’t include lots of boiler-plate code to
    mock/create complex objects in order for the test to run'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简洁**。创建测试不应包含大量的样板代码来模拟/创建复杂对象以便测试运行。'
- en: '**When it comes to writing unit-tests specifically for PySpark pipelines, writing
    focussed, fast, isolated and concise tests can be a challenge.**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**当涉及到专门为 PySpark 管道编写单元测试时，编写专注、快速、孤立和简洁的测试可能是一个挑战。**'
- en: Here are some of the hurdles you might come across…
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到一些障碍……
- en: Writing testable code in the first place
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 首先编写可测试的代码
- en: PySpark pipelines tend to be written as one giant function responsible for multiple
    transformations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 管道往往被编写成一个负责多个转换的巨型函数。
- en: 'For example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It is logical to think about transformations in this way, and in many ways is
    easier to reason about and read.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度考虑转换是合理的，并且在很多方面更容易理解和阅读。
- en: But, when you start trying to write a test for this function you quickly realise
    it is very difficult to write a test to cover all functionality.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当你开始尝试为这个函数编写测试时，你会很快意识到，编写一个覆盖所有功能的测试是非常困难的。
- en: This is because the function is highly coupled and there many different paths
    that the function can take.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为函数高度耦合，并且函数可能有许多不同的路径。
- en: Even if you did write a test that verified the input data and output data were
    as expected. If the test failed for any reason it would be very difficult to understand
    which part of the very long function was at fault.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你编写了一个测试来验证输入数据和输出数据是否符合预期。如果测试因任何原因失败，将很难理解长函数的哪个部分出现了问题。
- en: Instead, you should break your transformations into blocks of reusable functions
    which are responsible for a single task. You can then write a unit-test for each
    individual function (task) which. When each of these unit-tests pass, you can
    be more confident in the output of the final pipeline when you compose all the
    functions together.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你应该将你的转换拆分为负责单一任务的可重用函数块。然后你可以为每个单独的函数（任务）编写单元测试。当这些单元测试都通过时，你可以更有信心地将所有函数组合在一起，得到最终管道的输出。
- en: Writing tests is a good practice and forces you to think about design principles.
    If it is difficult to test your code then you probably need to rethink the design
    of your code.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 编写测试是一种良好的实践，并迫使你考虑设计原则。如果测试你的代码很困难，那么你可能需要重新考虑你的代码设计。
- en: Speed
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速度
- en: Spark is optimised to work on very large data and the compute is optimised to
    be distributed across many machines.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 被优化用于处理非常大的数据，计算也被优化以分布在多台机器上。
- en: This works great for a large cluster of machines but actually really hurts performance
    on a single machine that you might use for your unit-testing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这在大型集群中表现得很好，但实际上在你可能用于单元测试的单台机器上会严重影响性能。
- en: When you run a PySpark pipeline, spark will evaluate the entire pipeline and
    calculate an optimised ‘plan’ to perform the computation across a distributed
    cluster.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行 PySpark 管道时，spark 会评估整个管道并计算一个优化的‘计划’以在分布式集群上执行计算。
- en: The planning comes with significant overhead. This makes sense when you are
    processing terabytes of data on a distributed cluster of machines. But when working
    on a single machine on a small dataset it can be surprisingly slow. Especially
    compared with what you might have experienced with Pandas.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 计划带来了显著的开销。当你在分布式集群上处理数TB的数据时，这是合理的。但在单台机器上处理小数据集时，速度可能会出奇地慢。尤其是与 Pandas 的体验相比。
- en: Without optimising your SparkSession configuration parameters your unit-tests
    will take an agonizingly long time to run.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不优化你的 SparkSession 配置参数，你的单元测试将会运行得非常缓慢。
- en: Dependency on a Spark Session
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 依赖 Spark Session
- en: To run PySpark code in your unit-test, you need a SparkSession.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要在你的单元测试中运行 PySpark 代码，你需要一个 SparkSession。
- en: As stated above, ideally each test should be isolated from others and not require
    complex external objects. Unfortunately, there is no escaping the requirement
    to initiate a spark session for your unit-tests.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，理想情况下，每个测试应与其他测试隔离，并且不需要复杂的外部对象。不幸的是，不能避免为单元测试启动Spark会话的要求。
- en: Creating a spark session is the first hurdle to overcome when writing a unit-test
    for your PySpark pipeline.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Spark会话是编写PySpark管道单元测试时要克服的第一个障碍。
- en: How should you create a SparkSession for your tests?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该如何为测试创建SparkSession？
- en: Initiating a new spark session for each test would dramatically increase the
    time to run the tests and introduce a ton of boiler-plate code to your tests.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个测试初始化一个新的Spark会话会显著增加运行测试的时间，并给测试引入大量样板代码。
- en: Efficiently, creating and sharing a SparkSession across your tests is vital
    to keep the performance of your tests at an acceptable level.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 高效地创建和共享SparkSession对于保持测试性能在可接受的水平至关重要。
- en: Data
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: Your tests will require input data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你的测试将需要输入数据。
- en: There are two main problems with creating example data for big data pipelines.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 创建大数据管道的示例数据存在两个主要问题。
- en: The first is size. Obviously, you cannot run your tests against the full dataset
    that will be used in production. You have to use a much smaller subset.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是大小。显然，你不能在生产中使用的完整数据集上运行测试。你必须使用一个更小的子集。
- en: But, by using a small dataset, you run into the second problem which is providing
    enough test data to cover all the edge cases you want to handle.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，通过使用一个小数据集，你会遇到第二个问题，即提供足够的测试数据来覆盖你想要处理的所有边界情况。
- en: It is *really* hard to mock realistic data for testing. There is not much you
    can do about this, however, you can use smaller, targeted datasets for your tests.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*真正*很难为测试模拟现实数据。虽然对此没有太多办法，但你可以使用更小、目标明确的数据集进行测试。'
- en: Steps to unit-test your PySpark code with Pytest
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Pytest对PySpark代码进行单元测试的步骤
- en: Let’s work through an example of writing unit-tests for a PySpark pipline using
    [PyTest](https://docs.pytest.org/en/7.2.x/) .
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个使用[PyTest](https://docs.pytest.org/en/7.2.x/)为PySpark管道编写单元测试的示例来进行实践。
- en: '*💻 The full code is available in this* [*GitHub repository*](https://github.com/julian-west/e4ds-snippets/tree/master/pyspark/pyspark_unit_testing)'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*💻 完整代码可在此* [*GitHub存储库*](https://github.com/julian-west/e4ds-snippets/tree/master/pyspark/pyspark_unit_testing)
    *中获取*'
- en: Example code
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例代码
- en: Here is an example PySpark pipeline to process some bank transactions. In this
    scenario we want to take the raw transactions and classify them as debit account
    or credit account transactions by joining them to some reference data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个处理银行交易的PySpark管道示例。在这个场景中，我们希望将原始交易分类为借记账户或信用账户交易，通过将其与一些参考数据连接起来。
- en: Each transaction record comes with an account ID. We will use this account ID
    to join to account information table which has information on whether this account
    ID is from a debit or credit account.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每个交易记录都附带一个账户ID。我们将使用这个账户ID连接到包含该账户ID是借记账户还是信用账户的信息的账户信息表。
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There are few issues with this example pipeline:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例管道存在几个问题：
- en: Difficult to read. Lots of complex logic in one place. For example, regex replacements,
    joining on substrings etc.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以阅读。一个地方包含了大量复杂的逻辑。例如，正则表达式替换、按子字符串连接等。
- en: Difficult to test. Single function responsible for multiple actions
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以测试。单个函数负责多个操作
- en: Difficult to reuse. The debit/credit classification is business logic which
    cannot easily be reused across the project
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以重用。借记/信用分类是业务逻辑，无法轻松在项目中重用
- en: 'Step 1: Refactor into smaller logical units'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：将代码重构为更小的逻辑单元
- en: Let’s first refactor the code into individual functions, then compose the functions
    together for the main `classify_debit_credit_transactions` function.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先将代码重构为单独的函数，然后将这些函数组合成主`classify_debit_credit_transactions`函数。
- en: We can then write an test for each individual function to ensure it is behaving
    as expected.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以为每个单独的函数编写测试，以确保其按预期行为。
- en: While this increases the overall number of lines of code, it is easier to test
    and we can now reuse the functions across other parts of the project.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这增加了代码的总体行数，但测试起来更容易，我们现在可以在项目的其他部分重用这些函数。
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 3\. Creating a resuable SparkSession using Fixtures
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 使用Fixtures创建可重用的SparkSession
- en: Before writing our unit-tests, we need to create a SparkSession which we can
    reuse across all our tests.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写单元测试之前，我们需要创建一个可以在所有测试中重用的SparkSession。
- en: To do this, we create a [PyTest fixture](https://docs.pytest.org/en/6.2.x/fixture.html)
    in a `conftest.py` file.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们在 `conftest.py` 文件中创建一个 [PyTest fixture](https://docs.pytest.org/en/6.2.x/fixture.html)。
- en: Pytest fixtures are objects which are created once and then reused across multiple
    tests. This is particularly useful for complex objects like the SparkSession which
    have a significant overhead to create.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Pytest fixtures 是一次创建然后在多个测试中重复使用的对象。这对于像 SparkSession 这样的复杂对象特别有用，因为创建它们的开销很大。
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It is important to set a number of configuration parameters in order to optimise
    the SparkSession for processing small data on a single machine for testing:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一些配置参数以优化 SparkSession，用于在单台机器上处理小数据进行测试是很重要的：
- en: '`master = local[1]` – specifies that spark is running on a local machine with
    one thread'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`master = local[1]` – 指定 spark 在一台本地机器上运行，并且使用一个线程'
- en: '`spark.executor.cores = 1` – set number of cores to one'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.executor.cores = 1` – 设置核心数为一个'
- en: '`spark.executor.instances = 1` - set executors to one'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.executor.instances = 1` - 设置执行器为一个'
- en: '`spark.sql.shuffle.partitions = 1` - set the maximum number of partitions to
    1'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.sql.shuffle.partitions = 1` - 设置最大分区数为 1'
- en: '`spark.driver.bindAddress = 127.0.0.1` – (optional) Explicitly specify the
    driver bind address. Useful if your machine has also has a live connection to
    a remote cluster'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.driver.bindAddress = 127.0.0.1` – （可选）显式指定驱动程序绑定地址。如果你的机器也与远程集群有活动连接，这将很有用。'
- en: These config parameters essentially tell spark that you are processing on a
    single machine and spark should not try to distribute the computation. This will
    save a significant amount of time in both the planning of the pipeline execution
    and the computation itself.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些配置参数本质上告诉 spark 你在单台机器上处理数据，并且 spark 不应该尝试分发计算。这将大大节省管道执行和计算本身的时间。
- en: Note, it is recommended to `yield` the spark session instead of using `return`.
    Read the [PyTest documentation](https://docs.pytest.org/en/7.1.x/how-to/fixtures.html#yield-fixtures-recommended)
    for more information. Using `yield` also allows you to perform any clean up actions
    after your tests have run (e.g. deleting any local temp directories, databases
    or tables etc.).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，建议使用 `yield` 而不是 `return` 来返回 spark session。有关更多信息，请阅读 [PyTest 文档](https://docs.pytest.org/en/7.1.x/how-to/fixtures.html#yield-fixtures-recommended)。使用
    `yield` 还允许你在测试运行后执行任何清理操作（例如，删除本地临时目录、数据库或表等）。
- en: 4\. Creating unit-tests for the code
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 为代码创建单元测试
- en: Now let’s write some tests for our code.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们为我们的代码编写一些测试。
- en: 'I find it most efficient to organise my PySpark unit tests with the following
    structure:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现最有效的方式是用以下结构组织我的 PySpark 单元测试：
- en: Create the input dataframe
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建输入 DataFrame
- en: Create the output dataframe using the function we want to test
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我们要测试的函数创建输出 DataFrame
- en: Specify the expected output values
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定预期的输出值
- en: Compare the results
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较结果
- en: I also try to ensure the test covers positive test cases and at least one negative
    test case.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我还尽量确保测试覆盖正面测试用例和至少一个负面测试用例。
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We now have unit-test for each component in the PySpark pipeline.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在为 PySpark 管道中的每个组件都有了单元测试。
- en: As each test reuses the same SparkSession the overhead of running multiple tests
    is significantly reduced.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个测试都重用相同的 SparkSession，运行多个测试的开销大大减少。
- en: Further tips for unit testing PySpark code
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark 代码单元测试的进一步提示
- en: Create test dataframes with the minimum required information
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建包含最少所需信息的测试 DataFrame
- en: When creating dataframes with test data, only create columns relevant to the
    transformation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建测试数据的 DataFrame 时，只创建与转换相关的列。
- en: You only really need to create data with columns required for the function.
    You don’t need all the other columns which might be present in the production
    data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要创建函数所需的列的数据。你不需要所有可能存在于生产数据中的其他列。
- en: This helps write concise functions and is more readable as it is clear which
    columns are required and impacted by the function. If you find you need a big
    dataframe with many columns in order to carry out a transformation you are probably
    trying to do too much at once.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于编写简洁的函数，并且更具可读性，因为可以清楚地知道哪些列是函数所需的以及受函数影响的。如果你发现需要一个包含许多列的大 DataFrame 来执行转换，你可能一次尝试做得太多了。
- en: This is just a guideline, your own usecase might require more complicated test
    data, but if possible keep it small, concise and localised to the test.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个指导方针，你自己的用例可能需要更复杂的测试数据，但如果可能，保持数据小巧、简洁，并局限于测试范围。
- en: Remember to call an ‘action’ in order to trigger the PySpark computation
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记得调用一个“操作”以触发 PySpark 计算
- en: PySpark uses lazy evaluation. You need to call an ‘action’ (e.g. `collect`,
    `count` etc.) during your test in order to compute a result that you can compare
    to the expected output.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 使用惰性计算。你需要在测试期间调用一个‘action’（例如 `collect`、`count` 等），以计算一个可以与预期输出进行比较的结果。
- en: Don’t run all PySpark tests if you don’t need to
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如果不需要的话，不要运行所有的 PySpark 测试
- en: PySpark tests generally take longer than normal unit tests to run as there is
    overhead to calculate a computation plan and then execute it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 测试通常比正常单元测试运行时间更长，因为有计算计划和执行的开销。
- en: During development, make use of some of Pytest’s features such as the `-k` flag
    to run single tests or just run tests in a single file. Then only run the full
    test suite before committing your code.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中，利用 Pytest 的一些功能，例如 `-k` 标志来运行单个测试或仅运行单个文件中的测试。然后，仅在提交代码之前运行完整的测试套件。
- en: Keep the unit-tests isolated
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保持单元测试的隔离性
- en: Be careful not to modify your spark session during a test (e.g. creating a table,
    but not deleting it afterwards).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意不要在测试期间修改你的 spark 会话（例如，创建一个表，但之后不删除它）。
- en: The table will be persisted across all tests which may interfere with their
    expected behaviour.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表格将在所有测试中持续存在，这可能会干扰预期行为。
- en: Try and keep the creation of data close to where it is used.
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尽量将数据的创建保持在使用它的地方附近。
- en: You could use Pytest fixtures to share dataframes or even load test data from
    csv files etc. across multiple tests.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 Pytest fixtures 在多个测试之间共享数据框，甚至从 CSV 文件等中加载测试数据。
- en: However, in my experience, it is easier and more readable to create data as
    required for each individual test.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，根据我的经验，为每个单独的测试创建所需的数据更容易且更具可读性。
- en: Test positive and negative outcomes
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试正面和负面结果
- en: For example, when testing a joining condition you should include data which
    should not satisfy the join condition. This helps ensure you are excluding the
    right data as well as including the right data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在测试连接条件时，你应该包括一些不满足连接条件的数据。这有助于确保你既排除了错误的数据，又包含了正确的数据。
- en: This blog was originally published on [engineeringfordatascience.com](https://engineeringfordatascience.com/posts/pyspark_unit_testing_with_pytest/)
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本博客最初发布在 [engineeringfordatascience.com](https://engineeringfordatascience.com/posts/pyspark_unit_testing_with_pytest/)
- en: 'If you use Pytest for your unit testing, check out my other article with some
    useful tips for using Pytest:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 Pytest 进行单元测试，可以查看我另一篇包含 Pytest 使用技巧的文章：
- en: '[](/13-tips-for-using-pytest-5341e3366d2d?source=post_page-----b5ab2fd54415--------------------------------)
    [## 13 Tips for using PyTest'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[13 Tips for using PyTest](https://towardsdatascience.com/13-tips-for-using-pytest-5341e3366d2d?source=post_page-----b5ab2fd54415--------------------------------)
    [## 13 Tips for using PyTest'
- en: Unit-testing is a really important skill for software development. There are
    some great Python libraries to help us…
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单元测试是软件开发中非常重要的技能。有一些很棒的 Python 库可以帮助我们……
- en: towardsdatascience.com](/13-tips-for-using-pytest-5341e3366d2d?source=post_page-----b5ab2fd54415--------------------------------)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[13 Tips for using PyTest](https://towardsdatascience.com/13-tips-for-using-pytest-5341e3366d2d?source=post_page-----b5ab2fd54415--------------------------------)'
