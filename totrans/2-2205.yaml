- en: Unit testing PySpark code using Pytest
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Pytestå¯¹PySparkä»£ç è¿›è¡Œå•å…ƒæµ‹è¯•
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415](https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415](https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415)
- en: '[](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)[![Julian
    West](../Images/c7740cd407ed17e84af2c49a37dbbe92.png)](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)
    [Julian West](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)[![Julian
    West](../Images/c7740cd407ed17e84af2c49a37dbbe92.png)](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)
    [Julian West](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)
    Â·10 min readÂ·Jan 16, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)
    Â·10åˆ†é’Ÿé˜…è¯»Â·2023å¹´1æœˆ16æ—¥
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/314b8ebf1cb2596498d48e1f09861a50.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/314b8ebf1cb2596498d48e1f09861a50.png)'
- en: Photo by [Jez Timms](https://unsplash.com/@jeztimms?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/_Ch_onWf38o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºäº[Jez Timms](https://unsplash.com/@jeztimms?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)åœ¨[Unsplash](https://unsplash.com/photos/_Ch_onWf38o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: '**I am a big fan of unit-testing.**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**æˆ‘éå¸¸å–œæ¬¢å•å…ƒæµ‹è¯•ã€‚**'
- en: '**Reading two books â€”** [**The Pragmatic Programmer**](https://engineeringfordatascience.com/book-notes/pragmatic-programmer/)
    **and** [**Refactoring**](https://engineeringfordatascience.com/book-notes/refactoring/)
    **â€” completely changed the way I viewed unit-testing.**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**é˜…è¯»äº†ä¸¤æœ¬ä¹¦â€”â€”** [**ã€ŠåŠ¡å®ç¨‹åºå‘˜ã€‹**](https://engineeringfordatascience.com/book-notes/pragmatic-programmer/)
    **å’Œ** [**ã€Šé‡æ„ã€‹**](https://engineeringfordatascience.com/book-notes/refactoring/)
    **â€”â€”å½»åº•æ”¹å˜äº†æˆ‘å¯¹å•å…ƒæµ‹è¯•çš„çœ‹æ³•ã€‚**'
- en: â€œTesting is not about finding bugs.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œæµ‹è¯•ä¸æ˜¯ä¸ºäº†æ‰¾å‡ºæ¼æ´ã€‚
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We believe that the major benefits of testing happen when you think about and
    write the tests, not when you run them.â€
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç›¸ä¿¡ï¼Œæµ‹è¯•çš„ä¸»è¦å¥½å¤„å‘ç”Ÿåœ¨ä½ æ€è€ƒå’Œç¼–å†™æµ‹è¯•çš„æ—¶å€™ï¼Œè€Œä¸æ˜¯è¿è¡Œå®ƒä»¬çš„æ—¶å€™ã€‚â€
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*â€” The Pragmatic Programmer, David Thomas and Andrew Hunt*'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*â€” ã€ŠåŠ¡å®ç¨‹åºå‘˜ã€‹ï¼Œå¤§å«Â·æ‰˜é©¬æ–¯å’Œå®‰å¾·é²Â·äº¨ç‰¹*'
- en: Instead of seeing testing as a chore to complete after I have finished my data
    pipelines, I see it as a powerful tool to improve the design of my code, reduce
    coupling, iterate more quickly and build trust with others in my work.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸å†æŠŠæµ‹è¯•è§†ä¸ºåœ¨å®Œæˆæ•°æ®ç®¡é“åéœ€è¦å®Œæˆçš„ç¹çå·¥ä½œï¼Œè€Œæ˜¯å°†å…¶è§†ä¸ºä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œç”¨äºæ”¹å–„ä»£ç è®¾è®¡ï¼Œå‡å°‘è€¦åˆï¼Œæ›´å¿«é€Ÿåœ°è¿­ä»£ï¼Œå¹¶ä¸å·¥ä½œä¸­çš„å…¶ä»–äººå»ºç«‹ä¿¡ä»»ã€‚
- en: However, writing good tests for data applications can be difficult.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä¸ºæ•°æ®åº”ç”¨ç¨‹åºç¼–å†™è‰¯å¥½çš„æµ‹è¯•å¯èƒ½å¾ˆå›°éš¾ã€‚
- en: Unlike traditional software applications with relatively well defined inputs,
    data applications in production depend on large and constantly changing input
    data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¼ ç»Ÿçš„è½¯ä»¶åº”ç”¨ç¨‹åºå…·æœ‰ç›¸å¯¹æ˜ç¡®çš„è¾“å…¥ä¸åŒï¼Œç”Ÿäº§ç¯å¢ƒä¸­çš„æ•°æ®åº”ç”¨ç¨‹åºä¾èµ–äºå¤§é‡ä¸”ä¸æ–­å˜åŒ–çš„è¾“å…¥æ•°æ®ã€‚
- en: It can be very challenging to accurately represent this data in a test environment
    in enough detail to cover all edge cases. Some people argue there is little point
    in unit-testing data pipelines, and focus on data validation techniques instead.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æµ‹è¯•ç¯å¢ƒä¸­å‡†ç¡®åœ°è¡¨ç¤ºè¿™äº›æ•°æ®ï¼Œä»¥è¶³å¤Ÿçš„ç»†èŠ‚è¦†ç›–æ‰€æœ‰è¾¹ç•Œæƒ…å†µï¼Œå¯èƒ½æ˜¯éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚æœ‰äº›äººè®¤ä¸ºå¯¹æ•°æ®ç®¡é“è¿›è¡Œå•å…ƒæµ‹è¯•æ²¡æœ‰ä»€ä¹ˆæ„ä¹‰ï¼Œè€Œæ˜¯æ›´å…³æ³¨æ•°æ®éªŒè¯æŠ€æœ¯ã€‚
- en: I strongly believe in implementing both unit-testing and data validation in
    your data pipelines. **Unit-testing isnâ€™t just about finding bugs, it is about
    creating better designed code and building trust with colleagues and end users.**
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åšä¿¡åœ¨æ•°æ®ç®¡é“ä¸­åŒæ—¶å®ç°å•å…ƒæµ‹è¯•å’Œæ•°æ®éªŒè¯ã€‚**å•å…ƒæµ‹è¯•ä¸ä»…ä»…æ˜¯ä¸ºäº†æ‰¾å‡ºæ¼æ´ï¼Œæ›´æ˜¯ä¸ºäº†åˆ›å»ºè®¾è®¡æ›´å¥½çš„ä»£ç ï¼Œå¹¶ä¸åŒäº‹å’Œæœ€ç»ˆç”¨æˆ·å»ºç«‹ä¿¡ä»»ã€‚**
- en: If you can get in the habit of writing tests, you will write better designed
    code, save time in the long run and reduce the pain of pipelines failing or giving
    incorrect results in production.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ èƒ½å…»æˆç¼–å†™æµ‹è¯•çš„ä¹ æƒ¯ï¼Œä½ å°†ç¼–å†™å‡ºè®¾è®¡æ›´å¥½çš„ä»£ç ï¼ŒèŠ‚çœé•¿è¿œçš„æ—¶é—´ï¼Œå¹¶å‡å°‘ç®¡é“å¤±è´¥æˆ–åœ¨ç”Ÿäº§ä¸­ç»™å‡ºä¸æ­£ç¡®ç»“æœçš„ç—›è‹¦ã€‚
- en: Challenges with Unit testing PySpark code
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å•å…ƒæµ‹è¯• PySpark ä»£ç çš„æŒ‘æˆ˜
- en: 'A good unit-test should have the following characteristics:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¥½çš„å•å…ƒæµ‹è¯•åº”å…·å¤‡ä»¥ä¸‹ç‰¹å¾ï¼š
- en: '**Focused**. Each test should test a single behaviour/functionality.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸“æ³¨**ã€‚æ¯ä¸ªæµ‹è¯•åº”æµ‹è¯•ä¸€ä¸ªå•ä¸€çš„è¡Œä¸º/åŠŸèƒ½ã€‚'
- en: '**Fast**. Allowing you to iterate and gain feedback quickly'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¿«é€Ÿ**ã€‚å…è®¸ä½ å¿«é€Ÿè¿­ä»£å¹¶è·å¾—åé¦ˆã€‚'
- en: '**Isolated**. Each test should be responsible for testing a specific functionality
    and not depend on external factors in order to run successfully'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¤ç«‹**ã€‚æ¯ä¸ªæµ‹è¯•åº”è¯¥è´Ÿè´£æµ‹è¯•ç‰¹å®šçš„åŠŸèƒ½ï¼Œå¹¶ä¸”ä¸ä¾èµ–äºå¤–éƒ¨å› ç´ ä»¥ä¾¿æˆåŠŸè¿è¡Œã€‚'
- en: '**Concise**. Creating a test shouldnâ€™t include lots of boiler-plate code to
    mock/create complex objects in order for the test to run'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç®€æ´**ã€‚åˆ›å»ºæµ‹è¯•ä¸åº”åŒ…å«å¤§é‡çš„æ ·æ¿ä»£ç æ¥æ¨¡æ‹Ÿ/åˆ›å»ºå¤æ‚å¯¹è±¡ä»¥ä¾¿æµ‹è¯•è¿è¡Œã€‚'
- en: '**When it comes to writing unit-tests specifically for PySpark pipelines, writing
    focussed, fast, isolated and concise tests can be a challenge.**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**å½“æ¶‰åŠåˆ°ä¸“é—¨ä¸º PySpark ç®¡é“ç¼–å†™å•å…ƒæµ‹è¯•æ—¶ï¼Œç¼–å†™ä¸“æ³¨ã€å¿«é€Ÿã€å­¤ç«‹å’Œç®€æ´çš„æµ‹è¯•å¯èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚**'
- en: Here are some of the hurdles you might come acrossâ€¦
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šé‡åˆ°ä¸€äº›éšœç¢â€¦â€¦
- en: Writing testable code in the first place
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¦–å…ˆç¼–å†™å¯æµ‹è¯•çš„ä»£ç 
- en: PySpark pipelines tend to be written as one giant function responsible for multiple
    transformations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark ç®¡é“å¾€å¾€è¢«ç¼–å†™æˆä¸€ä¸ªè´Ÿè´£å¤šä¸ªè½¬æ¢çš„å·¨å‹å‡½æ•°ã€‚
- en: 'For example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼š
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It is logical to think about transformations in this way, and in many ways is
    easier to reason about and read.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™ä¸ªè§’åº¦è€ƒè™‘è½¬æ¢æ˜¯åˆç†çš„ï¼Œå¹¶ä¸”åœ¨å¾ˆå¤šæ–¹é¢æ›´å®¹æ˜“ç†è§£å’Œé˜…è¯»ã€‚
- en: But, when you start trying to write a test for this function you quickly realise
    it is very difficult to write a test to cover all functionality.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œå½“ä½ å¼€å§‹å°è¯•ä¸ºè¿™ä¸ªå‡½æ•°ç¼–å†™æµ‹è¯•æ—¶ï¼Œä½ ä¼šå¾ˆå¿«æ„è¯†åˆ°ï¼Œç¼–å†™ä¸€ä¸ªè¦†ç›–æ‰€æœ‰åŠŸèƒ½çš„æµ‹è¯•æ˜¯éå¸¸å›°éš¾çš„ã€‚
- en: This is because the function is highly coupled and there many different paths
    that the function can take.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å› ä¸ºå‡½æ•°é«˜åº¦è€¦åˆï¼Œå¹¶ä¸”å‡½æ•°å¯èƒ½æœ‰è®¸å¤šä¸åŒçš„è·¯å¾„ã€‚
- en: Even if you did write a test that verified the input data and output data were
    as expected. If the test failed for any reason it would be very difficult to understand
    which part of the very long function was at fault.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿ä½ ç¼–å†™äº†ä¸€ä¸ªæµ‹è¯•æ¥éªŒè¯è¾“å…¥æ•°æ®å’Œè¾“å‡ºæ•°æ®æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚å¦‚æœæµ‹è¯•å› ä»»ä½•åŸå› å¤±è´¥ï¼Œå°†å¾ˆéš¾ç†è§£é•¿å‡½æ•°çš„å“ªä¸ªéƒ¨åˆ†å‡ºç°äº†é—®é¢˜ã€‚
- en: Instead, you should break your transformations into blocks of reusable functions
    which are responsible for a single task. You can then write a unit-test for each
    individual function (task) which. When each of these unit-tests pass, you can
    be more confident in the output of the final pipeline when you compose all the
    functions together.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œä½ åº”è¯¥å°†ä½ çš„è½¬æ¢æ‹†åˆ†ä¸ºè´Ÿè´£å•ä¸€ä»»åŠ¡çš„å¯é‡ç”¨å‡½æ•°å—ã€‚ç„¶åä½ å¯ä»¥ä¸ºæ¯ä¸ªå•ç‹¬çš„å‡½æ•°ï¼ˆä»»åŠ¡ï¼‰ç¼–å†™å•å…ƒæµ‹è¯•ã€‚å½“è¿™äº›å•å…ƒæµ‹è¯•éƒ½é€šè¿‡æ—¶ï¼Œä½ å¯ä»¥æ›´æœ‰ä¿¡å¿ƒåœ°å°†æ‰€æœ‰å‡½æ•°ç»„åˆåœ¨ä¸€èµ·ï¼Œå¾—åˆ°æœ€ç»ˆç®¡é“çš„è¾“å‡ºã€‚
- en: Writing tests is a good practice and forces you to think about design principles.
    If it is difficult to test your code then you probably need to rethink the design
    of your code.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–å†™æµ‹è¯•æ˜¯ä¸€ç§è‰¯å¥½çš„å®è·µï¼Œå¹¶è¿«ä½¿ä½ è€ƒè™‘è®¾è®¡åŸåˆ™ã€‚å¦‚æœæµ‹è¯•ä½ çš„ä»£ç å¾ˆå›°éš¾ï¼Œé‚£ä¹ˆä½ å¯èƒ½éœ€è¦é‡æ–°è€ƒè™‘ä½ çš„ä»£ç è®¾è®¡ã€‚
- en: Speed
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€Ÿåº¦
- en: Spark is optimised to work on very large data and the compute is optimised to
    be distributed across many machines.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Spark è¢«ä¼˜åŒ–ç”¨äºå¤„ç†éå¸¸å¤§çš„æ•°æ®ï¼Œè®¡ç®—ä¹Ÿè¢«ä¼˜åŒ–ä»¥åˆ†å¸ƒåœ¨å¤šå°æœºå™¨ä¸Šã€‚
- en: This works great for a large cluster of machines but actually really hurts performance
    on a single machine that you might use for your unit-testing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨å¤§å‹é›†ç¾¤ä¸­è¡¨ç°å¾—å¾ˆå¥½ï¼Œä½†å®é™…ä¸Šåœ¨ä½ å¯èƒ½ç”¨äºå•å…ƒæµ‹è¯•çš„å•å°æœºå™¨ä¸Šä¼šä¸¥é‡å½±å“æ€§èƒ½ã€‚
- en: When you run a PySpark pipeline, spark will evaluate the entire pipeline and
    calculate an optimised â€˜planâ€™ to perform the computation across a distributed
    cluster.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ è¿è¡Œ PySpark ç®¡é“æ—¶ï¼Œspark ä¼šè¯„ä¼°æ•´ä¸ªç®¡é“å¹¶è®¡ç®—ä¸€ä¸ªä¼˜åŒ–çš„â€˜è®¡åˆ’â€™ä»¥åœ¨åˆ†å¸ƒå¼é›†ç¾¤ä¸Šæ‰§è¡Œè®¡ç®—ã€‚
- en: The planning comes with significant overhead. This makes sense when you are
    processing terabytes of data on a distributed cluster of machines. But when working
    on a single machine on a small dataset it can be surprisingly slow. Especially
    compared with what you might have experienced with Pandas.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡åˆ’å¸¦æ¥äº†æ˜¾è‘—çš„å¼€é”€ã€‚å½“ä½ åœ¨åˆ†å¸ƒå¼é›†ç¾¤ä¸Šå¤„ç†æ•°TBçš„æ•°æ®æ—¶ï¼Œè¿™æ˜¯åˆç†çš„ã€‚ä½†åœ¨å•å°æœºå™¨ä¸Šå¤„ç†å°æ•°æ®é›†æ—¶ï¼Œé€Ÿåº¦å¯èƒ½ä¼šå‡ºå¥‡åœ°æ…¢ã€‚å°¤å…¶æ˜¯ä¸ Pandas çš„ä½“éªŒç›¸æ¯”ã€‚
- en: Without optimising your SparkSession configuration parameters your unit-tests
    will take an agonizingly long time to run.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸ä¼˜åŒ–ä½ çš„ SparkSession é…ç½®å‚æ•°ï¼Œä½ çš„å•å…ƒæµ‹è¯•å°†ä¼šè¿è¡Œå¾—éå¸¸ç¼“æ…¢ã€‚
- en: Dependency on a Spark Session
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¾èµ– Spark Session
- en: To run PySpark code in your unit-test, you need a SparkSession.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨ä½ çš„å•å…ƒæµ‹è¯•ä¸­è¿è¡Œ PySpark ä»£ç ï¼Œä½ éœ€è¦ä¸€ä¸ª SparkSessionã€‚
- en: As stated above, ideally each test should be isolated from others and not require
    complex external objects. Unfortunately, there is no escaping the requirement
    to initiate a spark session for your unit-tests.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šæ‰€è¿°ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œæ¯ä¸ªæµ‹è¯•åº”ä¸å…¶ä»–æµ‹è¯•éš”ç¦»ï¼Œå¹¶ä¸”ä¸éœ€è¦å¤æ‚çš„å¤–éƒ¨å¯¹è±¡ã€‚ä¸å¹¸çš„æ˜¯ï¼Œä¸èƒ½é¿å…ä¸ºå•å…ƒæµ‹è¯•å¯åŠ¨Sparkä¼šè¯çš„è¦æ±‚ã€‚
- en: Creating a spark session is the first hurdle to overcome when writing a unit-test
    for your PySpark pipeline.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºSparkä¼šè¯æ˜¯ç¼–å†™PySparkç®¡é“å•å…ƒæµ‹è¯•æ—¶è¦å…‹æœçš„ç¬¬ä¸€ä¸ªéšœç¢ã€‚
- en: How should you create a SparkSession for your tests?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥å¦‚ä½•ä¸ºæµ‹è¯•åˆ›å»ºSparkSessionï¼Ÿ
- en: Initiating a new spark session for each test would dramatically increase the
    time to run the tests and introduce a ton of boiler-plate code to your tests.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ¯ä¸ªæµ‹è¯•åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„Sparkä¼šè¯ä¼šæ˜¾è‘—å¢åŠ è¿è¡Œæµ‹è¯•çš„æ—¶é—´ï¼Œå¹¶ç»™æµ‹è¯•å¼•å…¥å¤§é‡æ ·æ¿ä»£ç ã€‚
- en: Efficiently, creating and sharing a SparkSession across your tests is vital
    to keep the performance of your tests at an acceptable level.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ•ˆåœ°åˆ›å»ºå’Œå…±äº«SparkSessionå¯¹äºä¿æŒæµ‹è¯•æ€§èƒ½åœ¨å¯æ¥å—çš„æ°´å¹³è‡³å…³é‡è¦ã€‚
- en: Data
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®
- en: Your tests will require input data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çš„æµ‹è¯•å°†éœ€è¦è¾“å…¥æ•°æ®ã€‚
- en: There are two main problems with creating example data for big data pipelines.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºå¤§æ•°æ®ç®¡é“çš„ç¤ºä¾‹æ•°æ®å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ã€‚
- en: The first is size. Obviously, you cannot run your tests against the full dataset
    that will be used in production. You have to use a much smaller subset.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆæ˜¯å¤§å°ã€‚æ˜¾ç„¶ï¼Œä½ ä¸èƒ½åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨çš„å®Œæ•´æ•°æ®é›†ä¸Šè¿è¡Œæµ‹è¯•ã€‚ä½ å¿…é¡»ä½¿ç”¨ä¸€ä¸ªæ›´å°çš„å­é›†ã€‚
- en: But, by using a small dataset, you run into the second problem which is providing
    enough test data to cover all the edge cases you want to handle.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œé€šè¿‡ä½¿ç”¨ä¸€ä¸ªå°æ•°æ®é›†ï¼Œä½ ä¼šé‡åˆ°ç¬¬äºŒä¸ªé—®é¢˜ï¼Œå³æä¾›è¶³å¤Ÿçš„æµ‹è¯•æ•°æ®æ¥è¦†ç›–ä½ æƒ³è¦å¤„ç†çš„æ‰€æœ‰è¾¹ç•Œæƒ…å†µã€‚
- en: It is *really* hard to mock realistic data for testing. There is not much you
    can do about this, however, you can use smaller, targeted datasets for your tests.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*çœŸæ­£*å¾ˆéš¾ä¸ºæµ‹è¯•æ¨¡æ‹Ÿç°å®æ•°æ®ã€‚è™½ç„¶å¯¹æ­¤æ²¡æœ‰å¤ªå¤šåŠæ³•ï¼Œä½†ä½ å¯ä»¥ä½¿ç”¨æ›´å°ã€ç›®æ ‡æ˜ç¡®çš„æ•°æ®é›†è¿›è¡Œæµ‹è¯•ã€‚'
- en: Steps to unit-test your PySpark code with Pytest
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Pytestå¯¹PySparkä»£ç è¿›è¡Œå•å…ƒæµ‹è¯•çš„æ­¥éª¤
- en: Letâ€™s work through an example of writing unit-tests for a PySpark pipline using
    [PyTest](https://docs.pytest.org/en/7.2.x/) .
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªä½¿ç”¨[PyTest](https://docs.pytest.org/en/7.2.x/)ä¸ºPySparkç®¡é“ç¼–å†™å•å…ƒæµ‹è¯•çš„ç¤ºä¾‹æ¥è¿›è¡Œå®è·µã€‚
- en: '*ğŸ’» The full code is available in this* [*GitHub repository*](https://github.com/julian-west/e4ds-snippets/tree/master/pyspark/pyspark_unit_testing)'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ğŸ’» å®Œæ•´ä»£ç å¯åœ¨æ­¤* [*GitHubå­˜å‚¨åº“*](https://github.com/julian-west/e4ds-snippets/tree/master/pyspark/pyspark_unit_testing)
    *ä¸­è·å–*'
- en: Example code
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ä»£ç 
- en: Here is an example PySpark pipeline to process some bank transactions. In this
    scenario we want to take the raw transactions and classify them as debit account
    or credit account transactions by joining them to some reference data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¤„ç†é“¶è¡Œäº¤æ˜“çš„PySparkç®¡é“ç¤ºä¾‹ã€‚åœ¨è¿™ä¸ªåœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›å°†åŸå§‹äº¤æ˜“åˆ†ç±»ä¸ºå€Ÿè®°è´¦æˆ·æˆ–ä¿¡ç”¨è´¦æˆ·äº¤æ˜“ï¼Œé€šè¿‡å°†å…¶ä¸ä¸€äº›å‚è€ƒæ•°æ®è¿æ¥èµ·æ¥ã€‚
- en: Each transaction record comes with an account ID. We will use this account ID
    to join to account information table which has information on whether this account
    ID is from a debit or credit account.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªäº¤æ˜“è®°å½•éƒ½é™„å¸¦ä¸€ä¸ªè´¦æˆ·IDã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªè´¦æˆ·IDè¿æ¥åˆ°åŒ…å«è¯¥è´¦æˆ·IDæ˜¯å€Ÿè®°è´¦æˆ·è¿˜æ˜¯ä¿¡ç”¨è´¦æˆ·çš„ä¿¡æ¯çš„è´¦æˆ·ä¿¡æ¯è¡¨ã€‚
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There are few issues with this example pipeline:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç¤ºä¾‹ç®¡é“å­˜åœ¨å‡ ä¸ªé—®é¢˜ï¼š
- en: Difficult to read. Lots of complex logic in one place. For example, regex replacements,
    joining on substrings etc.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éš¾ä»¥é˜…è¯»ã€‚ä¸€ä¸ªåœ°æ–¹åŒ…å«äº†å¤§é‡å¤æ‚çš„é€»è¾‘ã€‚ä¾‹å¦‚ï¼Œæ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ã€æŒ‰å­å­—ç¬¦ä¸²è¿æ¥ç­‰ã€‚
- en: Difficult to test. Single function responsible for multiple actions
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éš¾ä»¥æµ‹è¯•ã€‚å•ä¸ªå‡½æ•°è´Ÿè´£å¤šä¸ªæ“ä½œ
- en: Difficult to reuse. The debit/credit classification is business logic which
    cannot easily be reused across the project
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éš¾ä»¥é‡ç”¨ã€‚å€Ÿè®°/ä¿¡ç”¨åˆ†ç±»æ˜¯ä¸šåŠ¡é€»è¾‘ï¼Œæ— æ³•è½»æ¾åœ¨é¡¹ç›®ä¸­é‡ç”¨
- en: 'Step 1: Refactor into smaller logical units'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤1ï¼šå°†ä»£ç é‡æ„ä¸ºæ›´å°çš„é€»è¾‘å•å…ƒ
- en: Letâ€™s first refactor the code into individual functions, then compose the functions
    together for the main `classify_debit_credit_transactions` function.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆå°†ä»£ç é‡æ„ä¸ºå•ç‹¬çš„å‡½æ•°ï¼Œç„¶åå°†è¿™äº›å‡½æ•°ç»„åˆæˆä¸»`classify_debit_credit_transactions`å‡½æ•°ã€‚
- en: We can then write an test for each individual function to ensure it is behaving
    as expected.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ªå•ç‹¬çš„å‡½æ•°ç¼–å†™æµ‹è¯•ï¼Œä»¥ç¡®ä¿å…¶æŒ‰é¢„æœŸè¡Œä¸ºã€‚
- en: While this increases the overall number of lines of code, it is easier to test
    and we can now reuse the functions across other parts of the project.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™å¢åŠ äº†ä»£ç çš„æ€»ä½“è¡Œæ•°ï¼Œä½†æµ‹è¯•èµ·æ¥æ›´å®¹æ˜“ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨é¡¹ç›®çš„å…¶ä»–éƒ¨åˆ†é‡ç”¨è¿™äº›å‡½æ•°ã€‚
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 3\. Creating a resuable SparkSession using Fixtures
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. ä½¿ç”¨Fixturesåˆ›å»ºå¯é‡ç”¨çš„SparkSession
- en: Before writing our unit-tests, we need to create a SparkSession which we can
    reuse across all our tests.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¼–å†™å•å…ƒæµ‹è¯•ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªå¯ä»¥åœ¨æ‰€æœ‰æµ‹è¯•ä¸­é‡ç”¨çš„SparkSessionã€‚
- en: To do this, we create a [PyTest fixture](https://docs.pytest.org/en/6.2.x/fixture.html)
    in a `conftest.py` file.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨ `conftest.py` æ–‡ä»¶ä¸­åˆ›å»ºä¸€ä¸ª [PyTest fixture](https://docs.pytest.org/en/6.2.x/fixture.html)ã€‚
- en: Pytest fixtures are objects which are created once and then reused across multiple
    tests. This is particularly useful for complex objects like the SparkSession which
    have a significant overhead to create.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Pytest fixtures æ˜¯ä¸€æ¬¡åˆ›å»ºç„¶ååœ¨å¤šä¸ªæµ‹è¯•ä¸­é‡å¤ä½¿ç”¨çš„å¯¹è±¡ã€‚è¿™å¯¹äºåƒ SparkSession è¿™æ ·çš„å¤æ‚å¯¹è±¡ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºåˆ›å»ºå®ƒä»¬çš„å¼€é”€å¾ˆå¤§ã€‚
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It is important to set a number of configuration parameters in order to optimise
    the SparkSession for processing small data on a single machine for testing:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®ä¸€äº›é…ç½®å‚æ•°ä»¥ä¼˜åŒ– SparkSessionï¼Œç”¨äºåœ¨å•å°æœºå™¨ä¸Šå¤„ç†å°æ•°æ®è¿›è¡Œæµ‹è¯•æ˜¯å¾ˆé‡è¦çš„ï¼š
- en: '`master = local[1]` â€“ specifies that spark is running on a local machine with
    one thread'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`master = local[1]` â€“ æŒ‡å®š spark åœ¨ä¸€å°æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œï¼Œå¹¶ä¸”ä½¿ç”¨ä¸€ä¸ªçº¿ç¨‹'
- en: '`spark.executor.cores = 1` â€“ set number of cores to one'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.executor.cores = 1` â€“ è®¾ç½®æ ¸å¿ƒæ•°ä¸ºä¸€ä¸ª'
- en: '`spark.executor.instances = 1` - set executors to one'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.executor.instances = 1` - è®¾ç½®æ‰§è¡Œå™¨ä¸ºä¸€ä¸ª'
- en: '`spark.sql.shuffle.partitions = 1` - set the maximum number of partitions to
    1'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.sql.shuffle.partitions = 1` - è®¾ç½®æœ€å¤§åˆ†åŒºæ•°ä¸º 1'
- en: '`spark.driver.bindAddress = 127.0.0.1` â€“ (optional) Explicitly specify the
    driver bind address. Useful if your machine has also has a live connection to
    a remote cluster'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.driver.bindAddress = 127.0.0.1` â€“ ï¼ˆå¯é€‰ï¼‰æ˜¾å¼æŒ‡å®šé©±åŠ¨ç¨‹åºç»‘å®šåœ°å€ã€‚å¦‚æœä½ çš„æœºå™¨ä¹Ÿä¸è¿œç¨‹é›†ç¾¤æœ‰æ´»åŠ¨è¿æ¥ï¼Œè¿™å°†å¾ˆæœ‰ç”¨ã€‚'
- en: These config parameters essentially tell spark that you are processing on a
    single machine and spark should not try to distribute the computation. This will
    save a significant amount of time in both the planning of the pipeline execution
    and the computation itself.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›é…ç½®å‚æ•°æœ¬è´¨ä¸Šå‘Šè¯‰ spark ä½ åœ¨å•å°æœºå™¨ä¸Šå¤„ç†æ•°æ®ï¼Œå¹¶ä¸” spark ä¸åº”è¯¥å°è¯•åˆ†å‘è®¡ç®—ã€‚è¿™å°†å¤§å¤§èŠ‚çœç®¡é“æ‰§è¡Œå’Œè®¡ç®—æœ¬èº«çš„æ—¶é—´ã€‚
- en: Note, it is recommended to `yield` the spark session instead of using `return`.
    Read the [PyTest documentation](https://docs.pytest.org/en/7.1.x/how-to/fixtures.html#yield-fixtures-recommended)
    for more information. Using `yield` also allows you to perform any clean up actions
    after your tests have run (e.g. deleting any local temp directories, databases
    or tables etc.).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå»ºè®®ä½¿ç”¨ `yield` è€Œä¸æ˜¯ `return` æ¥è¿”å› spark sessionã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·é˜…è¯» [PyTest æ–‡æ¡£](https://docs.pytest.org/en/7.1.x/how-to/fixtures.html#yield-fixtures-recommended)ã€‚ä½¿ç”¨
    `yield` è¿˜å…è®¸ä½ åœ¨æµ‹è¯•è¿è¡Œåæ‰§è¡Œä»»ä½•æ¸…ç†æ“ä½œï¼ˆä¾‹å¦‚ï¼Œåˆ é™¤æœ¬åœ°ä¸´æ—¶ç›®å½•ã€æ•°æ®åº“æˆ–è¡¨ç­‰ï¼‰ã€‚
- en: 4\. Creating unit-tests for the code
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. ä¸ºä»£ç åˆ›å»ºå•å…ƒæµ‹è¯•
- en: Now letâ€™s write some tests for our code.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„ä»£ç ç¼–å†™ä¸€äº›æµ‹è¯•ã€‚
- en: 'I find it most efficient to organise my PySpark unit tests with the following
    structure:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å‘ç°æœ€æœ‰æ•ˆçš„æ–¹å¼æ˜¯ç”¨ä»¥ä¸‹ç»“æ„ç»„ç»‡æˆ‘çš„ PySpark å•å…ƒæµ‹è¯•ï¼š
- en: Create the input dataframe
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºè¾“å…¥ DataFrame
- en: Create the output dataframe using the function we want to test
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æˆ‘ä»¬è¦æµ‹è¯•çš„å‡½æ•°åˆ›å»ºè¾“å‡º DataFrame
- en: Specify the expected output values
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŒ‡å®šé¢„æœŸçš„è¾“å‡ºå€¼
- en: Compare the results
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒç»“æœ
- en: I also try to ensure the test covers positive test cases and at least one negative
    test case.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜å°½é‡ç¡®ä¿æµ‹è¯•è¦†ç›–æ­£é¢æµ‹è¯•ç”¨ä¾‹å’Œè‡³å°‘ä¸€ä¸ªè´Ÿé¢æµ‹è¯•ç”¨ä¾‹ã€‚
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We now have unit-test for each component in the PySpark pipeline.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨ä¸º PySpark ç®¡é“ä¸­çš„æ¯ä¸ªç»„ä»¶éƒ½æœ‰äº†å•å…ƒæµ‹è¯•ã€‚
- en: As each test reuses the same SparkSession the overhead of running multiple tests
    is significantly reduced.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ¯ä¸ªæµ‹è¯•éƒ½é‡ç”¨ç›¸åŒçš„ SparkSessionï¼Œè¿è¡Œå¤šä¸ªæµ‹è¯•çš„å¼€é”€å¤§å¤§å‡å°‘ã€‚
- en: Further tips for unit testing PySpark code
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark ä»£ç å•å…ƒæµ‹è¯•çš„è¿›ä¸€æ­¥æç¤º
- en: Create test dataframes with the minimum required information
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºåŒ…å«æœ€å°‘æ‰€éœ€ä¿¡æ¯çš„æµ‹è¯• DataFrame
- en: When creating dataframes with test data, only create columns relevant to the
    transformation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ›å»ºæµ‹è¯•æ•°æ®çš„ DataFrame æ—¶ï¼Œåªåˆ›å»ºä¸è½¬æ¢ç›¸å…³çš„åˆ—ã€‚
- en: You only really need to create data with columns required for the function.
    You donâ€™t need all the other columns which might be present in the production
    data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åªéœ€è¦åˆ›å»ºå‡½æ•°æ‰€éœ€çš„åˆ—çš„æ•°æ®ã€‚ä½ ä¸éœ€è¦æ‰€æœ‰å¯èƒ½å­˜åœ¨äºç”Ÿäº§æ•°æ®ä¸­çš„å…¶ä»–åˆ—ã€‚
- en: This helps write concise functions and is more readable as it is clear which
    columns are required and impacted by the function. If you find you need a big
    dataframe with many columns in order to carry out a transformation you are probably
    trying to do too much at once.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æœ‰åŠ©äºç¼–å†™ç®€æ´çš„å‡½æ•°ï¼Œå¹¶ä¸”æ›´å…·å¯è¯»æ€§ï¼Œå› ä¸ºå¯ä»¥æ¸…æ¥šåœ°çŸ¥é“å“ªäº›åˆ—æ˜¯å‡½æ•°æ‰€éœ€çš„ä»¥åŠå—å‡½æ•°å½±å“çš„ã€‚å¦‚æœä½ å‘ç°éœ€è¦ä¸€ä¸ªåŒ…å«è®¸å¤šåˆ—çš„å¤§ DataFrame æ¥æ‰§è¡Œè½¬æ¢ï¼Œä½ å¯èƒ½ä¸€æ¬¡å°è¯•åšå¾—å¤ªå¤šäº†ã€‚
- en: This is just a guideline, your own usecase might require more complicated test
    data, but if possible keep it small, concise and localised to the test.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯ä¸€ä¸ªæŒ‡å¯¼æ–¹é’ˆï¼Œä½ è‡ªå·±çš„ç”¨ä¾‹å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æµ‹è¯•æ•°æ®ï¼Œä½†å¦‚æœå¯èƒ½ï¼Œä¿æŒæ•°æ®å°å·§ã€ç®€æ´ï¼Œå¹¶å±€é™äºæµ‹è¯•èŒƒå›´ã€‚
- en: Remember to call an â€˜actionâ€™ in order to trigger the PySpark computation
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®°å¾—è°ƒç”¨ä¸€ä¸ªâ€œæ“ä½œâ€ä»¥è§¦å‘ PySpark è®¡ç®—
- en: PySpark uses lazy evaluation. You need to call an â€˜actionâ€™ (e.g. `collect`,
    `count` etc.) during your test in order to compute a result that you can compare
    to the expected output.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark ä½¿ç”¨æƒ°æ€§è®¡ç®—ã€‚ä½ éœ€è¦åœ¨æµ‹è¯•æœŸé—´è°ƒç”¨ä¸€ä¸ªâ€˜actionâ€™ï¼ˆä¾‹å¦‚ `collect`ã€`count` ç­‰ï¼‰ï¼Œä»¥è®¡ç®—ä¸€ä¸ªå¯ä»¥ä¸é¢„æœŸè¾“å‡ºè¿›è¡Œæ¯”è¾ƒçš„ç»“æœã€‚
- en: Donâ€™t run all PySpark tests if you donâ€™t need to
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚æœä¸éœ€è¦çš„è¯ï¼Œä¸è¦è¿è¡Œæ‰€æœ‰çš„ PySpark æµ‹è¯•
- en: PySpark tests generally take longer than normal unit tests to run as there is
    overhead to calculate a computation plan and then execute it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark æµ‹è¯•é€šå¸¸æ¯”æ­£å¸¸å•å…ƒæµ‹è¯•è¿è¡Œæ—¶é—´æ›´é•¿ï¼Œå› ä¸ºæœ‰è®¡ç®—è®¡åˆ’å’Œæ‰§è¡Œçš„å¼€é”€ã€‚
- en: During development, make use of some of Pytestâ€™s features such as the `-k` flag
    to run single tests or just run tests in a single file. Then only run the full
    test suite before committing your code.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨ Pytest çš„ä¸€äº›åŠŸèƒ½ï¼Œä¾‹å¦‚ `-k` æ ‡å¿—æ¥è¿è¡Œå•ä¸ªæµ‹è¯•æˆ–ä»…è¿è¡Œå•ä¸ªæ–‡ä»¶ä¸­çš„æµ‹è¯•ã€‚ç„¶åï¼Œä»…åœ¨æäº¤ä»£ç ä¹‹å‰è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶ã€‚
- en: Keep the unit-tests isolated
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¿æŒå•å…ƒæµ‹è¯•çš„éš”ç¦»æ€§
- en: Be careful not to modify your spark session during a test (e.g. creating a table,
    but not deleting it afterwards).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ä¸è¦åœ¨æµ‹è¯•æœŸé—´ä¿®æ”¹ä½ çš„ spark ä¼šè¯ï¼ˆä¾‹å¦‚ï¼Œåˆ›å»ºä¸€ä¸ªè¡¨ï¼Œä½†ä¹‹åä¸åˆ é™¤å®ƒï¼‰ã€‚
- en: The table will be persisted across all tests which may interfere with their
    expected behaviour.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æ ¼å°†åœ¨æ‰€æœ‰æµ‹è¯•ä¸­æŒç»­å­˜åœ¨ï¼Œè¿™å¯èƒ½ä¼šå¹²æ‰°é¢„æœŸè¡Œä¸ºã€‚
- en: Try and keep the creation of data close to where it is used.
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°½é‡å°†æ•°æ®çš„åˆ›å»ºä¿æŒåœ¨ä½¿ç”¨å®ƒçš„åœ°æ–¹é™„è¿‘ã€‚
- en: You could use Pytest fixtures to share dataframes or even load test data from
    csv files etc. across multiple tests.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä½¿ç”¨ Pytest fixtures åœ¨å¤šä¸ªæµ‹è¯•ä¹‹é—´å…±äº«æ•°æ®æ¡†ï¼Œç”šè‡³ä» CSV æ–‡ä»¶ç­‰ä¸­åŠ è½½æµ‹è¯•æ•°æ®ã€‚
- en: However, in my experience, it is easier and more readable to create data as
    required for each individual test.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæ ¹æ®æˆ‘çš„ç»éªŒï¼Œä¸ºæ¯ä¸ªå•ç‹¬çš„æµ‹è¯•åˆ›å»ºæ‰€éœ€çš„æ•°æ®æ›´å®¹æ˜“ä¸”æ›´å…·å¯è¯»æ€§ã€‚
- en: Test positive and negative outcomes
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ­£é¢å’Œè´Ÿé¢ç»“æœ
- en: For example, when testing a joining condition you should include data which
    should not satisfy the join condition. This helps ensure you are excluding the
    right data as well as including the right data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨æµ‹è¯•è¿æ¥æ¡ä»¶æ—¶ï¼Œä½ åº”è¯¥åŒ…æ‹¬ä¸€äº›ä¸æ»¡è¶³è¿æ¥æ¡ä»¶çš„æ•°æ®ã€‚è¿™æœ‰åŠ©äºç¡®ä¿ä½ æ—¢æ’é™¤äº†é”™è¯¯çš„æ•°æ®ï¼ŒåˆåŒ…å«äº†æ­£ç¡®çš„æ•°æ®ã€‚
- en: This blog was originally published on [engineeringfordatascience.com](https://engineeringfordatascience.com/posts/pyspark_unit_testing_with_pytest/)
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æœ¬åšå®¢æœ€åˆå‘å¸ƒåœ¨ [engineeringfordatascience.com](https://engineeringfordatascience.com/posts/pyspark_unit_testing_with_pytest/)
- en: 'If you use Pytest for your unit testing, check out my other article with some
    useful tips for using Pytest:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä½¿ç”¨ Pytest è¿›è¡Œå•å…ƒæµ‹è¯•ï¼Œå¯ä»¥æŸ¥çœ‹æˆ‘å¦ä¸€ç¯‡åŒ…å« Pytest ä½¿ç”¨æŠ€å·§çš„æ–‡ç« ï¼š
- en: '[](/13-tips-for-using-pytest-5341e3366d2d?source=post_page-----b5ab2fd54415--------------------------------)
    [## 13 Tips for using PyTest'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[13 Tips for using PyTest](https://towardsdatascience.com/13-tips-for-using-pytest-5341e3366d2d?source=post_page-----b5ab2fd54415--------------------------------)
    [## 13 Tips for using PyTest'
- en: Unit-testing is a really important skill for software development. There are
    some great Python libraries to help usâ€¦
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å•å…ƒæµ‹è¯•æ˜¯è½¯ä»¶å¼€å‘ä¸­éå¸¸é‡è¦çš„æŠ€èƒ½ã€‚æœ‰ä¸€äº›å¾ˆæ£’çš„ Python åº“å¯ä»¥å¸®åŠ©æˆ‘ä»¬â€¦â€¦
- en: towardsdatascience.com](/13-tips-for-using-pytest-5341e3366d2d?source=post_page-----b5ab2fd54415--------------------------------)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[13 Tips for using PyTest](https://towardsdatascience.com/13-tips-for-using-pytest-5341e3366d2d?source=post_page-----b5ab2fd54415--------------------------------)'
