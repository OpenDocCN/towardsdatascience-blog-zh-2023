# 机器学习中的分类问题关键评估指标

> 原文：[https://towardsdatascience.com/essential-evaluation-metrics-for-classification-problems-in-machine-learning-69e90665375b](https://towardsdatascience.com/essential-evaluation-metrics-for-classification-problems-in-machine-learning-69e90665375b)

## 了解和评估分类模型性能的全面指南

[](https://aaron-zhu.medium.com/?source=post_page-----69e90665375b--------------------------------)[![Aaron Zhu](../Images/42e9690c4b4aad63f396b171a74e29f7.png)](https://aaron-zhu.medium.com/?source=post_page-----69e90665375b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----69e90665375b--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----69e90665375b--------------------------------) [Aaron Zhu](https://aaron-zhu.medium.com/?source=post_page-----69e90665375b--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----69e90665375b--------------------------------) ·阅读时长 10 分钟·2023年3月10日

--

![](../Images/f1067a61f9cd203144c29c260ca6388d.png)

图片由 [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

你是否对评估机器学习模型性能时使用的术语感到困惑？当遇到混淆矩阵、精确度、召回率、特异性和敏感性等术语时，是否会感到迷茫？别担心，因为在这篇博客文章中，我们将深入探讨这些评估指标，帮助你理解这些术语。

## 什么是混淆矩阵？

让我们从混淆矩阵开始。它是用于评估分类模型性能的表格。它包含四个值：真正例 (**TP**)、真负例 (**TN**)、假正例 (**FP**) 和假负例 (**FN**)。真正例是指模型正确预测正类，真负例是指模型正确预测负类，假正例是指模型预测为正类但实际为负类，假负例是指模型预测为负类但实际为正类。

![](../Images/e858e19e7cdb87ed020d37c10bf86638.png)

图片作者

## 什么是准确率？

**准确率** 是衡量模型总体表现的指标。它是模型做出的正确预测占所有预测总数的比例。换句话说，它是正确预测的正例和负例数量除以预测总数。

> 准确率 = (TP+TN)/(TP+TN+FP+FN)

## 什么是精确度？

**Precision** 是衡量模型正例预测准确度的指标。它的计算方法是将真正例的数量与真正例和假正例的总和之比。高 Precision 表明模型大多数时间正确预测了正类。

> Precision = TP / (TP + FP)

## 什么是 Recall？

**Recall** 是衡量模型识别正类能力的指标。它的计算方法是将真正例的数量与真正例和假负例的总和之比。高 Recall 表明模型能够识别大多数正实例。

> Recall = TP / (TP + FN)

## 什么是 Specificity？

**Specificity** 是衡量模型识别负类能力的指标。它的计算方法是将真正例的数量与真正例和假正例的总和之比。高 Specificity 表明模型能够识别大多数负实例。

> Specificity = TN / (TN + FP)

你可以将 Specificity 视为带有不同正负标签定义的 Recall，即正标签被视为负，负标签被视为正。

## 什么是 Sensitivity？

**Sensitivity** 是 Recall 的另一种术语，特别是在医学背景下，它指的是医学测试在实际患病的个体中检测疾病或情况的能力。

> Sensitivity = TP / (TP + FN)

总结表：

![](../Images/fa901f75f5c2167956718a5bf5ab86e7.png)

作者提供的图片

## 什么时候 Precision-Recall 比准确率更好的指标？

Precision-Recall 用于数据**不平衡**时，而不是准确率，这意味着一个类别的样本显著多于另一个类别。

在这种情况下，准确率可能会产生误导，因为模型可以通过简单地预测多数类来实现高准确率。例如，在一个二分类问题中，如果 90% 的样本属于负类，那么一个总是预测负类的模型将有 90% 的准确率，即使它没有进行任何正确的正例预测。

> 当数据不平衡时，我们可能会有一个准确率非常高的模型。但由于 Precision-Recall 值低，模型是无用的。

Precision-Recall 是在这种情况下衡量模型性能的更好指标，因为它考虑了真正例和假正例的比例，以及假负例和真正例，这些在不平衡数据集中更为关键。Precision 衡量的是所有正例预测中真正例预测的比例，而 Recall 衡量的是所有实际正例样本中真正例预测的比例。

> Precision-Recall 更适合用于评估不平衡数据集中的模型性能，而准确率在类别平衡时更为合适。

## 什么是 Precision-Recall 权衡？

理想情况下，我们希望我们的模型具有高精确度和高召回率。通常很难同时实现这两者。

> 精确度-召回率权衡的出现是因为优化一个指标通常会以另一个指标为代价。

这是为什么

+   如果一个模型在预测中更为保守，它可能通过减少假阳性的数量来实现**更高的精确度**，但这也可能导致召回率降低，因为它可能会遗漏一些真实的正例。

+   相反，假设一个模型在预测中更为激进。在这种情况下，它可能通过捕获更多的真实正例来实现**更高的召回率**，但这也可能导致精确度降低，因为它可能会产生更多的假阳性预测。

> 因此，为了确定我们应该优先考虑精确度还是召回率，我们需要评估假阳性和假阴性的成本。

在情况1中，设计了一种医学检测测试来检测人们是否患有疾病。

+   假阴性案例的成本可能是——病人未能接受正确的治疗，如果疾病具有传染性，可能导致更多人感染。

+   另一方面，假阳性案例的成本可能是——浪费资源治疗健康人群，并不必要地将其隔离。

因此，假阴性案例的成本远高于假阳性案例的成本。在这种情况下，更关注召回率更为合理。

![](../Images/97c0aae8ec053fb3877a5a79f3186269.png)

图片由作者提供

在情况2中，一家银行设计了一个机器学习模型来检测信用卡欺诈。

+   假阴性案例的成本可能是——银行在欺诈交易中损失金钱。

+   假阳性案例的成本可能是——虚假的欺诈警报影响了客户体验，导致客户留存率下降。

因此，假阳性案例的成本远高于假阴性案例的成本。根据关于信用卡欺诈的研究，假阳性信用卡欺诈的损失收入是实际欺诈的13倍。在这种情况下，更关注精确度更为合理。

![](../Images/48f9e0833e14b9e5418e7fb251a72254.png)

图片由作者提供

## 什么是F1-Score？

在上述示例中，我们尝试优先考虑召回率或精确度，但这可能以牺牲另一个指标为代价。然而，也有许多情况，其中**召回率和精确度是** **同等重要的**。在这种情况下，我们应使用另一种度量标准，称为F1分数。

**F1分数**考虑了精确度和召回率，并提供一个总结模型整体表现的单一分数。其范围从0到1，得分为1表示完美的精确度和召回率。

计算F1分数的公式是：

![](../Images/0890623c7974fe4e87d056c37fec34ad.png)

它也可以通过精确度和召回率的调和平均数来计算：

![](../Images/2b7b4a6d57e8f811b81d6374b0477584.png)

## 接收者操作特征曲线（ROC曲线）和**曲线下面积**（**AUC**）是什么？

接收者操作特征（ROC）曲线是二分类模型性能的图形表示，它预测事件发生的概率。ROC曲线是通过将真正例率（TPR）与假正例率（FPR）在**不同阈值**设置下绘制出来的。

> TPR=TP/(TP+FN)
> 
> FPR=FP/(FP+TN)

让我们考虑一个二分类问题的例子，我们希望预测一封电子邮件是否是垃圾邮件。假设我们有一个模型，它预测电子邮件是垃圾邮件的**概率**，我们希望使用ROC曲线来评估其性能。

要创建ROC曲线，我们需要设置一个**阈值**（即从0到1），根据这个阈值我们将电子邮件分类为垃圾邮件，低于这个阈值则分类为非垃圾邮件。阈值是决定真正例率（TPR）和假正例率（FPR）之间权衡的决策边界。

例如，如果我们将阈值设置为0.5，则任何预测概率大于0.5的电子邮件将被分类为垃圾邮件，而任何预测概率小于或等于0.5的电子邮件将被分类为非垃圾邮件。这个阈值会给我们一定的TPR和FPR。

通常，随着阈值的增加，TPR和FPR都会减少。在最极端的情况下，当阈值为0时，所有预测值都是正的，因此TPR=FPR=1。相反，当阈值为1时，所有预测值都是负的，因此TPR=FPR=0。

假设对于给定的数据集，我们计算了**各种阈值**下的TPR和FPR，得到如下结果：

![](../Images/37721065980b5ead2dbf5a1ac5516358.png)

作者提供的图像

我们可以将这些值绘制在ROC曲线上，y轴为TPR，x轴为FPR，如下所示：

![](../Images/a88c859d3bd567bacab8bc6e49c0e459.png)

作者提供的图像

从图中可以看出，ROC曲线是不同阈值下TPR和FPR之间的权衡。

**曲线下面积**（**AUC**）衡量模型的整体性能，AUC为1表示完美的性能，AUC为0.5表示随机猜测（即表示随机预测的对角线）。

ROC的主要要点：

+   当类别**平衡**且假正例和假负例的成本相似时，ROC在评估不同模型时效果更好。AUC-ROC值越高，模型越好。

+   从ROC曲线中，我们可以选择**最佳阈值**，这取决于分类器的应用目的——如果假阳性和假阴性的成本相似，那么接近ROC曲线左上角的阈值是最佳的。如果假阳性和假阴性的成本较高，我们可以选择较高的阈值。相反，如果假阴性和假阳性的成本较高，我们可以选择较低的阈值。

+   ROC是**阈值不变的**。它测量了模型在一系列阈值下的表现。这意味着我们不需要提前使用ROC确定阈值，与基于特定阈值的精确度、召回率、准确性和F1分数不同。

## 什么是精确度-召回曲线（PRC）？

与绘制TPR和FPR的ROC曲线相比，精确度-召回曲线（PRC）在y轴上绘制精确度，在x轴上绘制召回率。PRC曲线显示了模型在识别正例时如何避免假阳性。

PRC曲线下面积可以衡量模型的性能。AUC-PRC值越高，模型越好。AUC-PRC为0.5的模型不比随机猜测好，而AUC-PRC为1.0的模型是完美的。

一般来说，随着阈值的增加，精确度会提高，而召回率会降低。在最极端的情况下，当阈值为0时，所有预测值都是正值，因此，召回率=1，精确度=0。相反，当阈值为1时，所有预测值都是负值，因此，召回率=0，精确度=1。

假设对于给定的数据集，我们计算了**各种阈值**下的精确度和召回率，并得到了以下结果：

![](../Images/e687e008df1fd8694d5fd10614dc1a7a.png)

图片由作者提供

![](../Images/63201c9b57f1ad33a72e5a8d8594152b.png)

图片由作者提供

PRC的关键要点：

+   在ROC和PRC曲线之间的选择取决于当前的问题。当类别平衡且假阳性和假阴性的成本相似时，ROC曲线是有用的。PRC曲线在类别不平衡或假阳性和假阴性的成本不同的情况下很有用。

+   通过查看PRC，你可以选择一个**最佳阈值**，根据你的具体使用案例平衡精确度和召回率。

我们已经覆盖了许多分类问题的评估指标。这些指标是相互关联的，每个指标在衡量模型准确性方面都有其优缺点。总体而言，理解这些指标对于开发有效的机器学习模型并根据预测做出明智的决策至关重要。

如果你想了解更多与**统计学**相关的文章，请查看我的文章：

+   [**假设检验中的I型和II型错误及样本量计算**](/type-i-ii-errors-and-sample-size-calculation-in-hypothesis-testing-760dae42a065)

+   [**中心极限定理的7个常见问题**](/7-most-asked-questions-on-central-limit-theorem-82e95eb7d964)

+   [**标准差与标准误差：有什么区别？**](/standard-deviation-vs-standard-error-whats-the-difference-ae969f48adef)

+   [**3种最常见的误解：假设检验、置信区间、P值**](/the-most-common-misinterpretations-hypothesis-testing-confidence-interval-p-value-4548a10a5b72)

+   [**线性回归模型中的误差项是否服从正态分布？**](/are-the-error-terms-normally-distributed-in-a-linear-regression-model-15e6882298a4)

+   [**线性回归模型中的OLS估计量是否服从正态分布？**](/are-ols-estimators-normally-distributed-in-a-linear-regression-model-89b688fa8dc3)

+   [**什么是正则化：偏差-方差权衡**](/machine-learning-bias-variance-tradeoff-and-regularization-94846f945131)

+   [**方差与协方差与相关性：有什么区别？**](https://medium.com/geekculture/variance-vs-covariance-vs-correlation-what-is-the-difference-95adff96d542)

+   [**置信区间与预测区间：有什么区别？**](/confidence-interval-vs-prediction-interval-what-is-the-difference-64c45146d47d)

+   [**哪种错误更糟糕，I型错误还是II型错误？**](https://medium.com/geekculture/which-is-worse-type-i-or-type-ii-errors-f40a0f040fcc)

# 感谢阅读！

如果你喜欢这篇文章，请点击**掌声**图标。如果你想看到我和其他数千位作者在Medium上的更多文章，你可以：

+   [**订阅**](https://aaron-zhu.medium.com/subscribe)我的新闻通讯，以便在我发布新文章时收到邮件通知。

+   注册成为[**会员**](https://aaron-zhu.medium.com/membership)以解锁对Medium上所有内容的完全访问权限。
