- en: Practical Guide for Anomaly Detection in Time Series with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/practical-guide-for-anomaly-detection-in-time-series-with-python-d4847d6c099f](https://towardsdatascience.com/practical-guide-for-anomaly-detection-in-time-series-with-python-d4847d6c099f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A hands-on article on detecting outliers in time series data using Python and
    sklearn
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcopeixeiro?source=post_page-----d4847d6c099f--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----d4847d6c099f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d4847d6c099f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d4847d6c099f--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----d4847d6c099f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d4847d6c099f--------------------------------)
    ·13 min read·Mar 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b3f1bd78c54730bd401ce05e9b15b28.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Will Myers](https://unsplash.com/@will_myers?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection is a task in which we want to identify rare events that deviate
    significantly from the majority of the data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection in time series has a wide range of real-life applications,
    from manufacturing to healthcare. Anomalies indicate unexpected events, and they
    can be caused by production faults or system defects. For example, if we are monitoring
    the number of visitors on a website, and the number falls to 0, it might mean
    that the server is down.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: It is also useful to detect anomalies in time series data before modelling for
    forecasting. Many forecasting models are autoregressive, meaning that they take
    into account past values to make predictions. A past outlier will definitely affect
    the model, and it can be a good idea to remove that outlier to get more reasonable
    predictions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will take a look a three different anomaly detection techniques,
    and implement them in Python.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute deviation (MAD)
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Isolation forest
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Local outlier factor (LOF)
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first one is a baseline method that can work well if the series satisfies
    certain assumptions. The other two methods are machine learning approaches.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '***Learn the latest time series analysis techniques with my*** [***free time
    series cheat sheet***](https://www.datasciencewithmarco.com/pl/2147608294) ***in
    Python! Get the implementation of statistical and deep learning techniques, all
    in Python and TensorFlow!***'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s get started!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Types of anomaly detection tasks in time series
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two main types of anomaly detection tasks with time series data:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Point-wise anomaly detection
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pattern-wise anomaly detection
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first type, we wish to find single points in time that are considered
    abnormal. For example, a fraudulent transaction is a point-wise anomaly.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: The second type is interested in finding subsequences that are outliers. An
    example of that could be a stock that is trading at an abnormal level for many
    hours or days.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will focus only on point-wise anomaly detection, meaning
    that our outliers are isolated points in time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'The scenario: CPU utilization on the AWS cloud'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We apply the different anomaly detection techniques on a dataset that monitors
    the CPU utilization on an EC2 instance in the AWS cloud. This is real-world data
    that was recorded every 5 minutes, starting on February 14th, 2014 at 14:30\.
    The dataset contains 4032 data points. It was made available through the [Numenta
    Anomaly Benchmark (NAB)](https://github.com/numenta/NAB) under the AGPL-3.0 license.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The particular dataset for this article can be found [here](https://github.com/numenta/NAB/blob/master/data/realAWSCloudwatch/ec2_cpu_utilization_24ae8d.csv),
    and the associated labels are [here](https://github.com/numenta/NAB/blob/master/labels/combined_labels.json).
    Full source code is available on [GitHub](https://github.com/marcopeix/datasciencewithmarco/blob/master/time_series_anomaly_detection.ipynb).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Before we get started, we need to format our data in order to label each value
    as either an outlier or an inlier.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, an outlier gets a label of -1, and an inlier gets a label of 1\. This matches
    the output of anomaly detection algorithms in scikit-learn.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/48b39a86bf1504a0c73dd2b1e7226575.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Formatted data with the timestamp, the value, and a label to determine if is
    it an outlier (-1) or an inlier (1). Image by the author.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have a dataset with the right timestamp format, the value,
    and a label to indicated whether the value is an outlier (-1) or an inlier (1).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s plot the data to visualize the anomalies.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/8803146e1e998c3319afffb1cf83ca5d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: Monitoring CPU usage on an EC2 instance. The two red dots indicated anomalous
    points, while the other blue dots are considered normal. Image by the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can see that our data only contains two outliers,
    as indicated by the red dots.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: This shows how challenging anomaly detection can be! Because they are rare events,
    we have few occasions to learn from them. In this case, only 2 points are outliers,
    which represent 0.05% of the data. It also makes the evaluation of the models
    more challenging. A method basically has two occasions of getting it right, and
    4030 occasions of being wrong.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: With all that in mind, let’s apply some techniques for anomaly detection in
    time series, starting with the mean absolute deviation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute deviation (MAD)
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If our data is normally distributed, we can reasonably say that data points
    at each end of the tails can be considered an outlier.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: To identify them, we can use the Z-score, which is a measurement in terms of
    standard deviations from the mean. If the Z-score is 0, the value is equal to
    the mean. Typically, we set a Z-score threshold of 3 or 3.5 to indicate if a value
    is an outlier or not.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Now, recall that the Z-score is calculated as
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87d492189d18a7d3083679f6d2eeeed6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Equation for the Z-score. Image by the author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Where *mu* is the mean of the sample and *sigma* is the standard deviation.
    Basically, if the Z-score is large, it means that the value is far from the mean
    and towards one end of the distribution’s tail, which in turn can mean that it
    is an outlier.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de982122e3febefb67266f9215c7f351.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: A normal distribution with the Z-score. We can see that when the Z-score is
    3, we reach the tails of the distribution, and so we can say that beyond that
    threshold, the data are outliers. Image by the author.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can visualize the classical Z-score threshold of 3
    to determine if a value is an outlier or not. As shown by the black dashed lines,
    a Z-score of 3 brings us to the ends of the normal distribution. So, any value
    with a Z-score greater than 3 (or less than -3 if we are not working in absolute
    values) can be labelled as an outlier.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Now, this works great under the assumption that we have a perfectly normal distribution,
    but the presence of outliers necessarily affects the mean, which in turns affect
    the Z-score. Therefore, we turn our attention to the median absolute deviation
    or MAD.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The robust Z-score method
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To avoid the influence of outliers on the Z-score, instead use the median, which
    is a more robust metric in the presence of outliers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'The median absolute deviation or MAD is defined as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d608cd3d231b2bdc8fe5375a6663d86.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: The MAD is the median of the absolute difference between a value and the median
    of the sample. Image by the author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, the MAD is the median of the absolute difference between the values
    of a sample and the median of the sample. Then, we can calculate the robust Z-score
    with:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/484d48fce120d5619883d7d44cc8958f.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: Robust Z-score formula. Note that 0.6745 is the 75th percentile of the standard
    normal distribution to which the MAD converges to. Image by the author.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Here, the robust Z-score takes the difference between a value and the median
    of the sample, multiplies it by 0.6745 and we divide everything by the MAD. Note
    that 0.6745 represents the 75th percentile of a standard normal distribution.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '**Why 0.6745? (optional read)**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the traditional Z-score, the robust Z-score uses the median absolute
    deviation, which is always smaller than the standard deviation. Thus, to obtain
    a value that resembles a Z-score, we must scale it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: In a normal distribution with no outliers, the MAD is about 2/3 (0.6745 to be
    precise) as big as the standard deviation. Therefore, because we are dividing
    by the MAD, we multiply by 0.6745 to get back to the scale of the normal Z-score.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'The robust Z-score method will work best under two important assumptions:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: The data is close to a normal distribution
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The MAD is not equal to 0 (happens when more than 50% of the data has the same
    value)
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second point is interesting, because if that is the case, then any value
    that is not equal to the median will be flagged as an outlier, no matter the threshold,
    since the robust Z-score will be incredibly large.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: With all that in mind, let’s apply this method to our scenario.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Applying the MAD for outlier detection
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we need to check the distribution of our data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/e216f583307e317516864b7cb9e47561.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: Distribution of our data. Already, we see that we do not have a normal distribution!
    Even worse, a lot of data points fall right on the median (black dashed line),
    meaning that the MAD is either 0 or very close to 0\. Image by the author.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can already see two problems. First, the data is close
    to a normal distribution. Second, the black dashed line indicates the median of
    the sample, and it fall right on the peak of the distribution. This means that
    a lot of data points are equal to the median, meaning that we are in a situation
    where the MAD is potentially 0 or very close to 0.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, let’s continue applying the method, just so that we understand
    how to work with it.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The next step, is to compute the MAD and the median of the sample to calculate
    the robust Z-score. The *scipy* package comes with an implementation of the MAD
    formula.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then, we simply write a function to calculate the robust Z-score and create
    a new column to store the score.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that we get a MAD of 0.002, which definitely close to 0, meaning that this
    baseline is likely not going to perform very well.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Once this is done, we decide on a threshold to flag outliers. A typical threshold
    is 3 or 3.5\. In this case, any value with a robust Z-score greater than 3.5 (right-hand
    tail) or smaller than -3.5 (left-hand tail) will be flagged as an outlier.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finally, we can plot a confusion matrix to see if our baseline correctly identified
    outliers and inliers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/d6bffc49f5cd8ef27ecc24323f0be5d1.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix of the baseline outlier detection method. Clearly, a lot of
    inliers were flagged as anomalies, which is expected since our data did not respect
    the assumptions of the MAD method. Image by the author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly, we see that the baseline method performs poorly, since 1066
    inliers were flagged as outliers. Again, this was expected since our data did
    not respect the assumptions of the method, and the MAD was very close to 0\. Still,
    I wanted to cover the implementation of this method in case it serves you in another
    scenario.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Although the results are disappointing, this method still holds when the assumptions
    are true for your dataset, and you now know how to apply it when it makes sense.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move on to the machine learning approached, starting with isolation
    forest.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Isolation Forest
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The isolation forest algorithm is a tree-based algorithm that is often used
    for anomaly detection.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm starts by randomly selecting an attribute and randomly selecting
    a split value between the maximum and minimum values for that attribute. This
    partitioning is done many times until the algorithm has isolated each point in
    the dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Then, the intuition behind this algorithm is that an outlier will take fewer
    partitions to be isolated than a normal point, as shown in the figures below.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27a7c0261401deb2477d19830352ec9b.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Isolating an inlier. Notice how the data has to be partitioned many times before
    the point is isolated. Image by Sai Borrelli — [Wikipedia](https://commons.wikimedia.org/w/index.php?curid=82709489)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d456164b1a2801e575d9499036d1997f.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: Isolating an outlier. Now, we see that less partitions were required to isolate
    it. Therefore, it it likely an anomaly. Image by Sai Borrelli — [Wikipedia](https://commons.wikimedia.org/w/index.php?curid=82709491)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: In the two figures above, we can see how the number of partitions differ when
    isolating an inlier and an outlier. In the top figure, isolating an inlier required
    many splits. In the bottom figure, fewer splits were necessary to isolate the
    point. Therefore, it is likely to be an anomaly.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: So we see how in an isolation forest, if the path to isolate a data point is
    short, then it is an anomaly!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Applying the isolation forest
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let’s split our data into a training and a test set. That way, we can
    evaluate if the model is able to flag an anomaly on unseen data. This is sometimes
    called novelty detection, instead of anomaly detection.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Then, we can train our isolation forest algorithm. Here, we need to specify
    a level of contamination, which is simply the fraction of outliers in the training
    data. In this case, we only have one outlier in the training set.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once trained, we can then generate predictions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Again, we can plot the confusion matrix to see how the model performs.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/b7a0966b20d32421c0fb468077a81ee7.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix of the isolation forest algorithm. Here, we can see that the
    algorithm did not flag any of the anomalies. It also mislabelled an anomaly as
    a normal point. Image by the author.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we notice that the algorithm was not able to flag the
    new anomaly. It also labelled an anomaly as a normal point.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Again, this is a disappointing result, but we still have one more method to
    cover, which is the local outlier factor.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Local outlier factor
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Intuitively, the local outlier factor (LOF) works by comparing the local density
    of a point to the local densities of its neighbours. If the densities of the point
    and its neighbours are similar, then the point is an inlier. However, if the density
    of the point is much smaller than the densities of its neighbours, then it must
    be an outlier, because a lower density means that the point is more isolated.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we need to set the number of neighbours to look at, and *scikit-learn*’s
    default parameter is 20, which works well in most cases.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Once the number of neighbours is set, we calculate the *reachability distance.*
    It is a bit tricky to explain this with words and pictures only, but I will do
    my best.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/44737a3754b4972abba7ac5125c01aa0.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: Visualizing the reachability distance, Image by the author.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we are studying point A, and that we set the number of neighbours
    to 3 (k=3). Drawing a circle while keeping point A in the middle results in the
    black dotted circle you see in the figure above. Points B, C and D are the three
    closest neighbours to A, and point E is too far in this case, so it is ignored.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the reachability distance is defined as:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69369779b5f660ab492552f1d7bba59f.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Reachability equation. Image by the author.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: In words, the reachability distance from A to B is the largest value between
    the k-distance of B and the actual distance from A to B.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The k-distance of B is simply the distance from point B to its third nearest
    neighbour. That’s why in the figure above, we drew a blue dotted circle with B
    at its center, to realize that the distance from B to C is the k-distance of B.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Once the reachability distance is calculated for all the k-nearest neighbours
    of A, the local reachability density will be computed. This is simply the inverse
    of the average of the reachability distances.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, the reachability density tells us how far we have to travel to
    reach a neighbouring point. If the density is large, then points are close together
    and we don’t have too travel for long.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the local outlier factor, is simply a ratio of the local reachability
    densities. In the figure above, we set *k* to 3, so we would have three ratios
    that we would average. This allows us to compare the local density of a point
    to its neighbour.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, if that factor is close to 1 or smaller than 1, then it
    is a normal point. If it is much larger than 1, then it is an outlier.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this methods comes with drawbacks, as a value that is larger than
    1 is not a perfect threshold. For example, an LOF of 1.1 could mean an outlier
    for a dataset, and not for another one.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Applying the local outlier factor method
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the use of *scikit-lean* applying the local outlier factor method is straightforward.
    We use the same train/test split as for the isolation forest to have comparable
    results.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Then, we can generate the predictions to flag potential outliers in the test
    set.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Finally, we plot the confusion matrix to evaluate the performance.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/203bb5e4b53dfff6cab3f7fdd6761e4d.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix for local outlier factor. We see that the algorithm successfully
    identified the only outlier in the test set. Image by the author.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above we see that the LOF method was able to flag the only outlier
    in the test set, and correctly labelled every other point as a normal point.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: As always, this does not mean that local outlier factor is better method than
    isolation forest. It simply means that it worked better in this particular situation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，这并不意味着局部异常因子比孤立森林方法更好。这只是表示在这个特定情况下，局部异常因子效果更好。
- en: Conclusion
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we explored three different methods for outlier detection in
    time series data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了三种不同的时间序列数据异常检测方法。
- en: First, we explored a robust Z-score that uses the mean absolute deviation (MAD).
    This works well when your data is normally distributed and if your MAD is not
    0.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们探讨了一种使用平均绝对偏差（MAD）的强健 Z-score。这在数据呈正态分布且 MAD 不为 0 时效果良好。
- en: Then, we looked at isolation forest, which a machine learning algorithm that
    determines how many times a dataset has to be partitioned in order to isolate
    a single point. If few partitions are necessary, then the point is an outlier.
    If many partitions are required, then the point is likely an inlier.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们看了孤立森林方法，这是一种机器学习算法，它确定数据集需要多少次分割才能孤立一个点。如果需要的分割次数很少，那么这个点就是一个异常点。如果需要很多分割，那么这个点很可能是内点。
- en: Finally, we looked at the local outlier factor (LOF) method, which is an unsupervised
    learning method that compares the local density of a point to that of its neighbours.
    Basically, if the density of a point is small compared to its neighbours, it means
    it is an isolated point, and likely an outlier.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看了局部异常因子（LOF）方法，这是一种无监督学习方法，它将一个点的局部密度与其邻居的密度进行比较。基本上，如果一个点的密度相对于其邻居较小，这意味着它是一个孤立点，很可能是异常点。
- en: I hope that you enjoyed this article and learned something new!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢这篇文章，并学到了新知识！
- en: Interested in mastering time series forecasting? Check out [Applied Time Series
    Forecasting in Python](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10),
    the only course that covers statistical, deep learning, and state-of-the-art models,
    in 100% Python.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 想要掌握时间序列预测吗？查看[Python中的应用时间序列预测](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10)，这是唯一涵盖统计学、深度学习和最先进模型的100%
    Python课程。
- en: Cheers 🍻
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 干杯 🍻
- en: Support me
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持我
- en: Enjoying my work? Show your support with [Buy me a coffee](http://buymeacoffee.com/dswm),
    a simple way for you to encourage me, and I get to enjoy a cup of coffee! If you
    feel like it, just click the button below 👇
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 享受我的工作吗？通过[请我喝咖啡](http://buymeacoffee.com/dswm)来支持我，这是一种简单的方式来鼓励我，而我也可以享受一杯咖啡！如果你愿意，点击下面的按钮
    👇
- en: '[![](../Images/7ad9438bd50b1698fdd722fa6661b16c.png)](http://buymeacoffee.com/dswm)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/7ad9438bd50b1698fdd722fa6661b16c.png)](http://buymeacoffee.com/dswm)'
