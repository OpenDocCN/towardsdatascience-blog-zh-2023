["```py\ndef detect_document_type(document_path):\n\n    guess_file = guess(document_path)\n    file_type = \"\"\n    image_types = ['jpg', 'jpeg', 'png', 'gif']\n\n    if(guess_file.extension.lower() == \"pdf\"):\n        file_type = \"pdf\"\n\n    elif(guess_file.extension.lower() in image_types):\n        file_type = \"image\"\n\n    else:\n        file_type = \"unkown\"\n\n    return file_type\n```", "```py\nresearch_paper_path = \"./data/transformer_paper.pdf\"\narticle_information_path = \"./data/zoumana_article_information.png\"\n\nprint(f\"Research Paper Type: {detect_document_type(research_paper_path)}\")\nprint(f\"Article Information Document Type: {detect_document_type(article_information_path)}\")\n```", "```py\nfrom langchain.document_loaders.image import UnstructuredImageLoader\nfrom langchain.document_loaders import UnstructuredFileLoader\n\ndef extract_file_content(file_path):\n\n    file_type = detect_document_type(file_path)\n\n    if(file_type == \"pdf\"):\n        loader = UnstructuredFileLoader(file_path)\n\n    elif(file_type == \"image\"):\n        loader = UnstructuredImageLoader(file_path)\n\n    documents = loader.load()\n    documents_content = '\\n'.join(doc.page_content for doc in documents)\n\n    return documents_content\n```", "```py\nresearch_paper_content = extract_file_content(research_paper_path)\narticle_information_content = extract_file_content(article_information_path)\n\nnb_characters = 400\n\nprint(f\"First {nb_characters} Characters of the Paper: \\n{research_paper_content[:nb_characters]} ...\")\nprint(\"---\"*5)\nprint(f\"First {nb_characters} Characters of Article Information Document :\\n {research_paper_content[:nb_characters]} ...\")\n```", "```py\ntext_splitter = CharacterTextSplitter(        \n    separator = \"\\n\\n\",\n    chunk_size = 1000,\n    chunk_overlap  = 200,\n    length_function = len,\n)\n```", "```py\nresearch_paper_chunks = text_splitter.split_text(research_paper_content)\narticle_information_chunks = text_splitter.split_text(article_information_content)\n\nprint(f\"# Chunks in Research Paper: {len(research_paper_chunks)}\")\nprint(f\"# Chunks in Article Document: {len(article_information_chunks)}\")\n```", "```py\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"<YOUR_KEY>\"\nembeddings = OpenAIEmbeddings()\n```", "```py\nfrom langchain.vectorstores import FAISS\n\ndef get_doc_search(text_splitter):\n\n    return FAISS.from_texts(text_splitter, embeddings)\n```", "```py\ndoc_search_paper = get_doc_search(research_paper_chunks)\nprint(doc_search_paper)\n```", "```py\nfrom langchain.llms import OpenAI\nfrom langchain.chains.question_answering import load_qa_chain\nchain = load_qa_chain(OpenAI(), chain_type = \"map_rerank\",  \n                      return_intermediate_steps=True)\n\ndef chat_with_file(file_path, query):\n\n    file_content = extract_file_content(file_path)\n    text_splitter = text_splitter.split_text(file_content)\n\n    document_search = get_doc_search(text_splitter)\n    documents = document_search.similarity_search(query)\n\n    results = chain({\n                        \"input_documents\":documents, \n                        \"question\": query\n                    }, \n                    return_only_outputs=True)\n    answers = results['intermediate_steps'][0]\n\n    return answers\n```", "```py\nquery = \"What is the document about\"\n\nresults = chat_with_file(article_information_path, query)\n\nanswer = results[\"answer\"]\nconfidence_score = results[\"score\"]\n\nprint(f\"Answer: {answer}\\n\\nConfidence Score: {confidence_score}\")\n```", "```py\nquery = \"Why is the self-attention approach used in this document?\"\n\nresults = chat_with_file(research_paper_path, query)\n\nanswer = results[\"answer\"]\nconfidence_score = results[\"score\"]\n\nprint(f\"Answer: {answer}\\n\\nConfidence Score: {confidence_score}\")\n```"]