- en: Gradient Descent Algorithm 101
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gradient-descent-algorithm-101-c226c69d756c](https://towardsdatascience.com/gradient-descent-algorithm-101-c226c69d756c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Beginner-friendly guide
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understand the optimization algorithm widely used in Machine and Deep Learning
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://polmarin.medium.com/?source=post_page-----c226c69d756c--------------------------------)[![Pol
    Marin](../Images/a4f69a96717d453db9791f27b8f85e86.png)](https://polmarin.medium.com/?source=post_page-----c226c69d756c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c226c69d756c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c226c69d756c--------------------------------)
    [Pol Marin](https://polmarin.medium.com/?source=post_page-----c226c69d756c--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c226c69d756c--------------------------------)
    ·6 min read·Apr 25, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a718894153ffc2de3d1b8a5ecdc19786.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: The slope of a mountain — Photo by [Ralph (Ravi) Kayden](https://unsplash.com/fr/@ralphkayden?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are a drop of water on top of a mountain, and your goal is to get
    to the lake situated right at the base of the mountain. That tall mountain has
    different slopes and obstacles, so going down following a straight line might
    not be the best solution. How would you approach this problem? The best solution
    would arguably be taking little steps, one at a time, always heading toward the
    direction that brings you closer to your end goal.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent (GD) is the algorithm that does just that, and it is essential
    for any data scientist to understand. It’s basic and rather simple but crucial,
    and anyone willing to enter the field should be able to explain what it is.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In this post, my goal is to make a complete and beginner-friendly guide to make
    everyone understand what GD is, what’s it used for, how it works, and mention
    different variations of it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: As always, you’ll find the *resources* section at the end of the post.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: But first things first.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using Wikipedia’s definition[1], **Gradient descent is a first-order iterative
    optimization algorithm for finding a local minimum of a differentiable function**.
    Even though it’s surely not the most effective method, it’s commonly used in Machine
    Learning and Deep Learning, especially in Neural Networks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: It’s basically used to minimize the value of a function by updating a set of
    parameters on each iteration. Mathematically speaking, it uses the derivative
    (gradient) to gradually decrease (descent) its value.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'But there’s a catch: **not all functions are optimizable**. We require a function
    — either uni or multivariate — that’s **differentiable**, which means derivatives
    exist at each point in the function’s domain, and **convex** (U-shape or similar).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但有一个问题：**并非所有函数都是可优化的**。我们需要一个函数——无论是单变量还是多变量——**可微分**，即函数定义域中的每一点都有导数，并且**凸**（U形或类似）。
- en: Now, after this simple introduction, we can start digging a little bit deeper
    into the math behind it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在这个简单的介绍之后，我们可以开始深入探讨其背后的数学。
- en: Practical Case
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际案例
- en: Because all gets clearer when going beyond the theory, let’s use real numbers
    and values to understand what it does.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因为超越理论会更清晰，让我们使用实际的数字和数值来理解它的作用。
- en: Let’s use a common data science case in which we want to develop a regression
    model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个常见的数据科学案例，我们希望开发一个回归模型。
- en: 'Disclaimer: I have totally invented this and there’s no logical reasoning behind
    using these functions, all came randomly. The goal is to show the process itself.'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 免责声明：我完全发明了这个，没有逻辑依据来使用这些函数，所有内容都是随机的。目标是展示这个过程本身。
- en: 'The cost function or loss function in any data science problem is the function
    we want to optimize. As we’re using regression, we’re going to use this one:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何数据科学问题中，成本函数或损失函数是我们要优化的函数。由于我们在使用回归，所以我们将使用这个：
- en: '![](../Images/a6a4e52524b004d07368220bd2def7af.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6a4e52524b004d07368220bd2def7af.png)'
- en: Random regression function — Image by the author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 随机回归函数 — 图片由作者提供
- en: 'The goal is to find the optimal minimum of f(x,y). Let me plot what it looks
    like:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是找到f(x,y)的最优最小值。让我绘制一下它的样子：
- en: '![](../Images/1c52d8ed9fb74c38f1f10e7f86150157.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c52d8ed9fb74c38f1f10e7f86150157.png)'
- en: f(x,y) plotted with a=1 — Image by the author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: f(x,y)绘制a=1 — 图片由作者提供
- en: 'Now our goal is to get the proper values for “x” and “y” that let us find the
    optimal values of this cost function. We can already see it graphically:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的目标是找到“x”和“y”的合适值，以便找到这个成本函数的最优值。我们已经可以从图形上看到它：
- en: y=0
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: y=0
- en: x being either -1 or 1
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x为-1或1
- en: Onto the GD itself, because we want to make our machine learn to do the same.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 进入GD本身，因为我们希望让我们的机器学会做相同的事情。
- en: The Algorithm
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法
- en: As said, gradient descent is an iterative process in which we compute the gradient
    and move in the opposite direction. The reasoning behind this is that the gradient
    of a function is used to determine the slope of that function. As we want to move
    down, not up, then we move in the opposite way.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，梯度下降是一个迭代过程，我们计算梯度并向相反方向移动。这样做的理由是，函数的梯度用于确定函数的斜率。由于我们想向下移动，而不是向上移动，所以我们朝相反的方向移动。
- en: 'It’s a simple process in which we update x and y in each iteration, by following
    the next approach:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的过程，在每次迭代中我们更新x和y，按照以下方法进行：
- en: '![](../Images/453e12771a46cd3bfc0508f7eb728401.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/453e12771a46cd3bfc0508f7eb728401.png)'
- en: Parameter update in gradient descent — Image by the author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降中的参数更新 — 图片由作者提供
- en: 'Explained in words, at iteration k:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 用语言解释，在第k次迭代时：
- en: Compute the gradient using the values of x and y at that iteration.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用x和y在该迭代中的值计算梯度。
- en: For each of those variables — x and y — multiply its gradient times lambda (𝜆),
    which is a float number called the learning rate.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个变量——x和y——将其梯度乘以lambda (𝜆)，这是一个称为学习率的浮点数。
- en: Remove from x and y respectively the computed values in step 2.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从x和y中分别移除第2步中计算出的值。
- en: Make x and y have the new value in the next iteration.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一次迭代中，让x和y具有新的值。
- en: This process is then repeated until a certain condition is met (that’s not important
    today). Once that happens, the training finishes and the optimization does too.
    We are (or should be) at a minimum (either local or global).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会重复进行，直到满足某个条件（今天不重要）。一旦满足条件，训练结束，优化也随之结束。我们（或者应该）达到了一个最小值（无论是局部还是全局）。
- en: Now, let’s put this theory into practice.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这个理论付诸实践。
- en: 'The first thing we need to do is compute the gradient of f(x,y). The gradient
    corresponds to a vector of partial derivatives:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是计算f(x,y)的梯度。梯度对应于一个偏导数的向量：
- en: '![](../Images/f8467d1bff5f82c281fd7c44e1994644.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8467d1bff5f82c281fd7c44e1994644.png)'
- en: Gradient of f(x,y) — Image by the author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: f(x,y)的梯度 — 图片由作者提供
- en: Now, using Python, all I’m going to do is create a loop that iteratively computes
    the gradient — using the corresponding x and y — and updates these parameters
    as specified above.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用Python，我要做的就是创建一个循环，迭代计算梯度——使用相应的x和y——并按照上面指定的方式更新这些参数。
- en: 'Before that, I’ll define two more values:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，我将定义两个额外的值：
- en: The learning rate (𝜆) can be fixed or mobile. For this simple tutorial, it’ll
    be 0.01.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率 (𝜆) 可以是固定的也可以是可变的。对于这个简单的教程，它将设置为 0.01。
- en: I’ll also use a value called eps (epsilon) to determine when to finish iterating.
    Once both partial derivatives are below this threshold, the gradient descent will
    stop. I’m setting it to 0.0001.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我还会使用一个叫做 eps（epsilon）的值来决定何时停止迭代。一旦两个偏导数都低于这个阈值，梯度下降将停止。我将其设置为 0.0001。
- en: 'Now, let’s do some code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来写一些代码：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of a random iteration was:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一次随机迭代的结果是：
- en: '![](../Images/75c600dbd1be375066b2f6553410f166.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75c600dbd1be375066b2f6553410f166.png)'
- en: Sample GD output — Image by the author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 GD 输出 — 图片来源于作者
- en: We can see these values are pretty close to x=1 and y=0, which were indeed function
    minimums.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这些值接近 x=1 和 y=0，它们确实是函数的最小值。
- en: One thing I forgot to mention was the x and y initializations. I chose to randomly
    generate a number within random ranges. In real-world problems, using more time
    to think about this is always required. Same with the learning rate, the stopping
    condition, and many other hyperparameters.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我忘了提到的是 x 和 y 的初始化。我选择了在随机范围内生成一个数字。在实际问题中，通常需要更多的时间来考虑这些问题。学习率、停止条件以及许多其他超参数也是如此。
- en: But for our case, this was more than enough.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于我们的情况，这已经足够了。
- en: Variations of Gradient Descent
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降法的变体
- en: I’m sure you now understand the basic algorithm. However, multiple versions
    of it are being used out there and I think some of those are worth being mentioned.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信你现在已经理解了基本算法。然而，市面上存在多个版本，我认为其中一些值得一提。
- en: '**Stochastic Gradient Descent (SGD)**. SGD is the variation that randomly picks
    one data point from the whole dataset at each iteration. This reduces the number
    of computations but it obviously has its downsides like, for example, not being
    able to converge to the global minimum.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降法 (SGD)**。SGD 是一种变体，在每次迭代时随机选择一个数据点。这减少了计算次数，但显然有其缺点，例如，可能无法收敛到全局最小值。'
- en: '**Batch Gradient Descent (BGD)**. BGD uses the whole dataset in each iteration.
    This isn’t fully desired for big datasets as can be computationally expensive
    and slow but, on the other hand, the convergence to the global minimum is theoretically
    guaranteed.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量梯度下降法 (BGD)**。BGD 在每次迭代时使用整个数据集。这对于大型数据集来说并不完全理想，因为计算开销和速度较慢，但另一方面，理论上保证收敛到全局最小值。'
- en: '**Mini-Batch Gradient Descent (MBGD)**. This can be considered a middle point
    between SGD and BGD. It does not use one data point at the time nor the whole
    dataset, but a subset of it. On each iteration, we pick a random number of samples
    (previously defined) and perform the gradient descent using only those.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迷你批量梯度下降法 (MBGD)**。这可以被视为 SGD 和 BGD 之间的中间点。它既不使用单个数据点，也不使用整个数据集，而是一个子集。在每次迭代中，我们随机选择一定数量的样本（之前定义过）并仅使用这些样本进行梯度下降。'
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: The Gradient Descent algorithm is widely used in machine and deep learning,
    but in other areas as well. That’s why understanding it is a must for everyone
    willing to become a data scientist.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法在机器学习和深度学习中被广泛使用，但在其他领域也有应用。因此，理解它是任何希望成为数据科学家的人的必修课。
- en: I hope this post clarified what it is, what it does, and how it does it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章澄清了它是什么、它做了什么以及它是如何做到的。
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you’d like to support me further, consider subscribing to Medium’s Membership
    through the link you find below: it won’t cost you any extra penny but it’ll help
    me through this process. Thanks a lot!'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进一步支持我，请考虑通过下面的链接订阅 Medium 的会员：这不会额外花费你任何钱，但会帮助我完成这个过程。非常感谢！
- en: '[](https://medium.com/@polmarin/membership?source=post_page-----c226c69d756c--------------------------------)
    [## Join Medium with my referral link - Pol Marin'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@polmarin/membership?source=post_page-----c226c69d756c--------------------------------)
    [## 使用我的推荐链接加入 Medium - Pol Marin'
- en: Read every story from Pol Marin (and thousands of other writers on Medium).
    Your membership fee directly supports Pol…
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Pol Marin 的每一个故事（以及 Medium 上成千上万其他作家的故事）。您的会员费直接支持 Pol……
- en: medium.com](https://medium.com/@polmarin/membership?source=post_page-----c226c69d756c--------------------------------)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@polmarin/membership?source=post_page-----c226c69d756c--------------------------------)
- en: Resources
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[1] [Gradient descent — Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [梯度下降法 — 维基百科](https://en.wikipedia.org/wiki/Gradient_descent)'
