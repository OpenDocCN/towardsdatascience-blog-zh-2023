- en: Gradient Descent Algorithm 101
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/gradient-descent-algorithm-101-c226c69d756c](https://towardsdatascience.com/gradient-descent-algorithm-101-c226c69d756c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Beginner-friendly guide
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understand the optimization algorithm widely used in Machine and Deep Learning
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://polmarin.medium.com/?source=post_page-----c226c69d756c--------------------------------)[![Pol
    Marin](../Images/a4f69a96717d453db9791f27b8f85e86.png)](https://polmarin.medium.com/?source=post_page-----c226c69d756c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c226c69d756c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c226c69d756c--------------------------------)
    [Pol Marin](https://polmarin.medium.com/?source=post_page-----c226c69d756c--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c226c69d756c--------------------------------)
    Â·6 min readÂ·Apr 25, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a718894153ffc2de3d1b8a5ecdc19786.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: The slope of a mountain â€” Photo by [Ralph (Ravi) Kayden](https://unsplash.com/fr/@ralphkayden?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are a drop of water on top of a mountain, and your goal is to get
    to the lake situated right at the base of the mountain. That tall mountain has
    different slopes and obstacles, so going down following a straight line might
    not be the best solution. How would you approach this problem? The best solution
    would arguably be taking little steps, one at a time, always heading toward the
    direction that brings you closer to your end goal.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent (GD) is the algorithm that does just that, and it is essential
    for any data scientist to understand. Itâ€™s basic and rather simple but crucial,
    and anyone willing to enter the field should be able to explain what it is.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In this post, my goal is to make a complete and beginner-friendly guide to make
    everyone understand what GD is, whatâ€™s it used for, how it works, and mention
    different variations of it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: As always, youâ€™ll find the *resources* section at the end of the post.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: But first things first.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using Wikipediaâ€™s definition[1], **Gradient descent is a first-order iterative
    optimization algorithm for finding a local minimum of a differentiable function**.
    Even though itâ€™s surely not the most effective method, itâ€™s commonly used in Machine
    Learning and Deep Learning, especially in Neural Networks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Itâ€™s basically used to minimize the value of a function by updating a set of
    parameters on each iteration. Mathematically speaking, it uses the derivative
    (gradient) to gradually decrease (descent) its value.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'But thereâ€™s a catch: **not all functions are optimizable**. We require a function
    â€” either uni or multivariate â€” thatâ€™s **differentiable**, which means derivatives
    exist at each point in the functionâ€™s domain, and **convex** (U-shape or similar).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æœ‰ä¸€ä¸ªé—®é¢˜ï¼š**å¹¶éæ‰€æœ‰å‡½æ•°éƒ½æ˜¯å¯ä¼˜åŒ–çš„**ã€‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°â€”â€”æ— è®ºæ˜¯å•å˜é‡è¿˜æ˜¯å¤šå˜é‡â€”â€”**å¯å¾®åˆ†**ï¼Œå³å‡½æ•°å®šä¹‰åŸŸä¸­çš„æ¯ä¸€ç‚¹éƒ½æœ‰å¯¼æ•°ï¼Œå¹¶ä¸”**å‡¸**ï¼ˆUå½¢æˆ–ç±»ä¼¼ï¼‰ã€‚
- en: Now, after this simple introduction, we can start digging a little bit deeper
    into the math behind it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œåœ¨è¿™ä¸ªç®€å•çš„ä»‹ç»ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹æ·±å…¥æ¢è®¨å…¶èƒŒåçš„æ•°å­¦ã€‚
- en: Practical Case
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®é™…æ¡ˆä¾‹
- en: Because all gets clearer when going beyond the theory, letâ€™s use real numbers
    and values to understand what it does.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºè¶…è¶Šç†è®ºä¼šæ›´æ¸…æ™°ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨å®é™…çš„æ•°å­—å’Œæ•°å€¼æ¥ç†è§£å®ƒçš„ä½œç”¨ã€‚
- en: Letâ€™s use a common data science case in which we want to develop a regression
    model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå¸¸è§çš„æ•°æ®ç§‘å­¦æ¡ˆä¾‹ï¼Œæˆ‘ä»¬å¸Œæœ›å¼€å‘ä¸€ä¸ªå›å½’æ¨¡å‹ã€‚
- en: 'Disclaimer: I have totally invented this and thereâ€™s no logical reasoning behind
    using these functions, all came randomly. The goal is to show the process itself.'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å…è´£å£°æ˜ï¼šæˆ‘å®Œå…¨å‘æ˜äº†è¿™ä¸ªï¼Œæ²¡æœ‰é€»è¾‘ä¾æ®æ¥ä½¿ç”¨è¿™äº›å‡½æ•°ï¼Œæ‰€æœ‰å†…å®¹éƒ½æ˜¯éšæœºçš„ã€‚ç›®æ ‡æ˜¯å±•ç¤ºè¿™ä¸ªè¿‡ç¨‹æœ¬èº«ã€‚
- en: 'The cost function or loss function in any data science problem is the function
    we want to optimize. As weâ€™re using regression, weâ€™re going to use this one:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»»ä½•æ•°æ®ç§‘å­¦é—®é¢˜ä¸­ï¼Œæˆæœ¬å‡½æ•°æˆ–æŸå¤±å‡½æ•°æ˜¯æˆ‘ä»¬è¦ä¼˜åŒ–çš„å‡½æ•°ã€‚ç”±äºæˆ‘ä»¬åœ¨ä½¿ç”¨å›å½’ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªï¼š
- en: '![](../Images/a6a4e52524b004d07368220bd2def7af.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6a4e52524b004d07368220bd2def7af.png)'
- en: Random regression function â€” Image by the author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºå›å½’å‡½æ•° â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'The goal is to find the optimal minimum of f(x,y). Let me plot what it looks
    like:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯æ‰¾åˆ°f(x,y)çš„æœ€ä¼˜æœ€å°å€¼ã€‚è®©æˆ‘ç»˜åˆ¶ä¸€ä¸‹å®ƒçš„æ ·å­ï¼š
- en: '![](../Images/1c52d8ed9fb74c38f1f10e7f86150157.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c52d8ed9fb74c38f1f10e7f86150157.png)'
- en: f(x,y) plotted with a=1 â€” Image by the author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: f(x,y)ç»˜åˆ¶a=1 â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'Now our goal is to get the proper values for â€œxâ€ and â€œyâ€ that let us find the
    optimal values of this cost function. We can already see it graphically:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°â€œxâ€å’Œâ€œyâ€çš„åˆé€‚å€¼ï¼Œä»¥ä¾¿æ‰¾åˆ°è¿™ä¸ªæˆæœ¬å‡½æ•°çš„æœ€ä¼˜å€¼ã€‚æˆ‘ä»¬å·²ç»å¯ä»¥ä»å›¾å½¢ä¸Šçœ‹åˆ°å®ƒï¼š
- en: y=0
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: y=0
- en: x being either -1 or 1
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: xä¸º-1æˆ–1
- en: Onto the GD itself, because we want to make our machine learn to do the same.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿›å…¥GDæœ¬èº«ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›è®©æˆ‘ä»¬çš„æœºå™¨å­¦ä¼šåšç›¸åŒçš„äº‹æƒ…ã€‚
- en: The Algorithm
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç®—æ³•
- en: As said, gradient descent is an iterative process in which we compute the gradient
    and move in the opposite direction. The reasoning behind this is that the gradient
    of a function is used to determine the slope of that function. As we want to move
    down, not up, then we move in the opposite way.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œæ¢¯åº¦ä¸‹é™æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œæˆ‘ä»¬è®¡ç®—æ¢¯åº¦å¹¶å‘ç›¸åæ–¹å‘ç§»åŠ¨ã€‚è¿™æ ·åšçš„ç†ç”±æ˜¯ï¼Œå‡½æ•°çš„æ¢¯åº¦ç”¨äºç¡®å®šå‡½æ•°çš„æ–œç‡ã€‚ç”±äºæˆ‘ä»¬æƒ³å‘ä¸‹ç§»åŠ¨ï¼Œè€Œä¸æ˜¯å‘ä¸Šç§»åŠ¨ï¼Œæ‰€ä»¥æˆ‘ä»¬æœç›¸åçš„æ–¹å‘ç§»åŠ¨ã€‚
- en: 'Itâ€™s a simple process in which we update x and y in each iteration, by following
    the next approach:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç®€å•çš„è¿‡ç¨‹ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­æˆ‘ä»¬æ›´æ–°xå’Œyï¼ŒæŒ‰ç…§ä»¥ä¸‹æ–¹æ³•è¿›è¡Œï¼š
- en: '![](../Images/453e12771a46cd3bfc0508f7eb728401.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/453e12771a46cd3bfc0508f7eb728401.png)'
- en: Parameter update in gradient descent â€” Image by the author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ä¸­çš„å‚æ•°æ›´æ–° â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'Explained in words, at iteration k:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨è¯­è¨€è§£é‡Šï¼Œåœ¨ç¬¬kæ¬¡è¿­ä»£æ—¶ï¼š
- en: Compute the gradient using the values of x and y at that iteration.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨xå’Œyåœ¨è¯¥è¿­ä»£ä¸­çš„å€¼è®¡ç®—æ¢¯åº¦ã€‚
- en: For each of those variables â€” x and y â€” multiply its gradient times lambda (ğœ†),
    which is a float number called the learning rate.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªå˜é‡â€”â€”xå’Œyâ€”â€”å°†å…¶æ¢¯åº¦ä¹˜ä»¥lambda (ğœ†)ï¼Œè¿™æ˜¯ä¸€ä¸ªç§°ä¸ºå­¦ä¹ ç‡çš„æµ®ç‚¹æ•°ã€‚
- en: Remove from x and y respectively the computed values in step 2.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»xå’Œyä¸­åˆ†åˆ«ç§»é™¤ç¬¬2æ­¥ä¸­è®¡ç®—å‡ºçš„å€¼ã€‚
- en: Make x and y have the new value in the next iteration.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­ï¼Œè®©xå’Œyå…·æœ‰æ–°çš„å€¼ã€‚
- en: This process is then repeated until a certain condition is met (thatâ€™s not important
    today). Once that happens, the training finishes and the optimization does too.
    We are (or should be) at a minimum (either local or global).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤è¿›è¡Œï¼Œç›´åˆ°æ»¡è¶³æŸä¸ªæ¡ä»¶ï¼ˆä»Šå¤©ä¸é‡è¦ï¼‰ã€‚ä¸€æ—¦æ»¡è¶³æ¡ä»¶ï¼Œè®­ç»ƒç»“æŸï¼Œä¼˜åŒ–ä¹Ÿéšä¹‹ç»“æŸã€‚æˆ‘ä»¬ï¼ˆæˆ–è€…åº”è¯¥ï¼‰è¾¾åˆ°äº†ä¸€ä¸ªæœ€å°å€¼ï¼ˆæ— è®ºæ˜¯å±€éƒ¨è¿˜æ˜¯å…¨å±€ï¼‰ã€‚
- en: Now, letâ€™s put this theory into practice.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†è¿™ä¸ªç†è®ºä»˜è¯¸å®è·µã€‚
- en: 'The first thing we need to do is compute the gradient of f(x,y). The gradient
    corresponds to a vector of partial derivatives:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯è®¡ç®—f(x,y)çš„æ¢¯åº¦ã€‚æ¢¯åº¦å¯¹åº”äºä¸€ä¸ªåå¯¼æ•°çš„å‘é‡ï¼š
- en: '![](../Images/f8467d1bff5f82c281fd7c44e1994644.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8467d1bff5f82c281fd7c44e1994644.png)'
- en: Gradient of f(x,y) â€” Image by the author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: f(x,y)çš„æ¢¯åº¦ â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Now, using Python, all Iâ€™m going to do is create a loop that iteratively computes
    the gradient â€” using the corresponding x and y â€” and updates these parameters
    as specified above.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä½¿ç”¨Pythonï¼Œæˆ‘è¦åšçš„å°±æ˜¯åˆ›å»ºä¸€ä¸ªå¾ªç¯ï¼Œè¿­ä»£è®¡ç®—æ¢¯åº¦â€”â€”ä½¿ç”¨ç›¸åº”çš„xå’Œyâ€”â€”å¹¶æŒ‰ç…§ä¸Šé¢æŒ‡å®šçš„æ–¹å¼æ›´æ–°è¿™äº›å‚æ•°ã€‚
- en: 'Before that, Iâ€™ll define two more values:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘å°†å®šä¹‰ä¸¤ä¸ªé¢å¤–çš„å€¼ï¼š
- en: The learning rate (ğœ†) can be fixed or mobile. For this simple tutorial, itâ€™ll
    be 0.01.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡ (ğœ†) å¯ä»¥æ˜¯å›ºå®šçš„ä¹Ÿå¯ä»¥æ˜¯å¯å˜çš„ã€‚å¯¹äºè¿™ä¸ªç®€å•çš„æ•™ç¨‹ï¼Œå®ƒå°†è®¾ç½®ä¸º 0.01ã€‚
- en: Iâ€™ll also use a value called eps (epsilon) to determine when to finish iterating.
    Once both partial derivatives are below this threshold, the gradient descent will
    stop. Iâ€™m setting it to 0.0001.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜ä¼šä½¿ç”¨ä¸€ä¸ªå«åš epsï¼ˆepsilonï¼‰çš„å€¼æ¥å†³å®šä½•æ—¶åœæ­¢è¿­ä»£ã€‚ä¸€æ—¦ä¸¤ä¸ªåå¯¼æ•°éƒ½ä½äºè¿™ä¸ªé˜ˆå€¼ï¼Œæ¢¯åº¦ä¸‹é™å°†åœæ­¢ã€‚æˆ‘å°†å…¶è®¾ç½®ä¸º 0.0001ã€‚
- en: 'Now, letâ€™s do some code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥å†™ä¸€äº›ä»£ç ï¼š
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of a random iteration was:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ¬¡éšæœºè¿­ä»£çš„ç»“æœæ˜¯ï¼š
- en: '![](../Images/75c600dbd1be375066b2f6553410f166.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75c600dbd1be375066b2f6553410f166.png)'
- en: Sample GD output â€” Image by the author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ GD è¾“å‡º â€” å›¾ç‰‡æ¥æºäºä½œè€…
- en: We can see these values are pretty close to x=1 and y=0, which were indeed function
    minimums.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™äº›å€¼æ¥è¿‘ x=1 å’Œ y=0ï¼Œå®ƒä»¬ç¡®å®æ˜¯å‡½æ•°çš„æœ€å°å€¼ã€‚
- en: One thing I forgot to mention was the x and y initializations. I chose to randomly
    generate a number within random ranges. In real-world problems, using more time
    to think about this is always required. Same with the learning rate, the stopping
    condition, and many other hyperparameters.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¿˜äº†æåˆ°çš„æ˜¯ x å’Œ y çš„åˆå§‹åŒ–ã€‚æˆ‘é€‰æ‹©äº†åœ¨éšæœºèŒƒå›´å†…ç”Ÿæˆä¸€ä¸ªæ•°å­—ã€‚åœ¨å®é™…é—®é¢˜ä¸­ï¼Œé€šå¸¸éœ€è¦æ›´å¤šçš„æ—¶é—´æ¥è€ƒè™‘è¿™äº›é—®é¢˜ã€‚å­¦ä¹ ç‡ã€åœæ­¢æ¡ä»¶ä»¥åŠè®¸å¤šå…¶ä»–è¶…å‚æ•°ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: But for our case, this was more than enough.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¯¹äºæˆ‘ä»¬çš„æƒ…å†µï¼Œè¿™å·²ç»è¶³å¤Ÿäº†ã€‚
- en: Variations of Gradient Descent
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™æ³•çš„å˜ä½“
- en: Iâ€™m sure you now understand the basic algorithm. However, multiple versions
    of it are being used out there and I think some of those are worth being mentioned.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç›¸ä¿¡ä½ ç°åœ¨å·²ç»ç†è§£äº†åŸºæœ¬ç®—æ³•ã€‚ç„¶è€Œï¼Œå¸‚é¢ä¸Šå­˜åœ¨å¤šä¸ªç‰ˆæœ¬ï¼Œæˆ‘è®¤ä¸ºå…¶ä¸­ä¸€äº›å€¼å¾—ä¸€æã€‚
- en: '**Stochastic Gradient Descent (SGD)**. SGD is the variation that randomly picks
    one data point from the whole dataset at each iteration. This reduces the number
    of computations but it obviously has its downsides like, for example, not being
    able to converge to the global minimum.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éšæœºæ¢¯åº¦ä¸‹é™æ³• (SGD)**ã€‚SGD æ˜¯ä¸€ç§å˜ä½“ï¼Œåœ¨æ¯æ¬¡è¿­ä»£æ—¶éšæœºé€‰æ‹©ä¸€ä¸ªæ•°æ®ç‚¹ã€‚è¿™å‡å°‘äº†è®¡ç®—æ¬¡æ•°ï¼Œä½†æ˜¾ç„¶æœ‰å…¶ç¼ºç‚¹ï¼Œä¾‹å¦‚ï¼Œå¯èƒ½æ— æ³•æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼ã€‚'
- en: '**Batch Gradient Descent (BGD)**. BGD uses the whole dataset in each iteration.
    This isnâ€™t fully desired for big datasets as can be computationally expensive
    and slow but, on the other hand, the convergence to the global minimum is theoretically
    guaranteed.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³• (BGD)**ã€‚BGD åœ¨æ¯æ¬¡è¿­ä»£æ—¶ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†ã€‚è¿™å¯¹äºå¤§å‹æ•°æ®é›†æ¥è¯´å¹¶ä¸å®Œå…¨ç†æƒ³ï¼Œå› ä¸ºè®¡ç®—å¼€é”€å’Œé€Ÿåº¦è¾ƒæ…¢ï¼Œä½†å¦ä¸€æ–¹é¢ï¼Œç†è®ºä¸Šä¿è¯æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼ã€‚'
- en: '**Mini-Batch Gradient Descent (MBGD)**. This can be considered a middle point
    between SGD and BGD. It does not use one data point at the time nor the whole
    dataset, but a subset of it. On each iteration, we pick a random number of samples
    (previously defined) and perform the gradient descent using only those.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¿·ä½ æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³• (MBGD)**ã€‚è¿™å¯ä»¥è¢«è§†ä¸º SGD å’Œ BGD ä¹‹é—´çš„ä¸­é—´ç‚¹ã€‚å®ƒæ—¢ä¸ä½¿ç”¨å•ä¸ªæ•°æ®ç‚¹ï¼Œä¹Ÿä¸ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†ï¼Œè€Œæ˜¯ä¸€ä¸ªå­é›†ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬éšæœºé€‰æ‹©ä¸€å®šæ•°é‡çš„æ ·æœ¬ï¼ˆä¹‹å‰å®šä¹‰è¿‡ï¼‰å¹¶ä»…ä½¿ç”¨è¿™äº›æ ·æœ¬è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚'
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: The Gradient Descent algorithm is widely used in machine and deep learning,
    but in other areas as well. Thatâ€™s why understanding it is a must for everyone
    willing to become a data scientist.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ç®—æ³•åœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†åœ¨å…¶ä»–é¢†åŸŸä¹Ÿæœ‰åº”ç”¨ã€‚å› æ­¤ï¼Œç†è§£å®ƒæ˜¯ä»»ä½•å¸Œæœ›æˆä¸ºæ•°æ®ç§‘å­¦å®¶çš„äººçš„å¿…ä¿®è¯¾ã€‚
- en: I hope this post clarified what it is, what it does, and how it does it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« æ¾„æ¸…äº†å®ƒæ˜¯ä»€ä¹ˆã€å®ƒåšäº†ä»€ä¹ˆä»¥åŠå®ƒæ˜¯å¦‚ä½•åšåˆ°çš„ã€‚
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If youâ€™d like to support me further, consider subscribing to Mediumâ€™s Membership
    through the link you find below: it wonâ€™t cost you any extra penny but itâ€™ll help
    me through this process. Thanks a lot!'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³è¿›ä¸€æ­¥æ”¯æŒæˆ‘ï¼Œè¯·è€ƒè™‘é€šè¿‡ä¸‹é¢çš„é“¾æ¥è®¢é˜… Medium çš„ä¼šå‘˜ï¼šè¿™ä¸ä¼šé¢å¤–èŠ±è´¹ä½ ä»»ä½•é’±ï¼Œä½†ä¼šå¸®åŠ©æˆ‘å®Œæˆè¿™ä¸ªè¿‡ç¨‹ã€‚éå¸¸æ„Ÿè°¢ï¼
- en: '[](https://medium.com/@polmarin/membership?source=post_page-----c226c69d756c--------------------------------)
    [## Join Medium with my referral link - Pol Marin'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@polmarin/membership?source=post_page-----c226c69d756c--------------------------------)
    [## ä½¿ç”¨æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium - Pol Marin'
- en: Read every story from Pol Marin (and thousands of other writers on Medium).
    Your membership fee directly supports Polâ€¦
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é˜…è¯» Pol Marin çš„æ¯ä¸€ä¸ªæ•…äº‹ï¼ˆä»¥åŠ Medium ä¸Šæˆåƒä¸Šä¸‡å…¶ä»–ä½œå®¶çš„æ•…äº‹ï¼‰ã€‚æ‚¨çš„ä¼šå‘˜è´¹ç›´æ¥æ”¯æŒ Polâ€¦â€¦
- en: medium.com](https://medium.com/@polmarin/membership?source=post_page-----c226c69d756c--------------------------------)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@polmarin/membership?source=post_page-----c226c69d756c--------------------------------)
- en: Resources
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: '[1] [Gradient descent â€” Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [æ¢¯åº¦ä¸‹é™æ³• â€” ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Gradient_descent)'
