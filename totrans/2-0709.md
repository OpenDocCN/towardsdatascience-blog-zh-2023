# 揭秘DreamBooth：一种个性化文本到图像生成的新工具

> 原文：[https://towardsdatascience.com/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30](https://towardsdatascience.com/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30)

## 探索将无聊图像转化为创意杰作的技术

[](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)[![Mario Larcher](../Images/b5b443807fe06f096ed4fc5139b3cb42.png)](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------) [Mario Larcher](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)

·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------) ·阅读时间13分钟·2023年6月13日

--

![](../Images/cd417c1c9a70877d284ffb874caaea6a.png)

Dougie和他的新个性由作者使用DreamBooth创建。你能猜出提示是什么吗？

# 介绍

想象一下，你轻松地生成一张你心爱的幼犬在雅典卫城背景下的新图像的喜悦。如果还不满足，你还想看看梵高会如何绘制你的好友，或者他如果被狮子所构思会是什么样子😱！感谢DreamBooth，这一切都变为现实，如今可以让任何动物、物体或我们自己从一小堆图像中旅行于幻想世界。

尽管我们许多人已经在社交媒体上看到了利用这项技术可以取得的令人瞩目的成果，而且有大量教程可以让我们在自己的照片上进行尝试，但很少有人尝试回答这样一个问题：是的，那么它到底是如何工作的呢？

在本文中，我将尽力解析Ruiz等人发表的科学论文[DreamBooth: 针对主题驱动生成的文本到图像扩散模型微调](https://arxiv.org/abs/2208.12242)，这篇论文是所有这一切的起点。但别担心，我会简化复杂的部分，并在需要一些先验知识的地方进行解释。现在，请注意，这是一项高级话题，因此我假设你已经掌握了深度学习及相关内容的基础知识。如果你想深入了解扩散模型或其他有趣的话题，我会在过程中提供一些参考。让我们开始吧！

# 相关工作

![](../Images/5008a1303ab977b9e8dd3ce5fdd17410.png)

图7来自[DreamBooth: 针对主题驱动生成的文本到图像扩散模型微调](https://arxiv.org/abs/2208.12242)。

在我们深入探讨DreamBooth的方法之前，让我们先仔细了解一下与该技术相关的工作和任务。

## 图像合成

在日常生活的喧嚣中，你心爱的背包已经很久没有踏上环球之旅。现在是给它注入刺激冒险的时刻，同时你也在规划下一次假期。通过图像合成，将你的背包无缝融入新的背景，让它在几秒钟内从大峡谷到波士顿。

如果简单地复制粘贴主题不能满足你对新视角的渴望，可以尝试探索3D重建技术的应用。然而，需要注意的是，这些技术主要针对刚性物体，并且通常需要大量的起始视图。

DreamBooth引入了一项卓越的能力，可以在新的背景中生成新姿势，同时顺畅地融入关键元素，如光线、阴影和其他与场景相关的方面。实现这种一致性在以往的方法中一直是一个挑战。在论文中，这项任务也被称为重新背景化。

## 文本到图像编辑与合成

基于文本输入的图像编辑是许多照片编辑软件爱好者的一个秘密梦想。早期的方法，例如使用GANs的方法，展示了令人印象深刻的结果，但仅限于像编辑人脸这样结构良好的场景。

即使是利用扩散模型的新方法也有其局限性，通常仅限于全局编辑。直到最近，像[Text2LIVE](https://text2live.github.io/)这样的进展才允许局部编辑。然而，这些技术都无法在新的背景中生成特定的主题。

尽管像[Imagen](https://imagen.research.google/)、[DALL·E 2](https://openai.com/product/dall-e-2)和[Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)这样的文本图像合成模型取得了显著进展，但在合成图像中实现精细控制并保留主题身份仍然面临重大挑战。

## 可控生成模型

为了避免对主题进行修改，许多方法依赖于用户提供的掩码来限制修改的区域。逆转技术，如[DALL·E 2](https://openai.com/product/dall-e-2)使用的技术，提供了一个有效的解决方案，可以在修改背景的同时保留主题。

[Prompt-to-Prompt](https://prompt-to-prompt.github.io/)使得本地和全局编辑成为可能，无需输入掩码。

然而，这些方法在生成新样本时无法充分保留主题的身份。

尽管一些基于GAN的方法专注于生成实例变体，但它们往往有局限性。例如，它们主要设计用于面部领域，需要大量的输入样本，难以处理独特的主题，并且无法保留重要的主题细节。

最后，最近Gal等人提出了文本反演，这是一种具有DreamBooth共同特征的方法论，但正如我们将看到的，它受到基于其的冻结扩散模型表现力的限制。

![](../Images/d3075f2bc0ac3f085e9b3dd435e0f272.png)

图2来自[图像胜于千言：使用文本反演个性化文本到图像生成](https://arxiv.org/abs/2208.01618)。

由于这是作者用来与DreamBooth进行比较的工作，值得提供一个简要的描述。

**文本反演**从一个预训练的扩散模型开始，如潜在扩散模型，并定义一个新的占位符字符串S*，以表示需要学习的新概念。在此阶段，保持扩散模型冻结，新的嵌入从仅3-5张图像中进行微调，类似于DreamBooth。如果这个简要描述不够清楚，请等到你阅读更详细的DreamBooth描述时，它与这项工作有许多共同点。

# 方法

![](../Images/98f3ebfcca65a8f1de98a0d286677bf1.png)

图3来自[DreamBooth：为主题驱动生成微调文本到图像扩散模型](https://arxiv.org/abs/2208.12242)。

在详细描述**DreamBooth**的组件之前，让我们**简要**了解一下这项技术的工作原理：

1.  选择3-5张你喜欢的主题图像，可以是动物、物体，甚至是像艺术风格这样的抽象概念。

1.  将这个概念与一个稀有词汇关联，该词汇对应一个唯一的标记，将从现在开始表示它，在科学论文中，作者称这个词为[V]。

1.  使用兴趣主题的图像，通过简单的提示如“一个[V] [类别名]”来微调模型，例如，如果输入图像是你的狗的照片，则为“一个[V] 狗”。

1.  由于我们正在微调模型的所有参数，因此有风险在这个阶段所有的狗（或我们主题的任何类别）都会变成与我们的输入图像相同。为了避免模型的这种退化，我们从冻结的模型生成图像，使用像“狗”（或“[类别名]”）这样的提示，并添加一个损失函数，当我们为这个提示微调的模型生成的图像偏离冻结模型生成的图像时，会受到惩罚。

好的，现在我们对过程有了一个高层次的了解，让我们详细讨论各种组件。

## 文本到图像扩散模型

你真的想了解扩散模型的工作原理，尤其是像稳定扩散这样的潜在扩散模型吗？请阅读我之前的文章，当你读完后，我会在这里等你！

[## 论文解读——基于潜在扩散模型的高分辨率图像合成](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----70f8bb0cfa30--------------------------------)

### 虽然OpenAI凭借其生成文本模型主导了自然语言处理领域，但他们的图像…

[towardsdatascience.com](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----70f8bb0cfa30--------------------------------)

好吧，也许你不需要完整的解释，在这种情况下，我将提供**扩散模型**背后的直观理解，这非常简单。

![](../Images/13faf499b66a3c034aa4a875c8587d68.png)

图 2\. 来自 [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)。

1.  取一个图像x0，并添加一定量的噪声（例如，高斯噪声），噪声量与某个时间步* t *成比例。如果*t*为零，则添加的噪声为零，如果*t* > 0，则添加的噪声将与*t*的大小一样，直到你得到一个仅由噪声组成的图像。

1.  训练一个模型，如U-Net，通过将时间步* t *和受损图像作为输入来预测无噪声图像（或添加的噪声）。

1.  此时，经过训练一个可以去除图像噪声的模型，我们可以采样一个仅由噪声组成的图像，并逐渐去除噪声（一次性完成效果不好），可以通过预测无噪声的图像或预测噪声并从图像中减去来实现。

1.  前三点描述了无条件扩散模型。为了根据文本提示生成条件输出，文本使用像 [CLIP](https://openai.com/research/clip) 的模型进行编码，或者使用如 [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)、[T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) 等语言模型。这个编码步骤允许集成额外的信息，然后将其与受损图像和时间步* t *一起输入模型。

论文中的作者使用了两个扩散模型：Google的 [Imagen](https://imagen.research.google/)（DreamBooth 也来自 [Google Research](https://research.google/)）和 [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)来自 [Stability AI](https://stability.ai/)，这是主要的开源文本到图像模型。

**Imagen** 采用多分辨率策略来提高生成图像的质量。最初，使用低分辨率 64x64 图像训练扩散模型。然后，低分辨率模型的输出通过两个额外的扩散模型进行放大，这些模型在更高分辨率下操作，分别为 256x256 和 1024x1024。第一个模型专注于捕捉宏观细节，而随后的模型则通过利用较低分辨率模型生成图像的条件效应来精细化输出。这种迭代优化有助于生成高分辨率的图像，具有更好的质量和保真度。

**Stable Diffusion** 作为一种潜在扩散模型，采用三步法来提高训练和生成高分辨率图像的效率。最初，训练一个变分自编码器（VAE）以压缩高分辨率图像。从此之后，过程与标准扩散模型非常相似，一个关键区别在于：不是使用原始图像作为输入，而是使用由 VAE 编码器生成的潜在表示。随后，逆扩散过程的输出通过 VAE 解码器恢复到原始分辨率。为了更全面地理解整个过程，我在上述文章中进行了更详细的探讨。

## 文本到图像模型的个性化

DreamBooth 旨在将主题实例（例如你的狗）置于模型的输出领域内，使模型能够在查询时生成主题的新图像。扩散模型的一个优势是，与 GANs 相比，它们能够有效地将新信息纳入其领域，同时保留对先前数据的知识，并避免对有限的训练图像集的过拟合。

## 为少样本个性化设计提示

如前所述，该模型通过使用“一个 [identifier] [class noun]”结构的简单提示进行训练。这里，[identifier] 代表与主题相关的独特标识符，而 [class noun] 作为主题类别的一般描述（如猫、狗、手表等）。作者将类名纳入提示中，以建立通用类别与我们个体主题之间的联系，观察到使用不正确或缺失的类名会导致更长的训练时间和语言漂移，*最终影响性能*。本质上，主要目的是利用特定类别与我们的主题之间的关系，利用模型对该类别已有的知识。这使我们能够在各种上下文中生成新颖的姿势和变体。

## 稀有标记标识符

论文强调，普通的英语单词在这种情况下并不理想，因为模型需要将它们与原始含义脱离，并重新整合以指代我们的主题。

为了解决这个问题，作者提出使用在语言和扩散模型中都有弱先验的标识符。虽然选择像“xxy5syt00”这样的随机字符可能最初看起来很吸引人，但这也存在潜在风险。需要考虑的是，分词器可能会将每个字母单独分词。那么解决方案是什么？最有效的方法是识别词汇表中不常见的标记，然后在文本空间中反转这些标记。这可以最小化标识符具有强先验的可能性。

有趣的是，大多数教程使用“sks”来实现这一目的，但正如其中一位作者指出的，这个看似无害的词可能会产生副作用……

## 类别特定先验保持损失

![](../Images/5aa015342afc90ecfbdf58e6f45ff2ce.png)

[DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242)中的图6。

与文本反演不同，DreamBooth 微调模型的所有层以最大化性能。不幸的是，这样做会遇到众所周知的**语言漂移**问题，即当一个模型最初在一个广泛的文本语料库上进行预训练，然后再针对特定任务进行微调时，它会逐渐减少对语言语法和语义的理解。

另一个问题是输出多样性的潜在减少。这可以从图6的第二行中观察到，在该图中，模型，除非进一步调整，否则有倾向仅复制输入图像中找到的姿势。当模型训练时间较长时，这种效果变得更加明显。

为了减轻这些问题，作者引入了类别特定先验保持损失，让我们先看看其整体损失公式，然后再解释其组成部分。

![](../Images/ed2415b9465140a1046fad3604722833.png)

[DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242)中的公式2。

第一部分是标准的L2去噪误差，这是任何扩散模型的典型特征。*α_t* 将初始图像**x** 缩放，然后添加高斯噪声 **ε** ∼ *N* (0, I)，乘以 *σ_t*。随机变量 **z_*t*** := *α_t****x** + *σ_t****ε** 的分布为 *N*(*α_t****x**, *σ_t²*)。此时，模型 **x**ˆ*_θ* 将尝试从 **z**_*t*、*t* 和条件向量 **c** = Γ(**P**) 预测原始图像，其中在 DreamBooth 的情况下，Γ 是 [T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)，而提示 **P** 的形式为“一个 [标识符] [类别名词]”。

第二部分是 **先验保留损失**，在这里，**x** 被替换为 **x**_pr，即由模型生成的图像，模型的权重被冻结（在微调之前），从随机初始噪声 **z**_*1* ∼ *N* (0, I) 和条件向量 **c**_pr = Γ(“一个 [类别名词]”)。这一部分促使模型从其损坏版本中重新获取 **x**_pr，从而促使模型生成类似于在微调过程之前生成的图像。

最后，*w_t* 和 *w_t*’ 是与噪声调度相关的术语，*λ* 定义了两个损失之间的相对权重。

# 实验

![](../Images/03e0a4db15e6f5cf7791ceb1e992ad69.png)

图 4 来自 [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242)。

## 数据集

实验使用的数据集由作者生成，包含 30 个主题，包括独特的物品，如背包或太阳镜，以及动物，如狗、猫等。在这 30 个主题中，21 个是物体，9 个是活体主题/宠物。

作者定义了 25 个提示：20 个重新背景化提示和 5 个物体属性修改提示；10 个重新背景化提示，10 个配件化提示和 5 个活体主题/宠物属性修改提示。

## 评估指标

为了评估，每个主题和每个提示生成四张图像，共计 3,000 张图像。

为了测量 **主题一致性**，使用 CLIP-I 和 DINO。

**CLIP-I**，在之前的工作中已经使用，计算生成图像和真实图像的[CLIP](https://openai.com/research/clip)嵌入的平均成对余弦相似度。CLIP的训练目标是使文本描述的嵌入与其所指的图像具有相同的嵌入，因此如果两个图像表示相同的文本，它们将具有相似的嵌入。

**DINO**，由作者引入的新指标，类似于 CLIP-I，但生成嵌入的方式是使用[ViT-S/16 DINO](https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/)，一个自监督训练的模型。

论文中观察到，由于 CLIP 的训练方式，CLIP-I 不区分可能具有非常相似文本描述的不同主题。另一方面，DINO（指的是模型，而不是指标）以自监督的方式进行训练，这有助于区分主题或图像中的独特特征。因此，他们将 DINO 视为主要指标。

最后，引入了第三个度量指标 CLIP-T，用来衡量另一个重要方面：**提示一致性**，即生成的图像与输入提示的接近程度。

**CLIP-T** 与之前的指标类似，测量从CLIP中获得的两个嵌入之间的平均余弦相似度：一个来自提示，另一个来自图像。值得注意的是，CLIP特别训练以生成与对应图像的文本嵌入相似的嵌入。

## 比较

![](../Images/b78a564495b55a43c1c143aa0fa5f4cd.png)

表1来自于 [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242)。

从表1可以看出，当使用DINO和CLIP-T指标测量时，DreamBooth明显优于Textual Inversion，而在使用CLIP-I测量时差距较小，但如前所述，CLIP-I并不是一个好的衡量特定主题保真度的指标。

![](../Images/3b847e303394e05cc38aa89975d9fcbe.png)

表2来自于 [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242)。

很难找到一个与个人判断结果好坏完全一致的指标。因此，作者们还测量了一组72名用户的偏好。结果突显出，对于主题保真度和提示保真度，偏好DreamBooth的用户百分比要高于仅凭之前的指标所能得出的结论。我们可以通过查看论文中的图4来判断，两种方法之间的显著差异在这个特定例子中是显而易见的。

# 结论

图像生成和生成式AI领域近年来获得了显著关注。特别是通过扩散模型的使用，图像合成的进展推动了这一领域的发展。

在本文中，我们深入探讨了DreamBooth的科学论文——这是一种令人印象深刻的解决方案，能够生成具有不同姿势和背景的新图像，同时保持对期望主题的忠实。这种创新的方法展示了图像合成领域取得的显著进展，并对未来的发展具有巨大潜力。

感谢您抽出时间阅读本文，如有任何意见或问题，请随时留言或与我联系。要了解我的最新文章，您可以关注我在 [Medium](https://medium.com/@mnslarcher)、[LinkedIn](https://www.linkedin.com/in/mnslarcher/) 或 [Twitter](https://twitter.com/mnslarcher) 上的动态。

[](https://medium.com/@mnslarcher/membership?source=post_page-----70f8bb0cfa30--------------------------------) [## 通过我的推荐链接加入Medium - Mario Namtao Shianti Larcher

### 阅读Mario Namtao Shianti Larcher的每一个故事（以及Medium上的其他成千上万位作家的故事）。您的会员费……

medium.com](https://medium.com/@mnslarcher/membership?source=post_page-----70f8bb0cfa30--------------------------------)
