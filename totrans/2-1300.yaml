- en: Illuminating the Black Box of Textual GenAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/illuminating-the-black-box-of-ai-ddea07e65c35](https://towardsdatascience.com/illuminating-the-black-box-of-ai-ddea07e65c35)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The need for insights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alcarazanthony1?source=post_page-----ddea07e65c35--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----ddea07e65c35--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ddea07e65c35--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ddea07e65c35--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----ddea07e65c35--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ddea07e65c35--------------------------------)
    ·8 min read·Dec 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs like ChatGPT, Claude 3, Gemini, and Mistral captivate the world with their
    articulateness and erudition. Yet these large language models remain black boxes,
    concealing the intricate machinery powering their responses. Their prowess at
    generating human-quality text outstrips our prowess at understanding how their
    machine minds function.
  prefs: []
  type: TYPE_NORMAL
- en: But as artificial intelligence is set loose upon scenarios where trust and transparency
    are paramount, like hiring and risk assessment, explicability now moves to the
    fore. Explainability is no longer an optional bell or whistle on complex systems,
    it is an essential prerequisite to safely progressing AI in high-impact domains.
  prefs: []
  type: TYPE_NORMAL
- en: To unpack these black box models, the vibrant field of explainable NLP offers
    a growing toolkit — from attention visualizations revealing patterns in focus,
    to probing random parts of input to quantify influence. Some approaches like LIME
    create simplified models that mimic key decisions locally. Other methods like
    SHAP adapt concepts from cooperative game theory to distribute “credits” and “blame”
    across different parts of a model’s input based on its final output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of technique, all pursue the same crucial end: elucidating how language
    models utilize the abundance of text we feed them to compose coherent passages
    or carry out consequential assessments.'
  prefs: []
  type: TYPE_NORMAL
- en: AI already makes decisions affecting human lives — selectively judging applicants,
    moderating hateful content, diagnosing illness.
  prefs: []
  type: TYPE_NORMAL
- en: Explanations aren’t mere accessories — they will prove instrumental in overseeing
    these powerful models as they proliferate through society.
  prefs: []
  type: TYPE_NORMAL
- en: As large language models continue to advance, their inner workings remain veiled
    in obscurity. Yet trustworthy AI necessitates transparency into their reasoning
    on impactful decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The vibrant field of explainable NLP offers two major approaches to elucidate
    model logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Perturbation-based Methods**: Techniques like LIME and SHAP systematically
    probe models by masking input components and quantify importance based on output
    changes. These external perspectives treat models as black boxes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Self-Explanations**: An alternative paradigm enables models to explain their
    own reasoning via generated texts. For instance, highlighting pivotal input features
    that informed a prediction. This relies on introspective model awareness rather
    than imposing interpretations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Early analysis finds promise in both approaches — LIME and SHAP excel at faithfully
    capturing model behaviors while self-explanations align better with human rationales.
    Yet current practices also struggle to adequately assess either, suggesting the
    need for rethinking evaluation strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, synergies between both camps could unite to propel progress. For instance,
    self-declared important factors could be verified against perturbation experiments.
    And attribution scores could add validation signals anchoring free-form explanations.
  prefs: []
  type: TYPE_NORMAL
- en: As models continue ingesting more world knowledge, elucidating their multifaceted
    reasoning grows increasingly crucial. A diversity of emerging ideas may prove
    essential to meet this challenge.
  prefs: []
  type: TYPE_NORMAL
- en: The Balancing Act of Explaining AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Constructing explanations inevitably requires simplification. But oversimplifying
    breeds distortion. Take common attention-based explanations — they highlight parts
    of input that models supposedly focus on. However, attention scores often misalign
    with an AI system’s actual reasoning process.
  prefs: []
  type: TYPE_NORMAL
- en: More rigorous techniques like **SHAP** avoid this by systematically masking
    different input components and directly measuring the impact on output. By comparing
    predictions with and without each feature present, SHAP assigns each an “importance
    score” representing its influence. This perturbation-based approach better reflects
    models’ logic.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, faithfulness comes at the cost of intelligibility. Removing combinations
    of words and clauses quickly becomes cognitively taxing for humans to parse explanations.
    Thus the research community emphasizes balancing two key criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Faithfulness**: How accurately does the explanation capture the model’s actual
    decision making process? Masking-based perturbation methods excel here.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Understandability**: How intuitive and digestible is the explanation for
    the intended audience? Simplified linear models facilitate comprehension but can
    distort.'
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, an explanation exhibits both traits. But even SHAP, which lands high
    on faithfulness, struggles when models process extensive texts and unrestricted
    generation — exponentially blowing up the output combinations to account for.
    Running computations over all possible masked permutations of a 10,000 word essay
    is infeasible!
  prefs: []
  type: TYPE_NORMAL
- en: This impedes progress on critical applications like explaining essay scoring
    models or question answering systems that handle documents. Creating simplified
    models that mimic predictions, a la LIME, also grows intractable for complex textual
    reasoning. More tailored solutions are needed to extend explainability to large
    language models.
  prefs: []
  type: TYPE_NORMAL
- en: The key challenges are highlighted — specifically the exponential complexity
    introduced by long inputs and open-ended outputs. Please let me know if any parts
    need more clarification!
  prefs: []
  type: TYPE_NORMAL
- en: 'TextGenSHAP: Optimizing Explanations for Language Tasks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2312.01279?source=post_page-----ddea07e65c35--------------------------------)
    [## TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) have attracted huge interest in practical applications
    given their increasingly accurate…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2312.01279?source=post_page-----ddea07e65c35--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: To surmount the obstacles of explainability for complex language models, researchers
    develop **TextGenSHAP** — extending SHAP by baking in optimizations for both efficiency
    and accounting for linguistic structure.
  prefs: []
  type: TYPE_NORMAL
- en: Several innovations address the exponential computational complexity. **Speculative
    decoding** predicts likely text outputs first, avoiding wasted decoding attempts.
    **Flash attention** simplifies memory-intensive attention calculations. **In-place
    resampling** precomputes input encodings once for efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Together these accelerator techniques reduce runtimes from hours to minutes,
    enabling practical turnaround. The authors verify orders-of-magnitude speedups
    across model types and dataset complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'But raw efficiency alone is not enough — the intricacies of language itself
    must be represented. TextGenSHAP tackles explanatory challenges unique to NLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hierarchical structure** — Beyond individual words, language models learn
    conceptual connections across sentences, paragraphs, even documents. TextGenSHAP’s
    hierarchical attribution allows importance scores to be assigned at both coarse-grained
    and fine-grained levels.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exponential output space** — Open-ended text generation produces a colossal
    set of possible outputs, unlike confined classification tasks. Via reformulations
    like the Shapley-Shubik index, TextGenSHAP bypasses exhaustive enumeration to
    estimate feature importance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Autoregressive dependence** — Generated tokens probabilistically depend on
    those preceding them. TextGenSHAP’s adapted decoding algorithms such as speculative
    decoding explicitly respect these inter-token dependencies during attribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Together, the architectural and linguistic advances pave the pathway for TextGenSHAP
    to handle complexity at the scale of modern NLP. The door now opens to tackling
    explainability in long-standing challenges like question answering over documents.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4082cbd9d8fadca02166150efb0ff90f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author generated by Dall-E-3.0
  prefs: []
  type: TYPE_NORMAL
- en: 'Applications: Explaining Question-Answering Over Documents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Question answering over documents represents a coveted milestone for AI — synthesizing
    information scattered across passages to address nuanced queries. TextGenSHAP
    now makes explaining these complex text reasoning workflows possible.
  prefs: []
  type: TYPE_NORMAL
- en: The authors evaluate TextGenSHAP on challenging datasets requiring deducing
    answers from contexts spanning 10,000+ words. Impressively, it accurately identifies
    the pivotal sentences and phrases dispersed throughout extended texts that most
    informed each answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'By properly crediting different parts of lengthy documents, TextGenSHAP enables
    powerful applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improving document retrieval** — Ranking and filtering contexts by influence
    scores extracted more relevant passages. Just by re-ordering based on TextGenSHAP,
    the authors demonstrate substantial gains in retrieval recall — from 84% to almost
    89%. This helps better supply information to downstream reasoning steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distilling evidence** — Using importance scores to cull the most integral
    supporting passages for answering each question, accuracy improves from 50% to
    70% on datasets with diverse evidence. Ensuring models focus on concise explanatory
    extracts counters overfitting to spurious patterns in large corpora.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Human oversight** — By surfacing the most influential text snippets, TextGenSHAP
    allows auditors to rapidly validate if models utilized appropriate supportive
    content instead of latching onto unintended cues. Monitoring complex reasoning
    is otherwise intractable.'
  prefs: []
  type: TYPE_NORMAL
- en: The success on reasoning-intensive question answering suggests wider applicability
    explaining AI capabilities with societal impacts — like scoring essay content
    and prose or interpreting medical diagnoses. By exposing the crucial connections
    within language, TextGenSHAP moves us towards accountable and trustworthy NLP
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://anon832098265.github.io/](https://anon832098265.github.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: Investigating Self-Explanations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2310.11207?source=post_page-----ddea07e65c35--------------------------------)
    [## Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) such as ChatGPT have demonstrated superior performance
    on a variety of natural language…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2310.11207?source=post_page-----ddea07e65c35--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: We’ve discussed traditional post-hoc methods that treat models as black boxes.
    An intriguing alternative is enabling systems to explain their own reasoning —
    **self-explanations**.
  prefs: []
  type: TYPE_NORMAL
- en: Recent research analyzed these for sentiment analysis using ChatGPT. The model
    highlighted input words informing its predictions. Unlike external techniques
    directly perturbing inputs, self-explanations rely on introspective model awareness
    to declare important factors.
  prefs: []
  type: TYPE_NORMAL
- en: The paper systematically compared formats, finding that both predicting then
    explaining or vice versa worked reasonably well. The models readily produced feature
    attribution scores for all words or just top highlights. But interestingly, importance
    scores frequently clustered into “well-rounded” levels (e.g. 0.0, 0.5, 0.75) resembling
    human judgment more than discrete machine precision.
  prefs: []
  type: TYPE_NORMAL
- en: While self-explanations aligned fairly with human rationales, widely used evaluation
    practices struggled to differentiate quality. Metrics easily fooled on classifiers
    depended on fine-grained model prediction changes often insensitive for ChatGPT.
    The researchers concluded that the classic interpretability pipeline needs rethinking
    for large language models.
  prefs: []
  type: TYPE_NORMAL
- en: Fully realizing self-explanation potential requires new assessment frameworks
    attuned to their blended human-machine nature. Anchoring them to directly observable
    signals like attention weights could enhance faithfulness. Architecting modular
    reasoning/explaining components might also enable purer introspective elucidation.
  prefs: []
  type: TYPE_NORMAL
- en: With careful co-design accommodating their emergent characteristics, self-explanations
    could unlock unprecedented model transparency — converting black boxes into “glass
    boxes” with systems not just displaying but also discussing their inner workings.
  prefs: []
  type: TYPE_NORMAL
- en: The TextGenSHAP method focuses on enabling efficient Shapley value attribution
    for text generation models. It makes progress on quantifying feature importance
    for tasks like long-document question answering.
  prefs: []
  type: TYPE_NORMAL
- en: However, TextGenSHAP still relies on an external perspective, perturbing inputs
    and observing output changes rather than asking models to introspect on their
    own reasoning. This leaves room for integration with self-explanation methods.
  prefs: []
  type: TYPE_NORMAL
- en: Self-explanations could provide a more qualitative, intuitive understanding
    to complement the quantitative attribution scores from TextGenSHAP. For example,
    TextGenSHAP may identify a pivotal paragraph in a document and highlight certain
    sentences as most influential in answering a question. Self-explanations could
    then enrich this by discussing the logic for focusing on those areas.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, self-explanations today often take the form of free generation without
    any grounding. Combining with attribution scores that synthesize model reasoning
    into token importance rankings could help validate and enhance the meaningfulness
    of self-explanations.
  prefs: []
  type: TYPE_NORMAL
- en: Architecturally, TextGenSHAP modules could first digest documents and questions,
    produce attention distributions and passagens rankings. Then self-explanation
    modules could consume these quantitative signals to generate free-form rationales
    discussing the assessment, with the attribution scores steering the interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: Joint evaluation could also assess whether self-declared explanatory factors
    align with input components that perturbation-based scoring designates as influential.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, self-explanations provide the “what” of model understanding while
    attribution scores offer the “why”. Their symbiosis could enable rich explainability
    blending quantitative and qualitative insights into system reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Path Ahead: Towards Trust through Transparency'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TextGenSHAP furnishes a pivotal advancement — the means to peer inside the intricate
    workings of large language models as they ingest volumes of text. By creating
    efficient and accurate explanations, it circumvents existing barriers that constrained
    explainability methods to tiny snippets of language.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, rich fluency alone does not guarantee trustworthy AI. Mastery of language
    — the hallmark of progress powering ChatGPT’s eloquence — must couple with mastery
    of elucidation.
  prefs: []
  type: TYPE_NORMAL
- en: Elucidation entails more than spitting out a few keywords in attention rolls
    — it requires replicating the complex chains of inferential reasoning that yield
    final assessments. Advances like TextGenSHAP bring this requisite transparency
    closer to reality.
  prefs: []
  type: TYPE_NORMAL
- en: As models continue absorbing more world knowledge, their inner representational
    tapestries grow vastly multifaceted. Attempting oversight via reductionist attention
    scores or small perturbation samples will only muddle, not illuminate. More holistic
    methodologies in the spirit of TextGenSHAP that respect dependencies in structure
    and logic will prove critical.
  prefs: []
  type: TYPE_NORMAL
- en: Learning without transparency risks power devoid of responsibility. Observation
    without illumination risks rubber stamps lacking in rigor. Part and parcel with
    the remarkable renaissance of neural networks must come techniques that expose
    their intricacies.
  prefs: []
  type: TYPE_NORMAL
- en: Progress on this frontier remains nascent — but vital seeds are taking root.
    By striving to perfect hybrids of understandability and faithfulness, whether
    via efficient approximations or innately interpretable architectures, perhaps
    future systems can masterfully explain their mastery to lift the veil of black
    box mysticism once and for all.
  prefs: []
  type: TYPE_NORMAL
