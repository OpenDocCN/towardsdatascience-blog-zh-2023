- en: Gaussian Mixture Model Clearly Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合模型清晰解释
- en: 原文：[https://towardsdatascience.com/gaussian-mixture-model-clearly-explained-115010f7d4cf](https://towardsdatascience.com/gaussian-mixture-model-clearly-explained-115010f7d4cf)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gaussian-mixture-model-clearly-explained-115010f7d4cf](https://towardsdatascience.com/gaussian-mixture-model-clearly-explained-115010f7d4cf)
- en: The only guide you need to learn everything about GMM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习GMM所需的唯一指南
- en: '[](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)[![Ransaka
    Ravihara](../Images/ac09746938c10ad8f157d46ea0de27ca.png)](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)
    [Ransaka Ravihara](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)[![Ransaka
    Ravihara](../Images/ac09746938c10ad8f157d46ea0de27ca.png)](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)
    [Ransaka Ravihara](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)
    ·9 min read·Jan 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)
    ·阅读时间9分钟·2023年1月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7b9ace71504e42f3bf45229c93dff566.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b9ace71504e42f3bf45229c93dff566.png)'
- en: Photo by [Planet Volumes](https://unsplash.com/@planetvolumes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Planet Volumes](https://unsplash.com/@planetvolumes?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: When we talk about Gaussian Mixture Model (later, this will be denoted as GMM
    in this article), it's essential to know how the KMeans algorithm works. Because
    GMM is quite similar to the KMeans, more likely it's a probabilistic version of
    KMeans. This probabilistic feature allows GMM to be applied to many complex problems
    that KMeans can't fit into.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论高斯混合模型（以下简称GMM）时，了解KMeans算法的工作原理是至关重要的。因为GMM与KMeans非常相似，实际上它是KMeans的概率版本。这种概率特性使GMM可以应用于KMeans无法适应的许多复杂问题。
- en: In summary, KMeans have below limitations,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，KMeans具有以下限制：
- en: It assumed that the clusters were spherical and equally sized, which is not
    valid in most real-world scenarios.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它假设簇是球形且大小相等，这在大多数现实世界场景中并不成立。
- en: It's a hard clustering method. Meaning each data point is assigned to a single
    cluster.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一种硬聚类方法，意味着每个数据点被分配到一个单独的簇中。
- en: Due to these limitations, we should know alternatives for KMeans when working
    on our machine learning projects. In this article, we will explore one of the
    best alternatives for KMeans clustering, called the Gaussian Mixture Model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些限制，当我们进行机器学习项目时，应了解KMeans的替代方案。本文将探讨KMeans聚类的最佳替代方案之一，即高斯混合模型。
- en: Throughout this article, we will be covering the below points.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将涵盖以下要点。
- en: How Gaussian Mixture Model (GMM) algorithm works — in plain English.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMM）算法的工作原理——通俗易懂。
- en: Mathematics behind GMM.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GMM背后的数学。
- en: Implement GMM using Python from scratch.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从零开始使用Python实现GMM。
- en: How Gaussian Mixture Model (GMM) algorithm works — in plain English
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMM）算法的工作原理——通俗易懂
- en: 'As I have mentioned earlier, we can call GMM probabilistic KMeans because the
    starting point and training process of the KMeans and GMM are the same. However,
    KMeans uses a distance-based approach, and GMM uses a probabilistic approach.
    There is one primary assumption in GMM: the dataset consists of multiple Gaussians,
    in other words, a mixture of the gaussian.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，我们可以将GMM称为概率KMeans，因为KMeans和GMM的起点和训练过程是相同的。然而，KMeans使用基于距离的方法，而GMM使用概率方法。GMM有一个主要的假设：数据集由多个高斯分布组成，换句话说，是高斯混合。
- en: '![](../Images/dd0670c45770f4f7fb7fa5425e4b8e44.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd0670c45770f4f7fb7fa5425e4b8e44.png)'
- en: Mixtures of Gaussians | Image by Author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型的混合体 | 作者图像
- en: The above kind of distribution is often called [multi-model distribution](https://en.wikipedia.org/wiki/Multimodal_distribution).
    Each peak represents the different gaussian distribution or the cluster in our
    dataset. But the question is,
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 上述分布通常称为[多模态分布](https://en.wikipedia.org/wiki/Multimodal_distribution)。每个峰值代表数据集中的不同高斯分布或簇。但问题是，
- en: how do we estimate these distributions?
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们如何估计这些分布？
- en: Before answering this question, let's create some gaussian distribution first.
    Please note here I am generating [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution);
    it's a higher dimensional extension of the [univariate normal distribution](https://en.wikipedia.org/wiki/Normal_distribution).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在回答这个问题之前，让我们先创建一些高斯分布。请注意，我生成的是[多变量正态分布](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)；它是[单变量正态分布](https://en.wikipedia.org/wiki/Normal_distribution)的高维扩展。
- en: Let's define the mean and covariance of our data points. Using mean and [covariance](https://en.wikipedia.org/wiki/Covariance),
    we can generate the distribution as follows.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义数据点的均值和协方差。使用均值和[协方差](https://en.wikipedia.org/wiki/Covariance)，我们可以生成如下分布。
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's plot the data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制数据。
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/9d78e019b6818268039a9b8e39504ff7.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d78e019b6818268039a9b8e39504ff7.png)'
- en: As you can see here, we generated random gaussian distribution using mean and
    covariance matrices. What about reversing this process? That's what exactly GMM
    is doing. But how?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们使用均值和协方差矩阵生成了随机的高斯分布。那么反转这一过程呢？这正是GMM在做的。但怎么做呢？
- en: Because, in the beginning, we didn’t have any insights about clusters nor their
    associated mean and covariance matrices.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因为一开始我们对簇及其相关的均值和协方差矩阵没有任何见解。
- en: Well, It happens according to the below steps,
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，按照以下步骤进行，
- en: Decide the number of clusters (to decide this, we can use domain knowledge or
    other methods such as [BIC/AIC](https://stats.stackexchange.com/questions/368560/elbow-test-using-aic-bic-for-identifying-number-of-clusters-using-gmm))
    for the given dataset. Assume that we have 1000 data points, and we set the number
    of groups as 2.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定簇的数量（为此，我们可以使用领域知识或其他方法，如[BIC/AIC](https://stats.stackexchange.com/questions/368560/elbow-test-using-aic-bic-for-identifying-number-of-clusters-using-gmm)）以适应给定数据集。假设我们有1000个数据点，并将组数设置为2。
- en: Initiate mean, covariance, and weight parameter per cluster. (we will explore
    more about this in a later section)
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个簇初始化均值、协方差和权重参数。（我们将在后面的部分进一步探讨）
- en: Use the [Expectation Maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)
    algorithm to do the following,
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[期望最大化](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)算法完成以下任务，
- en: 'Expectation Step (E step): Calculate the probability of each data point belonging
    to each distribution, then evaluate the likelihood function using the current
    estimate for the parameters'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期望步骤（E步骤）：计算每个数据点属于每个分布的概率，然后使用当前参数估计值评估似然函数。
- en: 'Maximization step (M step): Update the previous mean, covariance, and weight
    parameters to maximize the expected likelihood found in the E step'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化步骤（M步骤）：更新之前的均值、协方差和权重参数，以最大化在E步骤中找到的期望似然。
- en: Repeat these steps until the model converges.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复这些步骤直到模型收敛。
- en: With this information, I am concluding the no-math explanation of the GMM algorithm.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，我将结束对GMM算法的无数学解释。
- en: Mathematics behind GMM
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GMM背后的数学
- en: The core of GMM lies within Expectation Maximization (EM) algorithm described
    in the previous section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: GMM的核心在于前一部分描述的期望最大化（EM）算法。
- en: Let's demonstrate how the EM algorithm is applied in the GMM.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们演示EM算法如何应用于GMM。
- en: '***Step 01: Initialize mean, covariance, and weight parameters***'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤01：初始化均值、协方差和权重参数***'
- en: 'mean (μ): initialize randomly.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 均值（μ）：随机初始化。
- en: 'covariance (Σ): initialize randomly'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 协方差（Σ）：随机初始化
- en: 'weight (mixing coefficients) (π): fraction per class refers to the likelihood
    that a particular data point belongs to each class. In the beginning, this will
    be equal for all clusters. Assume that we fit a GMM with three components. In
    this case weight parameter might be set to 1/3 for each component, resulting in
    a probability distribution of (1/3, 1/3, 1/3).'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权重（混合系数）（π）：每类的比例表示特定数据点属于每个类别的可能性。一开始，这对于所有簇都是相等的。假设我们拟合一个具有三个组件的GMM。在这种情况下，权重参数可能被设置为1/3，对应于概率分布（1/3，1/3，1/3）。
- en: '**Step 02: Expectation Step (E step)**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤02：期望步骤（E步骤）**'
- en: 'For each data point 𝑥𝑖:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个数据点𝑥𝑖：
- en: Calculate the probability that the data point belongs to cluster (𝑐) using the
    below equation. *k* is the number of distributions we are supposed to find.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下方程计算数据点属于集群(𝑐)的概率。*k*是我们需要找到的分布数量。
- en: '![](../Images/769cd67923d1271b3d53fb8e9cf99c23.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/769cd67923d1271b3d53fb8e9cf99c23.png)'
- en: Equation 01 | Image by Autor
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 方程01 | 作者提供的图片
- en: Where 𝜋_𝑐 is the mixing coefficient (sometimes called weight) for the Gaussian
    distribution c, which was initialized in the previous stage, and **𝑁**(𝒙 | 𝝁,𝚺)
    describes the probability density function (PDF) of a Gaussian distribution with
    mean 𝜇 and covariance Σ with respect to data point *x;* We can denote it as below.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中𝜋_𝑐是高斯分布c的混合系数（有时称为权重），在上一个阶段初始化，**𝑁**(𝒙 | 𝝁,𝚺)描述了具有均值𝜇和协方差Σ的高斯分布的概率密度函数（PDF），相对于数据点*x*；我们可以如下表示它。
- en: '![](../Images/96895db76a2c696f855f8925909c2005.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96895db76a2c696f855f8925909c2005.png)'
- en: Equation 02 | Image by Autor
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 方程02 | 作者提供的图片
- en: The E-step computes these probabilities using the current estimates of the model's
    parameters. These probabilities are typically referred to as the "responsibilities"
    of the Gaussian distributions. They are represented by the variables *r_ic****,***
    where ***i*** is the index of the data point, and ***c*** is the index of the
    Gaussian distribution. The responsibility measures how much the ***c***-th Gaussian
    distribution is responsible for generating the ***i***-th data point. Conditional
    probability is used here, more specifically, *Bayes theorem.*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: E步使用当前模型参数的估计值计算这些概率。这些概率通常被称为高斯分布的“责任”。它们由变量*r_ic****,***表示，其中***i***是数据点的索引，***c***是高斯分布的索引。责任度量***c***-th高斯分布对生成***i***-th数据点的责任。这里使用了条件概率，更具体地说，是*贝叶斯定理*。
- en: Let's take a simple example. Assume we have 100 data points and need to cluster
    them into two groups. We can write *r_ic(i=20,c=1)* as follows. Where ***i***
    represents the data point's index, and ***c*** represents the index of the cluster
    we are considering***.***
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个简单的例子。假设我们有100个数据点，并且需要将它们聚类为两个组。我们可以将*r_ic(i=20,c=1)*写作如下。这里***i***表示数据点的索引，***c***表示我们考虑的集群的索引***.***
- en: Please note at the beginning, 𝜋_𝑐initialized to equal for each cluster c = 1,2,3,..,k.
    In our case, 𝜋_1 = 𝜋_2 **=** 1/2**.**
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在开始时，𝜋_𝑐初始化为每个集群c = 1,2,3,..,k相等。在我们的例子中，𝜋_1 = 𝜋_2 **=** 1/2**.**
- en: '![](../Images/446d5f3f7d05aca178fff63309f16e06.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/446d5f3f7d05aca178fff63309f16e06.png)'
- en: Equation 03 | Image by Autor
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 方程03 | 作者提供的图片
- en: The result of the E-step is a set of responsibilities for each data point and
    each Gaussian distribution in the mixture model. These responsibilities are used
    in the M-step to update the estimates of the model's parameters.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: E步的结果是每个数据点和混合模型中每个高斯分布的责任集合。这些责任在M步中用于更新模型参数的估计值。
- en: '**Step 03: Maximization Step (M step)**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤03：最大化步骤（M步）**'
- en: In this step, the algorithm uses the responsibilities of the Gaussian distributions
    (computed in the E-step) to update the estimates of the model's parameters.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步骤中，算法使用高斯分布的责任（在E步中计算得出）来更新模型参数的估计值。
- en: 'The M-step updates the estimates of the parameters as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: M步更新参数的估计值如下：
- en: '![](../Images/a08bd30739da7c606b86e9b06ca44a7f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a08bd30739da7c606b86e9b06ca44a7f.png)'
- en: Image by Author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Update the *πc (*mixing coefficients) using equation 4 above.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上述方程4更新*πc*（混合系数）。
- en: Update the *μc* using equation number 5 above.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上述方程5更新*μc*。
- en: Then update the *Σc* using the 6th equation.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用第6个方程更新*Σc*。
- en: This updated estimate is used in the next E-step to compute new responsibilities
    for the data points.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的估计值在下一次E步中用于计算数据点的新责任。
- en: So on and so forth, this process will repeat until algorithm convergence, typically
    achieved when the model parameters do not change significantly from one iteration
    to the next.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 依此类推，这个过程会重复进行，直到算法收敛，通常在模型参数在一次迭代到下一次迭代之间变化不大时实现。
- en: '*Lots of ugly and complex equations, right? :)*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*很多丑陋且复杂的方程，对吧？ :)*'
- en: Let’s summarize the above facts into one simple diagram,
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们将上述事实总结为一个简单的图表，
- en: '![](../Images/44b1d8f721e9508c320fbfbd86660349.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44b1d8f721e9508c320fbfbd86660349.png)'
- en: Summary of EM steps of GMM | Image by Author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: GMM的EM步骤总结 | 作者提供的图片
- en: Don't worry; when it comes to coding, it will be one line per each equation.
    Let's start to implement GMM from scratch using Python.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Implement GMM using Python from scratch.
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/4bbf6d8731d911455f368d2a41d35df4.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Animated GMM | Image by Author
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: First thing first, let's create a fake dataset. In this section, I will implement
    GMM for the 1-D dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let's create a helper function to plot our data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: And plot the generated data as follows. Please note that instead of plotting
    the data itself, I have plotted the probability density of each sample.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/a317e25e0f8344504f5cba94c9f7ed7d.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: Original Distribution | Image by Author
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Let's build each step described in the previous section,
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 01: Initialize mean, covariance, and weights*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Step 02: Expectation Step (E step)*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After this function, we covered the first two equations we discussed in *E Step.*
    Here we have generated the gaussian distribution for the current model parameter
    *means* and *variances.* We accomplished that by using the scipy's stat module.
    After, we used the pdf method to calculate the likelihood of belonging to each
    data point for each cluster.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 03: Maximization Step (M step)*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let's implement the training loop.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When we start the model training, we will do E and M steps according to the
    *n_steps* parameter we set.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: But in the actual use cases, you will use the scikit-learn version of the GMM
    more often. There you can find additional parameters, such as
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '***tol****: d*efining the model’s stop criteria. EM iterations will stop when
    the lower bound average gain is below the *tol* parameter.'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**init_params:** The method used to initialize the weights'
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You may refer to the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Alright, let's see how our handcrafted GMM performs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f350caecabe0a5bfde8ab260b5a23564.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: In the above diagrams, red dashed lines represent the original distribution,
    while other graphs represent the learned distributions. After the 30th iteration,
    we can see that our model performed well on this toy dataset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: You can find the codes in this [GitHub repo](https://github.com/Ransaka/GMM-from-scratch)
    if you want to follow along with this article.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '*Conclusion*'
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article aims to give a comprehensive guide to Gaussian Mixture Model; however,
    readers are encouraged to experiment with different machine learning algorithms
    because ***no one best*** ***algorithm*** will work well for every problem. Also,
    the complexity of the machine learning algorithms we choose is worth noting. One
    common issue with GMM is it doesn't scale well to large datasets.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading! Connect with me on [LinkedIn](https://www.linkedin.com/in/ransaka/).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
