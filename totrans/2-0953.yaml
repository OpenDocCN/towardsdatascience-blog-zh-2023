- en: Gaussian Mixture Model Clearly Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é«˜æ–¯æ··åˆæ¨¡å‹æ¸…æ™°è§£é‡Š
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/gaussian-mixture-model-clearly-explained-115010f7d4cf](https://towardsdatascience.com/gaussian-mixture-model-clearly-explained-115010f7d4cf)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/gaussian-mixture-model-clearly-explained-115010f7d4cf](https://towardsdatascience.com/gaussian-mixture-model-clearly-explained-115010f7d4cf)
- en: The only guide you need to learn everything about GMM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å­¦ä¹ GMMæ‰€éœ€çš„å”¯ä¸€æŒ‡å—
- en: '[](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)[![Ransaka
    Ravihara](../Images/ac09746938c10ad8f157d46ea0de27ca.png)](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)
    [Ransaka Ravihara](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)[![Ransaka
    Ravihara](../Images/ac09746938c10ad8f157d46ea0de27ca.png)](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)
    [Ransaka Ravihara](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)
    Â·9 min readÂ·Jan 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[æ•°æ®ç§‘å­¦å‰æ²¿](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)
    Â·é˜…è¯»æ—¶é—´9åˆ†é’ŸÂ·2023å¹´1æœˆ10æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7b9ace71504e42f3bf45229c93dff566.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b9ace71504e42f3bf45229c93dff566.png)'
- en: Photo by [Planet Volumes](https://unsplash.com/@planetvolumes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Planet Volumes](https://unsplash.com/@planetvolumes?utm_source=medium&utm_medium=referral)æä¾›ï¼Œæ¥æºäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: When we talk about Gaussian Mixture Model (later, this will be denoted as GMM
    in this article), it's essential to know how the KMeans algorithm works. Because
    GMM is quite similar to the KMeans, more likely it's a probabilistic version of
    KMeans. This probabilistic feature allows GMM to be applied to many complex problems
    that KMeans can't fit into.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è°ˆè®ºé«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆä»¥ä¸‹ç®€ç§°GMMï¼‰æ—¶ï¼Œäº†è§£KMeansç®—æ³•çš„å·¥ä½œåŸç†æ˜¯è‡³å…³é‡è¦çš„ã€‚å› ä¸ºGMMä¸KMeanséå¸¸ç›¸ä¼¼ï¼Œå®é™…ä¸Šå®ƒæ˜¯KMeansçš„æ¦‚ç‡ç‰ˆæœ¬ã€‚è¿™ç§æ¦‚ç‡ç‰¹æ€§ä½¿GMMå¯ä»¥åº”ç”¨äºKMeansæ— æ³•é€‚åº”çš„è®¸å¤šå¤æ‚é—®é¢˜ã€‚
- en: In summary, KMeans have below limitations,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼ŒKMeanså…·æœ‰ä»¥ä¸‹é™åˆ¶ï¼š
- en: It assumed that the clusters were spherical and equally sized, which is not
    valid in most real-world scenarios.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒå‡è®¾ç°‡æ˜¯çƒå½¢ä¸”å¤§å°ç›¸ç­‰ï¼Œè¿™åœ¨å¤§å¤šæ•°ç°å®ä¸–ç•Œåœºæ™¯ä¸­å¹¶ä¸æˆç«‹ã€‚
- en: It's a hard clustering method. Meaning each data point is assigned to a single
    cluster.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§ç¡¬èšç±»æ–¹æ³•ï¼Œæ„å‘³ç€æ¯ä¸ªæ•°æ®ç‚¹è¢«åˆ†é…åˆ°ä¸€ä¸ªå•ç‹¬çš„ç°‡ä¸­ã€‚
- en: Due to these limitations, we should know alternatives for KMeans when working
    on our machine learning projects. In this article, we will explore one of the
    best alternatives for KMeans clustering, called the Gaussian Mixture Model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™äº›é™åˆ¶ï¼Œå½“æˆ‘ä»¬è¿›è¡Œæœºå™¨å­¦ä¹ é¡¹ç›®æ—¶ï¼Œåº”äº†è§£KMeansçš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬æ–‡å°†æ¢è®¨KMeansèšç±»çš„æœ€ä½³æ›¿ä»£æ–¹æ¡ˆä¹‹ä¸€ï¼Œå³é«˜æ–¯æ··åˆæ¨¡å‹ã€‚
- en: Throughout this article, we will be covering the below points.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¶µç›–ä»¥ä¸‹è¦ç‚¹ã€‚
- en: How Gaussian Mixture Model (GMM) algorithm works â€” in plain English.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰ç®—æ³•çš„å·¥ä½œåŸç†â€”â€”é€šä¿—æ˜“æ‡‚ã€‚
- en: Mathematics behind GMM.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GMMèƒŒåçš„æ•°å­¦ã€‚
- en: Implement GMM using Python from scratch.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»é›¶å¼€å§‹ä½¿ç”¨Pythonå®ç°GMMã€‚
- en: How Gaussian Mixture Model (GMM) algorithm works â€” in plain English
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰ç®—æ³•çš„å·¥ä½œåŸç†â€”â€”é€šä¿—æ˜“æ‡‚
- en: 'As I have mentioned earlier, we can call GMM probabilistic KMeans because the
    starting point and training process of the KMeans and GMM are the same. However,
    KMeans uses a distance-based approach, and GMM uses a probabilistic approach.
    There is one primary assumption in GMM: the dataset consists of multiple Gaussians,
    in other words, a mixture of the gaussian.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä¹‹å‰æåˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥å°†GMMç§°ä¸ºæ¦‚ç‡KMeansï¼Œå› ä¸ºKMeanså’ŒGMMçš„èµ·ç‚¹å’Œè®­ç»ƒè¿‡ç¨‹æ˜¯ç›¸åŒçš„ã€‚ç„¶è€Œï¼ŒKMeansä½¿ç”¨åŸºäºè·ç¦»çš„æ–¹æ³•ï¼Œè€ŒGMMä½¿ç”¨æ¦‚ç‡æ–¹æ³•ã€‚GMMæœ‰ä¸€ä¸ªä¸»è¦çš„å‡è®¾ï¼šæ•°æ®é›†ç”±å¤šä¸ªé«˜æ–¯åˆ†å¸ƒç»„æˆï¼Œæ¢å¥è¯è¯´ï¼Œæ˜¯é«˜æ–¯æ··åˆã€‚
- en: '![](../Images/dd0670c45770f4f7fb7fa5425e4b8e44.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd0670c45770f4f7fb7fa5425e4b8e44.png)'
- en: Mixtures of Gaussians | Image by Author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯æ··åˆæ¨¡å‹çš„æ··åˆä½“ | ä½œè€…å›¾åƒ
- en: The above kind of distribution is often called [multi-model distribution](https://en.wikipedia.org/wiki/Multimodal_distribution).
    Each peak represents the different gaussian distribution or the cluster in our
    dataset. But the question is,
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°åˆ†å¸ƒé€šå¸¸ç§°ä¸º[å¤šæ¨¡æ€åˆ†å¸ƒ](https://en.wikipedia.org/wiki/Multimodal_distribution)ã€‚æ¯ä¸ªå³°å€¼ä»£è¡¨æ•°æ®é›†ä¸­çš„ä¸åŒé«˜æ–¯åˆ†å¸ƒæˆ–ç°‡ã€‚ä½†é—®é¢˜æ˜¯ï¼Œ
- en: how do we estimate these distributions?
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•ä¼°è®¡è¿™äº›åˆ†å¸ƒï¼Ÿ
- en: Before answering this question, let's create some gaussian distribution first.
    Please note here I am generating [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution);
    it's a higher dimensional extension of the [univariate normal distribution](https://en.wikipedia.org/wiki/Normal_distribution).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›ç­”è¿™ä¸ªé—®é¢˜ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆåˆ›å»ºä¸€äº›é«˜æ–¯åˆ†å¸ƒã€‚è¯·æ³¨æ„ï¼Œæˆ‘ç”Ÿæˆçš„æ˜¯[å¤šå˜é‡æ­£æ€åˆ†å¸ƒ](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)ï¼›å®ƒæ˜¯[å•å˜é‡æ­£æ€åˆ†å¸ƒ](https://en.wikipedia.org/wiki/Normal_distribution)çš„é«˜ç»´æ‰©å±•ã€‚
- en: Let's define the mean and covariance of our data points. Using mean and [covariance](https://en.wikipedia.org/wiki/Covariance),
    we can generate the distribution as follows.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å®šä¹‰æ•°æ®ç‚¹çš„å‡å€¼å’Œåæ–¹å·®ã€‚ä½¿ç”¨å‡å€¼å’Œ[åæ–¹å·®](https://en.wikipedia.org/wiki/Covariance)ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆå¦‚ä¸‹åˆ†å¸ƒã€‚
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's plot the data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»˜åˆ¶æ•°æ®ã€‚
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/9d78e019b6818268039a9b8e39504ff7.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d78e019b6818268039a9b8e39504ff7.png)'
- en: As you can see here, we generated random gaussian distribution using mean and
    covariance matrices. What about reversing this process? That's what exactly GMM
    is doing. But how?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ‚¨æ‰€è§ï¼Œæˆ‘ä»¬ä½¿ç”¨å‡å€¼å’Œåæ–¹å·®çŸ©é˜µç”Ÿæˆäº†éšæœºçš„é«˜æ–¯åˆ†å¸ƒã€‚é‚£ä¹ˆåè½¬è¿™ä¸€è¿‡ç¨‹å‘¢ï¼Ÿè¿™æ­£æ˜¯GMMåœ¨åšçš„ã€‚ä½†æ€ä¹ˆåšå‘¢ï¼Ÿ
- en: Because, in the beginning, we didnâ€™t have any insights about clusters nor their
    associated mean and covariance matrices.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å› ä¸ºä¸€å¼€å§‹æˆ‘ä»¬å¯¹ç°‡åŠå…¶ç›¸å…³çš„å‡å€¼å’Œåæ–¹å·®çŸ©é˜µæ²¡æœ‰ä»»ä½•è§è§£ã€‚
- en: Well, It happens according to the below steps,
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼Œ
- en: Decide the number of clusters (to decide this, we can use domain knowledge or
    other methods such as [BIC/AIC](https://stats.stackexchange.com/questions/368560/elbow-test-using-aic-bic-for-identifying-number-of-clusters-using-gmm))
    for the given dataset. Assume that we have 1000 data points, and we set the number
    of groups as 2.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å†³å®šç°‡çš„æ•°é‡ï¼ˆä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é¢†åŸŸçŸ¥è¯†æˆ–å…¶ä»–æ–¹æ³•ï¼Œå¦‚[BIC/AIC](https://stats.stackexchange.com/questions/368560/elbow-test-using-aic-bic-for-identifying-number-of-clusters-using-gmm)ï¼‰ä»¥é€‚åº”ç»™å®šæ•°æ®é›†ã€‚å‡è®¾æˆ‘ä»¬æœ‰1000ä¸ªæ•°æ®ç‚¹ï¼Œå¹¶å°†ç»„æ•°è®¾ç½®ä¸º2ã€‚
- en: Initiate mean, covariance, and weight parameter per cluster. (we will explore
    more about this in a later section)
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸ºæ¯ä¸ªç°‡åˆå§‹åŒ–å‡å€¼ã€åæ–¹å·®å’Œæƒé‡å‚æ•°ã€‚ï¼ˆæˆ‘ä»¬å°†åœ¨åé¢çš„éƒ¨åˆ†è¿›ä¸€æ­¥æ¢è®¨ï¼‰
- en: Use the [Expectation Maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)
    algorithm to do the following,
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[æœŸæœ›æœ€å¤§åŒ–](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)ç®—æ³•å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼Œ
- en: 'Expectation Step (E step): Calculate the probability of each data point belonging
    to each distribution, then evaluate the likelihood function using the current
    estimate for the parameters'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœŸæœ›æ­¥éª¤ï¼ˆEæ­¥éª¤ï¼‰ï¼šè®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹å±äºæ¯ä¸ªåˆ†å¸ƒçš„æ¦‚ç‡ï¼Œç„¶åä½¿ç”¨å½“å‰å‚æ•°ä¼°è®¡å€¼è¯„ä¼°ä¼¼ç„¶å‡½æ•°ã€‚
- en: 'Maximization step (M step): Update the previous mean, covariance, and weight
    parameters to maximize the expected likelihood found in the E step'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€å¤§åŒ–æ­¥éª¤ï¼ˆMæ­¥éª¤ï¼‰ï¼šæ›´æ–°ä¹‹å‰çš„å‡å€¼ã€åæ–¹å·®å’Œæƒé‡å‚æ•°ï¼Œä»¥æœ€å¤§åŒ–åœ¨Eæ­¥éª¤ä¸­æ‰¾åˆ°çš„æœŸæœ›ä¼¼ç„¶ã€‚
- en: Repeat these steps until the model converges.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‡å¤è¿™äº›æ­¥éª¤ç›´åˆ°æ¨¡å‹æ”¶æ•›ã€‚
- en: With this information, I am concluding the no-math explanation of the GMM algorithm.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™äº›ä¿¡æ¯ï¼Œæˆ‘å°†ç»“æŸå¯¹GMMç®—æ³•çš„æ— æ•°å­¦è§£é‡Šã€‚
- en: Mathematics behind GMM
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GMMèƒŒåçš„æ•°å­¦
- en: The core of GMM lies within Expectation Maximization (EM) algorithm described
    in the previous section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: GMMçš„æ ¸å¿ƒåœ¨äºå‰ä¸€éƒ¨åˆ†æè¿°çš„æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ç®—æ³•ã€‚
- en: Let's demonstrate how the EM algorithm is applied in the GMM.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¼”ç¤ºEMç®—æ³•å¦‚ä½•åº”ç”¨äºGMMã€‚
- en: '***Step 01: Initialize mean, covariance, and weight parameters***'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ­¥éª¤01ï¼šåˆå§‹åŒ–å‡å€¼ã€åæ–¹å·®å’Œæƒé‡å‚æ•°***'
- en: 'mean (Î¼): initialize randomly.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‡å€¼ï¼ˆÎ¼ï¼‰ï¼šéšæœºåˆå§‹åŒ–ã€‚
- en: 'covariance (Î£): initialize randomly'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åæ–¹å·®ï¼ˆÎ£ï¼‰ï¼šéšæœºåˆå§‹åŒ–
- en: 'weight (mixing coefficients) (Ï€): fraction per class refers to the likelihood
    that a particular data point belongs to each class. In the beginning, this will
    be equal for all clusters. Assume that we fit a GMM with three components. In
    this case weight parameter might be set to 1/3 for each component, resulting in
    a probability distribution of (1/3, 1/3, 1/3).'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æƒé‡ï¼ˆæ··åˆç³»æ•°ï¼‰ï¼ˆÏ€ï¼‰ï¼šæ¯ç±»çš„æ¯”ä¾‹è¡¨ç¤ºç‰¹å®šæ•°æ®ç‚¹å±äºæ¯ä¸ªç±»åˆ«çš„å¯èƒ½æ€§ã€‚ä¸€å¼€å§‹ï¼Œè¿™å¯¹äºæ‰€æœ‰ç°‡éƒ½æ˜¯ç›¸ç­‰çš„ã€‚å‡è®¾æˆ‘ä»¬æ‹Ÿåˆä¸€ä¸ªå…·æœ‰ä¸‰ä¸ªç»„ä»¶çš„GMMã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæƒé‡å‚æ•°å¯èƒ½è¢«è®¾ç½®ä¸º1/3ï¼Œå¯¹åº”äºæ¦‚ç‡åˆ†å¸ƒï¼ˆ1/3ï¼Œ1/3ï¼Œ1/3ï¼‰ã€‚
- en: '**Step 02: Expectation Step (E step)**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤02ï¼šæœŸæœ›æ­¥éª¤ï¼ˆEæ­¥éª¤ï¼‰**'
- en: 'For each data point ğ‘¥ğ‘–:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªæ•°æ®ç‚¹ğ‘¥ğ‘–ï¼š
- en: Calculate the probability that the data point belongs to cluster (ğ‘) using the
    below equation. *k* is the number of distributions we are supposed to find.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä»¥ä¸‹æ–¹ç¨‹è®¡ç®—æ•°æ®ç‚¹å±äºé›†ç¾¤(ğ‘)çš„æ¦‚ç‡ã€‚*k*æ˜¯æˆ‘ä»¬éœ€è¦æ‰¾åˆ°çš„åˆ†å¸ƒæ•°é‡ã€‚
- en: '![](../Images/769cd67923d1271b3d53fb8e9cf99c23.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/769cd67923d1271b3d53fb8e9cf99c23.png)'
- en: Equation 01 | Image by Autor
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹01 | ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Where ğœ‹_ğ‘ is the mixing coefficient (sometimes called weight) for the Gaussian
    distribution c, which was initialized in the previous stage, and **ğ‘**(ğ’™ | ğ,ğšº)
    describes the probability density function (PDF) of a Gaussian distribution with
    mean ğœ‡ and covariance Î£ with respect to data point *x;* We can denote it as below.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ğœ‹_ğ‘æ˜¯é«˜æ–¯åˆ†å¸ƒcçš„æ··åˆç³»æ•°ï¼ˆæœ‰æ—¶ç§°ä¸ºæƒé‡ï¼‰ï¼Œåœ¨ä¸Šä¸€ä¸ªé˜¶æ®µåˆå§‹åŒ–ï¼Œ**ğ‘**(ğ’™ | ğ,ğšº)æè¿°äº†å…·æœ‰å‡å€¼ğœ‡å’Œåæ–¹å·®Î£çš„é«˜æ–¯åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ï¼Œç›¸å¯¹äºæ•°æ®ç‚¹*x*ï¼›æˆ‘ä»¬å¯ä»¥å¦‚ä¸‹è¡¨ç¤ºå®ƒã€‚
- en: '![](../Images/96895db76a2c696f855f8925909c2005.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96895db76a2c696f855f8925909c2005.png)'
- en: Equation 02 | Image by Autor
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹02 | ä½œè€…æä¾›çš„å›¾ç‰‡
- en: The E-step computes these probabilities using the current estimates of the model's
    parameters. These probabilities are typically referred to as the "responsibilities"
    of the Gaussian distributions. They are represented by the variables *r_ic****,***
    where ***i*** is the index of the data point, and ***c*** is the index of the
    Gaussian distribution. The responsibility measures how much the ***c***-th Gaussian
    distribution is responsible for generating the ***i***-th data point. Conditional
    probability is used here, more specifically, *Bayes theorem.*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Eæ­¥ä½¿ç”¨å½“å‰æ¨¡å‹å‚æ•°çš„ä¼°è®¡å€¼è®¡ç®—è¿™äº›æ¦‚ç‡ã€‚è¿™äº›æ¦‚ç‡é€šå¸¸è¢«ç§°ä¸ºé«˜æ–¯åˆ†å¸ƒçš„â€œè´£ä»»â€ã€‚å®ƒä»¬ç”±å˜é‡*r_ic****,***è¡¨ç¤ºï¼Œå…¶ä¸­***i***æ˜¯æ•°æ®ç‚¹çš„ç´¢å¼•ï¼Œ***c***æ˜¯é«˜æ–¯åˆ†å¸ƒçš„ç´¢å¼•ã€‚è´£ä»»åº¦é‡***c***-thé«˜æ–¯åˆ†å¸ƒå¯¹ç”Ÿæˆ***i***-thæ•°æ®ç‚¹çš„è´£ä»»ã€‚è¿™é‡Œä½¿ç”¨äº†æ¡ä»¶æ¦‚ç‡ï¼Œæ›´å…·ä½“åœ°è¯´ï¼Œæ˜¯*è´å¶æ–¯å®šç†*ã€‚
- en: Let's take a simple example. Assume we have 100 data points and need to cluster
    them into two groups. We can write *r_ic(i=20,c=1)* as follows. Where ***i***
    represents the data point's index, and ***c*** represents the index of the cluster
    we are considering***.***
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸¾ä¸ªç®€å•çš„ä¾‹å­ã€‚å‡è®¾æˆ‘ä»¬æœ‰100ä¸ªæ•°æ®ç‚¹ï¼Œå¹¶ä¸”éœ€è¦å°†å®ƒä»¬èšç±»ä¸ºä¸¤ä¸ªç»„ã€‚æˆ‘ä»¬å¯ä»¥å°†*r_ic(i=20,c=1)*å†™ä½œå¦‚ä¸‹ã€‚è¿™é‡Œ***i***è¡¨ç¤ºæ•°æ®ç‚¹çš„ç´¢å¼•ï¼Œ***c***è¡¨ç¤ºæˆ‘ä»¬è€ƒè™‘çš„é›†ç¾¤çš„ç´¢å¼•***.***
- en: Please note at the beginning, ğœ‹_ğ‘initialized to equal for each cluster c = 1,2,3,..,k.
    In our case, ğœ‹_1 = ğœ‹_2 **=** 1/2**.**
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåœ¨å¼€å§‹æ—¶ï¼Œğœ‹_ğ‘åˆå§‹åŒ–ä¸ºæ¯ä¸ªé›†ç¾¤c = 1,2,3,..,kç›¸ç­‰ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œğœ‹_1 = ğœ‹_2 **=** 1/2**.**
- en: '![](../Images/446d5f3f7d05aca178fff63309f16e06.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/446d5f3f7d05aca178fff63309f16e06.png)'
- en: Equation 03 | Image by Autor
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹03 | ä½œè€…æä¾›çš„å›¾ç‰‡
- en: The result of the E-step is a set of responsibilities for each data point and
    each Gaussian distribution in the mixture model. These responsibilities are used
    in the M-step to update the estimates of the model's parameters.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Eæ­¥çš„ç»“æœæ˜¯æ¯ä¸ªæ•°æ®ç‚¹å’Œæ··åˆæ¨¡å‹ä¸­æ¯ä¸ªé«˜æ–¯åˆ†å¸ƒçš„è´£ä»»é›†åˆã€‚è¿™äº›è´£ä»»åœ¨Mæ­¥ä¸­ç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°çš„ä¼°è®¡å€¼ã€‚
- en: '**Step 03: Maximization Step (M step)**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤03ï¼šæœ€å¤§åŒ–æ­¥éª¤ï¼ˆMæ­¥ï¼‰**'
- en: In this step, the algorithm uses the responsibilities of the Gaussian distributions
    (computed in the E-step) to update the estimates of the model's parameters.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€æ­¥éª¤ä¸­ï¼Œç®—æ³•ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒçš„è´£ä»»ï¼ˆåœ¨Eæ­¥ä¸­è®¡ç®—å¾—å‡ºï¼‰æ¥æ›´æ–°æ¨¡å‹å‚æ•°çš„ä¼°è®¡å€¼ã€‚
- en: 'The M-step updates the estimates of the parameters as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Mæ­¥æ›´æ–°å‚æ•°çš„ä¼°è®¡å€¼å¦‚ä¸‹ï¼š
- en: '![](../Images/a08bd30739da7c606b86e9b06ca44a7f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a08bd30739da7c606b86e9b06ca44a7f.png)'
- en: Image by Author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Update the *Ï€c (*mixing coefficients) using equation 4 above.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸Šè¿°æ–¹ç¨‹4æ›´æ–°*Ï€c*ï¼ˆæ··åˆç³»æ•°ï¼‰ã€‚
- en: Update the *Î¼c* using equation number 5 above.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸Šè¿°æ–¹ç¨‹5æ›´æ–°*Î¼c*ã€‚
- en: Then update the *Î£c* using the 6th equation.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åä½¿ç”¨ç¬¬6ä¸ªæ–¹ç¨‹æ›´æ–°*Î£c*ã€‚
- en: This updated estimate is used in the next E-step to compute new responsibilities
    for the data points.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ–°åçš„ä¼°è®¡å€¼åœ¨ä¸‹ä¸€æ¬¡Eæ­¥ä¸­ç”¨äºè®¡ç®—æ•°æ®ç‚¹çš„æ–°è´£ä»»ã€‚
- en: So on and so forth, this process will repeat until algorithm convergence, typically
    achieved when the model parameters do not change significantly from one iteration
    to the next.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾æ­¤ç±»æ¨ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤è¿›è¡Œï¼Œç›´åˆ°ç®—æ³•æ”¶æ•›ï¼Œé€šå¸¸åœ¨æ¨¡å‹å‚æ•°åœ¨ä¸€æ¬¡è¿­ä»£åˆ°ä¸‹ä¸€æ¬¡è¿­ä»£ä¹‹é—´å˜åŒ–ä¸å¤§æ—¶å®ç°ã€‚
- en: '*Lots of ugly and complex equations, right? :)*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¾ˆå¤šä¸‘é™‹ä¸”å¤æ‚çš„æ–¹ç¨‹ï¼Œå¯¹å§ï¼Ÿ :)*'
- en: Letâ€™s summarize the above facts into one simple diagram,
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°†ä¸Šè¿°äº‹å®æ€»ç»“ä¸ºä¸€ä¸ªç®€å•çš„å›¾è¡¨ï¼Œ
- en: '![](../Images/44b1d8f721e9508c320fbfbd86660349.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44b1d8f721e9508c320fbfbd86660349.png)'
- en: Summary of EM steps of GMM | Image by Author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: GMMçš„EMæ­¥éª¤æ€»ç»“ | ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Don't worry; when it comes to coding, it will be one line per each equation.
    Let's start to implement GMM from scratch using Python.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Implement GMM using Python from scratch.
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/4bbf6d8731d911455f368d2a41d35df4.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Animated GMM | Image by Author
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: First thing first, let's create a fake dataset. In this section, I will implement
    GMM for the 1-D dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let's create a helper function to plot our data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: And plot the generated data as follows. Please note that instead of plotting
    the data itself, I have plotted the probability density of each sample.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/a317e25e0f8344504f5cba94c9f7ed7d.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: Original Distribution | Image by Author
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Let's build each step described in the previous section,
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 01: Initialize mean, covariance, and weights*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Step 02: Expectation Step (E step)*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After this function, we covered the first two equations we discussed in *E Step.*
    Here we have generated the gaussian distribution for the current model parameter
    *means* and *variances.* We accomplished that by using the scipy's stat module.
    After, we used the pdf method to calculate the likelihood of belonging to each
    data point for each cluster.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 03: Maximization Step (M step)*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let's implement the training loop.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When we start the model training, we will do E and M steps according to the
    *n_steps* parameter we set.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: But in the actual use cases, you will use the scikit-learn version of the GMM
    more often. There you can find additional parameters, such as
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '***tol****: d*efining the modelâ€™s stop criteria. EM iterations will stop when
    the lower bound average gain is below the *tol* parameter.'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**init_params:** The method used to initialize the weights'
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You may refer to the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Alright, let's see how our handcrafted GMM performs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f350caecabe0a5bfde8ab260b5a23564.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: In the above diagrams, red dashed lines represent the original distribution,
    while other graphs represent the learned distributions. After the 30th iteration,
    we can see that our model performed well on this toy dataset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: You can find the codes in this [GitHub repo](https://github.com/Ransaka/GMM-from-scratch)
    if you want to follow along with this article.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '*Conclusion*'
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article aims to give a comprehensive guide to Gaussian Mixture Model; however,
    readers are encouraged to experiment with different machine learning algorithms
    because ***no one best*** ***algorithm*** will work well for every problem. Also,
    the complexity of the machine learning algorithms we choose is worth noting. One
    common issue with GMM is it doesn't scale well to large datasets.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading! Connect with me on [LinkedIn](https://www.linkedin.com/in/ransaka/).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
