- en: Gaussian Mixture Model Clearly Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/gaussian-mixture-model-clearly-explained-115010f7d4cf](https://towardsdatascience.com/gaussian-mixture-model-clearly-explained-115010f7d4cf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The only guide you need to learn everything about GMM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)[![Ransaka
    Ravihara](../Images/ac09746938c10ad8f157d46ea0de27ca.png)](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)
    [Ransaka Ravihara](https://ransakaravihara.medium.com/?source=post_page-----115010f7d4cf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----115010f7d4cf--------------------------------)
    ¬∑9 min read¬∑Jan 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b9ace71504e42f3bf45229c93dff566.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Planet Volumes](https://unsplash.com/@planetvolumes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: When we talk about Gaussian Mixture Model (later, this will be denoted as GMM
    in this article), it's essential to know how the KMeans algorithm works. Because
    GMM is quite similar to the KMeans, more likely it's a probabilistic version of
    KMeans. This probabilistic feature allows GMM to be applied to many complex problems
    that KMeans can't fit into.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, KMeans have below limitations,
  prefs: []
  type: TYPE_NORMAL
- en: It assumed that the clusters were spherical and equally sized, which is not
    valid in most real-world scenarios.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It's a hard clustering method. Meaning each data point is assigned to a single
    cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Due to these limitations, we should know alternatives for KMeans when working
    on our machine learning projects. In this article, we will explore one of the
    best alternatives for KMeans clustering, called the Gaussian Mixture Model.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this article, we will be covering the below points.
  prefs: []
  type: TYPE_NORMAL
- en: How Gaussian Mixture Model (GMM) algorithm works ‚Äî in plain English.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mathematics behind GMM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement GMM using Python from scratch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How Gaussian Mixture Model (GMM) algorithm works ‚Äî in plain English
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As I have mentioned earlier, we can call GMM probabilistic KMeans because the
    starting point and training process of the KMeans and GMM are the same. However,
    KMeans uses a distance-based approach, and GMM uses a probabilistic approach.
    There is one primary assumption in GMM: the dataset consists of multiple Gaussians,
    in other words, a mixture of the gaussian.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd0670c45770f4f7fb7fa5425e4b8e44.png)'
  prefs: []
  type: TYPE_IMG
- en: Mixtures of Gaussians | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The above kind of distribution is often called [multi-model distribution](https://en.wikipedia.org/wiki/Multimodal_distribution).
    Each peak represents the different gaussian distribution or the cluster in our
    dataset. But the question is,
  prefs: []
  type: TYPE_NORMAL
- en: how do we estimate these distributions?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Before answering this question, let's create some gaussian distribution first.
    Please note here I am generating [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution);
    it's a higher dimensional extension of the [univariate normal distribution](https://en.wikipedia.org/wiki/Normal_distribution).
  prefs: []
  type: TYPE_NORMAL
- en: Let's define the mean and covariance of our data points. Using mean and [covariance](https://en.wikipedia.org/wiki/Covariance),
    we can generate the distribution as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let's plot the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9d78e019b6818268039a9b8e39504ff7.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see here, we generated random gaussian distribution using mean and
    covariance matrices. What about reversing this process? That's what exactly GMM
    is doing. But how?
  prefs: []
  type: TYPE_NORMAL
- en: Because, in the beginning, we didn‚Äôt have any insights about clusters nor their
    associated mean and covariance matrices.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Well, It happens according to the below steps,
  prefs: []
  type: TYPE_NORMAL
- en: Decide the number of clusters (to decide this, we can use domain knowledge or
    other methods such as [BIC/AIC](https://stats.stackexchange.com/questions/368560/elbow-test-using-aic-bic-for-identifying-number-of-clusters-using-gmm))
    for the given dataset. Assume that we have 1000 data points, and we set the number
    of groups as 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initiate mean, covariance, and weight parameter per cluster. (we will explore
    more about this in a later section)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the [Expectation Maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)
    algorithm to do the following,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expectation Step (E step): Calculate the probability of each data point belonging
    to each distribution, then evaluate the likelihood function using the current
    estimate for the parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maximization step (M step): Update the previous mean, covariance, and weight
    parameters to maximize the expected likelihood found in the E step'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat these steps until the model converges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this information, I am concluding the no-math explanation of the GMM algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematics behind GMM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core of GMM lies within Expectation Maximization (EM) algorithm described
    in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Let's demonstrate how the EM algorithm is applied in the GMM.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 01: Initialize mean, covariance, and weight parameters***'
  prefs: []
  type: TYPE_NORMAL
- en: 'mean (Œº): initialize randomly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'covariance (Œ£): initialize randomly'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'weight (mixing coefficients) (œÄ): fraction per class refers to the likelihood
    that a particular data point belongs to each class. In the beginning, this will
    be equal for all clusters. Assume that we fit a GMM with three components. In
    this case weight parameter might be set to 1/3 for each component, resulting in
    a probability distribution of (1/3, 1/3, 1/3).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Step 02: Expectation Step (E step)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each data point ùë•ùëñ:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the probability that the data point belongs to cluster (ùëê) using the
    below equation. *k* is the number of distributions we are supposed to find.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/769cd67923d1271b3d53fb8e9cf99c23.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 01 | Image by Autor
  prefs: []
  type: TYPE_NORMAL
- en: Where ùúã_ùëê is the mixing coefficient (sometimes called weight) for the Gaussian
    distribution c, which was initialized in the previous stage, and **ùëÅ**(ùíô | ùùÅ,ùö∫)
    describes the probability density function (PDF) of a Gaussian distribution with
    mean ùúá and covariance Œ£ with respect to data point *x;* We can denote it as below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96895db76a2c696f855f8925909c2005.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 02 | Image by Autor
  prefs: []
  type: TYPE_NORMAL
- en: The E-step computes these probabilities using the current estimates of the model's
    parameters. These probabilities are typically referred to as the "responsibilities"
    of the Gaussian distributions. They are represented by the variables *r_ic****,***
    where ***i*** is the index of the data point, and ***c*** is the index of the
    Gaussian distribution. The responsibility measures how much the ***c***-th Gaussian
    distribution is responsible for generating the ***i***-th data point. Conditional
    probability is used here, more specifically, *Bayes theorem.*
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a simple example. Assume we have 100 data points and need to cluster
    them into two groups. We can write *r_ic(i=20,c=1)* as follows. Where ***i***
    represents the data point's index, and ***c*** represents the index of the cluster
    we are considering***.***
  prefs: []
  type: TYPE_NORMAL
- en: Please note at the beginning, ùúã_ùëêinitialized to equal for each cluster c = 1,2,3,..,k.
    In our case, ùúã_1 = ùúã_2 **=** 1/2**.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/446d5f3f7d05aca178fff63309f16e06.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 03 | Image by Autor
  prefs: []
  type: TYPE_NORMAL
- en: The result of the E-step is a set of responsibilities for each data point and
    each Gaussian distribution in the mixture model. These responsibilities are used
    in the M-step to update the estimates of the model's parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 03: Maximization Step (M step)**'
  prefs: []
  type: TYPE_NORMAL
- en: In this step, the algorithm uses the responsibilities of the Gaussian distributions
    (computed in the E-step) to update the estimates of the model's parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The M-step updates the estimates of the parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a08bd30739da7c606b86e9b06ca44a7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Update the *œÄc (*mixing coefficients) using equation 4 above.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the *Œºc* using equation number 5 above.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then update the *Œ£c* using the 6th equation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This updated estimate is used in the next E-step to compute new responsibilities
    for the data points.
  prefs: []
  type: TYPE_NORMAL
- en: So on and so forth, this process will repeat until algorithm convergence, typically
    achieved when the model parameters do not change significantly from one iteration
    to the next.
  prefs: []
  type: TYPE_NORMAL
- en: '*Lots of ugly and complex equations, right? :)*'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs summarize the above facts into one simple diagram,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/44b1d8f721e9508c320fbfbd86660349.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary of EM steps of GMM | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Don't worry; when it comes to coding, it will be one line per each equation.
    Let's start to implement GMM from scratch using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Implement GMM using Python from scratch.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/4bbf6d8731d911455f368d2a41d35df4.png)'
  prefs: []
  type: TYPE_IMG
- en: Animated GMM | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: First thing first, let's create a fake dataset. In this section, I will implement
    GMM for the 1-D dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let's create a helper function to plot our data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: And plot the generated data as follows. Please note that instead of plotting
    the data itself, I have plotted the probability density of each sample.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a317e25e0f8344504f5cba94c9f7ed7d.png)'
  prefs: []
  type: TYPE_IMG
- en: Original Distribution | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Let's build each step described in the previous section,
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 01: Initialize mean, covariance, and weights*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 02: Expectation Step (E step)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: After this function, we covered the first two equations we discussed in *E Step.*
    Here we have generated the gaussian distribution for the current model parameter
    *means* and *variances.* We accomplished that by using the scipy's stat module.
    After, we used the pdf method to calculate the likelihood of belonging to each
    data point for each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 03: Maximization Step (M step)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Let's implement the training loop.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: When we start the model training, we will do E and M steps according to the
    *n_steps* parameter we set.
  prefs: []
  type: TYPE_NORMAL
- en: But in the actual use cases, you will use the scikit-learn version of the GMM
    more often. There you can find additional parameters, such as
  prefs: []
  type: TYPE_NORMAL
- en: '***tol****: d*efining the model‚Äôs stop criteria. EM iterations will stop when
    the lower bound average gain is below the *tol* parameter.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**init_params:** The method used to initialize the weights'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You may refer to the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html).
  prefs: []
  type: TYPE_NORMAL
- en: Alright, let's see how our handcrafted GMM performs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f350caecabe0a5bfde8ab260b5a23564.png)'
  prefs: []
  type: TYPE_IMG
- en: In the above diagrams, red dashed lines represent the original distribution,
    while other graphs represent the learned distributions. After the 30th iteration,
    we can see that our model performed well on this toy dataset.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the codes in this [GitHub repo](https://github.com/Ransaka/GMM-from-scratch)
    if you want to follow along with this article.
  prefs: []
  type: TYPE_NORMAL
- en: '*Conclusion*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article aims to give a comprehensive guide to Gaussian Mixture Model; however,
    readers are encouraged to experiment with different machine learning algorithms
    because ***no one best*** ***algorithm*** will work well for every problem. Also,
    the complexity of the machine learning algorithms we choose is worth noting. One
    common issue with GMM is it doesn't scale well to large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading! Connect with me on [LinkedIn](https://www.linkedin.com/in/ransaka/).
  prefs: []
  type: TYPE_NORMAL
