- en: Kaiming He Initialization in Neural Networks ‚Äî Math Proof
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4](https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deriving optimal initial variance of weight matrices in neural network layers
    with ReLU activation function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[![Ester
    Hlav](../Images/5fe679b93c42d568d0d5b331a8bf92b9.png)](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)
    [Ester Hlav](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)
    ¬∑10 min read¬∑Feb 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e072a98caece16e7471a537515610cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Initialization techniques are one of the prerequisites for successfully training
    a deep learning architecture. Traditionally, weight initialization methods need
    to be compatible with the choice of an activation function as a mismatch can potentially
    affect training negatively.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU is one of the most commonly used activation functions in deep learning.
    Its properties make it a very convenient choice for scaling to large neural networks.
    On one hand, it is inexpensive to calculate the derivative during backpropagation
    because it is a linear function with a step-function derivative. On the other
    hand, ReLU helps reduce feature correlation as it is a non-negative activation
    function, *i.e.* features can only contribute positively to subsequent layers.
    It is a prevalent choice in convolutional architectures where the input dimension
    is large and neural networks tend to be very deep.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *‚ÄúDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification‚Äù* ‚ÅΩ*¬π* ‚Åæ by He *et al.* (2015), the authors present a methodology
    to optimally initialize neural network layers using a ReLU activation function.
    This technique allows the neural network to start in a regime with constant variance
    between inputs and outputs both in terms of forward and backward passes, which
    empirically showed meaningful improvement in training stability and speed. In
    the following sections, we provide a detailed and complete derivation behind the
    He initialization technique.'
  prefs: []
  type: TYPE_NORMAL
- en: Notation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A layer in a neural network, composed of a weight matrix *W‚Çñ* and bias vector
    *b‚Çñ*, undergoes two consecutive transformations. The first transformation is *y‚Çñ
    = x‚Çñ W‚Çñ + b‚Çñ*, and the second is *x‚Çñ ‚Çä ‚ÇÅ = f(y‚Çñ)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x‚Çñ* is the actual layer and *y‚Çñ* is the pre-activation layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A layer has *n‚Çñ* units, thus *x‚Çñ ‚àà ‚Ñù‚Åø‚ÅΩ* ·µè*‚Åæ, W‚Çñ ‚àà ‚Ñù‚Åø‚ÅΩ* ·µè*‚Åæ*Àô*‚Åø‚ÅΩ* ·µè ‚Å∫ ¬π*‚Åæ, b‚Çñ
    ‚àà‚Ñù‚Åø‚ÅΩ* ·µè ‚Å∫ ¬π*‚Åæ*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x‚ÇñW‚Çñ + b‚Çñ* has dimension ( *1 √ó n‚Çñ ) √ó ( n‚Çñ √ó n‚Çñ ‚Çä* ‚ÇÅ*) + 1 √ó n‚Çñ ‚Çä* ‚Çó *= 1
    √ó n‚Çñ ‚Çä* ‚ÇÅ'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The activation function *f* is applied element-wise and does not change the
    shape of a vector. As a result, *x‚Çñ ‚Çä* ‚ÇÅ*= f(x‚Çñ W‚Çñ+ b‚Çñ)‚àà ‚Ñù‚Åø‚ÅΩ* ·µè ‚Å∫ ¬π*‚Åæ*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a neural network of depth *n*, the input layer is represented by *x‚ÇÄ* and
    the output layer by *x‚Çô*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss function of the network is represented by *L*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Œîx = ‚àÇL/‚àÇx* denotes gradients of the loss function with respect to vector
    *x*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Assumption 1*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We assume for this initialization setup a *non-linear* activation function ReLU
    defined as *f(x) =* *ReLU(x) = max(0, x).* As a function defined separately on
    two intervals, its derivative has a value of 1 on the strictly positive half of
    *‚Ñù* and 0 on the strictly negative half. Technically, the derivative of ReLU is
    not defined in 0 due to the limits of both sides not being equal, that is *f‚Äô(0‚Åª*‚Åª*)
    = 0 ‚â† 1 = f‚Äô(0‚Å∫).* In practice, for backpropagation‚Äôs purpose, ReLU‚Äô(0) is assumed
    to be 0.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Assumption 2:*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is assumed that all inputs, weights, and layers in the neural network are
    independent and identically distributed (*iid*) at initialization, as well as
    the gradients.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Assumption 3:*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inputs are assumed to be normalized with zero mean and the weights and biases
    are initialized from a symmetric distribution centered at zero, *i.e. ùîº[x‚ÇÄ] =
    ùîº[W‚Çñ] = ùîº[b‚Çñ] = 0*. This means that both *x‚Çñ* and y‚Çñ have an expectation of zero
    at initialization, and *y‚Çñ* has a symmetric distribution at initialization due
    to *f(0) = 0*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The aim of this proof is to determine the distribution of the weight matrix
    by finding *Var[W]* given two constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: ‚àÄ*k, Var[y‚Çñ] = Var[y‚Çñ ‚Çã* ‚ÇÅ*]*, *i.e.* constant variance in the forward signal
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ‚àÄ*k, Var[Œîx‚Çñ] = Var[Œîx‚Çñ ‚Çä* ‚ÇÅ*], i.e.* constant variance in the backward signal
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensuring that the variance of both layers and gradients is constant throughout
    the network at initialization helps prevent exploding and vanishing gradients
    in neural networks. If the gain is *above* one, it will result in exploding gradients
    and optimization divergence, while if the gain is *below* one, it will result
    in vanishing gradients and halt learning. The above two equations ensure that
    the signal gain is precisely one.
  prefs: []
  type: TYPE_NORMAL
- en: The motivation as well as the derivations in this paper are following the Xavier
    Glorot initialization‚ÅΩ¬≤‚Åæ paper published five years prior. While the previous
    work uses post-activation layers for constant variance in the forward signal,
    the He initialization proof uses pre-activation layers. Similarly, for the backward
    signal, He‚Äôs derivation uses post-activation layers instead of pre-activation
    layers in Glorot‚Äôs initialization. Given that these two proofs share some similarities,
    looking at both helps gain insights into why controlling for weights‚Äô variance
    is so important in *any* neural network. (See ‚Äú[Xavier Glorot Initialization in
    Neural Networks ‚Äî Math Proof](https://medium.com/towards-data-science/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3)‚Äù
    for more details)
  prefs: []
  type: TYPE_NORMAL
- en: '[](/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=post_page-----73b9a0d845c4--------------------------------)
    [## Xavier Glorot Initialization in Neural Networks ‚Äî Math Proof'
  prefs: []
  type: TYPE_NORMAL
- en: Detailed derivation for finding optimal initial distributions of weight matrices
    in deep learning layers with tanh‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=post_page-----73b9a0d845c4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Math Proof: Kaiming He Initialization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I. Forward Pass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are looking for *W‚Çñ* such that the variance of each subsequent pre-activation
    layer *y* is equal, *i.e.* *Var[y‚Çñ] = Var[y‚Çñ ‚Çã* ‚ÇÅ*].*
  prefs: []
  type: TYPE_NORMAL
- en: We know that *y‚Çñ = x‚Çñ W‚Çñ+ b‚Çñ.*
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, we look at the *i-th* element of the pre-activation layer *y‚Çñ*
    and apply the variance operator on both sides of the previous equation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16cb584b879d9662cbbfbfc8a3ac57bc.png)'
  prefs: []
  type: TYPE_IMG
- en: In the first step, we remove *b‚Çñ* entirely, as following *Assumption 1* it is
    initialized at value zero. Additionally, we leverage the independence of *W* and
    *x* to transform the variance of the sum into a sum of variances, following *Var[X+Y]
    = Var[X] + Var[Y] with X* ‚üÇ *Y*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second step, as *W* and *x* are *i.i.d.,* each term in the sum is equal,
    hence the sum is simply a *n‚Çñ* times repetition of *Var[xW]*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the third step, we follow the formula for *X* ‚üÇ *Y* which implies that *Var[XY]
    = E[X¬≤]E[Y¬≤] - E[X]¬≤E[Y]¬≤*. This allows us to separate *W* and *x* contributions
    to the pre-activation layer‚Äôs variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/dd865a2fc54dec78238b3aa7a6ccab66.png)'
  prefs: []
  type: TYPE_IMG
- en: In the fourth step, we leverage *Assumption 3* of zero expectation for weights
    and layers at initialization. This leaves us with a single term involving a squared
    expectation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the fifth step, we transform the squared expectation into a variance since
    *Var[X] = E[( X - E[X])¬≤] = E[X¬≤]* if *X* has a zero mean. Now we can express
    the pre-activation layer‚Äôs variance as a separate product of layer and weight
    variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, in order to link *Var[y‚Çñ] to Var[y‚Çñ ‚Çã* ‚ÇÅ*],* we express the squared
    expectation *E[x‚Çñ¬≤]* in terms of *Var[y‚Çñ ‚Çã* ‚ÇÅ*]* in the following steps using
    the *Law of the Unconscious Statistician*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/438ceff83f96e7fda4ee45850bfbeb9e.png)'
  prefs: []
  type: TYPE_IMG
- en: The theorem states that we can formulate any expectation of the function of
    a random variable as an integral of its function and probability density *p*.
    As we know that *x‚Çñ = max(0, y‚Çñ ‚Çã* ‚ÇÅ*)*, we can rewrite the squared expectation
    of *x‚Çñ* as an integral on *‚Ñù* of *y*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff34a215683d295eb7d4e630f9029d68.png)'
  prefs: []
  type: TYPE_IMG
- en: In the sixth step, we simplify the integral using that *y* is zero on ‚Ñù‚Åª.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the seventh step, we leverage the statistical property of *y* as a symmetric
    random variable, which hence has a symmetric density function *p*, and note that
    the entire integral‚Äôs term is an even function. Even functions are symmetric with
    respect to 0 on ‚Ñù, which means that integrating from 0 to *a* is the same as from
    *-a* to 0\. We use this trick to reformulate back the integral as an integral
    over ‚Ñù.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/f0613a7a42c13e34c3e7a65b014cffbb.png)'
  prefs: []
  type: TYPE_IMG
- en: In the ninth and tenth steps, we rewrite this integral as an integral of a function
    of a random variable. By applying the LOTUS ‚Äî this time from right to left ‚Äî we
    can change this integral to an expectation of the function over the random variable
    *y*. As a squared expectation of a zero mean variable, this is essentially a variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8a86c6fac095d23f1346be8be0d7bfa2.png)'
  prefs: []
  type: TYPE_IMG
- en: We finally get to put it all together using the results of steps five and ten
    ‚Äî the variance of a pre-activation layer is directly linked to its previous pre-activation
    variance as well as the variance of the layer‚Äôs weights. Since we require that
    *Var[y‚Çñ] = Var[y‚Çñ ‚Çã* ‚ÇÅ*]*, it allows us to confirm that a layer‚Äôs weights variance
    *Var[W‚Çñ]* should be 2/*n‚Çñ* .
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, here is again the whole derivation of the forward propagation reviewed
    in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3525ec2d5fdd6d851a28475d664d725.png)![](../Images/d09f218de73be6e53ae2a95eafb5b2b2.png)'
  prefs: []
  type: TYPE_IMG
- en: II. Backward Pass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are looking for *W‚Çñ* such that *Var[Œîx‚Çñ] = Var[Œîx‚Çñ ‚Çä* ‚ÇÅ*].*
  prefs: []
  type: TYPE_NORMAL
- en: Here*, x‚Çñ ‚Çä* ‚ÇÅ*= f (y‚Çñ)* and *y‚Çñ = x‚Çñ W‚Çñ + b‚Çñ.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Before applying the variance operator, let us first calculate the partial derivatives
    of the loss *L* with respect to *x* and *y* : *Œîx‚Çñ* and *Œîy‚Çñ* .'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c17c3bc893544ffa5202e2901d6cead.png)'
  prefs: []
  type: TYPE_IMG
- en: First, we use the chain rule and the fact that the derivative of a linear product
    is its linear coefficient ‚Äî in this case, *W‚Çñ*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, we leverage *Assumption 2* stating that gradients and weights are independent
    of each other. Using independence, the variance of the product becomes the product
    of variances, which is equal to zero since the weights are assumed to be initialized
    with zero means. Hence, the expectation of the gradient of *L w.r.t. x* is zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third, we use the chain rule to link *Œîy‚Çñ* and *Œîx‚Çñ ‚Çä* ‚ÇÅ as the partial derivative
    of *x* *w.r.t. y* is ReLU‚Äôs derivative taken in *y*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9b88943311d1a917e2bc3bc18529d10c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fourth, recalling the derivative of ReLU, we compute the expectation of *Œîy‚Çñ*
    using the previous equation. As *f‚Äô(x)* is split into two parts with an equal
    probability of *¬Ω*, we can write it as a sum of two terms: expectation over ‚Ñù‚Å∫
    and ‚Ñù‚Åª, respectively. From previous calculations, we know that the expectation
    of *Œîx‚Çñ* is zero, and we can thus confirm that both gradients have a mean of 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/197febab4ff1f2ccdf31069750903908.png)'
  prefs: []
  type: TYPE_IMG
- en: Fifth, we use the same rule as before to write a squared expectation as a variance,
    here with *Œîy‚Çñ* .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sixth, we leverage *Assumption 2* stating gradients are independent at initialization
    to split the variance of the two gradients *Œîx‚Çñ ‚Çä* ‚ÇÅ and *f‚Äô(y‚Çñ).* Further simplification
    stems from *Assumption 3* and we can finally compute ReLU‚Äôs squared expectation
    given its even split between positive and negative intervals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/313e8009108a1814e9b693539fcf9548.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, using the gathered results from the above sections, and reapplying
    the assumption of *iid,* we conclude the result of the backpropagation pass to
    be similar to the forward pass, *i.e.* given *Var[Œîx‚Çñ] = Var[Œîx‚Çñ ‚Çä* ‚ÇÅ*],* the
    variance of any layer‚Äôs weights *Var[W‚Çñ]* is equal to 2/*n‚Çñ .*
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, here is a reminder of the important step-by-step calculations
    included within this backward pass section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be426dc6afa5c2a15f929bc66627b32c.png)'
  prefs: []
  type: TYPE_IMG
- en: III. Weight Distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the two previous sections, we concluded the following for both backward
    and forward setups:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f745c22499c3eca5a1fafb02cc475cb.png)'
  prefs: []
  type: TYPE_IMG
- en: It is interesting to note that this result is different from the Glorot initialization‚ÅΩ¬≤‚Åæ,
    where the authors essentially have to average the *two* distinct results obtained
    in the forward and backward passes. Furthermore, we observe that the variance
    in the He method is doubled, which, intuitively, is due to the fact that ReLU‚Äôs
    zero negative section reduces variance by a factor of two.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, knowing the variance of the distribution, we can now initialize
    the weights with either normal distribution *N(0, ùúé¬≤)* or uniform distribution
    *U(-a, a)*. Empirically, there is no evidence that one distribution is superior
    to the other, and it seems that the performance improvement comes down solely
    to the symmetry and scale properties of a chosen distribution. Furthermore, we
    do need to keep in mind *Assumption 3*, restricting the distribution choice to
    be symmetric and centered in 0.
  prefs: []
  type: TYPE_NORMAL
- en: '**For Normal distribution *N(0, ùúé¬≤)***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If *X ~ N(0, ùúé¬≤),* then *Var[X] = ùúé¬≤*, thus the variance and standard deviation
    of the weight matrix can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1085f0b569de97721c8dd5bb2bc15f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can therefore conclude that *W‚Çñ* follows a normal distribution with coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ba793a077bfd84a7661bbde1ee61e1b.png)'
  prefs: []
  type: TYPE_IMG
- en: As a reminder, *n‚Çñ* is the number of inputs of the layer *k*.
  prefs: []
  type: TYPE_NORMAL
- en: '**For Uniform distribution *U(-a, a)***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If *X ~ U(-a, a)*, then using the below formula of a variance for a random
    variable following a uniform distribution, we can find the bound *a*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2adb9197e433a65dc09424df786bfce6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we can conclude that *W‚Çñ* follows a uniform distribution with coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a689c2c60b45301651c2fd13a4d62a25.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article provides a step-by-step derivation of why *He initialization method*
    is optimal for neural networks that use ReLU activation functions, given the constraints
    on forward and backward passes to have constant variances.
  prefs: []
  type: TYPE_NORMAL
- en: The methodology of this proof also extends to the broader family of linear rectifiers,
    like PReLU (discussed in (1) by He *et al.*) or Leaky ReLU (allowing for a minuscule
    gradient to flow in the negative interval). Similar optimal variance formulas
    can be derived for these variants of the ReLU activation function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Citations**'
  prefs: []
  type: TYPE_NORMAL
- en: '(1)[*Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification*](https://arxiv.org/abs/1502.01852), He *et al.* (2015)'
  prefs: []
  type: TYPE_NORMAL
- en: (2)[*Understanding the difficulty of training deep feedforward neural networks*](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf),
    Glorot *et al.* (2010)
  prefs: []
  type: TYPE_NORMAL
- en: '*Source: All the above equations and images are my own.*'
  prefs: []
  type: TYPE_NORMAL
