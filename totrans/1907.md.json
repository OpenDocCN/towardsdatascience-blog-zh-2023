["```py\nimport pandas as pd\nimport h2o\nfrom h2o.estimators.deeplearning import H2ODeepLearningEstimator\nfrom h2o.estimators import H2OXGBoostEstimator\nfrom h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\nimport optuna\nfrom tqdm import tqdm\n\nseed = 1\n```", "```py\nh2o.init()\n```", "```py\ndata = pd.read_csv('path_to_your_tabular_dataset')\n```", "```py\ndata_h2o = h2o.H2OFrame(data)\n\ncategorical_cols = [...]  #insert the names of the categorical features here\nfor col in categorical_cols:\n  data_h2o[col] = data_h2o[col].asfactor()\n```", "```py\nsplits = data_h2o.split_frame(ratios=[0.9], seed=seed)\ntrain = splits[0]\nval = splits[1]\n```", "```py\ny = '...'  #insert name of the target column here\nx = list(train.columns)\nx.remove(y) \n```", "```py\ndnn_models = []\n\ndef objective(trial):\n    #params to tune\n    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 10)\n    hidden_layer_size = trial.suggest_int('hidden_layer_size', 100, 300, step=50)\n\n    params = {\n        'hidden': [hidden_layer_size]*num_hidden_layers,\n        'epochs': trial.suggest_int('epochs', 5, 100),\n        'input_dropout_ratio': trial.suggest_float('input_dropout_ratio', 0.1, 0.3),  #dropout for input layer\n        'l1': trial.suggest_float('l1', 1e-5, 1e-1, log=True),  #l1 regularization\n        'l2': trial.suggest_float('l2', 1e-5, 1e-1, log=True),  #l2 regularization\n        'activation': trial.suggest_categorical('activation', ['rectifier', 'rectifierwithdropout', 'tanh', 'tanh_with_dropout', 'maxout', 'maxout_with_dropout'])\n}\n\n    #param 'hidden_dropout_ratios' is applicable only if the activation type is rectifier_with_dropout, tanh_with_dropout, or maxout_with_dropout\n    if params['activation'] in ['rectifierwithdropout', 'tanh_with_dropout', 'maxout_with_dropout']:\n        hidden_dropout_ratio = trial.suggest_float('hidden_dropout_ratio', 0.1, 1.0)  \n        params['hidden_dropout_ratios'] = [hidden_dropout_ratio]*num_hidden_layers  #dropout for hidden layers\n\n    #train model\n    model = H2ODeepLearningEstimator(**params,\n                                     standardize=True,  #h2o models can do this feature preprocessing automatically\n                                     categorical_encoding='auto',  #h2o models can do this feature preprocessing automatically\n                                     nfolds=5,\n                                     keep_cross_validation_predictions=True,  #need this for training the meta-model later\n                                     seed=seed)\n    model.train(x=x, y=y, training_frame=train)\n\n    #store model\n    dnn_models.append(model)\n\n    #get cross-validation rmse \n    cv_metrics_df = model.cross_validation_metrics_summary().as_data_frame()\n    cv_rmse_index = cv_metrics_df[cv_metrics_df[''] == 'rmse'].index\n    cv_rmse = cv_metrics_df['mean'].iloc[cv_rmse_index]\n    return cv_rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)\n```", "```py\nxgboost_lightgbm_models = []\n\ndef objective(trial):\n    #common params between xgboost and lightgbm\n    params = {\n        'ntrees': trial.suggest_int('ntrees', 50, 5000),\n        'max_depth': trial.suggest_int('max_depth', 1, 9),\n        'min_rows': trial.suggest_int('min_rows', 1, 5),\n        'sample_rate': trial.suggest_float('sample_rate', 0.8, 1.0),\n        'col_sample_rate': trial.suggest_float('col_sample_rate', 0.2, 1.0),\n        'col_sample_rate_per_tree': trial.suggest_float('col_sample_rate_per_tree', 0.5, 1.0)\n    }\n\n    grow_policy = trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide'])\n\n     #######################################################################################################################\n     #from H2OXGBoostEstimator's documentation, (https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html) # \n     #lightgbm is emulated when grow_policy=lossguide and tree_method=hist                                                 #\n     #so we will tune lightgbm-specific hyperparameters when this set of hyperparameters is used                           #\n     #and tune xgboost-specific hyperparameters otherwise                                                                  #\n     #######################################################################################################################\n\n    #add lightgbm-specific params\n    if grow_policy == 'lossguide':  \n        tree_method = 'hist'  \n        params['max_bins'] = trial.suggest_int('max_bins', 20, 256)\n        params['max_leaves'] = trial.suggest_int('max_leaves', 31, 1024)\n\n    #add xgboost-specific params\n    else:\n        tree_method = 'auto'\n        params['booster'] = trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart'])\n        params['reg_alpha'] = trial.suggest_float('reg_alpha', 0.001, 1)\n        params['reg_lambda'] = trial.suggest_float('reg_lambda', 0.001, 1)\n        params['min_split_improvement'] = trial.suggest_float('min_split_improvement', 1e-10, 1e-3, log=True)\n\n    #add grow_policy and tree_method into params dict\n    params['grow_policy'] = grow_policy\n    params['tree_method'] = tree_method\n\n    #train model\n    model = H2OXGBoostEstimator(**params,\n                                learn_rate=0.1,\n                                categorical_encoding='auto',  #h2o models can do this feature preprocessing automatically\n                                nfolds=5,\n                                keep_cross_validation_predictions=True,  #need this for training the meta-model later\n                                seed=seed) \n    model.train(x=x, y=y, training_frame=train)\n\n    #store model\n    xgboost_lightgbm_models.append(model)\n\n    #get cross-validation rmse\n    cv_metrics_df = model.cross_validation_metrics_summary().as_data_frame()\n    cv_rmse_index = cv_metrics_df[cv_metrics_df[''] == 'rmse'].index\n    cv_rmse = cv_metrics_df['mean'].iloc[cv_rmse_index]\n    return cv_rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)\n```", "```py\nbase_models = dnn_models + xgboost_lightgbm_models\n```", "```py\ndef objective(trial):\n    #GLM params to tune\n    meta_model_params = {\n        'alpha': trial.suggest_float('alpha', 0, 1),  #regularization distribution between L1 and L2\n        'family': trial.suggest_categorical('family', ['gaussian', 'tweedie']),  #read the documentation here on which family your target may fall into: https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html\n        'standardize': trial.suggest_categorical('standardize', [True, False]),\n        'non_negative': True  #predictions of each base model cannot be subtracted from one another\n    }\n\n    ensemble = H2OStackedEnsembleEstimator(metalearner_algorithm='glm',\n                                             metalearner_params=meta_model_params,\n                                             metalearner_nfolds=5,\n                                             base_models=base_models,  \n                                             seed=seed)\n\n    ensemble.train(x=x, y=y, training_frame=train)\n\n    #get cross-validation rmse\n    cv_metrics_df = ensemble.cross_validation_metrics_summary().as_data_frame()\n    cv_rmse_index = cv_metrics_df[cv_metrics_df[''] == 'rmse'].index\n    cv_rmse = cv_metrics_df['mean'].iloc[cv_rmse_index]\n    return cv_rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)\n```", "```py\nbest_meta_model_params = study.best_params\nbest_ensemble = H2OStackedEnsembleEstimator(metalearner_algorithm='glm',\n                                            metalearner_params=best_meta_model_params,\n                                            base_models=base_models,\n                                            seed=seed)\n\nbest_ensemble.train(x=x, y=y, training_frame=train)\n```", "```py\nbest_ensemble.summary()\n```", "```py\nensemble_val_rmse = best_ensemble.model_performance(val).rmse()\nensemble_val_rmse   #0.31475634111745304\n```", "```py\nbase_val_rmse = []\nfor i in range(len(base_models)):\n    base_val_rmse = base_models[i].model_performance(val).rmse()\n\nmodels = ['H2ODeepLearningEstimator'] * len(dnn_models) + ['H2OXGBoostEstimator'] * len(xgboost_lightgbm_models)\n\nbase_val_rmse_df = pd.DataFrame([models, base_val_rmse]).T\nbase_val_rmse_df.columns = ['model', 'val_rmse']\nbase_val_rmse_df = base_val_rmse_df.sort_values(by='val_rmse', ascending=True).reset_index(drop=True)\nbase_val_rmse_df.head(15)  #show only the top 15 in terms of lowest val_rmse\n```"]