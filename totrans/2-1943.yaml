- en: Supervised & Unsupervised Approach to Topic Modelling in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/supervised-unsupervised-approach-to-topic-modelling-in-python-d03e0b9da1dc](https://towardsdatascience.com/supervised-unsupervised-approach-to-topic-modelling-in-python-d03e0b9da1dc)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Build a Topic Modelling Pipeline from Scratch in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://vatsal12-p.medium.com/?source=post_page-----d03e0b9da1dc--------------------------------)[![Vatsal](../Images/f9648ff1f084b5b3361d90caf8c15959.png)](https://vatsal12-p.medium.com/?source=post_page-----d03e0b9da1dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d03e0b9da1dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d03e0b9da1dc--------------------------------)
    [Vatsal](https://vatsal12-p.medium.com/?source=post_page-----d03e0b9da1dc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d03e0b9da1dc--------------------------------)
    ·11 min read·Jan 31, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0091d080160c43289a310e2c3495bcb5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image taken from [Unsplash](https://unsplash.com/photos/c9OfrVeD_tQ) by [v2osk](https://unsplash.com/@v2osk)
  prefs: []
  type: TYPE_NORMAL
- en: This article will provide a high level intuition behind topic modelling and
    its associated applications. It will do a deep dive into various ways one can
    approach solving a problem which requires topic modelling and how you can solve
    those problems in both a supervised and unsupervised manner. I placed an emphasis
    on restructuring the data and initial problem such that the solution can be executed
    in a variety of methods. The following table breaks down the contents in the article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table of Contents**'
  prefs: []
  type: TYPE_NORMAL
- en: What is Topic Modelling?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of Topic Modelling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised vs Unsupervised Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem Breakdown
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- Load Data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Cleaning & Preprocessing'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Data Statistics'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unsupervised Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- Train Model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Visualization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Topic Analysis'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Supervised Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- Keyword Statistics'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Generate Labels'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Train Model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Evaluation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Concluding Remarks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Topic Modelling?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topic modelling is a subsection of natural language processing (NLP) or text
    mining which aims to build models in order to parse various bodies of text with
    the goal of identifying topics mapped to the text. These models assist in identifying
    big picture topics associated with documents at scale. It is a useful tool for
    understanding and organizing large collections of text data, and can help organizations
    make sense of large amounts of unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of Topic Modelling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Document classification — Categorize documents into various topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Social media analysis](https://medium.com/towards-data-science/mining-modelling-character-networks-part-i-e37e4878c467)
    — Identify the major topics associated to posts by users on social media'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Recommender systems](https://medium.com/towards-data-science/recommendation-systems-explained-a42fc60591ed)
    — Recommend products to users based on the topics that they’re interested in.
    A common application is customized advertisement recommendation based on the topics
    the user is interested in. For example, if a user was into cars, they might like
    advertisements from promising car brands like Honda / Toyota.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised vs Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There’s a clear distinction between supervised and unsupervised learning. Supervised
    learning is associated with training models given some label to map to the initial
    dataset. On the contrary, unsupervised learning is associated with training models
    given no labelled information present. Topic modelling is generally an unsupervised
    learning approach but this article will cover both a supervised and unsupervised
    learning approach to topic modelling.
  prefs: []
  type: TYPE_NORMAL
- en: The supervised learning approach will consist of binary classification. Binary
    classification is mapping the input data to exactly 2 targets, whereas multi-class
    classification is mapping the input data to more than 2 targets. The binary classification
    topic models will indicate whether the input article is or is not mapped to a
    topic we’ve labelled or not. The multi-class classification topic models will
    identify the topic this article is most likely to fall under given a set of topics.
    This article will showcase the implementation of the binary classification approach.
  prefs: []
  type: TYPE_NORMAL
- en: Problem Breakdown
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The problem this article is aiming to solve is to identify the main topics associated
    with research papers given the summary of the paper. Based on the topics identified,
    the user can then infer whether this paper is of interest to them or not. We will
    be using the arXiv database to query and fetch several research papers across
    a variety of domains.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following are the required modules and versions necessary to follow along
    with this tutorial. The version of Python in my environment is `3.10.0.` If an
    error does occur during the execution process, be mindful of the versions associated
    with the modules you’re referencing as this is commonly a problem in collaboration
    across platforms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you don’t have the gensim package installed, [here](https://pypi.org/project/gensim/)
    is the library documentation to install it through the command line. Similarly,
    you can install the arXiv package in Python with the following instructions [here](https://pypi.org/project/arxiv/).
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Based on the arXiv terms of use for their API it is completely free and encouraged
    to use. For more information regarding their terms of use, please reference their
    documentation which you can find [here](https://arxiv.org/help/api/tou).
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will show you how to hit the API through Python to collect
    the following information necessary for the models we’re building today. If you
    want to hit this API through other programming languages, or just want more information
    on how to use the API, I highly encourage you to reference their documentation
    which you can find [here](https://arxiv.org/help/api/user-manual).
  prefs: []
  type: TYPE_NORMAL
- en: Load Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code provided below will consist of importing the required modules, setting
    up constants to be used throughout the project and defining a function to query
    and load data from arXiv given a set of prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'That script should yield a resulting pandas DataFrame looking similar to the
    screenshot below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bea47659e5184e1ea335c1b17e122a63.png)'
  prefs: []
  type: TYPE_IMG
- en: Data queried from arXiv. Image provided by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning & Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we can clean and preprocess the summary information associated with each
    article. The cleaning and preprocessing phase of working with text data is essential
    in optimizing the performance of the underlying models. The lower the quality
    of data you feed into the model, the lower the performance of it will be in production
    settings. Furthermore, the amount of data you clean, preprocess and reduce will
    impact the training and inference time associated with the model. This will overall
    improve the experiments you run and performance in production. Topic modelling
    algorithms rely on the frequency of words within a document to identify patterns
    and topics, so any irrelevant information passed in can skew the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The text preprocessing we will be doing will consist of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Unicode the input data. This is critical when working with data in different
    languages. It will convert `à` into `á` , this will be critical during the cleaning
    phase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lowering the text such that all upper case characters are now lower case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The text cleaning we will be doing will consist of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing punctuations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When removing stop words, be mindful of the data you’re working with. The reason
    you would want to remove stop words is because they don’t provide any new information
    and it aids in optimizing the performance of the model. The instances where you
    wouldn’t want to remove stop words is when the context around the sentence matters.
    Not removing stop words would be useful for things like sentiment analysis and
    summarization. However, for our use case of topic modelling, we can proceed with
    removal of stop words.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the cleaned data should yield a new column called `cleaned_summary`
    . The resulting dataset should look something similar to the image shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/669bb78975b4e0893e2b32ee9e2281cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformed initial dataset through cleaning and preprocessing of the summary
    column. Image provided by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Data Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s investigate the word count breakdown associated with the cleaned dataset
    and identify the underlying distribution associated with the word count.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fdbeac5399acbb8670f8d5b5b7f5b1fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Word count distribution on the cleaned summaries associated with a sample of
    research papers from arXiv. Image provided by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Based on this, it seems that out of the 1548 sample of articles we queried from
    arXiv, ~700 of them have fewer than 100 words. This corresponds to 45.1% of the
    data having fewer than 100 words.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using LDA as the topic modelling algorithm in Python for the unsupervised
    learning approach associated with identifying the topics of research papers. LDA
    is a common approach to topic modelling and is the same approach large organizations
    like AWS provide as a service when using their `Comprehend` tool. This approach
    will essentially outline the backend code AWS would be using to process documents
    and generate topics for each of them in an unsupervised approach. At least this
    way, you won’t have to pay for it (aside from computing costs — which depends
    on the quantity of data you’re working with).
  prefs: []
  type: TYPE_NORMAL
- en: Train Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the model object associated with the data we prepared to train
    in it. We can create a few unique visualizations which would help provide insights
    associated to the topics and keywords the model has identified for each article.
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to be using the `pyLDAvis` library for the following visualization.
    Be aware that more recent versions of this library doesn’t support the visualization
    in JupyterNotebooks. I highly encourage this module to be installed with the specific
    version `2.1.2` as outlined in the requirements section of this article. [This](https://stackoverflow.com/questions/66096149/pyldavis-visualization-from-gensim-not-displaying-the-result-in-google-colab)
    thread on Stack Overflow highlights the difficulty of generating this visualization
    on different versions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6faac50bb440006f13625959d71a76d.png)'
  prefs: []
  type: TYPE_IMG
- en: LDA topic visualized for top 30 most frequent terms per topic. Image provided
    by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb4ae2e933746d1f181ea0ed7ab7df74.png)'
  prefs: []
  type: TYPE_IMG
- en: Word cloud of the topics identified by the LDA model. Image provided by the
    author.
  prefs: []
  type: TYPE_NORMAL
- en: There are 10 word cloud images created by the script above, but only 2 are showcased
    in this article. As you can see that there is quite a bit of overlap between the
    topics (namely with terms like model and models). Based on this it can be seen
    that further preprocessing and cleaning would be required to take the stem of
    a word, remove further stop words like `use, show, first, also, may, one, number,
    etc...` . The model development process is an iterative one but this highly outlines
    the importance of having high quality data being fed into the model.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of that we can also see that the two topics identified by this approach
    are quite unique. The first topic seems to dive into things centered around quantum
    computing and deep learning whereas the second topic is centered around machine
    learning, automated machine learning and data.
  prefs: []
  type: TYPE_NORMAL
- en: Topic Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/963f7a02cd564d5bad536b08e13c9c86.png)'
  prefs: []
  type: TYPE_IMG
- en: Frequency of learned topics with the threshold greater than 0.3\. Image provided
    by the author.
  prefs: []
  type: TYPE_NORMAL
- en: It seems that a majority of the articles which the model was trained on fall
    under the first and fifth topics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac19bdc5499848dcb39359d444b5baba.png)'
  prefs: []
  type: TYPE_IMG
- en: Term frequency of the top 30 words. Image provided by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, based on this it seems that further data cleaning and preprocessing
    can be done. As `data, model and models` are the most frequent terms, we wouldn’t
    want the model to be influenced by these words as they’re not distinctive enough.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00b86ede10472f6df671802522dbdae0.png)'
  prefs: []
  type: TYPE_IMG
- en: Top relevant keywords and document count associated with each topic. Image provided
    by the author.
  prefs: []
  type: TYPE_NORMAL
- en: As corroborated by the two images prior, the most popular topics being predicted
    are 5 and 0\. Both topics use words like `model` and `data` which should be removed
    on further iteration. This was the first iteration of the model development process
    for this approach. The model which goes into production will never be the first
    model you train, it is imperative to take the results of models from previous
    iterations to influence the changes required for models in future iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The supervised learning approach to topic modelling will consist of generating
    topic labels to train a binary classification model. This can be done by identifying
    the keywords associated to topics we are interested in labelling and predicting.
    I will mainly focus on the three topics of `machine learning, nlp (natural language
    processing) and mathematics` .
  prefs: []
  type: TYPE_NORMAL
- en: This is the set of keywords I’ve identified per topic. By no means is this list
    exhaustive but it is good enough to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We can parse through the cleaned summary associated with each article and identify
    which summaries had any keywords which we’re interested in and link it back to
    the original topic those keywords were mapped to. This will provide us with labels
    for each of the topics above. We can use TF-IDF to transform the input summary
    into a vector corresponding to the articles being fed into the model.
  prefs: []
  type: TYPE_NORMAL
- en: Keyword Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/707c7bb722e05073ec95c4ba44e7b420.png)'
  prefs: []
  type: TYPE_IMG
- en: Count of articles with the corresponding keyword counts. Image provided by the
    author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a43d4a9d2d90131d1edd1a6c503aeec3.png)'
  prefs: []
  type: TYPE_IMG
- en: Frequency of keyword occurrences throughout the articles. Image provided by
    the author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Generate Labels**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/5aa80bb712fe013c6a71af2a21e86d4f.png)'
  prefs: []
  type: TYPE_IMG
- en: From the 1548 documents we are working with, given the keywords associated to
    the topics defined above, this is the count of documents which have a positive
    label for the corresponding topic. Image provided by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98637e570aa4bd2378d74e278f1c5722.png)'
  prefs: []
  type: TYPE_IMG
- en: The corresponding data frame after the label generation. Image provided by the
    author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Train Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/73b167a06f40d8ff8ee0898a10107608.png)'
  prefs: []
  type: TYPE_IMG
- en: The script above will yield the following sklearn pipeline corresponding to
    the cleaned summaries and labels we’ve generated above. Image provided by the
    author.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we generated a holdout set during the training phase, we can now pass
    in the trained models against the holdout set to identify the performance of the
    model. Be advised that given that we’re working with a small sample of data with
    a class imbalance, there is a high likelihood that the trained model will be overfit.
    This can be resolved fairly easily (in our case) by increasing the number of articles
    we label and train the model on. All this means is that we should query arXiv
    for a larger dataset and generate better keywords and for labelling the articles.
    This might not be an easy issue for you to resolve if you’re working with a different
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: I would also highly encourage you to try out multiple classification models
    and not just the gradient boosting classifier. As stated before, iteration is
    an integral part of the machine learning development cycle!
  prefs: []
  type: TYPE_NORMAL
- en: Concluding Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article aimed to be a tutorial for the reader with the intention of providing
    both a supervised and unsupervised learning approach to topic modelling. I hope
    that I was able to outline the change in mindset when looking at the underlying
    data and how that can impact and broaden the approach to solving a particular
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: I also hope that this article outlined the importance of iteration in machine
    learning. The model which goes into production will never be the first model you
    train, it is imperative to take the results of models from previous iterations
    to influence the changes required for models in future iterations.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that it is also clear that the results of the unsupervised learning approach
    can influence the supervised learning approach. It could also bring forth a semi-supervised
    learning approach to topic modelling where you train a binary classification model
    on the results of the LDA model.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to download the jupyter notebook associated with this tutorial,
    I have provided it [here](https://github.com/vatsal220/medium_articles/blob/main/topic_modelling/topic_model.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Topic_model](https://en.wikipedia.org/wiki/Topic_model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html](https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you enjoyed reading through the article I wrote today, here are a few others
    I’ve written around the topic of natural language processing which you might also
    enjoy!
  prefs: []
  type: TYPE_NORMAL
- en: '[](/text-similarity-w-levenshtein-distance-in-python-2f7478986e75?source=post_page-----d03e0b9da1dc--------------------------------)
    [## Text Similarity w/ Levenshtein Distance in Python'
  prefs: []
  type: TYPE_NORMAL
- en: Building a Plagiarism Detection Pipeline in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/text-similarity-w-levenshtein-distance-in-python-2f7478986e75?source=post_page-----d03e0b9da1dc--------------------------------)
    [](/word2vec-explained-49c52b4ccb71?source=post_page-----d03e0b9da1dc--------------------------------)
    [## Word2Vec Explained
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the Intuition of Word2Vec & Implementing it in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/word2vec-explained-49c52b4ccb71?source=post_page-----d03e0b9da1dc--------------------------------)
    [](/text-summarization-in-python-with-jaro-winkler-and-pagerank-72d693da94e8?source=post_page-----d03e0b9da1dc--------------------------------)
    [## Text Summarization in Python with Jaro-Winkler and PageRank
  prefs: []
  type: TYPE_NORMAL
- en: Building a Text Summarizer with Jaro-Winkler and PageRank
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/text-summarization-in-python-with-jaro-winkler-and-pagerank-72d693da94e8?source=post_page-----d03e0b9da1dc--------------------------------)
    [](/identifying-tweet-sentiment-in-python-7c37162c186b?source=post_page-----d03e0b9da1dc--------------------------------)
    [## Identifying Tweet Sentiment in Python
  prefs: []
  type: TYPE_NORMAL
- en: How to use Tweepy and Textblob to Identify Tweet Sentiment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/identifying-tweet-sentiment-in-python-7c37162c186b?source=post_page-----d03e0b9da1dc--------------------------------)
  prefs: []
  type: TYPE_NORMAL
