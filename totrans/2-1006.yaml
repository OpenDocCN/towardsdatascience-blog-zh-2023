- en: 'GPT vs BERT: Which is Better?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gpt-vs-bert-which-is-better-2f1cf92af21a](https://towardsdatascience.com/gpt-vs-bert-which-is-better-2f1cf92af21a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Comparing two large-language models: Approach and example'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)[![Pranay
    Dave](../Images/accecc418ea23d26862761bf470fcf04.png)](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)
    [Pranay Dave](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)
    ·6 min read·Jun 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/948d3d929fc0378773816d8614d45deb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by DALLE and PPT by author ([https://labs.openai.com/s/qQgpAQbLi0srYlZHOHQjGKWh](https://labs.openai.com/s/qQgpAQbLi0srYlZHOHQjGKWh))
  prefs: []
  type: TYPE_NORMAL
- en: 'The rise in popularity of generative AI has also led to an increase in the
    number of large language models. In this story, I will make a comparison between
    two of them: GPT and BERT. GPT (Generative Pre-trained Transformer) is developed
    by OpenAI and is based on decoder-only architecture. On the other hand, BERT (Bidirectional
    Encoder Representations from Transformers) is developed by Google and is an encoder-only
    pre-trained model'
  prefs: []
  type: TYPE_NORMAL
- en: Both are technically different, but, they have a similar objective — to perform
    natural language processing tasks. Many articles compare the two from a technical
    point of view. However, in this story, I would compare them based on the quality
    of their objective, which is natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How to compare two completely different technical architectures? GPT is decoder-only
    architecture and BERT is encoder-only architecture. So a technical comparison
    of a decoder-only vs encoder-only architecture is like comparing Ferrari vs Lamborgini
    — both are great but with completely different technology under the chassis.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can make a comparison based on the quality of a common natural language
    task that both can do — which is the generation of embeddings. The embeddings
    are vector representations of a text. The embeddings form the basis of any natural
    language processing task. So if we can compare the quality of embeddings, then
    it can help us judge the quality of natural language tasks, as embeddings are
    foundational for natural language processing by transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Shown below is the comparison approach which I will take.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24359bd94d2eed67508bada548a63ec7.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison approach (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Let us start with GPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I made a toss of a coin, and GPT won the toss! So let us start with GPT first.
    I will take text from Amazon's fine food reviews dataset. Reviews are a good way
    to test both models, as reviews are expressed in natural language and are very
    spontaneous. They encompass the feeling of customers, and can contain all types
    of languages — good, bad, the ugly! In addition, they can have many misspelled
    words, emojis as well as commonly used slang.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example of the review text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ea4d5c9a51dd915cf658041d04f337e.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a customer review (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: In order to get the embeddings of the text using GPT, we need to make an API
    call to OpenAI. The result is embedding or vector of size of 1540 for each text.
    Here is a sample data which includes the embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a73855bdf98f492a735fdfbc55496629.png)'
  prefs: []
  type: TYPE_IMG
- en: Embeddings obtained from model (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The next step is clustering and visualization. One can use KMeans to cluster
    the embedding vector and use TSNE to reduce the 1540 dimensions to 2 dimensions.
    Shown below are the results after clustering and dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/543dc8fc36262e8c118c163b3f02e17c.png)'
  prefs: []
  type: TYPE_IMG
- en: GPT embedding clustering (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: One can observe that the clusters are very well formed. Hovering over some of
    the clusters can help understand the meaning of the clusters. For example, the
    red cluster is related to dog food. Further analysis also shows that GPT embeddings
    have correctly identified that the word ‘Dog’ and ‘Dawg’ are similar and placed
    them in the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, GPT embeddings give good results as indicated by the quality of clustering.
  prefs: []
  type: TYPE_NORMAL
- en: It's now BERT’s turn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can BERT perform better? Let us find out. There are multiple versions of the
    BERT model such as bert-base-case, bert-base-uncased, etc.. Essentially they have
    different embedding vector sizes. Here is the result based on Bert base which
    has an embedding size of 768.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fef028ff4f6f8e13ba43cafd60909911.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT embedding (768) clustering (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The green cluster corresponds to dog food. However one can observe that the
    clusters are widely spread and not very compact compared to GPT. The main reason
    is that the embedding vector length of 768 is inferior compared to the embedding
    vector length of 1540 of GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, BERT also offers a higher embedding size of 1024\. Here are the
    results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a38adeaea80af457eb5c32b792053121.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT embedding (1024) clustering (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Here the orange cluster corresponds to dog food. The cluster is relatively compact,
    which is a better result compared to the embedding of 768\. However, there are
    some points which are far away from the center. These points are incorrectly classified.
    For example, there is a review for coffee, but it has got incorrectly classified
    as dog food because it has got a word Dog in it.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clearly, GPT does a better job and provides higher-quality embeddings compared
    to BERT. However, I would not like to give all credit to GPT as there are other
    aspects to the comparison. Here is a summary table
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/275c1b72b0c27fc30ca42c4354f1fac3.png)'
  prefs: []
  type: TYPE_IMG
- en: GPT wins over BERT for the embedding quality provided by the higher embedding
    size. However, GPT required a paid API, while BERT is free. In addition, the BERT
    model is open-source, and not black-box so you can make further analysis to understand
    it better. The GPT models from OpenAI are black-box.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, I would recommend using BERT for medium complex text such as
    web pages or books which have curated text. GPT can be used for very complex text
    such as customer reviews which are completely in natural language and not curated.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here is a Python code snippet that implements the process described in the story.
    For illustration, I have given GPT example. The BERT one is similar.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Dataset citation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset is available here with license CC0 Public domain. [**Both commercial
    and non-commercial use of it is permitted**](https://www.ibm.com/community/terms-of-use/download/)**.**
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews?source=post_page-----2f1cf92af21a--------------------------------)
    [## Amazon Fine Food Reviews'
  prefs: []
  type: TYPE_NORMAL
- en: Analyze ~500,000 food reviews from Amazon
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.kaggle.com](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews?source=post_page-----2f1cf92af21a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Please **subscribe** to stay informed whenever I release a new story.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pranay-dave9.medium.com/subscribe?source=post_page-----2f1cf92af21a--------------------------------)
    [## Get an email whenever Pranay Dave publishes.'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Pranay Dave publishes. By signing up, you will create
    a Medium account if you don't already have…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pranay-dave9.medium.com](https://pranay-dave9.medium.com/subscribe?source=post_page-----2f1cf92af21a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: You can also **join Medium** with my referral link
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pranay-dave9.medium.com/membership?source=post_page-----2f1cf92af21a--------------------------------)
    [## Join Medium with my referral link - Pranay Dave'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pranay-dave9.medium.com](https://pranay-dave9.medium.com/membership?source=post_page-----2f1cf92af21a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Additional Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can visit my website to make analytics with zero coding. [**https://experiencedatascience.com**](https://experiencedatascience.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Youtube channel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please visit my YouTube channel to learn data science and AI use cases using
    demos
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.youtube.com/c/DataScienceDemonstrated?source=post_page-----2f1cf92af21a--------------------------------)
    [## Data Science Demonstrated'
  prefs: []
  type: TYPE_NORMAL
- en: Learn data science through demos. Whatever profession you are in , sit back
    , relax and enjoy the videos. My name is…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.youtube.com](https://www.youtube.com/c/DataScienceDemonstrated?source=post_page-----2f1cf92af21a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
