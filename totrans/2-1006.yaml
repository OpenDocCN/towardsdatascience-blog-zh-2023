- en: 'GPT vs BERT: Which is Better?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT与BERT：哪一个更好？
- en: 原文：[https://towardsdatascience.com/gpt-vs-bert-which-is-better-2f1cf92af21a](https://towardsdatascience.com/gpt-vs-bert-which-is-better-2f1cf92af21a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gpt-vs-bert-which-is-better-2f1cf92af21a](https://towardsdatascience.com/gpt-vs-bert-which-is-better-2f1cf92af21a)
- en: 'Comparing two large-language models: Approach and example'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较两个大型语言模型：方法与示例
- en: '[](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)[![Pranay
    Dave](../Images/accecc418ea23d26862761bf470fcf04.png)](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)
    [Pranay Dave](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)[![Pranay
    Dave](../Images/accecc418ea23d26862761bf470fcf04.png)](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)
    [Pranay Dave](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)
    ·6 min read·Jun 23, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)
    ·阅读时长6分钟·2023年6月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/948d3d929fc0378773816d8614d45deb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/948d3d929fc0378773816d8614d45deb.png)'
- en: Image created by DALLE and PPT by author ([https://labs.openai.com/s/qQgpAQbLi0srYlZHOHQjGKWh](https://labs.openai.com/s/qQgpAQbLi0srYlZHOHQjGKWh))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由DALLE创建，PPT由作者制作（[https://labs.openai.com/s/qQgpAQbLi0srYlZHOHQjGKWh](https://labs.openai.com/s/qQgpAQbLi0srYlZHOHQjGKWh)）
- en: 'The rise in popularity of generative AI has also led to an increase in the
    number of large language models. In this story, I will make a comparison between
    two of them: GPT and BERT. GPT (Generative Pre-trained Transformer) is developed
    by OpenAI and is based on decoder-only architecture. On the other hand, BERT (Bidirectional
    Encoder Representations from Transformers) is developed by Google and is an encoder-only
    pre-trained model'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI的流行也导致了大型语言模型数量的增加。在这个故事中，我将对两个模型进行比较：GPT和BERT。GPT（Generative Pre-trained
    Transformer）由OpenAI开发，基于仅解码器架构。而BERT（Bidirectional Encoder Representations from
    Transformers）由Google开发，是一个仅编码器的预训练模型。
- en: Both are technically different, but, they have a similar objective — to perform
    natural language processing tasks. Many articles compare the two from a technical
    point of view. However, in this story, I would compare them based on the quality
    of their objective, which is natural language processing.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然技术上有所不同，但它们有一个相似的目标——执行自然语言处理任务。许多文章从技术角度比较这两者。然而，在这个故事中，我将基于它们的目标质量，即自然语言处理，进行比较。
- en: Comparison approach
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较方法
- en: How to compare two completely different technical architectures? GPT is decoder-only
    architecture and BERT is encoder-only architecture. So a technical comparison
    of a decoder-only vs encoder-only architecture is like comparing Ferrari vs Lamborgini
    — both are great but with completely different technology under the chassis.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如何比较两种完全不同的技术架构？GPT是仅解码器架构，而BERT是仅编码器架构。因此，将仅解码器与仅编码器架构进行技术比较，就像比较法拉利与兰博基尼——两者都很棒，但底盘下的技术完全不同。
- en: However, we can make a comparison based on the quality of a common natural language
    task that both can do — which is the generation of embeddings. The embeddings
    are vector representations of a text. The embeddings form the basis of any natural
    language processing task. So if we can compare the quality of embeddings, then
    it can help us judge the quality of natural language tasks, as embeddings are
    foundational for natural language processing by transformer architecture.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以基于它们能够执行的共同自然语言任务的质量进行比较——即生成嵌入。嵌入是文本的向量表示。嵌入是任何自然语言处理任务的基础。因此，如果我们能够比较嵌入的质量，这将有助于我们判断自然语言任务的质量，因为嵌入是变换器架构进行自然语言处理的基础。
- en: Shown below is the comparison approach which I will take.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 下面展示的是我将采取的比较方法。
- en: '![](../Images/24359bd94d2eed67508bada548a63ec7.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24359bd94d2eed67508bada548a63ec7.png)'
- en: Comparison approach (image by author)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 比较方法（图片由作者制作）
- en: Let us start with GPT
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们从GPT开始
- en: I made a toss of a coin, and GPT won the toss! So let us start with GPT first.
    I will take text from Amazon's fine food reviews dataset. Reviews are a good way
    to test both models, as reviews are expressed in natural language and are very
    spontaneous. They encompass the feeling of customers, and can contain all types
    of languages — good, bad, the ugly! In addition, they can have many misspelled
    words, emojis as well as commonly used slang.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我掷了一枚硬币，GPT 赢了！所以我们先从 GPT 开始。我将从亚马逊的优质食品评论数据集中提取文本。评论是测试两个模型的好方法，因为评论用自然语言表达，非常自发。它们包含了客户的感受，可以包含所有类型的语言——好的、坏的、丑的！此外，它们可能有许多拼写错误的词、表情符号以及常用的俚语。
- en: Here is an example of the review text.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是评论文本的一个示例。
- en: '![](../Images/4ea4d5c9a51dd915cf658041d04f337e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ea4d5c9a51dd915cf658041d04f337e.png)'
- en: Example of a customer review (image by author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 客户评论示例（图像由作者提供）
- en: In order to get the embeddings of the text using GPT, we need to make an API
    call to OpenAI. The result is embedding or vector of size of 1540 for each text.
    Here is a sample data which includes the embeddings.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 GPT 获取文本的嵌入，我们需要向 OpenAI 发起 API 调用。结果是每个文本的嵌入或向量大小为 1540。以下是包括这些嵌入的示例数据。
- en: '![](../Images/a73855bdf98f492a735fdfbc55496629.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a73855bdf98f492a735fdfbc55496629.png)'
- en: Embeddings obtained from model (image by author)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型获得的嵌入（图像由作者提供）
- en: The next step is clustering and visualization. One can use KMeans to cluster
    the embedding vector and use TSNE to reduce the 1540 dimensions to 2 dimensions.
    Shown below are the results after clustering and dimensionality reduction.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是聚类和可视化。可以使用 KMeans 对嵌入向量进行聚类，并使用 TSNE 将 1540 维降至 2 维。下图展示了聚类和降维后的结果。
- en: '![](../Images/543dc8fc36262e8c118c163b3f02e17c.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/543dc8fc36262e8c118c163b3f02e17c.png)'
- en: GPT embedding clustering (image by author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 嵌入聚类（图像由作者提供）
- en: One can observe that the clusters are very well formed. Hovering over some of
    the clusters can help understand the meaning of the clusters. For example, the
    red cluster is related to dog food. Further analysis also shows that GPT embeddings
    have correctly identified that the word ‘Dog’ and ‘Dawg’ are similar and placed
    them in the same cluster.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可以观察到集群形成得很好。悬停在某些集群上可以帮助理解集群的含义。例如，红色集群与狗粮相关。进一步分析还表明，GPT 嵌入正确识别了“Dog”和“Dawg”是相似的，并将它们放在同一集群中。
- en: Overall, GPT embeddings give good results as indicated by the quality of clustering.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，GPT 嵌入提供了良好的结果，聚类质量也很高。
- en: It's now BERT’s turn
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在轮到 BERT 了
- en: Can BERT perform better? Let us find out. There are multiple versions of the
    BERT model such as bert-base-case, bert-base-uncased, etc.. Essentially they have
    different embedding vector sizes. Here is the result based on Bert base which
    has an embedding size of 768.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 能否表现得更好？让我们来看看。BERT 模型有多个版本，如 bert-base-case、bert-base-uncased 等。它们的嵌入向量大小不同。以下是基于
    Bert base 的结果，其嵌入大小为 768。
- en: '![](../Images/fef028ff4f6f8e13ba43cafd60909911.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fef028ff4f6f8e13ba43cafd60909911.png)'
- en: BERT embedding (768) clustering (image by author)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 嵌入（768）聚类（图像由作者提供）
- en: The green cluster corresponds to dog food. However one can observe that the
    clusters are widely spread and not very compact compared to GPT. The main reason
    is that the embedding vector length of 768 is inferior compared to the embedding
    vector length of 1540 of GPT.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 绿色集群对应于狗粮。然而，可以观察到这些集群分布较广，相较于 GPT 并不够紧凑。主要原因是 768 的嵌入向量长度相对于 GPT 的 1540 嵌入向量长度较低。
- en: Fortunately, BERT also offers a higher embedding size of 1024\. Here are the
    results.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，BERT 也提供了更高的嵌入大小 1024。以下是结果。
- en: '![](../Images/a38adeaea80af457eb5c32b792053121.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a38adeaea80af457eb5c32b792053121.png)'
- en: BERT embedding (1024) clustering (image by author)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 嵌入（1024）聚类（图像由作者提供）
- en: Here the orange cluster corresponds to dog food. The cluster is relatively compact,
    which is a better result compared to the embedding of 768\. However, there are
    some points which are far away from the center. These points are incorrectly classified.
    For example, there is a review for coffee, but it has got incorrectly classified
    as dog food because it has got a word Dog in it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里橙色集群对应于狗粮。该集群相对紧凑，相比于 768 的嵌入效果更好。然而，仍有一些点离中心较远。这些点被错误分类。例如，有一条咖啡评论，但由于包含了词“Dog”，被错误地分类为狗粮。
- en: Conclusion
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Clearly, GPT does a better job and provides higher-quality embeddings compared
    to BERT. However, I would not like to give all credit to GPT as there are other
    aspects to the comparison. Here is a summary table
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，GPT在嵌入质量上比BERT表现更好。然而，我不想将所有功劳归于GPT，因为还有其他方面需要比较。以下是总结表格
- en: '![](../Images/275c1b72b0c27fc30ca42c4354f1fac3.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/275c1b72b0c27fc30ca42c4354f1fac3.png)'
- en: GPT wins over BERT for the embedding quality provided by the higher embedding
    size. However, GPT required a paid API, while BERT is free. In addition, the BERT
    model is open-source, and not black-box so you can make further analysis to understand
    it better. The GPT models from OpenAI are black-box.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: GPT因提供更高嵌入质量而优于BERT。然而，GPT需要付费API，而BERT是免费的。此外，BERT模型是开源的，非黑箱，你可以进一步分析以更好地理解它。OpenAI的GPT模型是黑箱的。
- en: In conclusion, I would recommend using BERT for medium complex text such as
    web pages or books which have curated text. GPT can be used for very complex text
    such as customer reviews which are completely in natural language and not curated.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我建议使用BERT处理中等复杂度的文本，如有策划的网页或书籍。GPT可用于处理非常复杂的文本，如完全自然语言且未经过策划的客户评论。
- en: Technical Implementation
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术实现
- en: Here is a Python code snippet that implements the process described in the story.
    For illustration, I have given GPT example. The BERT one is similar.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个Python代码片段，实施了故事中描述的过程。为了说明，我给出了GPT的例子。BERT的实现类似。
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Dataset citation
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集引用
- en: The dataset is available here with license CC0 Public domain. [**Both commercial
    and non-commercial use of it is permitted**](https://www.ibm.com/community/terms-of-use/download/)**.**
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在这里提供，许可证为CC0公共领域。 [**商业和非商业使用均被允许**](https://www.ibm.com/community/terms-of-use/download/)**.**
- en: '[](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews?source=post_page-----2f1cf92af21a--------------------------------)
    [## Amazon Fine Food Reviews'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews?source=post_page-----2f1cf92af21a--------------------------------)
    [## 亚马逊优质食品评论'
- en: Analyze ~500,000 food reviews from Amazon
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析来自亚马逊的约500,000条食品评论
- en: www.kaggle.com](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews?source=post_page-----2f1cf92af21a--------------------------------)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.kaggle.com](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews?source=post_page-----2f1cf92af21a--------------------------------)'
- en: Please **subscribe** to stay informed whenever I release a new story.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请**订阅**以便在我发布新故事时保持信息更新。
- en: '[](https://pranay-dave9.medium.com/subscribe?source=post_page-----2f1cf92af21a--------------------------------)
    [## Get an email whenever Pranay Dave publishes.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pranay-dave9.medium.com/subscribe?source=post_page-----2f1cf92af21a--------------------------------)
    [## 每当 Pranay Dave 发布新内容时获取电子邮件通知。'
- en: Get an email whenever Pranay Dave publishes. By signing up, you will create
    a Medium account if you don't already have…
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当Pranay Dave发布新内容时获取电子邮件通知。注册后，如果你还没有Medium账户，将会创建一个...
- en: pranay-dave9.medium.com](https://pranay-dave9.medium.com/subscribe?source=post_page-----2f1cf92af21a--------------------------------)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[pranay-dave9.medium.com](https://pranay-dave9.medium.com/subscribe?source=post_page-----2f1cf92af21a--------------------------------)'
- en: You can also **join Medium** with my referral link
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过我的推荐链接**加入Medium**
- en: '[](https://pranay-dave9.medium.com/membership?source=post_page-----2f1cf92af21a--------------------------------)
    [## Join Medium with my referral link - Pranay Dave'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pranay-dave9.medium.com/membership?source=post_page-----2f1cf92af21a--------------------------------)
    [## 使用我的推荐链接加入 Medium - Pranay Dave'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为Medium会员，你的一部分会员费用将转给你阅读的作者，并且你可以完全访问每一个故事...
- en: pranay-dave9.medium.com](https://pranay-dave9.medium.com/membership?source=post_page-----2f1cf92af21a--------------------------------)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[pranay-dave9.medium.com](https://pranay-dave9.medium.com/membership?source=post_page-----2f1cf92af21a--------------------------------)'
- en: Additional Resources
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附加资源
- en: Website
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网站
- en: You can visit my website to make analytics with zero coding. [**https://experiencedatascience.com**](https://experiencedatascience.com/)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以访问我的网站进行零编码分析。[**https://experiencedatascience.com**](https://experiencedatascience.com/)
- en: Youtube channel
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YouTube 频道
- en: Please visit my YouTube channel to learn data science and AI use cases using
    demos
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请访问我的YouTube频道，通过演示学习数据科学和AI的应用案例
- en: '[](https://www.youtube.com/c/DataScienceDemonstrated?source=post_page-----2f1cf92af21a--------------------------------)
    [## Data Science Demonstrated'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.youtube.com/c/DataScienceDemonstrated?source=post_page-----2f1cf92af21a--------------------------------)
    [## 数据科学展示'
- en: Learn data science through demos. Whatever profession you are in , sit back
    , relax and enjoy the videos. My name is…
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.youtube.com](https://www.youtube.com/c/DataScienceDemonstrated?source=post_page-----2f1cf92af21a--------------------------------)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
