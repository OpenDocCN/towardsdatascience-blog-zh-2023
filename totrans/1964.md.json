["```py\nimport numpy as np \n\ndef init_policy(S: np.array, weight_dict: dict = {'right': 1}) -> dict:\n    # Saving all the unique states to a vector \n    states = np.unique(S)\n\n    # Getting the number of rows and columns of the S matrix\n    n_row = S.shape[0]\n    n_col = S.shape[1]\n\n    # Dictionary to hold each action for a given state\n    P = {}\n    for s in states: \n        s_dict = {}\n\n        # Checking which index is the current state in the S matrix \n        s_index = np.where(S == s)\n\n        # If the state is in the top left corner, we can only move right and down\n        if s_index == (0, 0):\n            s_dict['right'] = 0.5 * weight_dict['right']\n            s_dict['down'] = 1 - s_dict['right']\n\n        # If the state is in the top right corner, we can only move left and down\n        elif s_index == (0, n_col - 1):\n            s_dict['left'] = 0.5\n            s_dict['down'] = 0.5\n\n        # If the state is in the bottom left corner, we can only move right and up\n        elif s_index == (n_row - 1, 0):\n            s_dict['right'] = 0.5 * weight_dict['right']\n            s_dict['up'] = 1 - s_dict['right']\n\n        # If the state is in the bottom right corner, we can only move left and up\n        elif s_index == (n_row - 1, n_col - 1):\n            s_dict['left'] = 0.5\n            s_dict['up'] = 0.5\n\n        # If the state is in the first row, we can only move left, right, and down\n        elif s_index[0] == 0:\n            s_dict['right'] = 0.333 * weight_dict['right']\n            s_dict['left'] = (1 - s_dict['right']) / 2\n            s_dict['down'] =  (1 - s_dict['right']) / 2\n\n        # If the state is in the last row, we can only move left, right, and up\n        elif s_index[0] == n_row - 1:\n            s_dict['right'] = 0.333 * weight_dict['right']\n            s_dict['left'] =  (1 - s_dict['right']) / 2\n            s_dict['up'] = (1 - s_dict['right']) / 2\n\n        # If the state is in the first column, we can only move up, down, and right\n        elif s_index[1] == 0:\n            s_dict['right'] = 0.333 * weight_dict['right']\n            s_dict['up'] = (1 - s_dict['right']) / 2\n            s_dict['down'] = (1 - s_dict['right']) / 2\n\n        # If the state is in the last column, we can only move up, down, and left\n        elif s_index[1] == n_col - 1:\n            s_dict['up'] = 0.333\n            s_dict['down'] = 0.333\n            s_dict['left'] = 1 - s_dict['up'] - s_dict['down']\n\n        # If the state is in the middle, we can move in all directions\n        else:\n            s_dict['right'] = 0.25 * weight_dict['right']\n            s_dict['up'] = (1 - s_dict['right']) / 3\n            s_dict['down'] = (1 - s_dict['right']) / 3\n            s_dict['left'] = (1 - s_dict['right']) / 3\n\n        # Saving the current states trasition probabilities\n        P[s] = s_dict\n\n    return P\n\ndef generate_holes(nrow: int, ncol: int, start_coords: list, hole_coords: list, nholes: int = 1) -> list:\n    \"\"\"\n    Function that generates nholes in a gridworld \n\n    The holes cannot be: \n        - in the start state\n        - in the goal state\n    \"\"\"\n    # Generating the hole coordinates \n    # The hole cannot be in the start or goal state\n    hole_coords = []\n    for _ in range(nholes):\n\n        hole_row = np.random.randint(0, nrow - 1)\n        hole_col = np.random.randint(0, ncol - 1)\n\n        while (hole_row, hole_col) in start_coords or (hole_row, hole_col) in hole_coords:\n            hole_row = np.random.randint(0, nrow - 1)\n            hole_col = np.random.randint(0, ncol - 1)\n\n        # Appending to the hole coordinates list\n        hole_coords.append((hole_row, hole_col))\n\n    return hole_coords\n\ndef init_env(\n        n_rows: int, \n        n_cols: int,\n        step_reward: float = -1, \n        goal_reward: float = 10,\n        hole_reward: float = -10,\n        n_holes: int = 1,\n        random_seed: int = 42, \n        policy_weights: dict = {'right': 1}\n        ) -> np.array: \n    \"\"\"\n    Functionat that returns the initial environment: \n        S - the state matrix indexed by [row, col]\n        V - the initial value matrix indexed by [row, col]\n        R - the reward matrix indexed by [row, col]\n        A - the action matrix indexed by [row, col]\n        P - the probability dictionary where for each state, the keys are the actions and the values are the probabilities of the next state\n    \"\"\"\n    # Setting the random seed\n    np.random.seed(random_seed)\n\n    # Initiating the S matrix \n    S = np.arange(0, n_rows * n_cols).reshape(n_rows, n_cols)\n\n    # Creating the initial V matrix\n    V = np.zeros((n_rows, n_cols))\n\n    # The start state will be always the top left corner \n    # The goal state will be always the bottom right corner\n    # We will generate a random holes that our agent can fall in\n    # Any other state that is not the hole or the goal state will receive a step reward \n    goal_coord = (n_rows - 1, n_cols - 1)\n    R = np.zeros((n_rows, n_cols))\n    R.fill(step_reward)\n    R[0, 0] = step_reward\n    R[goal_coord] = goal_reward\n\n    # Generating the hole coordinates \n    # The hole cannot be in the start or goal state\n    hole_coords = generate_holes(n_rows, n_cols, [(0, 0)], [goal_coord], n_holes)\n\n    # Setting the hole reward\n    for hole_coord in hole_coords:\n        R[hole_coord] = hole_reward\n\n    # Initiating the policy \n    P = init_policy(S, weight_dict=policy_weights)\n\n    return S, V, R, P, hole_coords, [goal_coord]\n```", "```py\nS, V, R, P, hole_coords, goal_coard = init_env(5, 7, n_holes=4, random_seed=3)\n```", "```py\ndef array_index_to_matplot_coords(i: int, j: int, n_cols: int) -> Tuple[int, int]:\n    \"\"\"\n    Converts an array index to a matplot coordinate\n    \"\"\"\n    x = j\n    y = n_cols - i - 1\n    return x, y\n\ndef plot_matrix(\n    M: np.array, \n    goal_coords: list = [],\n    hole_coords: list = [],\n    img_width: int = 5, \n    img_height: int = 5, \n    title: str = None,\n    ) -> None: \n    \"\"\"\n    Plots a matrix as an image.\n    \"\"\"\n    height, width = M.shape\n\n    fig = plt.figure(figsize=(img_width, img_width))\n    ax = fig.add_subplot(111, aspect='equal')\n\n    for x in range(height):\n        for y in range(width):\n            # By default, the (0, 0) coordinate in matplotlib is the bottom left corner,\n            # so we need to invert the y coordinate to plot the matrix correctly\n            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n\n            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n            if (x, y) in goal_coords:\n                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n            # If there is a tuple of (x, y) in the hole_coords list, we color the cell salmon\n            elif (x, y) in hole_coords:\n                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='salmon'))\n\n            ax.annotate(str(M[x][y]), xy=(matplot_x, matplot_y), ha='center', va='center')\n\n    offset = .5    \n    ax.set_xlim(-offset, width - offset)\n    ax.set_ylim(-offset, height - offset)\n\n    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n\n    plt.title(title)\n    plt.show()\n\ndef plot_policy_matrix(P: dict, S:np.array, terminal_coords: list = [], img_width: int = 5, img_height: int = 5, title: str = None) -> None: \n    \"\"\" \n    Plots the policy matrix out of the dictionary provided; The dictionary values are used to draw the arrows \n    \"\"\"\n    height, width = S.shape\n\n    fig = plt.figure(figsize=(img_width, img_width))\n    ax = fig.add_subplot(111, aspect='equal')\n    for x in range(height):\n        for y in range(width):\n            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n\n            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n            if (x, y) in terminal_coords:\n                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n\n            else:\n                try:\n                    # Adding the arrows to the plot\n                    if 'up' in P[S[x, y]]:\n                        plt.arrow(matplot_x, matplot_y, 0, 0.3, head_width = 0.05, head_length = 0.05)\n                    if 'down' in P[S[x, y]]:\n                        plt.arrow(matplot_x, matplot_y, 0, -0.3, head_width = 0.05, head_length = 0.05)\n                    if 'left' in P[S[x, y]]:\n                        plt.arrow(matplot_x, matplot_y, -0.3, 0, head_width = 0.05, head_length = 0.05)\n                    if 'right' in P[S[x, y]]:\n                        plt.arrow(matplot_x, matplot_y, 0.3, 0, head_width = 0.05, head_length = 0.05)\n                except Exception as e:\n                    print(f\"Error: {e}\")\n                    print(f\"Current x and y: {x}, {y}\")\n\n    offset = .5    \n    ax.set_xlim(-offset, width - offset)\n    ax.set_ylim(-offset, height - offset)\n\n    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n\n    plt.title(title)\n```", "```py\nplot_matrix(S, goal_coords=goal_coard, hole_coords=hole_coords, title='State Matrix')\n```", "```py\nplot_matrix(R, goal_coords=goal_coard, hole_coords=hole_coords, title='Reward Matrix')\n```", "```py\nplot_policy_matrix(P, S, terminal_coords=hole_coords + goal_coard, title='Policy Matrix')\n```", "```py\nplot_matrix(V, goal_coords=goal_coard, hole_coords=hole_coords, title='Value Matrix')\n```", "```py\ndef select_move(s, S, P) -> int:\n    \"\"\"\n    Given the current state, returns the coordinates of the next state based on the current policy \n    \"\"\"\n    # Getting the current state index \n    s_index = np.where(S == s)\n\n    # Getting the current state policy\n    s_policy = P[s]\n\n    # Selecting the next action based on the current policy\n    next_action = np.random.choice(list(s_policy.keys()), p=list(s_policy.values()))\n\n    # Getting the next state coordinates based on the next action\n    try:\n        if next_action == 'up':\n            next_state = S[s_index[0] - 1, s_index[1]][0]\n        elif next_action == 'down':\n            next_state = S[s_index[0] + 1, s_index[1]][0]\n        elif next_action == 'left':\n            next_state = S[s_index[0], s_index[1] - 1][0]\n        elif next_action == 'right':\n            next_state = S[s_index[0], s_index[1] + 1][0]\n    except Exception as e: \n        print(f\"Current state: {s}\")\n        print(f'Next action: {next_action}')\n        print(f'Error: {e}')\n\n    return next_state\n```", "```py\ndef get_state_coords(s, S) -> tuple:\n    \"\"\"\n    Returns the state coordinates given the state index\n    \"\"\"\n    s_index = np.where(S == s)\n    return s_index[0][0], s_index[1][0]\n\ndef update_value(s, s_prime, S, P, V, R, alpha: float = 0.1, gamma: float = 0.9) -> float: \n    \"\"\"\n    Updates the current value function based on the current policy\n    \"\"\"\n    # Getting the CURRENT state's nrow and ncol index\n    s_index_now = get_state_coords(s, S)\n\n    # Getting the SELECTED state's nrow and ncol index\n    s_index_prime = get_state_coords(s_prime, S)\n\n    # Getting the reward by moving to the selected state \n    move_reward = R[s_index_prime[0], s_index_prime[1]]\n\n    # Getting the current estimated value of the selected state \n    current_value = V[s_index_now[0], s_index_now[1]]\n\n    # The next value \n    prime_value = V[s_index_prime[0], s_index_prime[1]]\n\n    # Returning the TD(0) current state value \n    return current_value + alpha * (move_reward + gamma * prime_value - current_value)\n```", "```py\ndef episode_exploration(S, P, V, R, terminal_state_coords: list, alpha: float = 0.1, gamma: float = 0.9) -> None: \n    \"\"\"\n    Agent exploration and value updating using TD(0) equation until a terminal state is reached\n    \"\"\"\n    # The starting state is 0 \n    s = 0 \n\n    # Keeping track of the number of moves\n    n_moves = 0\n\n    # Getting the coordinates of the s \n    s_coords = get_state_coords(s, S)\n\n    while s_coords not in terminal_state_coords:\n        # Selecting the next state based on the current policy\n        s_prime = select_move(s, S, P)\n\n        # Updating the current state value \n        V[s_coords] = update_value(s, s_prime, S, P, V, R, alpha, gamma)\n\n        # Updating the current state \n        s = s_prime\n\n        # Incrementing the number of moves\n        n_moves += 1\n\n        # Getting teh new s coords\n        s_coords = get_state_coords(s, S)\n\n    return n_moves\n```", "```py\n# Defining the number of episodes to explore \nn_episodes = 10000\n\n# We will plot the V matrix after each episode filling the same device plot to make an animation\nnumber_of_walks = []\nfor _ in tqdm(range(n_episodes)):\n    n = episode_exploration(S, P, V, R, terminal_state_coords=hole_coords + goal_coard, alpha=0.1, gamma=0.9)\n    number_of_walks.append(n)\n```", "```py\n# Ploting the distribution of the number of moves \nplt.figure(figsize=(10, 5))\nsns.kdeplot(number_of_walks, fill=True)\nplt.title(f'Number of moves distribution | Mean: {np.mean(number_of_walks):.2f} | Std: {np.std(number_of_walks):.2f}')\nplt.xlabel('Number of moves')\nplt.ylabel('Frequency')\nplt.show()\n```", "```py\n# Assiging a different policy\nS, V, R, P, hole_coords, goal_coard = init_env(5, 7, n_holes=4, random_seed=3, policy_weights={'right': 1.5}) \n\n# Defining the number of episodes to explore \nn_episodes = 10000\n\n# We will plot the V matrix after each episode filling the same device plot to make an animation\nnumber_of_walks = []\nfor _ in tqdm(range(n_episodes)):\n    n = episode_exploration(S, P, V, R, terminal_state_coords=hole_coords + goal_coard, alpha=0.1, gamma=0.9)\n    number_of_walks.append(n)\n```"]