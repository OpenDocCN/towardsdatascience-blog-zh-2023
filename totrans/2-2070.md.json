["```py\nimport tiktoken\n\ndef tokenize(text, batch_size):\n    \"\"\"Convert text to numerical tokens and repeat batch_size times.\"\"\"\n    encoding = tiktoken.encoding_for_model(\"davinci\")\n    token_list = encoding.encode(text)\n    token_tensor = torch.tensor(token_list, dtype=torch.long) # (input_seq_len)\n    token_tensor = token_tensor.unsqueeze(0) # (1, input_seq_len)\n    token_tensor = token_tensor.repeat(batch_size, 1) # (batch_size, input_seq_len)\n    return encoding, token_tensor\n\ndef limit_sequence_length(input_tokens, block_size):\n    \"\"\"Limit the input to at most block_size tokens.\"\"\"\n    input_seq_len = input_tokens.size(1)\n    seq_len = min(input_seq_len, block_size)\n    block_tokens = input_tokens[:, -seq_len:] # (batch_size, seq_len)\n    return block_tokens\n\ndef generate_next_token(model, tokens):\n    \"\"\"Use the highest probability from the Transformer model to choose the next token.\"\"\"\n    mask = subsequent_mask(tokens.size(1)) # (1, seq_len, seq_len)\n    decoder_output = model.decode(tokens, mask) # (batch_size, seq_len, vocab_size)\n    distribution = model.generator(decoder_output[:, -1, :]) # (batch_size, vocab_size)\n    next_token = torch.argmax(distribution, dim=1, keepdim=True) # (batch_size, 1)\n    return next_token\n\n# Define constants.\ninput_text = \"A long time ago\"\nnew_token_count = 10\nbatch_size = 1\nblock_size = 1024\n\n# Tokenize the input text.\nencoding, tokens = tokenize(input_text, batch_size)\n\n# Create the model.\nmodel = make_model(encoding.n_vocab)\n\n# Iterate until we've generated enough new tokens.\nfor _ in range(new_token_count):\n    block_tokens = limit_sequence_length(tokens, block_size) # (batch_size, seq_len)\n    next_token = generate_next_token(model, block_tokens) # (batch_size, 1)\n    tokens = torch.cat([tokens, next_token], dim=1) # (batch_size, input_seq_len + 1)\n\n# Print each of the generated token sequences.\nprint(tokens)\nfor row in tokens:\n    print(encoding.decode(row.tolist()))\n```", "```py\ntensor([[   32,   890,   640,  2084,  3556, 48241, 26430, 34350, 28146, 43264,\n          3556,  6787, 45859, 13884]])\nA long time ago</ spaghetti Rapiddx Rav unresolved</ rail MUCHkeeper\n```", "```py\nimport torch.nn as nn\n\nclass Embeddings(nn.Module):\n\n    def __init__(self, d_model, vocab_size):\n        super(Embeddings, self).__init__()\n        self.lut = nn.Embedding(vocab_size, d_model)\n        self.d_model = d_model\n\n    # input x: (batch_size, seq_len)\n    # output: (batch_size, seq_len, d_model)\n    def forward(self, x):\n        out =  self.lut(x) * math.sqrt(self.d_model)\n        return out\n```", "```py\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) *\n            -(math.log(10000.0) / d_model))  # (d_model/2)\n\n        pe[:, 0::2] = torch.sin(position * div_term)  # (max_len, d_model)\n        pe[:, 1::2] = torch.cos(position * div_term)  # (max_len, d_model)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n\n    # input x: (batch_size, seq_len, d_model)\n    # output: (batch_size, seq_len, d_model)\n    def forward(self, x):\n        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n        return self.dropout(x)\n```", "```py\nclass Decoder(nn.Module):\n\n    def __init__(self, layer, N):\n        super(Decoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\n```", "```py\ndef clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n```", "```py\nclass LayerNorm(nn.Module):\n\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n```", "```py\nclass DecoderLayer(nn.Module):\n\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n\n    def forward(self, x, mask):\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)\n```", "```py\nclass SublayerConnection(nn.Module):\n\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))\n```", "```py\nclass PositionwiseFeedForward(nn.Module):\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n```", "```py\nclass MultiHeadedAttention(nn.Module):\n\n    def __init__(self, h, d_model, dropout=0.1):\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            mask = mask.unsqueeze(1)  # (1, 1, seq_len, seq_len)\n\n        nbatches = query.size(0)  # batch_size\n\n        # (batch_size, seq_len, d_model) => (batch_size, h, seq_len, d_k)\n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n\n        # (batch_size, h, seq_len, d_k)\n        x, self.attn = attention(query,\n                                 key,\n                                 value,\n                                 mask=mask,\n                                 dropout=self.dropout)\n\n        # (batch_size, h, seq_len, d_k) => (batch_size, seq_len, d_model)\n        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n        return self.linears[-1](x)\n```", "```py\n# Dimensions of query, key, and value: (batch_size, h, seq_len, d_k)\n# Dimensions of mask: (1, 1, seq_len, seq_len)\ndef attention(query, key, value, mask=None, dropout=None):\n    d_k = query.size(-1)\n    # (batch_size, h, seq_len, d_k) x (batch_size, h, d_k, seq_len) -> (batch_size, h, seq_len, seq_len)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim=-1) # (batch_size, h, seq_len, seq_len)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    # (batch_size, h, seq_len, seq_len) x (batch_size, h, seq_len, d_k) -> (batch_size, h, seq_len, d_k)\n    return torch.matmul(p_attn, value), p_attn\n```", "```py\ndef subsequent_mask(size):\n    \"\"\"\n    Mask out subsequent positions.\n    \"\"\"\n    attn_shape = (1, size, size)\n    subsequent_mask = torch.triu(torch.ones(attn_shape),\n                                 diagonal=1).type(torch.uint8)\n    return subsequent_mask == 0\n```", "```py\nclass Generator(nn.Module):\n\n    def __init__(self, d_model, vocab):\n        super(Generator, self).__init__()\n        self.proj = nn.Linear(d_model, vocab)\n\n    def forward(self, x):\n        return F.log_softmax(self.proj(x), dim=-1)\n```", "```py\nclass DecoderModel(nn.Module):\n\n    def __init__(self, decoder, embed, generator):\n        super(DecoderModel, self).__init__()\n        self.embed = embed\n        self.decoder = decoder\n        self.generator = generator\n\n    def forward(self, x, mask):\n        return self.decode(x, mask)\n\n    def decode(self, x, mask):\n        return self.decoder(self.embed(x), mask)\n```", "```py\ndef make_model(vocab_size, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    c = copy.deepcopy\n    attn = MultiHeadedAttention(h, d_model)\n    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n    position = PositionalEncoding(d_model, dropout)\n    model = DecoderModel(\n        Decoder(DecoderLayer(d_model, c(attn), c(ff), dropout), N),\n        nn.Sequential(Embeddings(d_model, vocab_size), c(position)),\n        Generator(d_model, vocab_size))\n\n    for p in model.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n    return model\n```"]