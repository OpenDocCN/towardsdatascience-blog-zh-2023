["```py\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\n```", "```py\n# read the csv file\ndf = pd.read_csv(\"fish.csv\") \n\n# how many rows and columns?\nprint(df.shape)\n\n# print column names\nprint(df.columns)\n\n# print class distribution\nprint(df[\"type\"].value_counts())\n```", "```py\ndf[\"type\"] = df[\"type\"].apply(lambda x: 1 if x==\"tuna\" else 0)\n```", "```py\n# create a Figure\nfig = go.Figure()\n\n# specify custom colors for the plot\ncolor_map = {\n    0: \"red\",\n    1: \"blue\",\n}\n\n# apply the color map to 'type' column\ncolors = df[\"type\"].map(color_map)\n\n# add a scatter trace to the figure\nfig.add_trace(go.Scatter(x=df[\"length\"], \n            y=df[\"weight\"],\n            mode=\"markers\",\n            marker=dict(color=colors, size=8)))\n\n# add x-label, y-label and title\nfig.update_layout(\n    width=800,\n    height=600,  \n    title_text=\"Scatter Plot of Data\",\n    xaxis=dict(title=\"length\",\n               tickvals=[i for i in range(10)]),\n    yaxis=dict(title=\"weight\",\n               tickvals=[i for i in range(10)])\n)\n```", "```py\nfeatures = [\"length\", \"weight\"]\n```", "```py\n# Finding the first split:\n\n# initialize best_params which is a dictionary that will keep track\n# of best feature and split value at each node.\nbest_params = {\"feature\": None, \"impurity\": np.inf, \"split_value\": None}\n\n# for each feature in features, do the following:\n### for val in all feature values \n### (starting from the min possible value of the feature until max possible value, \n### incrementing by 'step_size'), check the following:\n###### if impurity at this feature val is less than previously recorded impurity, \n###### then update best_params\n\n# Following is the code for above interpretation\nfor feature in features:\n    curr_val = df[feature].min()\n    step_size = 0.1\n    while curr_val <= df[feature].max():\n        curr_feature_split_impurity = compute_impurity(df, feature, curr_val)\n        if curr_feature_split_impurity < best_params[\"impurity\"]:\n            best_params[\"impurity\"] = curr_feature_split_impurity\n            best_params[\"feature\"] = feature\n            best_params[\"split_value\"] = curr_val\n        curr_val += step_size\n```", "```py\ndef compute_impurity(df, feature, val, criterion):\n    \"\"\"\n    Inputs:\n    df: dataframe before splitting\n    feature: colname to test for best split\n    val: value of 'feature' to test for best split\n\n    Output: float\n    Returns the entropy after split\n    \"\"\"\n\n    # Make the split at (feature, val)\n    left = df[df[feature]<=val][\"type\"]\n    right = df[df[feature]>val][\"type\"]\n\n    # calculate impurity of both partitions\n\n    if criterion==\"entropy\":\n        left_impurity = compute_entropy(left)\n        right_impurity = compute_entropy(right)\n    else:\n        left_impurity = compute_gini(left)\n        right_impurity = compute_gini(right)\n\n    # return weighted entropy\n    n = len(df) # total number of data points\n    left_n = len(left) # number of data points in left partition\n    right_n = len(right) # number of data points in right partition\n\n    return (left_n/n)*left_impurity + (right_n/n)*right_impurity\n```", "```py\ndef compute_entropy(vals):\n    \"\"\"\n    Input:\n    vals: list of 0s and 1s corresponding to two classes\n\n    Output:\n    entropy: float\"\"\"\n\n    # probability of class labeled as 1 \n    # will be equal to the average of vals\n    p1 = np.mean(vals)\n    p0 = (1-p1)\n\n    # it means data is homoegeneous\n    # entropy is 0 in this case\n    if p1==0 or p0==0: \n        return 0\n\n    return - (p0*np.log2(p0) + p1*np.log2(p1)) # formula of entropy for two classes\n```", "```py\nprint(best_params)\n```", "```py\ncolor_map = {\n 0: \"red\", # salmon\n 1: \"blue\", # tuna\n}\n```", "```py\ndef compute_gini(vals):\n    \"\"\"\n    Input: vals is a list of 0s and 1s\n    Output:\n    gini: float\n    \"\"\"\n\n    # probability of '1' will be equal to the average of vals\n    p1 = np.mean(vals)\n    p0 = (1-p1) # since there are just two classes and p0+p1 = 1\n\n    if p1==0 or p0==0:\n        return 0\n\n    return 1 - p1**2 - p0**2\n```", "```py\ndef compute_impurity(df, feature, val, criterion):\n    \"\"\"\n    Inputs:\n    df: dataframe before splitting\n    feature: colname to test for best split\n    val: value of 'feature' to test for best split\n\n    Output: float\n    Returns the entropy after split\n    \"\"\"\n\n    # Make the split at (feature, val)\n    left = df[df[feature]<=val][\"type\"]\n    right = df[df[feature]>val][\"type\"]\n\n    # calculate impurity of both partitions\n\n    if criterion==\"entropy\":\n        left_impurity = compute_entropy(left)\n        right_impurity = compute_entropy(right)\n    else:\n        left_impurity = compute_gini(left)\n        right_impurity = compute_gini(right)\n\n    # return weighted entropy\n    n = len(df) # total number of data points\n    left_n = len(left) # number of data points in left partition\n    right_n = len(right) # number of data points in right partition\n\n    return (left_n/n)*left_impurity + (right_n/n)*right_impurity\n```", "```py\ndef get_best_params(df, features, criterion):\n    \"\"\"\n    A function to determine the best split at a node\n\n    Input:\n    df: dataframe before split\n    features: list of features\n    criterion: impurity measure to use (gini or entropy)\n\n    Output: \n    best_params: dict\n    \"\"\"\n\n    # Initialize best_params\n    best_params = {\"feature\": None, \"val\": None, \"impurity\": np.inf}\n\n    # iterate for all features\n    for feature in features:\n        curr_val = df[feature].min()\n        step_size = 0.1\n        # iterate for all values for a feature (according to step_size)\n        while curr_val<=df[feature].max():\n            # calculate impurity (gini or entropy) for the current value of feature\n            impurity = compute_impurity(df, feature, curr_val, criterion)\n\n            # update best_params if impurity is less than previous impurity\n            if impurity <= best_params[\"impurity\"]:\n                best_params[\"feature\"] = feature\n                best_params[\"val\"] = curr_val\n                best_params[\"impurity\"] = impurity\n            curr_val += step_size\n\n    best_params[\"val\"] = np.round(best_params[\"val\"], 2)\n    best_params[\"impurity\"] = np.round(best_params[\"impurity\"], 2)\n\n    return best_params\n```", "```py\ndef build_tree(data, features, curr_depth=0, max_depth=3, criterion=\"entropy\"):\n    \"\"\"A function to buil the decision tree recursively.\n\n    Input:\n    data: dataframe with columns length, weight, type\n    features: ['length', 'weight']\n    curr_depth: Keep track of depth at current node\n    max_depth: Decision tree will stop growing if max_depth reached\n    criterion: \"gini\" or \"entropy\" \n\n    \"\"\"\n\n    # Base case: max depth reached \n    if curr_depth >= max_depth:\n        classes, counts = np.unique(data['type'].values, return_counts=True)\n        predicted_class = classes[np.argmax(counts)]\n        print((\"--\" * curr_depth) + f\"Predict: {predicted_class}\")\n        return\n\n    # Get the best feature and value to split the data\n    best_params = get_best_params(data, features, criterion)\n\n    # Base case: pure node, single class case\n    if best_params[\"impurity\"] == 0:\n        predicted_class = data['type'].iloc[0]\n        print((\"--\" * curr_depth) + f\"Predict: {predicted_class}\")\n        return\n\n    # If there's no feature that can improve the purity (not possible to split)\n    if best_params[\"feature\"] is None:\n        predicted_class = data['type'].iloc[0]\n        print((\"--\" * curr_depth) + f\"Predict: {predicted_class}\")\n        return\n\n    # Print the current question (decision rule)\n    best_feature = best_params[\"feature\"]\n    best_split_val = best_params[\"val\"]\n    question = f\"Is {best_feature} <= {best_split_val}?\"\n    print((\"--\" * (curr_depth*2)) + \">\" + f\"{question}\")\n\n    # Split the dataset\n    left_df = data[data[best_feature] <= best_split_val]\n    right_df = data[data[best_feature] > best_split_val]\n\n    # Recursive calls for left and right subtrees\n    if not left_df.empty:\n        print((\"--\" * curr_depth) + f\"Yes ->\")\n        build_tree(left_df, features, curr_depth + 1, max_depth, criterion)\n\n    if not right_df.empty:\n        print((\"--\" * curr_depth) + f\"No ->\")\n        build_tree(right_df, features, curr_depth + 1, max_depth, criterion)\n```", "```py\nbuild_tree(df, [\"length\", \"weight\"], max_depth=4, criterion=\"entropy\")\n```"]