- en: Fill-in-the-blanks Self-Supervision in NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填空自监督在自然语言处理中的应用
- en: 原文：[https://towardsdatascience.com/fill-in-the-blanks-self-supervision-in-nlp-f0afb16dc7fd](https://towardsdatascience.com/fill-in-the-blanks-self-supervision-in-nlp-f0afb16dc7fd)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fill-in-the-blanks-self-supervision-in-nlp-f0afb16dc7fd](https://towardsdatascience.com/fill-in-the-blanks-self-supervision-in-nlp-f0afb16dc7fd)
- en: Why it's powerful and how to solve it
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么它强大以及如何解决
- en: '[](https://jagota-arun.medium.com/?source=post_page-----f0afb16dc7fd--------------------------------)[![Arun
    Jagota](../Images/3c3eb142f671b5fb933c2826d8ed78d9.png)](https://jagota-arun.medium.com/?source=post_page-----f0afb16dc7fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f0afb16dc7fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f0afb16dc7fd--------------------------------)
    [Arun Jagota](https://jagota-arun.medium.com/?source=post_page-----f0afb16dc7fd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jagota-arun.medium.com/?source=post_page-----f0afb16dc7fd--------------------------------)[![Arun
    Jagota](../Images/3c3eb142f671b5fb933c2826d8ed78d9.png)](https://jagota-arun.medium.com/?source=post_page-----f0afb16dc7fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f0afb16dc7fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f0afb16dc7fd--------------------------------)
    [Arun Jagota](https://jagota-arun.medium.com/?source=post_page-----f0afb16dc7fd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f0afb16dc7fd--------------------------------)
    ·14 min read·May 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f0afb16dc7fd--------------------------------)
    ·阅读时间14分钟·2023年5月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/03be7b5832a9d134961e51b08dd7a5a7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03be7b5832a9d134961e51b08dd7a5a7.png)'
- en: Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/Oaqk7qqNh_c?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    在 [Unsplash](https://unsplash.com/photos/Oaqk7qqNh_c?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: 'Predicting the next word has a long and successful history in language modeling,
    most recently in large language models. It leverages the existence of huge corpora
    of text: Wikipedia, public web pages strewn over the globe, and others. It is
    more powerful than other types of unsupervised learning on these corpora, as the
    learning is supervised. Furthermore, because it is self-supervised, no human effort
    is needed to create labeled data.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 预测下一个词在语言建模中有着悠久而成功的历史，尤其是在大型语言模型中。它利用了大量的文本语料库：维基百科、分布在全球的公共网页等。相比其他类型的无监督学习，这种方法在这些语料库上的效果更强，因为学习是有监督的。此外，由于它是自监督的，因此不需要人工创建标记数据。
- en: Consider
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑
- en: exposure to sunlight causes __
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 曝露在阳光下会导致__
- en: From a rich enough corpus of English sentences, machine learning should be able
    to learn a language model that can fill in the blanks sensibly, in this case with
    the term “skin cancer”.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个足够丰富的英语句子语料库中，机器学习应该能够学习一个语言模型，能够合理地填补这些空白，在这个例子中是“皮肤癌”。
- en: Why do we call this supervision? Because we start from a complete sequence of
    tokens, blur out the last word, ask our model to predict what it is, reward it
    if it predicts correctly, and penalize it if it gets it wrong. So this is supervised
    learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么称之为监督？因为我们从一个完整的标记序列开始，模糊掉最后一个词，要求模型预测它是什么，如果模型预测正确则奖励它，预测错误则惩罚它。所以这就是有监督学习。
- en: Recently, this self-supervision approach has been extended in a simple yet powerful
    way. Specifically, the blanks to be filled in may be anywhere in the text. In
    view of this, we might call it the text reconstruction task, not merely the text
    completion task.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，这种自监督方法以一种简单而强大的方式得到了扩展。具体而言，要填补的空白可以出现在文本的任何位置。因此，我们可能称之为文本重构任务，而不仅仅是文本补全任务。
- en: Below are some examples. Each instance is a sequence of words (in bold) followed
    by one or more sequences in which some of these words have been masked. The model’s
    aim is to fill in the blanks correctly.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些例子。每个实例是一个词的序列（以**粗体**标识），后面跟着一个或多个这些词被掩蔽的序列。模型的目标是正确填充这些空白。
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the subtitle of this post, we claimed that this extension turbo-charges predicting
    the next word. Here we explain why we said so.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章的副标题中，我们提到这种扩展大大提升了预测下一个词的能力。这里我们解释一下为什么我们这么说。
- en: Sure, predicting the next word can force the model to learn from a wide range
    of scenarios as it is easy to assemble a data set comprising billions of real
    word sequences. That said, generalizing this to predicting any subset of masked
    words in a sequence can amplify the set of learning scenarios a lot. This is why
    such masking-off plays a central role in BERT [2].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，预测下一个单词可以迫使模型从各种场景中学习，因为很容易组装一个包含数十亿个真实单词序列的数据集。也就是说，将其推广到预测序列中任何掩蔽单词的子集可以大大扩展学习场景的范围。这就是为什么这种掩蔽在
    BERT [2] 中扮演了核心角色。
- en: Consider a sequence comprising *n* words. If we allow only the last word to
    be masked off, we generate only one labeled instance — fill in the last word —
    from it. If we allow any suffix to be masked off, we can generate up to *n* labeled
    instances — fill in a suffix — from it. If we allow any subset of words to be
    masked off, we can generate an order of 2^*n* labeled instances from it. This
    is because there are this-many subsets that can be masked off.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含 *n* 个单词的序列。如果我们只允许最后一个单词被掩蔽，我们只会从中生成一个标记实例——填入最后一个单词。如果我们允许任何后缀被掩蔽，我们可以从中生成最多
    *n* 个标记实例——填入一个后缀。如果我们允许任何子集的单词被掩蔽，我们可以从中生成约 2^*n* 个标记实例。这是因为有这么多可以被掩蔽的子集。
- en: Moreover, as we can see in our example above, masking off different words will
    force the model to learn representations for the remaining words that predict
    the masked-off words when possible. In our example, filling in blanks on the right
    tail forces the model to understand what exposure to sunlight causes whereas masking
    off the two words on the left tail forces the model to learn what causes skin
    cancer.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如我们在上面的例子中所见，掩蔽不同的单词将迫使模型学习剩余单词的表示，从而在可能的情况下预测被掩蔽的单词。在我们的例子中，填补右尾的空白迫使模型理解暴露在阳光下的影响，而掩蔽左尾的两个单词则迫使模型学习导致皮肤癌的原因。
- en: Here is another example, we covered in [3].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个例子，我们在[3]中覆盖过。
- en: Imagine that we have a list of names of cars. Such as
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下我们有一个汽车名称的列表。例如：
- en: Honda Civic, Toyota Celica, Ford Mustang, Jeep Wrangler, …
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本田思域、丰田*Celica*、福特*Mustang*、吉普*Wrangler*……
- en: Predicting the next word will certainly help the model learn models of particular
    makes. Such as *Cherokee* and *Wrangler* (among others) for *Jeep*. However, additional
    self-supervision in the form of masking off any word will help the model also
    learn that model names tend to be strongly predictive of the make. For example,
    *Celica* is a *Toyota*, *Mustang* is a *Ford*, *Wrangler* is a *Jeep*, and so
    on.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 预测下一个单词无疑有助于模型学习特定品牌的模型。例如，*Cherokee* 和 *Wrangler*（以及其他）是*Jeep*的模型。然而，通过掩蔽任何单词的额外自我监督也有助于模型学习到模型名称往往能强烈预测品牌。例如，*Celica*
    是 *Toyota*，*Mustang* 是 *Ford*，*Wrangler* 是 *Jeep*，等等。
- en: Such learning would serve downstream use cases better. Such as answering the
    question
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习将更好地服务于下游应用场景。例如回答问题。
- en: What is the make of Celica?
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Celica* 的品牌是什么？'
- en: In the remainder of this post, we will present a few different ways to model
    this problem, followed by some discussion on their benefits and limitations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的其余部分，我们将介绍几种不同的建模方法，并讨论它们的优点和局限性。
- en: The approaches we will discuss are much more limited than state-of-the-art large
    language models for nuanced versions of this task. Such as entire paragraphs being
    masked off.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的方法在处理这种任务的细微版本时，远不如最先进的大型语言模型。例如，整个段落被掩蔽。
- en: That said, they are easier to understand, perhaps more familiar to some readers,
    and also can be tried out using widely available NLP toolkits or in some cases
    building from scratch.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，它们更容易理解，也许对一些读者更为熟悉，并且可以使用广泛可用的 NLP 工具包尝试，或者在某些情况下从头开始构建。
- en: '**The Data**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据**'
- en: Let’s say we have a corpus of text documents. A document is a sequence of tokens
    (words, punctuation symbols, etc). In this post, we will assume that whatever
    learning tasks we define do not cross document boundaries.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组文本文档。文档是一个令牌（单词、标点符号等）的序列。在这篇文章中，我们将假设我们定义的学习任务不会跨越文档边界。
- en: For the most part, we will focus on documents comprising single English sentences.
    We can imagine that this corpus of sentences has been derived from a corpus of
    longer documents by segmenting long documents into sentences using NLP. That said,
    the approaches we discuss will work for longer multi-sentence documents as well.
    They are just more convenient to describe for single-sentence documents.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will describe a method that is conceptually simple to understand,
    simple to implement, and fast during training and inference.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: That said, its scope is limited to filling in blanks in a single masked-off
    region, whether it be on the left tail, the right tail, or in the middle. A large
    language model could fill in blanks in multiple masked-off regions. Moreover,
    an LLM would generally be more accurate, especially on long sequences such as
    full paragraphs or even pages of text.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '**The Method**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Our method will comprise two tries, a forward trie, and a backward trie, fused
    together in a particular way. We will call this data structure a forward-backward
    trie (FB-Trie).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: We will use the following example to illustrate the forward-backward trie data
    structure. Imagine that we have just two sentences in our training set to build
    the trie.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: First, let’s see just the usual forward Trie.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e89c30bc4a0035dfb07eba8f6e495aac.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1 (By Author): Forward Trie On Our Data'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: This Trie just captures the two sequences of words on the two root-to-leaf paths
    shown.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: A node in the Trie implicitly represents the sequence of words in the root-to-node
    path that ends there.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Let’s fill in an important detail that was not depicted in Figure 1\. A node
    stores the number of sequences in the training set whose prefix matches it. So
    the root node in our example will store 2 since there are two sequences in our
    training set. Imagining a richer training set, the node representing [*work*,
    *causes*] would store 15 were there this-many sequences in it that began with
    *work causes*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Let *i* denote a node and *n*(i) the count on it. Let *j* denote a child of
    *i*. The probability of taking the arc from *i* to *j* is *n*(*j*)/*n*(*i*) which
    we will refer to as *P*(*i*, *j*).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: From this Trie, we can fill in a single blank on the right tail if there is
    a high-probability extension in an obvious way. Let’s illustrate this with examples.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Consider *work ____.* Wewouldn’t fill in the blanks as we can imagine that there
    are many different sentences that begin with *work*. If on the other hand, we
    had *work causes* ___*,* filling in the blank with stress would be more reasonable*.*
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '**Filling In Blanks On The Left Tail**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: What if the fill-in-the-blank query was ___ *causes stress*? To extend our model
    to answer such queries, we’ll add a backward Trie.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: The backward Trie is the trie we would get if we reversed the sequences in our
    training set. Its structure is the same as that of the forward trie (Fig 1). It
    also includes counts on its nodes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Left tail blanks can now be filled in (when possible) by following the same
    inference procedure, except that we look up the backward Trie on the reversed
    version of the sequence in which blanks have to be filled in.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 左尾空白现在可以通过相同的推理过程进行填充（如果可能的话），只不过我们查找的是空白需要填充的序列的反向字典树。
- en: In a fill-in-the-blanks input, we will know whether the blank is on the left
    or right tail, so we will know whether to look up the forward trie on the input
    or the backward trie on the input’s reverse.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在填空输入中，我们将知道空白是位于左尾还是右尾，因此我们将知道是查看输入的前向字典树还是输入反向的后向字典树。
- en: '**Filling In Blanks In The Middle**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**填充中间的空白**'
- en: Consider when the blank to be filled in is not on either tail. As in *work __
    stress*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑当待填充的空白不在任何尾部时。例如*工作 __ 压力*。
- en: For this task, we will design a new data structure, a special fusion of our
    forward and backward tries, with parent arcs added to both tries and with bridge
    edges added to jump from one Trie to the other.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们将设计一个新的数据结构，这是前向和后向字典树的特殊融合，向两个字典树添加了父节点弧线，并添加了桥接边以从一个字典树跳到另一个。
- en: Let’s start by adding parent arcs to the forward Trie.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从向前向字典树添加父节点弧线开始。
- en: '![](../Images/5c647f09fa299380c643294711c5dc7e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c647f09fa299380c643294711c5dc7e.png)'
- en: 'Figure 2 (By Author): Forward Trie On Our Data With Parent Arcs Added'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图2（作者提供）：带有父节点弧线的我们的数据上的前向字典树
- en: We were able to add parent arcs because by definition each non-root node in
    the trie has a unique parent.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够添加父节点弧线，因为根据定义，字典树中的每个非根节点都有唯一的父节点。
- en: Next, we’ll add parent arcs to the backward trie as well.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向后向字典树添加父节点弧线。
- en: '**Fusing The Forward And The Backward Trie**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**融合前向字典树和后向字典树**'
- en: Next, we will fuse the two tries. We depict the fused tries below for our example.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将融合这两个字典树。我们下面展示了我们示例中的融合字典树。
- en: '![](../Images/d7862aa84681368de6c41c5711c653a0.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7862aa84681368de6c41c5711c653a0.png)'
- en: 'Figure 3 (By Author): The forward-backward trie in our example'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图3（作者提供）：我们示例中的前向-后向字典树
- en: The nodes and arcs of the forward trie are shown in solid, while those in the
    backward trie in dashed. Parent arcs are shown in the forward trie with dotted
    curves. Parent arcs are not shown in the backward trie to reduce clutter, but
    they do exist.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 前向字典树的节点和弧线用实线表示，而后向字典树的节点和弧线用虚线表示。前向字典树中的父节点弧线用虚线曲线表示。为了减少混乱，后向字典树中不显示父节点弧线，但它们确实存在。
- en: The two tries are fused together using bridge edges. These are dashed edges
    with no arrows on them. Each edge can be implemented as a pair of arcs between
    the two nodes, covering the two directions (this is not depicted).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 两个字典树通过桥接边融合在一起。这些是没有箭头的虚线边。每条边可以实现为两个节点之间的一对弧线，覆盖两个方向（这未被描绘）。
- en: In Figure 3, only a few — in fact, two — of the many bridge edges are shown.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3中，只展示了许多桥接边中的几个——实际上是两个。
- en: Let’s now explain how these bridge edges are added. Once this is understood,
    the reader can imagine where the rest of the bridge edges are.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们解释这些桥接边是如何添加的。一旦理解了这一点，读者可以想象其余桥接边的位置。
- en: Imagine training on a new sequence. For concreteness say it is the tokenized
    version of *work causes stress*. First, we insert this sequence into the forward
    trie as well as the backward trie. Now we consider all possible (*prefix*, *suffix*)
    splits of this sequence. Next, we reverse the suffixes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下在一个新序列上进行训练。为了具体说明，假设这是*工作导致压力*的标记化版本。首先，我们将这个序列插入前向字典树和后向字典树。现在我们考虑这个序列的所有可能的（*前缀*，*后缀*）分割。接下来，我们反转后缀。
- en: Below are the (*prefix*, *suffix*, *reverse suffix*) triples we would get for
    *work causes stress,* excluding those in which the prefix or the suffix is empty*.*
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将获得的（*前缀*，*后缀*，*反向后缀*）三元组，*排除前缀或后缀为空的情况*。
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now we process each triple (*prefix*, *suffix*, *reverse suffix*) as follows.
    We look up the prefix in the forward trie to find the node it ends at. Similarly,
    we look up the reverse suffix in the backward trie to find the node it ends at.
    We now connect the two nodes by a bridge edge.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们按照如下方法处理每个三元组（*前缀*，*后缀*，*反向后缀*）。我们在前向字典树中查找前缀以找到它结束的节点。同样地，我们在后向字典树中查找反向后缀以找到它结束的节点。我们现在通过一条桥接边连接这两个节点。
- en: '**Model Size And Training Time**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型大小和训练时间**'
- en: Before describing inference, let’s take some time to reflect on the size of
    the model and the time it takes to train.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述推理之前，让我们花些时间反思模型的大小以及训练所需的时间。
- en: First off, the model can be huge because we have two tries — a forward trie
    and a backward trie. Additionally, we have bridge edges.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we take the viewpoint that the size of the model in itself is
    not a concern. State-of-the-art technologies easily accommodate huge models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: The more interesting question is how long it takes to train the model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: First of all, the training can be incremental. That is, a new sequence can be
    added to the FB-Trie at any time. As it turns out, training in fact is very fast.
    This is because (i) it's fast to insert a new sequence into the forward trie and
    its reverse into the backward trie, and (ii) it's fast to enumerate all splits
    of this sequence, do the lookups needed for discovering the nodes to be bridged,
    and add the various bridge edges.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**Illustrating Filling-In-The-Blanks Using The Forward-Backward Trie**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Let’s illustrate this for *work __ stress*. First, we look up the forward trie
    to find the node, call it *u*, at which [*work*] ends. Then we look up the backward
    trie to find the node, call it *v*, at which [*stress*] ends. Next, we walk over
    this bridge edge from the backward trie into the forward trie. Next, we walk over
    [*work*, *causes*]’s parent in the forward arc and end up at [*work*]. So now
    we have reached the same node in both directions. The sequence of labels on the
    parent arcs we walked during this process will, after reversing this sequence,
    be the value of the blanks. In our example, we walked only one backward arc, which
    was labeled *causes.* So our answer is *“causes”.*
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '**Inference: A More General Description**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: In our *work __ stress* example, the blank had a unique solution. In the general
    case, *L* __ *R,* there may be multiple solutions*.*
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: (We’ve switched terminology slightly because it will be helpful in this section.
    L and R are token sequences, not strings.)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Below we discuss how to treat the general case.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: First, we look up the largest prefix of L, call it L’, in the forward trie.
    Next, we look up the largest suffix of R, call it R’, using the backward trie.
    Let *u* denote the node that L’ ends at in the forward trie and *v* the node that
    the reverse of R’ ends up in the backward trie.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: This situation is depicted below.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ea12264b8dd2b4c9dd51f0790753e84.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4 (By Author): Inference in the general — multiple solutions — case'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Since v is not a leaf in the backward trie — otherwise there is no blank to
    fill in —there must be at least one bridge edge touching *v*, as depicted in the
    callout in Figure 4.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Consider any one such bridge edge and let *w* denote the node in the forward
    trie at its other end. We follow the parent arcs in the forward trie in sequence,
    starting from *w*. We stop when we first arrive on a node, call it *x,* that is
    on the path L’ in the forward Trie. This condition will always be met (see next
    paragraph).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: The best-case scenario is that *x* equals *v*. The worst-case scenario is that
    *x* is the forward trie’s root.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Reversing the sequence of labels on the path from *w* to *x* yields one solution
    to this inference. More accurately, this solution is for fill-in-the-blanks problem
    *L’’* __ *R’,* which may be (much) wider than the problem *L* __ *R* we started
    with*.*
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 逆转从 *w* 到 *x* 的路径上的标签顺序会产生一种推断的解决方案。更准确地说，这个解决方案是针对填空问题 *L’’* __ *R’*，这可能比我们开始时的问题
    *L* __ *R* 更宽泛。
- en: '**Expanding Further On The Solutions**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步扩展解决方案**'
- en: Every bridge edge touching *v* generates a unique solution. So the number of
    solutions of the form *L*’’ __ *R*’ is the number of bridge edges that touch *v*.
    Note that *L*” depends on *w*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 每条触及 *v* 的桥接边都会生成一个唯一的解决方案。因此，形式为 *L*’’ __ *R*’ 的解决方案的数量就是触及 *v* 的桥接边的数量。请注意
    *L*” 取决于 *w*。
- en: We can reverse the inference direction by starting from *u* instead of *v,*
    walking a bridge edge from *u* to *w.* This time *w* is a node in the backward
    trie*.* We then sequentially walk parent arcs in the backward Trie from *w* until
    the first time we hit a nodeon *R’* in the backward trie, which we will call *x.*
    (This time *x* is a node in the backward trie, not the forward one.)We get another
    set of solutions of the form *L*’ __ *R*’’. Note that R’’ depends on *w*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过从 *u* 开始而不是 *v* 来反向推断，从 *u* 到 *w* 走一条桥接边。这次 *w* 是反向字典树中的一个节点。然后我们在反向字典树中从
    *w* 顺序地沿父节点弧走，直到第一次遇到反向字典树中的 *R’* 节点，我们将其称为 *x*。 (这次 *x* 是反向字典树中的一个节点，而不是正向字典树中的。)
    我们得到另一组形式为 *L*’ __ *R*’’ 的解决方案。注意 *R’’* 取决于 *w*。
- en: The two sets of solutions can overlap. The sets are easy to dedupe as a solution
    is uniquely identified by the endpoints of the paths that represent the filled-in
    portion. Note that the paths themselves can be different, but the endpoints (after
    ignoring which is the starting point and which is the ending one) will be the
    same.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 两组解决方案可能会重叠。这些集合容易去重，因为解决方案是由表示填充部分的路径的端点唯一确定的。请注意，路径本身可能不同，但端点（忽略起点和终点）将是相同的。
- en: Let’s illustrate this in the example of “*work* __ *stress”* in Figure 3*.*
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过图3中的“*work* __ *stress*”的例子来说明这一点。
- en: '![](../Images/e99a6dd7e2a062d131ec69be8b99e437.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e99a6dd7e2a062d131ec69be8b99e437.png)'
- en: 'Figure 5 (By Author): Two ways to fill in “causes” in “work __ stress”.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图5（作者提供）：两种方法来填入“causes”在“work __ stress”中。
- en: 'If we follow the algorithm described, there are two ways to fill in “causes”
    into “work __ stress”. One starts from the bridge edge from [*stress*] in the
    backward trie, and the other starts from the bridge edge from [*work*] in the
    forward trie. While the two solutions are found by walking different paths, the
    endpoints on the two paths, after ignoring which starts the path and which ends
    it, are the same: [*work*] in the forward trie and [*stress*] in the backward
    trie.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们按照所描述的算法进行，有两种方法可以将“causes”填入“work __ stress”中。一种方法是从反向字典树中的[*stress*]的桥接边开始，另一种方法是从正向字典树中的[*work*]的桥接边开始。尽管这两种解决方案是通过不同的路径找到的，但在忽略起始点和终点后，两条路径上的端点是相同的：正向字典树中的[*work*]和反向字典树中的[*stress*]。
- en: If we only allowed starting from one of the tries, we wouldn’t get dupes. But
    we cannot rule out the possibility that we might miss some solutions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仅允许从其中一个字典树开始，我们不会得到重复。但我们不能排除可能会遗漏某些解决方案的可能性。
- en: Here is an example. Say we trained on the tokenized versions of the following
    two strings.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子。假设我们在以下两个字符串的分词版本上进行了训练。
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Consider the prompt “*work* __ *stress”*. Starting from the bridge edge touching
    [*stress*] in the backward trie we would get the solution “*work* ***causes***
    *stress”*. Starting from the bridge edge touching [*work*] in the forward trie
    we would get the solution “*work* ***causes burnout”***.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑提示“*work* __ *stress*”。从反向字典树中触及[*stress*]的桥接边开始，我们会得到解决方案“*work* ***causes***
    *stress*”。从正向字典树中触及[*work*]的桥接边开始，我们会得到解决方案“*work* ***causes burnout***”。
- en: It’s very reasonable to wonder whether the second answer should even be deemed
    a solution as it ignores the word *stress*. That’s a decision for the modeler
    to make. It can be argued both ways.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 很合理地怀疑第二个答案是否应该被认为是一个有效的解决方案，因为它忽略了词语*stress*。这是一个需要建模者做出的决定。这两种观点都有其合理性。
- en: In this post, our point is that should someone want to deem the second answer
    as a valid solution, this is achievable by enumerating all solutions in both directions
    and then deduping them.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们的观点是，如果有人想将第二个答案视为有效的解决方案，这可以通过枚举两个方向上的所有解决方案然后去重来实现。
- en: '**Scoring The Solutions**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**评分解决方案**'
- en: When there are many solutions to a fill-in-the-blanks prompt, it makes sense
    to score them, so they can be presented in the order of better solution first.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当填空题有多个解决方案时，为了能够按优劣顺序呈现它们，对它们进行评分是合理的。
- en: We’ve already seen an example where scoring makes sense. For the prompt “*work*
    __ *stress”* if we deem that “*work* ***causes burnout”*** *i*s also a solution
    then it should score lower than *“work* ***causes*** *stress”.* This is because
    the former ignores the word stress in the prompt but the latter doesn’t*.*
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到一个评分有意义的例子。对于提示“*工作* __ *压力”*，如果我们认为“*工作* ***导致倦怠”*** *也是一个解决方案，那么它应该比*“工作*
    ***导致*** *压力”*的评分低。这是因为前者忽略了提示中的单词压力，而后者则没有*。
- en: The fact that the former solution in this example has more words in bold than
    in the latter one plays no role in this*.* The number of words in the answer is
    in itself immaterial, its the amount of information from the prompt that is ignored
    that is.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，前一个解决方案的粗体字比后一个多这一事实在此*没有作用*。答案中的单词数量本身并不重要，重要的是被忽略的提示信息的量。
- en: 'Now consider this prompt: “*work* __”. Assuming the training set comprises
    only the two strings'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑这个提示：“*工作* __”。假设训练集仅包含这两个字符串
- en: '[PRE4]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: both “*work* ***causes burnout”*** and “*work* ***causes stress”***are good
    solutions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: “*工作* ***导致倦怠”*** 和 “*工作* ***导致压力”***都是不错的解决方案。
- en: Now imagine that the training set in fact contains multiple copies of these
    two strings, and the string “*work causes stress”* occurs much more frequently
    than *work causes burnout*. It would make sense to score “*work* ***causes stress”***
    as a better solution to *“work __”* than *“work* ***causes burnout”***.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，训练集中实际上包含了这两个字符串的多个副本，并且字符串“*工作导致压力*”的出现频率远高于*工作导致倦怠*。这就意味着，将“*工作* ***导致压力”***评为比*“工作
    __”*更好的解决方案是合理的，而不是*“工作* ***导致倦怠”***。
- en: For the scenario of the previous paragraph, a scoring that delivers the desired
    rank order is easy to achieve. From the node [*work*, *causes*] on the forward
    tries, the probability of taking the arc labeled “*stress”* is higher than the
    probability of taking the arc labeled *“burnout”.* These probabilitiesare easy
    to compute from information stored on the forward trie as discussed earlier.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上一段的场景，提供所需排名顺序的评分很容易实现。从前向查找树上的节点[*工作*, *导致*]开始，选择标记为“*压力*”的弧的概率高于选择标记为*“倦怠”*的弧的概率。这些概率可以从之前讨论的前向查找树中存储的信息中轻松计算出来。
- en: For a prompt in which the blank to be filled in is on the left tail, we can
    rank the solutions similarly. This time we use the backward trie instead of the
    forward trie.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于填空在左尾的提示，我们可以类似地对解决方案进行排名。这次我们使用的是反向查找树，而不是前向查找树。
- en: The only remaining case is when the blank is in the middle and two solutions
    use the same information from the prompt. That is the two solutions take the form
    L M1 R and L M2 R where L and R are identical in both solutions but the M1 and
    M2 are different.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一剩下的情况是当空白在中间，并且两个解决方案使用相同的提示信息时。也就是说，两个解决方案的形式为L M1 R和L M2 R，其中L和R在两个解决方案中是相同的，但M1和M2不同。
- en: To cover this scenario, we will define the score of a solution L M R as follows.
    Write M = [*m*1] M’ [*m*2] where *m*1 and *m*2 are the first and the last tokens
    in M. It's possible that M’ is empty in which case *m*1 equals *m*2.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了覆盖这种情况，我们将定义解决方案L M R的评分如下。写作M = [*m*1] M’ [*m*2]，其中*m*1和*m*2是M中的第一个和最后一个标记。M’可能为空，这种情况下*m*1等于*m*2。
- en: We’ll define the score of L M R as P(m1|L) in the forward trie multiplied by
    P(m2|R) in the backward trie.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义L M R的评分为前向查找树中P(m1|L)与反向查找树中P(m2|R)的乘积。
- en: '**Summary**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this post, we started by covering the problem of fill-in-the-blanks self-supervision
    as a powerful way to train a large language model. We then presented a solution
    using a custom fusion of a forward and a backward trie for this problem.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们首先讨论了填空自监督问题作为训练大型语言模型的一种强大方法。然后，我们提出了使用自定义融合的前向和反向查找树来解决这个问题的方法。
- en: This solution, while not competitive with modern deep learning and transformer-based
    large language models, is likely to be quite effective in somewhat small versions
    of the problem, specifically in answering fill-in-the-blank questions in a single
    sentence.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种解决方案在与现代深度学习和基于变换器的大型语言模型相比时并不具备竞争力，但在较小版本的问题中，特别是在回答单句填空题时，它可能非常有效。
- en: It's also relatively easy to understand implement from scratch, and fast to
    train. It also accommodates incremental training.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从零实现相对容易理解且快速训练，同时也支持增量训练。
- en: '**References**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[https://towardsdatascience.com/contextual-text-correction-using-nlp-81a1363c5fc3](/contextual-text-correction-using-nlp-81a1363c5fc3)'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/contextual-text-correction-using-nlp-81a1363c5fc3](/contextual-text-correction-using-nlp-81a1363c5fc3)'
- en: '[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)
    [BERT]'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)
    [BERT]'
- en: '[https://medium.com/towards-data-science/multi-task-learning-4531eb32d77b](https://medium.com/towards-data-science/multi-task-learning-4531eb32d77b)'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://medium.com/towards-data-science/multi-task-learning-4531eb32d77b](https://medium.com/towards-data-science/multi-task-learning-4531eb32d77b)'
