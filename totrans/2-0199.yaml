- en: A Guide to Building Effective Training Pipelines for Maximum Results
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é«˜æ•ˆè®­ç»ƒç®¡é“æ„å»ºæŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee](https://towardsdatascience.com/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee](https://towardsdatascience.com/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)
- en: '[THE FULL STACK 7-STEPS MLOPS FRAMEWORK](https://towardsdatascience.com/tagged/full-stack-mlops)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[å®Œæ•´çš„ 7 æ­¥ MLOps æ¡†æ¶](https://towardsdatascience.com/tagged/full-stack-mlops)'
- en: 'Lesson 2: Training Pipelines. ML Platforms. Hyperparameter Tuning.'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬2è¯¾ï¼šè®­ç»ƒç®¡é“ã€‚ML å¹³å°ã€‚è¶…å‚æ•°è°ƒæ•´ã€‚
- en: '[](https://pauliusztin.medium.com/?source=post_page-----6fdaef594cee--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----6fdaef594cee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6fdaef594cee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6fdaef594cee--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----6fdaef594cee--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pauliusztin.medium.com/?source=post_page-----6fdaef594cee--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----6fdaef594cee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6fdaef594cee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6fdaef594cee--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----6fdaef594cee--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6fdaef594cee--------------------------------)
    Â·19 min readÂ·May 9, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6fdaef594cee--------------------------------)
    Â·19åˆ†é’Ÿé˜…è¯»Â·2023å¹´5æœˆ9æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/27d8728645d28e45ee8757d4bedf42c2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27d8728645d28e45ee8757d4bedf42c2.png)'
- en: Photo by [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºäº [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    åœ¨ [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This tutorial represents **lesson 2 out of a 7-lesson course** that will walk
    you step-by-step through how to **design, implement, and deploy an ML system**
    using **MLOps good practices**. During the course, you will build a production-ready
    model to forecast energy consumption levels for the next 24 hours across multiple
    consumer types from Denmark.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹ä»£è¡¨äº†**7 èŠ‚è¯¾ç¨‹ä¸­çš„ç¬¬ 2 èŠ‚**ï¼Œå°†é€æ­¥æŒ‡å¯¼ä½ å¦‚ä½•**è®¾è®¡ã€å®ç°å’Œéƒ¨ç½²ä¸€ä¸ª ML ç³»ç»Ÿ**ï¼Œä½¿ç”¨**MLOps çš„æœ€ä½³å®è·µ**ã€‚åœ¨è¯¾ç¨‹æœŸé—´ï¼Œä½ å°†æ„å»ºä¸€ä¸ªç”Ÿäº§å°±ç»ªçš„æ¨¡å‹ï¼Œä»¥é¢„æµ‹æœªæ¥
    24 å°æ—¶å†…æ¥è‡ªä¸¹éº¦çš„å¤šä¸ªæ¶ˆè´¹è€…ç±»å‹çš„èƒ½æºæ¶ˆè€—æ°´å¹³ã€‚
- en: '*By the end of this course, you will understand all the fundamentals of designing,
    coding and deploying an ML system using a batch-serving architecture.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*é€šè¿‡æœ¬è¯¾ç¨‹çš„å­¦ä¹ ï¼Œä½ å°†æŒæ¡è®¾è®¡ã€ç¼–ç å’Œéƒ¨ç½²ä¸€ä¸ªä½¿ç”¨æ‰¹é‡æœåŠ¡æ¶æ„çš„ ML ç³»ç»Ÿçš„æ‰€æœ‰åŸºæœ¬çŸ¥è¯†ã€‚*'
- en: This course *targets mid/advanced machine learning engineers* who want to level
    up their skills by building their own end-to-end projects.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è¯¾ç¨‹*é’ˆå¯¹ä¸­çº§/é«˜çº§æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ*ï¼Œæ—¨åœ¨é€šè¿‡æ„å»ºè‡ªå·±çš„ç«¯åˆ°ç«¯é¡¹ç›®æ¥æå‡æŠ€èƒ½ã€‚
- en: '*Nowadays, certificates are everywhere. Building advanced end-to-end projects
    that you can later show off is the best way to get recognition as a professional
    engineer.*'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*å¦‚ä»Šï¼Œè¯ä¹¦éšå¤„å¯è§ã€‚æ„å»ºé«˜çº§çš„ç«¯åˆ°ç«¯é¡¹ç›®å¹¶å±•ç¤ºå‡ºæ¥æ˜¯è·å¾—ä¸“ä¸šè®¤å¯çš„æœ€ä½³æ–¹å¼ã€‚*'
- en: 'Table of Contents:'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›®å½•ï¼š
- en: Course Introduction
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ç®€ä»‹
- en: Course Lessons
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹å†…å®¹
- en: Data Source
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®æ¥æº
- en: 'Lesson 2: Training Pipelines. ML Platforms. Hyperparameter Tuning.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬2è¯¾ï¼šè®­ç»ƒç®¡é“ã€‚ML å¹³å°ã€‚è¶…å‚æ•°è°ƒæ•´ã€‚
- en: 'Lesson 2: Code'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬2è¯¾ï¼šä»£ç 
- en: Conclusion
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: References
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: Course Introduction
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ç®€ä»‹
- en: '***At the end of this 7 lessons course, you will know how to:***'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '***åœ¨è¿™ 7 èŠ‚è¯¾çš„è¯¾ç¨‹ç»“æŸæ—¶ï¼Œä½ å°†å­¦ä¼šå¦‚ä½•ï¼š***'
- en: design a batch-serving architecture
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¾è®¡ä¸€ä¸ªæ‰¹é‡æœåŠ¡æ¶æ„
- en: use Hopsworks as a feature store
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Hopsworks ä½œä¸ºç‰¹å¾å­˜å‚¨
- en: design a feature engineering pipeline that reads data from an API
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¾è®¡ä¸€ä¸ªä» API è¯»å–æ•°æ®çš„ç‰¹å¾å·¥ç¨‹ç®¡é“
- en: build a training pipeline with hyper-parameter tunning
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªå¸¦æœ‰è¶…å‚æ•°è°ƒæ•´çš„è®­ç»ƒç®¡é“
- en: use W&B as an ML Platform to track your experiments, models, and metadata
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ W&B ä½œä¸º ML å¹³å°æ¥è·Ÿè¸ªä½ çš„å®éªŒã€æ¨¡å‹å’Œå…ƒæ•°æ®
- en: implement a batch prediction pipeline
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ç°ä¸€ä¸ªæ‰¹é‡é¢„æµ‹ç®¡é“
- en: use Poetry to build your own Python packages
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Poetry æ„å»ºä½ è‡ªå·±çš„ Python åŒ…
- en: deploy your own private PyPi server
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éƒ¨ç½²ä½ è‡ªå·±çš„ç§æœ‰ PyPi æœåŠ¡å™¨
- en: orchestrate everything with Airflow
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Airflow åè°ƒä¸€åˆ‡
- en: use the predictions to code a web app using FastAPI and Streamlit
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é¢„æµ‹ç»“æœç¼–å†™ä¸€ä¸ªåŸºäº FastAPI å’Œ Streamlit çš„ç½‘é¡µåº”ç”¨
- en: use Docker to containerize your code
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Docker å®¹å™¨åŒ–ä½ çš„ä»£ç 
- en: use Great Expectations to ensure data validation and integrity
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Great Expectations ç¡®ä¿æ•°æ®éªŒè¯å’Œå®Œæ•´æ€§
- en: monitor the performance of the predictions over time
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›‘æ§é¢„æµ‹æ€§èƒ½çš„æ—¶é—´å˜åŒ–
- en: deploy everything to GCP
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰å†…å®¹éƒ¨ç½²åˆ° GCP
- en: build a CI/CD pipeline using GitHub Actions
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ GitHub Actions æ„å»º CI/CD ç®¡é“
- en: If that sounds like a lot, don't worry, after you will cover this course you
    will understand everything I said before. Most importantly, you will know WHY
    I used all these tools and how they work together as a system.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¿™å¬èµ·æ¥å¾ˆå¤šï¼Œä¸ç”¨æ‹…å¿ƒï¼Œå®Œæˆæœ¬è¯¾ç¨‹åä½ å°†ç†è§£æˆ‘ä¹‹å‰æ‰€è¯´çš„ä¸€åˆ‡ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä½ ä¼šçŸ¥é“**æˆ‘ä¸ºä»€ä¹ˆä½¿ç”¨æ‰€æœ‰è¿™äº›å·¥å…·**ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•ä½œä¸ºä¸€ä¸ªç³»ç»ŸååŒå·¥ä½œã€‚
- en: '**If you want to get the most out of this course,** [**I suggest you access
    the GitHub repository**](https://github.com/iusztinpaul/energy-forecasting) **containing
    all the lessons'' code. This course is designed to read and replicate the code
    along the articles quickly.**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚æœä½ æƒ³ä»è¿™é—¨è¯¾ç¨‹ä¸­è·å¾—æœ€å¤§æ”¶ç›Šï¼Œ** [**æˆ‘å»ºè®®ä½ è®¿é—®åŒ…å«æ‰€æœ‰è¯¾ç¨‹ä»£ç çš„ GitHub ä»“åº“**](https://github.com/iusztinpaul/energy-forecasting)
    **ã€‚æœ¬è¯¾ç¨‹æ—¨åœ¨è®©ä½ å¿«é€Ÿé˜…è¯»å’Œå¤åˆ¶æ–‡ç« ä¸­çš„ä»£ç ã€‚**'
- en: By the end of the course, you will know how to implement the diagram below.
    Don't worry if something doesn't make sense to you. I will explain everything
    in detail.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯¾ç¨‹ç»“æŸæ—¶ï¼Œä½ å°†çŸ¥é“å¦‚ä½•å®ç°ä¸‹å›¾æ‰€ç¤ºçš„å†…å®¹ã€‚å¦‚æœæœ‰äº›åœ°æ–¹å¯¹ä½ æ¥è¯´ä¸å¤ªæ˜ç™½ï¼Œä¸ç”¨æ‹…å¿ƒï¼Œæˆ‘ä¼šè¯¦ç»†è§£é‡Šä¸€åˆ‡ã€‚
- en: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
- en: Diagram of the architecture you will build during the course [Image by the Author].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹æœŸé—´ä½ å°†æ„å»ºçš„æ¶æ„å›¾ [ä½œè€…æä¾›çš„å›¾åƒ]ã€‚
- en: By the **end of Lesson 2**, you will know how to implement and integrate the
    **training pipeline** and **ML platform**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**ç¬¬äºŒè¯¾ç»“æŸæ—¶**ï¼Œä½ å°†çŸ¥é“å¦‚ä½•å®ç°å’Œé›†æˆ**è®­ç»ƒç®¡é“**å’Œ**MLå¹³å°**ã€‚
- en: '**Note:** This is the most extended lesson, as I couldn''t logically split
    the training pipeline from the ML platform. Enjoy!'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** è¿™æ˜¯æœ€é•¿çš„ä¸€èŠ‚è¯¾ï¼Œå› ä¸ºæˆ‘æ— æ³•ä»MLå¹³å°ä¸­é€»è¾‘æ€§åœ°åˆ†ç¦»å‡ºè®­ç»ƒç®¡é“ã€‚äº«å—å§ï¼'
- en: 'Course Lessons:'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹å†…å®¹ï¼š
- en: '[Batch Serving. Feature Stores. Feature Engineering Pipelines.](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ‰¹é‡æœåŠ¡ã€‚ç‰¹å¾å­˜å‚¨ã€‚ç‰¹å¾å·¥ç¨‹ç®¡é“ã€‚](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
- en: '**Training Pipelines. ML Platforms. Hyperparameter Tuning.**'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒç®¡é“ã€‚MLå¹³å°ã€‚è¶…å‚æ•°è°ƒæ•´ã€‚**'
- en: '[Batch Prediction Pipeline. Package Python Modules with Poetry.](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ‰¹é‡é¢„æµ‹ç®¡é“ã€‚ä½¿ç”¨ Poetry æ‰“åŒ… Python æ¨¡å—ã€‚](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
- en: '[Private PyPi Server. Orchestrate Everything with Airflow.](https://medium.com/towards-data-science/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç§æœ‰ PyPi æœåŠ¡å™¨ã€‚ä½¿ç”¨ Airflow åè°ƒä¸€åˆ‡ã€‚](https://medium.com/towards-data-science/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)'
- en: '[Data Validation for Quality and Integrity using GE. Model Performance Continuous
    Monitoring.](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨ GE è¿›è¡Œæ•°æ®è´¨é‡å’Œå®Œæ•´æ€§éªŒè¯ã€‚æ¨¡å‹æ€§èƒ½æŒç»­ç›‘æ§ã€‚](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
- en: '[Consume and Visualize your Modelâ€™s Predictions using FastAPI and Streamlit.
    Dockerize Everything.](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨ FastAPI å’Œ Streamlit æ¶ˆè€—å’Œå¯è§†åŒ–ä½ çš„æ¨¡å‹é¢„æµ‹ã€‚å°†ä¸€åˆ‡ Docker åŒ–ã€‚](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
- en: '[Deploy All the ML Components to GCP. Build a CI/CD Pipeline Using Github Actions.](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å°†æ‰€æœ‰ ML ç»„ä»¶éƒ¨ç½²åˆ° GCPã€‚ä½¿ç”¨ GitHub Actions æ„å»º CI/CD ç®¡é“ã€‚](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
- en: '[[Bonus] Behind the Scenes of an â€˜Imperfectâ€™ ML Project â€” Lessons and Insights](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[é™„èµ ] â€˜ä¸å®Œç¾â€™ ML é¡¹ç›®çš„å¹•åæ•…äº‹â€”â€”è¯¾ç¨‹å’Œè§è§£](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
- en: 'If you want to grasp this lesson fully, we recommend you check out the previous
    lesson, which talks about designing a batch-serving architecture, building a FE
    pipeline, and loading features into the feature store:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³å……åˆ†æŒæ¡æœ¬è¯¾å†…å®¹ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨æŸ¥çœ‹ä¹‹å‰çš„è¯¾ç¨‹ï¼Œè¯¥è¯¾ç¨‹è®¨è®ºäº†è®¾è®¡æ‰¹é‡æœåŠ¡æ¶æ„ã€æ„å»ºç‰¹å¾å·¥ç¨‹ç®¡é“å’Œå°†ç‰¹å¾åŠ è½½åˆ°ç‰¹å¾å­˜å‚¨ä¸­ï¼š
- en: '[](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f?source=post_page-----6fdaef594cee--------------------------------)
    [## A Framework for Building a Production-Ready Feature Engineering Pipeline'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[## æ„å»ºç”Ÿäº§å°±ç»ªç‰¹å¾å·¥ç¨‹ç®¡é“çš„æ¡†æ¶'
- en: 'Lesson 1: Batch Serving. Feature Stores. Feature Engineering Pipelines.'
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¬¬1è¯¾ï¼šæ‰¹é‡æœåŠ¡ã€‚ç‰¹å¾å­˜å‚¨ã€‚ç‰¹å¾å·¥ç¨‹ç®¡é“ã€‚
- en: towardsdatascience.com](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f?source=post_page-----6fdaef594cee--------------------------------)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f?source=post_page-----6fdaef594cee--------------------------------)'
- en: Data Source
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®æ¥æº
- en: We used a free & open API that provides hourly energy consumption values for
    all the energy consumer types within Denmark [1].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå…è´¹çš„å¼€æ”¾ APIï¼Œæä¾›ä¸¹éº¦æ‰€æœ‰èƒ½æºæ¶ˆè´¹è€…ç±»å‹çš„å°æ—¶èƒ½è€—å€¼ [1]ã€‚
- en: They provide an intuitive interface where you can easily query and visualize
    the data. [You can access the data here](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬æä¾›äº†ä¸€ä¸ªç›´è§‚çš„ç•Œé¢ï¼Œæ‚¨å¯ä»¥è½»æ¾æŸ¥è¯¢å’Œå¯è§†åŒ–æ•°æ®ã€‚[æ‚¨å¯ä»¥åœ¨è¿™é‡Œè®¿é—®æ•°æ®](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1]ã€‚
- en: 'The data has 4 main attributes:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®æœ‰4ä¸ªä¸»è¦å±æ€§ï¼š
- en: '**Hour UTC:** the UTC datetime when the data point was observed.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°æ—¶ UTCï¼š** æ•°æ®ç‚¹è§‚å¯Ÿåˆ°çš„ UTC æ—¥æœŸæ—¶é—´ã€‚'
- en: '**Price Area:** Denmark is divided into two price areas: DK1 and DK2 â€” divided
    by the Great Belt. DK1 is west of the Great Belt, and DK2 is east of the Great
    Belt.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä»·æ ¼åŒºåŸŸï¼š** ä¸¹éº¦åˆ†ä¸ºä¸¤ä¸ªä»·æ ¼åŒºåŸŸï¼šDK1 å’Œ DK2â€”â€”ç”±å¤§è´å°”ç‰¹åˆ†éš”ã€‚DK1 åœ¨å¤§è´å°”ç‰¹ä»¥è¥¿ï¼ŒDK2 åœ¨å¤§è´å°”ç‰¹ä»¥ä¸œã€‚'
- en: '**Consumer Type:** The consumer type is the Industry Code DE35, owned and maintained
    by Danish Energy.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¶ˆè´¹è€…ç±»å‹ï¼š** æ¶ˆè´¹è€…ç±»å‹æ˜¯ç”±ä¸¹éº¦èƒ½æºå…¬å¸æ‹¥æœ‰å’Œç»´æŠ¤çš„è¡Œä¸šä»£ç  DE35ã€‚'
- en: '**Total Consumption:** Total electricity consumption in kWh'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ€»æ¶ˆè€—ï¼š** ä»¥ kWh è®¡çš„æ€»ç”µåŠ›æ¶ˆè€—'
- en: '**Note:** The observations have a lag of 15 days! But for our demo use case,
    that is not a problem, as we can simulate the same steps as it would be in real-time.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** è§‚å¯Ÿå€¼æœ‰15å¤©çš„æ»åï¼ä½†å¯¹äºæˆ‘ä»¬çš„æ¼”ç¤ºç”¨ä¾‹æ¥è¯´ï¼Œè¿™ä¸æ˜¯é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥æ¨¡æ‹Ÿå®æ—¶ä¸­çš„ç›¸åŒæ­¥éª¤ã€‚'
- en: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
- en: A screenshot from our web app showing how we forecasted the energy consumption
    for area = 1 and consumer_type = 212 [Image by the Author].
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ web åº”ç”¨ç¨‹åºçš„æˆªå›¾ï¼Œæ˜¾ç¤ºäº†æˆ‘ä»¬å¦‚ä½•é¢„æµ‹åŒºåŸŸ = 1 å’Œæ¶ˆè´¹è€…ç±»å‹ = 212 çš„èƒ½è€— [ä½œè€…å›¾åƒ]ã€‚
- en: 'The data points have an hourly resolution. For example: â€œ2023â€“04â€“15 21:00Zâ€,
    â€œ2023â€“04â€“15 20:00Zâ€, â€œ2023â€“04â€“15 19:00Zâ€, etc.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç‚¹å…·æœ‰å°æ—¶åˆ†è¾¨ç‡ã€‚ä¾‹å¦‚ï¼šâ€œ2023â€“04â€“15 21:00Zâ€ï¼Œâ€œ2023â€“04â€“15 20:00Zâ€ï¼Œâ€œ2023â€“04â€“15 19:00Zâ€ç­‰ã€‚
- en: We will model the data as multiple time series. Each unique **price area** and
    **consumer type tuple represents its** unique time series.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ•°æ®å»ºæ¨¡ä¸ºå¤šä¸ªæ—¶é—´åºåˆ—ã€‚æ¯ä¸ªå”¯ä¸€çš„**ä»·æ ¼åŒºåŸŸ**å’Œ**æ¶ˆè´¹è€…ç±»å‹**ç»„åˆä»£è¡¨ä¸€ä¸ªç‹¬ç‰¹çš„æ—¶é—´åºåˆ—ã€‚
- en: Thus, we will build a model that independently forecasts the energy consumption
    for the next 24 hours for every time series.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œç‹¬ç«‹é¢„æµ‹æ¯ä¸ªæ—¶é—´åºåˆ—æœªæ¥24å°æ—¶çš„èƒ½è€—ã€‚
- en: '*Check out the video below to better understand what the data looks like* ğŸ‘‡'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*æŸ¥çœ‹ä¸‹é¢çš„è§†é¢‘ä»¥æ›´å¥½åœ°ç†è§£æ•°æ®çš„æ ·å­* ğŸ‘‡'
- en: Course & data source overview [Video by the Author].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹å’Œæ•°æ®æºæ¦‚è¿° [ä½œè€…è§†é¢‘]ã€‚
- en: 'Lesson 2: **Training Pipelines. ML Platforms. Hyperparameter Tuning.**'
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬2è¯¾ï¼š**è®­ç»ƒç®¡é“ã€‚æœºå™¨å­¦ä¹ å¹³å°ã€‚è¶…å‚æ•°è°ƒæ•´ã€‚**
- en: The Goal of Lesson 2
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬2è¯¾çš„ç›®æ ‡
- en: This lesson will teach you how to build the training pipeline and use an ML
    platform, as shown in the diagram below ğŸ‘‡
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è¯¾å°†æ•™æ‚¨å¦‚ä½•æ„å»ºè®­ç»ƒç®¡é“å¹¶ä½¿ç”¨æœºå™¨å­¦ä¹ å¹³å°ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º ğŸ‘‡
- en: '![](../Images/0596cb714571c8febed92fc310c1a6f4.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0596cb714571c8febed92fc310c1a6f4.png)'
- en: Diagram of the final architecture with the Lesson 2 components highlighted in
    blue [Image by the Author].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæ¶æ„å›¾ï¼Œå¸¦æœ‰ç¬¬2è¯¾ç»„ä»¶ç”¨è“è‰²çªå‡ºæ˜¾ç¤º [ä½œè€…å›¾åƒ]ã€‚
- en: More concretely, we will show you how to use the data from the Hopsworks feature
    store to train your model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Hopsworks ç‰¹å¾å­˜å‚¨ä¸­çš„æ•°æ®æ¥è®­ç»ƒæ‚¨çš„æ¨¡å‹ã€‚
- en: Also, we will show you how to build a forecasting model using LightGBM and Sktime
    that will predict the energy consumption levels for the next 24 hours between
    multiple consumer types across Denmark.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ LightGBM å’Œ Sktime æ„å»ºä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†é¢„æµ‹ä¸¹éº¦å¤šä¸ªæ¶ˆè´¹è€…ç±»å‹æœªæ¥24å°æ—¶çš„èƒ½è€—æ°´å¹³ã€‚
- en: Another critical step we will cover is how to use W&B as an ML platform that
    will track your experiments, register your models & configurations as artifacts,
    and perform hyperparameter tuning to find the best configuration for your model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¦†ç›–çš„å¦ä¸€ä¸ªå…³é”®æ­¥éª¤æ˜¯å¦‚ä½•ä½¿ç”¨ W&B ä½œä¸º ML å¹³å°æ¥è·Ÿè¸ªä½ çš„å®éªŒï¼Œæ³¨å†Œæ¨¡å‹å’Œé…ç½®ä¸ºå·¥ä»¶ï¼Œå¹¶æ‰§è¡Œè¶…å‚æ•°è°ƒä¼˜ä»¥æ‰¾åˆ°æ¨¡å‹çš„æœ€ä½³é…ç½®ã€‚
- en: Finally, based on the best config found in the hyperparameter tuning step, we
    will train the final model on the whole dataset and load it into the Hopsworks
    model registry to be used further by the batch prediction pipeline.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæ ¹æ®è¶…å‚æ•°è°ƒä¼˜æ­¥éª¤ä¸­æ‰¾åˆ°çš„æœ€ä½³é…ç½®ï¼Œæˆ‘ä»¬å°†ç”¨æ•´ä¸ªæ•°æ®é›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼Œå¹¶å°†å…¶åŠ è½½åˆ° Hopsworks æ¨¡å‹æ³¨å†Œè¡¨ä¸­ï¼Œä»¥ä¾¿åç»­æ‰¹é‡é¢„æµ‹ç®¡é“ä½¿ç”¨ã€‚
- en: '***NOTE:***This course is not about time series forecasting or hyperparameter
    tuning. This is an ML engineering course where I want to show you how multiple
    pieces come together into a single system. Thus, I will keep things straight to
    the point for the DS part of the code without going into too much detail.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ³¨æ„ï¼š*** æœ¬è¯¾ç¨‹ä¸æ¶‰åŠæ—¶é—´åºåˆ—é¢„æµ‹æˆ–è¶…å‚æ•°è°ƒä¼˜ã€‚è¿™æ˜¯ä¸€é—¨ ML å·¥ç¨‹è¯¾ç¨‹ï¼Œæˆ‘å¸Œæœ›å±•ç¤ºå¤šä¸ªéƒ¨åˆ†å¦‚ä½•æ±‡èšæˆä¸€ä¸ªå®Œæ•´çš„ç³»ç»Ÿã€‚å› æ­¤ï¼Œæˆ‘å°†ç›´æ¥åˆ‡å…¥ä»£ç çš„
    DS éƒ¨åˆ†ï¼Œè€Œä¸æ·±å…¥ç»†èŠ‚ã€‚'
- en: Theoretical Concepts & Tools
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†è®ºæ¦‚å¿µä¸å·¥å…·
- en: '**Sktime:** Sktimeis a Python package that provides tons of functionality for
    time series. It follows the same interface as Sklearn, hence its name. Using Sktime,
    we can quickly wrap LightGBM and perform forecasting for 24 hours in the future,
    cross-validation, and more. [Sktime official documentation](https://www.sktime.net/en/latest/index.html)
    [3]'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sktimeï¼š** Sktime æ˜¯ä¸€ä¸ª Python åŒ…ï¼Œæä¾›å¤§é‡æ—¶é—´åºåˆ—åŠŸèƒ½ã€‚å®ƒéµå¾ªä¸ Sklearn ç›¸åŒçš„æ¥å£ï¼Œå› æ­¤å¾—åã€‚ä½¿ç”¨ Sktimeï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿå°è£…
    LightGBM å¹¶æ‰§è¡Œæœªæ¥ 24 å°æ—¶çš„é¢„æµ‹ã€äº¤å‰éªŒè¯ç­‰ã€‚[Sktime å®˜æ–¹æ–‡æ¡£](https://www.sktime.net/en/latest/index.html)
    [3]'
- en: '**LightGBM:** LightGBM is a boosting tree-based model. It is built on top of
    Gradient Boosting and XGBoost, offering performance and speed improvements. Starting
    with XGBoost or LightGBM is a common practice. [LightGBM official documentation](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html)
    [4]'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**LightGBMï¼š** LightGBM æ˜¯ä¸€ä¸ªåŸºäºæå‡æ ‘çš„æ¨¡å‹ã€‚å®ƒå»ºç«‹åœ¨æ¢¯åº¦æå‡å’Œ XGBoost ä¹‹ä¸Šï¼Œæä¾›æ€§èƒ½å’Œé€Ÿåº¦çš„æå‡ã€‚ä»¥ XGBoost
    æˆ– LightGBM å¼€å§‹æ˜¯ä¸€ä¸ªå¸¸è§çš„åšæ³•ã€‚[LightGBM å®˜æ–¹æ–‡æ¡£](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html)
    [4]'
- en: If you want to learn more about LightGBM, check out my article, where I [explain
    in 15 minutes everything you need to know, from decision trees to LightGBM](https://medium.com/mlearning-ai/decision-trees-from-0-to-xgboost-lightgbm-a5f6827dfa23).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äº LightGBM çš„ä¿¡æ¯ï¼Œå¯ä»¥æŸ¥çœ‹æˆ‘çš„æ–‡ç« ï¼Œå…¶ä¸­æˆ‘ [åœ¨ 15 åˆ†é’Ÿå†…è§£é‡Šäº†ä»å†³ç­–æ ‘åˆ° LightGBM çš„ä¸€åˆ‡](https://medium.com/mlearning-ai/decision-trees-from-0-to-xgboost-lightgbm-a5f6827dfa23)ã€‚
- en: '**ML Platform:** An ML platform is a tool that allows you to easily track your
    experiments, log metadata about your training, upload and version artifacts, data
    lineage and more. An ML platform is a must in any training pipeline. You can intuitively
    see an ML platform as your central research & experimentation hub.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**ML å¹³å°ï¼š** ML å¹³å°æ˜¯ä¸€ç§å·¥å…·ï¼Œä½¿ä½ èƒ½å¤Ÿè½»æ¾è·Ÿè¸ªå®éªŒã€è®°å½•è®­ç»ƒå…ƒæ•°æ®ã€ä¸Šä¼ å’Œç‰ˆæœ¬åŒ–å·¥ä»¶ã€æ•°æ®è¡€ç¼˜ç­‰ã€‚ML å¹³å°åœ¨ä»»ä½•è®­ç»ƒç®¡é“ä¸­éƒ½æ˜¯å¿…ä¸å¯å°‘çš„ã€‚ä½ å¯ä»¥ç›´è§‚åœ°å°†
    ML å¹³å°è§†ä¸ºä½ çš„ä¸­å¤®ç ”ç©¶ä¸å®éªŒä¸­å¿ƒã€‚'
- en: '**Weights & Biases:** W&B is a popular serverless ML platform. We choose them
    as our ML platform because of 3 main reasons:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**Weights & Biasesï¼š** W&B æ˜¯ä¸€ä¸ªæµè¡Œçš„æ— æœåŠ¡å™¨ ML å¹³å°ã€‚æˆ‘ä»¬é€‰æ‹©å®ƒä½œä¸ºæˆ‘ä»¬çš„ ML å¹³å°æ˜¯å› ä¸º 3 ä¸ªä¸»è¦åŸå› ï¼š'
- en: their tool is fantastic & very intuitive to use
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»–ä»¬çš„å·¥å…·éå¸¸æ£’ä¸”éå¸¸ç›´è§‚ã€‚
- en: they provide a generous freemium version for personal research and projects
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒæä¾›äº†æ…·æ…¨çš„å…è´¹ç‰ˆä¾›ä¸ªäººç ”ç©¶å’Œé¡¹ç›®ä½¿ç”¨ã€‚
- en: it is serverless â€” no pain in deploying & maintaining your tools
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒæ˜¯æ— æœåŠ¡å™¨çš„â€”â€”æ— éœ€æ‹…å¿ƒéƒ¨ç½²å’Œç»´æŠ¤å·¥å…·ã€‚
- en: '**Training Pipeline:** The training pipeline is a logical construct (a single
    script, an application, or more) that takes curated and validated data as input
    (a result from the data and feature engineering pipelines) and outputs a working
    model as an artifact. Usually, the model is uploaded into a model registry that
    can later be accessed by various inference pipelines (the batch prediction pipeline
    from our series is an example of a concrete implementation of an inference pipeline).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒç®¡é“ï¼š** è®­ç»ƒç®¡é“æ˜¯ä¸€ä¸ªé€»è¾‘ç»“æ„ï¼ˆä¸€ä¸ªè„šæœ¬ã€ä¸€ä¸ªåº”ç”¨ç¨‹åºæˆ–æ›´å¤šï¼‰ï¼Œå®ƒæ¥å—ç»è¿‡ç­–åˆ’å’ŒéªŒè¯çš„æ•°æ®ä½œä¸ºè¾“å…¥ï¼ˆæ¥è‡ªæ•°æ®å’Œç‰¹å¾å·¥ç¨‹ç®¡é“çš„ç»“æœï¼‰ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªä½œä¸ºå·¥ä»¶çš„å·¥ä½œæ¨¡å‹ã€‚é€šå¸¸ï¼Œæ¨¡å‹ä¼šè¢«ä¸Šä¼ åˆ°ä¸€ä¸ªæ¨¡å‹æ³¨å†Œè¡¨ä¸­ï¼Œåæ¥å¯ä»¥è¢«å„ç§æ¨ç†ç®¡é“è®¿é—®ï¼ˆæˆ‘ä»¬ç³»åˆ—ä¸­çš„æ‰¹é‡é¢„æµ‹ç®¡é“æ˜¯æ¨ç†ç®¡é“çš„å…·ä½“å®ç°ç¤ºä¾‹ï¼‰ã€‚'
- en: 'Lesson 2: Code'
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ 2 è¯¾ï¼šä»£ç 
- en: '[You can access the GitHub repository here.](https://github.com/iusztinpaul/energy-forecasting)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä½ å¯ä»¥åœ¨è¿™é‡Œè®¿é—® GitHub ä»“åº“ã€‚](https://github.com/iusztinpaul/energy-forecasting)'
- en: '**Note:** All the installation instructions are in the READMEs of the repository.
    Here we will jump straight to the code.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** æ‰€æœ‰å®‰è£…è¯´æ˜éƒ½åœ¨ä»“åº“çš„ README æ–‡ä»¶ä¸­ã€‚è¿™é‡Œæˆ‘ä»¬ç›´æ¥è·³åˆ°ä»£ç éƒ¨åˆ†ã€‚'
- en: '*All the code within Lesson 2 is located under the* [***training-pipeline***](https://github.com/iusztinpaul/energy-forecasting/tree/main/training-pipeline)*folder.*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç¬¬2è¯¾ä¸­çš„æ‰€æœ‰ä»£ç éƒ½ä½äº* [***training-pipeline***](https://github.com/iusztinpaul/energy-forecasting/tree/main/training-pipeline)*æ–‡ä»¶å¤¹ä¸‹ã€‚*'
- en: 'The files under the [**training-pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/training-pipeline)folderare
    structured as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[**training-pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/training-pipeline)
    æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶ç»“æ„å¦‚ä¸‹ï¼š'
- en: '![](../Images/1df1548f211cb1fc256f23c4f77a28c4.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1df1548f211cb1fc256f23c4f77a28c4.png)'
- en: A screenshot that shows the structure of the training-pipeline folder [Image
    by the Author].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç¤º training-pipeline æ–‡ä»¶å¤¹ç»“æ„çš„æˆªå›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: All the code is located under the [**training_pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/training-pipeline/training_pipeline)directory
    (note the "_" instead of "-")**.**
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ä»£ç éƒ½ä½äº [**training_pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/training-pipeline/training_pipeline)ç›®å½•ä¸‹ï¼ˆæ³¨æ„ä½¿ç”¨"_"è€Œé"-"ï¼‰**ã€‚**
- en: Directly storing credentials in your git repository is a huge security risk.
    That is why you will inject sensitive information using a **.env** file.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´æ¥å°†å‡­è¯å­˜å‚¨åœ¨ä½ çš„ git ä»“åº“ä¸­æ˜¯ä¸€ä¸ªå·¨å¤§çš„å®‰å…¨éšæ‚£ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ å°†é€šè¿‡ **.env** æ–‡ä»¶æ³¨å…¥æ•æ„Ÿä¿¡æ¯çš„åŸå› ã€‚
- en: The **.env.default** is an example of all the variables you must configure.
    It is also helpful to store default values for attributes that are not sensitive
    (e.g., project name).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**.env.default** æ˜¯ä½ å¿…é¡»é…ç½®çš„æ‰€æœ‰å˜é‡çš„ç¤ºä¾‹ã€‚å®ƒè¿˜å¯ä»¥ç”¨æ¥å­˜å‚¨ä¸æ•æ„Ÿçš„å±æ€§çš„é»˜è®¤å€¼ï¼ˆä¾‹å¦‚ï¼Œé¡¹ç›®åç§°ï¼‰ã€‚'
- en: '![](../Images/1b4f40ca19a12ac8ff070610a8530d46.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b4f40ca19a12ac8ff070610a8530d46.png)'
- en: A screenshot of the .env.default file [Image by the Author].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: .env.default æ–‡ä»¶çš„æˆªå›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: '***Prepare Credentials***'
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***å‡†å¤‡å‡­è¯***'
- en: First of all, we have to create a **.env** filewhere we will add all our credentials.
    I already showed you in [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how to set up your **.env** file. Also, I explained in [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how the variables from the **.env** file are loaded from your **ML_PIPELINE_ROOT_DIR**
    directory into a **SETTINGS** Python dictionary to be used throughout your code.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»åˆ›å»ºä¸€ä¸ª **.env** æ–‡ä»¶ï¼Œæ·»åŠ æ‰€æœ‰å‡­è¯ã€‚æˆ‘å·²ç»åœ¨ [ç¬¬1è¯¾](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ä¸­å±•ç¤ºäº†å¦‚ä½•è®¾ç½®
    **.env** æ–‡ä»¶ã€‚å¦å¤–ï¼Œæˆ‘åœ¨ [ç¬¬1è¯¾](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ä¸­è§£é‡Šäº†
    **.env** æ–‡ä»¶ä¸­çš„å˜é‡å¦‚ä½•ä»ä½ çš„ **ML_PIPELINE_ROOT_DIR** ç›®å½•åŠ è½½åˆ° **SETTINGS** Python å­—å…¸ä¸­ï¼Œä»¥ä¾¿åœ¨ä½ çš„ä»£ç ä¸­ä½¿ç”¨ã€‚
- en: Thus, if you want to replicate what I have done, I strongly recommend checking
    out [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¦‚æœä½ æƒ³å¤åˆ¶æˆ‘æ‰€åšçš„å·¥ä½œï¼Œæˆ‘å¼ºçƒˆå»ºè®®ä½ æŸ¥çœ‹ [ç¬¬1è¯¾](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ã€‚
- en: '*If you only want a light read, you can completely skip the â€œ****Prepare Credentials****â€
    step.*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ åªæƒ³è¿›è¡Œè½»åº¦é˜…è¯»ï¼Œå¯ä»¥å®Œå…¨è·³è¿‡â€œ****å‡†å¤‡å‡­è¯****â€æ­¥éª¤ã€‚*'
- en: 'In Lesson 2, we will use two services:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬2è¯¾ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸¤ä¸ªæœåŠ¡ï¼š
- en: '[Hopsworks](https://www.hopsworks.ai/)'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Hopsworks](https://www.hopsworks.ai/)'
- en: '[Weights & Biases](https://wandb.ai/)'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Weights & Biases](https://wandb.ai/)'
- en: '[***Hopsworks***](https://www.hopsworks.ai/) ***(free)***'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[***Hopsworks***](https://www.hopsworks.ai/) ***(å…è´¹)***'
- en: We already showed you in [Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how to set up the credentials for **Hopsworks**. Please visit the ["Prepare Credentials"
    section from Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f),
    where we showed you in detail how to set up the API KEY for Hopsworks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»åœ¨ [ç¬¬1è¯¾](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ä¸­å±•ç¤ºäº†å¦‚ä½•ä¸º
    **Hopsworks** è®¾ç½®å‡­è¯ã€‚è¯·è®¿é—® [ç¬¬1è¯¾çš„â€œå‡†å¤‡å‡­è¯â€éƒ¨åˆ†](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†å¦‚ä½•ä¸º
    Hopsworks è®¾ç½® API KEYã€‚
- en: '[***Weights & Biases***](https://wandb.ai/) ***(free)***'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[***Weights & Biases***](https://wandb.ai/) ***(å…è´¹)***'
- en: To keep the lessons compact, we assume that you already read and applied the
    steps for preparing the credentials for Hopsworks from [Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¿æŒè¯¾ç¨‹ç®€æ´ï¼Œæˆ‘ä»¬å‡è®¾ä½ å·²ç»é˜…è¯»å¹¶åº”ç”¨äº†[ç¬¬1è¯¾](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ä¸­å‡†å¤‡Hopsworkså‡­æ®çš„æ­¥éª¤ã€‚
- en: The good news is that 90% of the steps are similar to the ones for configuring
    **Hopsworks**, except for how you can get your API key from W&B.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½æ¶ˆæ¯æ˜¯ï¼Œ90%çš„æ­¥éª¤ä¸é…ç½®**Hopsworks**çš„æ­¥éª¤ç±»ä¼¼ï¼Œé™¤äº†å¦‚ä½•ä»W&Bè·å–APIå¯†é’¥ã€‚
- en: First, create an account on W&B. After, create a team (aka entity) and a project
    (or use your default ones, if you have any).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œåœ¨W&Bä¸Šåˆ›å»ºä¸€ä¸ªè´¦æˆ·ã€‚ç„¶åï¼Œåˆ›å»ºä¸€ä¸ªå›¢é˜Ÿï¼ˆå³å®ä½“ï¼‰å’Œä¸€ä¸ªé¡¹ç›®ï¼ˆæˆ–è€…ä½¿ç”¨ä½ å·²æœ‰çš„é»˜è®¤é¡¹ç›®ï¼‰ã€‚
- en: '***Then, check the image below to see how to get your own W&B API KEY ğŸ‘‡***'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '***ç„¶åï¼ŒæŸ¥çœ‹ä¸‹é¢çš„å›¾ç‰‡ï¼Œäº†è§£å¦‚ä½•è·å–ä½ è‡ªå·±çš„W&B APIå¯†é’¥ ğŸ‘‡***'
- en: '![](../Images/42df4ffff19dc073c2ff162ff980c136.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42df4ffff19dc073c2ff162ff980c136.png)'
- en: Go to your W&B account. After, in the top-right corner, click your profile account,
    then "User settings." Once in your user settings, scroll down until you reach
    the "Danger Zone" card. Then, under the "API keys," hit the "New key" button.
    Copy your API key, and that is it. You have your API key [Image by the Author].
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å‰å¾€ä½ çš„W&Bè´¦æˆ·ã€‚åœ¨å³ä¸Šè§’ï¼Œç‚¹å‡»ä½ çš„ä¸ªäººèµ„æ–™è´¦æˆ·ï¼Œç„¶åé€‰æ‹©â€œç”¨æˆ·è®¾ç½®â€ã€‚è¿›å…¥ç”¨æˆ·è®¾ç½®åï¼Œå‘ä¸‹æ»šåŠ¨ç›´åˆ°ä½ çœ‹åˆ°â€œå±é™©åŒºåŸŸâ€å¡ç‰‡ã€‚ç„¶åï¼Œåœ¨â€œAPIå¯†é’¥â€ä¸‹ï¼Œç‚¹å‡»â€œæ–°å¯†é’¥â€æŒ‰é’®ã€‚å¤åˆ¶ä½ çš„APIå¯†é’¥ï¼Œå°±å®Œæˆäº†ã€‚ä½ ç°åœ¨æœ‰äº†ä½ çš„APIå¯†é’¥
    [å›¾ç‰‡ç”±ä½œè€…æä¾›]ã€‚
- en: 'Once you have all your W&B credentials, go to your **.env** file and replace
    them as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ æ‹¥æœ‰äº†æ‰€æœ‰çš„W&Bå‡­æ®ï¼Œå‰å¾€ä½ çš„**.env**æ–‡ä»¶å¹¶æŒ‰å¦‚ä¸‹æ–¹å¼æ›¿æ¢å®ƒä»¬ï¼š
- en: '**WANDB_ENTITY:** your entity/team name (ours: *â€œteaching-mlopsâ€*)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WANDB_ENTITYï¼š** ä½ çš„å®ä½“/å›¢é˜Ÿåç§°ï¼ˆæˆ‘ä»¬çš„ï¼š*â€œteaching-mlopsâ€*ï¼‰'
- en: '**WANDB_PROJECT:** your project name (ours: *â€œenergy_consumptionâ€*)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WANDB_PROJECTï¼š** ä½ çš„é¡¹ç›®åç§°ï¼ˆæˆ‘ä»¬çš„ï¼š*â€œenergy_consumptionâ€*ï¼‰'
- en: '**WANDB_API_KEY**: your API key'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WANDB_API_KEY**ï¼šä½ çš„APIå¯†é’¥'
- en: Loading the Data From the Feature Store
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»ç‰¹å¾å­˜å‚¨ä¸­åŠ è½½æ•°æ®
- en: As always, the first step is to access the data used to train and test the model.
    We already have all the data in the Hopsworks feature store. Thus, downloading
    it becomes a piece of cake.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å¦‚æ—¢å¾€ï¼Œç¬¬ä¸€æ­¥æ˜¯è®¿é—®ç”¨äºè®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹çš„æ•°æ®ã€‚æˆ‘ä»¬å·²ç»åœ¨Hopsworksç‰¹å¾å­˜å‚¨ä¸­æ‹¥æœ‰æ‰€æœ‰æ•°æ®ã€‚å› æ­¤ï¼Œä¸‹è½½å®ƒå˜å¾—è½»è€Œæ˜“ä¸¾ã€‚
- en: The code snippet below has the **load_dataset_from_feature_store**() IO function
    under the [**training_pipeline/data.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/training-pipeline/training_pipeline/data.py)
    file. You will use this function to download the data for a given **feature_view_version**
    and **training_dataset_version.**
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„ä»£ç ç‰‡æ®µåŒ…å«äº†**load_dataset_from_feature_store**() IOå‡½æ•°ï¼Œå®ƒä½äº[**training_pipeline/data.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/training-pipeline/training_pipeline/data.py)æ–‡ä»¶ä¸­ã€‚ä½ å°†ä½¿ç”¨è¿™ä¸ªå‡½æ•°ä¸‹è½½ç‰¹å®šçš„**feature_view_version**å’Œ**training_dataset_version**çš„æ•°æ®ã€‚
- en: '**NOTE:** By giving a specific data version, you will always know with what
    data you trained and evaluated the model. Thus, you can consistently reproduce
    your results.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** é€šè¿‡æä¾›ç‰¹å®šçš„æ•°æ®ç‰ˆæœ¬ï¼Œä½ å°†å§‹ç»ˆçŸ¥é“ä½ ä½¿ç”¨äº†å“ªäº›æ•°æ®æ¥è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚å› æ­¤ï¼Œä½ å¯ä»¥ä¸€è‡´åœ°é‡ç°ä½ çš„ç»“æœã€‚'
- en: 'Using the function below, we perform the following steps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸‹é¢çš„å‡½æ•°ï¼Œæˆ‘ä»¬æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
- en: We access the Hopsworks feature store.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¿é—®Hopsworksç‰¹å¾å­˜å‚¨ã€‚
- en: We get a reference to the given version of the feature view.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è·å–ç»™å®šç‰ˆæœ¬çš„ç‰¹å¾è§†å›¾çš„å¼•ç”¨ã€‚
- en: We get a reference to the given version of the training data.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è·å–ç»™å®šç‰ˆæœ¬çš„è®­ç»ƒæ•°æ®çš„å¼•ç”¨ã€‚
- en: We log to W&B all the metadata that relates to the used dataset.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ‰€æœ‰ä¸ä½¿ç”¨çš„æ•°æ®é›†ç›¸å…³çš„å…ƒæ•°æ®è®°å½•åˆ°W&Bã€‚
- en: Now that we downloaded the dataset, we run it through the **prepare_data()**
    function. We will detail it a bit later. For now, notice that we split the data
    between train and test.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ä¸‹è½½äº†æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†å…¶ä¼ é€’ç»™**prepare_data()**å‡½æ•°ã€‚ç¨åæˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»å®ƒã€‚ç›®å‰ï¼Œè¯·æ³¨æ„æˆ‘ä»¬å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
- en: We log to W&B all the metadata related to how we split the dataset, plus some
    basic statistics for every split, such as split size and features.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ‰€æœ‰ä¸æ•°æ®é›†æ‹†åˆ†ç›¸å…³çš„å…ƒæ•°æ®è®°å½•åˆ°W&Bä¸­ï¼Œä»¥åŠæ¯ä¸ªæ‹†åˆ†çš„ä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚æ‹†åˆ†å¤§å°å’Œç‰¹å¾ã€‚
- en: '**Important observation:** Using W&B, you log all the metadata that describes
    how you extracted and prepared the data. By doing so, you can easily understand
    for every experiment the origin of its data.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**é‡è¦è§‚å¯Ÿï¼š** ä½¿ç”¨W&Bï¼Œä½ å¯ä»¥è®°å½•æ‰€æœ‰æè¿°ä½ å¦‚ä½•æå–å’Œå‡†å¤‡æ•°æ®çš„å…ƒæ•°æ®ã€‚é€šè¿‡è¿™æ ·åšï¼Œä½ å¯ä»¥è½»æ¾äº†è§£æ¯æ¬¡å®éªŒçš„æ•°æ®æ¥æºã€‚'
- en: By using **run.use_artifact("<artifact_name>"),** you can link different artifacts
    between them. In our example, by calling **run.use_artifact(â€œenergy_consumption_denmark_feature_view:latestâ€)**
    we linked this W&B run with an artifact created in a different W&B run.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨ **run.use_artifact("<artifact_name>")**ï¼Œä½ å¯ä»¥åœ¨ä¸åŒçš„å·¥ä»¶ä¹‹é—´å»ºç«‹è”ç³»ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œé€šè¿‡è°ƒç”¨ **run.use_artifact(â€œenergy_consumption_denmark_feature_view:latestâ€)**ï¼Œæˆ‘ä»¬å°†è¿™ä¸ª
    W&B è¿è¡Œä¸åœ¨ä¸åŒ W&B è¿è¡Œä¸­åˆ›å»ºçš„å·¥ä»¶å…³è”èµ·æ¥ã€‚
- en: Check out the video below to see how the W&B runs & artifacts look like in the
    W&B interface ğŸ‘‡
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹ä¸‹é¢çš„è§†é¢‘ï¼Œä»¥äº†è§£ W&B è¿è¡Œå’Œå·¥ä»¶åœ¨ W&B ç•Œé¢ä¸­çš„æ ·å­ ğŸ‘‡
- en: W&B artifacts overview [Video by the Author].
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: W&B å·¥ä»¶æ¦‚è¿° [ä½œè€…çš„è§†é¢‘]ã€‚
- en: Now, let's dig into the **prepare_data()** function.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ·±å…¥äº†è§£ **prepare_data()** å‡½æ•°ã€‚
- en: '*I want to highlight that in the* ***prepare_data()*** *function, we won''t
    perform any feature engineering steps.*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘æƒ³å¼ºè°ƒçš„æ˜¯ï¼Œåœ¨* ***prepare_data()*** *å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬ä¸ä¼šæ‰§è¡Œä»»ä½•ç‰¹å¾å·¥ç¨‹æ­¥éª¤ã€‚*'
- en: As you can see below, in this function, you will restructure the data to be
    compatible with the **sktime** interface, pick the target, and split the data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸‹æ‰€ç¤ºï¼Œåœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œä½ å°†é‡æ„æ•°æ®ä»¥å…¼å®¹ **sktime** ç•Œé¢ï¼Œé€‰æ‹©ç›®æ ‡ï¼Œå¹¶æ‹†åˆ†æ•°æ®ã€‚
- en: The data is modeled for hierarchical time series, translating to multiple independent
    observations of the same variable in different contexts. In our example, we observe
    the energy consumption for various areas and energy consumption types.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®è¢«å»ºæ¨¡ä¸ºå±‚æ¬¡æ—¶é—´åºåˆ—ï¼Œè½¬åŒ–ä¸ºåŒä¸€å˜é‡åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„å¤šä¸ªç‹¬ç«‹è§‚å¯Ÿã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿäº†ä¸åŒåŒºåŸŸå’Œèƒ½æºæ¶ˆè€—ç±»å‹çš„èƒ½æºæ¶ˆè€—æƒ…å†µã€‚
- en: Sktime, for hierarchical time series, expects the data to be modeled using multi-indexes,
    where the datetime index is the last. To learn more about hierarchical forecasting,
    check out [Sktime's official tutorial](https://github.com/sktime/sktime/blob/main/examples/01c_forecasting_hierarchical_global.ipynb)
    [7].
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå±‚æ¬¡æ—¶é—´åºåˆ—ï¼ŒSktime æœŸæœ›æ•°æ®ä½¿ç”¨å¤šé‡ç´¢å¼•å»ºæ¨¡ï¼Œå…¶ä¸­æ—¥æœŸæ—¶é—´ç´¢å¼•æ˜¯æœ€åä¸€ä¸ªã€‚è¦äº†è§£æ›´å¤šæœ‰å…³å±‚æ¬¡é¢„æµ‹çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [Sktime å®˜æ–¹æ•™ç¨‹](https://github.com/sktime/sktime/blob/main/examples/01c_forecasting_hierarchical_global.ipynb)
    [7]ã€‚
- en: Also, we can safely split the data using **sktime's temporal_train_test_split()**
    function. The test split has the length of the given **fh (=forecast horizon)**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥å®‰å…¨åœ°ä½¿ç”¨ **sktime's temporal_train_test_split()** å‡½æ•°æ‹†åˆ†æ•°æ®ã€‚æµ‹è¯•é›†çš„é•¿åº¦ä¸ºç»™å®šçš„ **fh (=forecast
    horizon)**ã€‚
- en: One key observation is that the test split isn't sampled randomly but based
    on the latest observation. For example, if you have data from the 1st of May 2023
    until the 7th of May 2023 with a frequency of 1 hour, then the test split with
    a length of 24 hours will contain all the values from the last day of the data,
    which is 7th of May 2023.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…³é”®è§‚å¯Ÿç‚¹æ˜¯æµ‹è¯•é›†æ‹†åˆ†ä¸æ˜¯éšæœºæŠ½æ ·çš„ï¼Œè€Œæ˜¯åŸºäºæœ€æ–°çš„è§‚å¯Ÿæ•°æ®ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ çš„æ•°æ®ä» 2023 å¹´ 5 æœˆ 1 æ—¥åˆ° 2023 å¹´ 5 æœˆ 7 æ—¥ï¼Œé¢‘ç‡ä¸º
    1 å°æ—¶ï¼Œåˆ™é•¿åº¦ä¸º 24 å°æ—¶çš„æµ‹è¯•é›†å°†åŒ…å«æ•°æ®çš„æœ€åä¸€å¤©ï¼Œå³ 2023 å¹´ 5 æœˆ 7 æ—¥çš„æ‰€æœ‰å€¼ã€‚
- en: '**Building the Forecasting Model**'
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ„å»ºé¢„æµ‹æ¨¡å‹**'
- en: '**Baseline model**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸºå‡†æ¨¡å‹**'
- en: Firstly, you will create a naive baseline model to use as a reference. This
    model predicts the last value based on a given seasonal periodicity.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä½ å°†åˆ›å»ºä¸€ä¸ªæœ´ç´ åŸºå‡†æ¨¡å‹ä½œä¸ºå‚è€ƒã€‚è¯¥æ¨¡å‹åŸºäºç»™å®šçš„å­£èŠ‚æ€§å‘¨æœŸæ€§é¢„æµ‹æœ€åä¸€ä¸ªå€¼ã€‚
- en: For example, if **seasonal_periodicity = 24 hours**, it will return the value
    from "**present - 24 hours"**.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœ **seasonal_periodicity = 24 hours**ï¼Œå®ƒå°†è¿”å›ä»â€œ**å½“å‰ - 24 å°æ—¶**â€çš„å€¼ã€‚
- en: Using a baseline is a healthy practice that helps you compare your fancy ML
    model to something simpler. The ML model is useless if you can't beat the baseline
    model with your fancy model.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åŸºå‡†æ¨¡å‹æ˜¯ä¸€ä¸ªå¥åº·çš„å®è·µï¼Œå®ƒå¯ä»¥å¸®åŠ©ä½ å°†ä½ çš„é«˜çº§ ML æ¨¡å‹ä¸æ›´ç®€å•çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœä½ ä¸èƒ½ç”¨ä½ çš„é«˜çº§æ¨¡å‹è¶…è¶ŠåŸºå‡†æ¨¡å‹ï¼Œé‚£ä¹ˆ ML æ¨¡å‹æ˜¯æ²¡æœ‰ç”¨çš„ã€‚
- en: '**Fancy ML model**'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**é«˜çº§ ML æ¨¡å‹**'
- en: We will build the model using Sktime and LightGBM.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨ Sktime å’Œ LightGBM æ¥æ„å»ºæ¨¡å‹ã€‚
- en: Check out [Sktime documentation](https://www.sktime.net/en/latest/index.html)
    [3] and [LightGBM documentation](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html)
    [4] here.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹ [Sktime æ–‡æ¡£](https://www.sktime.net/en/latest/index.html) [3] å’Œ [LightGBM
    æ–‡æ¡£](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html) [4]ã€‚
- en: If you are into time series, check out this [Forecasting with Sktime tutorial](https://www.sktime.net/en/latest/examples/01_forecasting.html#1.-Basic-forecasting-workflows)
    [6]. If you only want to understand the system's big picture, you can continue.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹æ—¶é—´åºåˆ—æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹è¿™ä¸ª [Sktime é¢„æµ‹æ•™ç¨‹](https://www.sktime.net/en/latest/examples/01_forecasting.html#1.-Basic-forecasting-workflows)
    [6]ã€‚å¦‚æœä½ åªæƒ³äº†è§£ç³»ç»Ÿçš„å¤§è‡´æƒ…å†µï¼Œä½ å¯ä»¥ç»§ç»­ã€‚
- en: LightGBM will be your regressor that learns patterns within the data and forecasts
    future values.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM å°†ä½œä¸ºä½ çš„å›å½’æ¨¡å‹ï¼Œç”¨äºå­¦ä¹ æ•°æ®ä¸­çš„æ¨¡å¼å¹¶é¢„æµ‹æœªæ¥çš„å€¼ã€‚
- en: Using the **WindowSummarizer** class from **Sktime,** you can quickly compute
    lags and mean & standard deviation for various windows.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¥è‡ª**Sktime**çš„**WindowSummarizer**ç±»ï¼Œä½ å¯ä»¥å¿«é€Ÿè®¡ç®—å„ç§çª—å£çš„æ»åå’Œå‡å€¼åŠæ ‡å‡†å·®ã€‚
- en: For example, for the lag, we provide a default value of **list(range(1, 72 +
    1)),** which translates to "compute the lag for the last 72 hours".
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¯¹äºæ»åï¼Œæˆ‘ä»¬æä¾›äº†é»˜è®¤å€¼**list(range(1, 72 + 1)),** è¿™æ„å‘³ç€â€œè®¡ç®—è¿‡å»72å°æ—¶çš„æ»åâ€ã€‚
- en: Also, as an example of the mean lag, we have the default value of **[[1, 24],
    [1, 48], [1, 72]].** For example, **[1, 24]** translates to a lag of 1 and a window
    size of 24, meaning it will compute the mean in the last 24 days. Thus, in the
    end, for **[[1, 24], [1, 48], [1, 72]],** you will have the mean for the last
    24, 46, and 72 days.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œä½œä¸ºå‡å€¼æ»åçš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬æœ‰é»˜è®¤å€¼**[[1, 24], [1, 48], [1, 72]].** ä¾‹å¦‚ï¼Œ**[1, 24]** è¡¨ç¤ºæ»åä¸º1ï¼Œçª—å£å¤§å°ä¸º24ï¼Œè¿™æ„å‘³ç€å®ƒå°†è®¡ç®—è¿‡å»24å¤©çš„å‡å€¼ã€‚å› æ­¤ï¼Œæœ€ç»ˆå¯¹äº**[[1,
    24], [1, 48], [1, 72]],** ä½ å°†å¾—åˆ°è¿‡å»24å¤©ã€46å¤©å’Œ72å¤©çš„å‡å€¼ã€‚
- en: The same principle applies to the standard deviation values. [Check out this
    doc to learn more](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.transformations.series.summarize.WindowSummarizer.html?highlight=windowsummarizer)
    [2].
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åŒçš„åŸåˆ™é€‚ç”¨äºæ ‡å‡†å·®å€¼ã€‚[æŸ¥çœ‹æ­¤æ–‡æ¡£ä»¥äº†è§£æ›´å¤š](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.transformations.series.summarize.WindowSummarizer.html?highlight=windowsummarizer)
    [2]ã€‚
- en: You wrap the LightGBM model using the **make_reduction()** function from **Sktime.**
    By doing so, you can easily attach the **WindowSummarizer** you initialized earlier.
    Also, by specifying **strategy = "recursive",** you can easily forecast multiple
    values into the future using a recursive paradigm. For example, if you want to
    predict 3 hours into the future, the model will first forecast the value for T
    + 1\. Afterward, it will use as input the value it forecasted at T + 1 to forecast
    the value at T + 2, and so onâ€¦
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä½¿ç”¨**Sktime**ä¸­çš„**make_reduction()**å‡½æ•°æ¥åŒ…è£…LightGBMæ¨¡å‹ã€‚é€šè¿‡è¿™æ ·åšï¼Œä½ å¯ä»¥è½»æ¾é™„åŠ ä¹‹å‰åˆå§‹åŒ–çš„**WindowSummarizer**ã€‚æ­¤å¤–ï¼Œé€šè¿‡æŒ‡å®š**strategy
    = "recursive"**ï¼Œä½ å¯ä»¥ä½¿ç”¨é€’å½’èŒƒå¼è½»æ¾é¢„æµ‹å¤šä¸ªæœªæ¥å€¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³é¢„æµ‹æœªæ¥3å°æ—¶ï¼Œæ¨¡å‹å°†é¦–å…ˆé¢„æµ‹T + 1çš„å€¼ã€‚ç„¶åï¼Œå®ƒå°†ä½¿ç”¨T + 1é¢„æµ‹çš„å€¼ä½œä¸ºè¾“å…¥æ¥é¢„æµ‹T
    + 2çš„å€¼ï¼Œä»¥æ­¤ç±»æ¨â€¦â€¦
- en: 'Finally, we will build the **ForecastingPipeline** where we will attach two
    transformers:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°†æ„å»º**ForecastingPipeline**ï¼Œåœ¨å…¶ä¸­é™„åŠ ä¸¤ä¸ªè½¬æ¢å™¨ï¼š
- en: '**transformers.AttachAreaConsumerType():** a custom transformer that takes
    the area and consumer type from the index and adds it as an exogenous variable.
    We will show you how we defined it.'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**transformers.AttachAreaConsumerType():** ä¸€ä¸ªè‡ªå®šä¹‰è½¬æ¢å™¨ï¼Œå®ƒä»ç´¢å¼•ä¸­æå–åŒºåŸŸå’Œæ¶ˆè´¹è€…ç±»å‹ï¼Œå¹¶å°†å…¶ä½œä¸ºå¤–ç”Ÿå˜é‡æ·»åŠ ã€‚æˆ‘ä»¬å°†å‘ä½ å±•ç¤ºæˆ‘ä»¬å¦‚ä½•å®šä¹‰å®ƒã€‚'
- en: '**DateTimeFeatures():** a transformer from **Sktime** that computes different
    datetime-related exogenous features. In our case, we used only the day of the
    week and the hour of the day as additional features.'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**DateTimeFeatures():** ä¸€ä¸ªæ¥è‡ª**Sktime**çš„è½¬æ¢å™¨ï¼Œç”¨äºè®¡ç®—ä¸åŒçš„ä¸æ—¥æœŸæ—¶é—´ç›¸å…³çš„å¤–ç”Ÿç‰¹å¾ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨äº†æ˜ŸæœŸå‡ å’Œä¸€å¤©ä¸­çš„å°æ—¶ä½œä¸ºé¢å¤–ç‰¹å¾ã€‚'
- en: Note that these transformers are similar to the ones from **Sklearn,** as **Sktime**
    kept the same interface and design. Using transformers is a critical step in designing
    modular models. To learn more about Sklearn transformers and pipelines, check
    out my article about [How to Quickly Design Advanced Sklearn Pipelines](/how-to-quickly-design-advanced-sklearn-pipelines-3cc97b59ce16).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œè¿™äº›è½¬æ¢å™¨ç±»ä¼¼äº**Sklearn**ä¸­çš„é‚£äº›ï¼Œå› ä¸º**Sktime**ä¿æŒäº†ç›¸åŒçš„æ¥å£å’Œè®¾è®¡ã€‚ä½¿ç”¨è½¬æ¢å™¨æ˜¯è®¾è®¡æ¨¡å—åŒ–æ¨¡å‹ä¸­çš„å…³é”®æ­¥éª¤ã€‚è¦äº†è§£æ›´å¤šå…³äºSklearnè½¬æ¢å™¨å’Œç®¡é“çš„å†…å®¹ï¼Œè¯·æŸ¥çœ‹æˆ‘çš„æ–‡ç« ï¼š[å¦‚ä½•å¿«é€Ÿè®¾è®¡é«˜çº§Sklearnç®¡é“](/how-to-quickly-design-advanced-sklearn-pipelines-3cc97b59ce16)ã€‚
- en: Finally, we initialized the hyperparameters of the pipeline and model with the
    given configuration.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬ç”¨ç»™å®šçš„é…ç½®åˆå§‹åŒ–äº†ç®¡é“å’Œæ¨¡å‹çš„è¶…å‚æ•°ã€‚
- en: The **AttachAreaConsumerType** transformer is quite easy to comprehend. We implemented
    it as an example to show what is possible.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**AttachAreaConsumerType**è½¬æ¢å™¨å¾ˆå®¹æ˜“ç†è§£ã€‚æˆ‘ä»¬å°†å…¶å®ç°ä¸ºä¸€ä¸ªç¤ºä¾‹ï¼Œå±•ç¤ºå…¶å¯èƒ½æ€§ã€‚'
- en: Long story short, it just copies the values from the index into its own column.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: é•¿è¯çŸ­è¯´ï¼Œå®ƒåªæ˜¯å°†ç´¢å¼•ä¸­çš„å€¼å¤åˆ¶åˆ°å®ƒè‡ªå·±çš„åˆ—ä¸­ã€‚
- en: '**IMPORTANT OBSERVATION â€” DESIGN DECISION**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**é‡è¦è§‚å¯Ÿâ€”â€”è®¾è®¡å†³ç­–**'
- en: As you can see, all the feature engineering steps are built-in into the forecasting
    pipeline object.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œæ‰€æœ‰çš„ç‰¹å¾å·¥ç¨‹æ­¥éª¤éƒ½å†…ç½®åœ¨é¢„æµ‹ç®¡é“å¯¹è±¡ä¸­ã€‚
- en: 'You might ask: "But why? By doing so, don''t we keep the feature engineering
    logic in the training pipeline?"'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šé—®ï¼šâ€œä½†ä¸ºä»€ä¹ˆï¼Ÿè¿™æ ·åšï¼Œæˆ‘ä»¬ä¸æ˜¯å°†ç‰¹å¾å·¥ç¨‹é€»è¾‘ä¿ç•™åœ¨è®­ç»ƒæµç¨‹ä¸­å—ï¼Ÿâ€
- en: Well, yesâ€¦ and noâ€¦
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œæ˜¯çš„â€¦â€¦ä¹Ÿä¸æ˜¯â€¦â€¦
- en: We indeed defined the forecasting pipeline in the training script, but the key
    idea is that we will save the whole forecasting pipeline to the model registry.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç¡®å®åœ¨è®­ç»ƒè„šæœ¬ä¸­å®šä¹‰äº†é¢„æµ‹ç®¡é“ï¼Œä½†å…³é”®æ€æƒ³æ˜¯æˆ‘ä»¬å°†æ•´ä¸ªé¢„æµ‹ç®¡é“ä¿å­˜åˆ°æ¨¡å‹æ³¨å†Œè¡¨ä¸­ã€‚
- en: Thus, when we load the model, we will also load all the preprocessing and postprocessing
    steps included in the forecasting pipeline.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå½“æˆ‘ä»¬åŠ è½½æ¨¡å‹æ—¶ï¼Œä¹Ÿä¼šåŠ è½½é¢„æµ‹ç®¡é“ä¸­åŒ…å«çš„æ‰€æœ‰é¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ã€‚
- en: This means all the feature engineering is encapsulated in the forecasting pipeline,
    and we can safely treat it as a black box.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æ‰€æœ‰çš„ç‰¹å¾å·¥ç¨‹éƒ½å°è£…åœ¨é¢„æµ‹ç®¡é“ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°å°†å…¶è§†ä¸ºä¸€ä¸ªé»‘ç®±ã€‚
- en: This is one way to store the transformation + the raw data in the feature store,
    as discussed in [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§å°†è½¬æ¢ + åŸå§‹æ•°æ®å­˜å‚¨åœ¨ç‰¹å¾å­˜å‚¨ä¸­çš„æ–¹å¼ï¼Œå¦‚[Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ä¸­è®¨è®ºçš„é‚£æ ·ã€‚
- en: We could have also stored the transformation functions independently in the
    feature store, but composing a single pipeline object is cleaner.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ¬æ¥ä¹Ÿå¯ä»¥åœ¨ç‰¹å¾å­˜å‚¨ä¸­ç‹¬ç«‹å­˜å‚¨è½¬æ¢å‡½æ•°ï¼Œä½†ç»„åˆä¸€ä¸ªå•ä¸€çš„ç®¡é“å¯¹è±¡ä¼šæ›´ç®€æ´ã€‚
- en: Hyperparameter Tuning
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒæ•´
- en: '**How to use W&B sweeps**'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•ä½¿ç”¨W&B sweeps**'
- en: You will use W&B to perform hyperparameter tuning. They provide all the methods
    you need. Starting from a regular Grid Search until a Bayesian Search.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†ä½¿ç”¨W&Bè¿›è¡Œè¶…å‚æ•°è°ƒæ•´ã€‚ä»–ä»¬æä¾›äº†ä½ éœ€è¦çš„æ‰€æœ‰æ–¹æ³•ã€‚ä»å¸¸è§„çš„ç½‘æ ¼æœç´¢åˆ°è´å¶æ–¯æœç´¢ã€‚
- en: W&B uses ***sweeps*** to do hyperparameter tuning. A sweep is a fancy word for
    a single experiment within multiple experiments based on your hyperparameter search
    space.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: W&Bä½¿ç”¨***sweeps***æ¥è¿›è¡Œè¶…å‚æ•°è°ƒæ•´ã€‚sweepæ˜¯æŒ‡åœ¨åŸºäºè¶…å‚æ•°æœç´¢ç©ºé—´çš„å¤šä¸ªå®éªŒä¸­çš„å•ä¸ªå®éªŒçš„é«˜çº§æœ¯è¯­ã€‚
- en: We will use the MAPE (mean absolute percentage error) metric to compare experiments
    to find the best hyperparameter configuration. We chose MAPE over MAE or RMSE
    because the values are normalized between [0, 1], thus making it easier to analyze.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨MAPEï¼ˆå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼‰æŒ‡æ ‡æ¥æ¯”è¾ƒå®éªŒï¼Œä»¥æ‰¾åˆ°æœ€ä½³çš„è¶…å‚æ•°é…ç½®ã€‚æˆ‘ä»¬é€‰æ‹©MAPEè€Œä¸æ˜¯MAEæˆ–RMSEï¼Œå› ä¸ºå®ƒçš„å€¼åœ¨[0, 1]ä¹‹é—´å½’ä¸€åŒ–ï¼Œä»è€Œä½¿åˆ†ææ›´ä¸ºå®¹æ˜“ã€‚
- en: Check out the video below to see how the sweeps board looks in W&B ğŸ‘‡
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ä¸‹é¢çš„è§†é¢‘ï¼Œäº†è§£W&Bä¸­çš„sweepsé¢æ¿çš„æ ·å­ ğŸ‘‡
- en: Now that we understand our goal let's look at the code under the [**training_pipeline/hyperparamter_tuning.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/training-pipeline/training_pipeline/hyperparameter_tuning.py)
    file.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æ˜ç™½äº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œè®©æˆ‘ä»¬æŸ¥çœ‹[**training_pipeline/hyperparamter_tuning.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/training-pipeline/training_pipeline/hyperparameter_tuning.py)æ–‡ä»¶ä¸­çš„ä»£ç ã€‚
- en: As you can see in the function below, we load the dataset from the feature store
    for a specific **feature_view_version** and a **training_dataset_version.**
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸‹å‡½æ•°æ‰€ç¤ºï¼Œæˆ‘ä»¬ä»ç‰¹å¾å­˜å‚¨ä¸­åŠ è½½ç‰¹å®š**feature_view_version**å’Œ**training_dataset_version**çš„æ•°æ®é›†ã€‚
- en: Using solely the training data, we start the hyperparameter optimization.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…ä½¿ç”¨è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬å¼€å§‹è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚
- en: '**Note:** It is essential that you don''t use your test data for your hyperparameter
    optimization search. Otherwise, you risk overfitting your test split, and your
    model will not generalize. Your test split should be used only for the final decision.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** ä½ å¿…é¡»ç¡®ä¿ä¸è¦ä½¿ç”¨æµ‹è¯•æ•°æ®è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–æœç´¢ã€‚å¦åˆ™ï¼Œä½ å¯èƒ½ä¼šå¯¼è‡´æµ‹è¯•æ‹†åˆ†è¿‡æ‹Ÿåˆï¼Œä»è€Œä½¿æ¨¡å‹æ— æ³•æ³›åŒ–ã€‚æµ‹è¯•æ‹†åˆ†åº”ä»…ç”¨äºæœ€ç»ˆå†³ç­–ã€‚'
- en: Finally, we save the metadata of the run, which contains the **sweep_id** of
    the search.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬ä¿å­˜è¿è¡Œçš„å…ƒæ•°æ®ï¼Œå…¶ä¸­åŒ…å«æœç´¢çš„**sweep_id**ã€‚
- en: Now, let's look at the **run_hyperparameter_optimization()** function, which
    takes the training data, creates a new sweep and starts a W&B agent.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹**run_hyperparameter_optimization()**å‡½æ•°ï¼Œå®ƒæ¥æ”¶è®­ç»ƒæ•°æ®ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„sweepå¹¶å¯åŠ¨ä¸€ä¸ªW&Bä»£ç†ã€‚
- en: Within a single sweep run, we build the model and train the model using cross-validation.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å•æ¬¡sweepè¿è¡Œä¸­ï¼Œæˆ‘ä»¬æ„å»ºæ¨¡å‹å¹¶ä½¿ç”¨äº¤å‰éªŒè¯è®­ç»ƒæ¨¡å‹ã€‚
- en: As you can see, the config is provided by W&B based on the given hyperparameter
    search space (we will explain this in a bit). Also, we log the config as an artifact
    to access it later.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œé…ç½®ç”±W&Bæä¾›ï¼ŒåŸºäºç»™å®šçš„è¶…å‚æ•°æœç´¢ç©ºé—´ï¼ˆç¨åæˆ‘ä»¬ä¼šè¯¦ç»†è§£é‡Šï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†é…ç½®ä½œä¸ºä¸€ä¸ªå·¥ä»¶è¿›è¡Œæ—¥å¿—è®°å½•ï¼Œä»¥ä¾¿ä»¥åè®¿é—®ã€‚
- en: In our example, we used a simple grid search to perform hyperparameter tuning.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç®€å•çš„ç½‘æ ¼æœç´¢æ¥è¿›è¡Œè¶…å‚æ•°è°ƒæ•´ã€‚
- en: As you can see below, we created a Python dictionary called **sweep_config**
    with themethod, the metric to minimize, and the parameters to search for.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸‹æ‰€ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸º**sweep_config**çš„Pythonå­—å…¸ï¼Œå…¶ä¸­åŒ…å«äº†æ–¹æ³•ã€éœ€è¦æœ€å°åŒ–çš„æŒ‡æ ‡å’Œè¦æœç´¢çš„å‚æ•°ã€‚
- en: '[Check out W&B official docs to learn more about sweeps](https://docs.wandb.ai/guides/sweeps)
    [5].'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹W&Bå®˜æ–¹æ–‡æ¡£ä»¥äº†è§£æ›´å¤šå…³äºæ‰«é¢‘çš„ä¿¡æ¯](https://docs.wandb.ai/guides/sweeps) [5]ã€‚'
- en: '**Note:** With a few tweaks, you can quickly run multiple W&B agents in parallel
    within a single sweep. Thus, speeding up the hyperparameter tuning drastically.
    [Check out their docs if you want to learn more](https://docs.wandb.ai/guides/sweeps/parallelize-agents)
    [5].'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** é€šè¿‡ä¸€äº›è°ƒæ•´ï¼Œä½ å¯ä»¥åœ¨å•ä¸ªæ‰«é¢‘ä¸­å¿«é€Ÿå¹¶è¡Œè¿è¡Œå¤šä¸ªW&Bä»£ç†ã€‚å› æ­¤ï¼Œæ˜¾è‘—åŠ å¿«äº†è¶…å‚æ•°è°ƒä¼˜çš„é€Ÿåº¦ã€‚[å¦‚æœä½ æƒ³äº†è§£æ›´å¤šï¼Œè¯·æŸ¥çœ‹ä»–ä»¬çš„æ–‡æ¡£](https://docs.wandb.ai/guides/sweeps/parallelize-agents)
    [5]ã€‚'
- en: '**How to do cross-validation with time series data**'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•å¯¹æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œäº¤å‰éªŒè¯**'
- en: So, I highlighted that it is critical to do hyperparameter-tuning only using
    the training dataset.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæˆ‘å¼ºè°ƒäº†åªä½¿ç”¨è®­ç»ƒæ•°æ®é›†è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜æ˜¯è‡³å…³é‡è¦çš„ã€‚
- en: But then, on what split should you compute your metrics?
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œåº”è¯¥åœ¨ä»€ä¹ˆæ‹†åˆ†ä¸Šè®¡ç®—ä½ çš„æŒ‡æ ‡å‘¢ï¼Ÿ
- en: Well, you will be using cross-validation adapted to time series.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œä½ å°†ä½¿ç”¨é€‚åº”äºæ—¶é—´åºåˆ—çš„äº¤å‰éªŒè¯ã€‚
- en: As shown in the image below, we used a 3-fold cross-validation technique. The
    key idea is that because you are using time series data, you can't pick the whole
    dataset for every fold. It makes sense, as you can't learn from the future to
    predict the past.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨äº†3æŠ˜äº¤å‰éªŒè¯æŠ€æœ¯ã€‚å…³é”®ç‚¹æ˜¯ï¼Œç”±äºä½ ä½¿ç”¨çš„æ˜¯æ—¶é—´åºåˆ—æ•°æ®ï¼Œä½ ä¸èƒ½ä¸ºæ¯ä¸ªæŠ˜å é€‰æ‹©æ•´ä¸ªæ•°æ®é›†ã€‚è¿™æ˜¯åˆç†çš„ï¼Œå› ä¸ºä½ ä¸èƒ½ä»æœªæ¥å­¦ä¹ ä»¥é¢„æµ‹è¿‡å»ã€‚
- en: Thus, using the same principles as when we split the data between train and
    test, we sample 1/3 from the beginning of the dataset, where the **forecasting
    horizon (the orange segment)** is used to compute the validation metric. The next
    fold takes 2/3, and the last one 3/3 of the dataset.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½¿ç”¨ä¸æˆ‘ä»¬æ‹†åˆ†æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•æ—¶ç›¸åŒçš„åŸåˆ™ï¼Œæˆ‘ä»¬ä»æ•°æ®é›†å¼€å§‹çš„1/3ä¸­æŠ½æ ·ï¼Œå…¶ä¸­**é¢„æµ‹èŒƒå›´ï¼ˆæ©™è‰²éƒ¨åˆ†ï¼‰**ç”¨äºè®¡ç®—éªŒè¯æŒ‡æ ‡ã€‚ä¸‹ä¸€ä¸ªæŠ˜å ä½¿ç”¨2/3ï¼Œæœ€åä¸€ä¸ªæŠ˜å ä½¿ç”¨3/3çš„æ•°æ®é›†ã€‚
- en: '![](../Images/f6149c500f3a27660f20c03f1fc9d6c2.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6149c500f3a27660f20c03f1fc9d6c2.png)'
- en: Once again, **Sktime** makes our lives easier. Using the **ExpandingWindowSplitter**
    class and **cv_evaluate()** function, you can quickly train and evaluate the model
    using the specified cross-validation strategy â€” [official docs here](https://github.com/sktime/sktime/blob/main/examples/window_splitters.ipynb)
    [8]
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼Œ**Sktime**è®©æˆ‘ä»¬çš„ç”Ÿæ´»å˜å¾—æ›´ç®€å•ã€‚ä½¿ç”¨**ExpandingWindowSplitter**ç±»å’Œ**cv_evaluate()**å‡½æ•°ï¼Œä½ å¯ä»¥å¿«é€Ÿè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œä½¿ç”¨æŒ‡å®šçš„äº¤å‰éªŒè¯ç­–ç•¥â€”â€”[å®˜æ–¹æ–‡æ¡£åœ¨è¿™é‡Œ](https://github.com/sktime/sktime/blob/main/examples/window_splitters.ipynb)
    [8]ã€‚
- en: In the end, we restructured the **results** DataFrame, which the **cv_evaluate()**
    function returned to fit our interface.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬é‡æ„äº†**results**æ•°æ®æ¡†ï¼Œä½¿å…¶é€‚åº”æˆ‘ä»¬çš„æ¥å£ï¼Œè¿™ä¸ªæ•°æ®æ¡†æ˜¯**cv_evaluate()**å‡½æ•°è¿”å›çš„ã€‚
- en: Excellent, now you finished running your hyperparameter tuning step using W&B
    sweeps.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç°åœ¨ä½ å·²ç»å®Œæˆäº†ä½¿ç”¨W&Bæ‰«é¢‘çš„è¶…å‚æ•°è°ƒä¼˜æ­¥éª¤ã€‚
- en: At the end of this step, we have a **sweep_id** that has attached multiple experiments,
    where each experiment has a **config artifact.**
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€æ­¥ç»“æŸæ—¶ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªé™„åŠ äº†å¤šä¸ªå®éªŒçš„**sweep_id**ï¼Œæ¯ä¸ªå®éªŒéƒ½æœ‰ä¸€ä¸ª**config artifact**ã€‚
- en: Now we have to parse this information and create a **best_config artifact.**
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¿…é¡»è§£æè¿™äº›ä¿¡æ¯å¹¶åˆ›å»ºä¸€ä¸ª**best_config artifact**ã€‚
- en: Upload the Best Configuration from the Hyperparameter Tuning Search
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»è¶…å‚æ•°è°ƒä¼˜æœç´¢ä¸­ä¸Šä¼ æœ€ä½³é…ç½®
- en: Using the [**training_pipeline/best_config.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/training-pipeline/training_pipeline/best_config.py)script,
    we will parse all the experiments for the given **sweep_id** and find the best
    experiment with the lowest MAPE validation score.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[**training_pipeline/best_config.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/training-pipeline/training_pipeline/best_config.py)è„šæœ¬ï¼Œæˆ‘ä»¬å°†è§£æç»™å®š**sweep_id**çš„æ‰€æœ‰å®éªŒï¼Œå¹¶æ‰¾åˆ°å…·æœ‰æœ€ä½MAPEéªŒè¯åˆ†æ•°çš„æœ€ä½³å®éªŒã€‚
- en: Fortunately, this is done automatically by W&B when we call the **best_run()**
    function. After, you resume the **best_run** and rename the run to **best_experiment.**
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œå½“æˆ‘ä»¬è°ƒç”¨**best_run()**å‡½æ•°æ—¶ï¼ŒW&Bä¼šè‡ªåŠ¨å®Œæˆè¿™é¡¹å·¥ä½œã€‚ä¹‹åï¼Œä½ æ¢å¤**best_run**å¹¶å°†è¿è¡Œé‡å‘½åä¸º**best_experiment**ã€‚
- en: Also, you upload the config attached to the best configuration into its artifact
    called **best_config.**
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œä½ å°†é™„åŠ åˆ°æœ€ä½³é…ç½®çš„é…ç½®ä¸Šä¼ åˆ°å…¶ç§°ä¸º**best_config**çš„artifactä¸­ã€‚
- en: Later, we will use this artifact to train models from scratch as often as we
    want.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªartifactä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œæ— è®ºå¤šå°‘æ¬¡ã€‚
- en: Now you have the **best_config** artifact that tells you precisely what hyperparameters
    you should use to train your final model on.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ æœ‰äº†**best_config** artifactï¼Œå®ƒå‡†ç¡®åœ°å‘Šè¯‰ä½ åº”ä½¿ç”¨ä»€ä¹ˆè¶…å‚æ•°æ¥è®­ç»ƒä½ çš„æœ€ç»ˆæ¨¡å‹ã€‚
- en: Train the Final Model Using the Best Configuration
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æœ€ä½³é…ç½®è®­ç»ƒæœ€ç»ˆæ¨¡å‹
- en: Finally, training and loading the final model to the model registry is the last
    piece of the puzzle.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œå°†æœ€ç»ˆæ¨¡å‹è®­ç»ƒå¹¶åŠ è½½åˆ°æ¨¡å‹æ³¨å†Œè¡¨æ˜¯æœ€åä¸€æ­¥ã€‚
- en: 'Within the **from_best_config()** function from the [**training_pipeline/train.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/training-pipeline/training_pipeline/train.py)file,
    we perform the following steps:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[**training_pipeline/train.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/training-pipeline/training_pipeline/train.py)
    æ–‡ä»¶ä¸­çš„ **from_best_config()** å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
- en: Load the data from Hopsworks.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä» Hopsworks åŠ è½½æ•°æ®ã€‚
- en: Initialize a W&B run.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ– W&B è¿è¡Œã€‚
- en: Load the best_config artifact.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŠ è½½æœ€ä½³é…ç½®å·¥ä»¶ã€‚
- en: Build the baseline model.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ„å»ºåŸºçº¿æ¨¡å‹ã€‚
- en: Train and evaluate the baseline model on the test split.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æµ‹è¯•é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°åŸºçº¿æ¨¡å‹ã€‚
- en: Build the fancy model using the latest best configuration.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æœ€æ–°çš„æœ€ä½³é…ç½®æ„å»ºé«˜çº§æ¨¡å‹ã€‚
- en: Train and evaluate the fancy model on the test split.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æµ‹è¯•é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°é«˜çº§æ¨¡å‹ã€‚
- en: Render the results to see how they perform visually.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¸²æŸ“ç»“æœä»¥æŸ¥çœ‹å®ƒä»¬çš„è§†è§‰è¡¨ç°ã€‚
- en: Retrain the model on the whole dataset. This is critical for time series models
    as you must retrain them until the present moment to forecast the future.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šé‡æ–°è®­ç»ƒæ¨¡å‹ã€‚è¿™å¯¹æ—¶é—´åºåˆ—æ¨¡å‹è‡³å…³é‡è¦ï¼Œå› ä¸ºä½ å¿…é¡»å°†å®ƒä»¬é‡æ–°è®­ç»ƒåˆ°å½“å‰æ—¶åˆ»ï¼Œä»¥é¢„æµ‹æœªæ¥ã€‚
- en: Forecast future values.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¢„æµ‹æœªæ¥å€¼ã€‚
- en: Render the forecasted values.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¸²æŸ“é¢„æµ‹å€¼ã€‚
- en: Save the best model as an Artifact in W&B
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æœ€ä½³æ¨¡å‹ä¿å­˜ä¸º W&B ä¸­çš„å·¥ä»¶ã€‚
- en: Save the best model in the Hopsworks' model registry
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨ Hopsworks çš„æ¨¡å‹æ³¨å†Œè¡¨ä¸­ã€‚
- en: '**Note:** You can either use W&B Artifacts as a model registry or directly
    use the Hopsworks model registry feature. We will show you how to do it both ways.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** ä½ å¯ä»¥ä½¿ç”¨ W&B Artifacts ä½œä¸ºæ¨¡å‹æ³¨å†Œè¡¨ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ Hopsworks æ¨¡å‹æ³¨å†ŒåŠŸèƒ½ã€‚æˆ‘ä»¬å°†å±•ç¤ºè¿™ä¸¤ç§æ–¹æ³•ã€‚'
- en: '*Notice how we used* ***wandb.log()*** *to upload to W&B all the variables
    of interest.*'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨* ***wandb.log()*** *å°†æ‰€æœ‰æ„Ÿå…´è¶£çš„å˜é‡ä¸Šä¼ åˆ° W&Bã€‚*'
- en: Check out this video to visually see how we use W&B as an experiment tracker
    ğŸ‘‡
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹è¿™ä¸ªè§†é¢‘ï¼Œç›´è§‚åœ°äº†è§£æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ W&B ä½œä¸ºå®éªŒè·Ÿè¸ªå™¨ ğŸ‘‡
- en: '**Train & evaluate the model.**'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚**'
- en: To train any **Sktime** model, we implemented this general function that takes
    in any model, the data, and the forecast horizon.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®­ç»ƒä»»ä½•**Sktime**æ¨¡å‹ï¼Œæˆ‘ä»¬å®ç°äº†è¿™ä¸ªé€šç”¨å‡½æ•°ï¼Œæ¥å—ä»»ä½•æ¨¡å‹ã€æ•°æ®å’Œé¢„æµ‹èŒƒå›´ã€‚
- en: Using the method below, we evaluated the model on the test split using both
    aggregated metrics and slices over all the unique combinations of areas and consumer
    types.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸‹è¿°æ–¹æ³•ï¼Œæˆ‘ä»¬é€šè¿‡èšåˆæŒ‡æ ‡å’Œåœ¨æ‰€æœ‰ç‹¬ç‰¹çš„åŒºåŸŸå’Œæ¶ˆè´¹è€…ç±»å‹ç»„åˆä¸Šåˆ‡ç‰‡æ¥è¯„ä¼°æ¨¡å‹ã€‚
- en: By evaluating the model on slices, you can quickly investigate for fairness
    and bias.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨åˆ‡ç‰‡ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œä½ å¯ä»¥å¿«é€Ÿè°ƒæŸ¥å…¬å¹³æ€§å’Œåå·®ã€‚
- en: As you can see, most of the heavy lifting, such as the implementation of MAPE
    and RMSPE, is directly accessible from **Sktime**.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ æ‰€è§ï¼Œå¤§éƒ¨åˆ†ç¹é‡çš„å·¥ä½œï¼Œä¾‹å¦‚ MAPE å’Œ RMSPE çš„å®ç°ï¼Œéƒ½å¯ä»¥ç›´æ¥ä»**Sktime**ä¸­è®¿é—®ã€‚
- en: '**Render the results**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¸²æŸ“ç»“æœ**'
- en: Using **Sktime,** you can quickly render various time series into a single plot.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨**Sktime**ï¼Œä½ å¯ä»¥å¿«é€Ÿå°†å„ç§æ—¶é—´åºåˆ—æ¸²æŸ“åˆ°ä¸€ä¸ªå›¾è¡¨ä¸­ã€‚
- en: As shown in the video above, we rendered the results for every (area, consumer_type)
    combination in the W&B experiment tracker.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šè§†é¢‘æ‰€ç¤ºï¼Œæˆ‘ä»¬åœ¨ W&B å®éªŒè·Ÿè¸ªå™¨ä¸­æ¸²æŸ“äº†æ¯ä¸ªï¼ˆåŒºåŸŸï¼Œæ¶ˆè´¹è€…ç±»å‹ï¼‰ç»„åˆçš„ç»“æœã€‚
- en: '![](../Images/c490f268faaed419d02eeec4355a86f9.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c490f268faaed419d02eeec4355a86f9.png)'
- en: Visually comparing the prediction and real observations for area = 2 and consumer
    type = 119 [Image by the Author].
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚æ¯”è¾ƒåŒºåŸŸ = 2 å’Œæ¶ˆè´¹è€…ç±»å‹ = 119 çš„é¢„æµ‹ä¸å®é™…è§‚å¯Ÿ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: '![](../Images/d681a7c647abe694bb25917c6cb84b61.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d681a7c647abe694bb25917c6cb84b61.png)'
- en: Visually observing forecasted values into the future for area = 2 and consumer
    type = 119 [Image by the Author].
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚è§‚å¯Ÿæœªæ¥çš„é¢„æµ‹å€¼ï¼Œå¯¹äºåŒºåŸŸ = 2 å’Œæ¶ˆè´¹è€…ç±»å‹ = 119 [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: '**Upload the model to the model registry**'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**å°†æ¨¡å‹ä¸Šä¼ åˆ°æ¨¡å‹æ³¨å†Œè¡¨**'
- en: The last step is to upload the model to a model registry. After the model is
    uploaded, it will be downloaded and used by our batch prediction pipeline.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€æ­¥æ˜¯å°†æ¨¡å‹ä¸Šä¼ åˆ°æ¨¡å‹æ³¨å†Œè¡¨ã€‚ä¸Šä¼ åï¼Œæ¨¡å‹å°†è¢«ä¸‹è½½å¹¶ç”¨äºæˆ‘ä»¬çš„æ‰¹é‡é¢„æµ‹ç®¡é“ã€‚
- en: During the experiment, we already uploaded the model as a W&B Artifact. If you
    plan to have dependencies with W&B in your applications, using it directly from
    there is perfectly fine.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®éªŒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å·²ç»å°†æ¨¡å‹ä¸Šä¼ ä¸º W&B å·¥ä»¶ã€‚å¦‚æœä½ è®¡åˆ’åœ¨åº”ç”¨ç¨‹åºä¸­ä¾èµ– W&Bï¼Œç›´æ¥ä½¿ç”¨å®ƒæ˜¯å®Œå…¨å¯ä»¥çš„ã€‚
- en: But we wanted to keep the batch prediction pipeline dependent only on Hopsworks.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬å¸Œæœ›ä¿æŒæ‰¹é‡é¢„æµ‹ç®¡é“ä»…ä¾èµ– Hopsworksã€‚
- en: Thus, we used Hopswork's model registry feature.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† Hopswork çš„æ¨¡å‹æ³¨å†ŒåŠŸèƒ½ã€‚
- en: In the following code, based on the given **best_model_artifact,** we added
    a tag to the Hopsworks feature view to link the two. This is helpful for debugging.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹ä»£ç ä¸­ï¼ŒåŸºäºç»™å®šçš„**best_model_artifact**ï¼Œæˆ‘ä»¬åœ¨ Hopsworks ç‰¹å¾è§†å›¾ä¸­æ·»åŠ äº†ä¸€ä¸ªæ ‡ç­¾ï¼Œä»¥å°†ä¸¤è€…é“¾æ¥èµ·æ¥ã€‚è¿™æœ‰åŠ©äºè°ƒè¯•ã€‚
- en: Finally, we downloaded the best model weights and loaded them to the Hopsworks
    model registry using the **mr.python.create_model()** method.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬ä¸‹è½½äº†æœ€ä½³æ¨¡å‹æƒé‡ï¼Œå¹¶ä½¿ç”¨ **mr.python.create_model()** æ–¹æ³•å°†å…¶åŠ è½½åˆ° Hopsworks æ¨¡å‹æ³¨å†Œè¡¨ä¸­ã€‚
- en: Now with a few lines of code, you can download and run inference on your model
    without carrying any more about all the complicated steps we showed you in this
    lesson.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œé€šè¿‡å‡ è¡Œä»£ç ï¼Œä½ å¯ä»¥ä¸‹è½½å¹¶å¯¹ä½ çš„æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œè€Œæ— éœ€å†æ‹…å¿ƒæˆ‘ä»¬åœ¨æœ¬è¯¾ä¸­å±•ç¤ºçš„æ‰€æœ‰å¤æ‚æ­¥éª¤ã€‚
- en: '[Check out Lesson 3](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)
    to see how we will build a batch prediction pipeline using the model from the
    Hopsworks model registry.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹ç¬¬ 3 è¯¾](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)
    ä»¥äº†è§£æˆ‘ä»¬å°†å¦‚ä½•ä½¿ç”¨æ¥è‡ª Hopsworks æ¨¡å‹æ³¨å†Œè¡¨çš„æ¨¡å‹æ„å»ºæ‰¹é‡é¢„æµ‹æµç¨‹ã€‚'
- en: Conclusion
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Congratulations! You finished the **second lesson** from the **Full Stack 7-Steps
    MLOps Framework** course.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ ï¼ä½ å®Œæˆäº† **ç¬¬äºŒè¯¾** æ¥è‡ª **å…¨æ ˆ 7 æ­¥ MLOps æ¡†æ¶** è¯¾ç¨‹ã€‚
- en: 'If you have reached this far, you know how to:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å·²ç»è¯»åˆ°è¿™é‡Œï¼Œä½ åº”è¯¥çŸ¥é“å¦‚ä½•ï¼š
- en: use an ML platform for experiment & metadata tracking
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ ML å¹³å°è¿›è¡Œå®éªŒå’Œå…ƒæ•°æ®è·Ÿè¸ª
- en: use an ML platform for hyperparameter tuning
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ ML å¹³å°è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
- en: read data from the feature store based on a given version
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ¹æ®ç»™å®šç‰ˆæœ¬ä»ç‰¹å¾å­˜å‚¨ä¸­è¯»å–æ•°æ®
- en: build an encapsulated ML model and pipeline
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªå°è£…çš„ ML æ¨¡å‹å’Œæµç¨‹
- en: upload your model to a model registry
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†ä½ çš„æ¨¡å‹ä¸Šä¼ åˆ°æ¨¡å‹æ³¨å†Œè¡¨
- en: Now that you understand the power of using an ML platform, you can finally take
    control over your experiments and quickly export your model as an artifact to
    be easily used in your inference pipelines.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ äº†è§£äº†ä½¿ç”¨ ML å¹³å°çš„å¼ºå¤§åŠŸèƒ½ï¼Œä½ å¯ä»¥æœ€ç»ˆæŒæ§ä½ çš„å®éªŒï¼Œå¹¶å¿«é€Ÿå°†ä½ çš„æ¨¡å‹å¯¼å‡ºä¸ºå·¥ä»¶ï¼Œä»¥ä¾¿åœ¨æ¨æ–­æµç¨‹ä¸­è½»æ¾ä½¿ç”¨ã€‚
- en: '[Check out Lesson 3](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)
    to learn about implementing a batch prediction pipeline and packaging your Python
    modules using Poetry.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹ç¬¬ 3 è¯¾](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)
    ä»¥äº†è§£å¦‚ä½•å®ç°æ‰¹é‡é¢„æµ‹æµç¨‹å’Œä½¿ç”¨ Poetry æ‰“åŒ…ä½ çš„ Python æ¨¡å—ã€‚'
- en: '**Also,** [**you can access the GitHub repository here**](https://github.com/iusztinpaul/energy-forecasting)**.**'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦å¤–ï¼Œ** [**ä½ å¯ä»¥åœ¨è¿™é‡Œè®¿é—® GitHub ä»“åº“**](https://github.com/iusztinpaul/energy-forecasting)**ã€‚**'
- en: ğŸ’¡ My goal is to help machine learning engineers level up in designing and productionizing
    ML systems. Follow me on [LinkedIn](https://www.linkedin.com/in/pauliusztin/)
    or subscribe to my [weekly newsletter](https://pauliusztin.substack.com/) for
    more insights!
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ æˆ‘çš„ç›®æ ‡æ˜¯å¸®åŠ©æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆåœ¨è®¾è®¡å’Œç”Ÿäº§ ML ç³»ç»Ÿæ–¹é¢æå‡æ°´å¹³ã€‚å…³æ³¨æˆ‘åœ¨ [LinkedIn](https://www.linkedin.com/in/pauliusztin/)
    æˆ–è®¢é˜…æˆ‘çš„ [æ¯å‘¨é€šè®¯](https://pauliusztin.substack.com/) è·å–æ›´å¤šè§è§£ï¼
- en: ğŸ”¥ If you enjoy reading articles like this and wish to support my writing, consider
    [becoming a Medium member](https://pauliusztin.medium.com/membership). By using
    [my referral link](https://pauliusztin.medium.com/membership), you can support
    me without any extra cost while enjoying limitless access to Mediumâ€™s rich collection
    of stories.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ å¦‚æœä½ å–œæ¬¢é˜…è¯»è¿™æ ·çš„æ–‡ç« å¹¶å¸Œæœ›æ”¯æŒæˆ‘çš„å†™ä½œï¼Œå¯ä»¥è€ƒè™‘ [æˆä¸º Medium ä¼šå‘˜](https://pauliusztin.medium.com/membership)ã€‚é€šè¿‡ä½¿ç”¨
    [æˆ‘çš„æ¨èé“¾æ¥](https://pauliusztin.medium.com/membership)ï¼Œä½ å¯ä»¥åœ¨äº«å— Medium ä¸°å¯Œæ•…äº‹å†…å®¹çš„åŒæ—¶ï¼Œæ”¯æŒæˆ‘è€Œæ— éœ€é¢å¤–è´¹ç”¨ã€‚
- en: '[](https://pauliusztin.medium.com/membership?source=post_page-----6fdaef594cee--------------------------------)
    [## Join Medium with my referral link - Paul Iusztin'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pauliusztin.medium.com/membership?source=post_page-----6fdaef594cee--------------------------------)
    [## ä½¿ç”¨æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium - Paul Iusztin'
- en: ğŸ¤– Join to get exclusive content about designing and building production-ready
    ML systems ğŸš€ Unlock full access toâ€¦
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ğŸ¤– åŠ å…¥ä»¥è·å–å…³äºè®¾è®¡å’Œæ„å»ºç”Ÿäº§å°±ç»ª ML ç³»ç»Ÿçš„ç‹¬å®¶å†…å®¹ ğŸš€ è§£é”å®Œæ•´è®¿é—®æƒâ€¦
- en: pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----6fdaef594cee--------------------------------)
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----6fdaef594cee--------------------------------)
- en: References
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] [Energy Consumption per DE35 Industry Code from Denmark API](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour),
    [Denmark Energy Data Service](https://www.energidataservice.dk/about/)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [æ¥è‡ªä¸¹éº¦ API çš„æ¯å°æ—¶èƒ½æºæ¶ˆè€—æ•°æ®](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour),
    [ä¸¹éº¦èƒ½æºæ•°æ®æœåŠ¡](https://www.energidataservice.dk/about/)'
- en: '[2] [WindowSummarizer Documentation](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.transformations.series.summarize.WindowSummarizer.html?highlight=windowsummarizer),
    Sktime Documentation'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [WindowSummarizer æ–‡æ¡£](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.transformations.series.summarize.WindowSummarizer.html?highlight=windowsummarizer)ï¼ŒSktime
    æ–‡æ¡£'
- en: '[3] [Sktime Documentation](https://www.sktime.net/en/latest/index.html)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [Sktime æ–‡æ¡£](https://www.sktime.net/en/latest/index.html)'
- en: '[4] [LightGBM Documentation](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [LightGBM æ–‡æ¡£](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html)'
- en: '[5] [W&B Sweeps Documentation](https://docs.wandb.ai/guides/sweeps), W&B Documentation'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [W&B Sweeps æ–‡æ¡£](https://docs.wandb.ai/guides/sweeps)ï¼ŒW&B æ–‡æ¡£'
- en: '[6] [Sktime Forecasting Tutorial](https://www.sktime.net/en/latest/examples/01_forecasting.html#1.-Basic-forecasting-workflows),
    Sktime Documentation'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [Sktime é¢„æµ‹æ•™ç¨‹](https://www.sktime.net/en/latest/examples/01_forecasting.html#1.-Basic-forecasting-workflows)ï¼ŒSktime
    æ–‡æ¡£'
- en: '[7] [Sktime Hierarchical, Global, and Panel Forecasting Tutorial](https://github.com/sktime/sktime/blob/main/examples/01c_forecasting_hierarchical_global.ipynb),
    Sktime Documentation'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [Sktime åˆ†å±‚ã€å…¨çƒå’Œé¢æ¿é¢„æµ‹æ•™ç¨‹](https://github.com/sktime/sktime/blob/main/examples/01c_forecasting_hierarchical_global.ipynb)ï¼ŒSktime
    æ–‡æ¡£'
- en: '[8] [Sktime Window Splitters Tutorial](https://github.com/sktime/sktime/blob/main/examples/window_splitters.ipynb),
    Sktime Documentation'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [Sktime çª—å£åˆ†å‰²å™¨æ•™ç¨‹](https://github.com/sktime/sktime/blob/main/examples/window_splitters.ipynb)ï¼ŒSktime
    æ–‡æ¡£'
