- en: Map, Filter, and CombinePerKey Transforms in Writing Apache Beam Pipelines with
    Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/map-filter-and-combineperkey-transforms-in-writing-apache-beam-pipelines-with-examples-e06926124a02](https://towardsdatascience.com/map-filter-and-combineperkey-transforms-in-writing-apache-beam-pipelines-with-examples-e06926124a02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/3c0bc71612c5632ec1b351c55f68aa03.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [JJ Ying](https://unsplash.com/@jjying?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Practice with Some Real Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rashida00.medium.com/?source=post_page-----e06926124a02--------------------------------)[![Rashida
    Nasrin Sucky](../Images/42bd057e8eca255907c43c29a498f2ca.png)](https://rashida00.medium.com/?source=post_page-----e06926124a02--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e06926124a02--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e06926124a02--------------------------------)
    [Rashida Nasrin Sucky](https://rashida00.medium.com/?source=post_page-----e06926124a02--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e06926124a02--------------------------------)
    ·8 min read·Jul 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Beam is getting popularity as the unified programming model for efficient
    and portable big data processing pipelines. It can deal with both batch and streaming
    data. That’s how the name comes from. Beam is combination of the words Batch and
    Stream:'
  prefs: []
  type: TYPE_NORMAL
- en: B(from **B**atch) + eam(from str**eam**)= Beam
  prefs: []
  type: TYPE_NORMAL
- en: The portability also is a great feature. You just need to focus on running the
    pipeline and it can be run from anywhere such as Spark, Flink, Apex, or Cloud
    Dataflow. You don’t need to change the logic or syntax for that.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will focus on learning to write some ETL Pipelines using
    examples. We will try some transform operations using a good dataset and hopefully
    you will find all this transform operations useful in your work as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please feel free to download this [public dataset](https://creativecommons.org/publicdomain/zero/1.0/)
    and follow along:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Sample Sales Data | Kaggle](https://www.kaggle.com/datasets/kyanyoga/sample-sales-data)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Google Colab notebook is used for this exercise. So, installation is very
    easy. Just use this line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After installation is done, I made a directory for this exercise named ‘data’:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let’s dive into today’s topic that is the transform operations. To start with
    we will work on a simplest pipeline that is just read the CSV file and Write it
    to a text file.
  prefs: []
  type: TYPE_NORMAL
- en: This is not as simple as Padas read_csv() method. It requires a coder() opeartion.
    First a CustomCoder() class was defined here that first encode the objects into
    byte strings, then decode the bytes to its corresponding objects and finally specifies
    if this coder is guaranteed to encode values deterministically. Please check [the
    documentation here.](https://beam.apache.org/releases/pydoc/2.2.0/apache_beam.coders.coders.html)
  prefs: []
  type: TYPE_NORMAL
- en: If this is your first Pipeline, please notice the syntax for a pipeline. After
    the CustomCoder() class there is the simplest pipeline. We initiated the empty
    pipeline as ‘p1’ first. Then we wrote the ‘sales’ Pipeline where first read the
    CSV file from the data folder that we created earlier. In Apache beam each transform
    operation in the pipeline starts with this | sign. After reading the data from
    the CSV file we just write it to a text file. At the end, with a run () method
    we ran the pipeline. This is the standard and usual pipeline syntax in Apache
    beam.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you check your ‘data’ folder now you will see a ‘output-00000-of-00001’
    file there. Printing the first 5 rows from this file to check the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Map
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s see how to use a Map transform in the above pipeline. This is the most
    common transform operation. The transformation you specify in the Map will apply
    to every single element in the PCollection.
  prefs: []
  type: TYPE_NORMAL
- en: For example, I would like to add a split method to create lists out of every
    element in the PCollection. Here we will use lambda for Map transform. If you
    are not familiar with lambda, observe this code with lambda here. After lanbda
    we mentioned ‘row’, any other name of the variable is fine too. Whatever function
    or method we would apply to the ‘row’ that will be applied to every element in
    the PCollection.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Look, it is the exact same syntax. Just I put one extra line of code in between
    read and write operation. Again printing the first 5 rows of the output to check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Look, each element has become a list.
  prefs: []
  type: TYPE_NORMAL
- en: Filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, I will add Filter transform to the above code block as well. Lambda can
    be used here again for filter as well. We will filter out all the data and keep
    only the data for ‘Classic Cars’ from Produc line. The 11th column of the dataset
    is the product line. As you know Python is zero indexed. So, counting of column
    number also starts from zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, printing the first 5 rows for checking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Look at the 11th element of each list in the output above. It’s ‘Classic Cars’.
  prefs: []
  type: TYPE_NORMAL
- en: Answering Some Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**How much quantity was ordered for each type of automobile?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To find this out, we will first create tuples where the first element or the
    key will come from the 11th element of the dataset and the second element that
    means the value will be the second element of the dataset that is ‘QUANTITY ORDERED’.
    In the next step, we will use CombinePerKey () method. As the name suggest, it
    will combine the values with an aggregate function for each key.
  prefs: []
  type: TYPE_NORMAL
- en: It will be clearer when you will see the code. Here is the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we used Map function twice here. First to split and make lists
    as before and then from each row of data we took the product line that is 10th
    column, and the Quantity that is second column only.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Just printed out the first 10 rows of the output. As you can see, we have the
    quantity ordered for each row of the data here. The next and the final step to
    answer the question above is to combine all the values for each item. There is
    CombinePerKey method available in apache beam for that. As the name suggest, it
    will combine the values with an aggregate function for each key. In this case
    we need the ‘sum’.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: So, we have the total quantity ordered for each product.
  prefs: []
  type: TYPE_NORMAL
- en: '**From which states more than 2000 orders were placed?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is an interesting question where we need every transform, we have done
    earlier plus another filter transform. We need to calculate the total number of
    orders for each state the way we calculated the total number of orders for each
    product in the previous example. And then the quantity that are more than 2000
    should be filtered in.
  prefs: []
  type: TYPE_NORMAL
- en: In all the previous examples, lambda function were used in Map and Filter transforms.
    Here we will see how we can define a function and use that in the Map or Filter
    function. A function quantity_filter() is defined here that returns the items
    with value count more than 2000.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This is the output where if the quantity is not bigger than 2000, it returned
    ‘None’. I don’t like to see all this ‘None’ values. I will add another filter
    transform to filter out these ‘None’ values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: So, we have total 5 values returned where the order count is bigger than 2000.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial I wanted to show how to use Map, Filter, CombinePerKey transforms
    in Apache beam in writing ETL pipelines. Hopefully they are clear enough to use
    in your own projects. I will explain how to use ParDo in my next article.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to follow me on [Twitter](https://twitter.com/rashida048) and like
    my [Facebook](https://www.facebook.com/rashida.smith.161) page.
  prefs: []
  type: TYPE_NORMAL
- en: More Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[A Detailed Tutorial on Polynomial Regression in Python, Overview, Implementation,
    and Overfitting | by Rashida Nasrin Sucky | Jun, 2023 | Towards AI](https://pub.towardsai.net/a-detailed-tutorial-on-polynomial-regression-in-python-overview-implementation-and-overfitting-e319fc7e5b8f)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Complete Implementation of a Mini VGG Network for Image Recognition | by Rashida
    Nasrin Sucky | Towards Data Science](/complete-implementation-of-a-mini-vgg-network-for-image-recognition-849299480356)'
  prefs: []
  type: TYPE_NORMAL
- en: '[How to Define Custom Layer, Activation Function, and Loss Function in TensorFlow
    | by Rashida Nasrin Sucky | Towards Data Science](/how-to-define-custom-layer-activation-function-and-loss-function-in-tensorflow-bdd7e78eb67)'
  prefs: []
  type: TYPE_NORMAL
- en: '[A Step-by-Step Tutorial to Develop a Multi-Output Model in TensorFlow | by
    Rashida Nasrin Sucky | Towards Data Science](/a-step-by-step-tutorial-to-develop-a-multi-output-model-in-tensorflow-ec9f13e5979c)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Easy Method of Edge Detection in OpenCV Python | by Rashida Nasrin Sucky |
    Towards Data Science](/easy-method-of-edge-detection-in-opencv-python-db26972deb2d)'
  prefs: []
  type: TYPE_NORMAL
