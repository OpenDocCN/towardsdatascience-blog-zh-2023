- en: '🦜🔗 LangChain: Develop applications powered by Language Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/develop-applications-powered-by-language-models-with-langchain-d2f7a1d1ad1a](https://towardsdatascience.com/develop-applications-powered-by-language-models-with-langchain-d2f7a1d1ad1a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/eadb36548cf05e78b4161169a551f8aa.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Photo by [Choong Deng Xiang](https://unsplash.com/@dengxiangs?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Get started using LangChain with Python to leverage LLMs
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----d2f7a1d1ad1a--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----d2f7a1d1ad1a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2f7a1d1ad1a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2f7a1d1ad1a--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----d2f7a1d1ad1a--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2f7a1d1ad1a--------------------------------)
    ·12 min read·Apr 26, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LangChain is a framework that enables quick and easy development of applications
    that make use of Large Language Models**, for example, GPT-3.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The framework, however, introduces additional possibilities, for example, the
    one of easily using external data sources, such as Wikipedia, to amplify the capabilities
    provided by the model. I am sure that you have all probably tried to use Chat-GPT
    and find that it fails to answer about events that occurred beyond a certain date.
    In this case, a search on Wikipedia could help GPT to answer more questions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: LangChain Structure
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The framework is organized into six modules each module allows you to manage
    a different aspect of the interaction with the LLM. Let’s see what the modules
    are.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[**Models**](https://python.langchain.com/en/latest/modules/models.html): Allows
    you to instantiate and use different models.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Prompts**](https://python.langchain.com/en/latest/modules/prompts.html):
    The prompt is how we interact with the model to try to obtain an output from it.
    By now knowing how to write an effective prompt is of critical importance. This
    framework module allows us to better manage prompts. For example, by creating
    templates that we can reuse.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Indexes**](https://python.langchain.com/en/latest/modules/indexes.html):
    The best models are often those that are combined with some of your textual data,
    in order to add context or explain something to the model. This module helps us
    do just that.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Chains**](https://python.langchain.com/en/latest/modules/chains.html): Many
    times to solve tasks a single API call to an LLM is not enough. This module allows
    other tools to be integrated. For example, one call can be a composed chain with
    the purpose of getting information from Wikipedia and then giving this information
    as input to the model. This module allows multiple tools to be concatenated in
    order to solve complex tasks.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Memory**](https://python.langchain.com/en/latest/modules/memory.html): This
    module allows us to create a persisting state between calls of a model. Being
    able to use a model that remembers what has been said in the past will surely
    improve our application.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Agents**](https://python.langchain.com/en/latest/modules/agents.html): An
    agent is an LLM that makes a decision, takes an action, makes an observation about
    what it has done, and continues in this manner until it can complete its task.
    This module provides a set of agents that can be used.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s go into a little more detail and see how to implement code by taking
    advantage of the different modules.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Models
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Models* allows the use of three different types of language-models, which
    are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '**Large Language Models (LLMs):** these foundational machine learning models
    that are able to understand natural language. These accept strings in input and
    generate strings in output.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chat Models:** models powered by LLM but are specialized to chat with the
    user. You can read more [here](https://medium.com/aiguys/reinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text Embedding Models:** these models are used to project textual data into
    a geometric space. These models take text as input and return a list of numbers,
    the embedding of the text.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/81937b2be62eb65b9c3931398deda8af.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Open AI API Key
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start using this module. First, we install and import the libraries. To
    use this library you will need an API KEY from the open AI site.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now we are ready to instantiate a LLM model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***My Output****:* *A young woman went to a rave she had never heard of. She
    was the only person there with a light in the dark, and the only one who could
    see the good in people. She drank and danced and did anything to make it so. After
    hour after hour of dancing and drinkin'', she got to know one of the people in
    the party. He was a bit of a looking man, with a moustache and a goatee. She said,
    "Hi, I''m the only person there with a light in the dark." He said, "Hi, I''m
    the only person there with a light in the dark.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: With the generate() method you can also feed a list of input and receive multiple
    answers, let’s see how.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/2a2fbc6563852fd6091ef5140276f8b6.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: llm.generate output
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: You can also extract some additional information about the results of the large
    language model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***My Output:*** *{‘token_usage’: {‘completion_tokens’: 527, ‘total_tokens’:
    544, ‘prompt_tokens’: 17}, ‘model_name’: ‘text-ada-001’}*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are not able to understand input texts that are too long. In particular,
    text that contains too many tokens (think about the syllables of words if you
    don’t know what tokens are). Before passing the string into the model you can
    estimate the number of tokens with a simple method.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: In order to do this you need to install the [tiktoken](https://github.com/openai/tiktoken)
    library.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Prompts
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompting is the new way of programming NLP models. Creating a good prompt though
    is not trivial. Asking the same thing in a different way can lead to a different
    result that is more or less accurate. The prompt can also change depending on
    the use case you are facing. Let’s see how this module can help us create a good
    prompt.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Template
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name implies, the prompt template allows us to create templates that
    we can reuse to ask things of our model. The template will contain variables that
    will be the only thing that the user will change from time to time to adapt the
    prompt to its particular use case.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Let us now see how they can be used.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we can replace the variable ‘product’ within the prompt with whatever string
    we want. This way we can customize the prompt any way we like.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '***My Outupt:*** *I want you to act as businessman. You have few passions in
    your life which are:- Money- Data- Basketball. I want to write a Medium Blog post
    about how to make a bucnh of money. What is a good for a title of such post?*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: There are templates that have already been written and you can simply import
    them. To find out what they are you can look at the documentation. Let’s try to
    import one now.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '***My Outupt:*** *PromptTemplate(input_variables=[‘history’, ‘input’], output_parser=None,
    partial_variables={}, template=’The following is a friendly conversation between
    a human and an AI. The AI is talkative and provides lots of specific details from
    its context. If the AI does not know the answer to a question, it truthfully says
    it does not know.\n\nCurrent conversation:\n{history}\nHuman: {input}\nAI:’, template_format=’f-string’,
    validate_template=True)*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: In this template, we have multiple variables that we can fill in. One of them
    is *history*. We need the history to tell the template about something that happened
    previously so that it has more context. If we want to request the output of the
    model from this template, it is very simple.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Few shot examples
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes it happens that we want to question the Machine Learning model about
    things that are particularly tricky. One way to get a more accurate answer, in
    this case, is to show the model similar examples of the correct answer and then
    ask it our question. LangChain provides a method for saving templates that are
    designed specifically to save these examples. Let’s see how to do this.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, let’s create examples. In case I want to create the superlatives
    of the words: tall -> tallest.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now we create the template as done before by including two variables, one for
    the base word and one for the superlative.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now we combine everything with the FewShotPromptTemplate class that takes as
    input the examples, the template, the prefix which is usually some instruction
    we want to give to the template, and a suffix which is the form of the template
    output.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '***My Output:*** *Give the superlative of every input Word: cool Superlative:
    coolest Word: tall Superlative: tallest Word: big Superlative:*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Now you can feed the model and receive the actual output.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Indexes
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This module is the one that allows us to interact with external documents that
    we want to feed to the model. This module is based on the concept of Retriever.
    In fact, the most common thing we want to do with this module is to go and fetch
    the document that most answers our query. Thus an information retrieval system.
    Let’s see what the Retriever interface looks like to understand it better. ( For
    those who don’t already know, an interface is a class that cannot be instantiated,
    if you want to learn more you can read my articles on [design patterns](https://medium.com/towards-data-science/design-patterns-with-python-for-machine-learning-engineers-observer-23cde7ecb2ed)).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The *get_relevant_documents* method is very simple, you just need to know how
    to read English to understand what it does. The string can be changed to your
    liking, so if you want to modify or implement a custom Retriever it is nothing
    too complicated.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a practical example now. We want to create a question-answering
    application about a specific document. That is, the model needs to be able to
    answer questions of mine about a specific document.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: We first need to install *Chroma* which allows us to work with *Vectorstores*,
    we’ll see later what it is for.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Let’s import some classes we will need.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now let’s download a txt document from the web.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let’s load the document with the TextLoader.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The *Retriever* always relies on what is called *Vectorstore* retriever. So
    we can instantiate one *Vectorstore retriever* and pass to it our text loader.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now that we finally have an index, we can ask questions about the data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '***My Output:*** *Ohio Senator Sherrod Brown said, “It’s time to bury the label
    ‘Rust Belt.’”*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Chains
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chains allow us to create more complicated applications. **Often simply using
    an LLM is not enough**, we want to do more. For example, we want to first create
    a template and then give the compiled template as input to the LLM, this can be
    done simply with a chain. It’s easy for me to think of chains as the Pipeline
    that you use in scikit-learn, for example.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The LLMChain is a chain that does just that, takes an input, formats it within
    a template, and then passes it as input to a template.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s create a simple template.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now we create a chain by specifying the template and model to be used.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You can also create a custom chain but I’ll write a more detailed article about
    it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every time we interact with a model, it will give us an answer that won’t depend
    on the context because it doesn’t remember past events. It’s kind of like every
    time we are talking to a new model. **In applications we usually want the model
    to have some kind of memory so that it learns to reply to us by improving as it
    goes along depending on what we said before** especially if we are developing
    chatbots. That’s what this module is for.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Memory can be implemented in a variety of ways. For example, we can take the
    previous N messages, and feed them to the model as a single string or as a string
    sequence for instance. Now let’s look at the simplest type of memory we can implement
    called a buffer.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: We can use a ChatMessageHistory class that allows us to easily save all the
    messages sent to the model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now we can retrieve the messages easily.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now that we understand the concept, we can use a wrapper of the class we just
    used called ConversationBufferMemory that allows us to actually use the message
    history.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Let’s see finally how to use this feature in a chain for a chat.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: So let’s start by having a conversation with the model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](../Images/7836436ee44205cce342194f148231e1.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: '[PRE35]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](../Images/a108320e04ad98ef250e62891677400d.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: '[PRE36]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](../Images/52ae722345c2a49f47f024a439091eb0.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: You can retrieve the old messages by accessing the memory.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now you can save your messages to load them again and feed the model when you
    want to start a conversation from a certain point.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Agents
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The kind of chains we have seen, follow predetermined steps like a pipeline.
    But we often don’t know what steps the model has to take to answer a given question,
    because it also depends on what the user answers it from time to time.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: We can have a model use various tools, to improve its answers. A common example
    is to first go and read some information on Wikipedia and then answer a particular
    question.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We are going to use two tools in particular SERPAPI which allows the model to
    do browser searches and take information, llm-math to improve its math skills.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: To install SERPAPI you must register on the site and copy the API Token.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the website: [https://serpapi.com/](https://serpapi.com/)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Once done we install the library and set the token as an environment variable.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now we are ready to instantiate our agent with the tools it will need to use.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'We, therefore, need 3 things:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM: the large language model we want to use'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tools: we want to use to improve the basic LLM'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Agent: handles the interaction between the LLM and Tools'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now we can ask our model a question, relying on the fact that it is going to
    use the various tools available to it to answer us.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Final Thoughts
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we introduced LangChain with its various modules. Each module
    is useful for improving the capabilities of large language models and is essential
    for developing applications based on these models. Be careful because the model
    I used in this article is not among the best so the answers may not be optimal,
    plus the answers you get may be very different from mine. The purpose though was
    just to get familiar with this library. I am curious to see all the applications
    that will be developed in the future based on the new Large Language Models. Follow
    me to read my upcoming in-depth articles on these topics! [😉](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了LangChain及其各种模块。每个模块都对提高大型语言模型的能力有用，并且对于基于这些模型开发应用程序至关重要。请注意，因为我在本文中使用的模型不是最好的，所以回答可能不是最优的，而且你得到的回答可能与我的差异很大。不过，目的是为了熟悉这个库。我很期待看到未来基于新大型语言模型开发的所有应用程序。关注我，阅读我即将发布的关于这些话题的深入文章！
    [😉](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
- en: The End
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束
- en: '*Marcello Politi*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*Marcello Politi*'
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [Website](https://marcello-politi.super.site/)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[Linkedin](https://www.linkedin.com/in/marcello-politi/)，[Twitter](https://twitter.com/_March08_)，
    [Website](https://marcello-politi.super.site/)'
