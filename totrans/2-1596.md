# ONNX：用于可互操作深度学习模型的标准

> 原文：[https://towardsdatascience.com/onnx-the-standard-for-interoperable-deep-learning-models-a47dfbdf9a09](https://towardsdatascience.com/onnx-the-standard-for-interoperable-deep-learning-models-a47dfbdf9a09)

![](../Images/889af96423dbf877991bb0f406706a90.png)

图片由[Jonny Caspari](https://unsplash.com/@jonnyuiux?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上提供

## 了解使用ONNX标准在框架和硬件平台之间部署模型的好处

[](https://medium.com/@marcellopoliti?source=post_page-----a47dfbdf9a09--------------------------------)[![Marcello Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----a47dfbdf9a09--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a47dfbdf9a09--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a47dfbdf9a09--------------------------------) [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----a47dfbdf9a09--------------------------------)

·发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----a47dfbdf9a09--------------------------------) ·5分钟阅读·2023年1月24日

--

我第一次听说ONNX是在INRIA实习期间。我当时在用Julia语言开发神经网络剪枝算法。那时还没有很多预训练模型可以使用，因此利用ONNX导入其他语言和框架开发的模型可能是一个解决方案。

在本文中，我想介绍ONNX，并通过一个实际示例来解释其巨大的潜力。

## ONNX是什么？

ONNX，即开放神经网络交换，是一个用于表示深度学习模型的开源标准。它由Facebook和Microsoft开发，旨在使研究人员和工程师能够更轻松地在不同的深度学习框架和硬件平台之间迁移模型。

ONNX的一个主要优势是它允许模型轻松地**从一个框架（如PyTorch）导出，并导入到另一个框架（如TensorFlow）**。这对于那些想尝试不同框架来训练和部署模型的研究人员，或者需要在不同硬件平台上部署模型的工程师尤其有用。

![](../Images/c9e402a5fd921c63034240ae17e04d24.png)

框架互操作性（图片由作者提供）

ONNX还提供了一套工具，用于优化和量化模型，这有助于减少模型的内存和计算需求。这对于在边缘设备和其他资源受限环境中部署模型尤其有用。

另一个ONNX的重要特点是它得到了广泛的公司和组织的支持。这不仅包括Facebook和Microsoft，还包括像Amazon、NVIDIA和Intel这样的公司。这种广泛的支持确保了ONNX将继续得到积极开发和维护，使其成为一个稳健和稳定的深度学习模型表示标准。

## ONNX Runtime

**ONNX Runtime是一个开源推断引擎，用于执行ONNX**（开放神经网络交换）**模型**。它被设计为**高性能**且轻量级，使其非常适合**在各种硬件平台上部署**，包括边缘设备、服务器和云服务。

ONNX Runtime提供了C++ API、C# API和Python API来执行ONNX模型。它还支持多种后端，包括CUDA和OpenCL，这使得它可以在各种硬件平台上运行，如NVIDIA GPUs和Intel CPUs。

ONNX Runtime非常有用，因为你可以在任何硬件上使用模型进行推断，无论你使用的是CPU、GPU、FPGA还是其他设备，而无需实际重写代码！

![](../Images/28c0d0696ea2096db955d905348c868c.png)

ONNX Runtime（图片来源于作者）

ONNX Runtime的主要优点之一是其性能。它使用多种技术，如即时编译（JIT）、内核融合和子图分区来优化模型性能。它还支持线程池和节点间通信进行分布式部署，使其成为大规模部署的合适选择。

我将在未来的文章中解释所有这些高级功能！

ONNX Runtime还支持多种模型，包括传统的机器学习模型和深度学习模型。这使得它成为一个多功能的推断引擎，可以用于从计算机视觉和自然语言处理到语音识别和自动驾驶等各种应用。

## 让我们开始编码吧！

现在让我们来看一个示例，我们将**使用**经典的**scikit-learn**来**创建一个机器学习模型**，然后**将**这个模型**转换**为ONNX格式，以便我们可以**与ONNX Runtime一起使用**。

首先，我们导入必要的库，将模型拉入sklearn并导出为经典的pickle格式。我们将使用鸢尾花数据集。

[PRE0]

现在我们已经训练并保存了模型，我们可以重新导入它并将其转换为ONNX模型。**每个框架都有其自己的转换库**。因此，如果你是在PyTorch或TensorFlow中开发的模型，你需要使用其他库。在这种情况下，库叫做**skl2onnx**。

所以我们导入了必要的库。

[PRE1]

现在我们终于可以进行转换了。我们应该指定`inital_type`，然后可以创建一个名为*model.onnx*的文件，用于保存onnx模型。

[PRE2]

现在我们已经有了ONNX格式的模型，我们可以导入它，并在一些数据上使用它进行推理。

然后我们安装ONNX Runtime。

[PRE3]

现在我们创建数据，并导入模型，从而创建一个会话。我们指定输入和输出名称（标签），然后在数据上运行会话！

[PRE4]

好吧，你通过利用ONNX Runtime得到了结果。这只需要几个简单的命令！

这只是对ONNX的一个介绍，你当然可以做更多，但我希望你发现这个例子有用。

## 最后的想法

ONNX 是一个开源标准，它使得在不同框架和硬件平台之间移动深度学习模型变得容易。它提供了一套优化和量化模型的工具，并且得到了众多公司和组织的支持。因此，ONNX 正在成为深度学习的重要标准，使得共享模型和跨平台部署变得简单。

# 结束

*Marcello Politi*

[Linkedin](https://www.linkedin.com/in/marcello-politi/)，[Twitter](https://twitter.com/_March08_)，[CV](https://march-08.github.io/digital-cv/)
