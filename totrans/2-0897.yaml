- en: Fine-tune MPT-7B on Amazon SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨ Amazon SageMaker ä¸Šå¾®è°ƒ MPT-7B
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa](https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa](https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa)
- en: '![](../Images/2f5691f2830e17d1af6c907a144f4567.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f5691f2830e17d1af6c907a144f4567.png)'
- en: Photo by [Jeffery Ho](https://unsplash.com/@jefferyho?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”± [Jeffery Ho](https://unsplash.com/@jefferyho?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ã€‚
- en: Learn how to prepare a dataset and create a training job to fine-tune MPT-7B
    on Amazon SageMaker
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•å‡†å¤‡æ•°æ®é›†å¹¶åˆ›å»ºä¸€ä¸ªè®­ç»ƒä»»åŠ¡ï¼Œä»¥ä¾¿åœ¨ Amazon SageMaker ä¸Šå¾®è°ƒ MPT-7Bã€‚
- en: '[](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[![JoÃ£o
    Pereira](../Images/2946b185eb134ddfaa71cf5af5e3cda6.png)](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    [JoÃ£o Pereira](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[![JoÃ£o
    Pereira](../Images/2946b185eb134ddfaa71cf5af5e3cda6.png)](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    [JoÃ£o Pereira](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    Â·9 min readÂ·Jun 20, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 9 åˆ†é’ŸÂ·2023 å¹´ 6 æœˆ 20 æ—¥
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: New large language models (LLMs) are being announced every week, each trying
    to beat its predecessor and take over the evaluation leaderboards. One of the
    latest models out there is [MPT-7B](https://www.mosaicml.com/blog/mpt-7b) that
    was released by MosaicML. Unlike other models of its kind, this 7-billion-parameter
    model is open-source and licensed for commercial use ([Apache 2.0 license](https://github.com/mosaicml/llm-foundry/blob/main/LICENSE))
    ğŸš€.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å‘¨éƒ½ä¼šæœ‰æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«å®£å¸ƒï¼Œæ¯ä¸ªæ¨¡å‹éƒ½è¯•å›¾è¶…è¶Šå…¶å‰ä»»å¹¶å æ®è¯„ä¼°æ’è¡Œæ¦œçš„é¦–ä½ã€‚å…¶ä¸­æœ€æ–°çš„æ¨¡å‹ä¹‹ä¸€æ˜¯ [MPT-7B](https://www.mosaicml.com/blog/mpt-7b)ï¼Œç”±
    MosaicML å‘å¸ƒã€‚ä¸åŒç±»å…¶ä»–æ¨¡å‹ä¸åŒï¼Œè¿™æ¬¾ 70 äº¿å‚æ•°çš„æ¨¡å‹æ˜¯å¼€æºçš„ï¼Œå¹¶ä¸”è·å¾—äº†å•†ä¸šä½¿ç”¨çš„è®¸å¯è¯ ([Apache 2.0 è®¸å¯è¯](https://github.com/mosaicml/llm-foundry/blob/main/LICENSE))
    ğŸš€ã€‚
- en: Foundation models like MPT-7B are pre-trained on datasets with trillions of
    tokens (100 tokens ~ 75 words) crawled from the web and, when prompted well, they
    can produce impressive outputs. However, to truly unlock the value of large language
    models in real-world applications, smart prompt-engineering might not be enough
    to make them work for your use case and, therefore, fine-tuning a foundation model
    on a domain-specific dataset is required.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åƒ MPT-7B è¿™æ ·çš„åŸºç¡€æ¨¡å‹æ˜¯åœ¨å…·æœ‰ä¸‡äº¿ä¸ªæ ‡è®°ï¼ˆ100 ä¸ªæ ‡è®°çº¦ç­‰äº 75 ä¸ªå•è¯ï¼‰çš„æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œå¹¶ä¸”å½“æç¤ºå¾—å½“æ—¶ï¼Œå®ƒä»¬èƒ½å¤Ÿç”Ÿæˆä»¤äººå°è±¡æ·±åˆ»çš„è¾“å‡ºã€‚ç„¶è€Œï¼Œè¦çœŸæ­£é‡Šæ”¾å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„ä»·å€¼ï¼Œä»…ä»…æ™ºèƒ½çš„æç¤ºå·¥ç¨‹å¯èƒ½ä¸è¶³ä»¥ä½¿å…¶é€‚ç”¨äºä½ çš„ç”¨ä¾‹ï¼Œå› æ­¤ï¼Œéœ€è¦åœ¨é¢†åŸŸç‰¹å®šçš„æ•°æ®é›†ä¸Šå¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
- en: LLMs have billions of parameters and, consequently, fine-tuning such large models
    is challenging. Good news is that fine-tuning is much cheaper and faster as compared
    to pre-training the foundation model given that 1) the domain-specific datasets
    are "small" and 2) fine-tuning requires only a few passes over the training data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰æ•°åäº¿ä¸ªå‚æ•°ï¼Œå› æ­¤å¾®è°ƒå¦‚æ­¤åºå¤§çš„æ¨¡å‹æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚å¥½æ¶ˆæ¯æ˜¯ï¼Œç›¸æ¯”äºé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå¾®è°ƒçš„æˆæœ¬æ›´ä½ã€é€Ÿåº¦æ›´å¿«ï¼Œå› ä¸º 1) é¢†åŸŸç‰¹å®šçš„æ•°æ®é›†æ˜¯â€œè¾ƒå°â€çš„ï¼Œ2)
    å¾®è°ƒåªéœ€å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå°‘é‡éå†ã€‚
- en: '***Here is what we will learn in this article:***'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '***åœ¨æœ¬æ–‡ä¸­æˆ‘ä»¬å°†å­¦ä¹ ï¼š***'
- en: How to create and structure a dataset for fine-tuning a large language model.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚ä½•åˆ›å»ºå’Œæ„å»ºç”¨äºå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°æ®é›†ã€‚
- en: What is and how to configure a distributed training job with fully sharded data
    parallel.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒä½œä¸šä»¥åŠå¦‚ä½•é…ç½®å®ƒã€‚
- en: How to define a ğŸ˜Š HuggingFace estimator.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚ä½•å®šä¹‰ä¸€ä¸ª ğŸ˜Š HuggingFace ä¼°ç®—å™¨ã€‚
- en: How to launch a training job in Amazon SageMaker that fine-tunes MPT-7B.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨ Amazon SageMaker ä¸­å¯åŠ¨ä¸€ä¸ªå¾®è°ƒ MPT-7B çš„è®­ç»ƒä½œä¸šã€‚
- en: 1\. Install dependencies and set S3 paths
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. å®‰è£…ä¾èµ–é¡¹å¹¶è®¾ç½® S3 è·¯å¾„
- en: Let's start by installing the [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk)
    and a few other packages. This SDK makes it possible to train and deploy machine
    learning models on AWS with a few lines of Python code. The code below is available
    in the `[sagemaker_finetuning.ipynb](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/sagemaker_finetuning.ipynb)`notebook
    in Github. Run the notebook in SageMaker Studio, a SageMaker notebook instance,
    or in your laptop after [authenticating to an AWS account](https://docs.aws.amazon.com/signin/latest/userguide/command-line-sign-in.html).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆå®‰è£… [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk)
    å’Œå…¶ä»–ä¸€äº›åŒ…ã€‚è¿™ä¸ª SDK ä½¿å¾—åœ¨ AWS ä¸Šè®­ç»ƒå’Œéƒ¨ç½²æœºå™¨å­¦ä¹ æ¨¡å‹å˜å¾—å¯èƒ½ï¼Œåªéœ€å‡ è¡Œ Python ä»£ç ã€‚ä¸‹é¢çš„ä»£ç å¯ä»¥åœ¨ Github çš„ `[sagemaker_finetuning.ipynb](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/sagemaker_finetuning.ipynb)`
    notebook ä¸­æ‰¾åˆ°ã€‚åœ¨ SageMaker Studioã€SageMaker notebook å®ä¾‹æˆ–åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šè¿è¡Œ notebookï¼Œå‰ææ˜¯ [è®¤è¯åˆ°
    AWS è´¦æˆ·](https://docs.aws.amazon.com/signin/latest/userguide/command-line-sign-in.html)ã€‚
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next step is to define the paths where the data will be saved in S3 and create
    a SageMaker session.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯å®šä¹‰æ•°æ®å°†ä¿å­˜åˆ° S3 çš„è·¯å¾„ï¼Œå¹¶åˆ›å»ºä¸€ä¸ª SageMaker ä¼šè¯ã€‚
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 2\. Build a fine-tuning dataset
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. æ„å»ºå¾®è°ƒæ•°æ®é›†
- en: We will create a dummy dataset to demonstrate how to fine-tune MPT-7B. Since
    training models of this size on a complete dataset takes long and is costly, it
    is a good idea to first test & debug the training job on a small dataset and second
    scale training to the complete dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ•°æ®é›†æ¥æ¼”ç¤ºå¦‚ä½•å¾®è°ƒ MPT-7Bã€‚ç”±äºåœ¨å®Œæ•´æ•°æ®é›†ä¸Šè®­ç»ƒæ­¤å¤§å°çš„æ¨¡å‹éœ€è¦è¾ƒé•¿æ—¶é—´ä¸”æˆæœ¬è¾ƒé«˜ï¼Œå› æ­¤é¦–å…ˆåœ¨å°æ•°æ®é›†ä¸Šæµ‹è¯•å’Œè°ƒè¯•è®­ç»ƒä»»åŠ¡æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œå…¶æ¬¡å°†è®­ç»ƒè§„æ¨¡æ‰©å¤§åˆ°å®Œæ•´æ•°æ®é›†ã€‚
- en: '**Format dataset as a list of dictionaries** â€” The dataset should be formatted
    as a list of dictionaries, where each example has a key-value structure, *e.g.*,'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°†æ•°æ®é›†æ ¼å¼åŒ–ä¸ºå­—å…¸åˆ—è¡¨** â€” æ•°æ®é›†åº”æ ¼å¼åŒ–ä¸ºå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªç¤ºä¾‹å…·æœ‰é”®å€¼ç»“æ„ï¼Œ*ä¾‹å¦‚*ï¼Œ'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `prompt` is the input given to the model (*e.g.*, a question). The `response`
    is the output that the model is trained to predict (*e.g.*, the answer to the
    question in the `prompt`). The raw prompt is often preprocessed to fit in a prompt
    template that helps the model to generate better outputs. Note that the model
    is trained for causal language modelling, so you can think of it as a "document
    completer". It is a good idea to design the prompt template in such a way that
    the model thinks that it is completing a document. Andrej Karpathy explains well
    this mechanism in his talk [*State of GPT*](https://youtu.be/bZQun8Y4L2A).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`prompt` æ˜¯ç»™æ¨¡å‹çš„è¾“å…¥ï¼ˆ*ä¾‹å¦‚*ï¼Œä¸€ä¸ªé—®é¢˜ï¼‰ã€‚`response` æ˜¯æ¨¡å‹è®­ç»ƒé¢„æµ‹çš„è¾“å‡ºï¼ˆ*ä¾‹å¦‚*ï¼Œ`prompt` ä¸­é—®é¢˜çš„ç­”æ¡ˆï¼‰ã€‚åŸå§‹æç¤ºé€šå¸¸ä¼šç»è¿‡é¢„å¤„ç†ï¼Œä»¥é€‚åº”æç¤ºæ¨¡æ¿ï¼Œè¿™æœ‰åŠ©äºæ¨¡å‹ç”Ÿæˆæ›´å¥½çš„è¾“å‡ºã€‚è¯·æ³¨æ„ï¼Œæ¨¡å‹æ˜¯ä¸ºå› æœè¯­è¨€å»ºæ¨¡è®­ç»ƒçš„ï¼Œå› æ­¤å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯ä¸€ä¸ªâ€œæ–‡æ¡£å®Œæˆå™¨â€ã€‚è®¾è®¡æç¤ºæ¨¡æ¿æ—¶ï¼Œæœ€å¥½è®©æ¨¡å‹è§‰å¾—å®ƒæ­£åœ¨å®Œæˆä¸€ä¸ªæ–‡æ¡£ã€‚Andrej
    Karpathy åœ¨ä»–çš„æ¼”è®² [*State of GPT*](https://youtu.be/bZQun8Y4L2A) ä¸­å¾ˆå¥½åœ°è§£é‡Šäº†è¿™ä¸€æœºåˆ¶ã€‚'
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Upload the training and test data to S3** â€” Once the training and test sets
    are ready and formatted as a list of dictionaries, we upload them to S3 as JSON
    lines using the utility function below:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¸Šä¼ åˆ° S3** â€” ä¸€æ—¦è®­ç»ƒå’Œæµ‹è¯•é›†å‡†å¤‡å¥½å¹¶æ ¼å¼åŒ–ä¸ºå­—å…¸åˆ—è¡¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢çš„å·¥å…·å‡½æ•°å°†å®ƒä»¬ä½œä¸º JSON è¡Œä¸Šä¼ åˆ° S3ï¼š'
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 3\. SageMaker Training job
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. SageMaker è®­ç»ƒä»»åŠ¡
- en: 'With the datasets available in S3, we will now create a training job in Amazon
    SageMaker. For that, we have to create an entry point script, modify the configuration
    file specifying the training settings, and define an HuggingFace estimator. We
    will (re-)use the training script from [LLM Foundry](https://github.com/mosaicml/llm-foundry)
    and [Composer](https://github.com/mosaicml/composer) libraryâ€™s CLI launcher that
    sets up the distributed training environment. Both of these packages are maintained
    by [MosaicML](https://www.mosaicml.com/), the company behind MPT-7B. The working
    folder should be structured like:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ S3 ä¸­å¯ç”¨çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬ç°åœ¨å°†åœ¨ Amazon SageMaker ä¸­åˆ›å»ºä¸€ä¸ªè®­ç»ƒä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªå…¥å£ç‚¹è„šæœ¬ï¼Œä¿®æ”¹é…ç½®æ–‡ä»¶ä»¥æŒ‡å®šè®­ç»ƒè®¾ç½®ï¼Œå¹¶å®šä¹‰ä¸€ä¸ª
    HuggingFace ä¼°ç®—å™¨ã€‚æˆ‘ä»¬å°†ï¼ˆé‡æ–°ï¼‰ä½¿ç”¨æ¥è‡ª [LLM Foundry](https://github.com/mosaicml/llm-foundry)
    å’Œ [Composer](https://github.com/mosaicml/composer) åº“çš„ CLI å¯åŠ¨å™¨ï¼Œè¿™äº›å¯åŠ¨å™¨è®¾ç½®äº†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒã€‚è¿™ä¸¤ä¸ªåŒ…å‡ç”±
    [MosaicML](https://www.mosaicml.com/) ç»´æŠ¤ï¼Œè¯¥å…¬å¸æ˜¯ MPT-7B çš„èƒŒåå…¬å¸ã€‚å·¥ä½œæ–‡ä»¶å¤¹åº”æŒ‰å¦‚ä¸‹ç»“æ„ç»„ç»‡ï¼š
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We will now dive deep into each of these files.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å°†æ·±å…¥äº†è§£è¿™äº›æ–‡ä»¶ä¸­çš„æ¯ä¸€ä¸ªã€‚
- en: '**Create a configuration file** `[**finetuning_config.yaml**](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/finetuning_config.yaml)`â€”
    The template provided in the [LLM Foundry](https://github.com/mosaicml/llm-foundry)
    repository is a good starting point, specifically the `[mpt-7b-dolly-sft.yaml](https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/yamls/finetune/mpt-7b_dolly_sft.yaml)`
    file. However, depending on your dataset size and training instance, you might
    have to adjust some of these configurations, such as the batch size. I have modified
    the file to fine-tune the model in SageMaker (check `[finetuning_config.yaml](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/finetuning_config.yaml)`).
    The parameters that you should pay attention to are the following:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åˆ›å»ºä¸€ä¸ªé…ç½®æ–‡ä»¶** `[**finetuning_config.yaml**](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/finetuning_config.yaml)`â€”
    [LLM Foundry](https://github.com/mosaicml/llm-foundry) ä»“åº“ä¸­æä¾›çš„æ¨¡æ¿æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ï¼Œç‰¹åˆ«æ˜¯ `[mpt-7b-dolly-sft.yaml](https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/yamls/finetune/mpt-7b_dolly_sft.yaml)`
    æ–‡ä»¶ã€‚ç„¶è€Œï¼Œæ ¹æ®æ•°æ®é›†å¤§å°å’Œè®­ç»ƒå®ä¾‹ï¼Œæ‚¨å¯èƒ½éœ€è¦è°ƒæ•´ä¸€äº›é…ç½®ï¼Œä¾‹å¦‚æ‰¹é‡å¤§å°ã€‚æˆ‘å·²ç»ä¿®æ”¹äº†è¯¥æ–‡ä»¶ä»¥åœ¨ SageMaker ä¸­å¾®è°ƒæ¨¡å‹ï¼ˆè¯·æŸ¥çœ‹ `[finetuning_config.yaml](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/finetuning_config.yaml)`ï¼‰ã€‚æ‚¨éœ€è¦å…³æ³¨çš„å‚æ•°å¦‚ä¸‹ï¼š'
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The`max_seq_length` indicates the maximum number of tokens of the input (remember
    that 100 tokens ~ 75 words). The training and test data will be loaded using the
    ğŸ˜Š [Datasets](https://huggingface.co/docs/datasets/index) library from the `/opt/ml/input/data/{train,
    test}` directory inside the container associated with the training job. Check
    out the [SageMaker Training Storage Folders](https://docs.aws.amazon.com/sagemaker/latest/dg/model-train-storage.html)â€™
    documentation to understand how the container directories are structured. The
    `max_duration` specifies the number of epochs for fine-tuning. Two to three epochs
    is typically a good choice. `eval_interval` indicates how often the model will
    be evaluated on the test set.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_seq_length` æŒ‡ç¤ºè¾“å…¥çš„æœ€å¤§ä»¤ç‰Œæ•°ï¼ˆè®°ä½ 100 ä¸ªä»¤ç‰Œçº¦ç­‰äº 75 ä¸ªå•è¯ï¼‰ã€‚è®­ç»ƒå’Œæµ‹è¯•æ•°æ®å°†ä½¿ç”¨ ğŸ˜Š [Datasets](https://huggingface.co/docs/datasets/index)
    åº“ä»å®¹å™¨å†…ä¸è®­ç»ƒä»»åŠ¡å…³è”çš„ `/opt/ml/input/data/{train, test}` ç›®å½•åŠ è½½ã€‚æŸ¥çœ‹ [SageMaker Training Storage
    Folders](https://docs.aws.amazon.com/sagemaker/latest/dg/model-train-storage.html)
    æ–‡æ¡£ï¼Œä»¥äº†è§£å®¹å™¨ç›®å½•çš„ç»“æ„ã€‚`max_duration` æŒ‡å®šå¾®è°ƒçš„è½®æ•°ã€‚ä¸¤åˆ°ä¸‰è½®é€šå¸¸æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚`eval_interval` æŒ‡ç¤ºæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¯„ä¼°é¢‘ç‡ã€‚'
- en: The distributed training strategy is Fully Sharded Data Parallel (FSDP), which
    enables efficient training of large models like MPT-7B. Unlike the traditional
    data parallel strategy, which keeps a copy of the model in each GPU, FSDP shards
    model parameters, optimizer states, and gradients across data parallel workers.
    If you want to learn more about FSDP, check this insightful [PyTorch intro post](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/).
    FSDP is integrated in [Composer](https://github.com/mosaicml/composer), the distributed
    training library used by [LLM Foundry](https://github.com/mosaicml/llm-foundry).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥æ˜¯ Fully Sharded Data Parallel (FSDP)ï¼Œè¿™ä½¿å¾—åƒ MPT-7B è¿™æ ·çš„å·¨å¤§æ¨¡å‹çš„è®­ç»ƒå˜å¾—é«˜æ•ˆã€‚ä¸ä¼ ç»Ÿçš„æ•°æ®å¹¶è¡Œç­–ç•¥ä¸åŒï¼Œåè€…åœ¨æ¯ä¸ª
    GPU ä¸­ä¿ç•™æ¨¡å‹å‰¯æœ¬ï¼ŒFSDP å°†æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦åœ¨æ•°æ®å¹¶è¡Œå·¥ä½œè€…ä¹‹é—´è¿›è¡Œåˆ†ç‰‡ã€‚å¦‚æœæ‚¨æƒ³æ·±å…¥äº†è§£ FSDPï¼Œå¯ä»¥æŸ¥çœ‹è¿™ç¯‡æœ‰è§åœ°çš„ [PyTorch
    ä»‹ç»æ–‡ç« ](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)ã€‚FSDP
    å·²é›†æˆåœ¨ [Composer](https://github.com/mosaicml/composer) ä¸­ï¼Œè¿™æ˜¯ [LLM Foundry](https://github.com/mosaicml/llm-foundry)
    ä½¿ç”¨çš„åˆ†å¸ƒå¼è®­ç»ƒåº“ã€‚
- en: '`save_folder` determines where the model checkpoint (`.pt` file) is saved.
    We set it to the temporary folder `/tmp/checkpoints`.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`save_folder` ç¡®å®šæ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆ`.pt` æ–‡ä»¶ï¼‰ä¿å­˜çš„ä½ç½®ã€‚æˆ‘ä»¬å°†å…¶è®¾ç½®ä¸ºä¸´æ—¶æ–‡ä»¶å¤¹ `/tmp/checkpoints`ã€‚'
- en: '**Create the entry point script** `[**launcher.sh**](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/launcher.sh)`â€”
    A bash script is used as entry point. The bash script clones the LLM Foundry repository,
    installs requirements, and, more importantly, runs the training script using Composer
    library''s distributed launcher. Note that, typically, training jobs in SageMaker
    run the training script using a command like `python train.py`. However, it is
    possible to pass a bash script as entry point, which provides more flexibility
    in our scenario. Finally, we convert the model checkpoint saved to `/tmp/checkpoints`
    to the HuggingFace model format and save the final artifacts into `/opt/ml/model/`.
    SageMaker will compress all files in this directory, create a tarball `model.tar.gz`,
    and upload it to S3\. The tarball is useful for inference.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åˆ›å»ºå…¥å£è„šæœ¬** `[**launcher.sh**](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/launcher.sh)`â€”
    ä¸€ä¸ª bash è„šæœ¬ä½œä¸ºå…¥å£ç‚¹ã€‚è¿™ä¸ª bash è„šæœ¬å…‹éš†äº† LLM Foundry ä»“åº“ï¼Œå®‰è£…äº†æ‰€éœ€çš„ä¾èµ–ï¼Œå¹¶ä¸”æ›´é‡è¦çš„æ˜¯ï¼Œä½¿ç”¨ Composer åº“çš„åˆ†å¸ƒå¼å¯åŠ¨å™¨è¿è¡Œè®­ç»ƒè„šæœ¬ã€‚è¯·æ³¨æ„ï¼Œé€šå¸¸åœ¨
    SageMaker ä¸­ï¼Œè®­ç»ƒä½œä¸šæ˜¯é€šè¿‡åƒ `python train.py` è¿™æ ·çš„å‘½ä»¤è¿è¡Œè®­ç»ƒè„šæœ¬ã€‚ç„¶è€Œï¼Œåœ¨æˆ‘ä»¬çš„åœºæ™¯ä¸­ï¼Œå¯ä»¥å°† bash è„šæœ¬ä½œä¸ºå…¥å£ç‚¹ï¼Œè¿™æä¾›äº†æ›´å¤šçš„çµæ´»æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä¿å­˜åˆ°
    `/tmp/checkpoints` çš„æ¨¡å‹æ£€æŸ¥ç‚¹è½¬æ¢ä¸º HuggingFace æ¨¡å‹æ ¼å¼ï¼Œå¹¶å°†æœ€ç»ˆçš„å·¥ä»¶ä¿å­˜åˆ° `/opt/ml/model/`ã€‚SageMaker
    ä¼šå‹ç¼©æ­¤ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ª tarball `model.tar.gz`ï¼Œå¹¶å°†å…¶ä¸Šä¼ åˆ° S3ã€‚è¿™ä¸ª tarball å¯¹äºæ¨ç†éå¸¸æœ‰ç”¨ã€‚'
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Define ğŸ˜Š HuggingFace Estimator** â€” The Estimator sets the Docker container
    used to run the training job. We will use an image with PyTorch 2.0.0 and Python
    3.10\. The bash script and the configuration file are automatically uploaded to
    S3 and made available inside the container (handled by the SageMaker Python SDK).
    We set the training instance to`[g5.48xlarge](https://aws.amazon.com/ec2/instance-types/g5/)`
    that has 8x NVIDIA A10G GPUs. The `[p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)`
    is also a good choice. Even though it is more expensive, it is equipped with 8x
    NVIDIA A100 GPUs. We also indicate the metrics to track on the training and test
    sets (Cross Entropy and Perplexity). The values of these metrics are captured
    via Regex expressions and sent to Amazon CloudWatch.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å®šä¹‰ ğŸ˜Š HuggingFace ä¼°ç®—å™¨** â€” ä¼°ç®—å™¨è®¾ç½®äº†ç”¨äºè¿è¡Œè®­ç»ƒä½œä¸šçš„ Docker å®¹å™¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå¸¦æœ‰ PyTorch 2.0.0
    å’Œ Python 3.10 çš„é•œåƒã€‚bash è„šæœ¬å’Œé…ç½®æ–‡ä»¶ä¼šè‡ªåŠ¨ä¸Šä¼ åˆ° S3ï¼Œå¹¶åœ¨å®¹å™¨å†…å¯ç”¨ï¼ˆç”± SageMaker Python SDK å¤„ç†ï¼‰ã€‚æˆ‘ä»¬å°†è®­ç»ƒå®ä¾‹è®¾ç½®ä¸º`[g5.48xlarge](https://aws.amazon.com/ec2/instance-types/g5/)`ï¼Œå®ƒé…å¤‡äº†
    8x NVIDIA A10G GPUã€‚`[p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)`
    ä¹Ÿæ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚è™½ç„¶å®ƒæ›´è´µï¼Œä½†é…å¤‡äº† 8x NVIDIA A100 GPUã€‚æˆ‘ä»¬è¿˜æŒ‡æ˜äº†è¦åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­è·Ÿè¸ªçš„æŒ‡æ ‡ï¼ˆäº¤å‰ç†µå’Œå›°æƒ‘åº¦ï¼‰ã€‚è¿™äº›æŒ‡æ ‡çš„å€¼é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ•è·å¹¶å‘é€åˆ°
    Amazon CloudWatchã€‚'
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: âš ï¸ Make sure to request the respective quotas for SageMaker Training, along
    with [Warm Pools](https://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-pools.html)â€™
    quota in case you are making use of this cool feature. If you plan to run many
    jobs in SageMaker, take a look at [SageMaker Saving Plans](https://aws.amazon.com/savingsplans/ml-pricing/).
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: âš ï¸ ç¡®ä¿è¯·æ±‚ SageMaker è®­ç»ƒçš„ç›¸åº”é…é¢ï¼Œå¹¶åœ¨ä½¿ç”¨æ­¤é…·ç‚«åŠŸèƒ½æ—¶è¯·æ±‚ [çƒ­æ± ](https://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-pools.html)
    çš„é…é¢ã€‚å¦‚æœä½ è®¡åˆ’åœ¨ SageMaker ä¸­è¿è¡Œå¤šä¸ªä½œä¸šï¼Œå¯ä»¥æŸ¥çœ‹ [SageMaker èŠ‚çœè®¡åˆ’](https://aws.amazon.com/savingsplans/ml-pricing/)ã€‚
- en: '**Launch the training job ğŸš€** â€” We have all set to start the training job on
    Amazon SageMaker:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¯åŠ¨è®­ç»ƒä½œä¸š ğŸš€** â€” æˆ‘ä»¬å·²å‡†å¤‡å¥½åœ¨ Amazon SageMaker ä¸Šå¼€å§‹è®­ç»ƒä½œä¸šï¼š'
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The training time will depend on the size of your dataset. With our dummy dataset,
    training takes roughly **20min** to complete. Once the model is trained and converted
    to ğŸ˜Š HuggingFace format, SageMaker will upload the model tarball (`model.tar.gz`)
    to the S3 `output_path`. I found that in practice the uploading step takes rather
    long (>1h), which might be due to the size of the model artifacts to compress
    (~25GB).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ—¶é—´å°†å–å†³äºæ•°æ®é›†çš„å¤§å°ã€‚ä½¿ç”¨æˆ‘ä»¬çš„è™šæ‹Ÿæ•°æ®é›†ï¼Œè®­ç»ƒå¤§çº¦éœ€è¦ **20min** å®Œæˆã€‚ä¸€æ—¦æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶è½¬æ¢ä¸º ğŸ˜Š HuggingFace æ ¼å¼ï¼ŒSageMaker
    å°†æŠŠæ¨¡å‹ tarball (`model.tar.gz`) ä¸Šä¼ åˆ° S3 `output_path`ã€‚æˆ‘å‘ç°å®é™…ä¸Šä¸Šä¼ æ­¥éª¤èŠ±è´¹çš„æ—¶é—´è¾ƒé•¿ï¼ˆ>1hï¼‰ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºæ¨¡å‹å·¥ä»¶å‹ç¼©çš„å¤§å°ï¼ˆ~25GBï¼‰ã€‚
- en: 4\. Summary
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. æ€»ç»“
- en: In this article, I showed how you can prepare a dataset and create a training
    job in SageMaker to fine-tune MPT-7B for your use case. The implementation leverages
    the training script from [LLM Foundry](https://github.com/mosaicml/llm-foundry)
    and uses [Composer](https://github.com/mosaicml/composer) libraryâ€™s distributed
    training launcher. Once you have fine-tuned your model and want to deploy it,
    I recommend to check out the [blog posts by Philipp Schmid](https://www.philschmid.de/);
    there are plenty of examples on how to deploy LLMs in SageMaker. Have fun with
    your fine-tuned MPT-7B model! ğŸ‰
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å±•ç¤ºäº†å¦‚ä½•å‡†å¤‡æ•°æ®é›†å¹¶åœ¨ SageMaker ä¸­åˆ›å»ºè®­ç»ƒä»»åŠ¡ï¼Œä»¥å¾®è°ƒ MPT-7B ä»¥é€‚åº”ä½ çš„ä½¿ç”¨æ¡ˆä¾‹ã€‚è¯¥å®ç°åˆ©ç”¨äº†æ¥è‡ª [LLM Foundry](https://github.com/mosaicml/llm-foundry)
    çš„è®­ç»ƒè„šæœ¬ï¼Œå¹¶ä½¿ç”¨äº† [Composer](https://github.com/mosaicml/composer) åº“çš„åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨å™¨ã€‚ä¸€æ—¦ä½ å¾®è°ƒäº†ä½ çš„æ¨¡å‹å¹¶æƒ³è¦éƒ¨ç½²å®ƒï¼Œæˆ‘å»ºè®®æŸ¥çœ‹
    [Philipp Schmid çš„åšå®¢æ–‡ç« ](https://www.philschmid.de/)ï¼›é‚£é‡Œæœ‰å¾ˆå¤šå…³äºå¦‚ä½•åœ¨ SageMaker ä¸­éƒ¨ç½² LLM
    çš„ç¤ºä¾‹ã€‚ç¥ä½ ç©å¾—æ„‰å¿«ï¼Œäº«å—ä½ çš„å¾®è°ƒ MPT-7B æ¨¡å‹ï¼ğŸ‰
- en: 'All the code used in this article is available in Github:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ‰€æœ‰ä»£ç éƒ½å¯ä»¥åœ¨ Github ä¸Šæ‰¾åˆ°ï¼š
- en: '[](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b?source=post_page-----1e68e71051fa--------------------------------)
    [## GitHub - jpcpereira/sagemaker-fine-tune-mpt-7b'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - jpcpereira/sagemaker-fine-tune-mpt-7b](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b?source=post_page-----1e68e71051fa--------------------------------)'
- en: github.com](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b?source=post_page-----1e68e71051fa--------------------------------)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b?source=post_page-----1e68e71051fa--------------------------------)'
- en: â€” JoÃ£o Pereira
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: â€” ä¹”æ˜‚Â·ä½©é›·æ‹‰
- en: '*Thank you for reading. Hope this article helps you getting started with fine-tuning
    large language models like MPT-7B in Amazon SageMaker. If you would like to read
    my future articles, please* [*follow me*](https://medium.com/@joao.pereira.abt/subscribe)*.
    Feedback is highly appreciated! Leave a comment below if you have any questions
    or reach out to me directly* [***by email***](mailto:mail@joao-pereira.pt) *or
    in* [***LinkedIn***](https://www.linkedin.com/in/jpcpereira/)*.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ„Ÿè°¢é˜…è¯»ã€‚å¸Œæœ›æœ¬æ–‡èƒ½å¸®åŠ©ä½ å…¥é—¨åœ¨ Amazon SageMaker ä¸­å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹å¦‚ MPT-7Bã€‚å¦‚æœä½ æƒ³é˜…è¯»æˆ‘æœªæ¥çš„æ–‡ç« ï¼Œè¯·* [*å…³æ³¨æˆ‘*](https://medium.com/@joao.pereira.abt/subscribe)*ã€‚éå¸¸æ„Ÿè°¢åé¦ˆï¼å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·åœ¨ä¸‹é¢ç•™è¨€ï¼Œæˆ–ç›´æ¥é€šè¿‡*
    [***ç”µå­é‚®ä»¶***](mailto:mail@joao-pereira.pt) *æˆ–åœ¨* [***LinkedIn***](https://www.linkedin.com/in/jpcpereira/)*è”ç³»æˆ‘ã€‚*'
