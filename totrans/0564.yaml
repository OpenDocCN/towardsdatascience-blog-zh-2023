- en: 'Converting Texts to Numeric Form with TfidfVectorizer: A Step-by-Step Guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/converting-texts-to-numeric-form-with-tfidfvectorizer-a-step-by-step-guide-bb9330562ae3](https://towardsdatascience.com/converting-texts-to-numeric-form-with-tfidfvectorizer-a-step-by-step-guide-bb9330562ae3)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/d2844dbee6bc63d8e734f8a44ae8298f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mohamed Nohassi](https://unsplash.com/@coopery?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: How to calculate Tfidf values manually and using sklearn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rashida00.medium.com/?source=post_page-----bb9330562ae3--------------------------------)[![Rashida
    Nasrin Sucky](../Images/42bd057e8eca255907c43c29a498f2ca.png)](https://rashida00.medium.com/?source=post_page-----bb9330562ae3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb9330562ae3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb9330562ae3--------------------------------)
    [Rashida Nasrin Sucky](https://rashida00.medium.com/?source=post_page-----bb9330562ae3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb9330562ae3--------------------------------)
    ·6 min read·Oct 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: TFIDF is a method to convert texts to numeric form for machine learning or AI
    models. In other words, TFIDF is a method to extract features from texts. This
    is a more sophisticated method than the CountVectorizer() method I discussed in
    [my last article](/countvectorizer-to-extract-features-from-texts-in-python-in-detail-0e7147c10753).
  prefs: []
  type: TYPE_NORMAL
- en: The TFIDF method provides a score for each word that represents the usefulness
    of that word or the relevance of the word. It measures the usage of the word compared
    to the other words present in the document.
  prefs: []
  type: TYPE_NORMAL
- en: This article will calculate the TFIDF scores manually so that you understand
    the concept of TFIDF clearly. Toward the end, we will see how to use the TFIDF
    vectorizer from the sklearn library as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two parts to it: TF and IDF. Let’s see how each part works.'
  prefs: []
  type: TYPE_NORMAL
- en: '**TF**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'TF is elaborated as ‘Term Frequency’. TF can be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TF = # of occurrence of a word in a Document'
  prefs: []
  type: TYPE_NORMAL
- en: OR
  prefs: []
  type: TYPE_NORMAL
- en: TF = (# of occurrence in a document) / (# of words in a document)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s work on an example. We will find the TF for each word for this document:'
  prefs: []
  type: TYPE_NORMAL
- en: '**My name is Lilly**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s see an example for each of the formulas.
  prefs: []
  type: TYPE_NORMAL
- en: 'TF = # of occurrence of a word in a Document'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If we take the first formula here which is simply the number of occurrences
    of a word in a document, TF for the word ‘MY’ is 1 as it appeared only once.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, the TF for the word
  prefs: []
  type: TYPE_NORMAL
- en: ‘name’ = 1, ‘is’ = 1, ‘Lilly’ = 1
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s use the second formula.
  prefs: []
  type: TYPE_NORMAL
- en: TF = (# of occurrence in a document) / (# of words in a document)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If we take the second formula, the first part of the formula (# of occurrences
    in a document) is 1, and the second part (# of words in a document) is 4.
  prefs: []
  type: TYPE_NORMAL
- en: So, the TF for the word ‘MY’ is 1/4 or 0.25.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, the TF for the words
  prefs: []
  type: TYPE_NORMAL
- en: name = ¼ = 0.25, is = ¼ = 0.25, Lilly = ¼ = 0.25.
  prefs: []
  type: TYPE_NORMAL
- en: '**IDF**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The elaboration of IDF is Inverse Document Frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the formula,
  prefs: []
  type: TYPE_NORMAL
- en: idf = 1 + LN[n/df(t)]
  prefs: []
  type: TYPE_NORMAL
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: idf = LN[n/df(t)]
  prefs: []
  type: TYPE_NORMAL
- en: Where, n = Number of documents available, and
  prefs: []
  type: TYPE_NORMAL
- en: df = Number of documents where the term appears
  prefs: []
  type: TYPE_NORMAL
- en: As per sklearn library’s documentation
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: idf = LN[(1+n) / (1+df(t))] + 1 (default setting)
  prefs: []
  type: TYPE_NORMAL
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: idf = LN[n / df(t)] + 1 (when smooth_idf = True)
  prefs: []
  type: TYPE_NORMAL
- en: We won’t work on all four formulas here. Let’s just work on the 2 formulas.
    You will get the idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate the IDF, only one document is not enough. I will these three
    documents:'
  prefs: []
  type: TYPE_NORMAL
- en: My name is Lilly
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lilly is my mom’s favorite flower
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: My mom loves flowers
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s use this formula to practice this time:'
  prefs: []
  type: TYPE_NORMAL
- en: IDF = LN[n/df(t)]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If we take the word ‘My’ first, n is 3 because we have 3 documents here, and
    df(t) is also 3 because ‘My’ appeared in all three documents.
  prefs: []
  type: TYPE_NORMAL
- en: IDF(MY) = LN(3/3) = 0 (As ln(1) is 0)
  prefs: []
  type: TYPE_NORMAL
- en: We will work on one more word to understand it clearly. Take the word ‘name’.
  prefs: []
  type: TYPE_NORMAL
- en: For the word ‘name’ again the n will be the same as before because the number
    of documents is 3 but the df(t) will be 1\. Because the word ‘name’ is present
    in only one document.
  prefs: []
  type: TYPE_NORMAL
- en: IDF(name) = ln(3/1) = 1.1 (I used Excel’s LN function for this)
  prefs: []
  type: TYPE_NORMAL
- en: '**How does the sklearn library calculate TFIDF?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'the sklearn library uses these two formulas for TF and IDF:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TF = # of occurrence of a word in a Document'
  prefs: []
  type: TYPE_NORMAL
- en: idf = LN[(1+n) / (1+df(t))] + 1
  prefs: []
  type: TYPE_NORMAL
- en: 'If I use the same three documents, for the word ‘MY’:'
  prefs: []
  type: TYPE_NORMAL
- en: TF(My) = 1
  prefs: []
  type: TYPE_NORMAL
- en: IDF(My) =LN((1+3)/(1+3)) + 1 = 1
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for TFIDF is :'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: TFIDF = TF*IDF
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the TFIDF for ‘My’ is :'
  prefs: []
  type: TYPE_NORMAL
- en: TFIDF(My) = 1* 1 = 1
  prefs: []
  type: TYPE_NORMAL
- en: 'For the word ‘name’:'
  prefs: []
  type: TYPE_NORMAL
- en: TF(name) = 1
  prefs: []
  type: TYPE_NORMAL
- en: IDF(name) = =LN((1+3)/(1+1)) +1 = 1.69
  prefs: []
  type: TYPE_NORMAL
- en: TFIDF(name) = 1* 1.69 = 1.69
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same way, the TFIDF for all the words are :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96e3862dd07f2a029b0440e5addadf72.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: 'The sklearn’s tfidf vectorizer normalizes the values to bring them in a 0 to
    1 scale. For that, we need to have SS(Sum of Squared) for the tfidfs of each document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee26e23ced9ed95c449e6b3f93f79d53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The normalized tfidf is:'
  prefs: []
  type: TYPE_NORMAL
- en: The tfidf value for the word/ the SS of the document
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take the word My. normalized tfidf for ‘My’ in the document-1 is:'
  prefs: []
  type: TYPE_NORMAL
- en: tfidf_normalized(My) = 1.00 / 7.871 = 0.356
  prefs: []
  type: TYPE_NORMAL
- en: 'tfidf for the word ‘mom’ in document-3 is:'
  prefs: []
  type: TYPE_NORMAL
- en: tfidf_normalized(name) = 1.42 / 9.005 = 0.472
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, tfidf for the word ‘mom’ in document-2 is:'
  prefs: []
  type: TYPE_NORMAL
- en: tfidf_normalized(name) = 1.42 / 13.009 = 0.392
  prefs: []
  type: TYPE_NORMAL
- en: Looks like the word ‘mom’ has a bit more relevance in the document 2 than in
    the document 3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The normalized tfidf for all the words are here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ef138e24d6aee2e9abca6f9cf268a2d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: Now, we should check how the [tfidf vectorizer in the sklearn library](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the Tfidf vectorizer from sklearn library and define the text
    to be used for feature extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the next code block,
  prefs: []
  type: TYPE_NORMAL
- en: the first line calls the TfidfVectorizer method and saves it in a variable named
    vectorizer.
  prefs: []
  type: TYPE_NORMAL
- en: the second line fit_transform the text into the vectorizer
  prefs: []
  type: TYPE_NORMAL
- en: the third line converts that to an array to display
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It will be helpful to convert this array into a DataFrame and use the words
    as the column names.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4031e7040dc6397a8528106f6699ffb6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: You can use the same parameters as I explained in [my tutorial on CountVectorizer](/countvectorizer-to-extract-features-from-texts-in-python-in-detail-0e7147c10753)
    to refine or limit the number of features. Please feel free to check that.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This tutorial explained in detail how Tfidf Vectorizer works. Though it is very
    simple to use the Tfidf Vectorizer from sklearn library, it is important to understand
    the concept behind it. When you know how a vectorizer works, it becomes easier
    to make the decision on what kind of vectorizer is suitable.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to follow me on [Twitter](https://twitter.com/rashida048) and like
    my [Facebook](https://www.facebook.com/rashida.smith.161) page.
  prefs: []
  type: TYPE_NORMAL
- en: '**The video version of this tutorial:**'
  prefs: []
  type: TYPE_NORMAL
- en: More Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](/a-complete-step-by-step-tutorial-on-sentiment-analysis-in-keras-and-tensorflow-ea420cc8913f?source=post_page-----bb9330562ae3--------------------------------)
    [## A Complete Step by Step Tutorial on Sentiment Analysis in Keras and Tensorflow'
  prefs: []
  type: TYPE_NORMAL
- en: Complete Working Code for Data Preparation, Deep Learning Model Development,
    and Training the network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-complete-step-by-step-tutorial-on-sentiment-analysis-in-keras-and-tensorflow-ea420cc8913f?source=post_page-----bb9330562ae3--------------------------------)
    [](https://pub.towardsai.net/a-complete-exploratory-data-analysis-in-python-a2148daac072?source=post_page-----bb9330562ae3--------------------------------)
    [## A Complete Exploratory Data Analysis in Python
  prefs: []
  type: TYPE_NORMAL
- en: Data Cleaning, Analysis, Visualization, Feature Selection, Predictive Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/a-complete-exploratory-data-analysis-in-python-a2148daac072?source=post_page-----bb9330562ae3--------------------------------)
    [](/30-very-useful-pandas-functions-for-everyday-data-analysis-tasks-f1eae16409af?source=post_page-----bb9330562ae3--------------------------------)
    [## 30 Very Useful Pandas Functions for Everyday Data Analysis Tasks
  prefs: []
  type: TYPE_NORMAL
- en: Pandas Cheatsheet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/30-very-useful-pandas-functions-for-everyday-data-analysis-tasks-f1eae16409af?source=post_page-----bb9330562ae3--------------------------------)
    [](/how-to-define-custom-layer-activation-function-and-loss-function-in-tensorflow-bdd7e78eb67?source=post_page-----bb9330562ae3--------------------------------)
    [## How to Define Custom Layer, Activation Function, and Loss Function in TensorFlow
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-step explanation and examples with complete code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-define-custom-layer-activation-function-and-loss-function-in-tensorflow-bdd7e78eb67?source=post_page-----bb9330562ae3--------------------------------)
    [](/morphological-operations-for-image-preprocessing-in-opencv-in-detail-15fccd1e5745?source=post_page-----bb9330562ae3--------------------------------)
    [## Morphological Operations for Image Preprocessing in OpenCV, in Detail
  prefs: []
  type: TYPE_NORMAL
- en: Erosion, dilation, opening, closing, morphological gradient, tophat / whitehat,
    and blackhat explained with examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/morphological-operations-for-image-preprocessing-in-opencv-in-detail-15fccd1e5745?source=post_page-----bb9330562ae3--------------------------------)
    [](/anomaly-detection-in-tensorflow-and-keras-using-the-autoencoder-method-5600aca29c50?source=post_page-----bb9330562ae3--------------------------------)
    [## Anomaly Detection in TensorFlow and Keras Using the Autoencoder Method
  prefs: []
  type: TYPE_NORMAL
- en: A cutting-edge unsupervised method for noise removal, dimensionality reduction,
    anomaly detection, and more
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/anomaly-detection-in-tensorflow-and-keras-using-the-autoencoder-method-5600aca29c50?source=post_page-----bb9330562ae3--------------------------------)
  prefs: []
  type: TYPE_NORMAL
