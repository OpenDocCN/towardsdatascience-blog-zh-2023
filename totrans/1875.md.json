["```py\nfrom sklearn.model_selection import train_test_split\ndata_train, data_test, target_train, target_test = train_test_split(\n    data,           # the input array that contains all the input features\n    target,         # the target array that contains the truth\n    test_size=0.25, # the percentage of data attributed to the test set, defaults to 0.25\n    shuffle=True,   # if the data should be randomly split (defaults to True)\n    random_state=42 # this is an optional parameter to be able to have reproducible splitting\n)\n```", "```py\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nscaler = StandardScaler()\nencoder = OneHotEncoder()\n\n# Example of StandardScaler\ndata = np.arange(100)\nscaler.fit(data)                            # scaler 'fits' to the data and stores the mean and variance for later use\nscaled_data = scaler.transform(data)        # apply the actual transformation\nscaled_data = scaler.fit_transform(data)    # do both at once\nother_scaled = scaler.transform(other_data) # note that one can apply the same transformation to other data\n\n# Example of OneHotEncoder\ndata = [['toto'], ['titi'], ['toto'], ['tata']]\nencoder.fit_transform(data).toarray() # fit and transform the column into OneHotEncoded columns\narray([[0., 0., 1.],\n [0., 1., 0.],\n [0., 0., 1.],\n [1., 0., 0.]])\n# The encoder creates 3 new columns, where the first one corresponds to 'toto', 2nd 'titi' and so on\n```", "```py\nfrom sklearn.compose import ColumnTransformer\n\ncategorical_cols = ['gender', 'country']\nnumerical_cols = ['age', 'weight', 'height']\n\npreprocessor = ColumnTransformer(\n     [ # a list of 3-tuple : (name of the preprocessor, the actual preprocessor, list of columns names to apply on)\n         (\"onehot_cat\", OneHotEncoder(), categorical_cols), \n         (\"stdsc_num\", StandardScaler(), numerical_cols),\n     ],\n     remainder=OneHotEncoder(), # the remaining cols that are not identified in cat/num cols will be OneHotEncoded\n)\n```", "```py\nfrom sklearn.compose import make_column_selector\n\nnum_selector = make_column_selector(dtype_exclude=object) # suppose every non-object dtype is numerical\ncat_selector = make_column_selector(dtype_include=object) # suppose every object dtype is categorical\n\npreprocessor = ColumnTransformer(\n    [\n         (\"onehot_cat\", OneHotEncoder(), cat_selector), \n         (\"stdsc_num\", StandardScaler(), num_selector),\n    ],\n    remainder=OneHotEncoder(), # the remaining cols that are not identified in cat/num cols will be OneHotEncoded\n)\n```", "```py\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\n\npipeline = Pipeline(\n    [\n        ('std', StandardScaler()),\n        ('lin_reg', LinearRegression())\n  ]\n)\n```", "```py\npipeline.fit(data_train, target_train) # make the model \"learn\"\ny_predicted = pipeline.transform(data_test) # make a prediction on unseen data\n# or directly compute a score to measure the performance on the test set\npipeline.score(data_test, target_test)\n# to check the training score, we can use\npipeline.score(data_train, target_train)\n```", "```py\nfrom sklearn.model_selection import cross_validate\ncv_results_dict = cross_validate(\n    pipeline,   # our model\n    data,       # the whole input data\n    target,     # the whole target data\n    cv=10,      # number of splits (defaults to 5)\n    return_estimator=True,   # so we can retrieve each fitted pipeline\n    return_train_score=True, # to get the train scores, in addition to the test scores\n)\n```", "```py\nimport numpy as np\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\n# Load the Iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Create models with StandardScaler in a Pipeline\nmodels = {\n    'Logistic Regression': make_pipeline(StandardScaler(), LogisticRegression()),\n    'Decision Tree': make_pipeline(StandardScaler(), DecisionTreeClassifier()),\n    'Random Forest': make_pipeline(StandardScaler(), RandomForestClassifier()),\n    'SVM': make_pipeline(StandardScaler(), SVC()),\n    'K-Nearest Neighbors': make_pipeline(StandardScaler(), KNeighborsClassifier())\n}\n\n# Perform 10-fold cross-validation for each model\nfor model_name, model in models.items():\n    cv_results = cross_validate(\n        model, X, y, \n        cv=KFold(n_splits=10, shuffle=True, random_state=42),\n        return_train_score=True\n    )\n\n    print(f'Model: {model_name}')\n    print('---------------------------')\n    print(f'Test Accuracy: {np.mean(cv_results[\"test_score\"]):.4f} ± {np.std(cv_results[\"test_score\"]):.4f}')\n    print(f'Train Accuracy: {np.mean(cv_results[\"train_score\"]):.4f} ± {np.std(cv_results[\"train_score\"]):.4f}')\n    print('\\n')\n```"]