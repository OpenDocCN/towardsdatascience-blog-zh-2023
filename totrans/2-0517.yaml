- en: 'Cloud-First Data Science: A Modern Approach to Analyzing and Modeling Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/cloud-first-data-science-a-modern-approach-to-analyzing-and-modeling-data-33695041f712](https://towardsdatascience.com/cloud-first-data-science-a-modern-approach-to-analyzing-and-modeling-data-33695041f712)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A guide for utilizing the cloud with every step of the data science workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://bench-5.medium.com/?source=post_page-----33695041f712--------------------------------)[![Ben
    Chamblee](../Images/ef3b7fe41dafe7ddec2dc877387f9f21.png)](https://bench-5.medium.com/?source=post_page-----33695041f712--------------------------------)[](https://towardsdatascience.com/?source=post_page-----33695041f712--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----33695041f712--------------------------------)
    [Ben Chamblee](https://bench-5.medium.com/?source=post_page-----33695041f712--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----33695041f712--------------------------------)
    ·11 min read·Nov 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08cbfcdd8890e8bb71c7f3b3275a16db.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Myriams-Fotos](https://pixabay.com/users/myriams-fotos-1627417/) on
    [Piaxabay](https://pixabay.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Data science is one of the fastest growing industries in the world, utilizing
    modern, cutting-edge technology to improve the way we use data. However, if you’ve
    worked in data science you probably know that one day you will inevitably find
    yourself staring at an Excel sheet. And there’s nothing wrong with Excel, it’s
    just not the kind of tool you would expect to use when working in one of the most
    modern industries.
  prefs: []
  type: TYPE_NORMAL
- en: Many organizations have begun utilizing modern cloud infrastructure but not
    to the full extent. So many data scientists will find themselves pulling data
    from a cloud data warehouse just to train a model on their local system. There’s
    nothing wrong with that too, but what if we could bring the entire data science
    workflow to the cloud? Well, we can!
  prefs: []
  type: TYPE_NORMAL
- en: From data cleaning to model deployment, there’s a cloud-based tool that you
    can use to modernize your workflow. In this article, I’m going to go through each
    step of the data science workflow and show how you can transition it to the cloud
    and provide some examples along the way. Feel free to skip around if you’ve already
    modernized part of your workflow but if you want the 100% cloud data science experience;
    stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Collection and Storage on the Cloud**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chances are you’re already familiar with the benefits of storing data on the
    cloud, but in case you haven’t heard: it’s pretty great! Storing your data on
    the cloud lets you access your data from anywhere with an internet connection,
    integrate it easily with other cloud services, scale your storage capacity to
    as much as you need, create backups for recovery, and many other very helpful
    things.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether or not you need a data warehouse, data lake, or object storage, your
    data will have to live somewhere if you want to deploy it to other applications.
    There are tons of services that offer cloud data storage; some of the more popular
    ones include:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Blob Storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud Storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snowflake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is not even close to the full list of cloud data storage services, but
    if you work in data science there’s a very good chance you’ll work with some,
    if not all of these eventually. Each service and cloud storage type has its strengths
    and weaknesses, so you should just pick whichever one that you think will work
    best for your projects!
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of which service you use for cloud data storage, the process of collecting
    and storing your data has the same general steps. You’ll usually have to make
    an account with the service, create a storage container or bucket, and then you
    should be able to upload your data. Depending on which service you use, this can
    be done through the web interface, command-line tools, SDKs, or APIs.
  prefs: []
  type: TYPE_NORMAL
- en: A best practice when storing data on the cloud is to set permissions and access
    control. This isn’t as relevant if you’re working on a solo project but it is
    crucial if you work on a team. It’s also important to manage your data including
    the structure, metadata, update frequency, and retention. Encryption can also
    ensure your data is secure and private, and creating a backup will protect you
    from losing any progress and improve your data availability!
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Cleaning and Transformation in the Cloud**'
  prefs: []
  type: TYPE_NORMAL
- en: Now that your data is stored in the cloud, it makes sense to keep it there and
    perform all necessary cleaning steps in the cloud too! The benefits are pretty
    similar to the ones discussed above; access from anywhere, scalability, easy integration,
    etc. but you also get the added benefit of not having to download your cloud data,
    clean it, and re-upload it. If done correctly the workflow should be pretty seamless!
  prefs: []
  type: TYPE_NORMAL
- en: Here are some examples of tools you can use for cloud data cleaning and transformation,
    I’ll keep them consistent with the same five listed in the section above but remember
    there are many, many other tools at your disposal!
  prefs: []
  type: TYPE_NORMAL
- en: AWS Glue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Data Factory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud Dataflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Hive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snowflake Data Integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some services will make the cleaning process simple by providing a sample of
    your data before and after ETL (Extract, Transform, Load). There are also tools
    that offer a “code-less” experience where sometimes you can just drag and drop
    commands, while others offer a highly customizable, coding experience. You can
    pick and choose which you you like based on your preferences! Generally, these
    tools can work with multiple cloud storage providers so the whole process is very
    flexible.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of my favorite things about online data transformation tools is the visual
    component, most tools will have an interface that shows you the data transformation
    process step-by-step like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/854fadb40d0eb502cf13602ca2274eda.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo from Google Cloud Dataflow [Documentation](https://cloud.google.com/dataflow/docs/guides/job-graph)
    (CC BY 4.0)
  prefs: []
  type: TYPE_NORMAL
- en: In my experience, it is substantially easier to explain how data is being transformed
    when presenting to a manager or an audience when you have a visual like this.
    Showing and explaining raw python code can be quite difficult but its easy to
    walk through each step and explain what is happening.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you were doing this process in Snowflake it would look something like this:
    Once your account is set up and your data is loaded into Snowflake, explore your
    dataset — you could look at the raw data or use their Snowsight tool to get a
    better look of your data’s structure and features. Once you know how your data
    looks you can easily clean it up using built-in tools or SQL. Then depending on
    your project needs you can also add new columns for further analysis. If you’re
    doing sentiment analysis on customer reviews, for example you could write a quick
    script like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then once the data is cleaned and/or transformed, you can save it as a new dataset
    and move on to the next step!
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud-Based Data Analysis**'
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ve got our data uploaded, cleaned, and ready for analysis! We’ve got
    a lot of options for analysis from notebooks to dashboards, but no matter what
    your preferences are; there is an option that keeps your workflow in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are your options if you stay within the ecosystems of the five cloud service
    providers we’ve been referencing:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS Redshift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Synapse Analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google BigQuery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snowflake Data Warehousing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many other tools out there but these five should get the job done,
    especially if your cleaned data is already residing on their respective platform.
    Depending on which tool you decide to use, you’ll have a wide range of capabilities
    for data analysis and just like with cleaning you’ll have many different ways
    of doing it regardless of your proficiency with python or R. As always, you should
    use the tool that you like best and the one that works with your project.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the complexity of your project, performing data analysis can be
    pretty simple with any of these tools. For example, in BigQuery you can write
    custom SQL query to analyze your data and in addition to that you’ll be able to
    quickly generate visuals and explore your data further. If you prefer working
    on notebooks you can also send your data directly from BigQuery to a Google Colab
    notebook, analyze it, and if you decide to make any changes you can then send
    it right back as a separate dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Now that your data is analyzed, you probably have a good idea of how you want
    to present it — luckily for you the next step in the process, visualization, can
    also be done fully on the cloud!
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Visualization on the Cloud**'
  prefs: []
  type: TYPE_NORMAL
- en: A theme you might be noticing throughout this article is how easy it is to integrate
    each step of this workflow. We’ve uploaded our data, cleaned it, analyzed it,
    and now we’re ready to visualize all without downloading a single file!
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many tools you can use to create awesome, cloud-based data visualizations.
    Each of the five cloud platforms we’ve been following have their own set of visualization
    tools but here are some other tools that easily integrate with our data management
    systems:'
  prefs: []
  type: TYPE_NORMAL
- en: Tableau Online
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Power BI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qlik Sense
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotly Dash
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can easily create a clean, informative visual for your analysis or create
    an interactive dashboard depending on your needs. Tableau Online, for example,
    also has a great community of creators that share their visualizations for all
    to see. Taking a look at their [Viz of the Day](https://public.tableau.com/app/discover/viz-of-the-day)
    in Tableau Public has been a great source of inspiration for some of my visuals.
  prefs: []
  type: TYPE_NORMAL
- en: The process is pretty straightforward, all you have to do is connect your visualization
    tool of choice to your data storage tool of choice and from there you can create
    amazing visuals all online! These tools will usually have awesome libraries of
    visuals that are informative and visually appealing! You’ll also usually be able
    to interact with the visuals and get real-time updates as your cloud-hosted data
    updates too. If you want, you can also embed your visuals in other web apps or
    sites; the whole process is very customizable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud-Based Machine Learning and Modeling**'
  prefs: []
  type: TYPE_NORMAL
- en: This is probably the area of data science where leveraging cloud computing makes
    the most sense. Training and testing a model can be very demanding for your computer,
    so why not offload that work to a dedicated server instead? This is just one of
    the advantages of cloud-based Machine Learning (ML) and modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud platforms will usually provide pre-built models as well to make it easy
    if you just need a quick model, and if you’re not an ML expert there are AutoML
    services that will make suggestions for you — all without writing a single line
    of code. Of course, for the ML engineers out there, there are also highly customizable
    applications that offer hyperparameter tuning, and MLOps capabilities to ensure
    your model is built to your exact specifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few examples of cloud tools you can use for machine learning and
    modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud AI Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databricks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubeflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you like to write the code for your own models, the process with SageMaker
    looks something like this. First you’ll load your data from S3, then create a
    SageMaker notebook to write code. SageMaker has built in algorithms like XGBoost,
    but you can also create custom models using classic Scikit-Learn libraries. You
    can specify your model’s algorithm and tune hyperparameters in your code. When
    you’re ready to train and test your model, SageMaker will handle all of the computing
    resources — which will save you a ton of time. One of the coolest parts about
    this whole process is once you’re done, you can make the trained model accessible
    via API and use the model wherever you want!
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t like to write code or want a tool to suggest a model for you, Azure
    Machine Learning has a tool called Azure AutoML that will work great for you.
    Similar to the example above, you’ll load your data from your respective data
    warehouse, but once you get to the modeling portion you can either have Azure
    suggest a model for you, or pick from their library of algorithms to create your
    own. The process is highly customizable, but can still be done with a no-code
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: However you want to create a machine learning model there’s likely a cloud-based
    tool out there for you. There’s also a very good chance that no matter which tool
    you use, it will integrate with the other tools we’ve discussed in earlier steps
    of the process.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deploying Data Science Solutions on the Cloud**'
  prefs: []
  type: TYPE_NORMAL
- en: Now we’re trained our models, we can utilize the cloud to transform our insights
    and algorithms into real-world solutions. Here, you can really see the benefit
    of using the cloud because your solutions will be accessible from anywhere and
    can scale at massive levels to answer all kinds of questions. Using the cloud
    also means your trained model can continue to learn and improve and as you get
    results from your model, you can upload those results, clean them, and visualize
    them using the methods we’ve discussed throughout this article.
  prefs: []
  type: TYPE_NORMAL
- en: As you can probable tell, I really enjoy this cloud workflow and how nicely
    everything integrates together.
  prefs: []
  type: TYPE_NORMAL
- en: 'For deploying your model you’ve got plenty of options, but here are a few to
    consider:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes (AWS, Azure, GCP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Lambda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure App Service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud App Engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heroku
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on what your data science solution is, your tool of choice will vary.
    If, for example, you’re designing a web app that uses a model you’ve built online,
    you can use Kubernetes to deploy and improve your solution on that web app. The
    process would start by packaging your app and model into a Docker contain, which
    is executable package that includes everything you need to run your app. You can
    store that container in a container registry that Kubernetes can access (GCP,
    AWS, and Azure will all have one!) Then you can create a cluster in the cloud
    and write a simple configuration file (YAML) to tell Kubernetes how to run your
    web app from the Docker container.
  prefs: []
  type: TYPE_NORMAL
- en: Once everything is configured to your liking, you can start running your web
    app to as many users as you need! You can get real-time feedback and analytics
    about your model, all of which can be stored in the cloud and visualized. Whichever
    service you use for Kubernetes will be able to run smoothly and handle all the
    computing, and you’ll be able to add additional data to your model to continually
    improve it!
  prefs: []
  type: TYPE_NORMAL
- en: That was a lot—this is definitely the most complicated step in the process,
    unfortunately. If you’d like a visual walkthrough you can also check out [this
    youtube video](https://www.youtube.com/watch?v=DQRNt8Diyw4) from mildlyoverfitted
    who does a great job showing the deployment process through Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Other Considerations**'
  prefs: []
  type: TYPE_NORMAL
- en: While I have just spent this entire article going over all the benefits of moving
    your workflow to the cloud, there are a few things you should also keep in mind
    before you fully commit to 100% cloud-based tools.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to know is that it can get VERY expensive if you have to rely
    on cloud-based tools for your entire workflow. While there are plenty of free
    options out there, if you scale up your work it will only be a matter of time
    until you get hit with a large bill for storage or computing power. There’s also
    the issue of dependency on internet connectivity where your workflow relies heavily
    on the quality of your internet. Some systems will also have outages which will
    disrupt your work and productivity. Its important to diversify your skills in
    the event that a system or service changes or abruptly ends, that way you can
    still do you work.
  prefs: []
  type: TYPE_NORMAL
- en: These downsides don’t apply to all the cloud-based tools however, its just important
    to keep these things in mind so you can make an informed decision about how you
    want to do you work.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve gone from uploading our data to deploying a machine learning model while
    using modern cloud-based tools the whole way. I think that’s pretty neat! I hope
    reading this has inspired you to modernize some or all of your workflow or if
    not I hope I’ve at least shown you that it is possible to do data science all
    on the cloud. A modern workflow for a modern industry — it just feels right!
  prefs: []
  type: TYPE_NORMAL
- en: 'This article is just an overview of what’s possible, if you’re interested in
    reading more about this topic here are a few resources you can check out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[An overview of all the data science tools on Google Cloud Platform](https://cloud.google.com/data-science)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Snowflake’s Data Science Guide](https://www.snowflake.com/guides/data-science)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Some practical reasons to move to the cloud](https://www.qubole.com/blog/5-reasons-to-move-data-science-to-the-cloud)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '**Want More From Me?**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow me on Medium](https://bench-5.medium.com/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Support my writing by signing up for Medium using [my referral link](https://bench-5.medium.com/membership)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect with me on [LinkedIn](https://www.linkedin.com/in/benchamblee/) and
    [Twitter](https://twitter.com/Bench__5)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check out my Data Science with Python guide on [benchamblee.blog](https://www.benchamblee.blog/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
