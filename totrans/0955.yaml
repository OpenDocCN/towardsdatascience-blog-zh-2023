- en: 'GenAI for Better NLP Systems I: A Tool for Generating Synthetic Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/genai-for-better-nlp-systems-i-a-tool-for-generating-synthetic-data-4b862ef3f88a](https://towardsdatascience.com/genai-for-better-nlp-systems-i-a-tool-for-generating-synthetic-data-4b862ef3f88a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An experiment on using GenAI for generating and augmenting synthetic data using
    Python for Prompt Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)[![Nabanita
    Roy](../Images/83ab7766a28c79371ebf9517e1f273d2.png)](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)
    [Nabanita Roy](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)
    ¬∑7 min read¬∑Sep 29, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c632c9d6ff85d169081e0ad66f249ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [SR](https://unsplash.com/@lemonmelon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: One of the key challenges of Machine Learning(ML) is unbalanced data and the
    biases they introduce in ML models. With the advent of powerful Generative AI
    (GenAI) models, we can augment imbalanced training data with synthetic data easily,
    especially for Natural Language Processing(NLP) tasks. As a result, we can train
    models using classic ML algorithms for better performances in scenarios where
    Deep Learning models or directly using LLMs are not an option for reasons like
    the cost of computation, memory, availability of infrastructure or model explainability.
    Besides, despite the great efficacy shown by LLMs, we still don‚Äôt fully trust
    them. However, we can use LLMs to aid our work as data professionals and overcome
    roadblocks in building NLP systems.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I have demonstrated how we can improve model performance for
    minority classes in imbalanced datasets using GenAI and Python to generate synthetic
    data and how we can iteratively engineer prompts to generate the desired outcome.
  prefs: []
  type: TYPE_NORMAL
- en: The Balancing Act
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/086715743b0108b8c9033b978701d277.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Tingey Injury Law Firm](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Long story short, ML models need enough examples to learn patterns and predict
    accurately. If the data contains fewer examples, then the model does not generalise
    and consequently, perform well. In such scenarios, the model can overfit for classes
    that have more examples and underfit for classes with fewer examples. To tackle
    unbalanced data we traditionally use statistical sampling methods like over or
    under-sampling, typically using [SMOTE](https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/).
  prefs: []
  type: TYPE_NORMAL
- en: Why balancing data in NLP is hard?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several examples of classification tasks in NLP where data is imbalanced
    and we have to resort to under-sampling to overcome this challenge. Information
    loss is the fundamental problem with under-sampling. While SMOTE implements efficient
    over-sampling strategies for numerical datasets, it is not suitable for texts
    or any sort of vectorized representations or embeddings for texts. This is because
    numeric datasets can easily be replicated with random sampling strategies, drawing
    from a combination of features‚Äô probability distributions. For texts, it is not
    as simple to generate the embedded patterns since it is difficult to capture syntactical
    and semantic diversity using these methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative AI: A Modern Tool for Synthetic Data for NLP'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This year, the power of GenAI is unleashed and prompt engineering is revolutionizing
    the way we work, and AI leaders are rethinking their operational strategies. Here
    is a demonstration of how GenAI can fuel NLP systems with synthetic data using
    prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use-Case: Enriching Emotions Dataset with Synthetic Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Background**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In my previous blogs on comparing [Multiclass Text Classifications with Keras
    Embedding Layer vs Word2vec embeddings](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0)
    and [Fine-tuning ANNs using KerasTuner](/how-i-improved-the-performance-of-a-multiclass-text-classifier-using-kerastune-and-other-basic-data-161a22625009),
    I presented models that performed well on the emotions ‚Äî ‚Äòjoy‚Äô and ‚Äòsadness‚Äô,
    as target classes. This is because they had a sizable number of samples among
    all the other emotions in the dataset. Below is the distribution of the train
    data where clearly the number of examples for ‚Äòjoy‚Äô and ‚Äòsadness‚Äô (majority classes)
    are higher than ‚Äòanger‚Äô, ‚Äòlove‚Äô, ‚Äòsurprise‚Äô, and ‚Äòfear‚Äô (minority classes):'
  prefs: []
  type: TYPE_NORMAL
- en: From the above figure, we can conclude that this data is highly imbalanced for
    emotions apart from ‚Äòjoy‚Äô and ‚Äòsadness‚Äô. ‚Äòanger‚Äô and ‚Äòfear‚Äô have a similar number
    of samples and the performance of the trained models on this data was slightly
    poorer than ‚Äòjoy‚Äô and ‚Äòsadness‚Äô. The poorest performance was observed for ‚Äòlove‚Äô
    and ‚Äòsurprise‚Äô with 78% and 71% f1-scores for the best-chosen model from the exercise.
    Below, is the results on the test dataset for reference.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a83bd0a20bcd5a2e7e2949e2a61fe7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Confusion Matrix for Model Performance without Augmented Synthetic Data | Image
    source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: The best-performing model (notebook [here](https://github.com/royn5618/Medium_Blog_Codes/blob/master/Emotion%20Detection/EmotionClassifier_KerasTuner_2.ipynb))
    was a fine-tuned model *without using word2vec* and directly passing the token
    representation to the Keras embedding layer. Evidently, the precision for ‚Äòsurprise‚Äô
    (class 5) is 84%, which is good but recall is 62%, which is quite low. Higher
    precision means that when the classifier predicts an emotion, it is more likely
    to be correct. In other words, it minimizes false positives. On the other hand,
    higher recall means that the classifier is better at capturing all instances of
    the target emotion, minimizing false negatives. A 62% recall means that 62% of
    all the texts with the emotion ‚Äòsurprise‚Äô were correctly identified. In real-world
    applications, achieving a balance between precision and recall is recommended,
    prioritizing one over another depending on business objectives. Here, the scores
    for precision and recall is unbalanced.
  prefs: []
  type: TYPE_NORMAL
- en: '**Objective: Increase recall for the emotion ‚Äòsurprise‚Äô, which is a minor class
    in this imbalanced dataset, using synthetic data generated by GPT.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. Prompt Engineering and Synthetic Data Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this experiment, I will be re-writing the 572 samples for ‚Äòsurprise‚Äô in the
    training data and augment them to increase the total number of samples to 1200
    for that emotion.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt Engineering:**'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering is the art of crafting instructions for an AI model that
    allows it to produce the most accurate response. In this use case, I will next
    be crafting a prompt that enables me to generate synthetic data. Not just that,
    I will provide examples and an output structure as well so that I can easily parse
    the synthetic data into a Pandas data frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'This experiment uses the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few lessons learnt about the process of prompting for this experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: This is the 15th-ish version of the prompt structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I asked to ‚Äúgenerate X samples‚Äù instead of re-writing. That did not work very
    well since it started being repetitive in the syntactic style of the texts, especially
    after generating the first 15-20 of the synthetic samples, the model stopped generating
    a variety of sentence structures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note how I instructed GPT-4 to remove punctuations and lowercase as well. This
    is useful for text cleaning tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I tried saying ‚ÄúGenerate text samples with emotion ‚Äòfear‚Äô‚Äù and GPT-4 conveniently
    added the word ‚Äòfear‚Äô in all samples. I had to use ‚Äòunderlying‚Äô, or ‚Äòembedded‚Äô
    emotion in the prompt while I was trying to generate the samples instead of rewriting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I tried adding ‚Äòusing synonyms‚Äô in the instructions already but in some texts,
    the synonyms did not match well. So I removed that and added the clause on preserving
    the emotion because, in some texts, the emotion in content appeared vague to me.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Addressing Token Limits and Full Code**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like everyone, I have the same token limits for prompting GPT-4\. Here‚Äôs what
    I have done. I *chunk*-ified the dataset and ran through all the data points.
    Here is the full code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few lessons learnt about the process of prompting for this part
    of the experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: I ran into ‚Äú503 ‚Äî The engine is currently overloaded, please try again later‚Äù
    error without giving the code some resting time using time.sleep()
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I instructed the model to produce ‚Äò;‚Äô separated samples so that I can parse
    them easily into a data frame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I placed the above instruction in the first line of the prompt. ‚ÄúDo not use
    any punctuation‚Äù instruction was placed where it is. This resulted into generating
    the samples without any separation at all. (Rookie mistake I know in üôÑ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that I straightened the synthetic data generation and augmentation, here
    is the updated result of the tuned classifier (trained [here](https://github.com/royn5618/Medium_Blog_Codes/blob/master/GenAI_4_NLP_Systems/EmotionClassifier_KerasTuner_2.ipynb))
    now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bea951e54bd2fe9008dda7e22f5f0147.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Confusion Matrix for Model Performance with Augmented Synthetic Data | Image
    source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, the recall has increased from 62% to 79% for the emotion ‚Äòsurprise‚Äô
    (Class 5), but the precision decreased from 84% to 72%. However, having a balance
    in the precision and recall scores is important, which is definitely better than
    the model without the synthetic data. In fact, the f1-score has improved from
    71% to 75% using this strategy. The overall model performance remains the same
    but is likely to be boosted on augmenting other classes as well ‚Äî that‚Äôs for a
    future experimnetation.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was a brief demonstration of how we can use GenAI to generate synthetic
    data for NLP-based use cases which otherwise could be a more complex task.
  prefs: []
  type: TYPE_NORMAL
- en: There is a crucial challenge here, however. GPT-4 produces data that might induce
    patterns or biases in the texts, leading models to overfit on the patterns in
    synthetic data and consequently impede the classifier‚Äôs performance. Therefore,
    quality testing of the generated data and the overall performance is essential.
  prefs: []
  type: TYPE_NORMAL
- en: Despite this limitation, Generative AI is incredibly helpful in expediting the
    generation of synthetic data for balancing classes in imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Hope you enjoyed this blog üôÇ. Here are the [Google Colab Notebooks](https://github.com/royn5618/Medium_Blog_Codes/tree/master/GenAI_4_NLP_Systems)
    on my GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: '*Thanks for visiting!*'
  prefs: []
  type: TYPE_NORMAL
- en: '**My Links:** [Medium](https://medium.com/@nroy0110) | [LinkedIn](https://www.linkedin.com/in/nabanita-roy/)
    | [GitHub](https://github.com/royn5618)'
  prefs: []
  type: TYPE_NORMAL
