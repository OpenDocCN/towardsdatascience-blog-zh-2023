- en: ChatGPT — Handle With Care
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT — 小心使用
- en: 原文：[https://towardsdatascience.com/chat-gpt3-handle-with-care-8b6634781608](https://towardsdatascience.com/chat-gpt3-handle-with-care-8b6634781608)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/chat-gpt3-handle-with-care-8b6634781608](https://towardsdatascience.com/chat-gpt3-handle-with-care-8b6634781608)
- en: Behind the Hype — Understanding what ChatGPT can do and what it cannot do, is
    critical to make the most of the technology. A recent research paper from the
    Center of Artificial Intelligence Research of Hong Kong University weights the
    limits and strengths of the OpenAi algorithm.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解 ChatGPT 的实际能力和限制对于充分利用这一技术至关重要。香港大学人工智能研究中心的最新研究论文权衡了 OpenAi 算法的局限性和优势。
- en: '[](https://misclassified.medium.com/?source=post_page-----8b6634781608--------------------------------)[![Giovanni
    Bruner](../Images/95283ba8cd3e700cdcb592975c501c47.png)](https://misclassified.medium.com/?source=post_page-----8b6634781608--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b6634781608--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b6634781608--------------------------------)
    [Giovanni Bruner](https://misclassified.medium.com/?source=post_page-----8b6634781608--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://misclassified.medium.com/?source=post_page-----8b6634781608--------------------------------)[![Giovanni
    Bruner](../Images/95283ba8cd3e700cdcb592975c501c47.png)](https://misclassified.medium.com/?source=post_page-----8b6634781608--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b6634781608--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b6634781608--------------------------------)
    [Giovanni Bruner](https://misclassified.medium.com/?source=post_page-----8b6634781608--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b6634781608--------------------------------)
    ·6 min read·Mar 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b6634781608--------------------------------)
    ·阅读时间 6 分钟·2023年3月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ffc519eec3fa34c4ea5ccb1ca9d769bb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffc519eec3fa34c4ea5ccb1ca9d769bb.png)'
- en: Photo by [Possessed Photography](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Possessed Photography](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'First, it came the language model. The intuition was easy: the next word in
    a sequence of words can be modeled with a probability distribution and is heavily
    dependent on the previous words. Words are part of a vocabulary, a limited corpus
    (170,000 tokens in the English vocabulary). Each word has a limited amount of
    meanings. A sequence of words, following a slow-changing, inner set of metadata:
    a grammar. Which is a predictable structure. You can expect a verb to be followed
    by a noun and not by another verb. Grammar and Meaning, act as constraints to
    limit the amount of randomness in the next word prediction. Which is arguably
    an easier task than predicting the next day''s share price value of a thousand
    companies. Plus, language models are by nature auto-regressive, the next word
    prediction depends on the previous words, and there are not that many latent,
    unobservable, variables to account for.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，出现了语言模型。直观上很简单：一个词序列中的下一个词可以用概率分布建模，并且严重依赖于前面的词。词汇是有限的语料库的一部分（英语词汇中有 170,000
    个标记）。每个词的含义是有限的。词序列遵循缓慢变化的内部元数据集：语法。这是一个可预测的结构。你可以期待一个动词后面跟着名词，而不是另一个动词。语法和意义，作为限制，限制了下一个词预测中的随机性。这无疑比预测一千家公司第二天的股票价格要容易得多。此外，语言模型本质上是自回归的，下一个词的预测依赖于前面的词，而且需要考虑的潜在不可观测的变量也不多。
- en: For all those reasons language models are amenable to pre-trained models and
    **transfer learning**. The key feature unlocking the new AI revolution. Transfer
    Learning means that you can use a model pre-trained by somebody else, say on 20
    GB of Wikipedia articles, without having to retrain it with your own data, just
    by making a few adjustments to fit to your problem.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正因为如此，语言模型适合使用预训练模型和**迁移学习**。这是解锁新 AI 革命的关键特性。迁移学习意味着你可以使用别人预训练的模型，比如在 20 GB
    的维基百科文章上训练的模型，而无需用自己的数据重新训练，只需进行少量调整以适应你的问题。
- en: How is that possible? Well, your language problem is unlikely to require the
    use of grammar and a vocabulary that is completely different from what you can
    find on Wikipedia. Transfer learning started a new AI summer just when people
    started arguing about a second AI winter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么可能呢？好吧，你的语言问题不太可能需要使用与维基百科上完全不同的语法和词汇。迁移学习在人们开始争论第二次人工智能寒冬的时候，开启了新的人工智能夏季。
- en: Pre-trained models became larger and faster, with performance improving alongside
    the increase in the number of parameters and the amount of data used. It was empirically
    found that the performance of language models would scale when the size of those
    models increased. Up to an upper bound of computational power to be accounted
    for. Computer chips are as capable as they get. Something had to happen for language
    models to keep growing. A way to efficiently parallelize training across many
    machines is a no-nonsense way. The advent of Transformers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型变得更大、更快，随着参数数量和使用的数据量的增加，性能也得到了提升。经验发现，语言模型的性能会随着模型大小的增加而提升，直到达到计算能力的上限。计算机芯片已尽可能地强大。为了让语言模型持续增长，必须发生一些事情。高效地在多台机器上并行训练的方式是显而易见的。变压器的出现。
- en: '![](../Images/d6f7b0f4625c887b78cb0cf6d929582d.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6f7b0f4625c887b78cb0cf6d929582d.png)'
- en: Photo by [Aditya Vyas](https://unsplash.com/@aditya1702?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Aditya Vyas](https://unsplash.com/@aditya1702?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The Transformers Language Models, released by a Google Brain team, nailed an
    impressive series of ingenious improvements to traditional language sequence models.
    The crux of it was an extensive usage of Multi Head Self-Attention and a model
    architecture designed to run on several concurrent GPUs. The Attention Mechanism
    massively improves the task of predicting the next word in a language model, spreading
    information from all the words in a sequence in a more efficient way than traditional
    recurrent neural networks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由Google Brain团队发布的变压器语言模型，在传统语言序列模型上进行了一系列令人印象深刻的创新改进。其核心是广泛使用多头自注意力机制和设计为在多个并行GPU上运行的模型架构。注意力机制大大改善了语言模型中预测下一个词的任务，以比传统递归神经网络更高效的方式传播序列中所有词汇的信息。
- en: Transformers allowed the pre-training of large, very large, language models.
    That went from the 1.5 Billion parameters of GPT 2, released in 2019 to the 175
    Billion parameters of GPT3 in 2020\. Paving the way to the phenomenal release
    of ChatGPT in 2022 and the era of Large Language Models (LLM)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器使得大规模、非常大规模的语言模型的预训练成为可能。从2019年的GPT 2的15亿参数，到2020年的GPT3的1750亿参数。铺平了2022年ChatGPT的惊人发布以及大型语言模型（LLM）时代的道路。
- en: ChatGPT excels at many things, but beware of hallucinations.
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT在许多方面表现出色，但请注意幻觉。
- en: This was a rather long introduction, but important to put things in context.
    Language models are not black magic, nor Artificial Generic Intelligence. They
    will not overtake humanity any time soon. They are incredibly useful tools, excelling
    at predicting the next word in a sequence. As with any tool invented by humans,
    they can cause damage if we don’t read the manual and the fine print.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这虽然是一个相当长的引言，但在放置背景时非常重要。语言模型不是黑魔法，也不是人工通用智能。它们不会在短时间内超越人类。它们是极其有用的工具，在预测序列中的下一个词方面表现出色。像所有人类发明的工具一样，如果我们不阅读说明书和细则，它们可能会造成伤害。
- en: Words are limited and have a limited set of meanings, they come together in
    a predictable way, following a grammar structure. However, information, which
    is how meanings combine, is not finite and is not necessarily predictable. Large
    Language Models like ChatGPT can create new information, completely made up. Which
    in turn generates new narratives of unchecked facts. They have a parametric memory,
    with no access to an external knowledge base. We have seen that they are good
    at probabilities, but they don’t have an inner mechanism to tell a truth from
    a lie. In a few words, they can hallucinate.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇是有限的，具有有限的意义，它们以可预测的方式组合，遵循语法结构。然而，信息，即意义的结合方式，是无限的，不一定是可预测的。像ChatGPT这样的超大语言模型可以生成全新的信息，完全是虚构的。这反过来又生成了未经验证的事实的新叙述。它们具有参数化的记忆，没有访问外部知识库的能力。我们已经看到它们在概率方面表现良好，但它们没有内部机制来分辨真相与谎言。简而言之，它们可能会出现幻觉。
- en: Check out below. Here I’m asking ChatGPT about myself, pretending to be a famous
    Data Scientist.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看下面。我在这里假装自己是一位著名的数据科学家，询问 ChatGPT 关于我自己的问题。
- en: '![](../Images/57af5cbc1bdab36c6b3ebe9cb7aaf809.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57af5cbc1bdab36c6b3ebe9cb7aaf809.png)'
- en: Image by the Author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: ChatGPT gets it right. Possibly I’m famous to my mom, but no more than that.
    But then I get offended and I completely make up extra information about myself.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 没错。可能我在妈妈面前很有名，但仅此而已。但接着我感到被冒犯了，完全编造了关于自己的额外信息。
- en: '![](../Images/71f18f6a68766699eb6b9f439b8f8ab1.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71f18f6a68766699eb6b9f439b8f8ab1.png)'
- en: Image by the Author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Of course, I’m not a Kaggle Grandmaster (I wish I was), yet the AI apologizes
    to me.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我不是 Kaggle 大师（我希望我能成为），但 AI 对我表示歉意。
- en: The sudden popularity of ChatGPT is unprecedented in this field, thanks to an
    interface to interact with, retaining accumulated knowledge. The dialog interface
    uses Reinforcement Learning with Human Feedback (RCHF). Problem is that the accumulated
    knowledge may be grounded on follow-up questions and corrections that are blatantly
    untrue.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 的突然流行在这一领域是前所未有的，这要归功于一个可以交互的界面，保留了积累的知识。对话界面使用了带有人工反馈的强化学习（RCHF）。问题在于，积累的知识可能基于显然不真实的后续问题和纠正。
- en: Yejin Bang, Pascale Fung, and their team of PHDs released a few weeks ago an
    extensive framework to quantitatively evaluate models such as ChatGPT on publicly
    available sets. As a Zero-Shot learner — a model that can answer any question
    without being explicitly fine-tuned for that task — ChatGPT results as State of
    The Art on the majority of tasks. With big improvements in question answering,
    sentiment analysis, and misinformation detection.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Yejin Bang、Pascale Fung 及其博士团队几周前发布了一个广泛的框架，用于定量评估像 ChatGPT 这样的模型在公开可用数据集上的表现。作为一个零样本学习者——一个无需专门调整即可回答任何问题的模型——ChatGPT
    在大多数任务中被评为最先进。其在问答、情感分析和虚假信息检测方面有了大幅提升。
- en: '![](../Images/d5cc89a7d05f863ef5f43b25544e463a.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5cc89a7d05f863ef5f43b25544e463a.png)'
- en: 'Image from paper: [https://arxiv.org/pdf/2302.04023.pdf](https://arxiv.org/pdf/2302.04023.pdf)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自论文： [https://arxiv.org/pdf/2302.04023.pdf](https://arxiv.org/pdf/2302.04023.pdf)
- en: When it comes to reasoning, by far one of the most debated features, researchers
    found that ChatGPT is very good at deductive reasoning and very bad at inductive
    reasoning and at solving Math problems.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 关于推理，这是一个最具争议的特性之一，研究人员发现 ChatGPT 在演绎推理方面表现非常好，但在归纳推理和解决数学问题方面表现非常差。
- en: Deductive reasoning takes you from general premises to a specific conclusion
    and works well when the premises have enough information to anchor you to the
    solution. The algorithm was found to have superior performance in those reasoning
    tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 演绎推理是从一般前提出发得出具体结论的过程，当前提包含足够的信息来引导你找到解决方案时效果良好。研究发现该算法在这些推理任务中表现优越。
- en: '![](../Images/000b77ef38e431bf1ea48196cca9645d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/000b77ef38e431bf1ea48196cca9645d.png)'
- en: 'The image was taken from Pascale Fung Youtube Video: [https://www.youtube.com/watch?v=ORoTJZcLXek](https://www.youtube.com/watch?v=ORoTJZcLXek)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 Pascale Fung 的 YouTube 视频：[https://www.youtube.com/watch?v=ORoTJZcLXek](https://www.youtube.com/watch?v=ORoTJZcLXek)
- en: Induction is the inverse process. It’s about drawing information from data to
    infer a general conclusion. Whilst deductive thinking is the intellectual process
    you follow when you have to test a theory, inductive thinking is what takes you
    to frame a theory. In other words, you can expect ChatGPT to come up with some
    sample data given a lot of detailed premises, but don’t expect it to come up with
    a generalized rule given some sample data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳是逆向过程。它是从数据中提取信息以推断出一个普遍结论。演绎思维则是在你需要检验一个理论时所遵循的智力过程，而归纳思维则是帮助你形成一个理论的过程。换句话说，给定大量详细前提，你可以期望
    ChatGPT 提出一些样本数据，但不要期望它根据一些样本数据提出一个通用规则。
- en: In practice, ChatGPT is not capable (at the moment) to draw an idea of the world,
    the same way a human would do.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，ChatGPT 目前还无法像人类那样形成对世界的概念。
- en: References
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '**Yejin Bang at all**, A Multitask, Multilingual, Multimodal Evaluation of
    ChatGPT on Reasoning, Hallucination, and Interactivity, [https://arxiv.org/pdf/2302.04023.pdf](https://arxiv.org/pdf/2302.04023.pdf)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**Yejin Bang 等**，《ChatGPT 在推理、幻想和互动方面的多任务、多语言、多模态评估》，[https://arxiv.org/pdf/2302.04023.pdf](https://arxiv.org/pdf/2302.04023.pdf)'
- en: '**Pascale Fung,** ChatGPT: What It Can and Cannot Do by Prof. Pascale Fung,
    [https://www.youtube.com/watch?v=ORoTJZcLXek](https://www.youtube.com/watch?v=ORoTJZcLXek)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pascale Fung**，ChatGPT: Prof. Pascale Fung 所讲的《ChatGPT 能做什么和不能做什么》，[https://www.youtube.com/watch?v=ORoTJZcLXek](https://www.youtube.com/watch?v=ORoTJZcLXek)'
- en: A**shish Vaswani at all**, Attention is All You Need, [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**阿希什·瓦斯瓦尼**等人，注意力机制就是你所需要的，[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)'
