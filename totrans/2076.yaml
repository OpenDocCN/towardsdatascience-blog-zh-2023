- en: 'The Ultimate Guide to Training BERT from Scratch: The Tokenizer'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'From Text to Tokens: Your Step-by-Step Guide to BERT Tokenization'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dpoulopoulos.medium.com/?source=post_page-----ddf30f124822--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page-----ddf30f124822--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ddf30f124822--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ddf30f124822--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----ddf30f124822--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ddf30f124822--------------------------------)
    ·13 min read·Sep 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e6bf3a97ca45029e42154b1648e0172.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Glen Carrie](https://unsplash.com/@glencarrie?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/oHoBIbDj7lo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: '[Part I](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f),
    [Part III](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5),
    and [Part IV](/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb)
    of this story are now live.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Did you know that the way you tokenize text can make or break your language
    model? Have you ever wanted to tokenize documents in a rare language or a specialized
    domain? Splitting text into tokens, it’s not a chore; *it’s a gateway to transforming
    language into actionable intelligence.* This story will teach you everything you
    need to know about tokenization, not only for BERT but for any LLM out there.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my last story, we talked about BERT, explored its theoretical foundations
    and training mechanisms, and discussed how to fine-tune it and create a questing-answering
    system. Now, as we go further into the intricacies of this groundbreaking model,
    it’s time to spotlight one of the unsung heroes: *tokenization*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----ddf30f124822--------------------------------)
    [## The Ultimate Guide to Training BERT from Scratch: Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: 'Demystifying BERT: The definition and various applications of the model that
    changed the NLP landscape.'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----ddf30f124822--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[Part III](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)
    of this story is now live.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I get it; tokenization might seem like the last boring obstacle between you
    and the thrilling process of training your model. Believe me, I used to think
    the same. But I’m here to tell you that tokenization is not just a “necessary
    evil”— *it’s an art form in its own right.*
  prefs: []
  type: TYPE_NORMAL
- en: In this story, we’ll examine every part of the tokenization pipeline. Some steps
    are trivial (like normalization and pre-processing), while others, like the modeling
    part, are what make each tokenizer unique.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d93f5ab57125747b9d469ab16241500.png)'
  prefs: []
  type: TYPE_IMG
- en: Tokenization pipeline — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**By the time you finish reading this article, you’ll not only understand the
    ins and outs of the BERT tokenizer, but you’ll also be equipped to train it on
    your own data.** And if you’re feeling adventurous, you’ll even have the tools
    to customize this crucial step when training your very own BERT model from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting text into tokens, it’s not a chore; *it’s a gateway to transforming
    language into actionable intelligence.*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'So, why is tokenization so critical? *At its essence, tokenization is a translator;*
    it takes in human language and translates it to the language machines can understand:
    numbers. But there’s a catch: During this translation process, the tokenizer must
    keep a crucial balance, finding the sweet spot between meaning and computational
    efficiency. So, you see, it’s not just about crunching numbers; *it’s about efficiently
    capturing the essence of language in a form that machines can comprehend.*'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our premise let’s start by becoming acquainted with the different
    types of tokenizers. The first hurdle in this journey is settling on a definition
    for what constitutes a ‘word’ — and it might not be as straightforward as what
    you or I typically consider when we discuss language.
  prefs: []
  type: TYPE_NORMAL
- en: '[Learning Rate](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-tokenization)
    is a newsletter for those who are curious about the world of ML and MLOps. If
    you want to learn more about topics like this subscribe [here](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-tokenization).
    You’ll hear from me on the last Sunday of every month with updates and thoughts
    on the latest MLOps news and articles!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Types of Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the modeling step of the tokenization pipeline is the more demanding one,
    it’s crucial to approach it while the day is still young.
  prefs: []
  type: TYPE_NORMAL
- en: There are various approaches to building a translator between human and machine
    language. Let’s explore the three main types and weigh their pros and cons to
    understand why subword tokenization is often the go-to method in today’s NLP landscape.
  prefs: []
  type: TYPE_NORMAL
- en: Word-based Tokenizers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The word-based approach is the simplest, essentially slicing up raw text into
    words based on whitespace or other delimiters like punctuation. Think of Python’s
    `split()` function as a classic example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/000081618b093f1b79fca5182e0f7616.png)'
  prefs: []
  type: TYPE_IMG
- en: Word-based tokenization-Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: However, this method has its limitations. To truly grasp a language in all its
    complexity, you'd need to manage a vast token vocabulary—English alone boasts
    over half a million words. Moreover, this method struggles with inflected forms;
    words like "dog" and "dogs" are treated as completely separate entities. And if
    you choose to limit your vocabulary size, the "unknown" token, a placeholder for
    any word that's not in there, will be all over the place, polluting your data’s
    meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Character-based Tokenizers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the other end of the spectrum, we have character-based tokenizers, which
    break down text into individual characters. This drastically reduces the vocabulary
    size and nearly eliminates the problem of unknown tokens. For example, the English
    language has 26 characters. If you add punctuation and other symbols, you’re looking
    at a vocabulary of a size into the hundreds.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/713c122e1b7e818d35b5662a8d60f028.png)'
  prefs: []
  type: TYPE_IMG
- en: Character-based tokenizers — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'But it comes with a cost: characters in isolation often lack meaningful context,
    and this method can result in cumbersome sequences that are computationally expensive
    to process since the context the model has to keep in its cash grows really fast.'
  prefs: []
  type: TYPE_NORMAL
- en: Subword Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Subword tokenization is the middle ground that often offers the best of both
    worlds. By dissecting rare or complex words into smaller, meaningful units— like
    breaking “annoyingly” into “annoy”, “ing”, and “ly” — this method strikes a balance
    between efficiency and expressiveness. It offers compact vocabularies without
    sacrificing the richness of language, making it particularly useful for languages
    where words can be naturally composed of smaller, meaningful pieces.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ac7506d27e6b2368180e32f8265eb37.png)'
  prefs: []
  type: TYPE_IMG
- en: Subword tokenizers — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the figure, we skip most of the steps in the pipeline for the sake
    of simplicity. In a real-world scenario, most of the time the tokenizer has already
    transformed each word to lower case.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Take the word “annoyingly” as a case in point for the power of subword tokenization.
    When split into three distinct subtokens — “annoy”, “ing”, and “ly”, each fragment
    serves as a mini-lesson in language comprehension for the model.
  prefs: []
  type: TYPE_NORMAL
- en: The first subtoken, “annoy,” instructs the tokenizer about the root verb, offering
    a base layer of meaning. The second subtoken, “ing”, captures a common suffix
    that serves multiple grammatical functions. It could indicate an ongoing action
    (as in “running”), the process or result of an action (as in “building”), or even
    help form nouns (as in “painting”). Finally, the “ly” subtoken is a clue to the
    tokenizer about how adverbs or adjectives are often constructed in English, helping
    the model understand that the word modifies an action or describes a characteristic
    of a noun.
  prefs: []
  type: TYPE_NORMAL
- en: So, why has subword tokenization become the gold standard? *It efficiently navigates
    the trade-offs of its word-based and character-based counterparts.* You get a
    vocabulary with almost zero unknown tokens and a system that’s robust enough to
    capture the nuances of human language, even in its most intricate forms.
  prefs: []
  type: TYPE_NORMAL
- en: BERT uses a subword tokenization method known as “WordPiece”. The rest of this
    story explores how this method works and how you can train it yourself.
  prefs: []
  type: TYPE_NORMAL
- en: The WordPiece Tokenizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, let’s build the WordPiece tokenizer from scratch to understand everything
    that’s going under the hood. **Our approach will be twofold: first, we’ll construct
    a mental framework by employing various illustrations to clarify the concepts.
    Then, we’ll put theory into practice by training our very own tokenizer on a custom
    corpus, utilizing the** `[**tokenizers**](https://huggingface.co/docs/tokenizers/index)`
    **library.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To kick things off, we need a corpus — a dataset of text that our tokenizer
    will learn from. Let’s consider the following paragraph as our starting point:'
  prefs: []
  type: TYPE_NORMAL
- en: The WordPiece algorithm serves a crucial role in the architecture of BERT, a
    state-of-the-art language model. Specifically, WordPiece is responsible for the
    tokenization process, breaking down text into smaller, manageable units or tokens.
    This tokenization step is critical during the pre-training phase of BERT, allowing
    the model to effectively learn the relationships between words or sub-words. By
    using WordPiece for tokenization, BERT can be more flexible in handling various
    linguistic constructs and nuances.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'So, the first step is to count the appearances of each word in our corpus and
    split them into the smallest possible unit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b7254334a16a7d24aa4338cd8b02028.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'In our hands-on exercise, you’ll notice that we initially split words into
    individual characters. But that’s not the whole story: we prepend the symbol `##`
    to characters that originally appeared in the middle of a word, setting them apart
    from those that were initial letters. You’ll see later why this is important.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we collect all the splits, we create our initial tokenizer vocabulary. WordPiece
    starts from the smallest possible vocabulary you could have (that of single characters)
    and keeps expanding it to a limit that we set. How does it do that? I’m glad you
    asked! Let’s see; the next step is to identify each possible pair of characters
    in our vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89c97b95fc626c28d2018327ff3bc78b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to assign a score to each pair. The formula that calculates the
    score goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe50e99ce65ed76fc5baded27f1778b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, if we try to calculate the score of the first pair (`T, ##h`) we get
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe1e2fc19c13459df48f59cf0eb04fa9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same approach, we calculate the score for every pair and identify
    the pair with the highest score. In our case, this is the pair (`##E, ##R`), so
    next, we can add this pair to our vocabulary and update the splits by merging
    these characters. Our new vocabulary has now one more token:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d73957a027ee55c51995a7219113ac8d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Next, we go back to our splits again, identify every possible pair, score it,
    and append the pair with the highest score to our vocabulary. What we’re doing
    here is essentially a form of optimization. By identifying commonly appearing
    pairs and treating them as single units, we make the tokenizer more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'We continue this process of identifying, scoring, and appending high-scoring
    pairs until we hit the target number of tokens we wish to include in our vocabulary.
    So, by setting the desired number of tokens to `60`, we get the following vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: ‘##E’, ‘##P’, ‘##R’, ‘##T’, ‘##a’, ‘##b’, ‘##c’, ‘##d’, ‘##e’, ‘##f’, ‘##g’,
    ‘##h’, ‘##i’, ‘##k’, ‘##l’, ‘##m’, ‘##n’, ‘##o’, ‘##p’, ‘##r’, ‘##s’, ‘##t’, ‘##u’,
    ‘##v’, ‘##w’, ‘##x’, ‘##y’, ‘##z’, ‘,’, ‘-’, ‘.’, ‘B’, ‘S’, ‘T’, ‘W’, ‘a’, ‘b’,
    ‘c’, ‘d’, ‘e’, ‘f’, ‘h’, ‘i’, ‘l’, ‘m’, ’n’, ‘o’, ‘p’, ‘r’, ‘s’, ‘t’, ‘u’, ‘v’,
    ‘w’, ‘##ERT’
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Finally, to calculate the subtokens of a word, we work from the beginning of
    the string, trying to identify the longest possible sequence that appears in our
    vocabulary. So, for example, the word `BERT` is split into the following subwords:
    `B`, `##ERT`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02190ed0c0c2b836b2a91e4de77984c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'As the finishing touch to our tokenizer’s vocabulary, we introduce a set of
    special tokens that serve specific functions within the BERT model and serve as
    linguistic markers. Generally, these special tokens include: `[[PAD], [UNK], [CLS],
    [SEP], [MASK]]`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a quick rundown of what each special token is designed to do:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[PAD]`: Padding token used to equalize the lengths of different input sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[UNK]`: Stands for “unknown” and is used to handle words that are not in the
    vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[CLS]`: Short for “classification”, this token is prepended to the input when
    the model is used for classification tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[SEP]`: Separator token, often used to indicate the end of one sentence and
    the beginning of another in multi-sentence tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[MASK]`: Used in the training process for the BERT model to indicate positions
    where the model should predict a missing word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s it! You now know how WordPiece, BERT’s tokenizer, learns to split each
    work into subwords. Next, let’s train a custom one on our corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Training WordPiece
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a native Greek speaker, I’m particularly excited about the prospect of training
    BERT’s WordPiece tokenizer for the Greek language. And I would strongly encourage
    you to do the same if your native tongue isn’t English. *Localizing these powerful
    models not only contributes to the diversification of NLP technologies but also
    offers a chance to tailor them to the nuances of your own language or dialect.*
  prefs: []
  type: TYPE_NORMAL
- en: But if you are an English speaker, consider honing your tokenizer for a specific
    domain or industry. For example, you could train it on legal documents to capture
    the idiosyncrasies of legal jargon. Or how about gearing it towards understanding
    Python code?
  prefs: []
  type: TYPE_NORMAL
- en: 'In my case, I need to find and download the dataset. These days getting the
    dataset you need is easier than ever, thanks to the Hugging Face Hub. I’ll work
    with the `greek_legal_code` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let’s quickly create our training corpus. For this, we’ll use only the
    first `1000` examples of the `train` split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s get a handle on the pre-trained BERT tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To compare the before and after, let’s see how the original BERT tokenizer
    would split the Greek intro of Wikipedia on [Machine Learning](https://el.wikipedia.org/wiki/%CE%9C%CE%B7%CF%87%CE%B1%CE%BD%CE%B9%CE%BA%CE%AE_%CE%BC%CE%AC%CE%B8%CE%B7%CF%83%CE%B7):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown below. In total, we get 274 different tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s train the tokenizer to understand Greek better. We’ll use a vocabulary
    of `50000` words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, repeat the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are amazing; we reduced the number of tokens to `67`, while the
    tokenizer seems to understand the nuances of the Greek language better now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: And there you have it! You’re now fully equipped to train a tokenizer tailored
    to your own corpus. But don’t stop there — feel free to experiment with different
    algorithms altogether! Want to switch lanes and explore another approach? You
    can easily load the GPT tokenizer, which employs the Byte Pair Encoding (BPE)
    algorithm instead of WordPiece. The sky really is the limit when it comes to fine-tuning
    and customizing your tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What a journey it’s been! From diving into the intricate details of tokenization
    to building our very own WordPiece tokenizer, we’ve covered a lot of ground.
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve followed along, you’re now equipped not just with theoretical knowledge
    but with the hands-on experience to create a tokenizer tailored to your needs.
    Maybe you’ll adapt it for your native language, perhaps for an industry-specific
    application, or even for something as intricate as source code analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the principles we’ve explored here extend beyond any single language
    or domain. They’re the foundations upon which all Natural Language Processing
    tasks are built. By understanding and mastering them, you’re setting yourself
    up for success in a field that’s only going to become more central to our lives.
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to gear up for the next exciting phase — preparing your dataset for
    model training. Trust me, if you found tokenization enthralling, dataset preparation
    will be equally eye-opening. This will be where all the pieces we’ve assembled
    — the tokenizer, the special tokens, and your domain or language-specific nuances
    — come together to fuel the BERT model’s learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My name is [Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-tokenization),
    and I’m a machine learning engineer working for [HPE](https://www.hpe.com/us/en/home.html).
    I have designed and implemented AI and software solutions for major clients such
    as the European Commission, IMF, the European Central Bank, IKEA, Roblox and others.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in reading more posts about Machine Learning, Deep Learning,
    Data Science, and DataOps, follow me on [Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow),
    [LinkedIn](https://www.linkedin.com/in/dpoulopoulos/), or [@james2pl](https://twitter.com/james2pl)
    on Twitter.
  prefs: []
  type: TYPE_NORMAL
- en: Opinions expressed are solely my own and do not express the views or opinions
    of my employer.
  prefs: []
  type: TYPE_NORMAL
