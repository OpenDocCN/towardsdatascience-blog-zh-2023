["```py\nimport polars as pl\ncsv_path = \"./youtube/GBvideos.csv\"\n\npl.read_csv(csv_path).with_columns(\n    # Original date is in string format like 17.01.01\n    pl.col(\"trending_date\").str.to_date(format=\"%y.%d.%m\")\n).filter(pl.col(\"trending_date\").dt.year() == 2018).with_columns(\n    pl.col(\"views\")\n    .rank(descending=True)\n    # Rank is calculated over a month \n    .over(pl.col(\"trending_date\").dt.month())\n    .alias(\"monthly_rank\")\n).filter(\n    pl.col(\"monthly_rank\") == 1\n).select(\n    pl.col(\"trending_date\"), pl.col(\"title\"), pl.col(\"channel_title\"), pl.col(\"views\")\n).write_parquet(\n    \"top_monthly_videos.parquet\"\n)\n```", "```py\ndef process_date(df, date_column, format):\n    result = df.with_columns(pl.col(date_column).str.to_date(format))\n    return result\n\ndef filter_year(df, date_column, year):\n    result = df.filter(pl.col(date_column).dt.year() == year)\n    return result\n\ndef get_first_by_month(df, date_column, metric):\n    result = df.with_columns(\n        pl.col(metric)\n        .rank(method=\"ordinal\", descending=True)\n        .over(pl.col(date_column).dt.month())\n        .alias(\"rank\")\n    ).filter(pl.col(\"rank\") == 1)\n\n    return result\n\ndef select_data_to_write(df, columns):\n    result = df.select([pl.col(c) for c in columns])\n    return result\n```", "```py\n(\n    pl.read_csv(csv_path)\n    .pipe(process_date, date_column=\"trending_date\", format=\"%y.%d.%m\")\n    .pipe(filter_year, date_column=\"trending_date\", year=2018)\n    .pipe(get_first_by_month, date_column=\"trending_date\", metric=\"views\")\n    .pipe(\n        select_data_to_write,\n        columns=[\"trending_date\", \"title\", \"channel_title\", \"views\"],\n    )\n).write_parquet(\"top_monthly_videos.parquet\")\n```", "```py\n(\n    pl.scan_csv(csv_path).pipe(process_date, date_column=\"trending_date\", format=\"%y.%d.%m\")\n    .pipe(filter_year, date_column=\"trending_date\", year=2018)\n    .pipe(get_first_by_month, date_column=\"trending_date\", metric=\"views\")\n    .pipe(\n        select_data_to_write,\n        columns=[\"trending_date\", \"title\", \"channel_title\", \"views\"],\n    )\n).collect().write_parquet(\"top_monthly_videos.parquet\")\n```", "```py\nimport yaml\n\n# Read config\nwith open(\"pipe_config.yaml\", \"r\") as file:\n    pipe_config = yaml.safe_load(file)\n```", "```py\ndata_path: \"./youtube/GBvideos.csv\"\ncategory_map_path: \"./youtube/GB_category_id.json\"\n```", "```py\ndef read_category_mappings(path: str) -> Dict[int, str]:\n    with open(path, \"r\") as f:\n        categories = json.load(f)\n\n    id_to_category = {}\n    for c in categories[\"items\"]:\n        id_to_category[int(c[\"id\"])] = c[\"snippet\"][\"title\"]\n\n    return id_to_category\n```", "```py\n# Create mapping\nid_to_category = read_category_mappings(pipe_config[\"category_map_path\"])\ncol_mappings = {\"category_id\": id_to_category}\n\n# Pipeline\noutput_data = pl.scan_csv(pipe_config[\"data_path\"]).collect()\n```", "```py\n# Pre-processing config\ndate_column_format:\n  trending_date: \"%y.%d.%m\"\n  publish_time: \"%Y-%m-%dT%H:%M:%S%.fZ\"\n```", "```py\nslow = df.with_columns(\n    # Process dates\n    pl.col(\"trending_date\").str.to_date(\"%y.%d.%m\"),\n    pl.col(\"publish_time\").str.to_date(\"%Y-%m-%dT%H:%M:%S%.fZ\"),\n).with_columns(\n    # Then process category\n    pl.col(\"category_id\").map_dict(id_to_category)\n)\n\nfast = df.with_columns(\n    # Process all together\n    pl.col(\"trending_date\").str.to_date(\"%y.%d.%m\"),\n    pl.col(\"publish_time\").str.to_date(\"%Y-%m-%dT%H:%M:%S%.fZ\"),\n    pl.col(\"category_id\").map_dict(id_to_category)\n)\n```", "```py\ndef parse_dates(date_cols: Dict[str, str]) -> List[pl.Expr]:\n    expressions = []\n    for date_col, fmt in date_cols.items():\n        expressions.append(pl.col(date_col).str.to_date(format=fmt))\n\n    return expressions\n\ndef map_dict_columns(\n    mapping_cols: Dict[str, Dict[str | int, str | int]]\n) -> List[pl.Expr]:\n    expressions = []\n    for col, mapping in mapping_cols.items():\n        expressions.append(pl.col(col).map_dict(mapping))\n    return expressions\n\ndef clean_data(\n    df: pl.DataFrame,\n    date_cols_config: Dict[str, str],\n    mapping_cols_config: Dict[str, Dict[str | int, str | int]],\n) -> pl.DataFrame:\n    parse_dates_expressions = parse_dates(date_cols=date_cols_config)\n    mapping_expressions = map_dict_columns(mapping_cols_config)\n\n    df = df.with_columns(parse_dates_expressions + mapping_expressions)\n    return df\n```", "```py\n# Create mapping\nid_to_category = read_category_mappings(pipe_config[\"category_map_path\"])\ncol_mappings = {\"category_id\": id_to_category}\n\n# Read in configs\ndate_column_format = pipe_config[\"date_column_format\"]\n\n# Pipeline\noutput_data = pl.scan_csv(pipe_config[\"data_path\"]).pipe(\n    clean_data, date_column_format, col_mappings\n).collect()\n```", "```py\n# Feature engineering config\nratio_features:\n  # feature name\n  likes_to_dislikes: \n    # features used in calculation \n    - likes           \n    - dislikes\n  likes_to_views:\n    - likes\n    - views\n  comments_to_views:\n    - comment_count\n    - views\n\ndifference_features:\n  days_to_trending:\n    - trending_date\n    - publish_time\n\ndate_features:\n  trending_date:\n    - weekday\n```", "```py\ndef ratio_features(features_config: Dict[str, List[str]]) -> List[pl.Expr]:\n    expressions = []\n    for name, cols in features_config.items():\n        expressions.append((pl.col(cols[0]) / pl.col(cols[1])).alias(name))\n\n    return expressions\n\ndef diff_features(features_config: Dict[str, List[str]]) -> List[pl.Expr]:\n    expressions = []\n    for name, cols in features_config.items():\n        expressions.append((pl.col(cols[0]) - pl.col(cols[1])).alias(name))\n\n    return expressions\n\ndef date_features(features_config: Dict[str, List[str]]) -> List[pl.Expr]:\n    expressions = []\n    for col, features in features_config.items():\n        if \"weekday\" in features:\n            expressions.append(pl.col(col).dt.weekday().alias(f\"{col}_weekday\"))\n        if \"month\" in features:\n            expressions.append(pl.col(col).dt.month().alias(f\"{col}_month\"))\n        if \"year\" in features:\n            expressions.append(pl.col(col).dt.year().alias(f\"{col}_year\"))\n\n    return expressions\n\ndef basic_feature_engineering(\n    data: pl.DataFrame,\n    ratios_config: Dict[str, List[str]],\n    diffs_config: Dict[str, List[str]],\n    dates_config: Dict[str, List[str]],\n) -> pl.DataFrame:\n    ratio_expressions = ratio_features(ratios_config)\n    date_diff_expressions = diff_features(diffs_config)\n    date_expressions = date_features(dates_config)\n\n    data = data.with_columns(\n        ratio_expressions + date_diff_expressions + date_expressions\n    )\n    return data\n```", "```py\n# Create mapping\nid_to_category = read_category_mappings(pipe_config[\"category_map_path\"])\ncol_mappings = {\"category_id\": id_to_category}\n\n# Read in configs\ndate_column_format = pipe_config[\"date_column_format\"]\nratios_config = pipe_config[\"ratio_features\"]\ndiffs_config = pipe_config[\"difference_features\"]\ndates_config = pipe_config[\"date_features\"]\n\n# Pipeline\noutput_data = (\n    pl.scan_csv(pipe_config[\"data_path\"])\n    .pipe(clean_data, date_column_format, col_mappings)\n    .pipe(basic_feature_engineering, ratios_config, diffs_config, dates_config)\n).collect()\n```", "```py\n# Filter videos\nmax_time_to_trending: 60\n\n# Features to join to the transformed data\nbase_columns:\n  - views\n  - likes\n  - dislikes\n  - comment_count\n  - comments_disabled\n  - ratings_disabled\n  - video_error_or_removed\n  - likes_to_dislikes\n  - likes_to_views\n  - comments_to_views\n  - trending_date_weekday\n  - channel_title\n  - tags\n  - description\n  - category_id\n\n# Use these columns to join transformed data with original\njoin_columns:\n  - video_id\n  - trending_date\n```", "```py\ndef join_original_features(\n    main: pl.DataFrame,\n    original: pl.DataFrame,\n    main_join_cols: List[str],\n    original_join_cols: List[str],\n    other_cols: List[str],\n) -> pl.DataFrame:\n    original_features = original.select(original_join_cols + other_cols).unique(\n        original_join_cols\n    )  # unique ensures one row per video + date\n    main = main.join(\n        original_features,\n        left_on=main_join_cols,\n        right_on=original_join_cols,\n        how=\"left\",\n    )\n\n    return main\n\ndef create_target_df(\n    df: pl.DataFrame,\n    time_to_trending_thr: int,\n    original_join_cols: List[str],\n    other_cols: List[str],\n) -> pl.DataFrame:\n    # Create a DF with video ID per row and corresponding days to trending and days in trending (target)\n    target = (\n        df.groupby([\"video_id\"])\n        .agg(\n            pl.col(\"days_to_trending\").min().dt.days(),\n            pl.col(\"trending_date\").min().dt.date().alias(\"first_day_in_trending\"),\n            pl.col(\"trending_date\").max().dt.date().alias(\"last_day_in_trending\"),\n            # our TARGET\n            (pl.col(\"trending_date\").max() - pl.col(\"trending_date\").min()).dt.days().alias(\"days_in_trending\"),\n        )\n        .filter(pl.col(\"days_to_trending\") <= time_to_trending_thr)\n    )\n\n    # Join features to the aggregates\n    target = join_original_features(\n        main=target,\n        original=df,\n        main_join_cols=[\"video_id\", \"first_day_in_trending\"],\n        original_join_cols=original_join_cols,\n        other_cols=other_cols,\n    )\n\n    return target\n```", "```py\n# Create mapping\nid_to_category = read_category_mappings(pipe_config[\"category_map_path\"])\ncol_mappings = {\"category_id\": id_to_category}\n\n# Read in configs\ndate_column_format = pipe_config[\"date_column_format\"]\nratios_config = pipe_config[\"ratio_features\"]\ndiffs_config = pipe_config[\"difference_features\"]\ndates_config = pipe_config[\"date_features\"]\n\n# Pipeline\noutput_data = (\n    pl.scan_csv(pipe_config[\"data_path\"])\n    .pipe(clean_data, date_column_format, col_mappings)\n    .pipe(basic_feature_engineering, ratios_config, diffs_config, dates_config)\n    .pipe(\n        create_target_df,\n        time_to_trending_thr=pipe_config[\"max_time_to_trending\"],\n        original_join_cols=join_cols,\n        other_cols=base_features,\n    )\n).collect()\n```", "```py\naggregate_windows:\n  - 7\n  - 30\n  - 180\n```", "```py\ndef build_channel_rolling(df: pl.DataFrame, date_col: str, period: int) -> pl.DataFrame:\n    channel_aggs = (\n        df.sort(date_col)\n        .groupby_rolling(\n            index_column=date_col,\n            period=f\"{period}d\",\n            by=\"channel_title\",\n            closed=\"left\",  # only left to not include the actual day\n        )\n        .agg(\n            pl.col(\"video_id\").n_unique().alias(f\"channel_num_trending_videos_last_{period}_days\"),\n            pl.col(\"days_in_trending\").max().alias(f\"channel_max_days_in_trending_{period}_days\"),\n            pl.col(\"days_in_trending\").mean().alias(f\"channel_avg_days_in_trending_{period}_days\"),\n        )\n        .fill_null(0)\n    )\n\n    return channel_aggs\n\ndef add_rolling_features(\n    df: pl.DataFrame, date_col: str, periods: List[int]\n) -> pl.DataFrame:\n    for period in periods:\n        rolling_features = build_channel_rolling(df, date_col, period)\n        df = df.join(rolling_features, on=[\"channel_title\", \"first_day_in_trending\"])\n\n    return df\n```", "```py\ndef build_period_features(df: pl.DataFrame, date_col: str, period: int) -> pl.DataFrame:\n    general_aggs = (\n        df.sort(date_col)\n        .groupby_dynamic(\n            index_column=date_col,\n            every=\"1d\",\n            period=f\"{period}d\",\n            closed=\"left\",\n        )\n        .agg(\n            pl.col(\"video_id\").n_unique().alias(f\"general_num_trending_videos_last_{period}_days\"),\n            pl.col(\"days_in_trending\").max().alias(f\"general_max_days_in_trending_{period}_days\"),\n            pl.col(\"days_in_trending\").mean().alias(f\"general_avg_days_in_trending_{period}_days\"),\n        )\n        .with_columns(\n            # shift match values with previous period\n            pl.col(f\"general_num_trending_videos_last_{period}_days\").shift(period),\n            pl.col(f\"general_max_days_in_trending_{period}_days\").shift(period),\n            pl.col(f\"general_avg_days_in_trending_{period}_days\").shift(period),\n        )\n        .fill_null(0)\n    )\n\n    return general_aggs\n\ndef add_period_features(\n    df: pl.DataFrame, date_col: str, periods: List[int]\n) -> pl.DataFrame:\n    for period in periods:\n        rolling_features = build_period_features(df, date_col, period)\n        df = df.join(rolling_features, on=[\"first_day_in_trending\"])\n\n    return df\n```", "```py\n# Create mapping\nid_to_category = read_category_mappings(pipe_config[\"category_map_path\"])\ncol_mappings = {\"category_id\": id_to_category}\n\n# Read in configs\ndate_column_format = pipe_config[\"date_column_format\"]\nratios_config = pipe_config[\"ratio_features\"]\ndiffs_config = pipe_config[\"difference_features\"]\ndates_config = pipe_config[\"date_features\"]\n\noutput_data = (\n        pl.scan_csv(pipe_config[\"data_path\"])\n        .pipe(clean_data, date_column_format, col_mappings)\n        .pipe(basic_feature_engineering, ratios_config, diffs_config, dates_config)\n        .pipe(\n            create_target_df,\n            time_to_trending_thr=pipe_config[\"max_time_to_trending\"],\n            original_join_cols=pipe_config[\"join_columns\"],\n            other_cols=pipe_config[\"base_columns\"],\n        )\n        .pipe(\n            add_rolling_features,\n            \"first_day_in_trending\",\n            pipe_config[\"aggregate_windows\"],\n        )\n        .pipe(\n            add_period_features,\n            \"first_day_in_trending\",\n            pipe_config[\"aggregate_windows\"],\n        )\n    ).collect()\n```", "```py\n def pipeline():\n    \"\"\"Pipeline that reads, cleans, and transofrms data into\n    the format we need for modelling\n    \"\"\"\n    # Read and unwrap the config\n    with open(\"pipe_config.yaml\", \"r\") as file:\n        pipe_config = yaml.safe_load(file)\n\n    date_column_format = pipe_config[\"date_column_format\"]\n    ratios_config = pipe_config[\"ratio_features\"]\n    diffs_config = pipe_config[\"difference_features\"]\n    dates_config = pipe_config[\"date_features\"]\n\n    id_to_category = read_category_mappings(pipe_config[\"category_map_path\"])\n    col_mappings = {\"category_id\": id_to_category}\n\n    output_data = (\n        pl.scan_csv(pipe_config[\"data_path\"])\n        .pipe(clean_data, date_column_format, col_mappings)\n        .pipe(basic_feature_engineering, ratios_config, diffs_config, dates_config)\n        .pipe(\n            create_target_df,\n            time_to_trending_thr=pipe_config[\"max_time_to_trending\"],\n            original_join_cols=pipe_config[\"join_columns\"],\n            other_cols=pipe_config[\"base_columns\"],\n        )\n        .pipe(\n            add_rolling_features,\n            \"first_day_in_trending\",\n            pipe_config[\"aggregate_windows\"],\n        )\n        .pipe(\n            add_period_features,\n            \"first_day_in_trending\",\n            pipe_config[\"aggregate_windows\"],\n        )\n    ).collect()\n\n    return output_data\n\nif __name__ == \"__main__\":\n    t0 = time.time()\n    output = pipeline()\n    t1 = time.time()\n    print(\"Pipeline took\", t1 - t0, \"seconds\")\n    print(\"Output shape\", output.shape)\n    print(\"Output columns:\", output.columns)\n    output.write_parquet(\"./data/modelling_data.parquet\")\n```", "```py\npython data_preparation_pipeline.py\n```", "```py\nPipeline took 0.3374309539794922 seconds\nOutput shape (3196, 38)\nOutput columns: [\n 'video_id', 'days_to_trending', 'first_day_in_trending',\n 'last_day_in_trending', 'days_in_trending', 'views', 'likes', 'dislikes', \n 'comment_count', 'comments_disabled', 'ratings_disabled', \n 'video_error_or_removed', 'likes_to_dislikes', 'likes_to_views',\n 'comments_to_views', 'trending_date_weekday', 'channel_title', \n 'tags', 'description', 'category_id', 'channel_num_trending_videos_last_7_days',\n 'channel_max_days_in_trending_7_days', 'channel_avg_days_in_trending_7_days',\n 'channel_num_trending_videos_last_30_days', 'channel_max_days_in_trending_30_days', \n 'channel_avg_days_in_trending_30_days', 'channel_num_trending_videos_last_180_days',\n 'channel_max_days_in_trending_180_days', 'channel_avg_days_in_trending_180_days', \n 'general_num_trending_videos_last_7_days', 'general_max_days_in_trending_7_days', \n 'general_avg_days_in_trending_7_days', 'general_num_trending_videos_last_30_days', \n 'general_max_days_in_trending_30_days', 'general_avg_days_in_trending_30_days',\n 'general_num_trending_videos_last_180_days', 'general_max_days_in_trending_180_days',\n 'general_avg_days_in_trending_180_days'\n]\n```"]