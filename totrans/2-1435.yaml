- en: 'LLMs for Everyone: Running LangChain and a MistralAI 7B Model in Google Colab'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每个人的 LLM：在 Google Colab 中运行 LangChain 和 MistralAI 7B 模型
- en: 原文：[https://towardsdatascience.com/llms-for-everyone-running-langchain-and-a-mistralai-7b-model-in-google-colab-246ca94d7c4d](https://towardsdatascience.com/llms-for-everyone-running-langchain-and-a-mistralai-7b-model-in-google-colab-246ca94d7c4d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llms-for-everyone-running-langchain-and-a-mistralai-7b-model-in-google-colab-246ca94d7c4d](https://towardsdatascience.com/llms-for-everyone-running-langchain-and-a-mistralai-7b-model-in-google-colab-246ca94d7c4d)
- en: Experimenting with Large Language Models for free
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 免费实验大型语言模型
- en: '[](https://dmitryelj.medium.com/?source=post_page-----246ca94d7c4d--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page-----246ca94d7c4d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----246ca94d7c4d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----246ca94d7c4d--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page-----246ca94d7c4d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dmitryelj.medium.com/?source=post_page-----246ca94d7c4d--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page-----246ca94d7c4d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----246ca94d7c4d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----246ca94d7c4d--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page-----246ca94d7c4d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----246ca94d7c4d--------------------------------)
    ·10 min read·Dec 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----246ca94d7c4d--------------------------------)
    ·阅读时间 10 分钟·2023年12月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/260367e3c69b2a0784e0677fcb3983bb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/260367e3c69b2a0784e0677fcb3983bb.png)'
- en: Artistic representation of the LangChain, Photo by Ruan Richard Rodrigues, [Unsplash](https://unsplash.com/@heeybooy)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 的艺术表现，照片由 Ruan Richard Rodrigues 提供，[Unsplash](https://unsplash.com/@heeybooy)
- en: Everybody knows that large language models are, by definition, large. And even
    not so long ago, they were available only for high-end hardware owners, or at
    least for people who paid for cloud access or even every API call. Nowadays, the
    time is changing. In this article, I will show how to run a LangChain Python library,
    a FAISS vector database, and a Mistral-7B model in Google Colab completely for
    free, and we will do some fun experiments with it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大家都知道大型语言模型的定义就是“大”。甚至在不久前，它们仅对高端硬件拥有者开放，或者至少对付费获取云访问或每次 API 调用的人开放。现在，情况正在改变。在这篇文章中，我将展示如何在
    Google Colab 中完全免费运行 LangChain Python 库、FAISS 向量数据库和 Mistral-7B 模型，并进行一些有趣的实验。
- en: Components
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组件
- en: 'There are many articles here on TDS about using large language models in Python,
    but often it is not so easy to reproduce them. For example, many examples of using
    a LangChain library use an [OpenAI](https://python.langchain.com/docs/integrations/llms/openai)
    class, the first parameter of which (guess what?) is OPENAI_API_KEY. Some other
    examples of RAG (Retrieval Augmented Generation) and vector databases use Weaviate;
    the first thing we see after opening their website is “Pricing.” Here, I will
    use a set of open-source libraries that can be used completely for free:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多关于在 Python 中使用大型语言模型的文章，但它们往往不容易重现。例如，许多使用 LangChain 库的示例使用了一个 [OpenAI](https://python.langchain.com/docs/integrations/llms/openai)
    类，其第一个参数（猜猜是什么？）是 OPENAI_API_KEY。一些其他 RAG（检索增强生成）和向量数据库的示例使用了 Weaviate；我们打开其网站后首先看到的是“定价”。在这里，我将使用一组可以完全免费使用的开源库：
- en: '[LangChain](https://github.com/langchain-ai/langchain). It is a Python framework
    for developing applications powered by language models. It is also model-agnostic,
    and the same code can be reused with different models.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LangChain](https://github.com/langchain-ai/langchain)。它是一个用于开发语言模型驱动应用程序的
    Python 框架。它也是模型无关的，相同的代码可以与不同的模型重复使用。'
- en: '[FAISS](https://github.com/facebookresearch/faiss) (Facebook AI Similarity
    Search). It’s a library designed for efficient similarity search and storage of
    dense vectors, which I will use for Retrieval Augmented Generation.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FAISS](https://github.com/facebookresearch/faiss)（Facebook AI 相似性搜索）。这是一个用于高效相似性搜索和稠密向量存储的库，我将使用它进行检索增强生成（Retrieval
    Augmented Generation）。'
- en: '[Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) is a 7.3B parameter
    large language model (released under the Apache 2.0 license), which, according
    to the authors, is outperforming 13B Llama2 on all benchmarks. It is also available
    on [HuggingFace](https://huggingface.co/mistralai/Mistral-7B-v0.1), so its use
    is pretty simple.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) 是一个拥有7.3B参数的大型语言模型（在Apache
    2.0许可证下发布），根据作者的说法，它在所有基准测试中超越了13B Llama2。它也可以在 [HuggingFace](https://huggingface.co/mistralai/Mistral-7B-v0.1)
    上使用，因此使用起来非常简单。'
- en: Last but not least, [Google Colab](https://colab.research.google.com/) is also
    an important part of this test. It provides free access to Python notebooks powered
    by CPU, 16 GB NVIDIA Tesla T4, or even 80 GB NVIDIA A100 (though I never saw the
    last one available for a free instance).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，[Google Colab](https://colab.research.google.com/)也是这次测试的重要组成部分。它提供了对由CPU、16
    GB NVIDIA Tesla T4或甚至80 GB NVIDIA A100（尽管我从未见过免费实例中提供最后一个）的Python笔记本的免费访问。
- en: Right now, let’s get into it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧。
- en: Install
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: 'As a first step, we need to open Google Colab and create a new notebook. The
    needed libraries can be installed by using `pip` in the first cell:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要打开Google Colab并创建一个新的笔记本。需要的库可以在第一个单元格中使用`pip`安装：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Before running the code, we need to **select the runtime** type:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行代码之前，我们需要**选择运行时**类型：
- en: '![](../Images/befe23bd5b3d3bb9ed2a9a7c2edb01d9.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/befe23bd5b3d3bb9ed2a9a7c2edb01d9.png)'
- en: Google Colab, Screenshot by author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab，作者截图
- en: 'Now, let’s import the libraries:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们导入这些库：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If everything was done correctly, the output should show the “cuda” device and
    a “Tesla T4” as the selected graphics card.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，输出应显示“cuda”设备和选定的“Tesla T4”显卡。
- en: 'The next step is the most important and resource-intensive: let’s **load the
    language model**:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是最重要和最耗资源的：让我们**加载语言模型**：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, I selected a 4-bit quantization mode, which allows the model to fit into
    GPU RAM. There is also another tricky part. Loading the original “mistralai/Mistral-7B-Instruct-v0.1”
    model causes the Colab instance to crash. Surprisingly, the GPU RAM is enough
    for 4-bit quantization, but the model files are about 16 GB in size, and there
    is just not enough “normal” RAM on a free Colab instance to quantize the model
    before loading it into the GPU! As a workaround, I was using a “sharded” version,
    which was split into 2GB chunks (if your PC or Colab instance has more than 16GB
    of RAM, this is not required).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我选择了4位量化模式，这允许模型适应GPU内存。还有另一个棘手的部分。加载原始“mistralai/Mistral-7B-Instruct-v0.1”模型会导致Colab实例崩溃。令人惊讶的是，GPU内存足够用于4位量化，但模型文件的大小约为16
    GB，免费Colab实例上没有足够的“正常”内存来在将其加载到GPU之前对模型进行量化！作为变通方法，我使用了一个“分片”版本，它被拆分成2GB的块（如果您的PC或Colab实例有超过16GB的内存，则不需要这样做）。
- en: 'As an aside note, those readers who wish to know more about how 4-bit quantization
    works are welcome to read another article:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，想了解更多关于4位量化如何工作的读者欢迎阅读另一篇文章：
- en: '[](/16-8-and-4-bit-floating-point-formats-how-does-it-work-d157a31ef2ef?source=post_page-----246ca94d7c4d--------------------------------)
    [## 16, 8, and 4-bit Floating Point Formats — How Does it Work?'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/16-8-and-4-bit-floating-point-formats-how-does-it-work-d157a31ef2ef?source=post_page-----246ca94d7c4d--------------------------------)
    [## 16、8和4位浮点格式——它是如何工作的？'
- en: Let’s go into bits and bytes
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让我们深入探讨
- en: towardsdatascience.com](/16-8-and-4-bit-floating-point-formats-how-does-it-work-d157a31ef2ef?source=post_page-----246ca94d7c4d--------------------------------)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/16-8-and-4-bit-floating-point-formats-how-does-it-work-d157a31ef2ef?source=post_page-----246ca94d7c4d--------------------------------)
- en: 'If everything was done correctly, the Colab output should look like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，Colab的输出应如下所示：
- en: '![](../Images/99178efab22a0b8427cc6b16db98f545.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99178efab22a0b8427cc6b16db98f545.png)'
- en: Loading the Mistral 7B model, Screenshot by author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 加载Mistral 7B模型，作者截图
- en: As we can see from the picture, the files that are required to be downloaded
    are huge, so if you run this code locally (not in Colab), ensure that your web
    traffic is not limited.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从图片中可以看出，需要下载的文件非常庞大，因此如果在本地运行此代码（而不是在Colab中），请确保您的网络流量没有限制。
- en: 'Now, let’s **create the LLM pipeline**:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们**创建LLM管道**：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Congratulations! Our installation is ready, and we successfully loaded a 7B
    language model. For a short test, let’s see if the LLM works:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们的安装已准备好，我们成功加载了一个7B语言模型。为了进行简单测试，让我们看看LLM是否有效：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If everything is okay, we’re ready to have fun and do further tests.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，我们准备好享受乐趣并进行进一步的测试。
- en: LangChain
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangChain
- en: '[LangChain](https://github.com/langchain-ai/langchain) is a Python framework
    specially designed to work with language models. As a warm-up, let’s test a **prompt
    template**:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[LangChain](https://github.com/langchain-ai/langchain) 是一个专门为与语言模型配合使用而设计的
    Python 框架。作为热身，让我们测试一个**提示模板**：'
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Interestingly, LangChain is “cross-platform,” and we can use different language
    models without code change. This example was taken from the official [library
    documentation](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/),
    where OpenAI is used for prompts, but I used the same template for Mistral.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，LangChain 是“跨平台”的，我们可以在不更改代码的情况下使用不同的语言模型。这个示例取自官方 [库文档](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)，其中
    OpenAI 用于提示，但我使用了相同的模板来进行 Mistral。
- en: 'How does it work? It is possible to add the `ConsoleCallbackHandler` to the
    config, so we can see all intermediate steps:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何工作的？可以将 `ConsoleCallbackHandler` 添加到配置中，这样我们就可以看到所有的中间步骤：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, the output will look like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，输出将如下所示：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As another example, let’s try a **ChatPromptTemplate** class:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个示例，让我们尝试 **ChatPromptTemplate** 类：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In my opinion, the answer “Yes, I am Mistral” is acceptable but linguistically
    not the best for a “What is your name?” question. Obviously, with large neural
    networks, interpretability can be an issue, and it’s impossible to tell *why*
    the model responded this or that way. It can be an artifact of the 4-bit quantization
    (which slightly reduces the model quality) or just a fundamental limitation of
    the abilities of the 7B model (obviously, other 33B or 70B models can perform
    better but will require much more computational resources).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，回答“是的，我是 Mistral”是可以接受的，但从语言学角度来看，对于“你叫什么名字？”这个问题并不是最好的回答。显然，对于大型神经网络来说，可解释性可能是一个问题，我们无法告诉*为什么*模型以这种或那种方式做出回应。这可能是
    4 位量化的副作用（这会稍微降低模型质量）或只是 7B 模型能力的根本限制（显然，其他 33B 或 70B 模型可能表现更好，但需要更多计算资源）。
- en: Retrieval-augmented generation (RAG)
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）
- en: Nowadays, RAG is a hot topic of research. It allows us to automatically add
    external documents to the LLM prompt and to add more information without fine-tuning
    the model. Let’s see how we can use it with LangChain and Mistral.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，RAG 是一个热门研究话题。它允许我们自动将外部文档添加到 LLM 提示中，并在不微调模型的情况下添加更多信息。让我们看看如何将其与 LangChain
    和 Mistral 一起使用。
- en: 'First, we need to create a separate **embedding model**:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个单独的**嵌入模型**：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This small [sentence-transformer model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
    is able to convert text strings into a vector representation; we will use it for
    our vector database. As a toy example, I will add only one “document” to the array:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小型 [sentence-transformer 模型](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
    能够将文本字符串转换为向量表示；我们将用它来构建我们的向量数据库。作为一个玩具示例，我将仅向数组中添加一个“文档”：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we need to create a vector database and a `VectorStoreRetriever` object:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要创建一个向量数据库和一个 `VectorStoreRetriever` 对象：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we can create a [RetrievalQA](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html#)
    object, which is specially designed for question-answering:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个 [RetrievalQA](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html#)
    对象，这个对象专门用于问答：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'I will ask the model questions about Airbus; they are highly likely unknown
    to the model:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我将询问模型有关空客的问题；这些问题模型很可能不知道：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: I was positively surprised by the answers. First, the Mistral 7B model was already
    aware of the Airbus A380 range (I checked with Google, and the result looks correct).
    Second, as I expected, the model was not aware of the A380 tire diameter, but
    it “honestly” answered “I don’t know” instead of providing the “hallucinated”
    and incorrect response.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我对这些回答感到积极惊讶。首先，Mistral 7B 模型已经知道了空客 A380 的航程（我用 Google 查了一下，结果看起来是正确的）。其次，正如我所预期的，模型并不知道
    A380 轮胎的直径，但它“诚实地”回答了“我不知道”，而不是提供“虚构的”错误回答。
- en: 'Now, let’s add an additional string to our “vector database”:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们向我们的“向量数据库”中添加一个额外的字符串：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then we can try again:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以再试一次：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This was amazing! The model not only found the information that the A380 tire
    diameter is 56 inches, but it correctly converted it into centimeters (56*2,54
    is indeed 142). We know that math tasks are usually hard for LLMs, so this accuracy
    is surprising.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是太棒了！模型不仅找到了 A380 轮胎直径为 56 英寸的信息，还正确地将其转换为厘米（56*2.54 确实是 142）。我们知道数学任务通常对
    LLM 来说很困难，所以这种准确性令人惊讶。
- en: 'We can also ask a model to explain the answer in steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以要求模型逐步解释答案：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This is great! Well, we are already used to the fact that large language models
    like GPT3 or GPT4 run on supercomputers in the cloud and can produce amazing results.
    But to see that on your local GPU (I tested this code in Google Colab and on my
    home PC as well) is a completely different feeling.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这太棒了！好吧，我们已经习惯于大型语言模型如 GPT3 或 GPT4 在云端超级计算机上运行并产生惊人结果的事实。但在你的本地 GPU 上看到这一点（我在
    Google Colab 和我的家用 PC 上都测试了这段代码）是完全不同的感觉。
- en: 'Attentive readers may ask the question. How do the Mistral and Retriever models
    work together? Indeed, I created a “Mistral-7B-Instruct-v0.1” model and an “all-MiniLM-l6-v2”
    sentence-embedding model. Are their vector spaces compatible? The answer is “no.”
    When we do a query, the `VectorStoreRetriever` does its own search first, finds
    the best documents in the vector store, and returns these documents in plain text
    format. We can see the final prompt if we change the `verbose` parameter to True:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 细心的读者可能会问，Mistral 和 Retriever 模型如何协同工作？确实，我创建了一个“Mistral-7B-Instruct-v0.1”模型和一个“all-MiniLM-l6-v2”句子嵌入模型。它们的向量空间是否兼容？答案是“否”。当我们进行查询时，`VectorStoreRetriever`
    会先进行自己的搜索，找到向量库中最好的文档，并以纯文本格式返回这些文档。如果我们将 `verbose` 参数设置为 True，就可以看到最终的提示：
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After running the same code, we can see the actual prompt, which was sent by
    LangChain to the model:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行相同的代码后，我们可以看到 LangChain 发送给模型的实际提示：
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this case, both documents were relevant to the “Airbus” question, and a `VectorStoreRetriever`
    placed them in the `context` placeholder.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这两个文档都与“空客”问题相关，`VectorStoreRetriever` 将它们放在 `context` 占位符中。
- en: Conclusion
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we were able to run a 7.3B Mistral large language model on
    a free Google Colab instance, using only free and open-source components. This
    is a great achievement and also a generous step from Google, considering that
    at the time of this writing, the cheapest 16GB video card on Amazon costs at least
    $500 (I must admit, though, that the Google Colab service is not a pure charity,
    and the free GPU backend may not be available 100% of the time; those who need
    it often should consider buying a paid subscription). We were also able to use
    retrieval-augmented generation to add extra information to the LLM prompt. If
    models like this are ready for production, it is still an open question and also
    an eternal “Buy vs. DIY” dilemma. The Mistral 7B model can still sometimes “hallucinate”
    and produce incorrect answers; it can also be outperformed by larger models. Anyway,
    the ability to test models like this for free is great for study, self-education,
    and prototyping.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们成功在免费的 Google Colab 实例上运行了一个 7.3B 的 Mistral 大型语言模型，完全依赖于免费的开源组件。这是一项伟大的成就，同时也是
    Google 的慷慨举措，因为在撰写本文时，Amazon 上最便宜的 16GB 显卡至少需要 $500（不过我必须承认，Google Colab 服务并非纯粹的慈善，免费的
    GPU 后端可能并不总是可用；需要经常使用的人应该考虑购买付费订阅）。我们还能够使用检索增强生成来为 LLM 提供额外的信息。如果像这样的模型已经准备好投入生产，是否真的合适仍然是一个悬而未决的问题，也永远是“购买还是自己动手”的难题。Mistral
    7B 模型有时仍会“幻觉”并产生错误的答案；它也可能被更大的模型超越。无论如何，能够免费测试这样的模型对于学习、自我教育和原型设计都是极好的。
- en: 'In the next part, I will show how to run a LLaMA-13B model and a LangChain
    framework in Google Colab:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分，我将展示如何在 Google Colab 中运行 LLaMA-13B 模型和 LangChain 框架：
- en: '[](/llms-for-everyone-running-the-llama-13b-model-and-langchain-in-google-colab-68d88021cf0b?source=post_page-----246ca94d7c4d--------------------------------)
    [## LLMs for Everyone: Running the LLaMA-13B model and LangChain in Google Colab'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/llms-for-everyone-running-the-llama-13b-model-and-langchain-in-google-colab-68d88021cf0b?source=post_page-----246ca94d7c4d--------------------------------)
    [## 适合所有人的 LLM：在 Google Colab 中运行 LLaMA-13B 模型和 LangChain'
- en: Experimenting with Large Language Models for free (Part 2)
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 免费实验大型语言模型（第 2 部分）
- en: towardsdatascience.com](/llms-for-everyone-running-the-llama-13b-model-and-langchain-in-google-colab-68d88021cf0b?source=post_page-----246ca94d7c4d--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/llms-for-everyone-running-the-llama-13b-model-and-langchain-in-google-colab-68d88021cf0b?source=post_page-----246ca94d7c4d--------------------------------)'
- en: 'Those who are interested in using language models and natural language processing
    are also welcome to read other articles:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对使用语言模型和自然语言处理感兴趣的人也欢迎阅读其他文章：
- en: '[Natural Language Processing For Absolute Beginners](/natural-language-processing-for-absolute-beginners-a195549a3164)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理绝对初学者指南](/natural-language-processing-for-absolute-beginners-a195549a3164)'
- en: '[16, 8, and 4-bit Floating Point Formats — How Does it Work?](/16-8-and-4-bit-floating-point-formats-how-does-it-work-d157a31ef2ef)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16、8 和 4 位浮点格式——如何运作？](/16-8-and-4-bit-floating-point-formats-how-does-it-work-d157a31ef2ef)'
- en: '[Python Data Analysis: What Do We Know About Pop Songs?](https://blog.devgenius.io/python-data-analysis-what-do-we-know-about-pop-songs-b6197d85d4)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 数据分析：我们对流行歌曲了解多少？](https://blog.devgenius.io/python-data-analysis-what-do-we-know-about-pop-songs-b6197d85d4)'
- en: If you enjoyed this story, feel free [to subscribe](https://medium.com/@dmitryelj/membership)
    to Medium, and you will get notifications when my new articles will be published,
    as well as full access to thousands of stories from other authors. You are also
    welcome to connect via [LinkedIn](https://www.linkedin.com/in/dmitrii-eliuseev/).
    If you want to get the full source code for this and other posts, feel free to
    visit my [Patreon page](https://www.patreon.com/deliuseev).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这个故事，欢迎[订阅](https://medium.com/@dmitryelj/membership)Medium，你将获得新文章发布的通知，并且可以完全访问其他作者的成千上万篇故事。也欢迎通过[LinkedIn](https://www.linkedin.com/in/dmitrii-eliuseev/)与我联系。如果你想获得此文及其他文章的完整源代码，可以访问我的[Patreon
    页面](https://www.patreon.com/deliuseev)。
- en: Thanks for reading.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。
