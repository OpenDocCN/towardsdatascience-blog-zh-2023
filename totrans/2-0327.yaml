- en: Are Prompts Generated by Large Language Models (LLMs) Reliable?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/are-prompt-generated-by-large-language-models-llms-reliable-4162fd10c845](https://towardsdatascience.com/are-prompt-generated-by-large-language-models-llms-reliable-4162fd10c845)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unleashing the Power of LLMs with Auto-Generated Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@a0987284901?source=post_page-----4162fd10c845--------------------------------)[![Henry
    Lai](../Images/eaa1b4eb6f6cebc131f4cf0cfdd4cda7.png)](https://medium.com/@a0987284901?source=post_page-----4162fd10c845--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4162fd10c845--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4162fd10c845--------------------------------)
    [Henry Lai](https://medium.com/@a0987284901?source=post_page-----4162fd10c845--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4162fd10c845--------------------------------)
    ·6 min read·Apr 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/427afef3b6084a0384d5114014f7246c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. An example of performance variability of two different ChatGPT-generated
    prompts
  prefs: []
  type: TYPE_NORMAL
- en: The rapid development of large language models (LLMs), including [ChatGPT](https://openai.com/blog/chatgpt)
    and [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf), has revolutionized data
    science. In the past, data scientists typically devoted a substantial amount of
    time to preparing data, designing models, and fine-tuning them to solve various
    problems. Nowadays, with the advent of LLMs, we can accomplish many tasks in a
    pure data-centric manner without spending any effort on modeling (see the [data-centric
    AI framework](https://github.com/daochenzha/data-centric-AI)).
  prefs: []
  type: TYPE_NORMAL
- en: One key idea drives the advancement is prompting, which refers to use of specific
    input text or questions to guide a language model in generating a desired output.
    For instance, when summarizing a lengthy article, we can provide the LLM with
    a prompt, such as “*Summarize the above in one sentence*”, and input the article
    text. This enables the LLM to generate a concise summary of the article, making
    it easier for researchers to extract relevant information quickly. The use of
    prompts has opened up new opportunities in data science, enabling scientists to
    streamline their workflows and increase their productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Creating effective prompts remains a significant challenge, as even prompts
    that seem similar can produce vastly different outputs. For example, using “*Write
    a brief summary*” or “*Provide a concise summary*” may lead to substantially different
    summaries, as illustrated in Figure 1\. This variation in output can make it difficult
    for data scientists to determine which prompt to use to achieve the desired results.
  prefs: []
  type: TYPE_NORMAL
- en: To address the challenge of creating effective prompts, automated prompting
    can be a viable solution that utilizes LLMs to generate prompt templates directly.
    For instance, when summarizing clinical notes, one can ask an LLM for prompt suggestions
    by posing the question “*What would be an effective prompt for summarizing clinical
    notes?*” The model can then generate a variety of prompt candidates tailored to
    this specific task, potentially accelerating the process of effective prompt creation.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-generated prompts are usually unpredictable in terms of their quality, resulting
    in outputs that exhibit significant variability. This, in turn, necessitates a
    significant amount of manual effort to examine each candidate prompt individually.
    In this article, we will introduce a framework named SPeC, to make the LLM-generated
    prompts more effective and reliable. SPeC exploits soft prompt tokens to calibrate
    performance variability while preserving performance gain brought by LLM-generated
    prompts, resulting in notably more consistent outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Tuning in LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/575d106a8983d3c576717cf33486b575.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure2\. Prompt Tuning. Image from the paper [https://arxiv.org/abs/2303.10158](https://arxiv.org/pdf/2303.10158.pdf)
    with original authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt tuning is a revolution to data science following the concpet of [data-centric
    AI](https://arxiv.org/abs/2303.10158). In addition to collecting more training
    data, prompt tuning is an alternative approach to improve the performance of LLMs
    without any further fine-tuning. Notably, effective prompts are a critical factor
    in the success of prompt tuning, as the specific input words can trigger the corresponding
    information learned by LLMs, resulting in a significant improvement in LLMs’ adaptation
    and performance on specific downstream tasks. Data scientists and researchers
    can benefit greatly from this approach as it enables them to efficiently and effectively
    utilize LLMs in various downstream tasks. It has also been advocated by [Jeff
    Dean](https://twitter.com/JeffDean), a leading director of Google Research.
  prefs: []
  type: TYPE_NORMAL
- en: How to Automatically Generate Prompts?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Designing an effective prompt is never a trivial task, as tremendous domain-specific
    expertise is still required to extract certain keywords and sentences to form
    the prompts. The advent of powerful LLMs has made it possible for users to increase
    their productivity in designated tasks by taking advantage of prompts that are
    automatically generated. When users input a question into an LLM, it can generate
    corresponding prompt templates. For instance, a data scientist could ask ChatGPT
    for guidance on a good prompt for text summarization, and then utilize the resulting
    feedback to summarize text. This approach can significantly streamline workflows,
    saving users considerable time and effort.
  prefs: []
  type: TYPE_NORMAL
- en: Are Automatically Generated Prompts Reliable?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: However, the quality of prompts generated by LLMs can be highly unpredictable,
    which in turn leads to a significant increase in the performance variance of LLMs.
    Even when prompts are semantically similar, they can produce vastly different
    outputs. For instance, as demonstrated in Figure 1, prompt-2 and prompt-1, generated
    from a frozen LLM and highly similar to each other, resulted in entirely different
    summarization. This issue is particularly problematic in high-stakes domains,
    such as the financial and healthcare industries, where the variance in generated
    prompts can erode trust in LLMs’ results among researchers and engineers. Therefore,
    it is critical to find ways to control the quality of prompts generated by LLMs
    to ensure the reliability of their outputs, especially in such domains.
  prefs: []
  type: TYPE_NORMAL
- en: '**Can We Trust the Results from the Generated Prompts?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In reality, the answer is a clear negative. The uncertainty that frequently
    arises in LLMs is a significant issue for scientists who need to trust the output
    produced by these models. If significant uncertainty also occurs in LLM-generated
    prompts, it can considerably erode scientists’ confidence in the results. Therefore,
    it is essential to have a mechanism in place that reduces the output variance
    caused by the quality of these auto-generated prompts in order to ensure that
    LLMs work more reliably.
  prefs: []
  type: TYPE_NORMAL
- en: A Soft Prompt-Based Calibration on LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/6ee6b7e43baff9b8fee655ba654deed3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. An overview of soft prompt-based calibration (SPeC) framework. Image
    from the paper [https://arxiv.org/abs/2303.10158](https://medium.com/r?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2303.13035v2.pdf)
    with original authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: '[*Full Paper Link*](https://arxiv.org/pdf/2303.13035v2.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Motivated by prompt tuning from [*data-centric AI*](https://arxiv.org/abs/2303.10158)
    concepts, a framework *Soft Prompt-Based Calibration (SPeC)*, as depicted in *Figure
    3,* discusses the techniques to reduce the outcome variance of different prompts.
    SpeC framework exploits soft prompt tokens to calibrate performance variability
    while preserving performance gain brought by LLM-generated prompts. The soft prompt
    tokens can be any sentence that is semantically related to the input text. For
    example, “radiologist describes the stable abnormality in the exam” can be good
    soft prompt tokens for clinical note summarization. This way, given a well-trained
    soft prompt encoder, by adding soft prompt tokens with the input text, we will
    be able to achieve stable inference outcomes of LLMs. For instance, medical doctors
    can easily provide the appropriate soft prompt tokens by using relevant keywords
    or terms to get desired outcomes with consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Experimental Analytics on Clinical Note Summarization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SPeC framework is evaluated on an important healthcare task, the clinical note
    summarization for medical doctors. In this work, the LLM-generated prompts are
    collected by the asking the question, “What is a good prompt for clinical note
    summarization?”, to ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: SPeC effectively guides pre-trained LLMs that have been frozen in place to perform
    with less variability in clinical note summarization. This ensures that the LLMs
    can maintain the performance improvements gained from using prompts generated
    by ChatGPT, while also reducing variability in performance to make sure the resulting
    clinical summaries are more accurate and faithful to the original data.
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of SPeC in maintaining consistent summarization performance
    in frozen pre-trained LLMs was demonstrated in their case study, which highlighted
    the potential for incorrect outcomes (highlighted in red) if SPeC was not used.
    The study’s results are displayed in *Figure 4.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5231136ab7a273b33deef678d0a42677.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Performance variability comparison of Flan-T5 w/ and w/o exploiting
    SPeC.
  prefs: []
  type: TYPE_NORMAL
- en: How Can SPeC Framework Be Used in Daily Workflow?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the era of [data-centric AI](https://arxiv.org/abs/2303.10158), LLMs have
    the potential to revolutionize data science by providing fast and accurate analysis
    with prompt tuning techniques, leading to more efficient and effective workflow.
    However, several concerns about the uncertainty of LLMs’ outputs have been raised,
    especially in situations where critical and emergent decisions are needed to make.
    It is important to address these concerns to ensure that LLMs can be effectively
    integrated into AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: SPeC framework has effectively mitigated the uncertainty concerns raised by
    scientists while using LLMs, increasing their willingness to trust the decisions
    made by LLMs. For example, for biomedical data scientists, the success of the
    SPeC framework in providing dependable and consistent medical information summaries
    has the potential to empower healthcare practitioners to make informed decisions
    for optimal patient care.
  prefs: []
  type: TYPE_NORMAL
- en: Resource
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can learn more about how SPeC helps in the healthcare industry and increase
    the willingness of healthcare experts to trust the decisions made by LLMs in the
    following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] [SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability
    in Clinical Notes Summarization](https://arxiv.org/abs/2303.13035)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] [Data-centric Artificial Intelligence: A Survey](https://arxiv.org/abs/2303.10158)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] [Awesome Data-centric AI](https://github.com/daochenzha/data-centric-AI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are interested in how to apply SPeC on different downstream tasks. Some
    more instructions can be found in the [Github Repository](https://github.com/ynchuang/SPeC-A-Soft-Prompt-Based-Calibration).
  prefs: []
  type: TYPE_NORMAL
