- en: 'Stable Diffusion: Mastering the Art of Interior Design'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳定扩散：掌握室内设计的艺术
- en: 原文：[https://towardsdatascience.com/stable-diffusion-mastering-the-art-of-interior-design-9fb4214544b0](https://towardsdatascience.com/stable-diffusion-mastering-the-art-of-interior-design-9fb4214544b0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/stable-diffusion-mastering-the-art-of-interior-design-9fb4214544b0](https://towardsdatascience.com/stable-diffusion-mastering-the-art-of-interior-design-9fb4214544b0)
- en: A deep dive into Stable Diffusion and its inpainting variant for interior design
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探索稳定扩散及其室内设计的修复变体
- en: '[](https://medium.com/@rjguedes?source=post_page-----9fb4214544b0--------------------------------)[![Rafael
    Guedes](../Images/b3d000b3bce0113d2b2727e84db04870.png)](https://medium.com/@rjguedes?source=post_page-----9fb4214544b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9fb4214544b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9fb4214544b0--------------------------------)
    [Rafael Guedes](https://medium.com/@rjguedes?source=post_page-----9fb4214544b0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rjguedes?source=post_page-----9fb4214544b0--------------------------------)[![Rafael
    Guedes](../Images/b3d000b3bce0113d2b2727e84db04870.png)](https://medium.com/@rjguedes?source=post_page-----9fb4214544b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9fb4214544b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9fb4214544b0--------------------------------)
    [Rafael Guedes](https://medium.com/@rjguedes?source=post_page-----9fb4214544b0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9fb4214544b0--------------------------------)
    ·9 min read·Dec 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----9fb4214544b0--------------------------------)
    ·阅读时间9分钟·2023年12月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this fast-paced world that we live in and after the pandemic, many of us
    realised that having a pleasant environment like home to escape from reality is
    priceless and a goal to be pursued.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们生活的这个快节奏的世界中以及经历了疫情后，我们中的许多人意识到，拥有一个舒适的家作为逃避现实的避风港是无价的，值得追求的目标。
- en: Whether you are looking for a Scandinavian, minimalist, or a glamorous style
    to decorate your home, it is not easy to imagine how every single object will
    fit in a space full of different pieces and colours. For that reason, we usually
    seek for professional help to create those amazing 3D images that help us understand
    how our future home will look like.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是寻找斯堪的纳维亚风格、极简风格，还是华丽风格来装饰你的家，很难想象每件物品如何在充满不同件数和颜色的空间中相互配合。因此，我们通常寻求专业帮助，以创建那些令人惊叹的3D图像，帮助我们理解未来的家会是什么样子。
- en: However, these 3D images are expensive, and if our initial idea does not look
    as good as we thought, getting new images will take time and more money, things
    that are scarce nowadays.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些3D图像费用昂贵，如果我们的初步构思不如预期好，获取新图像将需要时间和更多的金钱，这些在当今社会都很稀缺。
- en: In this article, I explore the Stable Diffusion model starting with a brief
    explanation of what it is, how it is trained and what is needed to adapt it for
    inpainting. Finally, I finish the article with its application on a 3D image from
    my future home where I change the kitchen island and cabinets to a different colour
    and material.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我探讨了稳定扩散模型，首先简要解释了它是什么、如何训练以及需要什么来适应修复。最后，我以对我未来家的3D图像的应用结束文章，我将厨房岛和橱柜的颜色和材料更换为不同的。
- en: '![](../Images/2a2fbf68e14d0e4d69be676ac27fb9c3.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a2fbf68e14d0e4d69be676ac27fb9c3.png)'
- en: 'Figure 1: Interior Design ([source](https://unsplash.com/photos/brown-wooden-framed-yellow-padded-chair-_HqHX3LBN18))'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：室内设计 ([source](https://unsplash.com/photos/brown-wooden-framed-yellow-padded-chair-_HqHX3LBN18))
- en: As always, the code is available on [Github](https://github.com/rjguedes8/stable_diffusion).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，代码可在[Github](https://github.com/rjguedes8/stable_diffusion)上找到。
- en: Stable Diffusion
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳定扩散
- en: What is it?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这是什么？
- en: Stable Diffusion [1] is a generative AI model released in 2022 by CompVis Group
    that produces photorealistic images from text and image prompts. It was primarily
    designed to generate images influenced by text descriptions but it can be used
    for other tasks such as inpainting or video creation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散[1]是CompVis Group于2022年发布的生成性AI模型，可以根据文本和图像提示生成逼真的图像。它主要设计用于生成受文本描述影响的图像，但也可以用于其他任务，如修复或视频创建。
- en: Its success comes from the **Perceptual Image Compression** step that converts
    a high dimensional image into a smaller latent space. This compression enables
    the use of the model in low-resourced machines making it accessible to everyone,
    something that was not possible with the previous state-of-the-art models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 它的成功源于 **感知图像压缩** 步骤，该步骤将高维图像转换为更小的潜在空间。这种压缩使得在资源有限的机器上使用该模型成为可能，使每个人都能使用，这在之前的最先进模型中是不可能的。
- en: '![](../Images/79fe033603707036fea343f9ec147dc1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79fe033603707036fea343f9ec147dc1.png)'
- en: 'Figure 2: Stable Diffusion architecture ([source](https://arxiv.org/pdf/2112.10752.pdf))'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Stable Diffusion 架构 ([source](https://arxiv.org/pdf/2112.10752.pdf))
- en: How does it learn?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何学习的？
- en: Stable Diffusion is a **Latent Diffusion Model** (LDM) with three main components
    (**variational autoencoder** (VAE) [2], **U-Net** [3] and an optional **text encoder**)
    that learns how to denoise images conditioned by a prompt (text or other image)
    in order to create a new image.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion 是一个 **潜在扩散模型** (LDM)，具有三个主要组件 (**变分自编码器** (VAE) [2]、**U-Net**
    [3] 和一个可选的 **文本编码器**)，它学习如何在提示（文本或其他图像）的条件下去噪图像，以创建新图像。
- en: 'The training process of Stable Diffusion has 5 main steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion 的训练过程有 5 个主要步骤：
- en: 1.The **Perceptual Image Compression** step consists in an **Encoder** that
    receives an image with a dimension of *512x512x3* and encodes it into a smaller
    latent space *Z* with a dimension of *64x64x4\.* To better preserve the details
    of an image (for example, the eyes in human face),the latent space *Z* is regularized
    using a low-weighted Kullback-Leibler-term to make it zero centered and to obtain
    a small variance.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 1. **感知图像压缩** 步骤包括一个 **编码器**，它接收一个尺寸为 *512x512x3* 的图像，并将其编码为一个尺寸为 *64x64x4*
    的更小的潜在空间 *Z*。为了更好地保留图像的细节（例如，人脸中的眼睛），潜在空间 *Z* 使用低权重的 Kullback-Leibler 项进行正则化，以使其零中心并获得小方差。
- en: '![](../Images/f30de379460e03b36399513a035cca84.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f30de379460e03b36399513a035cca84.png)'
- en: 'Figure 3: Perceptual Image Compression process where the Encoder converts a
    512x512x3 image to a latent space of 64x64x4 (image made by the author).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：感知图像压缩过程，其中编码器将 512x512x3 的图像转换为 64x64x4 的潜在空间（图像由作者制作）。
- en: 2\. The **Diffusion Process** is responsible to progressively add Gaussian noise
    to the latent space *Z,* until all that remains is random noise, generating a
    new latent space ***Zt.*** ***t*** is the number of times the diffusion process
    occurred to achieve a full noisy latent space.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **扩散过程** 负责逐步向潜在空间 *Z* 添加高斯噪声，直到所有剩下的是随机噪声，生成一个新的潜在空间 ***Zt.*** ***t***
    是扩散过程发生的次数，以实现完全的噪声潜在空间。
- en: This step is important because Stable Diffusion has to learn how to go from
    noise to the original image as we will see in the next steps.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步骤很重要，因为 Stable Diffusion 必须学习如何从噪声恢复到原始图像，正如我们将在下一步中看到的那样。
- en: '![](../Images/08685adba6bacd6db70f947657a72950.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08685adba6bacd6db70f947657a72950.png)'
- en: 'Figure 4: Diffusion Process where Gaussian noise is added gradually to the
    latent space (image made by the author)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：扩散过程，其中高斯噪声逐渐添加到潜在空间中（图像由作者制作）
- en: 3\. The **Denoising** **Process** trains a U-Net architecture to estimate the
    amount of noise in the latent space ***Zt*** in order to subtract it and restore
    *Z*. This process is able to recover the original latent space *Z* by gradually
    denoise *Zt,* basically, the inverse of the Diffusion Process.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. **去噪** **过程** 训练了一个 U-Net 架构来估计潜在空间 ***Zt*** 中的噪声量，以便减去它并恢复 *Z*。这个过程通过逐渐去噪
    *Zt* 来恢复原始的潜在空间 *Z*，基本上是扩散过程的逆过程。
- en: '![](../Images/5778395079858775325368a96391d2aa.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5778395079858775325368a96391d2aa.png)'
- en: 'Figure 5: Denoising Process where U-Net predicts the noise in a latent space
    and removes it until it completely restores the original latent space (image made
    by the author)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：去噪过程，其中 U-Net 预测潜在空间中的噪声并将其去除，直到完全恢复原始潜在空间（图像由作者制作）
- en: 4\. During the **Denoising** **Process** a prompt, usually text and/or other
    image, can be concatenated to the latent space *Zt.* This concatenation will condition
    the Denoising Process which allows the creation of new images. The authors added
    cross-attention mechanisms in the backbone of U-Net to handle these prompts since
    they are effective for learning attention-based models of various inputs types.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 在 **去噪** **过程** 中，可以将提示，通常是文本和/或其他图像，连接到潜在空间 *Zt*。这种连接将条件化去噪过程，从而允许创建新图像。作者在
    U-Net 的骨干网络中添加了交叉注意机制来处理这些提示，因为它们对学习各种输入类型的基于注意的模型是有效的。
- en: When it comes to text, the model uses a trained text encoder **CLIP** [4] that
    encodes the prompt into a 768-dimensional vector which is then concatenated to
    *Zt* and received by U-Net as input.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到文本时，模型使用训练好的文本编码器**CLIP** [4]，该编码器将提示编码为一个768维的向量，然后将其与*Zt*连接，并作为输入传递给U-Net。
- en: As we can see in Figure 6, we concatenated to *Zt* the text prompt *“remove
    the lamp”,* which conditioned the Diffusion Process restoring a *Zt* without the
    lamp near the chair that the original *Zt* had.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在图6中所见，我们将文本提示*“移除灯”*与*Zt*进行连接，这使得Diffusion Process对*Zt*进行调整，去除了原始*Zt*中靠近椅子的灯。
- en: '![](../Images/4b95fd7017e1a2147e6be92de15bf4f1.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b95fd7017e1a2147e6be92de15bf4f1.png)'
- en: 'Figure 6: Condition the denoising process with a text prompt to remove the
    lamp in the original image (image made by the author)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：使用文本提示来去除原始图像中的灯光的去噪过程（图片由作者制作）
- en: 5\. Finally, the **Decoder** receives the denoised latent space *Z* as input
    and it learns how to estimate the component-wise variance used to encode the image
    into a smaller latent space. After estimating the variance, the Decoder can generate
    a new image with the same dimension of the original one.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 最终，**解码器**接收去噪后的潜在空间*Z*作为输入，并学习如何估计用于将图像编码到更小潜在空间中的每个组件的方差。在估计方差后，解码器可以生成与原始图像相同尺寸的新图像。
- en: '![](../Images/fa1d17e8e78ea64e86b60bc59ec9046e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa1d17e8e78ea64e86b60bc59ec9046e.png)'
- en: 'Figure 7: Decoder restores the original image without the lamp and with the
    original size of 512x512x3 (image made by the author)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：解码器恢复了没有灯的原始图像，尺寸为512x512x3（图片由作者制作）
- en: Inpainting Variant of Stable Diffusion
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Stable Diffusion的修补变体
- en: Inpainting is the task of filling masked regions of an image with new content
    either because we want to uncorrupt the image or because we want to replace some
    undesired content.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 修补任务是用新内容填补图像中被遮蔽的区域，无论是因为我们想要修复图像还是因为我们想要替换一些不希望出现的内容。
- en: Stable Diffusion can be trained to generate new images based on an image, a
    text prompt and a mask. This type of model is already available in HuggingFace
    🤗 and its called *runwayml/stable-diffusion-inpainting*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion可以被训练生成基于图像、文本提示和掩膜的新图像。这种模型已经在HuggingFace 🤗 上可用，名称为*runwayml/stable-diffusion-inpainting*。
- en: To train Stable Diffusion to perform inpainting, we need to go through the same
    steps mentioned in the section above but with a slightly change on the input data.
    In this case apart from having the original image and text, we also have a mask
    (another image). For that, the U-Net needs to be adapted to receive an additional
    input channel for the mask.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练Stable Diffusion进行修补，我们需要经历上述部分提到的相同步骤，但输入数据稍有不同。在这种情况下，除了原始图像和文本，我们还需要一个掩膜（另一张图像）。为此，U-Net需要适应以接收额外的掩膜输入通道。
- en: During training, the area that is not under the mask remains untouched and it
    is only encoded to the latent space, while the masked area goes through the all
    process of encoding, diffusion and denoising. This way, the Stable Diffusion model
    knows which area should remain the same and which area should change (Figure 8
    illustrates this process).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，掩膜下的区域保持不变，只对其进行编码到潜在空间，而掩膜区域则经历整个编码、扩散和去噪过程。这样，Stable Diffusion模型知道哪些区域应该保持不变，哪些区域应该发生变化（图8展示了这一过程）。
- en: '![](../Images/a22262eba205e0bec3179cef1acab740.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a22262eba205e0bec3179cef1acab740.png)'
- en: 'Figure 8: Training process of Inpainting Diffusion Stable model (image made
    by the author)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：修补扩散Stable模型的训练过程（图片由作者制作）
- en: In Figure 9, we have an example of what is needed to perform inpaitining on
    our own uses cases. We give the original image together with a mask of what we
    want to change and a text prompt with the change we want to see and Stable Diffusion
    generates a new image. In the next section, we will see how to do it in practice.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在图9中，我们展示了在自己的使用案例中进行修补所需的示例。我们提供原始图像以及我们希望更改的掩膜和包含我们想看到更改的文本提示，Stable Diffusion生成新图像。在下一节中，我们将看到如何实际操作。
- en: '![](../Images/134c22d473a948035e00308ece16a734.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/134c22d473a948035e00308ece16a734.png)'
- en: 'Figure 9: Process to train Stable Diffusion for image inpainting (image made
    by the author)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：训练Stable Diffusion进行图像修补的过程（图片由作者制作）
- en: Interior Design using Stable Diffusion
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Stable Diffusion的室内设计
- en: In this section I will cover how to use Stable Diffusion in an inpainting scenario
    for interior design.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将讲解如何在室内设计中使用Stable Diffusion进行修补场景的操作。
- en: When it comes to buy a new house or apartment that is still under construction,
    usually we have access to 3D images of how it will look. Based on those images
    we can request to change colours or materials to make it tailored to our taste.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们购买一栋新房或仍在建设中的公寓时，通常可以获取它的 3D 图像。基于这些图像，我们可以要求更改颜色或材料，以使其符合我们的口味。
- en: However, it is not easy to imagine if the changes that we are requesting will
    fit the rest of the house or not, and asking for a new 3D might be expensive and
    time consuming. Therefore, we can use stable diffusion to quickly iterate and
    get a sense of how things will look if we apply the changes we want.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，很难想象我们请求的更改是否适合房子的其他部分，而请求新的 3D 可能会很昂贵且耗时。因此，我们可以使用 Stable Diffusion 快速迭代，并了解如果应用我们想要的更改，事物会是什么样子。
- en: For that we can use Python and HuggingFace 🤗 to build our own Stable Diffusion
    Interior Designer!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以使用 Python 和 HuggingFace 🤗 来构建我们自己的 Stable Diffusion 室内设计师！
- en: 'We start by importing the libraries:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入库：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we load the Stable Diffusion Inpainting model available in HuggingFace
    🤗:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们加载 HuggingFace 🤗 提供的 Stable Diffusion Inpainting 模型：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With the model loaded, we load the original image and the mask of what we want
    to change:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载了模型后，我们加载原始图像和我们想要更改的遮罩：
- en: The white part of the mask is what will change while the black part is what
    will remain untouched.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遮罩的白色部分是将要更改的部分，而黑色部分是将保持不变的部分。
- en: The mask was manually created, but we can also use a segmentation model to create
    it.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遮罩是手动创建的，但我们也可以使用分割模型来创建它。
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/b3f7c5c57a1d1d2bb992b2409a31eb42.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3f7c5c57a1d1d2bb992b2409a31eb42.png)'
- en: 'Figure 10: 3D image of a kitchen (purchased by the author) and a mask to change
    the island and the black wall (image made by the author)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：厨房的 3D 图像（由作者购买）和一个遮罩，用于更改岛台和黑色墙壁（图像由作者制作）
- en: With both image and mask loaded, it is time to create the prompt to condition
    the image generation to what we want and generate new images.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载了图像和遮罩后，现在是创建提示以将图像生成条件设置为我们想要的内容并生成新图像的时候了。
- en: In this case, I want to replace the black island and the black wall to a marble
    island and a marble wall.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种情况下，我想将黑色岛台和黑色墙壁替换为大理石岛台和大理石墙壁。
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/09742ef89e90a9391e0f55615bcffa7d.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09742ef89e90a9391e0f55615bcffa7d.png)'
- en: 'Figure 11: Comparison between original and my favourite generated image (they
    have different sizes because the model was trained with 512x512 images)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：原始图像与我最喜欢的生成图像的比较（它们的尺寸不同，因为模型是用 512x512 图像训练的）
- en: 'The result looks good, but I also want to replace the wooden kitchen cabinets
    with white ones, so let’s redo the process:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来不错，但我还想将木制厨房橱柜替换为白色的，所以让我们重新进行这个过程：
- en: 'Load the last image generated and a new mask:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载最后生成的图像和一个新的遮罩：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/c99eebffd0a561b6c60a0e14d261b659.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c99eebffd0a561b6c60a0e14d261b659.png)'
- en: 'Figure 12: Previous generated image and new mask (image made by the author)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：之前生成的图像和新的遮罩（图像由作者制作）
- en: '2\. Create a new prompt and generate new images:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 创建新的提示并生成新图像：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/0b7e996ee6af90b86a437dc19e4ed3d0.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b7e996ee6af90b86a437dc19e4ed3d0.png)'
- en: 'Figure 13: Generated image with white kitchen cabinets (image made by the author)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：带有白色厨房橱柜的生成图像（图像由作者制作）
- en: The result looks really good and I would like you to notice the details that
    Stable Diffusion is able to reproduced such as the kitchen tap or the reflection
    of lights in the cabinets. Although the reflection on the left is not aligned
    is incredible how it managed to take the lights into consideration.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来非常好，我希望你注意到 Stable Diffusion 能够重现的细节，例如厨房水龙头或橱柜上的光线反射。尽管左侧的反射没有对齐，但令人惊讶的是它如何考虑了光线。
- en: Conclusion
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: AI is not only useful for organisations with large amounts of data, it can be
    applied to anything we can think of!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: AI 不仅对拥有大量数据的组织有用，它还可以应用于我们能想到的任何事物！
- en: In this article, we explored Stable Diffusion for a non-traditional use case
    but for a traditional job that exists for decades. With a few lines of code we
    managed to generate as many different images as we wanted for each prompt which
    gives us a lot of possibilities to choose from.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探索了 Stable Diffusion 的一种非传统用途，但这是一个存在了几十年的传统工作。通过几行代码，我们能够为每个提示生成尽可能多的不同图像，这给了我们很多选择的可能性。
- en: However, like everything else, Stable Diffusion and, in particular the model
    we used, has its own limitations such as not achieving perfect photorealism as
    we saw in the light reflection or in Figure 14 where one of the chairs is inside
    of the kitchen island.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与其他事物一样，稳定扩散模型，特别是我们使用的模型，也有其自身的局限性，例如在光线反射中或在图 14 中，其中一把椅子位于厨房岛内，并未实现完美的真实感。
- en: Nevertheless, the future of AI looks bright and some of this limitations can
    be overcome with a fine-tune in our own data and for our own use cases.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，人工智能的未来仍然光明，其中一些局限性可以通过在我们自己的数据和使用案例中进行微调来克服。
- en: '![](../Images/cc6bc6cb87942b845703843d69171247.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc6bc6cb87942b845703843d69171247.png)'
- en: 'Figure 14: Generated image with some quality problems (image made by the author)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：生成的图像存在一些质量问题（图像由作者制作）
- en: '**Keep in touch:** [LinkedIn](https://www.linkedin.com/in/rafaelguedes97/),
    [Medium](https://medium.com/@rjguedes97)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**保持联系：** [LinkedIn](https://www.linkedin.com/in/rafaelguedes97/)， [Medium](https://medium.com/@rjguedes97)'
- en: References
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn
    Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. arXiv:2001.08210,
    2022'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn
    Ommer. 高分辨率图像合成与潜在扩散模型。arXiv:2001.08210, 2022'
- en: '[2] Diederik P. Kingma, Max Welling. An Introduction to Variational Autoencoders.
    arXiv:1906.02691, 2019'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Diederik P. Kingma, Max Welling. 变分自编码器简介。arXiv:1906.02691, 2019'
- en: '[3] Olaf Ronneberger, Philipp Fischer, Thomas Brox. U-Net: Convolutional Networks
    for Biomedical Image Segmentation. arXiv:1505.04597, 2015'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Olaf Ronneberger, Philipp Fischer, Thomas Brox. U-Net: 用于生物医学图像分割的卷积网络。arXiv:1505.04597,
    2015'
- en: '[4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
    Krueger, Ilya Sutskever. Learning Transferable Visual Models From Natural Language
    Supervision. arXiv:1911.02116, 2021'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
    Krueger, Ilya Sutskever. 从自然语言监督中学习可转移的视觉模型。arXiv:1911.02116, 2021'
