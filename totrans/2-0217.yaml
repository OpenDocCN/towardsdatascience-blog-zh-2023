- en: A Practical Introduction to LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ç”¨ä»‹ç»LLMs
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148](https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148](https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148)
- en: 3 levels of using LLMs in practice
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®è·µä¸­ä½¿ç”¨LLMçš„3ä¸ªå±‚æ¬¡
- en: '[](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)
    Â·7 min readÂ·Jul 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)
    Â·é˜…è¯»æ—¶é—´7åˆ†é’ŸÂ·2023å¹´7æœˆ13æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This is the first article in a [series](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)
    on using Large Language Models (LLMs) in practice. Here I will give an introduction
    to LLMs and present 3 levels of working with them. Future articles will explore
    practical aspects of LLMs, such as how to use [OpenAIâ€™s public API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971),
    the [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    Python library, how to [fine-tune LLMs](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91),
    and [how to build an LLM from scratch](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å…³äºå®è·µä¸­ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„[ç³»åˆ—æ–‡ç« ](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)çš„ç¬¬ä¸€ç¯‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å°†ä»‹ç»LLMå¹¶å±•ç¤º3ä¸ªå±‚æ¬¡çš„ä½¿ç”¨æ–¹å¼ã€‚æœªæ¥çš„æ–‡ç« å°†æ·±å…¥æ¢è®¨LLMçš„å®é™…åº”ç”¨ï¼Œä¾‹å¦‚å¦‚ä½•ä½¿ç”¨[OpenAIçš„å…¬å…±API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)ã€[Hugging
    Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    Pythonåº“ã€å¦‚ä½•[å¾®è°ƒLLM](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)ä»¥åŠ[å¦‚ä½•ä»é›¶å¼€å§‹æ„å»ºLLM](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9)ã€‚
- en: '![](../Images/a92d8deb44c331cde299747e18da5c7f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a92d8deb44c331cde299747e18da5c7f.png)'
- en: Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)æä¾›ï¼Œæ¥æºäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**What is an LLM?**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä»€ä¹ˆæ˜¯LLMï¼Ÿ**'
- en: '**LLM** is short for **Large Language Model**, which is a recent innovation
    in AI and machine learning. This powerful new type of AI went viral in Dec 2022
    with the release of ChatGPT.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM** æ˜¯**å¤§å‹è¯­è¨€æ¨¡å‹**çš„ç¼©å†™ï¼Œè¿™æ˜¯ä¸€ç§æœ€è¿‘åœ¨AIå’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„åˆ›æ–°ã€‚è¿™ä¸ªå¼ºå¤§çš„æ–°å‹AIåœ¨2022å¹´12æœˆé€šè¿‡ChatGPTçš„å‘å¸ƒè€Œè¿…é€Ÿä¼ æ’­ã€‚'
- en: For those enlightened enough to live outside the world of AI buzz and tech news
    cycles, **ChatGPT** is a chat interface that ran on an LLM called GPT-3 (now upgraded
    to either GPT-3.5 or GPT-4 at the time of writing this).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºé‚£äº›è¶³å¤Ÿå¼€æ˜ã€ç”Ÿæ´»åœ¨AIçƒ­ç‚¹å’ŒæŠ€æœ¯æ–°é—»å‘¨æœŸä¹‹å¤–çš„äººæ¥è¯´ï¼Œ**ChatGPT** æ˜¯ä¸€ä¸ªåŸºäºåä¸ºGPT-3ï¼ˆåœ¨æ’°å†™æ—¶å·²å‡çº§ä¸ºGPT-3.5æˆ–GPT-4ï¼‰çš„LLMçš„èŠå¤©ç•Œé¢ã€‚
- en: If youâ€™ve used ChatGPT, itâ€™s obvious that this is not your traditional chatbot
    from [AOL Instant Messenger](https://en.wikipedia.org/wiki/AIM_(software)) or
    your credit cardâ€™s customer care.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä½¿ç”¨è¿‡ChatGPTï¼Œå¾ˆæ˜æ˜¾è¿™ä¸æ˜¯ä½ ä¼ ç»Ÿçš„æ¥è‡ª[AOLå³æ—¶æ¶ˆæ¯](https://en.wikipedia.org/wiki/AIM_(software))æˆ–ä½ çš„ä¿¡ç”¨å¡å®¢æˆ·æœåŠ¡çš„èŠå¤©æœºå™¨äººã€‚
- en: This one *feels* different.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¯‡æ–‡ç« *æ„Ÿè§‰*æœ‰æ‰€ä¸åŒã€‚
- en: '**What makes an LLM â€œlargeâ€?**'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä»€ä¹ˆä½¿LLMâ€œå·¨å¤§â€ï¼Ÿ**'
- en: When I heard the term â€œLarge Language Model,â€ one of my first questions was,
    *how is this different from a â€œregularâ€ language model?*
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘å¬åˆ°â€œå¤§å‹è¯­è¨€æ¨¡å‹â€è¿™ä¸ªæœ¯è¯­æ—¶ï¼Œæˆ‘ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œ*è¿™ä¸â€œå¸¸è§„â€è¯­è¨€æ¨¡å‹æœ‰ä½•ä¸åŒï¼Ÿ*
- en: A language model is more generic than a large language model. Just like all
    squares are rectangles but not all rectangles are squares. **All LLMs are language
    models, but not all language models are LLMs**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹æ¯”å¤§è¯­è¨€æ¨¡å‹æ›´ä¸ºé€šç”¨ã€‚å°±åƒæ‰€æœ‰çš„æ­£æ–¹å½¢éƒ½æ˜¯çŸ©å½¢ï¼Œä½†å¹¶éæ‰€æœ‰çš„çŸ©å½¢éƒ½æ˜¯æ­£æ–¹å½¢ã€‚**æ‰€æœ‰çš„LLMéƒ½æ˜¯è¯­è¨€æ¨¡å‹ï¼Œä½†å¹¶éæ‰€æœ‰çš„è¯­è¨€æ¨¡å‹éƒ½æ˜¯LLM**ã€‚
- en: '![](../Images/f1538bc098dd64cea39915cbb331d339.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1538bc098dd64cea39915cbb331d339.png)'
- en: Large Language Models are a special type of Language Model. Image by author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§ç‰¹æ®Šç±»å‹çš„è¯­è¨€æ¨¡å‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Okay, so LLMs are a special type of language model, **but what makes them special?**
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼ŒLLMæ˜¯ä¸€ç§ç‰¹æ®Šç±»å‹çš„è¯­è¨€æ¨¡å‹ï¼Œ**ä½†æ˜¯ä»€ä¹ˆè®©å®ƒä»¬ä¸ä¼—ä¸åŒå‘¢ï¼Ÿ**
- en: There are **2 key properties** that distinguish LLMs from other language models.
    One is quantitative, and the other is qualitative.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰**2ä¸ªå…³é”®ç‰¹æ€§**å°†LLMä¸å…¶ä»–è¯­è¨€æ¨¡å‹åŒºåˆ†å¼€æ¥ã€‚å…¶ä¸­ä¸€ä¸ªæ˜¯å®šé‡çš„ï¼Œå¦ä¸€ä¸ªæ˜¯å®šæ€§çš„ã€‚
- en: '**Quantitatively**, what distinguishes an LLM is the number of parameters used
    in the model. Current LLMs have on the order of **10â€“100 billion parameters**
    [1].'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å®šé‡ä¸Š**ï¼ŒLLMçš„åŒºåˆ«åœ¨äºæ¨¡å‹ä¸­ä½¿ç”¨çš„å‚æ•°æ•°é‡ã€‚å½“å‰LLMçš„å‚æ•°æ•°é‡åœ¨**100äº¿åˆ°1000äº¿ä¹‹é—´** [1]ã€‚'
- en: '**Qualitatively**, something remarkable happens when a language model becomes
    â€œlarge.â€ It exhibits so-called ***emergent properties***e.g. zero-shot learning
    [1]. These are **properties that seem to suddenly appear** when a language model
    reaches a sufficiently large size.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å®šæ€§ä¸Š**ï¼Œå½“è¯­è¨€æ¨¡å‹å˜å¾—â€œåºå¤§â€æ—¶ï¼Œä¼šå‘ç”Ÿä¸€äº›æ˜¾è‘—çš„å˜åŒ–ã€‚å®ƒè¡¨ç°å‡ºæ‰€è°“çš„***çªç°ç‰¹æ€§***ä¾‹å¦‚é›¶ shot å­¦ä¹  [1]ã€‚è¿™äº›æ˜¯**åœ¨è¯­è¨€æ¨¡å‹è¾¾åˆ°è¶³å¤Ÿå¤§çš„è§„æ¨¡æ—¶ä¼¼ä¹çªç„¶å‡ºç°çš„ç‰¹æ€§**ã€‚'
- en: '**Zero-shot Learning**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**é›¶ shot å­¦ä¹ **'
- en: The major innovation of GPT-3 (and other LLMs) is that it is capable of **zero-shot
    learning** in a wide variety of contexts [2]. This means ChatGPT can **perform
    a task even if it has not been explicitly trained to do it**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3ï¼ˆåŠå…¶ä»–LLMï¼‰çš„ä¸»è¦åˆ›æ–°åœ¨äºå®ƒèƒ½å¤Ÿåœ¨å„ç§ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œ**é›¶ shot å­¦ä¹ ** [2]ã€‚è¿™æ„å‘³ç€ChatGPTå¯ä»¥**æ‰§è¡Œå³ä½¿æœªè¢«æ˜ç¡®è®­ç»ƒè¿‡çš„ä»»åŠ¡**ã€‚
- en: While this might be no big deal to us highly evolved humans, this zero-shot
    learning ability starkly contrasts the prior machine learning paradigm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™å¯¹æˆ‘ä»¬é«˜åº¦è¿›åŒ–çš„äººå·¥æ¥è¯´å¯èƒ½æ²¡ä»€ä¹ˆå¤§ä¸äº†ï¼Œä½†è¿™ç§é›¶-shot å­¦ä¹ èƒ½åŠ›ä¸ä¹‹å‰çš„æœºå™¨å­¦ä¹ èŒƒå¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚
- en: Previously, a model needed to be **explicitly trained on the task it aimed to
    do** in order to have good performance. This could require anywhere from 1k-1M
    pre-labeled training examples.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥å‰ï¼Œæ¨¡å‹éœ€è¦**æ˜ç¡®åœ°åœ¨å…¶æ—¨åœ¨å®Œæˆçš„ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒ**æ‰èƒ½æœ‰è‰¯å¥½çš„è¡¨ç°ã€‚è¿™å¯èƒ½éœ€è¦ä»1åƒåˆ°1ç™¾ä¸‡ä¸ªé¢„æ ‡è®°çš„è®­ç»ƒæ ·æœ¬ã€‚
- en: For instance, if you wanted a computer to do language translation, sentiment
    analysis, and identify grammatical errors. Each of these tasks would require a
    specialized model trained on a large set of labeled examples. Now, however, **LLMs
    can do all these things without explicit training**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³è®©è®¡ç®—æœºè¿›è¡Œè¯­è¨€ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æå’Œè¯†åˆ«è¯­æ³•é”™è¯¯ã€‚è¿™äº›ä»»åŠ¡ä¸­çš„æ¯ä¸€ä¸ªéƒ½éœ€è¦ä¸€ä¸ªåœ¨å¤§é‡æ ‡è®°æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒçš„ä¸“é—¨æ¨¡å‹ã€‚ç„¶è€Œï¼Œç°åœ¨**LLMå¯ä»¥åœ¨æ²¡æœ‰æ˜ç¡®è®­ç»ƒçš„æƒ…å†µä¸‹å®Œæˆæ‰€æœ‰è¿™äº›ä»»åŠ¡**ã€‚
- en: '**How do LLMs work?**'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**LLMæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ**'
- en: The core task used to train most state-of-the-art LLMs is **word prediction**.
    In other words, given a sequence of words, **what is the probability distribution
    of the next word**?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¤§å¤šæ•°æœ€å…ˆè¿›LLMçš„æ ¸å¿ƒä»»åŠ¡æ˜¯**è¯é¢„æµ‹**ã€‚æ¢å¥è¯è¯´ï¼Œç»™å®šä¸€ç³»åˆ—å•è¯ï¼Œ**ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒæ˜¯ä»€ä¹ˆ**ï¼Ÿ
- en: 'For example, given the sequence â€œListen to your ____,â€ the most likely next
    words might be: heart, gut, body, parents, grandma, etc. This might look like
    the probability distribution shown below.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¯¹äºåºåˆ—â€œå¬ä»ä½ çš„____â€ï¼Œæœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯å¯èƒ½æ˜¯ï¼šå¿ƒã€ç›´è§‰ã€èº«ä½“ã€çˆ¶æ¯ã€å¥¶å¥¶ç­‰ã€‚è¿™å¯èƒ½çœ‹èµ·æ¥åƒä¸‹å›¾æ‰€ç¤ºçš„æ¦‚ç‡åˆ†å¸ƒã€‚
- en: '![](../Images/5012798faee287a80d6de31d5d742bbd.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5012798faee287a80d6de31d5d742bbd.png)'
- en: Toy probability distribution of next word in sequence â€œListen to your ___.â€
    Image by author.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: â€œå¬ä»ä½ çš„___â€åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯çš„ç©å…·æ¦‚ç‡åˆ†å¸ƒã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Interestingly, this is the same way many (non-large) language models have been
    trained in the past (e.g. GPT-1) [3]. However, for some reason, when language
    models get beyond a certain size (say ~10B parameters), these (emergent) abilities,
    such as zero-shot learning, can start to pop up [1].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œè¿‡å»è®¸å¤šï¼ˆéå¤§è§„æ¨¡ï¼‰è¯­è¨€æ¨¡å‹ä¹Ÿæ˜¯ä»¥è¿™ç§æ–¹å¼è¿›è¡Œè®­ç»ƒçš„ï¼ˆä¾‹å¦‚GPT-1ï¼‰[3]ã€‚ç„¶è€Œï¼Œç”±äºæŸç§åŸå› ï¼Œå½“è¯­è¨€æ¨¡å‹è¶…è¿‡ä¸€å®šè§„æ¨¡ï¼ˆæ¯”å¦‚~10Bå‚æ•°ï¼‰æ—¶ï¼Œè¿™äº›ï¼ˆçªç°çš„ï¼‰èƒ½åŠ›ï¼Œå¦‚é›¶
    shot å­¦ä¹ ï¼Œå¯èƒ½ä¼šå¼€å§‹å‡ºç°[1]ã€‚
- en: Although there is no clear answer as to *why* this occurs (only speculations
    for now), it is clear that LLMs are a powerful technology with countless potential
    use cases.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç›®å‰å¯¹*ä¸ºä»€ä¹ˆ*ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µæ²¡æœ‰æ˜ç¡®çš„ç­”æ¡ˆï¼ˆç›®å‰åªæ˜¯çŒœæµ‹ï¼‰ï¼Œä½†æ˜¾ç„¶LLMæ˜¯ä¸€é¡¹å¼ºå¤§çš„æŠ€æœ¯ï¼Œå…·æœ‰æ— æ•°çš„æ½œåœ¨åº”ç”¨ã€‚
- en: '**3 Levels of Using LLMs**'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨LLMçš„3ä¸ªå±‚æ¬¡**'
- en: Now we turn to how to use this powerful technology in practice. While there
    are countless potential LLM use cases, here I categorize them into 3 levels **ordered
    by required technical knowledge and computational resources**. We start with the
    most accessible.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æ¥æ¢è®¨å¦‚ä½•åœ¨å®è·µä¸­ä½¿ç”¨è¿™ä¸€å¼ºå¤§æŠ€æœ¯ã€‚è™½ç„¶ LLM çš„æ½œåœ¨ç”¨ä¾‹æ— æ•°ï¼Œä½†æˆ‘åœ¨è¿™é‡Œå°†å…¶åˆ†ä¸º 3 ä¸ªçº§åˆ«ï¼Œ**æŒ‰æ‰€éœ€çš„æŠ€æœ¯çŸ¥è¯†å’Œè®¡ç®—èµ„æºæ’åº**ã€‚æˆ‘ä»¬ä»æœ€å®¹æ˜“çš„å¼€å§‹ã€‚
- en: '**Level 1: Prompt Engineering**'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç¬¬ 1 çº§ï¼šæç¤ºå·¥ç¨‹**'
- en: The first level of using LLMs in practice is **prompt engineering**, which I
    define as **any use of an LLM out-of-the-box** i.e. not changing any model parameters.
    While many technically-inclined individuals seem to scoff at the idea of prompt
    engineering, this is the most accessible way to use LLMs (both technically and
    economically) in practice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å®è·µä¸­ä½¿ç”¨ LLMs çš„ç¬¬ä¸€ä¸ªçº§åˆ«æ˜¯**æç¤ºå·¥ç¨‹**ï¼Œæˆ‘å°†å…¶å®šä¹‰ä¸º**å¼€ç®±å³ç”¨çš„ LLM ä»»ä½•ç”¨æ³•**ï¼Œå³ä¸æ›´æ”¹ä»»ä½•æ¨¡å‹å‚æ•°ã€‚è™½ç„¶è®¸å¤šæŠ€æœ¯å€¾å‘çš„ä¸ªäººä¼¼ä¹å¯¹æç¤ºå·¥ç¨‹å—¤ä¹‹ä»¥é¼»ï¼Œä½†è¿™æ˜¯åœ¨å®è·µä¸­ä½¿ç”¨
    LLMsï¼ˆæ— è®ºæ˜¯æŠ€æœ¯ä¸Šè¿˜æ˜¯ç»æµä¸Šï¼‰æœ€å¯åŠçš„æ–¹å¼ã€‚
- en: '[](/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----65194dda1148--------------------------------)
    [## Prompt Engineering â€” How to trick AI into solving your problems'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[## æç¤ºå·¥ç¨‹ â€” å¦‚ä½•è®© AI è§£å†³ä½ çš„é—®é¢˜](https://example.org/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----65194dda1148--------------------------------)'
- en: 7 prompting tricks, Langchain, and Python example code
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7 ä¸ªæç¤ºæŠ€å·§ã€Langchain å’Œ Python ç¤ºä¾‹ä»£ç 
- en: towardsdatascience.com](/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----65194dda1148--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----65194dda1148--------------------------------)
- en: 'There are 2 main ways to do prompt engineering: the **Easy Way** and the **Less
    Easy Way**.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºå·¥ç¨‹æœ‰ä¸¤ç§ä¸»è¦æ–¹å¼ï¼š**ç®€å•æ–¹å¼**å’Œ**ä¸é‚£ä¹ˆç®€å•çš„æ–¹å¼**ã€‚
- en: '**The Easy Way: ChatGPT (or another convenient LLM UI) â€”** The key benefit
    of this method is convenience. Tools like ChatGPT provide an intuitive, no-cost,
    and no-code way to use an LLM (it doesnâ€™t get much easier than that).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç®€å•æ–¹å¼ï¼šChatGPTï¼ˆæˆ–å…¶ä»–æ–¹ä¾¿çš„ LLM ç”¨æˆ·ç•Œé¢ï¼‰ â€”** è¿™ç§æ–¹æ³•çš„å…³é”®å¥½å¤„æ˜¯ä¾¿åˆ©ã€‚åƒ ChatGPT è¿™æ ·çš„å·¥å…·æä¾›äº†ä¸€ç§ç›´è§‚ã€æ— æˆæœ¬ä¸”æ— éœ€ç¼–ç çš„ä½¿ç”¨
    LLM çš„æ–¹å¼ï¼ˆæ²¡æœ‰æ¯”è¿™æ›´ç®€å•çš„äº†ï¼‰ã€‚'
- en: However, convenience often comes at a cost. In this case, there are **2 key
    drawbacks** to this approach. The **first** is a lack of functionality. For example,
    ChatGPT does not readily enable users to customize model input parameters (e.g.
    temperature or max response length), which are values that modulate LLM outputs.
    **Second**, interactions with the ChatGPT UI cannot be readily automated and thus
    applied to large-scale use cases.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä¾¿åˆ©é€šå¸¸ä¼´éšä»£ä»·ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™ç§æ–¹æ³•æœ‰**ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹**ã€‚**ç¬¬ä¸€ä¸ª**æ˜¯åŠŸèƒ½æ€§ä¸è¶³ã€‚ä¾‹å¦‚ï¼ŒChatGPT å¹¶ä¸å®¹æ˜“è®©ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹è¾“å…¥å‚æ•°ï¼ˆå¦‚æ¸©åº¦æˆ–æœ€å¤§å“åº”é•¿åº¦ï¼‰ï¼Œè¿™äº›å‚æ•°ä¼šè°ƒèŠ‚
    LLM è¾“å‡ºã€‚**ç¬¬äºŒ**ï¼Œä¸ ChatGPT ç”¨æˆ·ç•Œé¢çš„äº¤äº’æ— æ³•è½»æ¾è‡ªåŠ¨åŒ–ï¼Œå› æ­¤ä¸èƒ½åº”ç”¨äºå¤§è§„æ¨¡ç”¨ä¾‹ã€‚
- en: While these drawbacks may be dealbreakers for some use cases, both can be ameliorated
    if we take prompt engineering one step further.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™äº›ç¼ºç‚¹å¯èƒ½å¯¹æŸäº›ç”¨ä¾‹æ¥è¯´æ˜¯è‡´å‘½çš„ï¼Œä½†å¦‚æœæˆ‘ä»¬å°†æç¤ºå·¥ç¨‹æ›´è¿›ä¸€æ­¥ï¼Œè¿™äº›ç¼ºç‚¹éƒ½å¯ä»¥å¾—åˆ°æ”¹å–„ã€‚
- en: '**The Less Easy Way: Interact with LLM directly â€”** We can overcome some of
    the drawbacks of ChatGPT by interacting directly with an LLM via programmatic
    interfaces. This could be via public APIs (e.g. OpenAIâ€™s API) or running an LLM
    locally (using libraries like [Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸é‚£ä¹ˆç®€å•çš„æ–¹å¼ï¼šç›´æ¥ä¸ LLM äº’åŠ¨ â€”** æˆ‘ä»¬å¯ä»¥é€šè¿‡ç¨‹åºåŒ–æ¥å£ç›´æ¥ä¸ LLM äº’åŠ¨ï¼Œä»è€Œå…‹æœ ChatGPT çš„ä¸€äº›ç¼ºç‚¹ã€‚è¿™å¯ä»¥é€šè¿‡å…¬å…±
    APIï¼ˆä¾‹å¦‚ OpenAI çš„ APIï¼‰æˆ–æœ¬åœ°è¿è¡Œ LLMï¼ˆä½¿ç”¨å¦‚ [Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    çš„åº“ï¼‰æ¥å®ç°ã€‚'
- en: While this way of doing prompt engineering is less convenient (since it requires
    programming knowledge and potential API costs), it provides a customizable, flexible,
    and scalable way to use LLMs in practice. Future articles in this series will
    discuss [paid](/cracking-open-the-openai-python-api-230e4cae7971) and [cost-free](/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    ways to do this type of prompt engineering.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™ç§æç¤ºå·¥ç¨‹æ–¹å¼ä¸é‚£ä¹ˆæ–¹ä¾¿ï¼ˆå› ä¸ºéœ€è¦ç¼–ç¨‹çŸ¥è¯†å’Œå¯èƒ½çš„ API æˆæœ¬ï¼‰ï¼Œä½†å®ƒæä¾›äº†ä¸€ç§å¯å®šåˆ¶ã€çµæ´»å’Œå¯æ‰©å±•çš„æ–¹å¼æ¥å®é™…ä½¿ç”¨ LLMsã€‚æœ¬ç³»åˆ—æœªæ¥çš„æ–‡ç« å°†è®¨è®º
    [ä»˜è´¹çš„](/cracking-open-the-openai-python-api-230e4cae7971) å’Œ [æ— æˆæœ¬çš„](/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    è¿™ç§ç±»å‹çš„æç¤ºå·¥ç¨‹æ–¹å¼ã€‚
- en: Although prompt engineering (as defined here) can handle most potential LLM
    applications, relying on a generic model, out-of-the-box may result in sub-optimal
    performance for specific use cases. For these situations, we can go to the next
    level of using LLMs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ï¼ˆåœ¨è¿™é‡Œå®šä¹‰çš„ï¼‰æç¤ºå·¥ç¨‹å¯ä»¥å¤„ç†å¤§å¤šæ•°æ½œåœ¨çš„ LLM åº”ç”¨ï¼Œä½†ä¾èµ–äºé€šç”¨æ¨¡å‹å’Œç°æˆçš„è§£å†³æ–¹æ¡ˆå¯èƒ½ä¼šå¯¼è‡´ç‰¹å®šç”¨ä¾‹çš„æ€§èƒ½ä¸ä½³ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥è¿›å…¥ä½¿ç”¨
    LLM çš„ä¸‹ä¸€ä¸ªçº§åˆ«ã€‚
- en: '**Level 2: Model Fine-tuning**'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç¬¬äºŒçº§ï¼šæ¨¡å‹å¾®è°ƒ**'
- en: The second level of using an LLM is **model fine-tuning**, which Iâ€™ll define
    as taking an existing LLM and tweaking it for a particular use case by **training
    at least 1 (internal) model parameter** i.e. weights and biases. For the aficionados
    out there, this is an example of *transfer learning* i.e. using some part of an
    existing model to develop another model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ LLM çš„ç¬¬äºŒçº§æ˜¯**æ¨¡å‹å¾®è°ƒ**ï¼Œæˆ‘å°†å…¶å®šä¹‰ä¸ºå¯¹ç°æœ‰ LLM è¿›è¡Œè°ƒæ•´ï¼Œä»¥é€šè¿‡**è®­ç»ƒè‡³å°‘ 1 ä¸ªï¼ˆå†…éƒ¨ï¼‰æ¨¡å‹å‚æ•°**ï¼ˆå³æƒé‡å’Œåå·®ï¼‰æ¥é€‚åº”ç‰¹å®šç”¨ä¾‹ã€‚å¯¹äºé‚£äº›å–œæ¬¢çš„äººæ¥è¯´ï¼Œè¿™æ˜¯*è¿ç§»å­¦ä¹ *çš„ä¸€ä¸ªä¾‹å­ï¼Œå³ä½¿ç”¨ç°æœ‰æ¨¡å‹çš„ä¸€éƒ¨åˆ†æ¥å¼€å‘å¦ä¸€ä¸ªæ¨¡å‹ã€‚
- en: 'Fine-tuning typically consists of 2 steps. **Step 1**: Obtain a pre-trained
    LLM. **Step 2**: Update model parameters for a specific task given (typically
    1000s of) high-quality labeled examples.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®è°ƒé€šå¸¸åŒ…æ‹¬ 2 ä¸ªæ­¥éª¤ã€‚**æ­¥éª¤ 1**ï¼šè·å–ä¸€ä¸ªé¢„è®­ç»ƒçš„ LLMã€‚**æ­¥éª¤ 2**ï¼šæ ¹æ®ç‰¹å®šä»»åŠ¡æ›´æ–°æ¨¡å‹å‚æ•°ï¼ˆé€šå¸¸æ˜¯æˆåƒä¸Šä¸‡çš„ï¼‰é«˜è´¨é‡æ ‡è®°ç¤ºä¾‹ã€‚
- en: The model parameters define the LLMâ€™s internal representation of the input text.
    Thus, by tweaking these parameters for a particular task, the internal representations
    become optimized for the fine-tuning task (or at least thatâ€™s the idea).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°å®šä¹‰äº† LLM å¯¹è¾“å…¥æ–‡æœ¬çš„å†…éƒ¨è¡¨ç¤ºã€‚å› æ­¤ï¼Œé€šè¿‡è°ƒæ•´è¿™äº›å‚æ•°ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡ï¼Œå†…éƒ¨è¡¨ç¤ºä¼šé’ˆå¯¹å¾®è°ƒä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼ˆæˆ–è€…è‡³å°‘è¿™æ˜¯å…¶ç†å¿µï¼‰ã€‚
- en: This is a powerful approach to model development because a relatively **small
    number of examples** and computational resources **can produce exceptional model
    performance**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§å¼ºå¤§çš„æ¨¡å‹å¼€å‘æ–¹æ³•ï¼Œå› ä¸ºç›¸å¯¹**å°‘é‡çš„ç¤ºä¾‹**å’Œè®¡ç®—èµ„æº**å¯ä»¥äº§ç”Ÿå“è¶Šçš„æ¨¡å‹æ€§èƒ½**ã€‚
- en: The downside, however, is it requires significantly more technical expertise
    and computational resources than prompt engineering. In a [future article](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91),
    I will attempt to curb this downside by reviewing fine-tuning techniques and sharing
    example Python code.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œç¼ºç‚¹æ˜¯å®ƒéœ€è¦æ¯”æç¤ºå·¥ç¨‹æ˜¾è‘—æ›´å¤šçš„æŠ€æœ¯ä¸“é•¿å’Œè®¡ç®—èµ„æºã€‚åœ¨[æœªæ¥çš„æ–‡ç« ](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)ä¸­ï¼Œæˆ‘å°†å°è¯•é€šè¿‡å›é¡¾å¾®è°ƒæŠ€æœ¯å¹¶åˆ†äº«ç¤ºä¾‹
    Python ä»£ç æ¥å‡å°‘è¿™ä¸ªç¼ºç‚¹ã€‚
- en: '[](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----65194dda1148--------------------------------)
    [## Fine-Tuning Large Language Models (LLMs)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----65194dda1148--------------------------------)
    [## å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰'
- en: A conceptual overview with example Python code
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¦‚å¿µæ¦‚è¿°ï¼Œå¹¶é™„æœ‰ç¤ºä¾‹ Python ä»£ç 
- en: towardsdatascience.com](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----65194dda1148--------------------------------)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----65194dda1148--------------------------------)
- en: While prompt engineering and model fine-tuning can likely handle 99% of LLM
    applications, there are cases where one must go even further.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æç¤ºå·¥ç¨‹å’Œæ¨¡å‹å¾®è°ƒå¯èƒ½èƒ½å¤Ÿå¤„ç† 99% çš„ LLM åº”ç”¨ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¿…é¡»è¿›ä¸€æ­¥æ·±å…¥ã€‚
- en: '**Level 3: Build your own LLM**'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç¬¬ä¸‰çº§ï¼šæ„å»ºä½ è‡ªå·±çš„ LLM**'
- en: The third and final way to use an LLM in practice is to [**build your own**](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9).
    In terms of model parameters, this is where you **come up with all the model parameters**
    from scratch.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ LLM çš„ç¬¬ä¸‰ç§ä¹Ÿæ˜¯æœ€åä¸€ç§å®é™…æ–¹æ³•æ˜¯[**æ„å»ºä½ è‡ªå·±çš„**](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9)ã€‚åœ¨æ¨¡å‹å‚æ•°æ–¹é¢ï¼Œè¿™æ„å‘³ç€ä½ **ä»å¤´å¼€å§‹åˆ¶å®šæ‰€æœ‰æ¨¡å‹å‚æ•°**ã€‚
- en: An LLM is primarily a product of its training data. Thus, for some applications,
    it may be necessary to curate custom, high-quality text corpora for model trainingâ€”for
    example, a medical research corpus for the development of a clinical application.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: LLM ä¸»è¦æ˜¯å…¶è®­ç»ƒæ•°æ®çš„äº§ç‰©ã€‚å› æ­¤ï¼Œå¯¹äºæŸäº›åº”ç”¨ï¼Œå¯èƒ½éœ€è¦ä¸ºæ¨¡å‹è®­ç»ƒç­–åˆ’å®šåˆ¶çš„é«˜è´¨é‡æ–‡æœ¬è¯­æ–™åº“â€”â€”ä¾‹å¦‚ï¼Œä¸ºå¼€å‘ä¸´åºŠåº”ç”¨è€Œåˆ¶å®šçš„åŒ»å­¦ç ”ç©¶è¯­æ–™åº“ã€‚
- en: The biggest upside to this approach is you can **fully customize the LLM for
    your particular use case**. This is the ultimate flexibility. However, as is often
    the case, flexibility comes at the cost of convenience.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•æœ€å¤§çš„å¥½å¤„æ˜¯ä½ å¯ä»¥**å®Œå…¨å®šåˆ¶ LLM ä»¥é€‚åº”ä½ çš„ç‰¹å®šç”¨ä¾‹**ã€‚è¿™æ˜¯**æœ€ç»ˆçš„çµæ´»æ€§**ã€‚ç„¶è€Œï¼Œæ­£å¦‚å¸¸è§çš„æƒ…å†µä¸€æ ·ï¼Œçµæ´»æ€§å¾€å¾€ä»¥ä¾¿åˆ©æ€§ä¸ºä»£ä»·ã€‚
- en: Since the **key to LLM performance is scale**, building an LLM from scratch
    requires tremendous computational resources and technical expertise. In other
    words, this isnâ€™t going to be a solo weekend project but a full team working for
    months, if not years, with a 7â€“8F budget.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº**LLMæ€§èƒ½çš„å…³é”®åœ¨äºè§„æ¨¡**ï¼Œä»é›¶æ„å»ºLLMéœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºå’ŒæŠ€æœ¯ä¸“é•¿ã€‚æ¢å¥è¯è¯´ï¼Œè¿™ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå‘¨æœ«é¡¹ç›®ï¼Œè€Œæ˜¯ä¸€ä¸ªå›¢é˜Ÿè¦å·¥ä½œæ•°æœˆç”šè‡³æ•°å¹´çš„å·¥ç¨‹ï¼Œé¢„ç®—è¾¾åˆ°7â€“8Fã€‚
- en: Nevertheless, in a [future article](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9)
    in this series, we will explore popular techniques for developing LLMs from scratch.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨è¿™ç³»åˆ—çš„[æœªæ¥æ–‡ç« ](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9)ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ä»é›¶å¼€å‘LLMçš„æµè¡ŒæŠ€æœ¯ã€‚
- en: '[](/how-to-build-an-llm-from-scratch-8c477768f1f9?source=post_page-----65194dda1148--------------------------------)
    [## How to Build an LLM from Scratch'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-build-an-llm-from-scratch-8c477768f1f9?source=post_page-----65194dda1148--------------------------------)
    [## å¦‚ä½•ä»é›¶æ„å»ºLLM'
- en: Data Curation, Transformers, Training at Scale, and Model Evaluation
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®æ•´ç†ã€å˜æ¢å™¨ã€è§„æ¨¡åŒ–è®­ç»ƒå’Œæ¨¡å‹è¯„ä¼°
- en: towardsdatascience.com](/how-to-build-an-llm-from-scratch-8c477768f1f9?source=post_page-----65194dda1148--------------------------------)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-build-an-llm-from-scratch-8c477768f1f9?source=post_page-----65194dda1148--------------------------------)
- en: Conclusion
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: While there is more than enough hype about LLMs, they are a powerful innovation
    in AI. Here, I provided a primer on what LLMs are and framed how they can be used
    in practice. The [next article](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    in this series will give a beginnerâ€™s guide to OpenAIâ€™s Python API to help jumpstart
    your next LLM use case.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å…³äºLLMçš„ç‚’ä½œè¿‡å¤šï¼Œä½†å®ƒä»¬æ˜¯AIä¸­çš„ä¸€ç§å¼ºå¤§åˆ›æ–°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘æä¾›äº†LLMæ˜¯ä»€ä¹ˆçš„å…¥é—¨çŸ¥è¯†ï¼Œå¹¶æ¡†å®šäº†å®ƒä»¬å¦‚ä½•åœ¨å®è·µä¸­ä½¿ç”¨ã€‚[ç³»åˆ—ä¸­çš„ä¸‹ä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)å°†ä¸ºOpenAIçš„Python
    APIæä¾›åˆå­¦è€…æŒ‡å—ï¼Œå¸®åŠ©å¯åŠ¨ä½ çš„ä¸‹ä¸€ä¸ªLLMç”¨ä¾‹ã€‚
- en: 'ğŸ‘‰ **More on LLMs**: [OpenAI API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    | [Prompt Engineering](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    | [Fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [Build an LLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac)
    | [Text Embeddings](/text-embeddings-classification-and-semantic-search-8291746220be)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ‘‰ **æ›´å¤šLLMä¿¡æ¯**: [OpenAI API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    | [Prompt Engineering](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    | [Fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [æ„å»ºLLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac)
    | [æ–‡æœ¬åµŒå…¥](/text-embeddings-classification-and-semantic-search-8291746220be)'
- en: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
- en: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)'
- en: Large Language Models (LLMs)
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰
- en: '[View list](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----65194dda1148--------------------------------)13
    stories![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----65194dda1148--------------------------------)13ç¯‡æ•…äº‹![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
- en: Resources
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èµ„æº
- en: '**Connect**: [My website](https://shawhintalebi.com/) | [Book a call](https://calendly.com/shawhintalebi)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**è”ç³»**: [æˆ‘çš„ç½‘ç«™](https://shawhintalebi.com/) | [é¢„çº¦ç”µè¯](https://calendly.com/shawhintalebi)'
- en: '**Socials**: [YouTube ğŸ¥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¤¾äº¤åª’ä½“**: [YouTube ğŸ¥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
- en: '**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) â˜•ï¸'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ”¯æŒ**: [è¯·æˆ‘å–å’–å•¡](https://www.buymeacoffee.com/shawhint) â˜•ï¸'
- en: '[](https://shawhin.medium.com/subscribe?source=post_page-----65194dda1148--------------------------------)
    [## Get FREE access to every new story I write'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/subscribe?source=post_page-----65194dda1148--------------------------------)
    [## å…è´¹è·å–æˆ‘æ’°å†™çš„æ¯ä¸€ç¯‡æ–°æ•…äº‹'
- en: Get FREE access to every new story I write P.S. I do not share your email with
    anyone By signing up, you will create aâ€¦
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…è´¹è·å–æˆ‘æ’°å†™çš„æ¯ä¸€ç¯‡æ–°æ•…äº‹ P.S. æˆ‘ä¸ä¼šä¸ä»»ä½•äººåˆ†äº«æ‚¨çš„ç”µå­é‚®ä»¶ æ³¨å†Œåï¼Œæ‚¨å°†åˆ›å»ºä¸€ä¸ª...
- en: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----65194dda1148--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----65194dda1148--------------------------------)
- en: '[1] Survey of Large Language Models. [arXiv:2303.18223](https://arxiv.org/abs/2303.18223)
    **[cs.CL]**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] å¤§å‹è¯­è¨€æ¨¡å‹è°ƒæŸ¥ã€‚ [arXiv:2303.18223](https://arxiv.org/abs/2303.18223) **[cs.CL]**'
- en: '[2] GPT-3 Paper. [arXiv:2005.14165](https://arxiv.org/abs/2005.14165) **[cs.CL]**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] GPT-3 è®ºæ–‡ã€‚ [arXiv:2005.14165](https://arxiv.org/abs/2005.14165) **[cs.CL]**'
- en: '[3] Radford, A., & Narasimhan, K. (2018). Improving Language Understanding
    by Generative Pre-Training. ([GPT-1 Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Radford, A., & Narasimhan, K. (2018). é€šè¿‡ç”Ÿæˆé¢„è®­ç»ƒæé«˜è¯­è¨€ç†è§£ã€‚ ([GPT-1 è®ºæ–‡](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))'
