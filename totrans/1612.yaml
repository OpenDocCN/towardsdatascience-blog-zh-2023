- en: 'Other ML Jargons: Sparse and Dense Representations of Texts for Machine Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/other-ml-jargons-sparse-and-dense-representations-of-texts-for-machine-learning-21fcd7a01410](https://towardsdatascience.com/other-ml-jargons-sparse-and-dense-representations-of-texts-for-machine-learning-21fcd7a01410)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: OTHER ML JARGONS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Brief Introduction to Vectorization and its Importance in the Context of NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)[![Nabanita
    Roy](../Images/83ab7766a28c79371ebf9517e1f273d2.png)](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)[](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)
    [Nabanita Roy](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)
    ¬∑9 min read¬∑Feb 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2550ebb25c16994eb499f8e089b474a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Compare Fibre](https://unsplash.com/@comparefibre?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Matrices and vectors are quantified information that Machine Learning(ML) algorithms
    require for learning patterns and making predictions. For applying these techniques
    to textual data as well, numeric representations of the texts are engineered to
    form matrices that hold the relevant information from those texts. The concepts
    of ‚ÄúSparsity‚Äù and ‚ÄúDensity‚Äù arrive at efficiently designing and constructing these
    matrices for all high-dimensional data processing use-cases in the world of Artificial
    Intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Significance of Vector Representations for NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Representing text data as vectors are necessary for applying Machine Learning
    techniques to make predictions, recommendations, or clusters. In NLP, the concept
    of ‚Äú*similar words occur in similar contexts‚Äù* is fundamental. Let‚Äôs see how:'
  prefs: []
  type: TYPE_NORMAL
- en: In **Text Classification** use-cases like categorizing support tickets, spam
    detection, fake news detection, and feedback sentiment analysis, texts having
    similar words are classified into a particular category.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In R**ecommendation Systems**, people with similar profile details, browsing
    history, and past orders indicate similar choices or tastes in products. This
    information is used to make recommendations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Unsupervised Clustering** looks for patterns and similar words in the texts
    to group documents and articles. Typical applications include segregating news
    articles, trend analysis, and customer segmentation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In **Information Retrieval** systems, indexed documents are matched with queries,
    sometimes in a ‚Äúfuzzy‚Äù way and the collection of matched documents is returned
    to the user. Besides, the measure of similarity is used to rank the search.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hence, capturing similarities in these vectors is a primary research area in
    the NLP domain. These vectors are projected in an N-dimensional plane and then
    patterns in these vectors in the N-dimensional space are extracted to categorize
    the texts. Sometimes, dimensionality reduction techniques are applied, like PCA
    or t-SNE. The design of the vectors controls the overall performance of text-based
    ML models and is, hence, crucial.
  prefs: []
  type: TYPE_NORMAL
- en: The vector designs are broadly classified as ‚ÄúSparse‚Äù (meaning scarcely populated)
    and ‚ÄúDense‚Äù (meaning densely populated) vectors. In this article, I have recalled
    the concepts of matrices and vectors from a mathematical perspective, and then
    discussed these two classes of vectorization techniques ‚Äî sparse vector representations
    and dense vector representations. including a demo using Scikit Learn and Gensim,
    respectively. I have also concluded this article with an overview of the applications
    and usability of these representations.
  prefs: []
  type: TYPE_NORMAL
- en: A Primer to Matrices and Vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mathematically, *a matrix is defined as a 2-dimensional rectangular array of
    numbers.* If the array has *m* rows and *n* columns, then it is a matrix of size
    *m √ó n*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a matrix has only one row OR only one column it is called a vector. A *1√ón*
    matrix or vector is a row vector (where there are *n* columns but only *1* row)
    and an *m √ó 1* matrix or vector is a column vector (where there are *m* rows but
    only *1* column). Here‚Äôs an image that clearly demonstrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a79c68118010432cfdd21fbc389d8c03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [Linear Algebra for Data Science Ep1 ‚Äî Introduction to Vectors
    and Matrices using Python](/introduction-to-vectors-and-matrices-using-python-for-data-science-e836e014eb12)'
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a [primer for scalars, vectors, and matrices](https://www.mathsisfun.com/algebra/scalar-vector-matrix.html)
    and an [Introduction to Vectors and Matrices using Python](/introduction-to-vectors-and-matrices-using-python-for-data-science-e836e014eb12)
    for Data Science.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Representations | Matrices | Vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/3b14d8e5f8e6f8b1c9e99ed0eceaadf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Henning Witzel](https://unsplash.com/@henning?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In almost all real-world cases, the count-based quantified numeric representation
    of information is **sparse in nature**, in other words, ***the numeric representation*
    *contains only a fraction that is useful to you in an ocean of numbers.***
  prefs: []
  type: TYPE_NORMAL
- en: It is because, intuitively, in a collection of documents, only words that are
    articles, prepositions, conjunctions, and pronouns are overtly used and therefore,
    have a higher frequency of occurrence. However, in a collection of sports news
    articles, the terms ‚Äòsoccer‚Äô or ‚Äòbasketball‚Äô, occurrences of which would help
    us determine which sport is the article associated with, occurs only a few times
    but is not of a very high frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we construct a vector per new article, assuming there are 50 words per
    article, the word ‚Äòsoccer‚Äô would occur about 5 times. Hence, 45 out of 50 times,
    the elements in the vector will be zero, which indicates the absence of the word
    we are focusing on. Therefore, 90% of the vector of length 50 is redundant. This
    is an example of a [**one-hot vector**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)generation.
  prefs: []
  type: TYPE_NORMAL
- en: Another typical example of sparse matrix generation is the [**Count Vectorizer**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)which
    determines how many times a word has occurred in a document. It generates a matrix
    of ‚Äúcount vectors‚Äù per document to constitute a matrix of size *d √ó v* where d
    is the number of documents and v is the number of words or vocabulary in the collection
    of documents.
  prefs: []
  type: TYPE_NORMAL
- en: '**Here‚Äôs a demonstration of how a Count Vectorizer works:**'
  prefs: []
  type: TYPE_NORMAL
- en: Below are [four different meanings of the word ‚Äòdemo‚Äô](https://www.google.com/search?q=demo&oq=demo&aqs=chrome..69i57j69i61.775j0j4&sourceid=chrome&ie=UTF-8),
    each of which represent one document ~
  prefs: []
  type: TYPE_NORMAL
- en: '**Document 1:** a demonstration of a product or technique'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document 2:** a public meeting or march protesting against something or expressing
    views on a political issue'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document 3:** record a song or piece of music to demonstrate the capabilities
    of a musical group or performer or as preparation for a full recording'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document 4:** demonstrate the capabilities of software or another product'
  prefs: []
  type: TYPE_NORMAL
- en: I used [**Scikit Learn‚Äôs CountVectorizer**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
    implementation to generate this sparse matrix for these four ‚Äúdocuments‚Äù. Below
    is the code I have used üë©‚Äçüíª
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of *_cv.toarray()* is of the numeric representation of the words
    in a **4** *√ó* **34 array** (*converted using .toarray() from the vector*)as in
    the screenshot below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/977ab67300d896810371527907ee6841.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Find this example in this Jupiter Notebook | Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: zero represents no occurrence at all (basically no information)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: anything more than zero is the number of times the word has occurred in the
    four documents (some useful and some redundant information).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 34 is the size of the vocabulary (or the total number of unique words in the
    documents), hence the shape is 4 x 34.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The simplest measure of sparsity is the fraction of the total number of zeroes
    over the total number of elements, in this case ~
  prefs: []
  type: TYPE_NORMAL
- en: Number of zeros = 93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of elements in the array = 4 *√ó* 34 *=* 136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, more than 50% of the array has no information at all (93 out of 136),
    yet it is a high-dimensional matrix that needs more memory (increased [**Space
    complexity**](https://en.wikipedia.org/wiki/Space_complexity)) and computation
    time (increased [**Time complexity**](https://en.wikipedia.org/wiki/Time_complexity)).
    If a machine learning model is fed with this high-dimensional data, it will find
    it difficult to
  prefs: []
  type: TYPE_NORMAL
- en: find patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: will be far too expensive to tune weights for all the dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it will eventually lead to latency issues during prediction time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparsity is analogous to the concept of the curse of dimensionality. Recommender
    systems and collaborative filtering techniques, count-based data representations
    including the famous TF-IDF are prone to issues related to the sparsity of textual
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Dense Representations | Matrices | Vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c64852af08fb0022efd2856c916bbdfc.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mike L](https://unsplash.com/@wheremikeat?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The way out? A representation that has more information and less redundancy,
    is mathematically defined as a matrix or a vector where most elements are non-zero.
    Such data representations are called dense matrices or dense vectors. They are
    also usually smaller in size than sparse matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of using Dense Vectors for Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Smaller the dimension, the faster and easier to optimize weights for ML models
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even though dense vectors could have smaller dimensions than sparse matrices,
    they could also be large enough to challenge computational infrastructures (imagine
    Word2Vec or BERT-based representations), but still would contain rich and useful
    information like syntactical, semantical, or morphological relationships.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They generalize relationships between textual elements better which is clearly
    demonstrated by the success of word2vec algorithms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Common NLP techniques for transforming sparse to dense representations:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[**Word2Vec**](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html):
    It is one of the most popular schools of algorithms that learns dense representation
    using shallow Neural Networks (NN) while trying to predict the probable word(s)
    and capture semantic relations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[**FastText**](https://fasttext.cc/): Another algorithm with the same objective
    using shallow NNs, except that FastText character-level n-grams. However, Word2Vec
    has been noted to work best for English but FastText is better for morphologically
    rich languages like Arabic, German, and Russian. Besides, it captures syntactic
    features better than semantic ones.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[**GloVe**](https://nlp.stanford.edu/projects/glove/): Again same! Learns dense
    representation but based on probability of co-occurrence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is a compact demo of how to obtain dense representations using [Gensim‚Äôs
    word2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html)
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Below is the dense representation of the word ‚Äòdemonstration‚Äô of length 10 (note
    the value of *vector_size* argument for *word2vec* model is 10). Note how there
    are no zeros in this numeric representation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee568a02c0a6725fdbadd9a3e87f1a71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the higher the vector size, the better the knowledge captured, especially
    semantic information. These are extremely useful for assessing text similarities
    and in unsupervised techniques for text processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, below is a snapshot of the dense vectors obtained for each word in our
    first document ‚Äú*a demonstration of a product or technique‚Äù* appended in a list,
    corresponding to the sequence of occurrence within the document*.* This time I
    did not clean and kept the texts as is, hence there are seven word-vectors in
    the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89598c769571754a487e49dc67430f96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dense vectors of length 10 were obtained for the document ‚Äò*a demonstration
    of a product or technique‚Äô showing that this numeric representation contains no
    non-zero elements |* Image Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dense vectors like these could also be pre-trained** on a large corpus, usually
    made available to be accessed online. Imagine a dictionary with, as usual, an
    index of all the unique words but their lexical meanings are replaced by pre-trained
    word vectors containing their numeric representations, quite like a **‚ÄúQuantified
    Dictionary‚Äù**. Here are two popular ‚ÄúQuantified Dictionaries‚Äù ready to be downloaded
    and applied to text processing tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Google**](https://code.google.com/archive/p/word2vec/)**: Trained using
    Google News dataset** containing about 100 billion words. The word vectors of
    size 300 are available to download from [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing).
    The vocabulary size is 3 million.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**GloVe**](https://nlp.stanford.edu/projects/glove/)**: Trained using Wikipedia
    articles** and contains 50, 100, 300, and 500-dimensional word vectors, ready
    to download from [here](https://nlp.stanford.edu/data/glove.6B.zip). The vocabulary
    size is 400k.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, one-hot vectors, word-frequency matrices (or count vectors),
    and TF-IDF scores (Sparse representations) were used for text analytics. They
    do not preserve any information about semantics. However, the modern approach
    of obtaining word embeddings using Neural Networks(like Word2Vec) or more sophisticated
    statistical approaches with normalization techniques (like GloVe) does a better
    job of retaining the ‚Äúmeanings‚Äù of words and enables us to cluster words occurring
    in similar contexts. But, they also come with complexities in terms of higher
    computation times which makes them expensive to scale (depending on the learning
    hyperparameters, especially with higher vector representation size). Besides,
    they are also harder to explain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/481dda7eb5f0a946abc59bd48fe1787e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Micha≈Ç Parzuchowski](https://unsplash.com/@mparzuchowski?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In all real-world cases, both approaches are applicable. For example, in the
    famous [Spam Classification Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset/code),
    sparse representations yield near-perfect model performances. In such cases, we
    do not need to compute dense vector embeddings to achieve better performance since
    we successfully achieve our objectives with simple and transparent approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Hence the recommended starting point for a text processing task is with [frequency-based
    Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) models which produce
    sparse vectors. For such pipelines, cleaning and wrangling the text data is pivotal.
    With the right feature and vocabulary engineering, they could be the most efficient
    in terms of speed and performance, especially when semantic information is not
    required.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Here‚Äôs the Jupyter Notebook**](https://github.com/royn5618/Medium_Blog_Codes/blob/master/Sparsity_and_Density.ipynb)
    **with the complete Pythonic demonstration of obtaining Sparse and Dense vectors
    (Word2Vec using Gensim) for the same example.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**üí° Want to know more about Matrix Designs for NLP? Here is an article for
    you to learn further:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/matrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399?source=post_page-----21fcd7a01410--------------------------------)
    [## Matrix Design for Vector Space Models in Natural Language Processing'
  prefs: []
  type: TYPE_NORMAL
- en: A brief philosophy of knowledge representation for NLP and a concise guide to
    matrix design for distributed word‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/matrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399?source=post_page-----21fcd7a01410--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**üí°Want to implement Word2Vec for Text Classification? Here is a hands-on tutorial
    on Multiclass Text Classification by learning dense representations using Keras‚Äôs
    Embedding Layer as well as Gensim‚Äôs Word2Vec algorithm:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0?source=post_page-----21fcd7a01410--------------------------------)
    [## Multiclass Text Classification Using Keras to Predict Emotions: A Comparison
    with and without Word‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: Do word embeddings add value to text classification models? Let‚Äôs find out in
    this multiclass prediction task for‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0?source=post_page-----21fcd7a01410--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**References:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://machinelearningmastery.com/sparse-matrices-for-machine-learning/](https://machinelearningmastery.com/sparse-matrices-for-machine-learning/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://kavita-ganesan.com/fasttext-vs-word2vec/#.Y-OP7XbP02w](https://kavita-ganesan.com/fasttext-vs-word2vec/#.Y-OP7XbP02w)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Thanks for visiting!*'
  prefs: []
  type: TYPE_NORMAL
- en: '**My Links:** [Medium](https://medium.com/@nroy0110) | [LinkedIn](https://www.linkedin.com/in/nabanita-roy/)
    | [GitHub](https://github.com/royn5618)'
  prefs: []
  type: TYPE_NORMAL
