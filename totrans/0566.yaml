- en: Convolution Explained — Introduction to Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/convolution-explained-introduction-to-convolutional-neural-networks-5babc47fbcaa](https://towardsdatascience.com/convolution-explained-introduction-to-convolutional-neural-networks-5babc47fbcaa)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The fundamental building block of CNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page-----5babc47fbcaa--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----5babc47fbcaa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5babc47fbcaa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5babc47fbcaa--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----5babc47fbcaa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5babc47fbcaa--------------------------------)
    ·8 min read·Dec 27, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/725d855cae28fe1f795da02e71253227.png)'
  prefs: []
  type: TYPE_IMG
- en: ”[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)"
    title=”neural network icons.” Neural network icons created by Freepik — Flaticon.
  prefs: []
  type: TYPE_NORMAL
- en: 'My recent articles have been a series on neural networks where we go from the
    simple [***perceptron***](https://medium.com/gitconnected/intro-perceptron-architecture-neural-networks-101-2a487062810c)
    to complicated architectures and how to deal with [***common problems in deep
    learning***](https://medium.com/towards-data-science/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b).
    If you are interested, feel free to check the series here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Egor Howell](../Images/e969a9f3c3357e1c80dcd0092d9a1288.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Egor Howell](https://medium.com/@egorhowell?source=post_page-----5babc47fbcaa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@egorhowell/list/neural-networks-616db722dbbb?source=post_page-----5babc47fbcaa--------------------------------)9
    stories![](../Images/66f86f14ddf20d9da9cac53dee54bae3.png)![](../Images/d968b0bd4358bb0d60bd296aef08a720.png)![](../Images/30f3f4354836e6d3d56175fe20cf6b7c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: One exciting area neural networks have made significant strides in is [***computer
    vision***](https://en.wikipedia.org/wiki/Computer_vision). Think AI for self-driving
    cars and face recognition!
  prefs: []
  type: TYPE_NORMAL
- en: However, the regular fully connected neural network that most people know about
    is not suitable for many real-life image recognition tasks. It works on the famous
    [***MNIST***](https://en.wikipedia.org/wiki/MNIST_database#:~:text=Article%20Talk,the%20field%20of%20machine%20learning.)
    dataset, but it has small images of ***28×28*** pixels.
  prefs: []
  type: TYPE_NORMAL
- en: High-definition (HD) images have ***1280×720*** pixels. That’s approximately
    ***1,000,000*** pixels, which would mean ***1,000,000 neurons*** in the input
    layer. Not to mention the millions of weights required for the hidden layers,
    rendering regular neural networks unsuitable due to the dimensional complexity.
  prefs: []
  type: TYPE_NORMAL
- en: So, what do we do?
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks!
  prefs: []
  type: TYPE_NORMAL
- en: '[***Convolutional neural networks (CNN)***](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    are the gold standard for the majority of computer vision tasks today. Instead
    of fully connected layers, they have *partially* connected layers and share their
    weights, reducing the complexity of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, for each neuron in a fully connected neural network layer, we
    would require ***10,000*** weight of an image of ***100×100*** pixels. However,
    a CNN can have only ***25*** neurons to process the same image.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we are going to dive into the fundamental building block behind
    CNNs, [***convolution***](https://en.wikipedia.org/wiki/Convolution).
  prefs: []
  type: TYPE_NORMAL
- en: Intuition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like many things in machine learning, CNNs are inspired by nature. Computer
    scientists looked at how the [***visual cortex***](https://en.wikipedia.org/wiki/Visual_cortex)inthe
    brain works and applied a similar concept to neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: The visual cortex doesn’t process all the pixels in an image at the same time.
    The biological neurons only respond to a small region in the [***receptive field***](https://en.wikipedia.org/wiki/Receptive_field).
    However, these small regions overlap leading the animal to understand the whole
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, some biological neurons with the same receptive field can only
    detect lines of different orientations, an example of [***filtering***](https://medium.com/@sumit-kr-sharma/image-filtering-in-computer-vision-ec60ec8a3e1).
    Some neurons also have larger receptive fields, meaning that higher-level neurons
    are a combination of lower-level neurons.
  prefs: []
  type: TYPE_NORMAL
- en: These biological discoveries inspired the [***neocognitron***](https://en.wikipedia.org/wiki/Neocognitron)
    in 1979\. Over time, the neocognitron became the convolutional neural network,
    which is where we are today!
  prefs: []
  type: TYPE_NORMAL
- en: What is Convolution?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The naming behind the CNN is from the [***convolution***](https://en.wikipedia.org/wiki/Convolution#:~:text=In%20mathematics%20(in%20particular%2C%20functional,is%20modified%20by%20the%20other.)
    mathematical operation, which is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9e408c33cfac8e5cf1d05eaeae61c99.png)'
  prefs: []
  type: TYPE_IMG
- en: Continuous convolution theorem. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0b04fd904fe239a320a742507aadd1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Discrete convolution theorem. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: '***f*∗*g:*** Convolution between functions, ***f*** and ***g***.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***t:*** The point where the convolution is being evaluated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***f(τ):*** The value of function ***f*** at point ***τ***.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***g(t−τ):*** The value of ***g*** shifted by ***τ*** and evaluated at ***t***.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This expression doesn’t intuitively tell us what a convolution is. Let’s break
    it down in more layman’s terms.
  prefs: []
  type: TYPE_NORMAL
- en: What a convolution does is mix two functions. In the above case, we have the
    input, ***f***, and we are sliding over some function ***g*** (known as a kernel)
    over ***f.***
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: This is a function, in our case an image, to analyze or manipulate.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Kernel**: A small matrix to apply visual effects such as blurring, sharpening,
    edge detection, etc. to the image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sliding Kernel**: The kernel is slid over the input image one pixel at a
    time computing a new pixel value.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Computing Outpu**t: The output at each pixel is calculated by multiplying
    the overlapping values of the input and kernel together and summing these products.
    Then, it’s normalized by dividing it by the number of elements in the kernel.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Result**: The output is a new image, which has been transformed by the kernel.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example Convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s go through a simple convolution example for image processing using some
    visuals.
  prefs: []
  type: TYPE_NORMAL
- en: In the diagram below, we have an input grayscale image, which is ***5x5*** pixels,
    and a ***3x3*** kernel with all ***1s*** that will cause a blurring effect (specially
    a [***box blur***](https://en.wikipedia.org/wiki/Box_blur)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab2eb9f458f2ed5e17b27b1411c48567.png)'
  prefs: []
  type: TYPE_IMG
- en: Example convolution for applying a blurring effect on a grayscale image. Diagram
    created by author.
  prefs: []
  type: TYPE_NORMAL
- en: The smaller the pixel value, the darker it is. A value of 0 is black and 255
    is white, with values in-between being somewhere on the grayscale.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If we take the middle pixel (highlighted in red), its convolution output is
    computed by multiplying each pixel value with the corresponding element in the
    kernel and summing these products. It’s then normalised by the number of elements
    in the kernel to ensure the image doesn’t get brighter or darker.
  prefs: []
  type: TYPE_NORMAL
- en: Below is the step-by-step walkthrough of this process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is then repeated for all the other pixels in the input image (only where
    the kernel fits) to produce the shown output.
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the resultant output image pixels are now a lot closer in values
    than the original image. This is because our kernel has averaged the pixel values
    in the neighborhood, creating a blur effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'This convolutional can easily be implemented in Python and show the resulting
    output image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0608af17b582be4c58fc0df0fed9a0e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of blurring an image. Plot created by author in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code used to produce this image can be found on my GitHub here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/convolution_image.py?source=post_page-----5babc47fbcaa--------------------------------)
    [## Medium-Articles/Neural Networks/convolution_image.py at main · egorhowell/Medium-Articles'
  prefs: []
  type: TYPE_NORMAL
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/convolution_image.py?source=post_page-----5babc47fbcaa--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Certain kernel structures lead to different effects on the image. Below are
    some commonly used kernels along with their effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1fce459dc8be4ee5e91e85c3e6a787cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Kernel’s and their effects. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: A full list of kernels and their operations can be found [here](https://en.wikipedia.org/wiki/Kernel_(image_processing)).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dimensionality, Stride & Padding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Output Dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may have noticed that applying our kernel led to a reduction in the dimensions
    of the image compared to its original. The formula to deduce what the output dimensions
    are is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49542c2ac79fe5b6c3b51ab0b4aff936.png)'
  prefs: []
  type: TYPE_IMG
- en: Output dimension. Equation by author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: '***H_in***​ and ***W_in***​ are the height and width of the input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***H_filter***​ and ***W_filter***​ are the height and width of the kernel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Padding_height***​ and ***Padding_width***​ are the amount of padding applied
    to the height and width of the input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Stride_height*** ​and ***Stride_width***​ are the step sizes of the kernel
    along the height and width of the input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are two concepts we haven’t discussed yet that are present in the above
    equations, [***stride***](https://deepai.org/machine-learning-glossary-and-terms/stride)
    and [***padding***](https://www.geeksforgeeks.org/cnn-introduction-to-padding/).
  prefs: []
  type: TYPE_NORMAL
- en: Stride
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stride is how many pixels we slide the kernel over the input image. A stride
    of 1 moves the kernel one pixel and a stride of 2 moves the kernel two pixels
    at a time. The sliding can be done vertically, horizontally, or even both depending
    on what we set it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Increasing the stride leads to:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Reducing the output image’s dimension size.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lower compute cost due to fewer operations.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bigger view field of view of the input image.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Skipping pixels leads to loss of information.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/e8c11ade7b1193a03d5be9b18838096e.png)'
  prefs: []
  type: TYPE_IMG
- en: Stride of 1 example. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One problem with convolutions is that we tend to *lose pixels* and information
    on the perimeter of the image. This is down to how many times they are utilised
    by the kernel. The corner pixels will only ever get used once, whilst the middle
    pixels get used a lot more.
  prefs: []
  type: TYPE_NORMAL
- en: If we keep applying our kernel multiple times, which is what happens in a CNN,
    the output images will become a lot smaller than the original input image. This
    is not good as we will lose quite a bit of information, particularly about the
    boundaries of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'A way around this problem is to “pad” the input image with ***zeros*** around
    the edges. Below is an example using our previous image from the above section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b244fc04933d33006aaa84a4c73d642b.png)'
  prefs: []
  type: TYPE_IMG
- en: Zero padding. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: The input image is now ***7x7*** pixels. Using the above formula, we can verify
    that applying a ***3x3*** kernel to this padded image will give us an output with
    dimensions ***5x5***.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a216e333bd85f1c7de7128f1dfb824ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Example calculation of output dimension when padding is applied. Equation by
    author in LaTeX.
  prefs: []
  type: TYPE_NORMAL
- en: This would increase the utilization of the pixels on the perimeter and render
    the output image having the same dimensions as the original input image.
  prefs: []
  type: TYPE_NORMAL
- en: Zero padding is an example of **same padding.** Having no padding at all is
    known **as valid padding.** There is also another type called **casual padding**,
    which is asymmetric and typically applied to time series and natural language
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t necessarily always have to pad with zeros, although that is the most
    common technique. Below is a list of some other padding strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Reflect/Mirror Padding**](https://dev.to/sally20921/padding-in-neural-network-o8m):
    We pad by reflecting the image around a boundary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Replicate Padding**](https://www.educative.io/answers/types-of-padding-in-images):
    The padding values are values of the pixels closest to the boundary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary & Further Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional neural networks are the gold standard for computer vision tasks
    today. Their main feature is utilizing the convolution mathematical operation
    that allows us to “blend” two functions together. This is used in image processing
    by applying a kernel, which is a matrix, over our image, a matrix of pixels, to
    carry out effects such as blurring and sharpening. This allows us to manipulate
    and analyze images, which is the building blocks of CNNs and how they “see” images.
  prefs: []
  type: TYPE_NORMAL
- en: Another Thing!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----5babc47fbcaa--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----5babc47fbcaa--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Connect With Me!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**YouTube**](https://www.youtube.com/@egorhowell?sub_confirmation=1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**GitHub**](https://github.com/egorhowell)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References and Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Great computerphile video on kernels*](https://www.youtube.com/watch?v=C_zFhWdM4ic)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Good article on CNNs*](/convolutional-neural-networks-explained-9cc5188c4939)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition.
    Aurélien Géron. September 2019\. Publisher(s): O’Reilly Media, Inc. ISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*More info on padding*](https://www.knowledgehut.com/blog/data-science/padding-in-cnn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
