- en: How To Prepare Data For Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-prepare-data-for-machine-learning-eb9d9973832f](https://towardsdatascience.com/how-to-prepare-data-for-machine-learning-eb9d9973832f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Preparing data for modeling is one of the first most fundamental steps in
    the data science pipeline*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@theDrewDag?source=post_page-----eb9d9973832f--------------------------------)[![Andrea
    D''Agostino](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----eb9d9973832f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eb9d9973832f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eb9d9973832f--------------------------------)
    [Andrea D''Agostino](https://medium.com/@theDrewDag?source=post_page-----eb9d9973832f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eb9d9973832f--------------------------------)
    ·8 min read·Apr 19, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4201ec0fe98ad642b92ad48c8e24ac9.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [John Moeses Bauan](https://unsplash.com/it/@johnmoeses?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
  prefs: []
  type: TYPE_NORMAL
- en: Training predictive models requires that our data be in an appropriate format.
    We can’t feed our .csv file to a model and expect it to learn to generalize correctly.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will look at how to prepare data for machine learning, starting
    from the data preparation pipeline to dividing it into training, validation and
    test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A pipeline is a series of steps that are performed to prepare data for processing
    by the machine learning model. The pipeline can vary based on the type of data
    you have available, but usually includes the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data collection**: The first step is to collect the data that you want to
    use to train the machine learning model. The data can come from different sources,
    such as CSV files, databases, APIs, IoT sensors and others.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data cleaning**: Once data is collected, it is important to clean it by deleting
    missing data, correcting errors, and removing duplicate or irrelevant data. Data
    cleansing helps improve the quality of the data used to train the machine learning
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Preprocessing**: Data preprocessing includes data normalization, data
    standardization, categorical variable coding, and outlier handling. These steps
    help prepare the data so that it can be processed by the machine learning model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data transformation**: Data transformation includes dimensionality reduction,
    feature selection, and creation of new features. These steps help reduce data
    noise and improve the machine learning model’s ability to make accurate predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: The term *pipeline* is also used by a programmatic object belonging to the Scikit-learn
    library in Python. This allows us to specify data processing steps to facilitate
    coding and reduce the occurrence of errors.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you want to read about Scikit-Learn pipelines, read the article below
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/mlearning-ai/how-to-use-sklearns-pipelines-to-optimize-your-analysis-b6cd91999be?source=post_page-----eb9d9973832f--------------------------------)
    [## How To Use Sklearn’s Pipelines To Optimize Your Analysis'
  prefs: []
  type: TYPE_NORMAL
- en: In data science and machine learning, a pipeline is a set of sequential steps
    that allows us to control the flow of…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/mlearning-ai/how-to-use-sklearns-pipelines-to-optimize-your-analysis-b6cd91999be?source=post_page-----eb9d9973832f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to collect data from sources that are reliable and relevant
    to the problem you are trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you want to create a machine learning model for predicting the
    price of houses, you need to collect data that include features such as the location
    of the house, the number of rooms, the presence of a garden, proximity to public
    transport and more. This information, just as it informs us humans of a property’s
    value, will also inform the predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to choose the source that best fits your problem and data availability.
    For example, if you want to build a machine learning model for anomaly detection
    in an IoT system, you need to collect data from sensors in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Other data sources can be
  prefs: []
  type: TYPE_NORMAL
- en: Public and private APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Direct suppliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Surveys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, it is important to evaluate the **quality of the data collected.**
    The data can contain errors, duplicates, or omissions that can negatively affect
    the quality of the machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, you should perform a data cleanup and check for missing, duplicate,
    or bad data. Data cleaning can help improve the quality of your machine learning
    model and achieve more accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an article that focuses precisely on the process of creating a dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[](/building-your-own-dataset-benefits-approach-and-tools-6ab096e37f2?source=post_page-----eb9d9973832f--------------------------------)
    [## Building Your Own Dataset: Benefits, Approach, and Tools'
  prefs: []
  type: TYPE_NORMAL
- en: The importance of building your own dataset instead of using a pre-built solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/building-your-own-dataset-benefits-approach-and-tools-6ab096e37f2?source=post_page-----eb9d9973832f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Practical example of the data collection phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A practical example of how the data collection could appear would be building
    a model for classifying flowers. In this case, **we could collect data on different
    flowers, such as petals and sepals, and record their respective lengths and widths.**
  prefs: []
  type: TYPE_NORMAL
- en: To collect this data, **we may use a mobile application** that allows us to
    take pictures of the flowers and record their characteristics. Alternatively,
    we may collect data manually using a spreadsheet or database.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the data collection method you use, it’s important to make sure
    that **the data is representative of the problem to solve**. In this case, we
    may collect data on flowers of different species and from different geographical
    regions to ensure a good variety of data.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, data preprocessing **includes normalizing or standardizing data,
    encoding categorical variables, and handling outliers.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Data normalization / standardization** is used to reduce the scale of the
    data so that they are comparable to each other. Many machine learning models,
    such as K-nearest neighbors and neural networks require data to be normalized
    or standardized to perform well.'
  prefs: []
  type: TYPE_NORMAL
- en: Standardization and normalization are terms that often are used interchangeably.
    In reality, they are not quite the same.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I invite the interested reader to read this resource [explaining the differences
    between standardization and normalization to learn more.](https://www.simplilearn.com/normalization-vs-standardization-article)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Categorical variable encoding** is used when you have variables that are
    not in numeric format, such as gender or eye color. Categorical variable encoding
    turns these variables into numbers for use by the machine learning model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Outlier handling** is used to deal with data that deviates greatly from the
    rest of the data. This data can negatively affect the machine learning model,
    so it must be managed appropriately.'
  prefs: []
  type: TYPE_NORMAL
- en: Practical example of the data preprocessing phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s think about text. Lot’s of text organized in a corpus (list of documents).
    This data should be treated first before being fed to a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 'A hypothetical pipeline could be:'
  prefs: []
  type: TYPE_NORMAL
- en: text tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: stop word removal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: stemming / lemmatization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vectorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are some of the most common steps in preprocessing text data.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data transformation can include **dimensionality reduction, feature selection,
    and the creation of new features.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality reduction** is used to reduce the number of features in the
    data. This step can be useful when you have a lot of data but are resource constrained,
    such as machine learning model processing time. One of the most used techniques
    is **PCA (Principal Component Analysis).**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature selection** is used to select the most important features of the
    data. This step can be useful when you have many features but want to use only
    a subset of them to train the machine learning model.'
  prefs: []
  type: TYPE_NORMAL
- en: Read how to do feature selection with a technique called *Boruta* in Python
  prefs: []
  type: TYPE_NORMAL
- en: '[](/feature-selection-with-boruta-in-python-676e3877e596?source=post_page-----eb9d9973832f--------------------------------)
    [## Feature Selection with Boruta in Python'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how the Boruta algorithm works for feature selection. Explanation + template
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/feature-selection-with-boruta-in-python-676e3877e596?source=post_page-----eb9d9973832f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating new features** is used to create new features from existing data.
    This step can be useful when you want to create features that weren’t present
    in the original data but can be useful for training the machine learning model.'
  prefs: []
  type: TYPE_NORMAL
- en: Practical example of the data transformation phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA is mainly used to reduce the number of useable features from a machine learning
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing PCA in Sklearn is quite easy — this code snippet captures the general
    gist of training and applying PCA in Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Splitting data into training, validation and test sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Splitting data into training, validation, and test sets is an important step
    in preparing data for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training set**: The train set is used to train the machine learning model.
    It contains the data that the model will use to learn the relationships useful
    for the prediction'
  prefs: []
  type: TYPE_NORMAL
- en: '**Validation set**: The validation set is used to evaluate the performance
    of the machine learning model during training and to test its hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '**Test set**: The test set is used to evaluate the performance of the machine
    learning model after training.'
  prefs: []
  type: TYPE_NORMAL
- en: The division into train, validation and test set is important because it allows
    you to accurately evaluate the performance of the machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: If you use all the data to train your machine learning model and then evaluate
    the model’s performance on the data itself, you risk getting a distorted picture
    of the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important techniques to correctly evaluate whether the model
    is learning or not is called **cross-validation.**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It consists in the division of the training set into folds, which are used to
    evaluate the model iteratively.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you want to read about cross-validation and how to apply it in your codebase,
    read the article below
  prefs: []
  type: TYPE_NORMAL
- en: '[](/what-is-cross-validation-in-machine-learning-14d2a509d6a5?source=post_page-----eb9d9973832f--------------------------------)
    [## What is cross-validation in machine learning'
  prefs: []
  type: TYPE_NORMAL
- en: Learn what cross-validation is — a fundamental technique for building generalizable
    models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/what-is-cross-validation-in-machine-learning-14d2a509d6a5?source=post_page-----eb9d9973832f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Practical example using train_test_split from Sklearn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sklearn provides a cool API to manage data splitting. Check this code out.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we’re splitting our data in a 60/20/20 ratio between training
    sets, validation sets, and test sets. In other words, we are using 60% of the
    data to train our model, 20% to validate the model during training, and the remaining
    20% to test the model after training.
  prefs: []
  type: TYPE_NORMAL
- en: The `random_state` parameter allows us to set the random number generation seed,
    so that we always get the same data breakdown every time we run the code.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’re printing the sizes of the training, validation, and test sets
    so we can verify that the split was done correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Using `sklearn`, splitting the data into a training set, a validation set, and
    a test set becomes quick and easy, allowing us to focus on designing and training
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we’ve looked at the steps required to prepare your data for
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how gathering data from reliable and relevant sources to the problem,
    along with data cleansing, are key to ensuring the quality of the data used to
    train the machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at data preprocessing, which includes data normalization and
    standardization, categorical variable coding and outlier handling, as well as
    data transformation, which can include dimensionality reduction, feature selection,
    and creation. of new features.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have seen the importance of dividing the data into train, validation
    and test sets to evaluate the performance of the machine learning model accurately.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data for machine learning requires close attention to detail, but
    performing data preparation pipeline steps correctly can be the difference between
    a machine learning model that works and one that doesn’t.
  prefs: []
  type: TYPE_NORMAL
- en: '**If you want to support my content creation activity, feel free to follow
    my referral link below and join Medium’s membership program**. I will receive
    a portion of your investment and you’ll be able to access Medium’s plethora of
    articles on data science and more in a seamless way.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@theDrewDag/membership?source=post_page-----eb9d9973832f--------------------------------)
    [## Join Medium with my referral link - Andrea D''Agostino'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Andrea D'Agostino (and thousands of other writers on Medium).
    Your membership fee directly…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@theDrewDag/membership?source=post_page-----eb9d9973832f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Recommended Reads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the interested, here are a list of books that I recommended for each ML-related
    topic. There are ESSENTIAL books in my opinion and have greatly impacted my professional
    career.
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: these are Amazon affiliate links. I will receive a small commission
    from Amazon for referring you these items. Your experience won’t change and you
    won’t be charged more, but it will help me scale my business and produce even
    more content around AI.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intro to ML:** [*Confident Data Skills: Master the Fundamentals of Working
    with Data and Supercharge Your Career*](https://amzn.to/3ZzKTz6)by Kirill Eremenko'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sklearn / TensorFlow:** [*Hands-On Machine Learning with Scikit-Learn, Keras,
    and TensorFlow*](https://amzn.to/433F4Nm) by Aurelien Géron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NLP:** [*Text as Data: A New Framework for Machine Learning and the Social
    Sciences*](https://amzn.to/3zvH43j)by Justin Grimmer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sklearn / PyTorch:** [*Machine Learning with PyTorch and Scikit-Learn: Develop
    machine learning and deep learning models with Python*](https://amzn.to/3Gcavve)
    by Sebastian Raschka'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Viz:** [*Storytelling with Data: A Data Visualization Guide for Business
    Professionals*](https://amzn.to/3HUtGtB) by Cole Knaflic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Useful Links (written by me)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Learn how to perform a top-tier Exploratory Data Analysis in Python:** [*Exploratory
    Data Analysis in Python — A Step-by-Step Process*](/exploratory-data-analysis-in-python-a-step-by-step-process-d0dfa6bf94ee)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learn the basics of TensorFlow:** [*Get started with TensorFlow 2.0 — Introduction
    to deep learning*](https://medium.com/towards-data-science/a-comprehensive-introduction-to-tensorflows-sequential-api-and-model-for-deep-learning-c5e31aee49fa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perform text clustering with TF-IDF in Python:** [*Text Clustering with TF-IDF
    in Python*](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
